[
  {
    "text": "Okay. Welcome everyone. So, um, today we'll be going over learning theory.",
    "start": "3470",
    "end": "9750"
  },
  {
    "text": "Um, this is, um, this used to be taught in the main lectures in- and in previous offerings.",
    "start": "9750",
    "end": "15090"
  },
  {
    "text": "Ah, this year we're gonna cover it as, ah, as a Friday section. Um, however, some of the concepts here are,",
    "start": "15090",
    "end": "22320"
  },
  {
    "text": "ah, we gonna be covering today are- are, um, important in the sense that they kind of deepen",
    "start": "22320",
    "end": "28680"
  },
  {
    "text": "your understanding of how machine learning kind of works under the covers. What are the assumptions that we're making and you know,",
    "start": "28680",
    "end": "35280"
  },
  {
    "text": "um, why do things generalize, um, and- and so forth. So here's the rough agenda for today.",
    "start": "35280",
    "end": "40710"
  },
  {
    "text": "So, ah, we're going to quickly start off with, ah, framing the learning problem and, ah,",
    "start": "40710",
    "end": "46800"
  },
  {
    "text": "we'll go deep into bias-variance, um, trade off. We'll go- we'll spend some time over there and we look at, uh,",
    "start": "46800",
    "end": "55254"
  },
  {
    "text": "some other ways where you can kind of, ah, decompose the error, ah, as approximation error and estimation error.",
    "start": "55255",
    "end": "62360"
  },
  {
    "text": "Um, we'll see what empirical, ah, risk minimization is and then we'll spend some time",
    "start": "62360",
    "end": "67820"
  },
  {
    "text": "on uniform convergence and, um, VC dimensions. So, ah, let's jump right in.",
    "start": "67820",
    "end": "75180"
  },
  {
    "text": "Right. So the, um,",
    "start": "75580",
    "end": "83400"
  },
  {
    "text": "so the assumptions under which we are going to be operating, um, for- for this lecture and in fact for most of- most of",
    "start": "83400",
    "end": "91270"
  },
  {
    "text": "the- the algorithms that we'll be covering in this course, um, is that there are two main assumptions.",
    "start": "91270",
    "end": "99130"
  },
  {
    "text": "One is that there exists a data distribution,",
    "start": "99130",
    "end": "103970"
  },
  {
    "text": "distribution D from which x y pairs are sampled.",
    "start": "107130",
    "end": "115200"
  },
  {
    "text": "So this is, ah, this makes sense in the supervised learning setting where,",
    "start": "115200",
    "end": "120219"
  },
  {
    "text": "um, you're expected to learn a mapping from x to y. But, ah, the assumption also actually holds",
    "start": "120220",
    "end": "126860"
  },
  {
    "text": "more generally even in the unsupervised, ah, setting case. The- the main assumption is that there is",
    "start": "126860",
    "end": "132495"
  },
  {
    "text": "a ge- data-generating distribution and the examples that we have in our training set,",
    "start": "132495",
    "end": "139129"
  },
  {
    "text": "and the ones we will be encountering when we test it,",
    "start": "139130",
    "end": "144370"
  },
  {
    "text": "ah, are all coming from the same distribution. Right. That's- that's like the core assumption. Um, without this, um,",
    "start": "144370",
    "end": "151435"
  },
  {
    "text": "coming up with any theory is- is- is gonna be much harder. So the assumption here is that you know, um,",
    "start": "151435",
    "end": "157219"
  },
  {
    "text": "there is some kind of a data ge-, ah, generating process. And we have a few samples from the data generating",
    "start": "157220",
    "end": "163070"
  },
  {
    "text": "process that becomes our training set and that is a finite number. Um, you can get an infinite number of samples from this data generating process,",
    "start": "163070",
    "end": "172570"
  },
  {
    "text": "and the examples that we're gonna encounter, ah, at test-time are also samples from the same process.",
    "start": "172570",
    "end": "179290"
  },
  {
    "text": "Right. That's- that's the assumption. And there is a second assumption. Um, which is that all the samples are sampled independently.",
    "start": "179290",
    "end": "188870"
  },
  {
    "text": "Um, so, um, with these two assumptions, ah, we can imagine a learning,",
    "start": "195030",
    "end": "202200"
  },
  {
    "text": "ah, the process of learning to look something like this. So, we have a set of x y pairs which we call as s. Um,",
    "start": "202200",
    "end": "215040"
  },
  {
    "text": "these are just x 1, y 1, x m y m. So we have m samples from- from- sample from the data",
    "start": "215040",
    "end": "228430"
  },
  {
    "text": "generating process and we feed this into",
    "start": "228430",
    "end": "233995"
  },
  {
    "text": "a learning algorithm and",
    "start": "233995",
    "end": "244930"
  },
  {
    "text": "the output of the learning algorithm is what we call as a hypothesis. Hypothesis, ah, is- is a function, um,",
    "start": "244930",
    "end": "253819"
  },
  {
    "text": "which accepts an input- a new input x and makes a prediction about- about y for that x.",
    "start": "253820",
    "end": "260239"
  },
  {
    "text": "So, ah, this hypothesis is sometimes also in the form of Theta hat.",
    "start": "260240",
    "end": "266294"
  },
  {
    "text": "So if we- if we restrict ourselves to a class of hypothesis. For example, ah, all possible logistic regression models of,",
    "start": "266295",
    "end": "273949"
  },
  {
    "text": "ah, of dimension n, for example, then, um, it's, you know, um, obtaining those parameters is equivalent to obtaining the hypothesis function itself.",
    "start": "273950",
    "end": "283955"
  },
  {
    "text": "So a key thing to note here is that this s is a random variable.",
    "start": "283955",
    "end": "291229"
  },
  {
    "text": "All right. This is a random variable.",
    "start": "291530",
    "end": "297175"
  },
  {
    "text": "This is a deterministic function.",
    "start": "297175",
    "end": "300470"
  },
  {
    "text": "And what happens when you feed a random variable through a deterministic function you get a? Random variable.",
    "start": "306530",
    "end": "314740"
  },
  {
    "text": "Exactly. So, um, the hypothesis that we get is also a random variable.",
    "start": "314740",
    "end": "321169"
  },
  {
    "text": "Right. So all random variables have a distribution associated with them.",
    "start": "325160",
    "end": "330880"
  },
  {
    "text": "The distribution associated with the data is the distribution of- of capital D. Um,",
    "start": "330880",
    "end": "336720"
  },
  {
    "text": "this just a fixed, ah, deterministic function. And there is a distribution associated with the,",
    "start": "336720",
    "end": "344780"
  },
  {
    "text": "um, um, with the- with the parameters that we obtain. That has a certain distribution as well.",
    "start": "344780",
    "end": "351350"
  },
  {
    "text": "In, um, in the sta- in- in a more statistical setting,",
    "start": "351350",
    "end": "358640"
  },
  {
    "text": "um, we call this an estimator. So if you take some advanced statistics courses you will call,",
    "start": "358640",
    "end": "364670"
  },
  {
    "text": "ah, what you will come across as an estimator. Here we call it a learning algorithm.",
    "start": "364670",
    "end": "369815"
  },
  {
    "text": "Right, and the distribution of Theta, um, is also called the sampling distribution.",
    "start": "369815",
    "end": "377400"
  },
  {
    "text": "And the, um, what's implied in this process is that there exists some Theta star,",
    "start": "382340",
    "end": "390904"
  },
  {
    "text": "ah, or in A star. However you want to view it which is in a sense a true parameter.",
    "start": "390904",
    "end": "399639"
  },
  {
    "text": "A true parameter that we wish,",
    "start": "399640",
    "end": "407025"
  },
  {
    "text": "ah, to be the output of the learning algorithm, ah, but of course, we never know- we never know what, ah,",
    "start": "407025",
    "end": "412900"
  },
  {
    "text": "Theta star is, um, and when, um, what we get out of the learning algorithm, um,",
    "start": "412900",
    "end": "419775"
  },
  {
    "text": "is- is going to be just a- a sample from a random, um, random variable.",
    "start": "419775",
    "end": "426449"
  },
  {
    "text": "Now, a thing to note is that this the Theta star or A star is not random.",
    "start": "426450",
    "end": "433310"
  },
  {
    "text": "It's just an unknown constant.",
    "start": "433310",
    "end": "435930"
  },
  {
    "text": "Not a- when we say it's not random it means there is no probability distribution associated with it.",
    "start": "438550",
    "end": "445639"
  },
  {
    "text": "It's just a constant which we don't know, that- that's- that's the assumption under which you operate.",
    "start": "445640",
    "end": "450724"
  },
  {
    "text": "Right. Now, um, let- let's see what's,",
    "start": "450725",
    "end": "455970"
  },
  {
    "text": "ah, let's see what's- what's- what are some properties about this Theta- Theta-hat.",
    "start": "455970",
    "end": "461415"
  },
  {
    "text": "So all the, um, all- all the- all the entities that we estimate are generally,",
    "start": "461415",
    "end": "467764"
  },
  {
    "text": "um, decorated with a hat on top, which- which indicates that it's- it's something that we estimated.",
    "start": "467765",
    "end": "473865"
  },
  {
    "text": "Um, and anything with a star is like, you know, the true or the right answer which we don't have access to it generally.",
    "start": "473865",
    "end": "481100"
  },
  {
    "text": "So any questions with this so far? Yeah. [BACKGROUND]",
    "start": "481100",
    "end": "491884"
  },
  {
    "text": "Yeah. So, yeah, this could be, uh, um, in case of like, uh, uh,",
    "start": "491885",
    "end": "497949"
  },
  {
    "text": "linear or, or logi- logistic regression or linear regression generally happens to be a vector. It could be a scalar,",
    "start": "497950",
    "end": "503470"
  },
  {
    "text": "it could be, you know, a matrix, it could be anything. Right. Uh, it's just an entity that we estimate.",
    "start": "503470",
    "end": "508750"
  },
  {
    "text": "Um, and sometimes, uh, H star can also be so generic that it,",
    "start": "508750",
    "end": "513820"
  },
  {
    "text": "it need not even be parameterized. It's just some function that you estimate. So, uh, yeah, so it could,",
    "start": "513820",
    "end": "521169"
  },
  {
    "text": "it could be a vector or a scalar or, or a matrix, it could be anything. Right? So, uh, let's see what happens when we- so in the lecture,",
    "start": "521170",
    "end": "534580"
  },
  {
    "text": "we saw, uh, this diagram for in, in the - when we were talking about bias-variance. So in case of, uh, regression,",
    "start": "534580",
    "end": "548019"
  },
  {
    "text": "[NOISE] and, um, we saw that this was one fit,",
    "start": "548020",
    "end": "562900"
  },
  {
    "text": "this was just, uh, let me use a different color,",
    "start": "562900",
    "end": "567440"
  },
  {
    "text": "straight line, and, right?",
    "start": "568140",
    "end": "580435"
  },
  {
    "text": "And we saw this as, uh, the concepts of [NOISE] sorry,",
    "start": "580435",
    "end": "586705"
  },
  {
    "text": "underfitting and this is overfit and this is like just right.",
    "start": "586705",
    "end": "596870"
  },
  {
    "text": "Right, so the concept of underfitting and overfitting are, kind of, closely related to bias and variance.",
    "start": "598350",
    "end": "604870"
  },
  {
    "text": "Uh, so this is how you would view it from the data. So this is from the data view, right?",
    "start": "604870",
    "end": "612220"
  },
  {
    "text": "Cause this is x, this is y. You know this is your data. Um, and if, if you look at, you know, um,",
    "start": "612220",
    "end": "619720"
  },
  {
    "text": "look at it from a data point of view, these are the kind of, uh, different algorithms that you might get, right?",
    "start": "619720",
    "end": "626319"
  },
  {
    "text": "However, uh, to get a more formal sense, uh, formal view into what's, what's bias and variance,",
    "start": "626320",
    "end": "631824"
  },
  {
    "text": "it's more useful to see it from the parameter view. [NOISE].",
    "start": "631825",
    "end": "641410"
  },
  {
    "text": "So let's imagine we have four different learning algorithms, right? I'm just going to plot four different.",
    "start": "641410",
    "end": "654430"
  },
  {
    "text": "And here this is the parameter space, let's say theta 1, theta 2. Let's imagine, you know,",
    "start": "654430",
    "end": "659514"
  },
  {
    "text": "uh, we have just two parameters. It's easier to visualize theta 1 and theta 2, right.",
    "start": "659515",
    "end": "667795"
  },
  {
    "text": "And this corresponds to algorithm A, algorithm B, C, and D. Right.",
    "start": "667795",
    "end": "675220"
  },
  {
    "text": "There is, there is a true theta star. Let's, let's- which is unknown, right?",
    "start": "675220",
    "end": "686360"
  },
  {
    "text": "Now, let's imagine we run through this, this process of sampling m examples running it through the algorithm,",
    "start": "690330",
    "end": "699235"
  },
  {
    "text": "obtain a theta hat, right? And then we start with a new sample- sample from",
    "start": "699235",
    "end": "705310"
  },
  {
    "text": "D run it through the algorithm we get a different theta hat, right? And theta hat is going to be different for different learning algorithms.",
    "start": "705310",
    "end": "712584"
  },
  {
    "text": "So, so let's imagine first we, we sample some data that's our training set,",
    "start": "712585",
    "end": "719755"
  },
  {
    "text": "run it through algorithm A and let's say this is the parameter we got and then we run it through",
    "start": "719755",
    "end": "726340"
  },
  {
    "text": "Algorithm B and let's say this is the parameter we got and through C here and through D over here.",
    "start": "726340",
    "end": "734425"
  },
  {
    "text": "And we're gonna repeat this, you know, second one maybe here, maybe here, here, here and so on and you repeat this process over and over and over.",
    "start": "734425",
    "end": "745750"
  },
  {
    "text": "The, the key is that the number of samples per input is m, that is fixed, right?",
    "start": "745750",
    "end": "751480"
  },
  {
    "text": "But we're gonna repeat this process and over and over and for every time we repeat it, we get a different point over here.",
    "start": "751480",
    "end": "758170"
  },
  {
    "text": "[NOISE]",
    "start": "758170",
    "end": "773769"
  },
  {
    "text": "Right? So, uh, each point each dot corresponds to a sample of size M, right?",
    "start": "773770",
    "end": "781290"
  },
  {
    "text": "The number of points is basically the number of times we repeated the experiment, right? And what we see is that",
    "start": "781290",
    "end": "788115"
  },
  {
    "text": "these dots are basically samples from the sampling distribution, right?",
    "start": "788115",
    "end": "793445"
  },
  {
    "text": "Now, the concept of, of bias and variance is kind of visible over here.",
    "start": "793445",
    "end": "801535"
  },
  {
    "text": "So if we were to classify this, now we would call this as bias and variance, right?",
    "start": "801535",
    "end": "817165"
  },
  {
    "text": "So these two are algorithms that have low bias, these two are- have high variance,",
    "start": "817165",
    "end": "824230"
  },
  {
    "text": "these two have low varia- I'm so- these two have low bias, high bias low variance, high variance.",
    "start": "824230",
    "end": "830050"
  },
  {
    "text": "So what does this mean? Uh, so bias is basically, um,",
    "start": "830050",
    "end": "836380"
  },
  {
    "text": "checking are the- is, is the sampling distribution kind of centered around the true parameter,",
    "start": "836380",
    "end": "842214"
  },
  {
    "text": "the true unknown parameter? Is it centered around the true parameter? Right? And variance is, um, is,",
    "start": "842215",
    "end": "848154"
  },
  {
    "text": "is measuring basically how dispersed the, the sampling distribution is, right?",
    "start": "848155",
    "end": "855220"
  },
  {
    "text": "So, so formally speaking, this is bias and variance and it becomes, uh, you know pretty clear when we see it in the parameter view instead of in,",
    "start": "855220",
    "end": "862990"
  },
  {
    "text": "uh, uh, uh, the data view. And essentially bias and variance are basically just properties of the first and second moments of your sampling distribution.",
    "start": "862990",
    "end": "871720"
  },
  {
    "text": "So you're asking the first moment that's the mean, is it centered around the true parameter and the second moment that variance - that's",
    "start": "871720",
    "end": "878500"
  },
  {
    "text": "literally variance of the bias-variance trade-off. Yeah. [inaudible].",
    "start": "878500",
    "end": "897100"
  },
  {
    "text": "Yeah. [inaudible]. Um, so this is, a, a diagram where I am using only two thetas just to fit,",
    "start": "897100",
    "end": "902815"
  },
  {
    "text": "you know write on a whiteboard. So you, you would imagine something that has high variance, for example,",
    "start": "902815",
    "end": "907870"
  },
  {
    "text": "this one to probably be of a much, much higher dimension, not just two, but it would still be spread out.",
    "start": "907870",
    "end": "913720"
  },
  {
    "text": "It would still have like high variance. There would be points in a higher-dimensional space, you know but more spread out.",
    "start": "913720",
    "end": "920450"
  },
  {
    "text": "Right, so, so the question was, um, the question was,",
    "start": "925140",
    "end": "930490"
  },
  {
    "text": "um, in over here we, uh, we actually had more number of thetas but, uh, here with the higher variance,",
    "start": "930490",
    "end": "936984"
  },
  {
    "text": "um, uh, plots we are having the same number of thetas. So, uh, yeah so you could imagine this to be higher-dimensional.",
    "start": "936984",
    "end": "944785"
  },
  {
    "text": "And also, different algorithms could have different, uh, bias and variance even though they have the same number of parameters.",
    "start": "944785",
    "end": "954670"
  },
  {
    "text": "For example, if you had regularization, the variance would come down, for example. Let me go over that, um, um, um,",
    "start": "954670",
    "end": "961870"
  },
  {
    "text": "a few observations that we want to make, uh, is that as we increase the size of the data,",
    "start": "961870",
    "end": "967765"
  },
  {
    "text": "every time we feed in, so if this were to, to be made bigger, if you take a bigger sample for every, um,",
    "start": "967765",
    "end": "975535"
  },
  {
    "text": "every time we learn, uh, the variance of theta hat would become small, right?",
    "start": "975535",
    "end": "983770"
  },
  {
    "text": "So if we repeat the same thing but with, with larger number of examples,",
    "start": "983770",
    "end": "988985"
  },
  {
    "text": "this would be more- all of these would be more, um, tightly concentrated, right? So, so the spread is, uh, uh,",
    "start": "988985",
    "end": "996654"
  },
  {
    "text": "so the spread is a function of how many examples we have in each, um, in each, uh, uh, iteration.",
    "start": "996655",
    "end": "1003380"
  },
  {
    "text": "Right? So, uh, as m tends to infinity, right?",
    "start": "1005160",
    "end": "1012384"
  },
  {
    "text": "The variance tends to zero, right?",
    "start": "1012385",
    "end": "1017930"
  },
  {
    "text": "If you were to collect an infinite number of samples, run it through the algorithm,",
    "start": "1017930",
    "end": "1023594"
  },
  {
    "text": "you would get some particular, um, um, theta-hat. And if you were to repeat that with an infinite number of examples",
    "start": "1023594",
    "end": "1030579"
  },
  {
    "text": "we'll always keep getting the same, um, uh, theta hat. Now the rate at which the variance goes,",
    "start": "1030580",
    "end": "1038770"
  },
  {
    "text": "goes to 0 as you increase m, is you can think of it as what's also, uh, called the statistical efficiency.",
    "start": "1038770",
    "end": "1046460"
  },
  {
    "text": "It's basically a measure of how efficient your algorithm is in squeezing out information from a given amount of data.",
    "start": "1051180",
    "end": "1058570"
  },
  {
    "text": "And if theta hat tends",
    "start": "1058570",
    "end": "1064080"
  },
  {
    "text": "to theta star as m tends to infinity,",
    "start": "1064080",
    "end": "1070455"
  },
  {
    "text": "you call such algorithms as consistent.",
    "start": "1070455",
    "end": "1074320"
  },
  {
    "text": "So, um, consistent and if",
    "start": "1076790",
    "end": "1084310"
  },
  {
    "text": "the expected value of your theta hat is equal to theta star for all m, right?",
    "start": "1084310",
    "end": "1093765"
  },
  {
    "text": "So no matter how big your, um, sample size is, if you always end up with",
    "start": "1093765",
    "end": "1099595"
  },
  {
    "text": "a sampling distribution that's centered around the true parameter, then your estimator is called an unbiased estimator. Yes.",
    "start": "1099595",
    "end": "1107549"
  },
  {
    "text": "[inaudible]. So efficiency is, is, uh, basically the rate at which, uh,",
    "start": "1107550",
    "end": "1114075"
  },
  {
    "text": "the variance drops to 0 as m tends to 0.",
    "start": "1114075",
    "end": "1121649"
  },
  {
    "text": "So for example, you may have one algorithm which, uh, which, which, where the variance is a function of 1 over M square.",
    "start": "1121650",
    "end": "1130315"
  },
  {
    "text": "Another algorithm where the variance is a function of e to the, uh, uh minus m. You,",
    "start": "1130315",
    "end": "1137310"
  },
  {
    "text": "you can have- the variance can, uh, drive down at different rates, uh, relative to m. So that's kind of captures, um, uh,",
    "start": "1137310",
    "end": "1144250"
  },
  {
    "text": "what- what's efficiency here. [NOISE] Right? Yeah.",
    "start": "1144250",
    "end": "1150820"
  },
  {
    "text": "[inaudible]",
    "start": "1150820",
    "end": "1158940"
  },
  {
    "text": "Yeah. So uh, theta-theta hat approaches um,",
    "start": "1158940",
    "end": "1164919"
  },
  {
    "text": "so um, this is a random variable here so so here's one thing to be clear about here.",
    "start": "1165770",
    "end": "1172890"
  },
  {
    "text": "This is ah, a number, a constant, and this is a constant but here this is a random variable, right?",
    "start": "1172890",
    "end": "1180660"
  },
  {
    "text": "So what we're seeing is that as m tends to infinity, theta hat,",
    "start": "1180660",
    "end": "1185760"
  },
  {
    "text": "that is the distribution, converges towards being a constant and that constant is going to be a theta star.",
    "start": "1185760",
    "end": "1192360"
  },
  {
    "text": "Which means at smaller values of m, your algorithm might be centered elsewhere,",
    "start": "1192360",
    "end": "1197895"
  },
  {
    "text": "but as you get more and more data, your sampling distribution variance reduces and also gets",
    "start": "1197895",
    "end": "1203250"
  },
  {
    "text": "centered around the true theta star eventually. Okay. So um, informally speaking,",
    "start": "1203250",
    "end": "1213045"
  },
  {
    "text": "if your algorithm has high bias, it essentially means no matter how much data or evidence you provided,",
    "start": "1213045",
    "end": "1221340"
  },
  {
    "text": "it kind of always keeps away from from theta star, right? You cannot change its mind no matter how much data you feed it,",
    "start": "1221340",
    "end": "1228225"
  },
  {
    "text": "it's never going to center itself around theta star. That's like a high biased algorithm, it's biased away from the true parameter.",
    "start": "1228225",
    "end": "1235155"
  },
  {
    "text": "And variance is, you can think of it as your algorithm that's kind of highly distracted by",
    "start": "1235155",
    "end": "1242400"
  },
  {
    "text": "the noise in the data and kind of easily get swayed away, you know, far away depending on the noise in your data.",
    "start": "1242400",
    "end": "1250034"
  },
  {
    "text": "So uh, these algorithms you would call them as those having high variance, because they can easily get swayed by noise in the data.",
    "start": "1250035",
    "end": "1258855"
  },
  {
    "text": "And as we are seeing here, bias and variance are kind of independent of each other.",
    "start": "1258855",
    "end": "1265110"
  },
  {
    "text": "You can have algorithms that have, you know, an independent amount of bias and variance in them,",
    "start": "1265110",
    "end": "1270720"
  },
  {
    "text": "you know, there is there is no um, um correlation between ah, ah bias and variance.",
    "start": "1270720",
    "end": "1277480"
  },
  {
    "text": "And one way- so the- how do we how- do we kind of fight variance?",
    "start": "1278150",
    "end": "1283665"
  },
  {
    "text": "So first let's look at how we can address variance. Yes.",
    "start": "1283665",
    "end": "1291120"
  },
  {
    "text": "[BACKGROUND]. So bias and variance are properties of the algorithm at a given size m. Right?",
    "start": "1291120",
    "end": "1303285"
  },
  {
    "text": "So these plots were from um, were from a fixed size m and for that fixed size data,",
    "start": "1303285",
    "end": "1311945"
  },
  {
    "text": "this algorithm has high bias, low variance, this algorithm has high variance and high bias and so on.",
    "start": "1311945",
    "end": "1321195"
  },
  {
    "text": "Yeah. Yeah. You can you can um, you can think of it as yeah, it, you- you assume like a fixed data size.",
    "start": "1321195",
    "end": "1327560"
  },
  {
    "text": "Right? So uh, fighting variance.",
    "start": "1327560",
    "end": "1338860"
  },
  {
    "text": "Okay. So uh, one way to kind of ah,",
    "start": "1338860",
    "end": "1343934"
  },
  {
    "text": "address if you're in a high variance situation, this will just increase the amount of data that you have,",
    "start": "1343935",
    "end": "1350955"
  },
  {
    "text": "and that would naturally just reduce the variance in your algorithm. Yes. [BACKGROUND].",
    "start": "1350955",
    "end": "1360090"
  },
  {
    "text": "That is true. So you don't know upfront what uh, whether you're you're uh, in a in a high bias or high variance um, um, scenario.",
    "start": "1360090",
    "end": "1368535"
  },
  {
    "text": "One way to kind of um- one way to kind of uh, uh,",
    "start": "1368535",
    "end": "1374025"
  },
  {
    "text": "test that is by looking at your training performance versus test performance uh,",
    "start": "1374025",
    "end": "1380985"
  },
  {
    "text": "we'll go- we'll go over that um. In fact we're gonna go into um, you know, much more detail in the main lectures of how do you identify bias and variance,",
    "start": "1380985",
    "end": "1388875"
  },
  {
    "text": "here we're just going over the concepts of what are bias and what are variance. So one way to um,",
    "start": "1388875",
    "end": "1396419"
  },
  {
    "text": "address variance is you just get more data, right? As you as you get more data, the- your sampling distributions kind of tend to get more concentrated.",
    "start": "1396420",
    "end": "1404985"
  },
  {
    "text": "Um, the other way is what's called as regularization.",
    "start": "1404985",
    "end": "1409900"
  },
  {
    "text": "So when you- when you um, add regularization like L2 regularization or L1 regularization um,",
    "start": "1412070",
    "end": "1420360"
  },
  {
    "text": "what we're effectively doing is let's say we have an algorithm with high variance maybe low bias,",
    "start": "1420360",
    "end": "1430780"
  },
  {
    "text": "low bias, high variance and you add regularization, right?",
    "start": "1432170",
    "end": "1441140"
  },
  {
    "text": "What you end up with is an algorithm that",
    "start": "1441140",
    "end": "1446480"
  },
  {
    "text": "has maybe a small bias,",
    "start": "1446480",
    "end": "1451804"
  },
  {
    "text": "you increase the bias by adding regularization but low variance.",
    "start": "1451805",
    "end": "1457770"
  },
  {
    "text": "So if what you care about is your predictive accuracy, you're probably better off trading off",
    "start": "1459460",
    "end": "1468125"
  },
  {
    "text": "high variance to some bias and getting down- reducing your your um, variance ah, to a large extent. Yeah.",
    "start": "1468125",
    "end": "1475460"
  },
  {
    "text": "[BACKGROUND]. Yeah. We'll- we- we- we're gonna uh,",
    "start": "1475460",
    "end": "1481559"
  },
  {
    "text": "uh, look into that next.",
    "start": "1481560",
    "end": "1484540"
  },
  {
    "text": "Right. So in order to kind of um, get a better understanding of this uh, let's imagine um.",
    "start": "1491690",
    "end": "1502360"
  },
  {
    "text": "So think of this as the space of hypothesis, space of, right?",
    "start": "1502820",
    "end": "1509440"
  },
  {
    "text": "So um, let's assume there is a true- there exists, this hypothesis.",
    "start": "1512510",
    "end": "1521550"
  },
  {
    "text": "Let's call it g, right? Which is like the best possible hypothesis you can think of.",
    "start": "1521550",
    "end": "1528764"
  },
  {
    "text": "By best possible hypothesis, I mean if you were to kind of take this uh, um, um,",
    "start": "1528765",
    "end": "1537420"
  },
  {
    "text": "take this hypothesis and take the expected value of the loss with respect to the data generating distribution across an infinite amount of data,",
    "start": "1537420",
    "end": "1546150"
  },
  {
    "text": "you kind of have the lowest error with this. So this is, you know, um, you know the best possible hypothesis.",
    "start": "1546150",
    "end": "1552015"
  },
  {
    "text": "And then, there is this class of hypotheses. Let's call this classes h, right?",
    "start": "1552015",
    "end": "1561225"
  },
  {
    "text": "So this, for example, can be the set of all logistic regression ah,",
    "start": "1561225",
    "end": "1567059"
  },
  {
    "text": "hypotheses, or the set of all ah, SVMs you know. So this is a class of hypotheses and what we,",
    "start": "1567060",
    "end": "1577380"
  },
  {
    "text": "what we end up with when we ah, take a finite amount of data, is some member over here, right?",
    "start": "1577380",
    "end": "1584520"
  },
  {
    "text": "So let me call h star. Okay. There is also some hypothesis in this class,",
    "start": "1584520",
    "end": "1594840"
  },
  {
    "text": "let me call it kind of h star, which is the best in-class hypotheses.",
    "start": "1594840",
    "end": "1601590"
  },
  {
    "text": "So within the set of all logistic regression functions, there exists some, you know, some model which would give you the lowest um,",
    "start": "1601590",
    "end": "1610605"
  },
  {
    "text": "lowest error if you were to ah, test it on the full data distribution, right? Um, the best possible hypothesis may not be inside ah, your ah, um,",
    "start": "1610605",
    "end": "1621510"
  },
  {
    "text": "inside your hypothesis class, it's just some, you know, some hypothesis that that's um, um,",
    "start": "1621510",
    "end": "1626865"
  },
  {
    "text": "that's conceptually something outside the class, right? Now g is not the best possible hypothesis,",
    "start": "1626865",
    "end": "1637870"
  },
  {
    "text": "h star is best in-class h,",
    "start": "1641810",
    "end": "1651195"
  },
  {
    "text": "and h hat is one you learned from finite data, right?",
    "start": "1651195",
    "end": "1660730"
  },
  {
    "text": "So, uh, we also introduce some new notation. Um, so epsilon of H is,",
    "start": "1666530",
    "end": "1676935"
  },
  {
    "text": "you will call this the risk or generalization error.",
    "start": "1676935",
    "end": "1682140"
  },
  {
    "text": "[NOISE] Right?",
    "start": "1682140",
    "end": "1688590"
  },
  {
    "text": "And it is defined to be equal to the expectation of xy sampled from",
    "start": "1688590",
    "end": "1696059"
  },
  {
    "text": "E of indicator of h of x not equal to y.",
    "start": "1696060",
    "end": "1704820"
  },
  {
    "text": "Right? So you sample examples from the data-generating process,",
    "start": "1704820",
    "end": "1711120"
  },
  {
    "text": "run it through the hypothesis, check whether it matches with,",
    "start": "1711120",
    "end": "1716190"
  },
  {
    "text": "uh, with your output and if it matches, you get a 1, if it does, uh, if it- if it, uh,",
    "start": "1716190",
    "end": "1722370"
  },
  {
    "text": "doesn't match you get a 1, if it matches you get a 0. So on average, this is, you know,",
    "start": "1722370",
    "end": "1727800"
  },
  {
    "text": "roughly speaking the fraction of all examples on which you make a mistake.",
    "start": "1727800",
    "end": "1733215"
  },
  {
    "text": "And here we are kind of thinking about this, um, from a classification point of view to check if, you know,",
    "start": "1733215",
    "end": "1739155"
  },
  {
    "text": "the class of your output matches the true class or not. But you can also extend this to,",
    "start": "1739155",
    "end": "1744360"
  },
  {
    "text": "uh, the regression setting. Uh, but that's a little harder to analyze but, you know, the generalization holds to, um uh,",
    "start": "1744360",
    "end": "1751575"
  },
  {
    "text": "the regression setting as well but we'll stick to classification for now. And we have an epsilon hat,",
    "start": "1751575",
    "end": "1759450"
  },
  {
    "text": "s of h and this is called the empirical risk.",
    "start": "1759450",
    "end": "1766659"
  },
  {
    "text": "This is the empirical risk or empirical error. And this over here is 1 over m,",
    "start": "1769790",
    "end": "1778780"
  },
  {
    "text": "i equal to 1 to m, indicator of h of x_i not equal to y_i, right?",
    "start": "1779150",
    "end": "1792159"
  },
  {
    "text": "The difference here is that here this is like an infinite process. You're- you're- you're, um,",
    "start": "1793610",
    "end": "1799139"
  },
  {
    "text": "sampling from D forever and calculating like the long-term average. Whereas this is you have a finite number that's given to you",
    "start": "1799140",
    "end": "1805980"
  },
  {
    "text": "and what's the fraction of examples on which you make - you make an error. Right. All right, uh,",
    "start": "1805980",
    "end": "1814185"
  },
  {
    "text": "before we go further, uh, there was a question of how, um, adding regularization reduces your variance.",
    "start": "1814185",
    "end": "1822030"
  },
  {
    "text": "So what you can see, um, or actually let me- let me get back to that,",
    "start": "1822030",
    "end": "1828570"
  },
  {
    "text": "um - um, in a- in a bit. Uh, so E of g and this is called the Bayes error.",
    "start": "1828570",
    "end": "1841170"
  },
  {
    "text": "[NOISE] So this essentially means if you take the best possible hypothesis,",
    "start": "1841170",
    "end": "1849525"
  },
  {
    "text": "what's the fraction, uh, what's - what's the rate at which you make errors? You know, uh, and that can be non-zero, right?",
    "start": "1849525",
    "end": "1855585"
  },
  {
    "text": "Even if you take the best possible hypothesis ever and that can still - still make some - some mistakes and - and this is also called irreducible error.",
    "start": "1855585",
    "end": "1863940"
  },
  {
    "text": "[NOISE] For example if your data-generating process you know, uh,",
    "start": "1863940",
    "end": "1874004"
  },
  {
    "text": "spits out examples where for the same x you have different y's, uh, in two different examples then no - no learning algorithm can,",
    "start": "1874005",
    "end": "1884025"
  },
  {
    "text": "you know, uh - uh, do well in such cases. That's just one- one kind of irreducible error,",
    "start": "1884025",
    "end": "1891180"
  },
  {
    "text": "they can be other kinds of irreducible, uh, errors as well. And epsilon of h_*,",
    "start": "1891180",
    "end": "1902610"
  },
  {
    "text": "epsilon of g is called the approximation error. [NOISE] So this essentially",
    "start": "1902610",
    "end": "1914640"
  },
  {
    "text": "means what is the price that we are paying for limiting ourselves to some class, right?",
    "start": "1914640",
    "end": "1920685"
  },
  {
    "text": "So it's the - it's the error between - it's the difference between the best possible error that you can get and",
    "start": "1920685",
    "end": "1927150"
  },
  {
    "text": "the best possible error you can get from h_*. Right, so this is, um, this is an attribute of the class.",
    "start": "1927150",
    "end": "1935880"
  },
  {
    "text": "So what's the cost we are paying for restricting yourself to a class? Then you have, uh,",
    "start": "1935880",
    "end": "1942540"
  },
  {
    "text": "epsilon of h_i minus epsilon h_* and this you call it the estimation error.",
    "start": "1942540",
    "end": "1949680"
  },
  {
    "text": "[NOISE] The estimation error is,",
    "start": "1949680",
    "end": "1956070"
  },
  {
    "text": "given the data that we got, the m examples that we got and we estimated,",
    "start": "1956070",
    "end": "1962655"
  },
  {
    "text": "you know, using our estimator sum h - h - h_i. What's the - what's",
    "start": "1962655",
    "end": "1973415"
  },
  {
    "text": "the - what's the error due to estimation and this is like approximation.",
    "start": "1973415",
    "end": "1984090"
  },
  {
    "text": "All right. So, this - this, uh, the error on G is the Bayes error.",
    "start": "1984160",
    "end": "1991995"
  },
  {
    "text": "The gap between this error and the best in class is the approximation error and the gap",
    "start": "1991995",
    "end": "1998100"
  },
  {
    "text": "between the best in class and the hypothesis that you end up with is called the estimation error, right?",
    "start": "1998100",
    "end": "2004100"
  },
  {
    "text": "And, uh, it's easy to see that, um, h hat is actually equal to",
    "start": "2004100",
    "end": "2013800"
  },
  {
    "text": "estimation error",
    "start": "2014890",
    "end": "2018240"
  },
  {
    "text": "plus approximation error plus irreducible error.",
    "start": "2020500",
    "end": "2029790"
  },
  {
    "text": "Right? It's pretty easy. You know, if you just add them up all these cancel out and you're just left with,",
    "start": "2033160",
    "end": "2038884"
  },
  {
    "text": "uh um, epsilon of H hat. Um, so it's - it's kind of useful to think about",
    "start": "2038885",
    "end": "2045815"
  },
  {
    "text": "your generalization error as different components. Um, some error which you just cannot,",
    "start": "2045815",
    "end": "2053764"
  },
  {
    "text": "you know, uh - uh, reduce it no matter what - no matter what hypothesis you pick no matter how much of training data you have.",
    "start": "2053765",
    "end": "2058970"
  },
  {
    "text": "There's no way you can get rid of the irreducible error. And then you make some - some decisions about",
    "start": "2058970",
    "end": "2064878"
  },
  {
    "text": "- that you're going to limit yourself to neural networks or Logistic regression or whatever and thereby you're defining a class of",
    "start": "2064879",
    "end": "2072169"
  },
  {
    "text": "all possible models and that has a cost itself and that's your approximation error. And then you are working with limited data.",
    "start": "2072170",
    "end": "2078875"
  },
  {
    "text": "And this is generally due to data, right? And with the limited data that you have and",
    "start": "2078875",
    "end": "2084110"
  },
  {
    "text": "possibly due to some nuances of your algorithm, you also have an estimation error, right?",
    "start": "2084110",
    "end": "2089429"
  },
  {
    "text": "We can further see that the estimation error can be broken down into",
    "start": "2089980",
    "end": "2095465"
  },
  {
    "text": "estimation variance and the estimation bias, right?",
    "start": "2095465",
    "end": "2103415"
  },
  {
    "text": "Um, and, uh, you can not, therefore,",
    "start": "2103415",
    "end": "2108425"
  },
  {
    "text": "write this as approximation error plus irreducible error.",
    "start": "2108425",
    "end": "2117150"
  },
  {
    "text": "And what we commonly call as bias and variance are - this we call it as variance",
    "start": "2118750",
    "end": "2125435"
  },
  {
    "text": "and this we call it as bias and this is just irreducible.",
    "start": "2125435",
    "end": "2132570"
  },
  {
    "text": "So sometimes you see the bias-variance decomposition and",
    "start": "2133000",
    "end": "2140089"
  },
  {
    "text": "sometimes you see the estimation approximation error decomposition. There are somewhat related, they're not exactly the same.",
    "start": "2140090",
    "end": "2145595"
  },
  {
    "text": "So, uh, the bias is basically why is,",
    "start": "2145595",
    "end": "2152119"
  },
  {
    "text": "you know, bias is basically trying to capture why is H hat far from a - from G, right?",
    "start": "2152119",
    "end": "2157910"
  },
  {
    "text": "Why is it staying away from G? You know. Why did our hypothesis stay away from the true hypotheses?",
    "start": "2157910",
    "end": "2163175"
  },
  {
    "text": "And that could be because your classes, uh, is- is kind of too small or it could be due to other reasons,",
    "start": "2163175",
    "end": "2169970"
  },
  {
    "text": "uh, such as, you know, um - um, as we'll see maybe regularization that kind of keeps you away from a certain- certain,",
    "start": "2169970",
    "end": "2177619"
  },
  {
    "text": "uh - uh, hypothesis, right? And the variance is generally due to it like - it's almost always due to having small data.",
    "start": "2177620",
    "end": "2184940"
  },
  {
    "text": "It could be due to other, uh - uh, reasons as well. But these are two different ways of,",
    "start": "2184940",
    "end": "2191395"
  },
  {
    "text": "uh, of decomposing your, um, your error. So now, um, if you have high bias,",
    "start": "2191395",
    "end": "2198339"
  },
  {
    "text": "how do you fight high bias? Fight high bias.",
    "start": "2198340",
    "end": "2211310"
  },
  {
    "text": "So how would you fight high bias? Any guesses. [inaudible] Yeah exactly.",
    "start": "2211310",
    "end": "2221510"
  },
  {
    "text": "So one way is to just, you know make your h bigger, right.",
    "start": "2221510",
    "end": "2230750"
  },
  {
    "text": "Make your h bigger. And also you can - you can try, you know different algorithms, um - um uh, after making your h bigger.",
    "start": "2230750",
    "end": "2239555"
  },
  {
    "text": "And what this generally means is what we saw there was regularization kind of,",
    "start": "2239555",
    "end": "2245585"
  },
  {
    "text": "you know reduces your - your, um, variance by paying a small cost in bias and over here,",
    "start": "2245585",
    "end": "2253775"
  },
  {
    "text": "you know, um. [NOISE]",
    "start": "2253775",
    "end": "2269674"
  },
  {
    "text": "So let's say your algorithm has some bias, right.",
    "start": "2269675",
    "end": "2277040"
  },
  {
    "text": "So it has a high bias and some variance, right,",
    "start": "2277040",
    "end": "2286505"
  },
  {
    "text": "and you make H bigger, your,",
    "start": "2286505",
    "end": "2292744"
  },
  {
    "text": "your class bigger right and this generally results in something which reduces your bias but also increases your variance, right?",
    "start": "2292745",
    "end": "2301560"
  },
  {
    "text": "So, with, with this picture you can, you can also see, you know, what's the effect of, um,",
    "start": "2303610",
    "end": "2309245"
  },
  {
    "text": "how, how does variance come into the picture? Now just by having a bigger class, there is a higher probability that",
    "start": "2309245",
    "end": "2315815"
  },
  {
    "text": "the hypothesis that you estimate can vary a lot, right, if you reduce your- the space of hypothesis,",
    "start": "2315815",
    "end": "2325655"
  },
  {
    "text": "you may be increasing your bias because you may be moving away from g, but you're also effectively reducing your variance, right.",
    "start": "2325655",
    "end": "2332825"
  },
  {
    "text": "So that's, that's the, the one of the, you know, trade off that you observe that any step,",
    "start": "2332825",
    "end": "2338270"
  },
  {
    "text": "you- a step that you take for example in, um, reducing bias by making it",
    "start": "2338270",
    "end": "2345140"
  },
  {
    "text": "bigger also makes it possible for your h hat to land at much, you know, a- at a wider space and increases your variance.",
    "start": "2345140",
    "end": "2352670"
  },
  {
    "text": "And if you take a step to reducing your variance by maybe making your, um, your, your class smaller,",
    "start": "2352670",
    "end": "2359660"
  },
  {
    "text": "you may end up making it smaller by being away from the end thereby increase your, your, um, um, increase your bias.",
    "start": "2359660",
    "end": "2367040"
  },
  {
    "text": "So, when you, when you add regularization, you know, th- the question, uh, uh,",
    "start": "2367040",
    "end": "2373460"
  },
  {
    "text": "somebody asked before of how does, um, in, how does adding regularization decrease the variance?",
    "start": "2373460",
    "end": "2382355"
  },
  {
    "text": "By adding regularization, you are effectively, kind of shrinking the class of hypothesis that you have.",
    "start": "2382355",
    "end": "2388190"
  },
  {
    "text": "You start penalizing those hypotheses whose Theta is very, is very large, and in a way you're kind of,",
    "start": "2388190",
    "end": "2394535"
  },
  {
    "text": "you know, shrinking the class of hypothesis that you have. So, if you shrink the class of hypothesis your,",
    "start": "2394535",
    "end": "2400295"
  },
  {
    "text": "your variance is kind of reduced because, you know, there's much smaller wiggles room for your estimator to place your h hat.",
    "start": "2400295",
    "end": "2407900"
  },
  {
    "text": "And, you know, if you shrink it by going away from, from, uh, from g, you,",
    "start": "2407900",
    "end": "2413795"
  },
  {
    "text": "you also introduce bias. That's like, you know, the bias variance, uh, um, trade off.",
    "start": "2413795",
    "end": "2419495"
  },
  {
    "text": "Any questions on this so far?",
    "start": "2419495",
    "end": "2422160"
  },
  {
    "text": "Yeah. [BACKGROUND].Yeah, you, you probably wanna think of each of these, you probably wanna think of this as a generalized version of this,",
    "start": "2431800",
    "end": "2440375"
  },
  {
    "text": "right, so here we have, like, fixed Theta 1, Theta 2, but you know, uh, because you could parameterize them into,",
    "start": "2440375",
    "end": "2446855"
  },
  {
    "text": "uh, uh, a few parameters you can kind of plot it in a metric space but that's like a more general, um, um, like a bag of hypotheses, and, you know,",
    "start": "2446855",
    "end": "2455375"
  },
  {
    "text": "but in any case in both of- both those diagrams, a point here  is one hypothesis,",
    "start": "2455375",
    "end": "2461330"
  },
  {
    "text": "a point there is one hypothesis. Here it's parameterized, here it's not parameterized. Yes. [BACKGROUND]. The thing",
    "start": "2461330",
    "end": "2475970"
  },
  {
    "text": "is we differ, d, um, so the question is, how- what if we,",
    "start": "2475970",
    "end": "2481040"
  },
  {
    "text": "we shrink it towards h star, right. The thing is, uh, we don't know where h star is, right.",
    "start": "2481040",
    "end": "2486680"
  },
  {
    "text": "If we knew it, we didn't even need to learn anything. We could just go straight there, right. So, um, yeah.",
    "start": "2486680",
    "end": "2492920"
  },
  {
    "text": "[BACKGROUND].",
    "start": "2492920",
    "end": "2505250"
  },
  {
    "text": "With regularization? So the question is, when we add regularization, are we sure that the bias is going up?",
    "start": "2505250",
    "end": "2511280"
  },
  {
    "text": "No, we, we don't know and, and this is a common scenario what happens, right. You, when you add regularization, you, you,",
    "start": "2511280",
    "end": "2518210"
  },
  {
    "text": "you reduce the variance for sure but you're very likely gonna introduce some bias in that process.",
    "start": "2518210",
    "end": "2524359"
  },
  {
    "text": "[BACKGROUND].",
    "start": "2524360",
    "end": "2529850"
  },
  {
    "text": "So if you add regularization, you're shrinking your hypothesis space in some ways. So you're kind of moving away from 2g. So you're kind of adding a little bit of bias.",
    "start": "2529850",
    "end": "2539119"
  },
  {
    "text": "You're very likely to add some bias in that process. Yes, so, it's, uh, so I, I,",
    "start": "2539120",
    "end": "2547235"
  },
  {
    "text": "I would encourage you to, you know, kind of, after this lecture to think about this a little more slowly, it's, it's,",
    "start": "2547235",
    "end": "2552454"
  },
  {
    "text": "it takes a while to kind of internalize this, the concept of bias and variance and, and, um,",
    "start": "2552455",
    "end": "2557510"
  },
  {
    "text": "uh, It's not very intuitive but, but, uh thinking about it more definitely helps.",
    "start": "2557510",
    "end": "2564440"
  },
  {
    "text": "All right, an- any other questions before we move on? [BACKGROUND].",
    "start": "2565710",
    "end": "2573050"
  },
  {
    "text": "So an example for a hypothesis class, right? So the- an example would be, um, the set of all logistic regression models, right?",
    "start": "2573050",
    "end": "2582155"
  },
  {
    "text": "And, uh, when you do gradient descent on your, you know, logistic regression class, you're kind of implicitly restricting yourself to set",
    "start": "2582155",
    "end": "2589670"
  },
  {
    "text": "up possible logistic regression models, that's kind of implicit. [BACKGROUND].",
    "start": "2589670",
    "end": "2601160"
  },
  {
    "text": "So, the h is the output of the learning algorithm, right?",
    "start": "2601160",
    "end": "2606740"
  },
  {
    "text": "So you feed and input your algorithm. Like this is not the model. This the learning algorithm like, this is,",
    "start": "2606740",
    "end": "2612109"
  },
  {
    "text": "like gradient descent for example. And the output of that is the parameters that you learned that converge to.",
    "start": "2612110",
    "end": "2619595"
  },
  {
    "text": "Right. So d- so, yeah, you, you probably don't wanna think about this as the model that you learned but this as the,",
    "start": "2619595",
    "end": "2627815"
  },
  {
    "text": "like the training process and the output of the training process is a model that you learn.",
    "start": "2627815",
    "end": "2633454"
  },
  {
    "text": "And that is a point in your, in the class of hypotheses. [BACKGROUND]. Yes, so, so,",
    "start": "2633455",
    "end": "2645980"
  },
  {
    "text": "you fix, um, that, uh, th- the class of learning models, you, you, say I'm gonna only gonna learn logistic regression models, right?",
    "start": "2645980",
    "end": "2653420"
  },
  {
    "text": "For different, different samples of data that you feed it as your training set, you're gonna get, learn a different Theta hat.",
    "start": "2653420",
    "end": "2660740"
  },
  {
    "text": "[BACKGROUND]. Yes, the- they have to be within the class of hypotheses.",
    "start": "2660740",
    "end": "2667170"
  },
  {
    "text": "All right, so let's move on.",
    "start": "2668980",
    "end": "2673140"
  },
  {
    "text": "Next, we come across this concept called empirical risk minimization. [NOISE].",
    "start": "2696620",
    "end": "2722579"
  },
  {
    "text": "ERM. So this is the Empirical Risk Minimizer.",
    "start": "2722580",
    "end": "2730660"
  },
  {
    "text": "Right. So, so the empirical risk minimizer is a learning algorithm.",
    "start": "2735560",
    "end": "2741480"
  },
  {
    "text": "Right. It is one of those kind of boxes that we drew. It is, you know, ah- so in the box",
    "start": "2741480",
    "end": "2754859"
  },
  {
    "text": "that we drew earlier as learning algorithm, right.",
    "start": "2754860",
    "end": "2764040"
  },
  {
    "text": "So the- the- the diagram that we drew earlier based on which we- we ah, reasoned everything so far,",
    "start": "2764040",
    "end": "2769635"
  },
  {
    "text": "didn't actually tell you what actually happens inside. It could be doing gradient descent, it could just do something else.",
    "start": "2769635",
    "end": "2775549"
  },
  {
    "text": "It could be, you know, some- some, you know, smart programmer who's written a whole bunch of if,",
    "start": "2775550",
    "end": "2780890"
  },
  {
    "text": "else and just returns a theta, it could be anything. Right. Uh, and no matter what kind of algorithm was used,",
    "start": "2780890",
    "end": "2787845"
  },
  {
    "text": "the- the bias-variance theory still holds. Right. Now we're going to look at, ah,",
    "start": "2787845",
    "end": "2793725"
  },
  {
    "text": "a very specific type of learning algorithms called the empirical risk minimizer.",
    "start": "2793725",
    "end": "2799330"
  },
  {
    "text": "Right. So, um, and this was feed into your algorithm and",
    "start": "2799850",
    "end": "2806520"
  },
  {
    "text": "you get h star, h hat ERM.",
    "start": "2806520",
    "end": "2818285"
  },
  {
    "text": "Right? Now, h, um, h hat ERM is equal to- what is ERM, empirical risk minimization?",
    "start": "2818285",
    "end": "2831465"
  },
  {
    "text": "It's what we've been doing so far in the course. Right? We, we tried to find",
    "start": "2831465",
    "end": "2839700"
  },
  {
    "text": "a minimizer in a class of hypotheses that minimizes the average training error.",
    "start": "2839700",
    "end": "2845800"
  },
  {
    "text": "Right. Um, so for example, um, this is trying to minimize the training error from a classification, ah, ah, perspective.",
    "start": "2856250",
    "end": "2866040"
  },
  {
    "text": "This is kind of minimizing the- or increasing the training accuracy,",
    "start": "2866040",
    "end": "2871350"
  },
  {
    "text": "which is different from what actually logistic regression did with, where we were doing the maximum likelihood or minimizing the negative log-likelihood.",
    "start": "2871350",
    "end": "2877875"
  },
  {
    "text": "It can be shown that, ah, losses like the logistic loss are - are can be well approximated by,",
    "start": "2877875",
    "end": "2883935"
  },
  {
    "text": "um, by the ERM. And, and, and this theory should- should, ah, ah, hold nonetheless. Um, All right.",
    "start": "2883935",
    "end": "2893130"
  },
  {
    "text": "So if- if we are limiting ourselves to do that class of algorithms which,",
    "start": "2893130",
    "end": "2903840"
  },
  {
    "text": "which worked by minimizing the training loss, right, um, as opposed to something that say returns a",
    "start": "2903840",
    "end": "2911250"
  },
  {
    "text": "constant all the time or- or- or does something else. If we limit ourselves to, um,",
    "start": "2911250",
    "end": "2917550"
  },
  {
    "text": "empirical risk minimizers, then we can come up with more theoretical results.",
    "start": "2917550",
    "end": "2923145"
  },
  {
    "text": "For example, uniform convergence, which we are gonna look at right now. [NOISE].",
    "start": "2923145",
    "end": "2942960"
  },
  {
    "text": "Right. So, so we're limiting ourselves to empirical risk minimizers and starting off, er, uniform convergence.",
    "start": "2942960",
    "end": "2955420"
  },
  {
    "text": "Right. So there are two central questions that we are kind of interested in.",
    "start": "2965300",
    "end": "2971085"
  },
  {
    "text": "So, ah, one question is, if we do empirical risk minimization,",
    "start": "2971085",
    "end": "2977940"
  },
  {
    "text": "that is if we just reduce the training loss, right, what- what does that say about the generalization of an effect?",
    "start": "2977940",
    "end": "2985705"
  },
  {
    "text": "So that is basically, um, e hat of h versus h. So for,",
    "start": "2985705",
    "end": "2996890"
  },
  {
    "text": "you know, consider some hypotheses. Right. And that gives you some amount of training error.",
    "start": "2996890",
    "end": "3001900"
  },
  {
    "text": "Right. What does that say about its generalization error? And that's one central question we wanna, um, um, consider.",
    "start": "3001900",
    "end": "3010675"
  },
  {
    "text": "And the second one is, how does the generalization error of our learned hypothesis",
    "start": "3010675",
    "end": "3019340"
  },
  {
    "text": "compare to the best possible generalization error in that class?",
    "start": "3019340",
    "end": "3026765"
  },
  {
    "text": "Right. Note we're- you know, we're only talking about h star and not, g um, there.",
    "start": "3026765",
    "end": "3032525"
  },
  {
    "text": "So h star is- is- is the best in class um, um. So these are- these are two central questions that we wanna- we wanna um, explore.",
    "start": "3032525",
    "end": "3042259"
  },
  {
    "text": "And for this, we're gonna use our two tools.",
    "start": "3042260",
    "end": "3046410"
  },
  {
    "text": "Right. So one is called the union bound. [NOISE] Right.",
    "start": "3047410",
    "end": "3055759"
  },
  {
    "text": "What's the union bound? Um, if we have um, k different events A_2, A_K.",
    "start": "3055760",
    "end": "3065270"
  },
  {
    "text": "Then, ah, these need not be independent.",
    "start": "3065270",
    "end": "3070110"
  },
  {
    "text": "Independent. Then the probability of A_1 union A_2 union A_k,",
    "start": "3073750",
    "end": "3083615"
  },
  {
    "text": "is less than equal to the sum of-",
    "start": "3083615",
    "end": "3088650"
  },
  {
    "text": "If this looks trivial, it is trivial. It's- it's um, it's probably one of the axioms in- in- in your,",
    "start": "3097840",
    "end": "3104240"
  },
  {
    "text": "ah, undergrad probability class. But the, the probability of any one of these events happening",
    "start": "3104240",
    "end": "3110330"
  },
  {
    "text": "is less than or equal to the sum of the probabilities of, ah, each of them, ah, happening.",
    "start": "3110330",
    "end": "3117140"
  },
  {
    "text": "Right. And then we have a second tool. Right. It's called the Hoeffding's inequality.",
    "start": "3117140",
    "end": "3129320"
  },
  {
    "text": "[NOISE].",
    "start": "3129320",
    "end": "3136760"
  },
  {
    "text": "We're only going to state the- the inequality here, ah, there is ah, um, a supplemental notes on the website that actually proves the Hoeffding inequality.",
    "start": "3136760",
    "end": "3145130"
  },
  {
    "text": "You can, ah, go through that, um, but here we're only going to state the result.",
    "start": "3145130",
    "end": "3150230"
  },
  {
    "text": "In fact, throughout this session, we are going to state results. We're not gonna prove anything. Um, so, ah, let Z_1, Z_2, Z_m,",
    "start": "3150230",
    "end": "3165635"
  },
  {
    "text": "be sampled from some Bernoulli distribution of parameter phi.",
    "start": "3165635",
    "end": "3171875"
  },
  {
    "text": "And let's call well, phi hat to be the average of them,",
    "start": "3171875",
    "end": "3180299"
  },
  {
    "text": "m of z_i, and let there be a Gamma greater than zero,",
    "start": "3181900",
    "end": "3193895"
  },
  {
    "text": "which we call it as the margin. So the Hoeffding Inequality basically says,",
    "start": "3193895",
    "end": "3200434"
  },
  {
    "text": "the probability that the absolute difference between",
    "start": "3200435",
    "end": "3207140"
  },
  {
    "text": "the estimated phi parameter and the true phi parameter is greater than some margin,",
    "start": "3207140",
    "end": "3216500"
  },
  {
    "text": "can be bounded by 2 times the exponential",
    "start": "3216500",
    "end": "3222710"
  },
  {
    "text": "of minus 2 gamma square m. Right?",
    "start": "3222710",
    "end": "3231300"
  },
  {
    "text": "Not very obvious but you know, you can, you can show this. What, what it's basically saying,",
    "start": "3231520",
    "end": "3236900"
  },
  {
    "text": "is there is some- there is some- some ber- ah, parameter between 0 and 1 of a Bernoulli distribution.",
    "start": "3236900",
    "end": "3246019"
  },
  {
    "text": "The fact that it is between 0 and 1 means it's- it's bounded. And- and that's a key requirement for,",
    "start": "3246020",
    "end": "3253460"
  },
  {
    "text": "ah, the Hoeffding's inequality. And now, we take samples from this Bernoulli distribution,",
    "start": "3253460",
    "end": "3258905"
  },
  {
    "text": "and the estimator for this is basically- and these are just 0s or 1s.",
    "start": "3258905",
    "end": "3264740"
  },
  {
    "text": "Z- Z- each of the Z is either a 0 or 1. The sample of 0 or a 1 with probability, um, um,",
    "start": "3264740",
    "end": "3270485"
  },
  {
    "text": "Phi, and the estimator is basically just the averages of your samples.",
    "start": "3270485",
    "end": "3275955"
  },
  {
    "text": "Right. And, um, the absolute difference between the estimated value and the true value,",
    "start": "3275955",
    "end": "3283580"
  },
  {
    "text": "the probability that this difference becomes greater than some margin Gamma is bounded by this expression.",
    "start": "3283580",
    "end": "3292085"
  },
  {
    "text": "Right. So there are a lot of things happening here. So you probably want to, um, um, you know, slowly think through this.",
    "start": "3292085",
    "end": "3297440"
  },
  {
    "text": "So this is a margin. All right.",
    "start": "3297440",
    "end": "3303920"
  },
  {
    "text": "And this is like- basically like the deviation of the error.",
    "start": "3303920",
    "end": "3309309"
  },
  {
    "text": "[NOISE] Right. Um, the absolute value of how- how- how far away",
    "start": "3309310",
    "end": "3316040"
  },
  {
    "text": "your estimated values from- from the true. And you'd like it to be small- closer.",
    "start": "3316040",
    "end": "3322460"
  },
  {
    "text": "So you- you- you probably want, ah, your -your Phi hat and phi, to be not more than,",
    "start": "3322460",
    "end": "3328220"
  },
  {
    "text": "I don't know, 0.001. Right. So in which case, if the absolute value between,",
    "start": "3328220",
    "end": "3334330"
  },
  {
    "text": "ah, ah, the estimated and, um, the true parameter is greater than 0.01,",
    "start": "3334330",
    "end": "3340744"
  },
  {
    "text": "if that's the margin your- that you're interested in. Then this, ah, the Hoeffding's inequality proves",
    "start": "3340745",
    "end": "3346940"
  },
  {
    "text": "that if you were to repeat this process over and over and over, the number of times phi hat is going to be",
    "start": "3346940",
    "end": "3355310"
  },
  {
    "text": "great- is going to be farther than 0.001 from the true parameter, it's going to be less than this expression,",
    "start": "3355310",
    "end": "3361355"
  },
  {
    "text": "which is a function of m. Right. And that is- you- you- you can kind of, ah, believe it because as m increases, this becomes smaller,",
    "start": "3361355",
    "end": "3369350"
  },
  {
    "text": "which means the probability of, um, your estimate deviating more than a certain margin only reduces as you increase m. Right.",
    "start": "3369350",
    "end": "3378635"
  },
  {
    "text": "So this is Hoeffding's inequality and we're gonna use this. [inaudible].",
    "start": "3378635",
    "end": "3387200"
  },
  {
    "text": "Oh, yeah. Questions? [inaudible].",
    "start": "3387200",
    "end": "3396125"
  },
  {
    "text": "Not, so, so the question is, is h star, uh, the limit of h_r as M goes to infinity?",
    "start": "3396125",
    "end": "3403339"
  },
  {
    "text": "Uh, it is h star in, in the limit as M goes to infinity,",
    "start": "3403340",
    "end": "3409444"
  },
  {
    "text": "if it is a consistent estimator, right? So we, we, we went over the concept of consistency.",
    "start": "3409444",
    "end": "3415550"
  },
  {
    "text": "Given infinite data, will you eventually get to the right answer? And if your estimator is not consistent,",
    "start": "3415550",
    "end": "3420800"
  },
  {
    "text": "then it will- it need not be. So, uh, in general h hat need not converge to h star as you get an infinite amount of data.",
    "start": "3420800",
    "end": "3427790"
  },
  {
    "text": "[NOISE] All right? So, uh, now we wanna use, um, uh,",
    "start": "3427790",
    "end": "3434480"
  },
  {
    "text": "these tools, tool 1 and tool 2 to answer our- like the central questions.",
    "start": "3434480",
    "end": "3440240"
  },
  {
    "text": "[NOISE] Any other questions?",
    "start": "3440240",
    "end": "3445955"
  },
  {
    "text": "Yeah. [BACKGROUND]",
    "start": "3445955",
    "end": "3458420"
  },
  {
    "text": "This is a more limited version of Hoeffding's inequality and yes, uh, if we limit ourselves to a Bernoulli variable, uh,",
    "start": "3458420",
    "end": "3465140"
  },
  {
    "text": "ba- um, which has some parameter phi and you take samples from it. And you construct an estimator which",
    "start": "3465140",
    "end": "3473120"
  },
  {
    "text": "is the average of th- the samples of the 0s and 1s, then, um, this inequality holds.",
    "start": "3473120",
    "end": "3480215"
  },
  {
    "text": "That's- thi- this inequality is called the Hoeffding's inequality. Yes.",
    "start": "3480215",
    "end": "3489440"
  },
  {
    "text": "[BACKGROUND] So if you're, um, in general, there, there are- there are- there",
    "start": "3489440",
    "end": "3497960"
  },
  {
    "text": "is this class of algorithms called maximum likelihood algorithms, maximum likelihood estimators and a pure",
    "start": "3497960",
    "end": "3503480"
  },
  {
    "text": "maximum likelihood estimator is generally consistent. If you include regularization,",
    "start": "3503480",
    "end": "3509270"
  },
  {
    "text": "then it need not be- it need not be, uh, uh, uh, consistent though,",
    "start": "3509270",
    "end": "3515704"
  },
  {
    "text": "uh, I'm not very sure about that. I'm not very sure about that. [NOISE] Yeah, sure.",
    "start": "3515705",
    "end": "3522500"
  },
  {
    "text": "Yeah. So basically like- if you think about a neural net where you have something that's completely",
    "start": "3522500",
    "end": "3527690"
  },
  {
    "text": "[inaudible] neural net is not always consistent.",
    "start": "3527690",
    "end": "3548060"
  },
  {
    "text": "Yeah. So the- basically, um, um, um, I know for the mic, uh,",
    "start": "3548110",
    "end": "3554735"
  },
  {
    "text": "wha- what, what, what he responded was, um, if you have an algorithm like a neural net which is, um,",
    "start": "3554735",
    "end": "3561170"
  },
  {
    "text": "which is non-convex, you may actually not end up with the same, uh, uh, result even if you, uh, um,",
    "start": "3561170",
    "end": "3567575"
  },
  {
    "text": "increase, um, increase like, uh, the number of, um, uh- though I would probably call the,",
    "start": "3567575",
    "end": "3574505"
  },
  {
    "text": "uh, uh, the fact- I, I would probably think of the non-convexity to be part of an estimation bias,",
    "start": "3574505",
    "end": "3581405"
  },
  {
    "text": "um, because you could in theory always find like the global minima of a neural network.",
    "start": "3581405",
    "end": "3586610"
  },
  {
    "text": "It's just that there's some bias in our estimator that we are using gradient descent and we cannot solve it.",
    "start": "3586610",
    "end": "3591540"
  },
  {
    "text": "Okay. So now, uh, let's- let's use these two tools, uh, and for that, uh, we're gonna start [NOISE] how do we look at this diagram, right?",
    "start": "3594610",
    "end": "3605165"
  },
  {
    "text": "So, [NOISE] so over here,",
    "start": "3605165",
    "end": "3614435"
  },
  {
    "text": "um, we have hypotheses. [NOISE] Here we have error,",
    "start": "3614435",
    "end": "3620990"
  },
  {
    "text": "[NOISE] and let's think of this.",
    "start": "3620990",
    "end": "3626180"
  },
  {
    "text": "[NOISE] There's actually one,",
    "start": "3626180",
    "end": "3636260"
  },
  {
    "text": "one curve which I'm trying to make it thick and probably make it look like multiple curves, this is just one curve and this we will call it as.",
    "start": "3636260",
    "end": "3644809"
  },
  {
    "text": "[NOISE] So this is the generalization risk or the,",
    "start": "3644810",
    "end": "3653645"
  },
  {
    "text": "uh, uh, the generalization error of every possible hypothesis,",
    "start": "3653645",
    "end": "3659210"
  },
  {
    "text": "uh, in our class, right? So pick one hypothesis that's gonna be somewhere on this axis,",
    "start": "3659210",
    "end": "3666890"
  },
  {
    "text": "calculate the generalization error, not the empirical, the generalization error",
    "start": "3666890",
    "end": "3673830"
  },
  {
    "text": "and- no that's the height of that curve, right? And we also have something like this.",
    "start": "3673830",
    "end": "3681430"
  },
  {
    "text": "[NOISE] Right?",
    "start": "3681430",
    "end": "3689155"
  },
  {
    "text": "So this dotted line now corresponds to sum each of s_h.",
    "start": "3689155",
    "end": "3699960"
  },
  {
    "text": "Now let's, let's sample a set of m examples and calculate",
    "start": "3701110",
    "end": "3707840"
  },
  {
    "text": "the empirical error of all our hypotheses in our class and plot it as a curve, right?",
    "start": "3707840",
    "end": "3715445"
  },
  {
    "text": "Any questions on what, what, what these two are? Yeah. [BACKGROUND] It need not meet.",
    "start": "3715445",
    "end": "3721790"
  },
  {
    "text": "I'm, I'm just, uh, uh, in fact, thi- this is very likely not even a straight line, you're just thinking of all, all possible hypotheses.",
    "start": "3721790",
    "end": "3728150"
  },
  {
    "text": "It may not be convex. Um, this just to, to, um, get some ideas, um,",
    "start": "3728150",
    "end": "3734585"
  },
  {
    "text": "um, get, get better intuitions on some of these ideas. Yes. [BACKGROUND] So, uh, the black line,",
    "start": "3734585",
    "end": "3741230"
  },
  {
    "text": "the thick black line is the generalization error of all your hypotheses, right?",
    "start": "3741230",
    "end": "3747200"
  },
  {
    "text": "And let's say you sample some, some, some data, right? Let's call it S. On that sample,",
    "start": "3747200",
    "end": "3753335"
  },
  {
    "text": "you have training error for all possible hypotheses, right? [NOISE] We haven't not learned anything, right?",
    "start": "3753335",
    "end": "3759395"
  },
  {
    "text": "It's, it's, uh, uh, this is the generalization error and this is the empirical error for the given S, right?",
    "start": "3759395",
    "end": "3767704"
  },
  {
    "text": "Now, uh, in order to app- apply Hoeffding's Inequality here, right?",
    "start": "3767705",
    "end": "3779075"
  },
  {
    "text": "So let's consider some h_i, right?",
    "start": "3779075",
    "end": "3783630"
  },
  {
    "text": "This is some hypothesis. We- we don't know. So we start with some random hypotheses, right?",
    "start": "3785740",
    "end": "3792320"
  },
  {
    "text": "And- so by starting with some hypotheses like think of this as you start with some parameter, [NOISE] right?",
    "start": "3792320",
    "end": "3801740"
  },
  {
    "text": "And, uh, let's- right.",
    "start": "3801740",
    "end": "3810000"
  },
  {
    "text": "So the height of this line up to the,",
    "start": "3810790",
    "end": "3816185"
  },
  {
    "text": "the thick black curve is basically, um, the generalization error of h_i is the height to the thick black curve.",
    "start": "3816185",
    "end": "3829714"
  },
  {
    "text": "So let me call this Epsilon of h_i, right?",
    "start": "3829715",
    "end": "3836720"
  },
  {
    "text": "And the height to the dotted curve until here. And this is Epsilon hat of h_i.",
    "start": "3836720",
    "end": "3848060"
  },
  {
    "text": "I'm gonna ignore the S for now, right?",
    "start": "3848060",
    "end": "3851040"
  },
  {
    "text": "And this corresponds to like the, the sample that we obtain.",
    "start": "3854020",
    "end": "3859205"
  },
  {
    "text": "Now one thing, ah, you can, you can check is that the expected value of- [NOISE]",
    "start": "3859205",
    "end": "3885110"
  },
  {
    "text": "where the expectation is with respect to the data's sample. So what this means is that, ah, for one particular sample you,",
    "start": "3885110",
    "end": "3892550"
  },
  {
    "text": "ah, -this is the generalization error you got. Take another set of samples, that curve might look som- some,",
    "start": "3892550",
    "end": "3898280"
  },
  {
    "text": "you know, some other way, and, you know, the height of the dotted line would be there. So in general on average,",
    "start": "3898280",
    "end": "3904279"
  },
  {
    "text": "if you sum average across all possible training samples that you can get, ah, the,",
    "start": "3904280",
    "end": "3909680"
  },
  {
    "text": "the expected value of the height to the dotted line is gonna be the height to the,",
    "start": "3909680",
    "end": "3916444"
  },
  {
    "text": "the, the thick line. Right? That's, that's justified. Now here if you apply Hoeffding's inequality,",
    "start": "3916445",
    "end": "3922295"
  },
  {
    "text": "you basically get probability of absolute difference between the empirical error versus",
    "start": "3922295",
    "end": "3932210"
  },
  {
    "text": "the generalization error to be greater than Gamma is",
    "start": "3932210",
    "end": "3939380"
  },
  {
    "text": "less than equal to 2 minus 2 Gamma square.",
    "start": "3939380",
    "end": "3947119"
  },
  {
    "text": "And- this is basically, you know, Hoeffding's inequality. We have right here except in place of phi and phi hat,",
    "start": "3947120",
    "end": "3955325"
  },
  {
    "text": "we have the true generalization error, and the empirical error.",
    "start": "3955325",
    "end": "3961115"
  },
  {
    "text": "Any questions on this so far? So what we are saying is essentially",
    "start": "3961115",
    "end": "3969050"
  },
  {
    "text": "the gap between the generalization error and the empirical error. All right.",
    "start": "3969050",
    "end": "3976730"
  },
  {
    "text": "Right. The gap being greater than",
    "start": "3976730",
    "end": "3982160"
  },
  {
    "text": "some margin Gamma is gonna be bounded by this expression.",
    "start": "3982160",
    "end": "3989434"
  },
  {
    "text": "Right? So loosely speaking what this means is, as we increase the size M,",
    "start": "3989435",
    "end": "3997520"
  },
  {
    "text": "if our training is up- if we plot the set of all dotted lines for a larger M,",
    "start": "3997520",
    "end": "4004120"
  },
  {
    "text": "they are gonna be more concentrated around the black line. Does that make sense? So, so take a moment and think about it.",
    "start": "4004120",
    "end": "4013869"
  },
  {
    "text": "This dotted line corresponds to S of some particular size M. We could take another sample of,",
    "start": "4013870",
    "end": "4022224"
  },
  {
    "text": "you know, a fixed set of examples, and that might look kinda something like this.",
    "start": "4022225",
    "end": "4029000"
  },
  {
    "text": "Right. And take another sample of size M, and that might look something like [NOISE] this.",
    "start": "4029340",
    "end": "4037000"
  },
  {
    "text": "Now- and now, consider the set of all deviations from the black line",
    "start": "4037000",
    "end": "4044830"
  },
  {
    "text": "to every possible dotted line along the vertical line of HI. Right? Now this gap is greater than some margin",
    "start": "4044830",
    "end": "4054520"
  },
  {
    "text": "Gamma with probability less than this term over here.",
    "start": "4054520",
    "end": "4060700"
  },
  {
    "text": "Right? So, so it essentially means that if you start plotting dotted lines with a bigger M, right,",
    "start": "4060700",
    "end": "4068050"
  },
  {
    "text": "where the set of all those dotted lines correspond to a bigger M, they are gonna be much more tightly",
    "start": "4068050",
    "end": "4075310"
  },
  {
    "text": "concentrated around the true generalization of that, of that edge.",
    "start": "4075310",
    "end": "4079700"
  },
  {
    "text": "That make sense? And you're basically applying Hoeffding's inequality to this gap over here instead of some phi.",
    "start": "4080490",
    "end": "4088375"
  },
  {
    "text": "That's basically what you're doing. Right? Now, that's good.",
    "start": "4088375",
    "end": "4094885"
  },
  {
    "text": "But there's a problem here. The problem here is that, we started with some hypotheses,",
    "start": "4094885",
    "end": "4101395"
  },
  {
    "text": "and then averaged across all possible data that you could sample. But in practice, this is useless.",
    "start": "4101395",
    "end": "4107125"
  },
  {
    "text": "Because in practice we start with some data, and run the empirical risk minimizer to find the lowest H for that particular data.",
    "start": "4107125",
    "end": "4115554"
  },
  {
    "text": "Right? And when you, when, when- which means that H,",
    "start": "4115555",
    "end": "4120730"
  },
  {
    "text": "and the data that you have are not really independent. Right? You, you chose the H to minimize, ah,",
    "start": "4120730",
    "end": "4126640"
  },
  {
    "text": "minimize the risk for the empirical risk for th- the particular data that you are given in the first place.",
    "start": "4126640",
    "end": "4133330"
  },
  {
    "text": "Right? So to, to fix this, what we wanna do is basically extend this result that we got",
    "start": "4133330",
    "end": "4143469"
  },
  {
    "text": "to account for all H. Right.",
    "start": "4143470",
    "end": "4151089"
  },
  {
    "text": "Now if we want to get a bound on the gap between the probabilistic bound,",
    "start": "4151090",
    "end": "4159099"
  },
  {
    "text": "and the gap between the generalization error, and the empirical error for all H. You know what's that bound gonna look like.",
    "start": "4159100",
    "end": "4172075"
  },
  {
    "text": "Right. And this is basically called uniform, uniform convergence. This is- I'll just call uniform convergence because we are trying to, ah,",
    "start": "4172075",
    "end": "4180295"
  },
  {
    "text": "we are trying to see how the risk curve converges uniformly to the generalization risk curve,",
    "start": "4180295",
    "end": "4187060"
  },
  {
    "text": "or how the empirical risk curve uniformly converges to the generalization risk curve.",
    "start": "4187060",
    "end": "4192415"
  },
  {
    "text": "And, ah, it's, ah, that's called uniform convergence which you can apply to functions in general, but here we are applying to the risk curves across our hypotheses.",
    "start": "4192415",
    "end": "4202090"
  },
  {
    "text": "And we can show- I'm gonna, ah, just, um, skip the math.",
    "start": "4202090",
    "end": "4207370"
  },
  {
    "text": "So, um, this we showed using Hoeffding's inequality, and you can apply the union bound for unioning across all H.",
    "start": "4207370",
    "end": "4217705"
  },
  {
    "text": "Except we can- first we're going to limit ourselves to, um- all right.",
    "start": "4217705",
    "end": "4224845"
  },
  {
    "text": "So let me start over. So we got this bound for a fixed H. Right?",
    "start": "4224845",
    "end": "4230725"
  },
  {
    "text": "But we are interested in getting the bound for any possible H. Right?",
    "start": "4230725",
    "end": "4236410"
  },
  {
    "text": "So that's our next step. Right? And the way we're gonna, gonna extend this pointwise result to across all of them,",
    "start": "4236410",
    "end": "4243865"
  },
  {
    "text": "is gonna look different for two possible cases. One is a case of a finite hypothesis class,",
    "start": "4243865",
    "end": "4249415"
  },
  {
    "text": "and the other case is gonna be the case for infinite hypothesis classes. So what does it look like?",
    "start": "4249415",
    "end": "4255460"
  },
  {
    "text": "So, [NOISE]",
    "start": "4255460",
    "end": "4267790"
  },
  {
    "text": "so let's first consider finite hypothesis classes.",
    "start": "4267790",
    "end": "4272120"
  },
  {
    "text": "So first we're gonna assume that the class of H has a finite number of hypotheses.",
    "start": "4279420",
    "end": "4288804"
  },
  {
    "text": "The result by itself is not very useful, but it's gonna be like a building block for, for the, for the other case.",
    "start": "4288805",
    "end": "4295195"
  },
  {
    "text": "So let's assume that the number of hypotheses in this class is some number K. Right?",
    "start": "4295195",
    "end": "4303610"
  },
  {
    "text": "Ah, we can show that- I'm not gonna go over the,",
    "start": "4303610",
    "end": "4309235"
  },
  {
    "text": "the derivation, but I'm just gonna, um, write out the result. It's, it's pretty intuitive.",
    "start": "4309235",
    "end": "4314889"
  },
  {
    "text": "So basically what we do is, ah, we apply the union bound for all K hypotheses,",
    "start": "4314890",
    "end": "4320650"
  },
  {
    "text": "and we end up just multiplying that by a factor of K. Right? So what we get is the probability that there exists some hypotheses",
    "start": "4320650",
    "end": "4335005"
  },
  {
    "text": "in H such that the empirical error",
    "start": "4335005",
    "end": "4342565"
  },
  {
    "text": "minus generalization error is greater than Gamma,",
    "start": "4342565",
    "end": "4352465"
  },
  {
    "text": "is less than equal to K times,",
    "start": "4352465",
    "end": "4356989"
  },
  {
    "text": "K times the probability of any 1 which is equal to K times,",
    "start": "4357600",
    "end": "4362664"
  },
  {
    "text": "ah, 2 minus xp 2 Gamma square M. And this we flip it over,",
    "start": "4362664",
    "end": "4374424"
  },
  {
    "text": "we negate it, and we get the probability that for all hypotheses in our class,",
    "start": "4374424",
    "end": "4382579"
  },
  {
    "text": "the empirical risk minus generalization risk is less than Gamma,",
    "start": "4384540",
    "end": "4394225"
  },
  {
    "text": "is gonna be greater than equal to 1 minus 2K,",
    "start": "4394225",
    "end": "4403990"
  },
  {
    "text": "minus 2 Gamma square. Okay. So with probability at least 1 minus,",
    "start": "4403990",
    "end": "4413755"
  },
  {
    "text": "you know this expression, which we can, we can call this Delta with probability at least so much.",
    "start": "4413755",
    "end": "4423475"
  },
  {
    "text": "For all hypotheses, our margin is gonna be less than some Gamma.",
    "start": "4423475",
    "end": "4430210"
  },
  {
    "text": "Right? This is, this is just, um, Hoeffding's inequality plus union bound,",
    "start": "4430210",
    "end": "4437860"
  },
  {
    "text": "and just negate the two sides, you get this. And you, you can go with this slowly, um, um,",
    "start": "4437860",
    "end": "4443515"
  },
  {
    "text": "you know later from the notes, the notes goes over this, um, in more detail.",
    "start": "4443515",
    "end": "4448809"
  },
  {
    "text": "Right? Now, basically now what we have is, you know, now let's let Delta equals to",
    "start": "4448810",
    "end": "4457915"
  },
  {
    "text": "K exp minus 2 Gamma squared M. So we basically now have,",
    "start": "4457915",
    "end": "4465385"
  },
  {
    "text": "um, a relation between Delta which is like the probability of error.",
    "start": "4465385",
    "end": "4474079"
  },
  {
    "text": "By here, um, ah, by error I mean that the, um, empirical risk, and the generalization risk are farther than some, some margin.",
    "start": "4477180",
    "end": "4487960"
  },
  {
    "text": "And Gamma is called the margin of error.",
    "start": "4487960",
    "end": "4491570"
  },
  {
    "text": "And M is your sample size.",
    "start": "4494070",
    "end": "4497750"
  },
  {
    "text": "So, so what this basically tells us, um, if your algorithm is the empirical risk minimizer,",
    "start": "4500970",
    "end": "4509130"
  },
  {
    "text": "it could have been any kind of algorithm. But if it is the kind that minimizes the training error, then you can get by, by,",
    "start": "4509130",
    "end": "4517739"
  },
  {
    "text": "by just changing the sample size, you can get a relation between the margin of error and",
    "start": "4517740",
    "end": "4525160"
  },
  {
    "text": "the probability of error and relate it to the sample size, right? So, um, what we can do with this relation is basically",
    "start": "4525160",
    "end": "4536739"
  },
  {
    "text": "fix any two and solve for the third, and that gives us,",
    "start": "4536740",
    "end": "4541990"
  },
  {
    "text": "nope, some actionable results. For example, you can fix any two and solve for the third from this relationship, right?",
    "start": "4541990",
    "end": "4553405"
  },
  {
    "text": "And what, what, what that could, uh, mean is for example,",
    "start": "4553405",
    "end": "4559465"
  },
  {
    "text": "so you, you can choose any two and solve for the third. Um, I'm only gonna go over one, one of those.",
    "start": "4559465",
    "end": "4564610"
  },
  {
    "text": "So let, let's fix, fix uh, Gamma and Delta to be greater than zero.",
    "start": "4564610",
    "end": "4576460"
  },
  {
    "text": "And we solve for m, and we get m [NOISE] weighted to, equal 1 over 2 Gamma square,",
    "start": "4576460",
    "end": "4587634"
  },
  {
    "text": "log 2K over Delta.",
    "start": "4587634",
    "end": "4592960"
  },
  {
    "text": "So what this means is with probability at least 1 minus Delta which means probably at least 99% or 99.9%.",
    "start": "4592960",
    "end": "4602020"
  },
  {
    "text": "For example, with probability at least, uh, 1 minus delta, the margin of error between",
    "start": "4602020",
    "end": "4609655"
  },
  {
    "text": "the empirical risk and the true generalization risk is gonna be less",
    "start": "4609655",
    "end": "4614739"
  },
  {
    "text": "than Gamma as long as your training size is bigger than this expression, right.",
    "start": "4614740",
    "end": "4623005"
  },
  {
    "text": "That's something actionable for us, right. Now, theory can be useful. So this is also called the sample complexity dessert.",
    "start": "4623005",
    "end": "4628900"
  },
  {
    "text": "[NOISE] right? And uh, basically,",
    "start": "4628900",
    "end": "4636480"
  },
  {
    "text": "what this means is as you increase m and you, you sample different [NOISE] sets of, uh, uh, data-sets,",
    "start": "4636480",
    "end": "4643665"
  },
  {
    "text": "your dotted lines are gonna get closer and closer to, to, uh, the thick line which means,",
    "start": "4643665",
    "end": "4651775"
  },
  {
    "text": "minimizing your- minimizing on the dotted line will also get you closer to the generalization error.",
    "start": "4651775",
    "end": "4658830"
  },
  {
    "text": "So this, this is basically telling you how minimizing on, on, um, minimizing on the empirical risk gets you closer to,",
    "start": "4658830",
    "end": "4667364"
  },
  {
    "text": "uh, gen- generalization, right? Okay, so that- so we started off with two questions,",
    "start": "4667365",
    "end": "4674935"
  },
  {
    "text": "relating the empirical risk to generalization risk. Now, let's, let's explore the second question.",
    "start": "4674935",
    "end": "4681010"
  },
  {
    "text": "What about, uh, the generalization error [NOISE] of [NOISE] our minimizer with the,",
    "start": "4681010",
    "end": "4690730"
  },
  {
    "text": "uh, um, best possible in class? So let's look at this diagram again.",
    "start": "4690730",
    "end": "4697345"
  },
  {
    "text": "Let's say we started with this dotted curve, right. And the minimizer of that would be h-star.",
    "start": "4697345",
    "end": "4704815"
  },
  {
    "text": "This is h-star. Sorry the diagram is a little, uh, [NOISE] let me erase the previous one [NOISE] right?",
    "start": "4704815",
    "end": "4717070"
  },
  {
    "text": "So this is h-hat. Sorry, this is h-hat.",
    "start": "4717070",
    "end": "4722030"
  },
  {
    "text": "And this has a particular generalization error, right? That is the point of, of- uh,",
    "start": "4723540",
    "end": "4729685"
  },
  {
    "text": "let- let- let's assume we got this data-set. We ran the empirical [NOISE] risk minimizer and we obtained this hypothesis.",
    "start": "4729685",
    "end": "4736345"
  },
  {
    "text": "And when we deploy this in the world- in the real world, its error is gonna be so much, right?",
    "start": "4736345",
    "end": "4742840"
  },
  {
    "text": "Now, how does this compare [NOISE] to the performance of the minimizer of the, the,",
    "start": "4742840",
    "end": "4749395"
  },
  {
    "text": "the best possible [NOISE] ,",
    "start": "4749395",
    "end": "4757270"
  },
  {
    "text": "so this is h-star, best in class, right?",
    "start": "4757270",
    "end": "4762835"
  },
  {
    "text": "Now, we want to get a relation between this error level and this error level.",
    "start": "4762835",
    "end": "4768355"
  },
  {
    "text": "We got one bound that relates this to this, and now we want something that relates this to this.",
    "start": "4768355",
    "end": "4775719"
  },
  {
    "text": "Now, how do we do that? It's pretty straightforward. Um, so the em- generalization error of h-hat,",
    "start": "4775720",
    "end": "4789445"
  },
  {
    "text": "that's this dot over here, is less than equal to the empirical risk of h-hat plus Gamma.",
    "start": "4789445",
    "end": "4801520"
  },
  {
    "text": "So we got the result, um, using a Hoeffding and union bound that the gap between the dotted line and the,",
    "start": "4801520",
    "end": "4809784"
  },
  {
    "text": "the thick black line is always less than Gamma, right? And it's the absolute value. So we can, we can, uh, um,",
    "start": "4809785",
    "end": "4816940"
  },
  {
    "text": "write it this way as well. And this, right? So basically, we, we start it from the,",
    "start": "4816940",
    "end": "4823645"
  },
  {
    "text": "the thick black line, drop down to the dotted line.",
    "start": "4823645",
    "end": "4829555"
  },
  {
    "text": "And this is gonna be less than the empirical error of h-star plus Gamma. Why is that?",
    "start": "4829555",
    "end": "4839440"
  },
  {
    "text": "Because em- empirical error, um, the empirical error, uh, uh,",
    "start": "4839440",
    "end": "4846220"
  },
  {
    "text": "of h-hat by definition, is less than or equal to the empirical error on any other hypotheses,",
    "start": "4846220",
    "end": "4853165"
  },
  {
    "text": "including the best in class. Because this is the training error, not, not, not the generalization error, right?",
    "start": "4853165",
    "end": "4859045"
  },
  {
    "text": "So which means, um- and,",
    "start": "4859045",
    "end": "4864730"
  },
  {
    "text": "and this is less than or equal to. So we, we dropped from the generalization to the test.",
    "start": "4864730",
    "end": "4871525"
  },
  {
    "text": "And we said, this test is, thi- this training error is always gonna to be less",
    "start": "4871525",
    "end": "4877900"
  },
  {
    "text": "than the empirical error of the best-in-class.",
    "start": "4877900",
    "end": "4882909"
  },
  {
    "text": "You see that the best-in-class is higher for the trade to be empirical error. And this again, is now- this gap is also bounded because we,",
    "start": "4882910",
    "end": "4892105"
  },
  {
    "text": "we proved uniform convergence. That the gap between the dotted line and thick line is bounded by Gamma for any h, right?",
    "start": "4892105",
    "end": "4899200"
  },
  {
    "text": "And this is therefore h-star plus 2 Gamma,",
    "start": "4899200",
    "end": "4909475"
  },
  {
    "text": "because we added the extra margin. So we wanted the relation between the, uh,",
    "start": "4909475",
    "end": "4914665"
  },
  {
    "text": "the- our, our hypothesis generalization error to the generalization error of the best in class hypotheses.",
    "start": "4914665",
    "end": "4922255"
  },
  {
    "text": "So we dropped from the generalization error to the empirical error of our hypotheses,",
    "start": "4922255",
    "end": "4929695"
  },
  {
    "text": "related that to the empirical one of the best in class and again bounded by the gap between these two.",
    "start": "4929695",
    "end": "4935710"
  },
  {
    "text": "So we- we've got a gap between the generalization bound, the generalized error for hypothesis to the best in",
    "start": "4935710",
    "end": "4941200"
  },
  {
    "text": "class generalization. Any questions on this?",
    "start": "4941200",
    "end": "4944930"
  },
  {
    "text": "So the result basically says,",
    "start": "4949200",
    "end": "4955015"
  },
  {
    "text": "with probability, 1 minus Delta, and for training size m,",
    "start": "4955015",
    "end": "4961480"
  },
  {
    "text": "[NOISE] the generalization error of [NOISE] the hypothesis from",
    "start": "4961480",
    "end": "4970989"
  },
  {
    "text": "the empirical risk minimizer is going to be within the",
    "start": "4970990",
    "end": "4977560"
  },
  {
    "text": "best in class generalization error plus 2 times 1 over,",
    "start": "4977560",
    "end": "4984940"
  },
  {
    "text": "1 over 2m plus log 2K over Delta.",
    "start": "4984940",
    "end": "4992080"
  },
  {
    "text": "So this was basically uh, so you can get this, uh, when you,",
    "start": "4992080",
    "end": "4998020"
  },
  {
    "text": "when you- so in this expression,",
    "start": "4998020",
    "end": "5004230"
  },
  {
    "text": "if you set this equal to Delta and solve for Gamma, you will get this.",
    "start": "5004230",
    "end": "5008410"
  },
  {
    "text": "Any questions? [NOISE] I think we're already over time.",
    "start": "5010790",
    "end": "5020085"
  },
  {
    "text": "So, uh, the case for infinite classes is an extension to this.",
    "start": "5020085",
    "end": "5026100"
  },
  {
    "text": "Maybe I'll just write the results. So there is a concept called VC dimension, which is a pretty simple concept but [NOISE] we won't be going over it today.",
    "start": "5026100",
    "end": "5035550"
  },
  {
    "text": "VC dimension basically says, um, what is the- so VC dimension is,",
    "start": "5035550",
    "end": "5041864"
  },
  {
    "text": "you can think of it as trying to assign a size to an infinitely,",
    "start": "5041865",
    "end": "5047730"
  },
  {
    "text": "uh, to an infinite size hypothesis class. For a fixed size hypothesis class, we had like, you know, K to be the size of the hypothesis class.",
    "start": "5047730",
    "end": "5053700"
  },
  {
    "text": "So VC [NOISE] of some hypothesis class is gonna be some number, right?",
    "start": "5053700",
    "end": "5061080"
  },
  {
    "text": "Some number which, which kind of, um, which is like the size of the hypothesis. It's basically, telling you how,",
    "start": "5061080",
    "end": "5066929"
  },
  {
    "text": "how expressive it is um, and, and, uh, on using, using the VC dimension,",
    "start": "5066930",
    "end": "5074375"
  },
  {
    "text": "uh, there are very nice uh, geometrical meanings of VC dimension. You can, you can get a bound, similar bound.",
    "start": "5074375",
    "end": "5081440"
  },
  {
    "text": "But now, it's not for, uh, uh, um, it's not for uh,",
    "start": "5081440",
    "end": "5086679"
  },
  {
    "text": "uh, finite classes anymore.",
    "start": "5086680",
    "end": "5090360"
  },
  {
    "text": "Some big O of [NOISE]",
    "start": "5092570",
    "end": "5120270"
  },
  {
    "text": "right? So in place of this margin, we ended up with, uh, a different margin that is, uh,",
    "start": "5120270",
    "end": "5127005"
  },
  {
    "text": "a function of the, the VC dimension. And the, the key takeaway from this is that uh,",
    "start": "5127005",
    "end": "5135869"
  },
  {
    "text": "the number of data examples, that the sample complexity that you want is generally,",
    "start": "5135870",
    "end": "5144330"
  },
  {
    "text": "uh, an order of the VC dimension to get good results. That's basically, the, uh, uh,",
    "start": "5144330",
    "end": "5149820"
  },
  {
    "text": "main result from that, right? From, uh- with that, I guess we'll, we'll, uh, we'll break for the day and,",
    "start": "5149820",
    "end": "5156045"
  },
  {
    "text": "uh, we'll take more questions.",
    "start": "5156045",
    "end": "5158830"
  }
]