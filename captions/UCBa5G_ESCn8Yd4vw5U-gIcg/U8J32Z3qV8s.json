[
  {
    "start": "0",
    "end": "5020"
  },
  {
    "text": "Today, Irwan and I are going\nto be giving a talk on scaling transformers through sparsity. And the kind of sparsity we're\ngoing to be talking about today",
    "start": "5020",
    "end": "12130"
  },
  {
    "text": "is the kind where each input\ncan get either a different set of weights or have a different\namount of computation applied",
    "start": "12130",
    "end": "19030"
  },
  {
    "text": "to it. Irwan, do you want\nto start it off? Yep.",
    "start": "19030",
    "end": "24460"
  },
  {
    "text": "So I guess, the\noverall motivation for this line of work is\nthat the community has",
    "start": "24460",
    "end": "31590"
  },
  {
    "text": "kind of realized\nthat scale is perhaps one of the most important\naxes to focus on for obtaining",
    "start": "31590",
    "end": "38680"
  },
  {
    "text": "strong performance. And there's almost this\nsort of ongoing arms race",
    "start": "38680",
    "end": "43717"
  },
  {
    "text": "right now with different labs\nand different institutions sort of competing for\ntraining the largest models.",
    "start": "43717",
    "end": "53290"
  },
  {
    "text": "And so maybe this dates back\nfrom early 2020 with a paper",
    "start": "53290",
    "end": "58450"
  },
  {
    "text": "from OpenAI called \"Scaling Laws\nfor Neural Language Models,\"",
    "start": "58450",
    "end": "63700"
  },
  {
    "text": "where they find that\nmodel performance follows a predictable power law,\nscales sort of as a power",
    "start": "63700",
    "end": "72117"
  },
  {
    "text": "law with model size in terms\nof either compute, also",
    "start": "72117",
    "end": "77450"
  },
  {
    "text": "just parameters.  And so this scaling\nlaw kind of generalizes",
    "start": "77450",
    "end": "85399"
  },
  {
    "text": "over multiple\norders of magnitude. And that gives us the\nconfidence that if we are to train very\nlarge models, we",
    "start": "85400",
    "end": "93290"
  },
  {
    "text": "can expect performance just\nby extrapolating these scaling",
    "start": "93290",
    "end": "99200"
  },
  {
    "text": "laws. So in that paper, they also\nfind the interesting observation",
    "start": "99200",
    "end": "107420"
  },
  {
    "text": "that, basically, larger models\nare more sample-efficient. And so if you have a\nfixed compute budget,",
    "start": "107420",
    "end": "118840"
  },
  {
    "text": "you can predict what\nis the size, what is the optimal model size,\nfor a fixed compute budget.",
    "start": "118840",
    "end": "126040"
  },
  {
    "text": "And the overall\nobservation is that you'd",
    "start": "126040",
    "end": "131860"
  },
  {
    "text": "rather train very large\nmodels for less tests than train smaller models\non more training steps.",
    "start": "131860",
    "end": "139599"
  },
  {
    "text": "And so these models\nare scaled through--",
    "start": "139600",
    "end": "144730"
  },
  {
    "text": "basically, the paper\nfocuses on dense models, right, where you just\nincrease the model dimensions.",
    "start": "144730",
    "end": "151720"
  },
  {
    "text": "But they're not\nlooking at sparsity. And so sparsity\nis a new dimension",
    "start": "151720",
    "end": "157120"
  },
  {
    "text": "that you can use to\nscale architectures. And this is sort of\nthe focus of the talk.",
    "start": "157120",
    "end": "165129"
  },
  {
    "text": "And so the sparsity we are\nmentioning here is basically, you will have sparsely\nactivated weights",
    "start": "165130",
    "end": "174220"
  },
  {
    "text": "based on the network inputs. So every input will go to\na roughly similar amount",
    "start": "174220",
    "end": "180280"
  },
  {
    "text": "of computation but will be\napplied different weights.",
    "start": "180280",
    "end": "185770"
  },
  {
    "text": "And so this dates back\nto 1991 with a paper called \"Adaptive Mixtures\nof Local Experts\"",
    "start": "185770",
    "end": "193900"
  },
  {
    "text": "and was recently revisited by\nNoam Shazeer and colleagues at Google Brain with\nLSTMs, where they replaced",
    "start": "193900",
    "end": "203110"
  },
  {
    "text": "some of the feedforward\nnetworks and LSTMs",
    "start": "203110",
    "end": "208540"
  },
  {
    "text": "with mixture of experts. And so the way this\nworks, or roughly, is that you will have multiple\nexperts, each implementing",
    "start": "208540",
    "end": "217360"
  },
  {
    "text": "a small network or,\nin that case, I think, just a dense matrix\nmultiplication.",
    "start": "217360",
    "end": "225890"
  },
  {
    "text": "And so you have an additional\ngating network, shown in green here, that outputs a probability\ndistribution of our experts",
    "start": "225890",
    "end": "237610"
  },
  {
    "text": "that each token\nshould be sent to. So this probability distribution\nis computed as a softmax.",
    "start": "237610",
    "end": "246610"
  },
  {
    "text": "And once you have it,\nyou select a few experts.",
    "start": "246610",
    "end": "251620"
  },
  {
    "text": "So there are\ndifferent strategies. Maybe we'll talk\nabout it later on. And the output is simply\nsort of the weighted mixture",
    "start": "251620",
    "end": "258700"
  },
  {
    "text": "of all selected\nexperts' outputs. ",
    "start": "258700",
    "end": "265210"
  },
  {
    "text": "So they've been\npretty successful, primarily in translation.",
    "start": "265210",
    "end": "273220"
  },
  {
    "text": "But there were some complexities\nthat have a broader use in NLP.",
    "start": "273220",
    "end": "281760"
  },
  {
    "text": "And so the switch transformer\npaper addresses some of those. And we'll be discussing how\nto fix training instabilities",
    "start": "281760",
    "end": "292890"
  },
  {
    "text": "or reduce communication costs\nand reduce model complexity.",
    "start": "292890",
    "end": "299200"
  },
  {
    "text": "All right, Barret,\ndo you want to go? Sure, yeah. So one kind of\napproach that we're going to have for sparsity\nis the switch transformer,",
    "start": "299200",
    "end": "307480"
  },
  {
    "text": "which is kind of like a\nsimplified mixture of expert variants, along with some other\nimproved training and fine",
    "start": "307480",
    "end": "314080"
  },
  {
    "text": "tuning techniques that allow\nit to be stably trained and also perform\nbetter when fine tuned",
    "start": "314080",
    "end": "320259"
  },
  {
    "text": "on a lot of downstream tasks. And so the switch\ntransformer kind of model",
    "start": "320260",
    "end": "326528"
  },
  {
    "text": "works with the following. So you have some\ntransformer model that has self-attention\nand feedforward layers.",
    "start": "326528",
    "end": "332800"
  },
  {
    "text": "And the idea is that we replace\nmaybe one every two or one every four feedforward layers\nwith a switch transformer",
    "start": "332800",
    "end": "338890"
  },
  {
    "text": "layer. So you can see on the left is\none kind of layer block, which",
    "start": "338890",
    "end": "344890"
  },
  {
    "text": "is self-attention,\nand add normalize, then a feedforward layer,\nthen add normalize. And in this case,\nwe're replacing",
    "start": "344890",
    "end": "350980"
  },
  {
    "text": "the normal feedforward\nlayer with the switch layer. And we can see an illustration\nof this on the right.",
    "start": "350980",
    "end": "357139"
  },
  {
    "text": "So on the right, we can see\nthat the layer has two inputs. One is the token more.",
    "start": "357140",
    "end": "362530"
  },
  {
    "text": "The other is the\ntoken parameters. And we can see that these\nembedding representations will",
    "start": "362530",
    "end": "367570"
  },
  {
    "text": "get sent to a router, which\nis exactly how it works in the mixture of experts. So the router is\nbasically just going",
    "start": "367570",
    "end": "372751"
  },
  {
    "text": "to be getting a distribution\nover all of the experts. So in this case, we can see\nthat the highest probability",
    "start": "372752",
    "end": "378100"
  },
  {
    "text": "is going to expert number\ntwo out of the four experts. And then the right\ntoken is actually",
    "start": "378100",
    "end": "384970"
  },
  {
    "text": "having the most probability on\nthe first feedforward weight, which is like the first expert.",
    "start": "384970",
    "end": "390273"
  },
  {
    "text": "So yeah, we can see here\nthat what we're going to do is in the switch transformer,\nwhich is very simple. It's just send it to the\nhighest probability expert.",
    "start": "390273",
    "end": "397900"
  },
  {
    "text": "And so here, we can see where\nthe adaptive computation lies, where we'll have\nfour sets of weights.",
    "start": "397900",
    "end": "403330"
  },
  {
    "text": "There's some shared\nweights and computation across all the tokens. For example, the\nself-attention layer is computed exactly the\nsame for the more token",
    "start": "403330",
    "end": "410530"
  },
  {
    "text": "and for the parameters token. But in the sparse\nswitch layer, we can see that,\nactually, the inputs,",
    "start": "410530",
    "end": "416038"
  },
  {
    "text": "while having the same amount\nof floating-point operations applied to them, actually have\ndifferent weight matrices. ",
    "start": "416038",
    "end": "423479"
  },
  {
    "text": "Next slide. Yeah, so that's the\nkind of high-level idea with switch transformer\nis that instead",
    "start": "423480",
    "end": "430450"
  },
  {
    "text": "of sending a token to multiple\ndifferent experts, which can also increase the\ncommunication costs, as I'll go into a\nlittle bit later,",
    "start": "430450",
    "end": "436300"
  },
  {
    "text": "it also just significantly\nkind of simplifies the algorithm by just only\nsending it to one expert.",
    "start": "436300",
    "end": "442520"
  },
  {
    "text": "So for the improved\ntraining methodology, we focused on three\ndifferent things to help improve the\ntraining of sparse models.",
    "start": "442520",
    "end": "449569"
  },
  {
    "text": "The first was\nselected precision, which allows these\nsparse models to be trained in lower-precision\nformats, which",
    "start": "449570",
    "end": "454870"
  },
  {
    "text": "is incredibly important. Most of the models\nwe train, we really don't want to be using\nfloat32 because it's",
    "start": "454870",
    "end": "460449"
  },
  {
    "text": "just slower to compute. And also, when you're\ncommunicating tensors across different\nprocessors and stuff,",
    "start": "460450",
    "end": "466599"
  },
  {
    "text": "it's twice as slow just because\nthere's twice as many things. Also, we have some\ninitialization tricks and some training\ntricks, as well,",
    "start": "466600",
    "end": "472690"
  },
  {
    "text": "for allowing them to\nbe trained more stably, especially as the\nmodels grow in size, which is like a new\ninitialization method,",
    "start": "472690",
    "end": "478930"
  },
  {
    "text": "along with like a change to\nthe learning rate schedule. And third, since our models\nhave so many more parameters,",
    "start": "478930",
    "end": "485320"
  },
  {
    "text": "we do notice definitely\ndifferent overfitting dynamics, especially once we fine\ntune these models that",
    "start": "485320",
    "end": "491377"
  },
  {
    "text": "have been pretrained\non all of the internet, on these small tasks,\nwith maybe only 50,000 to 100,000 examples, that\nthey can be much more",
    "start": "491377",
    "end": "497680"
  },
  {
    "text": "prone to overfitting. So we also look at some\ncustom regularization",
    "start": "497680",
    "end": "503080"
  },
  {
    "text": "to help prevent some of the\noverfitting that we observe. And finally, we also talk\nabout this differentiable load",
    "start": "503080",
    "end": "508900"
  },
  {
    "text": "balancing technique\nwe make, which kind of allows each expert to roughly\nget the same amount of tokens.",
    "start": "508900",
    "end": "515789"
  },
  {
    "text": "Because this is very\nimportant, especially given that we want the stuff\nto be efficient on hardware. We want, roughly, each expert to\nhave similar amounts of tokens",
    "start": "515789",
    "end": "523179"
  },
  {
    "text": "sent to it. And so to kind of\nencourage this, we tack on an additional\nload-balancing loss,",
    "start": "523179",
    "end": "528819"
  },
  {
    "text": "along with our cross-entropy\nloss that we're training with. Next slide.",
    "start": "528820",
    "end": "533980"
  },
  {
    "text": "OK, so here, I'm going to\ngo into selective precision. So, yeah, again, so when\nwe're training large models,",
    "start": "533980",
    "end": "538990"
  },
  {
    "text": "it's really important that we\nshould be able to train them in lower-precision formats. So instead of each weight\nand activation being 32 bits,",
    "start": "538990",
    "end": "545920"
  },
  {
    "text": "we want to shrink\nit down to 16 bits. And we use the bfloat16\nrepresentation.",
    "start": "545920",
    "end": "551055"
  },
  {
    "text": "And what we found\nout of the gate is that these models\nare just unstable, especially the sparse\nmodels are much more",
    "start": "551055",
    "end": "557320"
  },
  {
    "text": "unstable than the dense\nmodels in terms of you'll train it for 10,000,\n20,000 steps, and then the losses\nwould just diverge.",
    "start": "557320",
    "end": "563030"
  },
  {
    "text": "This was something that\nwe frequently encountered. And so one key\nthing that we found is that, basically,\nyou need to be casting",
    "start": "563030",
    "end": "570190"
  },
  {
    "text": "a part of the computation\nin float32 for these models to be able to be trained stably.",
    "start": "570190",
    "end": "577750"
  },
  {
    "text": "And the key component that we\nfound that you need to cast is the router computation. And essentially-- we can go\ninto the technical details",
    "start": "577750",
    "end": "585983"
  },
  {
    "text": "a little bit more\nlater-- but basically, any time that there's these\nexponentiation functions, it's very important\nthat we are having",
    "start": "585983",
    "end": "592930"
  },
  {
    "text": "higher and higher precision\nbecause of round-off errors that can then drastically\nchange the output of some kind",
    "start": "592930",
    "end": "599230"
  },
  {
    "text": "of exponentiation function. So for example, if you have\nan exponentiation function",
    "start": "599230",
    "end": "604420"
  },
  {
    "text": "and you change it by\n0.1 or 0.2 or 0.3, this can drastically change the\noutput of exponentiating it,",
    "start": "604420",
    "end": "611290"
  },
  {
    "text": "especially depending on\nhow large the input is. So yeah, this was a\nvery important thing. And it basically doesn't\nchange the compute at all",
    "start": "611290",
    "end": "617933"
  },
  {
    "text": "and allows the models to just\nbe significantly more stable. Next slide.",
    "start": "617933",
    "end": "624019"
  },
  {
    "text": "So the second thing\nwe looked at is also the initialization scale. So the standard way that we\nwere initializing these models,",
    "start": "624020",
    "end": "630662"
  },
  {
    "text": "we found to also just\nmake the models much more prone to being unstable\nand/or just performing worse.",
    "start": "630662",
    "end": "636065"
  },
  {
    "text": "So one thing that we did that\nwe found was very effective was to just simply\nmake the initialization",
    "start": "636065",
    "end": "641140"
  },
  {
    "text": "scale much smaller. And when we did this, we\nfound that the quality just drastically improved. And it was a very simple fix.",
    "start": "641140",
    "end": "648759"
  },
  {
    "text": "Next slide. And the third thing\nI mentioned, where, since we noticed that\nthese models are much more",
    "start": "648760",
    "end": "655400"
  },
  {
    "text": "prone to overfitting, since\nthey just have significantly more parameters, is that we\nalso use much more dropout",
    "start": "655400",
    "end": "661759"
  },
  {
    "text": "for the expert layers only. So here, we can see we\nhave the T5 base, which is a dense model.",
    "start": "661760",
    "end": "667640"
  },
  {
    "text": "And then we have a bunch of\ndifferent switch variants on that. And we've found to be the\nmost effective on these four different fine\ntuning tasks was just",
    "start": "667640",
    "end": "674000"
  },
  {
    "text": "to really significantly\nincrease the dropout rate inside the expert layers. And we found that this was\npretty effective for combating",
    "start": "674000",
    "end": "680180"
  },
  {
    "text": "the overfitting. Barret? Yeah? We have a question from\none of the students.",
    "start": "680180",
    "end": "686480"
  },
  {
    "text": "Oh, awesome. OK. OK, let me take a look. Do you want to go ahead? I can ask.",
    "start": "686480",
    "end": "691900"
  },
  {
    "text": "It was just, in reference\nto that previous table where you have throughput\nand precision,",
    "start": "691900",
    "end": "697733"
  },
  {
    "text": "it just seemed\nsurprising to me that you could match this 1390\nnumber while using selective precision.",
    "start": "697733",
    "end": "703270"
  },
  {
    "text": "It seems like I would expect\nit to be something in between. Yeah. So essentially, it\ncomes down to the fact",
    "start": "703270",
    "end": "709090"
  },
  {
    "text": "that there's maybe a\nlittle bit of noise sampled with the speed. And the only part we're casting\nis the router, which is maybe",
    "start": "709090",
    "end": "717220"
  },
  {
    "text": "such a insignificant\nportion of the computation, and there's zero\ncommunication there, that it's essentially like a\nfree operation in the network.",
    "start": "717220",
    "end": "723980"
  },
  {
    "text": "So whether you cast it\nto bfloat16 or float32, it doesn't actually\nimpact the speed at all, within the precision that we\ncan actually measure the speed.",
    "start": "723980",
    "end": "732279"
  },
  {
    "text": "And also, these architectures\nonly use fast layer around one every four layers.",
    "start": "732280",
    "end": "739180"
  },
  {
    "text": "And so, essentially,\nthe float32 policy is kind of very negligible\nin the entire architecture.",
    "start": "739180",
    "end": "747217"
  },
  {
    "text": "Yeah, it's like, for example, I\nthink, off the top of my head, it's like 1/40 the\ncomputation that it would cost",
    "start": "747217",
    "end": "753130"
  },
  {
    "text": "for you to do the first weight\nmatrix multiply in a ReLU dense layer or something.",
    "start": "753130",
    "end": "758876"
  },
  {
    "text": "So it's a very, very small part. And yeah, we're not using\nthem very frequently, like Irwan mentioned, as well.",
    "start": "758877",
    "end": "764130"
  },
  {
    "text": "Got it. OK, thanks.  Yeah.",
    "start": "764130",
    "end": "769550"
  },
  {
    "text": "And then, just a\nquick point on this-- I won't go into some of the\ntechnical details-- but yeah, we definitely, since we're\ntraining these things",
    "start": "769550",
    "end": "775580"
  },
  {
    "text": "on hardware and we really-- I think a big part of the\nmixture of experts paradigm is that these things are\ndesigned such that it maps",
    "start": "775580",
    "end": "781370"
  },
  {
    "text": "really efficiently to hardware. So we want to be doing\ndense matrix multiplies. And for this to\nwork really well,",
    "start": "781370",
    "end": "787970"
  },
  {
    "text": "we also want to be able to have\nroughly equal amount of tokens going to each of the\ndifferent experts.",
    "start": "787970",
    "end": "793250"
  },
  {
    "text": "And I think this isn't that\nsensitive to the load-balancing formulation. We tried a few things.",
    "start": "793250",
    "end": "798810"
  },
  {
    "text": "A lot of them worked. But essentially,\nyou definitely want some kind of load-balancing loss\nadded on when using sparsity.",
    "start": "798810",
    "end": "806030"
  },
  {
    "text": "Yeah, next slide. Oh, that's me. Yeah, Irwan, go ahead.",
    "start": "806030",
    "end": "811780"
  },
  {
    "start": "811780",
    "end": "819443"
  },
  {
    "text": "Yeah, so the frameworks\nwith the library we use rely on\nstatic shapes for--",
    "start": "819443",
    "end": "825040"
  },
  {
    "start": "825040",
    "end": "833009"
  },
  {
    "text": "so XLA, so the compiler for\nTensorFlow and Mesh TensorFlow",
    "start": "833010",
    "end": "839730"
  },
  {
    "text": "expect static\nshapes for tensors. However, the computations\nin switch transformers",
    "start": "839730",
    "end": "846420"
  },
  {
    "text": "are dynamic because\nof the router, right?",
    "start": "846420",
    "end": "851610"
  },
  {
    "text": "Different inputs will be\nrouted to different experts. And so we need to specify ahead\nof time how many tokens will",
    "start": "851610",
    "end": "860579"
  },
  {
    "text": "be sent to each expert. And so we will introduce this\nexpert capacity hyperparameter",
    "start": "860580",
    "end": "867720"
  },
  {
    "text": "to specify that. And it's going to be\na static number which says how many tokens\neach expert can process.",
    "start": "867720",
    "end": "874755"
  },
  {
    "text": " And so in practice, we\ninstead parameterize this",
    "start": "874755",
    "end": "880410"
  },
  {
    "text": "by having a quantity\ncalled the capacity factor. So we have an example here.",
    "start": "880410",
    "end": "885779"
  },
  {
    "text": " So the bottom row is a bunch\nof tokens on one device.",
    "start": "885780",
    "end": "896530"
  },
  {
    "text": "And then you need\nto sort of route those tokens to multiple\ndevices, or multiple experts.",
    "start": "896530",
    "end": "902810"
  },
  {
    "text": "So if too many tokens are\nrouted to a single expert, some tokens will be dropped\nbecause, as we said,",
    "start": "902810",
    "end": "909850"
  },
  {
    "text": "an expert has a fixed capacity. So that's the\nexample on the left,",
    "start": "909850",
    "end": "915490"
  },
  {
    "text": "where the capacity factor is 1. And that basically means\nthat the toll does not",
    "start": "915490",
    "end": "921730"
  },
  {
    "text": "like extra buffer\nfor routing tokens.",
    "start": "921730",
    "end": "927070"
  },
  {
    "text": "So instead of that, we\ncan use a capacity factor that's larger than 1. So on the right, you\nhave an example with 1.5.",
    "start": "927070",
    "end": "934610"
  },
  {
    "text": "So that means that now, each\nexpert has sort of three slots. They can process three tokens.",
    "start": "934610",
    "end": "942500"
  },
  {
    "text": "And so that prevents\ntoken dropping because we have more capacity. But the issue is that\nthis means higher--",
    "start": "942500",
    "end": "950860"
  },
  {
    "text": "this means more expensive\ncommunication across devices. ",
    "start": "950860",
    "end": "958029"
  },
  {
    "text": "Yeah. OK, so-- yeah, go ahead. Yeah. So one thing that we\nalso experimented with",
    "start": "958030",
    "end": "964450"
  },
  {
    "text": "was this method called\nno token left behind. And the idea was the following. So since we have to have a\nfixed batch size for each expert",
    "start": "964450",
    "end": "973600"
  },
  {
    "text": "and there can be token\ndropping, we kind of were thinking that, hey,\nyeah, having tokens drop, or having some tokens not\nhaving any computation",
    "start": "973600",
    "end": "980470"
  },
  {
    "text": "applied to it is probably\nhurting the model performance. So what if we do a\nmulti-stage routing procedure?",
    "start": "980470",
    "end": "986100"
  },
  {
    "text": "So first, you do\nthe normal routing, where it's like you send\neach token to its highest probability expert. But then any dropped\ntokens, you then",
    "start": "986100",
    "end": "992800"
  },
  {
    "text": "send to their second-highest\nprobability expert and so forth and so on,\nwhere you can basically",
    "start": "992800",
    "end": "998290"
  },
  {
    "text": "repeat this process to\nguarantee that no tokens are being dropped. Interestingly,\nactually, this approach",
    "start": "998290",
    "end": "1003720"
  },
  {
    "text": "didn't empirically\nimprove model performance. If anything, it actually\nkind of hurt it. And we thought that was\nactually very interesting.",
    "start": "1003720",
    "end": "1011110"
  },
  {
    "text": "And I think the intuition is\nthat once the model learns it wants to send a\ntoken to one expert, it really wants to have that\ncomputation applied to it.",
    "start": "1011110",
    "end": "1018029"
  },
  {
    "text": "And just applying\nsome other computation doesn't have, at all,\nthe same property,",
    "start": "1018030",
    "end": "1023250"
  },
  {
    "text": "along with it, actually, maybe\nbeing potentially detrimental. So yeah, we thought that\nwas pretty interesting,",
    "start": "1023250",
    "end": "1028349"
  },
  {
    "text": "as we were very optimistic\nthis would potentially get improved performance. But it ended up not really\nmaking a difference.",
    "start": "1028349",
    "end": "1033689"
  },
  {
    "text": "And we found this\nquite surprising. We have a question from-- ",
    "start": "1033690",
    "end": "1041270"
  },
  {
    "text": "I think it will\nactually kind of address the majority of the last\npoint that you brought up. I think when I think about a\nmixture of experts, usually,",
    "start": "1041270",
    "end": "1050510"
  },
  {
    "text": "they specialize in\ndifferent things, right? So I think this just a lot--",
    "start": "1050510",
    "end": "1058040"
  },
  {
    "text": "I was just wondering if you\nsend it to the second best or whatever, what if\nall of your tokens",
    "start": "1058040",
    "end": "1067610"
  },
  {
    "text": "would be particularly\ngood for one expert, and then you only process,\nlet's say, 20% of your tokens.",
    "start": "1067610",
    "end": "1076909"
  },
  {
    "text": "So that ends up being\nbetter than rerouting them to anything else.",
    "start": "1076910",
    "end": "1082039"
  },
  {
    "text": "Exactly. Yeah, so even if you're\ndropping a lot of tokens, it's not beneficial to be\nsending them to the second,",
    "start": "1082040",
    "end": "1087690"
  },
  {
    "text": "third, or fourth best thing. And one, actually,\ninteresting property that we noticed about\nthese models is they're",
    "start": "1087690",
    "end": "1092810"
  },
  {
    "text": "surprisingly robust\nto token dropping, especially during fine tuning. So yeah, in the standard\nparadigm, what we'll do",
    "start": "1092810",
    "end": "1098540"
  },
  {
    "text": "is we'll pretrain this thing. We'll have some\nload-balancing loss, which makes the tokens\npretty balanced, actually.",
    "start": "1098540",
    "end": "1104420"
  },
  {
    "text": "But then, during\nfine tuning, where it's like we really want to\nfine tune it on a specific task, we actually studied\nthis exact question.",
    "start": "1104420",
    "end": "1109903"
  },
  {
    "text": "And we were studying,\ndoes it help to have a load-balancing loss\nduring fine tuning or not? And so if you have the\nload-balancing loss,",
    "start": "1109903",
    "end": "1116059"
  },
  {
    "text": "yeah, that kind\nof is encouraging. For the specific\ntask, we want to try to have all the experts be\nused versus turning it off,",
    "start": "1116060",
    "end": "1121970"
  },
  {
    "text": "where there's definitely\nsome prior specialization. And it's actually\nmuch better to just turn the auxiliary loss off.",
    "start": "1121970",
    "end": "1127799"
  },
  {
    "text": "And even if it's 60% to 70% of\nthe tokens are being dropped, that actually\nperforms much better",
    "start": "1127800",
    "end": "1132980"
  },
  {
    "text": "than having all\nthe tokens balance. But doesn't a\nload-balancing loss encourage, basically,\nall the experts",
    "start": "1132980",
    "end": "1139490"
  },
  {
    "text": "to learn very similar weights\nand then just randomly assign your tokens?",
    "start": "1139490",
    "end": "1144795"
  },
  {
    "text": "Because then it\ndoesn't matter to which expert stuff is being sent to. So when we use the\nload-balancing loss,",
    "start": "1144795",
    "end": "1150260"
  },
  {
    "text": "the routing mechanism\nis definitely learned. So the model definitely\nis encouraged to choose an expert\nthat it wants to send it",
    "start": "1150260",
    "end": "1156710"
  },
  {
    "text": "to for good [INAUDIBLE]. Right. But if all the experts\nlearn the same weights, then the router learns,\nbasically, oh, it",
    "start": "1156710",
    "end": "1164390"
  },
  {
    "text": "doesn't matter\nwhere I send it to. So if you encourage\nload balancing, you encourage, technically,\nthat you want any loss",
    "start": "1164390",
    "end": "1172669"
  },
  {
    "text": "to fit with any expert, right? That's maybe the\nextreme behavior if you have a very high\nsort of load-balancing loss",
    "start": "1172670",
    "end": "1179630"
  },
  {
    "text": "coefficient. But in practice, that\ncoefficient is kind of tuned, and we observe that,\nfor small enough values,",
    "start": "1179630",
    "end": "1187669"
  },
  {
    "text": "the router still learns\nsemantic, meaningful routing. Yeah.",
    "start": "1187670",
    "end": "1192830"
  },
  {
    "text": "Because it's like a balance\nbetween this cross-entropy loss and this load-balancing loss. And so, on one hand,\nyeah, you definitely",
    "start": "1192830",
    "end": "1198890"
  },
  {
    "text": "want to encourage the\nmodel to be balanced. Then, on the other\nhand, you also want it to just get good\nempirical performance.",
    "start": "1198890",
    "end": "1204259"
  },
  {
    "text": "And yeah, the model is able\nto definitely, on one hand, learn and specialize\nthe experts where",
    "start": "1204260",
    "end": "1209390"
  },
  {
    "text": "they have different weights such\nthat it's like, definitely, it expects certain tokens to\nbe sent to certain experts but, on the other hand,\nstill be reasonably balanced",
    "start": "1209390",
    "end": "1215899"
  },
  {
    "text": "so that the models\nare efficiently run on modern hardware. Exactly.",
    "start": "1215900",
    "end": "1221929"
  },
  {
    "text": "And we also have a question\nfrom the classroom. So the question I want to ask\nis it seems to me like this",
    "start": "1221930",
    "end": "1227480"
  },
  {
    "text": "is a very experimental talk. We're talking about\nfloating-point precision. We're talking about\ndifferent approaches that currently work well.",
    "start": "1227480",
    "end": "1232897"
  },
  {
    "text": "And whenever we're dealing\nwith empirical science, there's a question of, what\nis the research question? And I feel like I missed that.",
    "start": "1232897",
    "end": "1239010"
  },
  {
    "text": "So what are we trying to answer\nwith all these experiments?",
    "start": "1239010",
    "end": "1244160"
  },
  {
    "text": "Yeah, I think the high-level\nresearch question is, can we create models that are\ndoing adaptive computation",
    "start": "1244160",
    "end": "1253340"
  },
  {
    "text": "from the standpoint\nof, can we try to make models more simulate the\ndynamics that we think models should most naturally use,\nwhich is different inputs have",
    "start": "1253340",
    "end": "1261140"
  },
  {
    "text": "different amounts of\ncomputation applied, have different weights\napplied to them, and basically all of this? Basically, we're trying to\nresearch and figure out,",
    "start": "1261140",
    "end": "1267915"
  },
  {
    "text": "how can we create a new\nframework for these models to be trained, as opposed to\ntheir dense counterparts that,",
    "start": "1267915",
    "end": "1273140"
  },
  {
    "text": "for every input,\nare always having the same exact\ncomputation applied? So that's interesting. Because when you say the same\nexact computation applied,",
    "start": "1273140",
    "end": "1280760"
  },
  {
    "text": "one might imagine that-- to me, the immediate\nthing is about how long",
    "start": "1280760",
    "end": "1285950"
  },
  {
    "text": "to deliberate about something. What I mean by that is if we\nwant to have variable length computation, you\ncould imagine that I",
    "start": "1285950",
    "end": "1291679"
  },
  {
    "text": "could have a short\namount of computation, or I could have much\nlonger computation. But this idea of why,\nthen, do we instead",
    "start": "1291680",
    "end": "1297860"
  },
  {
    "text": "consider the dimension of\ndifferent computation-- assuming, of course, that\nthese experts do indeed learn different\nthings, which I think",
    "start": "1297860",
    "end": "1304172"
  },
  {
    "text": "you'll get to in a minute. So why do we immediately\njump to thinking about specialized\nexperts, as opposed",
    "start": "1304172",
    "end": "1310520"
  },
  {
    "text": "to thinking about variable\nlength computation? So yeah, so this\nis actually-- we actually go into some variable\nlength computation stuff",
    "start": "1310520",
    "end": "1317299"
  },
  {
    "text": "later in the talk. And I feel like they're both,\nactually, just important axes that should both be pushed on.",
    "start": "1317300",
    "end": "1323750"
  },
  {
    "text": "Yeah, I guess it's kind of-- I guess what I'm\ntrying to understand-- I'm not phrasing\nmy question well. What I'm trying to understand is\nyour thinking about why did you",
    "start": "1323750",
    "end": "1330049"
  },
  {
    "text": "decide to attack this one first? I want to understand\nwhy your team chose to go this direction first. Yeah, absolutely.",
    "start": "1330050",
    "end": "1335660"
  },
  {
    "text": "So I think that,\none, empirically, it seems that sparsity has led\nto better empirical results",
    "start": "1335660",
    "end": "1340928"
  },
  {
    "text": "in the field of deep learning\nthan adaptive computation so far. And I think the way that we\nuse these things maps really",
    "start": "1340928",
    "end": "1346280"
  },
  {
    "text": "well to our modern hardware,\nwhich is also very promising. And I think the way we\nwere kind of looking at it is sparsity is a first\nstep towards doing",
    "start": "1346280",
    "end": "1353540"
  },
  {
    "text": "more interesting and general\nadaptive computation. Because I think it's like\nthis stuff is complicated.",
    "start": "1353540",
    "end": "1359540"
  },
  {
    "text": "And typically, starting from\nsomething that works well is better than necessarily\ntrying something",
    "start": "1359540",
    "end": "1366169"
  },
  {
    "text": "that's not necessarily\nas proven out and then trying to get\nit to work really well. So I think we're kind of\nstarting from sparsity, which",
    "start": "1366170",
    "end": "1372120"
  },
  {
    "text": "Noam Shazeer and others\ngot to work really well in the context of LSTMs. We were kind of\ninterested in, let's port",
    "start": "1372120",
    "end": "1377570"
  },
  {
    "text": "some of this to transformers. Let's get it\nworking really well. And then, let's\nslowly start expanding towards a lot of the\nother natural questions",
    "start": "1377570",
    "end": "1382970"
  },
  {
    "text": "that you mentioned,\nwhere it's like, OK, whereas instead of\ndifferent weights per core, let's also maybe have a\ndifferent computation per core",
    "start": "1382970",
    "end": "1389430"
  },
  {
    "text": "and all of this. So that's, I guess,\nhow we were kind of building the natural\nbuild-up and progression of our research.",
    "start": "1389430",
    "end": "1395760"
  },
  {
    "text": "Got it. Cool. Thank you. Yeah.  What do you think, Irwan?",
    "start": "1395760",
    "end": "1401150"
  },
  {
    "text": "Anything else to add?  Yeah, I guess I kind of\nsee adaptive computation",
    "start": "1401150",
    "end": "1407630"
  },
  {
    "text": "and sparsity as related\nbut separate things. So sparsity is more like\ndifferent parameters",
    "start": "1407630",
    "end": "1414740"
  },
  {
    "text": "for each example. And adaptive\ncomputation might be more different amount of flux.",
    "start": "1414740",
    "end": "1420320"
  },
  {
    "text": "And we have some of that\nwith the token dropping, but that's not the\nmain domain motivation.",
    "start": "1420320",
    "end": "1428899"
  },
  {
    "text": "Definitely, as\nBarret mentioned, I would say no one\nreally has figured out",
    "start": "1428900",
    "end": "1436130"
  },
  {
    "text": "adaptive computation\nyet for deep learning. And one reason is because\nwe have these accelerators",
    "start": "1436130",
    "end": "1443362"
  },
  {
    "text": "where I expect, sort of-- ",
    "start": "1443363",
    "end": "1448633"
  },
  {
    "text": "we need to work with a batch,\nlike data parallelism, right? And all of our accelerators\nand our frameworks",
    "start": "1448634",
    "end": "1455929"
  },
  {
    "text": "use this SPMD\nparadigm where we're kind of supposed to apply the\nsame computation to examples.",
    "start": "1455930",
    "end": "1465007"
  },
  {
    "text": "And so if you look\nat the literature, you have works like\nuniversal transformers",
    "start": "1465008",
    "end": "1470660"
  },
  {
    "text": "where they replaced the\nfeedforwards in the transformer by just a recurrent weight.",
    "start": "1470660",
    "end": "1476150"
  },
  {
    "text": "And so it's kind of like\nan LSTM on each token. And the LSTM can stop\nat different times",
    "start": "1476150",
    "end": "1482540"
  },
  {
    "text": "based on some criteria. But the way these\nthings are implemented is just through masking.",
    "start": "1482540",
    "end": "1489590"
  },
  {
    "text": "Because it needs\nto be implemented in the SPMD programming style.",
    "start": "1489590",
    "end": "1495380"
  },
  {
    "text": "And so definitely,\nsparsity was kind of easier to get to work first. And also, there were some prior\nresults with LSTMs, so yeah.",
    "start": "1495380",
    "end": "1502115"
  },
  {
    "text": " In terms of the first\nquestion, what's",
    "start": "1502115",
    "end": "1508712"
  },
  {
    "text": "the research question\nhere, it's just like, oh, can we design more\nefficient models? And sparsity is\nthis new axis that",
    "start": "1508712",
    "end": "1515800"
  },
  {
    "text": "hasn't been explored that much. And, yeah, I think that\nI'm happy with just setting",
    "start": "1515800",
    "end": "1521368"
  },
  {
    "text": "the research question, yeah.  Great.",
    "start": "1521368",
    "end": "1526770"
  },
  {
    "text": "OK. Yeah, so next slide. ",
    "start": "1526770",
    "end": "1533320"
  },
  {
    "text": "Yeah, again, so kind of\nputting it all together. So the switch transformer\nlayer selects an expert, just",
    "start": "1533320",
    "end": "1538830"
  },
  {
    "text": "the top expert, and\nthen incorporates a bunch of the general\nsparse model improvements to allow it to fine\ntune better, allow",
    "start": "1538830",
    "end": "1546150"
  },
  {
    "text": "it to be more\nregularized, allow it to be trained with\nlower-precision formats and a lot of technical\ndetails to just get",
    "start": "1546150",
    "end": "1552630"
  },
  {
    "text": "them training and working well. ",
    "start": "1552630",
    "end": "1557717"
  },
  {
    "text": "So yeah, one thing that\nwe also wanted to do was a comparison between\ntop-1 and top-2 routing,",
    "start": "1557717",
    "end": "1562740"
  },
  {
    "text": "since top-2 routing was\nthe most popular technique. And so here, we can see we have\ntwo different dense models,",
    "start": "1562740",
    "end": "1569660"
  },
  {
    "text": "trained on different sizes. And we're going to be looking\nat the pretraining negative log perplexity. So yeah, the bigger\nthe number, the better.",
    "start": "1569660",
    "end": "1579290"
  },
  {
    "text": "So next slide.  And what we're\ngoing to be doing is we're going to be studying them\nat different capacity factors.",
    "start": "1579290",
    "end": "1586013"
  },
  {
    "text": "So a capacity factor\nof 2.0 basically means that there is enough\nbuffer for two tokens to be sent to every\nsingle expert.",
    "start": "1586013",
    "end": "1592130"
  },
  {
    "text": "And we're going to be comparing\ntop one versus top two routing and also\ncomparing their speeds, along with their time to\nget some threshold quality.",
    "start": "1592130",
    "end": "1602240"
  },
  {
    "text": "OK, so here, we can see in\nthe capacity factor 2.0 case that the MoE models outperform\nswitch transformer, which",
    "start": "1602240",
    "end": "1609410"
  },
  {
    "text": "makes a lot of sense. Since switch transformer is\nonly sending a top-1 token to each expert, the mixture of\nexpert is sending two tokens.",
    "start": "1609410",
    "end": "1618490"
  },
  {
    "text": "So that makes sense\nthat this extra buffer will be disproportionately\nbeneficial for the mixture of expert models.",
    "start": "1618490",
    "end": "1624000"
  },
  {
    "text": "And so we noticed that. And next slide, or, yeah, next. ",
    "start": "1624000",
    "end": "1629820"
  },
  {
    "text": "Now so the really interesting\nparts for the top-1 routing becomes when we lower\nthe capacity factors.",
    "start": "1629820",
    "end": "1634930"
  },
  {
    "text": "So having a high\ncapacity factor is bad for many\nreasons, one of which is it really incurs more of\nthese communication costs",
    "start": "1634930",
    "end": "1641850"
  },
  {
    "text": "for sending tokens to\nthe correct experts. It also incurs\nmore compute costs and also incurs a lot\nof memory overhead.",
    "start": "1641850",
    "end": "1648220"
  },
  {
    "text": "So if you can get\nthis lower, it's usually a very, very good thing. And so what we see here is that\nswitch transformer actually",
    "start": "1648220",
    "end": "1655670"
  },
  {
    "text": "outperforms mixture of experts\nwhen you have a lower capacity factor. And we can see that the\ntime to quality threshold,",
    "start": "1655670",
    "end": "1665030"
  },
  {
    "text": "we get there much quicker. And so even across the 2.0\nand the 1.25 capacity factors,",
    "start": "1665030",
    "end": "1670242"
  },
  {
    "text": "the kind of Pareto optimal\nthing we saw in our setup is to use switch transformer\nat a lower capacity factor,",
    "start": "1670243",
    "end": "1675860"
  },
  {
    "text": "just due to the fact that\nwhile the quality is worse, a little bit worse,\non a step basis, it's just much faster to run.",
    "start": "1675860",
    "end": "1681440"
  },
  {
    "text": "So it's kind of the\nPareto optimal decision. Next slide.",
    "start": "1681440",
    "end": "1687400"
  },
  {
    "text": "And we could also be seeing\nthat, for capacity factor 1.0, again, we can see that this\nreally disproportionately",
    "start": "1687400",
    "end": "1692440"
  },
  {
    "text": "benefits switch transformer\nand is even better on a Pareto standpoint than the\n1.25 capacity factors.",
    "start": "1692440",
    "end": "1699820"
  },
  {
    "text": "And interestingly,\nsince MoE also does a little bit\nmore computation, you can also just increase\nthe amount of compute",
    "start": "1699820",
    "end": "1706913"
  },
  {
    "text": "done elsewhere in the model. And we can see that that's a\nmuch more efficient allocation of compute. So yeah, overall,\nour takeaway is",
    "start": "1706913",
    "end": "1713500"
  },
  {
    "text": "that lower capacity\nfactors using top-1 routing is more Pareto-efficient than\nusing top-2 routing at higher",
    "start": "1713500",
    "end": "1721870"
  },
  {
    "text": "capacity factors. Next slide. Irwan, you can take it over.",
    "start": "1721870",
    "end": "1728100"
  },
  {
    "text": "OK. So next, we'll look at how\nswitch transformer scales",
    "start": "1728100",
    "end": "1733760"
  },
  {
    "text": "as a function of the number of\nexperts in the switch layers. ",
    "start": "1733760",
    "end": "1739100"
  },
  {
    "text": "And so on the right\nside here, you see a plot that shows\nperplexity versus",
    "start": "1739100",
    "end": "1745310"
  },
  {
    "text": "training steps for different\nswitch architectures, ranging from T5 base,\nwhich is basically",
    "start": "1745310",
    "end": "1752240"
  },
  {
    "text": "no expert or a single\nexpert, up to 128 experts.",
    "start": "1752240",
    "end": "1757850"
  },
  {
    "text": "And so you see that, as we\nincrease the number of experts, which also increases the\nnumber of parameters,",
    "start": "1757850",
    "end": "1763810"
  },
  {
    "text": "sparse parameters, you\nget sort of speed-ups.",
    "start": "1763810",
    "end": "1770240"
  },
  {
    "text": "You get increasing speed-ups\nabove the dense baseline and also sort of diminishing\nreturns to multiplying,",
    "start": "1770240",
    "end": "1777980"
  },
  {
    "text": "to increasing, the number\nof experts, as well. ",
    "start": "1777980",
    "end": "1784490"
  },
  {
    "text": "So the previous figure\nwas looking at perplexity versus training steps.",
    "start": "1784490",
    "end": "1790630"
  },
  {
    "text": "Here, we look at perplexity\nversus training time. So that includes all the\nadditional communication costs",
    "start": "1790630",
    "end": "1800200"
  },
  {
    "text": "when you have more\nexperts, comparing",
    "start": "1800200",
    "end": "1805389"
  },
  {
    "text": "to the dense baseline. And so this is for\nswitch-based or dense T5 base.",
    "start": "1805390",
    "end": "1810940"
  },
  {
    "text": "And we observe up to 7X\nspeedups of our T5 base.",
    "start": "1810940",
    "end": "1817389"
  },
  {
    "text": "And so just to,\nmaybe, contextualize these numbers, 7X\nspeedups in deep learning",
    "start": "1817390",
    "end": "1825340"
  },
  {
    "text": "are pretty hard to obtain. And so I think this is\none of the results that",
    "start": "1825340",
    "end": "1833470"
  },
  {
    "text": "can spark a lot of\ninterest in sparse models, even if it's only for\npretraining for now,",
    "start": "1833470",
    "end": "1839019"
  },
  {
    "text": "just having that number is\nmaybe there's a significant--",
    "start": "1839020",
    "end": "1844090"
  },
  {
    "text": " there's something significant\nthat can be happening here. ",
    "start": "1844090",
    "end": "1853710"
  },
  {
    "text": "OK, so sparse scaling laws. So here, we'll look at sort\nof loss versus sparse model",
    "start": "1853710",
    "end": "1861809"
  },
  {
    "text": "parameters, which are\nincreased by increasing the number of experts.",
    "start": "1861810",
    "end": "1866820"
  },
  {
    "text": "And so similarly to the\nneural scaling lab paper, we observed that as you\nincrease the parameters,",
    "start": "1866820",
    "end": "1878370"
  },
  {
    "text": "the sparse parameters,\nand keep the FLOPS fixed, you get consistent gains\nbut diminishing gains.",
    "start": "1878370",
    "end": "1886410"
  },
  {
    "text": " OK, so now, we're going to\ncompare expert parallelism",
    "start": "1886410",
    "end": "1893630"
  },
  {
    "text": "and model parallelism. So we introduced sparsity,\nor expert parallelism,",
    "start": "1893630",
    "end": "1898700"
  },
  {
    "text": "as a new dimension\nto scale models. But of course,\nthere's the other one",
    "start": "1898700",
    "end": "1904460"
  },
  {
    "text": "for dense model, which is\nsimply model parallelism, where model weights are partitioned\nacross cores once they",
    "start": "1904460",
    "end": "1911960"
  },
  {
    "text": "are above the maximum size that\nyou can feed on a single core. ",
    "start": "1911960",
    "end": "1921130"
  },
  {
    "text": "All right, so to the left\nis expert parallelism here. Yeah.",
    "start": "1921130",
    "end": "1926530"
  },
  {
    "text": "So essentially, what\nwe're doing is yeah, we're kind of comparing\na switch-based model versus the dense-based.",
    "start": "1926530",
    "end": "1933250"
  },
  {
    "text": "And we're also comparing\nagainst the larger dense model that has used model parallelism.",
    "start": "1933250",
    "end": "1938620"
  },
  {
    "text": "And we can see that-- because basically, when we\nwant to scale up model size, we kind of have two axes that\nwe can either go through.",
    "start": "1938620",
    "end": "1944620"
  },
  {
    "text": "We can either increase\nthe number of FLOPS by scaling through\nmodel parallelism or increase the number\nof parameters by scaling",
    "start": "1944620",
    "end": "1950260"
  },
  {
    "text": "through sparsity. And so we can see that, even\ncompared to a dense model that's been scaled up\nthrough model parallelism,",
    "start": "1950260",
    "end": "1956590"
  },
  {
    "text": "that sparsity is\nstill, at this scale, a more effective way to scale up\nthe model by still getting 2.5X",
    "start": "1956590",
    "end": "1962740"
  },
  {
    "text": "speedups over this larger dense\nmodel that was using model parallelism.",
    "start": "1962740",
    "end": "1968060"
  },
  {
    "text": "Cool. So next slide. Yeah. Basically, here, T5-large\nis the dense model",
    "start": "1968060",
    "end": "1973960"
  },
  {
    "text": "that uses model parallelism. ",
    "start": "1973960",
    "end": "1980010"
  },
  {
    "text": "Barret, go ahead. OK. And so one thing that we\nalso wanted to look at is are these expert\nmodels effective",
    "start": "1980010",
    "end": "1986580"
  },
  {
    "text": "if you have a really\nsmall amount of compute or just a small\namount of experts? So typically, when we're\ndesigning these models,",
    "start": "1986580",
    "end": "1992130"
  },
  {
    "text": "we have one expert per core. But if you don't have a large\ncluster to run these things on-- let's say you just\nhave a GPU with two cores",
    "start": "1992130",
    "end": "1999210"
  },
  {
    "text": "or something-- I guess is having two\nexperts more effective than just a dense model? And the answer is yes.",
    "start": "1999210",
    "end": "2004927"
  },
  {
    "text": "So we can see even pretty\ngood scaling properties, even with a tiny\namount of experts, which is very promising for\nthese models to be used,",
    "start": "2004927",
    "end": "2011480"
  },
  {
    "text": "even in much lower\ncompute regimes. Next slide.",
    "start": "2011480",
    "end": "2017669"
  },
  {
    "text": "Irwan, you want to go ahead? OK, yeah. So we'll look at\nwhat things look",
    "start": "2017670",
    "end": "2027450"
  },
  {
    "text": "like when we use different types\nof parallelism, namely expert parallelism, to add experts,\nmodel parallelism to shared",
    "start": "2027450",
    "end": "2035399"
  },
  {
    "text": "model ways across cores and,\nalso, their parallelism, which is sort of the dominant paradigm\nin deep learning at the moment.",
    "start": "2035400",
    "end": "2043290"
  },
  {
    "text": " And so I guess, in\nthe previous slides,",
    "start": "2043290",
    "end": "2050989"
  },
  {
    "text": "we mostly talked about\nexpert parallelism. But of course, dense models\nand large-scale dense models",
    "start": "2050989",
    "end": "2056460"
  },
  {
    "text": "use model parallelism. So GPT-3 and other large\nmodels, what they do is that they will simply\nshove models weights",
    "start": "2056460",
    "end": "2063658"
  },
  {
    "text": "across different cores. We have a question. ",
    "start": "2063659",
    "end": "2070310"
  },
  {
    "text": "Oh. Yeah, I just wanted to know\nbecause I think there was-- I don't know if you're\ngoing to address later--",
    "start": "2070310",
    "end": "2076030"
  },
  {
    "text": "but I think\nsomewhere in a paper, it said that the more\nexperts you have, the more\nsample-efficient it gets.",
    "start": "2076030",
    "end": "2083199"
  },
  {
    "text": "And I was just hoping\nthat you could give us some intuition about that. Because I don't understand\nwhy that would be the case.",
    "start": "2083199",
    "end": "2092300"
  },
  {
    "text": "So I guess-- yeah, maybe\nIrwan should take this. So I guess there's all of\nthis work on larger models are",
    "start": "2092300",
    "end": "2100150"
  },
  {
    "text": "more sample-efficient. And larger in the context\nof the scaling law works means more parameters\nand more FLOPS.",
    "start": "2100150",
    "end": "2106817"
  },
  {
    "text": "As you increase the\nnumber of experts, there's more parameters\nbut not more FLOPS. But the model is still\nlarger in a similar sense.",
    "start": "2106817",
    "end": "2114289"
  },
  {
    "text": "So I guess, building\non the intuition that larger models are more\nsample-efficient, in my mind, it's not necessarily\nthat surprising",
    "start": "2114290",
    "end": "2121180"
  },
  {
    "text": "that these models\nwith more experts that have more parameters are\nmore sample-efficient.",
    "start": "2121180",
    "end": "2127560"
  },
  {
    "text": "I guess that's my kind of\nhigh-level intuition for it. Yeah, I would say\nthat's kind of expected",
    "start": "2127560",
    "end": "2133630"
  },
  {
    "text": "that more experts leads to\nbetter sample efficiency,",
    "start": "2133630",
    "end": "2140039"
  },
  {
    "text": "especially if you look at the\ntraining step and the training time. ",
    "start": "2140040",
    "end": "2149539"
  },
  {
    "text": "OK, cool. So where were we? ",
    "start": "2149540",
    "end": "2156650"
  },
  {
    "text": "OK, so we'll look at how models\nweights are split over cores for different scenarios. So data parallelism\nis the first one.",
    "start": "2156650",
    "end": "2164580"
  },
  {
    "text": "So that's kind of\nthe typical setup that deep learning\nuses, especially",
    "start": "2164580",
    "end": "2171110"
  },
  {
    "text": "for not-so-large\nnetworks which don't require model parallelism. And so let me explain how--",
    "start": "2171110",
    "end": "2180099"
  },
  {
    "text": "yeah, I'll just go to the\nfinal figure right now and explain how to\nlook at this figure.",
    "start": "2180100",
    "end": "2185500"
  },
  {
    "text": " OK, so we have 16 processes,\nwhich are organized in the 4",
    "start": "2185500",
    "end": "2192750"
  },
  {
    "text": "by 4 mesh, right? So each dotted line, each\n4 by 4 dotted line here,",
    "start": "2192750",
    "end": "2197760"
  },
  {
    "text": "represents a different core. And the first row studies\nhow the model weights",
    "start": "2197760",
    "end": "2204030"
  },
  {
    "text": "are split over cores. And the second row\nillustrates how data--",
    "start": "2204030",
    "end": "2209610"
  },
  {
    "text": "so literally, examples and\ntokens-- are split over cores. ",
    "start": "2209610",
    "end": "2215609"
  },
  {
    "text": "And then the final\nthing that's required to understand this\nfigure is that each color",
    "start": "2215610",
    "end": "2224190"
  },
  {
    "text": "of the shaded squares here\nidentifies a unique weight matrix",
    "start": "2224190",
    "end": "2230700"
  },
  {
    "text": "So let's start with\ndata parallelism. So for data parallelism,\nthe same model weights",
    "start": "2230700",
    "end": "2236760"
  },
  {
    "text": "are replicated across all cores. And the data is simply\npartitioned over cores.",
    "start": "2236760",
    "end": "2243840"
  },
  {
    "text": "And so that's what this\ncorresponds to, if you like,",
    "start": "2243840",
    "end": "2250530"
  },
  {
    "text": "using the description of\nthe caption, the explanation of the caption, I just gave.",
    "start": "2250530",
    "end": "2256710"
  },
  {
    "text": "So next, we have\nmodel parallelism. That's kind of just like\na theoretical example.",
    "start": "2256710",
    "end": "2261720"
  },
  {
    "text": "Because in practice,\npeople always use model parallelism\nin conjunction with data parallelism.",
    "start": "2261720",
    "end": "2267780"
  },
  {
    "text": "But if you were to do\nonly model parallelism, then you would have a\nsingle model weight that is partitioned over all cores.",
    "start": "2267780",
    "end": "2275130"
  },
  {
    "text": "And your data would just be\nreplicated over all cores, instead.",
    "start": "2275130",
    "end": "2281860"
  },
  {
    "text": "So now we have model\nand data parallelism. And that's kind of\nthe typical scenario for large dense networks.",
    "start": "2281860",
    "end": "2288010"
  },
  {
    "text": "So in that case, model\nweights are partitioned among a subset of the cores,\nthe subset of cores that process",
    "start": "2288010",
    "end": "2295079"
  },
  {
    "text": "different batches of data. And so in that example\nhere, we have sort of four--",
    "start": "2295080",
    "end": "2301120"
  },
  {
    "text": "so the first\nsub-square here means that the model weights are\npartitioned across four cores.",
    "start": "2301120",
    "end": "2307680"
  },
  {
    "text": " And this is replicated four\ntimes for the data parallelism",
    "start": "2307680",
    "end": "2315480"
  },
  {
    "text": "dimension. On the data side for modeling\nand data parallelism,",
    "start": "2315480",
    "end": "2323310"
  },
  {
    "text": "the data here is replicated\nacross model parallel cores and partitioned across\ndata parallel cores.",
    "start": "2323310",
    "end": "2331030"
  },
  {
    "text": "So next, we have expert\nand data parallelism. So in that scenario, that's kind\nof similar to data parallelism.",
    "start": "2331030",
    "end": "2337900"
  },
  {
    "text": "But now, each core will\nhold a different model weight, which is illustrated\nby the different colors.",
    "start": "2337900",
    "end": "2343600"
  },
  {
    "text": " And for the data side, the\ndata is simply replicated--",
    "start": "2343600",
    "end": "2351010"
  },
  {
    "text": "sorry, there is partition\nacross all cores, just like in the data\nparallelism scenario.",
    "start": "2351010",
    "end": "2358030"
  },
  {
    "text": "And so finally, we have the\nrightmost column, which is--",
    "start": "2358030",
    "end": "2364340"
  },
  {
    "text": "I guess that's the setup used\nin the switch transformer paper for the larger models.",
    "start": "2364340",
    "end": "2370849"
  },
  {
    "text": "And so here, for the\nmodel partitioning, each expert is partitioned\nacross multiple cores.",
    "start": "2370850",
    "end": "2376850"
  },
  {
    "text": "So in that example,\nwe have four experts, each partitioned\nacross four cores.",
    "start": "2376850",
    "end": "2382430"
  },
  {
    "text": "And the data is replicated\nacross multiple parallel cores and partitioned across\ndata parallel cores.",
    "start": "2382430",
    "end": "2388339"
  },
  {
    "text": "So that's all a bit complex\nto understand orally. But the switch transformer\npaper has the same figure",
    "start": "2388340",
    "end": "2396290"
  },
  {
    "text": "with a nice caption\nto explain it. And yeah, maybe\nwe can, Barret, we",
    "start": "2396290",
    "end": "2403580"
  },
  {
    "text": "can add something\nquickly about how this is implemented in practice.",
    "start": "2403580",
    "end": "2409640"
  },
  {
    "text": "So there's this paper called\nMesh Transformer, which kind of",
    "start": "2409640",
    "end": "2415130"
  },
  {
    "text": "extends batch or\ndata parallelism to more general purpose,\nSPMD-style programming.",
    "start": "2415130",
    "end": "2423860"
  },
  {
    "text": "And so different labs\nhave different frameworks. But this paper kind\nof lays the foundation for general\nSPMD-distributed computing,",
    "start": "2423860",
    "end": "2432410"
  },
  {
    "text": "which is required for\ntraining large-scale models. And so under the\nmesh abstraction,",
    "start": "2432410",
    "end": "2439609"
  },
  {
    "text": "basically, we have\na mesh of processes. ",
    "start": "2439610",
    "end": "2444740"
  },
  {
    "text": "And so that mesh has\ndimensions, named dimensions. And these named\ndimensions specify",
    "start": "2444740",
    "end": "2451190"
  },
  {
    "text": "how the tensor dimensions will\nbe partitioned, or replicated, across the mesh dimensions.",
    "start": "2451190",
    "end": "2458210"
  },
  {
    "text": "And so just that\nsimple abstraction supports sort of\ndata parallelism or some other parallelism and\nespecially expert parallelism",
    "start": "2458210",
    "end": "2467030"
  },
  {
    "text": "at once. And so I invite whoever is\ninterested to also check that paper.",
    "start": "2467030",
    "end": "2472579"
  },
  {
    "text": "Because that kind of\nlays the foundation for understanding these things.",
    "start": "2472580",
    "end": "2478280"
  },
  {
    "text": " All right, Barret, want to go? Cool. Yeah, so next, we\nare going to talk",
    "start": "2478280",
    "end": "2484369"
  },
  {
    "text": "about how we take these\nparallelism strategies and kind of combine them\ntogether to make a 1.6 trillion",
    "start": "2484370",
    "end": "2490160"
  },
  {
    "text": "parameter sparse model. So next slide. ",
    "start": "2490160",
    "end": "2495927"
  },
  {
    "text": "So what we ended up\ndoing in this work was we trained two different\nvery large sparse models.",
    "start": "2495927",
    "end": "2502280"
  },
  {
    "text": "And we compared them to\nthe largest T5 models. So we can see the T5-XXL,\nwhich is a dense model,",
    "start": "2502280",
    "end": "2507650"
  },
  {
    "text": "and it was the largest one\ntrained in the T5 paper. And it has around 13\nbillion parameters. And here, we list a lot\nof the model dimensions,",
    "start": "2507650",
    "end": "2514370"
  },
  {
    "text": "like dmodel, dff, which are just\nthe various sizes and shapes of the tensors and stuff,\nthe number of layers,",
    "start": "2514370",
    "end": "2520220"
  },
  {
    "text": "the number of heads. And importantly, we also mention\nthe negative log perplexity",
    "start": "2520220",
    "end": "2525849"
  },
  {
    "text": "at step 250k and at 500k. And yeah, so we designed\ntwo sparse models",
    "start": "2525850",
    "end": "2532360"
  },
  {
    "text": "to test how scaling versus\nsparsity versus scaling versus sparsity and FLOPS work.",
    "start": "2532360",
    "end": "2539480"
  },
  {
    "text": "So first, let me talk\nabout Switch-XXL. So that has the same amount\nof FLOPS per token as T5-XXL",
    "start": "2539480",
    "end": "2544750"
  },
  {
    "text": "but has 64 experts. And this leads it to have\naround 400 billion parameters.",
    "start": "2544750",
    "end": "2550760"
  },
  {
    "text": "And we can see that\non a step basis, it actually performs quite\nwell and outperforms the T5-XXL",
    "start": "2550760",
    "end": "2555970"
  },
  {
    "text": "by quite a good margin. Interestingly, though, the\nthird model we designed, Switch-C, which has\n1.6 trillion parameters",
    "start": "2555970",
    "end": "2563109"
  },
  {
    "text": "but has significantly\nfewer FLOPS, almost 10 less flops per token than either\nof the above two models.",
    "start": "2563110",
    "end": "2568839"
  },
  {
    "text": "So it's really trading\nby reducing FLOPS to have way more\nsparse parameters.",
    "start": "2568840",
    "end": "2574670"
  },
  {
    "text": "And we can see on a step\nbasis the switch-C model works well but not as\nwell as, actually,",
    "start": "2574670",
    "end": "2580180"
  },
  {
    "text": "the higher FLOP model. But on a kind of a\nPareto axis, where we are looking at TPU hours\non the x-axis and not step,",
    "start": "2580180",
    "end": "2587859"
  },
  {
    "text": "the Switch-C model\nactually outperforms them both by a pretty large margin. So for pretraining\nperformance, we're",
    "start": "2587860",
    "end": "2593793"
  },
  {
    "text": "seeing that,\nactually, just having a lot of sparsity and less FLOPS\nactually can be quite good.",
    "start": "2593793",
    "end": "2599880"
  },
  {
    "text": "Next slide.  So again, those\ntwo sparse models are really trying to\nget at this hypothesis",
    "start": "2599880",
    "end": "2606510"
  },
  {
    "text": "that, actually,\nNoam Shazeer had, which is that parameters\nare good for more knowledge, reasoning.",
    "start": "2606510",
    "end": "2613020"
  },
  {
    "text": "And compute, aka FLOPS,\nis good for intelligence. And so we're going to try\nto get at that by taking",
    "start": "2613020",
    "end": "2618930"
  },
  {
    "text": "these different sparse models\nand then fine tuning them on different tasks,\nsome of which require more knowledge and\nthen others which require more",
    "start": "2618930",
    "end": "2625589"
  },
  {
    "text": "of reasoning, for whatever\nlike hand-wavy definition we want to give that.",
    "start": "2625590",
    "end": "2631660"
  },
  {
    "text": "So yeah, so for a\nfixed-- oh, go back. So for a fixed-- could you go back to\nthe previous slide?",
    "start": "2631660",
    "end": "2637150"
  },
  {
    "text": "Oh, yeah. Sorry. OK, so for a fixed quality on\nan upstream pre-training task, yeah, do parameters\nindependently matter?",
    "start": "2637150",
    "end": "2645100"
  },
  {
    "text": "So we're going to look at\ntwo tasks here, one of which is SuperGLUE, which is\nkind of our reasoning task. And then another\nis TriviaQA, which",
    "start": "2645100",
    "end": "2651333"
  },
  {
    "text": "is some knowledge\ntask, where it's like you just give\nit a question, and you have it\noutput an answer. ",
    "start": "2651333",
    "end": "2658660"
  },
  {
    "text": "OK, and so here,\nwe're going to take a look at superGLUE quality. So we can see on the x-axis is\nthe pretraining performance.",
    "start": "2658660",
    "end": "2664750"
  },
  {
    "text": "And the y-axis is the SuperGLUE\nscore after fine tuning. And interestingly, we\ncan see, definitely,",
    "start": "2664750",
    "end": "2670570"
  },
  {
    "text": "that the sparse models\ndefinitely are, for a fixed, pretraining perplexity,\ndo worse on fine tuning.",
    "start": "2670570",
    "end": "2677589"
  },
  {
    "text": "This can be especially\nnoticed at the upper right portion of the plot, where the\ndense models are definitely fine tuning better than\ntheir sparse counterpart.",
    "start": "2677590",
    "end": "2687010"
  },
  {
    "text": "Next slide. Interestingly, when we\nstudy it on the more knowledge-heavy tasks,\nthe sparse model",
    "start": "2687010",
    "end": "2692890"
  },
  {
    "text": "for a fixed\npretraining perplexity does disproportionately well. So for a model that roughly\nhas the same perplexity,",
    "start": "2692890",
    "end": "2699400"
  },
  {
    "text": "we're getting\nreally large boosts for these knowledge-heavy tasks. So this is pretty interesting. And it also really shows some\nof the dangers of comparing only",
    "start": "2699400",
    "end": "2708190"
  },
  {
    "text": "on your pretraining metrics. So these models could have the\nsame exact pretraining metric but very different properties\nwhen fine tuning them",
    "start": "2708190",
    "end": "2716290"
  },
  {
    "text": "on different tasks. Next slide. And interestingly, so yeah,\nall of the switch models",
    "start": "2716290",
    "end": "2724440"
  },
  {
    "text": "here are just various\nmodels that have, still, a good amount of FLOPS.",
    "start": "2724440",
    "end": "2729900"
  },
  {
    "text": "But the red model is\nactually the 1.6 trillion parameter sparse model\nthat has very few FLOPS",
    "start": "2729900",
    "end": "2737250"
  },
  {
    "text": "but a lot, a lot of parameters. And you can see that\nas the red dot here. And it does, actually,\ndisproportionately bad compared",
    "start": "2737250",
    "end": "2742620"
  },
  {
    "text": "to other sparse models that also\nhave pretty good perplexities. And so yeah, it's\ndefinitely very interesting.",
    "start": "2742620",
    "end": "2748200"
  },
  {
    "text": "And it shows that, for\nmodels during pretraining that have a lot of\nsparsity, they definitely suffer on some of these\nmore reasoning-heavy metrics",
    "start": "2748200",
    "end": "2755700"
  },
  {
    "text": "but do disproportionately\nwell for more of these knowledge-heavy tasks. Next slide.",
    "start": "2755700",
    "end": "2762450"
  },
  {
    "text": "And so here, we can see\nit as just a huge outlier for pretraining perplexity,\ndoing just incredibly well",
    "start": "2762450",
    "end": "2769230"
  },
  {
    "text": "on this downstream\nquestion answering task. Next slide, yeah.",
    "start": "2769230",
    "end": "2776099"
  },
  {
    "text": "OK, so also, one thing\nthat we were going to do is just look at the fine tuning\nproperties of sparse models",
    "start": "2776100",
    "end": "2781530"
  },
  {
    "text": "across a few scales and\njust see how they perform. Next slide.",
    "start": "2781530",
    "end": "2787252"
  },
  {
    "text": "And yeah, so here, we\ntry two different models. One is T5-base, and then we\nmake a FLOP-matched sparse",
    "start": "2787252",
    "end": "2792828"
  },
  {
    "text": "counterpoint. And when they say\nFLOP-matched, it's like each token will have\nthe same amount of FLOPS, but now we just have experts.",
    "start": "2792828",
    "end": "2798267"
  },
  {
    "text": "So we do this for\nboth base and large. And we see that, actually,\nacross almost all tasks besides two ARC tasks,\nthe sparse models",
    "start": "2798267",
    "end": "2805070"
  },
  {
    "text": "perform quite well, which\nis definitely promising. So we are seeing that these\nmodels are pretty robust.",
    "start": "2805070",
    "end": "2810110"
  },
  {
    "text": "They pretrain well. And then they also\nfine tune well when scaled appropriately by scaling\nup both the FLOPS and sparsity,",
    "start": "2810110",
    "end": "2817280"
  },
  {
    "text": "whereas the negative\nresults we've really seen are like when you just have a\nhuge amount of sparsity and not",
    "start": "2817280",
    "end": "2822319"
  },
  {
    "text": "too many FLOPS. Next slide.  And one, also, thing\nwe wanted to look at",
    "start": "2822320",
    "end": "2828790"
  },
  {
    "text": "was the multilingual training. So we were previously studying\nall of this on English only. And we also wanted to\nsee how sparsity helps",
    "start": "2828790",
    "end": "2835630"
  },
  {
    "text": "in the multilingual setting. Because we also felt like this\nwould be a very natural place for sparsity to work well,\nwhere, potentially, experts",
    "start": "2835630",
    "end": "2841690"
  },
  {
    "text": "could specialize\nacross languages. And we do see strong results. So on 91% of the languages, I\nthink, of around 100 languages,",
    "start": "2841690",
    "end": "2849700"
  },
  {
    "text": "we see over at\nleast a 4X speed-up over the mT5 dense model.",
    "start": "2849700",
    "end": "2856060"
  },
  {
    "text": "Next slide. Irwan, you want to go ahead?",
    "start": "2856060",
    "end": "2861420"
  },
  {
    "text": "No. Go. Go ahead. OK. So yeah, another thing we wanted\nto talk about was distillation.",
    "start": "2861420",
    "end": "2867230"
  },
  {
    "text": "So one downside of\nthese sparse models is that they'll have a\nlot more parameters, which means that if you're serving\nthese things or something,",
    "start": "2867230",
    "end": "2874382"
  },
  {
    "text": "you either need high\nthroughput use cases or you need to maybe distill\nit back down into a smaller",
    "start": "2874382",
    "end": "2879440"
  },
  {
    "text": "dense model. So here, what we do is we\nlook at like the T5-Base and Switch-Base And we look at\nits pretraining performance.",
    "start": "2879440",
    "end": "2885500"
  },
  {
    "text": "And then we go\nthrough some ablations of different\ndistillation techniques and find that with\nthe best techniques,",
    "start": "2885500",
    "end": "2890720"
  },
  {
    "text": "we can keep around\n30% of the quality improvements of sparsity\nwhile distilling it back down",
    "start": "2890720",
    "end": "2896540"
  },
  {
    "text": "into its dense counterpart. So next slide. ",
    "start": "2896540",
    "end": "2903795"
  },
  {
    "text": "Yeah, and then we\nkind of studied this across multiple scales. And again, we see around\n30% to 40% of the gains",
    "start": "2903795",
    "end": "2909760"
  },
  {
    "text": "can be kept when going\nfrom a sparse model",
    "start": "2909760",
    "end": "2915490"
  },
  {
    "text": "and distilling it back down into\nits FLOP-matched dense model. So you can get rid of up\nto 99% of the parameters",
    "start": "2915490",
    "end": "2921933"
  },
  {
    "text": "and still keep around 30%\nof the improvements, which is very promising. Next slide. Wait, I'm sorry.",
    "start": "2921933",
    "end": "2928119"
  },
  {
    "text": "Yeah? All right. Sorry about that. Can you just say that\nlast sentence again?",
    "start": "2928120",
    "end": "2933130"
  },
  {
    "text": "You said that you can keep the\n30% of the teacher's benefit. Yeah, basically.",
    "start": "2933130",
    "end": "2938730"
  },
  {
    "text": "So exactly. Yeah. Sure. Go on. Yeah, so we're looking at,\nyeah, you train a sparse model,",
    "start": "2938730",
    "end": "2945420"
  },
  {
    "text": "and then you\ndistill it back down to a dense model versus training\na dense model from scratch.",
    "start": "2945420",
    "end": "2950442"
  },
  {
    "text": "And you look at the gap between\nthe sparse and dense model from scratch versus the gap\nbetween the dense and then",
    "start": "2950443",
    "end": "2957472"
  },
  {
    "text": "the distilled dense model. What do you mean\nby distilled model?",
    "start": "2957472",
    "end": "2962550"
  },
  {
    "text": "Yeah, go for it. [INTERPOSING VOICES] Yeah, maybe let me just do\na quick, high-level summary",
    "start": "2962550",
    "end": "2969150"
  },
  {
    "text": "again. So what we'll do for\nour comparisons is we'll train a dense\nmodel from scratch,",
    "start": "2969150",
    "end": "2974619"
  },
  {
    "text": "we'll train a sparse\nmodel from scratch, and then we'll also\nrun a third experiment where we distill that sparse\nmodel down into a dense model.",
    "start": "2974620",
    "end": "2983690"
  },
  {
    "text": "What does distilling mean? We're basically trying to match\nthe teacher's logits, the kind",
    "start": "2983690",
    "end": "2990310"
  },
  {
    "text": "of standard thing of\nmatching either the logits or the soft probabilities\nfor each token",
    "start": "2990310",
    "end": "2995502"
  },
  {
    "text": "or something like that. ",
    "start": "2995502",
    "end": "3003680"
  },
  {
    "text": "OK, if I can jump\nin with my question. So what I'm struggling\nwith is, how do I interpret the\nline that says percent",
    "start": "3003680",
    "end": "3008828"
  },
  {
    "text": "of teacher and performance? Yeah, OK. So it's basically\nlooking at the gap",
    "start": "3008828",
    "end": "3015050"
  },
  {
    "text": "between the dense\nand sparse model. So we'll have the dense\nmodel get some performance. We'll have the sparse\nmodel get some performance.",
    "start": "3015050",
    "end": "3021952"
  },
  {
    "text": "And then the dense model\ndistilled from the sparse model would be somewhere in\nbetween that range.",
    "start": "3021952",
    "end": "3027800"
  },
  {
    "text": "And we're basically saying\nit's 30% through that range. So in a 0 to 1\ninterval, it's like 0.3",
    "start": "3027800",
    "end": "3033950"
  },
  {
    "text": "of the way from the dense\nto the sparse model. I see. So this is not saying that the\npercent of teacher performance",
    "start": "3033950",
    "end": "3039402"
  },
  {
    "text": "does not mean that if\nthe teachers say it-- if we use the teacher's guesses\nor predictions as the ground",
    "start": "3039402",
    "end": "3045290"
  },
  {
    "text": "truth, this is not saying that\nthe distilled model matches with the teacher\n33% of the time?",
    "start": "3045290",
    "end": "3051230"
  },
  {
    "text": "No, no, exactly. It's basically saying you\nget 30% of the quality improvements. Yeah, exactly. OK, cool.",
    "start": "3051230",
    "end": "3056810"
  },
  {
    "text": "And then, if we can\nback up this slide, I had a different question,\nbut I didn't want to interrupt. When we were talking about all\nof these different T5 bases--",
    "start": "3056810",
    "end": "3063510"
  },
  {
    "text": "and then, also, a few\nslides before this-- I don't know that much about T5. I'm curious to know\nwhen T5 is trained,",
    "start": "3063510",
    "end": "3069710"
  },
  {
    "text": "is there a weight penalty\nin the loss function? Is there a weight decay term?",
    "start": "3069710",
    "end": "3076020"
  },
  {
    "text": "No, there's no\nweight decay training with any of the sparse\nor dense models. I see. So out of curiosity,\nthen, how do dense models",
    "start": "3076020",
    "end": "3083099"
  },
  {
    "text": "perform compared\nto the switch model if you add some sort of\nweight regularization that incentivizes getting\nrid of useless weights?",
    "start": "3083100",
    "end": "3089985"
  },
  {
    "text": " Oh, so some kind of maybe L1\nterm or something like that.",
    "start": "3089985",
    "end": "3095397"
  },
  {
    "text": "Exactly, yeah. So I'm just wondering how\nmuch of-- because here, we're talking about the\nbenefits of sparsity. And I'm wondering how much\nof this benefit from sparsity",
    "start": "3095397",
    "end": "3101823"
  },
  {
    "text": "is due to the fact that\njust some of this-- I mean, effectively what\nthe switch model is doing, if I understand correctly--\nmaybe I don't-- what I",
    "start": "3101823",
    "end": "3107950"
  },
  {
    "text": "understand is that the switch\nmodel in the feedforward layer, it's just like you fix\nsome the weight to be zero.",
    "start": "3107950",
    "end": "3113319"
  },
  {
    "text": "That's what it\nmeans to be sparse. Well, actually,\nwe're kind of really trying to inject more weights.",
    "start": "3113320",
    "end": "3119110"
  },
  {
    "text": "So we're actually\nkind of trying to do-- it's a little bit,\nmaybe, paradoxical. Because we're saying\nswitch transformer, but our idea is to be\nlike, hey, we actually",
    "start": "3119110",
    "end": "3125410"
  },
  {
    "text": "want to just have significantly\nmore weights, not less weights. It's kind of like you\nwould zero out weights",
    "start": "3125410",
    "end": "3130810"
  },
  {
    "text": "but within a much larger weight\nmatrix, if that makes sense. I see, yes. And so to me, it seems like a\nrelevant baseline to just ask,",
    "start": "3130810",
    "end": "3137470"
  },
  {
    "text": "what happens if I\nhave the dense matrix but I incentivize it\nwith, say, an L1 or L2 penalty on the weights? And I'd be curious to\nknow how that compares.",
    "start": "3137470",
    "end": "3145098"
  },
  {
    "text": "Yeah, we didn't run this. But also, that kind of gets rid\nof weights for the dense model. So if anything-- Sure, sure. [INAUDIBLE]",
    "start": "3145098",
    "end": "3150790"
  },
  {
    "text": "So yeah, that -- Also-- and the last point is\nif you just add an L1 penalty",
    "start": "3150790",
    "end": "3156820"
  },
  {
    "text": "loss, you're not going to\nhave structural sparsity, where it's like, here,\nit's not random weights",
    "start": "3156820",
    "end": "3166240"
  },
  {
    "text": "in your giant weight matrix\nthat are zeroed out, right? It's really blocks depending--\nor blocks corresponding",
    "start": "3166240",
    "end": "3171940"
  },
  {
    "text": "to each x part. And so that structure allows\nthe whole communication stuff.",
    "start": "3171940",
    "end": "3179050"
  },
  {
    "text": "And that's just the\nfact that you have multiple cores [INAUDIBLE].",
    "start": "3179050",
    "end": "3185410"
  },
  {
    "text": "I totally agree with\nthat block structure. And that's what\nI'm trying to say is that the switch\nhas this very rich-- it's not just sparse, it\nalso has this rich structure.",
    "start": "3185410",
    "end": "3193053"
  },
  {
    "text": "And what I'm trying to do\nin my mind is disentangle, is the sparsity what's\noffering an advantage? Or is this additional\nstructure that you've",
    "start": "3193053",
    "end": "3199030"
  },
  {
    "text": "built in, is that what\ngives the performance gain? So that's why I'm asking. So the block structure\nis what enables",
    "start": "3199030",
    "end": "3209200"
  },
  {
    "text": "to leverage the fact that\nyou have multiple cores. If you didn't have\nthat block structure,",
    "start": "3209200",
    "end": "3214390"
  },
  {
    "text": "you'd still have to\nroute to everything. And so you have more\ncommunication costs and so on.",
    "start": "3214390",
    "end": "3220599"
  },
  {
    "text": "And then your first\nquestion was what, sorry? I'm not actually sure\nif there was a question. I guess what I'm trying to say\nis I'm trying to disambiguate--",
    "start": "3220600",
    "end": "3227233"
  },
  {
    "text": "yeah, anyways. But I agree. It's a little bit weird. Because sparsity kind\nof is like a spectrum",
    "start": "3227233",
    "end": "3233680"
  },
  {
    "text": "of meaning false sparsity,\njust, for example, compression. And like model pruning\nis a form of sparsity",
    "start": "3233680",
    "end": "3240100"
  },
  {
    "text": "but also through\ntransformer and MoE also refer to it as sparsity.",
    "start": "3240100",
    "end": "3245200"
  },
  {
    "text": "And they're kind of related,\nbut definitely, they're aiming at different things. This is a really interesting\nidea of it's sparse,",
    "start": "3245200",
    "end": "3252250"
  },
  {
    "text": "but you have more parameters. I'll have to think\nabout that more. Thank you. Yeah, it's kind of sparse\nwithin this giant weight",
    "start": "3252250",
    "end": "3259892"
  },
  {
    "text": "matrix, which is [INAUDIBLE]. Exactly, yeah. Yeah, yeah. I hadn't appreciated that. So I appreciate you\npointing that out.",
    "start": "3259892",
    "end": "3267100"
  },
  {
    "text": "Thank you. I have a follow-up question\non the distillation part.",
    "start": "3267100",
    "end": "3272380"
  },
  {
    "text": "Yeah, of course. OK, so if you distill it\nback down, now you have one-- technically, you're back to\nthe dense layer architecture,",
    "start": "3272380",
    "end": "3280089"
  },
  {
    "text": "right? Mm-hmm. So now, the entire\nidea of expert",
    "start": "3280090",
    "end": "3285640"
  },
  {
    "text": "is that certain tokens would\nbe sent to different experts because they just\nlike, I don't know, are more specialized in figuring\nsomething out about this token.",
    "start": "3285640",
    "end": "3294050"
  },
  {
    "text": "So now if you go back\nto this dense layer, aren't you basically\nonly serving",
    "start": "3294050",
    "end": "3303730"
  },
  {
    "text": "whichever expert you\nbased this dense layer on? These tokens will\nprobably perform well,",
    "start": "3303730",
    "end": "3308950"
  },
  {
    "text": "and all the other tokens are\nkind of left behind, right? ",
    "start": "3308950",
    "end": "3316575"
  },
  {
    "text": "I'm actually-- sorry,\nI don't think I'm fully understanding your question. So are you getting\nat, we're distilling",
    "start": "3316575",
    "end": "3322110"
  },
  {
    "text": "this on a specific\ndata set so that-- I'm thinking about how\nto use that, like what--",
    "start": "3322110",
    "end": "3329099"
  },
  {
    "text": "Yeah, so maybe concretely,\nso for superGLUE, let's say you want a server\nmodel that does superGLUE well.",
    "start": "3329100",
    "end": "3334349"
  },
  {
    "text": "I think the idea\nis that you distill the sparse model into a\ndense model on superGLUE. So then you kind of get\nthis compressed dense model",
    "start": "3334350",
    "end": "3341280"
  },
  {
    "text": "that now performs\nbetter than if you were to just train\nit from scratch or train it from a\npretrained dense model.",
    "start": "3341280",
    "end": "3346808"
  },
  {
    "text": "So then it's like-- [INAUDIBLE] --to use, though. Say that again? You have to pick\none expert, right?",
    "start": "3346808",
    "end": "3353110"
  },
  {
    "text": "No, no, no. You can just\ndistill all of the-- because they're just\nmatching the model outputs. So you can just treat\nthe sparse model",
    "start": "3353110",
    "end": "3358950"
  },
  {
    "text": "as kind of a black box thing. All we're doing is\njust trying to have the dense model match the\nactual final token predictions.",
    "start": "3358950",
    "end": "3365790"
  },
  {
    "text": "Oh, god. OK, got it. OK. Sorry. I was not familiar with\nthe idea of distillation.",
    "start": "3365790",
    "end": "3371410"
  },
  {
    "text": "So I think that was\nmy entire confusion. OK, thanks. Yeah, of course.",
    "start": "3371410",
    "end": "3377369"
  },
  {
    "text": "Because I guess\none motivation here is that having experts\ncan make solving",
    "start": "3377370",
    "end": "3384300"
  },
  {
    "text": "a little bit more\ndifficult. Because it requires bigger topologies. Let's say you have\neight experts.",
    "start": "3384300",
    "end": "3392190"
  },
  {
    "text": "You need-- well, I guess you can\nhave multiple experts on fewer cores.",
    "start": "3392190",
    "end": "3397650"
  },
  {
    "text": "But let's just say they're a\nlittle bit harder to solve. And so if we can get the\nbenefits from sparsity",
    "start": "3397650",
    "end": "3406859"
  },
  {
    "text": "at pretraining, then use\ndistillation to a dense model for solving, that\ncan be beneficial.",
    "start": "3406860",
    "end": "3414820"
  },
  {
    "text": "So I think that would solve the\nmotivation for that experiment, right, Barret? Mm-hmm.",
    "start": "3414820",
    "end": "3420090"
  },
  {
    "text": "Yeah, exactly.  OK, where were we?",
    "start": "3420090",
    "end": "3428339"
  },
  {
    "text": "Yes, kind of just wrapping up. Yeah, go ahead. No, go ahead. But I just had, I think,\none more student question.",
    "start": "3428340",
    "end": "3434809"
  },
  {
    "text": "So yeah. Oh, yeah, go ahead. [INAUDIBLE], ask it now. Oh, yeah, yeah, sounds good.",
    "start": "3434810",
    "end": "3440610"
  },
  {
    "text": "Yeah, thanks, guys,\nfor the talk so far. Just a quick question. Was wondering if you think there\nare any interesting directions",
    "start": "3440610",
    "end": "3447089"
  },
  {
    "text": "around building models that\nare explicitly optimized for parallel training?",
    "start": "3447090",
    "end": "3454590"
  },
  {
    "text": "I guess the MoE model seems like\nit does a really good job here. And also, at\ninference time, it's",
    "start": "3454590",
    "end": "3461790"
  },
  {
    "text": "very useful to have fewer\nFLOPS per computation",
    "start": "3461790",
    "end": "3467810"
  },
  {
    "text": "or for forward tasks. But, I guess, do\nyou think that there are any interesting\ndirections around distributed",
    "start": "3467810",
    "end": "3474270"
  },
  {
    "text": "training where you might have\nmodels that are explicitly architected to have a\nlot of parallel heads",
    "start": "3474270",
    "end": "3482820"
  },
  {
    "text": "or other features that\nare kind of embarrassingly parallelizable?",
    "start": "3482820",
    "end": "3488549"
  },
  {
    "text": "Or does just using standard\nscale up the models by adding more layers\nand then just get away",
    "start": "3488550",
    "end": "3496589"
  },
  {
    "text": "with using model and data\nparallelism work well enough? Yeah. So I think-- so let me just make\nsure I'm fully understanding.",
    "start": "3496590",
    "end": "3503513"
  },
  {
    "text": "So I think also, right now,\neven our models are definitely very co-designed with the\nhardware and the shapes",
    "start": "3503513",
    "end": "3508849"
  },
  {
    "text": "and things. So yeah, I think\nat a high level, yes, I think there's a ton\nof interesting research",
    "start": "3508850",
    "end": "3514250"
  },
  {
    "text": "on co-designing the hardware,\nthe partitioning algorithms, and the models. I think, given that\nwe have this kind",
    "start": "3514250",
    "end": "3521000"
  },
  {
    "text": "of SPMD mesh-style\npartitioning, we are already designing our models\nin ways that fit it really",
    "start": "3521000",
    "end": "3526430"
  },
  {
    "text": "well. So for example, when we\nwant to scale up our model, one of the first dimensions\nwe go to scale up is the internal\nhidden dimension.",
    "start": "3526430",
    "end": "3532408"
  },
  {
    "text": "Because there's some\nreally nice properties of scaling up this dimension. It basically becomes kind\nof independent to some",
    "start": "3532408",
    "end": "3537500"
  },
  {
    "text": "of the communication costs. It's really good when\nlooking at the compute to memory operations on these\ncompute devices and stuff.",
    "start": "3537500",
    "end": "3544610"
  },
  {
    "text": "[INAUDIBLE] Yeah, exactly. I think when we're even\ndesigning these models, we're really setting\ndimensions such",
    "start": "3544610",
    "end": "3549980"
  },
  {
    "text": "that it maps well into hardware. So it's almost like, given\nthat we have this model data parallelism, we're actually\ndesigning models more for it.",
    "start": "3549980",
    "end": "3558180"
  },
  {
    "text": "But I also think\nthat there's a ton of new, interesting,\ndistributed algorithms and stuff like that which makes designing\nmodels very interesting.",
    "start": "3558180",
    "end": "3564240"
  },
  {
    "text": "I think one thing that\nI think is really cool is the Microsoft ZeRO\npartitioning, too, which also adds some really nice\nimplications for how to design",
    "start": "3564240",
    "end": "3571715"
  },
  {
    "text": "and scale models and stuff. So yeah, I think this is a very\nfruitful research direction,",
    "start": "3571715",
    "end": "3577670"
  },
  {
    "text": "if that kind of\nanswered your question. Yeah, no, that was super\nhelpful and interesting. Thanks. Yeah, yeah, definitely.",
    "start": "3577670",
    "end": "3583970"
  },
  {
    "text": "I'm very optimistic\non the future of us designing the\nhardware, the model, the partitioning\nstrategies all together.",
    "start": "3583970",
    "end": "3589340"
  },
  {
    "text": "Because really, to\nget it to work well, you kind of have to\nknow about all three and kind of intertwine\nthe development of them.",
    "start": "3589340",
    "end": "3595600"
  },
  {
    "text": "Yeah, that sounds awesome.  Cool. Yeah, so just to summarize,\nso switch transformer",
    "start": "3595600",
    "end": "3603110"
  },
  {
    "text": "is a nice simplification\nover mixture of experts. And we're seeing that we\nget really strong speedup improvements on pretraining over\na lot of the T5 models, which",
    "start": "3603110",
    "end": "3611390"
  },
  {
    "text": "are very strong baselines. We're seeing that\nwe can efficiently distill the sparse\nmodels back to dense ones",
    "start": "3611390",
    "end": "3617299"
  },
  {
    "text": "and get improved both\npretraining and fine tuning through some of these newer\ntechniques we talked about.",
    "start": "3617300",
    "end": "3623090"
  },
  {
    "text": "And we're also seeing that\nthe models are working on multilingual data and that\nwe can now easily successfully",
    "start": "3623090",
    "end": "3628640"
  },
  {
    "text": "train up to 1.6 trillion\nparameter models, which is pretty promising. And next slide.",
    "start": "3628640",
    "end": "3635290"
  },
  {
    "text": "And so we also wanted to go\ninto two slides about some newer work about actually using these\nkind of models for computer vision and, actually,\nalso a little bit",
    "start": "3635290",
    "end": "3642080"
  },
  {
    "text": "of how they can be\nused to actually do some level of\nadaptive computation, where not only now, each input\ngets different weights but also",
    "start": "3642080",
    "end": "3648998"
  },
  {
    "text": "sometimes, different inputs\nwill have different amounts of compute applied to it. And so there is some really\ngreat work of doing this out",
    "start": "3648998",
    "end": "3655910"
  },
  {
    "text": "of the Google Zurich team. And yeah, they're just doing\nit for image classification. And they're basically\nseeing a lot",
    "start": "3655910",
    "end": "3662060"
  },
  {
    "text": "of the similar types of\nscaling properties, where scaling up the number of\nexperts and using sparsity allows them to get good\nperformance design, image",
    "start": "3662060",
    "end": "3669259"
  },
  {
    "text": "classification. Next slide. ",
    "start": "3669260",
    "end": "3674407"
  },
  {
    "text": "And interestingly, one\nof the things they do is, as we talked about\nthe capacity factor. So we were talking about\nvalues of 1, 1.25, 2.0,",
    "start": "3674407",
    "end": "3681400"
  },
  {
    "text": "which means at a\nvalue of 2.0, there's buffer for two\ntokens per expert. But they actually study\nit going less than 1.",
    "start": "3681400",
    "end": "3688390"
  },
  {
    "text": "So that means that\nat 0.5, that means there's only room for\nhalf the number of tokens. And the nice part\nis that they did",
    "start": "3688390",
    "end": "3694580"
  },
  {
    "text": "this for image classification. And also, in images, there's\njust a lot of redundancy. And they notice that\nyou can actually",
    "start": "3694580",
    "end": "3699790"
  },
  {
    "text": "get really good performance\nby only allowing up to 1/10 of the parts of the image to\nbe processed by a sparse layer.",
    "start": "3699790",
    "end": "3707260"
  },
  {
    "text": "So yeah, we think this is a\nreally nice direction, too, in terms of combining\nsparsity, along with adaptive computation.",
    "start": "3707260",
    "end": "3715560"
  },
  {
    "text": "And yeah, thanks so\nmuch for having us. That's the talk.",
    "start": "3715560",
    "end": "3720755"
  },
  {
    "text": "Well, thank you, Barret\nand Irwan, for coming here.",
    "start": "3720756",
    "end": "3728220"
  },
  {
    "text": "So [CLAPS] thanks a lot. So I will just ask a\nbunch of questions.",
    "start": "3728220",
    "end": "3734610"
  },
  {
    "text": "And then we can have,\nafter the class, an open question panel\nfor the students.",
    "start": "3734610",
    "end": "3740019"
  },
  {
    "text": "So one thing is, have you tried\nusing more linear attention mechanisms, like\nreformers and other stuff,",
    "start": "3740020",
    "end": "3745950"
  },
  {
    "text": "to scale the computation?  I personally haven't.",
    "start": "3745950",
    "end": "3751770"
  },
  {
    "text": "Maybe-- I haven't,\npersonally, done this. ",
    "start": "3751770",
    "end": "3758530"
  },
  {
    "text": "Yeah, so I guess we\ncan maybe comment on how the cost coming\nfrom the attention maps",
    "start": "3758530",
    "end": "3765849"
  },
  {
    "text": "isn't the dominant cost in\nthese large transformers.",
    "start": "3765850",
    "end": "3772600"
  },
  {
    "text": "So the motivation for\nusing linear attention, like performance, is that it\nreduces the quadratic cost",
    "start": "3772600",
    "end": "3779829"
  },
  {
    "text": "of attention. That's right. But so far, at least\nin some typical NLP",
    "start": "3779830",
    "end": "3787880"
  },
  {
    "text": "setups like superGLUE, C4, and\nso on, as you scale the models, most of the memory comes\nfrom the model weights,",
    "start": "3787880",
    "end": "3797320"
  },
  {
    "text": "as opposed to the\nattention maps. That's also because using very\nlong context or sequence length",
    "start": "3797320",
    "end": "3806650"
  },
  {
    "text": "doesn't prove that fruitful. And so just working\nwith the [INAUDIBLE]",
    "start": "3806650",
    "end": "3813130"
  },
  {
    "text": "self-attention mechanism is a\nvery strong baseline already. Got it.",
    "start": "3813130",
    "end": "3818270"
  },
  {
    "text": "OK. So another question is, do\nyou think this mechanism is even more scalable?",
    "start": "3818270",
    "end": "3823930"
  },
  {
    "text": "Can it go on and be like 10\ntrillion parameter models, stuff like that? What do you think?",
    "start": "3823930",
    "end": "3830032"
  },
  {
    "text": "Yeah, definitely. I think totally. I think, honestly, one of the\nbiggest constraints is that-- and this isn't even\nnecessarily a constraint--",
    "start": "3830032",
    "end": "3837099"
  },
  {
    "text": "it's just you have to fit\nthe parameters somewhere. And there's just limited\nstorage on devices. But if you get enough\ndevices such that,",
    "start": "3837100",
    "end": "3843670"
  },
  {
    "text": "yeah, you can just\npartition the weights. It's like yeah, I don't\nsee anything stopping it.",
    "start": "3843670",
    "end": "3848990"
  },
  {
    "text": "Got it. So what do you\nthink, personally, is the thing that gives\nthe direction, like scaling",
    "start": "3848990",
    "end": "3855380"
  },
  {
    "text": "up transformers will go and do? Will there be more\nworks that are trying to just use this\ntransformer-like mechanisms,",
    "start": "3855380",
    "end": "3861470"
  },
  {
    "text": "like mixture of experts? Or do you think there's\ngoing to be other things that the community needs? I mean, definitely think mixture\nof experts should find its way,",
    "start": "3861470",
    "end": "3869060"
  },
  {
    "text": "or at least sparse layers, like\nswitch transformer and stuff, will definitely, I\nthink, find their way into the future of large models.",
    "start": "3869060",
    "end": "3874143"
  },
  {
    "text": "I think they really\nconfer a lot of benefits. And they're also very good in\nhigh throughput applications.",
    "start": "3874143",
    "end": "3879780"
  },
  {
    "text": "So I think the one thing-- so the one downside\non sparsity is if you look at the\nperformance per model weight,",
    "start": "3879780",
    "end": "3885320"
  },
  {
    "text": "they're going to always be\nworse than dense models. So it's like if you really\nare constrained on I want to design the\nbest model I can",
    "start": "3885320",
    "end": "3891710"
  },
  {
    "text": "to fit on as small\nof a device as I can, then they're probably not\ngoing to be the best solution. Because the sparse\nweights just aren't",
    "start": "3891710",
    "end": "3897795"
  },
  {
    "text": "as good as just the\ndense weight that's being used for everything. So I think it really\ndepends on the application. But I'm very optimistic\nfor when we're",
    "start": "3897795",
    "end": "3904233"
  },
  {
    "text": "training these models during\npretraining with lots of data parallelism and then we're\nserving them in medium to higher throughput examples,\nI feel like they could actually",
    "start": "3904233",
    "end": "3911543"
  },
  {
    "text": "just be a pretty big win. So that's kind of\nmy thoughts on how I think sparsity will be used.",
    "start": "3911543",
    "end": "3917070"
  },
  {
    "text": "In terms of other\nthings, yeah, I think-- I don't know. There's a ton of\nexciting research, from everything from a lot\nof the linear attention",
    "start": "3917070",
    "end": "3922693"
  },
  {
    "text": "stuff, adaptive computation,\nnew pretraining objectives. Yeah, it's hard to know what\nthe future will look like.",
    "start": "3922693",
    "end": "3928830"
  },
  {
    "text": "But a lot of exciting\nthings to look forward to. Sounds good. OK. So we can just now have a\nround of student questions.",
    "start": "3928830",
    "end": "3936660"
  },
  {
    "text": "So I'll just stop the recording. ",
    "start": "3936660",
    "end": "3944000"
  }
]