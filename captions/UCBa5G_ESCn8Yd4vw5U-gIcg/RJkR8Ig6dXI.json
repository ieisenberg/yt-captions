[
  {
    "text": "Now we are going to talk about the second method in this idea of neighborhood, uh,",
    "start": "4070",
    "end": "10260"
  },
  {
    "text": "sampling and trying to limit, uh, the- the,",
    "start": "10260",
    "end": "14100"
  },
  {
    "text": "uh, batch size, uh,",
    "start": "14100",
    "end": "15960"
  },
  {
    "text": "issues, uh, in graph neural networks.",
    "start": "15960",
    "end": "18600"
  },
  {
    "text": "Um, and the way we are going to do this is,",
    "start": "18600",
    "end": "21060"
  },
  {
    "text": "uh, to start with the following observation, right?",
    "start": "21060",
    "end": "23115"
  },
  {
    "text": "That the size of computational graphs, um,",
    "start": "23115",
    "end": "25935"
  },
  {
    "text": "can still become very big, uh,",
    "start": "25935",
    "end": "27685"
  },
  {
    "text": "or increases exponentially with the number of, uh, GNN layers.",
    "start": "27685",
    "end": "31355"
  },
  {
    "text": "And what we can also observe and notice is that, um,",
    "start": "31355",
    "end": "35020"
  },
  {
    "text": "computation in these, uh- computational graphs",
    "start": "35020",
    "end": "38300"
  },
  {
    "text": "can be very much redundant because of shared neighbors.",
    "start": "38300",
    "end": "41899"
  },
  {
    "text": "Like if you think for example, two nodes,",
    "start": "41900",
    "end": "43895"
  },
  {
    "text": "A and B, and you think about the computation graphs,",
    "start": "43895",
    "end": "47210"
  },
  {
    "text": "then they have, uh- because they share- they have a lot of friends in common,",
    "start": "47210",
    "end": "51645"
  },
  {
    "text": "these computation graphs are heavily redundant, right?",
    "start": "51645",
    "end": "54590"
  },
  {
    "text": "Like the- the, uh,",
    "start": "54590",
    "end": "55985"
  },
  {
    "text": "both to compute A and compute B,",
    "start": "55985",
    "end": "58745"
  },
  {
    "text": "the subgraphs the comp- the parts of the computation graphs",
    "start": "58745",
    "end": "61760"
  },
  {
    "text": "between- below node C and D are identical.",
    "start": "61760",
    "end": "64820"
  },
  {
    "text": "So this would be the duplicate- this will be duplicated, uh, um, computation.",
    "start": "64820",
    "end": "69430"
  },
  {
    "text": "And, uh, there are two approaches to do this.",
    "start": "69430",
    "end": "72870"
  },
  {
    "text": "One is to, uh, realize that this, uh,",
    "start": "72870",
    "end": "75610"
  },
  {
    "text": "computation is duplicated then computed only once.",
    "start": "75610",
    "end": "79360"
  },
  {
    "text": "Uh, there is a very nice paper about this at last year KDD called,",
    "start": "79360",
    "end": "83285"
  },
  {
    "text": "uh, uh, HAGs, hierarchical, um,",
    "start": "83285",
    "end": "85790"
  },
  {
    "text": "aggregation, uh, graphs that basically prevent,",
    "start": "85790",
    "end": "89499"
  },
  {
    "text": "um, uh, multiple computations, so redundant computations.",
    "start": "89499",
    "end": "93455"
  },
  {
    "text": "Um, and the- the second case will be about Cluster-GCN,",
    "start": "93455",
    "end": "97895"
  },
  {
    "text": "which is what I'm going to talk about today.",
    "start": "97895",
    "end": "100555"
  },
  {
    "text": "And the way we are, uh, motivating Cluster-GCN is to remember what is a full ba- batch,",
    "start": "100555",
    "end": "106770"
  },
  {
    "text": "uh, graph neural network, right?",
    "start": "106770",
    "end": "108210"
  },
  {
    "text": "A full batch graph neural network is that all the embeddings",
    "start": "108210",
    "end": "111979"
  },
  {
    "text": "for all the nodes are embedded together in a- in a single pass.",
    "start": "111980",
    "end": "115850"
  },
  {
    "text": "So basically, um, embeddings of all the nodes are computed for layer 1,",
    "start": "115850",
    "end": "120065"
  },
  {
    "text": "embeddings for all the nodes are then computed for layer 2,",
    "start": "120065",
    "end": "123200"
  },
  {
    "text": "uh, and so on, right?",
    "start": "123200",
    "end": "124399"
  },
  {
    "text": "The point is you need to have the embeddings of",
    "start": "124400",
    "end": "127040"
  },
  {
    "text": "all the nodes at lab- at layer L minus 1 to be",
    "start": "127040",
    "end": "130610"
  },
  {
    "text": "able to compute the embeddings for all the other nodes- for",
    "start": "130610",
    "end": "133940"
  },
  {
    "text": "all the nodes at layer L. So it means that,",
    "start": "133940",
    "end": "137579"
  },
  {
    "text": "uh, in each layer,",
    "start": "137579",
    "end": "138750"
  },
  {
    "text": "2 times number of edges,",
    "start": "138750",
    "end": "140595"
  },
  {
    "text": "uh, messages need to be, uh,",
    "start": "140595",
    "end": "142180"
  },
  {
    "text": "exchanged because every node needs to connect-",
    "start": "142180",
    "end": "144685"
  },
  {
    "text": "collect the- the message is from its neighbors,",
    "start": "144685",
    "end": "147825"
  },
  {
    "text": "so there are two messages along each undirected edge, you know,",
    "start": "147825",
    "end": "152030"
  },
  {
    "text": "for both, uh, endpoints,",
    "start": "152030",
    "end": "154375"
  },
  {
    "text": "and, uh, what is the observation?",
    "start": "154375",
    "end": "156975"
  },
  {
    "text": "The observation is that, uh,",
    "start": "156975",
    "end": "158305"
  },
  {
    "text": "the amount of computation we need to do in a single layer of, uh, let's say,",
    "start": "158305",
    "end": "162590"
  },
  {
    "text": "uh, full batch, uh,",
    "start": "162590",
    "end": "163819"
  },
  {
    "text": "implementation is linear in the size of the graph.",
    "start": "163820",
    "end": "166910"
  },
  {
    "text": "It's linear in the number of edges.",
    "start": "166910",
    "end": "168635"
  },
  {
    "text": "So it means it is very fast.",
    "start": "168635",
    "end": "170640"
  },
  {
    "text": "But the problem is that the graphs are too big.",
    "start": "170640",
    "end": "173330"
  },
  {
    "text": "Uh, so we cannot do this kind of at once,",
    "start": "173330",
    "end": "176880"
  },
  {
    "text": "uh, in the GPU.",
    "start": "176880",
    "end": "178410"
  },
  {
    "text": "So the insight from full-batch GNN is that",
    "start": "178410",
    "end": "181940"
  },
  {
    "text": "layer-wise node embedding updates allow us to reuse embeddings from the previous layer.",
    "start": "181940",
    "end": "187855"
  },
  {
    "text": "Um, and this significantly reduces the computational redundancy of,",
    "start": "187855",
    "end": "192440"
  },
  {
    "text": "uh, neighborhood sampling, right?",
    "start": "192440",
    "end": "193880"
  },
  {
    "text": "Uh, this means that this layer-wise update to generate",
    "start": "193880",
    "end": "197150"
  },
  {
    "text": "embeddings from one layer to the next is very cheap, right?",
    "start": "197150",
    "end": "201590"
  },
  {
    "text": "All I need to do is previous layer embeddings.",
    "start": "201590",
    "end": "203760"
  },
  {
    "text": "I need to aggregate them,",
    "start": "203760",
    "end": "205159"
  },
  {
    "text": "combine them with the previous layer embedding of a given node",
    "start": "205160",
    "end": "207920"
  },
  {
    "text": "and I have a new, uh- a new embedding.",
    "start": "207920",
    "end": "210650"
  },
  {
    "text": "So what this means is that,",
    "start": "210650",
    "end": "211879"
  },
  {
    "text": "uh, this can be done very fast.",
    "start": "211880",
    "end": "213680"
  },
  {
    "text": "It's only linear time operation in the size of the graph because it's aggregation,",
    "start": "213680",
    "end": "218719"
  },
  {
    "text": "uh, uh, basically takes time linear because",
    "start": "218720",
    "end": "221330"
  },
  {
    "text": "it's linear number of edges over which we aggregate.",
    "start": "221330",
    "end": "224275"
  },
  {
    "text": "So this sig- means that the computations are very fast.",
    "start": "224275",
    "end": "228700"
  },
  {
    "text": "But of course the problem is that layer-wise update is not feasible",
    "start": "228700",
    "end": "232099"
  },
  {
    "text": "for the entire graph at once because of the GPU memory.",
    "start": "232100",
    "end": "235970"
  },
  {
    "text": "So now, what the idea is in the Cluster-GCN is that can we sample",
    "start": "235970",
    "end": "242990"
  },
  {
    "text": "the entire graph into small sub-parts and then",
    "start": "242990",
    "end": "246920"
  },
  {
    "text": "perform full batch implementation on those subgraphs.",
    "start": "246920",
    "end": "251900"
  },
  {
    "text": "All right. So the idea is the following.",
    "start": "251900",
    "end": "254465"
  },
  {
    "text": "I'll take the large graph.",
    "start": "254465",
    "end": "255860"
  },
  {
    "text": "I'm going to sample a subgraph of it.",
    "start": "255860",
    "end": "258000"
  },
  {
    "text": "I don't know maybe I'll just say I'll sample the subgraph,",
    "start": "258000",
    "end": "260634"
  },
  {
    "text": "and then I'm going, uh, in this subgraph I'm able to fit it on a GPU.",
    "start": "260634",
    "end": "264740"
  },
  {
    "text": "So now I can apply a full batch implementation,",
    "start": "264740",
    "end": "268505"
  },
  {
    "text": "uh, in the GPU on that sampled subgraph.",
    "start": "268505",
    "end": "271825"
  },
  {
    "text": "All right? So that's essentially the idea.",
    "start": "271825",
    "end": "274295"
  },
  {
    "text": "So the difference between neighborhood sampling is that",
    "start": "274295",
    "end": "277040"
  },
  {
    "text": "there we compute the computation graph for each individual node.",
    "start": "277040",
    "end": "281080"
  },
  {
    "text": "Um, here, we are going to sample an entire subgraph of the original graph and",
    "start": "281080",
    "end": "286789"
  },
  {
    "text": "then pretend that this is the graph of interest and",
    "start": "286790",
    "end": "289400"
  },
  {
    "text": "just perform GNN on that, uh, subgraph.",
    "start": "289400",
    "end": "292990"
  },
  {
    "text": "Right? So then the question is,",
    "start": "292990",
    "end": "295710"
  },
  {
    "text": "how should I sample these subgraphs?",
    "start": "295710",
    "end": "297600"
  },
  {
    "text": "What are good subgraphs for training GNNs?",
    "start": "297600",
    "end": "300375"
  },
  {
    "text": "Um, and here's the reasoning.",
    "start": "300375",
    "end": "302250"
  },
  {
    "text": "The important point to remember is that GNN performs",
    "start": "302250",
    "end": "305990"
  },
  {
    "text": "embedding by passing messages via the edges of the network, right?",
    "start": "305990",
    "end": "310069"
  },
  {
    "text": "Every node computes its embedding by collecting",
    "start": "310070",
    "end": "312740"
  },
  {
    "text": "information from the neighbors, uh, right?",
    "start": "312740",
    "end": "315530"
  },
  {
    "text": "So the idea is now if I wanna have a good subgraph,",
    "start": "315530",
    "end": "318130"
  },
  {
    "text": "the good subgraph have- have- has to retain as many of the edges of the original graph as",
    "start": "318130",
    "end": "323600"
  },
  {
    "text": "possible so that my computations mimic",
    "start": "323600",
    "end": "326720"
  },
  {
    "text": "the computations on the big graph as well as possible.",
    "start": "326720",
    "end": "330260"
  },
  {
    "text": "So our subgraphs should retain",
    "start": "330260",
    "end": "332600"
  },
  {
    "text": "edge connectivity structure of the ne- original graph as much as possible.",
    "start": "332600",
    "end": "336535"
  },
  {
    "text": "Which means that the GNN over the subgraph generates embeddings that are close,",
    "start": "336535",
    "end": "341520"
  },
  {
    "text": "uh, to the- to the GNN over the original graph.",
    "start": "341520",
    "end": "345120"
  },
  {
    "text": "So to give you- to give you an idea,",
    "start": "345120",
    "end": "347810"
  },
  {
    "text": "if I have an original graph and let's say I sample, uh,",
    "start": "347810",
    "end": "351500"
  },
  {
    "text": "I- I sample a subgraph on four nodes like I show here on the left",
    "start": "351500",
    "end": "355650"
  },
  {
    "text": "or I sample another subgraph on four nodes like I show on the right,",
    "start": "355650",
    "end": "359945"
  },
  {
    "text": "then it's clear that the left subgraph is kind of much better for training.",
    "start": "359945",
    "end": "364220"
  },
  {
    "text": "Because if I now say,",
    "start": "364220",
    "end": "365645"
  },
  {
    "text": "let's compute the embedding of this corner node up here,",
    "start": "365645",
    "end": "368930"
  },
  {
    "text": "here, all the three neighbors of it are in the- in the subgraph.",
    "start": "368930",
    "end": "372740"
  },
  {
    "text": "So I'll be able to get a quite good estimate of the embedding of this,",
    "start": "372740",
    "end": "377240"
  },
  {
    "text": "uh, node, um, corner node up here.",
    "start": "377240",
    "end": "380574"
  },
  {
    "text": "So right, it's- all of its, um, uh,",
    "start": "380575",
    "end": "382750"
  },
  {
    "text": "uh, neighbors are- are part of my subgraph.",
    "start": "382750",
    "end": "386065"
  },
  {
    "text": "While if I take this other subgraph here,",
    "start": "386065",
    "end": "388555"
  },
  {
    "text": "we- denoted by green nodes.",
    "start": "388555",
    "end": "390280"
  },
  {
    "text": "And I wanna predict or generate an embedding for this green node as well,",
    "start": "390280",
    "end": "394420"
  },
  {
    "text": "then- then I- all I'm able to do is aggregate message from one of its neighbors,",
    "start": "394420",
    "end": "398935"
  },
  {
    "text": "because only one of its neighbors is part of my, uh, subgraph, right?",
    "start": "398935",
    "end": "403320"
  },
  {
    "text": "So the right subgraph drops many- many edges,",
    "start": "403320",
    "end": "407100"
  },
  {
    "text": "um, and leading to isolated nodes,",
    "start": "407100",
    "end": "409480"
  },
  {
    "text": "which means my computation graphs over this subgraph won't",
    "start": "409480",
    "end": "413320"
  },
  {
    "text": "be representative of the computation graphs in the original graph.",
    "start": "413320",
    "end": "417340"
  },
  {
    "text": "So left choice is far better,",
    "start": "417340",
    "end": "419980"
  },
  {
    "text": "uh, than the right choice.",
    "start": "419980",
    "end": "422075"
  },
  {
    "text": "And this now brings us back to",
    "start": "422075",
    "end": "424750"
  },
  {
    "text": "the network community structure because real world graphs exhibit community structure.",
    "start": "424750",
    "end": "430015"
  },
  {
    "text": "They exhibit clustering structure.",
    "start": "430015",
    "end": "431675"
  },
  {
    "text": "So large graphs can be decomposed into many small, uh, communities.",
    "start": "431675",
    "end": "435819"
  },
  {
    "text": "And the key insight would be that we can sample these subgraphs based on",
    "start": "435820",
    "end": "440170"
  },
  {
    "text": "the community structure so that each subgraph",
    "start": "440170",
    "end": "443110"
  },
  {
    "text": "retains most of the edges and only few edges, uh, are dropped.",
    "start": "443110",
    "end": "447469"
  },
  {
    "text": "So a Cluster-GCN, the way it works,",
    "start": "447470",
    "end": "450720"
  },
  {
    "text": "it has, uh, uh, a two-step process.",
    "start": "450720",
    "end": "453340"
  },
  {
    "text": "In the preprocessing step,",
    "start": "453340",
    "end": "454930"
  },
  {
    "text": "we take the original graph and split it,",
    "start": "454930",
    "end": "457555"
  },
  {
    "text": "cut it into many small,",
    "start": "457555",
    "end": "459205"
  },
  {
    "text": "uh- small, uh, subgraphs.",
    "start": "459205",
    "end": "461425"
  },
  {
    "text": "Uh, and then, uh, mini-batch training means that we sample one node,",
    "start": "461425",
    "end": "465430"
  },
  {
    "text": "group 1 subgraph, uh, and, uh,",
    "start": "465430",
    "end": "467565"
  },
  {
    "text": "perform the- the full batch message passing over that entire subgraph,",
    "start": "467565",
    "end": "471890"
  },
  {
    "text": "compute the node embe- uh- node embeddings of all the nodes in the subgraph,",
    "start": "471890",
    "end": "475805"
  },
  {
    "text": "evaluate the loss, compute the gradient,",
    "start": "475805",
    "end": "478910"
  },
  {
    "text": "and update the parameters, right?",
    "start": "478910",
    "end": "480500"
  },
  {
    "text": "So the idea is input graph,",
    "start": "480500",
    "end": "482120"
  },
  {
    "text": "split into many subgraphs.",
    "start": "482120",
    "end": "484445"
  },
  {
    "text": "We take one subgraph,",
    "start": "484445",
    "end": "486335"
  },
  {
    "text": "fit it into the GPU memory,",
    "start": "486335",
    "end": "488345"
  },
  {
    "text": "do the full computation on the subgraph, compute the loss,",
    "start": "488345",
    "end": "491840"
  },
  {
    "text": "update model parameters, load in another subgraph,",
    "start": "491840",
    "end": "495705"
  },
  {
    "text": "do the, uh, a full- full batch,",
    "start": "495705",
    "end": "498509"
  },
  {
    "text": "uh, layer embeddings of all the nodes in the subgraph.",
    "start": "498510",
    "end": "501355"
  },
  {
    "text": "Compute the loss, update the gradient,",
    "start": "501355",
    "end": "503750"
  },
  {
    "text": "load, uh, the next subgraphs.",
    "start": "503750",
    "end": "505700"
  },
  {
    "text": "So that's how vanilla Cluster-GCN, uh, would work.",
    "start": "505700",
    "end": "510720"
  },
  {
    "text": "So to give you more- more details,",
    "start": "510780",
    "end": "514059"
  },
  {
    "text": "the idea is that given a large, uh, graph,",
    "start": "514059",
    "end": "516490"
  },
  {
    "text": "we're going to partition it into,",
    "start": "516490",
    "end": "518140"
  },
  {
    "text": "let's say, capital C groups.",
    "start": "518140",
    "end": "520104"
  },
  {
    "text": "Um, we can use some scalable community detection method like Louvain, uh,",
    "start": "520105",
    "end": "524800"
  },
  {
    "text": "like METIS method, um,",
    "start": "524800",
    "end": "527110"
  },
  {
    "text": "or we can use, uh,",
    "start": "527110",
    "end": "528654"
  },
  {
    "text": "even BIGCLAM is fine,",
    "start": "528655",
    "end": "530560"
  },
  {
    "text": "um, and then basically this,",
    "start": "530560",
    "end": "532375"
  },
  {
    "text": "um, set of, uh,",
    "start": "532375",
    "end": "534370"
  },
  {
    "text": "node groups V. Uh,",
    "start": "534370",
    "end": "536500"
  },
  {
    "text": "we are going to take an induced subgraph on the subset of nodes,",
    "start": "536500",
    "end": "540715"
  },
  {
    "text": "um, and basically these induced subgraphs will",
    "start": "540715",
    "end": "542920"
  },
  {
    "text": "include all the edges between the members, ah,",
    "start": "542920",
    "end": "545310"
  },
  {
    "text": "of the- of the group, and of course,",
    "start": "545310",
    "end": "547720"
  },
  {
    "text": "these edges that go across the subgraphs,",
    "start": "547720",
    "end": "550613"
  },
  {
    "text": "those will be dropped.",
    "start": "550614",
    "end": "551920"
  },
  {
    "text": "[NOISE] So between group edges are going to be dropped in these,",
    "start": "551920",
    "end": "556149"
  },
  {
    "text": "uh, uh, graphs, G1 to GC which are these subgraphs we sampled.",
    "start": "556150",
    "end": "561100"
  },
  {
    "text": "So now, for each, uh, mini-batch,",
    "start": "561100",
    "end": "564040"
  },
  {
    "text": "we are going to sample one such node group, one such subgraph,",
    "start": "564040",
    "end": "568209"
  },
  {
    "text": "create an induced subgraph over that set of nodes,",
    "start": "568210",
    "end": "572275"
  },
  {
    "text": "and this is now our graph that we are going to use,",
    "start": "572275",
    "end": "575830"
  },
  {
    "text": "um, as a computation in the GPU.",
    "start": "575830",
    "end": "578575"
  },
  {
    "text": "So the point is that this induced subgraph has to fit into the GPU memory.",
    "start": "578575",
    "end": "582940"
  },
  {
    "text": "So now, we do layer-wise node update",
    "start": "582940",
    "end": "585760"
  },
  {
    "text": "basically the same thing as what we do in the full batch, uh,",
    "start": "585760",
    "end": "588745"
  },
  {
    "text": "GNN to compute now the embeddings of all the nodes in the subgraph, recompute, uh,",
    "start": "588745",
    "end": "594565"
  },
  {
    "text": "the loss, uh, and then do the, uh,",
    "start": "594565",
    "end": "597025"
  },
  {
    "text": "compute the gradient with respect to the loss and then do the gradient update.",
    "start": "597025",
    "end": "601420"
  },
  {
    "text": "So the point here is that when we were doing GraphSAGE neighborhood sampling, our nodes,",
    "start": "601420",
    "end": "607300"
  },
  {
    "text": "we were able to kind of sample them all- all- all across the graphs.",
    "start": "607300",
    "end": "610870"
  },
  {
    "text": "But in this case, um,",
    "start": "610870",
    "end": "612760"
  },
  {
    "text": "we are going to sample a subgraph of",
    "start": "612760",
    "end": "614500"
  },
  {
    "text": "the original graph and now only compute on those nodes,",
    "start": "614500",
    "end": "617470"
  },
  {
    "text": "uh, we have sampled, and that's the big difference between,",
    "start": "617470",
    "end": "620860"
  },
  {
    "text": "ah, the neighborhood sampling that can be kind of,",
    "start": "620860",
    "end": "623125"
  },
  {
    "text": "those neighborhoods can be distributed across the entire network while here,",
    "start": "623125",
    "end": "627175"
  },
  {
    "text": "we are going to create those neighborhoods basically just being limited to",
    "start": "627175",
    "end": "631360"
  },
  {
    "text": "the subgraph we sample and we feed that subgraph into the GPU memory.",
    "start": "631360",
    "end": "635785"
  },
  {
    "text": "So what are some issues and how do we fix them?",
    "start": "635785",
    "end": "639190"
  },
  {
    "text": "The issue with Cluster-GCN is that these induced subgraphs are removed in- in,",
    "start": "639190",
    "end": "645205"
  },
  {
    "text": "uh, between the group links, between subgraph links.",
    "start": "645205",
    "end": "648370"
  },
  {
    "text": "And as a result, messages from other groups",
    "start": "648370",
    "end": "651115"
  },
  {
    "text": "will be lost during message-passing which will hur- which can hurt,",
    "start": "651115",
    "end": "654535"
  },
  {
    "text": "uh, GNN performance, right?",
    "start": "654535",
    "end": "656769"
  },
  {
    "text": "So for example, if I sample this red graph here and I just zoom in into the node- uh,",
    "start": "656770",
    "end": "663250"
  },
  {
    "text": "into it, what I see is that,",
    "start": "663250",
    "end": "665050"
  },
  {
    "text": "for example, whenever, ah,",
    "start": "665050",
    "end": "666385"
  },
  {
    "text": "this particular node aggregates information,",
    "start": "666385",
    "end": "670375"
  },
  {
    "text": "it will never aggregate information from these two edges because these two- uh,",
    "start": "670375",
    "end": "674185"
  },
  {
    "text": "the node endpoints are part of the se- the different subgraphs.",
    "start": "674185",
    "end": "677710"
  },
  {
    "text": "So all the message aggregation will happen only between these four nodes,",
    "start": "677710",
    "end": "681160"
  },
  {
    "text": "uh, and the five,",
    "start": "681160",
    "end": "682300"
  },
  {
    "text": "uh, edges between them,",
    "start": "682300",
    "end": "683904"
  },
  {
    "text": "so there will be in some sense, uh, lost messages.",
    "start": "683905",
    "end": "688000"
  },
  {
    "text": "So, uh, the issue will be because the-",
    "start": "688000",
    "end": "691225"
  },
  {
    "text": "the graph community detection put similar nodes together into the same group,",
    "start": "691225",
    "end": "695785"
  },
  {
    "text": "uh, sample node groups tend to cover",
    "start": "695785",
    "end": "698004"
  },
  {
    "text": "only a small concentrated portion of the entire graph, right?",
    "start": "698005",
    "end": "701890"
  },
  {
    "text": "So we are going to learn only on a small concentrated portions of the graph while",
    "start": "701890",
    "end": "706420"
  },
  {
    "text": "neighborhood sampling allows us to kind of learn on small but very diverse,",
    "start": "706420",
    "end": "711475"
  },
  {
    "text": "uh, set of, uh, neighborhoods.",
    "start": "711475",
    "end": "713589"
  },
  {
    "text": "Um, and this means that because this sample node,",
    "start": "713590",
    "end": "716470"
  },
  {
    "text": "sample subgraph vor- will be- will be our concentrated part of the entire graph,",
    "start": "716470",
    "end": "721285"
  },
  {
    "text": "it won't be kind of diverse or a representative to",
    "start": "721285",
    "end": "724269"
  },
  {
    "text": "represent the entire, uh, training graphs.",
    "start": "724270",
    "end": "727330"
  },
  {
    "text": "So what this means is that when we compute the gradient and the loss, uh,",
    "start": "727330",
    "end": "731110"
  },
  {
    "text": "this is going to fluctuate a lot from one node group to",
    "start": "731110",
    "end": "734079"
  },
  {
    "text": "another because- because of the social communities,",
    "start": "734080",
    "end": "737350"
  },
  {
    "text": "uh, if you think of it that way.",
    "start": "737350",
    "end": "739014"
  },
  {
    "text": "Each subgroup will be very concentrated in one given",
    "start": "739015",
    "end": "742120"
  },
  {
    "text": "aspect and the- the model will have hard time to learn, right?",
    "start": "742120",
    "end": "745660"
  },
  {
    "text": "You will- I know- you know,",
    "start": "745660",
    "end": "746964"
  },
  {
    "text": "you have a cluster of computer scientists,",
    "start": "746965",
    "end": "749500"
  },
  {
    "text": "so you will learn how to make- you will compute",
    "start": "749500",
    "end": "751720"
  },
  {
    "text": "the gradient with respect to computer scientists,",
    "start": "751720",
    "end": "754149"
  },
  {
    "text": "then you have a cluster of, let's say, uh,",
    "start": "754150",
    "end": "756730"
  },
  {
    "text": "music students and now your gradient that",
    "start": "756730",
    "end": "759430"
  },
  {
    "text": "you have just- your models that were just computed over",
    "start": "759430",
    "end": "762130"
  },
  {
    "text": "the computer scientists in your social network are now going to",
    "start": "762130",
    "end": "765160"
  },
  {
    "text": "be computed over music people which are very, very different.",
    "start": "765160",
    "end": "768370"
  },
  {
    "text": "And then I don't know, you have mathematicians who are",
    "start": "768370",
    "end": "770740"
  },
  {
    "text": "different- community of mathematicians who are different from the two.",
    "start": "770740",
    "end": "774100"
  },
  {
    "text": "So the gradients are going to fluctuate quite a lot,",
    "start": "774100",
    "end": "777084"
  },
  {
    "text": "and training is quite unstable which in practice leads to slow convergence of,",
    "start": "777085",
    "end": "783250"
  },
  {
    "text": "uh, gradient descent or stochastic gradient descent.",
    "start": "783250",
    "end": "786070"
  },
  {
    "text": "So, uh, a way to improve this is to- is what is called Advanced Cluster-GCN.",
    "start": "786070",
    "end": "792235"
  },
  {
    "text": "And the idea is that here,",
    "start": "792235",
    "end": "793810"
  },
  {
    "text": "we wanna aggregate multiple node groups per mini-batch, right?",
    "start": "793810",
    "end": "797290"
  },
  {
    "text": "So the idea is to- to make the graphs we sampled even",
    "start": "797290",
    "end": "800829"
  },
  {
    "text": "smaller but then we are going to sample multiple of these,",
    "start": "800830",
    "end": "805510"
  },
  {
    "text": "uh, subgraphs, uh, into the mini-batch.",
    "start": "805510",
    "end": "808510"
  },
  {
    "text": "Um, and we are going to create, uh,",
    "start": "808510",
    "end": "810610"
  },
  {
    "text": "induced subgraph on the aggregated node group that is now composed of many small, uh,",
    "start": "810610",
    "end": "817000"
  },
  {
    "text": "node groups, and the rest will be the same is- I- as in the Cluster-GCN,",
    "start": "817000",
    "end": "821410"
  },
  {
    "text": "which is take now the induced subgraph,",
    "start": "821410",
    "end": "823740"
  },
  {
    "text": "put it into the GPU memory,",
    "start": "823740",
    "end": "825330"
  },
  {
    "text": "and create the update.",
    "start": "825330",
    "end": "826755"
  },
  {
    "text": "But the point is now that the subgroups can be more",
    "start": "826755",
    "end": "828900"
  },
  {
    "text": "diverse so the induced subgraph will be more diverse,",
    "start": "828900",
    "end": "832245"
  },
  {
    "text": "so your gradients will be more stable and more, uh, representative.",
    "start": "832245",
    "end": "836255"
  },
  {
    "text": "So here is the picture,",
    "start": "836255",
    "end": "837820"
  },
  {
    "text": "the picture is that now my node groups are smaller than what I had before,",
    "start": "837820",
    "end": "841510"
  },
  {
    "text": "but I can sample more of them.",
    "start": "841510",
    "end": "843265"
  },
  {
    "text": "So I sample here two node groups and now",
    "start": "843265",
    "end": "846220"
  },
  {
    "text": "the sample subgraph is the induced subgraph on the nodes belonging to the two groups.",
    "start": "846220",
    "end": "852685"
  },
  {
    "text": "So I will also include, uh, this,",
    "start": "852685",
    "end": "855625"
  },
  {
    "text": "uh, uh, blue, uh, edge that kind of connects the two groups.",
    "start": "855625",
    "end": "859300"
  },
  {
    "text": "So it means that, um, uh,",
    "start": "859300",
    "end": "860769"
  },
  {
    "text": "this makes the sample nodes more representative, more diverse.",
    "start": "860770",
    "end": "864820"
  },
  {
    "text": "They are more representative of the entire node population,",
    "start": "864820",
    "end": "868195"
  },
  {
    "text": "which will lead to better gradient estimate,",
    "start": "868195",
    "end": "870955"
  },
  {
    "text": "less variance in gradients,",
    "start": "870955",
    "end": "872844"
  },
  {
    "text": "and faster training, uh, and convergence.",
    "start": "872844",
    "end": "876265"
  },
  {
    "text": "So, uh, just to say more, right?",
    "start": "876265",
    "end": "879580"
  },
  {
    "text": "This is again a two-step approach.",
    "start": "879580",
    "end": "881545"
  },
  {
    "text": "I take the nodes, um,",
    "start": "881545",
    "end": "883660"
  },
  {
    "text": "I- I separate them into very small, uh,",
    "start": "883660",
    "end": "887095"
  },
  {
    "text": "subgroups, much smaller than the original version, but, uh,",
    "start": "887095",
    "end": "890439"
  },
  {
    "text": "when I'm doing mini-batch training,",
    "start": "890440",
    "end": "892285"
  },
  {
    "text": "I will sample multiple groups of nodes, um,",
    "start": "892285",
    "end": "895930"
  },
  {
    "text": "and then I will aggregate these groups of nodes into one super group,",
    "start": "895930",
    "end": "899635"
  },
  {
    "text": "and then I'm going to create an induced subgraph on the entire, uh, super group.",
    "start": "899635",
    "end": "904210"
  },
  {
    "text": "And then, um, I'm going to include the edges between all the members of the super group,",
    "start": "904210",
    "end": "909280"
  },
  {
    "text": "which means it will be edges between the- the small, uh,",
    "start": "909280",
    "end": "912085"
  },
  {
    "text": "subgroups as well as the edges between the small, uh, subgroups.",
    "start": "912085",
    "end": "916825"
  },
  {
    "text": "Um, and then I will perfo- perform, uh,",
    "start": "916825",
    "end": "919540"
  },
  {
    "text": "Cluster-GCN, uh, the same way as we did before.",
    "start": "919540",
    "end": "923875"
  },
  {
    "text": "So now, um, if I compare the Cluster-GCN approach,",
    "start": "923875",
    "end": "929124"
  },
  {
    "text": "with the neighborhood sampling approach,",
    "start": "929124",
    "end": "931285"
  },
  {
    "text": "here is how the two compare.",
    "start": "931285",
    "end": "932920"
  },
  {
    "text": "The neighborhood sampling says,",
    "start": "932920",
    "end": "935125"
  },
  {
    "text": "uh, sample H, uh,",
    "start": "935125",
    "end": "936655"
  },
  {
    "text": "nodes per layer, um,",
    "start": "936655",
    "end": "938230"
  },
  {
    "text": "and let's do this for,",
    "start": "938230",
    "end": "940000"
  },
  {
    "text": "uh, M nodes in the network.",
    "start": "940000",
    "end": "941620"
  },
  {
    "text": "So the total size of the mini-batch will be",
    "start": "941620",
    "end": "944875"
  },
  {
    "text": "M times H raised to the power of K. M is the number of computation graph,",
    "start": "944875",
    "end": "949690"
  },
  {
    "text": "uh, H is the fan out of computation graphs,",
    "start": "949690",
    "end": "953140"
  },
  {
    "text": "and K is the number of layers of the GNN.",
    "start": "953140",
    "end": "956455"
  },
  {
    "text": "So for M, uh,",
    "start": "956455",
    "end": "957940"
  },
  {
    "text": "nodes in the computation graph,",
    "start": "957940",
    "end": "959965"
  },
  {
    "text": "our cost in terms of memory as well as computation will be M times, uh, H_K.",
    "start": "959965",
    "end": "965665"
  },
  {
    "text": "In the Cluster-GCN, um, uh, the- uh,",
    "start": "965665",
    "end": "970000"
  },
  {
    "text": "the- we are performing message passing over a subgraph induced by M nodes,",
    "start": "970000",
    "end": "975025"
  },
  {
    "text": "and the subgraph over M nodes is going to",
    "start": "975025",
    "end": "978490"
  },
  {
    "text": "include M times average degree number of edges, right?",
    "start": "978490",
    "end": "982615"
  },
  {
    "text": "Each node in the subgraph has an average degree of, uh, D average,",
    "start": "982615",
    "end": "987279"
  },
  {
    "text": "so in total, we- we'll have M times,",
    "start": "987280",
    "end": "990250"
  },
  {
    "text": "uh, average degree, uh,",
    "start": "990250",
    "end": "991870"
  },
  {
    "text": "number of edges in the graph.",
    "start": "991870",
    "end": "993595"
  },
  {
    "text": "And because we are doing K hop, uh, message-passing,",
    "start": "993595",
    "end": "997615"
  },
  {
    "text": "the computational cost of this approach will be",
    "start": "997615",
    "end": "1000510"
  },
  {
    "text": "K times M size of the subgraph times the average degree.",
    "start": "1000510",
    "end": "1005895"
  },
  {
    "text": "So if you compare the two, in summary,",
    "start": "1005895",
    "end": "1009090"
  },
  {
    "text": "the cost to generate embeddings from M nodes in a K layer GNN is M to the- M times H_K.",
    "start": "1009090",
    "end": "1015735"
  },
  {
    "text": "In Cluster-GCN, it is, um,",
    "start": "1015735",
    "end": "1018675"
  },
  {
    "text": "K times M times average degree,",
    "start": "1018675",
    "end": "1021450"
  },
  {
    "text": "um, and, um, if you assume, let's say, uh,",
    "start": "1021450",
    "end": "1024375"
  },
  {
    "text": "H to be half of the average degree,",
    "start": "1024375",
    "end": "1027170"
  },
  {
    "text": "uh, then it would mean that, ah,",
    "start": "1027170",
    "end": "1029380"
  },
  {
    "text": "Cluster-GCN is much more computationally efficient,",
    "start": "1029380",
    "end": "1032464"
  },
  {
    "text": "um, than neighborhood sampling.",
    "start": "1032465",
    "end": "1034409"
  },
  {
    "text": "So, um, it is linear ins- instead of",
    "start": "1034410",
    "end": "1037000"
  },
  {
    "text": "exponential with respect to the- to the- to the depth.",
    "start": "1037000",
    "end": "1040464"
  },
  {
    "text": "So what do we do in practice?",
    "start": "1040465",
    "end": "1042350"
  },
  {
    "text": "It depends a bit on the- on the dataset.",
    "start": "1042350",
    "end": "1044679"
  },
  {
    "text": "Usually, we set H to be bigger than the half average degree,",
    "start": "1044680",
    "end": "1049150"
  },
  {
    "text": "uh, perhaps, you know, two times,",
    "start": "1049150",
    "end": "1050980"
  },
  {
    "text": "three times, ah, average degree, um, and, uh,",
    "start": "1050980",
    "end": "1054205"
  },
  {
    "text": "and because number of layers K is not- uh,",
    "start": "1054205",
    "end": "1057610"
  },
  {
    "text": "is not that deep, um,",
    "start": "1057610",
    "end": "1059429"
  },
  {
    "text": "the- the neighborhood sampling approach tends to be kind of more,",
    "start": "1059430",
    "end": "1063460"
  },
  {
    "text": "uh, used, uh, in practice.",
    "start": "1063460",
    "end": "1065945"
  },
  {
    "text": "So to summarize, Cluster-GCN first partitions the entire,",
    "start": "1065945",
    "end": "1071158"
  },
  {
    "text": "um, set of nodes in the graph into small node groups.",
    "start": "1071159",
    "end": "1074260"
  },
  {
    "text": "In each mini-batch, multiple node groups are sampled, um, and their,",
    "start": "1074260",
    "end": "1078700"
  },
  {
    "text": "uh, and their nodes are kind of,",
    "start": "1078700",
    "end": "1081865"
  },
  {
    "text": "um, uh, aggregated together.",
    "start": "1081865",
    "end": "1083980"
  },
  {
    "text": "Then an induced subgraph on this, uh, uh, um,",
    "start": "1083980",
    "end": "1088260"
  },
  {
    "text": "node- nodes, uh, from the union of",
    "start": "1088260",
    "end": "1091170"
  },
  {
    "text": "the- of the groups that we have sampled, uh, is created.",
    "start": "1091170",
    "end": "1094755"
  },
  {
    "text": "And then the GNN performs",
    "start": "1094755",
    "end": "1096525"
  },
  {
    "text": "layer-wise node embeddings update over this induced, uh, subgraph.",
    "start": "1096525",
    "end": "1101265"
  },
  {
    "text": "Generally, Cluster-GCN is more computationally efficient,",
    "start": "1101265",
    "end": "1104860"
  },
  {
    "text": "than neighborhood sampling, especially when the number of layers is large, um,",
    "start": "1104860",
    "end": "1110110"
  },
  {
    "text": "but Cluster-GCN leads to systematically biased gradients",
    "start": "1110110",
    "end": "1113785"
  },
  {
    "text": "because of missing cross-community edges and also because,",
    "start": "1113785",
    "end": "1117970"
  },
  {
    "text": "uh, if, uh, number of layers is deep,",
    "start": "1117970",
    "end": "1120789"
  },
  {
    "text": "then in the original graph,",
    "start": "1120790",
    "end": "1122780"
  },
  {
    "text": "um, if you do neighborhood sampling,",
    "start": "1122780",
    "end": "1124390"
  },
  {
    "text": "you can really go deep.",
    "start": "1124390",
    "end": "1125675"
  },
  {
    "text": "But in the Cluster-GCN,",
    "start": "1125675",
    "end": "1127885"
  },
  {
    "text": "you are only going to go to the edge of the original of the sampled graph,",
    "start": "1127885",
    "end": "1131980"
  },
  {
    "text": "and then you'll kind of bounce back.",
    "start": "1131980",
    "end": "1133390"
  },
  {
    "text": "And even though you have a lot of depth,",
    "start": "1133390",
    "end": "1135940"
  },
  {
    "text": "this depth would be kind of oscillating over the subgraph",
    "start": "1135940",
    "end": "1139134"
  },
  {
    "text": "and won't even ec- really explore the real depth,",
    "start": "1139135",
    "end": "1142480"
  },
  {
    "text": "uh, of the underlying original graph.",
    "start": "1142480",
    "end": "1145315"
  },
  {
    "text": "So, uh, overall, um,",
    "start": "1145315",
    "end": "1147669"
  },
  {
    "text": "I would say neighborhood sampling is used more because of additional, uh, flexibility.",
    "start": "1147670",
    "end": "1153500"
  }
]