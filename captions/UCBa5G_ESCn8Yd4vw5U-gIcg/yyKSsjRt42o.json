[
  {
    "start": "0",
    "end": "5480"
  },
  {
    "text": "All right, so welcome to the\nCS109 final review session. My name is Will. Will.",
    "start": "5480",
    "end": "11780"
  },
  {
    "text": "Thank you. I'm one of the TAs. I'll be leading this section.",
    "start": "11780",
    "end": "16800"
  },
  {
    "text": "So first up, here's\nCS109, the general advice.",
    "start": "16800",
    "end": "21980"
  },
  {
    "text": "Usually, we want\nusually students focus on how to apply\nformulas from class,",
    "start": "21980",
    "end": "27289"
  },
  {
    "text": "maybe try to get a\nnumerical answer. And these are done--\nthese are like-- OK, so one example, how to\ntake a derivative from an MLE,",
    "start": "27290",
    "end": "34903"
  },
  {
    "text": "OK, that's actually like--\nthat's quite challenging. But also, once you know\nhow to take a derivative, it's pretty straightforward\nto take a derivative.",
    "start": "34903",
    "end": "41630"
  },
  {
    "text": "The hard part is\nusually the modeling, so like actually finding\nthe distribution for a log likelihood, if we\ndon't tell you.",
    "start": "41630",
    "end": "48329"
  },
  {
    "text": "So on that note,\nespecially for the final, for those of you that have\nseen the practice finals,",
    "start": "48330",
    "end": "55100"
  },
  {
    "text": "there is a lot of reading. Every single problem has\ntwo or three paragraphs of just straight\ncontext, and then",
    "start": "55100",
    "end": "60769"
  },
  {
    "text": "three bullet point questions\nthat come right after it. So in the same sense,\nthe midterm exam have six questions or five\nor five questions or so,",
    "start": "60770",
    "end": "68520"
  },
  {
    "text": "with very small\namount of reading and a lot of problem solving. On the final similar\nnumber of problems,",
    "start": "68520",
    "end": "74340"
  },
  {
    "text": "a lot more reading and\nexpect a few more parts, OK? So please read the problems.",
    "start": "74340",
    "end": "80010"
  },
  {
    "text": "Very thoroughly, and then like\nproblems should be doable, OK?",
    "start": "80010",
    "end": "85580"
  },
  {
    "text": "All right. So here's how to\nread probability in the final exam edition.",
    "start": "85580",
    "end": "91130"
  },
  {
    "text": "So to make this slide, I\nwent through the first final that I saw and just\ntook a look at, OK,",
    "start": "91130",
    "end": "96437"
  },
  {
    "text": "what's the most common\nparadigm that we saw, what's the most common\namount of words. By the way, these slides\nwill be posted later.",
    "start": "96437",
    "end": "103100"
  },
  {
    "text": "So the first thing-- so\nfirst things are like, what's the probability of blank? This is in three out of--",
    "start": "103100",
    "end": "108787"
  },
  {
    "text": "or at least one as opposed\nto most of the problems that I've seen. How much more likely-- and then, OK, so\nin that case, you",
    "start": "108787",
    "end": "114460"
  },
  {
    "text": "want to write a\nprobability of something. A very common one we also\nsee is how much more likely",
    "start": "114460",
    "end": "119750"
  },
  {
    "text": "is a blank than a blank? Usually, this means you want\nto be dividing two likelihoods",
    "start": "119750",
    "end": "125630"
  },
  {
    "text": "or PDFs or products of PDFs.",
    "start": "125630",
    "end": "130849"
  },
  {
    "text": "In the discrete case, this\nwould be two probabilities, and you would be\ndividing them, OK?",
    "start": "130850",
    "end": "136460"
  },
  {
    "text": "So we also see the word-- yes? [INAUDIBLE]",
    "start": "136460",
    "end": "141830"
  },
  {
    "text": "Oh, likelihood is like\nthe product of a PDF. So like a pset for problem 7.",
    "start": "141830",
    "end": "147180"
  },
  {
    "text": "There was a-- OK, maybe I\nshould explain what that is. It's just this thing.",
    "start": "147180",
    "end": "153690"
  },
  {
    "text": "Your likelihood is like this. Like, you just-- at some point\nin time, in one of your psets,",
    "start": "153690",
    "end": "159330"
  },
  {
    "text": "you have something that\nlook like this, right? Or I guess, it wasn't\ntheta at this point.",
    "start": "159330",
    "end": "165510"
  },
  {
    "text": "It was like A, X given B.\nX, A and X given B, right?",
    "start": "165510",
    "end": "170586"
  },
  {
    "text": "I have to tuck my head down. I try to use a microphone. So that's what I\nmean by likelihood.",
    "start": "170586",
    "end": "175980"
  },
  {
    "text": "This is technically-- this is\nlike a form of a likelihood, OK?",
    "start": "175980",
    "end": "182340"
  },
  {
    "text": "All right. If we say the word\nparameterize and don't specify",
    "start": "182340",
    "end": "187980"
  },
  {
    "text": "the method, which we probably\nwill specify the method, it usually means you're going\nto do one of MLE or MAP.",
    "start": "187980",
    "end": "194100"
  },
  {
    "text": "And if you're going to be doing\nMLE or MAP, find the data. Find what your sums or what\nyour products are over,",
    "start": "194100",
    "end": "200910"
  },
  {
    "text": "especially I think,\non the last section, we had a problem about setting\nup a boba place on campus.",
    "start": "200910",
    "end": "207160"
  },
  {
    "text": "There was a hidden data\npoint in there somewhere. They had to do MLE for. It was a single data point.",
    "start": "207160",
    "end": "212230"
  },
  {
    "text": "So, that could happen, OK? If we say approximate--\nso during the midterm,",
    "start": "212230",
    "end": "218470"
  },
  {
    "text": "approximate just means\nuse a Gaussian or Poisson approximation. And on the final,\nall that means is,",
    "start": "218470",
    "end": "226220"
  },
  {
    "text": "if you see a set of\nrandom variables, you're going to use CLT, OK? That's usually what\napproximation means here.",
    "start": "226220",
    "end": "232760"
  },
  {
    "text": "By the way, caveat,\nif you use CLT, make sure the\ncontinuity correct. Continuity correct\nis everywhere. I did not put them in\nthe slides because that's",
    "start": "232760",
    "end": "239150"
  },
  {
    "text": "like a thing we covered\nbefore the midterm, but remember to\ndo that here, too.",
    "start": "239150",
    "end": "244920"
  },
  {
    "text": "Is blank significant? Usually, that means like\nbootstrap or like a P-value. So typically, right now, go\nor show, like, hey, count",
    "start": "244920",
    "end": "253320"
  },
  {
    "text": "some things and be like, hey,\nthe P-value is greater than 5%. Probably not good. So, OK.",
    "start": "253320",
    "end": "259549"
  },
  {
    "text": " All right, so with that we\nstart our journey with betas.",
    "start": "259550",
    "end": "265650"
  },
  {
    "text": "Betas are the first things\nthat we saw that were clearly post-midterm material. And in a beta, the way that\nthe beta distribution is set up",
    "start": "265650",
    "end": "275160"
  },
  {
    "text": "with two parameters\nA and B. And they represent a minus 1\nheads and b minus 1 tails from a biased coin y.",
    "start": "275160",
    "end": "281400"
  },
  {
    "text": "So betas are always\ntypically characterized a belief over a Bernoulli. And they represent the\nprobability of a Bernoulli",
    "start": "281400",
    "end": "289740"
  },
  {
    "text": "parameter, given a\ncertain amount of heads and a certain\namount of tails, OK?",
    "start": "289740",
    "end": "295470"
  },
  {
    "text": "The betas are in\nthe range 0 and 1. They have some-- they\nhave a form of a PDF",
    "start": "295470",
    "end": "300750"
  },
  {
    "text": "with some parameter\nwith some constant B that we write explicitly later.",
    "start": "300750",
    "end": "305830"
  },
  {
    "text": "The important part is the\nexpectation and the variance. So these are the two-- these are\ntwo values that I saw come up",
    "start": "305830",
    "end": "311940"
  },
  {
    "text": "in one of the practice exams. I have an example on\nthis in the end the side, so we'll take a look at it more. But generally, the reason\nwhy we put beta distribution",
    "start": "311940",
    "end": "318540"
  },
  {
    "text": "after the midterm\nand in its own thing is because the\nbeta distribution,",
    "start": "318540",
    "end": "324160"
  },
  {
    "text": "actually, it captures\nsomething about the parameters of a Bernoulli. Before, we just had\nBernoulli or just",
    "start": "324160",
    "end": "331030"
  },
  {
    "text": "binomials about random events. Now, we're like,\nOK, what's the--",
    "start": "331030",
    "end": "336610"
  },
  {
    "text": "what's the probability\nof a particular theta in like a Bernoulli, OK?",
    "start": "336610",
    "end": "344820"
  },
  {
    "text": "All right, then we talked about\nadding independent variables. So the three most\nheavily used forms",
    "start": "344820",
    "end": "350190"
  },
  {
    "text": "that I've seen of\nadding random variables are adding Poisson, adding\nnormals, adding binomials.",
    "start": "350190",
    "end": "355200"
  },
  {
    "text": "They all have to be-- they all have to be like IID. So if they're independent\nof each other,",
    "start": "355200",
    "end": "360892"
  },
  {
    "text": "and they're from the same\nPoisson distribution, and you sum them, that's just\nsomething to the lambdas, OK?",
    "start": "360892",
    "end": "367680"
  },
  {
    "text": "One particular example of\nthis is an EVGR-A. There's three elevators.",
    "start": "367680",
    "end": "372990"
  },
  {
    "text": "If both of the other\nelevators are out of service, the amount of people that\nare going to be trying to",
    "start": "372990",
    "end": "379740"
  },
  {
    "text": "go into an elevator on-- or they're going to be trying\nto go into the last remaining elevator will be 3\ntimes lambda, assuming",
    "start": "379740",
    "end": "388050"
  },
  {
    "text": "each one of the elevators\nis has a Poisson of lambda for students trying\nto use it, OK? ",
    "start": "388050",
    "end": "395470"
  },
  {
    "text": "The normal distribution. So, OK, when you add\nnormal distributions, you want to be adding variances.",
    "start": "395470",
    "end": "401440"
  },
  {
    "text": "You don't want to be\nadding standard deviations. So that's why we put the\nsigma squares over here. So if you have two normals, you\nadd the mean, add the variance.",
    "start": "401440",
    "end": "411340"
  },
  {
    "text": "And then one particular\nexample of this is, OK, heights are typically\nnormally distributed.",
    "start": "411340",
    "end": "416960"
  },
  {
    "text": "So if you had people sitting\npiggyback on top of each other, that's the sum of two\nGaussians on the height.",
    "start": "416960",
    "end": "422199"
  },
  {
    "text": "Yeah, it just literally taller. And the final one\nis on binomials.",
    "start": "422200",
    "end": "428500"
  },
  {
    "text": "So binomials is if you have-- when I think of binomial,\nI think of trials.",
    "start": "428500",
    "end": "434840"
  },
  {
    "text": "And I think of some\nunderlying Bernoulli event, like I have a coin. I'm trying to flip it. I'm trying to get how many\nheads or however many tails.",
    "start": "434840",
    "end": "442690"
  },
  {
    "text": "So one way to think-- One example to think\nabout binomials is, you want to co-author a\nconducting the same experiment,",
    "start": "442690",
    "end": "448120"
  },
  {
    "text": "right? But you ran N1 trials,\nand your co-author",
    "start": "448120",
    "end": "453590"
  },
  {
    "text": "ran N2 trials independently\nof each other. What is the probability\ndistribution of your results if you\ncoalesce the two sets of trials",
    "start": "453590",
    "end": "462230"
  },
  {
    "text": "together. And the solution\nis, well, just add. So you just increase\nthe number of trials.",
    "start": "462230",
    "end": "467853"
  },
  {
    "text": "You didn't change anything\nabout the underlying parameter. So you're just going to\nhave a binomial of number",
    "start": "467853",
    "end": "474259"
  },
  {
    "text": "of trials for one plus number of\ntrials for the second one, OK? So this is adding\nindependent random variables.",
    "start": "474260",
    "end": "481910"
  },
  {
    "text": "Yes? Previous slide, there's a\nconstant in front of the data distribution, B.",
    "start": "481910",
    "end": "486980"
  },
  {
    "text": "Yeah. What does that mean? That's not a course reader. It's some complicated number.",
    "start": "486980",
    "end": "493060"
  },
  {
    "text": "Yeah. We have it in the slides, but\nI just-- yeah, it's a constant. You can normalize\nit out, usually.",
    "start": "493060",
    "end": "499760"
  },
  {
    "text": "Yes. We need to know that\nconstant for the finals. We'll probably give it to\nyou, if you need it, honestly,",
    "start": "499760",
    "end": "505210"
  },
  {
    "text": "but real answer, you've got\n10 pages of review sheets. Just stick it on the\nmargin of one of them.",
    "start": "505210",
    "end": "513450"
  },
  {
    "text": "OK? All right, any other ones? No? OK. So here's a central\nlimit theorem.",
    "start": "513450",
    "end": "522049"
  },
  {
    "text": "There's two flavors of\nthe central limit theorem. You could take the sum of\nXi iiD random variables,",
    "start": "522049",
    "end": "528680"
  },
  {
    "text": "or you could take the average\nof some iiD random variables. One example of some\niiD random variables",
    "start": "528680",
    "end": "535250"
  },
  {
    "text": "is given enough trials,\nanything looks like a normal. ",
    "start": "535250",
    "end": "540589"
  },
  {
    "text": "So that little graph of, if\nI just keep rolling dice, the sum that I'm going\nto get from those dice is going to eventually\nlook like a normal,",
    "start": "540590",
    "end": "547310"
  },
  {
    "text": "and that's like\nthe approximation that we're going to use. Average of iiD random\nvariables, these",
    "start": "547310",
    "end": "553990"
  },
  {
    "text": "actually are used\na lot in denoising. So the one example is sample\nmeans are normally distributed.",
    "start": "553990",
    "end": "561199"
  },
  {
    "text": "So the bigger your sample mean,\nthe less variance of those means that you will see.",
    "start": "561200",
    "end": "567920"
  },
  {
    "text": "So the better-- so\nthat's a tighter average that you would find.",
    "start": "567920",
    "end": "574260"
  },
  {
    "text": "OK, makes sense? So this is one of\nthe critical theorems that we use, and we\nactually give it a name, OK?",
    "start": "574260",
    "end": "582120"
  },
  {
    "text": "All right. So the next thing that\nwe have is sampling. So in sampling, we\nactually saw how",
    "start": "582120",
    "end": "587190"
  },
  {
    "text": "to use the-- how to find\nthe sample mean, the sample standard deviation. And typically,\nwhenever we sample,",
    "start": "587190",
    "end": "593193"
  },
  {
    "text": "the image I have in my\nhead or the one that we have from the class\nis like, here's some underlying distribution. You construct some\nsample histogram,",
    "start": "593193",
    "end": "600180"
  },
  {
    "text": "and then each one\nof these points becomes an equivalent class\nof that kind of point. So that will be important\nlater on in bootstrapping.",
    "start": "600180",
    "end": "606450"
  },
  {
    "text": "But just know that you're trying\nto construct something that looks similar to what your\nunderlying distribution f looks like, OK?",
    "start": "606450",
    "end": "614460"
  },
  {
    "text": "So sample mean and\nsample variance are two particular quantities\nthat we can talk about. They are unbiased, meaning\nthat the expected value of all",
    "start": "614460",
    "end": "621722"
  },
  {
    "text": "of the sample means that we\ncould possibly take is actually going to be the mean. And the expectation of\nall the sample variances",
    "start": "621722",
    "end": "628470"
  },
  {
    "text": "should actually come\nout to be the sample-- sorry, should actually\nconverge to the variance, OK?",
    "start": "628470",
    "end": "637720"
  },
  {
    "text": "All right, makes sense? Looks good. OK. So then, we took an\ninterlude and started talking",
    "start": "637720",
    "end": "643889"
  },
  {
    "text": "about algorithmic analysis. I put bootstrapping\nafter this, I promise.",
    "start": "643890",
    "end": "650940"
  },
  {
    "text": "So there was some want about\nhow to do about an actual another example of\nalgorithmic analysis,",
    "start": "650940",
    "end": "658140"
  },
  {
    "text": "stepping through it\nstep by step, OK? And so I conjured up\nanother example of recurse.",
    "start": "658140",
    "end": "666172"
  },
  {
    "text": "So I took the\nrecurse from lecture, and then I made it a\nlittle bit more complicated by adding in this\nrandom variable q and the for loop that\nactually increments",
    "start": "666172",
    "end": "674770"
  },
  {
    "text": "based on whatever q is. This should look\nlike problem 9 from p set 6-- no, from p set 5.",
    "start": "674770",
    "end": "682270"
  },
  {
    "text": "Yeah. So this is the one where,\nOK, we actually need to show that q is independent\nof something, all right?",
    "start": "682270",
    "end": "688810"
  },
  {
    "text": "So here's a law of\ntotal expectation. And the prompt that\nwe're given is, OK, what is the expected return\nvalue of this recurse function?",
    "start": "688810",
    "end": "694750"
  },
  {
    "text": "OK, so how do we\nstart this usually? All right, our target\nis expectation of x.",
    "start": "694750",
    "end": "700000"
  },
  {
    "text": "So random variable call\nthe function signature the expectation, then we\nreplace the recursive function",
    "start": "700000",
    "end": "709180"
  },
  {
    "text": "signature with the expectation\nof x everywhere we see it, and it matches like verbatim.",
    "start": "709180",
    "end": "714339"
  },
  {
    "text": "OK, so here, we return to this. We replace the recurse on\nthe return statement, with 7",
    "start": "714340",
    "end": "720580"
  },
  {
    "text": "plus expectation of x. Now what gets complicated\nis in for loop here,",
    "start": "720580",
    "end": "727040"
  },
  {
    "text": "we technically have\nexpectation of x times q, where q is this other uniform\ndistribution here from q, OK?",
    "start": "727040",
    "end": "738410"
  },
  {
    "text": "So the way we work\nwith that is when we see that, whatever\nour choice q is here, it is going to be independent of\nwhatever this recurse is going",
    "start": "738410",
    "end": "745970"
  },
  {
    "text": "to return us. So we can just make the\nindependent assumption here and say that\nexpectation of x times q",
    "start": "745970",
    "end": "751339"
  },
  {
    "text": "is equal to the expectation of x\ntimes the expectation of q, OK? ",
    "start": "751340",
    "end": "758670"
  },
  {
    "text": "So after that, OK, return 3. 3 is 3. So no expectation here. It's just this.",
    "start": "758670",
    "end": "764750"
  },
  {
    "text": "All right. Yes? Over the [INAUDIBLE].",
    "start": "764750",
    "end": "770530"
  },
  {
    "text": "Oh, sure. So in for loop here, the value\nthat we're going to return here is like, OK, depending\non whatever q--",
    "start": "770530",
    "end": "776170"
  },
  {
    "text": "the random variable\nhere is big q, and then the random\nvariable that we're going to get out of recurse\nat every step is some x.",
    "start": "776170",
    "end": "785417"
  },
  {
    "text": "So when you have two\nrandom variables, and we're looking for how many-- they're independent\nof each other, and you're trying\nto multiply them.",
    "start": "785417",
    "end": "791500"
  },
  {
    "text": "So first, you say, OK,\nthe two random events multiply each other,\nand then you say-- and then you say,\nOK, they're actually",
    "start": "791500",
    "end": "797050"
  },
  {
    "text": "independent of each other. So you can just split\nup the expectation here by independence assumption.",
    "start": "797050",
    "end": "802240"
  },
  {
    "text": "There is one thing\nthat's tricky here is that one question\nis, OK, is q-- if we were to think\nabout this outer e of x,",
    "start": "802240",
    "end": "809320"
  },
  {
    "text": "is q independent of\nthis outer e of x? Or sorry, let me put\nit the other way.",
    "start": "809320",
    "end": "814990"
  },
  {
    "text": "Is the outer e of x\nhere independent of q? The answer is no.",
    "start": "814990",
    "end": "820110"
  },
  {
    "text": "But the trick here is\nthat the inner recurse",
    "start": "820110",
    "end": "825720"
  },
  {
    "text": "has another random variable x. This is not actually\nthe original x that we had at the outer\nscope of the function.",
    "start": "825720",
    "end": "831032"
  },
  {
    "text": "It's the one from the inner\nscope of the function, meaning that this should be\nlike xi or something, drawn from the same distribution\nas this x over here, OK?",
    "start": "831032",
    "end": "839760"
  },
  {
    "text": "So that was one thing that came\nup a few times on an outpost but should be reasonable.",
    "start": "839760",
    "end": "845580"
  },
  {
    "text": "OK, does that make\nsense to everyone? All right, so we're going to--",
    "start": "845580",
    "end": "850860"
  },
  {
    "text": "OK, now we just put it all\ntogether and do some math. So we multiply each\none of the branches",
    "start": "850860",
    "end": "856470"
  },
  {
    "text": "by their expected value. So we use the law of\ntotal expectation. The probability of entering this\nfirst branch was 7 plus recurse",
    "start": "856470",
    "end": "863580"
  },
  {
    "text": "is 1 out of these\n10, so that's 1/10. The probability of\nentering the recurse",
    "start": "863580",
    "end": "869550"
  },
  {
    "text": "here is 1 out of-- it's\nanother 1 out of 10, so that's this term right here.",
    "start": "869550",
    "end": "875460"
  },
  {
    "text": "Then, 8/10 of the\ntime, we return 3. The reason why we chose\n1/10, 1/10, and 8/10 is because if I\nmade it any bigger,",
    "start": "875460",
    "end": "881889"
  },
  {
    "text": "the expectation is\nactually infinite. And when it's infinite,\nwe can't really apply the law of\ntotal expectation.",
    "start": "881890",
    "end": "888810"
  },
  {
    "text": "But that's a detail, OK? So now we just-- so now we just start summing.",
    "start": "888810",
    "end": "894720"
  },
  {
    "text": " So sum it. OK, we distribute.",
    "start": "894720",
    "end": "899990"
  },
  {
    "text": "You got 10 expectation on\nthe left, 6 expectation on the right and 31. Collect like terms and\nsolve, and you get 31 over 4.",
    "start": "899990",
    "end": "908006"
  },
  {
    "text": "Did that make sense? Should I go through that a\nlittle slower or we're good?",
    "start": "908006",
    "end": "913720"
  },
  {
    "text": "OK. Yes? It should 37 over 9. Oh, no, never mind.",
    "start": "913720",
    "end": "919070"
  },
  {
    "text": "I didn't see the multiplication. Sorry, my bad. OK, cool. I mean, I would not be surprised\nif there was a typo in this.",
    "start": "919070",
    "end": "925020"
  },
  {
    "text": "This was written late at night. I didn't see the\nmultiplication sign. OK, all right.",
    "start": "925020",
    "end": "931130"
  },
  {
    "text": "So now, this bootstrapping. So when we visited\nbootstrapping,",
    "start": "931130",
    "end": "937750"
  },
  {
    "text": "it was from sampling. We cared about two values. We were doing sampling,\nand it was the sample mean",
    "start": "937750",
    "end": "943630"
  },
  {
    "text": "and the sample variance. That means there are two\nflavors of bootstrapping are going to be over\nmeans and over variances.",
    "start": "943630",
    "end": "949760"
  },
  {
    "text": "So what that means\nis whenever you-- so the typical\nbootstrapping-- actually, I have a good example of this.",
    "start": "949760",
    "end": "955300"
  },
  {
    "text": "I can just know the structure\nfor bootstrapping a means and bootstrapping of variances.",
    "start": "955300",
    "end": "960550"
  },
  {
    "text": "The key difference is,\nyou will be calculating a mean on the\nresample, and then you have a distribution of your\nmeans on the variance version.",
    "start": "960550",
    "end": "967930"
  },
  {
    "text": "You have a variance\nof your resample. And then, you have the\ndistribution of your variances.",
    "start": "967930",
    "end": "974512"
  },
  {
    "text": "These are the two\nmost common forms of bootstrapping\nthat we're going to throw at you in this class. And I'll run-- so\nhere's an example",
    "start": "974512",
    "end": "981790"
  },
  {
    "text": "of how to do the corgi problem\nthat you guys saw in section. So I realized that\nthe corgi solution",
    "start": "981790",
    "end": "987730"
  },
  {
    "text": "that we had on the\nwebsite actually has really jumbled up overleaf. So I took the step solution and\njust made an example out of it.",
    "start": "987730",
    "end": "997890"
  },
  {
    "text": "So here's what\nwe're going to state is there's two\nislands with corgis. We want to say that island B's\ncorgis have a 3-inch higher",
    "start": "997890",
    "end": "1006100"
  },
  {
    "text": "variance than the height\nof those in A, OK? So if we look at the heights of\nthe corgis in islands A and B,",
    "start": "1006100",
    "end": "1011890"
  },
  {
    "text": "we want to assert the claim\nthat island B's variance is higher by 3, OK?",
    "start": "1011890",
    "end": "1019160"
  },
  {
    "text": "The way we do that\nis, OK, here's the population of\nall the corgis. We draw a population one from\nthe island of corgis an island",
    "start": "1019160",
    "end": "1025025"
  },
  {
    "text": "A, and we draw a population--\noh, that's supposed to be two. Great. There's a typo. Population 2 is supposed\nto be from corgis of B.",
    "start": "1025025",
    "end": "1033839"
  },
  {
    "text": "And then we start-- and\nthen we combine them into a general population\nand start sampling.",
    "start": "1033839",
    "end": "1039839"
  },
  {
    "text": "So the way we start there is-- OK, so this blue part is\nthe execution of the code.",
    "start": "1039839",
    "end": "1045000"
  },
  {
    "text": "So we first create the total\npopulation of all the corgis, then we enter a for loop.",
    "start": "1045000",
    "end": "1051960"
  },
  {
    "text": "And we say, OK, so we repeat\nthe following 50 times. Notice that we only\nrepeat resampling from the total\npopulation and not",
    "start": "1051960",
    "end": "1058390"
  },
  {
    "text": "resampling from the space\nof all corgis from islands. A good way to know this\nis your researchers",
    "start": "1058390",
    "end": "1064380"
  },
  {
    "text": "have already left the\nisland, so they can't really go back to take more corgis. We could only have\nso many corgis.",
    "start": "1064380",
    "end": "1070590"
  },
  {
    "text": "So, OK, so repeat 50 times. And then, the first thing we\ndo is we resample two samples",
    "start": "1070590",
    "end": "1079410"
  },
  {
    "text": "from the total population. And then once we\nresolve the two samples from the total population\nis, OK, this part",
    "start": "1079410",
    "end": "1086040"
  },
  {
    "text": "is going to be fast. We're going to do five\nlines of coding one. We take the count. So we look at the difference\nbetween the sample",
    "start": "1086040",
    "end": "1096860"
  },
  {
    "text": "variance of island\n2 minus island 1, and we look at the difference.",
    "start": "1096860",
    "end": "1101900"
  },
  {
    "text": "If this difference\nis greater than 3, we just increment by one. So that's exactly\nwhat this is saying.",
    "start": "1101900",
    "end": "1107510"
  },
  {
    "text": "And one here is the\nindicator random variable. That says, OK, if\nthis is random event,",
    "start": "1107510",
    "end": "1113630"
  },
  {
    "text": "so it happens to be true, we'll\nincrement 1 and 2 the count. OK, does that help?",
    "start": "1113630",
    "end": "1121190"
  },
  {
    "text": "Any questions? Yes. So this is it. This is an example\nof a problem where",
    "start": "1121190",
    "end": "1127310"
  },
  {
    "text": "we'll be calculating\nP-value, where we maybe see an example of\none of the other ones",
    "start": "1127310",
    "end": "1133220"
  },
  {
    "text": "that we saw before, like\ncalculating mean square rather than combining two\npopulations that we just",
    "start": "1133220",
    "end": "1138710"
  },
  {
    "text": "have one sub-population. So in the mean example,\nall we did was just",
    "start": "1138710",
    "end": "1147980"
  },
  {
    "text": "have a subsection\nof the population. We didn't take two and\nthen recombine them.",
    "start": "1147980",
    "end": "1153769"
  },
  {
    "text": "Oh, yeah, I didn't\nput that in the-- I didn't put that\nin the examples because that was a\nlecture example already. So I was thinking-- I was trying to give\nsomething that was not too far",
    "start": "1153770",
    "end": "1161630"
  },
  {
    "text": "from the lecture but still\nwasn't completely totally like redundant. Could we potentially see both\nof those types of problems?",
    "start": "1161630",
    "end": "1168919"
  },
  {
    "text": "But also, if we give you\nguys a bootstrapping problem, we're not going to go\ngive you a giant list and say, OK, take a\nsum over all of these.",
    "start": "1168920",
    "end": "1174540"
  },
  {
    "text": "That's not like-- I'm also say, hey, here's\ntwo or three samples. Yes, in the back?",
    "start": "1174540",
    "end": "1180830"
  },
  {
    "text": "Why are you taking the absolute\nvalue of that difference when you want to\nshow that island B? So sample 2 has a\nhigher variance.",
    "start": "1180830",
    "end": "1187400"
  },
  {
    "text": "Oh, it's because\nwe want to see-- OK, we want to see. If we take the corgis\nfrom island A and island B",
    "start": "1187400",
    "end": "1194600"
  },
  {
    "text": "and put them into\na total population, we don't actually know\nwhich one of these corgis came from island A and island B.",
    "start": "1194600",
    "end": "1200420"
  },
  {
    "text": "And what our P-value is\ntrying to show is, hey, even if I don't know--",
    "start": "1200420",
    "end": "1205650"
  },
  {
    "text": "the claim about the\nvariance shouldn't just appear in nature. It shouldn't just\nappear if I just",
    "start": "1205650",
    "end": "1211410"
  },
  {
    "text": "randomly combine\nthese two samples and then just started\ntaking resamples.",
    "start": "1211410",
    "end": "1217740"
  },
  {
    "text": "So the idea is if\npopulation A has-- if we combine population\nA and population B,",
    "start": "1217740",
    "end": "1224370"
  },
  {
    "text": "and we just suddenly\nstart seeing, oh, yeah, there's\ndifferences in the variances,",
    "start": "1224370",
    "end": "1229530"
  },
  {
    "text": "like greater than\n3, just in general,",
    "start": "1229530",
    "end": "1235140"
  },
  {
    "text": "then the claim is\nprobably not good. So that would actually give\nus like a higher P-value. So the higher the P-value\nis higher in the probability",
    "start": "1235140",
    "end": "1242160"
  },
  {
    "text": "that a hypothesis is null. Does that explain it? OK. I think I was just a little\nconfused because sample 2 would",
    "start": "1242160",
    "end": "1249520"
  },
  {
    "text": "be a different size and\nthen variance is also dependent on the\nsize of the sample. But so you should always\ntake the absolute value",
    "start": "1249520",
    "end": "1256300"
  },
  {
    "text": "of the difference. [INAUDIBLE]",
    "start": "1256300",
    "end": "1261795"
  },
  {
    "text": " Yeah. OK, you have to--",
    "start": "1261796",
    "end": "1267670"
  },
  {
    "text": "OK, as for what is your-- if\nyou have a two-tailed test, take the absolute value.",
    "start": "1267670",
    "end": "1273290"
  },
  {
    "text": "What is it-- Do we need to-- We can talk this\noffline, I guess. Yeah. We can clarify offline.",
    "start": "1273290",
    "end": "1279240"
  },
  {
    "text": "But in general, if you have a\ntotal population here, right? And you try to split\ninto two samples, and you take the\ndifference, you don't",
    "start": "1279240",
    "end": "1286700"
  },
  {
    "text": "know if you're proving\nthat they're smaller by 3 or bigger than 3. ",
    "start": "1286700",
    "end": "1295343"
  },
  {
    "text": "Short answer is yes, but\nbe cautious about it.  The problem will\nprobably tell you",
    "start": "1295343",
    "end": "1301340"
  },
  {
    "text": "if you should take the\nabsolute value or not. OK. All right. ",
    "start": "1301340",
    "end": "1309649"
  },
  {
    "text": "Where do we get the 3 from? [INAUDIBLE] Oh, 3 inches higher.",
    "start": "1309650",
    "end": "1315169"
  },
  {
    "text": "Yeah. It was in a prompt. Yes? Is there a time we would ever\nsample without replacement",
    "start": "1315170",
    "end": "1321950"
  },
  {
    "text": "during [INAUDIBLE]? That's called the\npermutation test. And no.",
    "start": "1321950",
    "end": "1327170"
  },
  {
    "text": "Yes? OK. All right. Next. All right. Parameter estimation.",
    "start": "1327170",
    "end": "1334039"
  },
  {
    "text": "OK, so very, very-- this is a very old meme,\naccording to my associates.",
    "start": "1334040",
    "end": "1340850"
  },
  {
    "text": "In parameter\nestimation, you have-- the problem we\ntell, will probably ask you, what do we want?",
    "start": "1340850",
    "end": "1346860"
  },
  {
    "text": "And you'll say, Ah, we\nwant the parameters, and what do they look like? Some Greek symbols,\nor P, or A and B. So",
    "start": "1346860",
    "end": "1355005"
  },
  {
    "text": "these are given to you like,\nOK, if you have a Bernoulli, your theta is going to be a p. If you have a Poisson,\nlambda, your theta",
    "start": "1355005",
    "end": "1360440"
  },
  {
    "text": "is going to be a lambda. If you have uniform a and b,\nyour thetas are going to be a and b, and you have\nnormal, [INAUDIBLE] be a mu--",
    "start": "1360440",
    "end": "1366110"
  },
  {
    "text": "mu and sigma squared, right? So question is like,\nOK, why is theta the one that's left isolated here?",
    "start": "1366110",
    "end": "1371220"
  },
  {
    "text": "It's because we will\nconventionally-- the first Greek letter\nwe think of whenever we think of parameters is theta.",
    "start": "1371220",
    "end": "1376549"
  },
  {
    "text": "Probably because it's\neasy to write or so. So yeah. This is parameter estimation.",
    "start": "1376550",
    "end": "1381720"
  },
  {
    "text": "OK? All right. Then here is maximum\nlikelihood estimators.",
    "start": "1381720",
    "end": "1387040"
  },
  {
    "text": "So whenever I think of MLE, I\nthink of like a 2-step process.",
    "start": "1387040",
    "end": "1392610"
  },
  {
    "text": "The first step is to\nfind the likelihood. The second step\nis to optimize it.",
    "start": "1392610",
    "end": "1397740"
  },
  {
    "text": "So likelihood here is a big\nproduct over all the PDFs. Usually, you want\nthe log likelihood",
    "start": "1397740",
    "end": "1404580"
  },
  {
    "text": "because we want to take\na derivative of it. And then once we\nfind the derivative, we can actually solve for\nthe argmax of the theta.",
    "start": "1404580",
    "end": "1411220"
  },
  {
    "text": "So to solve for argmax\nof the theta here, you would either\nuse gradient ascent, or you would set the derivative\nequal to 0 and solve.",
    "start": "1411220",
    "end": "1418573"
  },
  {
    "text": "We will most likely tip you\noff about which one you should be using on a problem\nso you don't spend too much time trying\nto solve for 0,",
    "start": "1418573",
    "end": "1425520"
  },
  {
    "text": "or trying to solve derivative\nequals 0 on a problem where you should be\nusing gradient descent. Typically, those forms are\nlike, describe an algorithm--",
    "start": "1425520",
    "end": "1433140"
  },
  {
    "text": "I should have put this\nin the English part. If a problem says describe\nan algorithm to optimize, you almost always want to\ngo to gradient descent.",
    "start": "1433140",
    "end": "1440130"
  },
  {
    "text": "If the problem doesn't say\nthat, there is a chance that you could solve it using\nthe idea that the derivative",
    "start": "1440130",
    "end": "1445620"
  },
  {
    "text": "should be equal to 0. OK? Makes sense?",
    "start": "1445620",
    "end": "1451290"
  },
  {
    "text": "All right. So here's a MAP. What's the difference\nbetween MAP and MLE?",
    "start": "1451290",
    "end": "1458250"
  },
  {
    "text": "So what I wrote here was, I\nput the two steps of the MAP on the left, and I put the two\nsteps of the MAP on the right.",
    "start": "1458250",
    "end": "1463990"
  },
  {
    "text": "So MLE is on the left,\nMAP is on the right. Or I should have labeled\nthis a little bit better. So in MLE, I'll update these\nslides to actually say that--",
    "start": "1463990",
    "end": "1472320"
  },
  {
    "text": "there should be\nlike an MLE here. In the left side over here,\nyou just have the PDF.",
    "start": "1472320",
    "end": "1478320"
  },
  {
    "text": "On the right side,\nin the MAP, you have some prior g of theta here. We will almost always give\nyou what this theta is,",
    "start": "1478320",
    "end": "1484890"
  },
  {
    "text": "and the only point\nis, it's multiplied. So the optimal-- OK, I put a big space here,\nbut these are actually",
    "start": "1484890",
    "end": "1491400"
  },
  {
    "text": "multiplied by each other. You want to find the\noptimal parameter, like the argmax of\nthis likelihood,",
    "start": "1491400",
    "end": "1497828"
  },
  {
    "text": "where they actually\nmultiply by each other. OK? The main difference is here,\nin your argmax function.",
    "start": "1497828",
    "end": "1503490"
  },
  {
    "text": "You will have a log\nof the G of theta added to your original\nargmax target for MLE.",
    "start": "1503490",
    "end": "1511530"
  },
  {
    "text": "OK? So I mean, just for\nexposition, go back one, like, you're here, just taking\nthe argmax over this product,",
    "start": "1511530",
    "end": "1519770"
  },
  {
    "text": "or over the sum,\nand here, you're taking the argmax over\nthat sum and the log.",
    "start": "1519770",
    "end": "1527470"
  },
  {
    "text": "OK? That's main difference. Yes? So the right is the MAP? Yes, the right side is the MAP.",
    "start": "1527470",
    "end": "1534000"
  },
  {
    "text": "Yeah. I had labels on\nthese, but then I think they got shifted\naround when I was",
    "start": "1534000",
    "end": "1539100"
  },
  {
    "text": "trying to squeeze in the MAP. Yes? Key difference is that the\nMAP includes a prior piece?",
    "start": "1539100",
    "end": "1544500"
  },
  {
    "text": "Yes. So the MAP is like, if you\nhad a friend that has already done some\nexperimentation on this,",
    "start": "1544500",
    "end": "1549510"
  },
  {
    "text": "and you want to use their work. OK. Yes? Then by virtue of that, MAP\nwould be slightly more accurate",
    "start": "1549510",
    "end": "1559140"
  },
  {
    "text": "than MLE? If your friend is a good friend\nand he gave you correct data,",
    "start": "1559140",
    "end": "1564270"
  },
  {
    "text": "that's not noisy. Which is, I mean, good\nfriends could accidentally give you bad data, too. But",
    "start": "1564270",
    "end": "1569730"
  },
  {
    "text": "Is that why MLE is\njust more common then? MLE is like, you need\nMLE to solve MAP. So we like introduce\nMLE first, and then MAP.",
    "start": "1569730",
    "end": "1576480"
  },
  {
    "text": "Why it's more common? It probably is more common,\nbut I'm not quite sure. I don't actually know the\ngeneral claim of that. Yes?",
    "start": "1576480",
    "end": "1581910"
  },
  {
    "text": "Will the problem tell us\nwhether we need to use MLE or how will that be given? Like-- If it gives you a prior,\nthat's a tip off right?",
    "start": "1581910",
    "end": "1588512"
  },
  {
    "text": "If it gives you a prior, you\nshould probably use the prior. Yeah. ",
    "start": "1588512",
    "end": "1594330"
  },
  {
    "text": "OK? All righty. So next one is here,\nour MAP priors. I put this slide on here\njust for exposition.",
    "start": "1594330",
    "end": "1601260"
  },
  {
    "text": "But we will almost\nalways give you these. But the idea is like,\nwhatever your MLE estimate",
    "start": "1601260",
    "end": "1608730"
  },
  {
    "text": "is, this will actually influence\nwhat your prior should be. So if you're like estimate\nis like a Bernoulli,",
    "start": "1608730",
    "end": "1614460"
  },
  {
    "text": "you want to almost always\nuse a beta to be your prior. And these are the whole\nconjugate distribution things.",
    "start": "1614460",
    "end": "1620309"
  },
  {
    "text": "So like you guys don't have\nto know every little detail about conjugate distributions\nbecause we will almost always",
    "start": "1620310",
    "end": "1626310"
  },
  {
    "text": "give you whatever\nprior this is here, and all you guys need to know is\nhow to use it in the MAP sense.",
    "start": "1626310",
    "end": "1633070"
  },
  {
    "text": "OK? I mean, cheat sheet wise, you\nhave 10 pages, if you want. Like, just write down the\nnames of these distributions",
    "start": "1633070",
    "end": "1639817"
  },
  {
    "text": "and their parameters, and maybe\nlike a one sentence about what they do, or like, I don't know. Well, like a bike\nride or something,",
    "start": "1639817",
    "end": "1646270"
  },
  {
    "text": "and you're waiting\nfor a parking spot, just like, whip out a\nphone and like check it. What else? OK?",
    "start": "1646270",
    "end": "1653260"
  },
  {
    "text": "All right. So that's about MAP priors. Here's the general\nmachine learning paradigm.",
    "start": "1653260",
    "end": "1659110"
  },
  {
    "text": "The idea is, someone\ngives you data, and they want the\noptimal parameter",
    "start": "1659110",
    "end": "1665230"
  },
  {
    "text": "because it's totally\na fair trade. Yes? Sorry. [INAUDIBLE]? Sure.",
    "start": "1665230",
    "end": "1671430"
  },
  {
    "text": "Before you start trading? OK. Yeah? For this, what is\nit, [INAUDIBLE]??",
    "start": "1671430",
    "end": "1678530"
  },
  {
    "text": "[INAUDIBLE] is the conjugate\nprior of the multinomial. ",
    "start": "1678530",
    "end": "1684940"
  },
  {
    "text": "[INAUDIBLE]? Look at the course reader. They'll probably tell you.",
    "start": "1684940",
    "end": "1689987"
  },
  {
    "text": "Look at the course reader. We have a description of it. If you have to\nuse a [INAUDIBLE],, we'll give you the\nparameters for it.",
    "start": "1689987",
    "end": "1696370"
  },
  {
    "text": "OK? Sounds good. Yeah. Let's go trace some\nparameters for datas.",
    "start": "1696370",
    "end": "1702702"
  },
  {
    "text": "OK. All right. So in machine\nlearning, typically what you have is some\nreal world problem.",
    "start": "1702702",
    "end": "1709592"
  },
  {
    "text": "You want to model\nand then you want to model the problem\nand format, and use-- sorry, using a\nformal model theta.",
    "start": "1709592",
    "end": "1716030"
  },
  {
    "text": "Here is typically where you\nwould find log likelihood. And then you will have\nsome training data.",
    "start": "1716030",
    "end": "1722279"
  },
  {
    "text": "So this is the\nindices on your sum, and then you have some\nlearning algorithm. This learning\nalgorithm is usually",
    "start": "1722280",
    "end": "1727410"
  },
  {
    "text": "how you optimize the theta. OK? And then since we're all\nveterans on p set 6 by now,",
    "start": "1727410",
    "end": "1734429"
  },
  {
    "text": "here is the testing\ndata, and you're supposed to just get\na score after you get this optimal parameter.",
    "start": "1734430",
    "end": "1740340"
  },
  {
    "text": "So just a brief high level about\nhow the machine learning things work, we might ask you\na true-false question",
    "start": "1740340",
    "end": "1746940"
  },
  {
    "text": "about some parts of\nthe data pipeline. But we're not going to give\nyou a 30-pointer on just, hey,",
    "start": "1746940",
    "end": "1753360"
  },
  {
    "text": "design a machine\nlearning model for this. That's a little hard. OK?",
    "start": "1753360",
    "end": "1760309"
  },
  {
    "text": "All right. So after that, we\nsaw Naive Bayes. Naive Bayes is just using\nthe independence assumption,",
    "start": "1760310",
    "end": "1766900"
  },
  {
    "text": "where if I condition\non a Y, then all the Xi's are independent.",
    "start": "1766900",
    "end": "1772480"
  },
  {
    "text": "It's naive because we're\ntrying to condition on the independence. ",
    "start": "1772480",
    "end": "1779405"
  },
  {
    "text": "What's special about-- so what's\nthe hard parts about the Naive Bayes is, we're trying to-- we're given these like\ntuples of like X I and Y,",
    "start": "1779405",
    "end": "1786830"
  },
  {
    "text": "and we're trying\nto actually predict like, OK, what's the optimal\nY here given these labels.",
    "start": "1786830",
    "end": "1791909"
  },
  {
    "text": "Right?  The thing that you\ntypically want to do is a Laplace\nsmoothing, which means",
    "start": "1791910",
    "end": "1798020"
  },
  {
    "text": "OK, add 1 to all your\nnumerators, and then add 2 or however many to\nyour denominators",
    "start": "1798020",
    "end": "1803149"
  },
  {
    "text": "so that the data set\nis still balanced. Usually, this is just\nlike a counting problem where, OK, you just count\nthe number of times where",
    "start": "1803150",
    "end": "1809545"
  },
  {
    "text": "X I equals X I and\nbig Y equals little y, and you divide it by-- the samples were, like,\nY is equal to little y.",
    "start": "1809545",
    "end": "1815625"
  },
  {
    "text": " Then when you do\na prediction, no need to do a derivative here.",
    "start": "1815625",
    "end": "1821308"
  },
  {
    "text": "No need to do, any gradient\nascent, nothing of that sort. Just loop over all your\nY's, locate your data,",
    "start": "1821308",
    "end": "1827540"
  },
  {
    "text": "do a few sums, spit it out. That's Naive Bayes. OK, yes?",
    "start": "1827540",
    "end": "1833710"
  },
  {
    "text": "Can you explain that\np-hat [INAUDIBLE]?? P-hat? Oh, p-hat is actually\nprobably-- is the probability--",
    "start": "1833710",
    "end": "1843280"
  },
  {
    "text": "that is a probability\nthat were-- or estimated probability\nthat Y is the actual Y. Yes,",
    "start": "1843280",
    "end": "1849003"
  },
  {
    "text": "and we're going\nto try that, we're going to try to derive here. So p-hat is our estimate\nof whatever the X I equal--",
    "start": "1849003",
    "end": "1855670"
  },
  {
    "text": "of like the X I and Y. This is the thing that we\nactually find in training.",
    "start": "1855670",
    "end": "1861195"
  },
  {
    "text": "So [INAUDIBLE] it's just saying. Yes. Yes, it's the approximate.",
    "start": "1861195",
    "end": "1866330"
  },
  {
    "text": "Whenever you see a\nhat, it's approximate. OK. Yes? Is the probability of y-hat\nbeing 1, since 1 minus--",
    "start": "1866330",
    "end": "1874730"
  },
  {
    "text": "probability 0,\n[INAUDIBLE] prior?  Sorry.",
    "start": "1874730",
    "end": "1880260"
  },
  {
    "text": "Sorry, y-hat [INAUDIBLE]? Right? You plug-in 0 for Y. Sure. Right?",
    "start": "1880260",
    "end": "1885570"
  },
  {
    "text": "1 minus that-- Yeah. Probability that\ny-hat is equal to 1.",
    "start": "1885570",
    "end": "1891300"
  },
  {
    "text": "Probability that\ny-hat is a conjugate? That's if Y is a Bernoulli.",
    "start": "1891300",
    "end": "1896820"
  },
  {
    "text": "If it's 0 1, Y can\nbe 4, cross 4, right? It could be like happy\nface, smiley face, sad face,",
    "start": "1896820",
    "end": "1902970"
  },
  {
    "text": "and angry face. It could be it could\nbe size 4, right? Yeah. Yeah. If y is a Bernoulli,\nyeah, you can do 1 minus,",
    "start": "1902970",
    "end": "1908190"
  },
  {
    "text": "but if it's not, then-- So in this case, 0\nor 1 [INAUDIBLE].. Yeah, in this case, yeah.",
    "start": "1908190",
    "end": "1913830"
  },
  {
    "text": "Because it's 0 1. Yeah, like it explicitly\nsays like 0 1. OK?",
    "start": "1913830",
    "end": "1918970"
  },
  {
    "text": "All right. So next, here's\nlogistic regression. Logistic regression,\nin my opinion, is like, MLE with a Bernoulli.",
    "start": "1918970",
    "end": "1925380"
  },
  {
    "text": "But the thing about\na Bernoulli is it's a discrete random variable\nso we had to go conjure up",
    "start": "1925380",
    "end": "1930810"
  },
  {
    "text": "a PDF for the Bernoulli.  And that's where we\nget the inspiration",
    "start": "1930810",
    "end": "1936590"
  },
  {
    "text": "for like the log likelihood. Yes? Could you explain\nhow MLE and MAP",
    "start": "1936590",
    "end": "1943670"
  },
  {
    "text": "fit into Naive Bayes and\nlogistic regression one more time?",
    "start": "1943670",
    "end": "1949370"
  },
  {
    "text": "I can explain how MLE fits\ninto logistic regression because it's just\nlogistic regression where your PDF, or your\nlikelihood function",
    "start": "1949370",
    "end": "1956059"
  },
  {
    "text": "is chosen with the PDF of\na Bernoulli distribution. For MAP, MAP is like what\nhappens if you have a prior.",
    "start": "1956060",
    "end": "1964460"
  },
  {
    "text": "But in Naive Bayes-- OK, so MAP to\nlogistic regression,",
    "start": "1964460",
    "end": "1969980"
  },
  {
    "text": "you can have a logistic\nregression problem that is solvable using MAP.",
    "start": "1969980",
    "end": "1975509"
  },
  {
    "text": "Like if I give you a prior\nabout-- if I give you a prior about this\nBernoulli, it's going to take the\nform of a beta.",
    "start": "1975510",
    "end": "1982160"
  },
  {
    "text": "Then you just have\nanother data point for your data-- for your\nprediction algorithm.",
    "start": "1982160",
    "end": "1988640"
  },
  {
    "text": "How Naive Bayes fits\nin is a little bit more tricky because Naive\nBayes doesn't like-- Naive Bayes, you're just--",
    "start": "1988640",
    "end": "1994299"
  },
  {
    "text": "you're taking the\nindependence assumption, and you're just counting.",
    "start": "1994300",
    "end": "2000010"
  },
  {
    "text": "I think, yeah-- so Is it little\nparameter estimation?",
    "start": "2000010",
    "end": "2005220"
  },
  {
    "text": "Like just p-hat. Your parameter estimation\nis honestly done online. It's kind of like, you don't--",
    "start": "2005220",
    "end": "2010868"
  },
  {
    "text": "you have to loop\nover your whole data set every single time you\nwant to do a prediction, which is the key difference\nbetween MLE and Naive Bayes.",
    "start": "2010868",
    "end": "2019250"
  },
  {
    "text": "Right? OK. All right. Good?",
    "start": "2019250",
    "end": "2025920"
  },
  {
    "text": "All right. So with that, I have a\nbunch of problems lined up. Or two, or three, or something--",
    "start": "2025920",
    "end": "2032200"
  },
  {
    "text": "I think it's like\ntwo or three, yeah. I'll walk through them slowly. They're not going to be like\nthat bootstrapping example",
    "start": "2032200",
    "end": "2038010"
  },
  {
    "text": "where it was like four\nlines worth of problems. But sorry, not four lines\nwith a code, only one comment.",
    "start": "2038010",
    "end": "2044880"
  },
  {
    "text": "That won't happen. So here's the problems. The first one is an MLE example.",
    "start": "2044880",
    "end": "2050020"
  },
  {
    "text": "It's about how to do\nreliability engineering. Here's a problem\nwhere you say, OK, we",
    "start": "2050020",
    "end": "2055621"
  },
  {
    "text": "have a random variable\ncharacterized by A with some PDF, this. We wish to know how long a\nparticular model of a phone",
    "start": "2055622",
    "end": "2063989"
  },
  {
    "text": "will function before it breaks. That's what this PDF is here. We are going to use a\nreliability distribution.",
    "start": "2063989",
    "end": "2071040"
  },
  {
    "text": "To this N we collect N\nindependent measurements of how long the type of\nphone functions before it",
    "start": "2071040",
    "end": "2076919"
  },
  {
    "text": "breaks X1 to Xn, explain in\nwords how you would choose parameter A using\nthe MLE framework,",
    "start": "2076920",
    "end": "2082619"
  },
  {
    "text": "and provide any\nnecessary derivatives. OK, so what's the key-- what\nare the key data points here?",
    "start": "2082620",
    "end": "2089020"
  },
  {
    "text": "What's our parameter? A. Where's our data set? X1 through Xn.",
    "start": "2089020",
    "end": "2094600"
  },
  {
    "text": "Which are we using? MLE. What's our likelihood? This thing right here,\nthe center, big equation.",
    "start": "2094600",
    "end": "2103040"
  },
  {
    "text": "So how do we map\nthis to our 2-step? We draw arrows, and say,\nOK, the f just replace the--",
    "start": "2103040",
    "end": "2111680"
  },
  {
    "text": "like strictly with our f. Take the data points X1 through\nXn and replace Xi with them.",
    "start": "2111680",
    "end": "2118740"
  },
  {
    "text": "Theta hat here and\nnormal theta here, replace that with\nparameter A and then we apply the MLE like training--",
    "start": "2118740",
    "end": "2126960"
  },
  {
    "text": "we apply the estimator\nfrom the MLE. OK? So to that end, I wrote\nout all the steps.",
    "start": "2126960",
    "end": "2133240"
  },
  {
    "text": "We're going to move the\nproblem up to the upper right, and I hope that's not too small.",
    "start": "2133240",
    "end": "2138297"
  },
  {
    "text": "People in the back, are we good? Ish? OK. So, first, we need to\nfind a log likelihood.",
    "start": "2138297",
    "end": "2144710"
  },
  {
    "text": "So we're given the\nlikelihood first. We take the log.",
    "start": "2144710",
    "end": "2149920"
  },
  {
    "text": "Yeah, we just apply the log\nand put an L on the left. Then, we swap the\nlog and the product",
    "start": "2149920",
    "end": "2156040"
  },
  {
    "text": "so it becomes a\nsum over the log. Is it too small? Is it?",
    "start": "2156040",
    "end": "2161319"
  },
  {
    "text": "OK, that's X squared J\nand this is X squared J and A and A squared.",
    "start": "2161320",
    "end": "2167260"
  },
  {
    "text": "Then we use some more--\nwe use the log rules and start splitting\nup the terms. So this is just--",
    "start": "2167260",
    "end": "2173170"
  },
  {
    "text": "if you have multiplication\nin your log, they're added. Use log rules again to\nbring your exponents down.",
    "start": "2173170",
    "end": "2180290"
  },
  {
    "text": "OK, now you have this fully,\nalmost simplified summation term, and then just break\nup the sums a little bit",
    "start": "2180290",
    "end": "2187059"
  },
  {
    "text": "so it's easier to\ndo the derivatives. OK? OK, and then take\nout a few terms.",
    "start": "2187060",
    "end": "2195420"
  },
  {
    "text": "There's no-- this term right\nhere does not rely on I at all. So you can just sum\nover all n of them,",
    "start": "2195420",
    "end": "2201030"
  },
  {
    "text": "and that's negative 2n. OK? Any questions about how\nwe went from likelihood",
    "start": "2201030",
    "end": "2206540"
  },
  {
    "text": "to log likelihood? Assuming the text is\nnot too small for people in the back to see.",
    "start": "2206540",
    "end": "2211822"
  },
  {
    "text": "I realize that this might\nbe this a bit small, but the size have\nall the numbers.",
    "start": "2211822",
    "end": "2217410"
  },
  {
    "text": "Yes? Sufficiently simplified? I mean, sufficient simplified\nto take a derivative.",
    "start": "2217410",
    "end": "2224020"
  },
  {
    "text": "Yeah. I mean, this is\nsufficiently simplified. We're also not going to be\nlike, hey, how much did you simplify your log likelihood? It just has to be simple\nenough that you can actually",
    "start": "2224020",
    "end": "2230638"
  },
  {
    "text": "take a derivative\nin a reasonable way. OK? All right. So now we take the derivative.",
    "start": "2230638",
    "end": "2236640"
  },
  {
    "text": "So step two, optimize. To optimize, we need to optimize\nwith respect to our parameter,",
    "start": "2236640",
    "end": "2241850"
  },
  {
    "text": "A. So that's just, OK,\ntake the derivative of this giant thing down here.",
    "start": "2241850",
    "end": "2249212"
  },
  {
    "text": "Then if we apply the derivative\nto each one of these terms, we said, OK, this becomes\nlike negative 2 over A, this becomes just log of\nXi, this becomes nothing",
    "start": "2249212",
    "end": "2259560"
  },
  {
    "text": "because it's just-- there's no A term here,\nand this is positive 1 over A cubed, or so\nsomething of that sort.",
    "start": "2259560",
    "end": "2266500"
  },
  {
    "text": " If you propagate the\nderivative and simplify, you get just those\nterms that we saw.",
    "start": "2266500",
    "end": "2275410"
  },
  {
    "text": "Yes? These are not the\nsame terms because we dropped the A from this\nterm [INAUDIBLE] here.",
    "start": "2275410",
    "end": "2282810"
  },
  {
    "text": "OK? I think, at this point, yeah,\nyou set the derivative equal to 0 and you can solve for A.",
    "start": "2282810",
    "end": "2290010"
  },
  {
    "text": "One way that you can know\nthat you can solve for A here is that there's\nonly one parameter. Like we saw in the\nsection problem,",
    "start": "2290010",
    "end": "2296130"
  },
  {
    "text": "where there was two\nparameters that you had both like mu and sigma. But here, you only have A, you\nshould be able to solve for it.",
    "start": "2296130",
    "end": "2303520"
  },
  {
    "text": "If you don't,\nthat's probably OK. Yes? Have two parameters you're\ntrying to optimize, just",
    "start": "2303520",
    "end": "2309260"
  },
  {
    "text": "take the partial of [INAUDIBLE]\nwith respect to both parameters? Yes. Optimize that?",
    "start": "2309260",
    "end": "2314599"
  },
  {
    "text": "Yes, and then solve. Then in that case, you would\nhave two partial derivatives, right? You would have--\nwell, you know what?",
    "start": "2314600",
    "end": "2321260"
  },
  {
    "text": "Here, we only had partial\nLL partial A, right?",
    "start": "2321260",
    "end": "2328250"
  },
  {
    "text": "In that case, [INAUDIBLE]. That's not look [INAUDIBLE]. In that particular\ncase, you would have partial LL over\npartial mu, and partial LL.",
    "start": "2328250",
    "end": "2336950"
  },
  {
    "text": "So you have this\nthing, and like-- and then, like, comma,\npartial LL over partial sigma.",
    "start": "2336950",
    "end": "2345990"
  },
  {
    "text": "So you will take the\nderivative of both of those and then try to optimize. Usually, if you have\ntwo parameters that you need to optimize\nfor, we don't expect",
    "start": "2345990",
    "end": "2352052"
  },
  {
    "text": "you guys to solve,\nsetting equal to zero, and you will just\nuse gradient ascent. Yes?",
    "start": "2352052",
    "end": "2357290"
  },
  {
    "text": "Now, can you optimize both the\nmu and the sigma independently? One will become--\nso if you can solve",
    "start": "2357290",
    "end": "2364970"
  },
  {
    "text": "for one parameter in terms\nof the other one, then, yes. But if you can't, like if\nsigma is dependent on mu,",
    "start": "2364970",
    "end": "2370820"
  },
  {
    "text": "and the mu is also dependent on\nsigma, then it's not so simple. And you need to use ascent.",
    "start": "2370820",
    "end": "2376100"
  },
  {
    "text": "Yes? [INAUDIBLE] What's the sum\nterms in the middle? Oh, the sum terms in the middle?",
    "start": "2376100",
    "end": "2381410"
  },
  {
    "text": "Are you talking about this term? This one, we're taking the\nderivative with respect to A. So take your\npartial derivative,",
    "start": "2381410",
    "end": "2387750"
  },
  {
    "text": "pass it into-- oh, here. I have this-- I have--\noho, laser pointer. So we take this partial\nderivative [INAUDIBLE] with A,",
    "start": "2387750",
    "end": "2394920"
  },
  {
    "text": "pass it into the\nsum, and then you know that sum and the\npartial derivative, I can just pass the partial\nderivative into the sum.",
    "start": "2394920",
    "end": "2402180"
  },
  {
    "text": "Then just take partial\nderivative of A times log of Xi is just log of Xi, which is this\nexact term we have down here.",
    "start": "2402180",
    "end": "2410670"
  },
  {
    "text": "OK? And then sum term\njust disappears because there's no A in there.",
    "start": "2410670",
    "end": "2416450"
  },
  {
    "text": "Cool. Yes? So for the first--",
    "start": "2416450",
    "end": "2421587"
  },
  {
    "text": "on the solution, we\ngot negative 2 A and A because of negative\nlog of A, right?",
    "start": "2421587",
    "end": "2429560"
  },
  {
    "text": "Yes. So log of A here, just the\nderivative here just because 1 over A times negative 2n.",
    "start": "2429560",
    "end": "2434780"
  },
  {
    "text": "Yeah. Cool. Yes. Keep asking. Derivatives are hard.",
    "start": "2434780",
    "end": "2440680"
  },
  {
    "text": "We should practice them. OK. All right. ",
    "start": "2440680",
    "end": "2447250"
  },
  {
    "text": "Yes? Oh. Sorry. Yes? So for optimization\nMLE and MAP, we mostly",
    "start": "2447250",
    "end": "2454450"
  },
  {
    "text": "stick with taking the partial\nand setting it equal to 0, whereas gradient\ndescent, we'll mostly",
    "start": "2454450",
    "end": "2459490"
  },
  {
    "text": "stay in the realm of\nlogistic regression? You could actually\ntell if the problem",
    "start": "2459490",
    "end": "2464792"
  },
  {
    "text": "wants to use gradient\ndescent or not sometimes. If we-- we'll say at the\nend, give us a three sentence",
    "start": "2464792",
    "end": "2470560"
  },
  {
    "text": "algorithm about how to-- OK, this is also not a promise,\nbecause we haven't decided yet.",
    "start": "2470560",
    "end": "2477400"
  },
  {
    "text": "But usually at the\nend of a problem, if it tells you, hey, give\nus like a three sentence",
    "start": "2477400",
    "end": "2483797"
  },
  {
    "text": "thing about how you\nwould optimize it, that typically means\nyou can't solve it by setting it equal to 0.",
    "start": "2483797",
    "end": "2489250"
  },
  {
    "text": "But if we don't\nsay anything, you might be able to\nsolve it equal to 0, and might as in,\neven if you make",
    "start": "2489250",
    "end": "2495160"
  },
  {
    "text": "a mistake along\nthe way somewhere, you can always just fall\nback on gradient ascent and we'll give some\npartial credit for that.",
    "start": "2495160",
    "end": "2500200"
  },
  {
    "text": "And we can just explain\ngradient ascent in words? Oh, I have a slide with\ngradient ascent right. Like, oh, it's too far back.",
    "start": "2500200",
    "end": "2507200"
  },
  {
    "text": "But in the slides, there\nis a gradient ascent. And you can just be like,\ntheta new equals theta old minus partial\nderivative LL theta.",
    "start": "2507200",
    "end": "2513050"
  },
  {
    "text": "Done. Right? Yeah, you could just write\nthat, and be like, oh yeah, just execute this in\na loop or something.",
    "start": "2513050",
    "end": "2519670"
  },
  {
    "text": "Yeah. OK, all right. So here is an example\nabout how to use betas",
    "start": "2519670",
    "end": "2525049"
  },
  {
    "text": "and how to do pseudocode.  Let's say that we have observed\nsome 30 successes and 20",
    "start": "2525050",
    "end": "2533109"
  },
  {
    "text": "failures for an event\nwith unknown probability. Based on this\ninformation, we can",
    "start": "2533110",
    "end": "2538150"
  },
  {
    "text": "model the probability\nof success, X as a beta random variable. We want to make the claim\nof the form, the probability",
    "start": "2538150",
    "end": "2545560"
  },
  {
    "text": "of success is M plus or minus B.",
    "start": "2545560",
    "end": "2550887"
  },
  {
    "text": "So we need to select M here to\nbe the expectation of X. Select B to be the smallest value,\nrounded to two decimal places.",
    "start": "2550887",
    "end": "2558220"
  },
  {
    "text": "Such that probability of\nM minus B is less than X, less than M plus B is\ngreater than or equal to 95%.",
    "start": "2558220",
    "end": "2564277"
  },
  {
    "text": "And provide your answer\nas a pseudocode that prints out the values, M and B.",
    "start": "2564277",
    "end": "2570700"
  },
  {
    "text": "Here are some key points\nthat I had on a first pass of this problem. But looking at it\nagain this morning",
    "start": "2570700",
    "end": "2575980"
  },
  {
    "text": "when I didn't have time\nto edit the slides, there's another two. Oh, here, the bolded part\nthat I just didn't box in,",
    "start": "2575980",
    "end": "2584140"
  },
  {
    "text": "rounded to two decimal places. What this typically means is\nsomewhere in your pseudocode, you'll be doing a grid search.",
    "start": "2584140",
    "end": "2590500"
  },
  {
    "text": "OK? What does that mean? That means that if I'm\nafter some value, like--",
    "start": "2590500",
    "end": "2596010"
  },
  {
    "text": "they're both used, OK. So if I'm after some\nvalue, say X, right?",
    "start": "2596010",
    "end": "2603119"
  },
  {
    "text": "I want X between A and\nB. There should probably",
    "start": "2603120",
    "end": "2610110"
  },
  {
    "text": "be like a for loop in there that\ndoes like, X plus, equals 0.01.",
    "start": "2610110",
    "end": "2618390"
  },
  {
    "text": "Because they say up\nto two decimal places. And you're just going to be\nlooping through this until",
    "start": "2618390",
    "end": "2623579"
  },
  {
    "text": "like break. Break if, if you have\nsome condition on X.",
    "start": "2623580",
    "end": "2632335"
  },
  {
    "text": "This is like a grid search kind\nof strategy, where you're like, OK, you just want to sweep over\nall the possible values of X",
    "start": "2632335",
    "end": "2638190"
  },
  {
    "text": "up to some precision, and\nwe'll return to the value that you need. ",
    "start": "2638190",
    "end": "2645630"
  },
  {
    "text": "All right. So for this particular problem,\nwe have a starter function. Yes, because everything\nstarts with main,",
    "start": "2645630",
    "end": "2652520"
  },
  {
    "text": "and then, we need to combine\nthe 30 successes and 20 failures",
    "start": "2652520",
    "end": "2657680"
  },
  {
    "text": "into a beta. So here, the beta that we need\nto initialize is a beta with 30",
    "start": "2657680",
    "end": "2663260"
  },
  {
    "text": "plus 1 and 20 plus 1. Plus 1 because we always\nwant to initiate-- sorry, initialize beta\nwith an extra one,",
    "start": "2663260",
    "end": "2671240"
  },
  {
    "text": "as per normal beta\ninitialization. Then we say, OK, they want they\nwant something of the form M.",
    "start": "2671240",
    "end": "2678829"
  },
  {
    "text": "So we know that the M here\nis the expectation of X. So we use the second\npoint here for finding",
    "start": "2678830",
    "end": "2684680"
  },
  {
    "text": "what the expectation\nof what the beta is. OK?",
    "start": "2684680",
    "end": "2689980"
  },
  {
    "text": "So now, how do we find X? We're given that it\nhas to be accurate",
    "start": "2689980",
    "end": "2696400"
  },
  {
    "text": "up to two decimal\nplaces, and it has to be within this\nparticular range, and it has to be greater\nthan 0.95, right?",
    "start": "2696400",
    "end": "2703270"
  },
  {
    "text": "So what do we do that\nis if in a for loop-- OK, so here's what I'm supposed\nto say, just increment it",
    "start": "2703270",
    "end": "2708670"
  },
  {
    "text": "slowly. The way we do it is\nhere's a while True. So in the while True--",
    "start": "2708670",
    "end": "2715690"
  },
  {
    "text": "here's the loop that actually\ndoes a grid search approach. We want to say that OK, X has\nto be greater than the lower",
    "start": "2715690",
    "end": "2722560"
  },
  {
    "text": "bound, which is just\nthe mean minus X, but it has to be less than\nthe upper bound, which",
    "start": "2722560",
    "end": "2727570"
  },
  {
    "text": "is mean plus X. And then the\nprobability of that region is the CDF of whatever\nthe upper side is,",
    "start": "2727570",
    "end": "2733960"
  },
  {
    "text": "minus the CDF of whatever\nthe lower side is. And if that's greater than\n95%, which is our target,",
    "start": "2733960",
    "end": "2740619"
  },
  {
    "text": "then we break it. Otherwise, we continue\non with our grid search.",
    "start": "2740620",
    "end": "2746040"
  },
  {
    "text": "And then just print it out. Then just print out the values. Does this make sense?",
    "start": "2746040",
    "end": "2752320"
  },
  {
    "text": "I didn't go too fast?  I see confused faces. I see frowns.",
    "start": "2752320",
    "end": "2758670"
  },
  {
    "text": "Yes? [INAUDIBLE] That X [INAUDIBLE] 0 point,\nis that like [INAUDIBLE]??",
    "start": "2758670",
    "end": "2768680"
  },
  {
    "text": "Oh, this is a-- so we're trying\nto find X, right? ",
    "start": "2768680",
    "end": "2774890"
  },
  {
    "text": "The way that we find it\nis by doing a grid search. [INAUDIBLE]",
    "start": "2774890",
    "end": "2780690"
  },
  {
    "text": "I guess, yeah. I just [INAUDIBLE] Oh, sorry. It's not-- OK, this is\na bad naming convention.",
    "start": "2780690",
    "end": "2786490"
  },
  {
    "text": "We called it X.\nBut it's actually B. We're looking for the\nsmallest value B here.",
    "start": "2786490",
    "end": "2793569"
  },
  {
    "text": "That's why we have B's here. Like B minus X,\nthat's B's over here. Yeah. OK?",
    "start": "2793570",
    "end": "2798970"
  },
  {
    "text": "maybe I'll go edit the\nsolutions and actually change it to B, because otherwise it's\na little bit misleading.",
    "start": "2798970",
    "end": "2805150"
  },
  {
    "text": "Yeah. Yeah, because like we can't\nreally [INAUDIBLE] on that. Yes? How do you know that it's\na grid search approach,",
    "start": "2805150",
    "end": "2812440"
  },
  {
    "text": "and would there be\nanother way to solve it if you didn't get to that? There probably-- there most\nlikely is another way to do it,",
    "start": "2812440",
    "end": "2819610"
  },
  {
    "text": "but-- so I didn't peruse\nthrough the solutions for this particular problem. But the thing that tips it\noff that it's a grid search",
    "start": "2819610",
    "end": "2825680"
  },
  {
    "text": "is here. We want B rounded to\ntwo decimal places. So typically, if you want\nsomething rounded to something",
    "start": "2825680",
    "end": "2832150"
  },
  {
    "text": "places in a code,\nand it's approximate, it usually means I want to\nincrement it slowly somehow.",
    "start": "2832150",
    "end": "2839994"
  },
  {
    "text": " That's typically like research. Like, OK, I have a\nboundary on the left, I have a boundary\non the right, I'm",
    "start": "2839995",
    "end": "2846290"
  },
  {
    "text": "going to just increment\nit until it's done.  Sometimes we'll even\nprovide a range.",
    "start": "2846290",
    "end": "2852152"
  },
  {
    "text": "Here, we didn't actually\nprovide a range. We just said, OK,\nwhile true just find a region of B that will\nactually just satisfy this,",
    "start": "2852152",
    "end": "2858500"
  },
  {
    "text": "and we're going to pretend\nthat we're guaranteed that this will terminate.",
    "start": "2858500",
    "end": "2863780"
  },
  {
    "text": "In real life, this whole\nwhile true loop might actually",
    "start": "2863780",
    "end": "2868907"
  },
  {
    "text": "never break, because\nthe probability could-- it's possible that a probability\nis never like-- well,",
    "start": "2868907",
    "end": "2873930"
  },
  {
    "text": "actually no. It won't break because\neventually, this will become-- eventually,\nx will become so big that it becomes one.",
    "start": "2873930",
    "end": "2879830"
  },
  {
    "text": "Yeah. OK? [INAUDIBLE] kind of\nlike [INAUDIBLE]??",
    "start": "2879830",
    "end": "2885500"
  },
  {
    "text": "Yes. We're actually moving\nthe range this way. [INAUDIBLE] Yeah, we're expanding the range.",
    "start": "2885500",
    "end": "2892272"
  },
  {
    "text": "Oh OK. Yeah. So at some point-- the thing is, we're after\nlike the smallest value of B.",
    "start": "2892272",
    "end": "2898960"
  },
  {
    "text": "Then at some point,\nwe're going to get B so big it's going to be one. But then we don't want that\nbig one, the smallest one, such as greater than 0.95.",
    "start": "2898960",
    "end": "2906040"
  },
  {
    "text": "Yep? OK, yeah, we want\ntightness of lower bound. ",
    "start": "2906040",
    "end": "2914970"
  },
  {
    "text": "OK? I think-- OK, so I think\nthat was the last question. That was the last\nexample that I had.",
    "start": "2914970",
    "end": "2922770"
  },
  {
    "text": "Yeah, so I saved like the\nlast like 15 or 20 minutes for just general questions\nabout the final exam,",
    "start": "2922770",
    "end": "2929820"
  },
  {
    "text": "or for parts of 109. We can also revisit\nthe slides if people have questions about that. But other than that, yeah.",
    "start": "2929820",
    "end": "2937990"
  },
  {
    "text": "That's all the material I had. Yes? Yes, so on one of the\npractice finals that had the question about\ndeep learning networks,",
    "start": "2937990",
    "end": "2945299"
  },
  {
    "text": "and some math about that,\nbut then on the final, like, on the website,\nit says, you're",
    "start": "2945300",
    "end": "2952440"
  },
  {
    "text": "not going to be expected\nto do deep neural networks. So-- Yeah. Is there just not going to be\nany deep neural networks, only",
    "start": "2952440",
    "end": "2959130"
  },
  {
    "text": "up to logistic regression, or? Even if he gives you the--",
    "start": "2959130",
    "end": "2964730"
  },
  {
    "text": "we'll make a kind of--\nwe'll make it reasonable. So we're not going\nto be expecting you to take vector gradients\nor anything like that.",
    "start": "2964730",
    "end": "2969925"
  },
  {
    "text": " We'll make it so that\nit's doable with the stuff that we've covered.",
    "start": "2969925",
    "end": "2975430"
  },
  {
    "text": "In a problem like that\nparticular final, what likely happen was, so in past\niterations of 109, we used to have derivatives\nwhere we could--",
    "start": "2975430",
    "end": "2983890"
  },
  {
    "text": "people have, it's actually\npossible to be like, find the gradient of some of\nthe log likelihood, or do, like,",
    "start": "2983890",
    "end": "2992320"
  },
  {
    "text": "what's the partial log\nlikelihood with respect to some array of A's. That's probably going to be\nsomething like, I don't know,",
    "start": "2992320",
    "end": "2998170"
  },
  {
    "text": "X transpose something, right? We never did that in class. We're not, don't worry about it.",
    "start": "2998170",
    "end": "3003180"
  },
  {
    "text": "We will not give you\nthis in the exam. If you have a neural\nnetwork with multiple nodes,",
    "start": "3003180",
    "end": "3010940"
  },
  {
    "text": "it will probably be solvable\njust using partial derivatives on each 1D elements of\nwhatever small network we give.",
    "start": "3010940",
    "end": "3020010"
  },
  {
    "text": "OK? ",
    "start": "3020010",
    "end": "3025330"
  },
  {
    "text": "Yeah? All right. And if any other questions. If not, we can-- we can have a 20\nor so minutes back,",
    "start": "3025330",
    "end": "3032760"
  },
  {
    "text": "and good luck on the final. [APPLAUSE]",
    "start": "3032760",
    "end": "3037850"
  },
  {
    "start": "3037850",
    "end": "3042000"
  }
]