[
  {
    "start": "0",
    "end": "26000"
  },
  {
    "start": "0",
    "end": "5570"
  },
  {
    "text": "Hello.",
    "start": "5570",
    "end": "6090"
  },
  {
    "text": "In this module, I'm\ngoing to first show you",
    "start": "6090",
    "end": "7882"
  },
  {
    "text": "how minimizing the average\nerror on your training examples",
    "start": "7882",
    "end": "12170"
  },
  {
    "text": "can actually lead to\ndisparities between--",
    "start": "12170",
    "end": "14670"
  },
  {
    "text": "in performance between groups.",
    "start": "14670",
    "end": "16530"
  },
  {
    "text": "And then I'm going to show\nyou a simple approach called",
    "start": "16530",
    "end": "18950"
  },
  {
    "text": "group distribution and robust\noptimization that can mitigate",
    "start": "18950",
    "end": "22490"
  },
  {
    "text": "some of these disparities.",
    "start": "22490",
    "end": "25140"
  },
  {
    "text": "So let me begin with a very\nfamous example of disparities",
    "start": "25140",
    "end": "29689"
  },
  {
    "start": "26000",
    "end": "26000"
  },
  {
    "text": "or inequalities in\nmachine learning,",
    "start": "29690",
    "end": "31190"
  },
  {
    "text": "it's called the\nGender Shades project.",
    "start": "31190",
    "end": "33230"
  },
  {
    "text": "In this project, the\nauthors collected a data set",
    "start": "33230",
    "end": "36350"
  },
  {
    "text": "of images of faces of different\ngenders and different skin",
    "start": "36350",
    "end": "41839"
  },
  {
    "text": "tones.",
    "start": "41840",
    "end": "42830"
  },
  {
    "text": "And then, they evaluated\na gender classifier from",
    "start": "42830",
    "end": "46340"
  },
  {
    "text": "Microsoft, Face++, and IBM.",
    "start": "46340",
    "end": "49490"
  },
  {
    "text": "What they found was\nrather striking.",
    "start": "49490",
    "end": "51500"
  },
  {
    "text": "So for a group of\nlighter skinned males,",
    "start": "51500",
    "end": "55550"
  },
  {
    "text": "the classifier was\nalmost perfect.",
    "start": "55550",
    "end": "58820"
  },
  {
    "text": "But if you look at the\nperformance of this classifiers",
    "start": "58820",
    "end": "62870"
  },
  {
    "text": "on darker skinned\nfemales, you'll",
    "start": "62870",
    "end": "65030"
  },
  {
    "text": "see that the accuracies\nare much, much worse.",
    "start": "65030",
    "end": "68610"
  },
  {
    "text": "So this is a general problem\nin machine learning, which",
    "start": "68610",
    "end": "72260"
  },
  {
    "text": "is that inequalities\nbetween different groups",
    "start": "72260",
    "end": "74960"
  },
  {
    "text": "arise because machine\nlearning is generally where",
    "start": "74960",
    "end": "79159"
  },
  {
    "text": "you minimize the average loss.",
    "start": "79160",
    "end": "83550"
  },
  {
    "start": "83000",
    "end": "83000"
  },
  {
    "text": "So, these inequalities can\nhave real world consequences.",
    "start": "83550",
    "end": "89700"
  },
  {
    "text": "So in this vivid\ncase, a Black man",
    "start": "89700",
    "end": "92240"
  },
  {
    "text": "was wrongly arrested due\nto an incorrect match",
    "start": "92240",
    "end": "95720"
  },
  {
    "text": "with another Black man captured\nfrom a surveillance video.",
    "start": "95720",
    "end": "98450"
  },
  {
    "text": "And this mistake was\ndue to a mistake made",
    "start": "98450",
    "end": "101180"
  },
  {
    "text": "by a facial recognition system.",
    "start": "101180",
    "end": "104090"
  },
  {
    "text": "So given what we just saw on\nthe Gender Shades project,",
    "start": "104090",
    "end": "107990"
  },
  {
    "text": "we can see that lower\naccuracies for some groups",
    "start": "107990",
    "end": "110439"
  },
  {
    "text": "might lead to more\nfalse arrests, which",
    "start": "110440",
    "end": "112900"
  },
  {
    "text": "adds to already problematic\ninequalities that",
    "start": "112900",
    "end": "116320"
  },
  {
    "text": "exist in our society today.",
    "start": "116320",
    "end": "118790"
  },
  {
    "text": "So in this module,\nI'm going to focus",
    "start": "118790",
    "end": "120460"
  },
  {
    "text": "on this issue of performance\ndisparities between groups",
    "start": "120460",
    "end": "124120"
  },
  {
    "text": "and how we might be\nable to mitigate them.",
    "start": "124120",
    "end": "126890"
  },
  {
    "text": "But I also want to highlight\nthat even if we didn't have",
    "start": "126890",
    "end": "130419"
  },
  {
    "text": "any disparities\nbetween groups, there's",
    "start": "130419",
    "end": "132820"
  },
  {
    "text": "a question of whether facial\nrecognition technology should",
    "start": "132820",
    "end": "135760"
  },
  {
    "text": "be used in law enforcement or\nin surveillance or anything",
    "start": "135760",
    "end": "138939"
  },
  {
    "text": "at all.",
    "start": "138940",
    "end": "139810"
  },
  {
    "text": "And these are big thorny\nethical questions,",
    "start": "139810",
    "end": "142940"
  },
  {
    "text": "which we're not going\nto unfortunately",
    "start": "142940",
    "end": "144670"
  },
  {
    "text": "be able to spend much\ntime with in this module.",
    "start": "144670",
    "end": "147040"
  },
  {
    "text": "But I just want to\nhighlight that it's",
    "start": "147040",
    "end": "149110"
  },
  {
    "text": "important to remember that\nsometimes the issue is",
    "start": "149110",
    "end": "152050"
  },
  {
    "text": "not with the solution but in the\nframing of the problem itself.",
    "start": "152050",
    "end": "155185"
  },
  {
    "start": "155185",
    "end": "157910"
  },
  {
    "text": "So Gender Shades was an\nexample of classification.",
    "start": "157910",
    "end": "161020"
  },
  {
    "start": "158000",
    "end": "158000"
  },
  {
    "text": "But to make things simpler,\nlet us consider our friend",
    "start": "161020",
    "end": "164560"
  },
  {
    "text": "linear regression.",
    "start": "164560",
    "end": "166040"
  },
  {
    "text": "So recall in linear\nregression, we",
    "start": "166040",
    "end": "167920"
  },
  {
    "text": "start with a training set,\nwhich consists of examples.",
    "start": "167920",
    "end": "171970"
  },
  {
    "text": "Each example has an\ninput x and output y.",
    "start": "171970",
    "end": "176440"
  },
  {
    "text": "But in our case, we're going\nto assume each example is also",
    "start": "176440",
    "end": "179560"
  },
  {
    "text": "annotated with a group g.",
    "start": "179560",
    "end": "182560"
  },
  {
    "text": "So we're going to have a set\nof-- let's plot this over here.",
    "start": "182560",
    "end": "186739"
  },
  {
    "text": "So here's 1, 4.",
    "start": "186740",
    "end": "188590"
  },
  {
    "text": "And here's a second example,\n2, 8, which is up here.",
    "start": "188590",
    "end": "193540"
  },
  {
    "text": "And then these\nexamples down here",
    "start": "193540",
    "end": "196000"
  },
  {
    "text": "are going to come\nfrom group B. So we're",
    "start": "196000",
    "end": "198640"
  },
  {
    "text": "going to have two\ngroups, A and B,",
    "start": "198640",
    "end": "200740"
  },
  {
    "text": "and here they are over here, OK?",
    "start": "200740",
    "end": "204400"
  },
  {
    "text": "So the goal of machine\nlearning or linear regression,",
    "start": "204400",
    "end": "207310"
  },
  {
    "text": "in particular, is to produce\na predictor such as this one.",
    "start": "207310",
    "end": "213220"
  },
  {
    "text": "And the predictor is going\nto take new inputs such as 3",
    "start": "213220",
    "end": "218020"
  },
  {
    "text": "and produce an\noutput, such as 3.27.",
    "start": "218020",
    "end": "223070"
  },
  {
    "text": "So in linear\nregression, we assume",
    "start": "223070",
    "end": "226760"
  },
  {
    "text": "that the predictor has the form\na weight vector, dot, a feature",
    "start": "226760",
    "end": "233180"
  },
  {
    "text": "vector, phi of x.",
    "start": "233180",
    "end": "234944"
  },
  {
    "text": "In this simple example, we're\ngoing to restrict ourselves",
    "start": "234945",
    "end": "237320"
  },
  {
    "text": "to the case where the feature\nvector is simply the identity",
    "start": "237320",
    "end": "240830"
  },
  {
    "text": "map-- just x--",
    "start": "240830",
    "end": "242550"
  },
  {
    "text": "which gives us a\nhypothesis class,",
    "start": "242550",
    "end": "244980"
  },
  {
    "text": "which is the set of all lines\nthat go through the origin.",
    "start": "244980",
    "end": "249150"
  },
  {
    "text": "So you can think about sweeping\nlines through the origin here.",
    "start": "249150",
    "end": "253519"
  },
  {
    "text": "And the weight vector is just\ngoing to be a single number, w.",
    "start": "253520",
    "end": "259220"
  },
  {
    "text": "So already you can\nsee some tension here.",
    "start": "259220",
    "end": "262310"
  },
  {
    "text": "So which weight vector\nwould you choose?",
    "start": "262310",
    "end": "265110"
  },
  {
    "text": "Would you choose\none that's closer",
    "start": "265110",
    "end": "266750"
  },
  {
    "text": "to these points in\ngroup B or in group A?",
    "start": "266750",
    "end": "271925"
  },
  {
    "text": "And this tension means that\nwe have to compromise somehow.",
    "start": "271925",
    "end": "274940"
  },
  {
    "text": "And exactly how we\ncompromise is going",
    "start": "274940",
    "end": "277070"
  },
  {
    "text": "to have some implications.",
    "start": "277070",
    "end": "281130"
  },
  {
    "text": "So notice also that\nthe predictor doesn't",
    "start": "281130",
    "end": "284750"
  },
  {
    "text": "use group information, it just\ntakes an input x as before.",
    "start": "284750",
    "end": "288560"
  },
  {
    "text": "What's going to use\ngroup information",
    "start": "288560",
    "end": "290300"
  },
  {
    "text": "is the learning algorithm, and\nwe'll get to that a little bit",
    "start": "290300",
    "end": "292849"
  },
  {
    "text": "later.",
    "start": "292850",
    "end": "293350"
  },
  {
    "start": "293350",
    "end": "295950"
  },
  {
    "text": "So just as a review,\nfor linear regression,",
    "start": "295950",
    "end": "299880"
  },
  {
    "text": "we define the loss function of\nan input x y and a particular",
    "start": "299880",
    "end": "304490"
  },
  {
    "text": "weight vector to be simply\nthe difference between",
    "start": "304490",
    "end": "307039"
  },
  {
    "text": "the predicted value of that\nclassifier-- or sorry--",
    "start": "307040",
    "end": "310130"
  },
  {
    "text": "progressor f of w and the\ntarget value of y squared.",
    "start": "310130",
    "end": "318120"
  },
  {
    "text": "And remember that we\ndefined the training",
    "start": "318120",
    "end": "321169"
  },
  {
    "text": "loss of a particular\nweight vector as follows.",
    "start": "321170",
    "end": "324180"
  },
  {
    "text": "It's going to be the average.",
    "start": "324180",
    "end": "326280"
  },
  {
    "text": "So 1 over a number of\ntraining examples over sum",
    "start": "326280",
    "end": "328790"
  },
  {
    "text": "of the training examples\nof the per example loss.",
    "start": "328790",
    "end": "332480"
  },
  {
    "text": "So visually, we can see\nthis on this plot where",
    "start": "332480",
    "end": "336260"
  },
  {
    "text": "for each value of w--",
    "start": "336260",
    "end": "339080"
  },
  {
    "text": "in our case here,\nremember w is a scalar",
    "start": "339080",
    "end": "341449"
  },
  {
    "text": "for this particular example--",
    "start": "341450",
    "end": "342930"
  },
  {
    "text": "we get a loss value.",
    "start": "342930",
    "end": "344850"
  },
  {
    "text": "So this is the training loss,\nwhich is this curve here.",
    "start": "344850",
    "end": "351410"
  },
  {
    "text": "And what we can do\nis let's practice",
    "start": "351410",
    "end": "354910"
  },
  {
    "text": "evaluating this training loss\nat a particular value of w,",
    "start": "354910",
    "end": "358780"
  },
  {
    "text": "let's say 1.",
    "start": "358780",
    "end": "359860"
  },
  {
    "text": "So this is going to take the\naverage over this data set",
    "start": "359860",
    "end": "364599"
  },
  {
    "text": "and it's going to return\nsome value, 7.5, OK?",
    "start": "364600",
    "end": "369500"
  },
  {
    "text": "So the loss of--",
    "start": "369500",
    "end": "371960"
  },
  {
    "text": "the average loss at\nw equals 1 is 7.5.",
    "start": "371960",
    "end": "379660"
  },
  {
    "start": "379000",
    "end": "379000"
  },
  {
    "text": "Which seems OK.",
    "start": "379660",
    "end": "380890"
  },
  {
    "text": "But now let's remember-- let's\npeer a little bit closer at how",
    "start": "380890",
    "end": "385780"
  },
  {
    "text": "the loss is spread\nacross groups.",
    "start": "385780",
    "end": "388580"
  },
  {
    "text": "So we're going to define a\nnotion of a per-group loss.",
    "start": "388580",
    "end": "392129"
  },
  {
    "text": "So here's our training set.",
    "start": "392130",
    "end": "393590"
  },
  {
    "text": "So for group A,\nwhat is this loss?",
    "start": "393590",
    "end": "395650"
  },
  {
    "text": "In group B, group B\nwhat is this loss?",
    "start": "395650",
    "end": "397540"
  },
  {
    "text": "So formally, we're\ngoing to define",
    "start": "397540",
    "end": "399430"
  },
  {
    "text": "the per-group loss, writting\ntrain loss sub g for group g.",
    "start": "399430",
    "end": "403870"
  },
  {
    "text": "g can be either A or B. To\nbe the average now over only",
    "start": "403870",
    "end": "409000"
  },
  {
    "text": "those examples in that group.",
    "start": "409000",
    "end": "410270"
  },
  {
    "text": "So this notation D train of g is\nthe set of examples in Group g",
    "start": "410270",
    "end": "417384"
  },
  {
    "text": "and of the example\nloss again, OK?",
    "start": "417385",
    "end": "420920"
  },
  {
    "text": "So we're going to plot these\ntwo losses on this curve here.",
    "start": "420920",
    "end": "426360"
  },
  {
    "text": "And we see that we\nhave these two plots.",
    "start": "426360",
    "end": "432259"
  },
  {
    "text": "So train loss A it looks\nlike this and train loss B",
    "start": "432260",
    "end": "437450"
  },
  {
    "text": "looks like that.",
    "start": "437450",
    "end": "439580"
  },
  {
    "text": "And we can practice evaluating\nthese different loss functions",
    "start": "439580",
    "end": "443419"
  },
  {
    "text": "at our example\nweight vector 1 here.",
    "start": "443420",
    "end": "447350"
  },
  {
    "text": "So train loss A is going to\nbe an average-- remember,",
    "start": "447350",
    "end": "450020"
  },
  {
    "text": "only over the\nexamples in group A--",
    "start": "450020",
    "end": "453379"
  },
  {
    "text": "and that's going\nto give us 22.5.",
    "start": "453380",
    "end": "456560"
  },
  {
    "text": "You can see it looks\nlike about 22.5 here.",
    "start": "456560",
    "end": "461030"
  },
  {
    "text": "And then what about B?",
    "start": "461030",
    "end": "462830"
  },
  {
    "text": "So B actually gets\na loss of 0, which",
    "start": "462830",
    "end": "466639"
  },
  {
    "text": "you can see at this point.",
    "start": "466640",
    "end": "468510"
  },
  {
    "text": "So you can see that we have\na single weight vector 1.",
    "start": "468510",
    "end": "474200"
  },
  {
    "text": "Gets very different\nlosses on the two data",
    "start": "474200",
    "end": "477740"
  },
  {
    "text": "sets, on the two groups.",
    "start": "477740",
    "end": "480020"
  },
  {
    "text": "A is doing a lot\nworse, it has 22.5,",
    "start": "480020",
    "end": "483110"
  },
  {
    "text": "and B is doing\nmuch better, it has",
    "start": "483110",
    "end": "485990"
  },
  {
    "text": "a 0, which is the minimum\nloss you can hope for.",
    "start": "485990",
    "end": "489509"
  },
  {
    "text": "So this is an example\nof a disparity between--",
    "start": "489510",
    "end": "492680"
  },
  {
    "text": "if we were to choose\nweight vector 1,",
    "start": "492680",
    "end": "494468"
  },
  {
    "text": "there would be a huge disparity\nbetween the performance",
    "start": "494468",
    "end": "496759"
  },
  {
    "text": "on these two groups.",
    "start": "496760",
    "end": "498170"
  },
  {
    "start": "498170",
    "end": "502080"
  },
  {
    "text": "So we can look at the\nlosses of different groups,",
    "start": "502080",
    "end": "504419"
  },
  {
    "text": "but it will be helpful\nto kind of summarize",
    "start": "504420",
    "end": "506340"
  },
  {
    "text": "that as a single number.",
    "start": "506340",
    "end": "508120"
  },
  {
    "text": "Now we're going to capture\nit by a quantity called",
    "start": "508120",
    "end": "511169"
  },
  {
    "text": "the maximum group loss.",
    "start": "511170",
    "end": "512789"
  },
  {
    "text": "And you might\nguess from the name",
    "start": "512789",
    "end": "515190"
  },
  {
    "text": "that the maximum group\nloss, written TrainLossMax",
    "start": "515190",
    "end": "519419"
  },
  {
    "text": "is simply just going to be\nthe maximum over all groups",
    "start": "519419",
    "end": "524910"
  },
  {
    "text": "of the per-group loss.",
    "start": "524910",
    "end": "527470"
  },
  {
    "text": "And so visually what this\nlooks like is as follows.",
    "start": "527470",
    "end": "531430"
  },
  {
    "text": "So let me-- so remember\nwe had in yellow here",
    "start": "531430",
    "end": "538560"
  },
  {
    "text": "or orange the loss of\ngroup A and in blue, we",
    "start": "538560",
    "end": "543690"
  },
  {
    "text": "have the loss of group\nB, and the maximum group",
    "start": "543690",
    "end": "547320"
  },
  {
    "text": "loss is this function of w,\nas the other functions, which",
    "start": "547320",
    "end": "551400"
  },
  {
    "text": "is going to be the\npoint wise maximum.",
    "start": "551400",
    "end": "553260"
  },
  {
    "text": "So at every point, we\nchoose either the value",
    "start": "553260",
    "end": "556830"
  },
  {
    "text": "of loss of A or B that's\ngoing to be larger.",
    "start": "556830",
    "end": "562030"
  },
  {
    "text": "So as you can see, it traces\nout this kind of upper envelope",
    "start": "562030",
    "end": "564930"
  },
  {
    "text": "here.",
    "start": "564930",
    "end": "566250"
  },
  {
    "text": "Over here, the loss\nof A is higher,",
    "start": "566250",
    "end": "570480"
  },
  {
    "text": "so it's going to track that.",
    "start": "570480",
    "end": "571720"
  },
  {
    "text": "And over here the\nloss of B is higher so",
    "start": "571720",
    "end": "573386"
  },
  {
    "text": "it kind of hugs B from there on.",
    "start": "573387",
    "end": "578570"
  },
  {
    "text": "OK.",
    "start": "578570",
    "end": "579070"
  },
  {
    "text": "So let's evaluate at\na point w equals 1.",
    "start": "579070",
    "end": "583900"
  },
  {
    "text": "We see that-- remember\nfrom the previous slide",
    "start": "583900",
    "end": "586750"
  },
  {
    "text": "that the two losses are 22.5\nand 0 for the two groups.",
    "start": "586750",
    "end": "594550"
  },
  {
    "text": "And then to compute\nthe maximum, we",
    "start": "594550",
    "end": "597339"
  },
  {
    "text": "just take the max of these\ntwo values and you get 22.5.",
    "start": "597340",
    "end": "600440"
  },
  {
    "text": "So 22.5 is a single\nnumber that summarizes",
    "start": "600440",
    "end": "603490"
  },
  {
    "text": "how bad is the worst group off.",
    "start": "603490",
    "end": "606910"
  },
  {
    "text": "The max was the\nmaximum group loss.",
    "start": "606910",
    "end": "610449"
  },
  {
    "text": "And if you compare the\nmaximum group loss, 22.5,",
    "start": "610450",
    "end": "613440"
  },
  {
    "text": "with the average\nloss, which is 7.5,",
    "start": "613440",
    "end": "616680"
  },
  {
    "text": "you see that the maximum\ngroup loss is larger--",
    "start": "616680",
    "end": "619649"
  },
  {
    "text": "and it's always larger.",
    "start": "619650",
    "end": "620640"
  },
  {
    "start": "620640",
    "end": "624310"
  },
  {
    "start": "623000",
    "end": "623000"
  },
  {
    "text": "So now let's compare\nthese two loss functions.",
    "start": "624310",
    "end": "628200"
  },
  {
    "text": "We have the average loss\nand the maximum group loss.",
    "start": "628200",
    "end": "632590"
  },
  {
    "text": "So we can plot\nboth of these here.",
    "start": "632590",
    "end": "635820"
  },
  {
    "text": "And what-- pictorially, we\ncan see what's going on.",
    "start": "635820",
    "end": "644650"
  },
  {
    "text": "So here, let me just\nplot our data points",
    "start": "644650",
    "end": "648900"
  },
  {
    "text": "just so we have them available.",
    "start": "648900",
    "end": "652900"
  },
  {
    "text": "So these functions are\ndefinitely very different.",
    "start": "652900",
    "end": "655590"
  },
  {
    "text": "OK.",
    "start": "655590",
    "end": "656090"
  },
  {
    "text": "So what happens now when we try\nto minimize the average loss",
    "start": "656090",
    "end": "660290"
  },
  {
    "text": "versus the maximum group loss?",
    "start": "660290",
    "end": "662399"
  },
  {
    "text": "So let's start with\nminimizing the average loss.",
    "start": "662400",
    "end": "664490"
  },
  {
    "text": "So this is standard learning.",
    "start": "664490",
    "end": "666560"
  },
  {
    "text": "This is a status quo.",
    "start": "666560",
    "end": "667880"
  },
  {
    "text": "You find the minimum\nof the average loss,",
    "start": "667880",
    "end": "670590"
  },
  {
    "text": "which is going to be this\npoint, w equals 1.09,",
    "start": "670590",
    "end": "676010"
  },
  {
    "text": "it gets a loss of 7.29.",
    "start": "676010",
    "end": "678890"
  },
  {
    "text": "So it looks like we\nwere doing pretty well.",
    "start": "678890",
    "end": "680760"
  },
  {
    "text": "But if you remember, look at the\nworst group loss of that weight",
    "start": "680760",
    "end": "684580"
  },
  {
    "text": "vector, then we'll see that it's\nabove 20, which is not great.",
    "start": "684580",
    "end": "690950"
  },
  {
    "text": "So what you can do\ninstead is to do",
    "start": "690950",
    "end": "694900"
  },
  {
    "text": "what we call group\nDistributionally Robust",
    "start": "694900",
    "end": "697030"
  },
  {
    "text": "Optimization or group DRO, which\nis simply going to minimize",
    "start": "697030",
    "end": "701440"
  },
  {
    "text": "the maximum group loss.",
    "start": "701440",
    "end": "703300"
  },
  {
    "text": "So it's going to minimize\nthis purple plot here.",
    "start": "703300",
    "end": "707350"
  },
  {
    "text": "And what happens\nwhen you do that,",
    "start": "707350",
    "end": "709100"
  },
  {
    "text": "you get w equals 1.58,\nwhich gets a loss of 15.69,",
    "start": "709100",
    "end": "714430"
  },
  {
    "text": "which is better than\nthe 20 plus that you",
    "start": "714430",
    "end": "717100"
  },
  {
    "text": "would have gotten there.",
    "start": "717100",
    "end": "718240"
  },
  {
    "text": "Now of course, the\naverage loss is",
    "start": "718240",
    "end": "720580"
  },
  {
    "text": "worsened, because at this\npoint the average loss",
    "start": "720580",
    "end": "723580"
  },
  {
    "text": "on the red curve is\na little bit higher.",
    "start": "723580",
    "end": "725890"
  },
  {
    "text": "So there's a tradeoff here.",
    "start": "725890",
    "end": "728110"
  },
  {
    "text": "And we can see this\ntension kind of play",
    "start": "728110",
    "end": "730390"
  },
  {
    "text": "out over on this plot over here.",
    "start": "730390",
    "end": "733430"
  },
  {
    "text": "So here we see that if\nyou were to minimize",
    "start": "733430",
    "end": "736839"
  },
  {
    "text": "the average loss,\nwhat you would do",
    "start": "736840",
    "end": "738880"
  },
  {
    "text": "is find a regressor\nor a model that's",
    "start": "738880",
    "end": "742120"
  },
  {
    "text": "very close to the\npoints over here, B,",
    "start": "742120",
    "end": "744370"
  },
  {
    "text": "because there's four of them.",
    "start": "744370",
    "end": "745690"
  },
  {
    "text": "Kind of the majority\nclass dominates.",
    "start": "745690",
    "end": "748300"
  },
  {
    "text": "Whereas, if you minimize\nthe maximum group loss,",
    "start": "748300",
    "end": "751399"
  },
  {
    "text": "then you're going to get\nthis purple line, which",
    "start": "751400",
    "end": "753400"
  },
  {
    "text": "is going to be able to balance\nout the two groups no matter",
    "start": "753400",
    "end": "758080"
  },
  {
    "text": "how many points are over\nhere versus over here.",
    "start": "758080",
    "end": "760775"
  },
  {
    "text": "So you can think\nabout this purple line",
    "start": "760775",
    "end": "762400"
  },
  {
    "text": "is more fair because it\ntreats groups more equally.",
    "start": "762400",
    "end": "765700"
  },
  {
    "start": "765700",
    "end": "769590"
  },
  {
    "text": "So how do we minimize\nthe maximum group loss?",
    "start": "769590",
    "end": "775200"
  },
  {
    "start": "770000",
    "end": "770000"
  },
  {
    "text": "So as before, we're going\nto use gradient descent",
    "start": "775200",
    "end": "778350"
  },
  {
    "text": "and follow our nose.",
    "start": "778350",
    "end": "780029"
  },
  {
    "text": "So what this looks like--\nlet me just try to plot this.",
    "start": "780030",
    "end": "783930"
  },
  {
    "start": "783930",
    "end": "786880"
  },
  {
    "text": "So here's the\nobjective function.",
    "start": "786880",
    "end": "788520"
  },
  {
    "text": "The maximum group loss\nis train loss max,",
    "start": "788520",
    "end": "792870"
  },
  {
    "text": "is remember, the maximum\nover all the groups",
    "start": "792870",
    "end": "796680"
  },
  {
    "text": "of the per-group training loss.",
    "start": "796680",
    "end": "801640"
  },
  {
    "text": "And so how do you take\nthe gradient of a max?",
    "start": "801640",
    "end": "807180"
  },
  {
    "text": "Well, the gradient\nof a max, remember,",
    "start": "807180",
    "end": "809860"
  },
  {
    "text": "is equal to the\ngradient of the function",
    "start": "809860",
    "end": "813899"
  },
  {
    "text": "where we're evaluating at the\nparticular value of g star.",
    "start": "813900",
    "end": "819780"
  },
  {
    "text": "And what is g star?",
    "start": "819780",
    "end": "820920"
  },
  {
    "text": "g star is the arg max\nover the training loss.",
    "start": "820920",
    "end": "827760"
  },
  {
    "text": "So let's look at this picture.",
    "start": "827760",
    "end": "829397"
  },
  {
    "text": "So basically what are you doing?",
    "start": "829397",
    "end": "830730"
  },
  {
    "text": "So we want to diff--",
    "start": "830730",
    "end": "832139"
  },
  {
    "text": "take the gradient of\nthis purple curve, right?",
    "start": "832140",
    "end": "835110"
  },
  {
    "text": "And so if you're over here, the\ngradient of the purple curve",
    "start": "835110",
    "end": "839070"
  },
  {
    "text": "is exactly the gradient\nof the loss on group A.",
    "start": "839070",
    "end": "844980"
  },
  {
    "text": "And if you're over here, the\ngradient of the maximum group",
    "start": "844980",
    "end": "850649"
  },
  {
    "text": "loss is exactly the\ngradient of group B.",
    "start": "850650",
    "end": "855660"
  },
  {
    "text": "And it exactly corresponds\nto g star is A over here",
    "start": "855660",
    "end": "859920"
  },
  {
    "text": "because group A is worse,\nand g star is B over here",
    "start": "859920",
    "end": "865260"
  },
  {
    "text": "because group B is worse.",
    "start": "865260",
    "end": "868200"
  },
  {
    "text": "So to compute the gradient, it's\nactually kind of very simple.",
    "start": "868200",
    "end": "873000"
  },
  {
    "text": "You first just evaluate at\nyour current weight vector",
    "start": "873000",
    "end": "879120"
  },
  {
    "text": "what are the losses of\nthe different groups.",
    "start": "879120",
    "end": "882070"
  },
  {
    "text": "And you look at the group\nthat is hurting the most, has",
    "start": "882070",
    "end": "885810"
  },
  {
    "text": "the highest loss, and then you\njust update on those examples.",
    "start": "885810",
    "end": "890500"
  },
  {
    "text": "So it's a very\nintuitive process.",
    "start": "890500",
    "end": "891900"
  },
  {
    "text": "You find which group needs\nthe most amount of help",
    "start": "891900",
    "end": "894300"
  },
  {
    "text": "and then you only update your\nparameters based on that group.",
    "start": "894300",
    "end": "899910"
  },
  {
    "text": "So one note is\nthat it's important",
    "start": "899910",
    "end": "901819"
  },
  {
    "text": "that we're talking about\ngradient descent not",
    "start": "901820",
    "end": "904760"
  },
  {
    "text": "stochastic gradient descent.",
    "start": "904760",
    "end": "906470"
  },
  {
    "text": "Because stochastic gradient\ndescent relies on the objective",
    "start": "906470",
    "end": "910399"
  },
  {
    "text": "function being a sum\nover terms, but this",
    "start": "910400",
    "end": "914240"
  },
  {
    "text": "is a maximum over a sum.",
    "start": "914240",
    "end": "917760"
  },
  {
    "text": "So it exactly won't work.",
    "start": "917760",
    "end": "920810"
  },
  {
    "text": "How it exactly gets the casting\nmethods to work properly",
    "start": "920810",
    "end": "923779"
  },
  {
    "text": "is beyond the scope\nof this module,",
    "start": "923780",
    "end": "926150"
  },
  {
    "text": "but you can read the\nnotes for pointers",
    "start": "926150",
    "end": "930930"
  },
  {
    "start": "930000",
    "end": "930000"
  },
  {
    "text": "OK.",
    "start": "930930",
    "end": "931430"
  },
  {
    "text": "So let me summarize here.",
    "start": "931430",
    "end": "933300"
  },
  {
    "text": "So we've introduced the\nsetting where examples",
    "start": "933300",
    "end": "936589"
  },
  {
    "text": "are associated with groups.",
    "start": "936590",
    "end": "938282"
  },
  {
    "text": "We've done it for\ngroup regression,",
    "start": "938282",
    "end": "939740"
  },
  {
    "text": "but this generalizes\nfor classification",
    "start": "939740",
    "end": "941899"
  },
  {
    "text": "and other more general\nmachine learning problems.",
    "start": "941900",
    "end": "945620"
  },
  {
    "text": "We saw that we have the average\nloss and the maximum group",
    "start": "945620",
    "end": "950600"
  },
  {
    "text": "loss.",
    "start": "950600",
    "end": "951209"
  },
  {
    "text": "And these are different.",
    "start": "951210",
    "end": "952340"
  },
  {
    "text": "What is good on\naverage is not going",
    "start": "952340",
    "end": "954770"
  },
  {
    "text": "to be good for all groups.",
    "start": "954770",
    "end": "959790"
  },
  {
    "text": "And we see that there's always\na tension between the groups",
    "start": "959790",
    "end": "964110"
  },
  {
    "text": "if the groups are pulling you\nin kind of different directions.",
    "start": "964110",
    "end": "967350"
  },
  {
    "text": "And we saw that group\nDistributionally Robust",
    "start": "967350",
    "end": "969829"
  },
  {
    "text": "Optimization or group DRO is\na very simple algorithm that",
    "start": "969830",
    "end": "973280"
  },
  {
    "text": "minimizes the maximum group\nloss, the purple curve",
    "start": "973280",
    "end": "978050"
  },
  {
    "text": "over here.",
    "start": "978050",
    "end": "980180"
  },
  {
    "text": "And finally, I want to\nremark that this module has",
    "start": "980180",
    "end": "984370"
  },
  {
    "text": "kept things simple but there's\nmany, many more nuances.",
    "start": "984370",
    "end": "987279"
  },
  {
    "text": "So intersectionality is this\nprinciple or property where",
    "start": "987280",
    "end": "991390"
  },
  {
    "text": "you--",
    "start": "991390",
    "end": "992590"
  },
  {
    "text": "a group such as white\nwomen is actually",
    "start": "992590",
    "end": "996070"
  },
  {
    "text": "made out of multiple\nattributes, and these groups",
    "start": "996070",
    "end": "999190"
  },
  {
    "text": "might behave differently than\ntheir more coarse groups,",
    "start": "999190",
    "end": "1002700"
  },
  {
    "text": "the women or the\nset of white people.",
    "start": "1002700",
    "end": "1008050"
  },
  {
    "text": "And so we have to kind of take\ninto account finer gradient",
    "start": "1008050",
    "end": "1013529"
  },
  {
    "text": "groups.",
    "start": "1013530",
    "end": "1014710"
  },
  {
    "text": "There are also cases\nwhere we might not",
    "start": "1014710",
    "end": "1016928"
  },
  {
    "text": "know what the groups are.",
    "start": "1016928",
    "end": "1017970"
  },
  {
    "text": "Maybe we don't collect\ndemographic information",
    "start": "1017970",
    "end": "1020069"
  },
  {
    "text": "and we have to infer them.",
    "start": "1020070",
    "end": "1022210"
  },
  {
    "text": "There's also an issue\nwith overfitting.",
    "start": "1022210",
    "end": "1024119"
  },
  {
    "text": "We're talking only about\nthe training loss here",
    "start": "1024119",
    "end": "1026939"
  },
  {
    "text": "just for simplicity,\nbut of course",
    "start": "1026940",
    "end": "1028740"
  },
  {
    "text": "what we care about in machine\nlearning is doing well on out",
    "start": "1028740",
    "end": "1032730"
  },
  {
    "text": "of a test set, but\nwe're not talking",
    "start": "1032730",
    "end": "1036540"
  },
  {
    "text": "about this test set here.",
    "start": "1036540",
    "end": "1038079"
  },
  {
    "text": "So there's many more\nreferences in the notes.",
    "start": "1038079",
    "end": "1042119"
  },
  {
    "text": "And I hope this has\npiqued your interest",
    "start": "1042119",
    "end": "1044339"
  },
  {
    "text": "and realizing that inequalities\nshould be considered",
    "start": "1044339",
    "end": "1048449"
  },
  {
    "text": "a first class citizen when we\nthink about machine learning",
    "start": "1048450",
    "end": "1051720"
  },
  {
    "text": "methods.",
    "start": "1051720",
    "end": "1052900"
  },
  {
    "text": "So that's it.",
    "start": "1052900",
    "end": "1053520"
  },
  {
    "text": "Thank you.",
    "start": "1053520",
    "end": "1055340"
  },
  {
    "start": "1055340",
    "end": "1059000"
  }
]