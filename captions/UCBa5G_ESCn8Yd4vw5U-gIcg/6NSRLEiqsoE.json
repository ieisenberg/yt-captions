[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "start": "0",
    "end": "4258"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4258",
    "end": "5800"
  },
  {
    "text": "This is part 5 in our\nseries on contextual word",
    "start": "5800",
    "end": "7830"
  },
  {
    "text": "representations.",
    "start": "7830",
    "end": "8740"
  },
  {
    "text": "We're going to be talking\nabout the ELECTRA model.",
    "start": "8740",
    "end": "10823"
  },
  {
    "text": "ELECTRA stands for efficiently\nlearning an encoder that",
    "start": "10823",
    "end": "13139"
  },
  {
    "text": "classifies token\nreplacements accurately,",
    "start": "13140",
    "end": "15310"
  },
  {
    "text": "which is a helpfully descriptive\nbreakdown of a colorfully",
    "start": "15310",
    "end": "18030"
  },
  {
    "text": "named model.",
    "start": "18030",
    "end": "19950"
  },
  {
    "text": "Recall that I finished\nthe BERT screencast",
    "start": "19950",
    "end": "22080"
  },
  {
    "start": "20000",
    "end": "65000"
  },
  {
    "text": "by identifying some known\nlimitations of the BERT model.",
    "start": "22080",
    "end": "25320"
  },
  {
    "text": "ELECTRA is really keying\ninto 2 and 3 in that list.",
    "start": "25320",
    "end": "28150"
  },
  {
    "text": "So the second one identified\nby the BERT authors",
    "start": "28150",
    "end": "30869"
  },
  {
    "text": "is just that of\nthe MLM objective,",
    "start": "30870",
    "end": "32820"
  },
  {
    "text": "they say we're creating a\nmismatch between pre-training",
    "start": "32820",
    "end": "35730"
  },
  {
    "text": "and fine-tuning since the MASK\ntoken that we use is never",
    "start": "35730",
    "end": "39030"
  },
  {
    "text": "seen during fine-tuning.",
    "start": "39030",
    "end": "40230"
  },
  {
    "text": "So ideally for the\nmodel we fine-tune,",
    "start": "40230",
    "end": "42360"
  },
  {
    "text": "we would make no\nuse of a MASK token.",
    "start": "42360",
    "end": "45670"
  },
  {
    "text": "Devlin et al also observed\nof the MLM objective",
    "start": "45670",
    "end": "48730"
  },
  {
    "text": "that it has a downside.",
    "start": "48730",
    "end": "50410"
  },
  {
    "text": "We make predictions\nabout only 15%",
    "start": "50410",
    "end": "53350"
  },
  {
    "text": "of the tokens in each batch.",
    "start": "53350",
    "end": "55745"
  },
  {
    "text": "We have an intuition that\nthat's a pretty inefficient use",
    "start": "55745",
    "end": "58120"
  },
  {
    "text": "of the data we have\navailable to us.",
    "start": "58120",
    "end": "59649"
  },
  {
    "text": "Ideally, we would\nmake more predictions,",
    "start": "59650",
    "end": "61510"
  },
  {
    "text": "and ELECTRA seeks to make good\non that intuition as well.",
    "start": "61510",
    "end": "65417"
  },
  {
    "start": "65000",
    "end": "154000"
  },
  {
    "text": "So let's dive into the\ncore model structure.",
    "start": "65417",
    "end": "67250"
  },
  {
    "text": "Here we'll use a simple example.",
    "start": "67250",
    "end": "68700"
  },
  {
    "text": "We have an input\ntoken sequence x.",
    "start": "68700",
    "end": "70969"
  },
  {
    "text": "The chef cooked the meal.",
    "start": "70970",
    "end": "72860"
  },
  {
    "text": "And as usual with BERT, we can\nMASK out some of those tokens,",
    "start": "72860",
    "end": "76790"
  },
  {
    "text": "and then have a BERT\nor BERT-like model",
    "start": "76790",
    "end": "79070"
  },
  {
    "text": "try to reconstruct\nthose MASK tokens.",
    "start": "79070",
    "end": "81080"
  },
  {
    "text": "However, we're going to\ndo that with a twist.",
    "start": "81080",
    "end": "83360"
  },
  {
    "text": "Instead of always trying to\nlearn the actual input token,",
    "start": "83360",
    "end": "86900"
  },
  {
    "text": "we're going to sample tokens\nproportional to the generator",
    "start": "86900",
    "end": "90470"
  },
  {
    "text": "probabilities so that sometimes\nthe actual token will be input",
    "start": "90470",
    "end": "94460"
  },
  {
    "text": "as with the case\nwith \"the\" here.",
    "start": "94460",
    "end": "96229"
  },
  {
    "text": "And sometimes it will\nbe some other token,",
    "start": "96230",
    "end": "98270"
  },
  {
    "text": "as the case with \"cook\" going\nto \"ate\" in this position.",
    "start": "98270",
    "end": "101719"
  },
  {
    "text": "Now the job of ELECTRA,\nthe discriminator here,",
    "start": "101720",
    "end": "104690"
  },
  {
    "text": "is to figure out\nwhich of those tokens",
    "start": "104690",
    "end": "106970"
  },
  {
    "text": "were in the original\ninput sequence",
    "start": "106970",
    "end": "108980"
  },
  {
    "text": "and which have been replaced.",
    "start": "108980",
    "end": "110420"
  },
  {
    "text": "So that's a binary\nprediction task,",
    "start": "110420",
    "end": "112280"
  },
  {
    "text": "and we can make it about all\nof the tokens in our input",
    "start": "112280",
    "end": "114890"
  },
  {
    "text": "sequence if we choose to.",
    "start": "114890",
    "end": "117470"
  },
  {
    "text": "Actual loss for ELECTRA is\nthe sum of the generator loss",
    "start": "117470",
    "end": "121790"
  },
  {
    "text": "and a weighted version\nof the ELECTRA, that",
    "start": "121790",
    "end": "123740"
  },
  {
    "text": "is the discriminator loss.",
    "start": "123740",
    "end": "125840"
  },
  {
    "text": "However, that's kind of\nmasking an important asymmetry",
    "start": "125840",
    "end": "128360"
  },
  {
    "text": "in this model here.",
    "start": "128360",
    "end": "129750"
  },
  {
    "text": "Once we have trained\nthe generator,",
    "start": "129750",
    "end": "131450"
  },
  {
    "text": "we can let it fall away and\ndo all of our fine-tuning",
    "start": "131450",
    "end": "134510"
  },
  {
    "text": "on the discriminator.",
    "start": "134510",
    "end": "135590"
  },
  {
    "text": "That is on the\nELECTRA itself, which",
    "start": "135590",
    "end": "137810"
  },
  {
    "text": "means that we'll be\nfine-tuning a model that never",
    "start": "137810",
    "end": "139910"
  },
  {
    "text": "saw any of those MASK tokens.",
    "start": "139910",
    "end": "141890"
  },
  {
    "text": "So we address that first\nlimitation of BERT.",
    "start": "141890",
    "end": "144240"
  },
  {
    "text": "And we're also going to make\na prediction with ELECTRA",
    "start": "144240",
    "end": "146840"
  },
  {
    "text": "about every single one of\nthe input tokens, which",
    "start": "146840",
    "end": "149060"
  },
  {
    "text": "means that we're making more\nuse of the available data.",
    "start": "149060",
    "end": "154431"
  },
  {
    "start": "154000",
    "end": "303000"
  },
  {
    "text": "One thing I really like\nabout the ELECTRA paper",
    "start": "154432",
    "end": "156390"
  },
  {
    "text": "is that it offers a really\nrich set of analyses",
    "start": "156390",
    "end": "158880"
  },
  {
    "text": "of the efficiency of the model\nand of its optimal design.",
    "start": "158880",
    "end": "161708"
  },
  {
    "text": "So I'm going to highlight some\nof those results here starting",
    "start": "161708",
    "end": "164250"
  },
  {
    "text": "with this\ngenerator/discriminator",
    "start": "164250",
    "end": "166020"
  },
  {
    "text": "relationship results.",
    "start": "166020",
    "end": "167730"
  },
  {
    "text": "So the authors observe\nthat where the generator",
    "start": "167730",
    "end": "169830"
  },
  {
    "text": "and discriminator\nare the same size,",
    "start": "169830",
    "end": "172450"
  },
  {
    "text": "they can share all their\ntransformer parameters.",
    "start": "172450",
    "end": "174989"
  },
  {
    "text": "They can kind of be\none model in essence.",
    "start": "174990",
    "end": "177600"
  },
  {
    "text": "And they find that more\nsharing is indeed better, which",
    "start": "177600",
    "end": "180210"
  },
  {
    "text": "is encouraging.",
    "start": "180210",
    "end": "181630"
  },
  {
    "text": "However, they also observed\nthat the best results",
    "start": "181630",
    "end": "184860"
  },
  {
    "text": "come from having a\ngenerator that is small",
    "start": "184860",
    "end": "187500"
  },
  {
    "text": "compared to the discriminator.",
    "start": "187500",
    "end": "189470"
  },
  {
    "text": "And this plot kind of\nsummarizes the evidence there.",
    "start": "189470",
    "end": "191830"
  },
  {
    "text": "So we have our GLUE\nscore as the goal posts",
    "start": "191830",
    "end": "194402"
  },
  {
    "text": "that we're going to use\nto assess these models.",
    "start": "194402",
    "end": "196359"
  },
  {
    "text": "That's along the y-axis.",
    "start": "196360",
    "end": "198030"
  },
  {
    "text": "Along the x-axis, we\nhave the generator size.",
    "start": "198030",
    "end": "201000"
  },
  {
    "text": "And then they've\nplotted out a few sizes",
    "start": "201000",
    "end": "202980"
  },
  {
    "text": "for the discriminator.",
    "start": "202980",
    "end": "203900"
  },
  {
    "text": "And I think what you\ncan see quite clearly",
    "start": "203900",
    "end": "205650"
  },
  {
    "text": "is that, in general, you get\nthe best results on GLUE where",
    "start": "205650",
    "end": "209340"
  },
  {
    "text": "the discriminator is\ntwo to three times",
    "start": "209340",
    "end": "211349"
  },
  {
    "text": "larger than the generator.",
    "start": "211350",
    "end": "213300"
  },
  {
    "text": "And that's true even for this\nvery small model in green",
    "start": "213300",
    "end": "215710"
  },
  {
    "text": "down here.",
    "start": "215710",
    "end": "216210"
  },
  {
    "text": "The results are\noverall not very good,",
    "start": "216210",
    "end": "218220"
  },
  {
    "text": "but we see that\nsame relationship",
    "start": "218220",
    "end": "220020"
  },
  {
    "text": "where the optimal\ndiscriminator is",
    "start": "220020",
    "end": "221700"
  },
  {
    "text": "at size 256 and the\ngenerator at size 64.",
    "start": "221700",
    "end": "225535"
  },
  {
    "text": "That's where we reach\nour peak results.",
    "start": "225535",
    "end": "227160"
  },
  {
    "text": "And it's kind of comparable to\nthis very large model in blue",
    "start": "227160",
    "end": "230097"
  },
  {
    "text": "where our optimal size\nfor the discriminator",
    "start": "230097",
    "end": "231930"
  },
  {
    "text": "is 768 compared to\n256 for the generator.",
    "start": "231930",
    "end": "237810"
  },
  {
    "text": "They also do a bunch of\nreally interesting efficiency",
    "start": "237810",
    "end": "240090"
  },
  {
    "text": "analyses.",
    "start": "240090",
    "end": "240702"
  },
  {
    "text": "One thing I like\nabout the paper is",
    "start": "240702",
    "end": "242159"
  },
  {
    "text": "that it's kind of oriented\ntoward figuring out",
    "start": "242160",
    "end": "244860"
  },
  {
    "text": "how we can train these models\nmore efficiently with fewer",
    "start": "244860",
    "end": "247500"
  },
  {
    "text": "compute resources.",
    "start": "247500",
    "end": "249060"
  },
  {
    "text": "And this is a kind of\nsummary of central evidence",
    "start": "249060",
    "end": "251250"
  },
  {
    "text": "that they offer that ELECTRA\ncan be an efficient model.",
    "start": "251250",
    "end": "254650"
  },
  {
    "text": "So again we're going\nto use along the y-axis",
    "start": "254650",
    "end": "256528"
  },
  {
    "text": "the GLUE score as\nour goal posts.",
    "start": "256529",
    "end": "258799"
  },
  {
    "text": "Along the x-axis, here we\nhave pre-training FLOPs.",
    "start": "258800",
    "end": "261268"
  },
  {
    "text": "So this would be the number\nof compute operations",
    "start": "261269",
    "end": "263370"
  },
  {
    "text": "that you need to\npre-train the model.",
    "start": "263370",
    "end": "265919"
  },
  {
    "text": "In blue, along the\ntop here, is ELECTRA.",
    "start": "265920",
    "end": "267840"
  },
  {
    "text": "It's the very best model.",
    "start": "267840",
    "end": "269610"
  },
  {
    "text": "In orange, just below it,\nis adversarial ELECTRA,",
    "start": "269610",
    "end": "273030"
  },
  {
    "text": "which is an interesting approach\nto ELECTRA where we essentially",
    "start": "273030",
    "end": "275970"
  },
  {
    "text": "train the generator to try\nto fool the discriminator as",
    "start": "275970",
    "end": "278910"
  },
  {
    "text": "opposed to having the two\ncooperate as in core ELECTRA,",
    "start": "278910",
    "end": "281880"
  },
  {
    "text": "and that turns out\nto be pretty good.",
    "start": "281880",
    "end": "283627"
  },
  {
    "text": "And also these green lines\nare really interesting.",
    "start": "283627",
    "end": "285710"
  },
  {
    "text": "So two-stage ELECTRA is where\nI start by training just",
    "start": "285710",
    "end": "289470"
  },
  {
    "text": "against the BERT objectives.",
    "start": "289470",
    "end": "291030"
  },
  {
    "text": "And at a certain\npoint, switch over",
    "start": "291030",
    "end": "292980"
  },
  {
    "text": "to training the\nELECTRA objective.",
    "start": "292980",
    "end": "294640"
  },
  {
    "text": "And you can see that even that\nis better than just continuing",
    "start": "294640",
    "end": "297240"
  },
  {
    "text": "on with BERT all the\nway up to the maximum",
    "start": "297240",
    "end": "299669"
  },
  {
    "text": "for our compute budget here.",
    "start": "299670",
    "end": "301050"
  },
  {
    "start": "301050",
    "end": "303860"
  },
  {
    "start": "303000",
    "end": "404000"
  },
  {
    "text": "The paper also explores a bunch\nof variations on the ELECTRA",
    "start": "303860",
    "end": "307199"
  },
  {
    "text": "objective itself.",
    "start": "307200",
    "end": "308400"
  },
  {
    "text": "So I presented to\nyou full ELECTRA.",
    "start": "308400",
    "end": "310729"
  },
  {
    "text": "And it's full\nELECTRA in the sense",
    "start": "310730",
    "end": "312500"
  },
  {
    "text": "that over here on\nthe right we're",
    "start": "312500",
    "end": "313940"
  },
  {
    "text": "making predictions\nabout every single one",
    "start": "313940",
    "end": "315830"
  },
  {
    "text": "of the tokens in the input.",
    "start": "315830",
    "end": "318110"
  },
  {
    "text": "We could also explore something\nthat was analogous to BERT.",
    "start": "318110",
    "end": "321080"
  },
  {
    "text": "ELECTRA 15% would be the case\nwhere we make predictions",
    "start": "321080",
    "end": "324860"
  },
  {
    "text": "only about tokens that were\nway back here in the input",
    "start": "324860",
    "end": "328400"
  },
  {
    "text": "x-masked actually masked out.",
    "start": "328400",
    "end": "331658"
  },
  {
    "text": "Another variant that the\nteam considered actually",
    "start": "331658",
    "end": "333699"
  },
  {
    "text": "relates to how we train BERT.",
    "start": "333700",
    "end": "335020"
  },
  {
    "text": "So recall that for BERT\nwe train both by masking",
    "start": "335020",
    "end": "337960"
  },
  {
    "text": "and by replacing some\ntokens with other randomly",
    "start": "337960",
    "end": "341050"
  },
  {
    "text": "chosen tokens.",
    "start": "341050",
    "end": "342280"
  },
  {
    "text": "And we could try training\nthe generator just",
    "start": "342280",
    "end": "344350"
  },
  {
    "text": "with that approach, which\nwould eliminate the MASK",
    "start": "344350",
    "end": "346900"
  },
  {
    "text": "token entirely.",
    "start": "346900",
    "end": "347860"
  },
  {
    "text": "So that's this\narea here where we",
    "start": "347860",
    "end": "349240"
  },
  {
    "text": "have no masking on x-masked\nbut rather just randomly",
    "start": "349240",
    "end": "352690"
  },
  {
    "text": "replace tokens from\nthe actual vocabulary.",
    "start": "352690",
    "end": "356590"
  },
  {
    "text": "And then finally all tokens\nMLM would adopt some ideas",
    "start": "356590",
    "end": "359590"
  },
  {
    "text": "from ELECTRA into\nthe BERT model.",
    "start": "359590",
    "end": "361820"
  },
  {
    "text": "So recall that for\nthe MLM object,",
    "start": "361820",
    "end": "363430"
  },
  {
    "text": "we essentially turned it off\nfor tokens that weren't masked.",
    "start": "363430",
    "end": "366190"
  },
  {
    "text": "But there's no principal\nreason why we're doing that.",
    "start": "366190",
    "end": "368120"
  },
  {
    "text": "We could, of course,\nhave the loss",
    "start": "368120",
    "end": "369537"
  },
  {
    "text": "apply to every single one of\nthe tokens in the input stream.",
    "start": "369537",
    "end": "372819"
  },
  {
    "text": "And that gives us all tokens\nMLM on the generator side.",
    "start": "372820",
    "end": "377063"
  },
  {
    "text": "And the central finding\nof the paper I suppose",
    "start": "377063",
    "end": "378979"
  },
  {
    "text": "is that ELECTRA is the best\nof all of these models.",
    "start": "378980",
    "end": "381410"
  },
  {
    "text": "You also have a\nreally good model",
    "start": "381410",
    "end": "382870"
  },
  {
    "text": "if you do all-tokens MLM, which\nis something that might inform",
    "start": "382870",
    "end": "385660"
  },
  {
    "text": "development on the BERT\nside, in addition to",
    "start": "385660",
    "end": "388270"
  },
  {
    "text": "BERT in the context of ELECTRA.",
    "start": "388270",
    "end": "390520"
  },
  {
    "text": "Replace MLM is less good.",
    "start": "390520",
    "end": "392199"
  },
  {
    "text": "And ELECTRA 15% kind of down\nthere at the bottom there.",
    "start": "392200",
    "end": "395340"
  },
  {
    "text": "BERT, I think this is\nkind of showing us that we",
    "start": "395340",
    "end": "397540"
  },
  {
    "text": "should make more predictions.",
    "start": "397540",
    "end": "398870"
  },
  {
    "text": "That was a guiding\nintuition for ELECTRA.",
    "start": "398870",
    "end": "400705"
  },
  {
    "text": "And it seems to be borne\nout by these results.",
    "start": "400705",
    "end": "404449"
  },
  {
    "start": "404000",
    "end": "443000"
  },
  {
    "text": "And finally, as is\ncommon in the space,",
    "start": "404450",
    "end": "406578"
  },
  {
    "text": "the ELECTRA team did\nsome model releases",
    "start": "406578",
    "end": "408245"
  },
  {
    "text": "of pre-trained parameters\nthat you can make use of.",
    "start": "408245",
    "end": "411110"
  },
  {
    "text": "They did ELECTRA base\nand ELECTRA large,",
    "start": "411110",
    "end": "412892"
  },
  {
    "text": "which is kind of comparable\nto the corresponding BERT",
    "start": "412892",
    "end": "415100"
  },
  {
    "text": "releases.",
    "start": "415100",
    "end": "415875"
  },
  {
    "text": "I think an\ninteresting thing they",
    "start": "415875",
    "end": "417250"
  },
  {
    "text": "did is also released this\nELECTRA small model which",
    "start": "417250",
    "end": "420680"
  },
  {
    "text": "is designed to quickly be\ntrained on a single GPU,",
    "start": "420680",
    "end": "423889"
  },
  {
    "text": "again tying into the idea\nthat we ought to be thinking",
    "start": "423890",
    "end": "426980"
  },
  {
    "text": "about how we can train models\nlike this when we have highly",
    "start": "426980",
    "end": "430310"
  },
  {
    "text": "constrained compute resources.",
    "start": "430310",
    "end": "432210"
  },
  {
    "text": "ELECTRA was keyed into that\nidea from the very beginning.",
    "start": "432210",
    "end": "434960"
  },
  {
    "text": "I think the small model shows\nthat it can be productive.",
    "start": "434960",
    "end": "438880"
  },
  {
    "start": "438880",
    "end": "442000"
  }
]