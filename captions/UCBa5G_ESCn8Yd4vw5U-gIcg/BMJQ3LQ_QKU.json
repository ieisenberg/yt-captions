[
  {
    "start": "0",
    "end": "1020"
  },
  {
    "text": "Up to now, we've talked about\nestimating regression functions",
    "start": "1020",
    "end": "4319"
  },
  {
    "text": "for quantitative\nresponse, and we've",
    "start": "4320",
    "end": "7230"
  },
  {
    "text": "seen how to do model\nselection there.",
    "start": "7230",
    "end": "9220"
  },
  {
    "text": "Now we're going to move to\nclassification problems.",
    "start": "9220",
    "end": "12430"
  },
  {
    "text": "And here we've got a\ndifferent response variable,",
    "start": "12430",
    "end": "15280"
  },
  {
    "text": "it's what we call a\nqualitative variable.",
    "start": "15280",
    "end": "17529"
  },
  {
    "text": "For example, email is one\nof two classes spam or ham,",
    "start": "17530",
    "end": "21630"
  },
  {
    "text": "the ham being the good email.",
    "start": "21630",
    "end": "23880"
  },
  {
    "text": "And if we classify in digits,\nit's one of the classes 0, 1,",
    "start": "23880",
    "end": "27539"
  },
  {
    "text": "through up to 9.",
    "start": "27540",
    "end": "28890"
  },
  {
    "text": "And so it's a slightly\ndifferent problem,",
    "start": "28890",
    "end": "31230"
  },
  {
    "text": "and our goals are slightly\ndifferent as well.",
    "start": "31230",
    "end": "34489"
  },
  {
    "text": "And here our goals are to\nbuild a classifier which",
    "start": "34490",
    "end": "37070"
  },
  {
    "text": "we might call C of X,\nthat assigns a class",
    "start": "37070",
    "end": "40160"
  },
  {
    "text": "label from our set C, to a\nfuture unlabeled observation",
    "start": "40160",
    "end": "44780"
  },
  {
    "text": "X, where X is the\nfeature vector.",
    "start": "44780",
    "end": "49100"
  },
  {
    "text": "We'd also like to\nassess the uncertainty",
    "start": "49100",
    "end": "51170"
  },
  {
    "text": "in each classification,\nand we'd also",
    "start": "51170",
    "end": "53570"
  },
  {
    "text": "like to understand the roles\nof the different predictors",
    "start": "53570",
    "end": "56329"
  },
  {
    "text": "amongst the X's in\nproducing that classifier.",
    "start": "56330",
    "end": "60420"
  },
  {
    "text": "And so we're going\nto see how we do that",
    "start": "60420",
    "end": "63149"
  },
  {
    "text": "in the next number of slides.",
    "start": "63150",
    "end": "64985"
  },
  {
    "start": "64985",
    "end": "69150"
  },
  {
    "text": "To try and parallel\nour development",
    "start": "69150",
    "end": "71100"
  },
  {
    "text": "for the quantitative\nresponse, I've",
    "start": "71100",
    "end": "73380"
  },
  {
    "text": "made up a little\nsimulation example.",
    "start": "73380",
    "end": "76689"
  },
  {
    "text": "And we've got one x, one y.",
    "start": "76690",
    "end": "79750"
  },
  {
    "text": "The y takes on two values.",
    "start": "79750",
    "end": "81400"
  },
  {
    "text": "In this example, the values\nare just coded as 0 and 1.",
    "start": "81400",
    "end": "85320"
  },
  {
    "text": "And we've got a big sample of\nthese y's from a population.",
    "start": "85320",
    "end": "89880"
  },
  {
    "text": "So each little vertical bar here\nindicates an occurrence of a 0,",
    "start": "89880",
    "end": "98979"
  },
  {
    "text": "the orange vertical bars\nas a function of the x's.",
    "start": "98980",
    "end": "102420"
  },
  {
    "text": "And at the top we've got\nwhere the 1's occurred.",
    "start": "102420",
    "end": "107009"
  },
  {
    "text": "So this scatter plot\nis much harder to read,",
    "start": "107010",
    "end": "110680"
  },
  {
    "text": "you can't really\nsee what's going on.",
    "start": "110680",
    "end": "113370"
  },
  {
    "text": "And there's a black curve\ndrawn in the figure.",
    "start": "113370",
    "end": "117120"
  },
  {
    "text": "And the black curve was what\nactually generated the data.",
    "start": "117120",
    "end": "121230"
  },
  {
    "text": "The black curve is\nactually showing",
    "start": "121230",
    "end": "123120"
  },
  {
    "text": "us the probability\nof a 1 in the model",
    "start": "123120",
    "end": "125580"
  },
  {
    "text": "that I used to\ngenerate the data.",
    "start": "125580",
    "end": "128449"
  },
  {
    "text": "And so up in this region over\nhere, the high values of x,",
    "start": "128449",
    "end": "137380"
  },
  {
    "text": "there's a higher\nprobability of a 1,",
    "start": "137380",
    "end": "140260"
  },
  {
    "text": "it's close to 90%\nof getting a 1.",
    "start": "140260",
    "end": "143780"
  },
  {
    "text": "And so, of course, we\nsee more blue 1's there",
    "start": "143780",
    "end": "146830"
  },
  {
    "text": "than we see 0's down there.",
    "start": "146830",
    "end": "149350"
  },
  {
    "text": "And even though\nit's hard to see,",
    "start": "149350",
    "end": "151120"
  },
  {
    "text": "there's a higher density of\n0's in this region over here",
    "start": "151120",
    "end": "155290"
  },
  {
    "text": "where, the probability is\n0.4 of being a 1 versus a 0,",
    "start": "155290",
    "end": "162370"
  },
  {
    "text": "where it's 0.6.",
    "start": "162370",
    "end": "163840"
  },
  {
    "text": "So we want to talk about what\nis an ideal classifier C of X.",
    "start": "163840",
    "end": "173410"
  },
  {
    "text": "So let's define these\nprobabilities that I was talking",
    "start": "173410",
    "end": "176200"
  },
  {
    "text": "about.",
    "start": "176200",
    "end": "177099"
  },
  {
    "text": "And we'll call P sub k of\nx, this quantity over here,",
    "start": "177100",
    "end": "182260"
  },
  {
    "text": "that's the conditional\nprobability that Y",
    "start": "182260",
    "end": "185799"
  },
  {
    "text": "is k given X equals x.",
    "start": "185800",
    "end": "188200"
  },
  {
    "text": "In this case, we're just\nlooking at probability of one",
    "start": "188200",
    "end": "193000"
  },
  {
    "text": "in the plot, we're just showing\nthe probability that Y is 1.",
    "start": "193000",
    "end": "196502"
  },
  {
    "text": "There's only two classes,\nbut in general, there",
    "start": "196502",
    "end": "198459"
  },
  {
    "text": "will be, say, capital K classes.",
    "start": "198460",
    "end": "201280"
  },
  {
    "text": "And there'll be capital K of\nthese conditional probabilities",
    "start": "201280",
    "end": "206470"
  },
  {
    "text": "Now, In classification problems\nthose conditional probabilities",
    "start": "206470",
    "end": "212200"
  },
  {
    "text": "completely capture the\nconditional distribution",
    "start": "212200",
    "end": "215950"
  },
  {
    "text": "of Y given x.",
    "start": "215950",
    "end": "218890"
  },
  {
    "text": "And it turns out that those also\ndeliver the ideal classifier.",
    "start": "218890",
    "end": "224990"
  },
  {
    "text": "We call the Bayes optimal\nclassifier, the classifier",
    "start": "224990",
    "end": "229160"
  },
  {
    "text": "that classifies to\nthe class for which",
    "start": "229160",
    "end": "231380"
  },
  {
    "text": "the conditional probability\nfor that element of the class",
    "start": "231380",
    "end": "235740"
  },
  {
    "text": "is largest.",
    "start": "235740",
    "end": "238620"
  },
  {
    "text": "That makes sense.",
    "start": "238620",
    "end": "239950"
  },
  {
    "text": "You go to any\npoint, so here we've",
    "start": "239950",
    "end": "242700"
  },
  {
    "text": "gone to 0.5 in the x space.",
    "start": "242700",
    "end": "246540"
  },
  {
    "text": "And you look above\nit, and you see",
    "start": "246540",
    "end": "249360"
  },
  {
    "text": "that there's about 80%\nprobability of a 1,",
    "start": "249360",
    "end": "255180"
  },
  {
    "text": "and 20% probability of a 0.",
    "start": "255180",
    "end": "258359"
  },
  {
    "text": "And now we're\ngoing to say, well,",
    "start": "258360",
    "end": "259768"
  },
  {
    "text": "if you were to\nclassify to one class",
    "start": "259769",
    "end": "261930"
  },
  {
    "text": "at that point, which class\nwould you classify to?",
    "start": "261930",
    "end": "264370"
  },
  {
    "text": "Well, you're going to classify\nit to the majority class.",
    "start": "264370",
    "end": "268570"
  },
  {
    "text": "And that's called the\nBayes optimal classifier.",
    "start": "268570",
    "end": "273400"
  },
  {
    "text": "Here's the same example,\nexcept now we've only",
    "start": "273400",
    "end": "276699"
  },
  {
    "text": "got a handful of points, we've\ngot hundreds points, having",
    "start": "276700",
    "end": "281460"
  },
  {
    "text": "one of the two class labels.",
    "start": "281460",
    "end": "283750"
  },
  {
    "text": "The story is the same as before.",
    "start": "283750",
    "end": "287020"
  },
  {
    "text": "We can't compute the conditional\nprobabilities exactly, say,",
    "start": "287020",
    "end": "290470"
  },
  {
    "text": "at the 0.5, because in this\ncase, we've got 1-1 at 5,",
    "start": "290470",
    "end": "296380"
  },
  {
    "text": "and no 0's.",
    "start": "296380",
    "end": "298060"
  },
  {
    "text": "So we send out a\nneighborhood, say,",
    "start": "298060",
    "end": "300280"
  },
  {
    "text": "and gather 10% of\nthe data points,",
    "start": "300280",
    "end": "303370"
  },
  {
    "text": "and then estimate the\nconditional probabilities",
    "start": "303370",
    "end": "306070"
  },
  {
    "text": "by the proportions in\nthis case of 1's and 0's",
    "start": "306070",
    "end": "309520"
  },
  {
    "text": "in the neighborhood.",
    "start": "309520",
    "end": "311259"
  },
  {
    "text": "And those are indicated\nby these little bars here,",
    "start": "311260",
    "end": "316050"
  },
  {
    "text": "these are meant to\nrepresent the probabilities",
    "start": "316050",
    "end": "318050"
  },
  {
    "text": "or proportions at this 0.5.",
    "start": "318050",
    "end": "320810"
  },
  {
    "text": "And again, there's a higher\nproportion of 1's here than 0's.",
    "start": "320810",
    "end": "324020"
  },
  {
    "text": "I forgot to say that\nin the previous slide,",
    "start": "324020",
    "end": "325889"
  },
  {
    "text": "that's the same quantity\nover here that's indicating",
    "start": "325890",
    "end": "329060"
  },
  {
    "text": "the probabilities of\nthe 1's and the 0's.",
    "start": "329060",
    "end": "331889"
  },
  {
    "text": "This is exact in the\npopulation, and here it",
    "start": "331890",
    "end": "335420"
  },
  {
    "text": "is estimated with a\nnearest neighbor average.",
    "start": "335420",
    "end": "339270"
  },
  {
    "text": "So here we've done\nthe nearest neighbor",
    "start": "339270",
    "end": "341490"
  },
  {
    "text": "classifying in one\ndimension, where",
    "start": "341490",
    "end": "343759"
  },
  {
    "text": "we can draw a nice picture.",
    "start": "343760",
    "end": "345040"
  },
  {
    "text": "But of course, this works in\nmultiple dimensions as well,",
    "start": "345040",
    "end": "347970"
  },
  {
    "text": "just like it did for regression.",
    "start": "347970",
    "end": "349840"
  },
  {
    "text": "So suppose we, for example,\nhave two x's and they",
    "start": "349840",
    "end": "354300"
  },
  {
    "text": "lie on the floor, and\nwe have a target point,",
    "start": "354300",
    "end": "358590"
  },
  {
    "text": "say this point over\nhere, and we want",
    "start": "358590",
    "end": "362130"
  },
  {
    "text": "to classify a new observation\nit falls at this point.",
    "start": "362130",
    "end": "364930"
  },
  {
    "text": "Well, we can spread\nout a little,",
    "start": "364930",
    "end": "366630"
  },
  {
    "text": "say circular neighborhood and\ngather a bunch of observations.",
    "start": "366630",
    "end": "370150"
  },
  {
    "text": "We fall in this\nneighborhood, and we",
    "start": "370150",
    "end": "372030"
  },
  {
    "text": "can count how many are in\nclass one, how many in class 2,",
    "start": "372030",
    "end": "375330"
  },
  {
    "text": "and assign to the\nmajority class.",
    "start": "375330",
    "end": "377586"
  },
  {
    "text": "And of course, this\ncan be generalized",
    "start": "377587",
    "end": "379170"
  },
  {
    "text": "to multiple dimensions.",
    "start": "379170",
    "end": "381150"
  },
  {
    "text": "Then all the problems we\nhad with nearest neighbors",
    "start": "381150",
    "end": "384389"
  },
  {
    "text": "for regression, the\ncurse of dimensionality",
    "start": "384390",
    "end": "386790"
  },
  {
    "text": "when the number of dimensions\nget large also happens here.",
    "start": "386790",
    "end": "390660"
  },
  {
    "text": "In order to gather\nenough points,",
    "start": "390660",
    "end": "392340"
  },
  {
    "text": "when the dimension\nis really high,",
    "start": "392340",
    "end": "393960"
  },
  {
    "text": "we have to send out a\nbigger and bigger sphere",
    "start": "393960",
    "end": "397470"
  },
  {
    "text": "to capture the\npoints, and things",
    "start": "397470",
    "end": "400360"
  },
  {
    "text": "start to break down, because\nit's not local anymore.",
    "start": "400360",
    "end": "404900"
  },
  {
    "text": "So some details.",
    "start": "404900",
    "end": "407180"
  },
  {
    "text": "Typically, we'll measure the\nperformance of the classifier,",
    "start": "407180",
    "end": "410360"
  },
  {
    "text": "using what we call the\nmisclassification error rate.",
    "start": "410360",
    "end": "414530"
  },
  {
    "text": "And here it's written\non the test data set,",
    "start": "414530",
    "end": "416860"
  },
  {
    "text": "the error is just the average\nnumber of mistakes we make.",
    "start": "416860",
    "end": "422240"
  },
  {
    "text": "So it's the average\nnumber of times",
    "start": "422240",
    "end": "424729"
  },
  {
    "text": "that the classification,\nso C hat at the point Xi",
    "start": "424730",
    "end": "429510"
  },
  {
    "text": "is not equal to the class label\nYi, averaged over a test set.",
    "start": "429510",
    "end": "435780"
  },
  {
    "text": "It's just the\nnumber of mistakes.",
    "start": "435780",
    "end": "437660"
  },
  {
    "text": "So that's when we count\na mistake in a 1 for a 0,",
    "start": "437660",
    "end": "443040"
  },
  {
    "text": "and a 0 for a 1 as equal.",
    "start": "443040",
    "end": "445990"
  },
  {
    "text": "There are other ways\nof classifying error",
    "start": "445990",
    "end": "447990"
  },
  {
    "text": "where you can have a\ncost, which gives higher",
    "start": "447990",
    "end": "451530"
  },
  {
    "text": "cost to some\nmistakes than others,",
    "start": "451530",
    "end": "453400"
  },
  {
    "text": "but we won't go into that here.",
    "start": "453400",
    "end": "456540"
  },
  {
    "text": "So that Bayes'\nclassifier, the one",
    "start": "456540",
    "end": "460650"
  },
  {
    "text": "that used the true probabilities\nto decide on the classification",
    "start": "460650",
    "end": "464009"
  },
  {
    "text": "rule is the one\nthat makes the least",
    "start": "464010",
    "end": "466860"
  },
  {
    "text": "mistakes in the population.",
    "start": "466860",
    "end": "469199"
  },
  {
    "text": "And that makes sense.",
    "start": "469200",
    "end": "470670"
  },
  {
    "text": "If you look at our example over\nhere, by classifying to a 1",
    "start": "470670",
    "end": "476580"
  },
  {
    "text": "over here, we're going to\nmake mistakes on about 20%",
    "start": "476580",
    "end": "481979"
  },
  {
    "text": "of the conditional population\nat this value of x,",
    "start": "481980",
    "end": "484500"
  },
  {
    "text": "but we'll get it\ncorrect 80% of the time.",
    "start": "484500",
    "end": "487230"
  },
  {
    "text": "And so that's why\nit's obvious we",
    "start": "487230",
    "end": "489270"
  },
  {
    "text": "want to classify it\nto the largest class,",
    "start": "489270",
    "end": "490983"
  },
  {
    "text": "we'll make the fewest mistakes.",
    "start": "490983",
    "end": "492275"
  },
  {
    "start": "492275",
    "end": "495169"
  },
  {
    "text": "Later on in the\ncourse, we're going",
    "start": "495170",
    "end": "496700"
  },
  {
    "text": "to talk about support vector\nmachines, and they build",
    "start": "496700",
    "end": "499760"
  },
  {
    "text": "structured models for\nthe classifier C of x.",
    "start": "499760",
    "end": "502790"
  },
  {
    "text": "And we also build\nstructured models",
    "start": "502790",
    "end": "504470"
  },
  {
    "text": "for representing the\nprobabilities themselves,",
    "start": "504470",
    "end": "506820"
  },
  {
    "text": "and there we'll discuss methods\nlike logistic regression",
    "start": "506820",
    "end": "510110"
  },
  {
    "text": "and generalized additive models.",
    "start": "510110",
    "end": "511555"
  },
  {
    "start": "511555",
    "end": "515059"
  },
  {
    "text": "The high-dimensional\nproblem is worse",
    "start": "515059",
    "end": "518599"
  },
  {
    "text": "for modeling the probabilities\nthan it is for actually building",
    "start": "518600",
    "end": "523729"
  },
  {
    "text": "the classifier.",
    "start": "523730",
    "end": "524568"
  },
  {
    "text": "For the classifier.",
    "start": "524568",
    "end": "525360"
  },
  {
    "text": "You just have to\nthe classifier just",
    "start": "525360",
    "end": "527570"
  },
  {
    "text": "has to be accurate with regard\nto which of the probabilities",
    "start": "527570",
    "end": "530840"
  },
  {
    "text": "is largest, whereas\nif we're really",
    "start": "530840",
    "end": "532850"
  },
  {
    "text": "interested in the\nprobabilities themselves,",
    "start": "532850",
    "end": "534703"
  },
  {
    "text": "we're going to be measuring\nthem on a much finer scale.",
    "start": "534703",
    "end": "536995"
  },
  {
    "start": "536995",
    "end": "540080"
  },
  {
    "text": "We'll end up with a\ntwo-dimensional example",
    "start": "540080",
    "end": "543830"
  },
  {
    "text": "of nearest neighbors.",
    "start": "543830",
    "end": "545210"
  },
  {
    "text": "So here this\nrepresents the truth.",
    "start": "545210",
    "end": "548930"
  },
  {
    "text": "We've got an X1 and X2,\nand we've got a points",
    "start": "548930",
    "end": "552350"
  },
  {
    "text": "from some population.",
    "start": "552350",
    "end": "554089"
  },
  {
    "text": "And the purple dotted line is\nwhat's called the Bayes decision",
    "start": "554090",
    "end": "558350"
  },
  {
    "text": "boundary.",
    "start": "558350",
    "end": "559060"
  },
  {
    "text": "Since we know the truth here, we\nknow what the true probabilities",
    "start": "559060",
    "end": "562190"
  },
  {
    "text": "are everywhere in this domain.",
    "start": "562190",
    "end": "564690"
  },
  {
    "text": "And I've indicated all\nthe points in the domain",
    "start": "564690",
    "end": "567650"
  },
  {
    "text": "by the little dots\nin the figure.",
    "start": "567650",
    "end": "570380"
  },
  {
    "text": "And so if you classify according\nto the true probabilities, all",
    "start": "570380",
    "end": "575550"
  },
  {
    "text": "the points, all the\nregion, colored orange",
    "start": "575550",
    "end": "577980"
  },
  {
    "text": "would be classified\nas the one class,",
    "start": "577980",
    "end": "580589"
  },
  {
    "text": "and all the region,\ncolored blue would",
    "start": "580590",
    "end": "582660"
  },
  {
    "text": "be classified as the blue\nclass, and the dotted line's",
    "start": "582660",
    "end": "586620"
  },
  {
    "text": "called the decision boundary.",
    "start": "586620",
    "end": "588220"
  },
  {
    "text": "And so that's a contour of\nthe place where, in this case,",
    "start": "588220",
    "end": "591370"
  },
  {
    "text": "there are two classes,\nit's a contour",
    "start": "591370",
    "end": "593250"
  },
  {
    "text": "of where the probabilities\nare equal for the two classes.",
    "start": "593250",
    "end": "596830"
  },
  {
    "text": "So it's an undecided\nregion, it's",
    "start": "596830",
    "end": "598437"
  },
  {
    "text": "called the decision boundary.",
    "start": "598437",
    "end": "599645"
  },
  {
    "start": "599645",
    "end": "602320"
  },
  {
    "text": "So we can do nearest neighbor,\naveraging in two dimensions.",
    "start": "602320",
    "end": "607150"
  },
  {
    "text": "So of course, what we do\nhere is, at any given point,",
    "start": "607150",
    "end": "610660"
  },
  {
    "text": "when we want to\nclassify, let's say",
    "start": "610660",
    "end": "613480"
  },
  {
    "text": "we pick this point\nover here, we'll",
    "start": "613480",
    "end": "615910"
  },
  {
    "text": "spread out a little\nneighborhood in this case,",
    "start": "615910",
    "end": "619839"
  },
  {
    "text": "until we find the 10 closest\npoints to the target point.",
    "start": "619840",
    "end": "623770"
  },
  {
    "text": "And we'll estimate the\nprobability at this center point",
    "start": "623770",
    "end": "627430"
  },
  {
    "text": "here by the proportion\nof blues versus oranges.",
    "start": "627430",
    "end": "631660"
  },
  {
    "text": "And you do that at every point.",
    "start": "631660",
    "end": "633920"
  },
  {
    "text": "And if you use those\nas a probabilities,",
    "start": "633920",
    "end": "636279"
  },
  {
    "text": "you get this somewhat wiggly\nblack curve as the estimated",
    "start": "636280",
    "end": "640510"
  },
  {
    "text": "decision boundary.",
    "start": "640510",
    "end": "642230"
  },
  {
    "text": "And you can see it's actually,\napart from the somewhat",
    "start": "642230",
    "end": "645860"
  },
  {
    "text": "ugly wiggliness,\nit gets reasonably",
    "start": "645860",
    "end": "648740"
  },
  {
    "text": "close to the true\ndecision boundary,",
    "start": "648740",
    "end": "650660"
  },
  {
    "text": "which is, again, the purple\ndashed line, a curve.",
    "start": "650660",
    "end": "656560"
  },
  {
    "text": "In the last slide\nwe used K equals 10.",
    "start": "656560",
    "end": "659650"
  },
  {
    "text": "We can use other values of K.\nK equals 1 is a popular choice,",
    "start": "659650",
    "end": "664280"
  },
  {
    "text": "this is called the nearest\nneighbor classifier.",
    "start": "664280",
    "end": "667100"
  },
  {
    "text": "And literally at\neach target point,",
    "start": "667100",
    "end": "671560"
  },
  {
    "text": "we find the closest point\namongst the trainees data",
    "start": "671560",
    "end": "674170"
  },
  {
    "text": "and classify to its class.",
    "start": "674170",
    "end": "677260"
  },
  {
    "text": "So for example, if we\ntook a point over here,",
    "start": "677260",
    "end": "679690"
  },
  {
    "text": "which is the closest\ntraining point.",
    "start": "679690",
    "end": "681800"
  },
  {
    "text": "Well, this is it over here.",
    "start": "681800",
    "end": "683390"
  },
  {
    "text": "It's a blue, so we'd\nclassify this as blue.",
    "start": "683390",
    "end": "686850"
  },
  {
    "text": "When you're in the sea\nof blues, of course,",
    "start": "686850",
    "end": "688962"
  },
  {
    "text": "the nearest point is\nalways another blue,",
    "start": "688962",
    "end": "690670"
  },
  {
    "text": "and so you'd always\nclassify as blue.",
    "start": "690670",
    "end": "693750"
  },
  {
    "text": "What's interesting is\nas you get close to some",
    "start": "693750",
    "end": "696240"
  },
  {
    "text": "of the orange points.",
    "start": "696240",
    "end": "698459"
  },
  {
    "text": "And what this gives you, you\ncan see the boundary here,",
    "start": "698460",
    "end": "702360"
  },
  {
    "text": "is a piecewise linear boundary.",
    "start": "702360",
    "end": "705282"
  },
  {
    "text": "Of course, the\nprobabilities we estimate",
    "start": "705283",
    "end": "706950"
  },
  {
    "text": "is just 1 and 0, because\nthere's only 1 point to average,",
    "start": "706950",
    "end": "709810"
  },
  {
    "text": "so there's no real\nprobabilities.",
    "start": "709810",
    "end": "711520"
  },
  {
    "text": "But if you think about\nthe decision boundary,",
    "start": "711520",
    "end": "713790"
  },
  {
    "text": "it's a piecewise linear\ndecision boundary,",
    "start": "713790",
    "end": "717360"
  },
  {
    "text": "and it's gotten by looking\nat the the, the bisector",
    "start": "717360",
    "end": "721470"
  },
  {
    "text": "of the line separating\neach pair of points",
    "start": "721470",
    "end": "726089"
  },
  {
    "text": "when they of different colors.",
    "start": "726090",
    "end": "728830"
  },
  {
    "text": "So you get this very\nragged decision boundary.",
    "start": "728830",
    "end": "733100"
  },
  {
    "text": "You also get little islands.",
    "start": "733100",
    "end": "734480"
  },
  {
    "text": "So, for example, there's a blue\npoint in the sea of oranges",
    "start": "734480",
    "end": "737550"
  },
  {
    "text": "here.",
    "start": "737550",
    "end": "738050"
  },
  {
    "text": "So there's a little\npiecewise linear boundary",
    "start": "738050",
    "end": "739990"
  },
  {
    "text": "around that blue point.",
    "start": "739990",
    "end": "741500"
  },
  {
    "text": "Those are the points\nthat are closer",
    "start": "741500",
    "end": "746740"
  },
  {
    "text": "to the blue point\nthan the oranges.",
    "start": "746740",
    "end": "749320"
  },
  {
    "text": "Again, we see the\ntrue boundary here,",
    "start": "749320",
    "end": "751660"
  },
  {
    "text": "or the best Bayes' decision\nboundary is purple.",
    "start": "751660",
    "end": "754540"
  },
  {
    "text": "This nearest neighbor average\napproximates it in a noisy way.",
    "start": "754540",
    "end": "760100"
  },
  {
    "text": "On the other hand, you can\nmake K really large, here,",
    "start": "760100",
    "end": "763250"
  },
  {
    "text": "we've made K 100.",
    "start": "763250",
    "end": "764780"
  },
  {
    "text": "And the neighborhood's\nreally large.",
    "start": "764780",
    "end": "769160"
  },
  {
    "text": "There's 200 points here, so it's\ntaking half the points to be",
    "start": "769160",
    "end": "771860"
  },
  {
    "text": "in any given neighborhood.",
    "start": "771860",
    "end": "773420"
  },
  {
    "text": "So let's suppose our\ntest point was over here,",
    "start": "773420",
    "end": "778579"
  },
  {
    "text": "we'd be sending out quite a big\ncircle, gathering 100 points,",
    "start": "778580",
    "end": "783660"
  },
  {
    "text": "getting the proportion of\nblues, a proportion of oranges,",
    "start": "783660",
    "end": "786589"
  },
  {
    "text": "and then making the boundary.",
    "start": "786590",
    "end": "788150"
  },
  {
    "text": "So as K gets bigger,\nthis boundary",
    "start": "788150",
    "end": "790250"
  },
  {
    "text": "starts smoothing out and getting\nless interesting in this case.",
    "start": "790250",
    "end": "793580"
  },
  {
    "text": "It's almost like a linear\nboundary over here,",
    "start": "793580",
    "end": "796070"
  },
  {
    "text": "and doesn't pick up the nuances\nof the decision boundary.",
    "start": "796070",
    "end": "800270"
  },
  {
    "text": "Whereas with K\nequals 10, it seemed",
    "start": "800270",
    "end": "802610"
  },
  {
    "text": "like a pretty good choice, and\nit approximates the decision",
    "start": "802610",
    "end": "807200"
  },
  {
    "text": "boundary pretty well.",
    "start": "807200",
    "end": "810550"
  },
  {
    "text": "So the choice of K is\na tuning parameter,",
    "start": "810550",
    "end": "813399"
  },
  {
    "text": "and that needs to be selected.",
    "start": "813400",
    "end": "815360"
  },
  {
    "text": "And here we show\nwhat happens as you",
    "start": "815360",
    "end": "818050"
  },
  {
    "text": "vary K, first on the training\ndata, and then on the test data.",
    "start": "818050",
    "end": "823070"
  },
  {
    "text": "So on the training data, K tends\nto just keep on decreasing.",
    "start": "823070",
    "end": "829160"
  },
  {
    "text": "It's not actually monotone,\nbecause we've actually indexed",
    "start": "829160",
    "end": "832300"
  },
  {
    "text": "this as 1 over K, because\nas 1 over K gets big,",
    "start": "832300",
    "end": "839709"
  },
  {
    "text": "let's see K large means we\nget the high bias, so 1 over k",
    "start": "839710",
    "end": "844210"
  },
  {
    "text": "small, so this is the\nlow complexity region,",
    "start": "844210",
    "end": "846920"
  },
  {
    "text": "this is the high\ncomplexity region.",
    "start": "846920",
    "end": "850519"
  },
  {
    "text": "Now you notice that the training\nerror for one nearest neighbors,",
    "start": "850520",
    "end": "854480"
  },
  {
    "text": "which is right down at the\nend here, 1 over K is 1, is 0.",
    "start": "854480",
    "end": "860360"
  },
  {
    "text": "Well, if you think about it,\nthat's what it's going to be,",
    "start": "860360",
    "end": "863620"
  },
  {
    "text": "if you think about\nwhat training error is.",
    "start": "863620",
    "end": "865640"
  },
  {
    "text": "For the test error,\nit's actually",
    "start": "865640",
    "end": "867620"
  },
  {
    "text": "started increasing again.",
    "start": "867620",
    "end": "870110"
  },
  {
    "text": "This horizontal dotted\nline is the Bayes error,",
    "start": "870110",
    "end": "874279"
  },
  {
    "text": "which is you can't do better\nthan the Bayes error in theory.",
    "start": "874280",
    "end": "878750"
  },
  {
    "text": "And of course, this is\na finite test data set,",
    "start": "878750",
    "end": "880730"
  },
  {
    "text": "but it doesn't, it actually\njust touches on the Bayes error.",
    "start": "880730",
    "end": "883760"
  },
  {
    "text": "It starts decreasing,\nand then at some point",
    "start": "883760",
    "end": "886670"
  },
  {
    "text": "it levels off, and then\nstarts increasing again.",
    "start": "886670",
    "end": "889130"
  },
  {
    "text": "So if we had a\nvalidation set available,",
    "start": "889130",
    "end": "891860"
  },
  {
    "text": "that's what we'd\nuse to determine K.",
    "start": "891860",
    "end": "895290"
  },
  {
    "text": "So that's our nearest\nneighbor classification,",
    "start": "895290",
    "end": "898769"
  },
  {
    "text": "very powerful tool.",
    "start": "898770",
    "end": "900810"
  },
  {
    "text": "It said that about one-third\nof classification problems,",
    "start": "900810",
    "end": "904890"
  },
  {
    "text": "the best tool will be nearest\nneighbor classification.",
    "start": "904890",
    "end": "908640"
  },
  {
    "text": "On the handwritten zip code\nproblem, the classifying",
    "start": "908640",
    "end": "912750"
  },
  {
    "text": "handwritten digits, nearest\nneighbor classifiers",
    "start": "912750",
    "end": "915240"
  },
  {
    "text": "do about as well as\nany other method tried.",
    "start": "915240",
    "end": "918750"
  },
  {
    "text": "So it's a powerful technique\nto have in the tool bag,",
    "start": "918750",
    "end": "921840"
  },
  {
    "text": "and it's one of the\ntechniques that we'll",
    "start": "921840",
    "end": "923910"
  },
  {
    "text": "use for classification.",
    "start": "923910",
    "end": "925319"
  },
  {
    "text": "But in the rest of\nthe course, we'll",
    "start": "925320",
    "end": "926820"
  },
  {
    "text": "cover other techniques as well,\nin particular, the support",
    "start": "926820",
    "end": "930390"
  },
  {
    "text": "vector machines, various\nforms of logistic regression,",
    "start": "930390",
    "end": "933630"
  },
  {
    "text": "and linear\ndiscriminant analysis.",
    "start": "933630",
    "end": "936680"
  },
  {
    "start": "936680",
    "end": "938000"
  }
]