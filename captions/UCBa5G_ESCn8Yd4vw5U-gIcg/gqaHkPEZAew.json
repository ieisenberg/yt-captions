[
  {
    "start": "0",
    "end": "125000"
  },
  {
    "start": "0",
    "end": "5230"
  },
  {
    "text": "OK, so what are we\ngoing to do for today? So the main content\nfor today is to go",
    "start": "5230",
    "end": "12610"
  },
  {
    "text": "through sort of more\nstuff about word vectors, including touching\non word senses",
    "start": "12610",
    "end": "18940"
  },
  {
    "text": "and then introducing the notion\nof neural network classifiers. So our biggest goal is that\nby the end of today's class",
    "start": "18940",
    "end": "26740"
  },
  {
    "text": "you should feel like\nyou could confidently look at one of the\nword embeddings papers, such as the Google word2vec\npaper or the GLoVe paper",
    "start": "26740",
    "end": "35560"
  },
  {
    "text": "or Sanjeev Arora's paper\nthat we'll come to later. And feel like yeah, I\ncan understand this.",
    "start": "35560",
    "end": "40839"
  },
  {
    "text": "I know what they're\ndoing and it makes sense. So let's go back\nto where we were.",
    "start": "40840",
    "end": "45860"
  },
  {
    "text": "So this was introducing\nthis model of word2vec and [AUDIO OUT] idea was that\nwe started with random word",
    "start": "45860",
    "end": "55809"
  },
  {
    "text": "vectors and then\nwe're going to-- we have a big corpus\nof text and we're going to iterate through each\nword in the whole corpus.",
    "start": "55810",
    "end": "62890"
  },
  {
    "text": "And for each\nposition we're going to try and predict what words\nsurround our center word.",
    "start": "62890",
    "end": "70090"
  },
  {
    "text": "And we're going to do that\nwith a probability distribution that's defined in terms of the\ndot product between the word",
    "start": "70090",
    "end": "78460"
  },
  {
    "text": "vectors for the center\nword and the context words. And so that will\ngive a probability",
    "start": "78460",
    "end": "84520"
  },
  {
    "text": "estimate of a word appearing\nin the context of into. Well actual words did\noccur in the context",
    "start": "84520",
    "end": "90730"
  },
  {
    "text": "of into on this occasion. So what we're\ngoing to want to do is sort of make it more likely\nthat turning problems, banking,",
    "start": "90730",
    "end": "98080"
  },
  {
    "text": "and crises will turn up\nin the context of into. And so that's learning,\nupdating the word vectors",
    "start": "98080",
    "end": "105320"
  },
  {
    "text": "so that they can predict actual\nsurrounding words better. And then the thing\nthat's almost magical",
    "start": "105320",
    "end": "112420"
  },
  {
    "text": "is that doing no more than\nthis simple algorithm, this allows us to\nlearn word vectors that",
    "start": "112420",
    "end": "118510"
  },
  {
    "text": "capture well word similarity\nand meaningful directions in a wordspace.",
    "start": "118510",
    "end": "124910"
  },
  {
    "text": "So more precisely, right,\nfor this model the only parameters of this model\nare the word vectors.",
    "start": "124910",
    "end": "132720"
  },
  {
    "start": "125000",
    "end": "243000"
  },
  {
    "text": "So we have outside word\nvectors and center word vectors for each word. And then we're taking\ntheir dot product",
    "start": "132720",
    "end": "140060"
  },
  {
    "text": "to get a probability--\nwell, we're taking a dot product\nto get a score of how likely a\nparticular outside word is",
    "start": "140060",
    "end": "147769"
  },
  {
    "text": "to occur with the center word. And then we're using the\nsoftmax transformation to convert those scores into\nprobabilities as I discussed",
    "start": "147770",
    "end": "155540"
  },
  {
    "text": "last time, and I kind of come\nback to at the end this time. A couple of things\nto note, this model",
    "start": "155540",
    "end": "163550"
  },
  {
    "text": "is what we call in NLP,\na bag of words model. So bag of words\nmodels are models",
    "start": "163550",
    "end": "169910"
  },
  {
    "text": "that don't actually pay\nany attention to word order or position. It doesn't matter if you're\nnext to the center word",
    "start": "169910",
    "end": "176090"
  },
  {
    "text": "or a bit further away\non the left or right. The probability estimate\nwould be the same.",
    "start": "176090",
    "end": "181910"
  },
  {
    "text": "And that seems like a very\ncrude model of language that will offend any linguist.",
    "start": "181910",
    "end": "187670"
  },
  {
    "text": "And it is a very crude\nmodel of language. And we'll move on to better\nmodels of language as we go on.",
    "start": "187670",
    "end": "192980"
  },
  {
    "text": "But even that crude\nmodel of language is enough to learn quite a\nlot about the probabilities--",
    "start": "192980",
    "end": "200262"
  },
  {
    "text": "sorry, quite a lot about\nthe properties of words. And then the second\nnote is well,",
    "start": "200262",
    "end": "206990"
  },
  {
    "text": "with this model we\nwanted to give reasonably high probabilities\nto the words that",
    "start": "206990",
    "end": "214040"
  },
  {
    "text": "do occur in the context\nof the center word, at least if they\ndo so at all often.",
    "start": "214040",
    "end": "219680"
  },
  {
    "text": "But obviously lots of\ndifferent words can occur. So we're not talking about\nprobabilities like 0.3 and 0.5.",
    "start": "219680",
    "end": "226850"
  },
  {
    "text": "We're more likely\ngoing to be talking about probabilities like\n0.01 and numbers like that.",
    "start": "226850",
    "end": "233400"
  },
  {
    "text": "Well, how do we achieve that? And well, the way that the\nword2vec model achieves this--",
    "start": "233400",
    "end": "239700"
  },
  {
    "text": "and this is the learning\nphase of the model-- is to place words that\nare similar in meaning",
    "start": "239700",
    "end": "246720"
  },
  {
    "start": "243000",
    "end": "479000"
  },
  {
    "text": "close to each other in this\nhigh dimensional vector space. So again, you can't\nread this one.",
    "start": "246720",
    "end": "253150"
  },
  {
    "text": "But if we scroll\ninto this one we see lots of words that are\nsimilar in meaning grouped",
    "start": "253150",
    "end": "260278"
  },
  {
    "text": "close together in the space. So here are days of the week\nlike Tuesday, Thursday, Sunday, and also Christmas over.",
    "start": "260279",
    "end": "269220"
  },
  {
    "text": "What else do we have, we\nhave Samsung and Nokia,",
    "start": "269220",
    "end": "274890"
  },
  {
    "text": "this is a diagram I made\nquite a few years ago. So that's when Nokia was\nstill an important maker",
    "start": "274890",
    "end": "281460"
  },
  {
    "text": "of cell phones. We have various sort of fields\nlike mathematics and economics over here.",
    "start": "281460",
    "end": "287340"
  },
  {
    "text": "So we group words that\nare similar in meaning. Actually one more note I wanted\nto make on this, I mean again,",
    "start": "287340",
    "end": "295150"
  },
  {
    "text": "this is a two\ndimensional picture, which is all I can\nshow you on a slide.",
    "start": "295150",
    "end": "300889"
  },
  {
    "text": "And it's done with the\nprincipal components projection that you also used\nin the assignment.",
    "start": "300890",
    "end": "307919"
  },
  {
    "text": "Something important to\nremember but hard to remember is that high dimensional spaces\nhave very different properties",
    "start": "307920",
    "end": "316230"
  },
  {
    "text": "to the two dimensional\nspaces that we can look at. And so in particular,\nword vector",
    "start": "316230",
    "end": "323430"
  },
  {
    "text": "can be close to\nmany other things in a high dimensional\nspace but close to them",
    "start": "323430",
    "end": "328590"
  },
  {
    "text": "on different dimensions. OK, so I've mentioned\ndoing learning.",
    "start": "328590",
    "end": "336199"
  },
  {
    "text": "So the next question\nis, well how do we learn good word vectors?",
    "start": "336200",
    "end": "342670"
  },
  {
    "text": "And this was the bit that\nI didn't quite hook up at the end of last class.",
    "start": "342670",
    "end": "348050"
  },
  {
    "text": "So for a while in the last\nI said oh gee, calculus. And we have to work out the\ngradient of the loss function",
    "start": "348050",
    "end": "355810"
  },
  {
    "text": "with respect to\nthe parameters that will allow us to make progress. But I didn't sort of\naltogether put that together.",
    "start": "355810",
    "end": "362210"
  },
  {
    "text": "So what we're going\nto do is we start off with random word vectors.",
    "start": "362210",
    "end": "368740"
  },
  {
    "text": "We initialize them\nto small numbers, near 0 in each dimension. We've defined our\nloss function J,",
    "start": "368740",
    "end": "376630"
  },
  {
    "text": "which we looked at last time. And then we're going to\nuse a gradient descent algorithm, which is an\niterative algorithm that",
    "start": "376630",
    "end": "385030"
  },
  {
    "text": "learns to maximize J of\ntheta by changing theta. And so the idea\nof this algorithm",
    "start": "385030",
    "end": "391449"
  },
  {
    "text": "is that from the\ncurrent values of theta you calculate the\ngradient j of theta.",
    "start": "391450",
    "end": "398320"
  },
  {
    "text": "And then what you're going\nto do is make a small step in the direction of\nthe negative gradient.",
    "start": "398320",
    "end": "404120"
  },
  {
    "text": "So the gradient is\npointing upwards. And we're taking a small\nstep in the direction",
    "start": "404120",
    "end": "409420"
  },
  {
    "text": "of the negative of the\ngradient to gradually move down towards the minimum.",
    "start": "409420",
    "end": "415070"
  },
  {
    "text": "And so one of the\nparameters of neural nets that you can fiddle in\nyour software package",
    "start": "415070",
    "end": "420550"
  },
  {
    "text": "is what is the step size. So if you take a really,\nreally itsy bitsy step,",
    "start": "420550",
    "end": "425710"
  },
  {
    "text": "it might take you a long time\nto minimize the function. You do a lot of\nwasted computation.",
    "start": "425710",
    "end": "433010"
  },
  {
    "text": "On the other hand, if your\nstep size is much too big, well",
    "start": "433010",
    "end": "438610"
  },
  {
    "text": "then you can actually\ndiverge and start going to worse places. Or even if you are going\ndownhill a little bit,",
    "start": "438610",
    "end": "446100"
  },
  {
    "text": "then what's going to\nhappen is you're then going to end up\nbouncing back and forth. And it'll take you much\nlonger to get to the minimum.",
    "start": "446100",
    "end": "453400"
  },
  {
    "text": "OK, in this picture I\nhave a beautiful quadratic",
    "start": "453400",
    "end": "458410"
  },
  {
    "text": "and it's easy to minimize it. Something that you might\nknow about neural networks is that in general\nthey're not convex.",
    "start": "458410",
    "end": "465610"
  },
  {
    "text": "So you could think that this\nis just all going to go awry. But the truth is in practice\nlife works out to be OK.",
    "start": "465610",
    "end": "472840"
  },
  {
    "text": "But I think I won't get\ninto that more right now and come back to\nthat in the later class.",
    "start": "472840",
    "end": "479300"
  },
  {
    "start": "479000",
    "end": "519000"
  },
  {
    "text": "So this is our gradient descent. So we have the current values\nof the parameters theta.",
    "start": "479300",
    "end": "484600"
  },
  {
    "text": "We then walk a little bit\nin the negative direction",
    "start": "484600",
    "end": "489850"
  },
  {
    "text": "of the gradient using\nour learning rate or step size alpha. And that gives us\nnew parameter values",
    "start": "489850",
    "end": "497230"
  },
  {
    "text": "where that means that\nthese are vectors but for each\nindividual parameter we're updating it a\nlittle bit by working out",
    "start": "497230",
    "end": "505660"
  },
  {
    "text": "the partial derivative of j\nwith respect to that parameter.",
    "start": "505660",
    "end": "512169"
  },
  {
    "text": "So that's the simple\ngradient descent algorithm. Nobody uses it and\nyou shouldn't use it.",
    "start": "512169",
    "end": "519219"
  },
  {
    "start": "519000",
    "end": "689000"
  },
  {
    "text": "The problem is that\nour j is a function of all windows in the corpus.",
    "start": "519220",
    "end": "525180"
  },
  {
    "text": "Remember we're doing this\nsum over every center word in the entire corpus.",
    "start": "525180",
    "end": "531600"
  },
  {
    "text": "And we'll often have billions\nof words in the corpus. So actually working\nout J of theta",
    "start": "531600",
    "end": "537660"
  },
  {
    "text": "or the gradient of J of theta\nwould be extremely, extremely expensive. Because we have to iterate\nover our entire corpus.",
    "start": "537660",
    "end": "544500"
  },
  {
    "text": "So you'd wait a very\nlong time before you made a single gradient update. And so optimization\nwould be extremely slow.",
    "start": "544500",
    "end": "551790"
  },
  {
    "text": "And so basically 100% of the\ntime in neural network land",
    "start": "551790",
    "end": "557009"
  },
  {
    "text": "we don't use gradient descent. We instead use what's called\nstochastic gradient descent.",
    "start": "557010",
    "end": "562350"
  },
  {
    "text": "And stochastic gradient descent\nis a very simple modification of this. So rather than working out\nan estimate of the gradient",
    "start": "562350",
    "end": "571860"
  },
  {
    "text": "based on the entire\ncorpus, you simply take one center word or a small\nbatch like 32 center words",
    "start": "571860",
    "end": "580350"
  },
  {
    "text": "and you work out an estimate\nof the gradient based on them. Now that estimate\nof the gradient",
    "start": "580350",
    "end": "586320"
  },
  {
    "text": "will be noisy and bad\nbecause you've only looked at a small fraction\nof the corpus rather",
    "start": "586320",
    "end": "592710"
  },
  {
    "text": "than the whole corpus. But nevertheless, you can use\nthat estimate of the gradient to update your theta parameters\nin exactly the same way.",
    "start": "592710",
    "end": "601750"
  },
  {
    "text": "And so this is the\nalgorithm that we can do. And so then if we have\na billion word corpus",
    "start": "601750",
    "end": "608550"
  },
  {
    "text": "we can if we do it\non each center word. We can make a billion\nupdates to the parameters",
    "start": "608550",
    "end": "614720"
  },
  {
    "text": "we pass through the corpus\nonce rather than only making one more accurate\nupdate to the parameters",
    "start": "614720",
    "end": "623440"
  },
  {
    "text": "once you've been\nthrough the corpus. So overall, we can learn\nseveral orders of magnitude",
    "start": "623440",
    "end": "629850"
  },
  {
    "text": "more quickly. And so this is the\nalgorithm that you'll be using everywhere, including\nright from the beginning,",
    "start": "629850",
    "end": "638400"
  },
  {
    "text": "from our assignments.  Again, just an extra\ncomment of more complicated",
    "start": "638400",
    "end": "645830"
  },
  {
    "text": "stuff we'll come back to. [AUDIO OUT] the gradient descent\nis a sort of performance hack",
    "start": "645830",
    "end": "656345"
  },
  {
    "text": "that lets you learn\nmuch more quickly. It turns out it's not\nonly a performance hack. Neural nets have some quite\ncounter intuitive properties.",
    "start": "656345",
    "end": "665540"
  },
  {
    "text": "And actually the fact that\nstochastic gradient descent is kind of noisy and bounces\naround as it does its thing.",
    "start": "665540",
    "end": "673580"
  },
  {
    "text": "It actually means that\nin complex networks it learns better\nsolutions than if you",
    "start": "673580",
    "end": "680630"
  },
  {
    "text": "were to run plain gradient\ndescent very slowly. So you can both compute\nmuch more quickly",
    "start": "680630",
    "end": "686839"
  },
  {
    "text": "and do a better job.  OK, one final note on running\nstochastic gradients with word",
    "start": "686840",
    "end": "694070"
  },
  {
    "start": "689000",
    "end": "826000"
  },
  {
    "text": "vectors, this is an aside. But something to\nnote is that if we're",
    "start": "694070",
    "end": "699140"
  },
  {
    "text": "doing a stochastic gradient\nupdate based on one window, then actually in\nthat window we'll",
    "start": "699140",
    "end": "706400"
  },
  {
    "text": "have seen almost none\nof our parameters. Because if we have a window\nof something like five words",
    "start": "706400",
    "end": "712430"
  },
  {
    "text": "to either side of\nthe center word, we've seen at most 11\ndistinct word types.",
    "start": "712430",
    "end": "718340"
  },
  {
    "text": "So we will have gradient\ninformation for those 11 words but the other 100,000 odd\nwords in our vocabulary",
    "start": "718340",
    "end": "726290"
  },
  {
    "text": "will have no gradient\nupdate information. So this will be a very,\nvery sparse gradient update.",
    "start": "726290",
    "end": "733730"
  },
  {
    "text": "So if you're only\nthinking math, you can just have your\nentire gradient",
    "start": "733730",
    "end": "741500"
  },
  {
    "text": "and use the equation\nthat I showed before. But if you're thinking\nsystems optimization,",
    "start": "741500",
    "end": "748880"
  },
  {
    "text": "then you'd want to\nthink, well actually I only want to update the\nparameters for a few words.",
    "start": "748880",
    "end": "756680"
  },
  {
    "text": "And there have to be and there\nare much more efficient ways that I could do that.",
    "start": "756680",
    "end": "763470"
  },
  {
    "text": "And so this is\nanother aside but it will be useful for your\nassignment so I will say it.",
    "start": "763470",
    "end": "770630"
  },
  {
    "text": "Up until now when I\npresented word vectors, I presented them\nas column vectors.",
    "start": "770630",
    "end": "777590"
  },
  {
    "text": "And that makes the most\nsense if you think about it as a piece of math.",
    "start": "777590",
    "end": "783500"
  },
  {
    "text": "Whereas actually in all\ncommon deep learning packages including\nPyTorch that we're using,",
    "start": "783500",
    "end": "791600"
  },
  {
    "text": "word vectors are actually\nrepresented as row vectors. And if you remember back to\nthe representation of matrices",
    "start": "791600",
    "end": "800000"
  },
  {
    "text": "in CS107 or something\nlike that, that you'll know that that's then obviously\nefficient for representing",
    "start": "800000",
    "end": "808280"
  },
  {
    "text": "words. Because then you can access\nan entire word vector as a continuous range of memory,\ndifferent if you're in Fortran.",
    "start": "808280",
    "end": "816410"
  },
  {
    "text": "Anyway, so actually\nour word vectors will be row vectors when you\nlook at those inside PyTorch.",
    "start": "816410",
    "end": "826850"
  },
  {
    "start": "826000",
    "end": "1013000"
  },
  {
    "text": "OK, now I wanted\nto say a bit more about the word2vec\nalgorithm family.",
    "start": "826850",
    "end": "833420"
  },
  {
    "text": "And also what you're\ngoing to do in homework 2.",
    "start": "833420",
    "end": "838670"
  },
  {
    "text": "So if you're still meant to be\nworking on homework 1, which remember is due\nnext Tuesday, that",
    "start": "838670",
    "end": "844100"
  },
  {
    "text": "really actually\nwith today's content we're starting into homework 2. And I'll kind of go through the\nfirst part of homework 2 today",
    "start": "844100",
    "end": "851990"
  },
  {
    "text": "and the other stuff you\nneed to know for homework 2. So I mentioned briefly\nthe idea that we",
    "start": "851990",
    "end": "857839"
  },
  {
    "text": "have two separate vectors\nfor each word type. The center vector and\nthe outside vectors.",
    "start": "857840",
    "end": "864980"
  },
  {
    "text": "And we just average\nthem both at the end. They're similar but not\nidentical for multiple reasons,",
    "start": "864980",
    "end": "870390"
  },
  {
    "text": "including the random\ninitialization and the stochastic\ngradient descent.",
    "start": "870390",
    "end": "875870"
  },
  {
    "text": "You can implement a\nword2vec algorithm with just one vector per word.",
    "start": "875870",
    "end": "882290"
  },
  {
    "text": "And actually, if you do\nit works slightly better. But it makes the algorithm\nmuch more complicated.",
    "start": "882290",
    "end": "889380"
  },
  {
    "text": "And the reason for\nthat is sometimes you will have the same word type as\nthe center word and the context",
    "start": "889380",
    "end": "897769"
  },
  {
    "text": "word. And that means that when\nyou're doing your calculus at that point, you've then\ngot this sort of messy case",
    "start": "897770",
    "end": "905600"
  },
  {
    "text": "that just for that word\nyou're getting a dot product--",
    "start": "905600",
    "end": "911045"
  },
  {
    "text": "you're getting a\ndot product of x dot x term, which makes it sort\nof much messier to work out.",
    "start": "911045",
    "end": "916200"
  },
  {
    "text": "And so that's why we use\nthis simple optimization of having two vectors per word.",
    "start": "916200",
    "end": "921420"
  },
  {
    "text": "OK, so for the word2vec model as\nintroduced in the Mikolov et al",
    "start": "921420",
    "end": "929329"
  },
  {
    "text": "paper in 2013, it wasn't\nreally just one algorithm.",
    "start": "929330",
    "end": "936050"
  },
  {
    "text": "It was a family of algorithms. So there were two\nbasic model variants.",
    "start": "936050",
    "end": "941750"
  },
  {
    "text": "One was called the\nskip-gram model, which is the one that\nI've explained to you. That [AUDIO OUT] outside\nwords position independent",
    "start": "941750",
    "end": "953220"
  },
  {
    "text": "given the center word in a\nbag of words style model. The other one was called the\nContinuous Bag of Words model,",
    "start": "953220",
    "end": "960300"
  },
  {
    "text": "CBOW. And in this one you\npredict the center word from a bag of context words.",
    "start": "960300",
    "end": "967480"
  },
  {
    "text": "Both of these give\nsimilar results. The skip-gram one is more\nnatural in various ways.",
    "start": "967480",
    "end": "973329"
  },
  {
    "text": "So it's sort of normally the\none that people have gravitated to in subsequent work.",
    "start": "973330",
    "end": "979470"
  },
  {
    "text": "But then as to how\nyou train this model. What I've presented so far is\nthe naive softmax equation,",
    "start": "979470",
    "end": "988240"
  },
  {
    "text": "which is a simple but relatively\nexpensive training method.",
    "start": "988240",
    "end": "993600"
  },
  {
    "text": "And so that isn't\nreally what they suggest using in the paper.",
    "start": "993600",
    "end": "999000"
  },
  {
    "text": "They suggest using\na method that's called negative sampling. So an acronym\nyou'll see sometimes",
    "start": "999000",
    "end": "1004220"
  },
  {
    "text": "is SGNS, which means\nskip-grams negative sampling. So let me just say a little\nbit about what this is.",
    "start": "1004220",
    "end": "1013700"
  },
  {
    "start": "1013000",
    "end": "1086000"
  },
  {
    "text": "But actually, doing\nthe skip-gram model with negative sampling is\nthe part of homework 2.",
    "start": "1013700",
    "end": "1019550"
  },
  {
    "text": "So you'll get to\nknow this model well. So the point is that if\nyou use this naive softmax",
    "start": "1019550",
    "end": "1025880"
  },
  {
    "text": "even though people commonly\ndo use this naive softmax in various neural net models.",
    "start": "1025880",
    "end": "1031250"
  },
  {
    "text": "That working out the\ndenominator is pretty expensive. And that's because you\nhave to iterate over",
    "start": "1031250",
    "end": "1037970"
  },
  {
    "text": "every word in the vocabulary\nand work out these dot products. So if you have a\n100,000 word vocabulary,",
    "start": "1037970",
    "end": "1047250"
  },
  {
    "text": "you have to do\n100,000 dot products to work out the denominator. And that seems a\nlittle bit of a shame.",
    "start": "1047250",
    "end": "1054679"
  },
  {
    "text": "And so instead of that, the\nidea of negative sampling is instead of using\nthis softmax we're",
    "start": "1054680",
    "end": "1063080"
  },
  {
    "text": "going to train binary logistic\nregression models for both the",
    "start": "1063080",
    "end": "1071000"
  },
  {
    "text": "the true pair of center word and\nthe context word versus noise",
    "start": "1071000",
    "end": "1078710"
  },
  {
    "text": "pairs where we keep the\ntrue center word and we just randomly sample words\nfrom the vocabulary.",
    "start": "1078710",
    "end": "1086730"
  },
  {
    "start": "1086000",
    "end": "1215000"
  },
  {
    "text": "So as presented in the\npaper, the idea is like this. So overall what we\nwant to optimize",
    "start": "1086730",
    "end": "1093770"
  },
  {
    "text": "is still an average of the\nloss for each particular center",
    "start": "1093770",
    "end": "1100220"
  },
  {
    "text": "word. But for when we're\nworking out the loss for each particular center\nword, we're going to work out--",
    "start": "1100220",
    "end": "1107860"
  },
  {
    "text": "sorry, the loss for\neach particular center word in each particular\nwindow, we're going to take the dot product\nas before of the center",
    "start": "1107860",
    "end": "1115970"
  },
  {
    "text": "word and the outside word. And that's sort of\nthe main quantity.",
    "start": "1115970",
    "end": "1122000"
  },
  {
    "text": "But now instead of using\nthat inside the softmax, we're going to put it through\nthe logistic function, which",
    "start": "1122000",
    "end": "1128720"
  },
  {
    "text": "is sometimes often also\ncalled the sigmoid function, the name logistic\nis more precise",
    "start": "1128720",
    "end": "1133970"
  },
  {
    "text": "so that's this function here. So the logistic function\nis a handy function that will map any real number\nto a probability between 0 and 1",
    "start": "1133970",
    "end": "1143960"
  },
  {
    "text": "open interval. So basically if the\ndot product is large,",
    "start": "1143960",
    "end": "1149250"
  },
  {
    "text": "the logistic of the dot\nproduct will be virtually 1. OK, so we want this to be large.",
    "start": "1149250",
    "end": "1156440"
  },
  {
    "text": "And then what we'd\nlike is on average we'd like the dot product between\nthe center word and words",
    "start": "1156440",
    "end": "1163550"
  },
  {
    "text": "that we just chose\nrandomly, i.e. they most likely didn't\nactually occur in the context",
    "start": "1163550",
    "end": "1169730"
  },
  {
    "text": "of the center word to be small. And there's just\none little trick",
    "start": "1169730",
    "end": "1175880"
  },
  {
    "text": "of how this is done, which\nis this sigmoid function is symmetric and so if we want\nthis probability to be small,",
    "start": "1175880",
    "end": "1188660"
  },
  {
    "text": "we can take the negative\nof the dot product. So we're wanting\nit to be over here.",
    "start": "1188660",
    "end": "1194060"
  },
  {
    "text": "The dot product of a random\nword and the center word is a negative number.",
    "start": "1194060",
    "end": "1200850"
  },
  {
    "text": "And so then we're going to\ntake the negation of that and then again once we put\nthat through the sigmoid,",
    "start": "1200850",
    "end": "1206990"
  },
  {
    "text": "we'd like a big number. OK, so the way they're\npresenting things, they're actually\nmaximizing this quantity.",
    "start": "1206990",
    "end": "1214280"
  },
  {
    "text": "But if I go back to making it\na bit more similar to the way we had written\nthings, we'd worked",
    "start": "1214280",
    "end": "1221090"
  },
  {
    "start": "1215000",
    "end": "1407000"
  },
  {
    "text": "with minimizing the\nnegative log likelihood. So it looks like this.",
    "start": "1221090",
    "end": "1228420"
  },
  {
    "text": "So we're taking the negative\nlog likelihood of the sigmoid",
    "start": "1228420",
    "end": "1233900"
  },
  {
    "text": "of the dot product. Again, negative log\nlikelihood, we're using the same negated dot\nproduct through the sigmoid.",
    "start": "1233900",
    "end": "1242690"
  },
  {
    "text": "And then we're going to\nwork out this quantity for a handful of random words.",
    "start": "1242690",
    "end": "1250520"
  },
  {
    "text": "We k-negative samples and\nhow likely they are to sample a word depends on\ntheir probability.",
    "start": "1250520",
    "end": "1257419"
  },
  {
    "text": "And where this loss\nfunction is going to be minimized given this\nnegation by making these dot",
    "start": "1257420",
    "end": "1265170"
  },
  {
    "text": "products large, and these dot\nproducts small means negative.",
    "start": "1265170",
    "end": "1271010"
  },
  {
    "text": " So there's just then one\nother trick that they use.",
    "start": "1271010",
    "end": "1279730"
  },
  {
    "text": "Actually there's more\nthan one other trick that's used in\nthe word2vec paper to get it to perform well.",
    "start": "1279730",
    "end": "1284800"
  },
  {
    "text": "But I'll only mention one\nof their other tricks here. When they sample the\nwords, they don't simply",
    "start": "1284800",
    "end": "1291580"
  },
  {
    "text": "just sample the words based on\ntheir probability of occurrence",
    "start": "1291580",
    "end": "1297130"
  },
  {
    "text": "in the corpus or uniformly. What they do is they start\nwith what we call the unigram",
    "start": "1297130",
    "end": "1302290"
  },
  {
    "text": "distribution of words. So that is how\noften words actually",
    "start": "1302290",
    "end": "1307659"
  },
  {
    "text": "occur in our big corpus. So if you have a\nbillion word corpus and a particular word\noccurred 90 times in it,",
    "start": "1307660",
    "end": "1315610"
  },
  {
    "text": "you're taking 90\ndivided by a billion. And so that's the unigram\nprobability of the word.",
    "start": "1315610",
    "end": "1321160"
  },
  {
    "text": "But what they then do\nis that they take that to the three-quarters power. And the effect of that\nthree-quarters power,",
    "start": "1321160",
    "end": "1328390"
  },
  {
    "text": "which is then renormalized\nto make a probability distribution with z like we saw\nthe last time with the softmax.",
    "start": "1328390",
    "end": "1335169"
  },
  {
    "text": "By taking the\nthree-quarters power, that has the effect of\ndampening the difference",
    "start": "1335170",
    "end": "1341440"
  },
  {
    "text": "between common and rare words. So that less frequent words are\nsampled somewhat more often,",
    "start": "1341440",
    "end": "1348110"
  },
  {
    "text": "but still not nearly as much\nas they would be if you just use something like a\nuniform distribution",
    "start": "1348110",
    "end": "1354280"
  },
  {
    "text": "over the vocabulary. OK, so that's\nbasically everything",
    "start": "1354280",
    "end": "1361810"
  },
  {
    "text": "to say about the basics\nof how we have this very",
    "start": "1361810",
    "end": "1367600"
  },
  {
    "text": "simple neural network\nalgorithm word2vec and how we can train it\nand learn word vectors.",
    "start": "1367600",
    "end": "1376620"
  },
  {
    "text": "So for the next bit what I\nwant to do is step back a bit and say, well,\nhere's an algorithm",
    "start": "1376620",
    "end": "1382210"
  },
  {
    "text": "that I've shown you\nthat works great. What else could we have done.",
    "start": "1382210",
    "end": "1388030"
  },
  {
    "text": "And what can we say about that. And the first thing that\nyou might think about",
    "start": "1388030",
    "end": "1393610"
  },
  {
    "text": "is, well here's this\nfunny iterative algorithm to give you word vectors.",
    "start": "1393610",
    "end": "1402399"
  },
  {
    "text": "If we have a lot of\nwords in a corpus, seems like a more obvious\nthing that we could do",
    "start": "1402400",
    "end": "1409480"
  },
  {
    "start": "1407000",
    "end": "1519000"
  },
  {
    "text": "is just look at the counts of\nhow words occur with each other",
    "start": "1409480",
    "end": "1415299"
  },
  {
    "text": "and build a matrix of counts,\na co-occurrence matrix.",
    "start": "1415300",
    "end": "1420830"
  },
  {
    "text": "So here's the idea of\na co-occurrence matrix. So I've got a teeny\nlittle corpus.",
    "start": "1420830",
    "end": "1426220"
  },
  {
    "text": "I like deep learning. I like NLP, I enjoy flying. And I can define a window size.",
    "start": "1426220",
    "end": "1432890"
  },
  {
    "text": "I made my window\nsimply size one to make it easy to fill in\nmy matrix symmetric",
    "start": "1432890",
    "end": "1440200"
  },
  {
    "text": "just like our\nword2vec algorithm. And so then the\ncounts in these cells",
    "start": "1440200",
    "end": "1447700"
  },
  {
    "text": "are simply how often\nthings co-occur in the window of size 1. So \"I like\" occurs twice.",
    "start": "1447700",
    "end": "1456010"
  },
  {
    "text": "So we get twos in these\ncells because it's symmetric. \"Deep learning\" occurs\nonce so we get 1 here.",
    "start": "1456010",
    "end": "1463870"
  },
  {
    "text": "And lots of other\nthings occur 0. So we can build up a\nco-occurrence matrix like this.",
    "start": "1463870",
    "end": "1471130"
  },
  {
    "text": "And well, these actually\ngive us a representation of words as\nco-occurrence vectors.",
    "start": "1471130",
    "end": "1478510"
  },
  {
    "text": "So I can take the word \"I\" with\neither a row or a column vector since it's symmetric and say OK,\nmy representation of the word",
    "start": "1478510",
    "end": "1487299"
  },
  {
    "text": "\"I\" is this row vector. And that is a representation\nof the word \"I\".",
    "start": "1487300",
    "end": "1493750"
  },
  {
    "text": "And I think you can\nmaybe convince yourself that to the extent that words\nhave similar meaning and usage,",
    "start": "1493750",
    "end": "1502180"
  },
  {
    "text": "you'd sort of expect them to\nhave somewhat similar vectors, right? So if I had the word \"you\"\nas well on a larger corpus,",
    "start": "1502180",
    "end": "1509260"
  },
  {
    "text": "you might expect \"I\" and\n\"you\" to have similar vectors. Because I like, you\nlike, I enjoy, you enjoy.",
    "start": "1509260",
    "end": "1515710"
  },
  {
    "text": "You'd see the same\nkinds of possibilities. Hey Chris? Yeah? Do you think you can\nanswer some questions?",
    "start": "1515710",
    "end": "1522242"
  },
  {
    "start": "1519000",
    "end": "1789000"
  },
  {
    "text": "Sure. All right. So we've got some questions from\nsort of the negative sampling",
    "start": "1522242",
    "end": "1527500"
  },
  {
    "text": "slides. In particular can you\ngive some intuition",
    "start": "1527500",
    "end": "1533933"
  },
  {
    "text": "for negative sampling? What is the negative\nsampling doing? And why do we only take\none positive example?",
    "start": "1533933",
    "end": "1540170"
  },
  {
    "text": "These are two questions\nif you can answer. OK. That's a good question. OK, I'll try and\ngive more intuition.",
    "start": "1540170",
    "end": "1546730"
  },
  {
    "text": "So one is to work out something\nlike what the softmax did",
    "start": "1546730",
    "end": "1554919"
  },
  {
    "text": "in a much more efficient way.",
    "start": "1554920",
    "end": "1560360"
  },
  {
    "text": "So in the softmax\nwell, you wanted to give high probability\nin predicting",
    "start": "1560360",
    "end": "1569860"
  },
  {
    "text": "a context word that actually\ndid appear with the center word. And well, the way you\ndo that is by having",
    "start": "1569860",
    "end": "1577510"
  },
  {
    "text": "the dot product\nbetween those two words be as big as possible. And part of how--",
    "start": "1577510",
    "end": "1585159"
  },
  {
    "text": "you're going to be sort of-- it's more than that\nbecause in the denominator we were also working out the dot\nproduct with every other word",
    "start": "1585160",
    "end": "1593050"
  },
  {
    "text": "in the vocabulary. So as well as wanting the dot\nproduct with the actual word that you see in the\ncontext to be big.",
    "start": "1593050",
    "end": "1600490"
  },
  {
    "text": "You maximize your\nlikelihood by making the dot products of other words\nthat weren't in the context",
    "start": "1600490",
    "end": "1609070"
  },
  {
    "text": "smaller. Because that's shrinking your\ndenominator and therefore",
    "start": "1609070",
    "end": "1614290"
  },
  {
    "text": "you've got a bigger\nnumber coming out and you're maximizing the loss. So even for the softmax,\nthe general thing",
    "start": "1614290",
    "end": "1621100"
  },
  {
    "text": "that you want to do\nto maximize that is have a dot product with words\nactually in the context big.",
    "start": "1621100",
    "end": "1628330"
  },
  {
    "text": "Dot products with words\nnot in the context be small to the extent possible.",
    "start": "1628330",
    "end": "1634030"
  },
  {
    "text": "And obviously you\nhave to average this as best you can over all\nkinds of different contexts. Because sometimes\ndifferent words",
    "start": "1634030",
    "end": "1639700"
  },
  {
    "text": "appear in different\ncontexts obviously. So the negative sampling is\na way of therefore trying",
    "start": "1639700",
    "end": "1649690"
  },
  {
    "text": "to maximize the same objective. Now, you only have one positive\nterm because you're actually",
    "start": "1649690",
    "end": "1660250"
  },
  {
    "text": "wanting to use the actual data. So you're not wanting\nto invent data. So for working out\nthe entire J, we",
    "start": "1660250",
    "end": "1667840"
  },
  {
    "text": "do work this quantity\nout for every center word and every context word.",
    "start": "1667840",
    "end": "1674299"
  },
  {
    "text": "So we are iterating over the\ndifferent words in the context window. And then we're moving through\npositions in the corpus.",
    "start": "1674300",
    "end": "1681680"
  },
  {
    "text": "So we're doing different\nVCs, so gradually we do this. But for one particular center\nword and one particular context",
    "start": "1681680",
    "end": "1688870"
  },
  {
    "text": "word, we only have one real\npiece of data that's positive. So that's all we\nuse because we don't",
    "start": "1688870",
    "end": "1694779"
  },
  {
    "text": "know what other words should\nbe counted as positive words.",
    "start": "1694780",
    "end": "1700000"
  },
  {
    "text": "Now for the negative\nwords you could just sample one negative word and\nthat would probably work.",
    "start": "1700000",
    "end": "1709340"
  },
  {
    "text": "But if you want a sort of a\nslightly better more stable sense of, OK we'd like to\nin general have other words,",
    "start": "1709340",
    "end": "1718210"
  },
  {
    "text": "have low probability. That seems like you might\nbe able to get better more stable results if you instead\nsay let's have 10 or 15 sample",
    "start": "1718210",
    "end": "1727390"
  },
  {
    "text": "negative words. And indeed that's\nbeen found to be true. And for the negative\nwords, well, it's",
    "start": "1727390",
    "end": "1734049"
  },
  {
    "text": "easy to sample any number\nof random words you want. And at that point it's kind\nof a probabilistic argument.",
    "start": "1734050",
    "end": "1739930"
  },
  {
    "text": "The words that you're sampling\nmight not be actually bad words to appear in the context.",
    "start": "1739930",
    "end": "1746380"
  },
  {
    "text": "They might actually be other\nwords that are in the context. But 99.9% of the\ntime they will be",
    "start": "1746380",
    "end": "1752320"
  },
  {
    "text": "unlikely words to\noccur in the context. And so they're good ones to use.",
    "start": "1752320",
    "end": "1757480"
  },
  {
    "text": "And yes you only sample\n10 or 15 of them. But that's enough\nto make progress.",
    "start": "1757480",
    "end": "1763870"
  },
  {
    "text": "Because the center word is going\nto turn up on other occasions. And when it does, you'll\nsample different words over",
    "start": "1763870",
    "end": "1771190"
  },
  {
    "text": "here so that you\ngradually sample different parts of the\nspace and start to learn. We had this co-occurrence\nmatrix and it",
    "start": "1771190",
    "end": "1779860"
  },
  {
    "text": "gives a representation of\nwords as co-occurrence vectors.",
    "start": "1779860",
    "end": "1786400"
  },
  {
    "text": "And just one more\nnote on that, I mean there are actually two ways\nthat people have commonly made",
    "start": "1786400",
    "end": "1792430"
  },
  {
    "start": "1789000",
    "end": "1939000"
  },
  {
    "text": "these co-occurrence matrices. One corresponds to what\nwe've seen already, that you use a window\naround the word, which",
    "start": "1792430",
    "end": "1800260"
  },
  {
    "text": "is similar to word2vec. And that allows you to\ncapture some locality and some of the sort of\nsyntactic and semantic",
    "start": "1800260",
    "end": "1807700"
  },
  {
    "text": "proximity that's\nmore fine grained. The other way these co-occurence\nmatrices have been made",
    "start": "1807700",
    "end": "1817360"
  },
  {
    "text": "is that normally documents\nhave some structure, whether it's paragraphs or\njust the actual web pages",
    "start": "1817360",
    "end": "1823750"
  },
  {
    "text": "sort of size documents. So you can just make your\nwindow size a paragraph or a whole web page and\ncount co-occurrence in those.",
    "start": "1823750",
    "end": "1832149"
  },
  {
    "text": "And this is the kind\nof method that's often been used in\ninformation retrieval, in methods like latent\nsemantic analysis.",
    "start": "1832150",
    "end": "1840750"
  },
  {
    "text": "OK, so the question then is\nare these kind of count word",
    "start": "1840750",
    "end": "1846720"
  },
  {
    "text": "vectors good things to use? Well, people have used them.",
    "start": "1846720",
    "end": "1853450"
  },
  {
    "text": "They're not terrible. But they have certain problems. The kind of problems\nthat they have,",
    "start": "1853450",
    "end": "1859660"
  },
  {
    "text": "well, firstly they're\nhuge though very sparse. So this is back\nwhere I said before,",
    "start": "1859660",
    "end": "1865130"
  },
  {
    "text": "if we had a vocabulary\nof half a million words and then we have half a\nmillion dimensional vector",
    "start": "1865130",
    "end": "1871870"
  },
  {
    "text": "for each word, which\nis much, much bigger than the word vectors\nthat we typically use.",
    "start": "1871870",
    "end": "1879850"
  },
  {
    "text": "And it also means\nthat because we have these very high\ndimensional vectors",
    "start": "1879850",
    "end": "1886059"
  },
  {
    "text": "that we have a lot of sparsity\nand a lot of randomness. So the results that you get tend\nto be noisier and less robust",
    "start": "1886060",
    "end": "1894880"
  },
  {
    "text": "depending on what particular\nstuff was in the corpus. And so in general\npeople have found",
    "start": "1894880",
    "end": "1901360"
  },
  {
    "text": "that you can get much\nbetter results by working with low dimensional vectors.",
    "start": "1901360",
    "end": "1906370"
  },
  {
    "text": "So then the idea is\nwe can store most of the important information\nabout the distribution of words",
    "start": "1906370",
    "end": "1913990"
  },
  {
    "text": "and the context of other\nwords in a fixed small number of dimensions, giving\na dense vector.",
    "start": "1913990",
    "end": "1920980"
  },
  {
    "text": "And in practice\nthe dimensionality of the vectors that\nare used are normally somewhere between 25 and 1,000.",
    "start": "1920980",
    "end": "1928210"
  },
  {
    "text": "And so at that point,\nwe need to use some way to reduce the dimensionality\nof our count co-occurrence",
    "start": "1928210",
    "end": "1936544"
  },
  {
    "text": "vectors.  So if you have a good memory\nfrom a linear algebra class,",
    "start": "1936545",
    "end": "1946250"
  },
  {
    "start": "1939000",
    "end": "2177000"
  },
  {
    "text": "you hopefully saw singular\nvalue decomposition. And it has various\nmathematical properties",
    "start": "1946250",
    "end": "1953960"
  },
  {
    "text": "that I'm not going to talk\nabout here of singular value projection giving you an optimal\nway under a certain definition",
    "start": "1953960",
    "end": "1962660"
  },
  {
    "text": "of optimality of producing\na reduced dimensionality matrix that maximally, or\nsorry, a pair of matrices",
    "start": "1962660",
    "end": "1972410"
  },
  {
    "text": "that maximally well lets you\nrecover the original matrix. But the idea of the\nsingular value decomposition",
    "start": "1972410",
    "end": "1979460"
  },
  {
    "text": "is you can take any matrix\nsuch as our count matrix. And you can decompose\nthat into three matrices.",
    "start": "1979460",
    "end": "1991010"
  },
  {
    "text": "U, a diagonal matrix sigma,\nand a V transpose matrix.",
    "start": "1991010",
    "end": "1997720"
  },
  {
    "text": "And this works for any shape. Now in these matrices, some\nparts of it are never used.",
    "start": "1997720",
    "end": "2005549"
  },
  {
    "text": "Because since this\nmatrix is rectangular there's nothing over here. And so this part of the V\ntranspose matrix gets ignored.",
    "start": "2005550",
    "end": "2015360"
  },
  {
    "text": "But if you're wanting to\nget smaller dimensional representations, what you do\nis take advantage of the fact",
    "start": "2015360",
    "end": "2023760"
  },
  {
    "text": "that the singular values inside\nthe diagonal sigma matrix",
    "start": "2023760",
    "end": "2029310"
  },
  {
    "text": "are ordered from largest\ndown to smallest. So what we can do is just\ndelete out more of the matrix.",
    "start": "2029310",
    "end": "2038789"
  },
  {
    "text": "Delete out some singular\nvalues which effectively means that in this product,\nsome of U and some of V",
    "start": "2038790",
    "end": "2047639"
  },
  {
    "text": "is also not used. And so then as a\nresult of that, we're getting lower dimensional\nrepresentations for our words,",
    "start": "2047640",
    "end": "2058388"
  },
  {
    "text": "if we're wanting to\nhave word vectors. Which still do as\ngood as possible a job within the\ngiven dimensionality",
    "start": "2058389",
    "end": "2066899"
  },
  {
    "text": "of enabling you to recover the\noriginal co-occurrence matrix.",
    "start": "2066900",
    "end": "2073899"
  },
  {
    "text": "So from a linear\nalgebra background, this is the obvious\nthing to use.",
    "start": "2073900",
    "end": "2080330"
  },
  {
    "text": "So how does that work? Well, if you just build a raw\ncount co-occurrence matrix",
    "start": "2080330",
    "end": "2088750"
  },
  {
    "text": "and run SVD on that and try\nand use those as word vectors,",
    "start": "2088750",
    "end": "2093790"
  },
  {
    "text": "it actually works poorly. And it works poorly\nbecause if you",
    "start": "2093790",
    "end": "2100030"
  },
  {
    "text": "get into the mathematical\nassumptions of SVD, you're expecting to have these\nnormally distributed errors.",
    "start": "2100030",
    "end": "2107230"
  },
  {
    "text": "And what you're getting\nwith word counts looked not at all like\nsomeone's normal [INAUDIBLE]",
    "start": "2107230",
    "end": "2116500"
  },
  {
    "text": "because you have exceedingly\ncommon words like \"a,\" \"the,\" and \"and\". And you have a very large\nnumber of rare words.",
    "start": "2116500",
    "end": "2124060"
  },
  {
    "text": "So that doesn't work very well. But you can actually\nget something that works a lot better if you\nscale the counts in the cells.",
    "start": "2124060",
    "end": "2132490"
  },
  {
    "text": "So to deal with this problem\nof extremely frequent words, there are some things we can do.",
    "start": "2132490",
    "end": "2137680"
  },
  {
    "text": "We could just take the\nlog of the raw counts. We could kind of cap\nthe maximum count.",
    "start": "2137680",
    "end": "2144550"
  },
  {
    "text": "We could throw away\nthe function words. And any of these ideas\nthat you build then",
    "start": "2144550",
    "end": "2151090"
  },
  {
    "text": "have an co-occurrence matrix\nthat you get more useful word vectors from running\nsomething like SVD.",
    "start": "2151090",
    "end": "2158440"
  },
  {
    "text": "And indeed these kind of models\nwere explored in the 1990s",
    "start": "2158440",
    "end": "2163690"
  },
  {
    "text": "and in the 2000s. And in particular, Doug\nRohde explored a number",
    "start": "2163690",
    "end": "2169960"
  },
  {
    "text": "of these ideas as\nto how to improve the co-occurrence\nmatrix in a model that he built that\nwas called COALS.",
    "start": "2169960",
    "end": "2177910"
  },
  {
    "start": "2177000",
    "end": "2273000"
  },
  {
    "text": "And actually in his COALS\nmodel, he observed the fact",
    "start": "2177910",
    "end": "2184660"
  },
  {
    "text": "that you could get the same\nkind of linear components",
    "start": "2184660",
    "end": "2190869"
  },
  {
    "text": "that our semantic components\nthat we saw yesterday when talking about analogies.",
    "start": "2190870",
    "end": "2197320"
  },
  {
    "text": "So for example, this is\na figure from his paper and you can see that we seem to\nhave a meaning component going",
    "start": "2197320",
    "end": "2206230"
  },
  {
    "text": "from a verb to the\nperson who does the verb. So drive to driver, swim to\nswimmer, teach to teacher,",
    "start": "2206230",
    "end": "2213760"
  },
  {
    "text": "marry to priest. And that these vector\ncomponents are not perfectly",
    "start": "2213760",
    "end": "2219520"
  },
  {
    "text": "but are roughly parallel\nand roughly the same size.",
    "start": "2219520",
    "end": "2224590"
  },
  {
    "text": "And so we have a\nmeaning component there that we could\nadd on to another word",
    "start": "2224590",
    "end": "2229990"
  },
  {
    "text": "just like we did for\npreviously, for analogies. We could say drive is to\ndriver as marry is to what,",
    "start": "2229990",
    "end": "2237850"
  },
  {
    "text": "and we add on this screen\nvector component, which is roughly the same as this one.",
    "start": "2237850",
    "end": "2243010"
  },
  {
    "text": "And we'd say oh, priest. So that this space could\nactually get some word",
    "start": "2243010",
    "end": "2249160"
  },
  {
    "text": "vectors analogies right as well. And so that seemed\nreally interesting to us",
    "start": "2249160",
    "end": "2256359"
  },
  {
    "text": "around the time\nword2vec came out, of wanting to understand better\nwhat the iterative updating",
    "start": "2256360",
    "end": "2262690"
  },
  {
    "text": "algorithm of word2vec did. And how it related to these\nmore linear algebra based",
    "start": "2262690",
    "end": "2268210"
  },
  {
    "text": "methods that had been explored\nin the couple of decades previously. And so for the next bit I\nwant to tell you a little bit",
    "start": "2268210",
    "end": "2276280"
  },
  {
    "start": "2273000",
    "end": "2384000"
  },
  {
    "text": "about the GLoVe algorithm,\nwhich was an algorithm for word vectors that was made by\nJeffrey Pennington, Richard",
    "start": "2276280",
    "end": "2283690"
  },
  {
    "text": "Socher, and me, in 2014. And so the starting\npoint of this",
    "start": "2283690",
    "end": "2289089"
  },
  {
    "text": "was to try to connect together\nthe linear algebra based",
    "start": "2289090",
    "end": "2294250"
  },
  {
    "text": "methods on co-occurrence\nmatrices like LSA and COALS with the models like skip-gram,\nCBOW and their other friends,",
    "start": "2294250",
    "end": "2302650"
  },
  {
    "text": "which were iterative\nneural updating algorithms. So on the one hand, the linear\nalgebra methods actually",
    "start": "2302650",
    "end": "2310810"
  },
  {
    "text": "seemed like they had\nadvantages for fast training and efficient usage\nof statistics.",
    "start": "2310810",
    "end": "2316450"
  },
  {
    "text": "But although there had been work\non capturing words similarities",
    "start": "2316450",
    "end": "2321550"
  },
  {
    "text": "with them, by and\nlarge the results weren't as good perhaps because\nof disproportionate importance",
    "start": "2321550",
    "end": "2327850"
  },
  {
    "text": "given to large\ncounts in the main. Conversely, the\nneural models it seems",
    "start": "2327850",
    "end": "2336940"
  },
  {
    "text": "like if you're just doing these\ngradient updates on Windows, you're somehow\ninefficiently using",
    "start": "2336940",
    "end": "2342460"
  },
  {
    "text": "statistics versus the\nco-occurrence matrix. But on the other hand,\nit's actually easier",
    "start": "2342460",
    "end": "2348730"
  },
  {
    "text": "to scale to a very large corpus\nby trading time for space.",
    "start": "2348730",
    "end": "2353980"
  },
  {
    "text": "And at that time, it seemed\nlike the neural methods just worked better for people.",
    "start": "2353980",
    "end": "2360220"
  },
  {
    "text": "That they generated improved\nperformance on many tasks, not just on word similarity.",
    "start": "2360220",
    "end": "2365440"
  },
  {
    "text": "And that they could\ncapture complex patterns, such as the analogies that\nwent beyond word similarity.",
    "start": "2365440",
    "end": "2373369"
  },
  {
    "text": "And so what we wanted to\ndo was understand a bit more as to what\nproperties you need",
    "start": "2373370",
    "end": "2379670"
  },
  {
    "text": "to have these analogies work\nout as I showed last time.",
    "start": "2379670",
    "end": "2384839"
  },
  {
    "text": "And so what we realized\nwas that if you'd like to have these sort\nof vector subtractions",
    "start": "2384840",
    "end": "2394160"
  },
  {
    "text": "and additions work\nfor an analogy, the property that you want\nis for meaning components.",
    "start": "2394160",
    "end": "2404450"
  },
  {
    "text": "So a meaning\ncomponent is something like going from male to\nfemale, queen to king.",
    "start": "2404450",
    "end": "2411320"
  },
  {
    "text": "Or going from a verb to\nits agent, truck to driver.",
    "start": "2411320",
    "end": "2419540"
  },
  {
    "text": "That those meaning\ncomponents should be represented as ratios of\nco-occurrence probabilities.",
    "start": "2419540",
    "end": "2426090"
  },
  {
    "text": "So here's an example\nthat shows that. OK, so suppose the\nmeaning component",
    "start": "2426090",
    "end": "2432230"
  },
  {
    "text": "that we want to get out is\nthe spectrum from solid to gas",
    "start": "2432230",
    "end": "2438170"
  },
  {
    "text": "as in physics. Well, you'd think that you\ncan get the solid part of it",
    "start": "2438170",
    "end": "2444590"
  },
  {
    "text": "perhaps by saying does the\nword co-occur with ice? And the word \"solid\" occurs\nwith ice so that looks hopeful.",
    "start": "2444590",
    "end": "2451970"
  },
  {
    "text": "And gas doesn't occur with ice\nmuch, so that looks hopeful. But the problem is the\nword \"water\" will also",
    "start": "2451970",
    "end": "2458359"
  },
  {
    "text": "occur a lot with ice. And if you just take\nsome other random word like the word\n\"random\", it probably",
    "start": "2458360",
    "end": "2464660"
  },
  {
    "text": "doesn't occur with ice much. In contrast, if you look at\nwords co-occurring with steam,",
    "start": "2464660",
    "end": "2473570"
  },
  {
    "text": "solid won't occur with\nsteam much, but gas will. The water will again and\nrandom will be small.",
    "start": "2473570",
    "end": "2480869"
  },
  {
    "text": "So to get out the\nmeaning component we want of going from gas to\nsolid, what's actually really",
    "start": "2480870",
    "end": "2487220"
  },
  {
    "text": "useful is to look at the\nratio of these co-occurrence probabilities.",
    "start": "2487220",
    "end": "2492500"
  },
  {
    "text": "Because then we get a\nspectrum from large to small between solid and gas.",
    "start": "2492500",
    "end": "2499130"
  },
  {
    "text": "Whereas for water\nand a random word, it basically cancels\nout and gives you 1.",
    "start": "2499130",
    "end": "2505790"
  },
  {
    "text": "I just wrote these numbers in. But if you count them\nup in a large corpus",
    "start": "2505790",
    "end": "2511070"
  },
  {
    "text": "it is basically what you get. So here are actual\nco-occurrence probabilities.",
    "start": "2511070",
    "end": "2516290"
  },
  {
    "text": "And that for water and my random\nword which was \"fashion\" here, these are approximately 1.",
    "start": "2516290",
    "end": "2523430"
  },
  {
    "text": "Whereas for the\nratio of probability of co-occurrence of solid\nwith ice or steam is about 10.",
    "start": "2523430",
    "end": "2532610"
  },
  {
    "text": "And for gas it's about a 10th. So how can we\ncapture these ratios",
    "start": "2532610",
    "end": "2541910"
  },
  {
    "text": "of co-occurrence probabilities\nas linear meaning components? So that within our\nword vector space,",
    "start": "2541910",
    "end": "2548370"
  },
  {
    "text": "we can just add and subtract\nlinear meaning components. Well, it seems\nlike the way we can",
    "start": "2548370",
    "end": "2555200"
  },
  {
    "start": "2552000",
    "end": "2586000"
  },
  {
    "text": "achieve that is if we\nbuild a log-bilinear model. So that the dot product\nbetween two word vectors",
    "start": "2555200",
    "end": "2563990"
  },
  {
    "text": "attempts to approximate\nthe log of the probability of co-occurrence. So if you do that, you\nthen get this property",
    "start": "2563990",
    "end": "2573230"
  },
  {
    "text": "that the difference\nbetween two vectors,",
    "start": "2573230",
    "end": "2578330"
  },
  {
    "text": "its similarity to\nanother word corresponds to the log of the\nprobability ratio shown",
    "start": "2578330",
    "end": "2584960"
  },
  {
    "text": "on the previous slide. So the GloVe model\nwanted to try and unify",
    "start": "2584960",
    "end": "2592430"
  },
  {
    "start": "2586000",
    "end": "2655000"
  },
  {
    "text": "the thinking between\nthe co-occurrence matrix models and the neural\nmodels by being",
    "start": "2592430",
    "end": "2600380"
  },
  {
    "text": "in some way similar\nto a neural model. But actually calculated on\ntop of a co-occurrence matrix",
    "start": "2600380",
    "end": "2607820"
  },
  {
    "text": "count. So we had an explicit\nloss function. And our explicit\nloss function is",
    "start": "2607820",
    "end": "2615710"
  },
  {
    "text": "that we wanted the\ndot product to be similar to the log\nof the co-occurrence.",
    "start": "2615710",
    "end": "2621980"
  },
  {
    "text": "We actually added in\nsome bias terms here but I'll ignore\nthose for the moment. And we wanted to not have\nvery common words dominate.",
    "start": "2621980",
    "end": "2630020"
  },
  {
    "text": "And so we capped the\neffect of high word counts using this f\nfunction that's shown here.",
    "start": "2630020",
    "end": "2637680"
  },
  {
    "text": "And then we could optimize\nthis j function directly",
    "start": "2637680",
    "end": "2643010"
  },
  {
    "text": "on the co-occurrence\ncount matrix. So that gave us fast training\nscalable to huge corpora.",
    "start": "2643010",
    "end": "2651160"
  },
  {
    "text": "And so this algorithm\nworked very well. So if you run this\nalgorithm, ask what",
    "start": "2651160",
    "end": "2658540"
  },
  {
    "text": "are the nearest words to frog? You get \"frogs\",\n\"toad\", and then you get some complicated words.",
    "start": "2658540",
    "end": "2664090"
  },
  {
    "text": "But it turns out\nthey are all frogs. Until you get down to\nlizards, so litoria's",
    "start": "2664090",
    "end": "2669460"
  },
  {
    "text": "that lovely tree frog there. And so this actually seemed\nto work out pretty well.",
    "start": "2669460",
    "end": "2675880"
  },
  {
    "text": "How well did it work out? To discuss that a\nbit more I now want to say something about how\ndo we evaluate word vectors.",
    "start": "2675880",
    "end": "2685720"
  },
  {
    "start": "2685000",
    "end": "3599000"
  },
  {
    "text": "And are we good up to\nthere for questions?  We got some questions.",
    "start": "2685720",
    "end": "2691920"
  },
  {
    "text": "What do you mean by an\ninefficient use of statistics as a con for skip-gram? Well, what I mean is\nthat for word2vec you're",
    "start": "2691920",
    "end": "2702980"
  },
  {
    "text": "just looking at one\ncenter word at a time",
    "start": "2702980",
    "end": "2708240"
  },
  {
    "text": "and generating a few\nnegative samples. And so it sort of seems\nlike [AUDIO OUT] doing",
    "start": "2708240",
    "end": "2715170"
  },
  {
    "text": "something really precise there. Whereas if you're doing\nan optimization algorithm",
    "start": "2715170",
    "end": "2722130"
  },
  {
    "text": "on the whole matrix at\nonce, well, you actually know everything about\nthe matrix at once.",
    "start": "2722130",
    "end": "2727589"
  },
  {
    "text": "You're not just looking at\nwhat other words occurred in this one context\nof the center word.",
    "start": "2727590",
    "end": "2735180"
  },
  {
    "text": "You've got the entire vector\nof co-occurrence counts of the center word\nand another word.",
    "start": "2735180",
    "end": "2741060"
  },
  {
    "text": "And so therefore you can much\nmore efficiently and less noisily work out how\nto minimize your loss.",
    "start": "2741060",
    "end": "2748725"
  },
  {
    "text": " OK, I'll go on.",
    "start": "2748725",
    "end": "2753950"
  },
  {
    "text": "OK, so I've sort of said,\nlook at these word vectors, they're great.",
    "start": "2753950",
    "end": "2759080"
  },
  {
    "text": "And I showed you a\nfew things at the end of the last class, which\nargued, hey, these are great.",
    "start": "2759080",
    "end": "2765500"
  },
  {
    "text": "They work out these analogies. They show similarity\nand things like this.",
    "start": "2765500",
    "end": "2771680"
  },
  {
    "text": "We want to make this\na bit more precise. And indeed for natural\nlanguage processing",
    "start": "2771680",
    "end": "2777350"
  },
  {
    "text": "as in other areas\nof machine learning, a big part of what\npeople are doing is working out good ways\nto evaluate knowledge",
    "start": "2777350",
    "end": "2785210"
  },
  {
    "text": "that things have. So how can we really\nevaluate word vectors? So in general for\nan NLP evaluation,",
    "start": "2785210",
    "end": "2792830"
  },
  {
    "text": "people talk about two\nways of evaluation. Intrinsic and extrinsic.",
    "start": "2792830",
    "end": "2797970"
  },
  {
    "text": "So an intrinsic evaluation\nmeans that you evaluate directly",
    "start": "2797970",
    "end": "2804680"
  },
  {
    "text": "on the specific or intermediate\nsubtasks that you've been working on. So I want a measure where\nI can directly score",
    "start": "2804680",
    "end": "2812360"
  },
  {
    "text": "how good my word vectors are. And normally,\nintrinsic evaluations",
    "start": "2812360",
    "end": "2817820"
  },
  {
    "text": "are fast to compute. They help you to\nunderstand the component you've been working on.",
    "start": "2817820",
    "end": "2823650"
  },
  {
    "text": "But often, simply trying\nto optimize that component may or may not have a\nvery big good effect",
    "start": "2823650",
    "end": "2832040"
  },
  {
    "text": "on the overall system that\nyou're trying to build. So people have also\nbeen very interested",
    "start": "2832040",
    "end": "2840000"
  },
  {
    "text": "in extrinsic evaluations. So an extrinsic evaluation is\nthat you take some real task",
    "start": "2840000",
    "end": "2847170"
  },
  {
    "text": "of interest to human beings. Whether that's web search\nor machine translation or something like that,\nand you say your goal",
    "start": "2847170",
    "end": "2855570"
  },
  {
    "text": "is to actually improve\nperformance on that task. Well that's a real proof that\nthis is doing something useful.",
    "start": "2855570",
    "end": "2864260"
  },
  {
    "text": "So in some ways that's\njust clearly better. But on the other hand, it\nalso has some disadvantages.",
    "start": "2864260",
    "end": "2872230"
  },
  {
    "text": "It takes a lot longer to\nevaluate on an extrinsic task",
    "start": "2872230",
    "end": "2877500"
  },
  {
    "text": "because it's a\nmuch bigger system. And sometimes when\nyou change things,",
    "start": "2877500",
    "end": "2883990"
  },
  {
    "text": "it's unclear whether the fact\nthat the numbers went down was because you now\nhave worse word vectors",
    "start": "2883990",
    "end": "2891120"
  },
  {
    "text": "or whether it's just\nsomehow the other components of the system interacted better\nwith your old word vectors.",
    "start": "2891120",
    "end": "2899140"
  },
  {
    "text": "And if you change the\nother components as well, things would get better again. So in some ways it\ncan sometimes be",
    "start": "2899140",
    "end": "2905630"
  },
  {
    "text": "muddier to see if\nyou're making progress. But I'll touch on both\nof these methods here.",
    "start": "2905630",
    "end": "2914020"
  },
  {
    "text": "So for intrinsic\nevaluation of word vectors, one way which we\nmentioned last time",
    "start": "2914020",
    "end": "2921960"
  },
  {
    "text": "was this word vector analogy. So we could simply\ngive our models a big collection of word\nvector analogy problems.",
    "start": "2921960",
    "end": "2930000"
  },
  {
    "text": "So we could say man is to\nwoman as king is to what? And ask the model to find\nthe word that is closest",
    "start": "2930000",
    "end": "2938220"
  },
  {
    "text": "using that sort of word\nanalogy computation and hope that what comes\nout there is queen.",
    "start": "2938220",
    "end": "2945780"
  },
  {
    "text": "And so that's something people\nhave done and have worked out an accuracy score of how\noften that you are right.",
    "start": "2945780",
    "end": "2953940"
  },
  {
    "text": "At this point I should just\nmention one little trick of these word vector\nanalogies that everyone uses",
    "start": "2953940",
    "end": "2961470"
  },
  {
    "text": "but not everyone talks about\na lot in the first instance. I mean there's a little\ntrick which you can find",
    "start": "2961470",
    "end": "2968790"
  },
  {
    "text": "in the Jensen code if you look\nat it that when it does man is to woman as king is to what,\nsomething that could often",
    "start": "2968790",
    "end": "2982079"
  },
  {
    "text": "happen is that\nactually the word-- once you do your pluses\nand your minuses--",
    "start": "2982080",
    "end": "2987900"
  },
  {
    "text": "that the word that will actually\nbe closest is still king.",
    "start": "2987900",
    "end": "2994470"
  },
  {
    "text": "So the way people\nalways do this is that they don't allow\none of the three input",
    "start": "2994470",
    "end": "3000079"
  },
  {
    "text": "words in the selection process. So you're choosing\nthe nearest word that",
    "start": "3000080",
    "end": "3005840"
  },
  {
    "text": "isn't one of the input words.  OK, so since here it's showing\nresults from the GloVe vectors.",
    "start": "3005840",
    "end": "3016040"
  },
  {
    "text": "So the GloVe vectors have\nthis strong linear component property just like I\nshowed before for COALS.",
    "start": "3016040",
    "end": "3026400"
  },
  {
    "text": "So this is for the\nmale-female dimension. And so because of this you'd\nexpect in a lot of cases",
    "start": "3026400",
    "end": "3034400"
  },
  {
    "text": "that word analogies would work. Because I can take the vector\ndifference of man and woman",
    "start": "3034400",
    "end": "3039890"
  },
  {
    "text": "and then if I add that vector\ndifference onto brother, I expect to get to\nsister and king, queen,",
    "start": "3039890",
    "end": "3046880"
  },
  {
    "text": "and for many of these examples. But of course, they may\nnot always work, right? Because if I start from emperor,\nit's sort of more of a lean.",
    "start": "3046880",
    "end": "3055790"
  },
  {
    "text": "And so it might turn out that I\nget countess or duchess coming out instead.",
    "start": "3055790",
    "end": "3062250"
  },
  {
    "text": "You can do this for\nvarious different relations or different semantic relations\nSo these word vectors actually",
    "start": "3062250",
    "end": "3069599"
  },
  {
    "text": "learn quite a bit of\njust world knowledge. So here's the company's CEO. Or this is the company's\nCEO around 2010 to 2014",
    "start": "3069600",
    "end": "3079200"
  },
  {
    "text": "when the data was taken\nfrom word vectors. And as well as semantic things\nor pragmatic things like this,",
    "start": "3079200",
    "end": "3087420"
  },
  {
    "text": "they also learn\nsyntactic things. So here are vectors\nfor positive, comparative, and superlative\nforms of adjectives.",
    "start": "3087420",
    "end": "3095280"
  },
  {
    "text": "And you can see those also move\nin roughly linear components.",
    "start": "3095280",
    "end": "3100810"
  },
  {
    "text": "So the word2vec people built\na data set of analogies so you could evaluate different\nmodels on the accuracy",
    "start": "3100810",
    "end": "3108450"
  },
  {
    "text": "of their analogies. And so here's how\nyou can do this",
    "start": "3108450",
    "end": "3113940"
  },
  {
    "text": "and this gives some numbers. So there are semantic\nand syntactic analogies. I'll just look at the totals.",
    "start": "3113940",
    "end": "3120900"
  },
  {
    "text": "OK, so what I said\nbefore is if you just use unscaled\nco-occurrence counts",
    "start": "3120900",
    "end": "3128339"
  },
  {
    "text": "and pass them through an\nSVD, things work terribly. And you see that there,\nyou only get 7.3.",
    "start": "3128340",
    "end": "3134640"
  },
  {
    "text": "But then as I also pointed\nout if you do some scaling you can actually get SVD\nof a scaled count matrix",
    "start": "3134640",
    "end": "3141960"
  },
  {
    "text": "to work reasonably well. So this SVD-L is similar\nto the COALS model.",
    "start": "3141960",
    "end": "3148770"
  },
  {
    "text": "And now we're\ngetting up to 60.1, which actually isn't\na bad score, right? So you can actually do a decent\njob without a neural network.",
    "start": "3148770",
    "end": "3156630"
  },
  {
    "text": "And then here are the two\nvariants of the word2vec model.",
    "start": "3156630",
    "end": "3163180"
  },
  {
    "text": "And here are our results\nfrom the GloVe model. And of course, at the time 2014,\nwe took this as absolute proof",
    "start": "3163180",
    "end": "3172030"
  },
  {
    "text": "that our model was better\nand a more efficient use of statistics was really\nworking in our favor.",
    "start": "3172030",
    "end": "3179860"
  },
  {
    "text": "With seven years of retrospect,\nI think that's not really true, it turns out.",
    "start": "3179860",
    "end": "3184990"
  },
  {
    "text": "I think the main part\nof why we scored better is that we actually\nhad better data.",
    "start": "3184990",
    "end": "3190660"
  },
  {
    "text": "And so there's a bit of evidence\nabout that on this next slide here.",
    "start": "3190660",
    "end": "3195890"
  },
  {
    "text": "So this looks at the semantic,\nsyntactic and overall performance on word\nanalogies of GloVe models",
    "start": "3195890",
    "end": "3204970"
  },
  {
    "text": "that were trained on\ndifferent subsets of data. So in particular, the two on the\nleft are trained on Wikipedia.",
    "start": "3204970",
    "end": "3214390"
  },
  {
    "text": "And you can see that\ntraining on Wikipedia makes you do really well on\nsemantic analogies, which maybe",
    "start": "3214390",
    "end": "3221259"
  },
  {
    "text": "makes sense because\nWikipedia just tells you a lot of semantic facts. I mean that's kind of\nwhat encyclopedias do.",
    "start": "3221260",
    "end": "3228140"
  },
  {
    "text": "And so one of the big\nadvantages we actually had was that Wikipedia--",
    "start": "3228140",
    "end": "3234280"
  },
  {
    "text": "that the GloVe model was partly\ntrained on Wikipedia as well as other text.",
    "start": "3234280",
    "end": "3239440"
  },
  {
    "text": "Whereas the word2vec\nmodel that was released was trained exclusively\non Google News. So news wire data.",
    "start": "3239440",
    "end": "3245920"
  },
  {
    "text": "And if you only train on a\nsmallish amount of news wire",
    "start": "3245920",
    "end": "3251020"
  },
  {
    "text": "data, you can see\nthat for the semantics it's just not as good as\neven one quarter of the size",
    "start": "3251020",
    "end": "3258970"
  },
  {
    "text": "amount of Wikipedia data. Though if you get a lot of data\nyou can compensate for that.",
    "start": "3258970",
    "end": "3265160"
  },
  {
    "text": "So here on the right\nend, if you then have Common Crawl Web data. And so once there's a lot of web\ndata, so now 42 billion words,",
    "start": "3265160",
    "end": "3274480"
  },
  {
    "text": "you're then starting to\nget good scores again from the semantic side.",
    "start": "3274480",
    "end": "3279800"
  },
  {
    "text": "The graph on the\nright then shows how well do you do as you\nincrease the vector dimension.",
    "start": "3279800",
    "end": "3287029"
  },
  {
    "text": "And so what you can see there\nis 25 dimensional vectors aren't very good.",
    "start": "3287030",
    "end": "3293210"
  },
  {
    "text": "They go up to sort\nof 50 and then 100. And so 100 dimensional vectors\nalready work reasonably well.",
    "start": "3293210",
    "end": "3300030"
  },
  {
    "text": "So that's why I used 100\ndimensional vectors when I showed my example in class. [AUDIO OUT] and working\nreasonably well.",
    "start": "3300030",
    "end": "3311720"
  },
  {
    "text": "But you still get\nsignificant gains for 200 and somewhat to 300. So at least back around\nsort of 2013 to 2015,",
    "start": "3311720",
    "end": "3319500"
  },
  {
    "text": "everyone sort of\ngravitated to the fact that 300 dimensional\nvectors is the sweet spot.",
    "start": "3319500",
    "end": "3325380"
  },
  {
    "text": "So almost freakily, if you look\nthrough the best known sets of word vectors that include the\nword2vec vectors and the GloVe",
    "start": "3325380",
    "end": "3332720"
  },
  {
    "text": "vectors that\nusually what you get is 300 dimensional word vectors. ",
    "start": "3332720",
    "end": "3339720"
  },
  {
    "text": "That's not the only intrinsic\nevaluation you can do. Another intrinsic\nevaluation you can do",
    "start": "3339720",
    "end": "3345630"
  },
  {
    "text": "is see how these models\nmodel human judgments of word",
    "start": "3345630",
    "end": "3351150"
  },
  {
    "text": "similarity. So psychologists\nfor several decades have actually taken human\njudgments of word similarity.",
    "start": "3351150",
    "end": "3360660"
  },
  {
    "text": "Where literally you're asking\npeople for pairs of words like \"professor\" and \"doctor\"\nto give them a similarity",
    "start": "3360660",
    "end": "3368310"
  },
  {
    "text": "score that's being measured as\nsome continuous quantity giving you a score between,\nsay 0 and 10.",
    "start": "3368310",
    "end": "3376349"
  },
  {
    "text": "And so there are\nhuman judgments, which are then averaged over\nmultiple human judgments",
    "start": "3376350",
    "end": "3381359"
  },
  {
    "text": "as to how similar\ndifferent words are. So \"tiger\" and \"cat\"\nis pretty similar.",
    "start": "3381360",
    "end": "3386730"
  },
  {
    "text": "\"Computer and \"internet\"\nis pretty similar. \"Plane and \"cars\" less similar. \"Stock\" and \"CD\" aren't\nvery similar at all",
    "start": "3386730",
    "end": "3394079"
  },
  {
    "text": "but \"stock\" and \"jaguar\"\nare even less similar. So we could then\nsay for our models,",
    "start": "3394080",
    "end": "3402600"
  },
  {
    "text": "do they have the same\nsimilarity judgments. And in particular, we\ncan measure a correlation",
    "start": "3402600",
    "end": "3409200"
  },
  {
    "text": "coefficient of whether they give\nthe same ordering of similarity judgments. And so then we can\nget data for that.",
    "start": "3409200",
    "end": "3416920"
  },
  {
    "text": "And so there are various\ndifferent data sets of word similarities. And we can score\ndifferent models",
    "start": "3416920",
    "end": "3422850"
  },
  {
    "text": "as to how well they\ndo on similarities. And again, you see\nhere that plain SVD's",
    "start": "3422850",
    "end": "3430829"
  },
  {
    "text": "works comparatively better\nhere for similarities than it did for analogies.",
    "start": "3430830",
    "end": "3436380"
  },
  {
    "text": "It's not great but it's\nnot completely terrible because we no longer need\nthat linear property.",
    "start": "3436380",
    "end": "3441840"
  },
  {
    "text": "But again, scaled SVD's\nwork a lot better. Word2vec works a bit\nbetter than that.",
    "start": "3441840",
    "end": "3448570"
  },
  {
    "text": "And we got some of the same\nkind of minor advantages from the GloVe model.",
    "start": "3448570",
    "end": "3454330"
  },
  {
    "text": "Sorry to interrupt. A lot of those students who are\nasking if you could re-explain the objective function for\nthe GloVe model and also",
    "start": "3454330",
    "end": "3461589"
  },
  {
    "text": "what log-bilinear means. OK.",
    "start": "3461590",
    "end": "3467270"
  },
  {
    "text": "Sure, OK, here is my\nobjective function.",
    "start": "3467270",
    "end": "3476870"
  },
  {
    "text": "All right, if I go one\nslide before that-- right, so the\nproperty that we want",
    "start": "3476870",
    "end": "3484490"
  },
  {
    "text": "is that we want the dot\nproduct to represent the log",
    "start": "3484490",
    "end": "3491300"
  },
  {
    "text": "probability of co-occurrence. So that then gives me\nmy tricky log-bilinear.",
    "start": "3491300",
    "end": "3500030"
  },
  {
    "text": "So the \"bi\" is that\nthere's the wy and the wj so that there are\ntwo linear things.",
    "start": "3500030",
    "end": "3508760"
  },
  {
    "text": "And it's linear in\neach one of them. So this is sort of\nlike having and--",
    "start": "3508760",
    "end": "3513920"
  },
  {
    "text": "rather than having\na sort of an ax where you just have\nsomething that's linear in x and is a constant.",
    "start": "3513920",
    "end": "3521640"
  },
  {
    "text": "It's bi-linear because\nwe have the wy, wj and it's linear in both of them.",
    "start": "3521640",
    "end": "3527540"
  },
  {
    "text": "And that's then related to\nthe log of a probability. And so that gives us\nthe log-bilinear model.",
    "start": "3527540",
    "end": "3533525"
  },
  {
    "text": " And so since we'd like\nthese things to be equal,",
    "start": "3533525",
    "end": "3543340"
  },
  {
    "text": "what we're doing here, if\nyou ignore these two center terms is that we're\nwanting to say",
    "start": "3543340",
    "end": "3548950"
  },
  {
    "text": "the difference between these\ntwo is as small as possible.",
    "start": "3548950",
    "end": "3554900"
  },
  {
    "text": "So we're taking this difference\nand we're squaring it so it's always positive. And we want that squared term\nto be as small as possible.",
    "start": "3554900",
    "end": "3564700"
  },
  {
    "text": "And that's 90% of it. And you can\nbasically stop there.",
    "start": "3564700",
    "end": "3569770"
  },
  {
    "text": "But the other bit\nthat's in here, is a lot of the time when\nyou're building models",
    "start": "3569770",
    "end": "3577690"
  },
  {
    "text": "rather than simply having\nsort of an ax model, it seems useful to\nhave a bias term which",
    "start": "3577690",
    "end": "3585310"
  },
  {
    "text": "can move things up and down\nfor the word in general.",
    "start": "3585310",
    "end": "3591020"
  },
  {
    "text": "And so we added into\nthe model bias term so that there's a bias\nterm for both words.",
    "start": "3591020",
    "end": "3596950"
  },
  {
    "text": "So if in general probabilities\nare high for a certain word, this bias term can model that.",
    "start": "3596950",
    "end": "3603220"
  },
  {
    "text": "And for the other word this\nbias term can model it, okay?",
    "start": "3603220",
    "end": "3608830"
  },
  {
    "text": "So now I'll pop back and after-- oh actually I just\nsaw someone said",
    "start": "3608830",
    "end": "3615850"
  },
  {
    "text": "why multiplying by the f of-- sorry I did skip that last term.",
    "start": "3615850",
    "end": "3623700"
  },
  {
    "text": "OK, the y modifying\nby this f of xij. So this last bit\nwas to scale things",
    "start": "3623700",
    "end": "3634050"
  },
  {
    "text": "depending on the\nfrequency of a word because you want to pay\nmore attention to words",
    "start": "3634050",
    "end": "3645240"
  },
  {
    "text": "that are more common or word\npairs that are more common. Because if you think about\nit in word2vec terms,",
    "start": "3645240",
    "end": "3653730"
  },
  {
    "text": "you're seeing if things have\na co-occurrence count of 50 versus three.",
    "start": "3653730",
    "end": "3660809"
  },
  {
    "text": "You want to do a\nbetter job at modeling the co-occurrence of the\nthings that occurred together",
    "start": "3660810",
    "end": "3668390"
  },
  {
    "text": "50 times. And so you want to consider\nin the count of co-occurrence.",
    "start": "3668390",
    "end": "3676160"
  },
  {
    "text": "But then the argument is that\nthat actually leads you astray when you have extremely common\nwords like function words.",
    "start": "3676160",
    "end": "3683990"
  },
  {
    "text": "And so effectively you\npaid more attention to words that\nco-occurred together up",
    "start": "3683990",
    "end": "3691490"
  },
  {
    "text": "until a certain point. And then the curve\njust went flat, so it didn't matter if it\nwas an extremely, extremely",
    "start": "3691490",
    "end": "3697700"
  },
  {
    "text": "common word. So then for extrinsic\nword vector evaluation.",
    "start": "3697700",
    "end": "3705410"
  },
  {
    "text": "So at this point you're now\nwanting to sort of say, well, can we embed our word\nvectors in some end-user task",
    "start": "3705410",
    "end": "3714710"
  },
  {
    "text": "and do they help? And do different word\nvectors work better or worse",
    "start": "3714710",
    "end": "3721010"
  },
  {
    "text": "than other word vectors? So this is something that\nwe'll see a lot of later",
    "start": "3721010",
    "end": "3726230"
  },
  {
    "text": "in the class. I mean, in particular\nwhen you get on to doing assignment three.",
    "start": "3726230",
    "end": "3732050"
  },
  {
    "text": "That assignment three you get\nto build dependency parsers and you can then\nuse word vectors",
    "start": "3732050",
    "end": "3739210"
  },
  {
    "text": "in the dependency parser\nand see how much they help. We don't actually make you\ntest out different sets of word",
    "start": "3739210",
    "end": "3745000"
  },
  {
    "text": "vectors, but you could. Here's just one example of\nthis to give you a sense.",
    "start": "3745000",
    "end": "3751480"
  },
  {
    "text": "So the task of named\nentity recognition is going through a piece\nof text and identifying",
    "start": "3751480",
    "end": "3757560"
  },
  {
    "text": "mentions of a person's name\nor an organization name like a company or a location.",
    "start": "3757560",
    "end": "3763810"
  },
  {
    "text": "And so if you have\ngood word vectors,",
    "start": "3763810",
    "end": "3769380"
  },
  {
    "text": "do they help you do named\nentity recognition better? And the answer to that is yes.",
    "start": "3769380",
    "end": "3774970"
  },
  {
    "text": "So if one starts off\nwith a model that simply has discrete features, so it\nuses word identity as features,",
    "start": "3774970",
    "end": "3783420"
  },
  {
    "text": "you can build a pretty good\nnamed entity model doing that. But if you add into\nit word vectors",
    "start": "3783420",
    "end": "3789030"
  },
  {
    "text": "you get a better representation\nof the meaning of words. And so that you can have the\nnumbers go up quite a bit.",
    "start": "3789030",
    "end": "3796480"
  },
  {
    "text": "And then you can\ncompare different models to see how much gain\nthey give you in terms",
    "start": "3796480",
    "end": "3801930"
  },
  {
    "text": "of this extrinsic task. So skipping ahead,\nthis was a question",
    "start": "3801930",
    "end": "3807720"
  },
  {
    "text": "that was asked after class,\nwhich was word senses.",
    "start": "3807720",
    "end": "3812820"
  },
  {
    "text": "Because so far we've had\njust one particular string,",
    "start": "3812820",
    "end": "3822540"
  },
  {
    "text": "we've got some string house. And we're going to say for\neach of those strings there's",
    "start": "3822540",
    "end": "3827880"
  },
  {
    "text": "a word vector. And if you think\nabout it a bit more, that seems like it's very weird.",
    "start": "3827880",
    "end": "3836250"
  },
  {
    "text": "Because actually most words,\nespecially common words, and especially words that\nhave existed for a long time",
    "start": "3836250",
    "end": "3844500"
  },
  {
    "text": "actually have many meanings\nwhich are very different. So how could that be\ncaptured if you only have",
    "start": "3844500",
    "end": "3850470"
  },
  {
    "text": "one word vector for the word? Because you can't\nactually capture the fact that you've got different\nmeanings for the word.",
    "start": "3850470",
    "end": "3857345"
  },
  {
    "text": "Because your\nmeaning for the word is just one point in\nspace, one vector. And so as an example of\nthat, here's the word \"pike\".",
    "start": "3857345",
    "end": "3866404"
  },
  {
    "text": "Now it's actually\na very common word but it is an old Germanic word.",
    "start": "3866404",
    "end": "3872640"
  },
  {
    "text": "Well, what kind of meanings\ndoes the word \"pike\" have? So you can maybe just\nthink for a minute",
    "start": "3872640",
    "end": "3878790"
  },
  {
    "text": "and think what word meanings\nthe word \"pike\" has.",
    "start": "3878790",
    "end": "3886580"
  },
  {
    "text": "And it actually turns out it\nhas a lot of different meanings. So perhaps the\nmost basic meaning",
    "start": "3886580",
    "end": "3893120"
  },
  {
    "text": "is if you did fantasy games or\nsomething, medieval weapons,",
    "start": "3893120",
    "end": "3899180"
  },
  {
    "text": "a sharp pointed staff is a pike. But there's a kind\nof a fish that has a similar elongated\nshape that's a pike.",
    "start": "3899180",
    "end": "3907940"
  },
  {
    "text": "It was used for railroad lines. Maybe that usage isn't\nused much anymore.",
    "start": "3907940",
    "end": "3914240"
  },
  {
    "text": "But it's certainly still\nsurvives in referring to roads. So this is like when\nyou have turnpikes.",
    "start": "3914240",
    "end": "3920299"
  },
  {
    "text": "We have expressions where\n\"pike\" means the future, like coming down the pike.",
    "start": "3920300",
    "end": "3925520"
  },
  {
    "text": "It's a position in diving,\nthat divers do a pike. Those are all noun uses.",
    "start": "3925520",
    "end": "3932420"
  },
  {
    "text": "There are also verbal uses. So you can pike\nsomebody with your pike.",
    "start": "3932420",
    "end": "3937730"
  },
  {
    "text": "Different usages might\nhave different currency. In Australia you can\nalso use \"pike\" to mean",
    "start": "3937730",
    "end": "3944330"
  },
  {
    "text": "that you pull out\nof doing something, like I reckon he's\ngoing to pike.",
    "start": "3944330",
    "end": "3949350"
  },
  {
    "text": "I don't think that usage\nis used in America. But lots of meanings. And actually for\nwords that are common or if you start thinking of\nwords like \"line\" or \"field\",",
    "start": "3949350",
    "end": "3957690"
  },
  {
    "text": "I mean they just have even\nmore meanings than this. So what are we actually\ndoing with just one",
    "start": "3957690",
    "end": "3963630"
  },
  {
    "text": "vector for a word? And well, one way\nyou could go is",
    "start": "3963630",
    "end": "3968760"
  },
  {
    "text": "to say OK, up until now\nwhat we've done is crazy. \"Pike\" has, and\nother words have all",
    "start": "3968760",
    "end": "3974700"
  },
  {
    "text": "of these different meanings. So maybe what we should do is\nhave a different word vectors",
    "start": "3974700",
    "end": "3981210"
  },
  {
    "text": "for the different\nmeanings of \"pike\". So we'd have one word vector\nfor the medieval pointy weapon.",
    "start": "3981210",
    "end": "3988660"
  },
  {
    "text": "Another word vector\nfor the kind of fish. Another word vector\nfor the kind of road.",
    "start": "3988660",
    "end": "3994300"
  },
  {
    "text": "So that there'd then\nbe word sense vectors.  And you can do that.",
    "start": "3994300",
    "end": "4000530"
  },
  {
    "text": "I mean actually, we were working\non that in the early 2010s,",
    "start": "4000530",
    "end": "4006950"
  },
  {
    "text": "actually even before\nword2vec came out. So this picture is a\nlittle bit small to see.",
    "start": "4006950",
    "end": "4014990"
  },
  {
    "text": "But what we were\ndoing was for words, we were clustering\ninstances of a word hoping",
    "start": "4014990",
    "end": "4022910"
  },
  {
    "text": "that those clusters-- clustering the word tokens,\nhoping those clusters",
    "start": "4022910",
    "end": "4028069"
  },
  {
    "text": "have similar represented senses. And then for the\nclusters of word tokens,",
    "start": "4028070",
    "end": "4033155"
  },
  {
    "text": "we were also treating them\nlike they were separate words. And learning a word\nvector for each.",
    "start": "4033155",
    "end": "4038839"
  },
  {
    "text": "And basically that\nactually works. So in green we have two\nsenses for the word \"bank\".",
    "start": "4038840",
    "end": "4046010"
  },
  {
    "text": "And so there's one sense for the\nword \"bank\" that's over here, where it's close to words\nlike \"banking\", \"finance\",",
    "start": "4046010",
    "end": "4052039"
  },
  {
    "text": "\"transaction\", and \"laundering\". And then we have another sense\nfor the word \"bank\" over here,",
    "start": "4052040",
    "end": "4057080"
  },
  {
    "text": "where it's close to words like\n\"plateau\", \"boundary\", \"gap territory\", which is the river\nbank sense of the word \"bank\".",
    "start": "4057080",
    "end": "4064610"
  },
  {
    "text": "And for the word \"jaguar\"\nthat's in purple. Well, \"jaguar\" has\na number of senses",
    "start": "4064610",
    "end": "4071299"
  },
  {
    "text": "and so we have those as well. So this sense down here\nis close to \"hunter\"",
    "start": "4071300",
    "end": "4077000"
  },
  {
    "text": "so that's the big game\nanimals sense of \"jaguar\". Up the top here it's\nbeing shown close",
    "start": "4077000",
    "end": "4083630"
  },
  {
    "text": "to \"luxury\" and \"convertibles\",\nThis is the Jaguar car sense. Then \"jaguar\" here is near\n\"string\", \"keyboard\" and words",
    "start": "4083630",
    "end": "4092930"
  },
  {
    "text": "like that. So jaguar's the name\nof a kind of keyboard. And then this final\n\"jaguar\" over here",
    "start": "4092930",
    "end": "4100068"
  },
  {
    "text": "is close to \"software\"\nand \"Microsoft\". And then if you're\nold enough, you'll",
    "start": "4100069",
    "end": "4105170"
  },
  {
    "text": "remember that there was an\nold version of Mac OS that was called Jaguar. So that's then the\ncomputer sense.",
    "start": "4105170",
    "end": "4111119"
  },
  {
    "text": "So basically this does work\nand we can learn word vectors for different senses of a word.",
    "start": "4111120",
    "end": "4118339"
  },
  {
    "text": "But actually this\nisn't the majority way that things have then\ngone in practice.",
    "start": "4118340",
    "end": "4124759"
  },
  {
    "text": "And there are kind of a\ncouple of reasons for that. I mean one is just simplicity.",
    "start": "4124760",
    "end": "4131299"
  },
  {
    "text": "If you do this,\nit's kind of complex because you first of all\nhave to learn word senses",
    "start": "4131300",
    "end": "4138409"
  },
  {
    "text": "and then start learning word\nvectors in terms of the word senses But the other reason is\nalthough this model of having",
    "start": "4138410",
    "end": "4146060"
  },
  {
    "text": "word senses is traditional. It's what you see\nin dictionaries.",
    "start": "4146060",
    "end": "4151700"
  },
  {
    "text": "It's commonly what's been used\nin natural language processing. I mean it tends to be\nimperfect in its own way",
    "start": "4151700",
    "end": "4158989"
  },
  {
    "text": "because we're trying to take\nall the uses of the word \"pike\" and sort of cut them up\ninto key different senses.",
    "start": "4158990",
    "end": "4165560"
  },
  {
    "text": "Where [AUDIO OUT] differences\nkind of overlapping and it's",
    "start": "4165560",
    "end": "4172490"
  },
  {
    "text": "often not clear which\nones to count as distinct. So for example here,\nright, a railroad line",
    "start": "4172490",
    "end": "4177830"
  },
  {
    "text": "and a type of road,\nwell sort of that's the same sense of \"pike\". It's just that they're different\nforms of transportation.",
    "start": "4177830",
    "end": "4183830"
  },
  {
    "text": "And so that this could be a\ntype of transportation line and cover both of them.",
    "start": "4183830",
    "end": "4189149"
  },
  {
    "text": "So it's always sort\nof very unclear how you cut word meaning\ninto different senses.",
    "start": "4189149",
    "end": "4195798"
  },
  {
    "text": "And indeed, if you look\nat different dictionaries everyone does it differently.",
    "start": "4195798",
    "end": "4201650"
  },
  {
    "text": "So it actually turns\nout that in practice you can do rather well\nby simply having",
    "start": "4201650",
    "end": "4210880"
  },
  {
    "text": "one word vector per word type. And what happens if you do that?",
    "start": "4210880",
    "end": "4217510"
  },
  {
    "text": "Well, what you find is that--",
    "start": "4217510",
    "end": "4224110"
  },
  {
    "text": "what you learn is\nthe word vector is what gets referred\nto in fancy talk",
    "start": "4224110",
    "end": "4229960"
  },
  {
    "text": "as a superposition\nof the word vectors",
    "start": "4229960",
    "end": "4236320"
  },
  {
    "text": "for the different\nsenses of a word. Where the word \"superposition\"\nmeans no more or less",
    "start": "4236320",
    "end": "4242590"
  },
  {
    "text": "than a weighted sum. So the vector that\nwe learn for \"pike\" will be a weighted\naverage of the vectors",
    "start": "4242590",
    "end": "4250389"
  },
  {
    "text": "that you would have learned\nfor the medieval weapons sense, plus the fish\nsense, plus the road sense,",
    "start": "4250390",
    "end": "4256630"
  },
  {
    "text": "plus whatever other\nsenses that you have. Where the weighting that's\ngiven to these different sense",
    "start": "4256630",
    "end": "4262540"
  },
  {
    "text": "vectors corresponds\nto the frequencies of use of the different senses.",
    "start": "4262540",
    "end": "4267909"
  },
  {
    "text": "So we end up with\nthe vector for \"pike\"",
    "start": "4267910",
    "end": "4273970"
  },
  {
    "text": "being a kind of\nan average vector. And so if you say\nOK, you've just",
    "start": "4273970",
    "end": "4283090"
  },
  {
    "text": "added up several different\nvectors into an average. You might think\nthat that's useless",
    "start": "4283090",
    "end": "4289630"
  },
  {
    "text": "because you've lost the\nreal meanings of the word. You've just got some kind\nof funny average vector",
    "start": "4289630",
    "end": "4296410"
  },
  {
    "text": "that's in between them. But actually it\nturns out that if you",
    "start": "4296410",
    "end": "4302080"
  },
  {
    "text": "use this average\nvector in applications, it tends to sort of\nself-disambiguate.",
    "start": "4302080",
    "end": "4310030"
  },
  {
    "text": "Because if you say is the word\n\"pike\" similar to the word",
    "start": "4310030",
    "end": "4316179"
  },
  {
    "text": "for \"fish\" well, part of\nthis vector represents fish,",
    "start": "4316180",
    "end": "4321220"
  },
  {
    "text": "the fish sense of \"pike\". And so in those components,\nit will be kind of similar to the fish vector.",
    "start": "4321220",
    "end": "4327460"
  },
  {
    "text": "And so yes you'll say there's\nsubstantial similarity.",
    "start": "4327460",
    "end": "4333440"
  },
  {
    "text": "Whereas if in another\npiece of text that says,",
    "start": "4333440",
    "end": "4338560"
  },
  {
    "text": "the men were armed with pikes,\nand lances or pikes and maces, or whatever other medieval\nweapons you remember.",
    "start": "4338560",
    "end": "4345910"
  },
  {
    "text": "Well, actually some of that\nmeaning is in the \"pike\" vector as well.",
    "start": "4345910",
    "end": "4351350"
  },
  {
    "text": "And so it'll say, yeah that's\ngood similarity with mace, and staff and words\nlike that as well.",
    "start": "4351350",
    "end": "4358520"
  },
  {
    "text": "And in fact, we can work\nout which sense of \"pike\" is intended by just sort\nof seeing which components",
    "start": "4358520",
    "end": "4367120"
  },
  {
    "text": "are similar to other words that\nare used in the same context. And indeed there's actually\na much more surprising result",
    "start": "4367120",
    "end": "4375340"
  },
  {
    "text": "than that. And this is a result that's\ndue to Sanjeev Arora, Tengyu Ma, who is now on the Stanford\nfaculty and others in 2018.",
    "start": "4375340",
    "end": "4385588"
  },
  {
    "text": "And that's the following\nresult, which I'm not actually going to explain. But so if you think that\nthe vector for \"pike\"",
    "start": "4385588",
    "end": "4394530"
  },
  {
    "text": "is just a sum of the vectors\nfor the different senses.",
    "start": "4394530",
    "end": "4400070"
  },
  {
    "text": "Well, it should be you'd think\nthat it's just completely impossible to reconstruct the\nsense vectors from the vector",
    "start": "4400070",
    "end": "4410550"
  },
  {
    "text": "for the word type. Because normally, if I\nsay I've got two numbers,",
    "start": "4410550",
    "end": "4418199"
  },
  {
    "text": "the sum of them is 17, you just\nhave no information as to what my two numbers are, right?",
    "start": "4418200",
    "end": "4423390"
  },
  {
    "text": "You can't resolve it. And even worse, if I tell\nyou I've got three numbers and they sum to 17.",
    "start": "4423390",
    "end": "4430560"
  },
  {
    "text": "But it turns out\nthat when we have these high dimensional\nvector spaces that things",
    "start": "4430560",
    "end": "4437400"
  },
  {
    "text": "are so sparse in those high\ndimensional vector spaces that you can use ideas from\nsparse coding to actually",
    "start": "4437400",
    "end": "4445560"
  },
  {
    "text": "separate out the different\nsenses, providing they're relatively common.",
    "start": "4445560",
    "end": "4451510"
  },
  {
    "text": "So they show in\ntheir paper that you can start with the\nvector of say \"pike\" and actually separate\nout components",
    "start": "4451510",
    "end": "4459119"
  },
  {
    "text": "of that vector that correspond\nto different senses of the word \"pike\". And so here's an example at\nthe bottom of this slide, which",
    "start": "4459120",
    "end": "4466350"
  },
  {
    "text": "is for the word \"pike\". Separate out that vector\ninto five different senses.",
    "start": "4466350",
    "end": "4473070"
  },
  {
    "text": "And so the one sense\nis close to the words \"trousers\", \"blouse\",\n\"waistcoats\",",
    "start": "4473070",
    "end": "4478170"
  },
  {
    "text": "and that's that sort of\nclothing sense of \"tie\". Another sense is close to\n\"wires\", \"cables\", \"wiring\",",
    "start": "4478170",
    "end": "4484590"
  },
  {
    "text": "\"electrical\". So that's the tie sense of a tie\nused in the electrical stuff.",
    "start": "4484590",
    "end": "4490380"
  },
  {
    "text": "Then we have\n\"scoreline\", \"goalless\", \"equalizer\" this is the\nsporting game sense of \"tie\".",
    "start": "4490380",
    "end": "4496260"
  },
  {
    "text": "This one also seems\nto in a different way evoke a sporting\ngame sense of \"tie\".",
    "start": "4496260",
    "end": "4502170"
  },
  {
    "text": "And then there's\nfinally this one here. Maybe my music is\njust really bad. Maybe it's because you get ties\nin music when you tie notes",
    "start": "4502170",
    "end": "4509489"
  },
  {
    "text": "together, I guess. So you get these different\nsenses out of it. ",
    "start": "4509490",
    "end": "4518000"
  }
]