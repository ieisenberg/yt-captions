[
  {
    "start": "0",
    "end": "5867"
  },
  {
    "text": "All right, while we work\non getting these started, I'm just going to write\na couple things up about general\nlogistics and continue",
    "start": "5867",
    "end": "11570"
  },
  {
    "text": "to work on this for a sec. ",
    "start": "11570",
    "end": "71860"
  },
  {
    "text": "All right. OK, great. Well, why don't\nwe dive into this. ",
    "start": "71860",
    "end": "77140"
  },
  {
    "text": "Let's see. So I think everybody agreed from\nthe first one, which is great.",
    "start": "77140",
    "end": "83710"
  },
  {
    "text": "So this is true. So this is true. ",
    "start": "83710",
    "end": "89200"
  },
  {
    "text": "There was a bit of\ndisagreement about B and C",
    "start": "89200",
    "end": "94905"
  },
  {
    "text": "So why don't you talk to\nyour neighbor for a second and see if that changes your\nmind or resolves the confusion.",
    "start": "94905",
    "end": "102740"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "102740",
    "end": "125520"
  },
  {
    "text": "Yes, that's correct. Because one of the downsides\nis constantly evaluation. And I think that's what I\nthought where DAGGER needed",
    "start": "125520",
    "end": "133110"
  },
  {
    "text": "a little bit more\nthan just images because it's able\nto put together-- ",
    "start": "133110",
    "end": "153300"
  },
  {
    "text": "All right, so the first one is--",
    "start": "153300",
    "end": "158310"
  },
  {
    "text": "this is false,\nand this is false. And so, DAGGER,\nif you think back,",
    "start": "158310",
    "end": "163560"
  },
  {
    "text": "unfortunately required the\nhuman to keep around forever. They would constantly\nbe getting asked, hey,",
    "start": "163560",
    "end": "168890"
  },
  {
    "text": "for the policy that\nthe agent followed, was this an optimal\naction or not? And behavior cloning\ndoes not require",
    "start": "168890",
    "end": "175180"
  },
  {
    "text": "knowing the dynamics model. It allows us to reduce\nreinforcement learning to supervised learning. And the idea is that we take\nthe expert demonstrations,",
    "start": "175180",
    "end": "182560"
  },
  {
    "text": "and we just try to learn\nstate-to-action mappings. And so we can just treat it as\na standard supervised learning",
    "start": "182560",
    "end": "187810"
  },
  {
    "text": "problem. Great. all right, so I think as we\ngo further in the course,",
    "start": "187810",
    "end": "195800"
  },
  {
    "text": "we get to go to more and\nmore exciting topics. And today, we're really\ngoing to start to see how--",
    "start": "195800",
    "end": "201970"
  },
  {
    "text": "skipping all the NLP side,\nbut how do we actually get to reinforcement\nlearning that can do some of the amazing things\nthat we see large language",
    "start": "201970",
    "end": "208592"
  },
  {
    "text": "models doing? So for example, when I was\npreparing this lecture, I was like, please\nwrite me a program",
    "start": "208593",
    "end": "213700"
  },
  {
    "text": "to demonstrate how\nour life works. Be brief in your explanations,\nand then show me the code. And within about\nfive seconds, it",
    "start": "213700",
    "end": "221410"
  },
  {
    "text": "generated me code that used\nQ-learning and other things to generate an actual\nexample of how RLHF,",
    "start": "221410",
    "end": "228520"
  },
  {
    "text": "which stands for\nReinforcement Learning From Human Feedback, which\nis how they trained ChatGPT,",
    "start": "228520",
    "end": "234790"
  },
  {
    "text": "amongst a whole bunch\nof other things to do. So it could generate\nme a small example of how to do that in\ncode that you can run.",
    "start": "234790",
    "end": "241599"
  },
  {
    "text": "So that's pretty extraordinary. This was not possible\ntwo years ago.",
    "start": "241600",
    "end": "247930"
  },
  {
    "text": "I started offering\nthis class in 2017. So when I first started\noffering this class, this was definitely\nnot possible. And this only really became\npossible with ChatGPT.",
    "start": "247930",
    "end": "255250"
  },
  {
    "text": "So it's pretty phenomenal that\nwe now have AI that can do this. And the question is,\nhow do we get there?",
    "start": "255250",
    "end": "261278"
  },
  {
    "text": "And what sort of RL\ntechniques are being used to help accomplish this? So that's what we're going\nto start digging into now.",
    "start": "261279",
    "end": "268039"
  },
  {
    "text": "So today, what\nwe're going to do is we are going to continue\non from imitation learning",
    "start": "268040",
    "end": "274510"
  },
  {
    "text": "and talk a bit about\nReinforcement Learning From Human Feedback. And then next time, we're going\nto have a guest lecture from one",
    "start": "274510",
    "end": "282340"
  },
  {
    "text": "of the authors of the direct\npreference optimization work, which received best paper\nrunner up at neural information",
    "start": "282340",
    "end": "288400"
  },
  {
    "text": "processing systems,\nwhich is kind of the premier machine\nlearning conference. So he's going to come talk.",
    "start": "288400",
    "end": "293905"
  },
  {
    "text": "He's one of the graduate\nstudents here at Stanford. And this have become-- I guess, maybe like it's\nstarting to replace or exceed",
    "start": "293905",
    "end": "301720"
  },
  {
    "text": "performance on RLHF on a lot of\nbenchmarks, but super exciting. And it'll be great to have him. And in fact, because everybody\nhere always is innovating,",
    "start": "301720",
    "end": "309410"
  },
  {
    "text": "which is awesome, he was\nlike, oh, well, we actually have a new paper coming out\non archive like next week that shows how we can\nextend this to all",
    "start": "309410",
    "end": "314607"
  },
  {
    "text": "in all these different ways. So I asked him if he had time\nto cover that a little bit. So there's a lot of work\nto be done in this space",
    "start": "314607",
    "end": "322000"
  },
  {
    "text": "to think about how do we\nbetter use RL in combination with these incredible function\napproximators of large language",
    "start": "322000",
    "end": "327520"
  },
  {
    "text": "models to create the\namazing performance that we could see of a system that\ncould do something like this.",
    "start": "327520",
    "end": "335140"
  },
  {
    "text": "So that's where we're going. What we're going\nto focus on today is to continue talking\nabout imitation learning.",
    "start": "335140",
    "end": "340880"
  },
  {
    "text": "And I think imitation\nlearning is a nice way to build into this, because\nimitation learning is one form of using human feedback\nto try to train reinforcement",
    "start": "340880",
    "end": "350380"
  },
  {
    "text": "learning agents. And then when we get\ninto RLHF, well, that'll be sort of a different way\nto leverage human expertise.",
    "start": "350380",
    "end": "357760"
  },
  {
    "text": "So to start, we're going to\ngo back to imitation learning and to talk a lot today\nabout Max entropy inverse",
    "start": "357760",
    "end": "364600"
  },
  {
    "text": "reinforcement learning. So let's just remember\nwhere we were last time. What were talking about when we\ntalked about imitation learning",
    "start": "364600",
    "end": "371710"
  },
  {
    "text": "was the idea of taking\ndemonstrations from people. And these either could be\nexplicit demonstrations.",
    "start": "371710",
    "end": "377530"
  },
  {
    "text": "Like, I show the robot\nhow to pick up a cup, and it records all my\nmovements, and then",
    "start": "377530",
    "end": "383290"
  },
  {
    "text": "you can use that\nfor later training; or it could just be\nnatural trajectories. So you take electronic\nmedical record systems,",
    "start": "383290",
    "end": "390453"
  },
  {
    "text": "and you just look\nat the decisions that are made from doctors. And we use that to try to\neither equal doctor performance",
    "start": "390453",
    "end": "397300"
  },
  {
    "text": "or exceed doctor performance. So often, we just\nhave observation data, which may either just be done\nin normal sort of business",
    "start": "397300",
    "end": "403790"
  },
  {
    "text": "as usual, or that is explicitly\nbeing given as a demonstration trajectory. And this is just going to be the\nsequence of states and actions.",
    "start": "403790",
    "end": "412000"
  },
  {
    "text": "We're not going to have\nrewards in general. And so the idea was, well, it\nmight be easier in some cases",
    "start": "412000",
    "end": "419280"
  },
  {
    "text": "either because it's just\nsort of natural data traces that are being generated as\npart of their normal work,",
    "start": "419280",
    "end": "424930"
  },
  {
    "text": "like, electronic medical\nrecord systems are; or because it's hard\nfor people to write down",
    "start": "424930",
    "end": "432090"
  },
  {
    "text": "a reward function\nthat kind of captures all the complexity of\nwhat they're trying to do in their objective.",
    "start": "432090",
    "end": "437820"
  },
  {
    "text": "So that was one of the\nmotivations for this. And we saw a few\ndifferent ways to try to think about this setting\nlast time, including",
    "start": "437820",
    "end": "445430"
  },
  {
    "text": "behavior cloning, where\nwe just map things back to supervised learning. And we try to learn a policy\ndirectly to match the expert.",
    "start": "445430",
    "end": "452789"
  },
  {
    "text": "We saw DAGGER. I'll put that on here, too. So another thing that we saw\nkind of in between these two",
    "start": "452790",
    "end": "459150"
  },
  {
    "text": "was DAGGER, which tried to\naddress a challenge of behavior",
    "start": "459150",
    "end": "464250"
  },
  {
    "text": "cloning, which is that\nwhen you make mistakes in your supervised\nlearning system,",
    "start": "464250",
    "end": "469780"
  },
  {
    "text": "you may end up in parts of the\nstate and action distribution that you don't know. You don't have good coverage.",
    "start": "469780",
    "end": "475600"
  },
  {
    "text": "So we talked about this kind\nof race car track example where once you go off, you've\ngot a distribution mismatch.",
    "start": "475600",
    "end": "483629"
  },
  {
    "text": "And we'll hear more about\ndistribution mismatches in RL later in the course. And there we wouldn't\nnecessarily know what to do.",
    "start": "483630",
    "end": "490420"
  },
  {
    "text": "And so what DAGGER said is we\nhave to keep an expert around, and then they will always tell\nus what you should have done.",
    "start": "490420",
    "end": "496360"
  },
  {
    "text": "So they're kind of\na coach go back. They replay how you did\nin that hockey game; what you should have\ndone at each moment.",
    "start": "496360",
    "end": "502378"
  },
  {
    "text": "And there's a lot of really\ninteresting questions of thinking about\nthose counterfactuals. And then we thought about\nthis broad question of, well,",
    "start": "502378",
    "end": "509530"
  },
  {
    "text": "could we recover the reward from\nlooking at these demonstrations?",
    "start": "509530",
    "end": "514830"
  },
  {
    "text": "And this could be useful\nwithin its own right to try to understand\nthe objectives",
    "start": "514830",
    "end": "520112"
  },
  {
    "text": "that people are\nusing when they're making their decisions\nfor different areas, as well as potentially for\nlearning a better policy",
    "start": "520113",
    "end": "527500"
  },
  {
    "text": "or learning the policy. And then can we also-- one we have that r--\ngenerate a good policy",
    "start": "527500",
    "end": "533920"
  },
  {
    "text": "or generate a good\npolicy directly? So one of the ideas that we\ntalked about in this case",
    "start": "533920",
    "end": "540870"
  },
  {
    "text": "is, well, what is sufficient to\nbe able to accomplish mimicking?",
    "start": "540870",
    "end": "546690"
  },
  {
    "text": "So in particular,\nwe said, well, if we want to get a policy that\nmatches the expert, that",
    "start": "546690",
    "end": "553320"
  },
  {
    "text": "is equivalent to generating\ntrajectories, where that distribution over\nthose trajectories",
    "start": "553320",
    "end": "558750"
  },
  {
    "text": "is the same as what the\nexpert would have done. So we think of this strong\nrelationship between policies",
    "start": "558750",
    "end": "567660"
  },
  {
    "text": "to trajectories, which also\nis to states and actions",
    "start": "567660",
    "end": "574660"
  },
  {
    "text": "because we can\nthink of there being a policy that induces a\ndistribution over states",
    "start": "574660",
    "end": "580029"
  },
  {
    "text": "and actions. And two policies that\ninduce the same distribution over states and actions\nwill have the same reward",
    "start": "580030",
    "end": "587420"
  },
  {
    "text": "because we're assuming\nthat the rewards are only a function of the\nstates and actions. And so we talked\nabout how people",
    "start": "587420",
    "end": "594520"
  },
  {
    "text": "had sort of leveraged\nthis assumption to think about different ways\nto try to learn reward features.",
    "start": "594520",
    "end": "600740"
  },
  {
    "text": "So, for example, if you\nhave a set of features to buy your policy-- so this might be mu,\nwhich could be things",
    "start": "600740",
    "end": "607209"
  },
  {
    "text": "like, how quickly a call\nservice agent responds",
    "start": "607210",
    "end": "612850"
  },
  {
    "text": "to a calls; how many times\nthey use positive sentiment, things like that. And of course, in\nthe case of a robot,",
    "start": "612850",
    "end": "619160"
  },
  {
    "text": "it might be how many times they\nhit a wall, how far it went, others, any of these\nsorts of features.",
    "start": "619160",
    "end": "624730"
  },
  {
    "text": "You could imagine that your\nreward function is just a linear combination\nof those features.",
    "start": "624730",
    "end": "630190"
  },
  {
    "text": "And so we saw-- So these features are just\nthings that people can come up",
    "start": "630190",
    "end": "635620"
  },
  {
    "text": "with for every problem? Great question. So [INAUDIBLE] asked,\nare these features like people are writing\ndown per problem?",
    "start": "635620",
    "end": "642350"
  },
  {
    "text": "Historically, yes, I think\none of the big things with deep learning\nhas been like, let's at least go as close\nto the sensors as possible.",
    "start": "642350",
    "end": "649130"
  },
  {
    "text": "So can we use just images\ninstead of features on images? But in the case of something,\nlike, say, online marketing,",
    "start": "649130",
    "end": "656210"
  },
  {
    "text": "a lot of them would be\npotentially predefined. So purchases and what\nweb pages you looked at,",
    "start": "656210",
    "end": "661750"
  },
  {
    "text": "and what things you--\nsearch queries you did. So you would have\nto still enumerate a set of features in\nthis case that you're",
    "start": "661750",
    "end": "667960"
  },
  {
    "text": "defining your reward over. But ideally, it's sort of\nas close to the sensor level of the data you're\ncollecting as possible,",
    "start": "667960",
    "end": "674710"
  },
  {
    "text": "or at least that often\nhas a big advantage. So what we saw here\nis that essentially, because we assume if\nthings are linear,",
    "start": "674710",
    "end": "681800"
  },
  {
    "text": "and we assume there's just\nthis unknown weight vector-- so this is a vector. This is a vector-- we could say if you\ncould have, make sure",
    "start": "681800",
    "end": "688510"
  },
  {
    "text": "that your distribution over\nfeatures is really close. And if you bound the norm\nof the weight vector,",
    "start": "688510",
    "end": "694800"
  },
  {
    "text": "then being really\nclose in features is the same as being really\nclose in reward, which means if your policy can\ninduce the same features,",
    "start": "694800",
    "end": "701760"
  },
  {
    "text": "you can get the same reward. This is a recap from\nlast time, but it's useful to think about\nas we go forward.",
    "start": "701760",
    "end": "707870"
  },
  {
    "text": "So one of the big challenges\nwe talked about last time is that there is not a\nunique reward function that",
    "start": "707870",
    "end": "713510"
  },
  {
    "text": "is compatible with\nthe observed data, even if you assume your\nobserved data is optimal.",
    "start": "713510",
    "end": "719120"
  },
  {
    "text": "So we talked about\nhow even the zero reward is compatible with\nany policy you might see.",
    "start": "719120",
    "end": "724380"
  },
  {
    "text": "And so in general, it's\nnot can be identifiable. We can't just say, if we\nobserve these trajectories,",
    "start": "724380",
    "end": "729950"
  },
  {
    "text": "and we know the\npolicy is optimal, this is what the reward is. There's too many rewards\nthat are compatible.",
    "start": "729950",
    "end": "735692"
  },
  {
    "text": "And so what we're going to\nspend a lot of time on now is to think about one choice\nfor how to break that ambiguity.",
    "start": "735692",
    "end": "742310"
  },
  {
    "text": "And this is where we\nleft off last time. And what we're going to focus\non now is Maximum Entropy IRL.",
    "start": "742310",
    "end": "753480"
  },
  {
    "text": "GAIL is also-- the second\none is known as GAIL. This is also a popular approach.",
    "start": "753480",
    "end": "758570"
  },
  {
    "text": "This was developed by Stefano\nErmon's group here at Stanford. But we're going to\nstart with max entropy",
    "start": "758570",
    "end": "764340"
  },
  {
    "text": "because also, there's a lot\nof other follow-up things that could be useful\nfrom this idea.",
    "start": "764340",
    "end": "769610"
  },
  {
    "text": "OK, so we're going to talk\nabout Max Entropy Inverse RL. This came out in 2008.",
    "start": "769610",
    "end": "775820"
  },
  {
    "text": "And it goes first with the\nprinciple of maximum entropy. Raise your hand if you've heard\nof this before in the context",
    "start": "775820",
    "end": "782450"
  },
  {
    "text": "of probability distributions. OK, a few people-- more\nthan I would have expected.",
    "start": "782450",
    "end": "787460"
  },
  {
    "text": "Cool. All right, so remember that the\nentropy of a distribution p-- so think of this is a\nprobability distribution.",
    "start": "787460",
    "end": "794940"
  },
  {
    "text": "So remember, we've got this. This is something. So we'd have sum\nover all [? s ?] if you have a\ndiscrete state space.",
    "start": "794940",
    "end": "803160"
  },
  {
    "text": "This is just a\nprobability distribution. OK. So the entropy of a\nprobability distribution",
    "start": "803160",
    "end": "811340"
  },
  {
    "text": "is minus the sum over all of\nthe states, the probability of that state, times the log of\nthe probability of that state.",
    "start": "811340",
    "end": "818990"
  },
  {
    "text": "It helps capture how\ndistributed our distribution is. And what the principle\nof max entropy says",
    "start": "818990",
    "end": "826790"
  },
  {
    "text": "is that the probability\ndistribution, which best represents the current\nstate of knowledge--",
    "start": "826790",
    "end": "833925"
  },
  {
    "text": "what do we mean by\ncurrent state of knowledge is if we have some previous\ndata, the one that we should",
    "start": "833925",
    "end": "840350"
  },
  {
    "text": "pick is the-- so the probability distribution\nwe should write down is the one with the\nlargest entropy, given the constraints of the\nprecisely-stated prior data.",
    "start": "840350",
    "end": "850040"
  },
  {
    "text": "So you can imagine you\nhave your expert data. And what this says\nis that-- and we haven't talked about what these\nprobability distributions will",
    "start": "850040",
    "end": "857180"
  },
  {
    "text": "be yet. But what this says\nis that we're going to try to write down\ndistributions over your--",
    "start": "857180",
    "end": "862710"
  },
  {
    "text": "well, we're going to look at\ntrajectories in particular-- that are compatible with\nour observed trajectories,",
    "start": "862710",
    "end": "870620"
  },
  {
    "text": "but otherwise have\nthe highest entropy. And so, intuitively, you could\nthink of if you have some data,",
    "start": "870620",
    "end": "877590"
  },
  {
    "text": "you want to find probability\ndistributions that are consistent with that,\nbut have the highest entropy,",
    "start": "877590",
    "end": "882920"
  },
  {
    "text": "given that they're consistent. So we're going to\nend up with something where you have constraints. Yeah. I don't understand\nthe motivation",
    "start": "882920",
    "end": "889050"
  },
  {
    "text": "of imitation learning. I'm trying to not\ndeploy the expert model because it's expensive.",
    "start": "889050",
    "end": "895110"
  },
  {
    "text": "I would try to\ndistill the model. What are we trying to do\nwith imitation learning? Yeah, what's the\nmotivation for this?",
    "start": "895110",
    "end": "901650"
  },
  {
    "text": "Because we already have\naccess to an expert model while learning\nanother model.",
    "start": "901650",
    "end": "907160"
  },
  {
    "text": "Well, the idea is that you\nhave access to trajectories from the expert. So you don't have access\nto the expert at all point.",
    "start": "907160",
    "end": "914380"
  },
  {
    "text": "You don't have their policy. You just have observations. So you can imagine something\nlike, if I'm an expert doctor,",
    "start": "914380",
    "end": "921250"
  },
  {
    "text": "you could look at all of\nthe ways that I do surgery. And you could look at all\nof my movements and stuff. And then what I\nwant to do is have",
    "start": "921250",
    "end": "927527"
  },
  {
    "text": "a robot that can imitate that. And so I need to distill it\nand make it sort of intuitive, an explicit,\nparameterized policy.",
    "start": "927527",
    "end": "934060"
  },
  {
    "text": "Doe that answer your question? Yeah. OK, cool. All right, so this is\nan interesting idea.",
    "start": "934060",
    "end": "939899"
  },
  {
    "text": "This is sort of saying this\nis one way to break ties. There's a whole bunch of\ndifferent reward functions, a whole bunch of different\nways you could maybe",
    "start": "939900",
    "end": "946218"
  },
  {
    "text": "be compatible with\nthe observed data. Let's pick ones which have\nthe maximum entropy, OK?",
    "start": "946218",
    "end": "952170"
  },
  {
    "text": "So this is just a\nchoice you could make. What does it mean for a\nprobability distribution",
    "start": "952170",
    "end": "957262"
  },
  {
    "text": "to be consistent? Great question. So hold on to that for a second. I'm going to say-- yeah,\nso the question was,",
    "start": "957262",
    "end": "963200"
  },
  {
    "text": "what does this mean? How do we actually make this\nmathematically formal and algorithmic? And we'll see that\nin the next slide.",
    "start": "963200",
    "end": "969280"
  },
  {
    "text": "It's a good question. We're going to write this\ndown in a formal way. But this is the principle. And so this is what Brian\nZiebart and his colleagues",
    "start": "969280",
    "end": "976130"
  },
  {
    "text": "thought about in\nterms of this method. And I'll just say a little\nbit about the motivation. So Brian was a grad student at\nthe time at Carnegie Mellon,",
    "start": "976130",
    "end": "983850"
  },
  {
    "text": "and they were interested in\ntrying to understand taxi driver behavior. And so what they\nwanted to do is--",
    "start": "983850",
    "end": "989072"
  },
  {
    "text": "when you're driving, there's\nlots of different constraints. Particularly, if\nyou're a taxi driver, you want to think about\ndistance and potential traffic",
    "start": "989072",
    "end": "995420"
  },
  {
    "text": "and tolls and all these things. And so what they\nwanted to do is just to take trajectories\nof people driving",
    "start": "995420",
    "end": "1000760"
  },
  {
    "text": "through the streets\nof Pittsburgh and then try to infer what\nthe reward function was",
    "start": "1000760",
    "end": "1006100"
  },
  {
    "text": "that taxi drivers\nwere using, as well as be able to have\na policy that did as well as good taxi drivers.",
    "start": "1006100",
    "end": "1012830"
  },
  {
    "text": "So this was sort of\npart of the motivation. And again, they had to deal with\nthis question of how do you--",
    "start": "1012830",
    "end": "1018209"
  },
  {
    "text": "you can't just learn\na unique reward, so let's just try\nto find something that's got maximum entropy. And let's see what this\nmeans in this case.",
    "start": "1018210",
    "end": "1025569"
  },
  {
    "text": "All right, so in the\nlinear reward case, what we're going to\nbe interested in,",
    "start": "1025569",
    "end": "1031089"
  },
  {
    "text": "or how we're going to think\nabout where max entropy applies is to say, we're going to have\ndistributions over trajectories.",
    "start": "1031089",
    "end": "1038409"
  },
  {
    "text": "So we're going have\ndistributions over trajectories. And we want to find a\ndistribution of trajectories",
    "start": "1038410",
    "end": "1044410"
  },
  {
    "text": "that matches our observed\ndistribution over trajectories from the expert, but otherwise\nhas really high entropy.",
    "start": "1044410",
    "end": "1052990"
  },
  {
    "text": "So what you could be\nlearning in this case is a probability distribution\nover trajectories",
    "start": "1052990",
    "end": "1059260"
  },
  {
    "text": "that has the maximum\nentropy subject to the fact that it is a true\nprobability distribution.",
    "start": "1059260",
    "end": "1065710"
  },
  {
    "text": "So that's one constraint. So this is just subject to-- using a subject to or such\nthat, but the other ones",
    "start": "1065710",
    "end": "1070960"
  },
  {
    "text": "are constraints. And the other is\nthat, in this case, we're going to say we're going\nto want to match the features.",
    "start": "1070960",
    "end": "1079520"
  },
  {
    "text": "And we saw before that\nmatching the features was equivalent to being able to\nmatch the rewards in the case",
    "start": "1079520",
    "end": "1086060"
  },
  {
    "text": "where you have a\nlinear function. So in the linear\nreward case, what",
    "start": "1086060",
    "end": "1093600"
  },
  {
    "text": "we want to do is we\nwant to say, I've got my distribution\nof trajectories. Let's say mu is a function\nthat just takes the trajectory",
    "start": "1093600",
    "end": "1100200"
  },
  {
    "text": "and outputs a set of features. And we'll talk about some of\nthe choices for that soon. And we just want\nthat to match what",
    "start": "1100200",
    "end": "1106590"
  },
  {
    "text": "the features where we\nobserved from the trajectories from D, where D is a data\nset from our experts.",
    "start": "1106590",
    "end": "1113490"
  },
  {
    "text": "This is from our experts. ",
    "start": "1113490",
    "end": "1119950"
  },
  {
    "text": "OK, so this is how we\nwould write that down. Now, I haven't told\nyou yet how we're going to learn the reward function.",
    "start": "1119950",
    "end": "1125390"
  },
  {
    "text": "I haven't even told you how\nwe're going to learn this, but this is where\nthe maximum entropy",
    "start": "1125390",
    "end": "1130750"
  },
  {
    "text": "assumption is being applied. It's saying what we mean\nby maximum entropy is we want to think about getting\na distribution over trajectories",
    "start": "1130750",
    "end": "1137410"
  },
  {
    "text": "that is compatible\nwith our expert data, but otherwise has\nthe maximum entropy. Yeah. Remind me your\nname one more time.",
    "start": "1137410",
    "end": "1144580"
  },
  {
    "text": "When you say distribution\nover trajectories, does that mean\ndistribution over policies that create that trajectory?",
    "start": "1144580",
    "end": "1149930"
  },
  {
    "text": "Or is that something else? Great question. That sort of isomorphic. So you can just think\nof it directly as being distributions over state\naction, state action, et cetera.",
    "start": "1149930",
    "end": "1159230"
  },
  {
    "text": "Or you can think of\nit as it's implicitly going through a policy\nthat is generating this. Yeah. And we'll become\nclearer, too, about where",
    "start": "1159230",
    "end": "1166930"
  },
  {
    "text": "the policies come in. Great question. So this is what this\nwould say, but we",
    "start": "1166930",
    "end": "1172570"
  },
  {
    "text": "haven't got into rewards yet. And we need to think\nabout how do we go from this to thinking\nabout learning a reward model",
    "start": "1172570",
    "end": "1177670"
  },
  {
    "text": "and learning policies. So in general, we\ndon't have rewards,",
    "start": "1177670",
    "end": "1182863"
  },
  {
    "text": "but if we did have rewards,\nwhat we would like to do is to get a policy that\ninduces trajectories that match",
    "start": "1182863",
    "end": "1189830"
  },
  {
    "text": "the same reward as our expert. So we would like to\nget a policy that has",
    "start": "1189830",
    "end": "1196789"
  },
  {
    "text": "as high reward as our expert. If we knew what\nthose were, like,",
    "start": "1196790",
    "end": "1201840"
  },
  {
    "text": "if we had a way like\nthis r phi, then we would say we want\nr distribution.",
    "start": "1201840",
    "end": "1207420"
  },
  {
    "text": "So let's say we're going\nto learn a distribution over trajectories. We want this to be the\nsame as what the expert is.",
    "start": "1207420",
    "end": "1214230"
  },
  {
    "text": "And I'll just\nhighlight that here. I'm using this p hat for expert. OK. ",
    "start": "1214230",
    "end": "1221900"
  },
  {
    "text": "So this is expert. So this looks almost\nthe same as above, except for I've said,\nwell, let's imagine",
    "start": "1221900",
    "end": "1227990"
  },
  {
    "text": "that we don't necessarily\nhave to have a linear reward function. In general, we just want\nto say we would really",
    "start": "1227990",
    "end": "1233878"
  },
  {
    "text": "like that whatever our\ndistribution of trajectories is that it matches the\nreward of the experts",
    "start": "1233878",
    "end": "1239005"
  },
  {
    "text": "because we know the\nexperts is optimal. So if we achieve\nthis, we're good. So we would like to be\nable to solve this problem.",
    "start": "1239005",
    "end": "1245040"
  },
  {
    "text": "We don't know what r is\nstill, so we can't do this. But we're just going to\nlook at what would be",
    "start": "1245040",
    "end": "1250070"
  },
  {
    "text": "the solution to this problem. ",
    "start": "1250070",
    "end": "1255440"
  },
  {
    "text": "And where we're\ngoing to go from this is that we're ultimately\ngoing to end up with an algorithm that does\nsomething like the following.",
    "start": "1255440",
    "end": "1261940"
  },
  {
    "text": "We are going to assume we have a\nreward function, or compute one. Once we have a reward\nfunction, we're",
    "start": "1261940",
    "end": "1267570"
  },
  {
    "text": "going to learn an\noptimal policy, and then we're going to\nupdate our state or trajectory features to update\nour reward function,",
    "start": "1267570",
    "end": "1274029"
  },
  {
    "text": "and we're going to\ndo this many times. So we're going to be\nthinking really a lot about the relationship between reward\nfunctions to optimal policies,",
    "start": "1274030",
    "end": "1283480"
  },
  {
    "text": "optimal policies to\ndistributions over states and actions, distributions\nover states and actions to how can we update\nour reward function.",
    "start": "1283480",
    "end": "1291270"
  },
  {
    "text": "And we're going to step\nthrough all of those steps. And in the original paper there,\nassume the dynamics reward",
    "start": "1291270",
    "end": "1298290"
  },
  {
    "text": "model is known. All right, so let's step\nthrough the first part because I think it's\nreally helpful to see--",
    "start": "1298290",
    "end": "1306450"
  },
  {
    "text": "often, when people\ntalk about max entropy, then they introduce this\nsort of exponential family,",
    "start": "1306450",
    "end": "1311620"
  },
  {
    "text": "and it may or may not be\nclear where that comes from. So remember that we have\nthis constrained objective.",
    "start": "1311620",
    "end": "1317090"
  },
  {
    "text": "So we have this thing here. ",
    "start": "1317090",
    "end": "1323309"
  },
  {
    "text": "All right. So what we would like to\nunderstand in this case is given constrained\nobjective, if we",
    "start": "1323310",
    "end": "1339740"
  },
  {
    "text": "knew the costs, what would be\nthe form of the distribution.",
    "start": "1339740",
    "end": "1354914"
  },
  {
    "text": " Over tau?",
    "start": "1354914",
    "end": "1361840"
  },
  {
    "text": "OK. Because remember, what we've\ngot here is we have a max. So what this thing is\nthis is an objective.",
    "start": "1361840",
    "end": "1367990"
  },
  {
    "text": "This is an optimization\nproblem that says the right distribution\nover trajectories you want is the one that maximizes\nthat expression there.",
    "start": "1367990",
    "end": "1375644"
  },
  {
    "text": "And what we're\ngoing to do now is we're going to see\nwhat would it be like? If we knew all of\nthose things, what",
    "start": "1375645",
    "end": "1381580"
  },
  {
    "text": "would the sort of\nstructural form look like? And then we're going to use\nthat to make some other steps.",
    "start": "1381580",
    "end": "1386660"
  },
  {
    "text": "So now this is just\nto get intuition over this functional form. What we're going to do is\nwe're going to rewrite this",
    "start": "1386660",
    "end": "1392798"
  },
  {
    "text": "as using Lagrange multipliers. OK, so we've got p here. We're going to introduce lambda.",
    "start": "1392798",
    "end": "1399520"
  },
  {
    "text": "And I'm just going to\nwrite this as follows.",
    "start": "1399520",
    "end": "1405780"
  },
  {
    "start": "1405780",
    "end": "1415440"
  },
  {
    "text": "And I think this is\nillustrative because it'll make it really clear where\nthe structural forms come in",
    "start": "1415440",
    "end": "1421680"
  },
  {
    "text": "that we're going to use. OK. all right, so I'm just writing\ndown our first Lagrange",
    "start": "1421680",
    "end": "1429300"
  },
  {
    "text": "multiplier. And I suspect most of\nyou have seen this, but if you haven't, feel free\nto come up to me afterwards.",
    "start": "1429300",
    "end": "1435220"
  },
  {
    "text": "OK, we're just rewriting the\nconstraint optimization problem. So we rewrote our constraint\noptimization problem",
    "start": "1435220",
    "end": "1442320"
  },
  {
    "text": "as a single equation,\nand now we're going to take the\nderivative with respect to this because remember,\nwe want to optimize this.",
    "start": "1442320",
    "end": "1449200"
  },
  {
    "text": "So we're going to\ndo D. We're going to do it with respect\nto our trajectories.",
    "start": "1449200",
    "end": "1455970"
  },
  {
    "text": "So we're just going\nto get log of p of tau plus P of tau times\nthe derivative of log, which",
    "start": "1455970",
    "end": "1463200"
  },
  {
    "text": "is just 1 over p of tau plus-- ",
    "start": "1463200",
    "end": "1473280"
  },
  {
    "text": "and in the third case, the third\none doesn't have any p of tau.",
    "start": "1473280",
    "end": "1478990"
  },
  {
    "text": "This term does here. So you'll get lambda\n1 r phi of tau.",
    "start": "1478990",
    "end": "1485360"
  },
  {
    "text": " Yeah. Is this something\nthat shouldn't go away",
    "start": "1485360",
    "end": "1491037"
  },
  {
    "text": "because we're taking the\nderivative with respect to one specific trajectory? Yeah, exactly.",
    "start": "1491037",
    "end": "1496395"
  },
  {
    "text": "So we're going to\nassume we're taking-- we're trying to get what is the\nderivative with respect to 1-- this probability of this\nparticular trajectory tau.",
    "start": "1496395",
    "end": "1503930"
  },
  {
    "text": "That's why everything goes away. And the important\nthing to notice here is this goes away because\nthere's only a p hat there.",
    "start": "1503930",
    "end": "1514309"
  },
  {
    "text": "That was from the expert. So that term just disappears. It's not a function\nof p of tau at all.",
    "start": "1514310",
    "end": "1520279"
  },
  {
    "text": "all right, so now we want\nto set this equal to 0. Set this to 0 because\nwe want to find the max,",
    "start": "1520280",
    "end": "1526790"
  },
  {
    "text": "and then we're going to\njust do some algebra. OK. ",
    "start": "1526790",
    "end": "1540693"
  },
  {
    "text": "Now, we're just going\nto exponentiate. ",
    "start": "1540693",
    "end": "1550280"
  },
  {
    "text": "OK, why did we do this? Because I wanted to illustrate\nthat what this means is that the probability\ndistribution over trajectories,",
    "start": "1550280",
    "end": "1559250"
  },
  {
    "text": "which maximize the entropy\nsubject to some constraints, is exactly proportional to--",
    "start": "1559250",
    "end": "1566330"
  },
  {
    "text": "this is the proportional side-- the exponential of the reward\nfor that trajectory, which",
    "start": "1566330",
    "end": "1577180"
  },
  {
    "text": "means that, in general,\nif you observe this, you would put sort of\nexponential more weight",
    "start": "1577180",
    "end": "1582940"
  },
  {
    "text": "on things that\nhave higher reward, subject to a constraint that you\nhave a probability distribution.",
    "start": "1582940",
    "end": "1591790"
  },
  {
    "text": "All right. So what this shows\nhere is that if we",
    "start": "1591790",
    "end": "1599050"
  },
  {
    "text": "want to take this\nprinciple of max entropy, then what we end up getting\nis that the functional form over our trajectories\nis this exponential.",
    "start": "1599050",
    "end": "1607799"
  },
  {
    "text": "It's proportional\nto an exponential. And that's an exponential\nfamily for those of you who have seen this before or\nseen exponential families.",
    "start": "1607800",
    "end": "1615740"
  },
  {
    "text": "So this is like the\nstructural form. This is the distribution\nthat maximizes the entropy.",
    "start": "1615740",
    "end": "1621130"
  },
  {
    "text": "And so once we know\nthat, we can leverage that to now start to try\nto learn a reward function.",
    "start": "1621130",
    "end": "1626920"
  },
  {
    "text": "So let's see how we do this. Because remember,\nwe just did this, assuming that our r phi was\nknown, like, for a particular r",
    "start": "1626920",
    "end": "1632910"
  },
  {
    "text": "phi. We're not taking a derivative\nwith respect to phi here at all. It's just a derivative\nwith respect to p of tau.",
    "start": "1632910",
    "end": "1639280"
  },
  {
    "text": "All right, so what\nthis means is that we can think of maximizing the\nentropy over the probability",
    "start": "1639280",
    "end": "1647020"
  },
  {
    "text": "distribution with\nrespect to tau is equal to maximizing the\nlikelihood of the observed data under this particular\nmax entropy distribution.",
    "start": "1647020",
    "end": "1655690"
  },
  {
    "text": "So I'm going to just write\nout what that would be here. So remember, that's\nwhat we saw here.",
    "start": "1655690",
    "end": "1660980"
  },
  {
    "text": "We saw that if we max entropy,\nthe functional form we get, looks like this\nnormalized exponential.",
    "start": "1660980",
    "end": "1667270"
  },
  {
    "text": "So in particular, we'll just\nwrite that out again here. So what we get is we say\nthe probability trajectory,",
    "start": "1667270",
    "end": "1675970"
  },
  {
    "text": "probability of a\nparticular directory tau I, given some\nreward model phi,",
    "start": "1675970",
    "end": "1681730"
  },
  {
    "text": "is equal to 1 over z of phi. And I'll define that\nin just a second. e r phi tau I, where z\nof phi is our normalizing",
    "start": "1681730",
    "end": "1697230"
  },
  {
    "text": "constant because we have to\nhave a well-formed probability distribution.",
    "start": "1697230",
    "end": "1704230"
  },
  {
    "text": "So let's say this is\nstructurally like what it looks like. And notice that we can also\nwrite this in terms of states.",
    "start": "1704230",
    "end": "1715750"
  },
  {
    "text": "So this is also equal to e to\nthe sum over all the states",
    "start": "1715750",
    "end": "1721600"
  },
  {
    "text": "inside of your trajectory. ",
    "start": "1721600",
    "end": "1728052"
  },
  {
    "text": "where I'm sort of abusing\nnotation a little bit to both use r phi of\ntau or r phi of state just to mean the reward you\nget from a particular state,",
    "start": "1728052",
    "end": "1734480"
  },
  {
    "text": "or the reward you get\nfrom a whole trajectory. So notice we can use each of\nthese, and this is our thing.",
    "start": "1734480",
    "end": "1741179"
  },
  {
    "text": "So why is this helpful? So we don't know what\nthe reward function is.",
    "start": "1741180",
    "end": "1752470"
  },
  {
    "text": "We don't actually\nhave that, right? Yes. But what this means\nis that since we",
    "start": "1752470",
    "end": "1759420"
  },
  {
    "text": "know what the functional form\nis of the probability of tau under the max entropy\nprinciple, we can now say,",
    "start": "1759420",
    "end": "1767290"
  },
  {
    "text": "OK, I'm not going to\nworry about this part. I'm going to assume this\nis the structural form. Now my unknown is just phi.",
    "start": "1767290",
    "end": "1773790"
  },
  {
    "text": "Now I'm going to try to\nmaximize the likelihood of my observed data by changing\nthe parameterization of phi.",
    "start": "1773790",
    "end": "1780870"
  },
  {
    "text": "So this observation.",
    "start": "1780870",
    "end": "1786210"
  },
  {
    "text": "And when I say\nthis observation, I mean that the probability of\ntau that maximizes entropy,",
    "start": "1786210",
    "end": "1803580"
  },
  {
    "text": "constrained entropy, looks like\nnormalized exponential means",
    "start": "1803580",
    "end": "1820100"
  },
  {
    "text": "we can now estimate or\nlearn r phi by maximizing",
    "start": "1820100",
    "end": "1834630"
  },
  {
    "text": "the probability of\nour observed data. ",
    "start": "1834630",
    "end": "1842080"
  },
  {
    "text": "So we're going to treat this as\na maximum likelihood problem. ",
    "start": "1842080",
    "end": "1848290"
  },
  {
    "text": "All right, and I'll\njust note here. This is a really\nelegant observation.",
    "start": "1848290",
    "end": "1853820"
  },
  {
    "text": "This came all the way\nback from Jaynes in 1957. So when people were\nthinking about what does it mean to maximize the\nentropy of something-- subject",
    "start": "1853820",
    "end": "1861280"
  },
  {
    "text": "to some constraints--\nthey realized that you could make\nthis could convert it to this exponential family.",
    "start": "1861280",
    "end": "1866889"
  },
  {
    "text": "And then once you\nhave that, now you have something where\nyour uncertainty is only with respect to this phi.",
    "start": "1866890",
    "end": "1872950"
  },
  {
    "text": "And in fact, this type of\ninsight at a very high level will be related to what\nyou'll see next week in terms of direct preference\noptimization,",
    "start": "1872950",
    "end": "1879158"
  },
  {
    "text": "where sometimes we might\nbe able to reparametrize our objective function to\nbe able to get rid of-- you might call them almost\nnuisance parameters-- things",
    "start": "1879158",
    "end": "1886179"
  },
  {
    "text": "that you might not\ncare about directly-- where you have one parameter\nyou really want to learn.",
    "start": "1886180",
    "end": "1891280"
  },
  {
    "text": "OK, so let's see how we can\ndo the maximum likelihood. So now we're going\nto try to do is we're going to try to actually\nlearn that reward function,",
    "start": "1891280",
    "end": "1897250"
  },
  {
    "text": "and we're going to\nleverage the fact that we the structural form of this\nprobability distribution over the trajectories.",
    "start": "1897250",
    "end": "1902520"
  },
  {
    "text": "So what we're going\nto do is we're going to say we're\ngoing to maximize phi of log, the probability\nof all of our data.",
    "start": "1902520",
    "end": "1911300"
  },
  {
    "text": "This is our expert\nof the probability",
    "start": "1911300",
    "end": "1916470"
  },
  {
    "text": "of each of those trajectories. So we're just saying\nwe're going to try to maximize the probability that\nwe observed the data that we",
    "start": "1916470",
    "end": "1922342"
  },
  {
    "text": "did under our reward function. And because of our\nstructural form,",
    "start": "1922342",
    "end": "1927580"
  },
  {
    "text": "we can rewrite this as follows. This is going to be a sum. So I'm just going to\nsay log of product",
    "start": "1927580",
    "end": "1933460"
  },
  {
    "text": "is the same as\nsum over the logs.",
    "start": "1933460",
    "end": "1938799"
  },
  {
    "text": "Then I'm going to plug\nin what my form told me that my probability\ndistribution has to look",
    "start": "1938800",
    "end": "1945790"
  },
  {
    "text": "like for my trajectories. ",
    "start": "1945790",
    "end": "1958309"
  },
  {
    "text": "All right, so this\nis just me plugging in that sort of max entropy\nform of the trajectories.",
    "start": "1958310",
    "end": "1963920"
  },
  {
    "text": "And now I'm just going\nto split that apart. So I'm going to rewrite it.",
    "start": "1963920",
    "end": "1969880"
  },
  {
    "text": " The log and the\nexponential cancel,",
    "start": "1969880",
    "end": "1978040"
  },
  {
    "text": "and then I have log of\nthe normalizing term. ",
    "start": "1978040",
    "end": "1984590"
  },
  {
    "text": "Now notice that in\nterms of this part--",
    "start": "1984590",
    "end": "1992750"
  },
  {
    "text": "so notice that this is\nindependent of tau star.",
    "start": "1992750",
    "end": "2001970"
  },
  {
    "text": "So this is-- independent\nof all those, we end up with two things. We have max over phi\nsum over tau star",
    "start": "2001970",
    "end": "2009380"
  },
  {
    "text": "and our data set of r\nminus the size of our data",
    "start": "2009380",
    "end": "2015890"
  },
  {
    "text": "set times log sum over tau. ",
    "start": "2015890",
    "end": "2024659"
  },
  {
    "text": "And the reason that\nhappened there is this was all inside the sum. This sum, this was completely\nindependent of tau star.",
    "start": "2024660",
    "end": "2031060"
  },
  {
    "text": "So I could bring it out. The number of trajectories\nthat I have in D is just the cardinality of D.",
    "start": "2031060",
    "end": "2038430"
  },
  {
    "text": "All right, so now what we\ncan do is take a derivative. So I'm going to call\nthis whole thing J of phi",
    "start": "2038430",
    "end": "2051800"
  },
  {
    "text": "because it's all parametrized by\nmy particular reward function. I'm going to take the\nderivative of that",
    "start": "2051800",
    "end": "2057260"
  },
  {
    "text": "because, in general, we're\ngoing to do everything with gradient descent as usual. OK, so this is going to\nlook like the sum for all",
    "start": "2057260",
    "end": "2067820"
  },
  {
    "text": "my trajectories of\nmy expert data set, the derivative with respect\nto my reward function, minus--",
    "start": "2067820",
    "end": "2079158"
  },
  {
    "text": "let me see if I can make this\nnice and big for this part. OK.",
    "start": "2079159",
    "end": "2084360"
  },
  {
    "text": "So we're going to\nhave two things. We have this log. So we're going to have to\ntake the derivative of that. This all goes on the bottom.",
    "start": "2084360",
    "end": "2090349"
  },
  {
    "text": "And we're going\nto play a small-- we're going to observe\nsomething in just a second.",
    "start": "2090350",
    "end": "2095429"
  },
  {
    "text": "So then we're going to\nstill have our sum over tau e to the r 5 tau\ntimes the derivative.",
    "start": "2095429",
    "end": "2103950"
  },
  {
    "text": "So I'm just taking the\nderivative of that whole term. ",
    "start": "2103950",
    "end": "2110339"
  },
  {
    "text": "All right, the important\nthing to notice here-- so I just took the derivative\nof both of these parts.",
    "start": "2110340",
    "end": "2115450"
  },
  {
    "text": "This thing should look\na little bit familiar. This is, in fact, just\nthe exact expression",
    "start": "2115450",
    "end": "2122580"
  },
  {
    "text": "we got for what\nis the probability of a particular trajectory. let me just put that in.",
    "start": "2122580",
    "end": "2129890"
  },
  {
    "text": "So note this is just equal\nto the probability of tau,",
    "start": "2129890",
    "end": "2138880"
  },
  {
    "text": "given phi. Let me make sure I put given\nphi because that's just",
    "start": "2138880",
    "end": "2143950"
  },
  {
    "text": "this normalized exponential\ndivided by that. So we're going to have this.",
    "start": "2143950",
    "end": "2150730"
  },
  {
    "text": "When you put that-- I shall be careful. Let me just make sure\nI write that carefully. So this is going to go\nto this because this",
    "start": "2150730",
    "end": "2158530"
  },
  {
    "text": "is equal to e to the r phi of\ntau divided by the normalizing constant.",
    "start": "2158530",
    "end": "2164270"
  },
  {
    "text": "So we could move\nthis-- we can move this outside part into here.",
    "start": "2164270",
    "end": "2169760"
  },
  {
    "text": "And then that expression\nin there, which is this, is just equal to probability\nof tau given phi.",
    "start": "2169760",
    "end": "2177100"
  },
  {
    "text": "all right, so this-- move back to here.",
    "start": "2177100",
    "end": "2182670"
  },
  {
    "text": "OK, so we just end up\nwith the following-- we get the derivative\nwith respect",
    "start": "2182670",
    "end": "2188070"
  },
  {
    "text": "to the reward function for every\ntrajectory inside of our expert data minus the number of\ndifferent trajectories",
    "start": "2188070",
    "end": "2196060"
  },
  {
    "text": "we have times the sum\nover all trajectories, the probability of\nthat trajectory given",
    "start": "2196060",
    "end": "2202900"
  },
  {
    "text": "by times the derivative with\nrespect to phi at that point.",
    "start": "2202900",
    "end": "2213880"
  },
  {
    "text": "And that's our gradient step. So what this would\nsay here is if you",
    "start": "2213880",
    "end": "2220809"
  },
  {
    "text": "want to take a step\ntowards optimizing, then what we would do\nin this case is you",
    "start": "2220810",
    "end": "2227170"
  },
  {
    "text": "could compute the\nderivative with respect to your reward function. All right, we have a\nfew more steps to go.",
    "start": "2227170",
    "end": "2233890"
  },
  {
    "text": "So the next is-- this is all\nin terms of trajectories we'd like to get in terms of states.",
    "start": "2233890",
    "end": "2239310"
  },
  {
    "text": "So for that, we can just observe\nthe fact that, as before, the probability of a\ntrajectory can be broken down into its components.",
    "start": "2239310",
    "end": "2245346"
  },
  {
    "text": " So this is just\nequal to t equals 1",
    "start": "2245346",
    "end": "2252589"
  },
  {
    "text": "to length of your trajectory. The probability\nof your a given S is like your policy and\nthe probability of St",
    "start": "2252590",
    "end": "2261050"
  },
  {
    "text": "plus 1, given St and At. This is just the\nprobability of a trajectory. And we've seen this before.",
    "start": "2261050",
    "end": "2267115"
  },
  {
    "text": " If we have that the\nprobability of a trajectory",
    "start": "2267115",
    "end": "2275329"
  },
  {
    "text": "is proportional as we've\nseen to e to the minus r phi of a trajectory,\nand we know that we",
    "start": "2275330",
    "end": "2284180"
  },
  {
    "text": "can write that also is equal\nto e to the minus sum over s inside the trajectory\nof r phi of that state,",
    "start": "2284180",
    "end": "2294380"
  },
  {
    "text": "so then we can think of plugging\nthat in for our derivative. And what we get\nis the following.",
    "start": "2294380",
    "end": "2301158"
  },
  {
    "text": "So it's just the\nsame derivative now, but in terms of states\ninstead of trajectories. So this is for all the\nstates inside of your expert",
    "start": "2301158",
    "end": "2308359"
  },
  {
    "text": "demonstrations. You take the\nderivative with respect",
    "start": "2308360",
    "end": "2313610"
  },
  {
    "text": "to that state minus d sum\nover the states probability",
    "start": "2313610",
    "end": "2321500"
  },
  {
    "text": "of the state, given\nthat in the trajectory, and then the derivative\nwith respect to that state.",
    "start": "2321500",
    "end": "2327820"
  },
  {
    "text": " And why is this interesting?",
    "start": "2327820",
    "end": "2333398"
  },
  {
    "text": "This is interesting\nbecause basically what we're getting\nin this case is we're getting stuff that\nlooks like us trying",
    "start": "2333398",
    "end": "2338540"
  },
  {
    "text": "to match the distribution over\nstates that we see in the data set.",
    "start": "2338540",
    "end": "2345260"
  },
  {
    "text": "Now, when we think of doing\nthis, one other thing to note is where do these sort of\nstate densities come from?",
    "start": "2345260",
    "end": "2353900"
  },
  {
    "text": "So essentially you\ncould think of it as I have some observed\nstates and actions, and I'm going to think about it\nunder a different policy what",
    "start": "2353900",
    "end": "2361340"
  },
  {
    "text": "states and actions\nthat will induce? If you know the dynamics\nmodel, and if it's tabular--",
    "start": "2361340",
    "end": "2367920"
  },
  {
    "text": "so if it's tabular. So if you think back\nfor a few weeks ago,",
    "start": "2367920",
    "end": "2373770"
  },
  {
    "text": "tabular, and the\ndynamics are known-- ",
    "start": "2373770",
    "end": "2379890"
  },
  {
    "text": "then you can actually compute\nthe state action distribution directly using\ndynamic programming.",
    "start": "2379890",
    "end": "2387600"
  },
  {
    "text": "And pi is given. Then you can actually\njust directly compute",
    "start": "2387600",
    "end": "2393359"
  },
  {
    "text": "the states and actions. So let's see that. So you could say U1 of\nS is equal to P of Ss.",
    "start": "2393360",
    "end": "2400560"
  },
  {
    "text": "And then for t equals\n1 dot, dot, dot T. So this is time indexed.",
    "start": "2400560",
    "end": "2405642"
  },
  {
    "text": " OK, so this is like--\nagain, remember, the high level what\nwe're trying to do here",
    "start": "2405643",
    "end": "2411602"
  },
  {
    "text": "is we're learning that we're\ntrying to match the state action frequencies between our\nobserved policy from our experts",
    "start": "2411602",
    "end": "2418440"
  },
  {
    "text": "and what we induce under\nour reward function. What this is going\nto say is that you're going to try to estimate\na reward function.",
    "start": "2418440",
    "end": "2424680"
  },
  {
    "text": "You're going to try to compute\nan optimal policy given that reward function,\nand then you're going to try to count and\nsee what your state action",
    "start": "2424680",
    "end": "2430799"
  },
  {
    "text": "distribution looks like\nunder that resulting policy. If it matches your\nexperts, you're done. Otherwise, you need to\nkeep changing your reward",
    "start": "2430800",
    "end": "2438450"
  },
  {
    "text": "function, your policy, and\nthe resulting state action distribution until they match.",
    "start": "2438450",
    "end": "2444295"
  },
  {
    "text": "All right, so I'm just going\nto go through briefly so you can see how this is computable.",
    "start": "2444295",
    "end": "2450089"
  },
  {
    "text": "All right, so what\nthis would say is that your distribution of\nstates on the next time step",
    "start": "2450090",
    "end": "2456270"
  },
  {
    "text": "depends on your distribution\nof states on the previous time step, the probability\nunder your policy--",
    "start": "2456270",
    "end": "2467360"
  },
  {
    "text": "this is your policy. I shall be a little\ncareful there-- ",
    "start": "2467360",
    "end": "2475220"
  },
  {
    "text": "The probability of\nthe action, given the state, and the\nprobability of the state,",
    "start": "2475220",
    "end": "2481380"
  },
  {
    "text": "given the state and action. And you can use this then to\nsum up over all time steps.",
    "start": "2481380",
    "end": "2489310"
  },
  {
    "start": "2489310",
    "end": "2495890"
  },
  {
    "text": "what your average density\nis for a particular state. So what this means\nis that when you're",
    "start": "2495890",
    "end": "2503630"
  },
  {
    "text": "trying to actually\ncompute the derivative of your objective function\nwith respect to the reward, then what you end up getting\nis that you can plug these in.",
    "start": "2503630",
    "end": "2511260"
  },
  {
    "text": "And if you have-- so you can write down this. ",
    "start": "2511260",
    "end": "2525790"
  },
  {
    "text": "I'm going to get-- ",
    "start": "2525790",
    "end": "2532100"
  },
  {
    "text": "so you can see that\nit's fairly involved, but it is definitely possible. ",
    "start": "2532100",
    "end": "2538550"
  },
  {
    "text": "Sum over all your\nstates, probability of the states, given phi t,\nand your reward function.",
    "start": "2538550",
    "end": "2546170"
  },
  {
    "text": "And this will simplify\na bit if you have-- so let me just write out. If your r is equal to just this\ntimes some features, then when",
    "start": "2546170",
    "end": "2558320"
  },
  {
    "text": "you take the derivative\nwith respect to the phi, you just get the features.",
    "start": "2558320",
    "end": "2563819"
  },
  {
    "text": "So this would mean that dr phi\nof S. So if linear, just equal",
    "start": "2563820",
    "end": "2574120"
  },
  {
    "text": "to your features. OK, so I know this\nis a lot of algebra. But what this is saying\nis that your derivative",
    "start": "2574120",
    "end": "2581050"
  },
  {
    "text": "about how you want\nto change your reward will just end up being a sum\nover all the features you have inside of your data\nminus this additional term.",
    "start": "2581050",
    "end": "2590320"
  },
  {
    "text": "So you compute all\nof this with respect to your observed\nfeatures and the features",
    "start": "2590320",
    "end": "2596727"
  },
  {
    "text": "you have in your data set.  All right. So how does this all work when\nwe put this fully together?",
    "start": "2596727",
    "end": "2605300"
  },
  {
    "text": "What we have in this\ncase is you give as input some expert demonstrations. You initialize your phi.",
    "start": "2605300",
    "end": "2611570"
  },
  {
    "text": "And then what you do\nis the following-- you first compute an optimal\npolicy, given that r phi, e.g.",
    "start": "2611570",
    "end": "2619550"
  },
  {
    "text": "Was something like\nvalue iteration. You compute the state\nvisitation frequencies.",
    "start": "2619550",
    "end": "2624680"
  },
  {
    "text": "You compute the gradient\non the reward model, and then you update\nyour reward model phi,",
    "start": "2624680",
    "end": "2630410"
  },
  {
    "text": "and you repeat over\nand over again. ",
    "start": "2630410",
    "end": "2636800"
  },
  {
    "text": "All right, I'll just write out\nwhat that equation is here. So your derivative\nhere would be your sum",
    "start": "2636800",
    "end": "2642020"
  },
  {
    "text": "over all your trajectories\ninside of your data set, your features for each\nof those trajectories",
    "start": "2642020",
    "end": "2648230"
  },
  {
    "text": "minus the sum over the states,\nthe probability of the state, given your current\nparameterizations",
    "start": "2648230",
    "end": "2662080"
  },
  {
    "text": "and the features\nfor those states. So this is under a\nlinear reward, which",
    "start": "2662080",
    "end": "2669640"
  },
  {
    "text": "is what they derived it for. And so this is what you\nwould do over and over again.",
    "start": "2669640",
    "end": "2674823"
  },
  {
    "text": "All right, so let's\npop up a level and check our understanding. Given all of this, what\nsteps in the above algorithm",
    "start": "2674823",
    "end": "2681339"
  },
  {
    "text": "rely on knowing\nthe dynamics model? Is it computing\nthe optimal policy? Is it computing the state\nvisitation frequencies?",
    "start": "2681340",
    "end": "2687560"
  },
  {
    "text": "Is it computing the gradient\nor nothing required it? I told you that they did\nsay they assumed access",
    "start": "2687560",
    "end": "2694150"
  },
  {
    "text": "to the dynamics model. And I'll just write out that\ngradient again right here. So let's just take a second\nto review the algorithm,",
    "start": "2694150",
    "end": "2701410"
  },
  {
    "text": "check in, any questions\nwe might have about it. ",
    "start": "2701410",
    "end": "2712180"
  },
  {
    "text": "OK. ",
    "start": "2712180",
    "end": "2717319"
  },
  {
    "text": "All right. ",
    "start": "2717320",
    "end": "2733063"
  },
  {
    "text": "And all the slides\nare on the web, so you're welcome to\ngo back, though, I guess it might be helpful to--",
    "start": "2733063",
    "end": "2738345"
  },
  {
    "text": "most of them I've just\nbeen writing out here. ",
    "start": "2738345",
    "end": "2743650"
  },
  {
    "text": "But you can also just think\nback about value iteration and how I was just\nstarting to show that we could use dynamic\nprogramming to compute the state",
    "start": "2743650",
    "end": "2751090"
  },
  {
    "text": "visitation frequencies. So remember, you probably\nall remember value iteration.",
    "start": "2751090",
    "end": "2757430"
  },
  {
    "text": "And then this was the type of\nequations I was writing out. So to compute the distribution\nover the next time",
    "start": "2757430",
    "end": "2763613"
  },
  {
    "text": "step of the states,\nwe were doing things like summing over\nthe distribution for the previous\ntime step, as well",
    "start": "2763613",
    "end": "2768705"
  },
  {
    "text": "as the probability of action\ngiven a state, as well as the dynamics model. ",
    "start": "2768705",
    "end": "2774670"
  },
  {
    "text": "So that's the type of\ndynamic programming algorithm they were proposing there\nto be able to do this. ",
    "start": "2774670",
    "end": "2786408"
  },
  {
    "text": "And then you can\nalso just think back to what we need and value\niteration to be able to do this. ",
    "start": "2786408",
    "end": "2800253"
  },
  {
    "text": "All right, why don't you\ntalk to your neighbor and see what you got? There's a lot of variance.",
    "start": "2800253",
    "end": "2805329"
  },
  {
    "text": "[SIDE CONVERSATIONS] ",
    "start": "2805330",
    "end": "2899223"
  },
  {
    "text": "All right, good. So this is a nice sort\nof reminder of algorithms that we've seen from long ago.",
    "start": "2899223",
    "end": "2904630"
  },
  {
    "text": "So the answer in\nthis case is 1 and 2. So to compute the optimal policy\ngenerally with value iteration,",
    "start": "2904630",
    "end": "2912980"
  },
  {
    "text": "you need to have access\nto both the reward model and the dynamics model. So this one is true.",
    "start": "2912980",
    "end": "2918790"
  },
  {
    "text": "This is true. And then the dynamics\nprogramming algorithm we looked at also required\naccess to the dynamics model",
    "start": "2918790",
    "end": "2925740"
  },
  {
    "text": "in order to back up and say,\nif you're in this state now, what's the distribution\nover states you're going to be in the next time step?",
    "start": "2925740",
    "end": "2931710"
  },
  {
    "text": "Once you have those, don't\nneed this for the gradient. So I'm just bringing\nthat up there.",
    "start": "2931710",
    "end": "2936790"
  },
  {
    "text": "Once you have all the\nstate-- once you've computed all the\nfrequencies, and you have this don't need it again.",
    "start": "2936790",
    "end": "2942880"
  },
  {
    "text": "So assuming that you've\ndone these two things, you don't need it\nagain for the gradient. But it is being heavily used.",
    "start": "2942880",
    "end": "2949480"
  },
  {
    "text": "And as you might imagine, that's\nalso a pretty strong assumption. So do we actually know\nwhat the dynamics is?",
    "start": "2949480",
    "end": "2955690"
  },
  {
    "text": "The dynamics model is for, say,\nlike, a human physician making decisions or a surgeon,\nit seems quite unlikely.",
    "start": "2955690",
    "end": "2962530"
  },
  {
    "text": "You might know it for,\nlike, MuJoCo or something, but probably not in general. So let me just summarize\nwhere these things are.",
    "start": "2962530",
    "end": "2971180"
  },
  {
    "text": "This approach has been\nincredibly influential. As we said, the initial\none use linear rewards,",
    "start": "2971180",
    "end": "2976660"
  },
  {
    "text": "and it assumed the\ndynamics model is known. But there is a lot of follow-up\nwork to this, including",
    "start": "2976660",
    "end": "2983170"
  },
  {
    "text": "some really nice work by Chelsea\nFinn, who is faculty here now and have been for a while. But as part of her\nPhD, she showed",
    "start": "2983170",
    "end": "2990640"
  },
  {
    "text": "that you could use general\nreward and cost functions, like, things like deep\nneural networks and others. So you could use much\nmore complicated functions",
    "start": "2990640",
    "end": "2997995"
  },
  {
    "text": "and state spaces\nwhere you're not going to be able to use\ndynamic programming to be able to enumerate the\ndistribution over states.",
    "start": "2997995",
    "end": "3005590"
  },
  {
    "text": "And then also she removed the\nneed to know the dynamics model. So they had a really\nnice paper in 2016",
    "start": "3005590",
    "end": "3011392"
  },
  {
    "text": "showing how to do\nthis with really sort of very general rich,\ncomplex state spaces, which has also been highly influential.",
    "start": "3011393",
    "end": "3017970"
  },
  {
    "text": "But I think this idea of saying\nlike how-- at a high level the challenge was, what\ndo we do about the fact",
    "start": "3017970",
    "end": "3023077"
  },
  {
    "text": "that there are many\nreward models that are compatible with\npeople's behavior? One thing you could\ndo is you say,",
    "start": "3023077",
    "end": "3028270"
  },
  {
    "text": "well, the one we're\ngoing to learn is the one that has\nmaximum entropy. And this provides a\nrecipe or an approach",
    "start": "3028270",
    "end": "3034080"
  },
  {
    "text": "to trying to tackle\nthat problem. And it turns out that can be\nvery effective in many cases.",
    "start": "3034080",
    "end": "3039220"
  },
  {
    "text": "And in Brian Siebert's\napproach, they ended up using it for trying\nto model taxi drive cars--",
    "start": "3039220",
    "end": "3045509"
  },
  {
    "text": "taxi car drivers, et cetera. But it's been used\nin many cases since. So let's pop up a level.",
    "start": "3045510",
    "end": "3051495"
  },
  {
    "text": "We're just finishing\nsort of our introduction to imitation learning. What we've seen is\nthat imitation learning",
    "start": "3051495",
    "end": "3057600"
  },
  {
    "text": "is this nice approach where\nif you have access to existing demonstrations-- and it might\nbe hard to write down the reward",
    "start": "3057600",
    "end": "3062970"
  },
  {
    "text": "function-- you could try\nto learn from those what optimal behavior is to try to\nmatch the behavior of access to.",
    "start": "3062970",
    "end": "3071910"
  },
  {
    "text": "In some cases, it can greatly\nreduce the amount of data needed to learn a good policy. We haven't talked a lot\nabout that precisely,",
    "start": "3071910",
    "end": "3079060"
  },
  {
    "text": "but there's some\nreally nice work on the theory of\nimitation learning in RL and thinking about some\nideas we'll see later",
    "start": "3079060",
    "end": "3085559"
  },
  {
    "text": "in this course around sample\ncomplexity of is it provably harder to learn from\noptimal demonstrations versus in the RL setting.",
    "start": "3085560",
    "end": "3092820"
  },
  {
    "text": "So there's a lot of really nice\naspects for imitation learning. The things that I\nthink you should",
    "start": "3092820",
    "end": "3098745"
  },
  {
    "text": "know in terms of going forward\nis you should certainly be very familiar\nwith behavior cloning because it is a technique\nthat is used very frequently.",
    "start": "3098745",
    "end": "3104968"
  },
  {
    "text": "So you can just reduced\nRL to supervised learning when you have demonstrations. But it's also good\nto understand what",
    "start": "3104968",
    "end": "3110370"
  },
  {
    "text": "this principle of\nmaximum entropy is doing, how that relates to\ndistribution over trajectories, and how that is then formed\ninto a maximum likelihood",
    "start": "3110370",
    "end": "3118620"
  },
  {
    "text": "optimization problem to\nlearn the reward model. And I think one thing to\nnotice in this case is",
    "start": "3118620",
    "end": "3125010"
  },
  {
    "text": "when they did this,\nthey are not claiming that is actually the reward\nmodel used by people.",
    "start": "3125010",
    "end": "3130329"
  },
  {
    "text": "It is the reward model that\nis compatible with people's demonstrations that maximizes--",
    "start": "3130330",
    "end": "3136140"
  },
  {
    "text": "and a distribution\nthat maximizes entropy. So it is not necessarily\nclaiming that it is exactly",
    "start": "3136140",
    "end": "3143220"
  },
  {
    "text": "mapping human preferences. Awesome. So now we're going to get into--",
    "start": "3143220",
    "end": "3148500"
  },
  {
    "text": "this is one example\nof human feedback or human input to trying\nto use that to make",
    "start": "3148500",
    "end": "3153870"
  },
  {
    "text": "good sequences of decisions\nunder uncertainty, but there's actually a huge\nnumber of different ways to do this.",
    "start": "3153870",
    "end": "3159750"
  },
  {
    "text": "And so now we're going to--\nthis class and next class, we're going to talk some about\nhuman feedback and reinforcement learning from human preferences.",
    "start": "3159750",
    "end": "3166363"
  },
  {
    "text": "And I think you can think about\nthis from many different levels. You can think about it in terms\nof how could humans actively try",
    "start": "3166363",
    "end": "3172320"
  },
  {
    "text": "to help reinforcement\nlearning agents that they are trained to do something? Like, maybe they want\nto train the robot how",
    "start": "3172320",
    "end": "3178440"
  },
  {
    "text": "to clean up their\ncounter in their kitchen, and they have a particular\nway they want to do it. And so they might\nbe actively trying",
    "start": "3178440",
    "end": "3184180"
  },
  {
    "text": "to help an agent do\na particular task; or we might be just trying\nto align, say, large language",
    "start": "3184180",
    "end": "3189900"
  },
  {
    "text": "models with our\nvalues, our intents. And so then could we\nprovide information",
    "start": "3189900",
    "end": "3195570"
  },
  {
    "text": "that's going to shape their\nbehavior across many tasks? So it is relevant to both\nof these different types",
    "start": "3195570",
    "end": "3203770"
  },
  {
    "text": "of objectives. And I'm going to go\nthrough some different ways that people could be\nusing human input in terms",
    "start": "3203770",
    "end": "3209500"
  },
  {
    "text": "of these sort of training. So one thing to\nnote is that people have been thinking about\nthis for quite a long time.",
    "start": "3209500",
    "end": "3216650"
  },
  {
    "text": "I like this work by Andrea\nThomaz and Cynthia Breazeal from MIT. And they had this thing-- it\nlooks pretty primitive now,",
    "start": "3216650",
    "end": "3223550"
  },
  {
    "text": "but this thing called\nSophie's kitchen. And the idea in this\ncase is that you would be trying to teach\nan autonomous agent how",
    "start": "3223550",
    "end": "3230769"
  },
  {
    "text": "to make a recipe or do\nsome basic different tasks in the kitchen. And of course, as you\ncan see with this,",
    "start": "3230770",
    "end": "3235970"
  },
  {
    "text": "we've come a long\nway in the last 20 years, which is wonderful. But the kind of key\ninsight here was, well,",
    "start": "3235970",
    "end": "3242300"
  },
  {
    "text": "maybe we could learn much\nfaster if you have a human. Like, instead of having an\nagent that's trying out things",
    "start": "3242300",
    "end": "3249280"
  },
  {
    "text": "like Epsilon-Greedy and sort\nof exploring in the world by itself, that's not how\nhumans do it most of the time.",
    "start": "3249280",
    "end": "3255290"
  },
  {
    "text": "Most of the time, we have\nthings like schools or guardians or friends that are giving\nus lots of feedback and help",
    "start": "3255290",
    "end": "3261200"
  },
  {
    "text": "when we're trying\nto learn tasks. And so their insight\nwas to say, well, let's try to do more\neffective and efficient robot",
    "start": "3261200",
    "end": "3267933"
  },
  {
    "text": "learning by leveraging\nthe fact that you can have a human in the loop\nthat's providing feedback to the robot.",
    "start": "3267933",
    "end": "3274570"
  },
  {
    "text": "And in this case, I think one\nthing that's important to note is that the robot is getting\ntwo different forms of input",
    "start": "3274570",
    "end": "3279860"
  },
  {
    "text": "that they're trying to maximize. They're both getting\ninput from the human, and they're getting input\nfrom the environment.",
    "start": "3279860",
    "end": "3286550"
  },
  {
    "text": "So, for example-- I don't remember if this exactly\nwas in that particular domain-- but you could imagine\nsomething like maybe there's",
    "start": "3286550",
    "end": "3292243"
  },
  {
    "text": "intrinsic reward if you drop\nsomething like a big cost. But then maybe the human\nalso says that's good",
    "start": "3292243",
    "end": "3297980"
  },
  {
    "text": "when you make the right recipe. So there's two forms of signals\nthat are being used to train.",
    "start": "3297980",
    "end": "3304640"
  },
  {
    "text": "So this is an example where\nit's more like DAGGER. You have a human in the loop,\nand they are trying actively",
    "start": "3304640",
    "end": "3310070"
  },
  {
    "text": "to help the robot all the time. Another version of this\nis the TAMER framework",
    "start": "3310070",
    "end": "3316310"
  },
  {
    "text": "from Brad Knox and Peter\nStone over at UT Austin. And what they were,\nagain, looking at is like,",
    "start": "3316310",
    "end": "3322020"
  },
  {
    "text": "well, maybe we can\ntrain agents to do things much better\nand much quicker if we're willing\nto be in the loop.",
    "start": "3322020",
    "end": "3327380"
  },
  {
    "text": "And these are all\ndifferent approaches than the DAGGER approach. In this approach, so\nwhat are we looking at?",
    "start": "3327380",
    "end": "3333290"
  },
  {
    "text": "Again, this was older. So this is looking at\nTetris, a video game. We're trying to stack\nblocks and clear lines.",
    "start": "3333290",
    "end": "3341069"
  },
  {
    "text": "And what you could\nsee in this case is a lot of the previous work\nlike of policy iteration-- and it doesn't matter exactly\nwhat these algorithms are,",
    "start": "3341070",
    "end": "3347220"
  },
  {
    "text": "but there are these sort\nof competitive algorithms at the time. We're, at game three,\ngetting nothing. Like, they just weren't\nclearing any lines.",
    "start": "3347220",
    "end": "3354560"
  },
  {
    "text": "But after a while, they could\nstart to learn much better. They could get many more later.",
    "start": "3354560",
    "end": "3361020"
  },
  {
    "text": "And what they found here is\nthat by using human feedback, they were taking human\nfeedback, and they were learning",
    "start": "3361020",
    "end": "3367340"
  },
  {
    "text": "an explicit reward model. So one thing you could\nimagine you could be doing is doing something\nlike model-free RL",
    "start": "3367340",
    "end": "3372930"
  },
  {
    "text": "where you're getting\nsignals from the human, and you're using that to\nupdate the agent's policy,",
    "start": "3372930",
    "end": "3378545"
  },
  {
    "text": "but then you drop it. You're not doing any parametric\nmodeling of the reward model. In this case, they are trying to\nexplicitly build a reward model",
    "start": "3378545",
    "end": "3385350"
  },
  {
    "text": "from the human feedback. And you could see\nthat they could get much better\nperformance very quickly.",
    "start": "3385350",
    "end": "3390690"
  },
  {
    "text": "But just kind of maybe the\nproblem with DAGGER, people aren't going to stay around\nfor thousands of games.",
    "start": "3390690",
    "end": "3395860"
  },
  {
    "text": "And so you may not be able\nto exceed performance, at least in this case,\nif you allowed the agent",
    "start": "3395860",
    "end": "3401400"
  },
  {
    "text": "to train for much longer. But I think this is another\nexample of where-- so this is sort of a place where\nthey're starting",
    "start": "3401400",
    "end": "3407723"
  },
  {
    "text": "to do model-based approaches,\nwhere you were actually explicitly training a reward\nmodel from human feedback.",
    "start": "3407723",
    "end": "3413520"
  },
  {
    "text": "And I think it's nice to think\nabout there being at least one sort of continuum\nof the type of support",
    "start": "3413520",
    "end": "3419430"
  },
  {
    "text": "that humans could provide-- really, probably is\nmulti-dimensional, but at least one-- which is, you\ncould think of-- if humans are",
    "start": "3419430",
    "end": "3425880"
  },
  {
    "text": "willing to provide data\nat all to train RL agents, one might be, I'm only\ngoing to give demonstrations",
    "start": "3425880",
    "end": "3431579"
  },
  {
    "text": "that I'm going to do anyway\nas part of my normal behavior, or maybe that I'll do once. And then in another\nextreme is something",
    "start": "3431580",
    "end": "3438059"
  },
  {
    "text": "like this DAGGER or\nthis constant teaching, where I'm willing to be\na coach for my agent,",
    "start": "3438060",
    "end": "3443370"
  },
  {
    "text": "and I'm just going to\nsit there the whole time. And one of the things you\nmight wonder is, like, well,",
    "start": "3443370",
    "end": "3448630"
  },
  {
    "text": "what's in between. This is clearly a spectrum. And one thing that\na lot of people",
    "start": "3448630",
    "end": "3454230"
  },
  {
    "text": "have thought about quite a\nbit over the last 15 years is where their preference is. Pair comparisons\nis that sweet spot.",
    "start": "3454230",
    "end": "3461220"
  },
  {
    "text": "So the idea in this\ncase is that you're not going to ask people to\ndo constant teaching.",
    "start": "3461220",
    "end": "3466900"
  },
  {
    "text": "You're not going\nto-- but you are going to ask them\nto do a bit of work. And in particular,\nyou're going to ask",
    "start": "3466900",
    "end": "3472230"
  },
  {
    "text": "them to be able to compare\ndifferent types of behaviors and which do they like better.",
    "start": "3472230",
    "end": "3477290"
  },
  {
    "text": "This is kind of in between on\nthe level of human experts. So one of the first places\nthis was discussed a lot",
    "start": "3477290",
    "end": "3482900"
  },
  {
    "text": "was recommendation\nranking systems. So Yisong Yue, who's a professor\nnow at Caltech, together",
    "start": "3482900",
    "end": "3488330"
  },
  {
    "text": "with his PhD advisor, Thorsten\nJoachims and others at Cornell, did some really nice\nearly work on thinking",
    "start": "3488330",
    "end": "3493609"
  },
  {
    "text": "of if you have recommendation\nranking systems. So imagine you have two\ndifferent retrieval functions,",
    "start": "3493610",
    "end": "3498950"
  },
  {
    "text": "and you put in some query. And this gives you this-- the\nretrieval function A gives you",
    "start": "3498950",
    "end": "3504319"
  },
  {
    "text": "this series of outputs. And retrieval function\nB gives you the other. And you'd like to learn because\nyou are Google or Bing or things",
    "start": "3504320",
    "end": "3513520"
  },
  {
    "text": "like that. You like to learn which\nof these two is better. And so the idea that they\ncame up with in this case is,",
    "start": "3513520",
    "end": "3519839"
  },
  {
    "text": "well, we can ask people\nwhich one is better. And in particular, you could\nask people to compare, say, maybe the first item\nreturned or the second",
    "start": "3519840",
    "end": "3528470"
  },
  {
    "text": "or the complete ranking,\nwhich one is better. And that's something that\nmight be much easier for people",
    "start": "3528470",
    "end": "3534080"
  },
  {
    "text": "to do than to specify a scalar\nreward function for how good",
    "start": "3534080",
    "end": "3540020"
  },
  {
    "text": "it is that CS159 is\nreturned to your query. Is that 17.3?",
    "start": "3540020",
    "end": "3546500"
  },
  {
    "text": "Or is that 2,006? Or is it minus 7 billion? It seems very hard to\nask humans to do that,",
    "start": "3546500",
    "end": "3552540"
  },
  {
    "text": "but they probably can\ndo the comparison. And they can say, well, it\nseems a little bit better.",
    "start": "3552540",
    "end": "3557830"
  },
  {
    "text": "So that's one area. And that was sort of one of the\nearly areas of people thinking about how could you get feedback\non recommendation systems",
    "start": "3557830",
    "end": "3564253"
  },
  {
    "text": "so that we could\nmake them better? But there are lots of\nother applications. And as you can see,\nrobotics and driving",
    "start": "3564253",
    "end": "3569559"
  },
  {
    "text": "is one that people have\nthought lots about. So this is work by Dorsa\nSadigh, another one of my great colleagues here. And what they were doing\nhere is to think about",
    "start": "3569560",
    "end": "3576468"
  },
  {
    "text": "if you're training a car to have\ndifferent behaviors on the road,",
    "start": "3576468",
    "end": "3581680"
  },
  {
    "text": "how do you get input from humans\nabout which types of behaviors are going to be acceptable?",
    "start": "3581680",
    "end": "3586750"
  },
  {
    "text": "So, for example, most\nof us would probably prefer the thing on the left\nthan the thing on the right,",
    "start": "3586750",
    "end": "3592250"
  },
  {
    "text": "because the thing on the left\nis not involve a car accident. But it is hard to write down\nan objective function for all",
    "start": "3592250",
    "end": "3598780"
  },
  {
    "text": "the things you want to do when\nyou're driving, including, like, if it's hailing, or if a car\nsuddenly stops in front of you,",
    "start": "3598780",
    "end": "3604960"
  },
  {
    "text": "or it's pretty subtle. And so what Dorsa and\nher colleagues showed",
    "start": "3604960",
    "end": "3610420"
  },
  {
    "text": "is that people can do this\nsort of preference pairs. And in fact, she has\ndone-- she and her lab here",
    "start": "3610420",
    "end": "3615603"
  },
  {
    "text": "has done lots of\nreally interesting work on thinking of which preference\npairs do you show to people so",
    "start": "3615603",
    "end": "3620622"
  },
  {
    "text": "that you can quickly try\nto get a sense of what their preferences are to try\nto respect this human effort",
    "start": "3620622",
    "end": "3626450"
  },
  {
    "text": "aspect? So these are just two of\nthe examples of the places that people have\nthought about this.",
    "start": "3626450",
    "end": "3632119"
  },
  {
    "text": "And of course,\nChatGPT is another, and we'll see more\nabout that in a second. So in general,\npairwise comparisons",
    "start": "3632120",
    "end": "3638990"
  },
  {
    "text": "might be in this sweet\nspot, because it's often easier than writing\ndown a reward function. And it's much less work\nthan sort of DAGGER",
    "start": "3638990",
    "end": "3645950"
  },
  {
    "text": "and having to\nconstantly say-- you could imagine in\nrecommendation systems. What is the perfect\nresponse to this query",
    "start": "3645950",
    "end": "3653599"
  },
  {
    "text": "of, I don't know, which\ncourses involve this? That might be really hard\nfor people to write down, but it's easy for them\nto do the comparisons.",
    "start": "3653600",
    "end": "3662772"
  },
  {
    "text": "Now, how do we think about this? Well, one way we could think\nabout how do we mathematically",
    "start": "3662772",
    "end": "3668510"
  },
  {
    "text": "model this is the\nBradley-Terry model. And as we've seen with trying\nto understand modeling scalar",
    "start": "3668510",
    "end": "3675050"
  },
  {
    "text": "rewards, when we start to think\nabout having people compare preferences, often\nwe will still be",
    "start": "3675050",
    "end": "3681020"
  },
  {
    "text": "really interested in\nunderstanding a latent reward model. So the idea will\noften be that we",
    "start": "3681020",
    "end": "3687170"
  },
  {
    "text": "assume that people have some\nreward function in their head that maybe is hard for\nthem to articulate.",
    "start": "3687170",
    "end": "3692220"
  },
  {
    "text": "And what they can do is\nproduce preference pairs that are compatible with that\nunderlying reward function.",
    "start": "3692220",
    "end": "3697970"
  },
  {
    "text": "Now, they might be noisy. A lot of us all make mistakes. We all make mistakes. And so we're not going to\nassume that necessarily",
    "start": "3697970",
    "end": "3704180"
  },
  {
    "text": "what I say is perfectly\ncorresponding to my latent reward function, but it's going\nto be noisly related to that.",
    "start": "3704180",
    "end": "3710690"
  },
  {
    "text": "And so Bradley-Terry\nmodel is one of these types of\nmodels that tries to relate kind of internal\npreferences over items",
    "start": "3710690",
    "end": "3719120"
  },
  {
    "text": "to how we might compare them. All right, so let's first start\noff with a simpler setting before we get to RL\nof K-armed bandits.",
    "start": "3719120",
    "end": "3726750"
  },
  {
    "text": "We're going to see K-armed\nbandits shortly in the course. But for right now,\nyou don't really",
    "start": "3726750",
    "end": "3731840"
  },
  {
    "text": "need to know what it is except\nfor there's only actions. So there's no states\nright now, just like you have K different actions.",
    "start": "3731840",
    "end": "3738480"
  },
  {
    "text": "That's all. And they all have\ndifferent rewards. And what we're\ngoing to assume is that a human makes some noisy\npairwise comparisons where",
    "start": "3738480",
    "end": "3746150"
  },
  {
    "text": "the probability she\npreserves prefers Bi, so item B, compared to Bj,\nis like the exponential model",
    "start": "3746150",
    "end": "3754970"
  },
  {
    "text": "that we saw before. Exponential models\ncome up a lot. So it's going to be the\nexponential of the reward for Bi",
    "start": "3754970",
    "end": "3760400"
  },
  {
    "text": "divided by the\nexponential of reward for Bi plus the\nexponential reward for Bj.",
    "start": "3760400",
    "end": "3766320"
  },
  {
    "text": "OK, so what will\nbe the probability that I prefer Bi to Bj if\nactually their reward is",
    "start": "3766320",
    "end": "3773790"
  },
  {
    "text": "identical to me? Actually, I'm like, I\ndon't mind whether I have, I don't know, like deep\ndish pizza versus flat.",
    "start": "3773790",
    "end": "3780578"
  },
  {
    "text": "I actually do have preferences. But imagine that I don't. So what would the\nprobability be in that case,",
    "start": "3780578",
    "end": "3787460"
  },
  {
    "text": "according to this model? So my internal reward for\nboth of them is like plus 20 because I really like pizza. So what would that be\nfor this probability",
    "start": "3787460",
    "end": "3795110"
  },
  {
    "text": "if the two items are identical? ",
    "start": "3795110",
    "end": "3800710"
  },
  {
    "text": "Fisher said. Yeah, so this is\nautomatically normalized. So it's 50% at most.",
    "start": "3800710",
    "end": "3806210"
  },
  {
    "text": "If I like one thing much more-- I do like deep dish\npizza a lot more. So it's probably more like that\nwould be, say, 100 versus 10.",
    "start": "3806210",
    "end": "3814460"
  },
  {
    "text": "And in that case, my probability\nwould be more like, say, 0.9 or 0.95, something like that.",
    "start": "3814460",
    "end": "3821890"
  },
  {
    "text": "So this was just a\nparticular model. It is noisy. If you read the-- if I put a link later to\nthe reinforcement learning",
    "start": "3821890",
    "end": "3828940"
  },
  {
    "text": "from human feedback paper, they\nmake some additional assumptions of how people make preference\npairs on top of this model.",
    "start": "3828940",
    "end": "3836660"
  },
  {
    "text": "But this is the basic\nmodel that a lot of people have been looking at\nrecently to understand how internal latent rewards\nrelate to external preferences.",
    "start": "3836660",
    "end": "3845753"
  },
  {
    "text": "One of the important\nthings to note here is that this model\nis transitive, which means that\nif I want to know",
    "start": "3845753",
    "end": "3851290"
  },
  {
    "text": "what this sort of probability\nis between i and k-- so those are two\nparticular items--",
    "start": "3851290",
    "end": "3856370"
  },
  {
    "text": "I can deduce it from\nmy probability for i to j and my probability\nfrom j to k. So you can kind of chain things.",
    "start": "3856370",
    "end": "3862940"
  },
  {
    "text": "So this is a transitive\nprobability model. So this was introduced\nroughly 70 years ago.",
    "start": "3862940",
    "end": "3870470"
  },
  {
    "text": "It's a very popular model,\nand it came up early on in terms of recommendation\nsystems and others.",
    "start": "3870470",
    "end": "3876470"
  },
  {
    "text": "So another thing that's\nuseful to think about is, OK, in this\nsetting where I just have k different actions I can\ntake, and I want to understand--",
    "start": "3876470",
    "end": "3884480"
  },
  {
    "text": "I want to learn\nwhat the reward is for somebody, for all of them-- If I want to think about\nfinding a maximum one,",
    "start": "3884480",
    "end": "3891380"
  },
  {
    "text": "say, like what's the best arm\nor what is the best action, I might want to\ntry to understand how, under these different\npreference models,",
    "start": "3891380",
    "end": "3897960"
  },
  {
    "text": "what it means for\nsomething to be good. So in the class so\nfar, we've often talked about just maximizing\nthe value function,",
    "start": "3897960",
    "end": "3904700"
  },
  {
    "text": "and we want to find\na policy that's good. Now let's just think\nabout if I have no k arms, which of them is best?",
    "start": "3904700",
    "end": "3911300"
  },
  {
    "text": "So a Condorcet winner is one\nwhere, for every other item, you prefer item i to\nall the other items.",
    "start": "3911300",
    "end": "3919700"
  },
  {
    "text": "So like, of all\nthe types of pizza, if I like deep dish most,\nthat's the Condorcet winner. And it doesn't mean that it has\nto-- that those probabilities",
    "start": "3919700",
    "end": "3928190"
  },
  {
    "text": "have to be probably one. It just has to be\ngreater than 0.5. It means I have to beat\nall of the other options.",
    "start": "3928190",
    "end": "3935990"
  },
  {
    "text": "And I'm bringing\nthese up right now because there's\nalso been some later discussion of how does all the\nrecent RLHF work relate to ideas",
    "start": "3935990",
    "end": "3943760"
  },
  {
    "text": "from social choice and\ncomputational economics about what are we computing?",
    "start": "3943760",
    "end": "3949845"
  },
  {
    "text": "What is the sort of underlying\nobjective we're computing, and how are we distinguishing\nbetween different sorts",
    "start": "3949845",
    "end": "3955670"
  },
  {
    "text": "of responses LLMs could give us? So the second thing-- so\nthis is a pretty high bar. This means that there\nhas to be one thing that",
    "start": "3955670",
    "end": "3963110"
  },
  {
    "text": "beats everything else. A Copelan winner is\na little bit less. It just says it's\nthe winner if it",
    "start": "3963110",
    "end": "3970760"
  },
  {
    "text": "has the highest number\nof pairwise victories against everything else. So that doesn't mean that\nyou have to prefer it",
    "start": "3970760",
    "end": "3977000"
  },
  {
    "text": "to everything else. It just that means, on\naverage, it beats the others.",
    "start": "3977000",
    "end": "3982345"
  },
  {
    "text": " And a Borda winner-- an item is a border\nwinner, if it",
    "start": "3982345",
    "end": "3989300"
  },
  {
    "text": "maximizes the expected score,\nwhere the score for item Bj is 1. If you prefer Bi to Bj,\nit's 0.5 if they're equal,",
    "start": "3989300",
    "end": "3999300"
  },
  {
    "text": "then it's 0 otherwise. So it's sort of like this\ndiscretization of the wins and loses.",
    "start": "3999300",
    "end": "4004790"
  },
  {
    "text": "And typically, algorithms for\nK-armed or dueling bandits-- again, we'll go into what\nbandits are more later--",
    "start": "4004790",
    "end": "4010370"
  },
  {
    "text": "what they focus on doing\nis trying to find this. I don't necessarily\nneed to find an item,",
    "start": "4010370",
    "end": "4016230"
  },
  {
    "text": "say, like, a ranking\nsystem that is always better than everything else. I want to find one that on\naverage beats everything else,",
    "start": "4016230",
    "end": "4023337"
  },
  {
    "text": "and they often would construct\nthese kind of pairwise matrices where you can think do\nthese different actions, beat these other actions.",
    "start": "4023337",
    "end": "4028942"
  },
  {
    "text": " All right, so how\nwould we learn these? So the question, is we have\nall of these noisy pairwise",
    "start": "4028942",
    "end": "4038490"
  },
  {
    "text": "comparisons. And what we want\nto do now is to see if we can extract these\nunderlying reward functions.",
    "start": "4038490",
    "end": "4044340"
  },
  {
    "text": "Why would we want to do that? Well, once we have these\nunderlying reward functions, we can figure out which arm is\nbest or which action is best.",
    "start": "4044340",
    "end": "4050530"
  },
  {
    "text": "And in the reinforcement\nlearning case, we could try to optimize\nfor that reward function. So how do we do that?",
    "start": "4050530",
    "end": "4056075"
  },
  {
    "text": "What we're going to\ndo is we're going to assume we have N tuples\nof the following form. We have item 1, item 2,\nSo item i, item j, and mu,",
    "start": "4056075",
    "end": "4064079"
  },
  {
    "text": "where mu is 1 if you\nprefer the first thing. Mu is 0 if you prefer\nthe other thing.",
    "start": "4064080",
    "end": "4069630"
  },
  {
    "text": "And it's 0.5 if you don't care. So this is just like\na classification task. You can just think back to\nyour supervised learning",
    "start": "4069630",
    "end": "4076740"
  },
  {
    "text": "where you just\nhave-- it should look very much like a\nlogistic regression task.",
    "start": "4076740",
    "end": "4082080"
  },
  {
    "text": "And you can maximize the\nlikelihood with cross-entropy.",
    "start": "4082080",
    "end": "4087500"
  },
  {
    "text": "So we had to map it back\nto a standard logistic loss where we say these\nreward models--",
    "start": "4087500",
    "end": "4094428"
  },
  {
    "text": "and in general, we're\ngoing to parameterize these as like deep neural networks or\nsome other complicated function.",
    "start": "4094428",
    "end": "4099830"
  },
  {
    "text": "It could be linear. It just depends. But once we have\nthat, then we can try to find the\nset of parameters",
    "start": "4099830",
    "end": "4107839"
  },
  {
    "text": "that maximize the likelihood. So that's how we\ncould fit a reward model when we are\ngiven preference pairs",
    "start": "4107840",
    "end": "4115670"
  },
  {
    "text": "and observed preferences. Now, you might wonder,\nhow do we do this in RL? Because in RL, we have states.",
    "start": "4115670",
    "end": "4121870"
  },
  {
    "text": "We have multiple actions. We have trajectories. The idea is pretty similar\nin some ways to what we",
    "start": "4121870",
    "end": "4127859"
  },
  {
    "text": "were seeing with max entropy. And that what we're\ngoing to do is to say, well, we have a trajectory. If we have a trajectory,\nwe can think of there",
    "start": "4127859",
    "end": "4133957"
  },
  {
    "text": "being a series of rewards. So the reward of that\ntrajectory is just the sum.",
    "start": "4133957",
    "end": "4140009"
  },
  {
    "text": "So I plug in all of those sums. And I prefer a trajectory\nif I can get higher reward",
    "start": "4140010",
    "end": "4147060"
  },
  {
    "text": "for that trajectory\nthan the other, according to the same model. So I essentially just\nmap it back to as",
    "start": "4147060",
    "end": "4152100"
  },
  {
    "text": "if it was kind of like a\nbandit, just now that I have two different trajectories. And we'll see an example\nof this in just a minute.",
    "start": "4152100",
    "end": "4160240"
  },
  {
    "text": "OK, so what do we do? We now are going to ask people\nto compare trajectories.",
    "start": "4160240",
    "end": "4165660"
  },
  {
    "text": "We'll use that, then we're\ngoing to learn our reward model. And then once we have\nour learned reward model,",
    "start": "4165660",
    "end": "4170740"
  },
  {
    "text": "we can do PPO or\nsomething with it. So this gives us a reward\nmodel for our domain, and now we can try to optimize\nour policy with respect to it.",
    "start": "4170740",
    "end": "4179100"
  },
  {
    "text": "So let's look at an example. So in the reinforcement\nlearning from human feedback,",
    "start": "4179100",
    "end": "4185589"
  },
  {
    "text": "more precisely called deep\nRL from human preferences-- this came out in 2017. And they wanted to train\nsomething to do a backflip.",
    "start": "4185590",
    "end": "4194259"
  },
  {
    "text": "And what they\nnoticed here is they needed about 900 bits\nof human feedback in order to learn to do this.",
    "start": "4194260",
    "end": "4200420"
  },
  {
    "text": "So let's see what it looks like. All right. OK, so what someone's going\nto be doing in this case--",
    "start": "4200420",
    "end": "4207400"
  },
  {
    "text": "so remember, they're\ntrying to train this sort of little MuJoCo-like\nthing to do a backflip.",
    "start": "4207400",
    "end": "4213090"
  },
  {
    "text": "So what they're\ngoing to show people is they're going to\nshow them little clips. And they're going to say,\nis the thing on the left",
    "start": "4213090",
    "end": "4218733"
  },
  {
    "text": "doing a better job of\ntrying to do a backflip, or is the thing on the right? And they're just\ngetting people to click left, right, left, right,\nleft, right, left, right.",
    "start": "4218733",
    "end": "4225870"
  },
  {
    "text": "And so they're not having\nto say what is the reward function for doing a backflip? They're just saying,\nI don't know,",
    "start": "4225870",
    "end": "4230880"
  },
  {
    "text": "this one looks closer\nto a backflip or better. And so what you can see\nhere is that some of them",
    "start": "4230880",
    "end": "4238350"
  },
  {
    "text": "are going to be much\nbetter at getting close to doing a back flip. So some of those is\nactually pretty good.",
    "start": "4238350",
    "end": "4243750"
  },
  {
    "text": "And what they are saying\nis that they only needed about 900 examples\nin order to train it",
    "start": "4243750",
    "end": "4250260"
  },
  {
    "text": "so that it could actually\nlearn to do a backflip, which is pretty good, particularly\nif you think back",
    "start": "4250260",
    "end": "4256260"
  },
  {
    "text": "to deep Q-learning and the\nenormous amount of data and training that\nthey're often doing for,",
    "start": "4256260",
    "end": "4263550"
  },
  {
    "text": "say, trying to learn\nAtari, et cetera, which is literally millions. ",
    "start": "4263550",
    "end": "4269909"
  },
  {
    "text": "So this is really cool. This is possible to do. And this is something\nthat you're going to be doing in your homework.",
    "start": "4269910",
    "end": "4275118"
  },
  {
    "text": "So in homework 3, you're going\nto be doing both RLHF and DPO. So I'm really excited\nabout this assignment. This is the first\ntime we're doing it.",
    "start": "4275118",
    "end": "4281403"
  },
  {
    "text": "So you can actually\nsee how this works. So you can see how\nwe can actually learn from human preferences.",
    "start": "4281403",
    "end": "4286643"
  },
  {
    "text": "We're not making you do\nthe human preferences. We're going to give you a data\nset for how we can actually",
    "start": "4286643",
    "end": "4292030"
  },
  {
    "text": "train these agents. Now, I know we're\nalmost out of time, but I'll say just a\nlittle bit about this.",
    "start": "4292030",
    "end": "4297770"
  },
  {
    "text": "I'll probably have a\nbit of time on Monday before we have\nour guest lecture. But just I want to give you\nat least a little taste.",
    "start": "4297770",
    "end": "4304700"
  },
  {
    "text": "So this was in 2017,\nthat paper was. And there was attention to it.",
    "start": "4304700",
    "end": "4310850"
  },
  {
    "text": "But I feel like in\nmany ways, there wasn't a huge amount of work on\nthat until much more recently. So I just want to share\na couple of slides",
    "start": "4310850",
    "end": "4317500"
  },
  {
    "text": "from Tatsu\nHashimoto's NLP class. So if we just think back--",
    "start": "4317500",
    "end": "4323120"
  },
  {
    "text": "I think I showed this slide on\nthe very first day of class-- how is RLHF being\nused for ChatGPT?",
    "start": "4323120",
    "end": "4329739"
  },
  {
    "text": "What they're doing\nthere is they're getting demonstration\ndata, and they're doing supervised learning. This is basically what we\nwould call behavior cloning.",
    "start": "4329740",
    "end": "4336733"
  },
  {
    "text": "Then they're going to\nget this comparison data and train a reward model. Now, in their case, they\nmight not just use two.",
    "start": "4336733",
    "end": "4342320"
  },
  {
    "text": "You could actually have\npeople rank between, say, four or something like that. And you can extend\nthese models to do that.",
    "start": "4342320",
    "end": "4348580"
  },
  {
    "text": "So you get labelers to do that,\nthen you train the reward model. And then use PPO to actually\nupdate the large language model.",
    "start": "4348580",
    "end": "4361510"
  },
  {
    "text": "Now, one thing that I think is\nimportant to note in this case is that this is all really an\ninstance of meta reinforcement",
    "start": "4361510",
    "end": "4367090"
  },
  {
    "text": "learning in the sense\nthat what they're going to be trying to do here-- unlike where we've\nseen like you want",
    "start": "4367090",
    "end": "4372520"
  },
  {
    "text": "to train something\nto do one task, like, being able\nto do a backflip-- they're trying to learn in\ngeneral a reward function that",
    "start": "4372520",
    "end": "4379240"
  },
  {
    "text": "covers all the tasks that\npeople might want to do with large language models. And so it's this\nmultitask problem.",
    "start": "4379240",
    "end": "4386199"
  },
  {
    "text": "And so when they\ndo this, they're going to give you a new prompt,\nlike, write a story about frogs.",
    "start": "4386200",
    "end": "4391360"
  },
  {
    "text": "And then they will\nwant the agent to do well on that, which is\nlikely a task that has maybe never seen before in its data.",
    "start": "4391360",
    "end": "4397842"
  },
  {
    "text": "So I think that's also another\nimportant thing to note here is that the reward models\nthat are being trained now are things that probably would\nhave been considered multitask",
    "start": "4397842",
    "end": "4406119"
  },
  {
    "text": "settings before. But now we're sort of\nlifting them and saying your task is just to\ndo whatever humans",
    "start": "4406120",
    "end": "4411580"
  },
  {
    "text": "want to do with this ChatGPT in\nterms of answering questions. And so how do you train\na reward model that",
    "start": "4411580",
    "end": "4417790"
  },
  {
    "text": "will be good for any of those? So we'll continue talking\nabout this next week. We'll talk probably either\nbefore or after the guest",
    "start": "4417790",
    "end": "4424660"
  },
  {
    "text": "lecture a bit about how\nwe actually do this. But it basically just follows\nexactly along the framework",
    "start": "4424660",
    "end": "4430330"
  },
  {
    "text": "that we've just seen there. And I'll see you then. Thanks. ",
    "start": "4430330",
    "end": "4439000"
  }
]