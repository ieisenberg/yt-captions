[
  {
    "start": "0",
    "end": "163000"
  },
  {
    "start": "0",
    "end": "5500"
  },
  {
    "text": "Hey, everyone.",
    "start": "5500",
    "end": "6750"
  },
  {
    "text": "Welcome to the first and\nintroductory lecture for CS 25,",
    "start": "6750",
    "end": "11160"
  },
  {
    "text": "Transformers United.",
    "start": "11160",
    "end": "12870"
  },
  {
    "text": "So CS 25 was a class\nthat the three of us",
    "start": "12870",
    "end": "15870"
  },
  {
    "text": "created and taught at\nStanford in the fall of 2021,",
    "start": "15870",
    "end": "19560"
  },
  {
    "text": "and the subject of\nthe class is not",
    "start": "19560",
    "end": "22470"
  },
  {
    "text": "as the picture might suggest.",
    "start": "22470",
    "end": "23759"
  },
  {
    "text": "It's not about robots that\ncan transform into cars.",
    "start": "23760",
    "end": "27400"
  },
  {
    "text": "It's about deep learning\nmodels and specifically",
    "start": "27400",
    "end": "30060"
  },
  {
    "text": "a particular kind of\ndeep learning models",
    "start": "30060",
    "end": "32520"
  },
  {
    "text": "that have revolutionized\nmultiple fields,",
    "start": "32520",
    "end": "35310"
  },
  {
    "text": "starting from natural\nlanguage processing",
    "start": "35310",
    "end": "38220"
  },
  {
    "text": "to things like computer\nvision and reinforcement",
    "start": "38220",
    "end": "40470"
  },
  {
    "text": "learning to name a few.",
    "start": "40470",
    "end": "42930"
  },
  {
    "text": "We have an exciting set of\nvideos lined up for you.",
    "start": "42930",
    "end": "45940"
  },
  {
    "text": "We had some truly\nfantastic speakers",
    "start": "45940",
    "end": "48090"
  },
  {
    "text": "come and give talks about how\nthey were applying Transformers",
    "start": "48090",
    "end": "51120"
  },
  {
    "text": "in their own research.",
    "start": "51120",
    "end": "53160"
  },
  {
    "text": "And we hope you will enjoy\nand learn from these talks.",
    "start": "53160",
    "end": "56430"
  },
  {
    "text": "This video is purely\nan introductory lecture",
    "start": "56430",
    "end": "59400"
  },
  {
    "text": "to talk a little bit\nabout transformers.",
    "start": "59400",
    "end": "61530"
  },
  {
    "text": "And before we get\nstarted, I'd like",
    "start": "61530",
    "end": "63420"
  },
  {
    "text": "to introduce the instructors.",
    "start": "63420",
    "end": "65369"
  },
  {
    "text": "So my name is Advay.",
    "start": "65370",
    "end": "66810"
  },
  {
    "text": "I am a software\nengineer at a company",
    "start": "66810",
    "end": "68759"
  },
  {
    "text": "called Applied Intuition.",
    "start": "68760",
    "end": "70230"
  },
  {
    "text": "Before this, I was a master's\nstudent in CS at Stanford.",
    "start": "70230",
    "end": "74160"
  },
  {
    "text": "And I am one of the\nco-instructors for CS25.",
    "start": "74160",
    "end": "79560"
  },
  {
    "text": "Chetanya, Div, if the two of\nyou could introduce yourselves.",
    "start": "79560",
    "end": "82479"
  },
  {
    "text": "So hi, everyone.",
    "start": "82480",
    "end": "83790"
  },
  {
    "text": "I am a PhD student at Stanford.",
    "start": "83790",
    "end": "86280"
  },
  {
    "text": "Before this, I was\npursuing a master's here,",
    "start": "86280",
    "end": "89687"
  },
  {
    "text": "researching a lot in generative\nmodeling, reinforcement",
    "start": "89688",
    "end": "91980"
  },
  {
    "text": "learning, and robotics.",
    "start": "91980",
    "end": "94030"
  },
  {
    "text": "So nice to meet you all.",
    "start": "94030",
    "end": "95220"
  },
  {
    "text": "Yeah, that was Div, since\nhe didn't say his name.",
    "start": "95220",
    "end": "98400"
  },
  {
    "text": "Chetanya, if you want\nto introduce yourself.",
    "start": "98400",
    "end": "100570"
  },
  {
    "text": "Yeah.",
    "start": "100570",
    "end": "101145"
  },
  {
    "text": "Hi, everyone.",
    "start": "101145",
    "end": "101950"
  },
  {
    "text": "My name is Chetanya,\nand I'm currently",
    "start": "101950",
    "end": "104070"
  },
  {
    "text": "working as an ML engineer at\na start-up called Moveworks.",
    "start": "104070",
    "end": "108240"
  },
  {
    "text": "Before that, I was\na master's student",
    "start": "108240",
    "end": "110130"
  },
  {
    "text": "at Stanford specializing\nin NLP and was",
    "start": "110130",
    "end": "112500"
  },
  {
    "text": "a member of the\nprize-winning Stanford's team",
    "start": "112500",
    "end": "114900"
  },
  {
    "text": "for the Alexa Prize Challenge.",
    "start": "114900",
    "end": "118180"
  },
  {
    "text": "All right, awesome.",
    "start": "118180",
    "end": "119580"
  },
  {
    "text": "So moving on to the\nrest of this talk,",
    "start": "119580",
    "end": "124440"
  },
  {
    "text": "essentially, what we\nhope you will learn",
    "start": "124440",
    "end": "127230"
  },
  {
    "text": "watching these\nvideos and what we",
    "start": "127230",
    "end": "129179"
  },
  {
    "text": "hope the people who took our\nclass in the fall of 2021",
    "start": "129180",
    "end": "132959"
  },
  {
    "text": "learned is three things.",
    "start": "132960",
    "end": "135220"
  },
  {
    "text": "One is we hope you will\nhave an understanding of how",
    "start": "135220",
    "end": "137820"
  },
  {
    "text": "Transformers work.",
    "start": "137820",
    "end": "139780"
  },
  {
    "text": "Secondly, we hope you will learn\nand, by the end of these talks,",
    "start": "139780",
    "end": "144300"
  },
  {
    "text": "understand how Transformers\nare being applied beyond just",
    "start": "144300",
    "end": "147630"
  },
  {
    "text": "natural language processing.",
    "start": "147630",
    "end": "149280"
  },
  {
    "text": "And thirdly, we hope\nthat some of these talks",
    "start": "149280",
    "end": "152340"
  },
  {
    "text": "will spark some new\nideas within you",
    "start": "152340",
    "end": "155428"
  },
  {
    "text": "and hopefully lead to new\ndirections of research,",
    "start": "155428",
    "end": "157470"
  },
  {
    "text": "new kinds of innovation,\nand things of that sort.",
    "start": "157470",
    "end": "160050"
  },
  {
    "start": "160050",
    "end": "164110"
  },
  {
    "start": "163000",
    "end": "363000"
  },
  {
    "text": "And to begin, we're going\nto talk a little bit",
    "start": "164110",
    "end": "167750"
  },
  {
    "text": "about Transformers\nand introduce some",
    "start": "167750",
    "end": "169700"
  },
  {
    "text": "of the context behind\ntransformers as well.",
    "start": "169700",
    "end": "172114"
  },
  {
    "text": "And for that, I'd like\nto hand it off to Div.",
    "start": "172115",
    "end": "173990"
  },
  {
    "start": "173990",
    "end": "180110"
  },
  {
    "text": "So hi, everyone.",
    "start": "180110",
    "end": "182070"
  },
  {
    "text": "So welcome to our\nTransformer seminar.",
    "start": "182070",
    "end": "185030"
  },
  {
    "text": "So let's start first with\nan overview of the attention",
    "start": "185030",
    "end": "187760"
  },
  {
    "text": "timeline and how it came to be.",
    "start": "187760",
    "end": "190407"
  },
  {
    "text": "The key idea about Transformers\nwas the simple attention",
    "start": "190407",
    "end": "192739"
  },
  {
    "text": "mechanism that was\ndeveloped in 2017.",
    "start": "192740",
    "end": "195200"
  },
  {
    "text": "And this all started\nwith this one paper",
    "start": "195200",
    "end": "197030"
  },
  {
    "text": "called \"Attention is All\nyou Need,\" by Vaswani et al.",
    "start": "197030",
    "end": "199910"
  },
  {
    "text": "Before 2017, we used to have\nthis prehistoric era where",
    "start": "199910",
    "end": "202490"
  },
  {
    "text": "we had older models like RNNs,\nLSTMs, and simpler attention",
    "start": "202490",
    "end": "207680"
  },
  {
    "text": "mechanisms",
    "start": "207680",
    "end": "208400"
  },
  {
    "text": "And eventually, the\ngrowth in Transformers",
    "start": "208400",
    "end": "211028"
  },
  {
    "text": "has exploded into other fields\nand has become prominent",
    "start": "211028",
    "end": "213320"
  },
  {
    "text": "in all of machine learning.",
    "start": "213320",
    "end": "215300"
  },
  {
    "text": "And I'll go and see and\nshow how this has been used.",
    "start": "215300",
    "end": "219810"
  },
  {
    "text": "So in the prehistoric era,\nthere used to be RNNs.",
    "start": "219810",
    "end": "223670"
  },
  {
    "text": "There were different models like\nthe Sequence2Sequence, LSTMs,",
    "start": "223670",
    "end": "226520"
  },
  {
    "text": "GRUs.",
    "start": "226520",
    "end": "227720"
  },
  {
    "text": "They were good at encoding\nsome sort of memory,",
    "start": "227720",
    "end": "230210"
  },
  {
    "text": "but they did not work for\nencoding long sequences.",
    "start": "230210",
    "end": "233090"
  },
  {
    "text": "And they were very bad\nat encoding context.",
    "start": "233090",
    "end": "235110"
  },
  {
    "text": "So here is an example.",
    "start": "235110",
    "end": "237110"
  },
  {
    "text": "If you have a sentence like\nI grew up in France dot,",
    "start": "237110",
    "end": "239330"
  },
  {
    "text": "dot, dot, so I\nspeak fluent- then",
    "start": "239330",
    "end": "242510"
  },
  {
    "text": "you want to fill this with\nFrench based on the context,",
    "start": "242510",
    "end": "244879"
  },
  {
    "text": "but like a LSTM model might not\nknow what it is and might just",
    "start": "244880",
    "end": "247910"
  },
  {
    "text": "make a very big mistake here.",
    "start": "247910",
    "end": "249660"
  },
  {
    "text": "Similarly, we can show some\nsort of correlation map",
    "start": "249660",
    "end": "253040"
  },
  {
    "text": "here, where if you\nhave a pronoun like it,",
    "start": "253040",
    "end": "255110"
  },
  {
    "text": "we want it to correlate\nto one of the past nouns",
    "start": "255110",
    "end": "257989"
  },
  {
    "text": "that we have seen\nso far, like animal.",
    "start": "257990",
    "end": "260720"
  },
  {
    "text": "But again, older models\nwere really not good",
    "start": "260720",
    "end": "263960"
  },
  {
    "text": "at this context encoding.",
    "start": "263960",
    "end": "265970"
  },
  {
    "text": "So where we are currently now\nis on the verge of take-off.",
    "start": "265970",
    "end": "269620"
  },
  {
    "text": "We began to realize the\npotential of Transformers",
    "start": "269620",
    "end": "271729"
  },
  {
    "text": "in different fields.",
    "start": "271730",
    "end": "272880"
  },
  {
    "text": "We have started to use\nthem to solve long sequence",
    "start": "272880",
    "end": "275360"
  },
  {
    "text": "problems in protein folding,\nsuch as the AlphaFold",
    "start": "275360",
    "end": "279229"
  },
  {
    "text": "model from DeepMind,\nwhich gets 95% accuracy",
    "start": "279230",
    "end": "284360"
  },
  {
    "text": "on different challenges,\nand offline RL.",
    "start": "284360",
    "end": "287659"
  },
  {
    "text": "We can use it for few-shot\nand zero-shot generalization,",
    "start": "287660",
    "end": "290030"
  },
  {
    "text": "for text and image generation.",
    "start": "290030",
    "end": "291800"
  },
  {
    "text": "And we can also use\nfor content generation.",
    "start": "291800",
    "end": "293870"
  },
  {
    "text": "So here's an\nexample from OpenAI,",
    "start": "293870",
    "end": "295940"
  },
  {
    "text": "where you can give a\ndifferent text prompt",
    "start": "295940",
    "end": "298340"
  },
  {
    "text": "and have an AI generate a\nfictional image for you.",
    "start": "298340",
    "end": "302880"
  },
  {
    "text": "And so there's a doc on\nthis that you can also",
    "start": "302880",
    "end": "305875"
  },
  {
    "text": "watch on YouTube,\nwhich basically",
    "start": "305875",
    "end": "307250"
  },
  {
    "text": "says that LSTMs are dead,\nand long live Transformers.",
    "start": "307250",
    "end": "311620"
  },
  {
    "text": "So what's the future?",
    "start": "311620",
    "end": "313310"
  },
  {
    "text": "So we can enable a lot more\napplications for Transformers.",
    "start": "313310",
    "end": "317650"
  },
  {
    "text": "They can be applied to any\nform of sequence modeling.",
    "start": "317650",
    "end": "320530"
  },
  {
    "text": "So we could use them\nfor video understanding.",
    "start": "320530",
    "end": "323290"
  },
  {
    "text": "We can use them for\nfinance and a lot more.",
    "start": "323290",
    "end": "325610"
  },
  {
    "text": "So basically, imagine all\nsorts of generative modeling",
    "start": "325610",
    "end": "328030"
  },
  {
    "text": "problems.",
    "start": "328030",
    "end": "329080"
  },
  {
    "text": "Nevertheless, there are a\nlot of missing ingredients.",
    "start": "329080",
    "end": "331449"
  },
  {
    "text": "So like the human\nbrain, we need some sort",
    "start": "331450",
    "end": "333580"
  },
  {
    "text": "of external memory unit, which\nis the hippocampus for us.",
    "start": "333580",
    "end": "337599"
  },
  {
    "text": "And there are some\nearly works here.",
    "start": "337600",
    "end": "340480"
  },
  {
    "text": "So one nice work you\nmight want to check out",
    "start": "340480",
    "end": "342430"
  },
  {
    "text": "is called Neural\nTuring Machines.",
    "start": "342430",
    "end": "344259"
  },
  {
    "text": "Similarly, the current\nattention mechanisms",
    "start": "344260",
    "end": "346600"
  },
  {
    "text": "are very computationally\ncomplex in terms",
    "start": "346600",
    "end": "349540"
  },
  {
    "text": "of time and correlating\nwhich we will discuss later.",
    "start": "349540",
    "end": "352490"
  },
  {
    "text": "And we want to make\nthem more linear.",
    "start": "352490",
    "end": "354538"
  },
  {
    "text": "And the third problem\nis that we want",
    "start": "354538",
    "end": "356080"
  },
  {
    "text": "to align our current\nsort of language models",
    "start": "356080",
    "end": "358389"
  },
  {
    "text": "with how the human brain\nworks and human values.",
    "start": "358390",
    "end": "361150"
  },
  {
    "text": "And this is also a big issue.",
    "start": "361150",
    "end": "363220"
  },
  {
    "start": "363000",
    "end": "473000"
  },
  {
    "text": "OK.",
    "start": "363220",
    "end": "363760"
  },
  {
    "text": "So now I will deep dive--",
    "start": "363760",
    "end": "366490"
  },
  {
    "text": "I will dive deeper into\nthe attention mechanisms",
    "start": "366490",
    "end": "370150"
  },
  {
    "text": "and show how they\ncame out to be.",
    "start": "370150",
    "end": "373240"
  },
  {
    "text": "So initially, they used to\nbe very simple mechanisms.",
    "start": "373240",
    "end": "378069"
  },
  {
    "text": "Attention was inspired by\nthe process of importance",
    "start": "378070",
    "end": "380560"
  },
  {
    "text": "weighting, of putting attention\non different parts of an image",
    "start": "380560",
    "end": "384794"
  },
  {
    "text": "like similar to a human,\nwhere you might focus more",
    "start": "384795",
    "end": "386920"
  },
  {
    "text": "on a foreground if you have\nan image of a dog compared",
    "start": "386920",
    "end": "389560"
  },
  {
    "text": "to the rest of the background.",
    "start": "389560",
    "end": "391070"
  },
  {
    "text": "So in the case of soft\nattention, what you do is",
    "start": "391070",
    "end": "393220"
  },
  {
    "text": "you learn the simple\nsoft attention weighting",
    "start": "393220",
    "end": "395650"
  },
  {
    "text": "for each pixel, which can\nbe a weight between 0 to 1.",
    "start": "395650",
    "end": "399130"
  },
  {
    "text": "The problem over\nhere is that this is",
    "start": "399130",
    "end": "400900"
  },
  {
    "text": "a very expensive computation.",
    "start": "400900",
    "end": "402220"
  },
  {
    "text": "And you can show--",
    "start": "402220",
    "end": "403900"
  },
  {
    "text": "as is shown in the\nfigure on the left,",
    "start": "403900",
    "end": "406195"
  },
  {
    "text": "you can see we are\ncalculating this attention",
    "start": "406195",
    "end": "408070"
  },
  {
    "text": "map for the whole image.",
    "start": "408070",
    "end": "410230"
  },
  {
    "text": "What you can do instead is\nyou can just get a 0 to 1",
    "start": "410230",
    "end": "414620"
  },
  {
    "text": "attention map, where we directly\nput a 1 on wherever the dog is",
    "start": "414620",
    "end": "417880"
  },
  {
    "text": "and 0 wherever\nit's a background.",
    "start": "417880",
    "end": "420820"
  },
  {
    "text": "This is like less\ncomputationally expensive,",
    "start": "420820",
    "end": "423273"
  },
  {
    "text": "but the problem is it's\nnon-differentiable and makes",
    "start": "423273",
    "end": "425440"
  },
  {
    "text": "things harder to train.",
    "start": "425440",
    "end": "427510"
  },
  {
    "text": "Going forward, we also\nhave different varieties",
    "start": "427510",
    "end": "429880"
  },
  {
    "text": "of basic attention\nmechanisms that were",
    "start": "429880",
    "end": "432010"
  },
  {
    "text": "proposed before self-attention.",
    "start": "432010",
    "end": "434170"
  },
  {
    "text": "So the first term right here\nis global attention models.",
    "start": "434170",
    "end": "437350"
  },
  {
    "text": "So global attention model\nis for each hidden layer",
    "start": "437350",
    "end": "442243"
  },
  {
    "text": "input-- hidden layer output.",
    "start": "442243",
    "end": "443410"
  },
  {
    "text": "You learn attention\nweight of a of p",
    "start": "443410",
    "end": "446230"
  },
  {
    "text": "and this is\nelementwise multiplied",
    "start": "446230",
    "end": "448330"
  },
  {
    "text": "with your current output to\nget your final output, yt.",
    "start": "448330",
    "end": "452650"
  },
  {
    "text": "Similarly, you have the\nlocal attention models,",
    "start": "452650",
    "end": "455290"
  },
  {
    "text": "where instead of calculating\nthe global attention",
    "start": "455290",
    "end": "457480"
  },
  {
    "text": "for over the whole\nsequence length,",
    "start": "457480",
    "end": "459850"
  },
  {
    "text": "you only calculate the\nattention over a small window.",
    "start": "459850",
    "end": "464220"
  },
  {
    "text": "And then you weight by the\nattention of the window",
    "start": "464220",
    "end": "467260"
  },
  {
    "text": "into the current output to\nget the final output you need.",
    "start": "467260",
    "end": "472420"
  },
  {
    "text": "So moving on, I will\npass on to Chetanya",
    "start": "472420",
    "end": "475030"
  },
  {
    "start": "473000",
    "end": "698000"
  },
  {
    "text": "to discuss self-attention\nmechanisms and platforms.",
    "start": "475030",
    "end": "479290"
  },
  {
    "text": "Thank you, Div, for covering\na brief overview of how",
    "start": "479290",
    "end": "482280"
  },
  {
    "text": "the primitive versions\nof attention work.",
    "start": "482280",
    "end": "485220"
  },
  {
    "text": "Now just before we talk about\nself-attention, just a bit",
    "start": "485220",
    "end": "489000"
  },
  {
    "text": "of trivia, that term was\nfirst introduced by a paper",
    "start": "489000",
    "end": "493230"
  },
  {
    "text": "from Lin et al., which\nprovided a framework",
    "start": "493230",
    "end": "496440"
  },
  {
    "text": "for a self-attentive mechanism\nfor sentence and meanings.",
    "start": "496440",
    "end": "502440"
  },
  {
    "text": "And now moving on to the\nmain crux of the Transformers",
    "start": "502440",
    "end": "505920"
  },
  {
    "text": "paper, which was the\nself-attention block.",
    "start": "505920",
    "end": "508230"
  },
  {
    "text": "So self-attention is the\nbasis, is the main building",
    "start": "508230",
    "end": "512370"
  },
  {
    "text": "block for what makes a\nTransformers model work so well",
    "start": "512370",
    "end": "516450"
  },
  {
    "text": "and to enable them and\nmake them so powerful.",
    "start": "516450",
    "end": "520179"
  },
  {
    "text": "So to think of it\nmore easily, we",
    "start": "520179",
    "end": "522630"
  },
  {
    "text": "can break down\nthe self-attention",
    "start": "522630",
    "end": "524970"
  },
  {
    "text": "as a search retrieval problem.",
    "start": "524970",
    "end": "526870"
  },
  {
    "text": "So the problem is that\ngiven a query, q, and v,",
    "start": "526870",
    "end": "531010"
  },
  {
    "text": "we need to find\na set of keys, k,",
    "start": "531010",
    "end": "533070"
  },
  {
    "text": "which are most similar\nto q and return",
    "start": "533070",
    "end": "535230"
  },
  {
    "text": "the corresponding key\nvalues called v. Now",
    "start": "535230",
    "end": "538529"
  },
  {
    "text": "these three vectors can be\ndrawn from the same source.",
    "start": "538530",
    "end": "540990"
  },
  {
    "text": "For example, we can\nhave that q, k, and v",
    "start": "540990",
    "end": "543690"
  },
  {
    "text": "are all equal to a single\nvector x, where x can",
    "start": "543690",
    "end": "546360"
  },
  {
    "text": "be output of a previous layer.",
    "start": "546360",
    "end": "548459"
  },
  {
    "text": "In Transformers,\nthese vectors are",
    "start": "548460",
    "end": "551100"
  },
  {
    "text": "obtained by applying different\nlinear transformations to x.",
    "start": "551100",
    "end": "554220"
  },
  {
    "text": "So as to enable the\nmodel to capture",
    "start": "554220",
    "end": "557069"
  },
  {
    "text": "more complex interactions\nbetween the different tokens",
    "start": "557070",
    "end": "560550"
  },
  {
    "text": "at different places\nof the sentence.",
    "start": "560550",
    "end": "562779"
  },
  {
    "text": "Now how attention is computed\nis just a weighted summation",
    "start": "562780",
    "end": "566190"
  },
  {
    "text": "of the similarities between\nthe query and key vectors,",
    "start": "566190",
    "end": "569950"
  },
  {
    "text": "which is weighted by\nthe respective value",
    "start": "569950",
    "end": "571680"
  },
  {
    "text": "for those keys.",
    "start": "571680",
    "end": "573120"
  },
  {
    "text": "And in the Transformers\npaper, they",
    "start": "573120",
    "end": "575580"
  },
  {
    "text": "used the scaled dot-product\nas a similarity function",
    "start": "575580",
    "end": "578160"
  },
  {
    "text": "for the queries and keys.",
    "start": "578160",
    "end": "580110"
  },
  {
    "text": "And another important\naspect of the Transformers",
    "start": "580110",
    "end": "582540"
  },
  {
    "text": "was the introduction of\nMulti-head self-attention.",
    "start": "582540",
    "end": "586199"
  },
  {
    "text": "So what Multi-head\nSelf-attention means",
    "start": "586200",
    "end": "588150"
  },
  {
    "text": "is that the self-attention\nis for at every layer",
    "start": "588150",
    "end": "591600"
  },
  {
    "text": "the self-attention is\nperformed multiple times,",
    "start": "591600",
    "end": "594089"
  },
  {
    "text": "which enables the model to\nlearn multiple representations",
    "start": "594090",
    "end": "597270"
  },
  {
    "text": "of spaces.",
    "start": "597270",
    "end": "598600"
  },
  {
    "text": "So in a way, you can think of\nit that each head has a power",
    "start": "598600",
    "end": "605279"
  },
  {
    "text": "to look at different things and\nto learn different semantics.",
    "start": "605280",
    "end": "608460"
  },
  {
    "text": "For example, one\nhead can be learning",
    "start": "608460",
    "end": "611460"
  },
  {
    "text": "to try to predict what\nis the part of speech",
    "start": "611460",
    "end": "614460"
  },
  {
    "text": "for those tokens.",
    "start": "614460",
    "end": "615240"
  },
  {
    "text": "One head might be learning\nwhat is the syntactic structure",
    "start": "615240",
    "end": "618720"
  },
  {
    "text": "of the sentence and\nall those things",
    "start": "618720",
    "end": "621300"
  },
  {
    "text": "that are there to\nunderstand what the upcoming",
    "start": "621300",
    "end": "626399"
  },
  {
    "text": "sentence means.",
    "start": "626400",
    "end": "628800"
  },
  {
    "text": "Now to better understand\nwhat the self-attention works",
    "start": "628800",
    "end": "631303"
  },
  {
    "text": "and what are the\ndifferent computations,",
    "start": "631303",
    "end": "632970"
  },
  {
    "text": "there is a short video.",
    "start": "632970",
    "end": "635199"
  },
  {
    "text": "So in this-- so as\nyou can see, there",
    "start": "635200",
    "end": "639330"
  },
  {
    "text": "are three incoming tokens,\nso input 1, input 2, input 3.",
    "start": "639330",
    "end": "643260"
  },
  {
    "text": "We apply linear transformations\nto get the key value vectors",
    "start": "643260",
    "end": "647820"
  },
  {
    "text": "for each input and then\ngive-- once a query, q, comes,",
    "start": "647820",
    "end": "651600"
  },
  {
    "text": "we calculate its similarity\nwith the respective key vectors",
    "start": "651600",
    "end": "655050"
  },
  {
    "text": "and then multiply those\nscores with the value vector.",
    "start": "655050",
    "end": "659430"
  },
  {
    "text": "And then add them all\nup to get the output.",
    "start": "659430",
    "end": "662730"
  },
  {
    "text": "The same computation is then\nperformed on all the tokens.",
    "start": "662730",
    "end": "666510"
  },
  {
    "text": "And we get the output of\nthe self-attention layer.",
    "start": "666510",
    "end": "670350"
  },
  {
    "text": "So as you can see\nhere, the final output",
    "start": "670350",
    "end": "672690"
  },
  {
    "text": "of self-attention layer\nis in dark green that's",
    "start": "672690",
    "end": "675150"
  },
  {
    "text": "at the top of the screen.",
    "start": "675150",
    "end": "677750"
  },
  {
    "text": "So now again, for\nthe final token,",
    "start": "677750",
    "end": "679480"
  },
  {
    "text": "we perform everything same,\nqueries multiplied by keys.",
    "start": "679480",
    "end": "682810"
  },
  {
    "text": "We get the similarity scores.",
    "start": "682810",
    "end": "684460"
  },
  {
    "text": "And then those similarity\nscores weigh the value vectors.",
    "start": "684460",
    "end": "687790"
  },
  {
    "text": "And then we finally\nperform the addition",
    "start": "687790",
    "end": "689589"
  },
  {
    "text": "to get the self-attention output\nof the Transformers block.",
    "start": "689590",
    "end": "694090"
  },
  {
    "start": "694090",
    "end": "699240"
  },
  {
    "start": "698000",
    "end": "812000"
  },
  {
    "text": "Apart from\nself-attention, there are",
    "start": "699240",
    "end": "701459"
  },
  {
    "text": "some other necessary\ningredients that makes",
    "start": "701460",
    "end": "704640"
  },
  {
    "text": "the Transformers so powerful.",
    "start": "704640",
    "end": "706470"
  },
  {
    "text": "One important aspect\nis the presence",
    "start": "706470",
    "end": "709170"
  },
  {
    "text": "of positional representations\nor the embedding layer.",
    "start": "709170",
    "end": "711630"
  },
  {
    "text": "So the way RNNs\nworked very well was",
    "start": "711630",
    "end": "716280"
  },
  {
    "text": "that since they process\neach of the information",
    "start": "716280",
    "end": "718710"
  },
  {
    "text": "in a sequential ordering.",
    "start": "718710",
    "end": "720120"
  },
  {
    "text": "So there was this notion\nof ordering, right,",
    "start": "720120",
    "end": "723630"
  },
  {
    "text": "which is also very important\nin understanding language",
    "start": "723630",
    "end": "726180"
  },
  {
    "text": "because we all know that\nwe read any piece of text",
    "start": "726180",
    "end": "730020"
  },
  {
    "text": "from left to right in\nmost of the languages",
    "start": "730020",
    "end": "734970"
  },
  {
    "text": "and also right to left\nin some languages.",
    "start": "734970",
    "end": "737199"
  },
  {
    "text": "So there is a notion\nof ordering which",
    "start": "737200",
    "end": "739080"
  },
  {
    "text": "is lost in kind\nof self-attention",
    "start": "739080",
    "end": "740790"
  },
  {
    "text": "because every word is\nattending to every other word.",
    "start": "740790",
    "end": "744060"
  },
  {
    "text": "That's why this paper\nintroduced a separate embedding",
    "start": "744060",
    "end": "747900"
  },
  {
    "text": "layer for introducing\npositional representations.",
    "start": "747900",
    "end": "750990"
  },
  {
    "text": "The second important aspect\nis having nonlinearities.",
    "start": "750990",
    "end": "754230"
  },
  {
    "text": "So if you think of all\nthe computation that",
    "start": "754230",
    "end": "756821"
  },
  {
    "text": "is happening in the\nself-attention layer,",
    "start": "756822",
    "end": "758530"
  },
  {
    "text": "it's all linear because it's\nall matrix multiplication.",
    "start": "758530",
    "end": "761130"
  },
  {
    "text": "But as we all know, that\ndeep learning models",
    "start": "761130",
    "end": "764190"
  },
  {
    "text": "work well when\nthey are able to--",
    "start": "764190",
    "end": "766980"
  },
  {
    "text": "when they are able to learn more\ncomplex mappings between input",
    "start": "766980",
    "end": "770070"
  },
  {
    "text": "and output, which can be\nattained by a simple MLP.",
    "start": "770070",
    "end": "774270"
  },
  {
    "text": "And the third important\ncomponent of the self--",
    "start": "774270",
    "end": "776640"
  },
  {
    "text": "of the Transformers\nis the masking.",
    "start": "776640",
    "end": "779020"
  },
  {
    "text": "So masking is what allows to\nparallelize the operations.",
    "start": "779020",
    "end": "783060"
  },
  {
    "text": "Since every word can\nattend to every other word,",
    "start": "783060",
    "end": "785490"
  },
  {
    "text": "in the decoder part\nof our transformers,",
    "start": "785490",
    "end": "788391"
  },
  {
    "text": "which Advay is going to\nbe talking about later,",
    "start": "788392",
    "end": "790350"
  },
  {
    "text": "is the problem becomes that\nyou don't want the decoder",
    "start": "790350",
    "end": "793800"
  },
  {
    "text": "to look into the\nfuture because that",
    "start": "793800",
    "end": "795839"
  },
  {
    "text": "can result in data leakage.",
    "start": "795840",
    "end": "798210"
  },
  {
    "text": "So that's why masking\nhelps the decoder",
    "start": "798210",
    "end": "800730"
  },
  {
    "text": "to avoid that future information\nand learn only what has been--",
    "start": "800730",
    "end": "806805"
  },
  {
    "text": "what the model has\nprocessed so far.",
    "start": "806805",
    "end": "809589"
  },
  {
    "text": "So now onto the\nencoder-decoder architecture",
    "start": "809590",
    "end": "813450"
  },
  {
    "start": "812000",
    "end": "962000"
  },
  {
    "text": "of the Transformers.",
    "start": "813450",
    "end": "814410"
  },
  {
    "text": "Advay.",
    "start": "814410",
    "end": "816310"
  },
  {
    "text": "Yeah.",
    "start": "816310",
    "end": "816810"
  },
  {
    "text": "Thanks, Chetanya, for\ntalking about self-attention.",
    "start": "816810",
    "end": "819730"
  },
  {
    "text": "So self-attention is sort\nof the key ingredient or one",
    "start": "819730",
    "end": "824130"
  },
  {
    "text": "of the key ingredients\nthat allows Transformers",
    "start": "824130",
    "end": "826200"
  },
  {
    "text": "to work so well, but\nat a very high level,",
    "start": "826200",
    "end": "828910"
  },
  {
    "text": "the model that was proposed\nin the Vaswani et al.",
    "start": "828910",
    "end": "832019"
  },
  {
    "text": "paper of 2017 was like previous\nlanguage models in the sense",
    "start": "832020",
    "end": "836670"
  },
  {
    "text": "that it had an\nencoder-decoder architecture.",
    "start": "836670",
    "end": "839519"
  },
  {
    "text": "What that means is--",
    "start": "839520",
    "end": "840857"
  },
  {
    "text": "let's say you're working\non a translation problem.",
    "start": "840857",
    "end": "842940"
  },
  {
    "text": "You want to translate\nEnglish to French.",
    "start": "842940",
    "end": "844800"
  },
  {
    "text": "The way that would\nwork is you would",
    "start": "844800",
    "end": "846300"
  },
  {
    "text": "read in the entire input\nof your English sentence.",
    "start": "846300",
    "end": "850140"
  },
  {
    "text": "You would encode that input.",
    "start": "850140",
    "end": "851770"
  },
  {
    "text": "So that's the encoded\npart of the network.",
    "start": "851770",
    "end": "853870"
  },
  {
    "text": "And then you would\ngenerate token",
    "start": "853870",
    "end": "856020"
  },
  {
    "text": "by token the corresponding\nFrench translation.",
    "start": "856020",
    "end": "858420"
  },
  {
    "text": "And the decoder is the\npart of the network",
    "start": "858420",
    "end": "860579"
  },
  {
    "text": "that is responsible for\ngenerating those tokens.",
    "start": "860580",
    "end": "864430"
  },
  {
    "text": "So you can think of these\nencoder blocks and decoder",
    "start": "864430",
    "end": "867990"
  },
  {
    "text": "blocks as essentially\nsomething like LEGO.",
    "start": "867990",
    "end": "870990"
  },
  {
    "text": "They have these subcomponents\nthat make them up.",
    "start": "870990",
    "end": "874560"
  },
  {
    "text": "And in particular,\nthe encoder block",
    "start": "874560",
    "end": "876600"
  },
  {
    "text": "has three main subcomponents.",
    "start": "876600",
    "end": "878680"
  },
  {
    "text": "The first is the\nself-attention layer",
    "start": "878680",
    "end": "880860"
  },
  {
    "text": "that Chetanya talked\nabout earlier.",
    "start": "880860",
    "end": "883140"
  },
  {
    "text": "And, as talked about\nearlier as well,",
    "start": "883140",
    "end": "886180"
  },
  {
    "text": "you need a feed-forward\nlayer after that",
    "start": "886180",
    "end": "888510"
  },
  {
    "text": "because the\nself-attention layer only",
    "start": "888510",
    "end": "890460"
  },
  {
    "text": "performs linear operations.",
    "start": "890460",
    "end": "892140"
  },
  {
    "text": "And so you need something that\ncan capture the nonlinearities.",
    "start": "892140",
    "end": "895560"
  },
  {
    "text": "You also have a layer\nnorm after this.",
    "start": "895560",
    "end": "898120"
  },
  {
    "text": "And lastly, there are\nresidual connections",
    "start": "898120",
    "end": "900300"
  },
  {
    "text": "between different\nencoder blocks.",
    "start": "900300",
    "end": "902820"
  },
  {
    "text": "The decoder is very\nsimilar to the encoder,",
    "start": "902820",
    "end": "905195"
  },
  {
    "text": "but there's one\ndifference, which",
    "start": "905195",
    "end": "906570"
  },
  {
    "text": "is that it has this extra layer,\nbecause the decoder doesn't",
    "start": "906570",
    "end": "909720"
  },
  {
    "text": "just do Multi-head\nAttention on the output",
    "start": "909720",
    "end": "912600"
  },
  {
    "text": "of the previous layers.",
    "start": "912600",
    "end": "914250"
  },
  {
    "text": "So for context, the encoder\ndoes Multi-head Attention",
    "start": "914250",
    "end": "917425"
  },
  {
    "text": "for each self-attention\nlayer in the encoder block.",
    "start": "917425",
    "end": "921360"
  },
  {
    "text": "And each of the encoder blocks\ndoes Multi-head Attention",
    "start": "921360",
    "end": "925019"
  },
  {
    "text": "looking at the previous\nlayers of the encoder blocks.",
    "start": "925020",
    "end": "929550"
  },
  {
    "text": "The decoder, however,\ndoes that in the sense",
    "start": "929550",
    "end": "933000"
  },
  {
    "text": "that it also looks at the\nprevious layers of the decoder,",
    "start": "933000",
    "end": "935880"
  },
  {
    "text": "but it also looks at the\noutput of the encoder.",
    "start": "935880",
    "end": "938460"
  },
  {
    "text": "And so it needs a\nMulti-head Attention layer",
    "start": "938460",
    "end": "940590"
  },
  {
    "text": "over the encoder blocks.",
    "start": "940590",
    "end": "943910"
  },
  {
    "text": "And lastly, there's\nmasking as well.",
    "start": "943910",
    "end": "946860"
  },
  {
    "text": "So if you are--\nbecause every token can",
    "start": "946860",
    "end": "949519"
  },
  {
    "text": "look at every other\ntoken, you want",
    "start": "949520",
    "end": "951650"
  },
  {
    "text": "to sort of make\nsure in the decoder",
    "start": "951650",
    "end": "953480"
  },
  {
    "text": "that you're not looking\ninto the future.",
    "start": "953480",
    "end": "955260"
  },
  {
    "text": "So if you're in position\n3, for instance,",
    "start": "955260",
    "end": "957380"
  },
  {
    "text": "you shouldn't be able to look\nat position 4 and position 5.",
    "start": "957380",
    "end": "959960"
  },
  {
    "start": "959960",
    "end": "963120"
  },
  {
    "start": "962000",
    "end": "1084000"
  },
  {
    "text": "So those are sort of\nall the components",
    "start": "963120",
    "end": "965190"
  },
  {
    "text": "that led to the creation of\nthe model in the Vaswani et al.",
    "start": "965190",
    "end": "969750"
  },
  {
    "text": "paper.",
    "start": "969750",
    "end": "970770"
  },
  {
    "text": "And let's talk a little\nbit about the advantages",
    "start": "970770",
    "end": "974310"
  },
  {
    "text": "and drawbacks of this model.",
    "start": "974310",
    "end": "976630"
  },
  {
    "text": "So the two main advantages which\nare huge advantages and which",
    "start": "976630",
    "end": "980040"
  },
  {
    "text": "are why Transformers\nhave done such a good job",
    "start": "980040",
    "end": "982980"
  },
  {
    "text": "of revolutionizing many, many\nfields within deep learning",
    "start": "982980",
    "end": "988560"
  },
  {
    "text": "are as follows.",
    "start": "988560",
    "end": "989970"
  },
  {
    "text": "So the first is there\nis this constant path",
    "start": "989970",
    "end": "992235"
  },
  {
    "text": "length between any two\npositions in a sequence",
    "start": "992235",
    "end": "995279"
  },
  {
    "text": "because every token\nin the sequence",
    "start": "995280",
    "end": "997890"
  },
  {
    "text": "is looking at every other token.",
    "start": "997890",
    "end": "999780"
  },
  {
    "text": "And this basically\nsolves the problem",
    "start": "999780",
    "end": "1001850"
  },
  {
    "text": "that they've talked about\nearlier with long sequences.",
    "start": "1001850",
    "end": "1004579"
  },
  {
    "text": "You don't have this\nproblem with long sequences",
    "start": "1004580",
    "end": "1006740"
  },
  {
    "text": "where if you're trying\nto predict a token",
    "start": "1006740",
    "end": "1009110"
  },
  {
    "text": "that depends on a word that was\nfar, far behind in a sentence.",
    "start": "1009110",
    "end": "1013867"
  },
  {
    "text": "You don't have the problem\nof losing that context.",
    "start": "1013867",
    "end": "1015950"
  },
  {
    "text": "Now the distance\nbetween them is only one",
    "start": "1015950",
    "end": "1018440"
  },
  {
    "text": "in terms of the path length.",
    "start": "1018440",
    "end": "1020690"
  },
  {
    "text": "Also, because of the nature\nof the computation that's",
    "start": "1020690",
    "end": "1023120"
  },
  {
    "text": "happening, Transformer\nmodels lend themselves really",
    "start": "1023120",
    "end": "1025730"
  },
  {
    "text": "well to parallelization\nand because",
    "start": "1025730",
    "end": "1027770"
  },
  {
    "text": "of the advances that\nwe've had with GPUs.",
    "start": "1027770",
    "end": "1029990"
  },
  {
    "text": "Basically, if you take\na Transformer model",
    "start": "1029990",
    "end": "1032209"
  },
  {
    "text": "with n parameters and you take a\nmodel that isn't a Transformer,",
    "start": "1032210",
    "end": "1035359"
  },
  {
    "text": "say, like an LSTM also\nwith n parameters,",
    "start": "1035359",
    "end": "1038064"
  },
  {
    "text": "training the\nTransformer model is",
    "start": "1038065",
    "end": "1039439"
  },
  {
    "text": "going to be much faster\nbecause of the parallelization",
    "start": "1039440",
    "end": "1042829"
  },
  {
    "text": "that it leverages.",
    "start": "1042829",
    "end": "1044329"
  },
  {
    "text": "So those are the advantages.",
    "start": "1044329",
    "end": "1046169"
  },
  {
    "text": "The disadvantages are\nbasically self-attention takes",
    "start": "1046170",
    "end": "1050360"
  },
  {
    "text": "quadratic time because\nevery token looks",
    "start": "1050360",
    "end": "1052309"
  },
  {
    "text": "at every other token.",
    "start": "1052310",
    "end": "1053540"
  },
  {
    "text": "Order n squared as you\nmight know does not scale,",
    "start": "1053540",
    "end": "1056300"
  },
  {
    "text": "and there's actually\nbeen a lot of work",
    "start": "1056300",
    "end": "1058250"
  },
  {
    "text": "in trying to tackle this.",
    "start": "1058250",
    "end": "1060030"
  },
  {
    "text": "So we've linked to some here.",
    "start": "1060030",
    "end": "1061370"
  },
  {
    "text": "Big Bird, Linformer,\nand Reformer",
    "start": "1061370",
    "end": "1063200"
  },
  {
    "text": "are all approaches to try and\nmake this linear or quasilinear",
    "start": "1063200",
    "end": "1067039"
  },
  {
    "text": "essentially.",
    "start": "1067040",
    "end": "1068190"
  },
  {
    "start": "1068190",
    "end": "1071129"
  },
  {
    "text": "And yeah, we highly\nrecommend to--",
    "start": "1071130",
    "end": "1073920"
  },
  {
    "text": "recommend going through\nJay Alammar's blog,",
    "start": "1073920",
    "end": "1075780"
  },
  {
    "text": "\"The Illustrated\nTransformer.\" which",
    "start": "1075780",
    "end": "1077610"
  },
  {
    "text": "provides great visualizations\nand explains everything",
    "start": "1077610",
    "end": "1080670"
  },
  {
    "text": "that we just talked\nabout in great detail.",
    "start": "1080670",
    "end": "1084490"
  },
  {
    "start": "1084000",
    "end": "1364000"
  },
  {
    "text": "Yeah.",
    "start": "1084490",
    "end": "1084990"
  },
  {
    "text": "And I'd like to pass it on\nto Chetanya for applications",
    "start": "1084990",
    "end": "1087950"
  },
  {
    "text": "of Transformers.",
    "start": "1087950",
    "end": "1090149"
  },
  {
    "text": "Yeah.",
    "start": "1090150",
    "end": "1090650"
  },
  {
    "text": "So now moving on to some\nof the recent work--",
    "start": "1090650",
    "end": "1093560"
  },
  {
    "text": "some of the work that\nvery shortly followed",
    "start": "1093560",
    "end": "1096380"
  },
  {
    "text": "the Transformers paper.",
    "start": "1096380",
    "end": "1098190"
  },
  {
    "text": "So one of the\nmodels that came out",
    "start": "1098190",
    "end": "1101299"
  },
  {
    "text": "was GPT, the GPT architecture,\nwhich was released by OpenAI.",
    "start": "1101300",
    "end": "1105880"
  },
  {
    "text": "So OpenAI had the latest model\nthat OpenAI has and the GPT",
    "start": "1105880",
    "end": "1109700"
  },
  {
    "text": "series and the GPT-3.",
    "start": "1109700",
    "end": "1111380"
  },
  {
    "text": "So it consists of\nonly the decoder",
    "start": "1111380",
    "end": "1113630"
  },
  {
    "text": "blocks from Transformers.",
    "start": "1113630",
    "end": "1114980"
  },
  {
    "text": "And it's trained on a\ntraditional language modeling",
    "start": "1114980",
    "end": "1117140"
  },
  {
    "text": "task, which is predicting\nthe current token-- which",
    "start": "1117140",
    "end": "1120650"
  },
  {
    "text": "is creating the next token\ngiven the last t tokens",
    "start": "1120650",
    "end": "1124930"
  },
  {
    "text": "that the model has seen.",
    "start": "1124930",
    "end": "1126650"
  },
  {
    "text": "And for any downstream\ntasks, now the model",
    "start": "1126650",
    "end": "1129440"
  },
  {
    "text": "can just-- you can just\ntrain a classification layer",
    "start": "1129440",
    "end": "1132200"
  },
  {
    "text": "on the last hidden state, which\ncan have any number of labels.",
    "start": "1132200",
    "end": "1137630"
  },
  {
    "text": "And since the model is\ngenerative in nature,",
    "start": "1137630",
    "end": "1141870"
  },
  {
    "text": "you can also use the\npretrained network",
    "start": "1141870",
    "end": "1144950"
  },
  {
    "text": "for generative kind of\ntasks, such as summarization",
    "start": "1144950",
    "end": "1149120"
  },
  {
    "text": "and natural language\ngeneration for that instance.",
    "start": "1149120",
    "end": "1153440"
  },
  {
    "text": "Another important aspect\nthat GPT gained popularity",
    "start": "1153440",
    "end": "1157340"
  },
  {
    "text": "was its ability to\nbe able to perform",
    "start": "1157340",
    "end": "1160429"
  },
  {
    "text": "in-context learning,\nwhat the authors called",
    "start": "1160430",
    "end": "1162380"
  },
  {
    "text": "in-context learning.",
    "start": "1162380",
    "end": "1163680"
  },
  {
    "text": "So this is the ability\nwherein the model can",
    "start": "1163680",
    "end": "1167360"
  },
  {
    "text": "learn under few-shot\nsettings, what",
    "start": "1167360",
    "end": "1170210"
  },
  {
    "text": "the task is to complete\nthe task without performing",
    "start": "1170210",
    "end": "1172460"
  },
  {
    "text": "any gradient updates.",
    "start": "1172460",
    "end": "1173690"
  },
  {
    "text": "For example, let's\nsay, the model",
    "start": "1173690",
    "end": "1175460"
  },
  {
    "text": "has shown a bunch of\naddition examples.",
    "start": "1175460",
    "end": "1178770"
  },
  {
    "text": "And then if you pass in a\nnew input and leave the--",
    "start": "1178770",
    "end": "1183710"
  },
  {
    "text": "and just leave it\nat equal to sign,",
    "start": "1183710",
    "end": "1186649"
  },
  {
    "text": "the model tries to predict\nthe next token, which",
    "start": "1186650",
    "end": "1190340"
  },
  {
    "text": "very well comes out to be\nthe sum of the numbers that",
    "start": "1190340",
    "end": "1195080"
  },
  {
    "text": "is shown.",
    "start": "1195080",
    "end": "1195799"
  },
  {
    "text": "Another example can be\nalso the spell correction",
    "start": "1195800",
    "end": "1198590"
  },
  {
    "text": "task or the translation task.",
    "start": "1198590",
    "end": "1200539"
  },
  {
    "text": "So this was the ability that\nmade GPT-3 so much talked about",
    "start": "1200540",
    "end": "1206870"
  },
  {
    "text": "in the NLP world.",
    "start": "1206870",
    "end": "1208140"
  },
  {
    "text": "And right now also,\nmany applications",
    "start": "1208140",
    "end": "1211520"
  },
  {
    "text": "have been made using GPT-3\nwhich includes one of them being",
    "start": "1211520",
    "end": "1215930"
  },
  {
    "text": "the VS Code Copilot, which tries\nto generate a piece of code",
    "start": "1215930",
    "end": "1221930"
  },
  {
    "text": "given docstring kind of\nnatural language text.",
    "start": "1221930",
    "end": "1226310"
  },
  {
    "text": "Another major\nmodel that came out",
    "start": "1226310",
    "end": "1229370"
  },
  {
    "text": "that was based on the\nTransformers' architecture",
    "start": "1229370",
    "end": "1231380"
  },
  {
    "text": "was BERT.",
    "start": "1231380",
    "end": "1232280"
  },
  {
    "text": "So BERT lends its\nname from-- it's",
    "start": "1232280",
    "end": "1235010"
  },
  {
    "text": "an acronym for Bidirectional\nEncoder Representations",
    "start": "1235010",
    "end": "1238340"
  },
  {
    "text": "from Transformers.",
    "start": "1238340",
    "end": "1239630"
  },
  {
    "text": "It consists of only the encoder\nblocks of the Transformers,",
    "start": "1239630",
    "end": "1243350"
  },
  {
    "text": "which is unlike GPT-3, which\nhad only the decoder blocks.",
    "start": "1243350",
    "end": "1248450"
  },
  {
    "text": "Because of this change,\nthere comes a problem",
    "start": "1248450",
    "end": "1252710"
  },
  {
    "text": "because BERT has only\nthe encoder block.",
    "start": "1252710",
    "end": "1255210"
  },
  {
    "text": "So it sees the\nentire piece of text.",
    "start": "1255210",
    "end": "1257299"
  },
  {
    "text": "It cannot be pretrained on a\nnaive language modeling task",
    "start": "1257300",
    "end": "1260300"
  },
  {
    "text": "because of the problem of\ndata leakage from the future.",
    "start": "1260300",
    "end": "1263700"
  },
  {
    "text": "So what the authors came\nup with was a clever idea.",
    "start": "1263700",
    "end": "1268010"
  },
  {
    "text": "And they came up with a novel\ntask called Masked Language",
    "start": "1268010",
    "end": "1270590"
  },
  {
    "text": "Modeling, which included\nto replace certain words",
    "start": "1270590",
    "end": "1274340"
  },
  {
    "text": "with a placeholder.",
    "start": "1274340",
    "end": "1275210"
  },
  {
    "text": "And then the model tries to\npredict those words given",
    "start": "1275210",
    "end": "1277880"
  },
  {
    "text": "the entire context.",
    "start": "1277880",
    "end": "1280060"
  },
  {
    "text": "Now, apart from this\ntoken-level task,",
    "start": "1280060",
    "end": "1283600"
  },
  {
    "text": "the authors also added\na second objective",
    "start": "1283600",
    "end": "1285789"
  },
  {
    "text": "called the Next Sentence\nPrediction, which",
    "start": "1285790",
    "end": "1287540"
  },
  {
    "text": "was a sentence-level\ntask, wherein",
    "start": "1287540",
    "end": "1291250"
  },
  {
    "text": "given two chunks of text,\nthe model tried to predict",
    "start": "1291250",
    "end": "1294490"
  },
  {
    "text": "whether the second sentence\nfollowed the other sentence",
    "start": "1294490",
    "end": "1297550"
  },
  {
    "text": "or not, followed the\nfirst sentence or not.",
    "start": "1297550",
    "end": "1299800"
  },
  {
    "text": "And now after pretraining this\nmodel for any downstream tasks,",
    "start": "1299800",
    "end": "1303790"
  },
  {
    "text": "the model can be\nfurther fine-tuned",
    "start": "1303790",
    "end": "1305260"
  },
  {
    "text": "with an additional\nclassification layer",
    "start": "1305260",
    "end": "1306885"
  },
  {
    "text": "just like it was in GPT-3.",
    "start": "1306885",
    "end": "1309720"
  },
  {
    "text": "So these are the two models\nthat have been very popular",
    "start": "1309720",
    "end": "1314190"
  },
  {
    "text": "and have made a lot of\napplications, made their way",
    "start": "1314190",
    "end": "1317759"
  },
  {
    "text": "in a lot of applications.",
    "start": "1317760",
    "end": "1319120"
  },
  {
    "text": "But the landscape has\nchanged quite a lot",
    "start": "1319120",
    "end": "1321390"
  },
  {
    "text": "since we have taken this class.",
    "start": "1321390",
    "end": "1322710"
  },
  {
    "text": "There are models with\ndifferent computing techniques",
    "start": "1322710",
    "end": "1325470"
  },
  {
    "text": "like ELECTRA, DeBERTa.",
    "start": "1325470",
    "end": "1327059"
  },
  {
    "text": "And there are also\nmodels that do",
    "start": "1327060",
    "end": "1329490"
  },
  {
    "text": "well in like other\nmodalities and which",
    "start": "1329490",
    "end": "1332707"
  },
  {
    "text": "we are going to be talking\nabout in other lecture series",
    "start": "1332707",
    "end": "1335040"
  },
  {
    "text": "as well.",
    "start": "1335040",
    "end": "1336070"
  },
  {
    "text": "So yeah, that's all\nfrom this lecture.",
    "start": "1336070",
    "end": "1338799"
  },
  {
    "text": "And thank you for tuning in.",
    "start": "1338800",
    "end": "1341670"
  },
  {
    "text": "Yeah.",
    "start": "1341670",
    "end": "1342840"
  },
  {
    "text": "Just want to end by\nsaying, thank you",
    "start": "1342840",
    "end": "1344789"
  },
  {
    "text": "all for watching this.",
    "start": "1344790",
    "end": "1345720"
  },
  {
    "text": "And we have a\nreally exciting set",
    "start": "1345720",
    "end": "1348360"
  },
  {
    "text": "of videos with truly\namazing speakers.",
    "start": "1348360",
    "end": "1351000"
  },
  {
    "text": "And we hope you are able\nto derive value from that.",
    "start": "1351000",
    "end": "1353930"
  },
  {
    "text": "Sure.",
    "start": "1353930",
    "end": "1354810"
  },
  {
    "text": "Thanks a lot.",
    "start": "1354810",
    "end": "1355600"
  },
  {
    "text": "Thank you.",
    "start": "1355600",
    "end": "1356100"
  },
  {
    "text": "Thank you, everyone.",
    "start": "1356100",
    "end": "1358220"
  },
  {
    "start": "1358220",
    "end": "1363200"
  }
]