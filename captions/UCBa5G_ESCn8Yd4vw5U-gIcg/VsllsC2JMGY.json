[
  {
    "start": "0",
    "end": "5090"
  },
  {
    "text": "All right. So let's get started. Today, we're back talking\nabout diffusion models.",
    "start": "5090",
    "end": "11840"
  },
  {
    "text": "I think there's still\na few things that we didn't get a chance to cover. So specifically,\nwe're going to see",
    "start": "11840",
    "end": "18860"
  },
  {
    "text": "how to think about score-based\nmodels as a diffusion model.",
    "start": "18860",
    "end": "24840"
  },
  {
    "text": "And so where does\nthat name come from and what's the relationship\nbetween denoising score matching",
    "start": "24840",
    "end": "31460"
  },
  {
    "text": "and other kinds of\ntraining objectives you might have seen before. We'll see how we can think\nof a diffusion model or even",
    "start": "31460",
    "end": "39530"
  },
  {
    "text": "a score-based model\nto some extent as a type of\nvariational autoencoder at the end of the day,\na hierarchical one",
    "start": "39530",
    "end": "45170"
  },
  {
    "text": "but essentially, a\nvariational autoencoder. And there's going to be some\nconnection between evidence",
    "start": "45170",
    "end": "50270"
  },
  {
    "text": "lower bounds and the denoising\nscore-matching losses that we've been seeing.",
    "start": "50270",
    "end": "55520"
  },
  {
    "text": "Then we'll go back to\ninterpreting diffusion models as normalizing flows.",
    "start": "55520",
    "end": "60720"
  },
  {
    "text": "This is the idea of\nconverting an SDE to an ODE that we briefly talked about\nbut we didn't have time",
    "start": "60720",
    "end": "66740"
  },
  {
    "text": "to go into a lot\nof detail, which will allow us to compute\nlikelihoods exactly",
    "start": "66740",
    "end": "72890"
  },
  {
    "text": "because it's a flow model. And then we'll talk about how\nto make sampling efficient. So take advantage of the fact\nthat once you view generation",
    "start": "72890",
    "end": "81620"
  },
  {
    "text": "as solving some kind of\nordinary differential equation or stochastic\ndifferential equation then you can use advanced\nnumerical methods",
    "start": "81620",
    "end": "88189"
  },
  {
    "text": "to accelerate sampling. And then we'll talk about\ncontrollable generation. So if you want to build\na text-to-image model",
    "start": "88190",
    "end": "94130"
  },
  {
    "text": "or you want to use some kind\nof control, some kind of side information to let's\nsay generate an image,",
    "start": "94130",
    "end": "99720"
  },
  {
    "text": "how do you bring that\ninto the equation, how do you change the models\nto allow you to do that.",
    "start": "99720",
    "end": "106020"
  },
  {
    "text": "So let's start with a brief\nrecap of score-based models. Recall that the\nunderlying idea there",
    "start": "106020",
    "end": "112550"
  },
  {
    "text": "was that we're going\nto model a probability distribution by working with\nthe score function, which",
    "start": "112550",
    "end": "120620"
  },
  {
    "text": "is this gradient\nof the log density of the log-likelihood\nessentially with respect",
    "start": "120620",
    "end": "126630"
  },
  {
    "text": "to the input dimensions. So you think of it\nas a vector field that basically tells\nyou in which direction",
    "start": "126630",
    "end": "132630"
  },
  {
    "text": "you should move if you want\nto increase the likelihood. And we use a deep neural\nnetwork to model it and this score model, which\nis like a neural network that",
    "start": "132630",
    "end": "141329"
  },
  {
    "text": "takes, let's say, an image\nas an input and maps it to the corresponding score or\ngradient of the log-likelihood",
    "start": "141330",
    "end": "148260"
  },
  {
    "text": "evaluated at that point. And we've seen that the score\ncan be estimated from data",
    "start": "148260",
    "end": "157379"
  },
  {
    "text": "using score-matching losses. And it's a relatively\nsimple regression loss",
    "start": "157380",
    "end": "163980"
  },
  {
    "text": "where you try to\ncompare the estimated gradient to the true gradient.",
    "start": "163980",
    "end": "169020"
  },
  {
    "text": "And you look at the L2\ndistance between these two vectors averaged over\nthe data distribution.",
    "start": "169020",
    "end": "174923"
  },
  {
    "text": "And we've seen\nthat there are ways to rewrite that loss\ninto one that you can, at least in principle\ncompute and optimize",
    "start": "174923",
    "end": "181410"
  },
  {
    "text": "as a function of theta. That's intractable or at\nleast expensive with respect",
    "start": "181410",
    "end": "187250"
  },
  {
    "text": "to the data dimension. But we've seen that there is\nsomething called denoising score matching, which basically\nis much more efficient,",
    "start": "187250",
    "end": "196010"
  },
  {
    "text": "the basic idea that\ninstead of trying to estimate the score of\nthe data distribution,",
    "start": "196010",
    "end": "201140"
  },
  {
    "text": "you estimate the score of a\nversion of the data distribution",
    "start": "201140",
    "end": "206300"
  },
  {
    "text": "that has been perturbed with,\nlet's say, Gaussian noise. So you have some kind of\nkernel or noise kernel",
    "start": "206300",
    "end": "213830"
  },
  {
    "text": "which is just like a Gaussian\nat the end of the day that will take a sample x and\nwe'll add the noise to it.",
    "start": "213830",
    "end": "221430"
  },
  {
    "text": "And that defines basically\na new distribution q sigma, which is basically\njust what you get",
    "start": "221430",
    "end": "229190"
  },
  {
    "text": "by convolving the original\ndata distribution, which is unknown with some\nGaussian kernel.",
    "start": "229190",
    "end": "236480"
  },
  {
    "text": "So it's like a\nsmoothed-out version of the data distribution. And it turns out that\nestimating the score of this q",
    "start": "236480",
    "end": "244129"
  },
  {
    "text": "sigma instead of pdata\nis actually efficient and there is this--",
    "start": "244130",
    "end": "249840"
  },
  {
    "text": "if you look at the\nusual regression loss where you compare your\nmodel with the true score",
    "start": "249840",
    "end": "258540"
  },
  {
    "text": "of the noisy data distribution\naveraged over the noisy data distribution, it turns out that\nthat objective can be rewritten",
    "start": "258540",
    "end": "267479"
  },
  {
    "text": "into a denoising objective. So basically if you\ncan train a model as theta that can\ntake a noisy image,",
    "start": "267480",
    "end": "275910"
  },
  {
    "text": "x plus noise, and tries\nto basically estimate the noise vector that\nwas added to the image.",
    "start": "275910",
    "end": "283630"
  },
  {
    "text": "So if you can somehow\ngo from this noisy image to the clean image\nor equivalently",
    "start": "283630",
    "end": "289860"
  },
  {
    "text": "you can figure out what was\nthe vector of noise that was added to this image,\nwhich if you subtract",
    "start": "289860",
    "end": "296550"
  },
  {
    "text": "it will give you\nback the clean image. So if you can denoise,\nthen you can also estimate the score of the noisy\ndata distribution to sigma.",
    "start": "296550",
    "end": "305660"
  },
  {
    "text": "And this does not involve\nany trace of the Jacobian,",
    "start": "305660",
    "end": "310790"
  },
  {
    "text": "it doesn't involve\nany differentiation, it's just a\nstraightforward loss that",
    "start": "310790",
    "end": "316970"
  },
  {
    "text": "is basically just the noise. The reason we're\ndoing denoising is because by solving the denoising\nobjective that you see here",
    "start": "316970",
    "end": "323660"
  },
  {
    "text": "in the third line,\nyou are actually learning the score of\nthe noise-perturbed data",
    "start": "323660",
    "end": "329210"
  },
  {
    "text": "distribution. And that's a good\nthing to have access to because if you\nhave the score, then",
    "start": "329210",
    "end": "335030"
  },
  {
    "text": "you can basically use Langevin\ndynamics to effectively generate samples.",
    "start": "335030",
    "end": "340260"
  },
  {
    "text": "So if you know how to denoise,\nthen you know how in which",
    "start": "340260",
    "end": "345440"
  },
  {
    "text": "direction perturbing your image\nwould increase the likelihood",
    "start": "345440",
    "end": "350480"
  },
  {
    "text": "most rapidly. So you have a\nTaylor approximation of the log-likelihood\naround every data point.",
    "start": "350480",
    "end": "357540"
  },
  {
    "text": "And you can use that\ninformation to inform the way you produce,\nyou explore this place",
    "start": "357540",
    "end": "362979"
  },
  {
    "text": "and you generate samples. The trade-off is that\nyou're no longer estimating the score of the clean\ndata distribution",
    "start": "362980",
    "end": "368960"
  },
  {
    "text": "but you're estimating the score\nof the noise data distribution. And so, yeah, it's\nmuch more scalable.",
    "start": "368960",
    "end": "376070"
  },
  {
    "text": "It reduces the denoising,\nbut the trade-off is you're not estimating the\nscore of the data distribution, you're estimating the score\nof the noise-perturbed data",
    "start": "376070",
    "end": "383720"
  },
  {
    "text": "distribution. And then once you have the\nscore-- back to the question,",
    "start": "383720",
    "end": "390169"
  },
  {
    "text": "if you somehow are able\nto estimate the scores, then you can generate\nsamples by basically",
    "start": "390170",
    "end": "395180"
  },
  {
    "text": "doing some kind of noisy\nstochastic gradient ascent",
    "start": "395180",
    "end": "401630"
  },
  {
    "text": "procedure where you just\ninitialize your particles somewhere and then you follow\nthe arrows essentially,",
    "start": "401630",
    "end": "408215"
  },
  {
    "text": "adding a little bit\nof noise at every step and trying to move towards\nhigh probability regions.",
    "start": "408215",
    "end": "413800"
  },
  {
    "text": "And we've seen that in\norder to make this work, it actually makes sense\nto not only estimate",
    "start": "413800",
    "end": "418860"
  },
  {
    "text": "the score of the\ndata distribution perturbed with a\nsingle noise intensity,",
    "start": "418860",
    "end": "426120"
  },
  {
    "text": "but you actually\nwant to estimate the score of multiple versions\nof the data distributions",
    "start": "426120",
    "end": "433139"
  },
  {
    "text": "where each version\nhas been perturbed with a different\namount of noise. And so you have these different\nviews of the data distribution",
    "start": "433140",
    "end": "440640"
  },
  {
    "text": "that have been perturbed\nwith increasingly small, in this case, amounts of noise.",
    "start": "440640",
    "end": "446919"
  },
  {
    "text": "And what you do is you train\na single model, a single score network, which is conditional\non the noise level.",
    "start": "446920",
    "end": "452890"
  },
  {
    "text": "So it takes a sigma as an input\nand it will estimate the score for all these different\ndata distributions",
    "start": "452890",
    "end": "460680"
  },
  {
    "text": "perturbed with different\namounts of noise. And if you can train\nthis model, then you",
    "start": "460680",
    "end": "466139"
  },
  {
    "text": "can do basically Langevin\ndynamics where what you would do is you would initialize.",
    "start": "466140",
    "end": "475260"
  },
  {
    "text": "If you have this good\nmodel of the score, then you would initialize\nwhich you estimate it",
    "start": "475260",
    "end": "481259"
  },
  {
    "text": "by denoising score matching. What you would do is you would\nthen do Langevin dynamics where",
    "start": "481260",
    "end": "486270"
  },
  {
    "text": "you would initialize\nyour particles somehow, then you follow the gradients\ncorresponding to the data distribution perturbed with\nlarge amounts of noise.",
    "start": "486270",
    "end": "495690"
  },
  {
    "text": "You improve the quality of\nyour samples a little bit, and then you use these\nsamples to initialize",
    "start": "495690",
    "end": "501740"
  },
  {
    "text": "annealed Langevin dynamics\nchain where you're going to use the scores\nof the data distribution",
    "start": "501740",
    "end": "508610"
  },
  {
    "text": "perturbed with a\nsmaller amount of noise. And again, you follow these\ngradients a little bit, and then once again,\nyou take these particles",
    "start": "508610",
    "end": "517099"
  },
  {
    "text": "and you initialize a new chain\nfor an even smaller amount of noise. And you keep doing that until\nthe sigma is small enough",
    "start": "517100",
    "end": "524960"
  },
  {
    "text": "that basically you're sampling\nfrom something very close to the true data distribution.",
    "start": "524960",
    "end": "530910"
  },
  {
    "text": "And this is annealed\nLangevin dynamics and you can see here\nhow it would work. So you would start\nwith pure noise.",
    "start": "530910",
    "end": "536550"
  },
  {
    "text": "And then you would run this\nsequence of Langevin dynamics chains and you would\neventually generate",
    "start": "536550",
    "end": "542910"
  },
  {
    "text": "something that is pretty\nclose to a clean sample. So you can see that it has\nthis denoising flavor where",
    "start": "542910",
    "end": "548833"
  },
  {
    "text": "you would start with\npure noise and then you slowly remove noise until\nyou reveal a sample at the end.",
    "start": "548833",
    "end": "556330"
  },
  {
    "text": "And this is just again\na Langevin dynamics. At every step, you're just\nfollowing the gradient more or less and you go towards a\nclean data sample at the end.",
    "start": "556330",
    "end": "567160"
  },
  {
    "text": "So now this was all recap. Now, what we're\ngoing to do is we're going to start to think\nabout this process",
    "start": "567160",
    "end": "574695"
  },
  {
    "text": "as a variational autoencoder. So if you think about\nit, what's going on here",
    "start": "574695",
    "end": "581790"
  },
  {
    "text": "is that we are going\nfrom right to left. If you think about multiple\nversions of the data",
    "start": "581790",
    "end": "588899"
  },
  {
    "text": "distribution that has been\nperturbed with increasingly large amounts of\nnoise, what we're doing is we're starting\nwith pure noise,",
    "start": "588900",
    "end": "596790"
  },
  {
    "text": "and then we are\niteratively removing noise by running this Langevin chains.",
    "start": "596790",
    "end": "601829"
  },
  {
    "text": "So we run a Langevin\nchain, we try to transform xt into a sample\nfrom the data distribution",
    "start": "601830",
    "end": "609450"
  },
  {
    "text": "with a fairly large\namount of noise. And then we use these particles\nto initialize a new chain",
    "start": "609450",
    "end": "615209"
  },
  {
    "text": "where we follow the gradients\ncorresponding to a data distribution with a\nlittle bit less noise.",
    "start": "615210",
    "end": "620680"
  },
  {
    "text": "And then we run it\nfor a little bit, and then we keep going\nuntil we generate",
    "start": "620680",
    "end": "626680"
  },
  {
    "text": "a clean sample at the end. So we can think of\nthe procedure that we were seeing before as basically\ntrying to iteratively generate",
    "start": "626680",
    "end": "636400"
  },
  {
    "text": "samples from these\nrandom variables, x0 to xt, where these random\nvariables are essentially",
    "start": "636400",
    "end": "644230"
  },
  {
    "text": "what you would get if you were\nto take a real data sample and you were to add noise\nto it, because that's",
    "start": "644230",
    "end": "651459"
  },
  {
    "text": "we were estimating the scores\nof these noise-perturbed data distributions that were indeed\nobtained just by taking data",
    "start": "651460",
    "end": "658570"
  },
  {
    "text": "and adding noise to it. That was the whole idea of the\nnoise conditional score network.",
    "start": "658570",
    "end": "667130"
  },
  {
    "text": "So this is essentially at an\nintuitive level what's going on. We are iteratively reducing\nthe amount of noise",
    "start": "667130",
    "end": "674015"
  },
  {
    "text": "that we have in the sample.  So the inverse of this\nprocess is the one",
    "start": "674015",
    "end": "682189"
  },
  {
    "text": "that we've used to\nbasically train the network to generate samples for the\ndenoising score-matching loss.",
    "start": "682190",
    "end": "689850"
  },
  {
    "text": "And we can think about\nthe inverse process, which is the one that you\nwould use if you wanted to go from data to pure noise.",
    "start": "689850",
    "end": "697380"
  },
  {
    "text": "And that's a very simple\nprocess where at every step, you just add a\nlittle bit of noise.",
    "start": "697380",
    "end": "702910"
  },
  {
    "text": "So if you want to go from x0\nto x1, you take a data point and you add a\nlittle bit of noise.",
    "start": "702910",
    "end": "708570"
  },
  {
    "text": "If you want to go from x1 to\nx2, you take a sample from x1, you add a little bit more noise.",
    "start": "708570",
    "end": "713610"
  },
  {
    "text": "And as you go from\nleft to right, you add more and more\nnoise until at the end",
    "start": "713610",
    "end": "718709"
  },
  {
    "text": "there is no structure\nleft and you are left with basically pure noise.",
    "start": "718710",
    "end": "724750"
  },
  {
    "text": "And so you can start\nto see that this has the flavor of a\nlittle bit of a VAE, or there is an encoder\nprocess, and then",
    "start": "724750",
    "end": "732490"
  },
  {
    "text": "there is a decoder\nprocess down here. And we'll make that more formal\nbut that's the intuition.",
    "start": "732490",
    "end": "739589"
  },
  {
    "text": "And so more specifically,\nbasically, what's going on here",
    "start": "739590",
    "end": "745140"
  },
  {
    "text": "is that there is a\nrelatively simple procedure that we're using to generate\nthese random variables x1,",
    "start": "745140",
    "end": "752629"
  },
  {
    "text": "x2 all the way through xt. And that procedure\nis just adding noise.",
    "start": "752630",
    "end": "757770"
  },
  {
    "text": "So at every step, what you do\nis if you have a sample from xt and you want to generate\na sample from xt",
    "start": "757770",
    "end": "765360"
  },
  {
    "text": "plus 1, what you do is you take\nxt and you add noise to it, just Gaussian noise.",
    "start": "765360",
    "end": "771040"
  },
  {
    "text": "So that defines a set of\nconditional densities, q of xt given xt minus 1,\nwhich are just Gaussians, where",
    "start": "771040",
    "end": "779910"
  },
  {
    "text": "these Gaussians have basically a\ngiven mean and a given variance. And the mean is just like the\ncurrent sample, xt minus 1,",
    "start": "779910",
    "end": "790459"
  },
  {
    "text": "rescaled. It's not super important that\nthere is a rescaling there, but the way you would\ngenerate a sample xt given",
    "start": "790460",
    "end": "797030"
  },
  {
    "text": "a sample xt minus 1 is you would\ndraw a sample from a Gaussian with a mean, which\nis just xt minus 1",
    "start": "797030",
    "end": "804230"
  },
  {
    "text": "rescaled, and some fixed\nstandard deviation or fixed",
    "start": "804230",
    "end": "809269"
  },
  {
    "text": "covariance. So we can think of this process\nof going from data to noise",
    "start": "809270",
    "end": "815810"
  },
  {
    "text": "as some kind of Markov\nprocess where at every step, we add a little bit of\nnoise, and perhaps we rescale",
    "start": "815810",
    "end": "822410"
  },
  {
    "text": "by some fixed constant beta t. Not super important that\nyou do the rescaling, but that's how\nit's usually done.",
    "start": "822410",
    "end": "829770"
  },
  {
    "text": "And so I'm having it\nhere just to make it consistent with the literature.",
    "start": "829770",
    "end": "835460"
  },
  {
    "text": "And this basically defines\na joint distribution.",
    "start": "835460",
    "end": "841170"
  },
  {
    "text": "So given an initial\ndata point x0, there is a joint distribution\nover all these random variables",
    "start": "841170",
    "end": "847860"
  },
  {
    "text": "x1, x2 all the way\nthrough xt, which is just the product of all these\nconditionals, which",
    "start": "847860",
    "end": "856510"
  },
  {
    "text": "are just Gaussians. So that's defines a joint--\ngiven an initial data point x0,",
    "start": "856510",
    "end": "865120"
  },
  {
    "text": "there is a joint\ndistribution over all these other random variables x1\nto xt, where the joint is given",
    "start": "865120",
    "end": "874240"
  },
  {
    "text": "by a product of conditionals. It's like an autoregressive\nmodel but a little bit more simple because it's Markovian.",
    "start": "874240",
    "end": "879939"
  },
  {
    "text": "So the distribution\nof xt does not depend on all the previous\nones, but it basically",
    "start": "879940",
    "end": "885730"
  },
  {
    "text": "only depends on xt minus 1\non the previous time step. And I'm using the notation\nq because it will turn out",
    "start": "885730",
    "end": "896810"
  },
  {
    "text": "that this is indeed the encoder\nin a variational autoencoder. So you can think of this process\nof taking x0 and mapping it",
    "start": "896810",
    "end": "906950"
  },
  {
    "text": "through this vector of random\nvariables, x1 through xt, as some kind of encoder.",
    "start": "906950",
    "end": "913850"
  },
  {
    "text": "And the encoder happens\nto be pretty simple because all you have to do\nis you just have to add noise",
    "start": "913850",
    "end": "918950"
  },
  {
    "text": "to the original data point x0. So in a typical VAE what you\nwould do is you would take x0",
    "start": "918950",
    "end": "925643"
  },
  {
    "text": "and then you would maybe map it\nthrough some neural network that would give you a mean\nand a standard deviation for the distribution\nover the latence.",
    "start": "925643",
    "end": "933440"
  },
  {
    "text": "Here, the way we get a\ndistribution over the latence, which in this case are\njust x1 through xt,",
    "start": "933440",
    "end": "939080"
  },
  {
    "text": "is through this procedure. So there is nothing\nlearned, you just add noise to the\noriginal sample x0.",
    "start": "939080",
    "end": "946579"
  },
  {
    "text": " So this defines up\nsome valid procedure",
    "start": "946580",
    "end": "954529"
  },
  {
    "text": "of basically defining multiple\nviews of an original data point",
    "start": "954530",
    "end": "960650"
  },
  {
    "text": "x0 where every view is\na version of the data point with different\namounts of noise.",
    "start": "960650",
    "end": "966710"
  },
  {
    "text": "The output, technically,\nfor this encoder is higher dimensional\nthan x0 in the sense",
    "start": "966710",
    "end": "973370"
  },
  {
    "text": "that it's the whole collection\nof random variables. Each one of the\nrandom variables, xt,",
    "start": "973370",
    "end": "979560"
  },
  {
    "text": "has the same dimension as x0. It's t times the dimension\nof the original data point.",
    "start": "979560",
    "end": "986430"
  },
  {
    "text": "And yes, the mapping\nis not invertible. We're adding noise\nat every step. So that defines some way of\nbasically mapping a data point",
    "start": "986430",
    "end": "994040"
  },
  {
    "text": "to some vector of\nlatent variables",
    "start": "994040",
    "end": "999230"
  },
  {
    "text": "through this very simple\nprocedure where you just add noise to it. And it turns out that\nadding Gaussian noise",
    "start": "999230",
    "end": "1007210"
  },
  {
    "text": "is pretty convenient because\nyou can also compute--",
    "start": "1007210",
    "end": "1012730"
  },
  {
    "text": "because everything is\nbasically Gaussian, so the marginals of this\ndistribution are also Gaussian.",
    "start": "1012730",
    "end": "1019390"
  },
  {
    "text": "So if you want to compute what\nis the probability of observing",
    "start": "1019390",
    "end": "1025449"
  },
  {
    "text": "a certain noisy view of a\ndata point x0 after t steps, that's another Gaussian where\nthe parameters of that Gaussian",
    "start": "1025450",
    "end": "1033419"
  },
  {
    "text": "basically depend on these beta\ncoefficients that we had before.",
    "start": "1033420",
    "end": "1038880"
  },
  {
    "text": "Again, not super important\nhow you take the betas and you combine them\nto get the alphas. What's important is that if you\nadd a little bit of Gaussian",
    "start": "1038880",
    "end": "1045990"
  },
  {
    "text": "noise at every step the\nresult of applying this kernel multiple times is\nalso another Gaussian,",
    "start": "1045990",
    "end": "1053702"
  },
  {
    "text": "with just different mean and\na different standard deviation but you can basically\ncompute them in closed form.",
    "start": "1053702",
    "end": "1059320"
  },
  {
    "text": "So the probability of\ntransitioning from x0 to xt",
    "start": "1059320",
    "end": "1064470"
  },
  {
    "text": "is some other\nGaussian distribution where the parameters\nof this Gaussian basically depend on\nthe effects of each",
    "start": "1064470",
    "end": "1070500"
  },
  {
    "text": "of the individual\ntransitions that you would do to get through to time xt.",
    "start": "1070500",
    "end": "1077300"
  },
  {
    "text": "And this is important\nfor a couple of reasons. First of all, basically,\nit's efficient to simulate",
    "start": "1077300",
    "end": "1085200"
  },
  {
    "text": "this chain. So if you want to generate\na sample at time step t, you don't have to\ngenerate the whole process",
    "start": "1085200",
    "end": "1091259"
  },
  {
    "text": "of going through t steps. You can directly sample from\nthis marginal distribution",
    "start": "1091260",
    "end": "1097200"
  },
  {
    "text": "without having to\nsimulate the whole chain. And, yeah, if you choose the\nparameters in the right way,",
    "start": "1097200",
    "end": "1103919"
  },
  {
    "text": "this is essentially\nthe same exact way we were generating training data\nfor our denoising score-matching",
    "start": "1103920",
    "end": "1110580"
  },
  {
    "text": "procedure. Remember in the denoising\nscore-matching procedure what we were doing is we're\ntaking clean data",
    "start": "1110580",
    "end": "1115710"
  },
  {
    "text": "and we were adding different\namounts of noise corresponding to different time steps or\ndifferent noise levels sigma",
    "start": "1115710",
    "end": "1124830"
  },
  {
    "text": "generating all these different\nviews like the original data corresponding to different\namounts of noise levels.",
    "start": "1124830",
    "end": "1131590"
  },
  {
    "text": "So it still achieves\nthe same kind of effect, but we're thinking of\nit as a process that",
    "start": "1131590",
    "end": "1137850"
  },
  {
    "text": "adds noise incrementally at\nevery step of this process,",
    "start": "1137850",
    "end": "1143120"
  },
  {
    "text": "which you can also think of\nit as a diffusion process. You can think of what's going\non here as a diffusion process",
    "start": "1143120",
    "end": "1151090"
  },
  {
    "text": "where there is an initial\ndistribution over data points,",
    "start": "1151090",
    "end": "1156192"
  },
  {
    "text": "which is the data\ndistribution, which could be, for example, a mixture\nof two Gaussians. It looks like this.",
    "start": "1156192",
    "end": "1161899"
  },
  {
    "text": "Here are the colors basically\nindicate the intensity",
    "start": "1161900",
    "end": "1166960"
  },
  {
    "text": "how large the PDF\nis at that point. So yellow points tend to\nhave higher probability mass",
    "start": "1166960",
    "end": "1173559"
  },
  {
    "text": "than let's say these\nblue points that are more closer to the tails\nof these two Gaussians.",
    "start": "1173560",
    "end": "1179110"
  },
  {
    "text": "And what's going on is\nthat we're basically defining these noise-perturbed\ndata distributions",
    "start": "1179110",
    "end": "1186039"
  },
  {
    "text": "by basically adding noise. So we randomly draw a sample\nfrom the data distribution and we add noise to it.",
    "start": "1186040",
    "end": "1192350"
  },
  {
    "text": "And by doing that, we define\nall these noise-perturbed distributions.",
    "start": "1192350",
    "end": "1197840"
  },
  {
    "text": "As you can see, the shape\nof this distribution changes as you add\nmore and more noise.",
    "start": "1197840",
    "end": "1204190"
  },
  {
    "text": "You see that there\nis no probability mass here in the middle. But if you add a little bit of\nnoise to the original samples,",
    "start": "1204190",
    "end": "1210100"
  },
  {
    "text": "then you're going to get a\nlittle bit of probability mass here in the middle. And then if you\nadd a lot of noise,",
    "start": "1210100",
    "end": "1215380"
  },
  {
    "text": "then basically everything\njust becomes Gaussian. And so you can think of it as a\ndiffusion where basically giving",
    "start": "1215380",
    "end": "1225600"
  },
  {
    "text": "an initial condition, which is\njust a data point on this line,",
    "start": "1225600",
    "end": "1233100"
  },
  {
    "text": "you can imagine simulating this\nprocess where you add noise",
    "start": "1233100",
    "end": "1238110"
  },
  {
    "text": "at every step. And eventually, the\nprobability mass is going to be all spread\nout all over the space.",
    "start": "1238110",
    "end": "1244020"
  },
  {
    "text": "And this behaves like the\nprocess of heat diffusing,",
    "start": "1244020",
    "end": "1249180"
  },
  {
    "text": "let's say, in a\nsolid or some sort. And so that's why it's called\na diffusion because there",
    "start": "1249180",
    "end": "1254910"
  },
  {
    "text": "is some kind of process\nthat takes probability mass and then diffuses it\nover the whole space.",
    "start": "1254910",
    "end": "1260835"
  },
  {
    "text": " And that this\nprocess essentially",
    "start": "1260835",
    "end": "1268500"
  },
  {
    "text": "is defined by the\ntransition kernel which is just basically the Gaussian. In theory, if you\nthink of it as a--",
    "start": "1268500",
    "end": "1277049"
  },
  {
    "text": "well, maybe we'll come back\nto this in a few slides, but yes, to some extent,\nyou need several things.",
    "start": "1277050",
    "end": "1284460"
  },
  {
    "text": "You need to be\nable to smooth out. You destroy the structure\nso that you end up",
    "start": "1284460",
    "end": "1291179"
  },
  {
    "text": "with a distribution at the end\nthat is easy to sample from. Because essentially what we're\ngoing to do at inference time",
    "start": "1291180",
    "end": "1298017"
  },
  {
    "text": "is we're going to try\nto invert this process and we're going to try\nto go from noise to data.",
    "start": "1298017",
    "end": "1303120"
  },
  {
    "text": "So first, you have to define a\nprocess that destroys structure and goes from data to noise. And the noise that\nyou get at the end",
    "start": "1303120",
    "end": "1309750"
  },
  {
    "text": "has to be something simple. It has to be\nefficient so you need to be able to simulate any slice\nhere efficiently because we'll",
    "start": "1309750",
    "end": "1320550"
  },
  {
    "text": "see that the learning\nobjective will end up being denoising score matching. And so you need to be\nable to sample from it",
    "start": "1320550",
    "end": "1326500"
  },
  {
    "text": "efficiently if you\nwant to use denoising score-matching-like objectives.",
    "start": "1326500",
    "end": "1331900"
  },
  {
    "text": "Other than that,\npretty much, yes, you get a valid probabilistic model. If you have those two things,\nthen you can essentially",
    "start": "1331900",
    "end": "1340120"
  },
  {
    "text": "use this machinery. And it turns out that the way\nto invert this process exactly",
    "start": "1340120",
    "end": "1345370"
  },
  {
    "text": "involves the score. So if you have the score, then\nyou can invert the process.",
    "start": "1345370",
    "end": "1350530"
  },
  {
    "text": "Or if you think of it from a\nVAE perspective as we'll see, then you can also just try to\nbasically invert the process",
    "start": "1350530",
    "end": "1357880"
  },
  {
    "text": "by trying to learn some kind of\ndecoder that will try to invert.",
    "start": "1357880",
    "end": "1363340"
  },
  {
    "text": "You can think of this process\nare going from data to noise as an encoder, then you can\ntry training an ELBO just",
    "start": "1363340",
    "end": "1370480"
  },
  {
    "text": "by variationally try to\nlearn an operator that goes in the opposite direction,\nwhich may not involve",
    "start": "1370480",
    "end": "1378320"
  },
  {
    "text": "the score in general, but\nif everything is Gaussian, then it turns out that\nwhat you need is the score.",
    "start": "1378320",
    "end": "1385283"
  },
  {
    "text": "The important thing is that the\noriginal data distribution here, OK, it's a mixture of Gaussians,\nbut it can be anything.",
    "start": "1385283",
    "end": "1392100"
  },
  {
    "text": "It doesn't have to be\nremotely close to a Gaussian distribution.",
    "start": "1392100",
    "end": "1397430"
  },
  {
    "text": "It has to be continuous\nfor this machinery to be applicable directly,\nalthough we'll see later",
    "start": "1397430",
    "end": "1403240"
  },
  {
    "text": "when we talk about\nlatent diffusion models that you can actually\nalso embed discrete data",
    "start": "1403240",
    "end": "1409429"
  },
  {
    "text": "into a continuous\nspace and then it will fall out pretty naturally\nfrom our VAE perspective.",
    "start": "1409430",
    "end": "1415280"
  },
  {
    "text": "But the initial distribution\ndoesn't have to be Gaussian. It could be just a distribution\nover natural images, which",
    "start": "1415280",
    "end": "1421490"
  },
  {
    "text": "is far from Gaussian. What's important is that\nthe transition kernel that you use to spread\nout the probability mass",
    "start": "1421490",
    "end": "1428059"
  },
  {
    "text": "is Gaussian so that you destroy\nstructure in a controllable way.",
    "start": "1428060",
    "end": "1433790"
  },
  {
    "text": "And you know that after adding\na sufficiently large amount of Gaussian noise you have\na Gaussian distribution.",
    "start": "1433790",
    "end": "1439919"
  },
  {
    "text": "The signal-to-noise is basically\nextremely low and at that point,",
    "start": "1439920",
    "end": "1444930"
  },
  {
    "text": "sampling from a pure\nnoise is the same as starting from a data point\nand adding a huge amount",
    "start": "1444930",
    "end": "1450778"
  },
  {
    "text": "of noise, essentially. So this is just OK. This is a diffusion\nand that basically",
    "start": "1450778",
    "end": "1456360"
  },
  {
    "text": "maps from data on the left-hand\nside here to pure noise",
    "start": "1456360",
    "end": "1462440"
  },
  {
    "text": "on the right-hand side. And what this\nsuggests is that there",
    "start": "1462440",
    "end": "1470530"
  },
  {
    "text": "might be a way of generating\nsamples which basically involves the process of inverting\nthis procedure.",
    "start": "1470530",
    "end": "1479059"
  },
  {
    "text": "So we had a simple procedure\nthat goes from data to noise just by adding Gaussian\nnoise at every step. So we had a collection\nof random variables",
    "start": "1479060",
    "end": "1486070"
  },
  {
    "text": "with some well-defined\njoint distribution, which was just\nlike that Gaussian",
    "start": "1486070",
    "end": "1492789"
  },
  {
    "text": "defined in terms of the q\nthat given an xt minus 1,",
    "start": "1492790",
    "end": "1497800"
  },
  {
    "text": "you define the next one by\njust adding noise to it.",
    "start": "1497800",
    "end": "1504880"
  },
  {
    "text": "If we could, we could\ntry to generate samples by inverting this process.",
    "start": "1504880",
    "end": "1511190"
  },
  {
    "text": "So what we could do is we\ncould try by initially sampling",
    "start": "1511190",
    "end": "1516460"
  },
  {
    "text": "a value for this random\nvariable x capital T. And we know that\nx capital T comes",
    "start": "1516460",
    "end": "1523720"
  },
  {
    "text": "from some known distribution,\nfor example, just pure Gaussian noise.",
    "start": "1523720",
    "end": "1529390"
  },
  {
    "text": "And this notation\nhere, this pi, you can think of it as a prior,\nsome fixed distribution that this diffusion process\nbasically converges to.",
    "start": "1529390",
    "end": "1537682"
  },
  {
    "text": "So that's easy. And then what we\ncould try to do is we could try to basically\nreverse this process by sampling",
    "start": "1537682",
    "end": "1545559"
  },
  {
    "text": "from these conditionals. So you would sample xt minus 1\ngiven x capital T, and then we",
    "start": "1545560",
    "end": "1551679"
  },
  {
    "text": "could go back one step at a time\ngoing from pure noise to data.",
    "start": "1551680",
    "end": "1558940"
  },
  {
    "text": "And this procedure\nwould work perfectly if somehow we had\na way of knowing",
    "start": "1558940",
    "end": "1567280"
  },
  {
    "text": "what this distribution is. So we know how to define\nq of xt given xt minus 1",
    "start": "1567280",
    "end": "1574930"
  },
  {
    "text": "because that's just a Gaussian. But the reverse kernel which\ngoes from xt to xt minus 1,",
    "start": "1574930",
    "end": "1581920"
  },
  {
    "text": "the one that goes from right\nto left is actually unknown. And that's why this procedure\ncannot be directly used.",
    "start": "1581920",
    "end": "1593520"
  },
  {
    "text": "But what we can\ntry to do is we can try to learn some\nkind of approximation",
    "start": "1593520",
    "end": "1599030"
  },
  {
    "text": "of this reverse kernel that\ngoes from right to left",
    "start": "1599030",
    "end": "1610690"
  },
  {
    "text": "so that we can\nlearn basically how to remove noise from a sample. ",
    "start": "1610690",
    "end": "1617950"
  },
  {
    "text": "And so, basically, that's\nthe core underlying idea.",
    "start": "1617950",
    "end": "1624750"
  },
  {
    "text": "We're going to define-- now you can start\nto see we're going to define a decoder or\nan iterative decoder,",
    "start": "1624750",
    "end": "1631840"
  },
  {
    "text": "which has the flavor of a VAE. You start by sampling\na latent variable",
    "start": "1631840",
    "end": "1637500"
  },
  {
    "text": "from a simple prior, which could\nbe just a Gaussian distribution. And then we go\nfrom right to left",
    "start": "1637500",
    "end": "1647070"
  },
  {
    "text": "by sampling from these\nconditionals, p of xt minus 1 given xt, which are\ndefined variationally in a sense",
    "start": "1647070",
    "end": "1656750"
  },
  {
    "text": "that this is our generative\nmodel, this is how we usually-- just like in a\nVAE, the decoder is",
    "start": "1656750",
    "end": "1663890"
  },
  {
    "text": "defined through some\nsort of neural network. In this case, the probability\ndensity over xt minus 1 given xt",
    "start": "1663890",
    "end": "1670310"
  },
  {
    "text": "is a Gaussian. And as usual, the\nparameters of the Gaussian are computed by\nsome neural network.",
    "start": "1670310",
    "end": "1677510"
  },
  {
    "text": "So it's the same\nflavor of VAE where you would sample z\nfrom a simple prior",
    "start": "1677510",
    "end": "1682840"
  },
  {
    "text": "and you feed the z into some\nneural network like the mu theta here to get a parameter for a\nGaussian distribution over x",
    "start": "1682840",
    "end": "1692799"
  },
  {
    "text": "and then you would sample\nfrom that distribution. It has the similar\nflavor here in the sense",
    "start": "1692800",
    "end": "1697940"
  },
  {
    "text": "that the reverse process\nis defined variationally through these conditionals\nwhich are parameterized",
    "start": "1697940",
    "end": "1707120"
  },
  {
    "text": "in terms of neural networks. And so there is a true\ndenoising distribution",
    "start": "1707120",
    "end": "1712429"
  },
  {
    "text": "that would map you from\nxt through xt minus 1. We don't know what\nthis object is.",
    "start": "1712430",
    "end": "1717920"
  },
  {
    "text": "We're going to approximate\nit with some Gaussian where the parameters\nof the Gaussian are learned as usual like in\na variational approximation.",
    "start": "1717920",
    "end": "1725720"
  },
  {
    "text": "And we're going to\ntry to choose theta so that these two\ndistributions are close to each other intuitively.",
    "start": "1725720",
    "end": "1731300"
  },
  {
    "text": "So that what we get by\nsampling from this variational approximation of\nthe reverse process",
    "start": "1731300",
    "end": "1737450"
  },
  {
    "text": "is close to what\nwe would get if you were to sample from the\ntrue denoising distribution.",
    "start": "1737450",
    "end": "1744720"
  },
  {
    "text": "So more specifically,\nthis basically defines a joint\ndistribution, which is going to be our\ngenerative distribution where",
    "start": "1744720",
    "end": "1752340"
  },
  {
    "text": "we basically first-- which essentially\njust corresponds to that sampling\nprocedure that I just",
    "start": "1752340",
    "end": "1758460"
  },
  {
    "text": "described where there\nis a prior distribution over the rightmost variable,\nthis xt, which we know",
    "start": "1758460",
    "end": "1765360"
  },
  {
    "text": "comes from a simple\ndistribution like a Gaussian. And then you would sample from\nall the remaining variables",
    "start": "1765360",
    "end": "1772110"
  },
  {
    "text": "one at a time going\nfrom right to left by sampling from these\nconditionals which are all Gaussian with\nparameters defined",
    "start": "1772110",
    "end": "1779580"
  },
  {
    "text": "through some neural networks. The key thing here is that\nwe choose the parameters. So we choose this alpha t\nsuch that basically there is",
    "start": "1779580",
    "end": "1791670"
  },
  {
    "text": "no signal-to-noise at the end. And so you are basically\nleft with pure noise.",
    "start": "1791670",
    "end": "1797559"
  },
  {
    "text": "So basically, this alpha bar\nt goes to 0, essentially,",
    "start": "1797560",
    "end": "1805540"
  },
  {
    "text": "by choosing basically-- you might imagine that if you\nhad a sufficiently large amount",
    "start": "1805540",
    "end": "1810940"
  },
  {
    "text": "of noise, it doesn't\nmatter where you started from everything looks the same.",
    "start": "1810940",
    "end": "1816620"
  },
  {
    "text": "So that's the trick. You have to define a\ndiffusion process such that-- and you have to run it for\na sufficiently long amount",
    "start": "1816620",
    "end": "1823360"
  },
  {
    "text": "of time, which is the same\nthing, such that you forget",
    "start": "1823360",
    "end": "1828610"
  },
  {
    "text": "about the initial\ncondition, or you eventually reach a steady state which is\nknown as some sort of Gaussian",
    "start": "1828610",
    "end": "1836050"
  },
  {
    "text": "distribution, with some known\nmean and standard deviation that you can sample to the end.",
    "start": "1836050",
    "end": "1841750"
  },
  {
    "text": " And so that's what's\ngoing on here.",
    "start": "1841750",
    "end": "1848440"
  },
  {
    "text": "There is a\ndistribution qt, which is going to be close indeed\nto some Gaussian, for example,",
    "start": "1848440",
    "end": "1854080"
  },
  {
    "text": "which you can always set it up. The transition kernel\nis in the right way.",
    "start": "1854080",
    "end": "1860580"
  },
  {
    "text": "As we'll see, you can\nactually do something similar by using Langevin dynamics,\nbecause it turns out",
    "start": "1860580",
    "end": "1869010"
  },
  {
    "text": "that if you train this\nthing variationally, this mu that you learn\nis basically the score.",
    "start": "1869010",
    "end": "1874539"
  },
  {
    "text": "And so as we'll see,\nthere is basically one way of generating samples\nwhich basically just involves",
    "start": "1874540",
    "end": "1881040"
  },
  {
    "text": "t steps, where you just\ndo step, step, step, step t times until you get a\ngood somewhat whatever--",
    "start": "1881040",
    "end": "1890020"
  },
  {
    "text": "hopefully, a good\napproximation of a clean data point but you don't really know. It only depends\non how well you've learned this reverse process.",
    "start": "1890020",
    "end": "1897270"
  },
  {
    "text": "If you're willing to\nthrow more compute at it, you can actually do more\ncomputer-average step",
    "start": "1897270",
    "end": "1905260"
  },
  {
    "text": "to try to invert\nthe process better. One way to do it is to\ndo Langevin dynamics.",
    "start": "1905260",
    "end": "1911679"
  },
  {
    "text": "So Langevin dynamics\nis just a general way of generating samples. It's like an MCMC procedure\nto generate samples from Markov distribution.",
    "start": "1911680",
    "end": "1918557"
  },
  {
    "text": "At the end of the day, we\nknow what kind of distribution we're trying to sample\nfrom here, which is just the noisy data distribution.",
    "start": "1918557",
    "end": "1926035"
  },
  {
    "text": "And if you had the score\nof that distribution then you can generate\nsamples from it. And so we'll see\nthat there is a way",
    "start": "1926035",
    "end": "1931929"
  },
  {
    "text": "to correct the mistakes that\nyou would do if you just",
    "start": "1931930",
    "end": "1937300"
  },
  {
    "text": "were to use this vanilla\nprocedure by putting in more compute.",
    "start": "1937300",
    "end": "1942670"
  },
  {
    "text": "If you wanted to sample\nfrom this joint what you would do is you\nwould sample xt, then you would\nsample xt minus 1,",
    "start": "1942670",
    "end": "1948610"
  },
  {
    "text": "xt minus 2, all\nthe way through x0. There is no Langevin\ndynamics at this point.",
    "start": "1948610",
    "end": "1954020"
  },
  {
    "text": "So it's not deterministic,\nit's stochastic because this transition\nis a Gaussian.",
    "start": "1954020",
    "end": "1959169"
  },
  {
    "text": "So you would have to\nsample from a Gaussian where the parameters are\ngiven by some neural network.",
    "start": "1959170",
    "end": "1964759"
  },
  {
    "text": "So the neural network part\nwould be deterministic, but then just like in a VAE\ndecoder, it's stochastic,",
    "start": "1964760",
    "end": "1972150"
  },
  {
    "text": "the mapping is stochastic. OK. So now we've defined two things.",
    "start": "1972150",
    "end": "1977190"
  },
  {
    "text": "We basically have an encoder and\nwe have a decoder essentially here, which is parameterized by\nthese neural networks mu theta.",
    "start": "1977190",
    "end": "1984510"
  },
  {
    "text": "What we can do is-- so we can start viewing this as\na hierarchical VA or just a VA",
    "start": "1984510",
    "end": "1991650"
  },
  {
    "text": "where there is an encoder\nthat takes x0, a data point, and maps it stochastically\nto a sequence",
    "start": "1991650",
    "end": "1998280"
  },
  {
    "text": "of latent variables, which\nare just the x1, x2, x3 all the way through xt.",
    "start": "1998280",
    "end": "2004250"
  },
  {
    "text": "And there is some kind\nof prior distribution over the latents, which\nis just this p of xt,",
    "start": "2004250",
    "end": "2010520"
  },
  {
    "text": "this just simple Gaussian. And then there is a decoder\nthat would basically invert the process.",
    "start": "2010520",
    "end": "2016970"
  },
  {
    "text": "And the decoder\nis, in this case, as usual just parameterized\nusing neural networks, which",
    "start": "2016970",
    "end": "2023240"
  },
  {
    "text": "are going to be learned. So just like in a VAE,\nthere is this through-- there is an encoder\nand a decoder.",
    "start": "2023240",
    "end": "2031440"
  },
  {
    "text": "So it has the flavor of VAE,\nexcept that like the latent variables, there is a\nsequence of latent variables",
    "start": "2031440",
    "end": "2038370"
  },
  {
    "text": "that are indexed\nby time, and they have this specific structure\nwhere the encoder is actually",
    "start": "2038370",
    "end": "2044170"
  },
  {
    "text": "fixed. There is nothing learned\nabout the encoder. The encoder is just\nadding noise to the data.",
    "start": "2044170",
    "end": "2050679"
  },
  {
    "text": "So it's like a VAE\nwhere the encoder is fixed to have a very\nspecial kind of structure.",
    "start": "2050679",
    "end": "2057179"
  },
  {
    "text": "So recall that the vanilla VAE\nwould look something like this.",
    "start": "2057179",
    "end": "2062739"
  },
  {
    "text": "You have a latent\nvariable model, you have a latent\nvariable z, which has a simple prior\ndistribution a Gaussian.",
    "start": "2062739",
    "end": "2069419"
  },
  {
    "text": "And then there is a decoder,\nwhich would take a z, map it through a couple of\nneural networks mu and sigma,",
    "start": "2069420",
    "end": "2079050"
  },
  {
    "text": "and then p of x given z is\ndefined as a simple distribution of Gaussians where the\nparameters are given",
    "start": "2079050",
    "end": "2086145"
  },
  {
    "text": "by these two neural networks. And then you have the encoder,\nwhich does the opposite. It takes x and it basically\ntries to predict z.",
    "start": "2086145",
    "end": "2095138"
  },
  {
    "text": "And again, that's usually some\nsimple Gaussian distribution where the parameters\nare usually computed",
    "start": "2095139",
    "end": "2100450"
  },
  {
    "text": "by some other neural network\nthat takes x as an input and gives you the parameters\nof the distribution",
    "start": "2100450",
    "end": "2107440"
  },
  {
    "text": "over the latent. And we know that you\nwill train this model by maximizing an ELBO,\nan evidence lower bound",
    "start": "2107440",
    "end": "2115760"
  },
  {
    "text": "that would look\nsomething like this. So you would basically guess the\nvalues of the latent variables",
    "start": "2115760",
    "end": "2122240"
  },
  {
    "text": "using the encoder, then you\nhave the joint distribution over observed variables\nand latent variables as",
    "start": "2122240",
    "end": "2128780"
  },
  {
    "text": "inferred by the encoder. And then you have this\nterm that is basically just encouraging high\nentropy in the encoder.",
    "start": "2128780",
    "end": "2136660"
  },
  {
    "text": "So this is the vanilla version. What we have here is like\na hierarchical version of the vanilla VAE.",
    "start": "2136660",
    "end": "2142910"
  },
  {
    "text": "So if you were to replace\nthis single latent variable z with two latent\nvariables, you would",
    "start": "2142910",
    "end": "2149200"
  },
  {
    "text": "get something that\nlooks like this, where the generative process\nwould start by sampling z2",
    "start": "2149200",
    "end": "2157130"
  },
  {
    "text": "from a simple prior\nand then passing it through a first\ndecoder to generate z1",
    "start": "2157130",
    "end": "2162650"
  },
  {
    "text": "and then another\ndecoder to generate x. And so you have a\njoint distribution,",
    "start": "2162650",
    "end": "2168950"
  },
  {
    "text": "which is just the\nproduct of all the prior over z2, the first\nencoder and the second encoder--",
    "start": "2168950",
    "end": "2176510"
  },
  {
    "text": "the second decoder, sorry. And then you have\nan encoder, which is what you would use to infer\nthe latent variables given x.",
    "start": "2176510",
    "end": "2182960"
  },
  {
    "text": "So it's a distribution\nover z1 and z2 given x. And you would have an\nevidence lower bound",
    "start": "2182960",
    "end": "2188970"
  },
  {
    "text": "that would look like\nthis just like before. Here we have a\nsimple prior over z.",
    "start": "2188970",
    "end": "2194369"
  },
  {
    "text": "If you replace this with a VAE,\nthen you get what we have here. ",
    "start": "2194370",
    "end": "2201100"
  },
  {
    "text": "So that would be the\ntraining objective. And so that's exactly what we\nhave in the diffusion model.",
    "start": "2201100",
    "end": "2207040"
  },
  {
    "text": "So in the diffusion model,\nwell, we don't have just two. We have a sequence\nof latent variables,",
    "start": "2207040",
    "end": "2212550"
  },
  {
    "text": "but it's essentially\nthe same thing. We have a joint\ndecoding distribution, which is what you get by\ngoing from right to left.",
    "start": "2212550",
    "end": "2220300"
  },
  {
    "text": "And then we have an\nencoder, which is fixed, which is just adding\nGaussian noise to the images.",
    "start": "2220300",
    "end": "2225442"
  },
  {
    "text": "The way you would\ntrain this model is by minimizing some ELBO loss\nor maximizing the ELBO averaged",
    "start": "2225443",
    "end": "2232590"
  },
  {
    "text": "over the data distribution. And so just like before\nthe objective function would look something like this.",
    "start": "2232590",
    "end": "2237790"
  },
  {
    "text": "So here q of x0 is just\nthe data distribution. And so you would want to\nmaximize the true log-likelihood",
    "start": "2237790",
    "end": "2245550"
  },
  {
    "text": "over the data distribution. And we don't have access\nto it, so instead, you use the evidence\nlower bound, which",
    "start": "2245550",
    "end": "2252150"
  },
  {
    "text": "is just like the usual\nthing, q of z given x0,",
    "start": "2252150",
    "end": "2257220"
  },
  {
    "text": "p of x, z divided\nby q of z given x. That's the usual\nevidence lower bound.",
    "start": "2257220",
    "end": "2263470"
  },
  {
    "text": "And there is just a\nminus sign because I want to minimize that objective\nas a function of theta. ",
    "start": "2263470",
    "end": "2270819"
  },
  {
    "text": "And what would you plug in? So q is fixed. q is just this\nproduct of Gaussians,",
    "start": "2270820",
    "end": "2276830"
  },
  {
    "text": "which is this process of\nadding noise at every step. And p theta is the\ninteresting bit",
    "start": "2276830",
    "end": "2282020"
  },
  {
    "text": "is this distribution\nthat you get by starting from some simple\nprior like pure Gaussian noise",
    "start": "2282020",
    "end": "2288470"
  },
  {
    "text": "and then passing it through\nthe sequence of neural networks that will try to infer the\nparameters from other Gaussian",
    "start": "2288470",
    "end": "2296720"
  },
  {
    "text": "that you sample from to go\nthrough this sampling procedure. So that's how this joint in\nthe numerator is defined.",
    "start": "2296720",
    "end": "2303930"
  },
  {
    "text": "It's defined in terms of this\np theta xt minus 1 given xt.",
    "start": "2303930",
    "end": "2310319"
  },
  {
    "text": " So it's just like--",
    "start": "2310320",
    "end": "2317609"
  },
  {
    "text": "you can optimize this loss\nas a function of theta. And so it's actually a little\nbit simpler than the usual VAE.",
    "start": "2317610",
    "end": "2323490"
  },
  {
    "text": "In the usual VAE, q\nitself is learnable. Remember you have\nthose five parameters.",
    "start": "2323490",
    "end": "2330270"
  },
  {
    "text": "Here q is fixed, so you don't\nhave to actually optimize it. So it's just like a VAE, except\nthat the encoder is fixed.",
    "start": "2330270",
    "end": "2338840"
  },
  {
    "text": "It's not learnable. That's the usual ELBO objective. And recall that these\ndecoders are parameterized,",
    "start": "2338840",
    "end": "2345860"
  },
  {
    "text": "are all Gaussians, and\nthey have this simple form. To sample xt minus\n1, you would take xt,",
    "start": "2345860",
    "end": "2352610"
  },
  {
    "text": "you pass it through some\nneural network to get the mean, and then they will\nhave fixed variance. ",
    "start": "2352610",
    "end": "2360010"
  },
  {
    "text": "And the interesting thing\nis that if you parameterize",
    "start": "2360010",
    "end": "2366730"
  },
  {
    "text": "these neural networks\nthat will give you the means of these Gaussians\nusing this form, which is again,",
    "start": "2366730",
    "end": "2375530"
  },
  {
    "text": "it depends on these\nbetas and these alphas. It's not super important. But if you parameterize the\nnetwork in terms of an epsilon",
    "start": "2375530",
    "end": "2383240"
  },
  {
    "text": "network that takes\nxt as an input and then tries to predict the\nnoise that was added to xt",
    "start": "2383240",
    "end": "2390590"
  },
  {
    "text": "and subtracts this\nestimated noise from xt",
    "start": "2390590",
    "end": "2396560"
  },
  {
    "text": "to get a guess of what\nxt minus 1 should be,",
    "start": "2396560",
    "end": "2402750"
  },
  {
    "text": "then you can show that this\nELBO objective is actually equivalent to the usual\ndenoising score-matching loss.",
    "start": "2402750",
    "end": "2411980"
  },
  {
    "text": "So minimizing the negative ELBO\nor maximizing the lower bound",
    "start": "2411980",
    "end": "2418430"
  },
  {
    "text": "on the average log-likelihood\nis exactly the same",
    "start": "2418430",
    "end": "2423930"
  },
  {
    "text": "as trying to estimate the scores\nof these noise-perturbed data distributions.",
    "start": "2423930",
    "end": "2430650"
  },
  {
    "text": "And so if you work to-- yeah, basically, if you\nparameterize these mean networks",
    "start": "2430650",
    "end": "2438340"
  },
  {
    "text": "by saying, OK, take the\ndata point that you have and subtract something to\nmake it look more realistic,",
    "start": "2438340",
    "end": "2444570"
  },
  {
    "text": "this network essentially\nends up trying to estimate the score of\nthis noise-perturbed data",
    "start": "2444570",
    "end": "2450780"
  },
  {
    "text": "distributions. And so although we\nderived everything",
    "start": "2450780",
    "end": "2457220"
  },
  {
    "text": "from the perspective of a\nvariational autoencoder, it turns out that what\nyou're actually doing",
    "start": "2457220",
    "end": "2463849"
  },
  {
    "text": "is estimating scores. So the score-based model\nwould sample differently. So here I'm just claiming that\nthe loss would be the same up",
    "start": "2463850",
    "end": "2474340"
  },
  {
    "text": "to some, I guess, there\nare some scalings, but roughly if you\nlook at it, you're",
    "start": "2474340",
    "end": "2480140"
  },
  {
    "text": "basically starting from data,\nyou're sampling a noise vector, you're feeding the noisy\nimage to this network epsilon",
    "start": "2480140",
    "end": "2486829"
  },
  {
    "text": "and you're trying to estimate\nthe noise vector that was added to the data. And so the training\nobjective if you",
    "start": "2486830",
    "end": "2492890"
  },
  {
    "text": "choose this kind of\nparameterization are the same. Then you have different\nchoices in terms",
    "start": "2492890",
    "end": "2499020"
  },
  {
    "text": "of how you sample from it. In a score-based model, you\nwould sample by doing Langevin. Here, you are not\nsampling using Langevin,",
    "start": "2499020",
    "end": "2505620"
  },
  {
    "text": "you would sample based on just\ngoing through the decoding process.",
    "start": "2505620",
    "end": "2511420"
  },
  {
    "text": "So this is the\ntraining procedure of DDPM or the denoising\ndiffusion probabilistic model.",
    "start": "2511420",
    "end": "2517450"
  },
  {
    "text": "And if you look at\nthe loss, the loss is basically denoising\nscore matching.",
    "start": "2517450",
    "end": "2522750"
  },
  {
    "text": "If you look at the loss\nthat you have here, that's the same as the\ndenoising score-matching loss",
    "start": "2522750",
    "end": "2528510"
  },
  {
    "text": "that we had before. The sampling is also\nvery similar, actually,",
    "start": "2528510",
    "end": "2534660"
  },
  {
    "text": "to what you would do\nin a score-based model. If you look at the\nway you would generate samples is you start\nfrom pure noise, same as",
    "start": "2534660",
    "end": "2542010"
  },
  {
    "text": "score-based models. And then at every\nstep, you basically",
    "start": "2542010",
    "end": "2547980"
  },
  {
    "text": "follow the gradient,\nwhich is epsilon theta, and add a little bit\nof noise because that's",
    "start": "2547980",
    "end": "2554016"
  },
  {
    "text": "what you're supposed\nto do if you were to sample from a Gaussian\nthat is p of xt given or the xt minus 1 given xt.",
    "start": "2554017",
    "end": "2561450"
  },
  {
    "text": "And so the sampling\nprocedure that you get by iteratively sampling\nfrom these denoisers",
    "start": "2561450",
    "end": "2566820"
  },
  {
    "text": "actually is very similar\nto the Langevin dynamics. It's different scalings\nbasically of the amount of--",
    "start": "2566820",
    "end": "2577590"
  },
  {
    "text": "this is like follow\nthe gradient. You take a step in the gradient\ndirection and then add noise,",
    "start": "2577590",
    "end": "2582990"
  },
  {
    "text": "you basically just do different\namounts of noise, essentially. But it's roughly\nthe same procedure.",
    "start": "2582990",
    "end": "2588240"
  },
  {
    "text": "Usually, you would learn the\nencoder and the decoder together and they would try to help\neach other out in some sense.",
    "start": "2588240",
    "end": "2594360"
  },
  {
    "text": "The encoder is trying\nto find structure and the decoder is trying\nto leverage the structure to generate data\nmore efficiently.",
    "start": "2594360",
    "end": "2601510"
  },
  {
    "text": "Here the encoder is fixed\nand it's just adding noise. And so the decoder is just\ntrying its best basically",
    "start": "2601510",
    "end": "2608520"
  },
  {
    "text": "at minimizing--",
    "start": "2608520",
    "end": "2613740"
  },
  {
    "text": "they killed divergences\nthat you would have or basically maximizing\nthe ELBO which is the same as inverting\nthe generative process.",
    "start": "2613740",
    "end": "2621869"
  },
  {
    "text": "And it turns out that\nin order to do that, you need to estimate\nthe scores to the extent",
    "start": "2621870",
    "end": "2630760"
  },
  {
    "text": "that you can do\nthat process well, then you would be able\nto generate good samples. ",
    "start": "2630760",
    "end": "2639359"
  },
  {
    "text": "It seems like you would have an\neasier time by actually allowing yourself to learn\nthe encoder as well,",
    "start": "2639360",
    "end": "2647530"
  },
  {
    "text": "but that doesn't actually\nwork in practice. So it would give you\nbetter ELBOs but a worse",
    "start": "2647530",
    "end": "2654000"
  },
  {
    "text": "kind of sample quality. You can also do one\nstep of Langevin. In practice, that's\nwhat you would do. So at that point,\nwhat's the difference?",
    "start": "2654000",
    "end": "2660690"
  },
  {
    "text": "I mean, they are\nessentially the same thing. I think one advantage of this\nscore-based model perspective",
    "start": "2660690",
    "end": "2666869"
  },
  {
    "text": "is that you can\nactually think of it as in the limit of infinite\nnumber of noise levels",
    "start": "2666870",
    "end": "2672510"
  },
  {
    "text": "as we'll see, which\nis not something you would be a little bit\ntrickier to get with the VAE",
    "start": "2672510",
    "end": "2677609"
  },
  {
    "text": "perspective. But to some extent, they are\nessentially the same thing.",
    "start": "2677610",
    "end": "2683494"
  },
  {
    "text": "From the ELBO\nperspective, you're going to get better numbers\nif you learn the encoder. But then I don't know if\nit's an optimization issue,",
    "start": "2683495",
    "end": "2691380"
  },
  {
    "text": "but then in terms of\nthe sample quality you're going to get blurry\nsamples like in a VAE.",
    "start": "2691380",
    "end": "2697780"
  },
  {
    "text": "Well, there are some intuitions\nrelated to progressive coding, and in practice, people don't\nactually optimize the ELBO.",
    "start": "2697780",
    "end": "2704770"
  },
  {
    "text": "In practice, people optimize\na scaled version of the ELBO. So the ELBO-- do I have it?",
    "start": "2704770",
    "end": "2713270"
  },
  {
    "text": "Yeah. So the ELBO basically\nlooks like this where you have this lambda\nt's that basically control how",
    "start": "2713270",
    "end": "2719650"
  },
  {
    "text": "much you care about the\ndifferent time steps. And in the limit of infinite\ncapacity, it doesn't matter.",
    "start": "2719650",
    "end": "2725830"
  },
  {
    "text": "But then in practice,\npeople would set them to be 0,\n1, which is not",
    "start": "2725830",
    "end": "2732400"
  },
  {
    "text": "what you should do if you\nwanted to optimize the ELBO. So it's a matter of\noptimizing likelihood.",
    "start": "2732400",
    "end": "2739359"
  },
  {
    "text": "It doesn't necessarily\ncorrelate with sample quality. So even if the encoder\nis fixed and it's just something really simple\nlike adding Gaussian noise,",
    "start": "2739360",
    "end": "2746260"
  },
  {
    "text": "the reverse is not. It requires you\nto have the score. So it's nontrivial to\nactually invert it.",
    "start": "2746260",
    "end": "2753370"
  },
  {
    "text": "But you could argue\nthat maybe if you were to destroy the structure\nin a more structured way, then",
    "start": "2753370",
    "end": "2760900"
  },
  {
    "text": "maybe it would be even easier to\ninvert the generative process. So they generalize to some\nextent, not out-of-distribution.",
    "start": "2760900",
    "end": "2768370"
  },
  {
    "text": "So if you train it\non images of cats, they're not going to\ngenerate images of dogs because they've never\nseen them and there's",
    "start": "2768370",
    "end": "2774550"
  },
  {
    "text": "no point for them\nto put probability mass on those kinds of--",
    "start": "2774550",
    "end": "2779810"
  },
  {
    "text": "so it's really based on the\nactual data distribution",
    "start": "2779810",
    "end": "2785560"
  },
  {
    "text": "that you're using for\ntraining the model. So when you add a lot of\nnoise the best way to denoise",
    "start": "2785560",
    "end": "2791350"
  },
  {
    "text": "is to basically predict the\naverage image in the data set. And so there you already\nsee that if you train it",
    "start": "2791350",
    "end": "2797950"
  },
  {
    "text": "on images of cats, what\nthe network will do when t is equal to capital\nT, it will basically",
    "start": "2797950",
    "end": "2803800"
  },
  {
    "text": "output the average image\nin the training set. And so it's going to\nbe completely off.",
    "start": "2803800",
    "end": "2808849"
  },
  {
    "text": "I think one of the main reasons\nis that if you think about it, the amount of compute that\nyou can put at generation time",
    "start": "2808850",
    "end": "2816400"
  },
  {
    "text": "is very large because you're\ngoing pass it through 1,000 VAEs, essentially.",
    "start": "2816400",
    "end": "2822820"
  },
  {
    "text": "Maybe t is usually-- capital T here is\n1,000, usually. So it's a very\ndeep stack of VAEs",
    "start": "2822820",
    "end": "2829329"
  },
  {
    "text": "that you can use\nat generation time. However, because of how things\nare set up, at training time,",
    "start": "2829330",
    "end": "2835640"
  },
  {
    "text": "you never have to actually look\nat this whole very deep, very expensive computation graph.",
    "start": "2835640",
    "end": "2840820"
  },
  {
    "text": "You can train it layer\nby layer incrementally without actually having to\nlook at the whole process.",
    "start": "2840820",
    "end": "2848033"
  },
  {
    "text": "So even though you\njust train it locally to just get a little bit\nbetter at every step, which is very efficient.",
    "start": "2848033",
    "end": "2853985"
  },
  {
    "text": "It's all breaking down. It's all breaking down,\nyeah, over level by level.",
    "start": "2853985",
    "end": "2859410"
  },
  {
    "text": "So the stack of VAEs would\nexactly give you this. ",
    "start": "2859410",
    "end": "2867680"
  },
  {
    "text": "The problem is that-- [COUGH]  --if the encoders are not\nstructured in a certain way,",
    "start": "2867680",
    "end": "2876080"
  },
  {
    "text": "you might not be able to\ndo this trick of basically jumping forward.",
    "start": "2876080",
    "end": "2881359"
  },
  {
    "text": "Remember that we had this-- oh, no, it's back a lot. This process here, it's very\neasy to go from 0 to xt.",
    "start": "2881360",
    "end": "2890780"
  },
  {
    "text": "If these q's are\narbitrary neural network there is no way for you to\njump from x0 to xt in one step.",
    "start": "2890780",
    "end": "2899220"
  },
  {
    "text": "And so the fact that\nthese q's are very simple and you can compose\nthem in closed form,",
    "start": "2899220",
    "end": "2905045"
  },
  {
    "text": "it allows you to get a very\nefficient training process. So not all\nhierarchical VAEs would",
    "start": "2905045",
    "end": "2911510"
  },
  {
    "text": "be very efficient\nto train, but this is a particular type\nof hierarchical VAEs,",
    "start": "2911510",
    "end": "2917640"
  },
  {
    "text": "so there are certainly some\nthat would be efficient. If you were to just train\nthe VAE the usual way",
    "start": "2917640",
    "end": "2923789"
  },
  {
    "text": "you would get a loss. If you go through the math, it\nends up being the same thing.",
    "start": "2923790",
    "end": "2928860"
  },
  {
    "text": "It seems\ncounterintuitive that you would want to fix the encoder to\nbe something strange like this where you just add\nnoise and you're not",
    "start": "2928860",
    "end": "2935670"
  },
  {
    "text": "reducing the dimensionality. But once you start\nmaking that choice,",
    "start": "2935670",
    "end": "2941530"
  },
  {
    "text": "then the loss ends\nup being the same as the denoising\nscore-matching loss.",
    "start": "2941530",
    "end": "2946869"
  },
  {
    "text": "Historically, we\ncame up with a score matching first and\nshowing that it works. And then people show,\nOK, you can take a VAE",
    "start": "2946870",
    "end": "2952633"
  },
  {
    "text": "and you can get something\nessentially identical and that saves you a\nlittle bit of trouble at inference time because\nyou no longer have",
    "start": "2952633",
    "end": "2959380"
  },
  {
    "text": "to do Langevin dynamics, you\ncan just sample from VAE. So the lambda basically\nis just something",
    "start": "2959380",
    "end": "2965380"
  },
  {
    "text": "that turns out to\nbe how much you care about the\ndifferent denoising",
    "start": "2965380",
    "end": "2974550"
  },
  {
    "text": "losses over different\nnoise intensities. And there is a principle--\nif you just do the math",
    "start": "2974550",
    "end": "2980610"
  },
  {
    "text": "and you go through\nthe ELBO, there's going to be certain\nvalue of lambda t that you should choose\nif you really care",
    "start": "2980610",
    "end": "2986070"
  },
  {
    "text": "about the evidence lower bound. In practice, people just\nchoose that to be 1.",
    "start": "2986070",
    "end": "2992700"
  },
  {
    "text": "So you're not actually\noptimizing an ELBO. Beta is the parameter, beta t's. The alphas are computed in terms\nof the beta t's that basically",
    "start": "2992700",
    "end": "3000170"
  },
  {
    "text": "controls how quickly you add\nnoise to the data, essentially.",
    "start": "3000170",
    "end": "3005720"
  },
  {
    "text": "And you can choose it. So at inference time, we\ndo start from random noise and then move it back\nto the clean data.",
    "start": "3005720",
    "end": "3014660"
  },
  {
    "text": "But it makes sense to\ndo it incrementally as opposed-- you could\nalso do it in one step.",
    "start": "3014660",
    "end": "3021030"
  },
  {
    "text": "You could imagine a\nVAE where the encoder is fixed, takes the image\nand adds a lot of noise.",
    "start": "3021030",
    "end": "3028530"
  },
  {
    "text": "Presumably that\ninverse distribution that you would have to\nlearn, which is this--",
    "start": "3028530",
    "end": "3034289"
  },
  {
    "text": "where do I have it? This procedure here\nthat tries to invert",
    "start": "3034290",
    "end": "3041220"
  },
  {
    "text": "going from noise to data is\ngoing to be very complicated. While if q of x given xt minus\n1 is like the same thing just",
    "start": "3041220",
    "end": "3050670"
  },
  {
    "text": "adding a little bit\nof noise, presumably inverting that is also\ngoing to be relatively easy.",
    "start": "3050670",
    "end": "3056200"
  },
  {
    "text": "So we're breaking down\nthis complicated problem going from noise to data into\n1,000 little subproblems where",
    "start": "3056200",
    "end": "3064230"
  },
  {
    "text": "all you have to do is to just\nremove a little bit of noise. And that ends up being\nbetter in practice",
    "start": "3064230",
    "end": "3072080"
  },
  {
    "text": "because the subproblems are\nmuch more tractable to learn. So what you do is you\nstart with a data point,",
    "start": "3072080",
    "end": "3078590"
  },
  {
    "text": "you randomly pick a t, which is\nlike an index in this sequence of noisy random\nvariables, then you",
    "start": "3078590",
    "end": "3084740"
  },
  {
    "text": "start with just the\nstandard Gaussian noise, and then you generate an\nxt by basically adding",
    "start": "3084740",
    "end": "3091877"
  },
  {
    "text": "the right amount of noise. So it's the same as\ndenoising score matching. So this argument that you\nfeed into epsilon theta",
    "start": "3091877",
    "end": "3100710"
  },
  {
    "text": "is just a sample from\nq of x t given x0. So the architecture\nis the same as a noise",
    "start": "3100710",
    "end": "3108210"
  },
  {
    "text": "conditional score model. So you have the same\nproblem that you need to learn a bunch of\ndecoders, one for every t.",
    "start": "3108210",
    "end": "3115069"
  },
  {
    "text": "And instead of learning\n1,000 different decoders, you'll learn one\nthat is amortized",
    "start": "3115070",
    "end": "3122119"
  },
  {
    "text": "across the different keys. So you have the same like this\nepsilon theta network that",
    "start": "3122120",
    "end": "3129440"
  },
  {
    "text": "is trying to predict noise. Basically, it takes the\nimage, xt, the noisy image.",
    "start": "3129440",
    "end": "3135200"
  },
  {
    "text": "It takes t, which\nis encoded somehow, and those are both inputs\nto the network that then",
    "start": "3135200",
    "end": "3141950"
  },
  {
    "text": "are used to predict the noise. So that's the same as the\nnoise conditional score network where you take xt.",
    "start": "3141950",
    "end": "3147470"
  },
  {
    "text": "You take sigma or t and then\nyou use it to predict the noise.",
    "start": "3147470",
    "end": "3153020"
  },
  {
    "text": "And so yeah, the architecture\nwould basically be the-- usually some kind of\nU-Net because we're",
    "start": "3153020",
    "end": "3159140"
  },
  {
    "text": "doing some kind of dense\nimage prediction task where we go from image to image. And U-Net-type architectures\ntend to be pretty good at this",
    "start": "3159140",
    "end": "3165920"
  },
  {
    "text": "and this is still-- people are using\nTransformers too, but this is still one\nof the best-performing",
    "start": "3165920",
    "end": "3171140"
  },
  {
    "text": "models for learning denoising. So we have a paper that you\ncan look up and try to get",
    "start": "3171140",
    "end": "3178430"
  },
  {
    "text": "a hierarchical VAE to perform--\nbecause one problem with the hierarchical VAE is that\nbasically, there is a lot of--",
    "start": "3178430",
    "end": "3185630"
  },
  {
    "text": "if you start learning the\nencoder, it's not identifiable. There is many\ndifferent ways to--",
    "start": "3185630",
    "end": "3192839"
  },
  {
    "text": "you basically want to\nencode the information about the input across all\nthese latent variables.",
    "start": "3192840",
    "end": "3198585"
  },
  {
    "text": "But there are many\ndifferent ways to do it like which bits of\ninformation you store where",
    "start": "3198585",
    "end": "3205359"
  },
  {
    "text": "in the latent\nvariables is very much up to the encoder and\nthe decoder figure out.",
    "start": "3205360",
    "end": "3212420"
  },
  {
    "text": "It turns out that if you use\na certain kind of encoding strategy that is forcing\nthe model to spread out",
    "start": "3212420",
    "end": "3217760"
  },
  {
    "text": "the bits in a certain way,\nthen you can get pretty close to the performance of--",
    "start": "3217760",
    "end": "3223625"
  },
  {
    "text": "with a learned\nencoder you can get pretty close to the performance\nof these kinds of models.",
    "start": "3223625",
    "end": "3228710"
  },
  {
    "text": "But yeah, it's still not\nentirely clear how to do it. There are multiple papers on\ntrying to figure out good noise",
    "start": "3228710",
    "end": "3234800"
  },
  {
    "text": "schedules. There is many different choices. What's important is\nthat at the end--",
    "start": "3234800",
    "end": "3242090"
  },
  {
    "text": "the only real constraint is\nthat at the end, you basically-- the signal-to-noise ratio\nis zero, essentially.",
    "start": "3242090",
    "end": "3250730"
  },
  {
    "text": "So you destroy all\nthe information. Then there is a\nlot of flexibility in terms of how much noise\nyou add at different steps",
    "start": "3250730",
    "end": "3258470"
  },
  {
    "text": "such that you would\nget your that result. You can even try to optimize it.",
    "start": "3258470",
    "end": "3266090"
  },
  {
    "text": "If you take the\nELBO perspective, you could just maybe\nlearn a simple function",
    "start": "3266090",
    "end": "3271130"
  },
  {
    "text": "that kind of controls\nhow much noise you add at different steps. So you still just\nadd a Gaussian noise but you can try\nto learn how to--",
    "start": "3271130",
    "end": "3278420"
  },
  {
    "text": "so there are ways to\neven learn this schedule. I would go in the direction of\nmaybe let's learn the encoder",
    "start": "3278420",
    "end": "3286940"
  },
  {
    "text": "or let's make it more\nflexible in practice. I mean, there are\na number of papers",
    "start": "3286940",
    "end": "3292005"
  },
  {
    "text": "where people have tried\ndifferent strategies, different ways of destroying\nstructure called diffusion.",
    "start": "3292005",
    "end": "3297380"
  },
  {
    "text": "They've shown some\nempirical success. But in practice, people\nstill mostly use Gaussians.",
    "start": "3297380",
    "end": "3303650"
  },
  {
    "text": "What we can do now is start\nthinking about what happens. We have this diffusion model\nperspective, hierarchical VAE",
    "start": "3303650",
    "end": "3311599"
  },
  {
    "text": "perspective where we have clean\ndata, and then we have 1,000, let's say, different versions of\nthe data distribution, perturb",
    "start": "3311600",
    "end": "3319670"
  },
  {
    "text": "the increasingly large\namounts of noise. Really if you think about it in\nterms of a diffusion process,",
    "start": "3319670",
    "end": "3328369"
  },
  {
    "text": "a diffusion process is a\ncontinuous time process. If you think about how heat\ndiffuses where some metal bar,",
    "start": "3328370",
    "end": "3339190"
  },
  {
    "text": "that process is not happening\nat discrete time intervals. It's really more\nnaturally thought",
    "start": "3339190",
    "end": "3346540"
  },
  {
    "text": "as something happening\nover continuous time, where time is continuous.",
    "start": "3346540",
    "end": "3352940"
  },
  {
    "text": "Or another way to\nthink about it is you can imagine making this\ndiscretization finer and finer.",
    "start": "3352940",
    "end": "3360230"
  },
  {
    "text": "We're still going to take the\nhierarchical VAE perspective, but you can start\nthinking about what happens if we were to\ntake more and more steps.",
    "start": "3360230",
    "end": "3368000"
  },
  {
    "text": "If we go from\n1,000, 2000, 4,000, we make these steps smaller and\nsmaller and smaller and smaller",
    "start": "3368000",
    "end": "3375710"
  },
  {
    "text": "until eventually, we get this\ncontinuum of distributions,",
    "start": "3375710",
    "end": "3380780"
  },
  {
    "text": "which really correspond\nto the diffusion process. And so, as usual, on\nthe left-hand side,",
    "start": "3380780",
    "end": "3387869"
  },
  {
    "text": "we have the clean\ndata distribution, which is this mixture\nof two Gaussians where there's these two spots where\nmost of the probability mass is.",
    "start": "3387870",
    "end": "3396760"
  },
  {
    "text": "And then there is this\ncontinuous time diffusion process happening here\nthat is spreading out the probability mass over\ntime until at the end.",
    "start": "3396760",
    "end": "3407830"
  },
  {
    "text": "On the right-hand side, you\nget this pure noise kind of distribution.",
    "start": "3407830",
    "end": "3413003"
  },
  {
    "text": "So literally what's\nhappening here is we're thinking about\na very, very, very fine-grained discretization\nor a lot of different steps",
    "start": "3413003",
    "end": "3426780"
  },
  {
    "text": "over which we go from\ndata to pure noise. So if you were to destroy the\nstructure a very little bit",
    "start": "3426780",
    "end": "3434579"
  },
  {
    "text": "at a time, you can imagine\nin the limit you get",
    "start": "3434580",
    "end": "3439710"
  },
  {
    "text": "a process that is continuous.  So instead of having 1,000\ndifferent distributions,",
    "start": "3439710",
    "end": "3449330"
  },
  {
    "text": "we have an infinite\nnumber of distributions that are now indexed\nby t, where t is now a time variable going from 0\nto capital T just like before.",
    "start": "3449330",
    "end": "3457970"
  },
  {
    "text": "But instead of taking 1,000\ndiscrete different values, it takes an infinite\nnumber of values.",
    "start": "3457970",
    "end": "3463740"
  },
  {
    "text": "So it's a continuous variable\ngoing from 0 to capital T.",
    "start": "3463740",
    "end": "3472630"
  },
  {
    "text": "So we have as usual\ndata on the one hand and then pure noise\non the other extreme.",
    "start": "3472630",
    "end": "3478045"
  },
  {
    "text": " So how do we now\ndescribe the relationship",
    "start": "3478045",
    "end": "3485075"
  },
  {
    "text": "between all these\nrandom variables that are now indexed by time?",
    "start": "3485075",
    "end": "3490119"
  },
  {
    "text": "We can describe it in terms\nof a stochastic process. So there is basically a\ncollection of random variables.",
    "start": "3490120",
    "end": "3497840"
  },
  {
    "text": "And there is an infinite\nnumber of random variables now. In the VAE case, we had 1,000\ndifferent random variables,",
    "start": "3497840",
    "end": "3503710"
  },
  {
    "text": "now we have an infinite number\nof random variables, xt. And all these random\nvariables have densities",
    "start": "3503710",
    "end": "3511380"
  },
  {
    "text": "that again are indexed by time. And instead of describing\nthat relationship using",
    "start": "3511380",
    "end": "3517410"
  },
  {
    "text": "these encoders, we\ncan describe how they are related to each\nother through a stochastic",
    "start": "3517410",
    "end": "3523500"
  },
  {
    "text": "differential equation,\nwhich is basically the way you would\ndescribe how the values of these random\nvariables that are now",
    "start": "3523500",
    "end": "3530060"
  },
  {
    "text": "indexed by a continuous\ntime variable t are related to each other.",
    "start": "3530060",
    "end": "3535350"
  },
  {
    "text": "And so what you're saying\nis that over a small time interval dt dx, x\nby an amount which",
    "start": "3535350",
    "end": "3545430"
  },
  {
    "text": "is determined by some\ndeterministic drift and a small amount of noise that\nyou basically add at every step.",
    "start": "3545430",
    "end": "3552720"
  },
  {
    "text": "Not super important\nwhat that formula means, but without\nloss of generality,",
    "start": "3552720",
    "end": "3558310"
  },
  {
    "text": "you can think about a very\nsimple stochastic differential equation that describes\na diffusion process where",
    "start": "3558310",
    "end": "3564359"
  },
  {
    "text": "all that's happening is that\nover a small time increment",
    "start": "3564360",
    "end": "3570670"
  },
  {
    "text": "dt, what you do is you\nchange the value of x by adding an infinitesimally\nsmall amount of noise,",
    "start": "3570670",
    "end": "3576870"
  },
  {
    "text": "essentially. And that is basically how\nyou describe the encoder",
    "start": "3576870",
    "end": "3584270"
  },
  {
    "text": "or how all these\nrandom variables are related to each other through\nthis essentially diffusion",
    "start": "3584270",
    "end": "3589579"
  },
  {
    "text": "process. Now what's interesting is\nthat just like before, we can think about\nthe reverse process",
    "start": "3589580",
    "end": "3595579"
  },
  {
    "text": "of going from noise to data. ",
    "start": "3595580",
    "end": "3600760"
  },
  {
    "text": "And the random\nvariables are the same. We're not changing them. But it turns out that they\ncan be described equivalently",
    "start": "3600760",
    "end": "3609198"
  },
  {
    "text": "through a different\nstochastic differential equation where time goes from\nlarge to small, from capital T",
    "start": "3609198",
    "end": "3617080"
  },
  {
    "text": "to 0. And what's interesting is that\nthis stochastic differential",
    "start": "3617080",
    "end": "3624400"
  },
  {
    "text": "equation has a\nclosed-form solution.",
    "start": "3624400",
    "end": "3629750"
  },
  {
    "text": "And again not super important\nwhat the formula is, but the only thing\nthat you need to know",
    "start": "3629750",
    "end": "3636790"
  },
  {
    "text": "to be able to characterize this\nstochastic differential equation is the score function.",
    "start": "3636790",
    "end": "3642200"
  },
  {
    "text": "So just like in the discrete\ncase, in the VAE case, if you knew the score, then\nyou would get optimal decoders",
    "start": "3642200",
    "end": "3650230"
  },
  {
    "text": "and you would be able to\nreverse the generation process. In continuous time if you\nhave all the score functions,",
    "start": "3650230",
    "end": "3658030"
  },
  {
    "text": "you can reverse the generative\nprocess and go from pure noise to data.",
    "start": "3658030",
    "end": "3663400"
  },
  {
    "text": "So it's closed form\nup to the score function, which is unknown. So maybe a little bit of a--",
    "start": "3663400",
    "end": "3669579"
  },
  {
    "text": "but there is-- basically,\nthis is the equation. This exactly inverts the\noriginal stochastic differential",
    "start": "3669580",
    "end": "3676720"
  },
  {
    "text": "equation if you know the score\nfunction, which you don't. So you are right that\nwe don't know it. But if you knew\nit then you would",
    "start": "3676720",
    "end": "3682600"
  },
  {
    "text": "be able to exactly\ninvert the process. So the stochasticity\nis basically this dwt.",
    "start": "3682600",
    "end": "3689320"
  },
  {
    "text": "Basically, at every\ninfinitesimal step, you add a little bit of noise.",
    "start": "3689320",
    "end": "3694930"
  },
  {
    "text": "And in the reverse process,\nyou're also doing it. So in that sense, it's a\nstochastic differential",
    "start": "3694930",
    "end": "3701170"
  },
  {
    "text": "equation. So if that term was 0, so if\nyou didn't have that here,",
    "start": "3701170",
    "end": "3707730"
  },
  {
    "text": "then it would be an ordinary\ndifferential equation where the evolution is deterministic.",
    "start": "3707730",
    "end": "3713810"
  },
  {
    "text": "So given the initial\ncondition, if this gt here was 0 or this piece\ndoesn't exist,",
    "start": "3713810",
    "end": "3720450"
  },
  {
    "text": "then you would have just a\nregular ordinary differential equation. Given the initial condition,\nyou can integrate it",
    "start": "3720450",
    "end": "3726440"
  },
  {
    "text": "and you would get a solution. This one is a little\nbit more complicated because at every step you\nadd a little bit of noise.",
    "start": "3726440",
    "end": "3732690"
  },
  {
    "text": "And so that's why you\nhave these paths here",
    "start": "3732690",
    "end": "3738079"
  },
  {
    "text": "that are a little bit-- see all these little\njags in the curve? That's because there is\na little bit of noise",
    "start": "3738080",
    "end": "3743690"
  },
  {
    "text": "that is added at every step. If you want to do things in\ncontinuous time, what we can do",
    "start": "3743690",
    "end": "3748730"
  },
  {
    "text": "is we can try to learn a\nmodel of all these score functions, which is\njust like before is",
    "start": "3748730",
    "end": "3754640"
  },
  {
    "text": "going to be a neural network. It takes as input\nx and t and tries to estimate the score of the\nnoise-perturbed data density",
    "start": "3754640",
    "end": "3761839"
  },
  {
    "text": "at time t evaluated at x. So this is just like the\ncontinuous-time version",
    "start": "3761840",
    "end": "3767140"
  },
  {
    "text": "of what we had before. Before we were doing this for\n1,000 different t's, now we",
    "start": "3767140",
    "end": "3773160"
  },
  {
    "text": "do it for every t between\n0 and capital T, where t is a real-valued variable.",
    "start": "3773160",
    "end": "3781230"
  },
  {
    "text": "We estimate it again during\ndenoising score matching. So it's the usual thing\nwhere we estimate scores",
    "start": "3781230",
    "end": "3789510"
  },
  {
    "text": "of the noise-perturbed\ndata density, we can do denoising\nscore matching. And the solution to\nthat regression problem",
    "start": "3789510",
    "end": "3797040"
  },
  {
    "text": "is basically a\ndenoising objective. And then what we\ncan do is to sample.",
    "start": "3797040",
    "end": "3803579"
  },
  {
    "text": "Instead of using\nthe decoders and go through 1,000 steps\nof the decoders, we can actually\njust try to solve",
    "start": "3803580",
    "end": "3810780"
  },
  {
    "text": "numerically the reverse time\nstochastic differential equation where we plug in our estimate\nof the score for the true score",
    "start": "3810780",
    "end": "3820320"
  },
  {
    "text": "function. So here we have the exact\nstochastic differential equation. OK, sorry.",
    "start": "3820320",
    "end": "3825870"
  },
  {
    "text": "This doesn't show\nright, but let's see-- yeah.",
    "start": "3825870",
    "end": "3832680"
  },
  {
    "text": "So we had this\ndifferential equation which involves the\ntrue score, and now we are approximating that\nwith our score model.",
    "start": "3832680",
    "end": "3842010"
  },
  {
    "text": "And then what we can\ndo is we can try to-- in practice, we can solve\nthis in continuous time.",
    "start": "3842010",
    "end": "3847820"
  },
  {
    "text": "In practice, you will\nstill have to discretize it by taking small steps.",
    "start": "3847820",
    "end": "3853120"
  },
  {
    "text": "And there are numerical\nsolvers that you can use to solve a stochastic\ndifferential equation,",
    "start": "3853120",
    "end": "3858790"
  },
  {
    "text": "and they all have the\nsame flavor of basically, you update your x by\nfollowing the score",
    "start": "3858790",
    "end": "3865000"
  },
  {
    "text": "and then adding a little\nbit of noise at every step. If you were to take\n1,000 different steps",
    "start": "3865000",
    "end": "3870550"
  },
  {
    "text": "and you would essentially\ndo that kind of machinery of using the decoders,\nthat basically",
    "start": "3870550",
    "end": "3879850"
  },
  {
    "text": "corresponds to a\nparticular way of solving this stochastic\ndifferential equation, which is just this kind of\ndiscretization, Euler-Maruyama",
    "start": "3879850",
    "end": "3888099"
  },
  {
    "text": "discretization. What a score-based\nmodel would do instead",
    "start": "3888100",
    "end": "3895030"
  },
  {
    "text": "is it would attempt to correct.",
    "start": "3895030",
    "end": "3900040"
  },
  {
    "text": "Because there are\nnumerical errors you're going to\nmake some mistakes, and so what a score-based\nmodel would do",
    "start": "3900040",
    "end": "3908500"
  },
  {
    "text": "is it would try to basically fix\nthe mistakes by running Langevin for that time step.",
    "start": "3908500",
    "end": "3914090"
  },
  {
    "text": "So you can combine just regular\nsampling from a diffusion model",
    "start": "3914090",
    "end": "3921760"
  },
  {
    "text": "where you would take 1,000\ndifferent steps or even less with the MCMC style\nsampling to correct",
    "start": "3921760",
    "end": "3928390"
  },
  {
    "text": "the mistakes of a basic\nnumerical SDE solver.",
    "start": "3928390",
    "end": "3934720"
  },
  {
    "text": "It's normally distributed. So it's a normally\ndistributed condition on the initial condition but the\nmarginals are far from normal.",
    "start": "3934720",
    "end": "3943840"
  },
  {
    "text": "The transitions are. Yes. That's key. Yeah, because then that's why\nwe can simulate it forward very efficiently.",
    "start": "3943840",
    "end": "3949790"
  },
  {
    "text": "But marginally, they are not. DDPM is just this.",
    "start": "3949790",
    "end": "3955839"
  },
  {
    "text": "It's like a particular type of\ndiscretization of the underlying SDE.",
    "start": "3955840",
    "end": "3961990"
  },
  {
    "text": "Score-based models would\nattempt to solve this SDE in a slightly different way.",
    "start": "3961990",
    "end": "3967310"
  },
  {
    "text": "There is basically\ntwo types of solvers-- predictor solvers,\ncorrector solvers. Basically, a score-based models,\nwhich is MCMC or Langevin",
    "start": "3967310",
    "end": "3974640"
  },
  {
    "text": "dynamics. It's something called\na corrector method for SDE solving. So it's just a different way\nof solving the same underlying",
    "start": "3974640",
    "end": "3983020"
  },
  {
    "text": "stochastic\ndifferential equation. So DDPM is just predictor,\nscore-based model is just corrector.",
    "start": "3983020",
    "end": "3988960"
  },
  {
    "text": "You can combine them and just\nget a more accurate solver for the underlying SDE.",
    "start": "3988960",
    "end": "3995530"
  },
  {
    "text": "DDIM is a different beast. DDIM works by basically\nconverting the--",
    "start": "3995530",
    "end": "4002910"
  },
  {
    "text": "let me skip this, but basically\nit converts the SDE into an ODE.",
    "start": "4002910",
    "end": "4009549"
  },
  {
    "text": "So I guess we're out of time. But again, it\nturns out that it's",
    "start": "4009550",
    "end": "4014560"
  },
  {
    "text": "possible to define an ordinary\ndifferential equation that",
    "start": "4014560",
    "end": "4021430"
  },
  {
    "text": "has the same\nmarginals, whatever it is, as the original stochastic\ndifferential equation",
    "start": "4021430",
    "end": "4027180"
  },
  {
    "text": "that we started from. So now the evolution is\nentirely deterministic. There is no noise\nadded at every step.",
    "start": "4027180",
    "end": "4033700"
  },
  {
    "text": "So you see how these white\ntrajectories, they are very-- there is no noise\nadded at every step.",
    "start": "4033700",
    "end": "4040200"
  },
  {
    "text": "They are straight. But marginally, they define\nexactly the same density.",
    "start": "4040200",
    "end": "4045760"
  },
  {
    "text": "So the probability that\nyou see across time are the same, whether you\nrun this simple diffusion,",
    "start": "4045760",
    "end": "4052470"
  },
  {
    "text": "kind of Brownian\nmotion kind of thing, or you do this\ndeterministic, you",
    "start": "4052470",
    "end": "4058020"
  },
  {
    "text": "follow this deterministic\npaths, the marginals that you see how frequently do\nyou see these trajectories going",
    "start": "4058020",
    "end": "4063930"
  },
  {
    "text": "through different parts of the\nspace are exactly the same. So there are two advantages.",
    "start": "4063930",
    "end": "4070230"
  },
  {
    "text": "And again, it still depends\non the score function. One advantage is that-- as you said, you can\nbe more efficient.",
    "start": "4070230",
    "end": "4076810"
  },
  {
    "text": "The other advantage is that now\nit's a deterministic invertible",
    "start": "4076810",
    "end": "4082320"
  },
  {
    "text": "mapping. So now it's a flow model. So now we've converted\na VAE into a flow model.",
    "start": "4082320",
    "end": "4090200"
  },
  {
    "text": "Basically, what's\nhappening is that if you recall you can think of\nthese random variables",
    "start": "4090200",
    "end": "4095600"
  },
  {
    "text": "here as latent variables in\nsome kind of generative model.",
    "start": "4095600",
    "end": "4101009"
  },
  {
    "text": "And in the VAE\nperspective, we're inferring these latent variables\nby stochastically simulating",
    "start": "4101010",
    "end": "4108299"
  },
  {
    "text": "this diffusion process. But if you solve the ODE now\nyou are inferring the latent",
    "start": "4108300",
    "end": "4114810"
  },
  {
    "text": "variables deterministically. And because you ODEs\nhave unique solutions, the mapping is invertible,\nand so you can also",
    "start": "4114810",
    "end": "4122460"
  },
  {
    "text": "convert basically this model. Once you have the score,\nyou can convert it",
    "start": "4122460",
    "end": "4128130"
  },
  {
    "text": "into a flow model\nthat has exactly the same marginal\ndensities over time.",
    "start": "4128130",
    "end": "4136049"
  },
  {
    "text": "And one advantage\nof a flow model is now you can compute\nthe likelihoods exactly. So now you can use something\nsimilar to the change",
    "start": "4136050",
    "end": "4143250"
  },
  {
    "text": "of variable formula\nto actually compute exactly what is the\nprobability of landing",
    "start": "4143250",
    "end": "4149790"
  },
  {
    "text": "at any particular point. You can just solve\nthe ODE, which is the same as\ninverting the flow,",
    "start": "4149790",
    "end": "4156449"
  },
  {
    "text": "and then compute the\nprobability under the prior, and then you do change\nof variable formula, and you can get\nexact likelihoods.",
    "start": "4156450",
    "end": "4164210"
  },
  {
    "text": "And so by converting\na VAE into a flow, you also get exact\nlikelihood evaluation.",
    "start": "4164210",
    "end": "4172240"
  },
  {
    "start": "4172240",
    "end": "4176000"
  }
]