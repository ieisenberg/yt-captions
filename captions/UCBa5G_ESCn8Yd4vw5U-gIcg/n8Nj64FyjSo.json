[
  {
    "start": "0",
    "end": "34000"
  },
  {
    "text": "OK, so the final topic\nin this add-on section",
    "start": "0",
    "end": "4070"
  },
  {
    "text": "is generalized linear models.",
    "start": "4070",
    "end": "5790"
  },
  {
    "text": "So we've seen that\nlinear regression is used",
    "start": "5790",
    "end": "8600"
  },
  {
    "text": "for a quantitative response.",
    "start": "8600",
    "end": "10940"
  },
  {
    "text": "Logistic regression\nis the counterpart",
    "start": "10940",
    "end": "13099"
  },
  {
    "text": "for a binary response and models\nthe logit of the probability",
    "start": "13100",
    "end": "16700"
  },
  {
    "text": "as a linear model.",
    "start": "16700",
    "end": "18710"
  },
  {
    "text": "So these are two response\ntypes, but others exist,",
    "start": "18710",
    "end": "21750"
  },
  {
    "text": "such as non-negative responses,\nskewed distributions and more.",
    "start": "21750",
    "end": "26690"
  },
  {
    "text": "So generalized linear models\nprovide a unified framework",
    "start": "26690",
    "end": "30440"
  },
  {
    "text": "for dealing with many\ndifferent response types.",
    "start": "30440",
    "end": "33030"
  },
  {
    "text": "And so that's what we're\ngoing to talk about here.",
    "start": "33030",
    "end": "35309"
  },
  {
    "start": "34000",
    "end": "143000"
  },
  {
    "text": "We're going to go through\na simple example, which",
    "start": "35310",
    "end": "37970"
  },
  {
    "text": "is going to illustrate an\nimportant member of this family,",
    "start": "37970",
    "end": "41520"
  },
  {
    "text": "which uses Poisson regression.",
    "start": "41520",
    "end": "43580"
  },
  {
    "text": "And for that, we're going to\nuse the bikeshare data, which",
    "start": "43580",
    "end": "48230"
  },
  {
    "text": "is a data set that measures\nsome variables for bike",
    "start": "48230",
    "end": "51739"
  },
  {
    "text": "rental company in\nWashington, DC.",
    "start": "51740",
    "end": "55070"
  },
  {
    "text": "So, in this case,\nthe response is",
    "start": "55070",
    "end": "57110"
  },
  {
    "text": "bikers, which is the\nnumber of hourly users",
    "start": "57110",
    "end": "59510"
  },
  {
    "text": "in the bikeshare program\nin Washington DC.",
    "start": "59510",
    "end": "63210"
  },
  {
    "text": "You've got a bunch of variables.",
    "start": "63210",
    "end": "65018"
  },
  {
    "text": "You've got the\ntemperature, whether or not",
    "start": "65019",
    "end": "66780"
  },
  {
    "text": "it's a working day, the weather\nsituation, whether it's cloudy,",
    "start": "66780",
    "end": "71580"
  },
  {
    "text": "misty, light rain, snow.",
    "start": "71580",
    "end": "73480"
  },
  {
    "text": "There's a base level for\nweather, which is clear.",
    "start": "73480",
    "end": "76840"
  },
  {
    "text": "And then two other\nvariables with lots",
    "start": "76840",
    "end": "78719"
  },
  {
    "text": "of levels, which we haven't\nput in this kind of an over",
    "start": "78720",
    "end": "81450"
  },
  {
    "text": "plot, which is, the\nmonth of the year.",
    "start": "81450",
    "end": "84960"
  },
  {
    "text": "And we actually plot in\nthe coefficients just",
    "start": "84960",
    "end": "87570"
  },
  {
    "text": "as a function of month.",
    "start": "87570",
    "end": "89340"
  },
  {
    "text": "And the hour of the day\nis 24 hours of the day.",
    "start": "89340",
    "end": "92829"
  },
  {
    "text": "And so we plot each\nhour as a point here.",
    "start": "92830",
    "end": "96180"
  },
  {
    "text": "But ideally, these all\nbelong in this table.",
    "start": "96180",
    "end": "99330"
  },
  {
    "text": "We just fit a linear\nregression model.",
    "start": "99330",
    "end": "101230"
  },
  {
    "text": "We show the coefficients a\nstandard-- just the usual stuff.",
    "start": "101230",
    "end": "104400"
  },
  {
    "text": "It looks like\neverything's significant.",
    "start": "104400",
    "end": "106120"
  },
  {
    "text": "It's quite a large data set.",
    "start": "106120",
    "end": "108820"
  },
  {
    "text": "And we can see from\nthe month figure",
    "start": "108820",
    "end": "111840"
  },
  {
    "text": "that there's more bicycles\nrented in May and June.",
    "start": "111840",
    "end": "115329"
  },
  {
    "text": "Then during the hot months\nin DC, there's fewer.",
    "start": "115330",
    "end": "118840"
  },
  {
    "text": "Then in the fall, more again.",
    "start": "118840",
    "end": "120570"
  },
  {
    "text": "And, of course, in\nthe winter, very few.",
    "start": "120570",
    "end": "122880"
  },
  {
    "text": "And then hour of\nthe day, there's",
    "start": "122880",
    "end": "124979"
  },
  {
    "text": "a natural kind of\npartition as well,",
    "start": "124980",
    "end": "127380"
  },
  {
    "text": "more rented at the\nbeginning of the workday,",
    "start": "127380",
    "end": "130020"
  },
  {
    "text": "at the end of the workday,\nbecause people use it",
    "start": "130020",
    "end": "132120"
  },
  {
    "text": "as a means of public transport.",
    "start": "132120",
    "end": "134250"
  },
  {
    "text": "And then different\namounts during the day.",
    "start": "134250",
    "end": "136600"
  },
  {
    "text": "There's a lunchtime\nperiod as well.",
    "start": "136600",
    "end": "138760"
  },
  {
    "text": "So that's fine.",
    "start": "138760",
    "end": "139959"
  },
  {
    "text": "But the response is the\nnumber of hourly users.",
    "start": "139960",
    "end": "143760"
  },
  {
    "start": "143000",
    "end": "256000"
  },
  {
    "text": "So the number of\nusers, first of all,",
    "start": "143760",
    "end": "145890"
  },
  {
    "text": "it's a non-negative variable.",
    "start": "145890",
    "end": "148560"
  },
  {
    "text": "And yeah, we've shown a picture\njust of the response versus hour",
    "start": "148560",
    "end": "156989"
  },
  {
    "text": "of the day.",
    "start": "156990",
    "end": "158140"
  },
  {
    "text": "And we've put a smoothing\nspline fit through this",
    "start": "158140",
    "end": "160500"
  },
  {
    "text": "just so that you can see just\nfor the single variable what",
    "start": "160500",
    "end": "163920"
  },
  {
    "text": "the mean looks like as a\nfunction of hour of the day.",
    "start": "163920",
    "end": "167260"
  },
  {
    "text": "And what you see is that\nwhen the mean is low,",
    "start": "167260",
    "end": "171239"
  },
  {
    "text": "the spread is somewhat low,\nbut when the mean is higher,",
    "start": "171240",
    "end": "174930"
  },
  {
    "text": "the spread seems to get bigger.",
    "start": "174930",
    "end": "177810"
  },
  {
    "text": "So for the most\npart, the variance",
    "start": "177810",
    "end": "180239"
  },
  {
    "text": "seems to increase with the mean.",
    "start": "180240",
    "end": "182110"
  },
  {
    "text": "And when we fit linear\nregression models,",
    "start": "182110",
    "end": "183930"
  },
  {
    "text": "we really assuming that the\nvariance of y is constant.",
    "start": "183930",
    "end": "189000"
  },
  {
    "text": "This is just showing\none variable.",
    "start": "189000",
    "end": "190570"
  },
  {
    "text": "But if you fit the full linear\nmodel like we did before,",
    "start": "190570",
    "end": "193440"
  },
  {
    "text": "10% of the linear\nmodel predictions",
    "start": "193440",
    "end": "195630"
  },
  {
    "text": "are negative because there's no\nconstraints on the linear model",
    "start": "195630",
    "end": "199470"
  },
  {
    "text": "that the predictions\nshould be positive,",
    "start": "199470",
    "end": "201420"
  },
  {
    "text": "even though the response\nis always positive.",
    "start": "201420",
    "end": "204390"
  },
  {
    "text": "In the cases like\nthis, you might",
    "start": "204390",
    "end": "205830"
  },
  {
    "text": "be tempted to rather model\nthe log of the bikers that",
    "start": "205830",
    "end": "209520"
  },
  {
    "text": "counts of bikers, but\nit has its own problems.",
    "start": "209520",
    "end": "212740"
  },
  {
    "text": "Example predictions\non the wrong scale",
    "start": "212740",
    "end": "215100"
  },
  {
    "text": "because you really want\nto make predictions",
    "start": "215100",
    "end": "217200"
  },
  {
    "text": "on the bike's number of bikers\nscale, not on the log scale.",
    "start": "217200",
    "end": "220860"
  },
  {
    "text": "And some of the\ncounts could be 0.",
    "start": "220860",
    "end": "222880"
  },
  {
    "text": "And so you can't take logs.",
    "start": "222880",
    "end": "224760"
  },
  {
    "text": "So that's not, in\ngeneral, a good solution.",
    "start": "224760",
    "end": "227480"
  },
  {
    "text": "Here's a picture of\nthe log of the bikers",
    "start": "227480",
    "end": "230489"
  },
  {
    "text": "and with a similar\nsmooth curve put through.",
    "start": "230490",
    "end": "234210"
  },
  {
    "text": "It seems like the variance is\nmore stable on the log scale",
    "start": "234210",
    "end": "237810"
  },
  {
    "text": "as a function of the mean.",
    "start": "237810",
    "end": "239500"
  },
  {
    "text": "But there's some weird stuff\ngoing on down here when we're",
    "start": "239500",
    "end": "242130"
  },
  {
    "text": "taking logs of small counts.",
    "start": "242130",
    "end": "244420"
  },
  {
    "text": "And for the zeros--",
    "start": "244420",
    "end": "246240"
  },
  {
    "text": "well, these are\nlogs that are zeros,",
    "start": "246240",
    "end": "248130"
  },
  {
    "text": "so these must be the ones.",
    "start": "248130",
    "end": "249670"
  },
  {
    "text": "But any zero counts\nwere either left out",
    "start": "249670",
    "end": "253319"
  },
  {
    "text": "or some fudging had to be done.",
    "start": "253320",
    "end": "255450"
  },
  {
    "text": "So this is where the\nPoisson model comes in.",
    "start": "255450",
    "end": "258549"
  },
  {
    "start": "256000",
    "end": "366000"
  },
  {
    "text": "So just like we use the binomial\nmodel for zero one data,",
    "start": "258550",
    "end": "262078"
  },
  {
    "text": "it turns out the\nPoisson regression",
    "start": "262079",
    "end": "263730"
  },
  {
    "text": "or the Poisson distribution\nis good for modeling counts.",
    "start": "263730",
    "end": "266870"
  },
  {
    "text": "So for those who\nare interested, this",
    "start": "266870",
    "end": "269500"
  },
  {
    "text": "is the form for the Poisson\nprobability mass function.",
    "start": "269500",
    "end": "273760"
  },
  {
    "text": "So it gives an explicit form for\nthe probability that y equals k.",
    "start": "273760",
    "end": "278440"
  },
  {
    "text": "And it's controlled\nby a parameter",
    "start": "278440",
    "end": "280300"
  },
  {
    "text": "lambda, which is the mean\ncount in the distribution.",
    "start": "280300",
    "end": "285039"
  },
  {
    "text": "And for the Poisson,\nit turns out",
    "start": "285040",
    "end": "286840"
  },
  {
    "text": "that the variance is\nequal to the mean.",
    "start": "286840",
    "end": "289780"
  },
  {
    "text": "So when the mean is higher,\nthe variance is higher.",
    "start": "289780",
    "end": "292780"
  },
  {
    "text": "So that's a property of\nthe Poisson distribution.",
    "start": "292780",
    "end": "295840"
  },
  {
    "text": "By the way, for the\nbinomial distribution,",
    "start": "295840",
    "end": "299260"
  },
  {
    "text": "the variance also\ndepends on the mean.",
    "start": "299260",
    "end": "302560"
  },
  {
    "text": "For a binomial distribution,\nif the mean is p,",
    "start": "302560",
    "end": "305620"
  },
  {
    "text": "then the variance is\np times 1 minus p.",
    "start": "305620",
    "end": "309410"
  },
  {
    "text": "It's the Bernoulli\ndistribution, and that",
    "start": "309410",
    "end": "311600"
  },
  {
    "text": "tends to be a property\nof this class of GLMs",
    "start": "311600",
    "end": "314570"
  },
  {
    "text": "that we're talking about,\nexcept for the Gaussian.",
    "start": "314570",
    "end": "318240"
  },
  {
    "text": "OK, so this is just for a\nsingle Poisson distribution,",
    "start": "318240",
    "end": "321389"
  },
  {
    "text": "but we're interested in\na Poisson distribution",
    "start": "321390",
    "end": "323430"
  },
  {
    "text": "where the mean changes\nwith the covariates.",
    "start": "323430",
    "end": "326669"
  },
  {
    "text": "So now, instead of\njust writing lambda,",
    "start": "326670",
    "end": "329550"
  },
  {
    "text": "we're going to write\nlambda as a function of x.",
    "start": "329550",
    "end": "332009"
  },
  {
    "text": "And what we're\ngoing to do is we're",
    "start": "332010",
    "end": "333570"
  },
  {
    "text": "going to assume that the log of\nlambda is a linear function of",
    "start": "333570",
    "end": "337620"
  },
  {
    "text": "x.",
    "start": "337620",
    "end": "338430"
  },
  {
    "text": "So just like for the\nlogistic regression,",
    "start": "338430",
    "end": "340710"
  },
  {
    "text": "we assume the logit of the\nprobability was a function of x.",
    "start": "340710",
    "end": "346020"
  },
  {
    "text": "Here we are assuming the log\nof the mean is a function of x.",
    "start": "346020",
    "end": "350310"
  },
  {
    "text": "Or you can invert\nthat transformation",
    "start": "350310",
    "end": "352590"
  },
  {
    "text": "and says that lambda is e\nto the linear combination",
    "start": "352590",
    "end": "356430"
  },
  {
    "text": "of the coordinates of x.",
    "start": "356430",
    "end": "358560"
  },
  {
    "text": "So you can see\nthat automatically",
    "start": "358560",
    "end": "360330"
  },
  {
    "text": "guarantees that the mean\nis positive in this case.",
    "start": "360330",
    "end": "365229"
  },
  {
    "text": "We can fit this model, and we\nfit it by maximum likelihood.",
    "start": "365230",
    "end": "369110"
  },
  {
    "start": "366000",
    "end": "444000"
  },
  {
    "text": "We're not going to\ngo into the details,",
    "start": "369110",
    "end": "370900"
  },
  {
    "text": "but we use the Poisson as\nthe basis for the fitting.",
    "start": "370900",
    "end": "374290"
  },
  {
    "text": "And we can fit that\nmodel and get a summary.",
    "start": "374290",
    "end": "377510"
  },
  {
    "text": "And just like we did\nfor logistic regression",
    "start": "377510",
    "end": "380050"
  },
  {
    "text": "and for linear regression,\nyou get a summary",
    "start": "380050",
    "end": "382509"
  },
  {
    "text": "that identifies the\ncoefficients for each variable",
    "start": "382510",
    "end": "385750"
  },
  {
    "text": "and gets standard errors\nand p-values, and so on.",
    "start": "385750",
    "end": "389240"
  },
  {
    "text": "And so this is very similar.",
    "start": "389240",
    "end": "391330"
  },
  {
    "text": "You'll see these coefficients\nare different because these",
    "start": "391330",
    "end": "393819"
  },
  {
    "text": "are on the log scale than\nfor the linear model,",
    "start": "393820",
    "end": "397000"
  },
  {
    "text": "but they're going to\nhave the similar message.",
    "start": "397000",
    "end": "399290"
  },
  {
    "text": "But the nice thing is that when\nyou fit this Poisson model,",
    "start": "399290",
    "end": "402100"
  },
  {
    "text": "it takes this change in\nvariance into account",
    "start": "402100",
    "end": "404650"
  },
  {
    "text": "when it does a fitting.",
    "start": "404650",
    "end": "406639"
  },
  {
    "text": "And again, we just\nshow us dot plots--",
    "start": "406640",
    "end": "409610"
  },
  {
    "text": "the effects estimated\nfor month and for hour.",
    "start": "409610",
    "end": "413060"
  },
  {
    "text": "And if you compare\nback, you will",
    "start": "413060",
    "end": "415370"
  },
  {
    "text": "see that it's somewhat similar.",
    "start": "415370",
    "end": "417500"
  },
  {
    "text": "It turns out in this\ncase, the variance",
    "start": "417500",
    "end": "419510"
  },
  {
    "text": "is actually somewhat\nlarger than the mean.",
    "start": "419510",
    "end": "421732"
  },
  {
    "text": "For the Poisson, we\nassuming the variance",
    "start": "421732",
    "end": "423440"
  },
  {
    "text": "is the same as the mean.",
    "start": "423440",
    "end": "425180"
  },
  {
    "text": "But this is a well known\nsituation when you pass on data",
    "start": "425180",
    "end": "428699"
  },
  {
    "text": "and it's known as\noverdispersion.",
    "start": "428700",
    "end": "430670"
  },
  {
    "text": "So the p-values are\nmisleadingly small in this plot,",
    "start": "430670",
    "end": "434360"
  },
  {
    "text": "in this table here, but you\ncan get corrected p-values",
    "start": "434360",
    "end": "438560"
  },
  {
    "text": "by accommodating\nthis overdispersion",
    "start": "438560",
    "end": "441200"
  },
  {
    "text": "but we don't go into that yet.",
    "start": "441200",
    "end": "443480"
  },
  {
    "text": "So we've covered three\ngeneralized linear models",
    "start": "443480",
    "end": "446510"
  },
  {
    "start": "444000",
    "end": "575000"
  },
  {
    "text": "in this course, Gaussian\nbinomial and now Poisson.",
    "start": "446510",
    "end": "449850"
  },
  {
    "text": "So they each have a\ncharacteristic link function.",
    "start": "449850",
    "end": "453320"
  },
  {
    "text": "This is the transformation of\nthe mean that is represented",
    "start": "453320",
    "end": "456380"
  },
  {
    "text": "by a linear model.",
    "start": "456380",
    "end": "458330"
  },
  {
    "text": "And we write it like\nthis-- eta as the link",
    "start": "458330",
    "end": "461270"
  },
  {
    "text": "function of the mean.",
    "start": "461270",
    "end": "463069"
  },
  {
    "text": "And it's the transformation of\nthe mean that's a linear model.",
    "start": "463070",
    "end": "467705"
  },
  {
    "text": "So the link functions for linear\nlogistic and Poisson regression",
    "start": "467705",
    "end": "472129"
  },
  {
    "text": "are--",
    "start": "472130",
    "end": "473000"
  },
  {
    "text": "for the linear model, it's\njust the identity link.",
    "start": "473000",
    "end": "476810"
  },
  {
    "text": "It just uses the same--\nmodel to mean directly.",
    "start": "476810",
    "end": "480250"
  },
  {
    "text": "For logistic regression,\nit's a logit link",
    "start": "480250",
    "end": "483560"
  },
  {
    "text": "because in logistic regression,\nthe mean for the binomial",
    "start": "483560",
    "end": "486320"
  },
  {
    "text": "is the probability.",
    "start": "486320",
    "end": "488630"
  },
  {
    "text": "And for Poisson,\nit's the log link.",
    "start": "488630",
    "end": "491810"
  },
  {
    "text": "And how do you decide which\nof these models to use?",
    "start": "491810",
    "end": "494930"
  },
  {
    "text": "Well, it's the nature\nof the response.",
    "start": "494930",
    "end": "498560"
  },
  {
    "text": "So for Poisson, the\nresponse are these counts.",
    "start": "498560",
    "end": "502610"
  },
  {
    "text": "And so we decide that\nwe need a distribution",
    "start": "502610",
    "end": "505490"
  },
  {
    "text": "at suitable for counts.",
    "start": "505490",
    "end": "507780"
  },
  {
    "text": "For binary data, it's--",
    "start": "507780",
    "end": "510330"
  },
  {
    "text": "Bernoulli is pretty\nmuch only distribution.",
    "start": "510330",
    "end": "512769"
  },
  {
    "text": "Whereas if it's a\nquantitative variable,",
    "start": "512770",
    "end": "514469"
  },
  {
    "text": "we'll just use Gaussian\nif it looks like--",
    "start": "514470",
    "end": "518788"
  },
  {
    "text": "you know, if it's got a\nnice symmetric distribution.",
    "start": "518789",
    "end": "522169"
  },
  {
    "text": "Turns out they also have\ncharacteristic variance",
    "start": "522169",
    "end": "524390"
  },
  {
    "text": "functions, and we talked\nabout that the variance",
    "start": "524390",
    "end": "526730"
  },
  {
    "text": "for the Poisson is\nequal to the mean.",
    "start": "526730",
    "end": "528920"
  },
  {
    "text": "Then the models are fit by\nmaximum likelihood and summaries",
    "start": "528920",
    "end": "532370"
  },
  {
    "text": "like the ones we showed are\nproduced by the GLM function",
    "start": "532370",
    "end": "535190"
  },
  {
    "text": "in R.",
    "start": "535190",
    "end": "537350"
  },
  {
    "text": "And the other GLMs that\nwe haven't talked about",
    "start": "537350",
    "end": "540139"
  },
  {
    "text": "include the gamma distribution,\na negative binomial,",
    "start": "540140",
    "end": "544310"
  },
  {
    "text": "which is despite the\nname, is actually",
    "start": "544310",
    "end": "547310"
  },
  {
    "text": "used in the case of counts.",
    "start": "547310",
    "end": "549120"
  },
  {
    "text": "But when you've got\nextra overdispersion,",
    "start": "549120",
    "end": "553070"
  },
  {
    "text": "there's also the inverse\nGaussian distribution.",
    "start": "553070",
    "end": "556250"
  },
  {
    "text": "The gamma distribution\nis often used",
    "start": "556250",
    "end": "557930"
  },
  {
    "text": "when you've got\npositive observations",
    "start": "557930",
    "end": "560313"
  },
  {
    "text": "or non-negative\nobservations, and you've",
    "start": "560313",
    "end": "561980"
  },
  {
    "text": "got long tails to the right,\nthe gamma is often used.",
    "start": "561980",
    "end": "566420"
  },
  {
    "text": "And there's more.",
    "start": "566420",
    "end": "567600"
  },
  {
    "text": "So the GLMs are\nquite a big family.",
    "start": "567600",
    "end": "571220"
  },
  {
    "text": "We've just told you about\nthe more important ones.",
    "start": "571220",
    "end": "574480"
  },
  {
    "start": "574480",
    "end": "575000"
  }
]