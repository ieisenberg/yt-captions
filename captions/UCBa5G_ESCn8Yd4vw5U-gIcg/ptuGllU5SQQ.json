[
  {
    "start": "0",
    "end": "121000"
  },
  {
    "start": "0",
    "end": "5440"
  },
  {
    "text": "Hi, everyone. Welcome to CS224N, lecture\nnine, Self Attention",
    "start": "5440",
    "end": "12460"
  },
  {
    "text": "and Transformers. If I am not able to\nbe heard right now, please someone send\na message in the chat",
    "start": "12460",
    "end": "18130"
  },
  {
    "text": "because I can't see anyone. But I'm excited to get\ninto the content for today,",
    "start": "18130",
    "end": "26439"
  },
  {
    "text": "we'll be talking about self\nattention and transformers. Let us dive into\nthe lecture plan",
    "start": "26440",
    "end": "33880"
  },
  {
    "text": "and we'll talk about some\nsort of to do's for the course as well. So we'll start with\nwhere we were back",
    "start": "33880",
    "end": "41649"
  },
  {
    "text": "last week with recurrence,\nrecurrent neural networks and we'll talk about a\nmovement from recurrence",
    "start": "41650",
    "end": "47320"
  },
  {
    "text": "to attention based\non NLP models, we talked about\nattention and we're going to just go all in on attention.",
    "start": "47320",
    "end": "53860"
  },
  {
    "text": "We'll introduce the\ntransformer model, which is a particular type of\nattention based model that's very popular, you need to know\nit, you're going to learn it.",
    "start": "53860",
    "end": "62242"
  },
  {
    "text": "We'll talk about\nsome great results with transformers and then\nsome drawbacks and variants and sort of very recent\nwork on improving them.",
    "start": "62242",
    "end": "70509"
  },
  {
    "text": "So some reminders before we\njump in, assignment 4 is due,",
    "start": "70510",
    "end": "75790"
  },
  {
    "text": "the mid-quarter feedback survey\nis due Tuesday, February 16th. You get some small number\nof points for doing that",
    "start": "75790",
    "end": "83680"
  },
  {
    "text": "and we really\nappreciate your feedback on what we've done well,\nwhat we can improve on. And then final project\nproposal is also due,",
    "start": "83680",
    "end": "93310"
  },
  {
    "text": "one note on the proposals, part\nof the goal of the proposal is to, I'd say the main part\nof the goal of the proposal",
    "start": "93310",
    "end": "102760"
  },
  {
    "text": "is to give you feedback on the\nidea that you have presented and make sure that\nit is a viable option",
    "start": "102760",
    "end": "113380"
  },
  {
    "text": "for a final project and make\nsure we kind of recenter if not. And so we want to get feedback\nto you very quickly on that,",
    "start": "113380",
    "end": "120159"
  },
  {
    "text": "OK. All right. So with that, let's\nstart in on the content",
    "start": "120160",
    "end": "126979"
  },
  {
    "start": "121000",
    "end": "269000"
  },
  {
    "text": "of this week's lecture. So we were in this place\nin NLP as of last week,",
    "start": "126980",
    "end": "133520"
  },
  {
    "text": "where we had recurrent\nneural networks, sort of for a lot of things\nthat you wanted to do.",
    "start": "133520",
    "end": "138769"
  },
  {
    "text": "So it's around 2016\nand the strategy if you want to build a strong\nand healthy model, is you have",
    "start": "138770",
    "end": "145609"
  },
  {
    "text": "sentences that\nyou need to encode and you have a\nbidirectional LSTM say,",
    "start": "145610",
    "end": "150680"
  },
  {
    "text": "and maybe it looks a little\nbit like this pictographically and maybe it's a source\nsentence in a translation,",
    "start": "150680",
    "end": "156690"
  },
  {
    "text": "for example, we saw\nmachine translation. And then you define your\noutput, which is maybe a sequence of words which is the\ntarget translation that we're",
    "start": "156690",
    "end": "164120"
  },
  {
    "text": "trying to predict or\nmaybe it's a parse tree, or it's a summary, and you\nuse an LSTM with one direction",
    "start": "164120",
    "end": "173250"
  },
  {
    "text": "to generate it. And this works really well. We use these architectures\nto do all kinds",
    "start": "173250",
    "end": "178950"
  },
  {
    "text": "of interesting things, but\none thing that we said, we talked about is information\nsort of bottleneck that you're",
    "start": "178950",
    "end": "184440"
  },
  {
    "text": "trying to encode, maybe a\nvery long sequence in sort of the very last vector in your,\nor one vector in your encoder,",
    "start": "184440",
    "end": "191099"
  },
  {
    "text": "and so we use the\ntension as this mechanism to take a representation from\nour decoder and sort of look",
    "start": "191100",
    "end": "198450"
  },
  {
    "text": "back and treat the encoded\nrepresentations as a memory, that we can reference\nand sort of pick out",
    "start": "198450",
    "end": "204990"
  },
  {
    "text": "what's important to any given\ntime, and that was attention. And this week, we're going to\ndo something slightly different.",
    "start": "204990",
    "end": "214360"
  },
  {
    "text": "So we learned about sequence to\nsequence models, the encoder, decoder way of thinking\nabout problems, more or less",
    "start": "214360",
    "end": "221790"
  },
  {
    "text": "in order to deal with this\nidea of building a machine translation system that's end\nto end differentiable, right?",
    "start": "221790",
    "end": "228210"
  },
  {
    "text": "And so this is sort of\na really interesting way of thinking about problems. What we'll do this\nweek is different.",
    "start": "228210",
    "end": "234455"
  },
  {
    "text": "We're not trying\nto motivate sort of an entirely new way of\nthinking about problems",
    "start": "234455",
    "end": "240150"
  },
  {
    "text": "like machine translation,\ninstead we're going to take the building\nblocks that we were using,",
    "start": "240150",
    "end": "245489"
  },
  {
    "text": "recurrent neural\nnetworks and we're going to spend a lot\nof trial and error in the field trying to figure\nout if there are building",
    "start": "245490",
    "end": "252270"
  },
  {
    "text": "blocks that just work\nbetter across a broad range of problems, sort of\nslot the new thing",
    "start": "252270",
    "end": "260130"
  },
  {
    "text": "in for recurrent neural\nnetworks and say, voila, maybe it works better.",
    "start": "260130",
    "end": "265590"
  },
  {
    "text": "And so I want to take us\non this sort of journey to self attention\nnetworks, and we'll",
    "start": "265590",
    "end": "272867"
  },
  {
    "start": "269000",
    "end": "436000"
  },
  {
    "text": "start with some problems with\nrecurrent neural networks. So we spent a bit of time\ntrying to convince you",
    "start": "272867",
    "end": "279660"
  },
  {
    "text": "that recurrent neural\nnetworks were very useful. Now I'm going to\ntalk about reasons",
    "start": "279660",
    "end": "285150"
  },
  {
    "text": "why they can be improved. So we know that recurrent\nnetworks are enrolled",
    "start": "285150",
    "end": "291449"
  },
  {
    "text": "left to right in air quotes, it\ncould be right to left as well.",
    "start": "291450",
    "end": "297010"
  },
  {
    "text": "So what does this mean? A recurrent neural network\nencodes linear locality, right?",
    "start": "297010",
    "end": "303240"
  },
  {
    "text": "So once I'm looking at\ntasty in this phrase, I'm about to look\nat Pizza or if I'm",
    "start": "303240",
    "end": "309197"
  },
  {
    "text": "going in the other direction\nonce I look at Pizza, I'm about to look at tasty. And so it's very easy\nfor their meanings,",
    "start": "309197",
    "end": "314813"
  },
  {
    "text": "for their presence\nin the sentence to affect the meaning, to\naffect the representation of the other word.",
    "start": "314813",
    "end": "320490"
  },
  {
    "text": "And this is actually\nquite useful because nearby words frequently\ndo influence each other, that's practically one of\nthe things we talked",
    "start": "320490",
    "end": "326550"
  },
  {
    "text": "about with the distributional\nhypothesis as encoded by something like Word2vec.",
    "start": "326550",
    "end": "332280"
  },
  {
    "text": "But if words are\ndistant linearly, they can still interact\nwith each other.",
    "start": "332280",
    "end": "337660"
  },
  {
    "text": "This is something that we\nsaw in dependency parsing. So if I have say the phrase the\nchef, notice chef bolded here,",
    "start": "337660",
    "end": "345450"
  },
  {
    "text": "I'm running a recurrent neural\nnetwork over this, and then the chef who, and we're going\nto have this long sequence",
    "start": "345450",
    "end": "351870"
  },
  {
    "text": "that I'm going to encode. And then the word\nwas, right, maybe it",
    "start": "351870",
    "end": "358780"
  },
  {
    "text": "is the chef who was,\nbut in between I have O of sequence length many\nsteps of the computation that I",
    "start": "358780",
    "end": "366220"
  },
  {
    "text": "need to get to before chef\nand was can interact, right?",
    "start": "366220",
    "end": "372580"
  },
  {
    "text": "And so in the middle,\nthings might go wrong. Maybe it's hard to\nlearn things with where",
    "start": "372580",
    "end": "382544"
  },
  {
    "text": "they should interact. So in particular, it might be\nhard to learn long distance dependencies because we\nhave gradient problems.",
    "start": "382545",
    "end": "388840"
  },
  {
    "text": "We saw that LSTMs\npropagate gradients better than simple\nRNNs but not perfectly.",
    "start": "388840",
    "end": "394290"
  },
  {
    "text": "And so if chef and\nwas are very far, it becomes hard to learn\nthat they should interact. And the linear order\nof words is sort",
    "start": "394290",
    "end": "401048"
  },
  {
    "text": "of baked into the model, right? You have to unroll the RNN\nthroughout the sequence,",
    "start": "401048",
    "end": "406360"
  },
  {
    "text": "and it's not really the right\nway to think about sentences necessarily, obviously\nlinear order isn't really",
    "start": "406360",
    "end": "413650"
  },
  {
    "text": "how sentences are\nkind of structured.",
    "start": "413650",
    "end": "418930"
  },
  {
    "text": "And so here you have\nChef, and then you've got all this sort of\ncomputation in the middle, all",
    "start": "418930",
    "end": "424480"
  },
  {
    "text": "of those applications of\nthe recurrent weight matrix before you allow it\nto interact with was.",
    "start": "424480",
    "end": "429820"
  },
  {
    "text": "And again, sort of dependence\nis O of sequence length, and then you got\nthe word \"was\" okay?",
    "start": "429820",
    "end": "436550"
  },
  {
    "start": "436000",
    "end": "575000"
  },
  {
    "text": "A second problem\nis very related, this is the lack of\nparallelizability.",
    "start": "436550",
    "end": "441830"
  },
  {
    "text": "So this is going to be a\nhuge refrain, now that we've gotten to the transformers\nlectures, is parallelizability,",
    "start": "441830",
    "end": "448850"
  },
  {
    "text": "it's what you get\nfrom your GPU and it's what you want to exploit. So when you run\nan RNN, you have O",
    "start": "448850",
    "end": "458600"
  },
  {
    "text": "of sequence length many\nunparallellelizable operations. And so while you have a\nGPU that can kind of chunk",
    "start": "458600",
    "end": "466190"
  },
  {
    "text": "through a bunch of independent\noperations at once, you're unable to sort\nof do them all at once",
    "start": "466190",
    "end": "471919"
  },
  {
    "text": "because you have this\nexplicit time dependence in their current equations. In particular, a future\nRNN state down the line",
    "start": "471920",
    "end": "480890"
  },
  {
    "text": "can't be computed until you've\ncomputed one that's earlier on, and this inhibits training\non very large data sets.",
    "start": "480890",
    "end": "486860"
  },
  {
    "text": "So let's take a look at\nthis, unrolling an RNN. If this is say the first\nlayer of an RNN or an LSTM,",
    "start": "486860",
    "end": "495000"
  },
  {
    "text": "maybe it doesn't depend\non effectively anything, you can just compute\nit immediately. And then the second\nlayer, so this",
    "start": "495000",
    "end": "501600"
  },
  {
    "text": "is a stacked set of two LSTMS. The second layer depends\non the first layer here.",
    "start": "501600",
    "end": "507719"
  },
  {
    "text": "In the time dimension though,\nthis cell here depends on this,",
    "start": "507720",
    "end": "516159"
  },
  {
    "text": "so you've got a 1, and then\nthis depends on this so you've got a 1, so you have at most,\nI'm sorry, at least two things",
    "start": "516159",
    "end": "523030"
  },
  {
    "text": "that you need to\ncompute here before you can compute the value of this\ncell, likewise three here.",
    "start": "523030",
    "end": "529300"
  },
  {
    "text": "And with the sequence\nlength, it grows with O of the sequence length. So here I have been unable to\neven try to compute this value",
    "start": "529300",
    "end": "538000"
  },
  {
    "text": "when I get here because I had\nto sort of do all of this first, so I can't parallelize\nover the time dimension",
    "start": "538000",
    "end": "545410"
  },
  {
    "text": "and this inhibits training\non very large data sets. ",
    "start": "545410",
    "end": "552690"
  },
  {
    "text": "OK, so and then I guess,\nChristopher, or TAs, feel free to stop\nme with a question",
    "start": "552690",
    "end": "558750"
  },
  {
    "text": "if it feels like that's\nthe right thing to do. OK, and so you can see how\nit's a related problem, right?",
    "start": "558750",
    "end": "566190"
  },
  {
    "text": "It's really directly related\nto the recurrence of the model. The thing that we\nthought was really useful",
    "start": "566190",
    "end": "571350"
  },
  {
    "text": "now is being problematic. OK, so what I'm trying\nto say with that is,",
    "start": "571350",
    "end": "580220"
  },
  {
    "start": "575000",
    "end": "825000"
  },
  {
    "text": "we seemingly want to\nreplace recurrence as the building block itself. So let's go through\nsome alternatives,",
    "start": "580220",
    "end": "586020"
  },
  {
    "text": "and we've seen each of these\nalternatives in the class so far. We'll start with word\nwindow, sort of building",
    "start": "586020",
    "end": "592220"
  },
  {
    "text": "blocks for our NLP models. If we wanted to\nreplace our encoders and our decoders with something\nthat sort of fit the same goal",
    "start": "592220",
    "end": "601040"
  },
  {
    "text": "but had different properties. So a word window model will\naggregate local context, right?",
    "start": "601040",
    "end": "606720"
  },
  {
    "text": "We saw this with our sort\nof word window classifiers that we've built already. You take a local\ncontext of words,",
    "start": "606720",
    "end": "612980"
  },
  {
    "text": "you use it to represent\ninformation about the center word, this is also known as\none dimensional convolution,",
    "start": "612980",
    "end": "620510"
  },
  {
    "text": "we'll go over this in\ndepth later in the course, right now we'll consider\nit as word window contexts.",
    "start": "620510",
    "end": "627540"
  },
  {
    "text": "So the number of\nunparallellelizable operations with these word\nwindow building blocks does not grow with\nthe sequence length.",
    "start": "627540",
    "end": "634130"
  },
  {
    "text": "And here's sort of\na picture of that. You have the embedding\nlayer say, so you can embed every\nword independently,",
    "start": "634130",
    "end": "640730"
  },
  {
    "text": "right, So you don't need to know\nthe other words surrounding it in order to pick the right\nembedding dimension out.",
    "start": "640730",
    "end": "647610"
  },
  {
    "text": "And so these all have\nsort of zero dependence in this sort of\nhand waving notion of how much\nparallelizability there is.",
    "start": "647610",
    "end": "655370"
  },
  {
    "text": "Now you can walk a\nword window classifier on top of each one to build a\nrepresentation of the word that",
    "start": "655370",
    "end": "662960"
  },
  {
    "text": "takes into account\nits local context. But in order to apply\nit to this word h1",
    "start": "662960",
    "end": "669050"
  },
  {
    "text": "I don't need to know\nanything, sorry, I don't need to have\napplied it to h1 in order to apply it to h2.",
    "start": "669050",
    "end": "675230"
  },
  {
    "text": "Likewise, in order to apply\na word window contextualizer to ht, I can just look at its\nlocal window independently.",
    "start": "675230",
    "end": "682230"
  },
  {
    "text": "And so again, none of these\nhave a dependence in time. I can keep stacking\nlayers like this, right?",
    "start": "682230",
    "end": "689210"
  },
  {
    "text": "So this can look\nlike an encoder, right, an encoder like\nour LSTM encoders.",
    "start": "689210",
    "end": "694790"
  },
  {
    "text": "If I didn't allow you\nto look at the future by just cutting\noff the window, it could look like a decoder\nfor language models.",
    "start": "694790",
    "end": "701060"
  },
  {
    "text": "And this is nice. And we get this beautiful,\nO(1) dependence in time, right?",
    "start": "701060",
    "end": "708178"
  },
  {
    "text": "No dependence at all\nin the time dimension, that's an improvement. But there are some problems. So what about long distance\ndependencies, right?",
    "start": "708178",
    "end": "714120"
  },
  {
    "text": "This is why we said we wanted to\nuse recurrent neural networks, it's because they\nwould do better",
    "start": "714120",
    "end": "719270"
  },
  {
    "text": "at encoding long\ndistance dependencies. It's a problem, just like\nit was a problem before",
    "start": "719270",
    "end": "725420"
  },
  {
    "text": "but by stacking\nword window layers, we can get to wider,\nlonger contexts.",
    "start": "725420",
    "end": "730890"
  },
  {
    "text": "So if you have some\nsort of window size and then you stack two\nlayers, so red states",
    "start": "730890",
    "end": "737780"
  },
  {
    "text": "here interact, a state,\nkind of how you can look, how far away you can look in\norder to encode hk, right?",
    "start": "737780",
    "end": "744620"
  },
  {
    "text": "So in the embedding layer, you\nhave these sort of words here.",
    "start": "744620",
    "end": "749930"
  },
  {
    "text": "So this is the last layer, this\ntop layer of the word window classifier. Here is the embedding of hk\nat the output of your encoder,",
    "start": "749930",
    "end": "758000"
  },
  {
    "text": "and so it looks at\nthe local five words because that's the window size. And then as well, the\nfarthest word over here",
    "start": "758000",
    "end": "766190"
  },
  {
    "text": "has also looked a couple\nof words away, right? So if you stack these and\nstack these and stack these",
    "start": "766190",
    "end": "771380"
  },
  {
    "text": "without growing the window\nsize at all at any given layer, you can look pretty far. And actually there\nare tricks you",
    "start": "771380",
    "end": "777675"
  },
  {
    "text": "can use to look even farther. But you still have this sort\nof, at least in principle, problem where you've got\na word like this, h1.",
    "start": "777675",
    "end": "785370"
  },
  {
    "text": "And you can see how it's in\nblue and with these two layers of the network, I don't\nknow anything about h1",
    "start": "785370",
    "end": "792740"
  },
  {
    "text": "at all when I'm building up the\nrepresentation of hk over here. I could solve that by adding\nanother layer in depth,",
    "start": "792740",
    "end": "799430"
  },
  {
    "text": "but in principle you always\nhave some finite field. ",
    "start": "799430",
    "end": "804550"
  },
  {
    "text": "So this is actually\npretty useful. These word window kind\nof contextualizers,",
    "start": "804550",
    "end": "810480"
  },
  {
    "text": "and we will learn\nmore about them later. And there was sort of\na lot of this effort that I talked about at the\nbeginning of the class,",
    "start": "810480",
    "end": "816433"
  },
  {
    "text": "was actually sort\nof partly deciding which of the word window\nstuff, convolutional it's called stuff, or\nattention, and attention",
    "start": "816433",
    "end": "822930"
  },
  {
    "text": "has won out for the time being. And so yeah, what\nabout attention? So why could it be useful\nas a fundamental building",
    "start": "822930",
    "end": "831600"
  },
  {
    "start": "825000",
    "end": "982000"
  },
  {
    "text": "block instead of sort of\nsugar on top of our LSTMs? So just to recall some of\nthe intuitions of attention,",
    "start": "831600",
    "end": "839820"
  },
  {
    "text": "it treats a word's\nrepresentation as a query and it looks somewhere and tries\nto sort of access information",
    "start": "839820",
    "end": "848100"
  },
  {
    "text": "from a set of values, right? So we had a word\nrepresentation in our decoder in our machine translation\nsystems, the set of values",
    "start": "848100",
    "end": "854790"
  },
  {
    "text": "were all of the encoder states\nfor the source sentence.",
    "start": "854790",
    "end": "859810"
  },
  {
    "text": "And today we'll think\nabout instead of attention from the decoder to the encoder,\nwe'll think about attention",
    "start": "859810",
    "end": "866080"
  },
  {
    "text": "within a single sentence. So just a very\nquick picture of it,",
    "start": "866080",
    "end": "871366"
  },
  {
    "text": "you've got your\nembedding layer again, I'm putting the\ncomputational dependence counts here so all\nof these sort of",
    "start": "871367",
    "end": "876699"
  },
  {
    "text": "can be done in parallel for\nthe embedding layer again. And now you're doing\nattention, right?",
    "start": "876700",
    "end": "882160"
  },
  {
    "text": "So you're kind of looking\nat every single word in the embedding layer\nto attend to this word,",
    "start": "882160",
    "end": "887860"
  },
  {
    "text": "and I'm omitting a\nbunch of arrows here, so these are all arrows. All words interact\nwith all words",
    "start": "887860",
    "end": "893833"
  },
  {
    "text": "and we'll get deep into\nthis today, I promise. But I just wanted to make this\na little bit less dense looking",
    "start": "893833",
    "end": "899800"
  },
  {
    "text": "of a graph. And then so in the second\nlayer, again all pairs of words interact and this is\nall parallelizable.",
    "start": "899800",
    "end": "906820"
  },
  {
    "text": "So you can't parallelize\nin depth, right, because you need to encode\nthis layer before you",
    "start": "906820",
    "end": "912100"
  },
  {
    "text": "can do that layer but in\ntime, it is parallelizable, so it checks that box.",
    "start": "912100",
    "end": "918470"
  },
  {
    "text": "So again, we have O(1) sort\nof computational dependence,",
    "start": "918470",
    "end": "924339"
  },
  {
    "text": "a number of\nunparallelizable operations as a function of sequence length\nand as an added benefit, right,",
    "start": "924340",
    "end": "931960"
  },
  {
    "text": "the interaction distance\nbetween words is O(1) as well. So whereas before we\nhad recurrent networks",
    "start": "931960",
    "end": "939070"
  },
  {
    "text": "where if you are far, so t is\nthe last word in the sentence, you could have O(t)\noperations between you",
    "start": "939070",
    "end": "946600"
  },
  {
    "text": "and a far away word. With attention, you\ninteract immediately, that's the very first layer, you\nget to see your far away word.",
    "start": "946600",
    "end": "953430"
  },
  {
    "text": "And so that's O(1). And this ends up being seemingly\nfascinatingly powerful,",
    "start": "953430",
    "end": "959350"
  },
  {
    "text": "and we'll get into a\nlot of details today. OK, so this is sort of why\nattention solves the two",
    "start": "959350",
    "end": "969150"
  },
  {
    "text": "problems that we brought up\nwith recurrent neural networks but with our\nempiricist hats on, it",
    "start": "969150",
    "end": "974850"
  },
  {
    "text": "shouldn't be proof yet that it\nshould be a good building block and in fact, it takes a\nlittle bit of thinking",
    "start": "974850",
    "end": "979890"
  },
  {
    "text": "to think about how to turn\nattention into a building block like RNNs were. So let's start by digging\nright into just the equations",
    "start": "979890",
    "end": "989310"
  },
  {
    "start": "982000",
    "end": "1324000"
  },
  {
    "text": "for self attention,\nwhich again is attention to whether everything is\nlooking within itself, we'll formalize this for you.",
    "start": "989310",
    "end": "996040"
  },
  {
    "text": "So we're going to be talking\nall lecture today about queries, keys and values. ",
    "start": "996040",
    "end": "1002840"
  },
  {
    "text": "Our queries are going to\nbe a set of t queries, each query is a\nvector in dimension d.",
    "start": "1002840",
    "end": "1008930"
  },
  {
    "text": "You can just think of them as\njust those vectors right now, not worrying necessarily\nabout where they came from.",
    "start": "1008930",
    "end": "1015600"
  },
  {
    "text": "We have a set of keys, k1 to kt. Again, each vector k\nis in dimensionality d,",
    "start": "1015600",
    "end": "1022700"
  },
  {
    "text": "and we have some\nvalues, each value is going to be also\nin dimensionality d.",
    "start": "1022700",
    "end": "1028010"
  },
  {
    "text": "And for now, we're\ngoing to assume that we have the same number\nof all of them, that's",
    "start": "1028010",
    "end": "1033530"
  },
  {
    "text": "not necessarily the case later. So in self attention, the\nkeys, queries and values",
    "start": "1033530",
    "end": "1039949"
  },
  {
    "text": "come from the same\nsource of information, the same sentence for example.",
    "start": "1039950",
    "end": "1045289"
  },
  {
    "text": "And so yeah, in\npractice when they all come from the same\nsentence, right, this is going to be the\nsame number of all of them,",
    "start": "1045290",
    "end": "1051060"
  },
  {
    "text": "it's all going to\nbe t, in practice, you can have the numbers differ. So where do these come from?",
    "start": "1051060",
    "end": "1057539"
  },
  {
    "text": "We'll get into the specifics\nof this later but for now, think about the output\nof the previous layer.",
    "start": "1057540",
    "end": "1062940"
  },
  {
    "text": "So imagine the\noutput is you have like the embedding\nlayer, right, and that's the input to something that's\ngoing to do self attention.",
    "start": "1062940",
    "end": "1070100"
  },
  {
    "text": "Think of all of these\noutputs or the embeddings as some vectors xi.",
    "start": "1070100",
    "end": "1077700"
  },
  {
    "text": "And now we can just say that\nthe value is equal to the key, is equal to the query,\nis equal to that xi.",
    "start": "1077700",
    "end": "1084498"
  },
  {
    "text": "So we're going to use the\nsame vectors for all of them, but labeling them as\nkeys, queries and values,",
    "start": "1084498",
    "end": "1089940"
  },
  {
    "text": "I promise will be very useful\nin how we sort of think about what's going on and how\nwe look at the equations that",
    "start": "1089940",
    "end": "1095820"
  },
  {
    "text": "implement this. So self attention\npretty generally",
    "start": "1095820",
    "end": "1102150"
  },
  {
    "text": "but with this dot product,\nso dot product self attention here is just the math. Math is, you compute key, query\naffinities and the dot product",
    "start": "1102150",
    "end": "1111000"
  },
  {
    "text": "bit is the fact that\nyou're using the product function here. So you take a dot product\nfor all pairs i and j of qi",
    "start": "1111000",
    "end": "1119879"
  },
  {
    "text": "dotted with kg. So that is a T by\nT matrix, capital T, right, by T\nmatrix of affinities.",
    "start": "1119880",
    "end": "1128400"
  },
  {
    "text": "These are scalar values,\nnot bounded in size. Next you compute the\nattention weights.",
    "start": "1128400",
    "end": "1134600"
  },
  {
    "text": "We saw this as well using\nthe softmax function. I've just written out the\nsoftmax function here. So you exponentiate the\naffinity and then you",
    "start": "1134600",
    "end": "1142870"
  },
  {
    "text": "sum over, in this\ncase, right, you're summing over all of the keys.",
    "start": "1142870",
    "end": "1149000"
  },
  {
    "text": "So you've got a given query\nand you're summing overall all the keys for the normalization. So where should this\nquery be looking?",
    "start": "1149000",
    "end": "1155360"
  },
  {
    "text": "Remember you've got T\ndifferent queries that were doing this for here.",
    "start": "1155360",
    "end": "1160760"
  },
  {
    "text": "And so for a given query,\nyou sum over all the keys to get your\nnormalization constant, normalizing by that gives you a\ndistribution over the sequence",
    "start": "1160760",
    "end": "1170780"
  },
  {
    "text": "length T. So now you\nhave sort of a weight on all of the indices.",
    "start": "1170780",
    "end": "1175850"
  },
  {
    "text": "And again, we do our\nweighted average, right? So we've got our\nweights for our average",
    "start": "1175850",
    "end": "1181910"
  },
  {
    "text": "and then the output,\nright, there's going to be one\noutput per query. And the output is the\nweights for that multiplied",
    "start": "1181910",
    "end": "1189129"
  },
  {
    "text": "by the value vectors, right? So again if you set the\nkeys, the queries, the values",
    "start": "1189130",
    "end": "1194830"
  },
  {
    "text": "to all be x, this\nmakes sense, but it's nice to have the qs and the\nks to know sort of, which",
    "start": "1194830",
    "end": "1202577"
  },
  {
    "text": "thing is doing what? You can think of\nthe query as being sort of looking for\ninformation somewhere the key as interacting\nwith the query,",
    "start": "1202577",
    "end": "1210010"
  },
  {
    "text": "and then the value is the\nthing that you're actually going to weight in your\naverage and output.",
    "start": "1210010",
    "end": "1216870"
  },
  {
    "text": "John, a question you\nmight like to answer is, so if now we're connecting\neverything to everything,",
    "start": "1216870",
    "end": "1223020"
  },
  {
    "text": "how is this different to\nusing a fully connected layer? That's a great question.",
    "start": "1223020",
    "end": "1229340"
  },
  {
    "text": "A couple of reasons. One is that unlike a\nfully connected layer, you get to learn the\ninteraction weights.",
    "start": "1229340",
    "end": "1238580"
  },
  {
    "text": "Well, the interaction\nweights are dynamic as a function of what\nthe actual values here are,",
    "start": "1238580",
    "end": "1244610"
  },
  {
    "text": "right? So in a fully\nconnected layer, you have these weights that\nyou're learning slowly over the course of the\ntraining your network, that",
    "start": "1244610",
    "end": "1251809"
  },
  {
    "text": "allow you to say sort\nof which hidden units you should be looking at. In attention, it's the\nactual interactions",
    "start": "1251810",
    "end": "1257720"
  },
  {
    "text": "between the key and\nthe query vectors which are dependent on\nthe actual content, that",
    "start": "1257720",
    "end": "1263360"
  },
  {
    "text": "are allowed to vary by time. And so the actual strengths\nof all the interactions, of all the sort of\nattention weights, which",
    "start": "1263360",
    "end": "1270259"
  },
  {
    "text": "you could think of as connected\nto the weights in the fully connected layer, are\nallowed to change",
    "start": "1270260",
    "end": "1276860"
  },
  {
    "text": "as a function of the input. A separate thing, is\nthat the parametization is much different. So you're not learning an\nindependent connection weight",
    "start": "1276860",
    "end": "1283812"
  },
  {
    "text": "for all pairs of\nthings, instead you're allowed to parameterize the\nattention as these sort of dot",
    "start": "1283812",
    "end": "1294320"
  },
  {
    "text": "product functions\nbetween vectors that are representations,\nand you end up having the parameters work\nout more nicely, which",
    "start": "1294320",
    "end": "1303613"
  },
  {
    "text": "we'll see later. We haven't gone into\nhow we're paramitizing these functions yet. So those are the two\nanswers I'd say, is one, is",
    "start": "1303613",
    "end": "1309230"
  },
  {
    "text": "you have this sort of\ndynamic connectivity and two,",
    "start": "1309230",
    "end": "1314360"
  },
  {
    "text": "it has this\ninductive bias that's not just connect everything\nto everything feed forward. ",
    "start": "1314360",
    "end": "1322350"
  },
  {
    "text": "Great. OK, I think that's a very\ninteresting question. Yeah, so I'm glad you asked it.",
    "start": "1322350",
    "end": "1329250"
  },
  {
    "start": "1324000",
    "end": "1448000"
  },
  {
    "text": "OK, so we've talked\nabout self attention now, the equations are going\nto self attention. But can we just use this\nas a building block?",
    "start": "1329250",
    "end": "1337020"
  },
  {
    "text": "I mean, you take\nall of your LSTMs, throw them out, use the self\nattention that we've just defined instead, why not?",
    "start": "1337020",
    "end": "1342419"
  },
  {
    "text": "Well, here's a couple\nof reasons why. So look at self attention\nas a building block.",
    "start": "1342420",
    "end": "1348530"
  },
  {
    "text": "So we have some words in\na sentence, The chef who, some stuff, long sentence,\nfood is the last word",
    "start": "1348530",
    "end": "1356160"
  },
  {
    "text": "of the sentence, OK. And they have an embedding\nand from that, you",
    "start": "1356160",
    "end": "1361200"
  },
  {
    "text": "get your key, query, and value. We've said so far,\nright, there's the same vector actually, but\nkey, query, value, key, query,",
    "start": "1361200",
    "end": "1367980"
  },
  {
    "text": "value, key, query, value. And we might stack\nthem like LSTM layers. So you have key, query\nvalue perform self attention",
    "start": "1367980",
    "end": "1375450"
  },
  {
    "text": "on the key's queries and values. As we said, self\nattention is a function on keys, queries and values. So perform self attention\nnow that you have these, get",
    "start": "1375450",
    "end": "1382769"
  },
  {
    "text": "new keys, queries,\nvalues, and then perform self attention again. Look, this a lot\nlike stacking LSTMs.",
    "start": "1382770",
    "end": "1392323"
  },
  {
    "text": "But it actually has a\nfew issues as it stands. So we're going to need\nto go on a journey to determine what's missing\nfrom our self attention.",
    "start": "1392323",
    "end": "1398400"
  },
  {
    "text": "And the first thing\nis that self attention is an operation on sets. ",
    "start": "1398400",
    "end": "1404990"
  },
  {
    "text": "OK, so for the equations\nthat we had before, the self attention\nequation never referred to the\nindices of k, q or v,",
    "start": "1404990",
    "end": "1413390"
  },
  {
    "text": "except to sort of\nsay which pairs were interacting with each other. It doesn't know what the\norder of your sentences.",
    "start": "1413390",
    "end": "1419270"
  },
  {
    "text": "When it's computing though,\nthe weights it has no idea. And so if I were to input this\nsentence, The chef who food,",
    "start": "1419270",
    "end": "1426170"
  },
  {
    "text": "it would be the same as if I\njust swapped though with chef and then swapped who with the,\nand it just would have no idea.",
    "start": "1426170",
    "end": "1434000"
  },
  {
    "text": "So already this is\nnot going to work because the order in which words\nappear in sentences matters.",
    "start": "1434000",
    "end": "1441030"
  },
  {
    "text": "So here's the first problem\nthat we need to work with. So I'm going to have\na list of barriers, this is just the first, we have\na whole journey ahead of us.",
    "start": "1441030",
    "end": "1447110"
  },
  {
    "text": "And then we're going to\nhave a list of solutions. So we need to represent\nthe sequence order somehow.",
    "start": "1447110",
    "end": "1452510"
  },
  {
    "start": "1448000",
    "end": "1557000"
  },
  {
    "text": "We can't just lose that\ninformation entirely because we wouldn't know what\norder the words showed up in.",
    "start": "1452510",
    "end": "1458240"
  },
  {
    "text": "So somehow if we're not going\nto change the self attention equations themselves, we need\nto encode the order in the keys,",
    "start": "1458240",
    "end": "1467060"
  },
  {
    "text": "queries and values, and let the\nnetwork sort of figure it out on its own. So think about this, we\nhave T sequence indices",
    "start": "1467060",
    "end": "1475850"
  },
  {
    "text": "and we're going to bound\nt to some finite constant, so T is never going to be\nbigger than something for us,",
    "start": "1475850",
    "end": "1481846"
  },
  {
    "text": "and we call it T. And now we're\ngoing to represent the sequence index as a vector. So pi is going to be the\nvector representing index i,",
    "start": "1481846",
    "end": "1490690"
  },
  {
    "text": "and it's going to be n\ndimensionality d just like our keys,\nqueries and values. And so we're going to have\none of these for 1 to T.",
    "start": "1490690",
    "end": "1498670"
  },
  {
    "text": "So don't worry yet\nabout what the pi or like how they're constructed,\nwe'll get right into that.",
    "start": "1498670",
    "end": "1503840"
  },
  {
    "text": "But think about this, it's easy\nto incorporate this information into our attention\nbuilding blocks.",
    "start": "1503840",
    "end": "1509170"
  },
  {
    "text": "At the first layer if\nyou let tilde v, tilde k, tilde q be our old values, keys\nand queries, we can just add.",
    "start": "1509170",
    "end": "1518490"
  },
  {
    "text": "We could do other stuff too,\nbut in practice we just add. So vi is equal to v tilde i,\nour orderless value vector,",
    "start": "1518490",
    "end": "1525990"
  },
  {
    "text": "plus pi. So this might be\nyour embedding vector",
    "start": "1525990",
    "end": "1531330"
  },
  {
    "text": "and then you add the index\nthat it's at to its vector. And you might only do\nthis at the first layer",
    "start": "1531330",
    "end": "1538110"
  },
  {
    "text": "of the network for example. So you do the same thing\nfor the query and the key. So this is something you\ncould do, in practice which",
    "start": "1538110",
    "end": "1544080"
  },
  {
    "text": "is only slightly different. But this is\nsomething that now it knows the order of the\nsequence because if these",
    "start": "1544080",
    "end": "1552799"
  },
  {
    "text": "pis you set properly\nsomehow, then now the network is able to\nfigure out what to do with it.",
    "start": "1552800",
    "end": "1558630"
  },
  {
    "start": "1557000",
    "end": "1674000"
  },
  {
    "text": "So what's one way of\nactually making this happen? ",
    "start": "1558630",
    "end": "1564299"
  },
  {
    "text": "One way of making this happen\nis through the concatenation of sinusoids. And this was an interesting\ntake when the first transformers",
    "start": "1564300",
    "end": "1572039"
  },
  {
    "text": "paper came out, they used\nthey use this method. So let's dig into it. So you have varying wavelengths,\nso sinusoidal functions",
    "start": "1572040",
    "end": "1581100"
  },
  {
    "text": "in each of your dimensions. So in the first\ndimension you have this sine function\nwith a given period,",
    "start": "1581100",
    "end": "1586860"
  },
  {
    "text": "and then this cosine\nfunction with a given period, and then sort dot,\ndot, dot, you sort of change the periods until you\nget to much different periods.",
    "start": "1586860",
    "end": "1595405"
  },
  {
    "text": "And what does it look like? It looks like that. So imagine here in\nthe vertical axis,",
    "start": "1595405",
    "end": "1601760"
  },
  {
    "text": "we've got the dimensionality\nof the network, right? So this is d and then\nthis is sequence length.",
    "start": "1601760",
    "end": "1608860"
  },
  {
    "text": "And by just\nspecifying in each row is sort of one of these signs\nwith different frequencies.",
    "start": "1608860",
    "end": "1617140"
  },
  {
    "text": "And you can see how this\nis encoding position, these things have different\nvalues at different indices.",
    "start": "1617140",
    "end": "1623750"
  },
  {
    "text": "And that's pretty\ncool, I don't really know how they thought\nof it immediately.",
    "start": "1623750",
    "end": "1628900"
  },
  {
    "text": "But one cool thing about it is\nthis periodicity notion, right? The fact that the sinusoids\nhave periods that might be less",
    "start": "1628900",
    "end": "1634870"
  },
  {
    "text": "than the sequence\nlength, indicates that maybe the absolute\nposition of a word isn't so important, right,\nbecause if the period is less",
    "start": "1634870",
    "end": "1643600"
  },
  {
    "text": "than the sequence length,\nyou lose information maybe about where you are. Of course, you have the\nconcatenation of many of them.",
    "start": "1643600",
    "end": "1649809"
  },
  {
    "text": "So that's a pro. Maybe you can extrapolate\nto longer sequences because again you sort of have\nthis repetition of values,",
    "start": "1649810",
    "end": "1656920"
  },
  {
    "text": "right, because the periods\nwill when they complete, you'll see that value again.",
    "start": "1656920",
    "end": "1661990"
  },
  {
    "text": "The cons are that\nit's not learnable. I mean, this is cool\nbut you can't, there's no learnable parameters\nin any of this.",
    "start": "1661990",
    "end": "1668150"
  },
  {
    "text": "And also the extrapolation\ndoesn't really work. So this is an interesting\nand definitely still done,",
    "start": "1668150",
    "end": "1673179"
  },
  {
    "text": "but what's done more frequently\nnow is, what do we do? We learn to position\nrepresentations from scratch.",
    "start": "1673180",
    "end": "1679580"
  },
  {
    "start": "1674000",
    "end": "1757000"
  },
  {
    "text": "So we're going to learn\nthem from scratch.",
    "start": "1679580",
    "end": "1685370"
  },
  {
    "text": "So let all the pi just\nbe learnable parameters. So what we're going to do is\nwe can have a matrix p, that's",
    "start": "1685370",
    "end": "1691299"
  },
  {
    "text": "going to be in dimensionality\nd, dimensionality over network again by the sequence length. So this is just a big matrix,\nright, of the size here,",
    "start": "1691300",
    "end": "1700630"
  },
  {
    "text": "of this size effectively,\nd by sequence length. But every single\nvalue in that matrix",
    "start": "1700630",
    "end": "1706240"
  },
  {
    "text": "is just a learnable parameter. Pros, flexibility,\nnow you get to learn what positions is sort of\nsupposed to mean according",
    "start": "1706240",
    "end": "1713080"
  },
  {
    "text": "to your data end to end. So that's cool. Cons, you definitely can't\nextrapolate the indices",
    "start": "1713080",
    "end": "1719920"
  },
  {
    "text": "outside 1 to T,\ngreat, because you set the size of this parameter\nmatrix at the beginning and you learned them all.",
    "start": "1719920",
    "end": "1725620"
  },
  {
    "text": "Now if you want to\ngo beyond position T, you have no way to\nrepresent it effectively.",
    "start": "1725620",
    "end": "1731950"
  },
  {
    "text": "But most systems use\nthis, this is super useful and sometimes people try\nmore flexible representations",
    "start": "1731950",
    "end": "1737679"
  },
  {
    "text": "of position because again,\nthe absolute index of a word is not sort of its natural\nrepresentation of its position",
    "start": "1737680",
    "end": "1746410"
  },
  {
    "text": "in the sentence, and\nso people have looked at kind of the relative\nposition between words as well as position\nrepresentations that",
    "start": "1746410",
    "end": "1753010"
  },
  {
    "text": "depend on syntax,\nbut we're not going to be able to go too\nfar into those today.",
    "start": "1753010",
    "end": "1758800"
  },
  {
    "start": "1757000",
    "end": "1909000"
  },
  {
    "text": "OK, so that was\nproblem one, right? No matter what we did, if we\ndidn't have representation of position, there was no way\nwe could use self attention",
    "start": "1758800",
    "end": "1765945"
  },
  {
    "text": "as our new building block. And we've solved it with\nposition representations that we just sort of\nadd to the inputs.",
    "start": "1765945",
    "end": "1772840"
  },
  {
    "text": "Next, we're going to see\nthis problem that you don't have nonlinearities. We've been saying\nnonlinearities,",
    "start": "1772840",
    "end": "1779890"
  },
  {
    "text": "abstract features, they're\ngreat deep learning, end to end learning of\nrepresentations is awesome.",
    "start": "1779890",
    "end": "1786430"
  },
  {
    "text": "But right now, we're just\ndoing weighted averages. And so what is our\nsolution going to be?",
    "start": "1786430",
    "end": "1792110"
  },
  {
    "text": "I mean, it's not going\nto be all that complex. So all we're doing right\nnow, is re-averaging vectors.",
    "start": "1792110",
    "end": "1798820"
  },
  {
    "text": "So you've got sort of\nthe self attention here and if you just\nstacked another one, you just keep sort of averaging\nprojections of vectors.",
    "start": "1798820",
    "end": "1807519"
  },
  {
    "text": "But what if we just add\na feed forward network for every individual word? So within this layer,\neach of these feed",
    "start": "1807520",
    "end": "1814340"
  },
  {
    "text": "forward neural networks\nshares parameters, but it gets in just the output\nof self attention for this word",
    "start": "1814340",
    "end": "1821300"
  },
  {
    "text": "as we defined it, processes\nit, and admits something else.",
    "start": "1821300",
    "end": "1827400"
  },
  {
    "text": "And so you have output\ni from self attention, which we saw slides ago. Apply a feed forward layer,\nwhere you take the output,",
    "start": "1827400",
    "end": "1835460"
  },
  {
    "text": "multiply it by a matrix,\nnon-linearity of the matrix. And the intuition here you can\nthink of at least, is well,",
    "start": "1835460",
    "end": "1843420"
  },
  {
    "text": "you know, something like\nthe feed forward network processes the result of the\nattention for each thing.",
    "start": "1843420",
    "end": "1850190"
  },
  {
    "text": "But more fundamentally,\nyou needed some kind of non-linearity there\nand a feed forward network",
    "start": "1850190",
    "end": "1856160"
  },
  {
    "text": "will do a good job, OK? So that that's another\nproblem solved, easy fix.",
    "start": "1856160",
    "end": "1861530"
  },
  {
    "text": "Add a feed forward network,\nget your non-linearity, now your self attention output,\nyou can sort of process it,",
    "start": "1861530",
    "end": "1867260"
  },
  {
    "text": "have that sort of\ndepth increasing as the layers of the network\nincrease, which we know is useful.",
    "start": "1867260",
    "end": "1873929"
  },
  {
    "text": "Another problem. OK, so bear with me on this one. We don't want to look\nat the future when we're",
    "start": "1873930",
    "end": "1879890"
  },
  {
    "text": "doing language modeling, right? So language modeling,\nyou're trying to predict words in the future.",
    "start": "1879890",
    "end": "1886020"
  },
  {
    "text": "And with a recurrent model,\nit's very natural, right, like you just don't\nunroll it farther.",
    "start": "1886020",
    "end": "1892070"
  },
  {
    "text": "Once you unrolled your\nLSTM to a given word, there's sort of no way to\nhave given it to the next word",
    "start": "1892070",
    "end": "1899740"
  },
  {
    "text": "as well. But in self attention,\nwe'll see that this is a little bit trickier. So you can't cheat\nand look at the stuff",
    "start": "1899740",
    "end": "1905077"
  },
  {
    "text": "we're trying to be\npredicting because then we would train networks that\nwere totally useless. So what are we going to do?",
    "start": "1905077",
    "end": "1910549"
  },
  {
    "start": "1909000",
    "end": "2178000"
  },
  {
    "text": "We're going to mask,\nmasking is a word that's going to keep coming up. We're going to mask the\nfuture in self attention.",
    "start": "1910550",
    "end": "1916710"
  },
  {
    "text": "So in particular,\nthis is important when we have decoders, right? One of the reasons why we\ncould use bidirectional LSTMs",
    "start": "1916710",
    "end": "1923049"
  },
  {
    "text": "in our encoders was that, we\ncould see the whole source sentence in neural\nmachine translation. But when we're predicting\nthe output sentence,",
    "start": "1923050",
    "end": "1929437"
  },
  {
    "text": "right, we can't see\nthe future if we're going to train the model to\ndo the actual prediction. So to use self\nattention in a decoder,",
    "start": "1929437",
    "end": "1936370"
  },
  {
    "text": "you would mask the future. One thing that you could do, is\nyou could just every time you",
    "start": "1936370",
    "end": "1941590"
  },
  {
    "text": "compute attention, you change\nthe set of keys and values",
    "start": "1941590",
    "end": "1947110"
  },
  {
    "text": "that should be. Keys and values, to\nonly include past words. So you're sort of dynamically\nchanging the stuff",
    "start": "1947110",
    "end": "1952750"
  },
  {
    "text": "that you're attending over. But that doesn't let us\ndo stuff with tensors as well as parallelizability\nas we will see.",
    "start": "1952750",
    "end": "1960409"
  },
  {
    "text": "So we don't want to do that. Instead, we're going to\nmask out the future words through the attention\nweights themselves.",
    "start": "1960410",
    "end": "1966910"
  },
  {
    "text": "So in math, don't worry we'll\nget to the sort of diagram. But in math, we have\nthese attention scores,",
    "start": "1966910",
    "end": "1973120"
  },
  {
    "text": "and they were equal to\njust this dot product before, for all pairs, right? ",
    "start": "1973120",
    "end": "1979160"
  },
  {
    "text": "But now only if\nthe key is strictly less than the key\nindex, is strictly less",
    "start": "1979160",
    "end": "1986780"
  },
  {
    "text": "than, this should be i. If only if the key\nindex is strictly less than the query index, so\nthis would be j less than i,",
    "start": "1986780",
    "end": "1995330"
  },
  {
    "text": "should we let the\nnetwork look at the word and it should be negative\ninfinity otherwise, so we don't let you\nlook at the output.",
    "start": "1995330",
    "end": "2002340"
  },
  {
    "text": "So let's go to the picture. For encoding the words\nthat we'll see here, so maybe we'll\nhave a start token.",
    "start": "2002340",
    "end": "2009170"
  },
  {
    "text": "You want to decide this is\nyour whole sentence now. You want to decide which\nwords in the sentence",
    "start": "2009170",
    "end": "2014240"
  },
  {
    "text": "you're allowed to look at\nwhen making your predictions. So I want to predict\nthe first word.",
    "start": "2014240",
    "end": "2021000"
  },
  {
    "text": "And in order to\nprotect The, I'm not allowed to look at the word The. I'm also not allowed to look\nat any of the future words.",
    "start": "2021000",
    "end": "2029010"
  },
  {
    "text": "I am allowed to look\nat the word Start. So this kind of block\nis not shaded here.",
    "start": "2029010",
    "end": "2036150"
  },
  {
    "text": "In order to predict\nthe word Chef, I can look at Start and The. Start, The but not chef\nnaturally, or the word",
    "start": "2036150",
    "end": "2044130"
  },
  {
    "text": "that comes after it. And likewise for\nthe other words. So you can see this sort\nof matrix here, right?",
    "start": "2044130",
    "end": "2050888"
  },
  {
    "text": "So we just want to make sure\nthat our attention weights are 0 everywhere here.",
    "start": "2050889",
    "end": "2056649"
  },
  {
    "text": "And so in the\naffinity's calculation, we add negative infinity to all\nof these, in this big matrix.",
    "start": "2056650",
    "end": "2064560"
  },
  {
    "text": "And that guarantees that we\ncan't look to the future. OK, so now we can do big\nmatrix multiplications",
    "start": "2064560",
    "end": "2072440"
  },
  {
    "text": "to compute our attention\nas we will see, and we sort of don't worry\nabout looking at the future",
    "start": "2072440",
    "end": "2077989"
  },
  {
    "text": "because we've added these\nnegative infinities. And that's the last problem\nwith self attention,",
    "start": "2077989",
    "end": "2084199"
  },
  {
    "text": "sort of that comes\nup fundamentally as like, what do we need\nfor this building block?",
    "start": "2084199",
    "end": "2090760"
  },
  {
    "text": "You didn't have an\ninherent notion of order, now you have a good\nnotion of order or at least something\nof a notion of order.",
    "start": "2090760",
    "end": "2097390"
  },
  {
    "text": "You didn't have nonlinearities,\nadd feed forward networks, and then you didn't want\nto look at the future,",
    "start": "2097390",
    "end": "2104829"
  },
  {
    "text": "you add the masks\nfor the decoders. ",
    "start": "2104830",
    "end": "2110530"
  },
  {
    "text": "So self attention is the basis\nof any self attention based building block, hello, position\nrepresentations are useful,",
    "start": "2110530",
    "end": "2117430"
  },
  {
    "text": "nonlinearities are good. You don't have to use a\nfeed forward network, right, like you could have just\ndone other stuff I guess,",
    "start": "2117430",
    "end": "2124329"
  },
  {
    "text": "but in practice actually\nit's really easy to parallelize these feed\nforward networks as well. So we end up doing that.",
    "start": "2124330",
    "end": "2130839"
  },
  {
    "text": "And then the masking, Yeah,\nyou don't want information to leak from the future to\nthe past in your decoder.",
    "start": "2130840",
    "end": "2139150"
  },
  {
    "text": "So let me be clear, we haven't\ntalked about the transformer yet.",
    "start": "2139150",
    "end": "2144630"
  },
  {
    "text": "But this is all you would need\nif you were thinking like gosh, what do I need in order to\nbuild my self attention building",
    "start": "2144630",
    "end": "2149672"
  },
  {
    "text": "block? We'll see that there are a lot\nmore details in the transformer that we're going to spend\nthe rest of the lecture going through.",
    "start": "2149672",
    "end": "2155090"
  },
  {
    "text": " But I want you to\nsort of at least as you're thinking about\nwhat's going to come next",
    "start": "2155090",
    "end": "2162000"
  },
  {
    "text": "after the transformer and how\nyou're going to invent it, think about the\nfact that these are",
    "start": "2162000",
    "end": "2167250"
  },
  {
    "text": "the things that were necessary. And then the other things end\nup being very, very important it turns out.",
    "start": "2167250",
    "end": "2173010"
  },
  {
    "text": "But there's a lot\nof design space here that hasn't been explored yet.",
    "start": "2173010",
    "end": "2179290"
  },
  {
    "start": "2178000",
    "end": "2343000"
  },
  {
    "text": "OK, so let's talk about\nthe transformer model. And I'm going to pause there\nand this is a good question, I can take it now.",
    "start": "2179290",
    "end": "2185350"
  },
  {
    "start": "2185350",
    "end": "2190940"
  },
  {
    "text": "OK, so transformers,\nlet's get to it.",
    "start": "2190940",
    "end": "2197310"
  },
  {
    "text": "Let's look at the\ntransformer, encoder, decoder blocks at\na high level first. This should look a lot\nlike the encoder, decoders",
    "start": "2197310",
    "end": "2203810"
  },
  {
    "text": "that we saw with the recurrent\nneural network, machine",
    "start": "2203810",
    "end": "2210460"
  },
  {
    "text": "translation systems that we saw. OK, so we have our\nword embeddings, we're going to add in our\nposition representations,",
    "start": "2210460",
    "end": "2216840"
  },
  {
    "text": "we saw that, and that's\nfrom our input sequence. We'll have a sequence of\nencoder blocks, each of them",
    "start": "2216840",
    "end": "2222820"
  },
  {
    "text": "is called a transformer encoder. And then we have our output\nsequence, word embeddings,",
    "start": "2222820",
    "end": "2228460"
  },
  {
    "text": "position representation again,\nwe have a transformer decoder. ",
    "start": "2228460",
    "end": "2235040"
  },
  {
    "text": "The last layer of\nencoders is going to be used in each layer\nof the transformer decoder,",
    "start": "2235040",
    "end": "2242200"
  },
  {
    "text": "and then we get some\noutputs, some predictions. OK, so this looks\npretty much the same at a very high level,\nmaybe minus the fact",
    "start": "2242200",
    "end": "2248010"
  },
  {
    "text": "that now we need to do the\nposition representation edition at the very beginning.",
    "start": "2248010",
    "end": "2253180"
  },
  {
    "text": "So now let's look at\nthese blocks themselves. So the encoder and\ndecoder blocks. What's left that we\nhaven't covered, right?",
    "start": "2253180",
    "end": "2259060"
  },
  {
    "text": "Because we could\njust put the building blocks that we just came up\nwith in the first part of class in these things, right?",
    "start": "2259060",
    "end": "2265290"
  },
  {
    "text": "Encoders, we need\nour self attention, our feed forward networks,\nwe have our position",
    "start": "2265290",
    "end": "2270750"
  },
  {
    "text": "representations, we get the\nmasking for the decoders, right, we can just\nslot these in. But it turns out they wouldn't\nwork all that well compared",
    "start": "2270750",
    "end": "2277890"
  },
  {
    "text": "to transformers. So what's left? So the first thing is key,\nquery, value attention.",
    "start": "2277890",
    "end": "2283080"
  },
  {
    "text": "This is a specific way\nof getting the k, q and v vectors from the single\nword embedding, right?",
    "start": "2283080",
    "end": "2288660"
  },
  {
    "text": "So instead of having k, q and\nv equal to x like the output from the last layer, we're\ngoing to do something",
    "start": "2288660",
    "end": "2295200"
  },
  {
    "text": "a little bit more. Next is multi-headed attention. We're going to attend\nto multiple places",
    "start": "2295200",
    "end": "2300270"
  },
  {
    "text": "in a single layer, and\nwe'll see that gets us sort kind of interesting\nproperties in the homework",
    "start": "2300270",
    "end": "2306810"
  },
  {
    "text": "later on, but we'll talk a\nlittle bit about it today. And then there's\na bunch of things that just help with training.",
    "start": "2306810",
    "end": "2312180"
  },
  {
    "text": "These seemed like they were\nvery hard to train at first, a lot of these tricks\nare very useful. So we'll talk about\nresidual connections,",
    "start": "2312180",
    "end": "2318218"
  },
  {
    "text": "layer normalization and\nscaling the dot product. Everything in bullet\npoint three here,",
    "start": "2318218",
    "end": "2323460"
  },
  {
    "text": "tricks to help\nwith training don't improve what the\nmodel is able to do, but they're crucial in that they\nimprove the training process.",
    "start": "2323460",
    "end": "2331070"
  },
  {
    "text": "So modeling improvements\nof both kinds are really, really important. So it's good that we're\nusing self attention which",
    "start": "2331070",
    "end": "2338050"
  },
  {
    "text": "is this cool thing that\nhad these properties. But if we couldn't train\nit, it wouldn't be useful. OK, so here's how the\ntransformer builds the key,",
    "start": "2338050",
    "end": "2347270"
  },
  {
    "start": "2343000",
    "end": "2450000"
  },
  {
    "text": "query, and value vectors. ",
    "start": "2347270",
    "end": "2352460"
  },
  {
    "text": "We have x1 to xt,\nthe input vectors to our transformer layer.",
    "start": "2352460",
    "end": "2358730"
  },
  {
    "text": "OK, and we're gonna go ahead\nand put the transformer encoder here. So we have one of these\nvectors per word, you can say.",
    "start": "2358730",
    "end": "2366529"
  },
  {
    "text": "And again, each\nxi is going to be a vector in dimensionality d. And here's how we compute\nthe keys, queries and values.",
    "start": "2366530",
    "end": "2374059"
  },
  {
    "text": "We're going to let each key\nki, which we saw before, be equal to some matrix k times\nxi where k is d by d, right?",
    "start": "2374060",
    "end": "2382910"
  },
  {
    "text": "So this is a transformation\nfrom dimensionality d to dimensionality d.",
    "start": "2382910",
    "end": "2387920"
  },
  {
    "text": "We're going to call\nthis the key matrix k, and we're going to do the\nsame thing for the queries. OK, so we're going to take the\nxi, multiply it by a matrix,",
    "start": "2387920",
    "end": "2395690"
  },
  {
    "text": "get the query vector, and we'll\ndo the same thing for v. OK, so you can just\nplug this in, right?",
    "start": "2395690",
    "end": "2402110"
  },
  {
    "text": "Now instead of saying that\nall the k, q and the v are all the same\nas x, they all are slightly different because you\napply a linear transformation.",
    "start": "2402110",
    "end": "2409070"
  },
  {
    "text": "What does this do? Well, you can think about it\nas like well, the matrices k, q",
    "start": "2409070",
    "end": "2415010"
  },
  {
    "text": "and v can be very different\nfrom each other, right? And so they sort of\nemphasize or allow",
    "start": "2415010",
    "end": "2421309"
  },
  {
    "text": "different aspects\nof the x vectors is to be used in each of\nthe three roles, right?",
    "start": "2421310",
    "end": "2426380"
  },
  {
    "text": "So we wrote out the\nself attention equations with the three roles to indicate\nthe different things are being done with each of them.",
    "start": "2426380",
    "end": "2431970"
  },
  {
    "text": "So maybe k and q are helping\nyou figure out where to look, and so they should\nbe a certain way,",
    "start": "2431970",
    "end": "2438322"
  },
  {
    "text": "they should look at\ndifferent parts of x. And then v the\nvalue, maybe you want to pass along a\ndifferent information",
    "start": "2438322",
    "end": "2444793"
  },
  {
    "text": "than the thing\nthat actually helps you access that information. So this is important.",
    "start": "2444793",
    "end": "2451170"
  },
  {
    "text": "How do we do this? In practice, we compute it\nwith really big tensors. So we had our X\nvectors, which is",
    "start": "2451170",
    "end": "2457164"
  },
  {
    "text": "what we've been talking\nabout sort of word by word, is where you had the\nsequence xi, x1 to xt.",
    "start": "2457165",
    "end": "2462355"
  },
  {
    "text": "Now we're going\nto represent them all in a matrix X, which\nis in our sequence length",
    "start": "2462355",
    "end": "2467540"
  },
  {
    "text": "bidimensionality. So sequence length\nby d, capital T by d.",
    "start": "2467540",
    "end": "2473060"
  },
  {
    "text": "And now if we have the\nmatrix for each of our key, query, and value, right,\nwe're going to apply,",
    "start": "2473060",
    "end": "2478773"
  },
  {
    "text": "like we're going to\nlook at these things XK, XQ and XV, which are all\nof the same dimensionality as x",
    "start": "2478773",
    "end": "2485359"
  },
  {
    "text": "because of the d by\nd transformations. So how do we compute\nself attention?",
    "start": "2485360",
    "end": "2490940"
  },
  {
    "text": "We have our output tensor,\nwhich is the same dimensionality as the input x. This is going to be\nequal to soft max,",
    "start": "2490940",
    "end": "2497090"
  },
  {
    "text": "there's a soft-max of this\nmatrix multiplication, which we'll get into,\ntimes the value vector.",
    "start": "2497090",
    "end": "2503080"
  },
  {
    "text": "So the matrix multiplication\nhere is computing affinities between keys and\nqueries we'll see, and then here's our averaging.",
    "start": "2503080",
    "end": "2509150"
  },
  {
    "text": "What does that look\nlike pictorially? So you take the key,\nquery dot products. So this term here,\nXQ, XK transpose,",
    "start": "2509150",
    "end": "2518240"
  },
  {
    "text": "is giving you all\ndot products, all T by T pairs of\nattention scores. So our e, i, j are in this\nmatrix right here, it's T by T",
    "start": "2518240",
    "end": "2526400"
  },
  {
    "text": "and this is just a big\nmatrix multiplication. So you do the matrix\nmultiplication XQ and then XK",
    "start": "2526400",
    "end": "2531800"
  },
  {
    "text": "and then you get all of the\ndot products by through this. OK, so now you have this\nbig T by T set of scores,",
    "start": "2531800",
    "end": "2538580"
  },
  {
    "text": "that's what we wanted. And now, you can soft-max\nthat directly as a matrix",
    "start": "2538580",
    "end": "2544790"
  },
  {
    "text": "and then do a matrix\nmultiplication here with XV in order to give\nyour output vector.",
    "start": "2544790",
    "end": "2550230"
  },
  {
    "text": "So this is actually doing\nthe weighted average that we saw at the\nbeginning of the class. And this, there's\nno for loops here,",
    "start": "2550230",
    "end": "2556519"
  },
  {
    "text": "it's really beautifully\nI would say vectorized and it gives us our output\nwhich again, remember,",
    "start": "2556520",
    "end": "2562020"
  },
  {
    "text": "same dimensionality, T by d. OK, so all periods\nof attention scores then compute the averages\nby applying the softmax",
    "start": "2562020",
    "end": "2570349"
  },
  {
    "text": "of the scores to the XV matrix. ",
    "start": "2570350",
    "end": "2578660"
  },
  {
    "start": "2577000",
    "end": "2782000"
  },
  {
    "text": "So that's it for key,\nquery, value attention, that's how we implement\nit with tensors.",
    "start": "2578660",
    "end": "2584120"
  },
  {
    "text": "Next we'll look at the\nnext thing that ends up being quite important\nfor training transformers",
    "start": "2584120",
    "end": "2589460"
  },
  {
    "text": "in practice, which is\nmulti-headed attention. So transformer encoder,\nmulti-headed attention.",
    "start": "2589460",
    "end": "2594869"
  },
  {
    "text": "So the question\nis, what if we want to look at multiple places\nin the sentence at once?",
    "start": "2594870",
    "end": "2600529"
  },
  {
    "text": "It's possible to do that\nwith normal self attention. But think about this,\nwhere do you end up",
    "start": "2600530",
    "end": "2606920"
  },
  {
    "text": "looking in self attention? You end up looking where the dot\nproducts of Xi, your Q matrix",
    "start": "2606920",
    "end": "2614580"
  },
  {
    "text": "transpose your K\nmatrix, XJ is high. So those are the Xij\npairs, I'm sorry,",
    "start": "2614580",
    "end": "2620876"
  },
  {
    "text": "those are the ij pairs that end\nup interacting with each other. But maybe for some\nquery, for some word",
    "start": "2620876",
    "end": "2627860"
  },
  {
    "text": "that you want to focus\non different other words in the sentence for\ndifferent reasons. The way that you\ncan encode this,",
    "start": "2627860",
    "end": "2634030"
  },
  {
    "text": "is by having multiple query,\nkey and value matrices, which all encode different\nthings about the Xi,",
    "start": "2634030",
    "end": "2640840"
  },
  {
    "text": "they all learn different\ntransformations. So instead of a single Q,\na single K and a single V,",
    "start": "2640840",
    "end": "2646375"
  },
  {
    "text": "what we get are a Q sub\nL, K sub L, V sub L, all of a different n\ndimensionality now.",
    "start": "2646375",
    "end": "2653020"
  },
  {
    "text": "So by their dimensionality\nd by d over h, where h is the number of heads.",
    "start": "2653020",
    "end": "2658250"
  },
  {
    "text": "So they're going to still\napply to the X matrix, but they're going to\ntransform it to a smaller",
    "start": "2658250",
    "end": "2663640"
  },
  {
    "text": "dimensionality, d by h. And then each attention head\nis going to perform attention",
    "start": "2663640",
    "end": "2669840"
  },
  {
    "text": "independently, it's\nlike you just did it a whole bunch of times, right? And so output l is equal to\nsoftmax of, here's your QK",
    "start": "2669840",
    "end": "2677640"
  },
  {
    "text": "but now it's in\nl form, times XVl and now you have sort of\nthese indexed outputs.",
    "start": "2677640",
    "end": "2686339"
  },
  {
    "text": "And in order to sort of have\nthe output dimensionality be equal to the input\ndimensionality and sort of mix",
    "start": "2686340",
    "end": "2691470"
  },
  {
    "text": "things around, combine\nall the information from the different heads,\nyou concatenate the heads.",
    "start": "2691470",
    "end": "2697230"
  },
  {
    "text": "So that's output 1 through\noutput h, stack them together. Now, the dimensionality\nof this, is",
    "start": "2697230",
    "end": "2702690"
  },
  {
    "text": "equal to the\ndimensionality of X again. And then we use a\nlearned matrix Y in order to sort of do the mixing\nY is d by d, and that's",
    "start": "2702690",
    "end": "2711510"
  },
  {
    "text": "the output of multi-headed\nattention, multi-headed self attention.",
    "start": "2711510",
    "end": "2716990"
  },
  {
    "text": "And so each head gets to\nlook at different things, right, because they\ncan all sort of,",
    "start": "2716990",
    "end": "2722990"
  },
  {
    "text": "the linear\ntransformations you can stay focused on different\nparts of the X vectors, and the value vectors also\nget to be different as well.",
    "start": "2722990",
    "end": "2733060"
  },
  {
    "text": "So pictorially, this\nis what we had before, single headed attention. You had X multiplied by\nQ in order to get XQ.",
    "start": "2733060",
    "end": "2741869"
  },
  {
    "text": "And what's interesting\nand you can see this, you can see this from\nthis diagram I think, is that multi-headed\nattention doesn't necessarily",
    "start": "2741870",
    "end": "2748770"
  },
  {
    "text": "have to be more work, right? We saw that the Q,\nK and V matrices",
    "start": "2748770",
    "end": "2757140"
  },
  {
    "text": "in multi-head attention, have\na lower output dimensionality. So here's two of\nthem right here, here's Q1 and Q2,\nthe same size as Q,",
    "start": "2757140",
    "end": "2765923"
  },
  {
    "text": "and then you hit\noutputs XQ1 and XQ2. And so you're effectively doing\nthe same amount of computation",
    "start": "2765924",
    "end": "2772040"
  },
  {
    "text": "as before, but now\nyou're sort of doing, you have different\nattention distributions for each of the different\nheads, so this is pretty cool.",
    "start": "2772040",
    "end": "2779599"
  },
  {
    "text": "OK, so those are the main\nmodeling differences, right?",
    "start": "2779600",
    "end": "2786200"
  },
  {
    "start": "2782000",
    "end": "3024000"
  },
  {
    "text": "We did key, query\nvalue attention, that's how we got the key,\nqueries and values from the X",
    "start": "2786200",
    "end": "2797020"
  },
  {
    "text": "vectors, and we saw how to\nimplement that in the matrices that we're looking at.",
    "start": "2797020",
    "end": "2802990"
  },
  {
    "text": "And then we looked at the\nmulti-headed attention, which allows us to look\nin different places",
    "start": "2802990",
    "end": "2808530"
  },
  {
    "text": "in the sequence in order to have\nmore flexibility within a given layer.",
    "start": "2808530",
    "end": "2814390"
  },
  {
    "text": "Now we're going to talk\nabout our training tricks. These are really important,\nit turns out and so,",
    "start": "2814390",
    "end": "2820190"
  },
  {
    "text": "Yeah, thinking\nabout them I think is something that we don't\ndo enough in the field",
    "start": "2820190",
    "end": "2825420"
  },
  {
    "text": "and so let's really\nwalk through them. So residual connections,\nresidual connections have been around,\nresidual connections,",
    "start": "2825420",
    "end": "2832160"
  },
  {
    "text": "you can think of them as\nhelping the model train better for a number of reasons. Let's look at what\nthey're doing first.",
    "start": "2832160",
    "end": "2838340"
  },
  {
    "text": "Our residual connection\nlooks like this. So you have a normal\nlayer X in some layer i,",
    "start": "2838340",
    "end": "2843635"
  },
  {
    "text": "i is representing sort of the\nlayer in depth in the network.",
    "start": "2843635",
    "end": "2849090"
  },
  {
    "text": "So Xi is equal to some\nlayer of Xi minus 1. So you had, I don't know\nwhat this layer is doing",
    "start": "2849090",
    "end": "2855230"
  },
  {
    "text": "necessarily, but this layer is a\nfunction of the previous layer, OK.",
    "start": "2855230",
    "end": "2861090"
  },
  {
    "text": "And so you've got this. So again, I want\nto abstract over what the layer is doing but\nyou just pass it through.",
    "start": "2861090",
    "end": "2867590"
  },
  {
    "text": "A residual connection is\ndoing something very simple. It's saying, OK,\nI'm going to take",
    "start": "2867590",
    "end": "2873717"
  },
  {
    "text": "the function I was computing\nat my previous layer before and then I'm going to add\nit to the previous layer.",
    "start": "2873717",
    "end": "2879880"
  },
  {
    "text": "So now, Xi is not equal\nto a layer of Xi minus 1, it's equal to Xi minus 1\nplus layer of x minus 1.",
    "start": "2879880",
    "end": "2887700"
  },
  {
    "text": "This is it, these are\nresidual connections. And the intuition,\nright, is that like,",
    "start": "2887700",
    "end": "2893760"
  },
  {
    "text": "before you started\nlearning anything sort of, you have this notion that\nyou should be learning only",
    "start": "2893760",
    "end": "2900600"
  },
  {
    "text": "how layer i should be\ndifferent from layer i minus 1,",
    "start": "2900600",
    "end": "2906153"
  },
  {
    "text": "instead of learning from scratch\nwhat it should look like. So this value here\nlayer of Xi minus 1,",
    "start": "2906153",
    "end": "2911890"
  },
  {
    "text": "should be something\nin some sense and you have to learn\nhow it's different from the previous layer.",
    "start": "2911890",
    "end": "2917089"
  },
  {
    "text": "This is a sort of a\nnice inductive bias. So here you can\nkind of represent it as you have this layer Xi\nminus 1 goes through the layer,",
    "start": "2917090",
    "end": "2925360"
  },
  {
    "text": "it also goes around\nand just gets added in. Now think about the\ngradients, right? We talked about vanishing\ngradients, they're a problem.",
    "start": "2925360",
    "end": "2934660"
  },
  {
    "text": "The gradient of this\nconnection here is beautiful, right, even if\neverything's saturating,",
    "start": "2934660",
    "end": "2940960"
  },
  {
    "text": "all of your sigmoids\nare saturating or your ReLUs are all negative,\nso the gradients are all 0. But you get\ngradients propagating",
    "start": "2940960",
    "end": "2947808"
  },
  {
    "text": "back through the rest\nof the network anyway through this connection here. That's pretty cool. Turns out to be\nmassively useful.",
    "start": "2947808",
    "end": "2955400"
  },
  {
    "text": "And just to take a\nquick visualization, this plot just never ceases\nto look really interesting.",
    "start": "2955400",
    "end": "2962180"
  },
  {
    "text": "Here is sort of a visualization\nof a lost landscape.",
    "start": "2962180",
    "end": "2967280"
  },
  {
    "text": "So each sort of\npoint in the 2D plane is like it's a sort of a\nsetting of a parameters",
    "start": "2967280",
    "end": "2975020"
  },
  {
    "text": "of your network, and\nthen sort of the z-axis is the loss of the\nnetwork that it's",
    "start": "2975020",
    "end": "2980150"
  },
  {
    "text": "being optimized for, right? And here's a network\nwith no residuals, and you are stochastic\ngradient descent",
    "start": "2980150",
    "end": "2985877"
  },
  {
    "text": "and you sort of have\nto find a local minimum and it's really hard to\nfind the nice local minimum.",
    "start": "2985877",
    "end": "2991080"
  },
  {
    "text": "And then with the residual\nnetwork, it's much smoother. So you can imagine how\nstochastic gradient descent",
    "start": "2991080",
    "end": "2997760"
  },
  {
    "text": "is sort of walking down\nhere to this nice, very low local minimum. This is a paper that\nwas trying to explain",
    "start": "2997760",
    "end": "3005560"
  },
  {
    "text": "why residual connections\nare so useful. So this might be an intuition\nthat might be useful for you.",
    "start": "3005560",
    "end": "3011990"
  },
  {
    "text": "So this is the so-called\nloss landscape. So those are\nresidual connections.",
    "start": "3011990",
    "end": "3018730"
  },
  {
    "text": "And they seem simple but\na lot of simple ideas end up being super\nuseful in deep learning.",
    "start": "3018730",
    "end": "3024440"
  },
  {
    "start": "3024000",
    "end": "3230000"
  },
  {
    "text": "So in layer\nnormalization, we're doing something sort of\nsimilar, we're trying",
    "start": "3024440",
    "end": "3031180"
  },
  {
    "text": "to help the network\ntrain better but we're doing it via a pretty\ndifferent intuition.",
    "start": "3031180",
    "end": "3036910"
  },
  {
    "text": "So layer normalization\nis thought to say at different\ntimes in my network",
    "start": "3036910",
    "end": "3042790"
  },
  {
    "text": "when I'm training it\ndoing the forward pass, there's a lot of\nvariation in what the forward pass looks like.",
    "start": "3042790",
    "end": "3048859"
  },
  {
    "text": "And a lot of it is uninformative\nand that can harm training. But if we normalize\nwithin a layer",
    "start": "3048860",
    "end": "3057010"
  },
  {
    "text": "to unit, mean and\nstandard deviation, then it sort of cuts\ndown on all this sort",
    "start": "3057010",
    "end": "3063400"
  },
  {
    "text": "of uninformative variation. And the informative\nvariation, sort of how the units were different\nfrom each other is maintained.",
    "start": "3063400",
    "end": "3072200"
  },
  {
    "text": "So it's also thought\nthat the successive layer norm, and there's been a lot\nof successive layer norm, has been to actually\nto helping normalize",
    "start": "3072200",
    "end": "3080470"
  },
  {
    "text": "the gradients of each\nlayer, this is recent work. So let's talk about\nhow it's implemented.",
    "start": "3080470",
    "end": "3086570"
  },
  {
    "text": "So we're going to go back to X\nand I'm not going to index it here, so it's just X is some\nvector, some word vector",
    "start": "3086570",
    "end": "3092109"
  },
  {
    "text": "in our transformer. We're going to compute\nan estimate of the mean,",
    "start": "3092110",
    "end": "3097960"
  },
  {
    "text": "OK, just by summing\nthe hidden units. We're going to\ncompute an estimate of the standard\ndeviation similarly.",
    "start": "3097960",
    "end": "3105360"
  },
  {
    "text": "So like you've taken a single\nRd vector, you've just sum them, you compute the mean,\nyou estimate the mean,",
    "start": "3105360",
    "end": "3112289"
  },
  {
    "text": "you estimate the\nstandard deviation, OK. Now you also potentially and\nthis is optional, learn element",
    "start": "3112290",
    "end": "3123369"
  },
  {
    "text": "wise gain and bias parameters to\ntry to sort of re-scale things, if certain hidden\nunits sort of should",
    "start": "3123370",
    "end": "3129910"
  },
  {
    "text": "have larger value in general or\nshould be multiplicative larger",
    "start": "3129910",
    "end": "3135760"
  },
  {
    "text": "in general. So these are vectors in Rd,\njust like X was a vector in Rd.",
    "start": "3135760",
    "end": "3141700"
  },
  {
    "text": "And then here's what layer\nnormalization computes, you have your output,\nwhich is going to be an Rd,",
    "start": "3141700",
    "end": "3147789"
  },
  {
    "text": "just like your input, OK. And you take your vector\nX, you subtract the mean",
    "start": "3147790",
    "end": "3154570"
  },
  {
    "text": "from all of them, you divide\nby standard deviation,",
    "start": "3154570",
    "end": "3160230"
  },
  {
    "text": "should have. Yeah, sorry, this\nshouldn't be square root. And then you add\nan epsilon that's small, in order if the standard\ndeviation becomes very small,",
    "start": "3160230",
    "end": "3169270"
  },
  {
    "text": "you don't want the denominator\nto become too, too, too small because then you\nget huge numbers",
    "start": "3169270",
    "end": "3174760"
  },
  {
    "text": "and then your network goes\nto NaN and doesn't train. OK, so you have some\nsort of tolerance there.",
    "start": "3174760",
    "end": "3181810"
  },
  {
    "text": "And then so you normalize there,\nand then our element-wise gain in bias. Now remember this\nfraction X is a vector,",
    "start": "3181810",
    "end": "3188800"
  },
  {
    "text": "everything is being done sort\nof element-wise here, right? So this is Rd, and then\nyou have this element-wise",
    "start": "3188800",
    "end": "3194200"
  },
  {
    "text": "multiplication, this Hadamard\nproduct with your gain, then you add the bias.",
    "start": "3194200",
    "end": "3199849"
  },
  {
    "text": "Whether the gain and bias\nare necessary is unclear. This paper here suggests\nthat they're not helpful,",
    "start": "3199850",
    "end": "3206590"
  },
  {
    "text": "but they're frequently used. So it's sort of an engineering\nquestion at this point and a science question\nwhether we can figure out why",
    "start": "3206590",
    "end": "3214330"
  },
  {
    "text": "in general. But yes, that's\nlayer normalization and it ends up being very\nimportant in transformers,",
    "start": "3214330",
    "end": "3222789"
  },
  {
    "text": "you remove it and they\nreally don't train very well. OK, so that's our second trick.",
    "start": "3222790",
    "end": "3230830"
  },
  {
    "text": "The third trick is\nprobably the simplest one but it's useful to know.",
    "start": "3230830",
    "end": "3237280"
  },
  {
    "text": "And it's just, you can call it\nscaled dot product attention because we're going to scale\nthe dot products like so.",
    "start": "3237280",
    "end": "3244530"
  },
  {
    "text": "OK, so what we're\ngoing to do, is we're going to\nhave this intuition",
    "start": "3244530",
    "end": "3249630"
  },
  {
    "text": "that our dimensionality d in\nreally big neural networks, is going to become very large.",
    "start": "3249630",
    "end": "3256380"
  },
  {
    "text": "So maybe our hidden\nlayer in our transformer is 1,000 or 2000 or 3,000\nanyway, it gets big.",
    "start": "3256380",
    "end": "3263579"
  },
  {
    "text": "And when the dimensionality\nbecomes large, the dot products between\nvectors tend to become large.",
    "start": "3263580",
    "end": "3269520"
  },
  {
    "text": "So for example, if you take\nthe dot product between two random vectors in Rd,\nit grows quite quickly,",
    "start": "3269520",
    "end": "3276996"
  },
  {
    "text": "their dot products\ngrow quite quickly. Now are the vectors\nrandom in transformers, well, they're not\nuniform random but you",
    "start": "3276997",
    "end": "3283619"
  },
  {
    "text": "can imagine there's\nsort a lot of variation and in general as the\ndimensionality is growing, all these dot products\nare getting pretty big.",
    "start": "3283620",
    "end": "3290100"
  },
  {
    "text": "And this can become a problem\nfor the following reason, right? We're taking all these dot\nproducts directly and putting",
    "start": "3290100",
    "end": "3297090"
  },
  {
    "text": "them into the softmax. So there's this variation in the\ndot products and some of them",
    "start": "3297090",
    "end": "3302280"
  },
  {
    "text": "are very large, then the\nsoftmax can become very peaky, putting most of most\nof its probability mass",
    "start": "3302280",
    "end": "3310440"
  },
  {
    "text": "on a small number\nof things, which makes the gradient small\nfor everything else effectively, right?",
    "start": "3310440",
    "end": "3315480"
  },
  {
    "text": "Because the softmax\nis trying to be well, it's a soft argmax, right,\nso it's sort of saying, which one of these is\nlike the max or these sort",
    "start": "3315480",
    "end": "3323760"
  },
  {
    "text": "of relative to how close they\nare to the max of the function. And so if some of them are very,\nvery large, you sort of just",
    "start": "3323760",
    "end": "3330570"
  },
  {
    "text": "zero out the connections\nto everything that's not being\nattended to, that has low probability\ndistribution and then",
    "start": "3330570",
    "end": "3338310"
  },
  {
    "text": "they don't get gradients. And so here's the self attention\noperation we've seen, OK. I've taken, this is the\nmulti-headed variant here,",
    "start": "3338310",
    "end": "3345500"
  },
  {
    "text": "right, because we've got\nthe other indices on output, I've got the indices on Q, K\nand V. And all I'm going to do",
    "start": "3345500",
    "end": "3352680"
  },
  {
    "text": "is I'm going to say,\nwell, the things that I'm about to dot\ntogether are vectors",
    "start": "3352680",
    "end": "3358950"
  },
  {
    "text": "of dimensionality\nd over h, because of the multi-headed\nattention again.",
    "start": "3358950",
    "end": "3366070"
  },
  {
    "text": "And in order to stop them\nfrom growing, the dot products from growing\ntoo large, I'm just going to divide\nall of my scores.",
    "start": "3366070",
    "end": "3373260"
  },
  {
    "text": "So just remember up here, XQ,\nK top, X top is a T by T matrix",
    "start": "3373260",
    "end": "3379410"
  },
  {
    "text": "of scores, you're going to\ndivide them all by d over h. And as d grows, d\nover h grows, right,",
    "start": "3379410",
    "end": "3387869"
  },
  {
    "text": "and so your dot\nproducts don't grow, and this ends up\nbeing helpful as well. ",
    "start": "3387870",
    "end": "3395800"
  },
  {
    "text": "OK. Any questions? ",
    "start": "3395800",
    "end": "3403090"
  },
  {
    "text": "Yeah. John, could you--\ninteresting question-- when you're doing the\ndecoder attention,",
    "start": "3403090",
    "end": "3409820"
  },
  {
    "text": "do you only do the\nmasking in the first layer or do you do the masking\nalso in the middle layer,",
    "start": "3409820",
    "end": "3416346"
  },
  {
    "text": "in the decoder? Yeah, nice, nice. So if we were to only do\nmasking in the first layer,",
    "start": "3416346",
    "end": "3426819"
  },
  {
    "text": "we would get information\nleakage in the later layers. So if we look at this, if we\nwere to look at this diagram",
    "start": "3426820",
    "end": "3438530"
  },
  {
    "start": "3432000",
    "end": "3599000"
  },
  {
    "text": "again, right, so here's the\nfirst layer of the decoder, and we said that\nthere's masking, right, and you're able to look at\nany of the encoder states",
    "start": "3438530",
    "end": "3445310"
  },
  {
    "text": "and you're only able to\nlook at the previous words in the decoder. In the second layer\nif I'm suddenly",
    "start": "3445310",
    "end": "3451520"
  },
  {
    "text": "allowed to look at all\nof the future words now, hey, even though I didn't\nin the first layer, it's just as good that I\ncan in the second layer.",
    "start": "3451520",
    "end": "3458082"
  },
  {
    "text": "And so I can just learn\nto look right at what my word is supposed to be. So every single\nlayer of the decoder has to have that masking\nor it's sort of moot,",
    "start": "3458082",
    "end": "3466480"
  },
  {
    "text": "like it says if you didn't\nmask it at all effectively. ",
    "start": "3466480",
    "end": "3471770"
  },
  {
    "text": "Thanks. ",
    "start": "3471770",
    "end": "3483760"
  },
  {
    "text": "OK, so scaled dot product\nin the bag, we've got it.",
    "start": "3483760",
    "end": "3488810"
  },
  {
    "text": "So let's look back at\nour full transformer encoder, decoder framework. We've looked at\nthe encoder blocks",
    "start": "3488810",
    "end": "3494680"
  },
  {
    "text": "themselves, so let's sort\nof expand one of these, zoom and enhance.",
    "start": "3494680",
    "end": "3500140"
  },
  {
    "text": "And we've got our word\nembeddings position representations,\nand first we put it",
    "start": "3500140",
    "end": "3505180"
  },
  {
    "text": "through multi-headed attention. So we've seen that. We put it through a residual\nlayer and layer norm,",
    "start": "3505180",
    "end": "3511415"
  },
  {
    "text": "right, so you have\nthe word embedding and the position\nrepresentations going",
    "start": "3511415",
    "end": "3516680"
  },
  {
    "text": "through the residual\nconnection here, and also going through multi-headed attention,\nadd them, layer norm.",
    "start": "3516680",
    "end": "3523410"
  },
  {
    "text": "Next, you put the result of that\nthrough a feed forward network. There should be an arrow\nbetween the feed forward",
    "start": "3523410",
    "end": "3530390"
  },
  {
    "text": "and the next residual here. But the output of this\nresidual and layer norm, is added into that\nresidual and layer norm",
    "start": "3530390",
    "end": "3537290"
  },
  {
    "text": "along with the output\nof the feed forward. And then the output of this\nresidual and layer norm,",
    "start": "3537290",
    "end": "3542450"
  },
  {
    "text": "is the output of the\ntransformer encoder block. So when we had each of\nthese encoders here,",
    "start": "3542450",
    "end": "3548608"
  },
  {
    "text": "internally, each one\nof them was just this, and we've seen all these\nbuilding blocks before. And this is multi-headed\nscaled dot product attention,",
    "start": "3548608",
    "end": "3558230"
  },
  {
    "text": "I omit the scaled word. Yeah, so this is\nthe block, and you",
    "start": "3558230",
    "end": "3563367"
  },
  {
    "text": "notice interestingly how\nyou're doing residual and layer norm after the initial\nmulti-headed attention as well as after\nthe feed forward.",
    "start": "3563367",
    "end": "3571990"
  },
  {
    "text": "OK, so each one of these is\njust identical, right, different parameters for the different\nlayers but the same things",
    "start": "3571990",
    "end": "3579760"
  },
  {
    "text": "that we've seen. Now let's look at the\ntransformer, decoder block.",
    "start": "3579760",
    "end": "3585230"
  },
  {
    "text": "So this is actually\nmore complex. In particular, you've got\nthat masked multi-headed self",
    "start": "3585230",
    "end": "3590809"
  },
  {
    "text": "attention. And now remember, this is\nnot just for the first one, this is for all of the\ntransformer blocks. So we've got masked\nmulti-headed self attention,",
    "start": "3590810",
    "end": "3597120"
  },
  {
    "text": "where we can't\nlook at the future because we've added\nnegative infinity to the negative infinity\nto the affinity scores.",
    "start": "3597120",
    "end": "3604280"
  },
  {
    "text": "Residual and layer norm,\nlike we did for the encoder. Now we've got multi\nhead cross attention.",
    "start": "3604280",
    "end": "3610579"
  },
  {
    "text": "So this connection to\nthe transformer encoder, this is actually a lot like\nwhat we saw in attention so far,",
    "start": "3610580",
    "end": "3617100"
  },
  {
    "text": "right? We're attending from the\ndecoder to the encoder. So we actually in each\ntransformer decoder block,",
    "start": "3617100",
    "end": "3624590"
  },
  {
    "text": "we've got two different\nattention functions going on. So we do the cross\nattention, we add the result",
    "start": "3624590",
    "end": "3632119"
  },
  {
    "text": "of the residual and layer\nnorm to the next residual in layer norm along with that of\nthe multi head cross attention,",
    "start": "3632120",
    "end": "3639260"
  },
  {
    "text": "okay? And only after both of those\napplications of attention, next we do the feed forward\nand residual and layer norm,",
    "start": "3639260",
    "end": "3648260"
  },
  {
    "text": "where the residual is\ncoming, so the Xi minus 1 is the residual layer norm\nhere, goes into this one",
    "start": "3648260",
    "end": "3654559"
  },
  {
    "text": "along with the feed forward. And so you can think of\nthe residual layer norm as coming after each of the\ninteresting things we're",
    "start": "3654560",
    "end": "3659960"
  },
  {
    "text": "doing, right? We're doing one\ninteresting thing here, multi-headed masked self\nattention, cross attention",
    "start": "3659960",
    "end": "3665600"
  },
  {
    "text": "after each one, do\nresidual and layer normal, help the gradients pass,\net cetera, et cetera.",
    "start": "3665600",
    "end": "3671365"
  },
  {
    "text": "And then the output of this\nresidual and layer norm is the output of the\ntransformer decoder. OK, and so the only thing\nso far that we really",
    "start": "3671365",
    "end": "3678770"
  },
  {
    "text": "haven't seen in this lecture, is\nthe multi-head cross attention.",
    "start": "3678770",
    "end": "3683990"
  },
  {
    "text": "And I want to go over it,\nit is the same equations",
    "start": "3683990",
    "end": "3690140"
  },
  {
    "text": "as the multi-headed\nself attention,",
    "start": "3690140",
    "end": "3695950"
  },
  {
    "text": "but the inputs are coming\nfrom different places. And so I want to be\nprecise about it. Let's take a look.",
    "start": "3695950",
    "end": "3702710"
  },
  {
    "text": "Cross attention details. So, right, self attention\nrecall is that when",
    "start": "3702710",
    "end": "3707839"
  },
  {
    "text": "we're taking the keys,\nthe queries and the values of attention from\nthe same information source like the same\nsentence for example,",
    "start": "3707840",
    "end": "3715730"
  },
  {
    "text": "and we saw last week, right,\nattention from the decoder to the encoder. So this is going\nto look similar.",
    "start": "3715730",
    "end": "3721400"
  },
  {
    "text": "We'll see some\ndifferent notation. So we're going to have\nh1 to ht, the output vectors from the transformer\nencoder which are all Xi in rd.",
    "start": "3721400",
    "end": "3733130"
  },
  {
    "text": "Remember, this is the last\ntransformer encoder here, right, you never attend to\nthe middle encoder blocks,",
    "start": "3733130",
    "end": "3739400"
  },
  {
    "text": "it's the output of the\nlast encoder block. So this is the output vectors\nfrom the last transformer encoder block and\nnow we have Z1 to Zt,",
    "start": "3739400",
    "end": "3747822"
  },
  {
    "text": "the input vectors from\nthe transformer decoder. So here maybe that is\nthe input is the word",
    "start": "3747822",
    "end": "3755550"
  },
  {
    "text": "embeddings plus their\nposition representations or, right, it's\nactually the output of the previous\ntransformer decoder,",
    "start": "3755550",
    "end": "3762437"
  },
  {
    "text": "are going to be the\ninputs for the next one.  So yeah, we've got\na Z1 to Zt and we're",
    "start": "3762437",
    "end": "3769613"
  },
  {
    "text": "letting them be the same\nsequence length again T and T, just for simplicity. These are also vectors Zi\nand Rd and then the keys",
    "start": "3769613",
    "end": "3778640"
  },
  {
    "text": "and the, sorry, the\nkeys and the values are all drawn from\nthe encoder, right? So when we're talking about\nattention and allowing us",
    "start": "3778640",
    "end": "3788030"
  },
  {
    "text": "to sort of access\na memory, right? The memory is sort of what the\nvalue vectors are encoding,",
    "start": "3788030",
    "end": "3795200"
  },
  {
    "text": "and the way that the values are\nsort of indexed or able to be accessed is through\nthe keys, and then",
    "start": "3795200",
    "end": "3802370"
  },
  {
    "text": "the value and then\nthe queries are what you're using to try to\nlook for something, right?",
    "start": "3802370",
    "end": "3808480"
  },
  {
    "text": "So we're looking into\nthe encoder as a memory and we're using keys from\nthe decoder to figure out",
    "start": "3808480",
    "end": "3813500"
  },
  {
    "text": "where to look for each one. So pictorially\nagain, we can look",
    "start": "3813500",
    "end": "3819547"
  },
  {
    "text": "at how cross attention is\ncomputed and matrices like we did for self attention. So we've got the same thing\nhere before, we had X,",
    "start": "3819548",
    "end": "3825740"
  },
  {
    "text": "now we have h, these\nare the encoder vectors, these is going to be RT by d.",
    "start": "3825740",
    "end": "3832100"
  },
  {
    "text": "Likewise, we have Z, notice\nwe had two of these before. Before we just had\nX, right, we had X because X was going to be\nfor the keys, the queries",
    "start": "3832100",
    "end": "3839810"
  },
  {
    "text": "and the values. Now we have h and Z,\nboth are in RT by d. And the output is\ngoing to be, well, you",
    "start": "3839810",
    "end": "3848240"
  },
  {
    "text": "take your Z for the\nqueries, right, Z is being multiplied\nby the queries.",
    "start": "3848240",
    "end": "3854060"
  },
  {
    "text": "You take your h for the\nkeys and your h for the V's. So you are trying to take the\nquery, key dot products, all T",
    "start": "3854060",
    "end": "3863480"
  },
  {
    "text": "squared of them in one\nmatrix multiplication. So the purple is saying, this\nis coming from the decoder,",
    "start": "3863480",
    "end": "3871460"
  },
  {
    "text": "the brown is saying it's\ncoming from the encoder.",
    "start": "3871460",
    "end": "3876980"
  },
  {
    "text": "Now you've got\nyour dot products, soft-max them as you did before\nand now your values are also",
    "start": "3876980",
    "end": "3882990"
  },
  {
    "text": "coming from the encoder. So again, same operation,\ndifferent sources",
    "start": "3882990",
    "end": "3888030"
  },
  {
    "text": "for the inputs. And now you've got your\noutput, which again is just an average of the value\nvectors from the encoder hv,",
    "start": "3888030",
    "end": "3898140"
  },
  {
    "text": "the average is determined\nby your weights. OK, so results\nwith transformers,",
    "start": "3898140",
    "end": "3904650"
  },
  {
    "text": "results with transformers. First off was\nmachine translation.",
    "start": "3904650",
    "end": "3911260"
  },
  {
    "text": "So we built our entire encoder,\ndecoder transformer block and you know, how does it work?",
    "start": "3911260",
    "end": "3917380"
  },
  {
    "text": "It works really well. So these are a bunch of\nmachine translation systems that were out when the original\nAttention Is All You Need",
    "start": "3917380",
    "end": "3924150"
  },
  {
    "text": "transformers paper came out. And first, you saw that\ntransformers were getting",
    "start": "3924150",
    "end": "3929550"
  },
  {
    "text": "really good BLEU scores. So this is on the Workshop\non Machine Translation 2014, English, German, and\nEnglish French test sets,",
    "start": "3929550",
    "end": "3937260"
  },
  {
    "text": "you get higher BLEU\nscores, which means better translations, right? Notice how our\nBLEU scores on this",
    "start": "3937260",
    "end": "3943517"
  },
  {
    "text": "are higher than\nfor assignment 4, lots more training\ndata here for example. But then also, not only do\nyou get better BLEU scores,",
    "start": "3943517",
    "end": "3950650"
  },
  {
    "text": "you also had more\nefficient training, right? And we had a lot of tricks\nthat went into getting training",
    "start": "3950650",
    "end": "3957519"
  },
  {
    "text": "to work better, right? So you have more\nefficient training here. OK. So, that's a nice result. That\nwas in the original paper.",
    "start": "3957520",
    "end": "3965940"
  },
  {
    "text": "Past that, there are a number\nof interesting results, summarization is one of them.",
    "start": "3965940",
    "end": "3971360"
  },
  {
    "text": "So, here's the result\non summarization. These are part of a larger\nsummarization system,",
    "start": "3971360",
    "end": "3977240"
  },
  {
    "text": "but you have-- I liked this table because\nyou have sort of seq2seq with attention,\nwhich we saw before. And it got perplexity.",
    "start": "3977240",
    "end": "3983300"
  },
  {
    "text": "Lower is better with perplexity. Higher is better with ROUGE\non this WikiSum dataset.",
    "start": "3983300",
    "end": "3988670"
  },
  {
    "text": "And then sort of a bunch of\ntransformer models they tried. And at a certain point,\nit becomes transformers",
    "start": "3988670",
    "end": "3996050"
  },
  {
    "text": "all the way down, and\nthe old standard of RNN sort of falls out of practice.",
    "start": "3996050",
    "end": "4001240"
  },
  {
    "text": "And actually, before too\nlong, right, transformers became dominant for an\nentirely different reason, which was related more to\ntheir parallelizablity.",
    "start": "4001240",
    "end": "4009010"
  },
  {
    "text": "Because they'll allow you to\npretrain on just a ton of data, very quickly.",
    "start": "4009010",
    "end": "4014430"
  },
  {
    "text": "And this has made them\nthe de facto standard. So there's-- A lot of results,\nrecently with Transformers,",
    "start": "4014430",
    "end": "4020950"
  },
  {
    "text": "include pretraining. And I'm sort of\nintentionally excluding them from this lecture so that\nyou come to the next lecture",
    "start": "4020950",
    "end": "4027220"
  },
  {
    "text": "and learn about pretraining. But there's a popular\naggregate benchmark. This took a bunch of\nvery difficult tasks",
    "start": "4027220",
    "end": "4033339"
  },
  {
    "text": "and said, do well on\nall of them if you want to score highly\non our leaderboard. And the names of\nthese models you",
    "start": "4033340",
    "end": "4039645"
  },
  {
    "text": "can look up if\nyou're interested. But all of them are transformer\nbased after a certain point. So the benchmark is called GLUE.",
    "start": "4039645",
    "end": "4045220"
  },
  {
    "text": "It has a successor\ncalled SuperGLUE. Everything is just transformers\nafter a certain time",
    "start": "4045220",
    "end": "4051579"
  },
  {
    "text": "period, partly because of\ntheir pretraining ability.",
    "start": "4051580",
    "end": "4057010"
  },
  {
    "text": "OK. Great. So, we'll discuss\npretraining more on Thursday.",
    "start": "4057010",
    "end": "4064849"
  },
  {
    "text": "And so our transformer is it. The way that we\ndescribed the Attention",
    "start": "4064850",
    "end": "4071359"
  },
  {
    "text": "Is All You Need paper. So, the transformer\nencoder decoder we saw was from that paper.",
    "start": "4071360",
    "end": "4077150"
  },
  {
    "text": "And at some point, we want\nto build new systems, what are some drawbacks?",
    "start": "4077150",
    "end": "4082415"
  },
  {
    "text": "And we've already started-- People have already started to\nbuild variants of transformers, which we'll go into today. And it definitely has issues\nthat we can try to work on.",
    "start": "4082415",
    "end": "4090650"
  },
  {
    "text": " So I can also take a question\nif anyone wants to ask one.",
    "start": "4090650",
    "end": "4098839"
  },
  {
    "text": "I mean, is that the bit\nthat something there were several questions on\nwas the scale dot product.",
    "start": "4098840",
    "end": "4106278"
  },
  {
    "text": "And the questions\nincluded y square root of d divided by h, as opposed\nto just d divided by h,",
    "start": "4106279",
    "end": "4115250"
  },
  {
    "text": "or any other function\nof d divided by h. And another one was that,\nwhy do you need that",
    "start": "4115250",
    "end": "4127250"
  },
  {
    "text": "at all, given that later on\nyou're going to use layer norm?",
    "start": "4127250",
    "end": "4133312"
  },
  {
    "text": "The second question\nis really interesting and not one that I\nhad thought of before. Well, right, so even if\nthe individual components",
    "start": "4133312",
    "end": "4142450"
  },
  {
    "text": "are small-- So let's start with\nthe second question. Why does this matter even if\nyou're going to use layer norm?",
    "start": "4142450",
    "end": "4148960"
  },
  {
    "text": "If layer norm is\naveraging everything out, say, making the unit\nstandard deviation and mean,",
    "start": "4148960",
    "end": "4155649"
  },
  {
    "text": "then actually, right,\nnothing is going to get too small in\nthose vectors either. So when you have a\nvery, very large vector,",
    "start": "4155649",
    "end": "4162880"
  },
  {
    "text": "all with things that\naren't too small, yeah,",
    "start": "4162880",
    "end": "4169179"
  },
  {
    "text": "you're still going to have\nthe norm of the dot products",
    "start": "4169180",
    "end": "4174430"
  },
  {
    "text": "increase. I think. And I think it's\na good question. I hadn't thought\nabout it too much.",
    "start": "4174430",
    "end": "4179500"
  },
  {
    "text": "That's my off the cuff answer. But it's worth\nthinking about more. I think the answer is\nthat the effect you",
    "start": "4179500",
    "end": "4186700"
  },
  {
    "text": "get of kind of losing\ndynamic range as things get longer, that that's\ngoing to happen anyway.",
    "start": "4186700",
    "end": "4195550"
  },
  {
    "text": "And the layer norm\ncan't fix that. It's sort of coming\nalong too late. And, therefore, you gain\nby doing this scaling.",
    "start": "4195550",
    "end": "4204250"
  },
  {
    "text": "I think so. But I think it's worth-- I think it's worth\nthinking about more. why square root?",
    "start": "4204250",
    "end": "4210980"
  },
  {
    "text": "Well, let's see. The norms of the\ndot product grows with O of D. And so\nwhen you square root 1--",
    "start": "4210980",
    "end": "4219590"
  },
  {
    "text": "No. I guess it scales with O of\nroot D. I can't remember. There's a little\nnote in the Attention Is All You Need paper\nabout why it's root D.",
    "start": "4219590",
    "end": "4226510"
  },
  {
    "text": "But I actually can't take it\noff the top of my head here. But it is in that paper.",
    "start": "4226510",
    "end": "4232330"
  },
  {
    "start": "4232330",
    "end": "4237890"
  },
  {
    "text": "OK. Anything else before we go on? ",
    "start": "4237890",
    "end": "4244860"
  },
  {
    "text": "Great.  All right. So, what would we like to fix?",
    "start": "4244860",
    "end": "4252220"
  },
  {
    "text": "The thing that shows\nup most frequently as a pain point in\nTransformers is actually",
    "start": "4252220",
    "end": "4257230"
  },
  {
    "text": "the quadratic compute in\nthe self-attention itself. So we're having all\npairs of interactions.",
    "start": "4257230",
    "end": "4263470"
  },
  {
    "text": "We had that t by t\nmatrix that was computed by taking these dot products\nbetween all pairs of word",
    "start": "4263470",
    "end": "4269560"
  },
  {
    "text": "vectors. And so, even though we argued\nat the beginning of the class that we don't have this\nsort of temporal dependence",
    "start": "4269560",
    "end": "4276130"
  },
  {
    "text": "in the computation graph that\nstops us from parallelizing things, we still need to\ndo all that computation,",
    "start": "4276130",
    "end": "4281260"
  },
  {
    "text": "and that grows quadratically. For recurrent models, right,\nit only grew linearly. Every time you\napplied the RNN cell,",
    "start": "4281260",
    "end": "4288309"
  },
  {
    "text": "you did sort of more work, but\nyou're not adding quadratically to the amount of\nwork you have to do as you get to longer sequences.",
    "start": "4288310",
    "end": "4295270"
  },
  {
    "text": "Separately, position\nrepresentations. I mean, the absolute\nposition of a word, it's just maybe not the\nbest way to represent",
    "start": "4295270",
    "end": "4304030"
  },
  {
    "text": "the structure of a sentence. And so there have been these\ntwo, among other advancements",
    "start": "4304030",
    "end": "4310840"
  },
  {
    "text": "that I won't be able\nto get into today, but you can take a look at\nthese papers and the papers that cite them.",
    "start": "4310840",
    "end": "4316010"
  },
  {
    "text": "There are other ways\nto represent position. People are working on it. But I want to focus more\ntoday on the problem",
    "start": "4316010",
    "end": "4322630"
  },
  {
    "text": "of the quadratic compute. So, how do we reason about this,\nwhy is this a problem, right?",
    "start": "4322630",
    "end": "4330528"
  },
  {
    "text": "So it's highly\nparallelizable, but we still have to do these operations. We have T squared,\nthat's a sequence length, and then d is the\ndimensionality.",
    "start": "4330528",
    "end": "4337670"
  },
  {
    "text": "And so in computing\nthis matrix, we have O of T squared d\ncomputations that our GPU needs",
    "start": "4337670",
    "end": "4342700"
  },
  {
    "text": "to chunk through. If we think of d as around\n1,000, or 2,000, or 3,000.",
    "start": "4342700",
    "end": "4349610"
  },
  {
    "text": "If we had sort of single,\nshortish sentences, then maybe T is like 30-ish,\nand then T squared is 900.",
    "start": "4349610",
    "end": "4355820"
  },
  {
    "text": "So it's like, yeah, it's\nactually not that big a deal. And in practice,\nfor a lot of models we'll set an actual\nbound like 512.",
    "start": "4355820",
    "end": "4363100"
  },
  {
    "text": "So it's like, if your document\nis longer than 512 words, you're out of luck, you're\ntruncated or something.",
    "start": "4363100",
    "end": "4369010"
  },
  {
    "text": "But what if we want to\nwork on documents that are 10,000 words or greater? 10,000 squared is not feasible.",
    "start": "4369010",
    "end": "4377179"
  },
  {
    "text": "So we have to somehow\nremove the dependence on T squared if we're\ngoing to work with these.",
    "start": "4377180",
    "end": "4383179"
  },
  {
    "text": "There have been a\ncouple of ways that have been taught how to do this. This is all very,\nvery recent work and it's only a smattering of\nthe efforts that have come up.",
    "start": "4383180",
    "end": "4390340"
  },
  {
    "text": "So the question is,\ncan we build models like transformers that get\naway without the O of T squared",
    "start": "4390340",
    "end": "4396900"
  },
  {
    "text": "all-pairs interactions cost? One example is the Linformer.",
    "start": "4396900",
    "end": "4402239"
  },
  {
    "text": "And the idea here is that\nyou're going to actually map the sequence length dimension\nto a lower dimensional",
    "start": "4402240",
    "end": "4410280"
  },
  {
    "text": "space for values and keys. So you had values,\nkeys, and queries,",
    "start": "4410280",
    "end": "4415560"
  },
  {
    "text": "and you had your\nnormal linear layers, now you're going to project\nto a much lower dimension then the sequence length.",
    "start": "4415560",
    "end": "4421170"
  },
  {
    "text": "And in doing so, right, you're\nsort of getting rid of that T",
    "start": "4421170",
    "end": "4426480"
  },
  {
    "text": "by mapping it to\nsomething smaller. You're saying just combine\nall the information from all these time steps\ninto something that's",
    "start": "4426480",
    "end": "4432330"
  },
  {
    "text": "lower dimensional. And so, in this\nplot from the paper, as the sequence length goes from\n512 with a batch size of 128",
    "start": "4432330",
    "end": "4440190"
  },
  {
    "text": "to the sequence length being\n65,000 with a batch size of 1, you get the transformer\ninference time",
    "start": "4440190",
    "end": "4447389"
  },
  {
    "text": "growing very large. And then the Linformer\nwith various bottleneck",
    "start": "4447390",
    "end": "4452940"
  },
  {
    "text": "dimensionalities, k\nis 128, 256, they're doing much, much better.",
    "start": "4452940",
    "end": "4458340"
  },
  {
    "text": "A separate option has\nbeen to kind of take",
    "start": "4458340",
    "end": "4463770"
  },
  {
    "text": "a totally different\ntake on can we get away without these\nall-pairs interactions, which is the following.",
    "start": "4463770",
    "end": "4469890"
  },
  {
    "text": "Do we need to even\ntry to compute all pairs of\ninteractions if we can do sort of a bunch\nof other stuff",
    "start": "4469890",
    "end": "4475560"
  },
  {
    "text": "that's going to be more\nefficient to compute? So, like looking\nat local windows. We know that's useful, but\nnot sufficient in some sense.",
    "start": "4475560",
    "end": "4483810"
  },
  {
    "text": "Looking at everything. So, if you were to just take\nan average of vectors, just all the averaging of\nvectors, you don't",
    "start": "4483810",
    "end": "4488858"
  },
  {
    "text": "need to compute\ninteractions for that. And if you look sort\nof at random pairs, you don't need to take\nall that much time",
    "start": "4488858",
    "end": "4495300"
  },
  {
    "text": "to compute that as well. And so, what this paper did\nis they did all of them.",
    "start": "4495300",
    "end": "4500440"
  },
  {
    "text": "So you have random attention,\nyou have a word window attention where you're looking\nat your local neighbors, and you have sort\nof global attention",
    "start": "4500440",
    "end": "4507330"
  },
  {
    "text": "where you're sort of attending\nwithout interacting with stuff, attending broadly over\nthe whole sequence.",
    "start": "4507330",
    "end": "4513060"
  },
  {
    "text": "You do a whole\nbunch of it, right, and you end up being\nable to approximate a lot of good things.",
    "start": "4513060",
    "end": "4519780"
  },
  {
    "text": "These are not\nnecessarily the answer. The normal transformer variant\nis by far the most popular,",
    "start": "4519780",
    "end": "4524900"
  },
  {
    "text": "currently. But it's a fascinating\nquestion to look into. So now, as the time\nmore or less expires,",
    "start": "4524900",
    "end": "4533240"
  },
  {
    "text": "I'll say we're working on\npretraining on Thursday. Good luck on assignment 4. And remember to work on\nyour project proposal.",
    "start": "4533240",
    "end": "4539495"
  },
  {
    "text": "And I think we have time\nfor a final question if anyone wants to. ",
    "start": "4539495",
    "end": "4550576"
  },
  {
    "text": "I guess, are you\naware of any use case where an RNN might\noutperform a transformer?",
    "start": "4550576",
    "end": "4556407"
  },
  {
    "text": "That's a good question.  I mean, I believe still places\nin reinforcement learning.",
    "start": "4556407",
    "end": "4566740"
  },
  {
    "text": "I mean, places where\nthe recurrent inductive bias is clearly well\nspecified or useful.",
    "start": "4566740",
    "end": "4572107"
  },
  {
    "text": "There was a conversation--  I don't know of places in\nNLP where people are still",
    "start": "4572107",
    "end": "4579580"
  },
  {
    "text": "broadly using RNNs. It was thought for a while that\nTransformers took a lot more data to train than RNNs.",
    "start": "4579580",
    "end": "4584890"
  },
  {
    "text": "And so you sort of should use\nRNNs on smaller data problems. But with pretraining, I'm not\nsure that that's the case.",
    "start": "4584890",
    "end": "4591560"
  },
  {
    "text": "I think the answer is, yes,\nthere are still use cases. But they should be where the\nrecurrence seems to really",
    "start": "4591560",
    "end": "4599800"
  },
  {
    "text": "be the thing that is winning\nyou something, as opposed to maybe needing more\ndata for Transformers.",
    "start": "4599800",
    "end": "4605965"
  },
  {
    "text": "Because it seems like\nthat might not actually be the case, even though we\nthought so back in like 2017.",
    "start": "4605965",
    "end": "4611880"
  },
  {
    "start": "4611880",
    "end": "4616000"
  }
]