[
  {
    "text": "excellent so uh how many of you know a lot about neuron networks or medium",
    "start": "11639",
    "end": "18439"
  },
  {
    "text": "amount of knowledge okay okay that's good um I",
    "start": "18439",
    "end": "23760"
  },
  {
    "text": "have a fairly building up on like a little bit of neuronet Networks background um",
    "start": "23760",
    "end": "31320"
  },
  {
    "text": "trying to put things in perspective with the New Renaissance of uh what now it's",
    "start": "31320",
    "end": "36680"
  },
  {
    "text": "called Deep learning I myself I am a research scientist at Google I actually",
    "start": "36680",
    "end": "42200"
  },
  {
    "text": "moved last week um from Google brain which is down the road to deep M which is in London so last week I was in",
    "start": "42200",
    "end": "48440"
  },
  {
    "text": "London and so I'm doubly jetl but um hopefully Still Still Still all here and",
    "start": "48440",
    "end": "55359"
  },
  {
    "text": "um and yeah so a little bit about myself and how I got into to deep learning I",
    "start": "55359",
    "end": "61800"
  },
  {
    "text": "have a background in mathematics and telecommunication engineering from from Spain I I'm from Barcelona and I did a",
    "start": "61800",
    "end": "68520"
  },
  {
    "text": "masters at UCSD my PhD at Berkeley um but it really was actually uh uh during",
    "start": "68520",
    "end": "74840"
  },
  {
    "text": "my internships at Microsoft research where I got to meet a lot of the",
    "start": "74840",
    "end": "80000"
  },
  {
    "text": "students by Jeff Hinton uh who is a professor from Toronto who who is a Pioneer in neuron networks and as a",
    "start": "80000",
    "end": "87560"
  },
  {
    "text": "result my thesis topic was actually like like in deep learning uh namely optimization and recruting your own",
    "start": "87560",
    "end": "93759"
  },
  {
    "text": "networks and uh once I finished um from Berkeley I joined Google brain and now",
    "start": "93759",
    "end": "98920"
  },
  {
    "text": "very recently I I moved to Deep Mind um which which are and these two groups",
    "start": "98920",
    "end": "105079"
  },
  {
    "text": "definitely have neuron networks are kind of their driving force um in several ways I hope to uh communicate today um",
    "start": "105079",
    "end": "113000"
  },
  {
    "text": "first and for foremost though I would say that a lot of what I'm going to talk about uh is done with great",
    "start": "113000",
    "end": "118560"
  },
  {
    "text": "collaborators um most of them at Google or have been at Google uh brain uh it it's been a great uh two",
    "start": "118560",
    "end": "126320"
  },
  {
    "text": "years for me to to learn and grow and of course uh it was nice out of the PHD to",
    "start": "126320",
    "end": "131400"
  },
  {
    "text": "to sort of join a group where um where deep learning was the driving force uh you might know a bit about history about",
    "start": "131400",
    "end": "137319"
  },
  {
    "text": "Google brain but actually it was uh I mean Andre who was who is a professor here I believe still um was uh part of",
    "start": "137319",
    "end": "144440"
  },
  {
    "text": "the the initiator at Google back in the day um so so I have to say that a lot of",
    "start": "144440",
    "end": "151680"
  },
  {
    "text": "the work I mean I'm presenting here it's done in collaboration and it's of course extremely important to acknowledge um",
    "start": "151680",
    "end": "158560"
  },
  {
    "text": "those guys and many others that are not here and so I I I hope to convey maybe a",
    "start": "158560",
    "end": "164400"
  },
  {
    "text": "little bit of of deep learning and my my own take on deep learning so I'm going to give a little bit of an overview",
    "start": "164400",
    "end": "170040"
  },
  {
    "text": "please if you have questions if you're curious about anything you can ask anything you know throughout the talk um",
    "start": "170040",
    "end": "175800"
  },
  {
    "text": "let's keep it informal and and so on um then I'm going to dive into more of a technical aspects that that is something",
    "start": "175800",
    "end": "182879"
  },
  {
    "text": "that I've done a lot of research uh on the last couple years um and and this is definitely sort of a new paradigm that's",
    "start": "182879",
    "end": "189959"
  },
  {
    "text": "sort of starting to explode again after some uh recent advances in computer",
    "start": "189959",
    "end": "195400"
  },
  {
    "text": "vision namely uh recruit neuron networks uh to for to deal with sequences um so",
    "start": "195400",
    "end": "200799"
  },
  {
    "text": "it's quite quite interesting and then if we have time I'm going to describe some of the new paradigms uh which deal with",
    "start": "200799",
    "end": "207360"
  },
  {
    "text": "memories and attention mechanisms um that you can add to these neuron networks that allow them to be more and",
    "start": "207360",
    "end": "212840"
  },
  {
    "text": "more closer to to computers so you probably have seen some",
    "start": "212840",
    "end": "218920"
  },
  {
    "text": "version of this slide where you know it's basically saying yeah deep learning",
    "start": "218920",
    "end": "224080"
  },
  {
    "text": "um and machine learning in fact are are very hot topics nowadays there's a lot of money um from big companies from",
    "start": "224080",
    "end": "231480"
  },
  {
    "text": "startups uh lots of lots of investing lots of you know um from Finance to you",
    "start": "231480",
    "end": "238040"
  },
  {
    "text": "know to medicine it's sort of taking um especially machine learning rather",
    "start": "238040",
    "end": "243799"
  },
  {
    "text": "than deep learning is is really like being used everywhere I would say um it's hard to not find applications of",
    "start": "243799",
    "end": "250040"
  },
  {
    "text": "machine learning or places where it could be applied and this is just simply showing sort of Google Trends um where",
    "start": "250040",
    "end": "257440"
  },
  {
    "text": "you kind of see some sort of maybe exponential grow on machine learning queries and quite correlated with deep",
    "start": "257440",
    "end": "264120"
  },
  {
    "text": "learning and then the previous hot thing was super Vector machine it sort of was discarded",
    "start": "264120",
    "end": "269880"
  },
  {
    "text": "by this naive graph uh in 2012 which makes sense which that's that was the",
    "start": "269880",
    "end": "275120"
  },
  {
    "text": "big uh image net year for convolutional neuron networks so on that on that uh",
    "start": "275120",
    "end": "280919"
  },
  {
    "text": "note um simply I like I like to see deep learning not as something like magical",
    "start": "280919",
    "end": "286400"
  },
  {
    "text": "that solves Ai and so on I really have a very pragmatic view on what deep learning is or rather supervised",
    "start": "286400",
    "end": "293039"
  },
  {
    "text": "learning and what it is is basically a mapping from an input to an output and",
    "start": "293039",
    "end": "299400"
  },
  {
    "text": "here the input would be an image so set of pixels um which you could think as a as of a vector or or a matrix and um",
    "start": "299400",
    "end": "307440"
  },
  {
    "text": "given this image you want to classify whether there's a cat or a dog and you're given many pairs of examples um",
    "start": "307440",
    "end": "314280"
  },
  {
    "text": "through supervision so through some human labelers and with that you essentially train this convolutional",
    "start": "314280",
    "end": "320759"
  },
  {
    "text": "neuron Network or neuron Network to fit a function um so more precisely uh we",
    "start": "320759",
    "end": "326280"
  },
  {
    "text": "are trying to find a mapping that goes from X to y as best as possible for given set training set of X and Y Pairs",
    "start": "326280",
    "end": "334680"
  },
  {
    "text": "and uh we do so by simply optimization and here is where like distributed optimization and systems uh come into",
    "start": "334680",
    "end": "341520"
  },
  {
    "text": "role especially where something that typically has been done at Google brain",
    "start": "341520",
    "end": "346680"
  },
  {
    "text": "so you define this mapping F ofx which can be a neuron Network or it could be also of course a linear classifier or or",
    "start": "346680",
    "end": "353160"
  },
  {
    "text": "or whatever you want and you define a loss over this function over the whole training set and then you set to find",
    "start": "353160",
    "end": "360319"
  },
  {
    "text": "parameters of this function f that minimizes this loss and this is almost",
    "start": "360319",
    "end": "365680"
  },
  {
    "text": "all you need to know so perhaps um this is why entering or understanding machine",
    "start": "365680",
    "end": "371199"
  },
  {
    "text": "learning is is sort of easy because this is really almost all it is except there's lots of tricks of course and",
    "start": "371199",
    "end": "377039"
  },
  {
    "text": "making things work is is sometimes not as as easy as just applying these mindlessly um and I think deep learning",
    "start": "377039",
    "end": "385039"
  },
  {
    "text": "has really been successful as sort of fulfilled This Promise of I don't want to know too much about the underlying",
    "start": "385039",
    "end": "391599"
  },
  {
    "text": "x's and y's um but I have sort of a generic algorithm that allows me me to",
    "start": "391599",
    "end": "396759"
  },
  {
    "text": "map all sorts of modalities and interesting signals input to output and",
    "start": "396759",
    "end": "402360"
  },
  {
    "text": "deep learning indeed has done quite well so uh it is part of or currently mostly",
    "start": "402360",
    "end": "409360"
  },
  {
    "text": "um in state-ofthe-art on several tasks um some of which uh I've been sort of",
    "start": "409360",
    "end": "416319"
  },
  {
    "text": "involved or more involved uh through because they're involving sequences such as language modeling machine translation",
    "start": "416319",
    "end": "422840"
  },
  {
    "text": "or or image captioning and but there's really a very wide variety of applications where neuron networks uh",
    "start": "422840",
    "end": "430080"
  },
  {
    "text": "seem to help push a state-ofthe-art um and that's why probably it's such a a a",
    "start": "430080",
    "end": "438160"
  },
  {
    "text": "Hot Topic to to work on um but I think for for the most part even though I have",
    "start": "438160",
    "end": "445120"
  },
  {
    "text": "a speech uh recognition background and the speech deep learning Revolution started it actually quite early in 2008",
    "start": "445120",
    "end": "451479"
  },
  {
    "text": "perhaps um this was a result that made the computer vision Community really",
    "start": "451479",
    "end": "458440"
  },
  {
    "text": "believe in this convolutional neuron networks which is a particular kind of neuron networks and this was done",
    "start": "458440",
    "end": "465440"
  },
  {
    "text": "largely thanks to a data set which was collected by fa at and others here at",
    "start": "465440",
    "end": "470919"
  },
  {
    "text": "Stanford uh which just basically had a bunch of images with labels of objects",
    "start": "470919",
    "end": "476199"
  },
  {
    "text": "that were present in the image so here is an example and by having a lot of",
    "start": "476199",
    "end": "481479"
  },
  {
    "text": "images namely for the for the classification Task 1 million images um",
    "start": "481479",
    "end": "486800"
  },
  {
    "text": "thousand classes and a thousand images for each class um you would want to train this F ofx that maps from X the",
    "start": "486800",
    "end": "494080"
  },
  {
    "text": "image to one of these 1,000 classes and uh one of the architectures that did",
    "start": "494080",
    "end": "501240"
  },
  {
    "text": "fairly well um last year and also this year with some modifications uh is what people call",
    "start": "501240",
    "end": "508120"
  },
  {
    "text": "this uh Inception architecture which is essentially many layers of computation I",
    "start": "508120",
    "end": "514599"
  },
  {
    "text": "haven't quite described what these layers are are exactly but um bear with me on that this is just showing the",
    "start": "514599",
    "end": "521800"
  },
  {
    "text": "input there on the left very small that's the X and then there's a bunch of layers that are all differentiable so we",
    "start": "521800",
    "end": "527519"
  },
  {
    "text": "can all compute the gradients efficiently and at the end we just predict one of these thousand classes",
    "start": "527519",
    "end": "533399"
  },
  {
    "text": "and now the winning architecture this year I believe is this is 20 something layers and the winning is 50 so really",
    "start": "533399",
    "end": "541399"
  },
  {
    "text": "it's it's getting a little out of control especially in Vision I would say um but you know apparently add some the",
    "start": "541399",
    "end": "549560"
  },
  {
    "text": "layers tune a little bit the architecture you get better at and better at classifying these objects uh",
    "start": "549560",
    "end": "555279"
  },
  {
    "text": "in fact it's getting at a level where perhaps untrained or a human that might",
    "start": "555279",
    "end": "561640"
  },
  {
    "text": "get tired to classify objects um across a thousand uh might do even worse than",
    "start": "561640",
    "end": "567320"
  },
  {
    "text": "the current State ofth art um and this must be updated because the new result",
    "start": "567320",
    "end": "572519"
  },
  {
    "text": "actually Hales the error rate of the previous best result which was last year",
    "start": "572519",
    "end": "577680"
  },
  {
    "text": "archive paper um so that's not so the 2014 was the official result and the",
    "start": "577680",
    "end": "582880"
  },
  {
    "text": "2015 is is as I said having that archive 2015 or almost halfing the error rate so",
    "start": "582880",
    "end": "589079"
  },
  {
    "text": "the progress and the pace of papers out and so on is is quite almost unbearable",
    "start": "589079",
    "end": "594200"
  },
  {
    "text": "um but there's really like a lot of uh exciting ideas and many people people",
    "start": "594200",
    "end": "599640"
  },
  {
    "text": "definitely working on this field so it's it's fairly exciting um and then you know the the",
    "start": "599640",
    "end": "605320"
  },
  {
    "text": "bottom line is you can do fairly difficult things like I don't even know this flower what these flowers are I",
    "start": "605320",
    "end": "611680"
  },
  {
    "text": "mean I know they're flowers um but these neuron networks give you some sort of classification that's fairly fine",
    "start": "611680",
    "end": "618839"
  },
  {
    "text": "grained um and they kind of statistically speaking do a reasonable",
    "start": "618839",
    "end": "624240"
  },
  {
    "text": "job whenever there's some out I mean there's lots of objects in this image so what should the network do in general",
    "start": "624240",
    "end": "629720"
  },
  {
    "text": "whatever it outputs is fairly accurate um and then whenever it makes mistakes",
    "start": "629720",
    "end": "635320"
  },
  {
    "text": "they happen to be within reason I mean that's not a snake but it certainly",
    "start": "635320",
    "end": "640480"
  },
  {
    "text": "looks like one and I mean that's not a dog but um you know it looks like a dog at least and and so the net result of",
    "start": "640480",
    "end": "649040"
  },
  {
    "text": "all these uh which is why perhaps large companies such as Facebook or Google are",
    "start": "649040",
    "end": "654600"
  },
  {
    "text": "interested is that now you're able to do large amounts of computation automatically on large amounts of data",
    "start": "654600",
    "end": "663000"
  },
  {
    "text": "uh at very very cheaply in sense basically just using a bunch of computational resources so uh in Google",
    "start": "663000",
    "end": "670079"
  },
  {
    "text": "photos for example you can do cool things like look for pictures you've taken on on the sea or even videos",
    "start": "670079",
    "end": "676160"
  },
  {
    "text": "without you needing to go over every picture that you take and labeling them so uh that's a feature that that got",
    "start": "676160",
    "end": "682639"
  },
  {
    "text": "launched and the key technology behind this is obviously these neuron Nets or convolutional neuron Nets that I was",
    "start": "682639",
    "end": "688079"
  },
  {
    "text": "describing um and then you can even do some other funkier classes like drawing or Yoda and",
    "start": "688079",
    "end": "695160"
  },
  {
    "text": "it fairly I mean it that's kind of a strange Yoda but actually I believe that's a real example from someone in",
    "start": "695160",
    "end": "701279"
  },
  {
    "text": "our group so um I keep it there I hope it's a real one and not uh you know um",
    "start": "701279",
    "end": "707959"
  },
  {
    "text": "then you might have seen also like all sorts of things so so I think the amount of people in the field makes really like",
    "start": "707959",
    "end": "713360"
  },
  {
    "text": "surprising you know you get surprising side effects because you have this social network effect where like people",
    "start": "713360",
    "end": "718920"
  },
  {
    "text": "start trying all sorts of crazy things so this is sort of an interesting uh",
    "start": "718920",
    "end": "724200"
  },
  {
    "text": "experiment which was done um by several groups but Alexander is actually also at Google not at Google brain but he's in",
    "start": "724200",
    "end": "730399"
  },
  {
    "text": "the computer vision team and you might have seen this but essentially um you",
    "start": "730399",
    "end": "736360"
  },
  {
    "text": "can take any picture and sort of let the network um imagine uh or rather you",
    "start": "736360",
    "end": "743639"
  },
  {
    "text": "optimize instead of just passing the the image through the network you say I want",
    "start": "743639",
    "end": "748680"
  },
  {
    "text": "to have certain class at the output I'm going to optimize with respect to the input so you sort of reverse the",
    "start": "748680",
    "end": "754720"
  },
  {
    "text": "optimization problem instead of finding the best parameters you find the best X for a given Y and you initialize that x",
    "start": "754720",
    "end": "761120"
  },
  {
    "text": "with an image and then you get all sorts of um cool images that look like art um",
    "start": "761120",
    "end": "766399"
  },
  {
    "text": "and and further some some some guys from Europe took this further and and did this sort of merging um a base image",
    "start": "766399",
    "end": "773560"
  },
  {
    "text": "which is a picture you can take like that River with all sorts of Artistic Styles and here sort of the the way to",
    "start": "773560",
    "end": "779920"
  },
  {
    "text": "do this is also through a neuron Network that was trained on imag net and you just take the base image and you say I",
    "start": "779920",
    "end": "786760"
  },
  {
    "text": "want the high level features of this network to map to the to the real image",
    "start": "786760",
    "end": "792279"
  },
  {
    "text": "but the lowlevel features that map more to the filters and sort of the the low frequency information in the image to",
    "start": "792279",
    "end": "798440"
  },
  {
    "text": "map to whatever artistic style and then people are doing now you know all sorts",
    "start": "798440",
    "end": "803560"
  },
  {
    "text": "of startups applications and and you can run this yourself if you have GPU and two minutes pretty much so it's not",
    "start": "803560",
    "end": "810120"
  },
  {
    "text": "something we might want to do on every image we ever take but it certainly is pretty impressive the the kind of",
    "start": "810120",
    "end": "815959"
  },
  {
    "text": "results of merging um and and sort of the art in neuron networks is was something that last year was kind of a",
    "start": "815959",
    "end": "822079"
  },
  {
    "text": "surprise to me that so many people got interested and there was so many cool things out of this and then more",
    "start": "822079",
    "end": "829079"
  },
  {
    "text": "recently um specifically today as well but but last year um there there's some",
    "start": "829079",
    "end": "834440"
  },
  {
    "text": "work on using uh neuron networks that again map inputs in this case images",
    "start": "834440",
    "end": "840560"
  },
  {
    "text": "from video games to outputs which are how to move the joystick or press buttons on Atari for example so so last",
    "start": "840560",
    "end": "847800"
  },
  {
    "text": "year um more or less a year ago uh people at from Deep mine uh had this",
    "start": "847800",
    "end": "854240"
  },
  {
    "text": "very simple model in sort of in describing it which was I'm going to give you a video game please maximize",
    "start": "854240",
    "end": "862000"
  },
  {
    "text": "the score and all I allow you to see are the pixels and all I I allow you to do is touch these four buttons for example",
    "start": "862000",
    "end": "869240"
  },
  {
    "text": "in Atari and then you train a neuron Network and through some some technique",
    "start": "869240",
    "end": "875560"
  },
  {
    "text": "called reinforcement learning um you just try to maximize the score let the agent train and eventually it actually",
    "start": "875560",
    "end": "882120"
  },
  {
    "text": "got um in most games super human of course um these these games are in some",
    "start": "882120",
    "end": "887920"
  },
  {
    "text": "ways cheat because you can get zero Reaction Time whereas humans can't and",
    "start": "887920",
    "end": "893519"
  },
  {
    "text": "you know you then never get tired and so on but it still it was it was quite a cool accomplishment",
    "start": "893519",
    "end": "899519"
  },
  {
    "text": "um I have a video that I can show but um this is online so I'll probably skip it actually um and then today actually the",
    "start": "899519",
    "end": "908440"
  },
  {
    "text": "there there was a new paper um by De mine as well where they tackled a different game um namely go um so if you",
    "start": "908440",
    "end": "916880"
  },
  {
    "text": "don't know go is sort of a chess but far fairly more complicated in sort of Brute Force search um aspect and uh and again",
    "start": "916880",
    "end": "925440"
  },
  {
    "text": "the system was slightly more complex than the one that played game but a a",
    "start": "925440",
    "end": "931399"
  },
  {
    "text": "key component was a neuron Network that took a go 19 by9 board um which which",
    "start": "931399",
    "end": "937959"
  },
  {
    "text": "can be seen as a black and white image and from that board decided where it would put the the next piece so that was",
    "start": "937959",
    "end": "945360"
  },
  {
    "text": "part of the system there's other components um but uh that alone actually",
    "start": "945360",
    "end": "950600"
  },
  {
    "text": "gets you a fairly uh good go player and the announcement was that they actually",
    "start": "950600",
    "end": "956240"
  },
  {
    "text": "beat the best European player and there's going to be a game against uh the best player in the world perhaps in",
    "start": "956240",
    "end": "962199"
  },
  {
    "text": "go in a couple of months so if you're an Enthusiast of AI and and sort of U heart",
    "start": "962199",
    "end": "968360"
  },
  {
    "text": "problems that have been around for a while uh I suggest you read the the nature paper and there's a lot of press",
    "start": "968360",
    "end": "974560"
  },
  {
    "text": "coverage about this as well so um and and again all the components here are neuron networks which are these very",
    "start": "974560",
    "end": "980560"
  },
  {
    "text": "powerful functions you can learn from from data um which is sort of the key so",
    "start": "980560",
    "end": "986839"
  },
  {
    "text": "so then now I've been talking about neuron networks so what are neuron networks or at least the modern version",
    "start": "986839",
    "end": "993560"
  },
  {
    "text": "of them well a neuron network has this key component which is the neuron uh and",
    "start": "993560",
    "end": "999639"
  },
  {
    "text": "a neuron is essentially a linear combination of inputs so it basically a",
    "start": "999639",
    "end": "1005759"
  },
  {
    "text": "neuron has a bunch of inputs it combines them with a weighted sum and the weights",
    "start": "1005759",
    "end": "1011680"
  },
  {
    "text": "are the parameters that you will learn and it computes the output by doing this weighted sum Plus nonlinearity and the",
    "start": "1011680",
    "end": "1020040"
  },
  {
    "text": "nonlinearity historically was a sigmoidal nonlinearity now people use this uh sort of rectified linear unit",
    "start": "1020040",
    "end": "1026438"
  },
  {
    "text": "it's been called but it's a halfwave rectification or whatever there's many names for this thing and um and that's",
    "start": "1026439",
    "end": "1033640"
  },
  {
    "text": "it so the nice thing is that a single neuron if you set the weights appropriately can actually compute um",
    "start": "1033640",
    "end": "1041360"
  },
  {
    "text": "the three basic uh uh Boolean operations so in a sense if you have a a a network",
    "start": "1041360",
    "end": "1047000"
  },
  {
    "text": "of these neurons you have in some ways a general computer of course how to set",
    "start": "1047000",
    "end": "1052160"
  },
  {
    "text": "the weights is part it's a big part of the problem but in principle these",
    "start": "1052160",
    "end": "1057720"
  },
  {
    "text": "neural networks are quite powerful in that they can represent lots of",
    "start": "1057720",
    "end": "1063000"
  },
  {
    "text": "different functions of course they can actually represent any continuous functions there's theorems for that if you have a single hidden layer um but",
    "start": "1063000",
    "end": "1070120"
  },
  {
    "text": "the way I like to see it more is of like representing Boolean logic um that has these weights that are learned so again",
    "start": "1070120",
    "end": "1077320"
  },
  {
    "text": "to be specific at that f of X is now a neural network that has the X and of course the parameters W those are these",
    "start": "1077320",
    "end": "1083919"
  },
  {
    "text": "weights this that Define the weighted sums in neural networks and it's called",
    "start": "1083919",
    "end": "1089159"
  },
  {
    "text": "Deep learning because and creatively so you just apply over and over the same",
    "start": "1089159",
    "end": "1095880"
  },
  {
    "text": "principle of having neurons that take inputs and treat these as the next step",
    "start": "1095880",
    "end": "1101799"
  },
  {
    "text": "and so on so you essentially are composing a function by so W1 uh with",
    "start": "1101799",
    "end": "1109159"
  },
  {
    "text": "the nonlinearity defines a function over X you take that as the output you define r of W2 that takes the input as that and",
    "start": "1109159",
    "end": "1116919"
  },
  {
    "text": "so on and as I was saying in in image net this actually goes up to like 150",
    "start": "1116919",
    "end": "1123559"
  },
  {
    "text": "layers and in recurent neuronet as you'll see it also can be quite the composition is applied several times um",
    "start": "1123559",
    "end": "1130960"
  },
  {
    "text": "so that's sort of the more specific form of the of this function and then as I said you just train it so that it",
    "start": "1130960",
    "end": "1137080"
  },
  {
    "text": "approximates the true true Target and to do so you define some sort of loss which is a sort of estimate of of",
    "start": "1137080",
    "end": "1143640"
  },
  {
    "text": "for example the distance between the targets and what you predict or the cross entropy or or depends on the T",
    "start": "1143640",
    "end": "1149520"
  },
  {
    "text": "these these changes in reinforcement learning for playing games actually the loss is something which is in like like",
    "start": "1149520",
    "end": "1156240"
  },
  {
    "text": "long Horizon and so on um but in the end whatever loss you defined the advantage",
    "start": "1156240",
    "end": "1161559"
  },
  {
    "text": "is that you can very efficiently compute gradients uh with respect to the parameters W for lots of given X and Y",
    "start": "1161559",
    "end": "1168919"
  },
  {
    "text": "Pairs and this is where you can kind of choose the optimization algorithm that",
    "start": "1168919",
    "end": "1175120"
  },
  {
    "text": "you please but generally speaking stochastic grad in descent has been fairly consistently being used and",
    "start": "1175120",
    "end": "1182320"
  },
  {
    "text": "successful at many of the applications that we've tried um and stochastic GR in the S essentially takes a subset of the",
    "start": "1182320",
    "end": "1189559"
  },
  {
    "text": "XY pairs that you're given in this training set and you compute the gradient which is now an noisy estimate",
    "start": "1189559",
    "end": "1194919"
  },
  {
    "text": "of the true loss and you update your weights a little bit and you keep going uh um until you",
    "start": "1194919",
    "end": "1201360"
  },
  {
    "text": "converge and so that sounds simple but actually it's not because uh the loss",
    "start": "1201360",
    "end": "1207039"
  },
  {
    "text": "and that's probably why neuron Nets became a little unpopular is a very complicated non-convex function so so",
    "start": "1207039",
    "end": "1214280"
  },
  {
    "text": "there's only very few kinds of neural networks that yield a cost that is comx namely has a unique solution whatever",
    "start": "1214280",
    "end": "1220799"
  },
  {
    "text": "you start and so on so that's definitely a challenge to solve the optimization",
    "start": "1220799",
    "end": "1226679"
  },
  {
    "text": "problem also nowadays we have lots of data so we have many input output examples um in text for example like if",
    "start": "1226679",
    "end": "1234280"
  },
  {
    "text": "you take Wikipedia and you want to learn something on top of Wikipedia you you're talking about billions of words um so if",
    "start": "1234280",
    "end": "1240919"
  },
  {
    "text": "each example is a word you have lots of examples like this is 1 e to the N9 okay and also W the weights this Vector of",
    "start": "1240919",
    "end": "1249159"
  },
  {
    "text": "Weights is actually quite large so to M having millions of parameters to learn",
    "start": "1249159",
    "end": "1254240"
  },
  {
    "text": "is definitely very common nowadays so you can for foret about things like also",
    "start": "1254240",
    "end": "1259880"
  },
  {
    "text": "second order Methods at least directly applied because Computing for example The hesan Matrix would require you to",
    "start": "1259880",
    "end": "1265320"
  },
  {
    "text": "store a 1 million by 1 million Matrix which is quite substantial even with",
    "start": "1265320",
    "end": "1270559"
  },
  {
    "text": "today's standards computers nonetheless there's some efforts on solving optimization and so",
    "start": "1270559",
    "end": "1276159"
  },
  {
    "text": "on but it turns out that stochastic gradient desent Works quite well in practice and the reason of this is as I",
    "start": "1276159",
    "end": "1282559"
  },
  {
    "text": "said first firstly you have a very small sample to compute gradients that allow you to do progress without Computing a",
    "start": "1282559",
    "end": "1289440"
  },
  {
    "text": "gradient over billions of examples and secondly the chain rule because in the end you have this composition of many",
    "start": "1289440",
    "end": "1295679"
  },
  {
    "text": "functions that are all differentiable allows you to compute gradients actually quite fast um so the computation of",
    "start": "1295679",
    "end": "1302080"
  },
  {
    "text": "gradients if it was intractable that would make basically uh training impossible um and so here is just a",
    "start": "1302080",
    "end": "1309679"
  },
  {
    "text": "silly example of a one hidden layer actually zero hidden layer neural network where X and Y are just connected",
    "start": "1309679",
    "end": "1316640"
  },
  {
    "text": "with a weight Matrix plus a bias with the nonlinearity and as you can imagine",
    "start": "1316640",
    "end": "1321840"
  },
  {
    "text": "you can do chain rues at each stage um to compute derivatives more",
    "start": "1321840",
    "end": "1327080"
  },
  {
    "text": "interestingly I think is the fact that it turns out I mean at least in practice",
    "start": "1327080",
    "end": "1332919"
  },
  {
    "text": "you train these networks and you sort of train them again and almost all the times you don't seem to be affected by",
    "start": "1332919",
    "end": "1340720"
  },
  {
    "text": "non-convexity namely you start from a random position you let the algorithm",
    "start": "1340720",
    "end": "1346320"
  },
  {
    "text": "run and you kind of get the same accuracy the same level of precision regardless of where you started from so",
    "start": "1346320",
    "end": "1353520"
  },
  {
    "text": "this catastrophic area where you would have a local Optima that's really bad seems to not hold very much in practice",
    "start": "1353520",
    "end": "1361200"
  },
  {
    "text": "at least nowadays with the size of networks that we have of course there's noise in the system like the gradients",
    "start": "1361200",
    "end": "1366799"
  },
  {
    "text": "and there's techniques that regularize that add noise further noise but there's also some interesting theories that are",
    "start": "1366799",
    "end": "1373200"
  },
  {
    "text": "coming up from Montreal actually that seem to suggest that in fact if if you",
    "start": "1373200",
    "end": "1378600"
  },
  {
    "text": "have a very high dimensional space um local Minima is not going to be an issue anyways and of course we're optimizing",
    "start": "1378600",
    "end": "1385120"
  },
  {
    "text": "millions of parameters so we definitely have an optimization problem over High dimensional space and the in practice",
    "start": "1385120",
    "end": "1391400"
  },
  {
    "text": "and some some Theory especially from from physics um which deals with gum",
    "start": "1391400",
    "end": "1396520"
  },
  {
    "text": "processes which is not quite the surfaces we have um seem to suggest that most local Minima are actually very",
    "start": "1396520",
    "end": "1403960"
  },
  {
    "text": "close to the global Minima so who cares anyways um and indeed things work so so",
    "start": "1403960",
    "end": "1410400"
  },
  {
    "text": "to to recap and this is a slide I steal from Ilia we have these powerful models",
    "start": "1410400",
    "end": "1417120"
  },
  {
    "text": "that have these parameters that we somehow know how to learn via this optimization process and the models we",
    "start": "1417120",
    "end": "1425400"
  },
  {
    "text": "are want to we want models that we can learn from data and that are very expressive and deep neural networks",
    "start": "1425400",
    "end": "1431279"
  },
  {
    "text": "happen to fall into this sweet intersection of these two models and so that's you know the view of why also",
    "start": "1431279",
    "end": "1439120"
  },
  {
    "text": "deep learning or deep neural networks are the kind of right model to use when you deal with machine learning",
    "start": "1439120",
    "end": "1444919"
  },
  {
    "text": "especially if you have lots of data um although there's some interesting work also on very like small data sets and so",
    "start": "1444919",
    "end": "1451320"
  },
  {
    "text": "on but of course imag net and and these data sets generally have lots of data",
    "start": "1451320",
    "end": "1456559"
  },
  {
    "text": "however this is not to be said like that you cannot just take this as a blackbox",
    "start": "1456559",
    "end": "1461880"
  },
  {
    "text": "there's always some pain um that's to be had and this I mean the least is um I",
    "start": "1461880",
    "end": "1468679"
  },
  {
    "text": "guess in a good way for researchers like myself quite long so there's lots of open sort of problems or annoyances",
    "start": "1468679",
    "end": "1475960"
  },
  {
    "text": "rather when you are a practitioner but you have to sort of pick learning rates",
    "start": "1475960",
    "end": "1481159"
  },
  {
    "text": "sizes of the networks architectures what kind of pre-processing techniques you want to use so there's a lot of issues",
    "start": "1481159",
    "end": "1488279"
  },
  {
    "text": "that you have to deal with when in practice then you go ahead and you try to solve a problem with the neuron Network um so it it really is not as",
    "start": "1488279",
    "end": "1496799"
  },
  {
    "text": "good as it sounds and um and that's more like bridging to the next part of the",
    "start": "1496799",
    "end": "1502720"
  },
  {
    "text": "talk is mostly what I've talked about is mappings from X to Y where X and Y are",
    "start": "1502720",
    "end": "1508880"
  },
  {
    "text": "sort of fixed um objects like images and whether a binary label that tells you if",
    "start": "1508880",
    "end": "1514880"
  },
  {
    "text": "it's a cat or a dog um but what I'm very interested in so especially because I I have this speech recognition background",
    "start": "1514880",
    "end": "1521679"
  },
  {
    "text": "is in sequences so how do you deal with recurrency and sequences and first I have a very I'll",
    "start": "1521679",
    "end": "1528919"
  },
  {
    "text": "go quickly through this um but it's important because a lot of uh things I'm",
    "start": "1528919",
    "end": "1533960"
  },
  {
    "text": "going to say about sequences regard is is involving wordss and so uh words are",
    "start": "1533960",
    "end": "1540279"
  },
  {
    "text": "how do you think of wordss as a as a continuous signal and um I mean this",
    "start": "1540279",
    "end": "1545760"
  },
  {
    "text": "this again has been done over and over but it's been renamed perhaps uh again",
    "start": "1545760",
    "end": "1551159"
  },
  {
    "text": "recently with this word embeddings which is to say the way to input words into a",
    "start": "1551159",
    "end": "1557480"
  },
  {
    "text": "neuro Network is as simple as representing the word as one hot encoding so if you have a dictionary",
    "start": "1557480",
    "end": "1563760"
  },
  {
    "text": "that has 100,000 different words you literally take a word and put a one in",
    "start": "1563760",
    "end": "1569360"
  },
  {
    "text": "the position of the vocabulary where that word occurs and then you have this huge vector and you can then convert it",
    "start": "1569360",
    "end": "1576399"
  },
  {
    "text": "to these more like amenable vectors like like that you've seen like X and so on",
    "start": "1576399",
    "end": "1581679"
  },
  {
    "text": "Via a matrix multiply which is of course the sparse so you never want to do the Matrix multiply but you have a Matrix",
    "start": "1581679",
    "end": "1589080"
  },
  {
    "text": "that let's say is a thousand um by uh so you have this Vector that is very sparse",
    "start": "1589080",
    "end": "1595679"
  },
  {
    "text": "and then you have a huge Matrix this this Matrix is actually I mean I we are",
    "start": "1595679",
    "end": "1600919"
  },
  {
    "text": "actually preparing a a submission for icml and this Matrix right now has about",
    "start": "1600919",
    "end": "1606720"
  },
  {
    "text": "a billion parameters by itself because we have a vocabulary of that's 800,000 words and then we have a thousand",
    "start": "1606720",
    "end": "1612880"
  },
  {
    "text": "dimensional Vector to represent every word so do the multiplication it's 800 million parameters",
    "start": "1612880",
    "end": "1619159"
  },
  {
    "text": "and by multiplying and essentially the sparse multiplication is picking one of",
    "start": "1619159",
    "end": "1624440"
  },
  {
    "text": "these thousand dimensional vectors that correspond to whichever word you want to represent and even if you train very",
    "start": "1624440",
    "end": "1631000"
  },
  {
    "text": "simple models um using this sparse representation of words um such as this",
    "start": "1631000",
    "end": "1637880"
  },
  {
    "text": "was um recently done by thas mikolov who was at Google brain now at Facebook you",
    "start": "1637880",
    "end": "1643440"
  },
  {
    "text": "can sort of predict neighbors of of so you take text and you take two words uh",
    "start": "1643440",
    "end": "1648679"
  },
  {
    "text": "that are close by and you try to predict from the word sharks a word that surrounds the the word sharks so in this",
    "start": "1648679",
    "end": "1655520"
  },
  {
    "text": "case ocean and you train a model just to do that from this word input try to predict a word that's the neighbor of of",
    "start": "1655520",
    "end": "1662080"
  },
  {
    "text": "the input word and all you do is embed these word sharks with this sparse multiplication you train this on a large",
    "start": "1662080",
    "end": "1668760"
  },
  {
    "text": "Corpus and then cool things start happening like these vectors now each",
    "start": "1668760",
    "end": "1675039"
  },
  {
    "text": "word is a vector of let's say 200 dimensions have nearest neighbor and the nearest neighbors happen to be somewhat",
    "start": "1675039",
    "end": "1682559"
  },
  {
    "text": "related I mean you have all kinds of different shark and for cars you get automobile and pickup Tru and so on and",
    "start": "1682559",
    "end": "1689399"
  },
  {
    "text": "further what this they observed in the paper by the way this is very cool but I don't think we found a use case for this",
    "start": "1689399",
    "end": "1695840"
  },
  {
    "text": "other than being cool but you can actually take these vectors for all these words and also do sort of basic",
    "start": "1695840",
    "end": "1703240"
  },
  {
    "text": "arithmetics so I can sort of take Queen minus King and that difference Vector",
    "start": "1703240",
    "end": "1709519"
  },
  {
    "text": "happens to be similar than woman minus men so these These are kind of",
    "start": "1709519",
    "end": "1714919"
  },
  {
    "text": "interesting properties and there's sort of in the paper you can see there's all sorts of things I mean if you take Paris",
    "start": "1714919",
    "end": "1720640"
  },
  {
    "text": "minus France plus Italy you get Rome and so on so there's definitely it's kind of cool that this happened with a very",
    "start": "1720640",
    "end": "1725799"
  },
  {
    "text": "simple model it's not very deep in this in this case um so deep wasn't in that",
    "start": "1725799",
    "end": "1731320"
  },
  {
    "text": "paper but it's still was was very nice and it sort of bridges well how to",
    "start": "1731320",
    "end": "1736640"
  },
  {
    "text": "incorporate words or things of words as vectors um which then allow you to do uh to use neuron networks so the other",
    "start": "1736640",
    "end": "1743760"
  },
  {
    "text": "important bit uh in in dealing with sequences is understanding how difficult",
    "start": "1743760",
    "end": "1749519"
  },
  {
    "text": "would it be to model sequences or probability distribution over sequences",
    "start": "1749519",
    "end": "1755600"
  },
  {
    "text": "naively so imagine I all all I want is model probability distribution over sequences of two words and I have 10,000",
    "start": "1755600",
    "end": "1763360"
  },
  {
    "text": "words in my vocabulary so to do that I basically if I was training what's",
    "start": "1763360",
    "end": "1769320"
  },
  {
    "text": "called a Byram model you would simply count the occurrences of every pair of words and I would store that count okay",
    "start": "1769320",
    "end": "1777880"
  },
  {
    "text": "what that means is I need to store 10,000 square numbers in principle if",
    "start": "1777880",
    "end": "1783640"
  },
  {
    "text": "all the pairs could occur and as you can see as the vocabulary grows and especially as the length of the sequence",
    "start": "1783640",
    "end": "1790360"
  },
  {
    "text": "grows this becomes quite impossible statistic in in a statistical efficient",
    "start": "1790360",
    "end": "1796279"
  },
  {
    "text": "uh sense so there is is fortunately another chain rule derived from base rule which says wait a second don't just",
    "start": "1796279",
    "end": "1805360"
  },
  {
    "text": "produce the prob over pairs of words you can do that by doing this conditional",
    "start": "1805360",
    "end": "1811760"
  },
  {
    "text": "this chain rule that that's exact um so you can Pro do probability distribution",
    "start": "1811760",
    "end": "1817120"
  },
  {
    "text": "of the second word given the first word times the probability of the first word and as a result these two probability",
    "start": "1817120",
    "end": "1824000"
  },
  {
    "text": "distributions there that are factorized are over 10K so you are doing 10K",
    "start": "1824000",
    "end": "1829760"
  },
  {
    "text": "decisions and then you do another 10K decisions instead of 10K times 10 10K",
    "start": "1829760",
    "end": "1835279"
  },
  {
    "text": "and in general the chain rule reads as follows and it applies to any any sequence length so this is great because",
    "start": "1835279",
    "end": "1841919"
  },
  {
    "text": "it allows us to put proper joint probability distributions without any sort of um Independence assumptions over",
    "start": "1841919",
    "end": "1848880"
  },
  {
    "text": "sequences now these are probability distributions in general but how do we implement this with neuron networks so",
    "start": "1848880",
    "end": "1857159"
  },
  {
    "text": "to build this very quickly and very simply you can think of first modeling",
    "start": "1857159",
    "end": "1862360"
  },
  {
    "text": "diagrams as I said uh you could just essentially take previous word say am",
    "start": "1862360",
    "end": "1868120"
  },
  {
    "text": "and the next word would be from and you could learn a a a count model or a small",
    "start": "1868120",
    "end": "1874080"
  },
  {
    "text": "neuron net if you want that maps from that word to the next word and that would be a pretty naive model because",
    "start": "1874080",
    "end": "1879840"
  },
  {
    "text": "it's assuming um the words only depend on the previous words you could do that with a large Corpus of text and that's",
    "start": "1879840",
    "end": "1886559"
  },
  {
    "text": "fine that's a a very crappy language model but it's also like fast to train and it's actually used in in several",
    "start": "1886559",
    "end": "1893000"
  },
  {
    "text": "places um but so you have these neural networks that map YT minus one to",
    "start": "1893000",
    "end": "1898919"
  },
  {
    "text": "YT uh but then to implement the actual chain rule um is almost immediate to see",
    "start": "1898919",
    "end": "1906279"
  },
  {
    "text": "that all we need is to add connectivity not only from the input of the word t to",
    "start": "1906279",
    "end": "1912159"
  },
  {
    "text": "the next t plus one we have to add connectivity across time in the following way and this is clear to you",
    "start": "1912159",
    "end": "1919320"
  },
  {
    "text": "can clearly see that in that case to predict the word Barcelona um that",
    "start": "1919320",
    "end": "1924880"
  },
  {
    "text": "prediction depends on not only the word from but through connectivities of neuron networks depends also from the",
    "start": "1924880",
    "end": "1932080"
  },
  {
    "text": "word m i and then the special token start of sentence let's say so in a",
    "start": "1932080",
    "end": "1937720"
  },
  {
    "text": "sense we've implemented a neuron Network that predicts the next word given all",
    "start": "1937720",
    "end": "1943720"
  },
  {
    "text": "the previous words and that's actually what's called a recurrent neural network um and it's it's it's a fairly good",
    "start": "1943720",
    "end": "1950880"
  },
  {
    "text": "language model this was known before let's say uh the re um explosion of of",
    "start": "1950880",
    "end": "1957880"
  },
  {
    "text": "RNN research that happened maybe a a couple of years ago so this this actually has been around for the models",
    "start": "1957880",
    "end": "1965360"
  },
  {
    "text": "themselves actually were in the original back propagation paper so um and and one",
    "start": "1965360",
    "end": "1971200"
  },
  {
    "text": "of the cool things that I was mentioning is if you train um a five gram language model uh with s smoothing techniques and",
    "start": "1971200",
    "end": "1978880"
  },
  {
    "text": "so on you get perplexities that are 7 74 perplexity is just a branching Factor",
    "start": "1978880",
    "end": "1984080"
  },
  {
    "text": "more or less on every time I I have to decide on a new word what's how many",
    "start": "1984080",
    "end": "1989880"
  },
  {
    "text": "options do I have what's my I kind of I am in doubt to predict from the word I",
    "start": "1989880",
    "end": "1995639"
  },
  {
    "text": "am from I will kind of be out among 70 words right and the best you can do is",
    "start": "1995639",
    "end": "2000960"
  },
  {
    "text": "one word because then you know what word comes next um and rnn's or rather these",
    "start": "2000960",
    "end": "2007000"
  },
  {
    "text": "fky name things lstms that I'll explain in a second actually reduce the perplexity from 74 options down to 30 so",
    "start": "2007000",
    "end": "2013519"
  },
  {
    "text": "they are really good at modeling language um which of course has very longtime dependencies so a hidden Mark",
    "start": "2013519",
    "end": "2019399"
  },
  {
    "text": "of model or a simpler model would definitely not do as well um and then you can do already things like sample",
    "start": "2019399",
    "end": "2025960"
  },
  {
    "text": "this model so um despite the fact that the perplexity is quite low and it",
    "start": "2025960",
    "end": "2032000"
  },
  {
    "text": "actually is a state-of-the-art on a fairly this is a 1 billion work uh data set so it's very it's a lot of data the",
    "start": "2032000",
    "end": "2038279"
  },
  {
    "text": "samples you draw from this model are actually are good but also you could probably distinguish",
    "start": "2038279",
    "end": "2044720"
  },
  {
    "text": "these are probably not uh human samples I mean the last one for example some of",
    "start": "2044720",
    "end": "2049919"
  },
  {
    "text": "the obese people live five to eight years longer than others that seems I mean the World Knowledge we have is",
    "start": "2049919",
    "end": "2055919"
  },
  {
    "text": "probably if there's a study that we say obes people maybe live actually not longer but but shorter but nonetheless",
    "start": "2055919",
    "end": "2062040"
  },
  {
    "text": "it's a wellform sentence that the model gives us um if we sample this joint",
    "start": "2062040",
    "end": "2067079"
  },
  {
    "text": "probability space space um so we can do all sorts of things with these recurrent neural networks and so what are these",
    "start": "2067079",
    "end": "2072760"
  },
  {
    "text": "boxes that sort of connect elements in the simplest case it is almost as simple",
    "start": "2072760",
    "end": "2078679"
  },
  {
    "text": "as the neuronet I showed before so you read you have sort of a state that comes from previous time steps and an input",
    "start": "2078679",
    "end": "2085679"
  },
  {
    "text": "and you have two matrices that multiply these two vectors remember the X's are words and they are embedded in these 200",
    "start": "2085679",
    "end": "2092158"
  },
  {
    "text": "dimensional space say and the state could be 500 1,000 dimensional depend",
    "start": "2092159",
    "end": "2098079"
  },
  {
    "text": "depending on the on the task that you have and so to compute the current state there's this recurrent equation that",
    "start": "2098079",
    "end": "2104560"
  },
  {
    "text": "that's why it's called recurrent neuron networks that is a fairly simple neuron Network um and then from from this state",
    "start": "2104560",
    "end": "2111359"
  },
  {
    "text": "HT You can predict then a probability distribution over wordss and these W's there are learnable parameters you fit",
    "start": "2111359",
    "end": "2118720"
  },
  {
    "text": "uh the data set that you're given and you get this this very nice perplexities but it so happens that",
    "start": "2118720",
    "end": "2124760"
  },
  {
    "text": "there is a Model A recruit neuronet model that's much better that's called lstm that stands from long shortterm",
    "start": "2124760",
    "end": "2131800"
  },
  {
    "text": "memory and this was introduced as usual many years ago it's not only now that",
    "start": "2131800",
    "end": "2136880"
  },
  {
    "text": "people start maybe paying more attention to it and it just has a sort of Fier",
    "start": "2136880",
    "end": "2142560"
  },
  {
    "text": "recurrent equations that sort of take a state um in that case CT and HT is the",
    "start": "2142560",
    "end": "2149240"
  },
  {
    "text": "state it's two vectors and then it has some sort of logic um that computes the",
    "start": "2149240",
    "end": "2156040"
  },
  {
    "text": "next state given the previous state and and it has some sort of gating mechanism that I I'll I'll intuitively explain",
    "start": "2156040",
    "end": "2161640"
  },
  {
    "text": "hopefully just in in the next slide and one of the cool things nowadays if you are I mean I actually knew about this",
    "start": "2161640",
    "end": "2168200"
  },
  {
    "text": "model in 2011 but it was sort of a pain to implement the gradients for this model because there's many parameters",
    "start": "2168200",
    "end": "2174240"
  },
  {
    "text": "and you could get confused in many places but nowadays we have all these open source libraries so there is almost",
    "start": "2174240",
    "end": "2180480"
  },
  {
    "text": "a onetoone mapping between the actual equations and then some python code that will just give you the gradients for",
    "start": "2180480",
    "end": "2187000"
  },
  {
    "text": "free so you could just Define any sort of model you want and then optimize given some data set so um it's it's very",
    "start": "2187000",
    "end": "2194079"
  },
  {
    "text": "simple to Define these models with Frameworks like theano or tensorflow or or whatever else is there in the open",
    "start": "2194079",
    "end": "2199880"
  },
  {
    "text": "source walk uh so that of course makes uh many more people try these things because it's relatively straightforward",
    "start": "2199880",
    "end": "2206280"
  },
  {
    "text": "to do so and just some intuition about these lstm so so all all I want you to note is",
    "start": "2206280",
    "end": "2213480"
  },
  {
    "text": "this part here which to the current state at time t instead of doing that Matrix multiply",
    "start": "2213480",
    "end": "2220960"
  },
  {
    "text": "and sigmoid all you do is you have this forget Gate Ft times CT minus one so that's sort of",
    "start": "2220960",
    "end": "2227839"
  },
  {
    "text": "an integrator that if the foret gate if this F was one or rather Infinity I",
    "start": "2227839",
    "end": "2233720"
  },
  {
    "text": "guess sigmoid of infinity is one so to CT you always are adding CT minus one so",
    "start": "2233720",
    "end": "2239319"
  },
  {
    "text": "you sort of have this leaky integrator you always add this state to yourself and what this allows you to do is",
    "start": "2239319",
    "end": "2246520"
  },
  {
    "text": "shortcut long-term dependencies so if I try to predict that I speak Catalan and",
    "start": "2246520",
    "end": "2252920"
  },
  {
    "text": "of course after speak there's going to be only a certain number of words that are possible like languages and and fast",
    "start": "2252920",
    "end": "2259440"
  },
  {
    "text": "maybe and slow and so on but if I had in my memory recorded that I've said",
    "start": "2259440",
    "end": "2265560"
  },
  {
    "text": "sometime in the past maybe 20 or 30 words ago I said Barcelona the now suddenly the probability that I speak",
    "start": "2265560",
    "end": "2272359"
  },
  {
    "text": "Catalan and not any other language should increase but that will only be carried if you indeed can learn these",
    "start": "2272359",
    "end": "2278760"
  },
  {
    "text": "long-term dependencies and if you actually unroll the equation over time",
    "start": "2278760",
    "end": "2284079"
  },
  {
    "text": "for a recur net you will observe the thing on above which is quite difficult",
    "start": "2284079",
    "end": "2290200"
  },
  {
    "text": "to optimize because you have all these nonlinearities times and and you have this wh The",
    "start": "2290200",
    "end": "2296599"
  },
  {
    "text": "Matrix um that is the recuring Matrix applied over and over and over T times steps and what happens when you if you",
    "start": "2296599",
    "end": "2304079"
  },
  {
    "text": "simplify this and you think of a linear system if you have a matrix multiplied by itself many times it either explodes",
    "start": "2304079",
    "end": "2310119"
  },
  {
    "text": "because the largest Zen value is greater than one or it goes to zero very quickly if the largest Z value is less than one",
    "start": "2310119",
    "end": "2317359"
  },
  {
    "text": "so it poses a very difficult um problem to transmit information in for T time",
    "start": "2317359",
    "end": "2324000"
  },
  {
    "text": "steps where T is large whereas these lstms uh if you unroll the equation and",
    "start": "2324000",
    "end": "2329760"
  },
  {
    "text": "assuming the forget Gates do not forget you actually get a fairly simple straightforward connection between the",
    "start": "2329760",
    "end": "2335920"
  },
  {
    "text": "outputs and inputs that are th time steps ago Bea through like basically one Matrix multiply plus one nonlinearity so",
    "start": "2335920",
    "end": "2343000"
  },
  {
    "text": "that's kind of why the big deal on these alms that if you read a little bit of literature you'll see there all all over",
    "start": "2343000",
    "end": "2349400"
  },
  {
    "text": "everyone that uses rnns uses some sort of lstm um and then okay so that's sort",
    "start": "2349400",
    "end": "2355680"
  },
  {
    "text": "of basic RNN and then what we worked on um mostly two years ago we started this",
    "start": "2355680",
    "end": "2363680"
  },
  {
    "text": "idea of mapping from sequences to sequences instead of mapping from a fixed X dimensional to Y dimensional so",
    "start": "2363680",
    "end": "2371560"
  },
  {
    "text": "how to do that will become clear in the next slide but this SL is largely updated and note that all the dates are",
    "start": "2371560",
    "end": "2377640"
  },
  {
    "text": "almost 2015 but there's been a large uh body of work on applying this sequence",
    "start": "2377640",
    "end": "2384680"
  },
  {
    "text": "to sequence framework now that's been called sequence to sequence to all sorts of task where the inputs the outputs",
    "start": "2384680",
    "end": "2390960"
  },
  {
    "text": "both or neither well neither wouldn't be the case but at least one of them is a sequence and so how would you do that",
    "start": "2390960",
    "end": "2398800"
  },
  {
    "text": "given what we know about recurent neuron Nets and that's the main idea it's very simple uh superficially uh but it",
    "start": "2398800",
    "end": "2405599"
  },
  {
    "text": "actually surprised to us when we tried this for machine translation it works very well so imagine I want to map a",
    "start": "2405599",
    "end": "2412880"
  },
  {
    "text": "sequence to a sequence let's say a bunch of French words a sentence in French to",
    "start": "2412880",
    "end": "2418160"
  },
  {
    "text": "the corresponding sentence in English so I just want to translate the sentence literally well I know how to model",
    "start": "2418160",
    "end": "2425160"
  },
  {
    "text": "language in English language very well with the recter neuronet that is just the right part there where I can predict",
    "start": "2425160",
    "end": "2432760"
  },
  {
    "text": "um Z given YX and beginning of sentence so that works very well but now how",
    "start": "2432760",
    "end": "2438720"
  },
  {
    "text": "could I also sort of model the English condition on being good English and also",
    "start": "2438720",
    "end": "2444760"
  },
  {
    "text": "condition on there's a sentence I want to translate from and then The Simple Solution was you add a recruit neon",
    "start": "2444760",
    "end": "2450880"
  },
  {
    "text": "Network that encodes the input sequence it doesn't produce any output its mission is to write to the hidden State",
    "start": "2450880",
    "end": "2458960"
  },
  {
    "text": "whatever is needed for it to then spit out the correct translation into the different language and the magic thing",
    "start": "2458960",
    "end": "2466839"
  },
  {
    "text": "here is that everything all the parameters are randomly initialized so we literally have the simplest almost",
    "start": "2466839",
    "end": "2473599"
  },
  {
    "text": "the simplest model you could imagine for machine translation namely I I will need to have supervision so I need to have",
    "start": "2473599",
    "end": "2479839"
  },
  {
    "text": "pairs of sentences that are translation of each other and after I I'm given that",
    "start": "2479839",
    "end": "2485520"
  },
  {
    "text": "I put a model that literally models the probability distribution of I want to",
    "start": "2485520",
    "end": "2490560"
  },
  {
    "text": "maximize the probability distribution of the correct English sentence given the input French sentences and I again will",
    "start": "2490560",
    "end": "2498960"
  },
  {
    "text": "run stochastic gradi in the S pass a lot of pairs of examples um and see hope for",
    "start": "2498960",
    "end": "2505359"
  },
  {
    "text": "the best basically so um and and as I said intuitively this hidden state is",
    "start": "2505359",
    "end": "2512000"
  },
  {
    "text": "sort of recording first it's reading sort of the French sentence putting it in memory and then decoding that that",
    "start": "2512000",
    "end": "2519359"
  },
  {
    "text": "memory of the French onto the hopefully correct um English translation um and then a note a little",
    "start": "2519359",
    "end": "2527240"
  },
  {
    "text": "like technical note of training versus inference so during training we will be given the pairs of sentences so we know",
    "start": "2527240",
    "end": "2534119"
  },
  {
    "text": "what Y is and we know what x is and then we just maximize the parameters of this neuron network but then once we trained",
    "start": "2534119",
    "end": "2541640"
  },
  {
    "text": "the model and we are happy with how it does in the training set we need to do",
    "start": "2541640",
    "end": "2546880"
  },
  {
    "text": "and a different task which is I'm going to be given the French words x and I",
    "start": "2546880",
    "end": "2552040"
  },
  {
    "text": "need to find the best possible English translation for this given by my model so I want to maximize why and this of",
    "start": "2552040",
    "end": "2560119"
  },
  {
    "text": "course is a is a sort of challenging problem because um you have many sequences and doing it naively would be",
    "start": "2560119",
    "end": "2566520"
  },
  {
    "text": "terrible so what you end up doing is you do some beam search aristic which is to say you predict the first word and you",
    "start": "2566520",
    "end": "2574200"
  },
  {
    "text": "keep a bunch of candidates of good first words given by the model and then you try all the first words as as as input",
    "start": "2574200",
    "end": "2581319"
  },
  {
    "text": "for the second word prediction and you keep doing that and it's a very straightforward thing to write I've",
    "start": "2581319",
    "end": "2586800"
  },
  {
    "text": "never wrote a decoder in my life and this was very straightforward actually so um and that actually works quite well",
    "start": "2586800",
    "end": "2592440"
  },
  {
    "text": "so to recap um in I think in this slide is more English to French instead of French to English but um never mind that",
    "start": "2592440",
    "end": "2599920"
  },
  {
    "text": "we have a model that takes any length uh sequence it passes through this Rec",
    "start": "2599920",
    "end": "2605200"
  },
  {
    "text": "neonet and it generates this vector from which it will then decode hopefully",
    "start": "2605200",
    "end": "2611119"
  },
  {
    "text": "onto French and it turns out this actually is I mean improves on most uh",
    "start": "2611119",
    "end": "2617480"
  },
  {
    "text": "language pairs over state-ofthe-art which which I mean machine translation is a fairly old um NLP task and this in",
    "start": "2617480",
    "end": "2625880"
  },
  {
    "text": "fact are results from Stanford because this is Tang who is a student here who has an intern with us a couple of years",
    "start": "2625880",
    "end": "2631400"
  },
  {
    "text": "ago and also last year and uh he just won a competition where um I guess as",
    "start": "2631400",
    "end": "2636720"
  },
  {
    "text": "you means Stanford University I guess so anyways uh the numbers look good and so all the other systems or most of the",
    "start": "2636720",
    "end": "2643760"
  },
  {
    "text": "other systems are sort of phrase-based machine translation systems which need the alignment between the words I mean",
    "start": "2643760",
    "end": "2650040"
  },
  {
    "text": "they need a lot of information and they really have a lot of tweaks so the fact that in kind of a relatively short uh",
    "start": "2650040",
    "end": "2657559"
  },
  {
    "text": "time span we were able to sort of achieve very good results on translation really made the community and ourselves",
    "start": "2657559",
    "end": "2664119"
  },
  {
    "text": "believe that this model that map sequences to sequence was good so then we tried all all others things",
    "start": "2664119",
    "end": "2669559"
  },
  {
    "text": "you you might have seen this too um but this is sort of a different kind of mapping where we want to caption an",
    "start": "2669559",
    "end": "2675480"
  },
  {
    "text": "image so in this case the input is not a sequence but the output is a sequence and since we can do probability of",
    "start": "2675480",
    "end": "2681599"
  },
  {
    "text": "English given French to do captions very un originally so we try to think of it",
    "start": "2681599",
    "end": "2687240"
  },
  {
    "text": "as exactly the same um uh way as translation is so we will be given many",
    "start": "2687240",
    "end": "2693119"
  },
  {
    "text": "pairs of images and captions and we again will fit a model that can map from the image to English and in this case",
    "start": "2693119",
    "end": "2701520"
  },
  {
    "text": "the input is a very special sequence that is of length one um and it doesn't happen to be a word It's actually an",
    "start": "2701520",
    "end": "2708000"
  },
  {
    "text": "image um but since we have these very good models that classify objects what we can do is take features from these",
    "start": "2708000",
    "end": "2714720"
  },
  {
    "text": "neuron networks that are trained to classify objects take those features as a special word Vector if you want on",
    "start": "2714720",
    "end": "2721800"
  },
  {
    "text": "which we condition to produce things like a young girl is asleep with a teddy bear or whatever um the human",
    "start": "2721800",
    "end": "2728400"
  },
  {
    "text": "transcriber decided this this image was like and luckily there was a very cool",
    "start": "2728400",
    "end": "2734599"
  },
  {
    "text": "data set put together by Microsoft actually and there was a competition where you know people could submit to c",
    "start": "2734599",
    "end": "2740480"
  },
  {
    "text": "a server where the test data was held in place and you could only submit five times yada yada so the typical rules um",
    "start": "2740480",
    "end": "2748319"
  },
  {
    "text": "but basically there were like I think 16 teams a lot of them actually converged",
    "start": "2748319",
    "end": "2754520"
  },
  {
    "text": "to use this recruiter neuron networks or some form of sequence to sequence some other teams actually used other",
    "start": "2754520",
    "end": "2760559"
  },
  {
    "text": "techniques for example one that's very different is MSR had this entry which was nearest neighbor which is sort of",
    "start": "2760559",
    "end": "2766640"
  },
  {
    "text": "tell me which image is closest to my training set and that's going to be the sentence that I caption this image with",
    "start": "2766640",
    "end": "2772440"
  },
  {
    "text": "and that actually does not too bad as you can see um in the ranking I only put the top eight so this was great the",
    "start": "2772440",
    "end": "2780079"
  },
  {
    "text": "results of automatic metrics of quality were Superior to human which of course",
    "start": "2780079",
    "end": "2785559"
  },
  {
    "text": "should make you very suspicious and indeed the case was that in the competition they had they asked a bunch",
    "start": "2785559",
    "end": "2792960"
  },
  {
    "text": "of mechanical T graders for uh whether they you know kind of the system passed the touring test and in that case the",
    "start": "2792960",
    "end": "2800800"
  },
  {
    "text": "two metrics on the right are the human metrics which are the ones we probably should care about and as you can see",
    "start": "2800800",
    "end": "2807720"
  },
  {
    "text": "even though there has there is correlation between automatic metrics and and so to so to speak human metrics",
    "start": "2807720",
    "end": "2814359"
  },
  {
    "text": "humans happen to be much better at being humans than than all the systems that",
    "start": "2814359",
    "end": "2819640"
  },
  {
    "text": "participated so 6 points versus two or so points that the best systems were",
    "start": "2819640",
    "end": "2825000"
  },
  {
    "text": "able to do is quite a difference and so it's this problem is largely unsolved",
    "start": "2825000",
    "end": "2830079"
  },
  {
    "text": "and uh but it was sort of going Beyond this mapping to a single object so you could map to a",
    "start": "2830079",
    "end": "2836400"
  },
  {
    "text": "sentence and then I mean looking at at how the model performs um especially",
    "start": "2836400",
    "end": "2842720"
  },
  {
    "text": "after tuning all these hyper parameters that I was mentioning this this is the painful part of NE on networks um the",
    "start": "2842720",
    "end": "2848640"
  },
  {
    "text": "best model after the paper submission and which we submitted for the competition was much better even though",
    "start": "2848640",
    "end": "2855599"
  },
  {
    "text": "the initial model graciously gave some answer that was somewhat related to the image um but in the end the same",
    "start": "2855599",
    "end": "2863319"
  },
  {
    "text": "architecture but tuning hyper parameters and doing a little bit of of sort of search over the search space that is",
    "start": "2863319",
    "end": "2870000"
  },
  {
    "text": "neon networks um yielded something that was in some cases clearly Superior uh",
    "start": "2870000",
    "end": "2876599"
  },
  {
    "text": "than than the initial model however the initial model was way better than most",
    "start": "2876599",
    "end": "2881680"
  },
  {
    "text": "of the systems that were prior to last year I must say as you saw like lots of groups published on on this task and uh",
    "start": "2881680",
    "end": "2889000"
  },
  {
    "text": "now they have actually moved to sort of visual question answering or describing videos instead of Steel images in which",
    "start": "2889000",
    "end": "2895960"
  },
  {
    "text": "sequence to sequence clearly applies um but nonetheless the results",
    "start": "2895960",
    "end": "2901640"
  },
  {
    "text": "superficially at least look quite impressive in that wow it seems like the computer really got uh it of course",
    "start": "2901640",
    "end": "2908000"
  },
  {
    "text": "there's a lot of language modeling going on too um so if you detect there's a train saying there is a train on the",
    "start": "2908000",
    "end": "2913839"
  },
  {
    "text": "tracks is probably a very good sentence because it's very unlikely the train is going to be flying around um and indeed",
    "start": "2913839",
    "end": "2920599"
  },
  {
    "text": "if you present A Train That's flying around it's not going to work so um the priors are very important in this model",
    "start": "2920599",
    "end": "2928440"
  },
  {
    "text": "um yes resources requ get all those outputs the same from all the different teams",
    "start": "2928440",
    "end": "2935160"
  },
  {
    "text": "um I would say the model so the model sizes which relate to how much you have",
    "start": "2935160",
    "end": "2940599"
  },
  {
    "text": "to use um so our model was fairly small I would say uh it was 500 the the hidden",
    "start": "2940599",
    "end": "2947400"
  },
  {
    "text": "state was 500 dimension for example the Berkeley model was had a th000 dimensions and had three depths of layer",
    "start": "2947400",
    "end": "2954119"
  },
  {
    "text": "so so their model was bigger so so it it required more more compute power typically gpus um so so because of this",
    "start": "2954119",
    "end": "2962440"
  },
  {
    "text": "data set is relatively small overfitting is more of an issue so actually actually",
    "start": "2962440",
    "end": "2968079"
  },
  {
    "text": "maybe the computational resources you use to search over parameters but in the end the last the the model you end up",
    "start": "2968079",
    "end": "2973920"
  },
  {
    "text": "with is fairly small and quick to run this actually can run um I mean you can",
    "start": "2973920",
    "end": "2979960"
  },
  {
    "text": "get a caption in in less than a second for any image so you could you could",
    "start": "2979960",
    "end": "2985880"
  },
  {
    "text": "potentially um use this to you know to label things that you have and so on um",
    "start": "2985880",
    "end": "2991760"
  },
  {
    "text": "but indeed computational power in general for some models is important and for imet for example it it is true that",
    "start": "2991760",
    "end": "2999400"
  },
  {
    "text": "using um so I think we now use 32 gpus in parallel to train and it takes you",
    "start": "2999400",
    "end": "3004920"
  },
  {
    "text": "know maybe two or three days but if a student has only eight gpus well then it will take some more days right um but",
    "start": "3004920",
    "end": "3013400"
  },
  {
    "text": "luckily most of the data sets we use especially when we do research are publicly available and not that large so",
    "start": "3013400",
    "end": "3020400"
  },
  {
    "text": "it's still manageable for a grad student that has a bunch of gpus um so another thing we which was",
    "start": "3020400",
    "end": "3027319"
  },
  {
    "text": "kind of more of a silly thing but it the history of this is I I wondered whether we could um apply machine translation",
    "start": "3027319",
    "end": "3034960"
  },
  {
    "text": "but to translate from a question to an answer so we had this this silly idea of",
    "start": "3034960",
    "end": "3040040"
  },
  {
    "text": "taking movie subtitles and essentially try to predict the next conversation turn given the previous conversations if",
    "start": "3040040",
    "end": "3046440"
  },
  {
    "text": "that makes sense and um I mean the experiment resulted on",
    "start": "3046440",
    "end": "3051720"
  },
  {
    "text": "some some interesting sort of uh let's say conversations if you will will that",
    "start": "3051720",
    "end": "3057079"
  },
  {
    "text": "you could have with a machine definitely very far from anything that will you know would pass the tring test but it",
    "start": "3057079",
    "end": "3064119"
  },
  {
    "text": "was very straightforward to train these models I didn't have to make a rule-based system that you know dealt",
    "start": "3064119",
    "end": "3070040"
  },
  {
    "text": "with conversation State and whether I said high or no all this was sort of given by a large set of uh subtitles",
    "start": "3070040",
    "end": "3077760"
  },
  {
    "text": "namely all the subtitles that were in some website open subtitles do something I guess and uh in in general it was sort",
    "start": "3077760",
    "end": "3086000"
  },
  {
    "text": "of believable that the model would give an interesting answer um and and this",
    "start": "3086000",
    "end": "3092040"
  },
  {
    "text": "there's again this is again sequence to sequence and the cool part is that",
    "start": "3092040",
    "end": "3097200"
  },
  {
    "text": "somehow Google decided this was nice enough that they um actually there is a",
    "start": "3097200",
    "end": "3102599"
  },
  {
    "text": "product out there that you know does the obvious thing which is given some short email especially if you're on your phone",
    "start": "3102599",
    "end": "3108799"
  },
  {
    "text": "it it replies to your email and it's using sort of the same the same model",
    "start": "3108799",
    "end": "3113880"
  },
  {
    "text": "that we did some earlier research on so I mean the system just essentially reads",
    "start": "3113880",
    "end": "3120559"
  },
  {
    "text": "your email and decides first if if it should reply automatically or not with a",
    "start": "3120559",
    "end": "3126119"
  },
  {
    "text": "classifier and if it decides that it should reply it will put some sort of short replies like kind of not very",
    "start": "3126119",
    "end": "3132520"
  },
  {
    "text": "exciting but um that you could just click and immediately get the answer so so you know this is sort of a cool",
    "start": "3132520",
    "end": "3139079"
  },
  {
    "text": "example from me that from the initial research paper to the applied research paper which which was the conversation",
    "start": "3139079",
    "end": "3145440"
  },
  {
    "text": "paper to then then something that we launched there was quite a quick I would say quick um and there's a research blog",
    "start": "3145440",
    "end": "3152359"
  },
  {
    "text": "post on that if you're interested or you could try it on your phone um please don't send emails to me um and then I'm",
    "start": "3152359",
    "end": "3159400"
  },
  {
    "text": "I'm not going to explain much of these um except the fact that there is a very",
    "start": "3159400",
    "end": "3165599"
  },
  {
    "text": "challenging thing here if you didn't realize yet is which is we need to cram",
    "start": "3165599",
    "end": "3172119"
  },
  {
    "text": "all the sentences that we read or the sequence input rather into to this fix dimensional Vector so if we have a very",
    "start": "3172119",
    "end": "3179640"
  },
  {
    "text": "long input which we might if we want to say read a book and Summarize the book",
    "start": "3179640",
    "end": "3184680"
  },
  {
    "text": "now we are trying to read word by word a whole book and hope that the state will",
    "start": "3184680",
    "end": "3190599"
  },
  {
    "text": "Summarize the book so that is a very almost impossible like to imagine this",
    "start": "3190599",
    "end": "3196359"
  },
  {
    "text": "would work and one of the cool advances last year was the addition of memory and",
    "start": "3196359",
    "end": "3202160"
  },
  {
    "text": "and or attention this has several names and the the the brilliant idea is as follows is instead of trying to put this",
    "start": "3202160",
    "end": "3210160"
  },
  {
    "text": "ABCD into this box and keep putting information into it to then decode it we",
    "start": "3210160",
    "end": "3216480"
  },
  {
    "text": "will instead allow the model for as it decodes words out go and take a look at",
    "start": "3216480",
    "end": "3222839"
  },
  {
    "text": "all the inputs it's in um and by take a look I mean do a memory read um sort of",
    "start": "3222839",
    "end": "3229839"
  },
  {
    "text": "what you would do in a machine so now ABCD instead of trying to put all the",
    "start": "3229839",
    "end": "3236200"
  },
  {
    "text": "information information through these long boxes here they are just kind of a",
    "start": "3236200",
    "end": "3241960"
  },
  {
    "text": "memory so you have this Matrix which is number of time uh inputs um and of size",
    "start": "3241960",
    "end": "3249160"
  },
  {
    "text": "whatever the hidden state of that neuron recur neuronet is and then at the output",
    "start": "3249160",
    "end": "3254520"
  },
  {
    "text": "you say okay before I output X I will go to all the inputs and see what what I",
    "start": "3254520",
    "end": "3262480"
  },
  {
    "text": "want to read from it and then bring it back to the output decod and use it to maximally predict X and you do that I",
    "start": "3262480",
    "end": "3269920"
  },
  {
    "text": "just put that for X but you do you would have this neuron net being used over and over every time you produce a new output",
    "start": "3269920",
    "end": "3276640"
  },
  {
    "text": "and the math is there's some details on like position based attention when where",
    "start": "3276640",
    "end": "3282200"
  },
  {
    "text": "the where where you attend is pos position based so it's the beginning or the end and the thing that got",
    "start": "3282200",
    "end": "3287839"
  },
  {
    "text": "introduced which was cooler in my opinion was content based attention which is I don't care where it is but",
    "start": "3287839",
    "end": "3294680"
  },
  {
    "text": "let's say I want to produce an animal because the next word seems to be an animal please look for animal like",
    "start": "3294680",
    "end": "3300319"
  },
  {
    "text": "things in the input whatever they are and then what what looks like an animal you bring it and then you decide which",
    "start": "3300319",
    "end": "3306040"
  },
  {
    "text": "animal it is so you can do computation so rather to have a single Vector doing",
    "start": "3306040",
    "end": "3311280"
  },
  {
    "text": "all the memorization of the input you distribute the memory uh across these boxes and you allow through more",
    "start": "3311280",
    "end": "3317960"
  },
  {
    "text": "computation to read the whole memory um so that that's a very cool Model H I'm not going to say much more about this",
    "start": "3317960",
    "end": "3324680"
  },
  {
    "text": "and there was a related paper from both uh Deep Mind and and Facebook the first",
    "start": "3324680",
    "end": "3329920"
  },
  {
    "text": "one is called neural toing machine and the second is memory networks but essentially the mechanism is sort of",
    "start": "3329920",
    "end": "3335119"
  },
  {
    "text": "easily described as a memory accessing mechanism and then for machine translation for example without any",
    "start": "3335119",
    "end": "3341640"
  },
  {
    "text": "supervision on alignments you can you can see that the model is attending to the word that is translating from and",
    "start": "3341640",
    "end": "3347760"
  },
  {
    "text": "and and cool things like this um there was a paper on on again captioning where the word being output also many times",
    "start": "3347760",
    "end": "3355079"
  },
  {
    "text": "were focusing on the out on the region of the image that was being output so a stop sign it's focusing on the stop sign",
    "start": "3355079",
    "end": "3361720"
  },
  {
    "text": "and trees is focusing on everything but the giraffe and so on and a paper that we did was which",
    "start": "3361720",
    "end": "3368319"
  },
  {
    "text": "kind of pushed the envelope of what would be defined to be saying to do um",
    "start": "3368319",
    "end": "3374240"
  },
  {
    "text": "was to try to use these mechanisms to solve algorithms so now my supervision",
    "start": "3374240",
    "end": "3380760"
  },
  {
    "text": "will be I'm going to give you points on the plane literally so I'm going to give a sequence of points and I'm going to",
    "start": "3380760",
    "end": "3387160"
  },
  {
    "text": "give you also the solution of certain properties of these points so you could imagine traveling Salman problem so I'm",
    "start": "3387160",
    "end": "3393440"
  },
  {
    "text": "going to you know NP hardly solve it and give you supervision on pairs of input",
    "start": "3393440",
    "end": "3398599"
  },
  {
    "text": "sequences namely the points and output sequences namely the vertices and can we",
    "start": "3398599",
    "end": "3404440"
  },
  {
    "text": "train the same models uh to learn sort of these functions that I actually don't",
    "start": "3404440",
    "end": "3409960"
  },
  {
    "text": "know how the algorithms work or I it would have a hard time for me to design an algorithm to do this in a reasonable",
    "start": "3409960",
    "end": "3416359"
  },
  {
    "text": "amount of time and so we tried this for several geometric properties and the result is essentially is that for like",
    "start": "3416359",
    "end": "3423520"
  },
  {
    "text": "very moderate number of cities namely maybe 50 cities um uh the neuron network",
    "start": "3423520",
    "end": "3429440"
  },
  {
    "text": "was able to uh sort of understand the concept of minimum path given the points",
    "start": "3429440",
    "end": "3435920"
  },
  {
    "text": "and so typical errors there's not many errors but typical errors would be something like this where you can see I",
    "start": "3435920",
    "end": "3442119"
  },
  {
    "text": "think here there is some sort of error where you don't go through the quite",
    "start": "3442119",
    "end": "3447280"
  },
  {
    "text": "shortest path but it kind of got the this idea that given the points it needs",
    "start": "3447280",
    "end": "3452799"
  },
  {
    "text": "to Traverse them in some like Optimal order and it got that just from the data not hardcoding anything so that was kind",
    "start": "3452799",
    "end": "3458599"
  },
  {
    "text": "of cool and um just to conclude I I I think there's many open problems that",
    "start": "3458599",
    "end": "3465079"
  },
  {
    "text": "many people could could uh contribute from several fields and this is already happening um given the interest on just",
    "start": "3465079",
    "end": "3472280"
  },
  {
    "text": "machine learning in general really deep learning is just the the F you know part of of the story um but you know",
    "start": "3472280",
    "end": "3481000"
  },
  {
    "text": "there's I mean there here is a list it's there's lots of open problems unsupervised learning optimization has",
    "start": "3481000",
    "end": "3487200"
  },
  {
    "text": "bothered me for for a while and I unfortunately haven't worked on it since I left Berkeley but um you know someday",
    "start": "3487200",
    "end": "3493760"
  },
  {
    "text": "maybe and um and you know it needs lots of data so so you know for example to learn to learn this concept of shortest",
    "start": "3493760",
    "end": "3501119"
  },
  {
    "text": "path uh we needed at least let's say thousands or tens of thousands of of examples pairs of points and shortest",
    "start": "3501119",
    "end": "3508000"
  },
  {
    "text": "path and uh I mean I can I can literally show a human even without describing",
    "start": "3508000",
    "end": "3513359"
  },
  {
    "text": "input outputs and maybe the human will get at least the concept of comx Halls are quite visual so um like data",
    "start": "3513359",
    "end": "3520920"
  },
  {
    "text": "efficiency is definitely a problem uh although you know very good research has been going on on on on that and",
    "start": "3520920",
    "end": "3527880"
  },
  {
    "text": "basically I think the daycom message the most important from from my experience",
    "start": "3527880",
    "end": "3533720"
  },
  {
    "text": "uh doing research the last two years is that se have really become first class citizens so so now like there's a very",
    "start": "3533720",
    "end": "3539720"
  },
  {
    "text": "cool set of research where it it's no longer scary to have a sequences of observation as the input or as the",
    "start": "3539720",
    "end": "3545880"
  },
  {
    "text": "output and we are even moving beyond that um I've myself done some work on",
    "start": "3545880",
    "end": "3550920"
  },
  {
    "text": "thinking about sets instead of sequences because some of these models as you can imagine for example the the tsp the",
    "start": "3550920",
    "end": "3558079"
  },
  {
    "text": "input is not a sequence of points even though we read them one by one it's really a set of points and so there's",
    "start": "3558079",
    "end": "3564160"
  },
  {
    "text": "there's all sorts of interesting aspect of learning algorithms and sort of other data structures that are interesting and",
    "start": "3564160",
    "end": "3570799"
  },
  {
    "text": "I want to emphasize that to me deep learning is not AI um it's really it's",
    "start": "3570799",
    "end": "3576680"
  },
  {
    "text": "just a mapping function that can do quite quite cool things I would say um",
    "start": "3576680",
    "end": "3581920"
  },
  {
    "text": "but uh you know it's it's sort of I guess if you do search like like in the",
    "start": "3581920",
    "end": "3587839"
  },
  {
    "text": "in the recent paper of today where they combine deep learning with search to solve go you could mention well that",
    "start": "3587839",
    "end": "3594480"
  },
  {
    "text": "that's sort of a very hard task for for from an AI perspective um and deep learning certainly helped nail down or",
    "start": "3594480",
    "end": "3601200"
  },
  {
    "text": "narrow down the search space that would be massive if you were to solve go uh naively and with that I I I'm happy to",
    "start": "3601200",
    "end": "3609240"
  },
  {
    "text": "take questions I think we have about 15 minutes left and this is me in Halloween",
    "start": "3609240",
    "end": "3614640"
  },
  {
    "text": "I don't usually wear dog dresses but um maybe I'll delete the original picture so it's less clear what I was doing here",
    "start": "3614640",
    "end": "3622640"
  },
  {
    "text": "uh anyways thank you very much thanks [Applause]",
    "start": "3622640",
    "end": "3632000"
  },
  {
    "text": "so any questions if you want um since you talk",
    "start": "3632000",
    "end": "3639200"
  },
  {
    "text": "touring tests why not first do the Shannon test what's the Shannon test well we should have worked that",
    "start": "3639200",
    "end": "3645760"
  },
  {
    "text": "out guessing the next character given the previous ones that's it okay we know",
    "start": "3645760",
    "end": "3651079"
  },
  {
    "text": "we can do that with maximum likelihood match",
    "start": "3651079",
    "end": "3656559"
  },
  {
    "text": "I haven't seen any deep learning so in a in a sense perplexity",
    "start": "3656559",
    "end": "3663319"
  },
  {
    "text": "yeah speech process yeah so I was wondering that myself they appli that actually",
    "start": "3663319",
    "end": "3668720"
  },
  {
    "text": "that so so one question that would be interesting for me to answer is how",
    "start": "3668720",
    "end": "3674559"
  },
  {
    "text": "what's what's for a given data set like the one that we train this gigantic model on what would be human performance",
    "start": "3674559",
    "end": "3682280"
  },
  {
    "text": "for example if they take 60,000 characters to for a",
    "start": "3682280",
    "end": "3689720"
  },
  {
    "text": "sparse sparse marov model okay to match it using maximum light C okay used to",
    "start": "3689799",
    "end": "3696640"
  },
  {
    "text": "run on a cray one teletype speed so matching Human Performance okay well so",
    "start": "3696640",
    "end": "3702240"
  },
  {
    "text": "can you beat that if your deep learning I'm I would say yes anyway that's a CH",
    "start": "3702240",
    "end": "3708520"
  },
  {
    "text": "okay others have redone it some people in Europe that also didn't know some of the ear work my students the so I'm not",
    "start": "3708520",
    "end": "3715520"
  },
  {
    "text": "cool people ever read it well I I will definitely read it now so that's cool I I was actually wondering myself I was",
    "start": "3715520",
    "end": "3721839"
  },
  {
    "text": "asking some linguist at the conference last last month about human performance because I have this perplexity of let's",
    "start": "3721839",
    "end": "3728520"
  },
  {
    "text": "say 25 is that I mean are humans are we close or not um and it's not clear how",
    "start": "3728520",
    "end": "3736240"
  },
  {
    "text": "humans would do in this task because it's sort of weird to have to give probabilities right plexy came up from",
    "start": "3736240",
    "end": "3742559"
  },
  {
    "text": "uh IBM speech recognition right they knew our work so right I I think speech has the nice",
    "start": "3742559",
    "end": "3748480"
  },
  {
    "text": "property where they didn't they didn't go deep warning the problem in speech I guess is",
    "start": "3748480",
    "end": "3754079"
  },
  {
    "text": "that the function you learn is deterministic to a large extent whereas",
    "start": "3754079",
    "end": "3759319"
  },
  {
    "text": "if I want to if if I ask you what's the probability of a certain sentence I mean there's you don't know right I mean you",
    "start": "3759319",
    "end": "3765200"
  },
  {
    "text": "know kind of you will know if it's a b sentence but you can estimate that prob yeah so that that's that's interesting I",
    "start": "3765200",
    "end": "3771599"
  },
  {
    "text": "I'll so Shannon I'll I'll definitely read on that I was oh I would take your shoe with that the problem with speech",
    "start": "3771599",
    "end": "3777200"
  },
  {
    "text": "is that you have a naive audience that thinks it's going to use these things and it doesn't see the important",
    "start": "3777200",
    "end": "3783240"
  },
  {
    "text": "pathological cases with a lot of these things that only the people who work in speech oh yeah that's that's like the",
    "start": "3783240",
    "end": "3791000"
  },
  {
    "text": "phrase how to recognize speech and how to recognize how to recog aize speech as",
    "start": "3791000",
    "end": "3797960"
  },
  {
    "text": "example the the naive the naive audience does doesn't see that how difficult that",
    "start": "3797960",
    "end": "3803599"
  },
  {
    "text": "comparison is and that's only within one langage the English language right but there's actually lots of examples of of",
    "start": "3803599",
    "end": "3810079"
  },
  {
    "text": "of that and it gets even tougher in in non-english languages right but if it makes the same mistakes that people make",
    "start": "3810079",
    "end": "3817200"
  },
  {
    "text": "yeah it does it does make the same mistakes ours did make the same mistakes",
    "start": "3817200",
    "end": "3823799"
  },
  {
    "text": "make different one but the the analysis of the two the comparison that's the big issue yes that's the interesting part",
    "start": "3823799",
    "end": "3831400"
  },
  {
    "text": "how stupid or How Smart errors did the machine May don't foret that the first",
    "start": "3831400",
    "end": "3836839"
  },
  {
    "text": "stupidity can be measured on one dimension that's a good point that pierce would make no the big problem is",
    "start": "3836839",
    "end": "3842119"
  },
  {
    "text": "actually a lot of the work initially was done it was funded decades ago for only one language that most people are",
    "start": "3842119",
    "end": "3847799"
  },
  {
    "text": "unaware of it was basically Russian and English it was there was no interest in general speech recognition that only",
    "start": "3847799",
    "end": "3853960"
  },
  {
    "text": "that only came decades later think because DARPA funded most of it it wasn't just DARPA it was the rest of the",
    "start": "3853960",
    "end": "3860000"
  },
  {
    "text": "NSA and the other military connected",
    "start": "3860000",
    "end": "3865720"
  },
  {
    "text": "question there yeah yeah uh with the traveling sales program uh is there any Prospect",
    "start": "3866119",
    "end": "3874000"
  },
  {
    "text": "getting a bound on the inefficiency of the solution definitely not um that",
    "start": "3874000",
    "end": "3882160"
  },
  {
    "text": "that's that's the bothering point of neuron networks that generally speaking it's hard to quantify",
    "start": "3882160",
    "end": "3890119"
  },
  {
    "text": "or sort of give any assurances of correctness um and definitely in that",
    "start": "3890119",
    "end": "3895400"
  },
  {
    "text": "case it's it's hard to know whether you cannot prove anything like it's 1.5 opt",
    "start": "3895400",
    "end": "3901520"
  },
  {
    "text": "or 2.0 opt or or anything like that um everything is probabalistic and not um I",
    "start": "3901520",
    "end": "3908119"
  },
  {
    "text": "mean it's it would be it would be quite a I mean an advancement to actually be",
    "start": "3908119",
    "end": "3914160"
  },
  {
    "text": "able to say look I can guarantee you with some probability you'll you'll get an algorithm that's 1.1 optimal um but",
    "start": "3914160",
    "end": "3921760"
  },
  {
    "text": "but a lot of this um field is for some of course unfortunately quite empirical",
    "start": "3921760",
    "end": "3928559"
  },
  {
    "text": "right so um you know that's I mean I I think in terms of special specifically",
    "start": "3928559",
    "end": "3934359"
  },
  {
    "text": "in neuronet and learning algorithms which I would say something that it res like again it kind of picked interest",
    "start": "3934359",
    "end": "3941039"
  },
  {
    "text": "maybe six months ago but but it was it has been attempted of course in the past uh you know like if you look more than a",
    "start": "3941039",
    "end": "3947039"
  },
  {
    "text": "decade ago um it's mostly of uh you know the questions you typically see can we",
    "start": "3947039",
    "end": "3953400"
  },
  {
    "text": "can we even do it and then hope F more deep questions will come uh as we say",
    "start": "3953400",
    "end": "3959039"
  },
  {
    "text": "well we want to make sure we don't you know waste gas by doing the most stupid",
    "start": "3959039",
    "end": "3964319"
  },
  {
    "text": "path you could ever do um to deliver these Goods or",
    "start": "3964319",
    "end": "3969079"
  },
  {
    "text": "whatever yeah patient does must some if you care about if you care about you know bounds and so on I I would say yes",
    "start": "3970279",
    "end": "3978200"
  },
  {
    "text": "um yet yet you could definitely combine this as an aristic for you know a real",
    "start": "3978200",
    "end": "3984720"
  },
  {
    "text": "algorithm right and that's precisely what they did in Go I mean there's so many moves I'm going",
    "start": "3984720",
    "end": "3990559"
  },
  {
    "text": "to consider only the top 10 given by a neuron Network and you know you might then prove that you could improve an",
    "start": "3990559",
    "end": "3997400"
  },
  {
    "text": "existing algorithm um by using some aristic that is not designed by a human",
    "start": "3997400",
    "end": "4004000"
  },
  {
    "text": "but it's actually learned from data that that I think is the first real kind of impact we might see from these learning",
    "start": "4004000",
    "end": "4010599"
  },
  {
    "text": "algorithms it will not do it end to end as we were we attempted to just solve the problem from from scratch I think it",
    "start": "4010599",
    "end": "4017039"
  },
  {
    "text": "will be a matter of integrating machine learning which again it's also been done in some I I believe in threat and things",
    "start": "4017039",
    "end": "4023240"
  },
  {
    "text": "like this to um to to like search algorithms or just algorithms in general",
    "start": "4023240",
    "end": "4030000"
  },
  {
    "text": "but it's it's a good point and this is I think why some some some time ago neets were not so popular because of lack of",
    "start": "4030000",
    "end": "4037640"
  },
  {
    "text": "understanding I guess what they really implemented right they still left that much right yeah not I mean in Vision I",
    "start": "4037640",
    "end": "4044920"
  },
  {
    "text": "think there maybe more some progress there but uh yeah I mean you analyze the",
    "start": "4044920",
    "end": "4050359"
  },
  {
    "text": "optimal inputs and you know this what's optimal you need some math for that yeah",
    "start": "4050359",
    "end": "4055520"
  },
  {
    "text": "well I need non comat so it's math it's not a sure yeah yeah indeed um over",
    "start": "4055520",
    "end": "4061400"
  },
  {
    "text": "there in your in your sequence to sequence conversion you you basically convert sequence into a state vector and",
    "start": "4061400",
    "end": "4068039"
  },
  {
    "text": "it doesn't seem to have very much SE information right was that done intentionally that was done uh sort of",
    "start": "4068039",
    "end": "4076039"
  },
  {
    "text": "honestly like that so that that was a paper at n in 2014 what what was the simplest",
    "start": "4076039",
    "end": "4083480"
  },
  {
    "text": "modification we could do to our code to go from generating language to generating language condition on a bunch",
    "start": "4083480",
    "end": "4089319"
  },
  {
    "text": "of things like French words or English words and um and it turned out that if",
    "start": "4089319",
    "end": "4096640"
  },
  {
    "text": "you had a large enough State space uh and for the language that we tried um it",
    "start": "4096640",
    "end": "4102719"
  },
  {
    "text": "it indeed that Vector became use big enough so that it could you know encode",
    "start": "4102719",
    "end": "4108960"
  },
  {
    "text": "a two or three word sentence which needs to know how to translate and that Corpus in particular has up to 70 or 80 wart",
    "start": "4108960",
    "end": "4116758"
  },
  {
    "text": "length uh sentences but it breaks down quickly in other cases and in that case",
    "start": "4116759",
    "end": "4122640"
  },
  {
    "text": "you need you need attention I mean yeah big point is the size of that state space inside so I assume that was that's",
    "start": "4122640",
    "end": "4129159"
  },
  {
    "text": "part of the thing yeah yeah okay so and my other question which is kind of related to that is if you think about",
    "start": "4129159",
    "end": "4136400"
  },
  {
    "text": "the further um expansion of that sort of intention and memory that is very interesting but that",
    "start": "4136400",
    "end": "4143960"
  },
  {
    "text": "seems like it also would make the training of that whole system much more complicated and longer because you you",
    "start": "4143960",
    "end": "4150920"
  },
  {
    "text": "need to now not only train the forward network but you have to basically have that Network decide what it wants to how",
    "start": "4150920",
    "end": "4158480"
  },
  {
    "text": "to name past right and it's not clear what's being connected in the past well",
    "start": "4158480",
    "end": "4164120"
  },
  {
    "text": "yeah so I I have not I mean I didn't explain in much detail what that memory",
    "start": "4164120",
    "end": "4169560"
  },
  {
    "text": "and I think the the what is very cool about that is",
    "start": "4169560",
    "end": "4175318"
  },
  {
    "text": "actually it's still a fit forward Network so this memory like if you",
    "start": "4175319",
    "end": "4182080"
  },
  {
    "text": "actually implement the computational graph of like Computing say a forward pass everything is still differentiable",
    "start": "4182080",
    "end": "4189520"
  },
  {
    "text": "and so the memory you don't you don't do a search over what places you want to read everything is learned from data so",
    "start": "4189520",
    "end": "4196000"
  },
  {
    "text": "these access patterns that emerge from translation you get these alignments that make sense and for programs like",
    "start": "4196000",
    "end": "4202600"
  },
  {
    "text": "the neural T machine paper had this sort of copying that you could see how the",
    "start": "4202600",
    "end": "4207640"
  },
  {
    "text": "the the twing machine was sort of writing to memory and then reading from memory all these patterns are learned in",
    "start": "4207640",
    "end": "4214920"
  },
  {
    "text": "a Feit forward fashion so so there's no like technical difference between no attention and",
    "start": "4214920",
    "end": "4220880"
  },
  {
    "text": "attention Loop unroll yeah you unroll it yeah so but that essentially says that",
    "start": "4220880",
    "end": "4226840"
  },
  {
    "text": "the complexity of the networks that you're learning the set of parameters you're learning then increases the set",
    "start": "4226840",
    "end": "4233199"
  },
  {
    "text": "so the set of parameters is constant because the tension mechanism has a",
    "start": "4233199",
    "end": "4238480"
  },
  {
    "text": "matrix an ex a couple extra matrices that are not a function of the length of the input but the complexity goes from o",
    "start": "4238480",
    "end": "4247239"
  },
  {
    "text": "n to o n squ roughly speaking so in sequence to sequence you read nword and you output",
    "start": "4247239",
    "end": "4254120"
  },
  {
    "text": "NW let's say if it's sort of the same length um and if you have memory you",
    "start": "4254120",
    "end": "4260159"
  },
  {
    "text": "input NW and then to Output the first word you sort of need to go and do this",
    "start": "4260159",
    "end": "4265199"
  },
  {
    "text": "Matrix multiply Over N input symbols and so on so the output becomes n squ but the number of parameters crucially",
    "start": "4265199",
    "end": "4272080"
  },
  {
    "text": "actually does not depend on the input length in this content based attention",
    "start": "4272080",
    "end": "4277640"
  },
  {
    "text": "in in the position based attention that that that's why like the content based attention is such a leap I think in",
    "start": "4277640",
    "end": "4283440"
  },
  {
    "text": "attention mechanisms and I mean saying the amount of content you can have doesn't scale the",
    "start": "4283440",
    "end": "4289320"
  },
  {
    "text": "number it has to depend on uh it does it does so let me I mean I",
    "start": "4289320",
    "end": "4295199"
  },
  {
    "text": "can show you the the equations um",
    "start": "4295199",
    "end": "4301000"
  },
  {
    "text": "so essentially like you have you have a vector a state Vector at the as you decode so here I'm decoding I'm trying",
    "start": "4301280",
    "end": "4308320"
  },
  {
    "text": "to predict y from X and there's this state Vector of the recur neuronet V okay what attention does",
    "start": "4308320",
    "end": "4315760"
  },
  {
    "text": "is it computes D transpose times the vectors that you encoded EI so let's",
    "start": "4315760",
    "end": "4323040"
  },
  {
    "text": "let's imagine you encoded a sequence of length uh in this case in this example length four okay so I have e E1 E2 E3 E4",
    "start": "4323040",
    "end": "4332360"
  },
  {
    "text": "I have this let's say the input is I am a cat Okay so I have these four hidden",
    "start": "4332360",
    "end": "4337560"
  },
  {
    "text": "States from the recur Network that encoded okay and then I compute D",
    "start": "4337560",
    "end": "4343639"
  },
  {
    "text": "transpose time EI so so there's no parameters at all right so you take the output State you multiply by the input",
    "start": "4343639",
    "end": "4349840"
  },
  {
    "text": "state for each of the input States that's the part that's o n right because I need to do four Matrix or like rather",
    "start": "4349840",
    "end": "4355560"
  },
  {
    "text": "Vector multiplies and I will get numbers that are that I can normalize to be from",
    "start": "4355560",
    "end": "4361679"
  },
  {
    "text": "0 to one and then these numbers I'm going to use them to read this E1",
    "start": "4361679",
    "end": "4368199"
  },
  {
    "text": "through E4 um so in this case I decide I want to read position number three with",
    "start": "4368199",
    "end": "4373880"
  },
  {
    "text": "weight point9 so it's sort of a soft pointer if you win if you will over all the inputs and if I have instead of four",
    "start": "4373880",
    "end": "4382560"
  },
  {
    "text": "inputs if I have eight inputs still no parameters you can just do the same equations you get a eight",
    "start": "4382560",
    "end": "4389000"
  },
  {
    "text": "long vector and you can still do the weighted sum with this Vector times the E1 through E8 you get D Prime that's the",
    "start": "4389000",
    "end": "4397040"
  },
  {
    "text": "read out from the input and you're in business so I mean it's I'm sorry if",
    "start": "4397040",
    "end": "4402840"
  },
  {
    "text": "it's not super clear from from this but but it really crucially does not depend on the length of course now if you train",
    "start": "4402840",
    "end": "4410800"
  },
  {
    "text": "with length up to 100 and at test time you decide to test it on sequences of of length",
    "start": "4410800",
    "end": "4416920"
  },
  {
    "text": "2,000 it very likely will fail but there's some amount of extrapolation these models can do actually",
    "start": "4416920",
    "end": "4425159"
  },
  {
    "text": "um um but so to speak you're at the output you have a vector I'm going to inner product this with all the input",
    "start": "4425159",
    "end": "4431159"
  },
  {
    "text": "vectors however many they are and I'm going to use that result to comp a weighted sum to reduce that to a single",
    "start": "4431159",
    "end": "4438080"
  },
  {
    "text": "Vector being a read out from the inputs it's it's it's a very it's a cool",
    "start": "4438080",
    "end": "4444280"
  },
  {
    "text": "thing that it works and again this is all like differentiable so it so happens",
    "start": "4444280",
    "end": "4449440"
  },
  {
    "text": "that the things we read um for example we we have a paper on parsing that it",
    "start": "4449440",
    "end": "4455639"
  },
  {
    "text": "turns out that you need to build a tree at the output and it it the mem the attention mechanism was doing s tracking",
    "start": "4455639",
    "end": "4461840"
  },
  {
    "text": "like sub trees and so on and we did not care what it did as long as it did you know fit the training",
    "start": "4461840",
    "end": "4469440"
  },
  {
    "text": "data um you mentioned earlier that there is the set of Weights associated with",
    "start": "4469440",
    "end": "4474679"
  },
  {
    "text": "each neuron can be considered to computer function yeah has there been any work done in relation to the total",
    "start": "4474679",
    "end": "4483360"
  },
  {
    "text": "space of functions that can be usefully feasibly calculated or computed by this",
    "start": "4483360",
    "end": "4489199"
  },
  {
    "text": "model yeah so I mean that there is a a uh you Universal approximation theorem",
    "start": "4489199",
    "end": "4497239"
  },
  {
    "text": "theorem that says basically sets the ground for saying that any like with a one hidden layer neuron network uh you",
    "start": "4497239",
    "end": "4505080"
  },
  {
    "text": "can approximate any any C1 function uh I forgot there there's",
    "start": "4505080",
    "end": "4511159"
  },
  {
    "text": "and there's as long as as as the nonlinearity is monotonically increasing and there's there's some like and that's",
    "start": "4511159",
    "end": "4517080"
  },
  {
    "text": "actually a pretty old result and then there's some attempts and attempts again to prove that going Beyond one hidden",
    "start": "4517080",
    "end": "4524639"
  },
  {
    "text": "layer it actually is helpful because you would you would say oh well one hidden layer is all I need to approximate any",
    "start": "4524639",
    "end": "4530320"
  },
  {
    "text": "function so why would I care um so there's you know Yoshua specifically and",
    "start": "4530320",
    "end": "4535360"
  },
  {
    "text": "and then people that actually are trying to analyze random networks have some interesting results um sanjie Arora",
    "start": "4535360",
    "end": "4542040"
  },
  {
    "text": "being one of them so there there's a bunch of work in done in actually theory on why depth matters and the kind of",
    "start": "4542040",
    "end": "4547440"
  },
  {
    "text": "functions they can approximate and how efficient they are at doing so um I",
    "start": "4547440",
    "end": "4553239"
  },
  {
    "text": "myself don't do this but yeah there's no there's no theoretical",
    "start": "4553239",
    "end": "4559120"
  },
  {
    "text": "advantage to going to this all that oh no there is so that that's the point there's there's like that's what I mean",
    "start": "4559120",
    "end": "4565239"
  },
  {
    "text": "in practice it's obvious it is it is useful in terms of let's say statistical efficiency and so on um but the the",
    "start": "4565239",
    "end": "4573880"
  },
  {
    "text": "paper I I haven't read it in carefully enough detail but there's a paper from James Martins very recently that",
    "start": "4573880",
    "end": "4580760"
  },
  {
    "text": "actually proves there's certain functions that cannot be learned with n n but can be learned with n plus1 and so",
    "start": "4580760",
    "end": "4587159"
  },
  {
    "text": "on so this I mean it's it's it's a sort of fascinating Sub sub area which um you",
    "start": "4587159",
    "end": "4594040"
  },
  {
    "text": "know in in practice I'm not sure how much we will learn but I hope we will learn like how to initialize things and",
    "start": "4594040",
    "end": "4599639"
  },
  {
    "text": "and other complicated annoying annoyances from learning deep deep uh or",
    "start": "4599639",
    "end": "4605000"
  },
  {
    "text": "deeper and deeper models",
    "start": "4605000",
    "end": "4610360"
  },
  {
    "text": "cool than e",
    "start": "4610360",
    "end": "4617880"
  }
]