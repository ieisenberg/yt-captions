[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "start": "0",
    "end": "4837"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4837",
    "end": "6170"
  },
  {
    "text": "This is part 2 in our series\non analysis methods for NLP.",
    "start": "6170",
    "end": "10090"
  },
  {
    "text": "We've come to our first\nmethod, and that is probing.",
    "start": "10090",
    "end": "13030"
  },
  {
    "start": "13000",
    "end": "13000"
  },
  {
    "text": "Here is an overview\nof how probing works.",
    "start": "13030",
    "end": "15340"
  },
  {
    "text": "The core idea is that we're\ngoing to use supervised models,",
    "start": "15340",
    "end": "18550"
  },
  {
    "text": "those are our probe\nmodels, to determine",
    "start": "18550",
    "end": "20980"
  },
  {
    "text": "what is latently encoded in\nthe hidden representations",
    "start": "20980",
    "end": "24189"
  },
  {
    "text": "of our target models, the ones\nthat we actually care about.",
    "start": "24190",
    "end": "27460"
  },
  {
    "text": "Probing is often applied in the\ncontext of so-called BERTology.",
    "start": "27460",
    "end": "31449"
  },
  {
    "text": "And I think Tenney et. al.",
    "start": "31450",
    "end": "32650"
  },
  {
    "text": "2019 is a really foundational\ncontribution in this space.",
    "start": "32650",
    "end": "35980"
  },
  {
    "text": "As I mentioned before,\nI think this was really",
    "start": "35980",
    "end": "38170"
  },
  {
    "text": "eye opening about the extent\nto which Bert is inducing",
    "start": "38170",
    "end": "41559"
  },
  {
    "text": "interesting structure\nabout language",
    "start": "41560",
    "end": "43570"
  },
  {
    "text": "from its training regimes.",
    "start": "43570",
    "end": "46590"
  },
  {
    "text": "Probing can be a source of\nvaluable insights, I believe,",
    "start": "46590",
    "end": "49800"
  },
  {
    "text": "but we do need to\nproceed with caution.",
    "start": "49800",
    "end": "51630"
  },
  {
    "text": "And there are really two\ncautionary notes here.",
    "start": "51630",
    "end": "53850"
  },
  {
    "text": "First, a very\npowerful probe might",
    "start": "53850",
    "end": "56760"
  },
  {
    "text": "lead you to see things that\naren't in your target model,",
    "start": "56760",
    "end": "59620"
  },
  {
    "text": "but rather just stored\nin your probe model.",
    "start": "59620",
    "end": "62489"
  },
  {
    "text": "It is after all a\nsupervised model",
    "start": "62490",
    "end": "64440"
  },
  {
    "text": "that you trained in some way.",
    "start": "64440",
    "end": "66520"
  },
  {
    "text": "Second and maybe more\nimportantly for the current",
    "start": "66520",
    "end": "69299"
  },
  {
    "text": "unit, probes cannot tell us\nabout whether the information",
    "start": "69300",
    "end": "72750"
  },
  {
    "text": "that we identify has any causal\nrelationship with the target",
    "start": "72750",
    "end": "76260"
  },
  {
    "text": "model's input/output behavior.",
    "start": "76260",
    "end": "78210"
  },
  {
    "text": "This is really concerning\nfor me because what",
    "start": "78210",
    "end": "80340"
  },
  {
    "text": "we're looking for\nfrom analysis methods",
    "start": "80340",
    "end": "83159"
  },
  {
    "text": "is insights about\nthe causal mechanisms",
    "start": "83160",
    "end": "85560"
  },
  {
    "text": "that guide model behaviors.",
    "start": "85560",
    "end": "87579"
  },
  {
    "text": "So if probing falls\nshort on offering us",
    "start": "87580",
    "end": "90270"
  },
  {
    "text": "those causal insights,\nit's really intrinsically",
    "start": "90270",
    "end": "92939"
  },
  {
    "text": "limited as an analysis method.",
    "start": "92940",
    "end": "97090"
  },
  {
    "text": "I'm going to focus,\nfor this screencast,",
    "start": "97090",
    "end": "98920"
  },
  {
    "text": "on supervised probes\nto keep things simple,",
    "start": "98920",
    "end": "101049"
  },
  {
    "text": "but I will mention unsupervised\nprobes near the end.",
    "start": "101050",
    "end": "104770"
  },
  {
    "text": "They don't suffer\nfrom the concern",
    "start": "104770",
    "end": "106509"
  },
  {
    "text": "that they're overly\npowerful, but they do,",
    "start": "106510",
    "end": "109060"
  },
  {
    "text": "I think, still fall\nshort when it comes",
    "start": "109060",
    "end": "110920"
  },
  {
    "text": "to offering causal insights.",
    "start": "110920",
    "end": "114189"
  },
  {
    "start": "114000",
    "end": "114000"
  },
  {
    "text": "Let's start with\na kind of recipe",
    "start": "114190",
    "end": "116260"
  },
  {
    "text": "for probing to be\ncareful about this.",
    "start": "116260",
    "end": "118730"
  },
  {
    "text": "The first step is that\nyou state a hypothesis",
    "start": "118730",
    "end": "121150"
  },
  {
    "text": "about an aspect of the target\nmodel's internal structure.",
    "start": "121150",
    "end": "124330"
  },
  {
    "text": "You could hypothesize\nthat it stores information",
    "start": "124330",
    "end": "127150"
  },
  {
    "text": "about part of speech, or named\nentities, or dependency parses,",
    "start": "127150",
    "end": "131680"
  },
  {
    "text": "you name.",
    "start": "131680",
    "end": "132340"
  },
  {
    "text": "It the hypothesis space is open.",
    "start": "132340",
    "end": "135569"
  },
  {
    "text": "You then need to choose\na supervised task that",
    "start": "135570",
    "end": "138540"
  },
  {
    "text": "is a proxy for the internal\nstructure of interest.",
    "start": "138540",
    "end": "141269"
  },
  {
    "text": "If you're going to look\nfor part of speech,",
    "start": "141270",
    "end": "143070"
  },
  {
    "text": "you need a part of\nspeech data set.",
    "start": "143070",
    "end": "144745"
  },
  {
    "text": "And you're going to be\ndependent on that data",
    "start": "144745",
    "end": "146620"
  },
  {
    "text": "set when it comes to actually\ndefining the probe itself.",
    "start": "146620",
    "end": "150690"
  },
  {
    "text": "Then you identify a\nplace in the model,",
    "start": "150690",
    "end": "153600"
  },
  {
    "text": "a set of hidden representations,\nwhere you believe",
    "start": "153600",
    "end": "156360"
  },
  {
    "text": "the structure will be encoded.",
    "start": "156360",
    "end": "158250"
  },
  {
    "text": "And you train a supervised\nprobe on the chosen site",
    "start": "158250",
    "end": "161880"
  },
  {
    "text": "and then the extent to which\nyour probe is successful",
    "start": "161880",
    "end": "165300"
  },
  {
    "text": "is your estimate of\nthe degree to which you",
    "start": "165300",
    "end": "167550"
  },
  {
    "text": "are right about the\nunderlying hypothesis.",
    "start": "167550",
    "end": "170610"
  },
  {
    "text": "But there are some\ncaveats there.",
    "start": "170610",
    "end": "172930"
  },
  {
    "start": "172000",
    "end": "172000"
  },
  {
    "text": "Let's first walk\nthrough the core method.",
    "start": "172930",
    "end": "174780"
  },
  {
    "text": "What I have on the\nslide now is a kind",
    "start": "174780",
    "end": "177000"
  },
  {
    "text": "of very cartoonish look at\na BERT-like model with three",
    "start": "177000",
    "end": "180390"
  },
  {
    "text": "layers.",
    "start": "180390",
    "end": "181180"
  },
  {
    "text": "And you can see these\ninputs have come in.",
    "start": "181180",
    "end": "184200"
  },
  {
    "text": "And we're going to target\nthe hidden representation",
    "start": "184200",
    "end": "186930"
  },
  {
    "text": "h to start.",
    "start": "186930",
    "end": "187890"
  },
  {
    "text": "Let's suppose that's the\nsite that we chose to probe.",
    "start": "187890",
    "end": "191720"
  },
  {
    "text": "What we're going to do is\nfit a small linear model",
    "start": "191720",
    "end": "194780"
  },
  {
    "text": "on that internal representation\nusing some task labels.",
    "start": "194780",
    "end": "199280"
  },
  {
    "text": "The way that actually\nplays out in practice",
    "start": "199280",
    "end": "201709"
  },
  {
    "text": "is instructive, right?",
    "start": "201710",
    "end": "203120"
  },
  {
    "text": "We're going to run the BERT\nmodel on the current input.",
    "start": "203120",
    "end": "206180"
  },
  {
    "text": "And we're going to grab the\nvector representation there",
    "start": "206180",
    "end": "209269"
  },
  {
    "text": "and use it to start building a\nlittle supervised learning data",
    "start": "209270",
    "end": "213200"
  },
  {
    "text": "set, where this is some vector\nand this is a task label",
    "start": "213200",
    "end": "217069"
  },
  {
    "text": "for our input example.",
    "start": "217070",
    "end": "219170"
  },
  {
    "text": "Then we run the Bert model\nagain on a different sequence.",
    "start": "219170",
    "end": "222590"
  },
  {
    "text": "We get a different\nvector representation",
    "start": "222590",
    "end": "225200"
  },
  {
    "text": "at our target site.",
    "start": "225200",
    "end": "226459"
  },
  {
    "text": "And that also contributes\nto our supervised learning",
    "start": "226460",
    "end": "229220"
  },
  {
    "text": "data set with a new task label.",
    "start": "229220",
    "end": "231320"
  },
  {
    "text": "We do it again for\na different input.",
    "start": "231320",
    "end": "233960"
  },
  {
    "text": "We get a different vector\nand another task label,",
    "start": "233960",
    "end": "237350"
  },
  {
    "text": "and so forth, and so on.",
    "start": "237350",
    "end": "238890"
  },
  {
    "text": "And we continue this process\nfor maybe tens of thousands",
    "start": "238890",
    "end": "241850"
  },
  {
    "text": "of examples, whatever\nwe've got available to us",
    "start": "241850",
    "end": "244460"
  },
  {
    "text": "in our probe data set.",
    "start": "244460",
    "end": "246200"
  },
  {
    "text": "And then we fit a small linear\nmodel on this x, y pair.",
    "start": "246200",
    "end": "251120"
  },
  {
    "text": "Notice that we have used\nthe BERT model simply",
    "start": "251120",
    "end": "254430"
  },
  {
    "text": "as a kind of engine for\ngrabbing these vector",
    "start": "254430",
    "end": "257578"
  },
  {
    "text": "representations that we\nuse for our probe model.",
    "start": "257579",
    "end": "262760"
  },
  {
    "text": "And of course, I chose\na single representation,",
    "start": "262760",
    "end": "265190"
  },
  {
    "text": "but more commonly with Bert,\nwe're doing this layer-wise.",
    "start": "265190",
    "end": "268090"
  },
  {
    "text": "So you could decide that\nthe entire layer here",
    "start": "268090",
    "end": "270110"
  },
  {
    "text": "encodes part of speech.",
    "start": "270110",
    "end": "271242"
  },
  {
    "text": "And then you would\nbuild up a data",
    "start": "271243",
    "end": "272660"
  },
  {
    "text": "set consisting of\nlists of these vectors",
    "start": "272660",
    "end": "274880"
  },
  {
    "text": "with their associated\nlists of labels",
    "start": "274880",
    "end": "276860"
  },
  {
    "text": "and train a part of speech\ntagging model on that basis.",
    "start": "276860",
    "end": "279960"
  },
  {
    "text": "And that would be your probe.",
    "start": "279960",
    "end": "282380"
  },
  {
    "start": "282000",
    "end": "282000"
  },
  {
    "text": "The first question that arises\nfor probing is really pressing.",
    "start": "282380",
    "end": "286190"
  },
  {
    "text": "Are we probing the target\nmodel or are we simply learning",
    "start": "286190",
    "end": "289250"
  },
  {
    "text": "a new model that is\nthe probe model, right?",
    "start": "289250",
    "end": "291470"
  },
  {
    "text": "Probes in the current\nsense are supervised models",
    "start": "291470",
    "end": "294320"
  },
  {
    "text": "whose inputs are\nfrozen parameters",
    "start": "294320",
    "end": "296810"
  },
  {
    "text": "of the model we're probing.",
    "start": "296810",
    "end": "298550"
  },
  {
    "text": "We use the BERT model\nas a kind of engine",
    "start": "298550",
    "end": "300919"
  },
  {
    "text": "for creating these feature\nrepresentations that",
    "start": "300920",
    "end": "303230"
  },
  {
    "text": "were the input to a\nseparate modeling process.",
    "start": "303230",
    "end": "307150"
  },
  {
    "text": "This is very hard to\ndistinguish from simply fitting",
    "start": "307150",
    "end": "309790"
  },
  {
    "text": "a supervised model as usual\nwith some particular choice",
    "start": "309790",
    "end": "313090"
  },
  {
    "text": "of featurization,\nthe site that we",
    "start": "313090",
    "end": "315669"
  },
  {
    "text": "chose based on how BERT\ndid its calculations.",
    "start": "315670",
    "end": "319070"
  },
  {
    "text": "So based on one and two,\nwe know that at least some",
    "start": "319070",
    "end": "322750"
  },
  {
    "text": "of the information\nthat we're identifying",
    "start": "322750",
    "end": "324850"
  },
  {
    "text": "is likely stored in the probe\nmodel, not in the target model.",
    "start": "324850",
    "end": "329330"
  },
  {
    "text": "And of course, more\npowerful probes",
    "start": "329330",
    "end": "331699"
  },
  {
    "text": "might find more information\nin the target model,",
    "start": "331700",
    "end": "334400"
  },
  {
    "text": "but that's only because they're\nstoring more information",
    "start": "334400",
    "end": "337160"
  },
  {
    "text": "in the probe parameters.",
    "start": "337160",
    "end": "338330"
  },
  {
    "text": "They have a greater\ncapacity to do that.",
    "start": "338330",
    "end": "342080"
  },
  {
    "start": "341000",
    "end": "341000"
  },
  {
    "text": "To help address this,\nHewitt and Liang",
    "start": "342080",
    "end": "344389"
  },
  {
    "text": "introduced the notion\nof probe selectivity.",
    "start": "344390",
    "end": "347150"
  },
  {
    "text": "And this is just going\nto help us calibrate,",
    "start": "347150",
    "end": "349130"
  },
  {
    "text": "to some extent, how much\ninformation was actually",
    "start": "349130",
    "end": "352280"
  },
  {
    "text": "in the target model.",
    "start": "352280",
    "end": "353810"
  },
  {
    "text": "The first step here is\nto define a control task.",
    "start": "353810",
    "end": "356540"
  },
  {
    "text": "This would be a random task\nwith the same input/output",
    "start": "356540",
    "end": "359960"
  },
  {
    "text": "structure as your target\ntask, for example,",
    "start": "359960",
    "end": "362910"
  },
  {
    "text": "for word sense\nclassification, you",
    "start": "362910",
    "end": "364490"
  },
  {
    "text": "could just assign words\nrandom fixed senses.",
    "start": "364490",
    "end": "368419"
  },
  {
    "text": "For part of speech tagging,\nyou could assign words",
    "start": "368420",
    "end": "370850"
  },
  {
    "text": "to random fixed tags,\nmaybe keeping the same tag",
    "start": "370850",
    "end": "374480"
  },
  {
    "text": "distribution as your underlined\npart of speech data set.",
    "start": "374480",
    "end": "378050"
  },
  {
    "text": "Or for parsing, you could\nassign edges randomly",
    "start": "378050",
    "end": "381349"
  },
  {
    "text": "using some simple strategies to\ngive you tree structures that",
    "start": "381350",
    "end": "384140"
  },
  {
    "text": "are very different, presumably,\nfrom the ones in your gold data",
    "start": "384140",
    "end": "387260"
  },
  {
    "text": "set.",
    "start": "387260",
    "end": "388310"
  },
  {
    "text": "And then selectivity\nas a metric for probes",
    "start": "388310",
    "end": "391500"
  },
  {
    "text": "is just the difference\nbetween probe performance",
    "start": "391500",
    "end": "394110"
  },
  {
    "text": "on the task and probe\nperformance on the control",
    "start": "394110",
    "end": "397349"
  },
  {
    "text": "task.",
    "start": "397350",
    "end": "398070"
  },
  {
    "text": "So you've baked in how\nwell your model can",
    "start": "398070",
    "end": "400800"
  },
  {
    "text": "do on a kind of random task.",
    "start": "400800",
    "end": "403050"
  },
  {
    "text": "That's the idea.",
    "start": "403050",
    "end": "404550"
  },
  {
    "text": "And Hewitt and Liang\noffered this summary picture",
    "start": "404550",
    "end": "407460"
  },
  {
    "text": "which essentially shows that the\nmost reliable probes in terms",
    "start": "407460",
    "end": "411360"
  },
  {
    "text": "of giving you insights\nwill be very small ones,",
    "start": "411360",
    "end": "414090"
  },
  {
    "text": "here this is a model with\njust two hidden units.",
    "start": "414090",
    "end": "416940"
  },
  {
    "text": "That gives you very\nhigh selectivity.",
    "start": "416940",
    "end": "418950"
  },
  {
    "text": "There is likely to be\na very large difference",
    "start": "418950",
    "end": "421650"
  },
  {
    "text": "between performance on your\ntask and the performance",
    "start": "421650",
    "end": "424440"
  },
  {
    "text": "of this control model when\nthe model is very simple.",
    "start": "424440",
    "end": "427890"
  },
  {
    "text": "On the other hand, if you\nhave a very powerful probe",
    "start": "427890",
    "end": "430830"
  },
  {
    "text": "model with many\nparameters, you'll",
    "start": "430830",
    "end": "433080"
  },
  {
    "text": "have low selectivity\nbecause that model has",
    "start": "433080",
    "end": "435330"
  },
  {
    "text": "such a great capacity\nto simply memorize",
    "start": "435330",
    "end": "438120"
  },
  {
    "text": "aspects of the data set.",
    "start": "438120",
    "end": "441940"
  },
  {
    "start": "441000",
    "end": "441000"
  },
  {
    "text": "Let's move now to the\nsecond concern I have,",
    "start": "441940",
    "end": "444430"
  },
  {
    "text": "which is about causal inference.",
    "start": "444430",
    "end": "445919"
  },
  {
    "text": "And to build this argument,\nlet's use a simple example.",
    "start": "445920",
    "end": "449640"
  },
  {
    "text": "We imagine that we have a\nsmall neural network that",
    "start": "449640",
    "end": "452670"
  },
  {
    "text": "takes in three numbers as\ninputs and perfectly computes",
    "start": "452670",
    "end": "456330"
  },
  {
    "text": "their sums.",
    "start": "456330",
    "end": "457030"
  },
  {
    "text": "So when 1, 3, 5 comes in,\nit does its internal magic.",
    "start": "457030",
    "end": "460530"
  },
  {
    "text": "And it outputs 9.",
    "start": "460530",
    "end": "462000"
  },
  {
    "text": "And we'll presume that it does\nthat calculation perfectly",
    "start": "462000",
    "end": "464820"
  },
  {
    "text": "for all triples of\nintegers coming in.",
    "start": "464820",
    "end": "468150"
  },
  {
    "text": "The question is, how\ndoes it manage this feat?",
    "start": "468150",
    "end": "471060"
  },
  {
    "text": "How does this model work?",
    "start": "471060",
    "end": "472560"
  },
  {
    "text": "You might have a\nhypothesis that it does it",
    "start": "472560",
    "end": "475350"
  },
  {
    "text": "in a compositional way,\nwhere the first two inputs, x",
    "start": "475350",
    "end": "479010"
  },
  {
    "text": "and y come together to form\nan intermediate variable, S1.",
    "start": "479010",
    "end": "482760"
  },
  {
    "text": "The third one is copied\ninto an internal state w.",
    "start": "482760",
    "end": "486390"
  },
  {
    "text": "And then S1 and w are\nmodular representations",
    "start": "486390",
    "end": "489810"
  },
  {
    "text": "that are added together to\nform the output representation.",
    "start": "489810",
    "end": "493800"
  },
  {
    "text": "That's a hypothesis about\nhow this model might work.",
    "start": "493800",
    "end": "496860"
  },
  {
    "text": "And now the question\nis, can we use",
    "start": "496860",
    "end": "498479"
  },
  {
    "text": "probing to reliably\nassess that hypothesis?",
    "start": "498480",
    "end": "502875"
  },
  {
    "text": "So let's suppose we have\nthis neural network.",
    "start": "502875",
    "end": "504750"
  },
  {
    "text": "And what we decide is that L1\nprobably computes the input z.",
    "start": "504750",
    "end": "510150"
  },
  {
    "text": "And let's suppose we\nfit a probe model.",
    "start": "510150",
    "end": "512130"
  },
  {
    "text": "It could be a simple\nidentity probe.",
    "start": "512130",
    "end": "513900"
  },
  {
    "text": "And the probe says, yes.",
    "start": "513900",
    "end": "515760"
  },
  {
    "text": "L1 always perfectly encodes the\nidentity of the third input.",
    "start": "515760",
    "end": "520770"
  },
  {
    "text": "Suppose we continue that.",
    "start": "520770",
    "end": "522240"
  },
  {
    "text": "We probe L2.",
    "start": "522240",
    "end": "523289"
  },
  {
    "text": "And we find that it always\nperfectly computes X plus Y",
    "start": "523289",
    "end": "527160"
  },
  {
    "text": "according to our very\nsimple probe model.",
    "start": "527160",
    "end": "530370"
  },
  {
    "text": "That might look like\nevidence for the hypothesis",
    "start": "530370",
    "end": "533455"
  },
  {
    "text": "that we started with.",
    "start": "533455",
    "end": "534330"
  },
  {
    "text": "You say, aha, it's a\nbit counterintuitive",
    "start": "534330",
    "end": "536700"
  },
  {
    "text": "because L1 encodes\nz and L2 x, y.",
    "start": "536700",
    "end": "540210"
  },
  {
    "text": "So it's kind of out of order,\nbut nonetheless, the model",
    "start": "540210",
    "end": "543540"
  },
  {
    "text": "is obeying my hypothesis.",
    "start": "543540",
    "end": "547250"
  },
  {
    "text": "But the probes have misled you.",
    "start": "547250",
    "end": "549260"
  },
  {
    "text": "Here is a look at the\nfull internal structure",
    "start": "549260",
    "end": "551617"
  },
  {
    "text": "of this model.",
    "start": "551617",
    "end": "552200"
  },
  {
    "text": "This is all the\nweight parameters.",
    "start": "552200",
    "end": "553820"
  },
  {
    "text": "Again, this model performs\nour task perfectly,",
    "start": "553820",
    "end": "556730"
  },
  {
    "text": "but the point is that L2 has\nno impact at all on the output",
    "start": "556730",
    "end": "560959"
  },
  {
    "text": "behavior.",
    "start": "560960",
    "end": "561900"
  },
  {
    "text": "And one way to see that\nis to look at the output",
    "start": "561900",
    "end": "564350"
  },
  {
    "text": "vector of weights.",
    "start": "564350",
    "end": "565610"
  },
  {
    "text": "L2 is just zeroed out as\npart of this computation.",
    "start": "565610",
    "end": "568820"
  },
  {
    "text": "No causal impact.",
    "start": "568820",
    "end": "570200"
  },
  {
    "text": "The probe said it\nstored x plus y.",
    "start": "570200",
    "end": "573140"
  },
  {
    "text": "And it might be doing that,\nin fact, it is doing that,",
    "start": "573140",
    "end": "575930"
  },
  {
    "text": "but not in a way that tells\nus about the input/output",
    "start": "575930",
    "end": "578870"
  },
  {
    "text": "behavior.",
    "start": "578870",
    "end": "580050"
  },
  {
    "text": "So the probe in that deep way,\nin that causal way misled us.",
    "start": "580050",
    "end": "585580"
  },
  {
    "start": "584000",
    "end": "584000"
  },
  {
    "text": "The final goal\npost that I set up",
    "start": "585580",
    "end": "587710"
  },
  {
    "text": "was, do we have a path\nto improving models",
    "start": "587710",
    "end": "590350"
  },
  {
    "text": "from the analysis method\nthat we've chosen?",
    "start": "590350",
    "end": "593290"
  },
  {
    "text": "Here, I have a mixed answer.",
    "start": "593290",
    "end": "595060"
  },
  {
    "text": "There does seem to be a path\nfrom probing to what you might",
    "start": "595060",
    "end": "597760"
  },
  {
    "text": "call multi-task training,\nwhere I'm training this model",
    "start": "597760",
    "end": "601000"
  },
  {
    "text": "to do addition.",
    "start": "601000",
    "end": "601750"
  },
  {
    "text": "And in addition, I train it so\nthat this representation here",
    "start": "601750",
    "end": "605470"
  },
  {
    "text": "encodes Z and this\none encodes x plus y.",
    "start": "605470",
    "end": "608620"
  },
  {
    "text": "We can certainly\nhave such objectives.",
    "start": "608620",
    "end": "611020"
  },
  {
    "text": "I think it's an open question\nwhether or not it actually",
    "start": "611020",
    "end": "613780"
  },
  {
    "text": "induces the modularity\nthat we're interested in,",
    "start": "613780",
    "end": "617890"
  },
  {
    "text": "but the really\ndeep concern for me",
    "start": "617890",
    "end": "619870"
  },
  {
    "text": "is just that still here we\ndon't get causal guarantees.",
    "start": "619870",
    "end": "623200"
  },
  {
    "text": "We can do the\nmulti-task training,",
    "start": "623200",
    "end": "624730"
  },
  {
    "text": "but that does not guarantee\nthat the structure we induced,",
    "start": "624730",
    "end": "628300"
  },
  {
    "text": "whatever it's like\nis actually shaping",
    "start": "628300",
    "end": "630610"
  },
  {
    "text": "performance on the core task,\nin this case, of adding numbers.",
    "start": "630610",
    "end": "634430"
  },
  {
    "text": "So we have to\nproceed with caution.",
    "start": "634430",
    "end": "637460"
  },
  {
    "start": "637000",
    "end": "637000"
  },
  {
    "text": "Finally, a quick note, I\nmentioned unsupervised probes.",
    "start": "637460",
    "end": "640310"
  },
  {
    "text": "There's wonderful work\nin this space using",
    "start": "640310",
    "end": "642680"
  },
  {
    "text": "a variety of different methods.",
    "start": "642680",
    "end": "644450"
  },
  {
    "text": "Here are some references\nto really formative entries",
    "start": "644450",
    "end": "647750"
  },
  {
    "text": "into that literature.",
    "start": "647750",
    "end": "649070"
  },
  {
    "text": "Again, I think these\ntechniques do not",
    "start": "649070",
    "end": "650990"
  },
  {
    "text": "suffer from the concerns\nabout probe power",
    "start": "650990",
    "end": "653300"
  },
  {
    "text": "because they don't have their\nown parameters typically.",
    "start": "653300",
    "end": "657380"
  },
  {
    "text": "But they do, I think,\nsuffer that limitation",
    "start": "657380",
    "end": "660260"
  },
  {
    "text": "about causal inference.",
    "start": "660260",
    "end": "662640"
  },
  {
    "text": "So let's wrap up\nwith our scorecard.",
    "start": "662640",
    "end": "664970"
  },
  {
    "start": "663000",
    "end": "663000"
  },
  {
    "text": "Remember, probing can\ncharacterize representations",
    "start": "664970",
    "end": "667819"
  },
  {
    "text": "really well.",
    "start": "667820",
    "end": "668390"
  },
  {
    "text": "We use the supervised\nprobe for that.",
    "start": "668390",
    "end": "670350"
  },
  {
    "text": "So that's a smiley face, but\nprobes cannot offer causal",
    "start": "670350",
    "end": "674000"
  },
  {
    "text": "inferences.",
    "start": "674000",
    "end": "675020"
  },
  {
    "text": "And I've put a thinking\nemoji under improved models",
    "start": "675020",
    "end": "678710"
  },
  {
    "text": "because it's unclear to me\nwhether multitask training is",
    "start": "678710",
    "end": "682190"
  },
  {
    "text": "really a viable\ngeneral way of moving",
    "start": "682190",
    "end": "684620"
  },
  {
    "text": "from probes to better models.",
    "start": "684620",
    "end": "687730"
  },
  {
    "start": "687730",
    "end": "692000"
  }
]