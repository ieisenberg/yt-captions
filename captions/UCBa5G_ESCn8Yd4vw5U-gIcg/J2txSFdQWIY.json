[
  {
    "start": "0",
    "end": "11000"
  },
  {
    "text": "Today's featured speaker is Greg Valiant. Gregory Valiant is an assistant professor\nin Stanford's computer science department.",
    "start": "10391",
    "end": "17440"
  },
  {
    "start": "11000",
    "end": "393000"
  },
  {
    "text": "Some of his recent projects focus\non designing algorithms for accurately inferring information\nabout complex distributions",
    "start": "17440",
    "end": "23710"
  },
  {
    "text": "when given surprisingly little data. More broadly,\nhis research interests are in algorithms, learning, applied probability,\nstatistics, and evolution.",
    "start": "23710",
    "end": "32759"
  },
  {
    "text": "Prior to joining Stanford, Dr. Valiant was a postdoc at\nthe Microsoft Lab in New England. Which focused on integrating\nmathematical and algorithmic",
    "start": "32760",
    "end": "40402"
  },
  {
    "text": "sciences with both the social sciences and\naspects of the biomedical sciences. He received a PhD from UC Berkeley in\nComputer Science and degree from Harvard.",
    "start": "40402",
    "end": "49786"
  },
  {
    "text": "He currently teaches CS 161 design and analysis of algorithms this\nwinter quarter at Stanford.",
    "start": "49786",
    "end": "56170"
  },
  {
    "text": "Which is part of our SCPD portfolio\noffered to our industry students. So now,\nI'd like to turn the floor over to Greg.",
    "start": "56170",
    "end": "62940"
  },
  {
    "text": ">> Okay, well thanks Stephanie,\nand thanks for logging in. So today I was going to tell you\nabout a few perspectives on how",
    "start": "64220",
    "end": "69572"
  },
  {
    "text": "to get a surprising amount of\ninformation from a given amount of data.",
    "start": "69572",
    "end": "73278"
  },
  {
    "text": "So, a sort of mindset that underpins\nthis work is the following question.",
    "start": "77408",
    "end": "81770"
  },
  {
    "text": "So, if you're given some data that's\ndrawn from a very complex distribution. So, what do you mean by complex?",
    "start": "82940",
    "end": "87970"
  },
  {
    "text": "Well, maybe it's a very high\ndimensional distribution. Maybe if the distribution with\nvery large domain or alphabet,",
    "start": "87970",
    "end": "94070"
  },
  {
    "text": "it may be a distribution\nover rare genetic mutations. Or maybe it's a distribution\nwith very complex dependencies,",
    "start": "94070",
    "end": "100640"
  },
  {
    "text": "such as natural language. Then, and oftentimes,\neven if you have a lot of data,",
    "start": "100640",
    "end": "106860"
  },
  {
    "text": "the empirical distribution\nof the data is misleading. In the case of high\ndimensional distributions,",
    "start": "106860",
    "end": "113610"
  },
  {
    "text": "well because of the dimensionality,\nyou need an enormous amount of data to kind of fill out\nthe geometry of the space.",
    "start": "113610",
    "end": "120170"
  },
  {
    "text": "In the case of distributions\nover very large domains, often even when you have lots of data you\nstill fail to observe some of the domain",
    "start": "120170",
    "end": "128067"
  },
  {
    "text": "elements that do arise with\nnon-negligible probabilities. In the case of complex dependencies,\nwell even when you have lots of data,",
    "start": "128067",
    "end": "137760"
  },
  {
    "text": "sometimes you don't actually see\nall of these complex dependencies.",
    "start": "137760",
    "end": "142819"
  },
  {
    "text": "So in these cases, the empirical distribution of the data\nyou have is often misleading.",
    "start": "142820",
    "end": "148110"
  },
  {
    "text": "And the basic question is, well,\nin this undersampled regime, can we still recover accurate information\nabout the underlying distributions?",
    "start": "148110",
    "end": "155959"
  },
  {
    "text": "What can you say about the unseen\nportion of the distribution? Can you say anything about\nthat domain elements that you",
    "start": "155960",
    "end": "161250"
  },
  {
    "text": "haven't actually observed? Or in cases where maybe you can't assume\ngreat answers to the above two questions",
    "start": "161250",
    "end": "169250"
  },
  {
    "text": "could you still estimate\nthe value of collecting more data? Could you estimate well maybe if I\nwere to get 10 times more data or",
    "start": "169250",
    "end": "175293"
  },
  {
    "text": "100 times more data I would be\nable to answer these questions.",
    "start": "175293",
    "end": "178516"
  },
  {
    "text": "So I want to give two motivations for\nthis work. So the first is that even though\ntoday's machine learning systems",
    "start": "181240",
    "end": "187510"
  },
  {
    "text": "are extremely impressive, currently\nthey still use the data they have extremely inefficiently to just\nto give one concrete data point.",
    "start": "187510",
    "end": "197930"
  },
  {
    "text": "So humans learn to speak after\nhearing roughly 50 million words. So between when you are born and\nwhen you are three or",
    "start": "197930",
    "end": "203750"
  },
  {
    "text": "four or five years old, most people\nhear 50 million words spoken to them. So again, this isn't distinct words.",
    "start": "203750",
    "end": "210700"
  },
  {
    "text": "This is 50 million utterances of words. And so after hearing these 50 million\nwords, most children have a good grasp of",
    "start": "210700",
    "end": "218120"
  },
  {
    "text": "grammar, good grasp of semantics and\nthey basically speak like humans.",
    "start": "218120",
    "end": "223400"
  },
  {
    "text": "In contrast, even the state of the art\nnatural language processing systems today, they're fed the entire Wikipedia corpus\nover and over and over and over again.",
    "start": "224920",
    "end": "234294"
  },
  {
    "text": "And at the end of the day they still lack\nsome basic grammatical understanding.",
    "start": "234294",
    "end": "239610"
  },
  {
    "text": "And lack kind of sense of semantics. Okay, so even though today's machine\nlearning systems are very, very",
    "start": "239610",
    "end": "245400"
  },
  {
    "text": "impressive, they don't use information\nthat they have all that efficiently.",
    "start": "245400",
    "end": "248950"
  },
  {
    "text": "The second motivation I wanted to\nmention is this apparent paradox that as the number of datasets\naround us have grown and",
    "start": "251070",
    "end": "256760"
  },
  {
    "text": "as the sizes of these datasets have grown,\nin fact we are increasingly faced with this problem of seeing these\nunder sample distributions.",
    "start": "256760",
    "end": "264630"
  },
  {
    "text": "And in the next five to ten years I think\nwe're going to start to see many more advances that are based upon getting\nmore information from the given data,",
    "start": "264630",
    "end": "273531"
  },
  {
    "text": "getting more information from\nthe data that we actually have.",
    "start": "273531",
    "end": "277272"
  },
  {
    "text": "So with these motivations in mind, I\nwanted to discuss three sort of different settings that I've been\nworking on recently.",
    "start": "280504",
    "end": "287650"
  },
  {
    "text": "These three settings will all have\nthe property that for a given amount of data even when the empirical\ndistribution of the data is misleading,",
    "start": "287650",
    "end": "296979"
  },
  {
    "text": "we'll still be able to accurately say\nthings about the underlying distribution.",
    "start": "296979",
    "end": "302439"
  },
  {
    "text": "So in the first part,\nwhich is sort of a warm up, we'll consider accepting where there\nare lots and lots of people, but",
    "start": "302439",
    "end": "308043"
  },
  {
    "text": "each person only has\na tiny amount of data. And we'll see that you can leverage\nthe fact that there are lots and",
    "start": "308043",
    "end": "314200"
  },
  {
    "text": "lots of people to improve your empirical\nunderstanding of each person even though they only have\na little bit of data.",
    "start": "314200",
    "end": "320900"
  },
  {
    "text": "For the second part of the talk we'll\nconsider a setting where we have a sample drawn from a distribution over\nan extremely large domain.",
    "start": "322460",
    "end": "329645"
  },
  {
    "text": "That we'll actually be able to accurately\ninfer the structure of the portion of the domain that we haven't\nseen any samples from.",
    "start": "329645",
    "end": "336530"
  },
  {
    "text": "So we'll actually be able to infer things,\nbut the number of unseen domain elements\nthat arise with different probabilities.",
    "start": "336530",
    "end": "342485"
  },
  {
    "text": "In the last part of the talk,\nwe'll consider geometric setting. So we're given data drawn from some\nhigh dimensional distribution.",
    "start": "344536",
    "end": "350810"
  },
  {
    "text": "And we'll see that even when we have too\nlittle data to understand the geometric structure of the distribution, so\nwe'll have too little data to say,",
    "start": "351880",
    "end": "360409"
  },
  {
    "text": "these are the big principal\ncomponents of the distribution. We'll be able to describe\nwhat structure is there.",
    "start": "360410",
    "end": "366630"
  },
  {
    "text": "So we'll say, we don't know what\nthe principle components are but we know that there are 5\ndirections of big variants and",
    "start": "366630",
    "end": "374169"
  },
  {
    "text": "50 directions of medium variants and\nso on. So while all three of these settings\nare going to be rather different,",
    "start": "374170",
    "end": "380790"
  },
  {
    "text": "and the results will be\ndifferent in these settings, there's a common perspective\nthat underlies and",
    "start": "380790",
    "end": "385830"
  },
  {
    "text": "enables the algorithms that we'll\nsee in these different settings. Okay, so let's start.",
    "start": "385830",
    "end": "391424"
  },
  {
    "start": "393000",
    "end": "856000"
  },
  {
    "text": "So in this first setting,\nimagine you have n people, and think of n as being very large. And each person has some\nhidden parameter p sub i.",
    "start": "394060",
    "end": "402940"
  },
  {
    "text": "Maybe p sub i is the probability that\nthe i-th person gets the flu each year or maybe p sub i is the probability that each\nperson pays their credit card late each",
    "start": "402940",
    "end": "412167"
  },
  {
    "text": "month or changes jobs. Or maybe p sub i is some biological\ntraits, though maybe p sub i is probability that a person has male\nversus a female child and so on.",
    "start": "412167",
    "end": "420732"
  },
  {
    "text": "So imagine we have a data set, so we have\nlots and lots of people, we have n people.",
    "start": "422765",
    "end": "428220"
  },
  {
    "text": "However, each person we only say, in this\ncase, observe two flips of their coin,",
    "start": "428220",
    "end": "433430"
  },
  {
    "text": "so we observe two independent random\nvariables for each person with,",
    "start": "433430",
    "end": "439560"
  },
  {
    "text": "or yes with probability p sub i and\nno with probability 1- p sub i.",
    "start": "439560",
    "end": "445430"
  },
  {
    "text": "In this case, what can we hope to\nsay about each person's probability?",
    "start": "445430",
    "end": "449130"
  },
  {
    "text": "Well, the first thing to observe\nis that the empirical estimates of the p i's are extremely unreliable. We just have two flips\nof each person's coin.",
    "start": "451610",
    "end": "459612"
  },
  {
    "text": "What can we hope to say? So if we look at the first person,\nthey flipped their coin twice. They got two yeses.",
    "start": "459612",
    "end": "465810"
  },
  {
    "text": "The empirical estimate of this\nfirst person's probability is 1. In both tosses they got yes,\nempirical estimate is 1.",
    "start": "465810",
    "end": "473250"
  },
  {
    "text": "And obviously, this is very noisy because\nwe only have two flips of the coin.",
    "start": "473250",
    "end": "477580"
  },
  {
    "text": "Okay, so in general, if you want to\nestimate the bias of a coin to error epsilon, you do need to flip it roughly\none over epsilon squared times.",
    "start": "479040",
    "end": "486410"
  },
  {
    "text": "It's here we only have two tosses. What can we hope to say?",
    "start": "486410",
    "end": "490070"
  },
  {
    "text": "Well, suppose we look at all\nthe data in aggregate, and we happen to know just\nthe following phenomenon.",
    "start": "492160",
    "end": "498699"
  },
  {
    "text": "That suppose we see that actually\nthe number of people who got two yes' is roughly equal to the number of people who\ngot two no's, which is equal to the number",
    "start": "498700",
    "end": "506889"
  },
  {
    "text": "of people who got a no and a yes, a yes\nand a no, and these are very accurately,",
    "start": "506890",
    "end": "511950"
  },
  {
    "text": "the number of people with each of\nthese is very close to and over four.",
    "start": "511950",
    "end": "516540"
  },
  {
    "text": "So I claim that based on,\nif we work out seamless data, we can actually conclude that\nalmost all of these probabilities,",
    "start": "518840",
    "end": "525300"
  },
  {
    "text": "almost all of the pizza buys,\nmost be extremely close to a half. So what's the reasoning behind this?",
    "start": "525300",
    "end": "532312"
  },
  {
    "text": "Well, suppose that 10% of the people\nhad a probability of point six and 10% had a probability of point four and",
    "start": "532312",
    "end": "539660"
  },
  {
    "text": "maybe the rest of the people had\na probability of point five. Then the theme that we\nwould actually start to",
    "start": "539660",
    "end": "544970"
  },
  {
    "text": "see a noticeable amount of\ncorrelation in these two flips. We'd actually see more\npeople with the yes yes and",
    "start": "544970",
    "end": "550698"
  },
  {
    "text": "more people with the no no then with\nthe mixed no yes' and yes no's.",
    "start": "550698",
    "end": "554547"
  },
  {
    "text": "So if you think about this a little bit,\nif we actually saw data like this,",
    "start": "556803",
    "end": "562459"
  },
  {
    "text": "I claim that we could really conclude\nthat at least N minus to the two-thirds",
    "start": "562459",
    "end": "568299"
  },
  {
    "text": "of these UPI's are equal to a half plus or\nminus maybe 1 over the cube root of N.",
    "start": "568299",
    "end": "575080"
  },
  {
    "text": "And so we can make extremely concrete statements about this that really\npin down the of p sub i's.",
    "start": "575080",
    "end": "580590"
  },
  {
    "text": "And again, the intuition is that if\nthis wasn't the case, if there was more variation between the Pi's, we would have\nseen very different summary statistics.",
    "start": "582230",
    "end": "589769"
  },
  {
    "text": "Okay, so this is pretty amazing. So suppose we solved,\nwe looked at the aggregate statistics and",
    "start": "592400",
    "end": "598709"
  },
  {
    "text": "saw almost the exact,\nn/4 people with each of these outcomes.",
    "start": "598710",
    "end": "602000"
  },
  {
    "text": "And based on this we concluded that almost\nall of these Pi's are very close to half. And now suppose you said, look at\nthis fourth person, they got two nos.",
    "start": "604720",
    "end": "614259"
  },
  {
    "text": "What do you suppose their\ntrue probability is? What do you think p sub 4 is? Well, now their argument would be,\nlook almost all of the Pi's are half, so",
    "start": "614260",
    "end": "623959"
  },
  {
    "text": "I think p sub 4 is probably\nvery close to a half. And on average, this will be very good.",
    "start": "623960",
    "end": "631840"
  },
  {
    "text": "We will get extremely low average error\nover our estimates of all the people.",
    "start": "631840",
    "end": "637270"
  },
  {
    "text": "And this is pretty surprising, right? So if you think about what's\ngoing on here we're estimating P sub 4 more accurately than\nempirical distribution.",
    "start": "637270",
    "end": "645170"
  },
  {
    "text": "Based on using information about\nthe independent random variables corresponding to everybody\nelse's coin tosses.",
    "start": "646360",
    "end": "652800"
  },
  {
    "text": "So we're estimating p4,\nnot just based upon the tosses of the coin that land heads is called op4, but\nalso based upon the probability of",
    "start": "652800",
    "end": "662390"
  },
  {
    "text": "it landing hands of all these other\nindependent A random variable corresponding to different people,\nthis is pretty surprising.",
    "start": "662390",
    "end": "671589"
  },
  {
    "text": "So you might wonder to what extent have\nI cherry picked the special settings,",
    "start": "674122",
    "end": "679431"
  },
  {
    "text": "where exactly n over 4, the people\nanswered yes, yes, no, no, and so on.",
    "start": "679431",
    "end": "684760"
  },
  {
    "text": "So it turns out that even in\nthe worst case no matter what the set of underlying Pi's are you can\naccurately learn the set of Pi's to",
    "start": "685770",
    "end": "695220"
  },
  {
    "text": "average our epsilon using only 1 over\nepsilon samples from each individual.",
    "start": "695220",
    "end": "700350"
  },
  {
    "text": "Only 1 over epsilon opposite of each coin. Again as opposed to if you were to\nnaively use the empirical estimate.",
    "start": "700350",
    "end": "706480"
  },
  {
    "text": "Which you do require a 1 over epsilon\nsquared tosses of each point. So this is a very simple setting.",
    "start": "706480",
    "end": "713250"
  },
  {
    "text": "We're just trying to estimate\nthe steps of biases of coins that rises in many settings, very useful.",
    "start": "713250",
    "end": "719540"
  },
  {
    "text": "But there are lots of more\nA higher level analogue",
    "start": "719540",
    "end": "724899"
  },
  {
    "text": "to the setting which we're considering and\nwhich we are thinking about now. So there are these more\ncomplex distributions.",
    "start": "724900",
    "end": "730470"
  },
  {
    "text": "So instead of just each\nperson having one coin, imagine each person has something\nthat happens as probability p sub i,",
    "start": "730470",
    "end": "738730"
  },
  {
    "text": "something else that might happen\nas probability q sub i, and so on. So maybe, each person corresponds\nto a visit or to a website.",
    "start": "738730",
    "end": "746110"
  },
  {
    "text": "P sub i is whether they buy an item,\nQ sub i is whether they end up",
    "start": "746110",
    "end": "750820"
  },
  {
    "text": "asking for\nsomething on the help page and so on. So maybe, there are these more\ncomplicated distributions,",
    "start": "752450",
    "end": "759454"
  },
  {
    "text": "to what extent can we get\nan analogous results for these. You might also wonder how you can leverage\nadditional structure to the problem to be",
    "start": "759454",
    "end": "767618"
  },
  {
    "text": "able to do even better. So these are some avenues that\nwe're currently thinking about. So just to summarize, the punch lines\nof some of the first part of the talk.",
    "start": "767618",
    "end": "779009"
  },
  {
    "text": "So even with very little\ndata from each person We can accurately recover the stats or\npopulation",
    "start": "779010",
    "end": "784430"
  },
  {
    "text": "of distributions by leveraging the fact\nthat we have data from lots of people. And as we saw, so in many cases, we can\nthen use this information to go back and",
    "start": "784430",
    "end": "794329"
  },
  {
    "text": "improve our estimates of\nindividual people's distributions. This is the example where\nfirst we concluded that",
    "start": "794330",
    "end": "800540"
  },
  {
    "text": "almost all of the p sub bias\nmust be very close to half and then we go back and say well the fourth\nperson even though the empirical estimate",
    "start": "800540",
    "end": "807580"
  },
  {
    "text": "looks like they're a probability of\n0 their piece is also probably half.",
    "start": "807580",
    "end": "811810"
  },
  {
    "text": "And you can do this as a special case\nas what's known as Stein's Phenomena. So this is named after\nthe late Charles Stein who was",
    "start": "813740",
    "end": "820880"
  },
  {
    "text": "a statistician at Stanford and\npassed away a couple months ago. He was the first one who noticed that you\ncan actually improve estimates of some",
    "start": "820880",
    "end": "828110"
  },
  {
    "text": "parameters based on observing\nindependent random variables.",
    "start": "828110",
    "end": "833779"
  },
  {
    "text": "So even though I'm trying to estimate For the fourth person's probability,\nI can improve my estimates of this,",
    "start": "833780",
    "end": "840610"
  },
  {
    "text": "on average, if I look at these\nother independent random variables.",
    "start": "840610",
    "end": "844600"
  },
  {
    "text": "So now we'll transition into the second\npart where we'll talk about how to accurately estimate properties of\nthe unseen portion of a distribution.",
    "start": "846580",
    "end": "854539"
  },
  {
    "start": "856000",
    "end": "989000"
  },
  {
    "text": "So before I go on, let me stress that all\nof the techniques that I'm talking about",
    "start": "857170",
    "end": "861790"
  },
  {
    "text": "are ways of cleaning up the empirical\ndistribution of the samples, and",
    "start": "862840",
    "end": "867970"
  },
  {
    "text": "these are ways of cleaning up\nthe empirical distribution without making any prior assumptions\non the distribution in question.",
    "start": "867970",
    "end": "873950"
  },
  {
    "text": "So we're doing this based\npurely on an understanding Of how the randomness from\nthe sampling process ends up,",
    "start": "873950",
    "end": "883064"
  },
  {
    "text": "in some sense, noising up the empirical\ndistribution of what we see.",
    "start": "883065",
    "end": "890310"
  },
  {
    "text": "Okay, so in the second portion, We'll be\ntalking about settings where we're given independent draws from some\ndistribution over a discrete support.",
    "start": "891980",
    "end": "900000"
  },
  {
    "text": "So think about it as we're sampling\nspecies of fish from some population. So we are independently\ndrawing fish from an ocean,",
    "start": "900000",
    "end": "907170"
  },
  {
    "text": "and we should think about this\nas we're sampling each fish with probability proportional to\nthe probability of that fish in the ocean.",
    "start": "907170",
    "end": "914230"
  },
  {
    "text": "Okay, so in this example, we drew six\nfish, and we saw three different species.",
    "start": "914230",
    "end": "919459"
  },
  {
    "text": "What can we hope to say about\nthe underlying distribution of fish? Well, the empirical distribution of\nthe sample, this is the maximum likelihood",
    "start": "919460",
    "end": "930529"
  },
  {
    "text": "distribution that maximizes likelihood\nof drawing exactly what we observed.",
    "start": "930530",
    "end": "934690"
  },
  {
    "text": "However, it does not capture the\nunobserved portion of the distribution. So in this case,\nwe saw three different species of fish.",
    "start": "936410",
    "end": "943550"
  },
  {
    "text": "Maybe there's a fourth species or\na fifth species of fish in the ocean. The empirical distribution does\nnot account for this at all.",
    "start": "943550",
    "end": "949370"
  },
  {
    "text": "So there are a few natural questions. So one is,\nwhat could we even hope to infer",
    "start": "949370",
    "end": "955020"
  },
  {
    "text": "about the unobserved portion\nof the distribution? Can we hope to say anything about it?",
    "start": "955020",
    "end": "958899"
  },
  {
    "text": "As we'll see the answer is yes. We can actually say very\nnuanced things about this.",
    "start": "960610",
    "end": "964600"
  },
  {
    "text": "And the second point is well,\nhow can these instances about unobserved proportion of the distribution\nyield better estimates for",
    "start": "965960",
    "end": "972377"
  },
  {
    "text": "aspects of a distribution\nthat we care about. Suppose you want to estimate\nthe entropy of the distribution or",
    "start": "972377",
    "end": "978162"
  },
  {
    "text": "other properties of it. How can we leverage a better understanding\nof the unobserved portion to improve these",
    "start": "978162",
    "end": "985018"
  },
  {
    "text": "estimates over just\nthe empirical estimates?",
    "start": "985018",
    "end": "987668"
  },
  {
    "start": "989000",
    "end": "1582000"
  },
  {
    "text": "So in some sense the motivation for\nmy work, or the inspiration for my work on these problems goes back to very early\nwork of R.A. Fisher, and Allen Turing",
    "start": "992997",
    "end": "1002363"
  },
  {
    "text": "who are respectively the godfathers of\nmodern statistics and computer science.",
    "start": "1002363",
    "end": "1007360"
  },
  {
    "text": "So in the case of Fisher, he was given\nthis data of a biologist named Corbet.",
    "start": "1009374",
    "end": "1015180"
  },
  {
    "text": "So Corbet spent a year in the Malaysian\njungle in the 1940s collecting butterflies.",
    "start": "1015180",
    "end": "1021050"
  },
  {
    "text": "And this chart at the bottom of the page,\nthis is Corbet's Butterfly Data. So what does this represent?",
    "start": "1021050",
    "end": "1027328"
  },
  {
    "text": "Well, the left-most spike represents\nthe fact that, while he was in Malaysia,",
    "start": "1027328",
    "end": "1032545"
  },
  {
    "text": "he discovered or\nhe caught 110 species of butterflies.",
    "start": "1032545",
    "end": "1036340"
  },
  {
    "text": "And of these 110 species of butterflies, he only ever saw individuals from them,\nexactly once.",
    "start": "1037590",
    "end": "1042655"
  },
  {
    "text": "The second spike represents\nthe fact that there were 75 species of butterflies that he\ncaught exactly twice.",
    "start": "1044650",
    "end": "1051140"
  },
  {
    "text": "The third spike represents the fact\nthat were something like 45 species of butterflies that he caught three\ntimes in his year in Malaysia.",
    "start": "1052740",
    "end": "1059210"
  },
  {
    "text": "And way off on the right of the plot,\nwhich is cut off. Yes, there's the common ground moth, and\nyou probably caught that 1,000 times.",
    "start": "1060400",
    "end": "1067804"
  },
  {
    "text": "And that corresponds to\na spike of height 1, representing the fact that there's\n1 species that he saw 1,000 times.",
    "start": "1067804",
    "end": "1074486"
  },
  {
    "text": "So this would be a spike at\nlocation 1,000 of height 1. So while he was there he discovered\nsomething like 230 species of butterflies.",
    "start": "1074486",
    "end": "1084629"
  },
  {
    "text": "So he sent his data to Ari Fisher,\nwho's the big statistician of the time, and he said listen.",
    "start": "1084629",
    "end": "1090673"
  },
  {
    "text": "I'm thinking about staying for\nanother year in the jungle, can you tell me how many new\nspecies you think I would discover",
    "start": "1090673",
    "end": "1097988"
  },
  {
    "text": "if I catch butterflies at the same\nrate as I did during my first year. And it actually turns out that there is\na extremely accurate estimate for this.",
    "start": "1097988",
    "end": "1107592"
  },
  {
    "text": "So, if you compute this alternating sum. So, take the total number of\nspecies you've seen exactly once,",
    "start": "1107592",
    "end": "1113879"
  },
  {
    "text": "subtract the number of species you've\nobserved twice, add the number of species you've seen three times, subtract\nthe number you've seen four times.",
    "start": "1113879",
    "end": "1121700"
  },
  {
    "text": "Compute this alternating sum. And this gives an essentially\nunbiased estimate",
    "start": "1121700",
    "end": "1126779"
  },
  {
    "text": "of the total number of new species\nyou would expect to discover in the same time period if you're\ncollecting butterflies at the same rate.",
    "start": "1126780",
    "end": "1133500"
  },
  {
    "text": "And again, this assumes nothing other\nthan the fact that you're collecting butterflies at the same rate and that the underlying distribution\nof butterflies is fixed.",
    "start": "1135300",
    "end": "1142910"
  },
  {
    "text": "So this is pretty surprising right,\nwe are accurately saying something about the unobserved portion\nof the distribution.",
    "start": "1145580",
    "end": "1150160"
  },
  {
    "text": "So at roughly the same\ntime Alan Turing and I.J. Good were working at Bletchley Park\non the British World War II effort.",
    "start": "1152720",
    "end": "1159710"
  },
  {
    "text": "They were interested in\nestimating similar things. So they wanted to estimate,\nwhat's the probability that the next",
    "start": "1159710",
    "end": "1166040"
  },
  {
    "text": "German enigma machine, ciphertext, it\ndidn't use ciphertext that we hadn't seen.",
    "start": "1166040",
    "end": "1170150"
  },
  {
    "text": "Differently, in Corbet's language, this is\nestimating the probability that the next butterfly that he catches is a new\nspecies that he hasn't seen.",
    "start": "1171620",
    "end": "1179780"
  },
  {
    "text": "A third way of stating this, if you are trying to estimate\nthe total probability math in the true",
    "start": "1182280",
    "end": "1188380"
  },
  {
    "text": "distribution that consistent to mean\nelements that you haven't observed yet.",
    "start": "1188380",
    "end": "1192620"
  },
  {
    "text": "So this is the missing math in other\nwords, the math in the true distribution which is missing completely from\nthe empirical distribution.",
    "start": "1193780",
    "end": "1200440"
  },
  {
    "text": "And it turns out that there's\na very simple estimate for this, an essentially unbiased estimate for\nthis missing math.",
    "start": "1201800",
    "end": "1206820"
  },
  {
    "text": "And this is now known as the Goodâ€“Turing\nfrequency estimation scheme. This says look at the total number of\ndomain elements that you've seen exactly",
    "start": "1208100",
    "end": "1216160"
  },
  {
    "text": "once, divide it by your sample size,\nand this tells you a good estimate for",
    "start": "1216160",
    "end": "1222090"
  },
  {
    "text": "the probability that the next\ndomain element you see is new. So if you think about this.",
    "start": "1222090",
    "end": "1227150"
  },
  {
    "text": "What does this mean? Well, this means, if you want to know,\nwhat's the probability that the next",
    "start": "1227150",
    "end": "1232380"
  },
  {
    "text": "song you hear on the radio is a new\nsong you've never heard before. Well, think back. Count the total number of songs you've\nheard exactly once in your life.",
    "start": "1232380",
    "end": "1240610"
  },
  {
    "text": "Divide that by the number of\nsongs you've heard, in total. And this gives you a estimate for the probability you'll be surprised\nwith the next song you hear.",
    "start": "1240610",
    "end": "1249309"
  },
  {
    "text": "Okay and this makes sense. If in general you were surprised by 3% of\nthe songs you hear, the probability you're",
    "start": "1249309",
    "end": "1255424"
  },
  {
    "text": "going to be surprised by the next\nsong you hear is probably 3%.",
    "start": "1255424",
    "end": "1259097"
  },
  {
    "text": "Okay so Turing and\nFisher give extremely accurate estimates, in some sense of a single parameter of\nthe unseen portion of the distribution.",
    "start": "1262356",
    "end": "1272674"
  },
  {
    "text": "So, how much more can\nwe hope to understand? So, in our results we're actually going\nto be able to recover estimates of",
    "start": "1272674",
    "end": "1279945"
  },
  {
    "text": "the structure of the unseen portions. Not just estimate of the total amount of\nmass, but actually estimates of the number",
    "start": "1279946",
    "end": "1286554"
  },
  {
    "text": "of unseen elements that arise\nwith different probabilities.",
    "start": "1286554",
    "end": "1289669"
  },
  {
    "text": "So what is this good for? Well one can answer questions\nin the following form. One can ask questions like,\nwhat's the value of collecting more data?",
    "start": "1292640",
    "end": "1300290"
  },
  {
    "text": "If we were to collect\na hundred times more data, how many more domain elements\nare we likely to observe?",
    "start": "1300290",
    "end": "1307750"
  },
  {
    "text": "Or we could ask questions like, how much\ndata is necessary before actually we",
    "start": "1307750",
    "end": "1312950"
  },
  {
    "text": "are likely to see 99.9% of the phenomena\nto rise in the given distribution?",
    "start": "1312950",
    "end": "1318396"
  },
  {
    "text": "So this latter question\nmight be relevant for assessing like autonomous vehicles\nwhere you want to know how much more",
    "start": "1318396",
    "end": "1324531"
  },
  {
    "text": "do we need to revise these\nthings until we actually have a good coverage over the sort of events\nthat arrive when you're driving?",
    "start": "1324531",
    "end": "1331315"
  },
  {
    "text": "And there are a number of more\nscientific reasons why you might care about understanding the structure\nof the unseen portions,",
    "start": "1334284",
    "end": "1343275"
  },
  {
    "text": "including understanding properties\nof the unseen in humans.",
    "start": "1343275",
    "end": "1348332"
  },
  {
    "text": "And I'll give some concrete examples of\nthat towards the end of this portion.",
    "start": "1348332",
    "end": "1352730"
  },
  {
    "text": "Okay, so how can we do this? Well, let's begin with just a thought\nexperiment, suppose we actually take 100",
    "start": "1354140",
    "end": "1362170"
  },
  {
    "text": "million samples, we draw 100 million\nindependent draws from some distribution. And suppose we see about one\nmillion domain elements, and",
    "start": "1362170",
    "end": "1369879"
  },
  {
    "text": "each element, each domain element,\nwe see roughly 100 times. And if you were to look at\na histogram of the frequencies,",
    "start": "1369880",
    "end": "1377040"
  },
  {
    "text": "suppose we see something like this. So again, what does this plot represent?",
    "start": "1378390",
    "end": "1382765"
  },
  {
    "text": "Well the big spike at 100 represents\nthe fact that there are lots and lots",
    "start": "1384235",
    "end": "1389605"
  },
  {
    "text": "of domain elements that we see roughly\n100 times, we see exactly 100 times.",
    "start": "1389605",
    "end": "1394235"
  },
  {
    "text": "And yes there's some domain elements\nthat we only see 80 times or 90 times, and some domain elements that we see 120,\nor 130 times.",
    "start": "1395425",
    "end": "1402155"
  },
  {
    "text": "This is a, you can think of this as\na histogram of the histogram of the data. So, this is a histogram\nof the frequencies.",
    "start": "1403860",
    "end": "1410290"
  },
  {
    "text": "Suppose someone asks you, well, what distribution do you think\nthese samples were drawn from?",
    "start": "1413000",
    "end": "1416630"
  },
  {
    "text": "In other words, if someone said hey,\nlook at this one domain element. We observed it 130 times.",
    "start": "1418540",
    "end": "1424464"
  },
  {
    "text": "What do you think its true probability is? Would you answer? Well the empirical probability is\n130 divided by the sample size,",
    "start": "1424464",
    "end": "1434843"
  },
  {
    "text": "so 130 divided by 100M. Or Would you do something else?",
    "start": "1434843",
    "end": "1440983"
  },
  {
    "text": "So I claim that actually, based on no\nprior assumptions on the distribution, only an understanding of the mapping\nfrom a true distribution to an empirical",
    "start": "1440983",
    "end": "1450780"
  },
  {
    "text": "distribution, we can accurately\nde-noise this distribution, de-noise this empirical distribution.",
    "start": "1450780",
    "end": "1457690"
  },
  {
    "text": "And conclude that the true distribution\nfrom which these samples were drawn is probably extremely close to a uniform\ndistribution over a million domain",
    "start": "1457691",
    "end": "1466278"
  },
  {
    "text": "elements. So why is this the case? Well, suppose the true distribution\nactually were a uniform distribution over",
    "start": "1466278",
    "end": "1473761"
  },
  {
    "text": "a million domain elements. If we had drawn the 100\nmillion samples from this, we would've seen something\nextremely similar to this picture.",
    "start": "1473761",
    "end": "1481730"
  },
  {
    "text": "Most things we see roughly 100 times,\nsome things we see more, some things we see less.",
    "start": "1481730",
    "end": "1486790"
  },
  {
    "text": "And the standard deviation and the number\nof times we see different elements could be roughly ten, and\nit would roughly look like this picture.",
    "start": "1486790",
    "end": "1493620"
  },
  {
    "text": "If the true distribution had any\nsignificant variance in the probabilities, then after you take into account,",
    "start": "1495903",
    "end": "1502531"
  },
  {
    "text": "the extra sampling noise that's added just\nfrom the sampling process, I claim we would end up seeing this histogram that's\neven faster, that has even more variance.",
    "start": "1502531",
    "end": "1512470"
  },
  {
    "text": "One way of thinking about this is\nthat the mapping from the true distribution to the empirical distribution",
    "start": "1514690",
    "end": "1522490"
  },
  {
    "text": "can be almost thought of as\na sort of convolution operator. Something that can add noise and\nsneers out the probabilities.",
    "start": "1522490",
    "end": "1530559"
  },
  {
    "text": "And the question is just to what\nextent can you de-convolve? Can you de-noise? Can you remove the effect\nof the sampling noise?",
    "start": "1530560",
    "end": "1537429"
  },
  {
    "text": "In this case, I claim you can do it\nvery robustly and very accurately. And what does this mean?",
    "start": "1537430",
    "end": "1542720"
  },
  {
    "text": "Well, this means that someone comes and\nsays, hey, this element was observed 130 times,\nwhat do you think its true probability is?",
    "start": "1542720",
    "end": "1548790"
  },
  {
    "text": "You can safely say, well, yes,\nI know this guy was observed 130 times and this other domain element\nwas only observed 80 times.",
    "start": "1549820",
    "end": "1557340"
  },
  {
    "text": "But I think that actually, the true\nprobabilities of them are probably almost the same and\nprobably equal to just one over a million.",
    "start": "1557340",
    "end": "1564246"
  },
  {
    "text": "Just like almost all these other elements,\nokay? So this is an analog of the reasoning\nthat we did in the first part.",
    "start": "1564246",
    "end": "1570309"
  },
  {
    "text": "Okay, so we can do some de-noising. And one question is, well,\nhow far can we push this?",
    "start": "1573598",
    "end": "1580100"
  },
  {
    "start": "1582000",
    "end": "1751000"
  },
  {
    "text": "So let's just continue trying to\ndo this kind of mental experiment.",
    "start": "1583157",
    "end": "1590140"
  },
  {
    "text": "So suppose we have a histogram based on k\nsamples, and we see something like this. Well, we'd say this looks like maybe it\ncorresponds to uniform distribution over k",
    "start": "1590140",
    "end": "1599443"
  },
  {
    "text": "over 100 elements. This is what we just did. Why? Because if the true distribution had any\nsignificant variance in the probabilities,",
    "start": "1599443",
    "end": "1607951"
  },
  {
    "text": "we would expect to see something\nslightly from what we see. Of course, proving that might be\ndifficult, but that's the reason.",
    "start": "1607951",
    "end": "1617740"
  },
  {
    "text": "Okay, let's look at another example. Suppose this is actually a histogram\nbased on 10,000 samples drawn from some",
    "start": "1619860",
    "end": "1625450"
  },
  {
    "text": "distribution. What distribution do you\nthink this is drawn from? Well, you might recognize that this kind\nof looks like a Poisson distribution of",
    "start": "1625450",
    "end": "1634100"
  },
  {
    "text": "expectation five. So you could argue that, well, seeing this\nis consistent with having drawn these",
    "start": "1634100",
    "end": "1639944"
  },
  {
    "text": "10,000 samples from a uniform\ndistribution over 2,000 elements.",
    "start": "1639944",
    "end": "1644710"
  },
  {
    "text": "And in this case you'd be right. This histogram was made by\nme sampling 10,000 things in",
    "start": "1645860",
    "end": "1651750"
  },
  {
    "text": "that lab from a uniform\ndistribution over 2,000 elements. How far can we push this?",
    "start": "1651750",
    "end": "1656535"
  },
  {
    "text": "Well, you might recognize that this is\na Poisson distribution of expectation two and think that maybe this corresponds to\na uniform distribution over support size",
    "start": "1658950",
    "end": "1666789"
  },
  {
    "text": "10,000 divided by 2 over\nsupport size 5,000. And you would be correct.",
    "start": "1666789",
    "end": "1672225"
  },
  {
    "text": "Maybe this corresponds to a uniform\ndistribution over 10,000 elements.",
    "start": "1672225",
    "end": "1676223"
  },
  {
    "text": "Maybe this is the uniform\ndistribution over 20,000 elements. Maybe this is the uniform\ndistribution over 50,000 elements.",
    "start": "1677820",
    "end": "1684387"
  },
  {
    "text": "So how far can we push this? So at this point, what we're claiming\nis actually pretty impressive.",
    "start": "1686070",
    "end": "1692980"
  },
  {
    "text": "So, we have a histogram,\nwe have data based on 10,000 samples. We've only seen 9,000\ndistinct domain elements.",
    "start": "1692980",
    "end": "1699650"
  },
  {
    "text": "And yet, we're claiming we can reason that\nthe reason this data was probably drawn from some distribution supported\non 50,000 domain elements.",
    "start": "1701160",
    "end": "1708551"
  },
  {
    "text": "And that this distribution is\nprobably extremely close to uniform. So we haven't seen most of these domain\nelements that we're actually claiming that",
    "start": "1708551",
    "end": "1717019"
  },
  {
    "text": "we can accurately describe roughly\nhow many of them they are and roughly their true probability.",
    "start": "1717019",
    "end": "1722310"
  },
  {
    "text": "So the natural question is,\nwell, how general and how far can this reasoning be pushed?",
    "start": "1724530",
    "end": "1732540"
  },
  {
    "text": "So in all of these examples, we were\nthinking about uniform distributions. And the only reason we're thinking\nabout uniform distributions is because",
    "start": "1732540",
    "end": "1740309"
  },
  {
    "text": "it's easier to do our mental\ncalculations on them. So what can we say about non-uniform\ndistributions and how can we actually",
    "start": "1740310",
    "end": "1748540"
  },
  {
    "text": "make this approach rigorous and\nturn this into an actual algorithm?",
    "start": "1748540",
    "end": "1753626"
  },
  {
    "start": "1751000",
    "end": "1817000"
  },
  {
    "text": "So if you think about what we were doing,\nwe were basically asking what distribution has expected histogram similar to\nthe observed histogram of the samples.",
    "start": "1753626",
    "end": "1762120"
  },
  {
    "text": "And it turns out that you can write this\nproblem, you can write this problem we we're solving in our heads on\nthe previous slide, as a linear program.",
    "start": "1764430",
    "end": "1772089"
  },
  {
    "text": "So I won't explain exactly\nwhat this linear program is. But it basically just asks,\nfind me a distribution with",
    "start": "1772089",
    "end": "1777809"
  },
  {
    "text": "expected histogram that's\nclose to the observed one. Why did it end up being a linear program?",
    "start": "1777809",
    "end": "1783899"
  },
  {
    "text": "Well, because we only care\nabout expectations and linearity of expectations,\nthat's where it comes from.",
    "start": "1783900",
    "end": "1788932"
  },
  {
    "text": "But I won't go into the details. So what does the algorithm do?",
    "start": "1788932",
    "end": "1795310"
  },
  {
    "text": "Well, it finds one of these\ndistributions via linear programming. And this linear program basically\njust defines a feasible region of",
    "start": "1795311",
    "end": "1802570"
  },
  {
    "text": "distributions which are all\nconsistent with the observation.",
    "start": "1802570",
    "end": "1805980"
  },
  {
    "text": "Of course, the technical\nchallenge is then to argue about the diameter of this feasible region\nshrinks as our sample size grows.",
    "start": "1809820",
    "end": "1816380"
  },
  {
    "start": "1817000",
    "end": "2008000"
  },
  {
    "text": "So we have some concrete technical\nresults regarding this, which, they're tight to constant factor.",
    "start": "1818530",
    "end": "1825021"
  },
  {
    "text": "And I just wanted to give you a flavor\nof what these sorts of things say. So given n independent draws\nfrom some unknown distribution,",
    "start": "1825021",
    "end": "1834623"
  },
  {
    "text": "we can recover the sets of probabilities,\np sub i.",
    "start": "1834623",
    "end": "1839400"
  },
  {
    "text": "As well as an empirical distribution\nwould, if the empirical distribution would've been based on an extra\nlogarithmic factor more samples.",
    "start": "1840780",
    "end": "1848951"
  },
  {
    "text": "So two points. So the first is we are only recovering\nthe sets of probabilities p sub i.",
    "start": "1848951",
    "end": "1854720"
  },
  {
    "text": "We're not recovering\nthe underlying domain. So think of it this way. So suppose you go fishing,\nyou catch lots of fish.",
    "start": "1854720",
    "end": "1862059"
  },
  {
    "text": "We're going to recover a set of\nprobabilities of the species.",
    "start": "1862060",
    "end": "1868083"
  },
  {
    "text": "Although if you haven't caught any tuna, I'm not going to say,\nthere must be a tuna in the ocean.",
    "start": "1868083",
    "end": "1873492"
  },
  {
    "text": "Okay, so\nI'm not going to label the domain. I'm just going to try to recover\nthe sets of probabilities.",
    "start": "1873492",
    "end": "1880126"
  },
  {
    "text": "So one corollary of this result, which is\nmaybe easier to understand and easier to parse, is for a given n independent draws\nfrom some distribution, you can accurately",
    "start": "1880126",
    "end": "1889608"
  },
  {
    "text": "estimate the number of new elements\nthat you would see in a larger sample. And you can do this for samples of size\nup to O(n log n) in the worst case.",
    "start": "1889608",
    "end": "1900720"
  },
  {
    "text": "So in most practical settings,\nwe seem to be able to accurately infer the number of new things\nwe'll see from much, much larger samples.",
    "start": "1900720",
    "end": "1909330"
  },
  {
    "text": "But this is the best we can do\nin the worst case sense, and there's matching lower information\ntheoretical lower bound",
    "start": "1909330",
    "end": "1914360"
  },
  {
    "text": "I wanted to give a quick\npractical application of this. So this is in joint work with James Zou\nand this appeared Nature Communications.",
    "start": "1916400",
    "end": "1924396"
  },
  {
    "text": "So currently we have 60,000 genomes\nof healthy individuals that have been sequenced.",
    "start": "1924396",
    "end": "1931800"
  },
  {
    "text": "So one natural question is, what's\nthe value in sequencing larger cohorts? So say we sequence half a million\na people, two million people,",
    "start": "1931800",
    "end": "1939808"
  },
  {
    "text": "ten million people,\nwhat's the value in this? How many more new medically relevant\nmutations are we likely to see if",
    "start": "1939808",
    "end": "1946435"
  },
  {
    "text": "we sequence more people? So applying the linear programming\nbased algorithm from two slides ago,",
    "start": "1946436",
    "end": "1958120"
  },
  {
    "text": "we can do the following\nsort of validation.",
    "start": "1958120",
    "end": "1963210"
  },
  {
    "text": "So yes, we have 60,000 genomes\nof healthy individuals, suppose we only looked at 6,000 of them.",
    "start": "1963210",
    "end": "1970470"
  },
  {
    "text": "And based on those 6,000 we tried to\nestimate how many new mutations of a certain medically relevant type we\nwould be likely to see if we actually had",
    "start": "1970470",
    "end": "1978860"
  },
  {
    "text": "60,000 genomes. So based on the first 6,000 genomes we\ncan predict how many new things we would",
    "start": "1978860",
    "end": "1985184"
  },
  {
    "text": "have seen, given more genomes. And the blue line is our predictions.",
    "start": "1985184",
    "end": "1990340"
  },
  {
    "text": "The red line is the ground truth, because\nwe do actually have 60,000 genomes. And then the light blue area\ncorresponds to one standard deviation",
    "start": "1990340",
    "end": "1999020"
  },
  {
    "text": "over different choices of the 6,000\nsize cohort out of the 60,000 genomes.",
    "start": "1999020",
    "end": "2004390"
  },
  {
    "text": "So we're doing quite well. So then the fun part is to say well,\nwe do actually have 60,000 genomes,",
    "start": "2004390",
    "end": "2011704"
  },
  {
    "start": "2008000",
    "end": "2034000"
  },
  {
    "text": "what would we be likely to see,\nif we sequenced a quarter of a million or half a million more people?",
    "start": "2011704",
    "end": "2017026"
  },
  {
    "text": "So just to provide a tiny bit\nof background on this, so here we are looking at\nloss-of-function mutations.",
    "start": "2020020",
    "end": "2026649"
  },
  {
    "text": "So a loss-of-function mutation is a\nmutation which will break that given gene.",
    "start": "2026649",
    "end": "2032110"
  },
  {
    "text": "So on average, most healthy\npeople have maybe one or one and half loss-of- function mutations,\nand it's not a big deal.",
    "start": "2033580",
    "end": "2040990"
  },
  {
    "start": "2034000",
    "end": "2145000"
  },
  {
    "text": "So one thing people are doing at the\nStanford Hospital now, if you come in and you're very sick, it's usually children\nwho have very strange symptoms and",
    "start": "2040990",
    "end": "2050030"
  },
  {
    "text": "it's not clear what's wrong with them. The people at the hospital start by\njust sequencing their genome and",
    "start": "2050030",
    "end": "2057339"
  },
  {
    "text": "looking for loss-of-function mutations. So they ask are there\nany mutations which we",
    "start": "2057340",
    "end": "2063642"
  },
  {
    "text": "know break a given gene in this patient?",
    "start": "2063642",
    "end": "2068669"
  },
  {
    "text": "And then what do they do? For each of these broken genes, they ask,\nin this cohort of 60,000 healthy",
    "start": "2068670",
    "end": "2075282"
  },
  {
    "text": "individuals, were there any healthy\nindividuals who also had this broken gene?",
    "start": "2075282",
    "end": "2080080"
  },
  {
    "text": "And if the answer is yes, there are lots of healthy people that\nalso has this broken gene, then they say, well this broken gene probably isn't\ncausing this, isn't causing them sickness.",
    "start": "2081350",
    "end": "2090149"
  },
  {
    "text": "If none of the healthy people\nhave this broken gene, then they say maybe this broken gene\nis what's causing the sickness and",
    "start": "2091350",
    "end": "2099100"
  },
  {
    "text": "then they try to figure out\nwhat does this gene do? Maybe it codes for some protein,\nmaybe you can synthesize this protein and",
    "start": "2099100",
    "end": "2104730"
  },
  {
    "text": "inject it into the patient, and\nthen the patient gets better. Of course we have 20,000 genes,",
    "start": "2104730",
    "end": "2112003"
  },
  {
    "text": "we only have 60,000 genomes\nhealthy people's genomes.",
    "start": "2112003",
    "end": "2117069"
  },
  {
    "text": "So there are lots of genes which might be\nokay to break which we haven't observed. And this plot basically tells us how many\nmore individuals would we need to sequence",
    "start": "2117070",
    "end": "2126300"
  },
  {
    "text": "in other to get a pretty good\ncoverage over the genes for which it's okay to have\nbroken in a healthy person.",
    "start": "2126300",
    "end": "2132459"
  },
  {
    "text": "So this is a real value setting and we're going to be trying to\nestimate the covariance matrix.",
    "start": "2134660",
    "end": "2140630"
  },
  {
    "text": "And this is joint work with Weihao Kong.",
    "start": "2140630",
    "end": "2145136"
  },
  {
    "start": "2145000",
    "end": "2200000"
  },
  {
    "text": "So suppose we have an n independent draws\nfrom some d dimensional distribution. A natural question is to understand how\nlarge must n be, what must our sample",
    "start": "2148932",
    "end": "2158130"
  },
  {
    "text": "size be, in order to understand\nthe structure of the distribution? And one question is, well,\nif we have too little data to actually",
    "start": "2158130",
    "end": "2166839"
  },
  {
    "text": "find the structure, then maybe we're not\ngoing to be given enough information to accurately return top principal components\nor eigenvectors of our distribution.",
    "start": "2166840",
    "end": "2175700"
  },
  {
    "text": "Can we still say whether or not there\nis structure in the distribution? So can you say something like listen, I\ndon't have enough distribution to, sorry,",
    "start": "2176770",
    "end": "2185170"
  },
  {
    "text": "I don't have enough data,\nTo tell you which low dimensional space your\ndistribution lies in,",
    "start": "2185170",
    "end": "2194709"
  },
  {
    "text": "though I do know that it lies\nin some low dimensional space.",
    "start": "2194709",
    "end": "2200224"
  },
  {
    "start": "2200000",
    "end": "2405000"
  },
  {
    "text": "Okay, so\nthis is kind of a thought experiment. Suppose I have my data consisting\nof 1,000 independent draws from",
    "start": "2200224",
    "end": "2209428"
  },
  {
    "text": "some 1,000 dimensional\nGaussian distribution.",
    "start": "2209428",
    "end": "2213180"
  },
  {
    "text": "And the Gaussian distribution\nhas mean zero, and I'm not sure what the covariance is.",
    "start": "2214900",
    "end": "2220620"
  },
  {
    "text": "And if you were to look at\na histogram of the eigenvalues, of the empirical covariance of the data,\nso, again you have a 1,000 data points,",
    "start": "2222480",
    "end": "2230550"
  },
  {
    "text": "you just look at the covariance\nof these data and you look and you ask how many directions are there that\nlook like there are lots of variance?",
    "start": "2230550",
    "end": "2238210"
  },
  {
    "text": "And you plot this histogram,\nsuppose this is what you see. So what does this mean?",
    "start": "2238210",
    "end": "2243500"
  },
  {
    "text": "This means that well, there are lots of\ndirections that have almost no variance, that's the big spike at the left.",
    "start": "2243500",
    "end": "2249150"
  },
  {
    "text": "And there are a few directions that\nhave variants four and three and so on. Okay, so you have your data,\nit looks like there's lots of directions.",
    "start": "2250240",
    "end": "2258599"
  },
  {
    "text": "It looks like there are 200\ndirections with very little variance, a couple of directions with big variance. And now suppose someone asks you,",
    "start": "2258600",
    "end": "2266000"
  },
  {
    "text": "what was the true covariance structure\nof this underlying distribution?",
    "start": "2267140",
    "end": "2271369"
  },
  {
    "text": "Well, I claim that in this case the\nempirical eigenvalues are very misleading.",
    "start": "2273599",
    "end": "2278770"
  },
  {
    "text": "And in fact, the data was\ngenerated by taking 1,000 samples from an identity covariance Gaussian.",
    "start": "2279850",
    "end": "2285550"
  },
  {
    "text": "So this means that all of\nthe directions have the same variance. However, because we only had 1,000 samples\nfrom our 1,000 dimensional distribution,",
    "start": "2285550",
    "end": "2295570"
  },
  {
    "text": "just due to the randomness,\nit looks like there's some directions with big variance and\nsome some directions with small variance.",
    "start": "2295570",
    "end": "2300719"
  },
  {
    "text": "If you were to run this experiment again, if you were to take another thousand\nsamples from the same Gaussian, and plot",
    "start": "2302650",
    "end": "2308870"
  },
  {
    "text": "the eigenvalues of empirical covariance,\nyou would get this exact same plot again.",
    "start": "2308870",
    "end": "2313460"
  },
  {
    "text": "And if I kept doing this you would keep\ngetting a plot very similar to this. So now, suppose a year later,\nsome doctor came up to you and said hey,",
    "start": "2314920",
    "end": "2324041"
  },
  {
    "text": "I got this new data. I'm looking at the covariance structure in",
    "start": "2324041",
    "end": "2330386"
  },
  {
    "text": "varied mutations or\nsome medically important something or other, and\nthis is my covariance structure.",
    "start": "2330386",
    "end": "2337400"
  },
  {
    "text": "Look, there are a bunch of interesting, there are a bunch of interesting\nrelationships that have this big variance.",
    "start": "2337400",
    "end": "2343340"
  },
  {
    "text": "What would you say? You say I remember this spot,\nI know this curve.",
    "start": "2344400",
    "end": "2349790"
  },
  {
    "text": "This curve actually is the curve that\narises when you have no structure,",
    "start": "2349790",
    "end": "2355250"
  },
  {
    "text": "there are no interesting\nrelationships in the data, you just have too small of a sample size.",
    "start": "2355250",
    "end": "2359310"
  },
  {
    "text": "So I claim that every time you see a plot\nof this, a plot that looks like this,",
    "start": "2362021",
    "end": "2367057"
  },
  {
    "text": "reflecting the eigenvalues of an empirical\nprogramming matrix, you can safely",
    "start": "2367057",
    "end": "2372417"
  },
  {
    "text": "conclude that actually there's no\nstructure in the underlying distribution.",
    "start": "2372417",
    "end": "2377805"
  },
  {
    "text": "Okay, so one question is, suppose you\nactually do look at this histogram of eigenvalues of an empirical covariance and\nit looks something different than this.",
    "start": "2377805",
    "end": "2387339"
  },
  {
    "text": "To what extent can you then de-noise\nthis and reveal the true covariant structure of the distribution and\nwhile you can apply.",
    "start": "2387340",
    "end": "2397730"
  },
  {
    "text": "The same mindset as we did earlier, but using very different techniques\nto say things about this.",
    "start": "2397730",
    "end": "2402880"
  },
  {
    "text": "So maybe I'll skip the formal.",
    "start": "2402880",
    "end": "2404269"
  },
  {
    "start": "2405000",
    "end": "2409000"
  },
  {
    "text": "Yeah, I'll skip the formal statement. So, just to conclude with\na few final thoughts.",
    "start": "2408653",
    "end": "2414750"
  },
  {
    "start": "2409000",
    "end": "2502000"
  },
  {
    "text": "So, we discussed two, and\nmaybe three different settings. And in all three of these settings,\nthe underlying approaches are following.",
    "start": "2414750",
    "end": "2423500"
  },
  {
    "text": "We first understood the mapping\nfrom the underlying distribution to the empirical distribution.",
    "start": "2423500",
    "end": "2428420"
  },
  {
    "text": "And then the kind of crux of\nwhat we were actually doing,",
    "start": "2430390",
    "end": "2435700"
  },
  {
    "text": "which is try to invert this mapping. To say suppose we actually saw\nthis empirical distribution,",
    "start": "2435700",
    "end": "2441920"
  },
  {
    "text": "what does this tell us about\nthe true distribution underlying it? So we're inverting this kind of mapping\nfrom the truth to the empirical.",
    "start": "2441920",
    "end": "2450140"
  },
  {
    "text": "And in these three different settings,\nit yielded, in some sense, pretty surprising results that actually\nlead to meaningful algorithms.",
    "start": "2451300",
    "end": "2459394"
  },
  {
    "text": "Okay so if you get nothing else from\nthis talk, I hope you at least remember",
    "start": "2459394",
    "end": "2464726"
  },
  {
    "text": "that well in many settings, even when\nthe empirical distribution is misleading,",
    "start": "2464726",
    "end": "2470746"
  },
  {
    "text": "you can still often extract\nsurprisingly accurate information.",
    "start": "2470746",
    "end": "2475570"
  },
  {
    "text": "And the second point that there\nare many extremely basic settings. Settings that you would have thought\nstatisticians maybe might have",
    "start": "2477020",
    "end": "2483930"
  },
  {
    "text": "understood a century ago,\nthat we still don't understand. So there's much room for future research\nabout this intersection of algorithms and",
    "start": "2483930",
    "end": "2493150"
  },
  {
    "text": "statistics and\ninformation theory in learning. So thank you. So I guess now maybe we'll open it up for\nquestions.",
    "start": "2493150",
    "end": "2500090"
  },
  {
    "text": "So I guess there are a few\nquestions that I wanted to address. So there was one question from Andre Coke,",
    "start": "2501510",
    "end": "2507883"
  },
  {
    "start": "2502000",
    "end": "2849000"
  },
  {
    "text": "who asked how do you know that\nthe unseen distribution law is uniform? So this corresponds to\nthe second portion of the talk.",
    "start": "2507883",
    "end": "2517620"
  },
  {
    "text": "This was where we did\nthis thought experiment. Starting with, suppose you have 100\nmillion samples, and you happen to have",
    "start": "2517620",
    "end": "2524780"
  },
  {
    "text": "seen roughly 1 million domain elements,\neach observed roughly 100 times.",
    "start": "2524780",
    "end": "2529270"
  },
  {
    "text": "So, in this kind of thought experiment, it did turn out that the unseen\ndistribution law was uniform.",
    "start": "2530410",
    "end": "2538338"
  },
  {
    "text": "So all of these were actually\nuniform distributions. And so the question is,\nhow do we know this?",
    "start": "2538339",
    "end": "2546150"
  },
  {
    "text": "And well, the claim is,\nif your work has [INAUDIBLE] that actually looked like this data that I\nwas hypothetically giving, you",
    "start": "2546150",
    "end": "2556080"
  },
  {
    "text": "can actually robustly conclude that the\ntrue underlying distribution is uniform. The results apply much more generally.",
    "start": "2556080",
    "end": "2563550"
  },
  {
    "text": "So even if the true underlying\ndistribution isn't uniform, this linear programming approach\nto inferring the underlying",
    "start": "2563550",
    "end": "2571140"
  },
  {
    "text": "structure will actually give you, or\nwill accurately tell you the structure. Even if the underlying\ndistribution is not uniform.",
    "start": "2571140",
    "end": "2578950"
  },
  {
    "text": "The point in these examples was more\njust to give a concrete example that's",
    "start": "2578950",
    "end": "2584310"
  },
  {
    "text": "somehow simple enough that we\ncan actually think about it. But the underlying techniques\nwork more generally.",
    "start": "2584310",
    "end": "2590380"
  },
  {
    "text": "And this is actually a similar response\napplies to some of the earlier",
    "start": "2591470",
    "end": "2597280"
  },
  {
    "text": "questions about the kind of warm up,\nthe first part at the talk which asks, are we assuming that the individuals\nare identical and independent?",
    "start": "2597280",
    "end": "2607010"
  },
  {
    "text": "The answer is, well we're assuming\nthey're independent, but not identical. So maybe some of the people\nhave larger probabilities,",
    "start": "2607010",
    "end": "2615050"
  },
  {
    "text": "some of them have smaller probabilities. And we would actually be able to infer\nthe underlying set of these probabilities,",
    "start": "2615050",
    "end": "2623700"
  },
  {
    "text": "even if they are not all the same. And when I say we can infer the set of\nprobabilities, again, to start with,",
    "start": "2623700",
    "end": "2631373"
  },
  {
    "text": "we're not trying to figure out\neach person's probability. So I don't want to know,\nif someone just tosses a coin twice,",
    "start": "2631373",
    "end": "2639902"
  },
  {
    "text": "I maybe will never know what\ntheir true probability is. But I'll infer, roughly, how many people\nhave probabilities 0.4, how many people",
    "start": "2639902",
    "end": "2649220"
  },
  {
    "text": "have probability of roughly 0.5, how many\npeople have a probability of roughly 0.6.",
    "start": "2649220",
    "end": "2652934"
  },
  {
    "text": "Okay, so I guess one other question,\nthat's actually a really,",
    "start": "2654746",
    "end": "2661558"
  },
  {
    "text": "really nice question, from Yan Gokowski. So they ask well, what, if any, is the\nrelationship between these techniques, and",
    "start": "2661558",
    "end": "2670820"
  },
  {
    "text": "deconvolution approaches,\nsuch as used in signal processing?",
    "start": "2670820",
    "end": "2673880"
  },
  {
    "text": "So this is a very good question. If you think about what the mapping is\nbetween kind of a true distribution and",
    "start": "2676210",
    "end": "2685093"
  },
  {
    "text": "the empirical distribution, in many cases this does look like\nyou're convolving the true structure of",
    "start": "2685093",
    "end": "2691472"
  },
  {
    "text": "the distribution by some noise operator\nto yield the empirical distribution.",
    "start": "2691472",
    "end": "2696349"
  },
  {
    "text": "So for example, if we have a uniform\ndistribution over a million elements and",
    "start": "2697420",
    "end": "2703460"
  },
  {
    "text": "then we take 100 million\nsamples from this, the empirical frequencies will kind of\nlook like we convolved this big spike",
    "start": "2703460",
    "end": "2711870"
  },
  {
    "text": "of uniform distribution by some\nsort of Gaussian-type distribution.",
    "start": "2713230",
    "end": "2717510"
  },
  {
    "text": "And in that sense,\nyou can almost view these techniques",
    "start": "2718640",
    "end": "2723700"
  },
  {
    "text": "as deconvolving to remove\nthe effects of sampling, basically.",
    "start": "2723700",
    "end": "2729634"
  },
  {
    "text": "From a technical perspective,\nthis is actually a little different. So usually when you\nthink of convolution and",
    "start": "2729634",
    "end": "2736320"
  },
  {
    "text": "deconvolution, you have a fixed kernel. In this case the mapping from the true\ndistribution to the empirical",
    "start": "2736320",
    "end": "2744725"
  },
  {
    "text": "distribution, it's not\nactually convolusion, even though it's some kind of blurring\noperator, but you have a much more",
    "start": "2744725",
    "end": "2753208"
  },
  {
    "text": "complicated blurrning operator which\nmakes it in general harder to deconvolve.",
    "start": "2753208",
    "end": "2758581"
  },
  {
    "text": "So, particularly in the last setting\nthat I had discussed where you have some distribution of empirical Eigen\nvalues of the covariance matrix.",
    "start": "2758581",
    "end": "2768806"
  },
  {
    "text": "It becomes very clear that the mapping\nfrom the true structure of the geometric distribution, the true Eigen values and\nthe covariance, the empirical thing.",
    "start": "2768806",
    "end": "2778720"
  },
  {
    "text": "It's not just a convolution, it's some\nvery strange kind of noise operator that behaves very differently for\nthe big Eigen vectors and the small ones.",
    "start": "2778720",
    "end": "2786830"
  },
  {
    "text": "And so that's where kind of\nthe technical difficulty comes in.",
    "start": "2786830",
    "end": "2792370"
  },
  {
    "text": "That intuitively you want to do\nthis denoising, deconvolution, but it's not actually\na clean deconvolution.",
    "start": "2792370",
    "end": "2799100"
  },
  {
    "text": "Okay so, sorry, so the one of\nthe questions asked, do I have an example",
    "start": "2800150",
    "end": "2805180"
  },
  {
    "text": "of eigenvalue distribution for covariance\nmatrix with meaningful structure. So I do have some thoughts I can show.",
    "start": "2805180",
    "end": "2812240"
  },
  {
    "text": "So each of these plots corresponds to the,\nyou can think of this as",
    "start": "2814000",
    "end": "2819470"
  },
  {
    "text": "the cumulative distribution function\nof the eigenvalues of a distribution.",
    "start": "2819470",
    "end": "2825580"
  },
  {
    "text": "So what does this actually mean? So the blue line is corresponding\nto the true distribution.",
    "start": "2825580",
    "end": "2832610"
  },
  {
    "text": "And this means that well half of the\neigenvalues are one of the covariants and half of them are two.",
    "start": "2832610",
    "end": "2838260"
  },
  {
    "text": "What does this mean? This means that we have some\nhigh dimensional distribution.",
    "start": "2839350",
    "end": "2844540"
  },
  {
    "text": "And half of the direction to variance one,\nhalf of the direction to variance two.",
    "start": "2844540",
    "end": "2848880"
  },
  {
    "start": "2849000",
    "end": "3097000"
  },
  {
    "text": "If you were to look at\nthe empirical covariance matrix. The very light blue lines\nthat look like these curves,",
    "start": "2850890",
    "end": "2858627"
  },
  {
    "text": "this represents the eigenvalues of\nthe empirical covariance matrix.",
    "start": "2858627",
    "end": "2863732"
  },
  {
    "text": "So in particular if you have relatively\nfew samples, the directions of variance 1, the directions of variance 2 all just\nget blurred together and it looks like",
    "start": "2863732",
    "end": "2873076"
  },
  {
    "text": "some directions to variance 1.5,\nsome directions to variance 0.3 and so on.",
    "start": "2873076",
    "end": "2877910"
  },
  {
    "text": "Using this sort of, I guess you can almost\nthink of it as a de-convolution approach.",
    "start": "2879300",
    "end": "2885170"
  },
  {
    "text": "Even when the number of samples is linear\nor slightly smaller than the dimension, we can actually recover the underlying\nstructure, namely that there are half",
    "start": "2885170",
    "end": "2896255"
  },
  {
    "text": "the dimensions have variance one then\nhalf the dimensions have variance two.",
    "start": "2896255",
    "end": "2901295"
  },
  {
    "text": "And we have other examples\nof cases like this",
    "start": "2901295",
    "end": "2906380"
  },
  {
    "text": "where there is actually structure in\nthe underlying covariance matrix.",
    "start": "2906381",
    "end": "2914915"
  },
  {
    "text": ">> So maybe, Greg,\nnow that we're at the top of the hour, you can take one more question?",
    "start": "2914915",
    "end": "2920918"
  },
  {
    "text": ">> Okay, so, yeah, so I guess I'll finish with a slightly\nmore kind of general question.",
    "start": "2920918",
    "end": "2926926"
  },
  {
    "text": "So this question's from\nOrlando Tucker who asked, how important is this information in\nmachine learning and deep learning?",
    "start": "2926926",
    "end": "2933770"
  },
  {
    "text": "So this is a good question. So most of the work I do kind of\nfocuses on fundamental kind of",
    "start": "2933770",
    "end": "2939414"
  },
  {
    "text": "questions that have more\nof a theoretical nature. And I guess my answer",
    "start": "2939414",
    "end": "2949671"
  },
  {
    "text": "is twofold. So looking forward, at some point,\nI do think if we want progress in machine",
    "start": "2949671",
    "end": "2958391"
  },
  {
    "text": "learning, deep learning to continue at the\nrate that it has, we will actually need to",
    "start": "2958392",
    "end": "2963576"
  },
  {
    "text": "start thinking about how do you get more\ninformation from the data than currently.",
    "start": "2963576",
    "end": "2968626"
  },
  {
    "text": "So deep learning has done a lot of great\nthings, but it is extremely data hungry.",
    "start": "2968626",
    "end": "2973696"
  },
  {
    "text": "So you really do need to\ngive it a lot of data. And if you think about it, what's the\nchance that we're going to get an extra",
    "start": "2973696",
    "end": "2983034"
  },
  {
    "text": "factor of 1,000 to 10,000\nmore good text data? Not great.\nWe already have all of Wikipedia,",
    "start": "2983034",
    "end": "2990252"
  },
  {
    "text": "finding an extra factor\nof 10,000 more good data, it doesn't seem like we\nshould go down that path.",
    "start": "2990252",
    "end": "2999178"
  },
  {
    "text": "Instead it seems like we should try to\nfigure out how to get more from the data we have.",
    "start": "2999178",
    "end": "3004360"
  },
  {
    "text": "I guess from a very concrete standpoint,\nso one can plug in some of these techniques\nkind of below the deep learning or",
    "start": "3004360",
    "end": "3013494"
  },
  {
    "text": "more high level machine\nlearning portions of a system. So one can these sorts of techniques\nto come out correct empirical",
    "start": "3013495",
    "end": "3023367"
  },
  {
    "text": "distributions a little bit and then hand\nthem off to these higher level systems.",
    "start": "3023367",
    "end": "3030092"
  },
  {
    "text": "And also I guess one dimension that there\nis, yes there's a lot of kind of higher",
    "start": "3030092",
    "end": "3035348"
  },
  {
    "text": "level machine learning stuff, but there's\nalso need to actually concretely get",
    "start": "3035348",
    "end": "3040762"
  },
  {
    "text": "information about the value of collecting\nmore data and questions like this.",
    "start": "3040762",
    "end": "3045963"
  },
  {
    "text": "So one thing I'm starting to think about\nis how do you use these ideas together",
    "start": "3045963",
    "end": "3051781"
  },
  {
    "text": "with higher level machine learning things\nto ask if you were to take ten times more",
    "start": "3051781",
    "end": "3057867"
  },
  {
    "text": "data how would the performance of your\nsystems improve and that sort of thing.",
    "start": "3057867",
    "end": "3063796"
  },
  {
    "text": "So at least some of the ideas can\nwork nicely in conjunction with these machine learning and\ndeep learning approaches.",
    "start": "3063796",
    "end": "3073023"
  },
  {
    "text": "So thank you. >> Great, so thank you, Greg for\na wonderful presentation and for taking time to answer some\nof our attendee questions.",
    "start": "3073023",
    "end": "3080629"
  },
  {
    "text": "And we want to thank you for\ntaking the time to attend today. And hopefully we'll see\nyou at our next webinar. Have a great day.",
    "start": "3080629",
    "end": "3086077"
  }
]