[
  {
    "start": "0",
    "end": "177000"
  },
  {
    "start": "0",
    "end": "5410"
  },
  {
    "text": "Hello, everybody. Welcome to the\nCS224N, lecture 10.",
    "start": "5410",
    "end": "10720"
  },
  {
    "text": "This is going to be\nprimarily on pretraining, but we will also discuss\nsubword models a little bit,",
    "start": "10720",
    "end": "17050"
  },
  {
    "text": "and review transformers. OK. So we have a lot of exciting\nthings to get into today,",
    "start": "17050",
    "end": "24460"
  },
  {
    "text": "but some reminders\nabout the class. Assignment 5 is\nbeing released today,",
    "start": "24460",
    "end": "32470"
  },
  {
    "text": "assignment 4 was\ndue a minute ago, so if you're done with\nthat, congratulations. If not, I hope that\nthe late days go well.",
    "start": "32470",
    "end": "40870"
  },
  {
    "text": "Assignment 5 is on\npretraining and transformers. So these lectures are going to\nbe very useful to you for that.",
    "start": "40870",
    "end": "48980"
  },
  {
    "text": "And it doesn't cover anything\nafter these lectures. All right. So today let's kind of take\na little peek through what",
    "start": "48980",
    "end": "58240"
  },
  {
    "text": "the outline will be. We haven't talked about subword\nmodeling yet, and sort of we should have.",
    "start": "58240",
    "end": "63958"
  },
  {
    "text": "And so we're going to talk\na little bit about subwords. You saw these in\nassignment 4, just as",
    "start": "63958",
    "end": "70357"
  },
  {
    "text": "the data that we provided to you\nwith your machine translation system, but we're going to\ntalk a little bit about why",
    "start": "70357",
    "end": "75640"
  },
  {
    "text": "they're so ubiquitous in NLP. Because they are used\nin pretrained models.",
    "start": "75640",
    "end": "81892"
  },
  {
    "text": "I mean, they're used in a\nnumber of different models. But when we discuss\npretraining, it's important to know that\nsubwords are a part of it,",
    "start": "81892",
    "end": "89728"
  },
  {
    "text": "then we'll sort of\nmotivate-- we will go on another journey of\nmotivation, of motivating model pretraining from\nword embeddings.",
    "start": "89728",
    "end": "96070"
  },
  {
    "text": "So we've already\nseen pretraining in some sense in the very\nfirst lecture of this course,",
    "start": "96070",
    "end": "101200"
  },
  {
    "text": "because we pretrained\nindividual word embeddings that don't take into account their\ncontexts on very large text",
    "start": "101200",
    "end": "107170"
  },
  {
    "text": "corpora and saw that\nthey were able to encode a lot of useful\nthings about language.",
    "start": "107170",
    "end": "113270"
  },
  {
    "text": "So after we do the\nmotivation, we'll go through model\npretraining three ways. And we're going to\nreference actually",
    "start": "113270",
    "end": "119140"
  },
  {
    "text": "the lecture on Tuesday. So this is when we'll\nreview a little bit of the transformers stuff. We'll talk about model\npretraining and decoders,",
    "start": "119140",
    "end": "125468"
  },
  {
    "text": "like a transformer decoder\nthat we saw last week, and encoders, and\nthen encoder-decoders. And in each of\nthese three cases,",
    "start": "125468",
    "end": "131350"
  },
  {
    "text": "we're going to talk a\nlittle bit about sort of what things you\ncould be doing, and then popular models that\nare in use across research",
    "start": "131350",
    "end": "138595"
  },
  {
    "text": "and in industry. Then we're going to\ntalk a little bit about, what do we think\npretraining is teaching?",
    "start": "138595",
    "end": "144730"
  },
  {
    "text": "This going to be very brief. Actually a lot of the\ninterpretability and analysis lecture in two weeks\nis going to talk more",
    "start": "144730",
    "end": "150220"
  },
  {
    "text": "about sort of the mystery\nand the scientific problem of figuring out what\nthese models are learning",
    "start": "150220",
    "end": "156550"
  },
  {
    "text": "about language through\npretraining objectives, but we'll sort of get a peek. And then we'll talk\nabout very large models",
    "start": "156550",
    "end": "162490"
  },
  {
    "text": "and in-context learning. So if you've heard of\nGPT3, for example, we're going to just briefly\ntouch on that here,",
    "start": "162490",
    "end": "169400"
  },
  {
    "text": "and I think we'll\ndiscuss more about it in the course later on as well. OK. So we've got a lot to\ndo, let's jump right in.",
    "start": "169400",
    "end": "177500"
  },
  {
    "start": "177000",
    "end": "303000"
  },
  {
    "text": "So word structure\nand subword models. Lets think about sort\nof the assumptions we've been making in\nthis course so far.",
    "start": "177500",
    "end": "184000"
  },
  {
    "text": "When we give you\nan assignment, when we talk about training\nWord2vec for example,",
    "start": "184000",
    "end": "189010"
  },
  {
    "text": "we made this assumption about\na language's vocabulary. In particular, we've\nmade the assumption that it has a fixed\nvocabulary of something",
    "start": "189010",
    "end": "195250"
  },
  {
    "text": "like tens of thousands,\nmaybe 100,000, I don't know, a number of yeah. Some relatively large, it\nseems, number of words, and that",
    "start": "195250",
    "end": "202810"
  },
  {
    "text": "seems sort of like pretty\ngood so far at least, and what we've done. And we build this\nvocabulary from the set",
    "start": "202810",
    "end": "209980"
  },
  {
    "text": "that we train say, Word2vec on. And then, here's\nthe crucial thing. Any novel word, any\nword that you did not",
    "start": "209980",
    "end": "216100"
  },
  {
    "text": "see at training time, is sort\nof mapped to a single UNK token.",
    "start": "216100",
    "end": "222203"
  },
  {
    "text": "And there are other\nways to handle this, but you sort of\nhave to do something and a frequent method is\nto map them all to UNK.",
    "start": "222203",
    "end": "229120"
  },
  {
    "text": "So let's walk through what\nthis sort of means in English. You learn in embeddings,\nyou map them, it all works.",
    "start": "229120",
    "end": "236050"
  },
  {
    "text": "Then you have a variation\non a word, like \"taaaaasty.\" With a bunch of As.",
    "start": "236050",
    "end": "241443"
  },
  {
    "text": "Right, and your model\nisn't smart enough to that sort of means\nlike very tasty maybe.",
    "start": "241443",
    "end": "247760"
  },
  {
    "text": "And so it maps it\nto UNK because it's just a dictionary look up miss. And then, you have a typo\nlike \"laern,\" and that maps",
    "start": "247760",
    "end": "255739"
  },
  {
    "text": "to UNK as well potentially, if\nit wasn't in your training set. Some people make typos\nbut not all of them",
    "start": "255740",
    "end": "261828"
  },
  {
    "text": "will be seen at training time. And then you'll have\nnovel items, right? So this could be the first\ntime that you've ever seen,",
    "start": "261829",
    "end": "267090"
  },
  {
    "text": "you as the students\nin 224N have seen the word, \"transformerify.\" But I get the feeling you\nsort of have a notion of what",
    "start": "267090",
    "end": "274932"
  },
  {
    "text": "it's supposed to mean, right? Like maybe add\ntransformers to, or turn into using transformers,\nor turn into a transformer,",
    "start": "274932",
    "end": "281120"
  },
  {
    "text": "or something like that. And this is also going\nto be mapped to UNK. Even though you've seen\n\"transformer,\" and \"ify.\"",
    "start": "281120",
    "end": "288030"
  },
  {
    "text": "OK. And so somehow, the\nconclusion we have to come to is that, looking\nat words as just",
    "start": "288030",
    "end": "294620"
  },
  {
    "text": "like the individual\nsequences of characters uniquely identifies\nthat word, and that's",
    "start": "294620",
    "end": "299722"
  },
  {
    "text": "sort of how we should\nparameterize things is just wrong. And so, not only is this true in\nEnglish, but in many languages,",
    "start": "299722",
    "end": "308360"
  },
  {
    "start": "303000",
    "end": "390000"
  },
  {
    "text": "this finite\nvocabulary assumption makes even less sense. So already it doesn't\nmake sense in English. But English is-- it's not\nthe worst for English.",
    "start": "308360",
    "end": "316550"
  },
  {
    "text": "So morphology is the study\nof the structure of words.",
    "start": "316550",
    "end": "321860"
  },
  {
    "text": "And English is known to have\npretty simple morphology in kind of specific ways.",
    "start": "321860",
    "end": "327980"
  },
  {
    "text": "And when languages have\ncomplex morphology, it means you have longer\nwords, more complex words that",
    "start": "327980",
    "end": "334610"
  },
  {
    "text": "get modified more, and each one\nof them occurs less frequently. And that should sound\nlike a problem, right?",
    "start": "334610",
    "end": "340250"
  },
  {
    "text": "If a word occurs\nless frequently, it will be less likely to\nshow up in your training set,",
    "start": "340250",
    "end": "346310"
  },
  {
    "text": "and maybe it'll show\nup in your test set, never in your training set. Now it's mapped to UNK, and\nyou don't know what to do.",
    "start": "346310",
    "end": "351810"
  },
  {
    "text": "So an example, Swahili verbs can\nhave hundreds of conjugations. So each conjugation encodes\nimportant information",
    "start": "351810",
    "end": "361009"
  },
  {
    "text": "about the sentence that in\nEnglish might be represented through say, more words. But in Swahili it's mapped\nonto the verb as prefixes",
    "start": "361010",
    "end": "369875"
  },
  {
    "text": "and suffixes, and the like. This is called\ninflectional morphology. And so you can have\nhundreds of conjugations. I have just sort of pasted\nthis Wiktionary block just",
    "start": "369875",
    "end": "378290"
  },
  {
    "text": "to give you a small sample\nof just the huge number of conjugations there\nare-- and so trying to memorize\nindependently a meaning",
    "start": "378290",
    "end": "384890"
  },
  {
    "text": "of each one of these words\nis just not the right answer. ",
    "start": "384890",
    "end": "391979"
  },
  {
    "start": "390000",
    "end": "590000"
  },
  {
    "text": "So this is going to be\na very brief overview. And so what we're\ngoing to do, is take one, let's say a class of\nalgorithms for subword modeling",
    "start": "391980",
    "end": "403520"
  },
  {
    "text": "that have been kind\nof developed to try to take a middle ground\nbetween two options.",
    "start": "403520",
    "end": "410670"
  },
  {
    "text": "One option is saying, everything\nis just like individual words-- either I know the word and\nI saw it at training time,",
    "start": "410670",
    "end": "417169"
  },
  {
    "text": "or I don't know the\nword and it's like UNK. And then sort of another\nextreme option is to say, it's just characters.",
    "start": "417170",
    "end": "423680"
  },
  {
    "text": "Right? So like I get the\nsequence of characters, and then my neural\nnetwork on top of my sequence of\ncharacters has to learn",
    "start": "423680",
    "end": "430220"
  },
  {
    "text": "everything-- has to learn how\nto combine words and stuff. So subword models\nin general just",
    "start": "430220",
    "end": "435854"
  },
  {
    "text": "means looking at the sort of\ninternal structure of words, somehow looking\nbelow the word level. But this group of\nmodels is going",
    "start": "435855",
    "end": "443150"
  },
  {
    "text": "to try to meet a middle ground. So byte-pair encoding.",
    "start": "443150",
    "end": "448280"
  },
  {
    "text": "What we're going to\ndo is we're going to learn a vocabulary from\na training data set again.",
    "start": "448280",
    "end": "453782"
  },
  {
    "text": "So now we have a training data\nset, instead of just saying, oh, everything that was split\nby my heuristic word splitter",
    "start": "453782",
    "end": "459740"
  },
  {
    "text": "like spaces in\nEnglish for example, is going to be a word\nin my vocabulary,",
    "start": "459740",
    "end": "465470"
  },
  {
    "text": "we're going to learn\nthe vocabulary using a greedy algorithm in this case. So here's what\nwe're going to do.",
    "start": "465470",
    "end": "472550"
  },
  {
    "text": "We start with a vocabulary\ncontaining only characters. So that's our extreme, right? That's our-- at the\nvery least, if you've",
    "start": "472550",
    "end": "478850"
  },
  {
    "text": "seen all the characters,\nthen you know that you can-- you'll never have an UNK. Right? 'Cause you see a word,\nyou've never seen it before,",
    "start": "478850",
    "end": "485810"
  },
  {
    "text": "you just split it\ninto its characters, and then you try to\ndeal with it that way.",
    "start": "485810",
    "end": "491580"
  },
  {
    "text": "And then also an\nend of word symbol. And then we'll iterate\nover this algorithm we'll say, use the\ncorpus of text,",
    "start": "491580",
    "end": "497720"
  },
  {
    "text": "find common adjacent letters-- so maybe a and b are\nvery frequently adjacent.",
    "start": "497720",
    "end": "504000"
  },
  {
    "text": "Add the pair of them\ntogether as a single subword into your vocabulary.",
    "start": "504000",
    "end": "510330"
  },
  {
    "text": "Now replace instances\nof that character pair with the new subword, repeat\nuntil your desired vocabulary size.",
    "start": "510330",
    "end": "515450"
  },
  {
    "text": "So maybe you start with a\nsmall character vocabulary, and then you end up with\nthat same small character",
    "start": "515450",
    "end": "521880"
  },
  {
    "text": "vocabulary plus a bunch\nof sort of entire words or parts of words.",
    "start": "521880",
    "end": "527100"
  },
  {
    "text": "So notice how \"apple,\" an\nentire word, looks like \"apple.\" But then, \"app,\" maybe this\nis sort of the first part,",
    "start": "527100",
    "end": "533310"
  },
  {
    "text": "the first subword of\napplication or \"app.\" Yeah, and then \"ly,\"\nI guess I should",
    "start": "533310",
    "end": "541650"
  },
  {
    "text": "have not put the hash there. But maybe you learned \"ly\"\nas like the end of a word,",
    "start": "541650",
    "end": "548850"
  },
  {
    "text": "for example. And so, what you end up\nwith is a vocabulary where",
    "start": "548850",
    "end": "555120"
  },
  {
    "text": "common things, you get\nto map to themselves, and then rare sequences\nof characters, you kind of split as\nlittle as possible.",
    "start": "555120",
    "end": "562957"
  },
  {
    "text": "And it doesn't always\nend up so nicely that you learn like\nmorphologically relevant suffixes like \"ly,\" but you can\ntry to split things somewhat",
    "start": "562957",
    "end": "572100"
  },
  {
    "text": "reasonably and if\nyou have enough data, the subword vocabulary\nyou learn tends to be OK.",
    "start": "572100",
    "end": "577140"
  },
  {
    "text": "So this is originally used\nin machine translation, and now a similar\nmethod, WordPiece,",
    "start": "577140",
    "end": "582839"
  },
  {
    "text": "which we won't go\nover in this lecture is used in pretrained models,\nbut the idea is effectively the same and you end\nup with vocabularies",
    "start": "582840",
    "end": "589470"
  },
  {
    "text": "that look a lot like this. So if we go back to our--",
    "start": "589470",
    "end": "594660"
  },
  {
    "start": "590000",
    "end": "805000"
  },
  {
    "text": "if we go back to our examples\nof where word level NLP was failing us, then you\nhave \"hat\" mapping",
    "start": "594660",
    "end": "603959"
  },
  {
    "text": "to \"hat,\" OK that's good. We've \"hat\" mapping\nto \"hat,\" because that was a common enough\nsequence of characters",
    "start": "603960",
    "end": "609750"
  },
  {
    "text": "that it was actually\nincorporated into our subword vocabulary. Right? And then you have \"learn\"\nmapping to \"learn.\"",
    "start": "609750",
    "end": "615435"
  },
  {
    "text": "So common words, good. And that means that the model,\nthe neural network that you're going to process this\ntext with, does not",
    "start": "615435",
    "end": "621240"
  },
  {
    "text": "need to say, combine the\nletters of \"learn\" and \"hat\" in order to try to derive\nthe meaning of these words",
    "start": "621240",
    "end": "629342"
  },
  {
    "text": "from the letters,\nbecause you can imagine that might be\ndifficult. But then when you get a word that\nyou have not seen before,",
    "start": "629342",
    "end": "636450"
  },
  {
    "text": "you are able to decompose it. And so, if you've seen\ntasty with varying numbers",
    "start": "636450",
    "end": "642140"
  },
  {
    "text": "of As at training\ntime, maybe you actually get some of the same\nsubwords, or similar subwords",
    "start": "642140",
    "end": "649850"
  },
  {
    "text": "that you're splitting it\ninto at evaluation time. So we never saw tasty enough\nto work with however many",
    "start": "649850",
    "end": "655279"
  },
  {
    "text": "As in order to add it into\nour subword vocabulary. But we're still able to\nsplit it into things,",
    "start": "655280",
    "end": "660728"
  },
  {
    "text": "and then the neural\nnetwork that runs on top of these\nsubword embeddings could be able to sort\nof induce that oh, yeah.",
    "start": "660728",
    "end": "666769"
  },
  {
    "text": "This is one of those things\nwhere people chain letters together, chain vowels together\nin English for emphasis.",
    "start": "666770",
    "end": "675110"
  },
  {
    "text": "So misspellings still\npretty much mess you up. So now, \"laern,\"\nthis misspelling",
    "start": "675110",
    "end": "680990"
  },
  {
    "text": "might be mapped to two subwords. But if you saw misspellings\nlike this frequently enough, maybe you could learn\nsort of to handle it.",
    "start": "680990",
    "end": "688560"
  },
  {
    "text": "It still messes up\nthe model though. And-- but at the very least,\nit's not just an UNK, right?",
    "start": "688560",
    "end": "693770"
  },
  {
    "text": "It seems clearly\nbetter than that. And then, \"transformerify,\"\nmay be in the best, this is sort of optimistic,\nbut maybe in the best case,",
    "start": "693770",
    "end": "701015"
  },
  {
    "text": "right, you were able\nto say, oh, yes this is \"transformer,\" and \"ify.\"",
    "start": "701015",
    "end": "707360"
  },
  {
    "text": "Again, the subwords\nthat you learn don't actually tend to be this\nwell morphologically motivated, I think.",
    "start": "707360",
    "end": "712430"
  },
  {
    "text": "So \"ify\" is like a clear\nlike suffix in English that has a very common and replicable\nmeaning when you apply it",
    "start": "712430",
    "end": "719240"
  },
  {
    "text": "to nouns, that's\nderivational morphology. But you're able to sort of\ncompose the word of tran--",
    "start": "719240",
    "end": "726199"
  },
  {
    "text": "the meaning of \"transformerify\"\npossibly from its two subword constituents. And so, when we talk\nabout words being",
    "start": "726200",
    "end": "732860"
  },
  {
    "text": "input to transformer models,\npretrained to transformer models, throughout the\nentirety of this lecture,",
    "start": "732860",
    "end": "737970"
  },
  {
    "text": "we will be talking\nabout subwords. So I might say a\nword, and what I mean",
    "start": "737970",
    "end": "743000"
  },
  {
    "text": "is possibly a full word,\nalso possibly a subword. OK, so when we say a sequence\nof words, the transformer,",
    "start": "743000",
    "end": "749930"
  },
  {
    "text": "the pretrained\ntransformer has no idea, sort of whether it's dealing\nwith words or subwords, when",
    "start": "749930",
    "end": "757190"
  },
  {
    "text": "it's doing its\nself-attention operations. And so, this can be a problem. You can imagine if you\nhave really weird sequences",
    "start": "757190",
    "end": "764600"
  },
  {
    "text": "of characters, you can actually\nhave an individual single word mapped to as many words\nas it has characters.",
    "start": "764600",
    "end": "771350"
  },
  {
    "text": "That can be a problem,\nbecause suddenly you have a 10-word sentence\nbut one of the words is mapped to 20\nsubwords, and now you",
    "start": "771350",
    "end": "779090"
  },
  {
    "text": "have a 30-word sentence. Where 20 of the 30 words\nare just one real word. So keep this in\nmind, but I think",
    "start": "779090",
    "end": "786890"
  },
  {
    "text": "it's important for sort of this\nopen vocabulary assumption, it's important in English,\nand it's even more important",
    "start": "786890",
    "end": "791972"
  },
  {
    "text": "in many other languages. And the actual\nalgorithm, I mean, you can go into the actual\nalgorithms done for this,",
    "start": "791972",
    "end": "798845"
  },
  {
    "text": "byte-pair encoding is sort of my\nfavorite for going over briefly word piece you can\nalso take a look at.",
    "start": "798845",
    "end": "807089"
  },
  {
    "start": "805000",
    "end": "905000"
  },
  {
    "text": "OK. Any questions on subwords? I guess, John,\nsomebody was asking,",
    "start": "807090",
    "end": "812300"
  },
  {
    "text": "what does the hashtag mean? Oh great, great point. So this means that you should\nbe combining this subword.",
    "start": "812300",
    "end": "819420"
  },
  {
    "text": "So this subword it is\nnot the end of a word. \"taa##\" is sort of\ntelling the model.",
    "start": "819420",
    "end": "825300"
  },
  {
    "text": "So if I had \"taa\"\nwith no hashes, that's a separate\nsubword that means",
    "start": "825300",
    "end": "830510"
  },
  {
    "text": "there's an entire word that is\n\"taa,\" or at the very least, it's not the end of the word. See how here, I don't have\nthe hashes at the end,",
    "start": "830510",
    "end": "837560"
  },
  {
    "text": "it's because this is\nindicating that this is at the end of the word. Different subword\nschemes differ on whether you should put something\nat the beginning of the word,",
    "start": "837560",
    "end": "845120"
  },
  {
    "text": "if it does begin a word, or\nif you should put something at the end of the word if\nit doesn't end the word.",
    "start": "845120",
    "end": "850910"
  },
  {
    "text": "Right, so when the tokenizer\nis running over your data, so you've got something that's\ntokenizing this sentence,",
    "start": "850910",
    "end": "856730"
  },
  {
    "text": "\"in the worst case--\" Oh! Yeah. \"In the worst case--\"\nit says like, \"in,\"",
    "start": "856730",
    "end": "863570"
  },
  {
    "text": "that's a whole word. Give it just the\nword \"in,\" no hashes. That's a whole word. Give it just the word\n\"the,\" no hashes.",
    "start": "863570",
    "end": "870170"
  },
  {
    "text": "And then, maybe over\nhere at subwords, right, we've got this\nweird word subwords,",
    "start": "870170",
    "end": "876560"
  },
  {
    "text": "and it splits it\ninto sub and words. And so, \"sub,\" it's going\nto give it the subword with",
    "start": "876560",
    "end": "882110"
  },
  {
    "text": "S-U-B-#-# to indicate that\nit's part of this larger word,",
    "start": "882110",
    "end": "887839"
  },
  {
    "text": "subwords, as opposed to the\nword \"sub,\" like \"submarine,\" which would be different.",
    "start": "887840",
    "end": "894030"
  },
  {
    "text": "That was a great question. ",
    "start": "894030",
    "end": "902160"
  },
  {
    "text": "OK, great. So that was our note on\nsubword modeling, and you can--",
    "start": "902160",
    "end": "910140"
  },
  {
    "start": "905000",
    "end": "1180000"
  },
  {
    "text": "subwords are\nimportant for example in a lot of translation\napplications, that's",
    "start": "910140",
    "end": "916200"
  },
  {
    "text": "why we gave you subwords on\nthe translation assignment. Now let's talk about\nmodel pretraining and word",
    "start": "916200",
    "end": "921270"
  },
  {
    "text": "embeddings. So I love-- I love being able\nto go to this slide. So we saw this quote at\nthe beginning of the class.",
    "start": "921270",
    "end": "928110"
  },
  {
    "text": "\"You shall know a word\nby the company it keeps.\" And this was sort\nof one of the things that we used to summarize\ndistributional semantics.",
    "start": "928110",
    "end": "934840"
  },
  {
    "text": "This idea that Word2vec was sort\nof well motivated in some way, because the meaning of\na word can be thought",
    "start": "934840",
    "end": "940709"
  },
  {
    "text": "of as being derived\nfrom the kind of co-occurrence statistics of\nwords that co-occur around it,",
    "start": "940710",
    "end": "947160"
  },
  {
    "text": "and that was just fascinatingly\neffective, I think. But there's this other quote\nactually from the same person.",
    "start": "947160",
    "end": "954760"
  },
  {
    "text": "So we have J.R.\nFirth 1935 compared to our quote before from 1957.",
    "start": "954760",
    "end": "960450"
  },
  {
    "text": "And the second quote says,\n\"the complete meaning of a word is always contextual,\nand no study",
    "start": "960450",
    "end": "966240"
  },
  {
    "text": "of meaning apart from\na complete context can be taken seriously.\" Now again, these are just\nthings that we can sort of",
    "start": "966240",
    "end": "972720"
  },
  {
    "text": "think about and chew on. But it comes to\nmind, right, when",
    "start": "972720",
    "end": "977910"
  },
  {
    "text": "you embed words with\nWord2vec, one of the issues is that, you don't actually\nlook at its neighbors",
    "start": "977910",
    "end": "983160"
  },
  {
    "text": "as you're giving\nit an embedding. So if I have the sentence,\n\"I record the record.\"",
    "start": "983160",
    "end": "990360"
  },
  {
    "text": "The two instances of \"rec,\"\n\"ord\" mean different things, but they're given the\nsame Word2vec embedding.",
    "start": "990360",
    "end": "997120"
  },
  {
    "text": "Right? Because in Word2vec, you take\nthe string, you map it to, oh I've seen the\nword \"record\" before. You get that sort of vector\nfrom your learned matrix,",
    "start": "997120",
    "end": "1006050"
  },
  {
    "text": "and you give it the same\nthing in both cases. And so, what we're\ngoing to be doing today,",
    "start": "1006050",
    "end": "1011360"
  },
  {
    "text": "is actually not conceptually\nall that different from training Word2vec.",
    "start": "1011360",
    "end": "1016430"
  },
  {
    "text": "Word2vec training you can\nthink of as pretraining just a very simple\nmodel that only assigns",
    "start": "1016430",
    "end": "1021860"
  },
  {
    "text": "an individual vector\nto each unique word type, each unique element\nin your vocabulary.",
    "start": "1021860",
    "end": "1027319"
  },
  {
    "text": "Today, we'll be going a\nlot further than that. But the idea is very similar. So back in 2017, we would\nstart with pretrained word",
    "start": "1027319",
    "end": "1035209"
  },
  {
    "text": "embeddings. And again, remember,\nno context there. So you give a word an embedding\nindependent of the context",
    "start": "1035210",
    "end": "1041660"
  },
  {
    "text": "that it shows up in. And then, you learn how to\nincorporate the context. It's not like our NLP models\nnever used context, right?",
    "start": "1041660",
    "end": "1048410"
  },
  {
    "text": "Instead, you would learn to\nincorporate the context using your LSTM, or if it's\nlater in 2017, you know,",
    "start": "1048410",
    "end": "1054740"
  },
  {
    "text": "your transformer. And you will learn to\nincorporate context while",
    "start": "1054740",
    "end": "1060560"
  },
  {
    "text": "training on the task. So you have some supervision. Maybe it's machine\ntranslation supervision, maybe sentiment, maybe\nquestion answering.",
    "start": "1060560",
    "end": "1067343"
  },
  {
    "text": "And you would learn how\nto incorporate context in your LSTM or otherwise\nthrough the signal",
    "start": "1067343",
    "end": "1073400"
  },
  {
    "text": "of the training, instead of say,\nthrough the Word2vec signal. And so, sort of\npictographically,",
    "start": "1073400",
    "end": "1078950"
  },
  {
    "text": "you have these word\nembeddings here. So the red are sort of\nyour Word2vec embeddings and those are pretrained.",
    "start": "1078950",
    "end": "1084690"
  },
  {
    "text": "And those take up some of the\nparameters of your network. And then, you've got\nyour contextualization.",
    "start": "1084690",
    "end": "1090000"
  },
  {
    "text": "Now, this looks like an LSTM\nbut it could be whatever. So this maybe\nbidirectional encoder thing",
    "start": "1090000",
    "end": "1095060"
  },
  {
    "text": "here is not pretrained. And now that's a\nlot of parameters that are not pretrained,\nand then maybe you have some sort of readout\nfunction at the end, right?",
    "start": "1095060",
    "end": "1102740"
  },
  {
    "text": "To predict whatever thing\nyou're trying to predict again. Maybe it's sentiment, maybe\nyou're doing, I don't know,",
    "start": "1102740",
    "end": "1108650"
  },
  {
    "text": "topic labeling. Whatever you want to do,\nthis is sort of the paradigm, like you set some\ncode of architecture",
    "start": "1108650",
    "end": "1113780"
  },
  {
    "text": "and you only pretrained\nthe word embeddings. And so, this isn't\nactually conceptually",
    "start": "1113780",
    "end": "1119929"
  },
  {
    "text": "necessarily the biggest\nproblem, because we",
    "start": "1119930",
    "end": "1125330"
  },
  {
    "text": "like to think in\ndeep learning stuff that we have a lot of training\ndata for our objectives.",
    "start": "1125330",
    "end": "1130850"
  },
  {
    "text": "I mean one of the things that\nwe motivated big deep neural networks for, is that they\ncan take a lot of data,",
    "start": "1130850",
    "end": "1137450"
  },
  {
    "text": "and then we can learn\npatterns from it. But it does put the onus\non our downstream data",
    "start": "1137450",
    "end": "1142700"
  },
  {
    "text": "to be sort of sufficient to\nteach the contextual aspects of language.",
    "start": "1142700",
    "end": "1147870"
  },
  {
    "text": "So you can imagine\nif you only have a little bit of labeled\ndata for fine tuning,",
    "start": "1147870",
    "end": "1152900"
  },
  {
    "text": "you're putting a pretty big\nrole on that data to say, \"hey, maybe here's some\npretrained embeddings.\"",
    "start": "1152900",
    "end": "1157910"
  },
  {
    "text": "But how you handle\nlike sentences, and how they compose, and all\nthat stuff, that's up to you.",
    "start": "1157910",
    "end": "1163330"
  },
  {
    "text": "So if you don't have\na lot of labeled data for your downstream\ntask, you're asking it to do a lot with a large\nnumber of parameters that",
    "start": "1163330",
    "end": "1170930"
  },
  {
    "text": "have been initialized randomly. OK, so like a small\nportion of the parameters",
    "start": "1170930",
    "end": "1176409"
  },
  {
    "text": "have been pretrained OK. So where we're going is\npretraining whole models.",
    "start": "1176410",
    "end": "1183470"
  },
  {
    "start": "1180000",
    "end": "1332000"
  },
  {
    "text": "I mean, conceptually we're\npretty close to there. So nowadays, almost\nall parameters",
    "start": "1183470",
    "end": "1191150"
  },
  {
    "text": "in your neural\nnetwork, and let's say a lot of research settings,\nand increasingly in industry, are initialized via pretraining.",
    "start": "1191150",
    "end": "1198260"
  },
  {
    "text": "Just like Word2vec\nparameters were initialized. And pretraining\nmethods in general,",
    "start": "1198260",
    "end": "1203510"
  },
  {
    "text": "hide parts of the input\nfrom the model itself,",
    "start": "1203510",
    "end": "1208790"
  },
  {
    "text": "and then train the model\nto reconstruct those parts. How does this\nconnect to Word2vec?",
    "start": "1208790",
    "end": "1214730"
  },
  {
    "text": "In Word2vec people don't\nusually make this connection, but it's the following. You have an individual word,\nand it knows itself, right?",
    "start": "1214730",
    "end": "1223970"
  },
  {
    "text": "Because you have the\nembedding for the center word. Right? From assignment 2. You have the embedding\nfor the center word",
    "start": "1223970",
    "end": "1229530"
  },
  {
    "text": "and it knows itself,\nand you've masked out all of its neighbors. You've hidden all of\nits neighbors from it.",
    "start": "1229530",
    "end": "1235100"
  },
  {
    "text": "Right? All of its window neighbors,\nyou've hidden from it. You ask the center words\nto predict its neighbors.",
    "start": "1235100",
    "end": "1240679"
  },
  {
    "text": "Right? And so, this falls under\nthe category of pretraining.",
    "start": "1240680",
    "end": "1247092"
  },
  {
    "text": "All of these methods\nlook similar. You hide parts of the\ninput from the model, and train the model to\nreconstruct those parts.",
    "start": "1247093",
    "end": "1253940"
  },
  {
    "text": "The differences with\nfull model pretraining is that you don't give the\nmodel just the individual word",
    "start": "1253940",
    "end": "1259127"
  },
  {
    "text": "and have it learn an\nembedding of that word, you give it much\nmore of the sequence and have that predict held\nout parts of the sequence.",
    "start": "1259127",
    "end": "1266588"
  },
  {
    "text": "And we'll get into\nthe details there. But the takeaway is\nthat, everything here is pretrained jointly.",
    "start": "1266588",
    "end": "1273110"
  },
  {
    "text": "Possibly with the exception\nof the very last layer that predicts the label.",
    "start": "1273110",
    "end": "1279280"
  },
  {
    "text": "OK. And this has just\nbeen exceptionally effective at building\nrepresentations of language.",
    "start": "1279280",
    "end": "1285990"
  },
  {
    "text": "That just maps similar\nthings in language, to similar representations\nin these encoders,",
    "start": "1285990",
    "end": "1291090"
  },
  {
    "text": "just like how Word2vec\nmaps similar words to similar vectors. That's been exceptionally\neffective at making parameter",
    "start": "1291090",
    "end": "1298019"
  },
  {
    "text": "initializations, where you\nstart with these parameters that have been pretrained,\nand then you fine",
    "start": "1298020",
    "end": "1303450"
  },
  {
    "text": "tune them on your labeled data. And then third, they've\nbeen exceptionally effective at defining\nprobability distributions",
    "start": "1303450",
    "end": "1309750"
  },
  {
    "text": "over language, like\nin language modeling. They are actually\nreally useful to sample from in certain cases.",
    "start": "1309750",
    "end": "1316407"
  },
  {
    "text": "So these are three ways\nin which we interact with pretrained models. We use the representations\njust to compute similarities,",
    "start": "1316407",
    "end": "1322409"
  },
  {
    "text": "we use them for parameter\ninitializations, and we actually just use them\nas probability distributions.",
    "start": "1322410",
    "end": "1327690"
  },
  {
    "text": "Sort of how we trained them. OK. So lets-- we're going to get\ninto some technical parts here,",
    "start": "1327690",
    "end": "1336150"
  },
  {
    "start": "1332000",
    "end": "1660000"
  },
  {
    "text": "but I sort of want to\nthink broad thoughts about what we could\ndo with pretraining, and what kind of things we could\nexpect to potentially learn",
    "start": "1336150",
    "end": "1344159"
  },
  {
    "text": "from this general method\nof hide part of the input, and then see other\nparts of the input,",
    "start": "1344160",
    "end": "1349712"
  },
  {
    "text": "and then try to predict\nthe parts that you hid. OK. So Stanford University is\nlocated in blank California.",
    "start": "1349712",
    "end": "1356639"
  },
  {
    "text": "If we gave a model everything\nthat was not blanked out here, and asked to predict\nthe middle, right? The loss function\nwould train the model",
    "start": "1356640",
    "end": "1365490"
  },
  {
    "text": "to predict Palo\nAlto here, I expect. OK. So this is an\ninstance of something",
    "start": "1365490",
    "end": "1372461"
  },
  {
    "text": "that you could imagine being\na pretraining objective. You take in a sentence,\nyou remove part of it, and you say, recreate\nthe part that I removed.",
    "start": "1372462",
    "end": "1379530"
  },
  {
    "text": "And in this case, if I just\ngave a bunch of examples that looked like this, it might\nlearn this sort of trivia thing",
    "start": "1379530",
    "end": "1385930"
  },
  {
    "text": "here. OK. Here's another one I put\nblank fork down on the table.",
    "start": "1385930",
    "end": "1392490"
  },
  {
    "text": "This one is under-specified. Right? So this could be \"the fork,\" \"my\nfork,\" \"his fork,\" \"her fork,\"",
    "start": "1392490",
    "end": "1401660"
  },
  {
    "text": "\"some fork,\" \"a fork.\" So this is specifying the\nkinds of syntactic categories",
    "start": "1401660",
    "end": "1409100"
  },
  {
    "text": "of things that can sort\nof appear in this context. So this is another\nthing that you might be able to learn\nfrom such an objective.",
    "start": "1409100",
    "end": "1417230"
  },
  {
    "text": "Here-- so you have, the woman\nwalked across the street, checking for traffic\nover blank shoulder. One of the things that\ncould go over here",
    "start": "1417230",
    "end": "1423650"
  },
  {
    "text": "is, \"her,\" that's a\nco-reference statement. So you could learn sort of\nconnections between entities",
    "start": "1423650",
    "end": "1430160"
  },
  {
    "text": "in a text, where one\nword, woman can also co-refer to the same entity\nin the world, as this word,",
    "start": "1430160",
    "end": "1437840"
  },
  {
    "text": "this pronoun her.  Here, you could think\nabout, I went to the ocean",
    "start": "1437840",
    "end": "1443740"
  },
  {
    "text": "to see the fish, turtles,\nseals, and blank. Now here, I don't think there's\na single correct answer,",
    "start": "1443740",
    "end": "1448870"
  },
  {
    "text": "as to what we could see\ngoing into that blank. But a model could\nlearn a distribution of the kinds of\nthings that people",
    "start": "1448870",
    "end": "1454750"
  },
  {
    "text": "might be talking about when\nthey-- one go to the ocean, and two are excited\nto see marine life.",
    "start": "1454750",
    "end": "1460600"
  },
  {
    "text": "Right? So this is sort of\na semantic category. A lexical semantic\ncategory of things that might sort of be in the\nsame set of interest as fish,",
    "start": "1460600",
    "end": "1469090"
  },
  {
    "text": "turtles, and seals\nin the context of, \"I went to the ocean.\"",
    "start": "1469090",
    "end": "1474430"
  },
  {
    "text": "OK? So and you know, I\nexpect that there would be examples of this in\na large corpus of text, maybe",
    "start": "1474430",
    "end": "1481440"
  },
  {
    "text": "maybe a book. OK, here's another example.",
    "start": "1481440",
    "end": "1486520"
  },
  {
    "text": "Overall, the value I got from\nthe two hours watching it was the sum total of the\npopcorn and the drink.",
    "start": "1486520",
    "end": "1493510"
  },
  {
    "text": "The movie was blank. Right? And this is when I was\nsort of like, look out into the audience and say,\nwas the movie bad or good?",
    "start": "1493510",
    "end": "1500950"
  },
  {
    "text": "But the movie was bad,\nis my prediction here. Right? And so, this is teaching you\nsomething about sentiment.",
    "start": "1500950",
    "end": "1508419"
  },
  {
    "text": "About how people express\nsentiment in language. And so, this is-- even looks like a task itself.",
    "start": "1508420",
    "end": "1515950"
  },
  {
    "text": "Like do sentiment\nanalysis, is sort of what you need to do\nin order to figure out whether the movie\nwas bad or good.",
    "start": "1515950",
    "end": "1521500"
  },
  {
    "text": "Or maybe the word is\nneither bad or good, the movie was over or\nsomething like that. But like, if you\nhad to, if you had",
    "start": "1521500",
    "end": "1527500"
  },
  {
    "text": "to choose between is\nbad or good more likely, right, you sort of\nhave to figure out the sentiment of the text.",
    "start": "1527500",
    "end": "1534470"
  },
  {
    "text": "Now that's really fascinating. OK. Here's another one. Iroh went into the\nkitchen to make some tea.",
    "start": "1534470",
    "end": "1542550"
  },
  {
    "text": "Standing next to Iroh,\nZuko pondered his destiny. Zuko left the blank.",
    "start": "1542550",
    "end": "1548850"
  },
  {
    "text": "OK. So this is a little easy because\nwe really only show one place. So I guess we have\nanother noun, destiny.",
    "start": "1548850",
    "end": "1555870"
  },
  {
    "text": "But this is sort of reasoning\nabout spatial location, and the movement of sort of\nagents in an imagined world.",
    "start": "1555870",
    "end": "1562440"
  },
  {
    "text": "We could imagine text\nthat has lines like this, person went into\nthe place and was next to so-and-so,\nwho left and did that,",
    "start": "1562440",
    "end": "1569950"
  },
  {
    "text": "and so you have\nthese relationships. So here, Zuko left the kitchen,\nis the most likely thing",
    "start": "1569950",
    "end": "1575309"
  },
  {
    "text": "that I think would go here. And it sort of indicates\nthat in order for a model to learn to perform this,\nfill in the missing part task,",
    "start": "1575310",
    "end": "1585720"
  },
  {
    "text": "it might need to, in\ngeneral, figure out sort of where things are, and\nwhether statements mean or imply",
    "start": "1585720",
    "end": "1595409"
  },
  {
    "text": "that locality. So standing next to. Iroh went into the kitchen. Now Iroh is in the kitchen.",
    "start": "1595410",
    "end": "1601807"
  },
  {
    "text": "And then, standing\nnext to Iroh, means Zuko is now in the kitchen. Right? And then, Zuko now leaves where?",
    "start": "1601807",
    "end": "1608279"
  },
  {
    "text": "Well, he was in\nthe kitchen before. So this is sort of a very\nbasic sense of reasoning.",
    "start": "1608280",
    "end": "1613990"
  },
  {
    "text": "Now, this one. Here's the sentence. I was thinking about\nthe sequence that goes, 1, 1, 2, 3, 5, 8, 13, 21, blank.",
    "start": "1613990",
    "end": "1622750"
  },
  {
    "text": "Right? So I don't know. I can imagine people\nwriting stuff. So this is the\nFibonacci sequence.",
    "start": "1622750",
    "end": "1628680"
  },
  {
    "text": "And sort of you sum these\ntwo to get the next one, sum these two to get the next\none, sum these two, right?",
    "start": "1628680",
    "end": "1633930"
  },
  {
    "text": "And so, you have\nthis running sum, it's a famous\nsequence that shows up in a lot of text\non the internet.",
    "start": "1633930",
    "end": "1640480"
  },
  {
    "text": "And in general, you have to\nlearn the algorithm, or just the formula, I guess that\ndefines the Fibonacci sequence",
    "start": "1640480",
    "end": "1647040"
  },
  {
    "text": "in order to keep going. Do models learn\nthis in practice? Wait and find out.",
    "start": "1647040",
    "end": "1652980"
  },
  {
    "text": "But you would have\nto learn it in order to get the sequence to keep\ngoing, and going, and going.",
    "start": "1652980",
    "end": "1660550"
  },
  {
    "start": "1660000",
    "end": "1805000"
  },
  {
    "text": "OK. So we're going to get into\nspecific pretrained models, specific methods\nof pretraining now.",
    "start": "1660550",
    "end": "1667440"
  },
  {
    "text": "So I'm going to go over a brief\nreview of transformer encoders,",
    "start": "1667440",
    "end": "1672899"
  },
  {
    "text": "decoders, and\nencoder-decoders, because we're going to get into the sort\nof technical bits now.",
    "start": "1672900",
    "end": "1678780"
  },
  {
    "text": "So before I do that,\nI'm going to pause, are there any questions?",
    "start": "1678780",
    "end": "1685480"
  },
  {
    "text": "Oh, yeah. There's an important\nquestion asked about, the risk of overfitting our\nmodel on our input training",
    "start": "1685480",
    "end": "1691950"
  },
  {
    "text": "data when they're\ndoing pretraining. Then you could also\nanswer this question in the light of the\nhuge pretrained models",
    "start": "1691950",
    "end": "1697530"
  },
  {
    "text": "that we're seeing nowadays. Sorry the first part\nof that question, was it are we overfitting\nour models to what?",
    "start": "1697530",
    "end": "1706097"
  },
  {
    "text": "The risk of overfitting model\non our input training data when they're doing pretraining.",
    "start": "1706097",
    "end": "1711540"
  },
  {
    "text": "Got it. Yeah, so that's a good point. So we're using\nvery large models, and we might imagine that\nthere's a risk of overfitting.",
    "start": "1711540",
    "end": "1719960"
  },
  {
    "text": "And in practice,\nyeah, it's actually one of the more crucial things\nto do to make training work.",
    "start": "1719960",
    "end": "1726231"
  },
  {
    "text": "So that turns out that you need\nto have a lot, a lot, a lot of data, like a lot of data. And in fact, we'll\nshow results later on,",
    "start": "1726232",
    "end": "1733669"
  },
  {
    "text": "where people built\na pretrained model. Pretrained it on a lot of data,\nand then like six months later,",
    "start": "1733670",
    "end": "1739810"
  },
  {
    "text": "someone else came along\nand was like, hey, if you pretrained on\nit 10 months later, and changed almost nothing else,\nit would have gone even better.",
    "start": "1739810",
    "end": "1746120"
  },
  {
    "text": "Now was it overfitting? I mean, you can sort of hold out\nsome text during pretraining,",
    "start": "1746120",
    "end": "1752370"
  },
  {
    "text": "right? And sort of evaluate\nthe perplexity, right? The language modeling\nperformance on that held out text,\nand it tends to be",
    "start": "1752370",
    "end": "1759140"
  },
  {
    "text": "the case that actually these\nmodels are underfitting, right? That we need even\nlarger and larger models",
    "start": "1759140",
    "end": "1764660"
  },
  {
    "text": "to express the complex\ninteractions that allow us to fit these\ndata sets better,",
    "start": "1764660",
    "end": "1770600"
  },
  {
    "text": "and so we'll talk about that\nwhen we talk about BERT. And one of the really\ninteresting results is that BERT is underfit, not\noverfit, but in principle,",
    "start": "1770600",
    "end": "1778220"
  },
  {
    "text": "yes, it's a problem to-- It's potentially a\nproblem to overfit, but we end up having a ton\nof text in English at least,",
    "start": "1778220",
    "end": "1785060"
  },
  {
    "text": "although not in every language. And so, yeah, it's\nimportant to scale them, but currently our models\ndon't seem overfit",
    "start": "1785060",
    "end": "1791716"
  },
  {
    "text": "to their pretraining text. ",
    "start": "1791717",
    "end": "1798760"
  },
  {
    "text": "OK. Any other questions?  All right.",
    "start": "1798760",
    "end": "1805520"
  },
  {
    "start": "1805000",
    "end": "1998000"
  },
  {
    "text": "So we saw this figure\nbefore, right here. We saw this figure of a\ntransformer encoder-decoder,",
    "start": "1805520",
    "end": "1810830"
  },
  {
    "text": "from this paper Attention\nIs All You Need. And so we have a\ncouple of things.",
    "start": "1810830",
    "end": "1817430"
  },
  {
    "text": "We're not going to go over\nthe form of attention again today, because we\nhave a lot to go over, but I'm happy to chat\nabout it more on Ed.",
    "start": "1817430",
    "end": "1825380"
  },
  {
    "text": "But so in our encoder, we\nhave some input sequence. Remember this is a\nsequence of subwords now.",
    "start": "1825380",
    "end": "1831580"
  },
  {
    "text": "If subword gets a\nword embeddding, and each index in\nthe transformer gets a position\nembedding, now, remember",
    "start": "1831580",
    "end": "1838750"
  },
  {
    "text": "that we have a finite length\nthat our sequence can possibly be, like 512 tokens.",
    "start": "1838750",
    "end": "1844840"
  },
  {
    "text": "That was that capital\nT from last lecture. So you have some\nfinite length, so you have one embedding of a position\nfor every index for all 512",
    "start": "1844840",
    "end": "1853929"
  },
  {
    "text": "indices, and then you have\nall your word embeddings. And then the transformer\nencoder, right,",
    "start": "1853930",
    "end": "1859630"
  },
  {
    "text": "was this combination\nof sort of submodules that we walked through\nline by line on Tuesday.",
    "start": "1859630",
    "end": "1868210"
  },
  {
    "text": "Multi-headed attention was sort\nof the core building block. And then, we had\nresidual and LayerNorm,",
    "start": "1868210",
    "end": "1874540"
  },
  {
    "text": "right, to help with\npassing gradients, and to help make training\ngo better and faster. We had that feed-forward\nlayer to process",
    "start": "1874540",
    "end": "1884200"
  },
  {
    "text": "sort of the result of the\nmulti-headed attention, another residual and\nLayerNorm, and then pass to an identical\ntransformer encoder block here.",
    "start": "1884200",
    "end": "1891370"
  },
  {
    "text": "And these will be stacked. We'll see a number of\ndifferent configurations here, but I think 6, 12, of these\nsort of stack together.",
    "start": "1891370",
    "end": "1899740"
  },
  {
    "text": "OK, so that's a\ntransformer encoder. And we're actually going to\nsee whole models today that",
    "start": "1899740",
    "end": "1905680"
  },
  {
    "text": "are just transformer encoders. OK. So when we talked about\nmachine translation, when we talked about\nthe transformer",
    "start": "1905680",
    "end": "1912090"
  },
  {
    "text": "itself, the transformer\nencoder-decoder, we talked about\nthis whole thing. But you could actually\njust have this left column,",
    "start": "1912090",
    "end": "1918059"
  },
  {
    "text": "and you could actually just\nhave this right column as well. Although the right\ncolumn changes a little",
    "start": "1918060",
    "end": "1923460"
  },
  {
    "text": "bit if you just have it. So remember the\nright column, we had this masked multi-head\nself-attention, right,",
    "start": "1923460",
    "end": "1930160"
  },
  {
    "text": "so where you can't\nlook at the future. And someone asked\nactually about how",
    "start": "1930160",
    "end": "1935760"
  },
  {
    "text": "we decode from\ntransformers, given that you have this sort\nof big chunking operation. It's a great\nquestion, I won't be",
    "start": "1935760",
    "end": "1941250"
  },
  {
    "text": "able to get into\nit in detail today, but you have to run it once\nduring the decoding process.",
    "start": "1941250",
    "end": "1946980"
  },
  {
    "text": "For every time that\nyou decode to sort of predict the next word. I'll write out something\non Ed for this.",
    "start": "1946980",
    "end": "1954000"
  },
  {
    "text": "So in the masked\nmulti-head self-attention, you're not allowed to\nlook at the future, so that you sort of have\nthis well defined objective",
    "start": "1954000",
    "end": "1961679"
  },
  {
    "text": "of trying to do\nlanguage modeling, then we have residual\nand LayerNorm, the multi-head\ncross-attention remember",
    "start": "1961680",
    "end": "1967590"
  },
  {
    "text": "goes back to the last layer\nof the transformer encoder, or the last transformer\nencoder block.",
    "start": "1967590",
    "end": "1973350"
  },
  {
    "text": "And then, more\nresidual LayerNorm, another feed-foward layer,\nmore residual LayerNorm. Now, if we don't have\nan encoder here, right?",
    "start": "1973350",
    "end": "1981120"
  },
  {
    "text": "Then we get rid of the\ncross-attention and residual and LayerNorm here, right? So if we didn't have\nthis stack of encoders,",
    "start": "1981120",
    "end": "1987497"
  },
  {
    "text": "the decoders get\nsimpler, because you don't have to attend to them. But then again, you also\nhave these word embeddings",
    "start": "1987498",
    "end": "1992670"
  },
  {
    "text": "at the bottom and\nposition representations for the output sequence. OK.",
    "start": "1992670",
    "end": "1998190"
  },
  {
    "start": "1998000",
    "end": "2148000"
  },
  {
    "text": "So that's been review. Let's talk about pretraining\nthrough language modeling. So we've actually talked maybe\na little bit about this before,",
    "start": "1998190",
    "end": "2005690"
  },
  {
    "text": "and we've seen language\nmodeling in the context of maybe just wanting to do it apriori.",
    "start": "2005690",
    "end": "2010850"
  },
  {
    "text": "So language models were\nuseful, for example, in automatic speech\nrecognition systems. They were useful in statistical\nmachine translation systems.",
    "start": "2010850",
    "end": "2018170"
  },
  {
    "text": "So let's recall the\nlanguage modeling task. You can say it's\ndefined as modeling",
    "start": "2018170",
    "end": "2024380"
  },
  {
    "text": "the probability of a\nword at a given index T, of any word at\nany given index. Given all the words before it.",
    "start": "2024380",
    "end": "2031700"
  },
  {
    "text": "And this probability\ndistribution-- this is a distribution of words\ngiven their past contexts. ",
    "start": "2031700",
    "end": "2039490"
  },
  {
    "text": "And so this is just saying,\nfor any prefix here, Iroh goes to make--",
    "start": "2039490",
    "end": "2045059"
  },
  {
    "text": "I want a probability of whatever\nthe next word should be. So the observed\nnext word is tasty, but maybe this goes to\nmake tea, goes to make",
    "start": "2045060",
    "end": "2053940"
  },
  {
    "text": "hot water, et cetera. You can have a distribution\nof what the next word should be in this decoder,\nand remember that,",
    "start": "2053940",
    "end": "2060599"
  },
  {
    "text": "because of the masked\nself-attention, \"make\" can look back to\nthe word \"to,\" or \"goes,\"",
    "start": "2060600",
    "end": "2066179"
  },
  {
    "text": "or \"Iroh,\" but it can't\nlook forward to \"tasty.\" So there's a lot\nof data for this.",
    "start": "2066179",
    "end": "2071790"
  },
  {
    "text": "Right, you just\nhave text and voila! You have language modeling data. It's free.",
    "start": "2071790",
    "end": "2078149"
  },
  {
    "text": "Once you have the text,\nit's freely available. You don't need to label it. And in English you\nhave a lot of it.",
    "start": "2078150",
    "end": "2083699"
  },
  {
    "text": "Right, this is not true of\nevery language by any means, but in English you have a\nlot of pretraining data.",
    "start": "2083699",
    "end": "2091870"
  },
  {
    "text": "And so, the simple thing\nabout sort of pretraining is, well what we're\ngoing to do, is we're",
    "start": "2091870",
    "end": "2098070"
  },
  {
    "text": "going to train a neural\nnetwork to do language modeling on a large\namount of text, and we'll just\nsave the parameters",
    "start": "2098070",
    "end": "2104130"
  },
  {
    "text": "of our trained network to disk. So conceptually,\nit's not actually different from the things\nthat we've done before.",
    "start": "2104130",
    "end": "2110490"
  },
  {
    "text": "It's just sort of the intent. Right? We're training these\nparameters to start using them for something\nelse later down the line.",
    "start": "2110490",
    "end": "2116940"
  },
  {
    "text": "But the language modeling\nitself doesn't change, the decoder here\ndoesn't change, right?",
    "start": "2116940",
    "end": "2122170"
  },
  {
    "text": "It's a transformer in\npretrained models in a modern, because this is sort of\na newly popular concept.",
    "start": "2122170",
    "end": "2130230"
  },
  {
    "text": "Although back in 2015, was\nsort of when this, I think, was first effectively\ntried out and got",
    "start": "2130230",
    "end": "2136320"
  },
  {
    "text": "some interesting results. But this could be anything here.",
    "start": "2136320",
    "end": "2142297"
  },
  {
    "text": "Today it's mostly going to\nbe transformers in the models that we actually observe",
    "start": "2142297",
    "end": "2147509"
  },
  {
    "text": "OK. So once you have your\npretrained network, what's the sort of\nthe default thing you do to take to use it, right?",
    "start": "2147510",
    "end": "2154619"
  },
  {
    "start": "2148000",
    "end": "2234000"
  },
  {
    "text": "And if you take anything away\nfrom this lecture in terms of just like\nengineering practices that will be broadly\nuseful to you as you go off",
    "start": "2154620",
    "end": "2161720"
  },
  {
    "text": "and build things and\nstudy things maybe as a machine learning engineer,\nor a computational social",
    "start": "2161720",
    "end": "2169250"
  },
  {
    "text": "scientist or et cetera,\nwhat people tend to do, is you pretrain your network\non just a lot of data.",
    "start": "2169250",
    "end": "2175520"
  },
  {
    "text": "Lots of text, learn\nvery general things. And then, you adapt the network\nto whatever you wanted to do.",
    "start": "2175520",
    "end": "2182720"
  },
  {
    "text": "So we had a bunch\nof pretraining data, and then maybe this\nis a movie review that we're taking as\ninput here, and we just",
    "start": "2182720",
    "end": "2189020"
  },
  {
    "text": "apply the decoder that we\nsort of pretrained right. Start the parameters\nthere, and then",
    "start": "2189020",
    "end": "2197150"
  },
  {
    "text": "fine tune it on whatever we\nwere sort of wanting to do. Maybe this is a\nsentiment analysis task,",
    "start": "2197150",
    "end": "2203760"
  },
  {
    "text": "so we run the whole sequence\nthrough the decoder, right? Get a hidden state at the\nend of the very last thing,",
    "start": "2203760",
    "end": "2209660"
  },
  {
    "text": "and then we predict maybe\nplus or minus sentiment. And this is sort of adapting the\npretrained network to the task.",
    "start": "2209660",
    "end": "2215900"
  },
  {
    "text": "So this pretrain, fine tune\nparadigm is wildly successful",
    "start": "2215900",
    "end": "2221359"
  },
  {
    "text": "and you should really try it\nwhenever you're doing any NLP task nowadays effectively.",
    "start": "2221360",
    "end": "2228829"
  },
  {
    "text": "Because this tends to be what-- some variant of this tends\nto be what works best.",
    "start": "2228830",
    "end": "2234820"
  },
  {
    "start": "2234000",
    "end": "2389000"
  },
  {
    "text": "OK, so we've got a\ntechnical note now.  So if you don't like to think\nabout optimization, or gradient",
    "start": "2234820",
    "end": "2244600"
  },
  {
    "text": "descent, maybe take\na pass on this slide, but I encourage you to just\nthink for a second about,",
    "start": "2244600",
    "end": "2250299"
  },
  {
    "text": "like why should this help? Training neural nets, we're\nusing gradient descent",
    "start": "2250300",
    "end": "2257349"
  },
  {
    "text": "to try to find some\nglobal minimum, right, of this optimism-- of\nthis loss function,",
    "start": "2257350",
    "end": "2263230"
  },
  {
    "text": "and we're sort of doing\nthis in two steps. And the first step is we\nhave-- we get some parameters,",
    "start": "2263230",
    "end": "2270610"
  },
  {
    "text": "theta hat, by approximating\nmin over our-- sorry theta is the parameters\nof the neural network.",
    "start": "2270610",
    "end": "2277890"
  },
  {
    "text": "So all of the KQV vectors\nin our transformer, right, the word embeddings,\nthe position embeddings,",
    "start": "2277890",
    "end": "2283119"
  },
  {
    "text": "it's just all of the parameters\nof our neural network. And so, we're doing min over\nall the parameters of our theta",
    "start": "2283120",
    "end": "2289660"
  },
  {
    "text": "we're trying to approximate,\nmin over the parameters of our neural network of\nour pretraining loss, which here was language modeling\nof our parameters.",
    "start": "2289660",
    "end": "2297760"
  },
  {
    "text": "And we just get this\nsort of estimate of some parameters theta hat.",
    "start": "2297760",
    "end": "2303790"
  },
  {
    "text": "And then we fine\ntune by approximating this, min over theta of the\nfine tune loss, maybe that's",
    "start": "2303790",
    "end": "2311380"
  },
  {
    "text": "sentiment, right? Starting at theta hat. So we initialize our gradient\ndescent at theta hat. And then we just sort of\nlike let it do what it wants.",
    "start": "2311380",
    "end": "2318460"
  },
  {
    "text": "And it's just like,\nit just works. In part it has to\nbe, because something",
    "start": "2318460",
    "end": "2326440"
  },
  {
    "text": "about where we start\nis so important, not just in terms of sort of\ngradient flow, although that",
    "start": "2326440",
    "end": "2332110"
  },
  {
    "text": "is a big part of it, but also it\nseems like stochastic gradient",
    "start": "2332110",
    "end": "2337120"
  },
  {
    "text": "descent sticks relatively\nclose to that pretraining initialization\nduring fine tuning.",
    "start": "2337120",
    "end": "2343880"
  },
  {
    "text": "This is something that we seem\nto observe in practice, right, that somehow the locality of\nstochastic gradient descent,",
    "start": "2343880",
    "end": "2351099"
  },
  {
    "text": "finding local minima that are\nclose to this theta hat that was good for such a general\nproblem of language modeling.",
    "start": "2351100",
    "end": "2358090"
  },
  {
    "text": "It seems like, yeah, the local\nminima of the fine tuning loss, because we don't find-- oh, yeah. The local minima of\nthe fine tuning loss",
    "start": "2358090",
    "end": "2364557"
  },
  {
    "text": "tend to generalize well, when\nthey're near to this theta hat that we pretrained. And this is sort of a mystery\nthat we're still trying",
    "start": "2364557",
    "end": "2370918"
  },
  {
    "text": "to figure out more about. And then also, yeah,\nmaybe the gradients. Right, the gradients of the\nfine tuning loss near theta",
    "start": "2370918",
    "end": "2376582"
  },
  {
    "text": "propagate nicely so our\nnetwork training goes really well as well. OK. So this is something to chew\non, but in practice it works.",
    "start": "2376582",
    "end": "2385780"
  },
  {
    "text": "I think it's just still\nfascinating that it works. OK. So we've talked about mainly\nthe transformer encoder-decoder.",
    "start": "2385780",
    "end": "2396460"
  },
  {
    "start": "2389000",
    "end": "2444000"
  },
  {
    "text": "And in fact, right,\nI said that we could have just sort\nof the left hand",
    "start": "2396460",
    "end": "2401530"
  },
  {
    "text": "side encoders to be\npretrained, or just decoders to be pretrained,\nor encoder-decoders.",
    "start": "2401530",
    "end": "2408309"
  },
  {
    "text": "And there are actually\nreally popular sort of famous models in each\nof these three categories.",
    "start": "2408310",
    "end": "2413450"
  },
  {
    "text": "The kinds of\npretraining you can do, and the kinds of applications or\nuses of those pretrained models",
    "start": "2413450",
    "end": "2421840"
  },
  {
    "text": "that are most natural,\nactually depend strongly on whether you choose to\npretrain an encoder, a decoder,",
    "start": "2421840",
    "end": "2428380"
  },
  {
    "text": "or an encoder-decoder. And so, I think\nit's useful as we go through some of these\npopular sort of model names",
    "start": "2428380",
    "end": "2436830"
  },
  {
    "text": "that you need to know,\nand what the sort of, what their innovations were\nto actually split it up into these categories.",
    "start": "2436830",
    "end": "2444260"
  },
  {
    "start": "2444000",
    "end": "2559000"
  },
  {
    "text": "So we've already--\nso here's the thing. We're going to go\nthrough these three, and they all have sort of\nbenefits, and in some sense",
    "start": "2444260",
    "end": "2451420"
  },
  {
    "text": "drawbacks. So for the decoders, right? Really what we're talking about\nhere mainly is language models.",
    "start": "2451420",
    "end": "2458710"
  },
  {
    "text": "And we've seen\nthis so far, we've talked about\npretrained decoders, and these are nice\nto generate from.",
    "start": "2458710",
    "end": "2464420"
  },
  {
    "text": "Right? So you can just sample from\nyour pretrained language model, and get things that\nlook like the text",
    "start": "2464420",
    "end": "2469540"
  },
  {
    "text": "that you were pretraining on. But one problem\nis that you can't condition on future words.",
    "start": "2469540",
    "end": "2475030"
  },
  {
    "text": "Right? So we mentioned in our\nmodeling with LSTMs, that just like if you could--",
    "start": "2475030",
    "end": "2480970"
  },
  {
    "text": "when you can do it, we said that\nhaving a bidirectional LSTM, was actually just way more\nuseful than having a one",
    "start": "2480970",
    "end": "2488350"
  },
  {
    "text": "directional LSTM. Well, it's sort of true\nfor transformers as well. So if you can see how the\narrows are pointing here,",
    "start": "2488350",
    "end": "2495910"
  },
  {
    "text": "the arrows are pointing\nup and to the right. So this word is sort of looking\nback at its past history,",
    "start": "2495910",
    "end": "2502570"
  },
  {
    "text": "but this word can't see, can't\ncontextualize with the future.",
    "start": "2502570",
    "end": "2508700"
  },
  {
    "text": "Whereas in the encoder block\nhere in blue just below it, you sort of have all\npairs of interactions.",
    "start": "2508700",
    "end": "2514030"
  },
  {
    "text": "And so, when you're\nbuilding representations, that can actually be\nsuper useful to know what the future words are. So so that's what\nencoders get you, right?",
    "start": "2514030",
    "end": "2520690"
  },
  {
    "text": "You get bidirectional context. So you can condition\non the future, maybe that helps you build up better\nrepresentations of language,",
    "start": "2520690",
    "end": "2526748"
  },
  {
    "text": "but the question that will\nactually go through here is, well, how do\nyou pretrain them?",
    "start": "2526748",
    "end": "2531918"
  },
  {
    "text": "You can't pretrain them\nas language models, because you have\naccess to the future. So if you try to do that, the\nloss will just immediately be",
    "start": "2531918",
    "end": "2538930"
  },
  {
    "text": "0, because you can just\nsee what the future is, that's not useful. And then, we'll talk about\npretrained encoder-decoders,",
    "start": "2538930",
    "end": "2546069"
  },
  {
    "text": "which like may be the\nbest of both worlds. But also maybe unclear what's\nthe best way to pretrain them,",
    "start": "2546070",
    "end": "2553180"
  },
  {
    "text": "they definitely have\nbenefits for both. So let's get into\nsort of some general--",
    "start": "2553180",
    "end": "2561069"
  },
  {
    "start": "2559000",
    "end": "2856000"
  },
  {
    "text": "a more-- yeah, let's\nget into decoders first, we'll go through all three. OK.",
    "start": "2561070",
    "end": "2566380"
  },
  {
    "text": "So when we're pretraining\na language model,",
    "start": "2566380",
    "end": "2572105"
  },
  {
    "text": "right, we're pretraining\nit on this objective, we're trying to make it\napproximate this probability",
    "start": "2572105",
    "end": "2577220"
  },
  {
    "text": "of a word given all\nof its previous words. What we end up\ndoing, and I showed",
    "start": "2577220",
    "end": "2582692"
  },
  {
    "text": "this sort of pictographically,\nbut that's some math, right? We get a hidden state,\nh1 to hT for each",
    "start": "2582692",
    "end": "2589400"
  },
  {
    "text": "of the words in\nthe input w1 to wT, and remember words again\nmean subwords here. OK.",
    "start": "2589400",
    "end": "2594680"
  },
  {
    "text": "And then, when we're\nfine tuning this, right, we can take\nthe representation-- this should be hT\nAht plus b, and then",
    "start": "2594680",
    "end": "2602990"
  },
  {
    "text": "the picture here is right here. So hT, it's the very\nlast encoder state.",
    "start": "2602990",
    "end": "2609500"
  },
  {
    "text": "And now, this has sort of the-- it's seen all of\nits history, right, and so, you can apply a linear\nlayer here maybe multiplying it",
    "start": "2609500",
    "end": "2619190"
  },
  {
    "text": "by some parameters A and B,\nthat were not pretrained, and then you're predicting\nsentiment maybe.",
    "start": "2619190",
    "end": "2625190"
  },
  {
    "text": "Plus or minus\nsentiment, perhaps. And so, look at the\nred and the gray, so most of the parameters\nof my neural network",
    "start": "2625190",
    "end": "2631490"
  },
  {
    "text": "have now been pretrained,\nthe very last layer that's learning the sentiment\nsay decision has not",
    "start": "2631490",
    "end": "2639583"
  },
  {
    "text": "been pretrained. So those have been\nrandomly initialized. And when you take the loss\nof the sentiment loss, right, you train not just\nthe linear layer here,",
    "start": "2639583",
    "end": "2647869"
  },
  {
    "text": "but you actually back\npropagate the gradients all the way through the\nentire pretrained network,",
    "start": "2647870",
    "end": "2653030"
  },
  {
    "text": "and fine tune all\nof those parameters. Right? So it's not like you're\njust training this,",
    "start": "2653030",
    "end": "2658222"
  },
  {
    "text": "at fine tuning time,\nthis linear layer. You're training\nthe whole network as a function of this\nfine tuning loss.",
    "start": "2658222",
    "end": "2665580"
  },
  {
    "text": "And maybe it's bad that the\nlinear layer wasn't pretrained, in the grand scheme of things,\nit's not that many parameters",
    "start": "2665580",
    "end": "2672960"
  },
  {
    "text": "also. So this is useful,\nthis is just one way to interact with\npretrained models, right?",
    "start": "2672960",
    "end": "2678569"
  },
  {
    "text": "And so, what I want you\nto take away from this is that, there was\na contract that we had with the original model.",
    "start": "2678570",
    "end": "2683980"
  },
  {
    "text": "Right? The contract was\nthat it was defining probability distributions. But when we're fine\ntuning, when we're",
    "start": "2683980",
    "end": "2689820"
  },
  {
    "text": "interacting with the pretrained\nmodel, what we also have are just like the trained\nweights, and the network architecture.",
    "start": "2689820",
    "end": "2695370"
  },
  {
    "text": "We don't need to use\nit as a language model, we don't need to use it as\na probability distribution. When we're actually fine tuning\nit, we're really just using it",
    "start": "2695370",
    "end": "2702510"
  },
  {
    "text": "for its initialization of\nits parameters, and saying, oh, this is just a\ntransformer decoder",
    "start": "2702510",
    "end": "2708420"
  },
  {
    "text": "that was pretrained by-- and it happens to\nbe really great, in that when you fine tune\non some sentiment data",
    "start": "2708420",
    "end": "2715290"
  },
  {
    "text": "it does a really good job. OK, but there's a second way\nto interact with decoder--",
    "start": "2715290",
    "end": "2720540"
  },
  {
    "text": "with pretrained decoders. Which is in some sense\neven more natural, it actually is closer to the\ncontract that we started with.",
    "start": "2720540",
    "end": "2728050"
  },
  {
    "text": "So we don't have to just\nignore the fact that it was a probability\ndistribution entirely, we can make use of it\nwhile still fine tuning it.",
    "start": "2728050",
    "end": "2735400"
  },
  {
    "text": "So here's what\nwe're going to do. So we can use them as a\ngenerator at fine tuning time. By generator, I mean, it's going\nto define this distribution",
    "start": "2735400",
    "end": "2744240"
  },
  {
    "text": "of words given their contexts. And then we'll\nactually just fine tune",
    "start": "2744240",
    "end": "2749400"
  },
  {
    "text": "that probability distribution. So in a task like some kind\nof turn-based dialogue,",
    "start": "2749400",
    "end": "2756210"
  },
  {
    "text": "we might encode the dialogue\nhistory as your past context.",
    "start": "2756210",
    "end": "2761550"
  },
  {
    "text": "Right, so you have a dialogue\nhistory of some things that people are saying back\nand forth to each other,",
    "start": "2761550",
    "end": "2767280"
  },
  {
    "text": "you encode it as\nwords, and you try to predict the next\nwords in the dialogue. Right, and maybe your\npretraining objective,",
    "start": "2767280",
    "end": "2773730"
  },
  {
    "text": "you looked at very general\npurpose text from I don't know, Wikipedia, or\nbooks, or something. And you're fine tuning\nit as a language model,",
    "start": "2773730",
    "end": "2780750"
  },
  {
    "text": "but you're fine tuning\nas a language model on this sort of domain-specific\ndistribution of text,",
    "start": "2780750",
    "end": "2786870"
  },
  {
    "text": "like dialogue, or maybe\nsummarization, where you paste in the whole\ndocument and then say",
    "start": "2786870",
    "end": "2793140"
  },
  {
    "text": "a specific word, and\nthen the summary, and say predict the summary. And so what this\nlooks like is, again,",
    "start": "2793140",
    "end": "2799750"
  },
  {
    "text": "at fine tuning time here. You have your h1 to hT is equal\nto the decoder of the words.",
    "start": "2799750",
    "end": "2805860"
  },
  {
    "text": "And then, you have\nthis distribution that you're fine tuning, of wT\nis Ah, there's a typo again,",
    "start": "2805860",
    "end": "2813240"
  },
  {
    "text": "ht minus 1 plus b. So now, every time I have this--",
    "start": "2813240",
    "end": "2818789"
  },
  {
    "text": "I'm predicting these\nwords-- from word 1 I predict word 2, word 2 I\npredict word 3, et cetera, right?",
    "start": "2818790",
    "end": "2824040"
  },
  {
    "text": "The actual last\nlayer of the network unlike before, the last\nlayer of the network has been pretrained.",
    "start": "2824040",
    "end": "2830070"
  },
  {
    "text": "But I'm still fine\ntuning the whole thing. Right, so A and B\nhere are mapping to sort of a\nprobability distribution",
    "start": "2830070",
    "end": "2836549"
  },
  {
    "text": "over my vocabulary, or the\nlogits of a probability distribution. And I just get to sort\nof like tweak them now.",
    "start": "2836550",
    "end": "2843484"
  },
  {
    "text": "In order to have\nthe distribution that I'm going to use, reflect\nthe thing like dialogue that I want it to reflect.",
    "start": "2843485",
    "end": "2849420"
  },
  {
    "text": " OK. So those are two\nways of interacting",
    "start": "2849420",
    "end": "2854430"
  },
  {
    "text": "with a pretrained decoder. Now, here's an example\nof what is ended up",
    "start": "2854430",
    "end": "2859560"
  },
  {
    "text": "being the first of\nyour line of wildly successful, or at least talked\nabout, pretrained decoders.",
    "start": "2859560",
    "end": "2869010"
  },
  {
    "text": "So the generative\npretrained decoder, GPT,",
    "start": "2869010",
    "end": "2874530"
  },
  {
    "text": "was a huge success\nin some sense, or at least it\ngot a lot of buzz.",
    "start": "2874530",
    "end": "2880410"
  },
  {
    "text": "So it's a transformer decoder,\nno encoder, with 12 layers. I'm giving you\nthe details so you",
    "start": "2880410",
    "end": "2885900"
  },
  {
    "text": "can start to get a feeling\nfor how the size of things changes over the years as we'll\ncontinue to progress here.",
    "start": "2885900",
    "end": "2893460"
  },
  {
    "text": "Had each of-- each of the hidden\nstates with dimensionality 768. So if you remember\nback to last lecture,",
    "start": "2893460",
    "end": "2899770"
  },
  {
    "text": "we had a term D, which was our\ndimensionality, so D is 768. And then, an\ninteresting statement",
    "start": "2899770",
    "end": "2906930"
  },
  {
    "text": "that you should keep in mind\nfor the engineering-minded folks is that, the actual\nfeed-forward layers, right. You've got a hidden\nlayer in the feed--",
    "start": "2906930",
    "end": "2913320"
  },
  {
    "text": "excuse me, in the\nfeed-forward layer, and this is actually very large. So you had these\nsort of position wise feed-forward layers, right?",
    "start": "2913320",
    "end": "2919980"
  },
  {
    "text": "And the feed-forward layer\nwould take the 768 dimensional vector, sort of like project\nit to 3,000 dimensional space",
    "start": "2919980",
    "end": "2928620"
  },
  {
    "text": "through the sort\nof non-linearity, and then project it back to 768. This ends up being\nbecause you can squash",
    "start": "2928620",
    "end": "2934980"
  },
  {
    "text": "a lot more parameters\nin, for not too much more compute in this way\nbut-- so that's curious.",
    "start": "2934980",
    "end": "2941160"
  },
  {
    "text": "OK, and then,\nbyte-pair encoding. It's actually-- was this\none byte-pair encoding?",
    "start": "2941160",
    "end": "2946200"
  },
  {
    "text": "Was a subword vocabulary\nwith 40,000 merges. So 40,000 merges, so that's\nnot the size of the vocabulary,",
    "start": "2946200",
    "end": "2951780"
  },
  {
    "text": "because you started with\na bunch of characters, and I don't remember how many\ncharacters they started with.",
    "start": "2951780",
    "end": "2957330"
  },
  {
    "text": "But so it's a relatively small\nvocabulary you can see, right? And compared to if\nyou tried to say,",
    "start": "2957330",
    "end": "2963810"
  },
  {
    "text": "have every word have a\nunique representation. Now, it's going to be\ntrained on BooksCorpus. It's got 7,000 unique\nbooks, and it contains",
    "start": "2963810",
    "end": "2972600"
  },
  {
    "text": "long spans of contiguous text. So you have, instead\nof say training it on individual sentences,\njust small short sentences,",
    "start": "2972600",
    "end": "2980730"
  },
  {
    "text": "the model is able to learn\nlong distance dependencies, because you haven't split like\na book into random sentences,",
    "start": "2980730",
    "end": "2987180"
  },
  {
    "text": "and shuffled them all around. You've sort of\nkept it contiguous. So it can have that\nsort of consistency.",
    "start": "2987180",
    "end": "2993780"
  },
  {
    "text": "And then, a little\ntreat here, yeah. So GPT never showed up\nin the original paper, or in the original blog\npost like as an acronym.",
    "start": "2993780",
    "end": "3000872"
  },
  {
    "text": "And it could actually\nsort of refer to like, Generative\nPreTraining, just sort of what the title of\nthe paper would suggest,",
    "start": "3000872",
    "end": "3007010"
  },
  {
    "text": "or Generative\nPreTrained Transformer. And I sort of decided to\nsay, Generative PreTrained Transformer because this\nseemed like way too general.",
    "start": "3007010",
    "end": "3015170"
  },
  {
    "text": "So, GPT. OK. So they pretrained this huge\nlanguage model transformer,",
    "start": "3015170",
    "end": "3022010"
  },
  {
    "start": "3017000",
    "end": "3223000"
  },
  {
    "text": "this huge transformer\ndecoder, just on 7,000 books. And they fine tuned it on a\nnumber of different tasks.",
    "start": "3022010",
    "end": "3027860"
  },
  {
    "text": "And I want to talk a little\nbit about the details about how they fine tuned it. And so, they fine tuned it on\none particular task, or family",
    "start": "3027860",
    "end": "3035420"
  },
  {
    "text": "of tasks, called Natural\nLanguage Inference. So in Natural\nLanguage Inference, we're labeling\npairs of sentences",
    "start": "3035420",
    "end": "3041810"
  },
  {
    "text": "as entailing, or contradictory\nto each other or neutral. So you have a premise,\nand you hold the premise",
    "start": "3041810",
    "end": "3046920"
  },
  {
    "text": "that's sort of true. \"The man is in the doorway,\"\nand you have a hypothesis, \"the person is near the door.\"",
    "start": "3046920",
    "end": "3053810"
  },
  {
    "text": "If this person is\nreferring to that man, then it's sort of\nlike, Oh, yeah.",
    "start": "3053810",
    "end": "3059450"
  },
  {
    "text": "So this is sort of entailed,\nbecause there's a person, because a man is a person. And they're in the doorway,\nthen they are near the door.",
    "start": "3059450",
    "end": "3066410"
  },
  {
    "text": "So you have this sort of logical\nreasoning that you're doing, or you're supposed to\nbe able to be doing,",
    "start": "3066410",
    "end": "3072320"
  },
  {
    "text": "and you're labeling\nthese sentences. So it's a labeled \"task.\" You've got sort of an input\nthat's cut into two parts,",
    "start": "3072320",
    "end": "3078920"
  },
  {
    "text": "and then one of three outputs. OK. So the GPT paper\nevaluates on this task.",
    "start": "3078920",
    "end": "3085140"
  },
  {
    "text": "But what they've got is\na transformer decoder. So what do they do?",
    "start": "3085140",
    "end": "3090440"
  },
  {
    "text": "This is sort of one of the\nearlier examples of taking--",
    "start": "3090440",
    "end": "3096020"
  },
  {
    "text": "instead of changing your\nneural network architecture to adapt to the kind\nof task you're doing,",
    "start": "3096020",
    "end": "3102500"
  },
  {
    "text": "you're going to just format the\ntask as like a bunch of tokens and not change\nyour architecture.",
    "start": "3102500",
    "end": "3109580"
  },
  {
    "text": "Because the pretraining\nwas so useful. It's probably better to\nkeep the architecture fixed,",
    "start": "3109580",
    "end": "3114769"
  },
  {
    "text": "pretrain it, and\nthen change the task specification to sort of fit\nthe pretrained architecture.",
    "start": "3114770",
    "end": "3120320"
  },
  {
    "text": "So what they did right,\nthey put this token \"START,\" this is a special token. \"The man is in the doorway.\"",
    "start": "3120320",
    "end": "3126560"
  },
  {
    "text": "Some delimiter token, right? So this is just a linear\nsequence of tokens that we're giving as\none big prefix to GPT.",
    "start": "3126560",
    "end": "3134930"
  },
  {
    "text": "And then, \"the person\nis near the door,\" and then some extra token here. Like \"EXTRACT.\"",
    "start": "3134930",
    "end": "3141380"
  },
  {
    "text": "And then, the linear\nclassifier that we talked about in sort of the first way\nto interact with models,",
    "start": "3141380",
    "end": "3149030"
  },
  {
    "text": "with decoder models, it's\napplied to the represent of the EXTRACT token, right?",
    "start": "3149030",
    "end": "3154430"
  },
  {
    "text": "So you have the last hidden\nstate on top of extract, and then you fine\ntune the whole network",
    "start": "3154430",
    "end": "3159799"
  },
  {
    "text": "to predict these labels. And so this sort\nof input formatting is increasingly used to keep\nthe model architecture the same,",
    "start": "3159800",
    "end": "3169400"
  },
  {
    "text": "and allow for a variety\nof different problems to be solved with it. OK, and so, did it work?",
    "start": "3169400",
    "end": "3175309"
  },
  {
    "text": "On Natural Language\nInference, the answer is yes. So there's a number of\ndifferent numbers here, I wouldn't worry\ntoo much about it.",
    "start": "3175310",
    "end": "3181700"
  },
  {
    "text": "The fine tuned\ntransformer language model is sort of what you\nshould pay attention to.",
    "start": "3181700",
    "end": "3186839"
  },
  {
    "text": "There's a lot of effort that\nwent into the other models, right? And sort of, this is the\nstory of pretraining. People put a lot of\neffort into models",
    "start": "3186840",
    "end": "3193130"
  },
  {
    "text": "that do various sort\nof careful things, and then you take a\nsingle transformer and you say, I'm going to\npretrain it on a ton of text,",
    "start": "3193130",
    "end": "3200840"
  },
  {
    "text": "and not worry too much\nabout anything else, and just fine tune it, and you\nend up doing super, super well.",
    "start": "3200840",
    "end": "3207810"
  },
  {
    "text": "Sometimes not too much\nbetter in the GPT case than sort of the best known\nstate of the art methods,",
    "start": "3207810",
    "end": "3214260"
  },
  {
    "text": "but usually a little bit better. And again, the amount\nof effort, the amount of task-specific effort that\nyou have to put into it,",
    "start": "3214260",
    "end": "3220410"
  },
  {
    "text": "is very low. OK. And so, what about the other way\nof interacting with decoders?",
    "start": "3220410",
    "end": "3225850"
  },
  {
    "text": "Right, so we said that we\ncould interact with the code just by sampling from\nthem, just by saying, well, they're probability\ndistributions.",
    "start": "3225850",
    "end": "3231980"
  },
  {
    "text": "So we can use them in their\ncapacities as language models. And so, GPT-2, this is just,\nreally just a bigger GPT.",
    "start": "3231980",
    "end": "3240369"
  },
  {
    "text": "Don't worry too much about it. With larger hidden\nunits, more layers. When it was trained\non more data,",
    "start": "3240370",
    "end": "3246670"
  },
  {
    "text": "it was shown to produce sort of\nrelatively convincing samples of natural language.",
    "start": "3246670",
    "end": "3251960"
  },
  {
    "text": "So this is something that\nwent around Twitter a lot. Right, so you have this\nsort of contrived example",
    "start": "3251960",
    "end": "3257560"
  },
  {
    "text": "that probably didn't show\nup in the training data that has scientists discovering\na herd of unicorns,",
    "start": "3257560",
    "end": "3264609"
  },
  {
    "text": "and then they sort of sample\nfrom almost the distribution",
    "start": "3264610",
    "end": "3271028"
  },
  {
    "text": "of the model. They sort of give that model\nsome extra credit here.",
    "start": "3271028",
    "end": "3276870"
  },
  {
    "text": "They do something\ncalled, truncating the distribution of\nthe language models, to sort of cut out\nnoise in GPT-2.",
    "start": "3276870",
    "end": "3284500"
  },
  {
    "text": "So it's not exactly\na perfect sample, but more or less\nGPT-2 generated this.",
    "start": "3284500",
    "end": "3291820"
  },
  {
    "text": "And so, you have the scientists\ndiscovering unicorns. And then, you have this\nconsistency, OK, there's",
    "start": "3291820",
    "end": "3297730"
  },
  {
    "text": "the scientist, you have\nthem giving it a name.",
    "start": "3297730",
    "end": "3303340"
  },
  {
    "text": "You have-- you refer\nback to this well-- yeah, you refer back to\nthe scientist's name,",
    "start": "3303340",
    "end": "3311050"
  },
  {
    "text": "you sort of have these like\ntopic consistency things, also the syntax is really good.",
    "start": "3311050",
    "end": "3316550"
  },
  {
    "text": "It looks vaguely like\nEnglish, and so this is sort of continuing to be a trend. As we get larger and\nlarger language models,",
    "start": "3316550",
    "end": "3321970"
  },
  {
    "text": "we actually sample\nfrom them, even when we give them prompts\nthat look sort of odd, and they seem to be\nincreasingly convincing.",
    "start": "3321970",
    "end": "3329740"
  },
  {
    "text": "OK. So pretraining encoders.",
    "start": "3329740",
    "end": "3335530"
  },
  {
    "text": "OK, pretraining encoders. Let's take another second\nbecause I need some more water here.",
    "start": "3335530",
    "end": "3341552"
  },
  {
    "text": "If there's another\nquestion, let me know. ",
    "start": "3341552",
    "end": "3350030"
  },
  {
    "text": "All right.  So the benefit of encoders\nthat we talked about",
    "start": "3350030",
    "end": "3356890"
  },
  {
    "text": "was that, they get this\nbidirectional context. So you can-- while you're\nbuilding representations of your sentence, of\nyour parts of sentences,",
    "start": "3356890",
    "end": "3364900"
  },
  {
    "text": "you can look to the\nfuture, and that can help you build a better\nrepresentation of the word that you're looking\nat right now.",
    "start": "3364900",
    "end": "3370240"
  },
  {
    "text": "But the big problem is that we\ncan't do language modeling now. So we've pretty\nmuch only said we like we've relied on these\ntasks that we already",
    "start": "3370240",
    "end": "3376990"
  },
  {
    "text": "knew about language modeling\nto do our pretraining. But now we want to\npretrain encoders, and so, we can't use it.",
    "start": "3376990",
    "end": "3383305"
  },
  {
    "text": "So what are we going to do?  Here's the solution\nthat was come up",
    "start": "3383305",
    "end": "3389020"
  },
  {
    "start": "3386000",
    "end": "3599000"
  },
  {
    "text": "with in a paper that introduced\nthe language model of the model called BERT.",
    "start": "3389020",
    "end": "3394810"
  },
  {
    "text": "It's called masked\nlanguage modeling. So here's the idea. We get the sentence\nand then we just",
    "start": "3394810",
    "end": "3401260"
  },
  {
    "text": "take a fraction of the\nwords, and we replace them with a sort of a masked token. A token that's-- that means you\ndon't know what this is right",
    "start": "3401260",
    "end": "3410020"
  },
  {
    "text": "now. And then you\npredict these words. Some details we'll get\ninto in the next slide. But so here's what\nit looks like, right?",
    "start": "3410020",
    "end": "3416589"
  },
  {
    "text": "We have the sentence,\nI mask to the mask. We get some hidden states\nfor all of them, right?",
    "start": "3416590",
    "end": "3423140"
  },
  {
    "text": "So we haven't trained-- we\nhaven't changed the transformer encoder at all.",
    "start": "3423140",
    "end": "3428380"
  },
  {
    "text": "We've just said, OK, here\nis like this sequence. You get to see\neverything, right? Look at all the arrows\ngoing everywhere.",
    "start": "3428380",
    "end": "3433790"
  },
  {
    "text": "But then, we have\nthis prediction layer, that we were pretraining, right?",
    "start": "3433790",
    "end": "3440170"
  },
  {
    "text": "And we're using it, we\nonly have loss on the words where we had masks here.",
    "start": "3440170",
    "end": "3446170"
  },
  {
    "text": "So I had this masked,\nand then I have to predict that\nit was \"went\" that went here, and \"store\"\nthat went here.",
    "start": "3446170",
    "end": "3452650"
  },
  {
    "text": "And now, this is a lot\nlike language modeling you might say. But now you don't need to have\nthis sort of left to right",
    "start": "3452650",
    "end": "3458230"
  },
  {
    "text": "decomposition,\nyou're saying, I'm going to remove\nsome of the words, and you have to\npredict what they are.",
    "start": "3458230",
    "end": "3464178"
  },
  {
    "text": "So this is called masked\nlanguage modeling. And it's been very, very, very\neffective with a quick caveat,",
    "start": "3464178",
    "end": "3469630"
  },
  {
    "text": "it gets a little\nmore complicated. So what did they actually do? They proposed masked\nlanguage modeling,",
    "start": "3469630",
    "end": "3476283"
  },
  {
    "text": "and they released the weights\nof this pretrained transformer, but with a little\nbit more complexity so we get masked language\nmodeling to work.",
    "start": "3476283",
    "end": "3483400"
  },
  {
    "text": "So you are going to\ntake a random 15% of the subword tokens.",
    "start": "3483400",
    "end": "3489100"
  },
  {
    "text": "So that was true. But you're not always going\nto replace them with mask.",
    "start": "3489100",
    "end": "3494260"
  },
  {
    "text": "You can think of it like, if\nthe model sees a mask token, it gets a guarantee that it\nneeds to predict something,",
    "start": "3494260",
    "end": "3501490"
  },
  {
    "text": "and if the model doesn't\nsee a mask token, it gets a guarantee\nthat it doesn't need to predict anything.",
    "start": "3501490",
    "end": "3506980"
  },
  {
    "text": "So why should it bother\nbuilding strong representations of the words that aren't masked?",
    "start": "3506980",
    "end": "3512820"
  },
  {
    "text": "And I want my model to\nbuild strong representations of everything. So we're going to add some sort\nof uncertainty to the model.",
    "start": "3512820",
    "end": "3518990"
  },
  {
    "text": "So what we're going to do\nis, for those 15% of tokens, 80% of the time, we're going\nto replace it with a mask.",
    "start": "3518990",
    "end": "3524670"
  },
  {
    "text": "That was our original idea\nof masked language modeling. Then 10% of the\ntime, we're actually",
    "start": "3524670",
    "end": "3530190"
  },
  {
    "text": "going to replace the word\nwith just a random token, just a random vocabulary\nitem, can be anything.",
    "start": "3530190",
    "end": "3535832"
  },
  {
    "text": "And then the other\n10% of the time, we're going to leave the\nword unchanged. So now, when it sees a word,\nit could be a random token,",
    "start": "3535832",
    "end": "3544950"
  },
  {
    "text": "or it could be unchanged. Then if I see a mask, I\nknow I need to predict it.",
    "start": "3544950",
    "end": "3550869"
  },
  {
    "text": "So what these two\nthings do here is say, you have to sort\nof be doing this-- you have to be on your\ntoes for every word",
    "start": "3550870",
    "end": "3557820"
  },
  {
    "text": "in your representation. So here, \"I pizza to the mask.\" Right? It turns out, and the\nmodel didn't know this,",
    "start": "3557820",
    "end": "3563895"
  },
  {
    "text": "but it's getting three loss\nterms for the sentence. It only has one\nmask, but it's going",
    "start": "3563895",
    "end": "3569640"
  },
  {
    "text": "to be penalized for predicting\nthree different things. It needs to predict that this\nword is actually, \"went,\" so I",
    "start": "3569640",
    "end": "3575789"
  },
  {
    "text": "replaced this one. It needs to predict that this\nword is \"to,\" is in fact, the word \"to,\"\nand then, it needs",
    "start": "3575790",
    "end": "3582330"
  },
  {
    "text": "to predict that this\nword is in fact \"store.\" Now as a short interlude,\nyou might be thinking,",
    "start": "3582330",
    "end": "3588390"
  },
  {
    "text": "maybe thinking, John, there's no\nway the model could know this, it's so under-specified.",
    "start": "3588390",
    "end": "3593470"
  },
  {
    "text": "OK. \"I pizza,\" is a\nlittle weird, I admit. But there's just no way to know\nthat there's a store, and went, and to, I mean, the same thing\nis true of language modeling,",
    "start": "3593470",
    "end": "3600690"
  },
  {
    "text": "right? So it's going to end up learning\nthese average statistics about what things tend\nto be in a given context,",
    "start": "3600690",
    "end": "3606599"
  },
  {
    "text": "and it's going to sort\nof hedge its bets, and try to build a distribution\nof what things could appear there.",
    "start": "3606600",
    "end": "3611934"
  },
  {
    "text": "So for the people who are\nthinking that if there was any, that's what you\nshould be thinking. It has to sort of know\nwhat kinds of things",
    "start": "3611935",
    "end": "3617550"
  },
  {
    "text": "will end up in these slots, it\nhas other uncertainty, right, because it can't be sure\nthat any of the other words",
    "start": "3617550",
    "end": "3622590"
  },
  {
    "text": "are necessarily right. And then, it's predicting\nthese three words.",
    "start": "3622590",
    "end": "3629690"
  },
  {
    "text": "OK. And so, that's-- you can see why\nit's important to not just have",
    "start": "3629690",
    "end": "3635060"
  },
  {
    "text": "masks potentially, to have these\nsort of token randomization things, because again we don't\nactually care about its ability",
    "start": "3635060",
    "end": "3641270"
  },
  {
    "text": "to predict the masks. Right? Like I'm not going\nto usually, I'm not going to actually sample\nfrom the models distribution",
    "start": "3641270",
    "end": "3648260"
  },
  {
    "text": "over what should go here. Instead, I am going\nto use the parameters",
    "start": "3648260",
    "end": "3654260"
  },
  {
    "text": "of the neural\nnetwork, and expect that it built strong\nrepresentations of language. So I don't want it\nto think it's got",
    "start": "3654260",
    "end": "3659810"
  },
  {
    "text": "a free pass for\nrepresenting something, if it doesn't have a mask there.",
    "start": "3659810",
    "end": "3665630"
  },
  {
    "text": "OK. So there was one extra thing\nwith the BERT pretraining,",
    "start": "3665630",
    "end": "3673050"
  },
  {
    "text": "which is a next sentence\nprediction objective. So the inputs of\nBERT looks like this. This is straight\nfrom the BERT paper.",
    "start": "3673050",
    "end": "3679130"
  },
  {
    "text": "You have a label here\nbefore your first sentence. And then a separation, and\nthen a second sentence.",
    "start": "3679130",
    "end": "3685440"
  },
  {
    "text": "So you had all these two\ncontiguous chunks of text. You had the first chunk of\ntext here, \"my dog is cute.\"",
    "start": "3685440",
    "end": "3693260"
  },
  {
    "text": "And then a second chunk of\ntext, \"he likes playing.\" Right? And you see the subwords there. And now, these would actually\nbe both be much longer, right?",
    "start": "3693260",
    "end": "3702350"
  },
  {
    "text": "So this whole thing\nwould be 512 words, this would be about half,\nand that would be about half,",
    "start": "3702350",
    "end": "3707940"
  },
  {
    "text": "and they'd be contiguous\nchunks of text. But here is the deal. What they wanted to\ndo was they wanted",
    "start": "3707940",
    "end": "3715220"
  },
  {
    "text": "to try to teach the\nsystem to understand sort of relationships between\ndifferent whole pieces of text.",
    "start": "3715220",
    "end": "3721580"
  },
  {
    "text": "In order to better\npretrain for objectives-- for downstream applications,\nlike question answering,",
    "start": "3721580",
    "end": "3726710"
  },
  {
    "text": "where you have two pretty\ndifferent pieces of text, and you need to know how\nthey relate to each other.",
    "start": "3726710",
    "end": "3732330"
  },
  {
    "text": "So the objective they came up\nwith was, you should sometimes, have the second chunk of text.",
    "start": "3732330",
    "end": "3738620"
  },
  {
    "text": "The actual chunk of text\nthat directly follows the first in your data\nset, and sometimes",
    "start": "3738620",
    "end": "3747020"
  },
  {
    "text": "have the second check of\ntext be randomly sampled from somewhere else. So unrelated.",
    "start": "3747020",
    "end": "3752780"
  },
  {
    "text": "And the model should\npredict whether it's the first case or the second\nin order, again, to sort of,",
    "start": "3752780",
    "end": "3758480"
  },
  {
    "text": "have to reason about the\nrelationships between the two chunks of text. So this is next\nsentence prediction.",
    "start": "3758480",
    "end": "3764192"
  },
  {
    "text": "I think it's important to think\nabout, because again, it's a very different idea\nof pretraining objective",
    "start": "3764193",
    "end": "3769520"
  },
  {
    "text": "than language modeling and\nmasked language modeling. Even though, later\nwork sort of argued",
    "start": "3769520",
    "end": "3775640"
  },
  {
    "text": "that in the case of BERT,\nit's not necessary or useful. And one of the\narguments is actually",
    "start": "3775640",
    "end": "3781190"
  },
  {
    "text": "because it's actually way better\nto have a single context that's",
    "start": "3781190",
    "end": "3787010"
  },
  {
    "text": "twice as long. So you can learn even\nlonger distance dependencies and things.",
    "start": "3787010",
    "end": "3792030"
  },
  {
    "text": "And so, whether the\nobjective itself would be useful if you could\nalways just double the context size, I'm not sure if anyone's\ndone research on that.",
    "start": "3792030",
    "end": "3798625"
  },
  {
    "text": "But again, it's like a\ndifferent kind of objective. And it's still noising something\nabout the input, right? The input was this\nbig chunk of text.",
    "start": "3798625",
    "end": "3806480"
  },
  {
    "text": "And you've noised it\nto say like, now you don't know whether\nit really was that, or whether you sort of replaced\nit with a bunch of garbage.",
    "start": "3806480",
    "end": "3812839"
  },
  {
    "text": "This sort of second\nportion here. Whether the second portion has\nbeen replaced with something,",
    "start": "3812840",
    "end": "3818720"
  },
  {
    "text": "yeah. Has been replaced with\nsomething that didn't actually come from the same sequence. ",
    "start": "3818720",
    "end": "3826120"
  },
  {
    "text": "OK. So let's talk some\ndetails about BERT. So BERT had 12 or\n24 layers depending",
    "start": "3826120",
    "end": "3831960"
  },
  {
    "text": "on BERT-base or BERT-large. You'll probably use\none of these models or one of the sort of\ndescendants of these models,",
    "start": "3831960",
    "end": "3837013"
  },
  {
    "text": "if you choose to do something\nwith the custom final project potentially, or if you choose\nthe version of the default",
    "start": "3837013",
    "end": "3844380"
  },
  {
    "text": "final project. And you had 600 or 1,000\ndimension hidden states,",
    "start": "3844380",
    "end": "3850259"
  },
  {
    "text": "a bunch of attention heads. So this is that multi-headed\nattention you remember, had a bunch of them. So you're splitting all your\ndimensions into those 16 heads,",
    "start": "3850260",
    "end": "3858270"
  },
  {
    "text": "and we're talking on the\norder of a couple 100 million parameters. At the time, right in\n2018, we were like, whoa!",
    "start": "3858270",
    "end": "3865980"
  },
  {
    "text": "That's a lot of parameters. How do you-- that's\na lot of parameters.",
    "start": "3865980",
    "end": "3871710"
  },
  {
    "text": "And now, models are way,\nway, way, way bigger. So let's keep track of\nsort of the model sizes",
    "start": "3871710",
    "end": "3877460"
  },
  {
    "text": "as we're going through this. And let's come back now to\nthe corpus sizes as well. So we have BooksCorpus, this\nis the number of words there.",
    "start": "3877460",
    "end": "3884784"
  },
  {
    "text": "This is the same\nthing that GPT-2-- GPT-1 was trained on,\n800 million words.",
    "start": "3884784",
    "end": "3890060"
  },
  {
    "text": "Now we're going to train\non also English Wikipedia. It's 250, sorry\nthat's 2,500 million.",
    "start": "3890060",
    "end": "3896750"
  },
  {
    "text": "So that's 2.5 billion words. And again, to give\nyou an idea of what",
    "start": "3896750",
    "end": "3903230"
  },
  {
    "text": "is done in practice, right? Pretraining is expensive and\nimpractical for most users,",
    "start": "3903230",
    "end": "3909650"
  },
  {
    "text": "let's say. So if you are a researcher\nwith the GPU, or five GPUs,",
    "start": "3909650",
    "end": "3914810"
  },
  {
    "text": "or something like that. You tend to not really be\npretraining your whole own BERT model unless you're willing\nto spend a long time doing it.",
    "start": "3914810",
    "end": "3922250"
  },
  {
    "text": "BERT itself was pretrained\nwith 64 TPU chips. A TPU is a special kind\nof hardware accelerator",
    "start": "3922250",
    "end": "3928579"
  },
  {
    "text": "that accelerates the tensor\noperations effectively, it was developed by Google.",
    "start": "3928580",
    "end": "3934910"
  },
  {
    "text": "So TPUs are just fast,\nand can hold a lot. And for four days,\nthey had 64 chips.",
    "start": "3934910",
    "end": "3941900"
  },
  {
    "text": "So if you have\none GPU, which you can think of as less\nthan a single TPU, you're going to be waiting\na long time to pretrain.",
    "start": "3941900",
    "end": "3948260"
  },
  {
    "text": "But fine tuning actually. Fine tuning is so fast. It's so fast and practical,\nit's common on a single GPU.",
    "start": "3948260",
    "end": "3955640"
  },
  {
    "text": "You'll see how much faster\nfine tuning is than pretraining in assignment 5. And so, this becomes, I think\nlike a refrain of the field.",
    "start": "3955640",
    "end": "3964400"
  },
  {
    "text": "You pretrain once or\na handful of times. Like a couple of people\nrelease big pretrained models,",
    "start": "3964400",
    "end": "3970192"
  },
  {
    "text": "and then you fine tune\nmany times, right? So you you save those parameters\nfrom pretraining, you fine tune on all kinds of\ndifferent problems.",
    "start": "3970192",
    "end": "3976910"
  },
  {
    "text": " And that paradigm, right?",
    "start": "3976910",
    "end": "3982000"
  },
  {
    "text": "Taking something like\nBERT, or whatever the best descendant of BERT\nis, and taking it pretrained",
    "start": "3982000",
    "end": "3988270"
  },
  {
    "text": "and then fine tuning it and what\nyou want, is pretty close to--",
    "start": "3988270",
    "end": "3993310"
  },
  {
    "text": "it's a very, very strong\nbaseline in NLP right now. Right? So and the simplicity\nis pretty fascinating,",
    "start": "3993310",
    "end": "4000660"
  },
  {
    "text": "and there's one code\nbase called Transformers from a company called\nHugging Face, that",
    "start": "4000660",
    "end": "4006510"
  },
  {
    "text": "makes this just really, just\na couple of lines of Python to try out as well.",
    "start": "4006510",
    "end": "4011770"
  },
  {
    "text": "So it's sort of opened\nup very strong baselines without too, too much\neffort for a lot of tasks",
    "start": "4011770",
    "end": "4019000"
  },
  {
    "text": "OK. So let's talk about evaluation. So pretraining is\npitched as requiring all this different kind\nof language understanding.",
    "start": "4019000",
    "end": "4026160"
  },
  {
    "text": "And the field is-- the field of NLP has a\nhard time doing evaluation.",
    "start": "4026160",
    "end": "4031720"
  },
  {
    "text": "But we try our best,\nand we build datasets that we think are hard\nfor various reasons, because they require\nyou to know stuff",
    "start": "4031720",
    "end": "4036893"
  },
  {
    "text": "about language, and about the\nworld, and about reasoning. And so, when we evaluate\nwhether retraining",
    "start": "4036893",
    "end": "4042060"
  },
  {
    "text": "is getting you a lot of\nsort of general knowledge, we evaluate on a\nlot of these tasks.",
    "start": "4042060",
    "end": "4050710"
  },
  {
    "text": "So we evaluate on things\nlike-- paraphrase detection, on Quora Questions, natural\nlanguage inference we saw,",
    "start": "4050710",
    "end": "4059010"
  },
  {
    "text": "we have hard sentiment\nanalysis datasets, or what were hard sentiment analysis\ndatasets a couple of years ago.",
    "start": "4059010",
    "end": "4065280"
  },
  {
    "text": "Actually figuring out if\nsentences are grammatical tends to be hard. Determining the\ntextual similarity,",
    "start": "4065280",
    "end": "4071400"
  },
  {
    "text": "the semantic similarity\nof text can be hard. Paraphrasing again,\nnatural language inference on a very, very small data set.",
    "start": "4071400",
    "end": "4077820"
  },
  {
    "text": "So this is this--\nthis pretraining help you train on smaller\ndata sets, the answer is yes, sort of thing.",
    "start": "4077820",
    "end": "4083170"
  },
  {
    "text": "And so, right, so\nthe BERT folks, released their paper\nafter GPT was released,",
    "start": "4083170",
    "end": "4089130"
  },
  {
    "text": "and there were a lot of sort\nof state of the art results that came from various\nthings that you were supposed to be doing.",
    "start": "4089130",
    "end": "4095670"
  },
  {
    "text": "And the results that you get\nsort of with pretraining.",
    "start": "4095670",
    "end": "4100890"
  },
  {
    "text": "So here's OpenAI's GPT,\nhere is BERT-base and large. The last three rows\nare all pretrained. ELMo is sort of in the middle.",
    "start": "4100890",
    "end": "4108422"
  },
  {
    "text": "It's sort of in the\nmiddle between pretraining the whole model, and just\nhaving word embeddings, that's what this is.",
    "start": "4108423",
    "end": "4114000"
  },
  {
    "text": "And the numbers\nyou get are just, I think to the field were\nquite astounding actually. We were all surprised\nthat there was that much",
    "start": "4114000",
    "end": "4120839"
  },
  {
    "text": "left to even be gotten on\nsome of these datasets. And taking here, so\nthis line in the table",
    "start": "4120840",
    "end": "4127271"
  },
  {
    "text": "is unmarked, which is actually\nthe number of training examples. This dataset has 2.5\nthousand training examples.",
    "start": "4127271",
    "end": "4133589"
  },
  {
    "text": "And before sort of the big\ntransformers came around we had 60% accuracy on it.",
    "start": "4133590",
    "end": "4139739"
  },
  {
    "text": "We run transformers on\nit, we get 10 points just by pretraining. And this has been a trend\nthat is just continued.",
    "start": "4139740",
    "end": "4147028"
  },
  {
    "text": "So why do anything,\nbut pretrain encoders? But like we know\nencoders are good,",
    "start": "4147029",
    "end": "4152460"
  },
  {
    "text": "and we like the fact that you\nhave bidirectional context. We also saw that BERT\ndid better than GPT.",
    "start": "4152460",
    "end": "4158580"
  },
  {
    "text": "But it has-- if you\nwant to actually get it to do things, right?",
    "start": "4158580",
    "end": "4164380"
  },
  {
    "text": "You can't just generate it-- generate sequences\nfrom it the same way that you would from a model\nlike GPT, a pretrained decoder,",
    "start": "4164380",
    "end": "4171224"
  },
  {
    "text": "right? You can sort of sample what\nthings should go in a mask. So here's a mask, you\ncan put a mask somewhere,",
    "start": "4171224",
    "end": "4177210"
  },
  {
    "text": "sample the words\nthat should go there. But if you want to\nsample the whole context, right, if you want to get\nthat story about the unicorns,",
    "start": "4177210",
    "end": "4183442"
  },
  {
    "text": "for example, the encoder\nis not what you want to do. So they have sort of\ndifferent contracts in there.",
    "start": "4183442",
    "end": "4189710"
  },
  {
    "text": "It can be used naturally\nat least in different ways. ",
    "start": "4189710",
    "end": "4194900"
  },
  {
    "text": "OK. So let's talk very briefly\nabout extensions of BERT. So there are variants\nlike RoBERTa and SpanBERT,",
    "start": "4194900",
    "end": "4200770"
  },
  {
    "text": "and there's just\na bunch of papers with the word BERT in the\ntitle, that did various things. Two very strong takeaways.",
    "start": "4200770",
    "end": "4206450"
  },
  {
    "text": "RoBERTa, train BERT\nlonger, BERT is under fit. Train it on more data,\ntrain it four more steps.",
    "start": "4206450",
    "end": "4212710"
  },
  {
    "text": "SpanBERT, mask contiguous\nspans of subwords--",
    "start": "4212710",
    "end": "4218590"
  },
  {
    "text": "of words makes a harder,\nmore useful pretraining task. So this is the idea\nthat we can come up with better ways of noising\nthe input, of hiding stuff",
    "start": "4218590",
    "end": "4225970"
  },
  {
    "text": "in the input, or breaking stuff\nin the input for our model to correct. So, for example, if\nyou have the sentence,",
    "start": "4225970",
    "end": "4233019"
  },
  {
    "text": "\"Mask irr esi mask good.\" It's just not that\nhard to know that this",
    "start": "4233020",
    "end": "4239349"
  },
  {
    "text": "is \"irresistibly,\" right? Because like what\ncould this possibly be after these subwords?",
    "start": "4239350",
    "end": "4244630"
  },
  {
    "text": "So this is \"irresisti--\"\nsomething's got to come here,",
    "start": "4244630",
    "end": "4249670"
  },
  {
    "text": "and it's probably\nthe end of that word. Whereas if you mask a\nlong sequence of things.",
    "start": "4249670",
    "end": "4255039"
  },
  {
    "text": "Right, now this is much\nharder, and actually you're getting a useful signal\nthat it's irresistibly good.",
    "start": "4255040",
    "end": "4260590"
  },
  {
    "text": "And you sort of need\nto mask all of them to make the task interesting. So SpanBERT was like\noh, we should do this.",
    "start": "4260590",
    "end": "4267790"
  },
  {
    "text": "So this was super\nuseful as well. So RoBERTa-- just to\npoint you at the fact that RoBERTa showed\nthat BERT was under fit,",
    "start": "4267790",
    "end": "4276100"
  },
  {
    "text": "said BERT was trained on\nabout 13 gigabytes of text, it got some accuracies.",
    "start": "4276100",
    "end": "4281290"
  },
  {
    "text": "You can get above\nthe amazing results of BERT, four extra\npoints or so here,",
    "start": "4281290",
    "end": "4286780"
  },
  {
    "text": "right, just by taking\nthe identical model and training it on more\ndata, with a larger batch",
    "start": "4286780",
    "end": "4292870"
  },
  {
    "text": "size for a long time. And if you train it,\nyeah, on even longer",
    "start": "4292870",
    "end": "4298930"
  },
  {
    "text": "without sort of more data,\nyou don't get any better. ",
    "start": "4298930",
    "end": "4304980"
  },
  {
    "text": "Very briefly OK, so-- very briefly on the\nencoder-decoders. So we've seen,\ndecoders can be good",
    "start": "4304980",
    "end": "4311510"
  },
  {
    "text": "because we get to play with the\ncontracts that they gave us, we get to play with\nthem as language models, encoders give us that\nbidirectional context.",
    "start": "4311510",
    "end": "4318389"
  },
  {
    "text": "So encoder-decoders\nmaybe we get both. In practice they're actually,\nyeah, pretty, pretty strong.",
    "start": "4318390",
    "end": "4324720"
  },
  {
    "text": "So there was a-- right, we could-- so but I\nguess one of the questions",
    "start": "4324720",
    "end": "4330560"
  },
  {
    "text": "is like, what do we\ndo to pre-train them? So we could do something\nlike language modeling, right, where we take a sequence\nof words, word 1 to word 2T,",
    "start": "4330560",
    "end": "4342790"
  },
  {
    "text": "instead of T. Right,\nand instead of word 1 here, dot, dot, dot,\nword T, we provide those",
    "start": "4342790",
    "end": "4348850"
  },
  {
    "text": "all through our encoder, and\nwe predict on none of them. And then we have, word\nT plus 1 to word 2T,",
    "start": "4348850",
    "end": "4354430"
  },
  {
    "text": "here in our decoder, right? And we predict on these. So we're doing language\nmodeling on half the sequence,",
    "start": "4354430",
    "end": "4360790"
  },
  {
    "text": "and we're taking the\nother half to have our bidirectional\nencoder, right? So we're building strong\nrepresentations on the encoder",
    "start": "4360790",
    "end": "4367570"
  },
  {
    "text": "side, not predicting language\nmodeling on any of this, and then we on the other\nhalf of the tokens,",
    "start": "4367570",
    "end": "4373810"
  },
  {
    "text": "we predict as a\nlanguage model would do. And the hope is that you sort\nof pretrained both of these well",
    "start": "4373810",
    "end": "4380110"
  },
  {
    "text": "through the one language\nmodeling loss up here. And this is actually-- so\nthis works pretty well.",
    "start": "4380110",
    "end": "4386380"
  },
  {
    "text": "The encoder benefits from\nbidirectionality, the decoder, you can use to train the model. But what-- this paper showed\nthat introduced the Model T5",
    "start": "4386380",
    "end": "4397179"
  },
  {
    "text": "Raffel et al., found to work\nbest was actually a very-- well, at least a somewhat\ndifferent objective.",
    "start": "4397180",
    "end": "4402728"
  },
  {
    "text": "And this should keep in\nyour mind sort of that we have different ways of\nspecifying the pretraining objectives, and they will\nreally work differently",
    "start": "4402728",
    "end": "4409540"
  },
  {
    "text": "from each other. So what they said,\nlet's say you have an original text like this. Thank you for inviting me\nto your party last week.",
    "start": "4409540",
    "end": "4417670"
  },
  {
    "text": "We're going to define variable\nlength spans in the text, to replace with a\nunique symbol that",
    "start": "4417670",
    "end": "4424060"
  },
  {
    "text": "says something is missing here. And then we'll replace,\nand then we'll remove that. So now our input\nto our encoder is,",
    "start": "4424060",
    "end": "4430900"
  },
  {
    "text": "Thank you, symbol 1, me to your\nparty, symbol 2, week, right?",
    "start": "4430900",
    "end": "4437659"
  },
  {
    "text": "So we've noised the input,\nwe've hidden stuff in the input. Also really\ninterestingly, right, this doesn't say how long\nthis is supposed to be.",
    "start": "4437660",
    "end": "4445180"
  },
  {
    "text": "That's different from BERT. Right? BERT said, \"Oh, you masked\nthis many subwords.\"",
    "start": "4445180",
    "end": "4450550"
  },
  {
    "text": "This says, well I got\nsome token that says, Something's missing here. And I don't know what\nit is, I don't even know how many subwords it is.",
    "start": "4450550",
    "end": "4457480"
  },
  {
    "text": "And then, so you have\nthis in your encoder, and then your decoder predicts\nthe first special word,",
    "start": "4457480",
    "end": "4465159"
  },
  {
    "text": "this X here, and then what\nwas missing, for inviting. So, \"Thank you X,\nX for inviting,\"",
    "start": "4465160",
    "end": "4473010"
  },
  {
    "text": "and then it predicts\nY, here's this Y here. And then what was missing\nfrom the Y, \"last.\"",
    "start": "4473010",
    "end": "4478400"
  },
  {
    "text": "\"Last week.\" This is called span\ncorruption, and it's really interesting to\nme because, in terms",
    "start": "4478400",
    "end": "4485080"
  },
  {
    "text": "of the actual\nencoder-decoder, we don't have to change it\ncompared to whether we-- if we were just doing\nlanguage modeling pretraining.",
    "start": "4485080",
    "end": "4491230"
  },
  {
    "text": "Because I just do\nlanguage modeling on all of these things. I just predict these words\nas if I'm a language model.",
    "start": "4491230",
    "end": "4497440"
  },
  {
    "text": "I've just done a text\npre-processing step. Right? So the actual--",
    "start": "4497440",
    "end": "4503560"
  },
  {
    "text": "I've just pre-processed the\ntext to look like, oh, yeah. Take the input, make\nit look like this. Then make an output that\nlooks like that up there.",
    "start": "4503560",
    "end": "4510639"
  },
  {
    "text": "And the model gets to do what is\neffectively language modeling, but it actually works better.",
    "start": "4510640",
    "end": "4516050"
  },
  {
    "text": "So these are a lot of\nnumbers I realize, but take a look at the star here. This encoder-decoder with\na denoising objective",
    "start": "4516050",
    "end": "4523210"
  },
  {
    "text": "that tends to work the best. And they tried similar models\nlike a prefix language model,",
    "start": "4523210",
    "end": "4529270"
  },
  {
    "text": "that was sort of the\nfirst try that we had at defining a pretraining\nobjective for language models,",
    "start": "4529270",
    "end": "4535000"
  },
  {
    "text": "sorry, for encoder-decoders. And then they had another--\na number of other options,",
    "start": "4535000",
    "end": "4540820"
  },
  {
    "text": "but what worked best for\nthe encoder-decoders. And one of the fascinating\nthings about T5 is that you could pretrain it\nand fine tune it on questions.",
    "start": "4540820",
    "end": "4549040"
  },
  {
    "text": "Like, When was Franklin\nD, Roosevelt born? And fine tune it to\nproduce the answer.",
    "start": "4549040",
    "end": "4555660"
  },
  {
    "text": "And then, you could ask it\nnew questions at test time. And then it would retrieve\nthe answer from its parameters",
    "start": "4555660",
    "end": "4561239"
  },
  {
    "text": "with some accuracy. And it would do so\nrelatively well actually. And it would do so maybe 25% of\nthe time on some of these data",
    "start": "4561240",
    "end": "4568890"
  },
  {
    "text": "sets with 220\nmillion parameters. Then at 11 billion\nparameters, this is way bigger than BERT-large,\nit would do so even better,",
    "start": "4568890",
    "end": "4576810"
  },
  {
    "text": "sometimes even doing\nas well as systems that were allowed to\nlook at stuff other than their own parameters.",
    "start": "4576810",
    "end": "4582570"
  },
  {
    "text": "So again, this is just\nmaking this answer come from its parameters.",
    "start": "4582570",
    "end": "4588300"
  },
  {
    "text": "Yeah, we have to skip this. So if you look back at\nthis slide after class,",
    "start": "4588300",
    "end": "4594090"
  },
  {
    "text": "I have each of the\nexamples of the things that we could imagine\nlearning from pretraining, with the label of what\nyou might be learning.",
    "start": "4594090",
    "end": "4600010"
  },
  {
    "text": "So this example is, Stanford\nUniversity is located in blank. You might learn trivia. In all these cases, there's\nall these things you can learn.",
    "start": "4600010",
    "end": "4606810"
  },
  {
    "text": "One thing I will say, is that\nmodels also learn and can make even worse--",
    "start": "4606810",
    "end": "4612030"
  },
  {
    "text": "racism, sexism, all\nmanner of bad biases that are encoded in our text. When I say-- yeah, they do this.",
    "start": "4612030",
    "end": "4619663"
  },
  {
    "text": "And so, we'll learn more about\nthis in our later lectures, but it's important to keep\nin mind that when you're doing pretraining, you're\nlearning a lot of stuff",
    "start": "4619663",
    "end": "4626400"
  },
  {
    "text": "and not all of it is good. So with GPT-3, the\nlast thing here",
    "start": "4626400",
    "end": "4632440"
  },
  {
    "text": "is that there's this\nthird way of interacting with models that's\nrelated to treating them",
    "start": "4632440",
    "end": "4638260"
  },
  {
    "text": "as language models. So GPT-3 is this\nvery, very large model",
    "start": "4638260",
    "end": "4643360"
  },
  {
    "text": "that was released\nby OpenAI, and it seems to be able to learn from\nexamples in their contexts--",
    "start": "4643360",
    "end": "4649599"
  },
  {
    "text": "their decoder contexts,\nwithout gradient steps. Simply by looking sort\nof within their history.",
    "start": "4649600",
    "end": "4656500"
  },
  {
    "text": "And now, GPT-3 has 175\nbillion parameters, right? The last T5 model we saw\nwas 11 billion parameters.",
    "start": "4656500",
    "end": "4663520"
  },
  {
    "text": "And it seems to be sort\nof the canonical example of this working. And so what it\nlooks like is, you",
    "start": "4663520",
    "end": "4669940"
  },
  {
    "text": "give it as part of its prefix-- \"thanks\" goes to \"merci,\"\n\"hello\" goes to--",
    "start": "4669940",
    "end": "4675190"
  },
  {
    "text": "\"mint\" goes to-- so you've got\nthese translation examples. You ask it for the last\none, and it comes up",
    "start": "4675190",
    "end": "4680350"
  },
  {
    "text": "with the correct translation. Seemingly because it's learned\nsomething about the task",
    "start": "4680350",
    "end": "4685389"
  },
  {
    "text": "that you're sort of telling\nit to do through its prefix. And so, you might do the\nsame thing with addition. So something plus something--",
    "start": "4685390",
    "end": "4691989"
  },
  {
    "text": "5 plus 8 is 13. Give it addition examples,\nit might do the next addition example for you.",
    "start": "4691990",
    "end": "4697989"
  },
  {
    "text": "Or maybe trying to figure out\ngrammatical or spelling errors, for example.",
    "start": "4697990",
    "end": "4703970"
  },
  {
    "text": "And here is the French\nto English-- the English to French case.",
    "start": "4703970",
    "end": "4709600"
  },
  {
    "text": "So again, you're learning\njust to do pretraining. But when you're evaluating\nit, you don't even",
    "start": "4709600",
    "end": "4715929"
  },
  {
    "text": "fine tune the model, you\njust provide prefixes. And so, this especially\nis not well understood",
    "start": "4715930",
    "end": "4723587"
  },
  {
    "text": "and there's a lot\nof research is going into sort of what\nthe limitations of this so-called\nin-context learning are.",
    "start": "4723587",
    "end": "4729880"
  },
  {
    "text": "But it's a fascinating\ndirection for future work. In total, these models\nare not well understood.",
    "start": "4729880",
    "end": "4736800"
  },
  {
    "text": "However, \"small,\" in air\nquotes, models like BERT have become general tools\nin a wide range of settings,",
    "start": "4736800",
    "end": "4742719"
  },
  {
    "text": "and they do have these issues\nabout learning all these biases about the world. That will go into and further\nlectures in this course,",
    "start": "4742720",
    "end": "4750430"
  },
  {
    "text": "and so, yeah. What you've learned this week,\ntransformers and pretraining form the basis, or at\nleast the baselines",
    "start": "4750430",
    "end": "4757270"
  },
  {
    "text": "for much of natural\nlanguage processing today. And assignment 5\nis out, and you'll be able to look more into it.",
    "start": "4757270",
    "end": "4764672"
  },
  {
    "text": "And I'm over time, all right. ",
    "start": "4764672",
    "end": "4771989"
  },
  {
    "text": "Yeah. I guess, I can take a\nquestion if there is any.",
    "start": "4771990",
    "end": "4777360"
  },
  {
    "text": "But people can\nget going as well. ",
    "start": "4777360",
    "end": "4785460"
  },
  {
    "text": "Well, I think there is\na question about T5. Which was, how does the decoder\nknow that it's currently",
    "start": "4785460",
    "end": "4791280"
  },
  {
    "text": "predicting X or Y? Could you repeat that? Yeah.",
    "start": "4791280",
    "end": "4796425"
  },
  {
    "text": "About T5, there\nwas a question that asking, how does the decoder\nknow that it's currently predicting X or Y?",
    "start": "4796425",
    "end": "4803940"
  },
  {
    "text": "Its hierarchy of\npredicting X or Y? ",
    "start": "4803940",
    "end": "4809250"
  },
  {
    "text": "I guess if you can\nspecify it sort of says, how does it know that it's\ncurrently predicting X or Y?",
    "start": "4809250",
    "end": "4814860"
  },
  {
    "text": "OK. Yeah. Yeah, that makes sense. So what it does, right so,\nit knows from the encoder,",
    "start": "4814860",
    "end": "4820800"
  },
  {
    "text": "that it has to at\nsome point predict X, and at some point predict Y.\nBecause the encoder can just",
    "start": "4820800",
    "end": "4826753"
  },
  {
    "text": "remember that, oh, yeah,\nthere's two things missing. And if there were\nmore spans replaced, there would be a Z, and then an\nA, and then a B, and whatever.",
    "start": "4826753",
    "end": "4834349"
  },
  {
    "text": "Just a bunch of\nunique identifiers. And then up here, it gets to\nsay, OK, I have attention,",
    "start": "4834350",
    "end": "4844330"
  },
  {
    "text": "I suppose. I can look and I know that\nfirst I have to predict this, first masked thing. So I'm going to generate\nthat in my decoder.",
    "start": "4844330",
    "end": "4851150"
  },
  {
    "text": "And then it gets\nthat symbol, right? So we're doing training by\ngiving it the right symbol. Now, it gets that X, and it\nsays OK, I'm predicting X now.",
    "start": "4851150",
    "end": "4859010"
  },
  {
    "text": "right? Now it can predict, predict,\npredict, predict, then it gets Y. So we're doing\nthis teacher-forcing training",
    "start": "4859010",
    "end": "4864280"
  },
  {
    "text": "where we give it the right\nanswer after penalizing it if it's wrong. Now it gets this Y,\nright, and says OK, now I",
    "start": "4864280",
    "end": "4870990"
  },
  {
    "text": "have to predict what should\ngo in Y, and it can attend into the natural parts of this,\nas well as what it's already",
    "start": "4870990",
    "end": "4876780"
  },
  {
    "text": "predicted here, because\nthe decoder has attention within itself, and it can\nsee what should go there.",
    "start": "4876780",
    "end": "4881935"
  },
  {
    "text": "So what's fascinating here,\nis you're doing something like language modeling, but\nwhen you're predicting Y, right, you get to see\nwhat came after it.",
    "start": "4881935",
    "end": "4889028"
  },
  {
    "text": "And that's, I think, one of the\nbenefits of span corruptions. You're doing this thing where\nyou don't know how long you should be predicting for,\nlike language modeling,",
    "start": "4889028",
    "end": "4896219"
  },
  {
    "text": "but you get to know what came\nafter the thing that's missing. ",
    "start": "4896220",
    "end": "4905238"
  }
]