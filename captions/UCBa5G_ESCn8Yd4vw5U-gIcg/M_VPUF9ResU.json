[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "Hi everyone, happy Monday. Um, we're going to go ahead and",
    "start": "4370",
    "end": "10769"
  },
  {
    "text": "start off today by reviewing the bake-off results from last week. Um, so just as a refresher,",
    "start": "10770",
    "end": "17699"
  },
  {
    "text": "last week's bake-off was on the task of sentiment analysis. Um, yeah.",
    "start": "17699",
    "end": "23099"
  },
  {
    "text": "So it was the three class task, positive, neutral and negative. Um, we're evaluating on the Stanford Sentiment Treebank Test Set, um,",
    "start": "23100",
    "end": "31545"
  },
  {
    "text": "that had a total of 2200 sentences, um, and we used the Macro F1 Score is the evaluation metric.",
    "start": "31545",
    "end": "38430"
  },
  {
    "text": "Um, yeah. So, uh, basically like taking the accuracy on every class, um, and then averaging all those. Um, yeah.",
    "start": "38430",
    "end": "46145"
  },
  {
    "text": "So we have a pretty in-balance test set, um, with like 900 of the positive and negative,",
    "start": "46145",
    "end": "51440"
  },
  {
    "text": "uh, examples, and then 400 neutral. Um, and in general, um, the worst performance is seen on the neutral class,",
    "start": "51440",
    "end": "57860"
  },
  {
    "text": "um, which is kind of expected, given this class imbalance. So yeah, um, here is a histogram of the scores that you guys submitted.",
    "start": "57860",
    "end": "67400"
  },
  {
    "start": "60000",
    "end": "125000"
  },
  {
    "text": "Um, so yeah, the highest score was almost 0.7,",
    "start": "67400",
    "end": "73295"
  },
  {
    "text": "um, and, uh, the mode was around, uh, 0.5 or 0.55.",
    "start": "73295",
    "end": "78570"
  },
  {
    "text": "Um, and you can kinda see like where you fall in comparison to our like just basic unigrams plus softmax baseline,",
    "start": "78570",
    "end": "86060"
  },
  {
    "text": "um, which is actually kind of like above this peak here. So that was pretty interesting, um, that, you know, basic unigrams can get you so far. Um, yeah.",
    "start": "86060",
    "end": "94360"
  },
  {
    "text": "And so, we have, uh, the winners of the bake-off, oh, oh,",
    "start": "94360",
    "end": "99645"
  },
  {
    "text": "before that, um, what's gonna distinguish the high scores and low scores. Um, so we've kind of looked at, uh,",
    "start": "99645",
    "end": "105634"
  },
  {
    "text": "the like, tokens that appeared, um, in the submissions, um, for the top scores.",
    "start": "105635",
    "end": "110909"
  },
  {
    "text": "Which, uh, we can define as like scores that are about above 0.58, um, and as you can see,",
    "start": "110910",
    "end": "117620"
  },
  {
    "text": "uh, it's like, you know, mostly, kind of a lot of deep learning approaches, and people who leverage BERT representations.",
    "start": "117620",
    "end": "125314"
  },
  {
    "start": "125000",
    "end": "157000"
  },
  {
    "text": "Um, yeah and then in the low scores, um, it's kinda of a mixed bag. I, I see some of the like, uh,",
    "start": "125315",
    "end": "132300"
  },
  {
    "text": "baseline of feature functions, um, that were in the assignment originally, um, as well as like some glove tokens.",
    "start": "132300",
    "end": "139010"
  },
  {
    "text": "Um, so it's interesting to see like how far BERT has surpassed, you know, like, uh,",
    "start": "139010",
    "end": "144579"
  },
  {
    "text": "approaches that are not even really outdated. But, um, like, you know, things that we were using just last year that were like winning the bake off,",
    "start": "144580",
    "end": "152045"
  },
  {
    "text": "are now, you know, in this low scores section, so that's really interesting. Um, yeah. Now onto,uh,",
    "start": "152045",
    "end": "158394"
  },
  {
    "start": "157000",
    "end": "215000"
  },
  {
    "text": "our first place groups. So congratulations to group 13. Um, achieving the score of 0.692.",
    "start": "158395",
    "end": "166505"
  },
  {
    "text": "Um, using their balanced data-set and end-to-end BERT approach. Um, yes so, uh, group 13, um,",
    "start": "166505",
    "end": "174575"
  },
  {
    "text": "used some smart, like, data process- preprocessing, um, first like balancing the dataset by oversampling.",
    "start": "174575",
    "end": "179820"
  },
  {
    "text": "Which was very smart, because of the dataset imbalance, um, that I mentioned before. Um, and then actually like rejoining,",
    "start": "179820",
    "end": "187875"
  },
  {
    "text": "um, the contractions in the dataset. Um, and this is kind of like an artifact of how, uh, BERT was trained.",
    "start": "187875",
    "end": "194180"
  },
  {
    "text": "Um, so it's interesting that like,you know, this, uh, like, you know, undoing the preprocessing essentially,",
    "start": "194180",
    "end": "200530"
  },
  {
    "text": "um, actually helped a little bit in terms of performance. Um, and then they trained, um, the BERT model end to end.",
    "start": "200530",
    "end": "207440"
  },
  {
    "text": "Uh, basically, like fine-tuning BERT for the sentiment classification task. Um, yeah so, uh,",
    "start": "207440",
    "end": "213840"
  },
  {
    "text": "good job to that team. Um, now for our second place team,",
    "start": "213840",
    "end": "218970"
  },
  {
    "start": "215000",
    "end": "257000"
  },
  {
    "text": "group 51, um, used a similar approach, um, of using BERT to create the, uh,",
    "start": "218970",
    "end": "226260"
  },
  {
    "text": "feature representations, and then, uh, using a neural net classifier on top of that. So congratulations to group 51 as well.",
    "start": "226260",
    "end": "233330"
  },
  {
    "text": "Um, they achieved a score of 0.651. Um, you know, doing a similar thing of fine tuning BERT on SST, um,",
    "start": "233330",
    "end": "241220"
  },
  {
    "text": "and then running inference to, uh, generate the features for each sentence, and then feeding that into our TorchShallowNeuralClassifier.",
    "start": "241220",
    "end": "249020"
  },
  {
    "text": "Um, and they also used, uh, the strategy of kind of like up-sampling on examples from the neutral class to do better on that class.",
    "start": "249020",
    "end": "256269"
  },
  {
    "text": "Um, and then we also wanted to point out, um, some other interesting approaches in addition to our top two teams.",
    "start": "256270",
    "end": "263520"
  },
  {
    "text": "Um, so we wanted to shout out group 9, which did super well, actually getting a- uh,",
    "start": "263520",
    "end": "269525"
  },
  {
    "text": "like the same score as ou- our top team, but unfortunately can't qualify to win the bake-off, um, because they actually use the sub-tree labels.",
    "start": "269525",
    "end": "276210"
  },
  {
    "text": "But it's still a very interesting approach, we wanted to point it out. Um, this group actually, uh,",
    "start": "276210",
    "end": "281599"
  },
  {
    "text": "formulated the task as a sequence to sequence task, um, using, like the strings containing the sentence annotations and tree, tree structure, um,",
    "start": "281600",
    "end": "290535"
  },
  {
    "text": "as the input sequence, and then the sentiment label, um, as an output sequence of length 1.",
    "start": "290535",
    "end": "296060"
  },
  {
    "text": "Um, so that was pretty cool. Um, basically the only group, um, to think of formulating the problem that way.",
    "start": "296060",
    "end": "301729"
  },
  {
    "text": "Um, and actually one of the only groups, uh, among the top teams to not use BERT or ELMo.",
    "start": "301730",
    "end": "307895"
  },
  {
    "text": "Um, and then just using a-, uh, two-layer bidirectional LSTM, um, for a classification.",
    "start": "307895",
    "end": "315010"
  },
  {
    "start": "315000",
    "end": "483000"
  },
  {
    "text": "Uh, and then we also wanted to shout out anyone who tried feature engineering.",
    "start": "315010",
    "end": "320930"
  },
  {
    "text": "Um, unfortunately, uh, feature engineering didn't make it into, uh, I mean, like the top scoring systems,",
    "start": "320930",
    "end": "327949"
  },
  {
    "text": "but that was interesting to us on the teaching team, because, uh, last year's top two systems actually both used hand-built,",
    "start": "327950",
    "end": "334445"
  },
  {
    "text": "like bag of words based features with a simple logistic- logistic regression classifier.",
    "start": "334445",
    "end": "339485"
  },
  {
    "text": "Although this year, um, a lot of the top teams relied on deep learning, um, kind of shows like a shift in the field. Um, yeah.",
    "start": "339485",
    "end": "347390"
  },
  {
    "text": "So these are the top two teams from last year, um, and these scores aren't comparable to this year,",
    "start": "347390",
    "end": "353180"
  },
  {
    "text": "because they refer to the binary task. While we were doing the ternary task this year. But yeah, our teaching team is well represented here [LAUGHTER] um,",
    "start": "353180",
    "end": "360695"
  },
  {
    "text": "but on first place, um, uh, last year, Jada's team used, um,",
    "start": "360695",
    "end": "367169"
  },
  {
    "text": "preprocessing, uh, with just like removing punctuation, and then, um, character n-gram features of various lengths,",
    "start": "367170",
    "end": "374090"
  },
  {
    "text": "as well as tf-idf, um, and then just a logistic regression classifier. And that managed to get a really,",
    "start": "374090",
    "end": "379190"
  },
  {
    "text": "really good score, um, on this task. Um, so yeah, it's interesting to see like,",
    "start": "379190",
    "end": "385050"
  },
  {
    "text": "you know, how, what we were doing last year with like, manual on feature engineering, um,",
    "start": "385050",
    "end": "390430"
  },
  {
    "text": "has like kind of been totally replaced this year by, um, massive compute, created pre-trained BERT representation.",
    "start": "390430",
    "end": "397465"
  },
  {
    "text": "And second place, um, Lucy's team, shout out to Lucy. Um, also used similar like kinda feature engineering approach, um,",
    "start": "397465",
    "end": "405604"
  },
  {
    "text": "with like very, very rich features, uni-grams, bi-grams, and negation words, leveraging sentiment lexicon.",
    "start": "405605",
    "end": "411680"
  },
  {
    "text": "Um, yeah. So that's like also a direction that liking you could definitely go um, if you were to,",
    "start": "411680",
    "end": "417155"
  },
  {
    "text": "you do the sentiment task maybe like in your class project or something like that. Um, yeah. And I think that's about it.",
    "start": "417155",
    "end": "424574"
  },
  {
    "text": "Um, anyone have any questions about, uh, last week's bake-off?",
    "start": "424575",
    "end": "430780"
  },
  {
    "text": "I wonder if they're representatives from the top two teams here, I wonder whether those are kind of theoretically the same system.",
    "start": "435110",
    "end": "443305"
  },
  {
    "text": "Because it sounds like the first team used BERT and used a PyTorch layer on top to fit a classifier, maybe a deep one.",
    "start": "443305",
    "end": "451134"
  },
  {
    "text": "And the second placed team just used our classifier as opposed to the, um, one that the BERT team released.",
    "start": "451135",
    "end": "457669"
  },
  {
    "text": "So I wonder, if it's just a difference of like how they were optimized, or actually maybe there's a,",
    "start": "457670",
    "end": "462759"
  },
  {
    "text": "there's a deeper difference that I'm overlooking, but they look very similar. Yeah. Anyone from group 13 or 51 want to weigh in?",
    "start": "462760",
    "end": "470840"
  },
  {
    "text": "Because the- it must be that team 1 just didn't use TorchShallowNeuralClassifier, but rather, something that's part of the BERT,",
    "start": "472400",
    "end": "480285"
  },
  {
    "text": "the BERT, PyTorch code release. Yeah. I believe this group, um,",
    "start": "480285",
    "end": "485729"
  },
  {
    "start": "483000",
    "end": "516000"
  },
  {
    "text": "just kinda like tweaked the final layers of the BERT model, um, like the preexisting, like PyTorch implementation.",
    "start": "485730",
    "end": "492855"
  },
  {
    "text": "Yeah, this is cool to, this is common to see on deep learning code on the internet.",
    "start": "492855",
    "end": "498789"
  },
  {
    "text": "You find that in some file somewhere in the code base, someone has done some very careful preprocessing.",
    "start": "498790",
    "end": "505465"
  },
  {
    "text": "Presumably, this is to, to sort of get properly into the BERT vocabulary or something.",
    "start": "505465",
    "end": "511330"
  },
  {
    "text": "Yeah. I was curious about this, they actually say on the BERT page, you shouldn't like unprocess your token so it looks more like raw text. Okay.",
    "start": "511330",
    "end": "524360"
  },
  {
    "start": "516000",
    "end": "622000"
  },
  {
    "text": "Well, thank you. [APPLAUSE] Yeah.",
    "start": "524540",
    "end": "532500"
  },
  {
    "text": "And I've got my slides and everything, maybe this will be my new solution. Um, so okay.",
    "start": "532500",
    "end": "537810"
  },
  {
    "text": "So thank you [inaudible] Um, bake-off stuff. First, I just wanted to say that Amazon gave us enough codes so that we can give $50,",
    "start": "537810",
    "end": "546810"
  },
  {
    "text": "um, codes for AWS to people who win the bake-off. So the first team got their codes,",
    "start": "546810",
    "end": "552029"
  },
  {
    "text": "and that winning team there will get codes, and we'll keep that up. It has to be just the top scoring system numerically,",
    "start": "552030",
    "end": "558000"
  },
  {
    "text": "because we don't have that many extras. Um, some, some more teams might get extra credit than that, but it is nice that we can reward the very top teams.",
    "start": "558000",
    "end": "566295"
  },
  {
    "text": "Um, other bake-off stuff. So I've posted bake-off 3 on Piazza,",
    "start": "566295",
    "end": "571680"
  },
  {
    "text": "I think it's straightforward. I- it looks a lot like the procedure for bake-off 1, in the sense that you need to download some data, um,",
    "start": "571680",
    "end": "578040"
  },
  {
    "text": "and then you're pretty much just, just running code that you've written. Um, but of course, post on Piazza if there are any issues there.",
    "start": "578040",
    "end": "584310"
  },
  {
    "text": "Uh, I also wanted to mention that there was a PyTorch and numpy tutorial on Friday,",
    "start": "584310",
    "end": "589725"
  },
  {
    "text": "and the team produced great notebooks for that. I think it's especially [inaudible].",
    "start": "589725",
    "end": "596190"
  },
  {
    "text": "And so, I posted those at the homepage, um, at the top, on the first day. It's a bit revisionist,",
    "start": "596190",
    "end": "601649"
  },
  {
    "text": "but it's kind of like introduction to Jupyter Notebooks, and now you have the stuff for numpy and PyTorch. And I think the timing of that is really nice,",
    "start": "601649",
    "end": "608280"
  },
  {
    "text": "because you guys might be thinking about projects you wanna do, and for your projects, you might wanna, um, dive into the PyTorch a little bit.",
    "start": "608280",
    "end": "615240"
  },
  {
    "text": "[NOISE] And then the final thing I wanna do here, before I dive into NLI,",
    "start": "615240",
    "end": "620655"
  },
  {
    "text": "is just kind of introduce homework 4 and the bake-off. And the reason I wanted to do that first is,",
    "start": "620655",
    "end": "627210"
  },
  {
    "start": "622000",
    "end": "676000"
  },
  {
    "text": "work- it's a kind of reversal. And we're gonna talk- we're gonna talk about natural language inference all week this week.",
    "start": "627210",
    "end": "632460"
  },
  {
    "text": "Um, and the bake-off is about natural language inference, but because the really good NLI datasets are enormous,",
    "start": "632460",
    "end": "639930"
  },
  {
    "text": "I didn't want you to have to do that for a bake-off, because it would consume a lot of time and a lot of resources.",
    "start": "639930",
    "end": "645225"
  },
  {
    "text": "So instead, for homework 4 and bake-off 4, we're gonna do just a word-level entailment task.",
    "start": "645225",
    "end": "651209"
  },
  {
    "text": "[NOISE] Uh, and this is kind of nice, because it's like a microcosm of the whole NLI problem, it just removes a lot of the overhead from processing large datasets,",
    "start": "651210",
    "end": "660149"
  },
  {
    "text": "and it's kind of nicely constrained in some interesting ways. So let me just start by introducing it, because they're kind of two or three intellectual things",
    "start": "660149",
    "end": "668010"
  },
  {
    "text": "that I think are worth calling out. [NOISE] So the- the first, the framing, uh,",
    "start": "668010",
    "end": "673815"
  },
  {
    "text": "throughout this problem set, the training instances are gonna be pairs like, hippo and mammal, just words.",
    "start": "673815",
    "end": "681255"
  },
  {
    "start": "676000",
    "end": "731000"
  },
  {
    "text": "Um, [NOISE] people have actually developed labeled datasets that just tell you for these at the word level,",
    "start": "681255",
    "end": "686820"
  },
  {
    "text": "whether the first entails the second, and that's basically the task. So it's a binary task,",
    "start": "686820",
    "end": "692010"
  },
  {
    "text": "um, with vocabulary like this. Basically, because it's constrained to just words,",
    "start": "692010",
    "end": "700170"
  },
  {
    "text": "you have to make just a few decisions here. So first, kind of like in bake-off 1, you need to decide how you're gonna represent the pair of those words.",
    "start": "700170",
    "end": "708420"
  },
  {
    "text": "So hippo and mammal are just items in your vocabulary, but you can look them up in some embedding space,",
    "start": "708420",
    "end": "713639"
  },
  {
    "text": "first of all, and you can decide what embedding space, and it can be any embedding space. I guess the smart move based on past bake-offs is to use something like BERT or ELMo.",
    "start": "713640",
    "end": "722235"
  },
  {
    "text": "Sorry, my phone is hardwired to stop, to shut off very quickly.",
    "start": "722235",
    "end": "728055"
  },
  {
    "text": "I'll try to avoid that. [NOISE] So you'll wanna look up hippo and mammal in an embedding space.",
    "start": "728055",
    "end": "734760"
  },
  {
    "start": "731000",
    "end": "753000"
  },
  {
    "text": "You can also decide how you're gonna represent them, right? So I think like, what I've suggested here is concatenation,",
    "start": "734760",
    "end": "740355"
  },
  {
    "text": "but you could do something else if you wanted, uh, difference, or multiplication, or sum, whatever you want it to do.",
    "start": "740355",
    "end": "746160"
  },
  {
    "text": "So, so embedding and then representation. And then from there, you're gonna wanna fit some kind of model, right?",
    "start": "746160",
    "end": "753090"
  },
  {
    "text": "So the simplest thing would be to fit, uh, like a basic classifier, but you could have also a very deep network in there, right?",
    "start": "753090",
    "end": "762135"
  },
  {
    "text": "Um, lots of hidden representations. And I'm gonna leave it entirely up to you what you wanna do there.",
    "start": "762135",
    "end": "768030"
  },
  {
    "text": "I've fit a baseline that's kind of a shallow deep learning model, but you can do really whatever you want.",
    "start": "768030",
    "end": "773940"
  },
  {
    "text": "Uh, and that's the bake-off. That makes sense?",
    "start": "773940",
    "end": "779010"
  },
  {
    "text": "And you can see that this is a microcosm of the whole NLI problem, because, after all for NLI, it's just that hippo and mammal",
    "start": "779010",
    "end": "785010"
  },
  {
    "text": "could both be complete sentences or very complicated noun phrases. But in the end also we have three labels,",
    "start": "785010",
    "end": "790230"
  },
  {
    "text": "as you'll see for our output space. But fundamentally, this is the same kind of problem, it just means that you can iterate much more quickly",
    "start": "790230",
    "end": "796680"
  },
  {
    "text": "because the datasets are smaller and there's less to process. There's one other subtlety that I wanna point out here.",
    "start": "796680",
    "end": "803295"
  },
  {
    "start": "800000",
    "end": "879000"
  },
  {
    "text": "So you have a dataset that comes with the notebook, it's already in your data folder. And there are two conditions for it.",
    "start": "803295",
    "end": "810029"
  },
  {
    "text": "I've called the first one edge_disjoint, and that just means that the pairs,",
    "start": "810030",
    "end": "815585"
  },
  {
    "text": "uh, on training and testing, you're trained in dev, but also test, which I'll release as part of the bake-off.",
    "start": "815585",
    "end": "822095"
  },
  {
    "text": "At the edge level, things are disjoint. What that means is, that you could see a lot of the same vocabulary at train and test time, right?",
    "start": "822095",
    "end": "831600"
  },
  {
    "text": "So like hippo and mammal, if you saw them in training, you won't see exactly that pair,",
    "start": "831600",
    "end": "836955"
  },
  {
    "text": "but you might have seen hippo paired with other words, and mammal paired with other words. And that's a kind of standard formulation of this problem.",
    "start": "836955",
    "end": "844845"
  },
  {
    "text": "But, it's kind of easy to game, in the sense that, you know, the- the systems that result from this often look very good in terms of performance,",
    "start": "844845",
    "end": "853470"
  },
  {
    "text": "but you might doubt whether they have actually learned to generalize. Because it could be that they've just kind of learned",
    "start": "853470",
    "end": "858900"
  },
  {
    "text": "to triangulate within the space that they're in, to do pretty well on the test set. And you can kind of see that because some baselines,",
    "start": "858900",
    "end": "865679"
  },
  {
    "text": "which I'm gonna tell you about later, tend to be very high for this problem. Like, if I give you only the second word,",
    "start": "865679",
    "end": "871710"
  },
  {
    "text": "what we'll call only the hypothesis, maybe you get like 80%. So I- we're not gonna focus on that task, instead,",
    "start": "871710",
    "end": "879135"
  },
  {
    "start": "879000",
    "end": "969000"
  },
  {
    "text": "we're gonna focus on this condition that I've called word_disjoint. And what that means, is that, for train and dev,",
    "start": "879135",
    "end": "885405"
  },
  {
    "text": "and also for test, the vocabularies are disjoint. So that means, if you saw a hippo and mammal in training,",
    "start": "885405",
    "end": "892320"
  },
  {
    "text": "you will not see either of those words at test time. Um, and it, it entails therefore that you",
    "start": "892320",
    "end": "898620"
  },
  {
    "text": "won't see that- you'd haven't seen those edges before, right? So this is a strictly harder problem that is really testing your network's ability,",
    "start": "898620",
    "end": "905895"
  },
  {
    "text": "not only to do well at words it's seen, but also to generalize from that into other parts of the lexicon. Yeah.",
    "start": "905895",
    "end": "913620"
  },
  {
    "text": "I think you mentioned slightly above that one of the possibilities for the embedding is just a random embedding?",
    "start": "913620",
    "end": "920340"
  },
  {
    "text": "[NOISE] Does that work with word_disjoint if you had a random embedding? It certainly does not [LAUGHTER].",
    "start": "920340",
    "end": "925440"
  },
  {
    "text": "Yes, um, your intuition is exactly right. If you have a random initial embedding, then you will produce random results on the- not on the edge_disjoint,",
    "start": "925440",
    "end": "933540"
  },
  {
    "text": "you could do really well on edge_disjoint, but on word_disjoint, you will flounder. And what that means is,",
    "start": "933540",
    "end": "939150"
  },
  {
    "text": "that you need to initialize with a rich space, and you'll see in the baselines that I've put down here that, uh, I did GloVe.",
    "start": "939150",
    "end": "946140"
  },
  {
    "text": "Um, But kind of the striking and interesting thing about this, is that, initializing with GloVe or I assume with BERT and ELMo,",
    "start": "946140",
    "end": "953205"
  },
  {
    "text": "really gives you a lot of traction on this problem, even on the word_disjoint condition.",
    "start": "953205",
    "end": "959050"
  },
  {
    "text": "So I've walked through just to show you in a bit more detail in this notebook, um,",
    "start": "959420",
    "end": "964649"
  },
  {
    "text": "what these two datasets are like, and I've kind of built up this argument here for why we might wanna use the word_disjoint condition.",
    "start": "964650",
    "end": "971519"
  },
  {
    "text": "I also set a baseline, um, which uses these GloVe vectors here as the kind of, um,",
    "start": "971520",
    "end": "979740"
  },
  {
    "start": "975000",
    "end": "985000"
  },
  {
    "text": "fundamental representation of the words, you just do a GloVe look up in a way that you guys have done on earlier problems.",
    "start": "979740",
    "end": "985980"
  },
  {
    "start": "985000",
    "end": "992000"
  },
  {
    "text": "And for my baseline co- combination function for the two words, I just use concatenation.",
    "start": "985980",
    "end": "992085"
  },
  {
    "start": "992000",
    "end": "1003000"
  },
  {
    "text": "And then for the classifier, as I said before, I use this TorchShallowNeuralClassifier from the course repo,",
    "start": "992085",
    "end": "998865"
  },
  {
    "text": "50 dimensional hidden representations, a modest number of iterations.",
    "start": "998865",
    "end": "1004010"
  },
  {
    "start": "1003000",
    "end": "1036000"
  },
  {
    "text": "And, as you guys are probably accustomed to at this point, we have this kind of little experiment wrapper,",
    "start": "1004010",
    "end": "1010115"
  },
  {
    "text": "that should make it pretty easy for you to run experiments. So, you know, model is your, um, the model that you wanna test, it could be anything.",
    "start": "1010115",
    "end": "1017750"
  },
  {
    "text": "A glove_vec and vec_concatenate, those are really the three points that you'll want variation on,",
    "start": "1017750",
    "end": "1023630"
  },
  {
    "text": "and you'll probably just wanna use train and dev there for your, um, development phase.",
    "start": "1023630",
    "end": "1029180"
  },
  {
    "text": "So for my baseline, I got macro average 0.69,",
    "start": "1029180",
    "end": "1034459"
  },
  {
    "text": "uh, which is pretty good. But obviously, the goal for you in doing the homework in the bake-off,",
    "start": "1034460",
    "end": "1040704"
  },
  {
    "start": "1036000",
    "end": "1052000"
  },
  {
    "text": "is to see how much you can build off of that baseline, with a richer network or more interesting representations,",
    "start": "1040705",
    "end": "1046824"
  },
  {
    "text": "or more in different ways to combine the two-word vectors to form your input representation.",
    "start": "1046825",
    "end": "1053580"
  },
  {
    "start": "1052000",
    "end": "1071000"
  },
  {
    "text": "Is that making sense so far? Any questions? Comments? Concerns? [NOISE] It should be a fun one,",
    "start": "1053950",
    "end": "1060330"
  },
  {
    "text": "because you can really develop models fast, so you should be able to try a lot of different things.",
    "start": "1060330",
    "end": "1065360"
  },
  {
    "text": "This is sort of anticipating stuff that I wanna discuss for you, but let me just walk through the rationale for the homework.",
    "start": "1066210",
    "end": "1072040"
  },
  {
    "start": "1071000",
    "end": "1105000"
  },
  {
    "text": "So the first one is called hypothesis-only baseline. As you'll see as we go through the material,",
    "start": "1072040",
    "end": "1077110"
  },
  {
    "text": "people discovered a few years ago that you could actually do really well on many NLI datasets if you threw out the first part.",
    "start": "1077110",
    "end": "1085105"
  },
  {
    "text": "So in my hippo-mammal pair, just throw out hippo, and make your classification purely on the basis of the word mammal.",
    "start": "1085105",
    "end": "1091615"
  },
  {
    "text": "And I wanna discuss that with you, because I think it's kind of an interesting thing that mixes, maybe some artifacts in these datasets,",
    "start": "1091615",
    "end": "1098345"
  },
  {
    "text": "but also a pretty deep fact about how the lexicon is organized, and what it means to be in an entailment relationship.",
    "start": "1098345",
    "end": "1105460"
  },
  {
    "start": "1105000",
    "end": "1126000"
  },
  {
    "text": "I mean, if you think about that, it's not surprising that mammal would be more likely to occur on the right and an entailment pair than the left,",
    "start": "1105460",
    "end": "1112140"
  },
  {
    "text": "because of its generality. [NOISE] Oh, and actually, this fact about hypothesis-only baselines,",
    "start": "1112140",
    "end": "1118070"
  },
  {
    "text": "was discovered by a student in this class, which I think is really cool. For his- as far as I know,",
    "start": "1118070",
    "end": "1123115"
  },
  {
    "text": "the first person to observe this for the SNLI dataset, which is a big entailment dataset,",
    "start": "1123115",
    "end": "1128378"
  },
  {
    "start": "1126000",
    "end": "1181000"
  },
  {
    "text": "was a project in this course in 2015. And then, subsequently a number of,",
    "start": "1128379",
    "end": "1133425"
  },
  {
    "text": "um, other groups discovered it and quantified it. [NOISE] So what we're asking you to do here is to just see how",
    "start": "1133425",
    "end": "1139570"
  },
  {
    "text": "strong the hypothesis-only baseline is for this dataset. And what we're trying to signal there is that,",
    "start": "1139570",
    "end": "1146215"
  },
  {
    "text": "given these- for this prior work, if you tackle an NLI problem, then basically, you should fit a hypothesis-only baseline,",
    "start": "1146215",
    "end": "1153670"
  },
  {
    "text": "because random is not sure baseline at all, it's actually this one. So it asks you to do that, that's pretty straightforward.",
    "start": "1153670",
    "end": "1160929"
  },
  {
    "text": "And then it just asks you in the spirit of thinking freely about this to explore an alternative representation to concatenation,",
    "start": "1160930",
    "end": "1168085"
  },
  {
    "text": "so that you might think about some interesting kind of apriori ways that you might combine the two representations,",
    "start": "1168085",
    "end": "1173760"
  },
  {
    "text": "maybe difference is interesting for an entailment space or multiplication or something like that.",
    "start": "1173760",
    "end": "1180110"
  },
  {
    "text": "And then, this is where that PyTorch tutorial might come in. I'm gonna ask you to do a little bit of work,",
    "start": "1180850",
    "end": "1187930"
  },
  {
    "start": "1181000",
    "end": "1223000"
  },
  {
    "text": "kind of sub-classing one of our PyTorch classes and adding essentially another layer,",
    "start": "1187930",
    "end": "1194050"
  },
  {
    "text": "and a dropout layer. Dropout is a form of regularization. That's very easy to implement in PyTorch,",
    "start": "1194050",
    "end": "1199330"
  },
  {
    "text": "I've kind of presented the mathematical details here, but I think you'll find that the important thing is to just use the PyTorch documentation to figure out how to add into your sequential model,",
    "start": "1199330",
    "end": "1209080"
  },
  {
    "text": "a dropout layer, and then you'll be off and running. And again, the spirit of this is to get you thinking about,",
    "start": "1209080",
    "end": "1214990"
  },
  {
    "text": "how regularization for your networks might be a powerful technique to avoid over-fitting and stuff like that.",
    "start": "1214990",
    "end": "1221810"
  },
  {
    "text": "And I've kind of started you, and given you some instructions, and we're happy to talk with you about the best strategy for this kind of coding on Piazza.",
    "start": "1223400",
    "end": "1231285"
  },
  {
    "text": "And then, finally, as usual, you do your original system. We'll see what happens.",
    "start": "1231285",
    "end": "1236400"
  },
  {
    "text": "I don't think I have to impose any rules on this. You can download vectors from the internet. You can download code as long you do- as you do something original with it.",
    "start": "1236400",
    "end": "1244470"
  },
  {
    "text": "And the reason I can say that is because the bake-off will be conducted on a held out data set that I have not given you.",
    "start": "1244470",
    "end": "1250500"
  },
  {
    "text": "So you can do all the development you want on the data we've provided. Uh, just don't overfit too much,",
    "start": "1250500",
    "end": "1256155"
  },
  {
    "text": "because then you'll get hit at test time, in the bake-off.",
    "start": "1256155",
    "end": "1260470"
  },
  {
    "text": "Makes sense? Any questions about that before we dive into th- the regular material?",
    "start": "1263060",
    "end": "1268930"
  },
  {
    "text": "So that was a small glimpse of the problem that you'll be exploring. Um, but what we're gonna focus on this week is the problem in its fullness,",
    "start": "1269210",
    "end": "1278520"
  },
  {
    "text": "this problem of Natural Language Inference. Um, I think this is a wonderful problem,",
    "start": "1278520",
    "end": "1284235"
  },
  {
    "text": "because I think it gets at something really deep about what we're trying to do in NLU. And also, there is an abundance of really interesting data sets at this point.",
    "start": "1284235",
    "end": "1293220"
  },
  {
    "text": "Uh, because the field has kind of, um, triangulated on an interesting methodology [NOISE] for collecting these data sets,",
    "start": "1293220",
    "end": "1302024"
  },
  {
    "text": "and people have found that it's pretty easy to crowd source good ones. I mean, there are some issues that we'll discuss.",
    "start": "1302025",
    "end": "1307710"
  },
  {
    "text": "But by and large, this has kind of been productionized. And so, you have multiple datasets that are in the same format making it very easy to experiment.",
    "start": "1307710",
    "end": "1315600"
  },
  {
    "text": "So I don't encourage having a default project for this course, because I think it's interesting for you guys to get creative.",
    "start": "1315600",
    "end": "1321120"
  },
  {
    "text": "But if there were a default project, I would certainly say it should be something with NLI. Because it tests lots of the concepts that we've introduced,",
    "start": "1321120",
    "end": "1329653"
  },
  {
    "text": "there's a really wide space of models that you can explore, and these datasets are so good. I also want to say, that there are many special connections to this course for NLI.",
    "start": "1329654",
    "end": "1339600"
  },
  {
    "text": "So it used to be called RTE until Bill came along. For his thesis, he started calling it Natural Language Inference,",
    "start": "1339600",
    "end": "1346844"
  },
  {
    "text": "and Bill did lots of seminal work in this space, kind of exploring how you might apply natural logic,",
    "start": "1346844",
    "end": "1352200"
  },
  {
    "text": "which has a very rich algebra of inferential relationships to NLP tasks,",
    "start": "1352200",
    "end": "1357480"
  },
  {
    "text": "uh, in the space. And so, we'd like Bill is part of the resurgence of interest in this problem.",
    "start": "1357480",
    "end": "1364125"
  },
  {
    "text": "And then, Sam Bowman was a student in this course. Sam was also my student. And I think the full history of this is that,",
    "start": "1364125",
    "end": "1370950"
  },
  {
    "text": "Richard Socher came and gave a lecture on, you know, tree structured networks for sentiment analysis.",
    "start": "1370950",
    "end": "1376650"
  },
  {
    "text": "It was about the time that we were developing the Stanford Sentiment Treebank, and Sam was captivated. Uh, I think Sam was not focused on NLU at the time for his research,",
    "start": "1376650",
    "end": "1385695"
  },
  {
    "text": "but that was a kind of important moment for him. Uh, and he wanted to work on NLI, because I think he admired Bill's work and he liked thinking like a semanticist.",
    "start": "1385695",
    "end": "1393929"
  },
  {
    "text": "But there weren't really good data sets. He felt kind of stuck by the ones that were available, and he just did the really admirable and ambitious thing of saying,",
    "start": "1393930",
    "end": "1402360"
  },
  {
    "text": "let's collect a massive data set for NLI, so that we can test these new deep learning models.",
    "start": "1402360",
    "end": "1408899"
  },
  {
    "text": "Because one, one sticking point at that point was, that the available data sets were kind of too small",
    "start": "1408900",
    "end": "1414660"
  },
  {
    "text": "to really test these deep learning approaches. So Sam developed SNLI and the rest is history,",
    "start": "1414660",
    "end": "1419850"
  },
  {
    "text": "the, the task has been renamed. Nobody says RTE anymore. Um, and we have all these interesting data sets,",
    "start": "1419850",
    "end": "1425970"
  },
  {
    "text": "and Sam has been a real powerhouse in terms of developing new ones. So he's responsible for both SNLI and MultiNLI,",
    "start": "1425970",
    "end": "1432900"
  },
  {
    "text": "which is the two data sets that we're gonna concentrate on. And then, just while I'm in this vein, I want to mention also that both,",
    "start": "1432900",
    "end": "1439590"
  },
  {
    "text": "and so, both working hard on NLI problems right now, kind of using it to stress",
    "start": "1439590",
    "end": "1444660"
  },
  {
    "text": "test neural systems to see what they're actually learning about semantics, is using them as a test bed for",
    "start": "1444660",
    "end": "1451020"
  },
  {
    "text": "interesting new neural models that he's developing with some researchers at IBM. So there are lots of us on",
    "start": "1451020",
    "end": "1458789"
  },
  {
    "text": "the teaching team who are interested in talking with you about NLI. [NOISE] Here's my overview.",
    "start": "1458790",
    "end": "1464415"
  },
  {
    "start": "1464000",
    "end": "1561000"
  },
  {
    "text": "Um, I'm gonna give you a bit of background on the problem, and I'm gonna introduce these two datasets, SNLI and MultiNLI.",
    "start": "1464415",
    "end": "1471405"
  },
  {
    "text": "And then, we're gonna do the kind of standard narrative for this course. I'm gonna show you some hand-built feature functions,",
    "start": "1471405",
    "end": "1477600"
  },
  {
    "text": "which I think are really good and interesting baselines for the problem. And having done that, then I'm gonna show you that we have in the course repo,",
    "start": "1477600",
    "end": "1485294"
  },
  {
    "text": "a nice framework for doing experiments on all these kind of, um, SNLI style data sets.",
    "start": "1485295",
    "end": "1491145"
  },
  {
    "text": "From there, we're gonna start to look at deep learning models. And NLI is interesting as compared to sentiment,",
    "start": "1491145",
    "end": "1497250"
  },
  {
    "text": "because basically, you have two texts to work with, the premise and the hypothesis, and that opens up lots of interesting avenues for doing modeling.",
    "start": "1497250",
    "end": "1506235"
  },
  {
    "text": "Because, you can think about how to model the premise and hypothesis separately, and how to relate them,",
    "start": "1506235",
    "end": "1511890"
  },
  {
    "text": "and then finally how to fit a classifier. So we'll look at sentence-encoding models, chained models.",
    "start": "1511890",
    "end": "1519090"
  },
  {
    "text": "Those are kind of the main classes. And then, as an orthogonal development, I'm gonna show you some stuff on attention,",
    "start": "1519090",
    "end": "1526364"
  },
  {
    "text": "which has really risen to prominence recently. I think an NLI and also machine translation were kind of motivating problems for attention in NLU.",
    "start": "1526364",
    "end": "1534330"
  },
  {
    "text": "So we'll talk about those kind of things and how you might add them to your systems to do, uh, even better.",
    "start": "1534330",
    "end": "1539985"
  },
  {
    "text": "And then, finally, we'll talk about error analysis. These things will probably come on Wednesday. We'll see how things go.",
    "start": "1539985",
    "end": "1545415"
  },
  {
    "text": "But I think again, because of NLI's very rich grounding in linguistics, you have some very interesting avenues for thinking about error analysis,",
    "start": "1545415",
    "end": "1554039"
  },
  {
    "text": "and how it might relate to linguistic patterns, and logical patterns, and stuff like that.",
    "start": "1554040",
    "end": "1559275"
  },
  {
    "text": "So that's our full plot. Uh, as usual, we have a lot of material.",
    "start": "1559275",
    "end": "1564510"
  },
  {
    "start": "1561000",
    "end": "1647000"
  },
  {
    "text": "So nli.py is your main module, both for the homework, and if you want to work with these larger data sets.",
    "start": "1564510",
    "end": "1571230"
  },
  {
    "text": "And then, there are two notebooks. So the task in data one introduces SNLI and MultiNLI,",
    "start": "1571230",
    "end": "1577305"
  },
  {
    "text": "and shows you the code that we have for working with them. And then, the second notebook is a whole lot of modeling code,",
    "start": "1577305",
    "end": "1583695"
  },
  {
    "text": "to kind of get you started if you want to pursue this task. We've gone over Homework 4 and bake-off 4.",
    "start": "1583695",
    "end": "1590159"
  },
  {
    "text": "And then, I'll just mention that the core readings in my mind, we posted a few more, but I would say the core readings are Bowman et al.",
    "start": "1590160",
    "end": "1596490"
  },
  {
    "text": "2015, that introduced SNLI and set up some initial baselines. And then, this Rocktäschel et al.,",
    "start": "1596490",
    "end": "1602685"
  },
  {
    "text": "that's a seminal paper in terms of introducing attention, word by word attention into the NLI problem,",
    "start": "1602685",
    "end": "1610080"
  },
  {
    "text": "and showing that it could lead to big gains. And then, I also recommend some auxiliary readings. So the Goldberg 2015 is that primer on deep learning models,",
    "start": "1610080",
    "end": "1618900"
  },
  {
    "text": "that I also recommended in the last unit. [NOISE] So that's kind of separate. Then, for NLI in particular, Dagan et al.",
    "start": "1618900",
    "end": "1626159"
  },
  {
    "text": "is a very important foundational paper that more or less introduced the task of RTE. And then, MacCartney and Manning,",
    "start": "1626160",
    "end": "1632490"
  },
  {
    "text": "that's a nice presentation of this natural logical approach that Bill developed. And then, Williams et al.",
    "start": "1632490",
    "end": "1637710"
  },
  {
    "text": "is the paper that introduced MultiNLI, and that's another good resource for you in terms of baselines and in terms of understanding what's in that data set.",
    "start": "1637710",
    "end": "1646450"
  },
  {
    "text": "These are typical NLI examples. I picked them a little bit strategically just to give",
    "start": "1649130",
    "end": "1655200"
  },
  {
    "text": "you a sense for the kind of variation that we see. So the top one there, you would say that turtle and linguist contradict each other.",
    "start": "1655200",
    "end": "1662220"
  },
  {
    "text": "That would be the label for that pair. In our homework, we just have yes-no in entailment, but in the data sets that we'll focus on today and on Wednesday,",
    "start": "1662220",
    "end": "1670725"
  },
  {
    "text": "you have three-way labels, contradiction, entailment, and neutral. And so, we would say that,",
    "start": "1670725",
    "end": "1675915"
  },
  {
    "text": "turtle and linguist contradict each other. There's already some complexity there, because you can easily imagine a possible world in which",
    "start": "1675915",
    "end": "1683310"
  },
  {
    "text": "some turtles have- are qualified as linguists, right? Um, you can kind of already see that",
    "start": "1683310",
    "end": "1690810"
  },
  {
    "text": "we're not gonna be dealing with a notion of logical contradiction, but rather something closer to common sense inference based on world knowledge.",
    "start": "1690810",
    "end": "1698955"
  },
  {
    "text": "A turtle danced and a turtle moved, that actually might be closer to just being a true entailment fact,",
    "start": "1698955",
    "end": "1705360"
  },
  {
    "text": "because of the lexical relationship between dance and move. The only, a- assumption you need to [NOISE] bring in",
    "start": "1705360",
    "end": "1711120"
  },
  {
    "text": "there is that these two turtles are the same turtle. Uh, and that's actually",
    "start": "1711120",
    "end": "1716700"
  },
  {
    "text": "a touchy point when you get to naturalistic data and I'm gonna return to it. But kind of what we're doing now is making some assumptions about",
    "start": "1716700",
    "end": "1723495"
  },
  {
    "text": "event or entity co-reference across the premise and hypothesis. And in that case, dance entails move and so we get entailment. Yeah.",
    "start": "1723495",
    "end": "1731910"
  },
  {
    "text": "[inaudible] I guess like so you're assuming that turtles can dance.",
    "start": "1731910",
    "end": "1737745"
  },
  {
    "text": "And I guess the question is like, where do you get like this basic like common sense knowledge? That's interesting. I guess to,",
    "start": "1737745",
    "end": "1744540"
  },
  {
    "text": "to, to be careful about it, I'm not assuming that a turtle danced, but rather if a turtle danced, then the turtle moved.",
    "start": "1744540",
    "end": "1751125"
  },
  {
    "text": "I think the flavor is much more like that. Um, but again, with some qualifications,",
    "start": "1751125",
    "end": "1756540"
  },
  {
    "text": "because this is naturalistic data, as you can see, it's actually based on image captions. And so there is some commitment to the premises being realistic.",
    "start": "1756540",
    "end": "1764190"
  },
  {
    "text": "I guess I just like turtles, and that's why I picked it. So for the first example it's like, it's more like, a turtle can't be a linguist kind of thing, right?",
    "start": "1764190",
    "end": "1774405"
  },
  {
    "text": "Because you're interpreting that within like the real world.",
    "start": "1774405",
    "end": "1779950"
  },
  {
    "text": "Yeah. [OVERLAPPING]. It's hard for me to see like the difference between the two. Because you can imagine a world in which a turtle is a linguist?",
    "start": "1782060",
    "end": "1790155"
  },
  {
    "text": "Yeah. Yeah, fair enough. Then, you would be the- probably the outlier annotator if you were asked to do the annotation,",
    "start": "1790155",
    "end": "1795870"
  },
  {
    "text": "but you have a defensible point. The logician in me wants to say, you know, that's certainly not a contradiction, the first one.",
    "start": "1795870",
    "end": "1802440"
  },
  {
    "text": "Um, and that- the second one is not necessarily an entailment unless I have proper existential closure of the two variables across the premise and hypothesis.",
    "start": "1802440",
    "end": "1811980"
  },
  {
    "text": "Yeah. This is the kind of thing you have to get used to. How do you check for like common sense,",
    "start": "1811980",
    "end": "1817200"
  },
  {
    "text": "um, knowledge, like for the first example? I would frame it differently. I would say it's not so much checking for common sense knowledge",
    "start": "1817200",
    "end": "1824655"
  },
  {
    "text": "as acknowledging that this is pervasive in this task. And that you might- the way you might even think about the NLI problem is as, you know,",
    "start": "1824655",
    "end": "1832920"
  },
  {
    "text": "one that requires an understanding of common sense reasoning and kind of world knowledge, and that that's fueling the system doing well at these problems.",
    "start": "1832920",
    "end": "1840480"
  },
  {
    "text": "But it's, it's really difficult. [inaudible] Why-",
    "start": "1840480",
    "end": "1843350"
  },
  {
    "text": "I love it. Yes, you live in a magical world. [LAUGHTER] Well the turtle danced, the turtle moved.",
    "start": "1847820",
    "end": "1855570"
  },
  {
    "start": "1849000",
    "end": "2000000"
  },
  {
    "text": "I was a little curious why you're saying it has to be the same turtle. Is it- Is it because a turtle danced then that implies that",
    "start": "1855570",
    "end": "1861450"
  },
  {
    "text": "at least some turtles moved and so a turtle like doesn't have to be the same? Well that's a fair point actually for this one.",
    "start": "1861450",
    "end": "1867630"
  },
  {
    "text": "Yeah, we just follow logically that if the- if any old turtle danced than there is some turtle that moved. Yeah, that's fair.",
    "start": "1867630",
    "end": "1873165"
  },
  {
    "text": "Yeah. Every reptile danced, a turtle ate, those would be neutral.",
    "start": "1873165",
    "end": "1879510"
  },
  {
    "text": "Again, like that- that's sort of a logical fact, I think. Some turtles walk, no turtles move,",
    "start": "1879510",
    "end": "1885795"
  },
  {
    "text": "those contradict each other and this is a classic example from Bill's thesis.",
    "start": "1885795",
    "end": "1891180"
  },
  {
    "text": "James Byron Dean refused to move without blue jeans entails James Dean didn't dance without pants.",
    "start": "1891180",
    "end": "1898929"
  },
  {
    "text": "I guess, Bill is highlighting not only the kind of linguistic complexity of this reasoning task because it might take you",
    "start": "1900200",
    "end": "1906179"
  },
  {
    "text": "a minute to see that those are an entailment relationship. But also that as part of this you might have to solve some stuff like",
    "start": "1906180",
    "end": "1912090"
  },
  {
    "text": "[NOISE] figuring out that James Byron Dean and James Dean are the same person. And for that, you might need a lot of world knowledge of a particular kind.",
    "start": "1912090",
    "end": "1920580"
  },
  {
    "text": "But I think that those follow more or less logically, we don't need much common sense reasoning there.",
    "start": "1920580",
    "end": "1926595"
  },
  {
    "text": "That will be very different for the next one. Mitsubishi Motor Corp's new vehicle sales in the US",
    "start": "1926595",
    "end": "1932910"
  },
  {
    "text": "fell 46% in June and Mitsubishi sales rose 46%.",
    "start": "1932910",
    "end": "1938520"
  },
  {
    "text": "Oh no, that one's straightforward. That's just a contradiction, fall and rise. I guess, we have to make some assumptions about the domain of",
    "start": "1938520",
    "end": "1946140"
  },
  {
    "text": "these claims and I'll return to that but by and large those seem like they contradict each other.",
    "start": "1946140",
    "end": "1951345"
  },
  {
    "text": "This next one definitely involves common sense reasoning. Acme Corporation reported that its CEO resigned,",
    "start": "1951345",
    "end": "1957330"
  },
  {
    "text": "entails Acme's CEO resigned. It certainly doesn't because companies can report anything they",
    "start": "1957330",
    "end": "1963570"
  },
  {
    "text": "want and sometimes they report false things and so strictly speaking, if we were semanticists,",
    "start": "1963570",
    "end": "1968850"
  },
  {
    "text": "we would say that this report has no entailments with respect to, um, the complement, Acme's CEO resigned.",
    "start": "1968850",
    "end": "1975375"
  },
  {
    "text": "But again, it's a kind of common sense thing given that we know that this corporation is likely a reliable source about this kind of information,",
    "start": "1975375",
    "end": "1983745"
  },
  {
    "text": "then we might infer entailment or annotators might be encouraged to infer entailment.",
    "start": "1983745",
    "end": "1988770"
  },
  {
    "text": "[OVERLAPPING] James Byron Dean and James Dean one,",
    "start": "1988770",
    "end": "1994455"
  },
  {
    "text": "in my mind I feel like we're walking a fine line here because to assume they're",
    "start": "1994455",
    "end": "1999630"
  },
  {
    "text": "the same person because take the example of like George W Bush and George HW or something. If you call one George Bush and the other you had to",
    "start": "1999630",
    "end": "2007160"
  },
  {
    "text": "specified by quantifying their middle name or something. That's a great point. Yeah. Um, maybe in the- so,",
    "start": "2007160",
    "end": "2014705"
  },
  {
    "text": "in the back of your mind, you should be thinking that there's a practical side of this, which is that we want systems that could just read through newspaper texts for",
    "start": "2014705",
    "end": "2022159"
  },
  {
    "text": "example and figure out what the entailments are or what the logical relationships are. And in that case, this isn't a problem we can ignore",
    "start": "2022160",
    "end": "2028880"
  },
  {
    "text": "rather we need systems that are sensitive to that level of distinction and that know how many George Bushes there are at",
    "start": "2028880",
    "end": "2034190"
  },
  {
    "text": "some implicit level and that can also resolve these to the same entity. But the uncertainty that you guys are feeling,",
    "start": "2034190",
    "end": "2040804"
  },
  {
    "text": "this is wonderful you sound like linguists. This is the- the linguist's reaction to this problem is usually none of these things have any of these logical relationships.",
    "start": "2040805",
    "end": "2048169"
  },
  {
    "text": "It's all neutral because I can think of edge cases where it falls apart and I have to reti- retreat to the idea",
    "start": "2048170",
    "end": "2054770"
  },
  {
    "text": "that we're kind of trying to get a hold of what you as a reader would infer from these texts or infer from these pairs of sentences.",
    "start": "2054770",
    "end": "2062600"
  },
  {
    "text": "[OVERLAPPING] Is there a way to rephrase the task,",
    "start": "2062600",
    "end": "2067850"
  },
  {
    "text": "so that it can detect the ambiguity and sort of report it or relabel it? And if so,",
    "start": "2067850",
    "end": "2072860"
  },
  {
    "text": "how would you go about doing it? Sort of make finer distinctions between logical entailment and common sense reasoning and",
    "start": "2072860",
    "end": "2079490"
  },
  {
    "text": "abduction and all these other notions that are in play here. I think, yeah, here.",
    "start": "2079490",
    "end": "2085145"
  },
  {
    "text": "So, this kind of summarizes this and I would refer you to this trio of articles.",
    "start": "2085145",
    "end": "2090169"
  },
  {
    "text": "This is a kind of back and forth. These are all Stanford people or at one time they were all Stanford people, kind of having a debate about",
    "start": "2090170",
    "end": "2096860"
  },
  {
    "text": "how much non-logical reasoning we should allow to creep into this problem and so,",
    "start": "2096860",
    "end": "2102620"
  },
  {
    "text": "I think I won't resolve it but it's an interesting discussion. I do think though that the top of this kind of",
    "start": "2102620",
    "end": "2107840"
  },
  {
    "text": "summarizes the view that I'm trying to convey which is, does the premise justify an inference to the hypothesis given",
    "start": "2107840",
    "end": "2115400"
  },
  {
    "text": "some common sense assumptions about the world and the speaker and our intentions and so forth?",
    "start": "2115400",
    "end": "2120800"
  },
  {
    "text": "Um, yeah, that's the heart of it and then the other aspects of this problem is that there's been",
    "start": "2120800",
    "end": "2126260"
  },
  {
    "text": "a focus on kind of local inference. So just premise and hypothesis, um,",
    "start": "2126260",
    "end": "2131325"
  },
  {
    "text": "and there's a lot of emphasis on capturing variability in these expressions like different ways that you might refer to people,",
    "start": "2131325",
    "end": "2137165"
  },
  {
    "text": "um, different ways that the same concept might be articulated and so forth and so on.",
    "start": "2137165",
    "end": "2142380"
  },
  {
    "text": "I do think that what happens if you tr- try to be strict about this and say that you're gonna look only at logical entailment,",
    "start": "2142660",
    "end": "2149974"
  },
  {
    "text": "is that the problem gets kind of small and limited. It's basically, just like really reliable lexical relationships and then even then the linguists come in and say,",
    "start": "2149975",
    "end": "2159470"
  },
  {
    "text": "wait a second, word sense ambiguities are causing me even to doubt this thing that you thought was a solid entailment,",
    "start": "2159470",
    "end": "2165500"
  },
  {
    "text": "then it all kinda falls apart. And that's why- that's the story I tell myself about why it actually makes sense to think about this",
    "start": "2165500",
    "end": "2171950"
  },
  {
    "text": "as a common sense reasoning task as much as I might like logical inference.",
    "start": "2171950",
    "end": "2177810"
  },
  {
    "start": "2177000",
    "end": "2369000"
  },
  {
    "text": "This is kind of nice. This is from that important paper by Ido Dagan and colleagues that introduced RTE,",
    "start": "2177910",
    "end": "2183410"
  },
  {
    "text": "what we now call NLI. I'll just read this. It seems that major inferences, as needed by multiple applications,",
    "start": "2183410",
    "end": "2189859"
  },
  {
    "text": "can indeed be cast in terms of textual entailment. Consequently, we hypothesized that textual entailment recognition is",
    "start": "2189860",
    "end": "2197029"
  },
  {
    "text": "a suitable generic task for evaluating and comparing applied semantic inference models.",
    "start": "2197030",
    "end": "2202040"
  },
  {
    "text": "Eventually, such efforts can promote the development of entailment recognition engines, which may provide useful generic modules across applications.",
    "start": "2202040",
    "end": "2210530"
  },
  {
    "text": "I think it's a nice articulation of a dream for NLI as a task. The idea would be, if we can get our systems to be really",
    "start": "2210530",
    "end": "2217880"
  },
  {
    "text": "good at this kind of basic but pervasive logical reasoning, then the capabilities they have would apply in lots of domains.",
    "start": "2217880",
    "end": "2226400"
  },
  {
    "text": "And that certainly resonates with me as a semanticist because I kind of think of entailment and contradiction as being really central to how we think about language.",
    "start": "2226400",
    "end": "2235760"
  },
  {
    "text": "How we reason with it? How we use it? And so forth. Yeah. This could be used to like,",
    "start": "2235760",
    "end": "2241280"
  },
  {
    "text": "[NOISE] for example, say someone made a query but they like sort of misunderstand what they're like trying to get.",
    "start": "2241280",
    "end": "2247385"
  },
  {
    "text": "But it entails like the same like generic things that it like goes to the other thing that entails the same thing.",
    "start": "2247385",
    "end": "2253025"
  },
  {
    "text": "Does that even make sense? Sorry. I mean, you're speaking Ido Dagan's lang- language, right? You're keying in into his framing,",
    "start": "2253025",
    "end": "2259220"
  },
  {
    "text": "which is to say like, you know, challenging problems and NLI maybe they turn on misunderstanding or reformulation.",
    "start": "2259220",
    "end": "2264905"
  },
  {
    "text": "We can actually reduce them into an NLI task and that would be wonderful because if we could then the models I'm showing you today would have a lot more applicability.",
    "start": "2264905",
    "end": "2272225"
  },
  {
    "text": "So, I'm not sure precisely what you mean but I'm really open-minded about this and I've actually tried to do some of this here.",
    "start": "2272225",
    "end": "2278900"
  },
  {
    "text": "This is actually taken from Ido Dagan's article, um, you know, Paraphrase detection is a well-known task.",
    "start": "2278900",
    "end": "2285485"
  },
  {
    "text": "You could think of that as just the task of saying, is the text and its purported paraphrase equal in some extended sense of,",
    "start": "2285485",
    "end": "2294260"
  },
  {
    "text": "um, common sense reasoning, or summarization would say, does the input text entail the summary which would be like saying we want.",
    "start": "2294260",
    "end": "2303259"
  },
  {
    "text": "At least as a constraint on summarization that the result is somewhat more general than the input.",
    "start": "2303260",
    "end": "2309695"
  },
  {
    "text": "Um, information retrieval could go in the reverse direction. You could be- you could formulate that as saying that",
    "start": "2309695",
    "end": "2315619"
  },
  {
    "text": "you would like essentially to return documents that entailed the query in the extended sense of NLI.",
    "start": "2315620",
    "end": "2323075"
  },
  {
    "text": "And then question answering is one that you can basically reduce to saying that you want the answer to entail the question.",
    "start": "2323075",
    "end": "2329975"
  },
  {
    "text": "If you are willing to massage the question into a declarative form a little bit. So for example, if you were willing to replace in English",
    "start": "2329975",
    "end": "2337610"
  },
  {
    "text": "all the WH words with indefinites so that who left became someone left. Then you have a straightforward NLI task and",
    "start": "2337610",
    "end": "2344660"
  },
  {
    "text": "you're really saying you would like an answer like, Sandy left to entail someone left and you would call that your- your notion of answer-hood.",
    "start": "2344660",
    "end": "2352790"
  },
  {
    "text": "And I think there are other tasks out there that you could try to reduce to the same form and that's the inspiring idea that we've",
    "start": "2352790",
    "end": "2359420"
  },
  {
    "text": "really gone kinda down to the metal here in terms of understanding reasoning in language which is applicable to lots of different areas.",
    "start": "2359420",
    "end": "2367200"
  },
  {
    "text": "Again, by way of background this is sort of interesting. This is a landscape of different approaches that people have taken to the NLI problem.",
    "start": "2368920",
    "end": "2377000"
  },
  {
    "start": "2369000",
    "end": "2524000"
  },
  {
    "text": "So up here, I have logic and theorem proving, those are kind of the earliest approaches inspired by logic and linguistics.",
    "start": "2377000",
    "end": "2383690"
  },
  {
    "text": "And I've put them very high on the depth of representations axis because of course they give you very high fidelity pictures of the data.",
    "start": "2383690",
    "end": "2391565"
  },
  {
    "text": "But along the X axis, I have effectiveness which is kind of like, if you give me a whole lot of data,",
    "start": "2391565",
    "end": "2397640"
  },
  {
    "text": "does this model do good and interesting things with it in a comprehensive way? And, ah, the main limitation of logic in theorem",
    "start": "2397640",
    "end": "2404869"
  },
  {
    "text": "proving systems is because they tend to be handcrafted. They're high on the Y axis but low on the X axis.",
    "start": "2404870",
    "end": "2411200"
  },
  {
    "text": "Um, Bill developed this natural logic approach [NOISE] which is kind of intermediate.",
    "start": "2411200",
    "end": "2417155"
  },
  {
    "text": "So it's very rich logically, but because the logic is defined over natural language utterances,",
    "start": "2417155",
    "end": "2422630"
  },
  {
    "text": "you have a lot of power to apply it to larger data-sets and so it kinda moved us to the right along the X axis,",
    "start": "2422630",
    "end": "2429065"
  },
  {
    "text": "um, with some sacrifice in terms of the depth of representations but still very rich.",
    "start": "2429065",
    "end": "2434420"
  },
  {
    "text": "Um, below that I put semantic graphs, which was another kind of handcrafted approach that depended a lot",
    "start": "2434420",
    "end": "2440090"
  },
  {
    "text": "on very high fidelity alignments between the premise and hypothesis. Again, getting more effective with some sacrifices in terms of interpretability and so",
    "start": "2440090",
    "end": "2449600"
  },
  {
    "text": "forth and then over here at the top I have clever hand-built features, ah, kind of as exemplified in this production system called the Excitement Open Platform,",
    "start": "2449600",
    "end": "2459680"
  },
  {
    "text": "which is a good kind of classical system for solving the NLI problem. And then down just below it,",
    "start": "2459680",
    "end": "2466130"
  },
  {
    "text": "I put n-gram variations. We're gonna look at some of those today. So, by that point you're very effective,",
    "start": "2466130",
    "end": "2472925"
  },
  {
    "text": "those models can be very powerful but very low in terms of them offering a deep understanding of the NLI problem.",
    "start": "2472925",
    "end": "2479135"
  },
  {
    "text": "Basically, you just have a lot of superficial associations and then something interesting has happened,",
    "start": "2479135",
    "end": "2484640"
  },
  {
    "text": "so I put deep learning 2015 there. Because in 2015, when we really introduced this task, um,",
    "start": "2484640",
    "end": "2490400"
  },
  {
    "text": "into the course proper, the deep learning models we're not really a slam dunk. This was kinda before or just at the time SLI was being",
    "start": "2490400",
    "end": "2497930"
  },
  {
    "text": "introduced and by and large those systems were being beaten by hand-crafted ones. I do think it's safe to say that by now in 2019,",
    "start": "2497930",
    "end": "2506315"
  },
  {
    "text": "deep learning models have overtaken the hand built ones to the point where even for the little experiments that you see in my notebooks and so forth.",
    "start": "2506315",
    "end": "2514280"
  },
  {
    "text": "It's very easy for you to beat a very strong, um, handcrafted features baseline using some deep learning techniques.",
    "start": "2514280",
    "end": "2522569"
  },
  {
    "start": "2524000",
    "end": "2662000"
  },
  {
    "text": "And then I mentioned that there are a lot of datasets. Here's a whole bunch of them. Um, so the FraCaS textual inference test suite",
    "start": "2524440",
    "end": "2533210"
  },
  {
    "text": "is interesting historically [NOISE] because that is an attempt to have a data set [NOISE] where basically you have been confined at least for",
    "start": "2533210",
    "end": "2540109"
  },
  {
    "text": "the parts that we're talking about to logical inference problems. So it's small but I think still really interesting as a kind of test",
    "start": "2540110",
    "end": "2547190"
  },
  {
    "text": "set if you wanted to stress test your system for example. And then pre SNLI,",
    "start": "2547190",
    "end": "2553280"
  },
  {
    "text": "there were a few data sets like SemEval 2013 to SemEval 2014.",
    "start": "2553280",
    "end": "2558320"
  },
  {
    "text": "That was the sixth dataset which I actually distributed as part of the dataset, uh, for the course because Sam Bowman put it into the SNLI format.",
    "start": "2558320",
    "end": "2567050"
  },
  {
    "text": "So it's there if you wanna play around with it. But again, it's kind of small and had some idiosyncrasies and that was before SNLI.",
    "start": "2567050",
    "end": "2574820"
  },
  {
    "text": "And then after SNLI, you have a flourishing of datasets that are in the same mode. So MedNLI is for medical stuff.",
    "start": "2574820",
    "end": "2581615"
  },
  {
    "text": "Very challenging problem, very rich vocabulary. XNLI is multilingual NLI datasets that had been",
    "start": "2581615",
    "end": "2588800"
  },
  {
    "text": "derived from MultiNLI by a group at Facebook. Um, diverse natural language inference collection is again a really large-scale,",
    "start": "2588800",
    "end": "2596568"
  },
  {
    "text": "um, very diverse set of problems from a group at Hopkins. SciTail is SNLI like,",
    "start": "2596569",
    "end": "2604030"
  },
  {
    "text": "um, but it's derived from science questions and multiple choice questions and also web texts.",
    "start": "2604030",
    "end": "2609680"
  },
  {
    "text": "So that's also really challenging. And then I put two just to kind of get you thinking about this in the mode of Ido Dagan.",
    "start": "2609680",
    "end": "2615800"
  },
  {
    "text": "So like the factoid question answer corpus can be thought of as a kind of NLI problem if you wanna put it into",
    "start": "2615800",
    "end": "2621380"
  },
  {
    "text": "that mold or the Penn Paraphrase Database is just a simple version of NLI where you're trying to jus- get just one relationship.",
    "start": "2621380",
    "end": "2628970"
  },
  {
    "text": "And then really interesting, this is a newer development, the GLUE benchmark was released by Sam Bowman and his group,",
    "start": "2628970",
    "end": "2635690"
  },
  {
    "text": "and what they did is, it's a whole bunch of NLI tasks, I think five of them, and then a whole bunch of other tasks that aren't strictly",
    "start": "2635690",
    "end": "2643369"
  },
  {
    "text": "NLI problems but are meant to kind of test the Dagan assumption that if you have a good NLI system,",
    "start": "2643370",
    "end": "2648920"
  },
  {
    "text": "then those representations will transfer to other tasks. Um, and a lot of people have been working with that.",
    "start": "2648920",
    "end": "2654905"
  },
  {
    "text": "The- there's a lot of progress right now on the GLUE benchmark. So for a very ambitious final project, you could tackle that in whole or in part.",
    "start": "2654905",
    "end": "2661934"
  },
  {
    "text": "I think I'll skip over this. These are some different label sets that have been explored over the years. For better or worse,",
    "start": "2661935",
    "end": "2667359"
  },
  {
    "start": "2662000",
    "end": "2678000"
  },
  {
    "text": "we've kind of consolidated on this three-way one because of SNLI. Um, but before that there were four-way versions",
    "start": "2667360",
    "end": "2674169"
  },
  {
    "text": "and simpler two-way versions like you'll explore in the homework.",
    "start": "2674169",
    "end": "2678569"
  },
  {
    "start": "2678000",
    "end": "2887000"
  },
  {
    "text": "Uh, I mentioned this before because this is relevant for the homework.",
    "start": "2679600",
    "end": "2684710"
  },
  {
    "text": "I have your testing this hypothesis, yes. So Leonid Keselman in this pro, in this course for his final project observed the strength of",
    "start": "2684710",
    "end": "2691880"
  },
  {
    "text": "hypothesis-only baselines and it's been substantiated by a bunch of other groups including one by Sam Bowman.",
    "start": "2691880",
    "end": "2698720"
  },
  {
    "text": "It's interesting to ask why this holds. So why would it be, why could you get any traction at all if you were",
    "start": "2698720",
    "end": "2706369"
  },
  {
    "text": "just shown the hypothesis in a premise hypothesis pair? Initially, that sounds very worrying, right?",
    "start": "2706370",
    "end": "2711590"
  },
  {
    "text": "Because it sounds like there's something deeply problematic about the data that wou- would allow you to leave off so much of your example and still do well.",
    "start": "2711590",
    "end": "2720815"
  },
  {
    "text": "I think it is something to worry about but I'm not myself so worried because I think this has a principled explanation that is more or less grounded in the problem itself.",
    "start": "2720815",
    "end": "2729934"
  },
  {
    "text": "And I've kind of summarized that in these, um, three statements here. So specific claims are likely to be premises and entailments cases.",
    "start": "2729935",
    "end": "2739565"
  },
  {
    "text": "That's really easy for me to imagine if I think of the WordNet hierarchy. So WordNet is basically a huge hierarchy of entailment relations for nouns.",
    "start": "2739565",
    "end": "2748220"
  },
  {
    "text": "The things that are at the bottom of those, that hierarchy are very specific things like puppy and kitten,",
    "start": "2748220",
    "end": "2755255"
  },
  {
    "text": "um, and they entail almost everything else, else in the hierarchy on up to object.",
    "start": "2755255",
    "end": "2760550"
  },
  {
    "text": "So if I ha-, if you show me just the word puppy or kitten, [NOISE] I have a very good guess about whether it's in the premise or the hypothesis.",
    "start": "2760550",
    "end": "2768920"
  },
  {
    "text": "Conversely, if you show me something very general like mammal or object or dog,",
    "start": "2768920",
    "end": "2775535"
  },
  {
    "text": "then it's very likely to be on the right in that pair just because if I drew another word from the vocabulary,",
    "start": "2775535",
    "end": "2781685"
  },
  {
    "text": "it's unlikely to be above it because so few words are. And then, this extends also to contradiction.",
    "start": "2781685",
    "end": "2789349"
  },
  {
    "text": "So I think that specific things are more likely to lead to contradiction by the kind of rules of the SNLI game because,",
    "start": "2789350",
    "end": "2796160"
  },
  {
    "text": "we make some assumptions about event co-reference and so forth and if you've described something very specific in the,",
    "start": "2796160",
    "end": "2802325"
  },
  {
    "text": "in the- these terms, it probably excludes most other claims. And so again, very specific things likely to lead you to contradiction on either side.",
    "start": "2802325",
    "end": "2811125"
  },
  {
    "text": "So I think it's the result of these three observations that is driving a lot of the strength of hypothesis-only baselines.",
    "start": "2811125",
    "end": "2818605"
  },
  {
    "text": "And if you believe that argument, it means that this isn't really something we are gonna be able to naturally factor out because",
    "start": "2818605",
    "end": "2824950"
  },
  {
    "text": "this fact about the entailment hierarchy is something that we want our systems [NOISE] to learn.",
    "start": "2824950",
    "end": "2830750"
  },
  {
    "text": "It's a, it's an important semantic fact, and if you decided for example that you would get rid of it by balancing",
    "start": "2830750",
    "end": "2836329"
  },
  {
    "text": "your dataset so that every word had equal likelihood of appearing as a premise or hypothesis,",
    "start": "2836330",
    "end": "2841370"
  },
  {
    "text": "you could weaken the hypothesis-only baseline but it's not so clear to me that the resulting systems would be solving the problem as we encounter it in the world.",
    "start": "2841370",
    "end": "2850430"
  },
  {
    "text": "And for that reason, my response to this is just to say let's always have in the top of our results table,",
    "start": "2850430",
    "end": "2857780"
  },
  {
    "text": "a hypothesis-only baseline so that we know the starting point for our task.",
    "start": "2857780",
    "end": "2863210"
  },
  {
    "text": "Does that make sense? Anyone who wants to push back on that? Yeah.",
    "start": "2863210",
    "end": "2868880"
  },
  {
    "text": "Just out of curiosity, does a premise-only baseline do anything? It's under Explorer. If you believe my generalizations,",
    "start": "2868880",
    "end": "2875210"
  },
  {
    "text": "it ought to be better than random because I think there is information, er, in just that, that part but I haven't explored it too much.",
    "start": "2875210",
    "end": "2881420"
  },
  {
    "text": "My coach should make it really easy to explore.",
    "start": "2881420",
    "end": "2884490"
  },
  {
    "text": "Okay, this is good timing. Let's dive into what these datasets are like. So we're gonna look at SNLI and MultiNLI",
    "start": "2886870",
    "end": "2894125"
  },
  {
    "text": "which are kind of common mold for datasets like this at this point.",
    "start": "2894125",
    "end": "2899135"
  },
  {
    "start": "2898000",
    "end": "3105000"
  },
  {
    "text": "So here's a quick summary. It was released by Bowman et al, 2015. Um, all of the premises are image captions from the Flickr 30K data set,",
    "start": "2899135",
    "end": "2910234"
  },
  {
    "text": "and that's really important to keep in mind because image captions are kind of unusual bits of language and it also means that all",
    "start": "2910235",
    "end": "2917779"
  },
  {
    "text": "of these things int- at some level for people who produce them were grounded in a specific image and I think that has shaped the data.",
    "start": "2917780",
    "end": "2926765"
  },
  {
    "text": "The hypotheses were all written by crowdworkers. So this was a very large crowd-sourcing task because this has over 500,000 examples and",
    "start": "2926765",
    "end": "2936619"
  },
  {
    "text": "all of those hypotheses were written by people and then parts of the dataset were evaluated by other humans.",
    "start": "2936620",
    "end": "2943730"
  },
  {
    "text": "Right? Um, I should note that some of the sentences reflect stereotypes. This is a nice paper from a Hopkins group again.",
    "start": "2943730",
    "end": "2951065"
  },
  {
    "text": "Just pointing out that as you might expect, people who are doing lots of crowdwork kind of relied on some shortcuts to producing",
    "start": "2951065",
    "end": "2960020"
  },
  {
    "text": "contradiction and attainment pairs and a byproduct of that is that they end up relying on problematic stereotypes.",
    "start": "2960020",
    "end": "2966845"
  },
  {
    "text": "So this is something we would like to eliminate from these datasets because we don't wanna perpetuate these problems.",
    "start": "2966845",
    "end": "2973160"
  },
  {
    "text": "Um, but for now I can just say that you should be aware of this. Yeah, so about 550 training examples,",
    "start": "2973160",
    "end": "2981005"
  },
  {
    "text": "10K dev and 10K test. I've given some information of like the mean length of the examples,",
    "start": "2981005",
    "end": "2987515"
  },
  {
    "text": "um, the types of clauses, so they're not all sentences. Some of them are noun phrases.",
    "start": "2987515",
    "end": "2992585"
  },
  {
    "text": "Er, it's got a fairly large vocabulary, almost 40,000 words, er, and then it has a smaller subset of about 60,000 examples that were validated",
    "start": "2992585",
    "end": "3001690"
  },
  {
    "text": "by four additional annotators if you want increased confidence that the label is correct. Er, and that went rather well,",
    "start": "3001690",
    "end": "3009130"
  },
  {
    "text": "there was a very high level of agreement for the, um, labels. And then finally down here, there's a leaderboard which has",
    "start": "3009130",
    "end": "3017349"
  },
  {
    "text": "a truly overwhelming number of papers on it now. It's kind of hard to digest and I think we're past the day where you",
    "start": "3017350",
    "end": "3023560"
  },
  {
    "text": "would just like hope that you were the highest row. Um, I would think about it some more flexibly.",
    "start": "3023560",
    "end": "3029980"
  },
  {
    "text": "So Sam already divided into a few different model classes and you can think about where you want to slot in.",
    "start": "3029980",
    "end": "3035995"
  },
  {
    "text": "And then another distinction that has emerged is just whether or not your model is an ensemble.",
    "start": "3035995",
    "end": "3041500"
  },
  {
    "text": "I would say that for SNLI in particular, we've entered a phase where ensemble models are just doing best.",
    "start": "3041500",
    "end": "3047680"
  },
  {
    "text": "Um, and that's an interesting game if you want to just pursue raw performance but it's a kind of different thing you're doing on, in,",
    "start": "3047680",
    "end": "3054420"
  },
  {
    "text": "in the modeling sense than picking, uh, a single model that you think reflects an intuition about the data or the problem itself and pursuing it as far as you can.",
    "start": "3054420",
    "end": "3063545"
  },
  {
    "text": "So think in a nuanced way about how you're doing with your system with respect to the leaderboard.",
    "start": "3063545",
    "end": "3070580"
  },
  {
    "text": "This is a screenshot of the crowdsourcing methods that were used. It's kind of small.",
    "start": "3072690",
    "end": "3077920"
  },
  {
    "text": "It's here just as documentation. Um, obviously for crowdworkers, we couldn't just tell them, \"Hey, construct a sentence that entails the premise or contradicts the premise\",",
    "start": "3077920",
    "end": "3086500"
  },
  {
    "text": "because that's a kind of specialized logical notion. So I think we found a way of framing it so that",
    "start": "3086500",
    "end": "3092065"
  },
  {
    "text": "it was pretty clear what we were after but more naturalistic. But if you wanna do a deep dive on what the data actually mean,",
    "start": "3092065",
    "end": "3098890"
  },
  {
    "text": "this is important information because this is the starting point for how the examples were constructed.",
    "start": "3098890",
    "end": "3104810"
  },
  {
    "start": "3105000",
    "end": "3228000"
  },
  {
    "text": "And here are some actual examples. These are, these are the ones that are given in the paper.",
    "start": "3105270",
    "end": "3110904"
  },
  {
    "text": "Um, a man inspects the uniform of a figure in some East Asian country. So remember that was the ca- the caption of an image, uh,",
    "start": "3110905",
    "end": "3118720"
  },
  {
    "text": "and then the contradiction pair that the worker came up with was, um, the man is sleeping. And in a subsequent validation phase,",
    "start": "3118720",
    "end": "3125700"
  },
  {
    "text": "all five people agreed that that was a contradiction label. Um, I think these are pretty straightforward.",
    "start": "3125700",
    "end": "3134050"
  },
  {
    "text": "You can kind of see that con- it looks, anyway like contradiction is very consistent, er, and there's more nuance especially around what the neutral categories mean.",
    "start": "3134050",
    "end": "3142930"
  },
  {
    "text": "I think that's borne out in the data. Here are cases that I picked again just to reintroduce",
    "start": "3142930",
    "end": "3148080"
  },
  {
    "text": "this idea that it's not so obvious how we're defining this. So these aren't actual, er,",
    "start": "3148080",
    "end": "3154365"
  },
  {
    "text": "examples from the corpus but I think they abide by the standards that were adopted.",
    "start": "3154365",
    "end": "3160795"
  },
  {
    "text": "So here's the idea. A boat sank in the Pacific Ocean and a boat sank in the Atlantic Ocean would be labeled as contradiction.",
    "start": "3160795",
    "end": "3168890"
  },
  {
    "text": "Of course those two things could happen together. So there's no logical sense in which these are contradictory.",
    "start": "3169140",
    "end": "3176245"
  },
  {
    "text": "But if we assume that what we're doing is talking about a single image, like an, you know,",
    "start": "3176245",
    "end": "3182380"
  },
  {
    "text": "that from the image Flickr 30K data set, then it's likely that these contradict each other,",
    "start": "3182380",
    "end": "3187750"
  },
  {
    "text": "that only one of them could possibly be true and that's the sense in which this is a contradiction relation.",
    "start": "3187750",
    "end": "3193135"
  },
  {
    "text": "And here's a more extreme case that Sam came up with. So Ruth Bader Ginsburg was appointed to the Supreme Court and I had a sandwich for lunch today.",
    "start": "3193135",
    "end": "3200530"
  },
  {
    "text": "By this logic, you would say that those were contradictory. Again, not because they couldn't both be true.",
    "start": "3200530",
    "end": "3206020"
  },
  {
    "text": "They're,  maybe they're both true of you or of me, um, but rather that they couldn't be describing the same event in some sense.",
    "start": "3206020",
    "end": "3216350"
  },
  {
    "text": "Does that make sense? This is tricky stuff, I mean we're more more or less at mercy,",
    "start": "3216660",
    "end": "3221950"
  },
  {
    "text": "at the mercy of the data that we have but all this is worth keeping in mind.",
    "start": "3221950",
    "end": "3226010"
  },
  {
    "start": "3228000",
    "end": "3475000"
  },
  {
    "text": "Okay and then the other data set I wanted to introduce is MultiNLI, um, released by Williams at al.",
    "start": "3228620",
    "end": "3235440"
  },
  {
    "text": "In 2018. It's very similar in terms of its design with some added nuance.",
    "start": "3235440",
    "end": "3240570"
  },
  {
    "text": "So first, the training set is drawn from a bunch of different genres, fiction, government reports, the Slate website,",
    "start": "3240570",
    "end": "3248520"
  },
  {
    "text": "the Switchboard corpus and some travel guides. And then with- for the dev set,",
    "start": "3248520",
    "end": "3254325"
  },
  {
    "text": "you have one condition where you test on those same genres. And another where you,",
    "start": "3254325",
    "end": "3259470"
  },
  {
    "text": "where you test on some completely different genres and those are the 9/11 report,",
    "start": "3259470",
    "end": "3264974"
  },
  {
    "text": "um, face-to-face which is some narratives, fundraising letters, nonfiction, and articles about linguistics.",
    "start": "3264975",
    "end": "3272340"
  },
  {
    "text": "And that's a really cool idea because you might think that that's a way again of stress testing your system of forcing it to go outside of its training experiences.",
    "start": "3272340",
    "end": "3281100"
  },
  {
    "text": "Again it's very large. Um, it has the same kind of an, um, additional annotations for validation.",
    "start": "3281100",
    "end": "3287775"
  },
  {
    "text": "And then the one twist here is that I think to avoid kind of a lot of hill climbing on the test set. Uh, the test set is released only as",
    "start": "3287775",
    "end": "3295050"
  },
  {
    "text": "a Kaggle competition to kind of reduce the number of times that people can evaluate on it.",
    "start": "3295050",
    "end": "3300240"
  },
  {
    "text": "And you can check out the project page. I think it links to a leaderboard and some other information.",
    "start": "3300240",
    "end": "3305859"
  },
  {
    "text": "Is like this data set, um, [NOISE] I was curious if the captions for the images happening maximally informative.",
    "start": "3306770",
    "end": "3316049"
  },
  {
    "text": "Say you have like a picture of a soccer game happening and then the premise is like a man kicks the ball.",
    "start": "3316050",
    "end": "3323760"
  },
  {
    "text": "And then the hypothesis is just like a man is watching the game or something. So it's bo- they're both happening in the same photo but the photo is multiple events?",
    "start": "3323760",
    "end": "3332880"
  },
  {
    "text": "That's a great question. So basically like how comprehensive can we count on the captions being? I think you can't count on anything.",
    "start": "3332880",
    "end": "3339525"
  },
  {
    "text": "I think image captions are strange things that reflect what humans regard as salient.",
    "start": "3339525",
    "end": "3344880"
  },
  {
    "text": "Um, and so they have their own interesting biases. And there have been some attempts like here at",
    "start": "3344880",
    "end": "3350130"
  },
  {
    "text": "Stanford with a Visual Genome Project to do image captioning of a sort that's not so naturalistic to",
    "start": "3350130",
    "end": "3355530"
  },
  {
    "text": "really comprehensibly label what's in the images. Um, but that's not what was done for image Flickr 30k.",
    "start": "3355530",
    "end": "3360900"
  },
  {
    "text": "It's much more naturalistic. And so you're starting in an interesting social and linguistic space",
    "start": "3360900",
    "end": "3367320"
  },
  {
    "text": "and then building on top of it in an unusual direction. That's the long and short of it. I've always thought though that, um,",
    "start": "3367320",
    "end": "3374609"
  },
  {
    "text": "there's an opportunity here which is that you might leverage the actual images. This has been done infrequently or not at all.",
    "start": "3374610",
    "end": "3382140"
  },
  {
    "text": "But you can imagine that you solve the problem by reasoning not only about the premise, the image caption but also about an- a representation of the image itself.",
    "start": "3382140",
    "end": "3390760"
  },
  {
    "text": "Sorry about the phone doing that. Here's another really interesting thing about the multiNLI dataset.",
    "start": "3392690",
    "end": "3400230"
  },
  {
    "text": "So Sam and the group they released with the dataset a bunch of annotations for properties of the individual examples.",
    "start": "3400230",
    "end": "3408704"
  },
  {
    "text": "So for example they just labeled some as being active passive pairs in the premise and hypothesis.",
    "start": "3408705",
    "end": "3415830"
  },
  {
    "text": "And that's really interesting because you expect that to be kind of meaning preserving but it's an open question whether your system can be sensitive to that kind of transformation,",
    "start": "3415830",
    "end": "3424485"
  },
  {
    "text": "um, or also categories like whether there's a belief predicate like believe or claim or say.",
    "start": "3424485",
    "end": "3430425"
  },
  {
    "text": "Whether there's conditionals, whether co-reference is crucial. Um, because many systems just ignore kind of like the pronouns",
    "start": "3430425",
    "end": "3438255"
  },
  {
    "text": "but entailment can often hinge on the nature of those co-referential relationships. Uh, is it a long sentence?",
    "start": "3438255",
    "end": "3444360"
  },
  {
    "text": "Does it contain modals and negation? Um, could these two things be called paraphrases?",
    "start": "3444360",
    "end": "3449985"
  },
  {
    "text": "What's in- are their quantifiers in the sentence? These are kind of quantifying the, um, semantic difficulty of the examples.",
    "start": "3449985",
    "end": "3457619"
  },
  {
    "text": "And this is really nice. You'll see this later when we do error analysis. You can use these categories for a kind of out,",
    "start": "3457620",
    "end": "3463335"
  },
  {
    "text": "out of the box error analysis. You can ask for a bunch of different systems, which of these categories are they doing well on and where are they failing?",
    "start": "3463335",
    "end": "3471030"
  },
  {
    "text": "And that's often very illuminating. Okay. And just to round this out.",
    "start": "3471030",
    "end": "3478395"
  },
  {
    "start": "3475000",
    "end": "3550000"
  },
  {
    "text": "Just so you know about it, this is also in those notebooks. Here is some code for dealing with these datasets.",
    "start": "3478395",
    "end": "3484560"
  },
  {
    "text": "Um, you have basically snli_train_reader, snli_dev_reader, MultiNLITrainReader,",
    "start": "3484560",
    "end": "3490890"
  },
  {
    "text": "and then you have the two dev readers matched and mismatched. And so those should be good like easy ways for you to",
    "start": "3490890",
    "end": "3498180"
  },
  {
    "text": "interact with the data and feed it into machine learning systems. And I think the only thing you have to get used to is,",
    "start": "3498180",
    "end": "3503835"
  },
  {
    "text": "they yield what are called example objects. You can see that happening I guess here.",
    "start": "3503835",
    "end": "3509355"
  },
  {
    "text": "And that's because the underlying data has not only the strings and the relationship but also if it was multiply annotated all the",
    "start": "3509355",
    "end": "3517920"
  },
  {
    "text": "decisions the annotators made and also binary and non-binary parsers of the examples which are",
    "start": "3517920",
    "end": "3524430"
  },
  {
    "text": "automatic parses that come from Stanford's CoreNLP package. Um, but you know they,",
    "start": "3524430",
    "end": "3530085"
  },
  {
    "text": "they're good parsers by and large and worked well. And that gives you access to very rich tree structures if you want to make use of them.",
    "start": "3530085",
    "end": "3537360"
  },
  {
    "text": "And so that's a bit of a glimpse of how you deal with those example objects. And here's another example.",
    "start": "3537360",
    "end": "3544335"
  },
  {
    "text": "I know it's small but it's there for you to download and check out if you'd like.",
    "start": "3544335",
    "end": "3548619"
  },
  {
    "start": "3550000",
    "end": "3570000"
  },
  {
    "text": "And then here's a bit of additional code that just shows you the interface that we've provided for dealing with the annotated subsets,",
    "start": "3550370",
    "end": "3557940"
  },
  {
    "text": "so that there are no obstacles to making use of it. Because I think it's really exciting to stress test",
    "start": "3557940",
    "end": "3563220"
  },
  {
    "text": "your system by looking at how it does on different classes of problem, and they've made that very easy for us.",
    "start": "3563220",
    "end": "3569230"
  },
  {
    "text": "Excellent. Any questions about those datasets at any level before we started diving into the modeling parts of this?",
    "start": "3571790",
    "end": "3578589"
  },
  {
    "text": "We'll kind of embark on our familiar plot then. So the first thing that we'll do is look at some hand built feature functions and",
    "start": "3580040",
    "end": "3587130"
  },
  {
    "text": "some kind of classical linear models and then we'll move into deep learning. What I've done here is defined just two very simple baselines. Well that's nice.",
    "start": "3587130",
    "end": "3596610"
  },
  {
    "text": "I made that bigger. Um, that you see a lot in the literature. The first is what I've called word_overlap_phi.",
    "start": "3596610",
    "end": "3603795"
  },
  {
    "text": "And it's just creating a feature for every pair, every word that occurs both in the premise and the hypothesis.",
    "start": "3603795",
    "end": "3611580"
  },
  {
    "text": "So that gives you pretty sparse models. And they're pretty small because you're really looking at only the overlapping words.",
    "start": "3611580",
    "end": "3619545"
  },
  {
    "text": "You can contrast that with word_cross_product. And what that does,",
    "start": "3619545",
    "end": "3624615"
  },
  {
    "text": "is create a feature for every pair of words that occurs in the premise and the hypothesis.",
    "start": "3624615",
    "end": "3630480"
  },
  {
    "text": "So you know the, the full cross product in the sense if you think about them as two sets of words you look at every pair that you can draw from that pair of sets.",
    "start": "3630480",
    "end": "3638745"
  },
  {
    "text": "And that leads to really enormous feature spaces. Uh, I think that if you fit a model with this feature function on all of SLNI,",
    "start": "3638745",
    "end": "3648089"
  },
  {
    "text": "the training data, you end up with, uh, well over a million and a half features. So very high-dimensional very sparse feature space.",
    "start": "3648090",
    "end": "3656950"
  },
  {
    "text": "Uh, yeah here's a way of looking at that. So like if I have Tobi is a dog,",
    "start": "3657020",
    "end": "3662220"
  },
  {
    "text": "and Tobi is a big dog, then the word overlap thing just keeps track of Tobi,",
    "start": "3662220",
    "end": "3668490"
  },
  {
    "text": "dog, is, and a, and it drops out big. Whereas the word, oh, um, word cross-product thing creates",
    "start": "3668490",
    "end": "3675570"
  },
  {
    "text": "this enormous list of all the pairs that I can form from those two, um, sets of words, with their counts.",
    "start": "3675570",
    "end": "3683940"
  },
  {
    "text": "So much, much bigger [LAUGHTER].",
    "start": "3683940",
    "end": "3690540"
  },
  {
    "text": "Those are kinda baseline things. Here's something else, a whole class of features that you might think about that could come from WordNet.",
    "start": "3690540",
    "end": "3696450"
  },
  {
    "text": "So as I said before WordNet is a kind of very high-fidelity handcrafted resource full of",
    "start": "3696450",
    "end": "3703800"
  },
  {
    "text": "information about word-level relationships, in particular entailment relationships and",
    "start": "3703800",
    "end": "3708840"
  },
  {
    "text": "some contradiction information that is in the form of antonyms. Um, and what I've done here is just give you",
    "start": "3708840",
    "end": "3716010"
  },
  {
    "text": "a little bit of code that shows you how you might create feature functions relying on the WordNet hierarchy.",
    "start": "3716010",
    "end": "3722190"
  },
  {
    "text": "So as a quick example, if I just look up puppy, uh, I get all its synsets and they're like dog,",
    "start": "3722190",
    "end": "3729240"
  },
  {
    "text": "pup and young person. So you can see that the sense of the word puppy is kind of ambiguous and WordNet is capturing that.",
    "start": "3729240",
    "end": "3736545"
  },
  {
    "text": "If you go for just the first of those synsets then you get a kind of more conservative perspective because the first sense I think is the most frequent,",
    "start": "3736545",
    "end": "3745140"
  },
  {
    "text": "uh, according to their data. And so from that, and actually there's lots of methods you could use,",
    "start": "3745140",
    "end": "3751635"
  },
  {
    "text": "hypernym, hyponym and so forth. Here's a little example of how to create a WordNet feature.",
    "start": "3751635",
    "end": "3757515"
  },
  {
    "text": "And I've done it for hypernyms and hyponyms and I'll just show you an example. So again if I have the puppy moved and the dog danced,",
    "start": "3757515",
    "end": "3765585"
  },
  {
    "text": "then hypernym features would give me a feature for puppy and dog. And hyponym features would give me moved and danced.",
    "start": "3765585",
    "end": "3772994"
  },
  {
    "text": "Um, and you could do- with this code here, you could do this for lots of other WordNet relationships.",
    "start": "3772995",
    "end": "3778859"
  },
  {
    "text": "And I think carve out pretty interesting subspaces, uh, of like lexical entailment and contradiction and so forth.",
    "start": "3778860",
    "end": "3787690"
  },
  {
    "text": "And here's some other ideas. So you can do lots more stuff with WordNet. You could also think about some edit distance features",
    "start": "3788990",
    "end": "3796320"
  },
  {
    "text": "between the premise and hypothesis maybe at the word level. Um, I showed you word overlap but maybe",
    "start": "3796320",
    "end": "3802349"
  },
  {
    "text": "it's even more natural to think about word differences. Maybe the much more of the story is told by what is in the hypothesis but not in the premise or the reverse.",
    "start": "3802350",
    "end": "3811155"
  },
  {
    "text": "Um, alignment based features that would key into an older idea from NLI that it's kind of important to figure out how the two,",
    "start": "3811155",
    "end": "3818595"
  },
  {
    "text": "the premise and hypothesis relate to each other. Um, negation of course I think it's",
    "start": "3818595",
    "end": "3825000"
  },
  {
    "text": "worthwhile having separate features that would key into negation as well as other lexical classes because all of this stuff is",
    "start": "3825000",
    "end": "3830565"
  },
  {
    "text": "really deeply affecting entailment you know things like verbs of saying and attitudes and modals and hedges and all that stuff.",
    "start": "3830565",
    "end": "3837900"
  },
  {
    "text": "A lot of stuff that, that I think is relevant for sentiment carries over here because of the way it modulates claims that we make.",
    "start": "3837900",
    "end": "3844710"
  },
  {
    "text": "Quantify our relationships. Of course you'd want to capture those. They are not in WordNet but definitely worth maybe even the effort of hand coding them.",
    "start": "3844710",
    "end": "3853170"
  },
  {
    "text": "And then you could also have named entity features. That's kind of like the James Dean, Jimmy Dean, James Byron Dean situation.",
    "start": "3853170",
    "end": "3861369"
  },
  {
    "text": "And this is not meant to be exhaustive rather just to get you thinking about different ways that you could key into lexical information,",
    "start": "3861370",
    "end": "3868039"
  },
  {
    "text": "constructional information and also world knowledge. All that stuff should be brought in. Given the way we frame the NLI problem.",
    "start": "3868039",
    "end": "3876089"
  },
  {
    "text": "Questions about that or comments or ideas?",
    "start": "3878120",
    "end": "3883120"
  },
  {
    "text": "All right. NLI experiment. Again, this is the- the pattern that's repeating for all of this code stuff.",
    "start": "3886500",
    "end": "3893110"
  },
  {
    "text": "It's kind of standard language for thinking about running experiments in a productionized way. And I've provided a bunch of code for that.",
    "start": "3893110",
    "end": "3899770"
  },
  {
    "text": "Let me just walk through it at a high level, just to show you how easy this could be. So as usual, you point your repository to the,",
    "start": "3899770",
    "end": "3907720"
  },
  {
    "text": "um, data, here that's SNLI_Home. Here, just to illustrate,",
    "start": "3907720",
    "end": "3912760"
  },
  {
    "text": "I've defined word_ overlap_ phi as my feature function. As usual, we have these wrappers,",
    "start": "3912760",
    "end": "3918789"
  },
  {
    "text": "here it's fit_softmax, um, so that you can as part of the experiment pipeline, do cross-validation and fiddle with the parameters and do",
    "start": "3918790",
    "end": "3926800"
  },
  {
    "text": "other things that involves- that are part of model setup. Slightly different from the Stanford Sentiment Tree-bank,",
    "start": "3926800",
    "end": "3934569"
  },
  {
    "text": "we have these reader objects. Uh, so here I've set up train_reader_10, and the reason for that is they have this extra argument",
    "start": "3934570",
    "end": "3941349"
  },
  {
    "text": "called sample percentage, samp percentage. Ah, and here I've set it at 0.1,",
    "start": "3941350",
    "end": "3946915"
  },
  {
    "text": "I think that's really important for development because if you leave that off or say samp percentage equals none,",
    "start": "3946915",
    "end": "3952780"
  },
  {
    "text": "then your experiments are gonna be run on the full 550K examples,",
    "start": "3952780",
    "end": "3957880"
  },
  {
    "text": "and your progress will slow to a crawl because every experiment might take hours or days.",
    "start": "3957880",
    "end": "3963730"
  },
  {
    "text": "So what I would encourage as at least one way that you would develop on this dataset,",
    "start": "3963730",
    "end": "3968815"
  },
  {
    "text": "is to do it on small subsets which are randomly selected, and then once you feel like your system has stabilized,",
    "start": "3968815",
    "end": "3974650"
  },
  {
    "text": "expand out to a larger, um, chunk of the problem. So you set up that train reader and then it's gonna look a lot like the SST, right?",
    "start": "3974650",
    "end": "3983680"
  },
  {
    "text": "So you have the train reader argument, your feature function word_overlap_phi and fit_softmax,",
    "start": "3983680",
    "end": "3989755"
  },
  {
    "text": "it's like a Swiss Army knife you have a lot of other options, but those can be left off. And that's pretty much all it takes to run an experiment.",
    "start": "3989755",
    "end": "3997090"
  },
  {
    "text": "And as before, basic experiment, there is a dictionary that more or less stores",
    "start": "3997090",
    "end": "4002490"
  },
  {
    "text": "all the information about your experiment for reproducibility and error analysis, and so forth.",
    "start": "4002490",
    "end": "4007710"
  },
  {
    "text": "Does that make sense? That's the basic framework, very familiar, I think. Let me show you two other things that you- that I would encourage.",
    "start": "4007710",
    "end": "4015210"
  },
  {
    "text": "So first, hyperparameter selection on train subsets. So again, so unlike SST where it's small enough",
    "start": "4015210",
    "end": "4021750"
  },
  {
    "text": "that you can kind of do a lot of experiments in a pretty manageable way, when you move to the level of SNLI,",
    "start": "4021750",
    "end": "4027329"
  },
  {
    "text": "every experiment is a major investment, and again, you just won't make that much progress if you're not thinking about",
    "start": "4027330",
    "end": "4034290"
  },
  {
    "text": "how to do smaller stuff that will inform the full picture. So I'm gonna show you or give you- suggest to you two ways",
    "start": "4034290",
    "end": "4040860"
  },
  {
    "text": "that you might do that when it comes to hyperparameter selection. So first, let me just set this out.",
    "start": "4040860",
    "end": "4046620"
  },
  {
    "text": "This will be on train subset, so point your data, you know, SNLI_Home, as usual,",
    "start": "4046620",
    "end": "4052650"
  },
  {
    "text": "word_overlap_phi will be my feature function, fit_softmax_with_crossvalidation,",
    "start": "4052650",
    "end": "4058589"
  },
  {
    "text": "that's gonna look at a lot of different values of the regularization strength C, and both penalty types l1 and l2.",
    "start": "4058590",
    "end": "4065490"
  },
  {
    "text": "So that's a pretty big grid of hyperparameters, you have to fit a lot of models, especially with cv at 3 there.",
    "start": "4065490",
    "end": "4072390"
  },
  {
    "text": "So that's gonna have to fit a lot of models. And if again, if you do it on the full dataset, it will just take too long.",
    "start": "4072390",
    "end": "4077730"
  },
  {
    "text": "So one approach you might take is to set those hyperparameters based on a small subset of the data,",
    "start": "4077730",
    "end": "4083174"
  },
  {
    "text": "here I've done it at 10%, and you can see they get printed there. So this model chose C of 1 and penalty of l2.",
    "start": "4083175",
    "end": "4090540"
  },
  {
    "text": "And then having made that selection, you could play it forward, right?",
    "start": "4090540",
    "end": "4095820"
  },
  {
    "text": "So here- now, same feature function, but now it's fit_softmax classifier with pre-selected parameters,",
    "start": "4095820",
    "end": "4102060"
  },
  {
    "text": "where I just hard-code in the values that I selected from my earlier experiment.",
    "start": "4102060",
    "end": "4107490"
  },
  {
    "text": "And then you go forward. And in that way, you got some of the benefits of exploring the hyperparameter space without all the costs.",
    "start": "4107490",
    "end": "4114989"
  },
  {
    "text": "[NOISE] Makes sense? Any questions or comments or concerns you wanna bring out?",
    "start": "4114990",
    "end": "4122370"
  },
  {
    "text": "The random sampling method, [NOISE] is it like- just like assuming there's subclasses of questions like say",
    "start": "4122370",
    "end": "4129330"
  },
  {
    "text": "you need like, entailment, contradiction neutral, [inaudible] [NOISE] full number each of the matching distributions based on the datasets or it's truly random?",
    "start": "4129330",
    "end": "4137250"
  },
  {
    "text": "[NOISE] It's truly random but because the dataset is almost exactly balanced,",
    "start": "4137250",
    "end": "4142290"
  },
  {
    "text": "you get balanced subsets out. But I think you're raising an interesting weakness of the approach I just sketched,",
    "start": "4142290",
    "end": "4149250"
  },
  {
    "text": "which is, that 10%, how do I know how representative it is? It- is it of the true full dataset?",
    "start": "4149250",
    "end": "4155609"
  },
  {
    "text": "And in particular, the feature space that you form on that tiny subset might be very",
    "start": "4155610",
    "end": "4161190"
  },
  {
    "text": "different from the one that you get from the full training set. And for example, that might deeply affect how much regularization you wanna do,",
    "start": "4161190",
    "end": "4168975"
  },
  {
    "text": "um, and I think that's a weakness of the current approach. That makes sense?",
    "start": "4168975",
    "end": "4174089"
  },
  {
    "text": "So that's- that's just- just to be- like that's a compromise of- of that one.",
    "start": "4174090",
    "end": "4179889"
  },
  {
    "text": "Here's another method that you might use that has not that weakness but maybe",
    "start": "4179980",
    "end": "4185359"
  },
  {
    "text": "others and this would be hyperparameter selection with a few iterations. So all of these machine-learning models,",
    "start": "4185360",
    "end": "4192410"
  },
  {
    "text": "they go through a certain number of epochs of training. Uh, and, you know,",
    "start": "4192410",
    "end": "4197880"
  },
  {
    "text": "a single epoch might be manageable, but if it takes 1000 for your model to converge, that's where you're incurring the real costs.",
    "start": "4197880",
    "end": "4204840"
  },
  {
    "text": "So another approach you might take would be to, as I've done here, fit_softmax_with_crossvalidation_small_iter,",
    "start": "4204840",
    "end": "4212340"
  },
  {
    "text": "where I just find the parameter for the model I'm fitting, there it's max_iter there for logistic regression,",
    "start": "4212340",
    "end": "4217755"
  },
  {
    "text": "and set it to a very small value, like 3 or 5. And then I do the full grid search that I wanted to do before,",
    "start": "4217755",
    "end": "4225300"
  },
  {
    "text": "it's just that that takes much less time even if I do it over the whole dataset, as I'm doing down here because the-the iteration number is so low.",
    "start": "4225300",
    "end": "4234760"
  },
  {
    "text": "And again, you get a report down here. Here it shows l1 as the regularization parameter, not l2,",
    "start": "4234920",
    "end": "4242534"
  },
  {
    "text": "and the performance is slightly better, and then you apply that forward in the way that I showed you before.",
    "start": "4242535",
    "end": "4250409"
  },
  {
    "text": "So the implicit assumption you're making there is that the future for training epochs is gonna be like the past,",
    "start": "4250410",
    "end": "4258360"
  },
  {
    "text": "that a setting that was really bad early on is not gonna somehow emerge as the hero of this story later.",
    "start": "4258360",
    "end": "4265860"
  },
  {
    "text": "Once bad, always bad. And conversely, the ones that are good right out of the gate, are probably the ones that will be good even if you run the model to convergence.",
    "start": "4265860",
    "end": "4274469"
  },
  {
    "text": "I'm not offering you a guarantee because I think no such guarantee could be offered, but rather that this might be",
    "start": "4274470",
    "end": "4281550"
  },
  {
    "text": "a good compromise given a budget that you have on training time, or on compute resources, and so forth.",
    "start": "4281550",
    "end": "4289300"
  },
  {
    "text": "And you could mix and match these because both of these things, in the grand scheme of things,",
    "start": "4289400",
    "end": "4294510"
  },
  {
    "text": "are very, very quick compared to fitting a model on the entire dataset.",
    "start": "4294510",
    "end": "4299710"
  },
  {
    "text": "One more thing here and then we might wrap up. I just wanted to show you how easy it is in the framework tests to,",
    "start": "4301190",
    "end": "4308940"
  },
  {
    "text": "um, uh, set up and run a hypothesis-only experiment. Um, and I think this is pretty obvious.",
    "start": "4308940",
    "end": "4314400"
  },
  {
    "text": "There's the feature function, hypothesis_only_unigrams_phi takes in two trees, t1 and t2, but then it operates only on t2.",
    "start": "4314400",
    "end": "4322965"
  },
  {
    "text": "It just throws away t1. Uh, I used fits_softmax_classifier_with_preselected_params,",
    "start": "4322965",
    "end": "4330719"
  },
  {
    "text": "I ran the experiment, and lo and behold, 65.3 for the macro f1, right?",
    "start": "4330720",
    "end": "4337065"
  },
  {
    "text": "That is very different from a random baseline which would be like 33, 34%.",
    "start": "4337065",
    "end": "4342660"
  },
  {
    "text": "That shows you how powerful this can be. So if your model is at 68 and you feel like, oh that's wonderful compared to random,",
    "start": "4342660",
    "end": "4349830"
  },
  {
    "text": "well you should contextualize it in this way [LAUGHTER]. Um, I hope that's not dispiriting [LAUGHTER].",
    "start": "4349830",
    "end": "4356280"
  },
  {
    "text": "And I- i- i- it's worth checking to see how much information is in the premise, actually.",
    "start": "4356280",
    "end": "4361815"
  },
  {
    "text": "Uh, I'm gonna do that as a follow-up.",
    "start": "4361815",
    "end": "4364719"
  },
  {
    "text": "Well, yeah, there it is compared to random. And actually, here I just wanted to show you, just in case you didn't know this,",
    "start": "4369170",
    "end": "4374789"
  },
  {
    "text": "that scikit has all these great dummy classifier models, that allow you to fit models that just make use of",
    "start": "4374790",
    "end": "4381210"
  },
  {
    "text": "the like label distribution or just guess at random. Ah, and so here I just did the diligent thing of actually using one of those,",
    "start": "4381210",
    "end": "4388680"
  },
  {
    "text": "and sure enough, yeah, 33.4%. So the hypothesis-only baseline is a very strong baseline. Questions about that?",
    "start": "4388680",
    "end": "4400200"
  },
  {
    "text": "[inaudible] baseline, should we, um, uh,",
    "start": "4400200",
    "end": "4407520"
  },
  {
    "text": "should we make the baseline the- doing all the exact same operations as our model but only on the second tree,",
    "start": "4407520",
    "end": "4414930"
  },
  {
    "text": "or should it be just like the counter? Well, that's a great question. So like what is the- given the model that you're developing,",
    "start": "4414930",
    "end": "4420990"
  },
  {
    "text": "what is the true nature of the hypothesis-only baseline? Um, that's a judgment call.",
    "start": "4420990",
    "end": "4427140"
  },
  {
    "text": "Um, because, for example, if I picked the word cross-product features as my basic feature function.",
    "start": "4427140",
    "end": "4434010"
  },
  {
    "text": "That's undefined If I'm looking only at the hypothesis, and so I'd have to say like, well, the unigrams are pretty close to being like my true feature function.",
    "start": "4434010",
    "end": "4442935"
  },
  {
    "text": "Um, but it's - it's a tough thing to say if you've- especially if you do- if you define a lot of features that make use of both the premise and the hypothesis.",
    "start": "4442935",
    "end": "4450855"
  },
  {
    "text": "But if it's handled features, I might just default to unigrams. It seems pretty plausible.",
    "start": "4450855",
    "end": "4456930"
  },
  {
    "text": "And then, for the deep learning models, I think these questions are typically more straightforward.",
    "start": "4456930",
    "end": "4462040"
  },
  {
    "text": "Good. I think rather than diving into those, um, let me leave this for next time.",
    "start": "4465080",
    "end": "4470220"
  },
  {
    "text": "So next time, we're- we're gonna look at sentencing coding models, where you get a separate representation for premise and hypothesis,",
    "start": "4470220",
    "end": "4476639"
  },
  {
    "text": "and then we'll look at chain ones which more or less just put the two together into a single, typically an RNN of some kind.",
    "start": "4476640",
    "end": "4483800"
  },
  {
    "text": "And then we'll look at attention mechanisms and do some error analysis. And if we get through that, as usual, leaves- we'll leave some time for you to just code on",
    "start": "4483800",
    "end": "4491420"
  },
  {
    "text": "Homework 4 with the teaching team available for you. Great.",
    "start": "4491420",
    "end": "4495520"
  }
]