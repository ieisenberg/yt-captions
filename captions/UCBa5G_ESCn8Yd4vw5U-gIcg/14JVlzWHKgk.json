[
  {
    "text": "So let's look again in the\nsimple case when we've got",
    "start": "0",
    "end": "3490"
  },
  {
    "text": "one variable, one x.",
    "start": "3490",
    "end": "7210"
  },
  {
    "text": "Again, we're going to\nget a bit more mathy now.",
    "start": "7210",
    "end": "9610"
  },
  {
    "text": "This is the form, the\nmathematical form,",
    "start": "9610",
    "end": "12580"
  },
  {
    "text": "of the Gaussian density\nfunction for class k",
    "start": "12580",
    "end": "16900"
  },
  {
    "text": "when you've got a single x.",
    "start": "16900",
    "end": "19029"
  },
  {
    "text": "So there's some\nconstants over here.",
    "start": "19030",
    "end": "21580"
  },
  {
    "text": "The important part\nthat depends on x",
    "start": "21580",
    "end": "23680"
  },
  {
    "text": "is in this exponential\nform over here.",
    "start": "23680",
    "end": "26620"
  },
  {
    "text": "And we see there's\na new sub k, which",
    "start": "26620",
    "end": "29560"
  },
  {
    "text": "is the mean for\nobservations in class k",
    "start": "29560",
    "end": "32680"
  },
  {
    "text": "or the population\nmean in class k,",
    "start": "32680",
    "end": "34450"
  },
  {
    "text": "and sigma sub k is the variance\nfor that variable in class k.",
    "start": "34450",
    "end": "41790"
  },
  {
    "text": "Now, in the first\ninstance, we can",
    "start": "41790",
    "end": "44280"
  },
  {
    "text": "assume that the\nvariance, sigma sub k,",
    "start": "44280",
    "end": "47219"
  },
  {
    "text": "is actually the sigma, the\nsame in each of the classes.",
    "start": "47220",
    "end": "50460"
  },
  {
    "text": "Now, that's a convenience.",
    "start": "50460",
    "end": "52860"
  },
  {
    "text": "It turns out it's an\nimportant convenience",
    "start": "52860",
    "end": "54930"
  },
  {
    "text": "and it's going to determine\nwhether the discriminant",
    "start": "54930",
    "end": "57540"
  },
  {
    "text": "function that we get, the\ndiscriminant analysis,",
    "start": "57540",
    "end": "60300"
  },
  {
    "text": "gives us linear functions\nor quadratic functions.",
    "start": "60300",
    "end": "65140"
  },
  {
    "text": "So if we plug into\nBayes' formula,",
    "start": "65140",
    "end": "70130"
  },
  {
    "text": "the formula we had\non two slides back,",
    "start": "70130",
    "end": "72469"
  },
  {
    "text": "we get a rather\ncomplicated expression.",
    "start": "72470",
    "end": "75080"
  },
  {
    "text": "So we've just\nplugged in the form",
    "start": "75080",
    "end": "77150"
  },
  {
    "text": "of the density in the\nnumerator and there's",
    "start": "77150",
    "end": "79100"
  },
  {
    "text": "the sum over the classes\nin the denominator,",
    "start": "79100",
    "end": "82240"
  },
  {
    "text": "and it looks pretty nasty.",
    "start": "82240",
    "end": "84280"
  },
  {
    "text": "Well, luckily, there's\nsome simplifications",
    "start": "84280",
    "end": "86950"
  },
  {
    "text": "and cancellations.",
    "start": "86950",
    "end": "88340"
  },
  {
    "start": "88340",
    "end": "91369"
  },
  {
    "text": "Now we get this because\nto classify an observation",
    "start": "91370",
    "end": "95090"
  },
  {
    "text": "to a class, we don't need\nto necessarily evaluate",
    "start": "95090",
    "end": "98149"
  },
  {
    "text": "the probabilities.",
    "start": "98150",
    "end": "99030"
  },
  {
    "text": "We just need to see\nwhich is the largest.",
    "start": "99030",
    "end": "101750"
  },
  {
    "text": "So if we take logs--",
    "start": "101750",
    "end": "103408"
  },
  {
    "text": "whenever you see exponentials,\nthe first thing you want to do",
    "start": "103408",
    "end": "105950"
  },
  {
    "text": "is take log logs.",
    "start": "105950",
    "end": "107539"
  },
  {
    "text": "And if we discard terms\nthat do not depend on k--",
    "start": "107540",
    "end": "110510"
  },
  {
    "start": "110510",
    "end": "113360"
  },
  {
    "text": "so that's amounts to doing a\nlot of cancellation of things,",
    "start": "113360",
    "end": "116210"
  },
  {
    "text": "of terms that don't count.",
    "start": "116210",
    "end": "120260"
  },
  {
    "text": "This is equivalent to assigning\nto the class with the largest",
    "start": "120260",
    "end": "123770"
  },
  {
    "text": "discriminant score.",
    "start": "123770",
    "end": "125299"
  },
  {
    "text": "And so that\ncomplicated expression",
    "start": "125300",
    "end": "127250"
  },
  {
    "text": "boils down to a much simpler\nexpression over here.",
    "start": "127250",
    "end": "131420"
  },
  {
    "text": "And notice it involves x, the\nsingle variable in this case.",
    "start": "131420",
    "end": "137550"
  },
  {
    "text": "And then it involves\nthe mean and variance",
    "start": "137550",
    "end": "140000"
  },
  {
    "text": "for the distribution, and\nit involves the prior.",
    "start": "140000",
    "end": "144210"
  },
  {
    "text": "And importantly, in this case,\nthis is a linear function of x.",
    "start": "144210",
    "end": "148800"
  },
  {
    "text": "So there's a single coefficient\nfor x, there's a constant,",
    "start": "148800",
    "end": "152400"
  },
  {
    "text": "and then there's a constant\nterm, which consists",
    "start": "152400",
    "end": "155670"
  },
  {
    "text": "of these two pieces over here.",
    "start": "155670",
    "end": "158819"
  },
  {
    "text": "And so we get one\nof those functions",
    "start": "158820",
    "end": "160580"
  },
  {
    "text": "for each of the classes.",
    "start": "160580",
    "end": "162890"
  },
  {
    "text": "If they're two classes, you\ncan simplify even further.",
    "start": "162890",
    "end": "167430"
  },
  {
    "text": "And let's suppose now that\nthe probability of class 1",
    "start": "167430",
    "end": "171719"
  },
  {
    "text": "is equal to the probability\nof class 2, which is 0.05.",
    "start": "171720",
    "end": "176450"
  },
  {
    "text": "Then you can see in this case\nthat the decision boundary",
    "start": "176450",
    "end": "179300"
  },
  {
    "text": "is exactly at mu 1\nplus mu 2 over 2.",
    "start": "179300",
    "end": "184140"
  },
  {
    "text": "So it's back to this picture\nin the previous slide.",
    "start": "184140",
    "end": "191910"
  },
  {
    "text": "In this case, the\npriors were equal.",
    "start": "191910",
    "end": "193830"
  },
  {
    "text": "These are actually two\nGaussian distributions,",
    "start": "193830",
    "end": "196520"
  },
  {
    "text": "and the decision\nboundary here is at 0.",
    "start": "196520",
    "end": "199490"
  },
  {
    "text": "In this case, the two means\nwere exactly the equal amount",
    "start": "199490",
    "end": "202220"
  },
  {
    "text": "on the opposite side of 0.",
    "start": "202220",
    "end": "203550"
  },
  {
    "text": "So the average is at 0.",
    "start": "203550",
    "end": "205610"
  },
  {
    "text": "So intuitively, that's the right\nvalue for the decision boundary,",
    "start": "205610",
    "end": "210260"
  },
  {
    "text": "which is the point at which\nwe classify it to one class,",
    "start": "210260",
    "end": "213260"
  },
  {
    "text": "the boundary at which we\nswitch from classify into one",
    "start": "213260",
    "end": "215510"
  },
  {
    "text": "class versus the other.",
    "start": "215510",
    "end": "218739"
  },
  {
    "text": "It's not that hard to show.",
    "start": "218740",
    "end": "220100"
  },
  {
    "text": "So see if you can show that.",
    "start": "220100",
    "end": "221770"
  },
  {
    "text": "You basically use\nthis expression",
    "start": "221770",
    "end": "223870"
  },
  {
    "text": "for each of the two\nclasses and look",
    "start": "223870",
    "end": "226629"
  },
  {
    "text": "to see when the one's\nbigger than the other.",
    "start": "226630",
    "end": "229710"
  },
  {
    "text": "Not that hard to do.",
    "start": "229710",
    "end": "231070"
  },
  {
    "text": "I'm confused.",
    "start": "231070",
    "end": "231830"
  },
  {
    "text": "There was a square term\nin the previous expression",
    "start": "231830",
    "end": "234790"
  },
  {
    "text": "and it's gone.",
    "start": "234790",
    "end": "235680"
  },
  {
    "text": "Oh, Rob, are you causing\ntrouble here again?",
    "start": "235680",
    "end": "239950"
  },
  {
    "text": "I was hoping to\navoid that nasty bit.",
    "start": "239950",
    "end": "242650"
  },
  {
    "text": "Rob's right.",
    "start": "242650",
    "end": "244299"
  },
  {
    "text": "If you expand out\nthis square term here,",
    "start": "244300",
    "end": "247580"
  },
  {
    "text": "there's going to\nbe an x squared.",
    "start": "247580",
    "end": "250360"
  },
  {
    "text": "But there's an x\nsquared in the numerator",
    "start": "250360",
    "end": "254050"
  },
  {
    "text": "and there's x squared in each\nof the terms in the denominator.",
    "start": "254050",
    "end": "257750"
  },
  {
    "text": "And there's no\ncoefficients in front",
    "start": "257750",
    "end": "259540"
  },
  {
    "text": "of that x squared that's\nspecific to a class.",
    "start": "259540",
    "end": "262540"
  },
  {
    "text": "So that's one of the things\nthat cancel out in this ratio.",
    "start": "262540",
    "end": "265900"
  },
  {
    "text": "You knew that, didn't you?",
    "start": "265900",
    "end": "267729"
  },
  {
    "text": "Oh, yes.",
    "start": "267730",
    "end": "269360"
  },
  {
    "text": "Rob knew that.",
    "start": "269360",
    "end": "271629"
  },
  {
    "text": "All right, so this\nis with populations.",
    "start": "271630",
    "end": "274430"
  },
  {
    "text": "What happens if we have data?",
    "start": "274430",
    "end": "275919"
  },
  {
    "text": "We can't draw nice\ndensity functions",
    "start": "275920",
    "end": "278170"
  },
  {
    "text": "like we've done over here,\nbut we just estimate them.",
    "start": "278170",
    "end": "281480"
  },
  {
    "text": "So here's a picture where\nwe've actually got data.",
    "start": "281480",
    "end": "283820"
  },
  {
    "text": "So we've drawn histograms\ninstead of density functions.",
    "start": "283820",
    "end": "287650"
  },
  {
    "text": "Now what we do is we\nneed to estimate--",
    "start": "287650",
    "end": "291940"
  },
  {
    "text": "for the Gaussian rule, we\nneed to estimate the means",
    "start": "291940",
    "end": "295090"
  },
  {
    "text": "in the two populations and\nthe common standard deviation.",
    "start": "295090",
    "end": "298870"
  },
  {
    "text": "Well, in this case, the true\nmeans are minus 1.5 and 1.5,",
    "start": "298870",
    "end": "309120"
  },
  {
    "text": "which means the\naverage mean is 0.",
    "start": "309120",
    "end": "311320"
  },
  {
    "text": "And the probabilities were 0.05.",
    "start": "311320",
    "end": "315750"
  },
  {
    "text": "But we don't know these.",
    "start": "315750",
    "end": "317290"
  },
  {
    "text": "So we're going to estimate\nthem from the observed data,",
    "start": "317290",
    "end": "320620"
  },
  {
    "text": "and then plug them\ninto the rule.",
    "start": "320620",
    "end": "323169"
  },
  {
    "text": "So this is how we estimate them.",
    "start": "323170",
    "end": "326020"
  },
  {
    "text": "The priors we need to estimate.",
    "start": "326020",
    "end": "328009"
  },
  {
    "text": "So that's just the\nnumber in each class",
    "start": "328010",
    "end": "329950"
  },
  {
    "text": "divided by the total number.",
    "start": "329950",
    "end": "331400"
  },
  {
    "text": "That's obvious.",
    "start": "331400",
    "end": "332630"
  },
  {
    "text": "The means in each class,\nwe just compute the sample",
    "start": "332630",
    "end": "335170"
  },
  {
    "text": "mean in each of the classes.",
    "start": "335170",
    "end": "337240"
  },
  {
    "text": "This is a tricky\nlittle notation here.",
    "start": "337240",
    "end": "339530"
  },
  {
    "text": "This is 1 over nk.",
    "start": "339530",
    "end": "341720"
  },
  {
    "text": "That's the number in class k.",
    "start": "341720",
    "end": "343900"
  },
  {
    "text": "And this is a sum over i\nsuch that yi is equal to k.",
    "start": "343900",
    "end": "349250"
  },
  {
    "text": "So yi is recording\nthe class label.",
    "start": "349250",
    "end": "352310"
  },
  {
    "text": "So this will just sum those\nxi's that are in class k.",
    "start": "352310",
    "end": "356200"
  },
  {
    "text": "And clearly that's\nthe right thing",
    "start": "356200",
    "end": "357910"
  },
  {
    "text": "to do to get the sample mean.",
    "start": "357910",
    "end": "361260"
  },
  {
    "text": "The sigma squared is\na little trickier.",
    "start": "361260",
    "end": "363840"
  },
  {
    "text": "We assume the variance is the\nsame in each of the classes.",
    "start": "363840",
    "end": "368220"
  },
  {
    "text": "And so this is a formula, it's\ncalled the pooled variance",
    "start": "368220",
    "end": "372870"
  },
  {
    "text": "estimate.",
    "start": "372870",
    "end": "373830"
  },
  {
    "text": "So we subtract from each\nxi the mean for its class,",
    "start": "373830",
    "end": "380319"
  },
  {
    "text": "so this is what we do if we were\ncomputing the variance in class",
    "start": "380320",
    "end": "383620"
  },
  {
    "text": "k, but we sum all those\nsquared differences",
    "start": "383620",
    "end": "388080"
  },
  {
    "text": "and we sum them over\nall the classes,",
    "start": "388080",
    "end": "390090"
  },
  {
    "text": "and then divide it by n minus k.",
    "start": "390090",
    "end": "395580"
  },
  {
    "text": "So if that doesn't\nmake too much sense,",
    "start": "395580",
    "end": "400110"
  },
  {
    "text": "another way of writing that is\nin this form over here, which",
    "start": "400110",
    "end": "403889"
  },
  {
    "text": "says we estimate the\nsample variance separately",
    "start": "403890",
    "end": "408360"
  },
  {
    "text": "in each of the\nclasses, and then we",
    "start": "408360",
    "end": "411150"
  },
  {
    "text": "average them using\nthis formula over here.",
    "start": "411150",
    "end": "413910"
  },
  {
    "text": "So this is just like a weight\non each of those variances,",
    "start": "413910",
    "end": "417083"
  },
  {
    "text": "and that weights to do with\nhow many observations we",
    "start": "417083",
    "end": "419250"
  },
  {
    "text": "in that class relative to the\ntotal number of observations.",
    "start": "419250",
    "end": "423900"
  },
  {
    "text": "And then the minus 1 and the\nminus k, that's a detailed",
    "start": "423900",
    "end": "427530"
  },
  {
    "text": "and it's to do with how many\nparameters we've estimated",
    "start": "427530",
    "end": "430680"
  },
  {
    "text": "for each of these estimates.",
    "start": "430680",
    "end": "432580"
  },
  {
    "text": "There's one parameter.",
    "start": "432580",
    "end": "433720"
  },
  {
    "text": "It's the mean.",
    "start": "433720",
    "end": "436200"
  },
  {
    "text": "Rob's falling asleep.",
    "start": "436200",
    "end": "437260"
  },
  {
    "text": "Sorry.",
    "start": "437260",
    "end": "437760"
  },
  {
    "text": "Too much detail, Rob?",
    "start": "437760",
    "end": "438635"
  },
  {
    "text": "Yeah, exactly.",
    "start": "438635",
    "end": "439218"
  },
  {
    "text": "Way too much detail.",
    "start": "439218",
    "end": "440189"
  },
  {
    "text": "OK.",
    "start": "440190",
    "end": "441270"
  },
  {
    "text": "So there we have it.",
    "start": "441270",
    "end": "442180"
  },
  {
    "text": "Those are the formulas.",
    "start": "442180",
    "end": "443520"
  },
  {
    "text": "You plug those back in, you'll\nnow get estimated decision",
    "start": "443520",
    "end": "450660"
  },
  {
    "text": "boundary.",
    "start": "450660",
    "end": "451450"
  },
  {
    "text": "And instead of being\nexactly 0, in this case,",
    "start": "451450",
    "end": "454270"
  },
  {
    "text": "it's slightly to the left\nof 0, but pretty close.",
    "start": "454270",
    "end": "457460"
  }
]