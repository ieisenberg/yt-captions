[
  {
    "start": "0",
    "end": "30000"
  },
  {
    "start": "0",
    "end": "4058"
  },
  {
    "text": "CHRISTOPHER POTTS:\nWelcome, everyone.",
    "start": "4058",
    "end": "5600"
  },
  {
    "text": "This is part 7 in our series on\nsupervised sentiment analysis.",
    "start": "5600",
    "end": "8680"
  },
  {
    "text": "The focus of this screencast\nis on feature representation",
    "start": "8680",
    "end": "11770"
  },
  {
    "text": "of data.",
    "start": "11770",
    "end": "12288"
  },
  {
    "text": "There are really two\nthings I'd like to do.",
    "start": "12288",
    "end": "14080"
  },
  {
    "text": "First, just explore some\nideas for effective feature",
    "start": "14080",
    "end": "16780"
  },
  {
    "text": "representation in the context\nof sentiment analysis.",
    "start": "16780",
    "end": "19480"
  },
  {
    "text": "And, second, cover some of\nthe core technical concepts",
    "start": "19480",
    "end": "22449"
  },
  {
    "text": "that surround feature\nrepresentation that you'd",
    "start": "22450",
    "end": "24760"
  },
  {
    "text": "do well to have in mind as you\nwrite new feature functions",
    "start": "24760",
    "end": "27760"
  },
  {
    "text": "and optimize models.",
    "start": "27760",
    "end": "30280"
  },
  {
    "start": "30000",
    "end": "144000"
  },
  {
    "text": "Let's begin in a\nfamiliar place which",
    "start": "30280",
    "end": "31960"
  },
  {
    "text": "is N-gram feature functions.",
    "start": "31960",
    "end": "33620"
  },
  {
    "text": "To this point in the\nseries of screencasts,",
    "start": "33620",
    "end": "35980"
  },
  {
    "text": "I've been just focusing on\nunigram feature functions.",
    "start": "35980",
    "end": "39070"
  },
  {
    "text": "That's also called the\n\"bag-of-words\" model,",
    "start": "39070",
    "end": "41440"
  },
  {
    "text": "and we can easily generalize\nthat idea to bigrams,",
    "start": "41440",
    "end": "44980"
  },
  {
    "text": "and trigrams, and so forth.",
    "start": "44980",
    "end": "47150"
  },
  {
    "text": "All of these schemes\nwill be heavily",
    "start": "47150",
    "end": "48670"
  },
  {
    "text": "dependent on the\ntokenizer that you've",
    "start": "48670",
    "end": "50350"
  },
  {
    "text": "chosen because, of course in\nthe end, for every example",
    "start": "50350",
    "end": "52719"
  },
  {
    "text": "we represent, we are simply\ntokenizing that example",
    "start": "52720",
    "end": "55510"
  },
  {
    "text": "and then counting the\ntokens in that example.",
    "start": "55510",
    "end": "59899"
  },
  {
    "text": "This can be combined of course\nwith preprocessing steps.",
    "start": "59900",
    "end": "62530"
  },
  {
    "text": "In part 2 in this series, I\ncovered the preprocessing idea",
    "start": "62530",
    "end": "65830"
  },
  {
    "text": "of _NEG marking.",
    "start": "65830",
    "end": "66760"
  },
  {
    "text": "Which is essentially\nto mark words",
    "start": "66760",
    "end": "69460"
  },
  {
    "text": "as they appear in\na heuristic way",
    "start": "69460",
    "end": "71590"
  },
  {
    "text": "in the scope of\nnegative morphemes",
    "start": "71590",
    "end": "73780"
  },
  {
    "text": "as a way of indicating\nthat, for example, \"good\"",
    "start": "73780",
    "end": "76780"
  },
  {
    "text": "is positive in normal\ncontext but might",
    "start": "76780",
    "end": "79510"
  },
  {
    "text": "become negative when it is\nin the scope of a negation",
    "start": "79510",
    "end": "82870"
  },
  {
    "text": "like \"not\" or \"never.\"",
    "start": "82870",
    "end": "84562"
  },
  {
    "text": "We would handle that as a\npreprocessing step and that",
    "start": "84562",
    "end": "86770"
  },
  {
    "text": "would just create more unigrams\nthat our tokenizer would turn",
    "start": "86770",
    "end": "90759"
  },
  {
    "text": "into tokens and then would\nbe counted by these feature",
    "start": "90760",
    "end": "94120"
  },
  {
    "text": "representation schemes.",
    "start": "94120",
    "end": "97330"
  },
  {
    "text": "A hallmark of these\nfeature approaches",
    "start": "97330",
    "end": "99490"
  },
  {
    "text": "is that they create very\nlarge, very sparse feature",
    "start": "99490",
    "end": "102220"
  },
  {
    "text": "representations.",
    "start": "102220",
    "end": "103210"
  },
  {
    "text": "You are going to have a column\nin your feature representation",
    "start": "103210",
    "end": "106240"
  },
  {
    "text": "for every single word\nthat appears anywhere",
    "start": "106240",
    "end": "108640"
  },
  {
    "text": "in your training data.",
    "start": "108640",
    "end": "110392"
  },
  {
    "text": "And another important\nthing to keep",
    "start": "110392",
    "end": "111850"
  },
  {
    "text": "in mind about this approach\nis that, by and large, they",
    "start": "111850",
    "end": "114220"
  },
  {
    "text": "will fail to directly model\nrelationships between features",
    "start": "114220",
    "end": "117700"
  },
  {
    "text": "unless you make some special\neffort to effectively interact",
    "start": "117700",
    "end": "120680"
  },
  {
    "text": "these features.",
    "start": "120680",
    "end": "122485"
  },
  {
    "text": "All you'll be doing is studying\ntheir distribution with respect",
    "start": "122485",
    "end": "125110"
  },
  {
    "text": "to the class labels\nthat you have.",
    "start": "125110",
    "end": "127000"
  },
  {
    "text": "And it's very\nunlikely that you'll",
    "start": "127000",
    "end": "128830"
  },
  {
    "text": "recover, in any\ndeep way, the kind",
    "start": "128830",
    "end": "131170"
  },
  {
    "text": "of underlying synonymy of\nwords like \"couch\" and \"sofa,\"",
    "start": "131170",
    "end": "134380"
  },
  {
    "text": "for example.",
    "start": "134380",
    "end": "135470"
  },
  {
    "text": "And this is a\nshortcoming that we",
    "start": "135470",
    "end": "136960"
  },
  {
    "text": "might want to address as\nwe move into distributed",
    "start": "136960",
    "end": "140140"
  },
  {
    "text": "representations of\nexamples like deep learn.",
    "start": "140140",
    "end": "145030"
  },
  {
    "start": "144000",
    "end": "387000"
  },
  {
    "text": "So for our first\ntechnical concept,",
    "start": "145030",
    "end": "147209"
  },
  {
    "text": "I would like to just distinguish\nbetween feature functions",
    "start": "147210",
    "end": "150330"
  },
  {
    "text": "and features.",
    "start": "150330",
    "end": "151110"
  },
  {
    "text": "And to do this, I've just got\na fully worked out example",
    "start": "151110",
    "end": "153540"
  },
  {
    "text": "here using tools\nfrom scikit-learn",
    "start": "153540",
    "end": "155670"
  },
  {
    "text": "that I think will make the\nimportance of this distinction",
    "start": "155670",
    "end": "158520"
  },
  {
    "text": "really clear and concrete.",
    "start": "158520",
    "end": "160380"
  },
  {
    "text": "So in cell 1, I've just\nloaded a bunch of libraries.",
    "start": "160380",
    "end": "163320"
  },
  {
    "text": "In cell 2, I've got my\nstandard, kind of lazy unigrams",
    "start": "163320",
    "end": "166950"
  },
  {
    "text": "feature function, which is\ntaking in a stringed text,",
    "start": "166950",
    "end": "170099"
  },
  {
    "text": "downcasing it, and then simply\nsplitting on whitespace.",
    "start": "170100",
    "end": "173040"
  },
  {
    "text": "And then the counter\nhere is just turning that",
    "start": "173040",
    "end": "175200"
  },
  {
    "text": "into a count dictionary\nmapping each token",
    "start": "175200",
    "end": "177420"
  },
  {
    "text": "to the number of times that\nit appears in this example,",
    "start": "177420",
    "end": "180510"
  },
  {
    "text": "according to our tokenizer.",
    "start": "180510",
    "end": "182909"
  },
  {
    "text": "That would be fine for now.",
    "start": "182910",
    "end": "184160"
  },
  {
    "text": "In cell 3, I have a\ntiny little corpus that",
    "start": "184160",
    "end": "186680"
  },
  {
    "text": "has just two words, a and b.",
    "start": "186680",
    "end": "189640"
  },
  {
    "text": "In cell 4, I create a\nlist of dictionaries",
    "start": "189640",
    "end": "192340"
  },
  {
    "text": "by calling unigrams_phi on each\nof the texts in my corpus here.",
    "start": "192340",
    "end": "196670"
  },
  {
    "text": "So that gives me a list\nof count dictionaries.",
    "start": "196670",
    "end": "200290"
  },
  {
    "text": "In 5, I use a\nDictVectorizer, as covered",
    "start": "200290",
    "end": "203140"
  },
  {
    "text": "in a previous screencast.",
    "start": "203140",
    "end": "204360"
  },
  {
    "text": "And what that's going to do\nis when I call fit_transform",
    "start": "204360",
    "end": "207220"
  },
  {
    "text": "on my list of\nfeature dictionaries,",
    "start": "207220",
    "end": "209080"
  },
  {
    "text": "it will turn it\ninto a matrix, which",
    "start": "209080",
    "end": "211000"
  },
  {
    "text": "is the input that all of these\nscikit machine learning models",
    "start": "211000",
    "end": "214390"
  },
  {
    "text": "expect for their training data.",
    "start": "214390",
    "end": "216700"
  },
  {
    "text": "And in cell 7,\nI've just given you",
    "start": "216700",
    "end": "218319"
  },
  {
    "text": "what I hope is a pretty\nintuitive view of that design",
    "start": "218320",
    "end": "220880"
  },
  {
    "text": "matrix.",
    "start": "220880",
    "end": "221380"
  },
  {
    "text": "Underlyingly, it's\njust an NumPy array.",
    "start": "221380",
    "end": "224500"
  },
  {
    "text": "But if we use pandas, we can\nsee that the columns here",
    "start": "224500",
    "end": "227530"
  },
  {
    "text": "correspond to the names of\neach one of the features.",
    "start": "227530",
    "end": "230350"
  },
  {
    "text": "Because we have just two\nword types in our corpus,",
    "start": "230350",
    "end": "232720"
  },
  {
    "text": "there are two columns, a\nand b, and each of the rows",
    "start": "232720",
    "end": "235870"
  },
  {
    "text": "corresponds to an\nexample from our corpus.",
    "start": "235870",
    "end": "238060"
  },
  {
    "text": "And so you can see that our\nfirst example has been reduced",
    "start": "238060",
    "end": "240670"
  },
  {
    "text": "to a representation that has\n3 in its first dimension and 0",
    "start": "240670",
    "end": "244450"
  },
  {
    "text": "in its second, corresponding\nto the fact that it has three",
    "start": "244450",
    "end": "247150"
  },
  {
    "text": "a's and no b's.",
    "start": "247150",
    "end": "248829"
  },
  {
    "text": "Example 2, a, a, b\nis represented as a 2",
    "start": "248830",
    "end": "251950"
  },
  {
    "text": "in the first column and\na 1 in the second column,",
    "start": "251950",
    "end": "254690"
  },
  {
    "text": "and so forth.",
    "start": "254690",
    "end": "255890"
  },
  {
    "text": "So that's a first distinction.",
    "start": "255890",
    "end": "257170"
  },
  {
    "text": "We have this feature function\nhere, which is like a factory,",
    "start": "257170",
    "end": "260528"
  },
  {
    "text": "and depending on the data\nthat come in for our corpus,",
    "start": "260529",
    "end": "263680"
  },
  {
    "text": "we're going to get very\ndifferent features which",
    "start": "263680",
    "end": "266020"
  },
  {
    "text": "correspond to each one of\nthe columns in this feature",
    "start": "266020",
    "end": "269110"
  },
  {
    "text": "representation matrix.",
    "start": "269110",
    "end": "271849"
  },
  {
    "text": "Let's continue this a\nlittle bit and think",
    "start": "271850",
    "end": "273600"
  },
  {
    "text": "about how this actually\ninteracts with the optimization",
    "start": "273600",
    "end": "276120"
  },
  {
    "text": "process.",
    "start": "276120",
    "end": "276820"
  },
  {
    "text": "So in cell 7 here, I've just\nrepeated that previous matrix",
    "start": "276820",
    "end": "280170"
  },
  {
    "text": "for reference.",
    "start": "280170",
    "end": "281460"
  },
  {
    "text": "In cell 8, I have the class\nlabels for our four examples,",
    "start": "281460",
    "end": "284526"
  },
  {
    "text": "and you can see there are\nthree distinct classes--",
    "start": "284527",
    "end": "286610"
  },
  {
    "text": "C1, C2, and C3.",
    "start": "286610",
    "end": "289229"
  },
  {
    "text": "I set up a logistic\nregression model,",
    "start": "289230",
    "end": "290892"
  },
  {
    "text": "although that's not\nespecially important,",
    "start": "290892",
    "end": "292600"
  },
  {
    "text": "it's just a useful illustration.",
    "start": "292600",
    "end": "294480"
  },
  {
    "text": "And I call fit on my pair x, y.",
    "start": "294480",
    "end": "296580"
  },
  {
    "text": "That is my feature\nrepresentations and my labels,",
    "start": "296580",
    "end": "300000"
  },
  {
    "text": "and that's the\noptimization process.",
    "start": "300000",
    "end": "302070"
  },
  {
    "text": "As part of that, and for\na convention for scikit,",
    "start": "302070",
    "end": "304620"
  },
  {
    "text": "the optimization process\ncreates this new attribute coef_",
    "start": "304620",
    "end": "308639"
  },
  {
    "text": "and this new attribute classes_.",
    "start": "308640",
    "end": "312390"
  },
  {
    "text": "coef_ here, these are the\nweights that we learned as part",
    "start": "312390",
    "end": "314940"
  },
  {
    "text": "of the optimization process, and\nof course classes_ corresponds",
    "start": "314940",
    "end": "318300"
  },
  {
    "text": "to the classes that inferred\nfrom the label y that we input.",
    "start": "318300",
    "end": "322199"
  },
  {
    "text": "And here I'm just using\na pandas data frame again",
    "start": "322200",
    "end": "324360"
  },
  {
    "text": "to try to make this intuitive.",
    "start": "324360",
    "end": "325620"
  },
  {
    "text": "It's really just a NumPy\narray, this coef_ object here.",
    "start": "325620",
    "end": "329040"
  },
  {
    "text": "And you can see that\nthe resulting matrix has",
    "start": "329040",
    "end": "331980"
  },
  {
    "text": "a row for each\none of our classes",
    "start": "331980",
    "end": "334260"
  },
  {
    "text": "and a column for each\none of our features.",
    "start": "334260",
    "end": "337290"
  },
  {
    "text": "And that's a useful reminder\nthat what the optimization",
    "start": "337290",
    "end": "340020"
  },
  {
    "text": "process for models like\nthis is actually doing",
    "start": "340020",
    "end": "342720"
  },
  {
    "text": "is learning a weight that\nassociates class feature name",
    "start": "342720",
    "end": "346710"
  },
  {
    "text": "pairs with a weight, right?",
    "start": "346710",
    "end": "348867"
  },
  {
    "text": "So it's not just that we\nlearn individual weights",
    "start": "348868",
    "end": "350910"
  },
  {
    "text": "for features, but rather\nwe learn them with respect",
    "start": "350910",
    "end": "353340"
  },
  {
    "text": "to each one of the classes.",
    "start": "353340",
    "end": "354949"
  },
  {
    "text": "And that's a hallmark\nof optimization",
    "start": "354950",
    "end": "357090"
  },
  {
    "text": "for multi-class\nmodels like this one.",
    "start": "357090",
    "end": "359757"
  },
  {
    "text": "And then in cell 12, I've just\nshown you that you can actually",
    "start": "359757",
    "end": "362340"
  },
  {
    "text": "use the coef_ and this\nother bias term, intercept_,",
    "start": "362340",
    "end": "367260"
  },
  {
    "text": "to recreate the\npredictions of the model.",
    "start": "367260",
    "end": "369320"
  },
  {
    "text": "All you're doing is multiplying\nexamples by those coefficients",
    "start": "369320",
    "end": "373350"
  },
  {
    "text": "and adding in the bias term.",
    "start": "373350",
    "end": "375060"
  },
  {
    "text": "And this matrix here\nis identical to what",
    "start": "375060",
    "end": "377910"
  },
  {
    "text": "you get in scikit, if you simply\ndirectly call predict_proba",
    "start": "377910",
    "end": "381270"
  },
  {
    "text": "for predict probabilities\non your examples.",
    "start": "381270",
    "end": "383789"
  },
  {
    "start": "383790",
    "end": "388190"
  },
  {
    "start": "387000",
    "end": "562000"
  },
  {
    "text": "Let's turn back to\nwhat we're trying",
    "start": "388190",
    "end": "389690"
  },
  {
    "text": "to do to create good models,\nhaving those ideas in mind.",
    "start": "389690",
    "end": "392850"
  },
  {
    "text": "So let's just cover a few other\nideas for hand-built feature",
    "start": "392850",
    "end": "395607"
  },
  {
    "text": "functions that I think could\nbe effective for sentiment.",
    "start": "395607",
    "end": "397940"
  },
  {
    "text": "So, of course, we could have\nlexicon-derived features.",
    "start": "397940",
    "end": "400190"
  },
  {
    "text": "I earlier showed you a\nbunch of different lexicons.",
    "start": "400190",
    "end": "402890"
  },
  {
    "text": "And that could be used\nto group our unigrams.",
    "start": "402890",
    "end": "405020"
  },
  {
    "text": "So we could have these feature\nfunctions work in conjunction",
    "start": "405020",
    "end": "407960"
  },
  {
    "text": "with a \"bag-of-words\" or\n\"bag-of-acronyms\" model,",
    "start": "407960",
    "end": "410509"
  },
  {
    "text": "or we could use them to\nreplace that model and develop",
    "start": "410510",
    "end": "413120"
  },
  {
    "text": "a sparser feature\nrepresentation space.",
    "start": "413120",
    "end": "416810"
  },
  {
    "text": "We could also do\nthe negation marking",
    "start": "416810",
    "end": "418370"
  },
  {
    "text": "that I mentioned before, and\nwe could generalize that idea.",
    "start": "418370",
    "end": "421260"
  },
  {
    "text": "So many things in\nlanguage take scope",
    "start": "421260",
    "end": "423440"
  },
  {
    "text": "in a way that will affect\nthe semantics of words",
    "start": "423440",
    "end": "426050"
  },
  {
    "text": "that are in their scope.",
    "start": "426050",
    "end": "427400"
  },
  {
    "text": "So another classical example\nbehind-- besides negation",
    "start": "427400",
    "end": "430040"
  },
  {
    "text": "is these modal adverbs like\n\"quite possibly\" or \"totally\".",
    "start": "430040",
    "end": "433970"
  },
  {
    "text": "We might have the idea\nthat they are modulating",
    "start": "433970",
    "end": "436160"
  },
  {
    "text": "the extent to which\nthe speaker is",
    "start": "436160",
    "end": "437600"
  },
  {
    "text": "committed to \"masterpiece\"\nor \"amazing,\" in this case.",
    "start": "437600",
    "end": "440990"
  },
  {
    "text": "And keeping track of\nthat semantic association",
    "start": "440990",
    "end": "443330"
  },
  {
    "text": "with simple underscore\nmarking of some kind",
    "start": "443330",
    "end": "446120"
  },
  {
    "text": "might be useful for\ngiving our model a chance",
    "start": "446120",
    "end": "448550"
  },
  {
    "text": "to see that these\nunigrams are different",
    "start": "448550",
    "end": "450620"
  },
  {
    "text": "depending on their environment.",
    "start": "450620",
    "end": "453350"
  },
  {
    "text": "We can also have\nlength based features,",
    "start": "453350",
    "end": "454975"
  },
  {
    "text": "and that's just a useful\nreminder that these don't all",
    "start": "454975",
    "end": "457225"
  },
  {
    "text": "have to be count features.",
    "start": "457225",
    "end": "458400"
  },
  {
    "text": "We can have real valued\nfeatures of various kinds",
    "start": "458400",
    "end": "461449"
  },
  {
    "text": "and they could signal something\nimportant about the class",
    "start": "461450",
    "end": "464000"
  },
  {
    "text": "label.",
    "start": "464000",
    "end": "464750"
  },
  {
    "text": "For example, I think\nneutral reviews,",
    "start": "464750",
    "end": "467720"
  },
  {
    "text": "three star reviews tend to\nbe longer than one and five",
    "start": "467720",
    "end": "470360"
  },
  {
    "text": "star reviews, so might\nas well throw that in.",
    "start": "470360",
    "end": "473705"
  },
  {
    "text": "And we could expand that\nidea of float valued",
    "start": "473705",
    "end": "475580"
  },
  {
    "text": "features a little bit more.",
    "start": "475580",
    "end": "476705"
  },
  {
    "text": "I like the idea of\nthwarted expectations",
    "start": "476705",
    "end": "478979"
  },
  {
    "text": "which you might keep\ntrack of as the ratio",
    "start": "478980",
    "end": "481160"
  },
  {
    "text": "of positive to negative\nwords in a sentence.",
    "start": "481160",
    "end": "484670"
  },
  {
    "text": "The idea being that very often\nif that ratio is exaggerated,",
    "start": "484670",
    "end": "489050"
  },
  {
    "text": "it's telling you the opposite\nstory that you might expect",
    "start": "489050",
    "end": "491659"
  },
  {
    "text": "about the overall sentiment.",
    "start": "491660",
    "end": "493040"
  },
  {
    "text": "Many, many positive\nwords stacked up together",
    "start": "493040",
    "end": "495950"
  },
  {
    "text": "might actually be preparing\nyou for a negative assessment",
    "start": "495950",
    "end": "499250"
  },
  {
    "text": "and the reverse.",
    "start": "499250",
    "end": "500780"
  },
  {
    "text": "But the important thing\nabout this feature",
    "start": "500780",
    "end": "502670"
  },
  {
    "text": "is that it wouldn't decide for\nyou what these ratios mean.",
    "start": "502670",
    "end": "505700"
  },
  {
    "text": "You would just hope that\nit was a useful signal",
    "start": "505700",
    "end": "507770"
  },
  {
    "text": "that your model might pick\nup on as part of optimization",
    "start": "507770",
    "end": "510860"
  },
  {
    "text": "to figure out how to make\nuse of the information.",
    "start": "510860",
    "end": "514370"
  },
  {
    "text": "And then, finally, we could\ndo things, various kinds",
    "start": "514370",
    "end": "516620"
  },
  {
    "text": "of ad-hoc feature\nfunctions to try",
    "start": "516620",
    "end": "518659"
  },
  {
    "text": "to capture the fact that\nmany uses of language",
    "start": "518659",
    "end": "521479"
  },
  {
    "text": "are non-literal and might\nbe signaling exactly",
    "start": "521480",
    "end": "523880"
  },
  {
    "text": "the opposite of what they\nseem to do on their surface.",
    "start": "523880",
    "end": "526420"
  },
  {
    "text": "Like, \"Not exactly\na masterpiece.\"",
    "start": "526420",
    "end": "528529"
  },
  {
    "text": "is probably a pretty\nnegative review.",
    "start": "528530",
    "end": "531530"
  },
  {
    "text": "It was \"Like 50 hours long.\" is\nnot saying that it was actually",
    "start": "531530",
    "end": "534380"
  },
  {
    "text": "50 hours long but rather,\nwith hyperbole, indicating",
    "start": "534380",
    "end": "537560"
  },
  {
    "text": "that it was much too long\nor something like that.",
    "start": "537560",
    "end": "540163"
  },
  {
    "text": "And \"The best movie in the\nhistory of the universe.\"",
    "start": "540163",
    "end": "542330"
  },
  {
    "text": "could be a ringing endorsement,\nbut it could just as easily",
    "start": "542330",
    "end": "546290"
  },
  {
    "text": "be a bit of sarcasm.",
    "start": "546290",
    "end": "548420"
  },
  {
    "text": "Capturing those kind\nof subtle distinctions",
    "start": "548420",
    "end": "550370"
  },
  {
    "text": "is, of course, much\nmore difficult.",
    "start": "550370",
    "end": "551910"
  },
  {
    "text": "But the hand-built\nfeature functions",
    "start": "551910",
    "end": "553610"
  },
  {
    "text": "that you write where you\ncould try to capture it,",
    "start": "553610",
    "end": "555920"
  },
  {
    "text": "and if they have\na positive effect,",
    "start": "555920",
    "end": "557660"
  },
  {
    "text": "then maybe you've made\nsome real progress.",
    "start": "557660",
    "end": "560712"
  },
  {
    "text": "And that's a good\ntransition point",
    "start": "560713",
    "end": "562130"
  },
  {
    "start": "562000",
    "end": "739000"
  },
  {
    "text": "to this topic of assessing\nindividual feature functions.",
    "start": "562130",
    "end": "565320"
  },
  {
    "text": "As you can see, the philosophy\nin this mode of work",
    "start": "565320",
    "end": "567620"
  },
  {
    "text": "is that you write lots\nof feature functions",
    "start": "567620",
    "end": "569839"
  },
  {
    "text": "and kind of see how well\nthey can do at improving",
    "start": "569840",
    "end": "572720"
  },
  {
    "text": "your model overall.",
    "start": "572720",
    "end": "574399"
  },
  {
    "text": "You might end up with\na very large model",
    "start": "574400",
    "end": "576380"
  },
  {
    "text": "with many correlated\nfeatures and that",
    "start": "576380",
    "end": "578090"
  },
  {
    "text": "might lead you to want to do\nsome feature selection to weed",
    "start": "578090",
    "end": "580880"
  },
  {
    "text": "out the ones that are not\ncontributing in a positive way.",
    "start": "580880",
    "end": "584660"
  },
  {
    "text": "Now, scikit-learn\nhas a whole library",
    "start": "584660",
    "end": "586832"
  },
  {
    "text": "for doing this called\nfeature selection,",
    "start": "586833",
    "end": "588500"
  },
  {
    "text": "and it offers lots\nof functions that",
    "start": "588500",
    "end": "590600"
  },
  {
    "text": "will let you assess how much\ninformation your feature",
    "start": "590600",
    "end": "592910"
  },
  {
    "text": "functions contain with\nrespect to the labels",
    "start": "592910",
    "end": "595550"
  },
  {
    "text": "for your classification problem.",
    "start": "595550",
    "end": "597240"
  },
  {
    "text": "So this is very powerful.",
    "start": "597240",
    "end": "598399"
  },
  {
    "text": "And I encourage you to use them,\nbut you should be a little bit",
    "start": "598400",
    "end": "601400"
  },
  {
    "text": "cautious.",
    "start": "601400",
    "end": "602840"
  },
  {
    "text": "Take care when assessing\nfeature functions individually",
    "start": "602840",
    "end": "606500"
  },
  {
    "text": "because correlations\nbetween those features",
    "start": "606500",
    "end": "608930"
  },
  {
    "text": "will make the assessments\nvery hard to interpret.",
    "start": "608930",
    "end": "612589"
  },
  {
    "text": "The problem here is that\nyour model is holistically",
    "start": "612590",
    "end": "615063"
  },
  {
    "text": "thinking about how\nall of these features",
    "start": "615063",
    "end": "616730"
  },
  {
    "text": "relate to your class\nlabel and figuring out",
    "start": "616730",
    "end": "619070"
  },
  {
    "text": "how to optimize\nweights on that basis,",
    "start": "619070",
    "end": "621530"
  },
  {
    "text": "whereas the feature function\nmethods, many of them,",
    "start": "621530",
    "end": "623660"
  },
  {
    "text": "just look at individual\nfeatures and how",
    "start": "623660",
    "end": "625639"
  },
  {
    "text": "they relate to the class label.",
    "start": "625640",
    "end": "626960"
  },
  {
    "text": "So you're losing all that\ncorrelational context.",
    "start": "626960",
    "end": "629882"
  },
  {
    "text": "And to make that\na little concrete,",
    "start": "629882",
    "end": "631340"
  },
  {
    "text": "I just cooked up an example\nhere, an idealized one, that",
    "start": "631340",
    "end": "634430"
  },
  {
    "text": "shows how you could be misled.",
    "start": "634430",
    "end": "636080"
  },
  {
    "text": "So I have three\nfeatures, x1, x2, and x3",
    "start": "636080",
    "end": "639110"
  },
  {
    "text": "in a simple binary\nclassification problem.",
    "start": "639110",
    "end": "642019"
  },
  {
    "text": "And I use the chi-square\ntest from feature selection",
    "start": "642020",
    "end": "644990"
  },
  {
    "text": "to kind of assess how important\neach one of these features",
    "start": "644990",
    "end": "647540"
  },
  {
    "text": "is with respect to this\nclassification problem.",
    "start": "647540",
    "end": "651089"
  },
  {
    "text": "And what I found is\nthat, intuitively, it",
    "start": "651090",
    "end": "653060"
  },
  {
    "text": "looks like x1 and x2 are\nreally powerful features.",
    "start": "653060",
    "end": "656900"
  },
  {
    "text": "And that might lead me\nto think, well, I'll",
    "start": "656900",
    "end": "658920"
  },
  {
    "text": "drop the third\nfeature and include",
    "start": "658920",
    "end": "660950"
  },
  {
    "text": "just one and two in my model.",
    "start": "660950",
    "end": "663570"
  },
  {
    "text": "So far, so good.",
    "start": "663570",
    "end": "664530"
  },
  {
    "text": "However, if we thoroughly\nexplore this space, what",
    "start": "664530",
    "end": "667280"
  },
  {
    "text": "we find is that in truth a\nsimple linear model performs",
    "start": "667280",
    "end": "670880"
  },
  {
    "text": "best with just feature\nx1 and actually",
    "start": "670880",
    "end": "673490"
  },
  {
    "text": "including x2 hurts the\nmodel, despite the fact",
    "start": "673490",
    "end": "677000"
  },
  {
    "text": "that it has this positive\nfeature importance value.",
    "start": "677000",
    "end": "679970"
  },
  {
    "text": "So what you really\nought to be doing",
    "start": "679970",
    "end": "681470"
  },
  {
    "text": "is using just this\nsingle feature,",
    "start": "681470",
    "end": "683449"
  },
  {
    "text": "but these methods\ncan't tell us that.",
    "start": "683450",
    "end": "685310"
  },
  {
    "text": "And even a positive\nfeature selection value",
    "start": "685310",
    "end": "689210"
  },
  {
    "text": "might actually be\nsomething that's",
    "start": "689210",
    "end": "690860"
  },
  {
    "text": "at odds with what we're\ntrying to do with our model,",
    "start": "690860",
    "end": "693079"
  },
  {
    "text": "as this example shows.",
    "start": "693080",
    "end": "695210"
  },
  {
    "text": "So, ideally, what\nyou would do is",
    "start": "695210",
    "end": "697550"
  },
  {
    "text": "consider more holistic\nassessment methods",
    "start": "697550",
    "end": "699830"
  },
  {
    "text": "which scikit also offers.",
    "start": "699830",
    "end": "701600"
  },
  {
    "text": "This would be things like\nsystematically removing",
    "start": "701600",
    "end": "704000"
  },
  {
    "text": "or perturbing feature values in\nthe context of the full model",
    "start": "704000",
    "end": "707990"
  },
  {
    "text": "that you're optimizing\nand comparing performance",
    "start": "707990",
    "end": "710600"
  },
  {
    "text": "across those models.",
    "start": "710600",
    "end": "711680"
  },
  {
    "text": "This is much more\nexpensive because you're",
    "start": "711680",
    "end": "713600"
  },
  {
    "text": "optimizing many, many models.",
    "start": "713600",
    "end": "715550"
  },
  {
    "text": "So it might be prohibitive\nfor some classes of models",
    "start": "715550",
    "end": "718093"
  },
  {
    "text": "that you're exploring.",
    "start": "718093",
    "end": "719010"
  },
  {
    "text": "But if you can do it, this\nwill be more reliable.",
    "start": "719010",
    "end": "722210"
  },
  {
    "text": "However, if this\nis impossible, it",
    "start": "722210",
    "end": "724340"
  },
  {
    "text": "might still be productive\nto do some feature selection",
    "start": "724340",
    "end": "727700"
  },
  {
    "text": "using simpler methods.",
    "start": "727700",
    "end": "729295"
  },
  {
    "text": "You should just\nbe aware that you",
    "start": "729295",
    "end": "730670"
  },
  {
    "text": "might be doing\nsomething that's not",
    "start": "730670",
    "end": "732740"
  },
  {
    "text": "optimal for the actual\noptimization problem",
    "start": "732740",
    "end": "735485"
  },
  {
    "text": "that you've posed.",
    "start": "735485",
    "end": "736235"
  },
  {
    "start": "736235",
    "end": "738566"
  },
  {
    "text": "OK, and the final section\nof this screencast",
    "start": "738567",
    "end": "740400"
  },
  {
    "start": "739000",
    "end": "944000"
  },
  {
    "text": "is a kind of transition into\nthe world of deep learning.",
    "start": "740400",
    "end": "742980"
  },
  {
    "text": "I've called this distributed\nrepresentations as features.",
    "start": "742980",
    "end": "746269"
  },
  {
    "text": "This is a very different\nmode for thinking",
    "start": "746270",
    "end": "748020"
  },
  {
    "text": "about representing examples.",
    "start": "748020",
    "end": "750240"
  },
  {
    "text": "What we do in this case is\ntake our token stream as before",
    "start": "750240",
    "end": "754388"
  },
  {
    "text": "but instead of writing a lot of\nhand-built feature functions,",
    "start": "754388",
    "end": "756930"
  },
  {
    "text": "we simply look up each one of\nthose tokens in some embedding",
    "start": "756930",
    "end": "760348"
  },
  {
    "text": "that we have.",
    "start": "760348",
    "end": "760890"
  },
  {
    "text": "For example, it\ncould be an embedding",
    "start": "760890",
    "end": "762450"
  },
  {
    "text": "that you created in the\nfirst unit of this course.",
    "start": "762450",
    "end": "765390"
  },
  {
    "text": "Or it could be a GloVe\nembedding or a static embedding",
    "start": "765390",
    "end": "768390"
  },
  {
    "text": "that you derived from\nBERT representations,",
    "start": "768390",
    "end": "770740"
  },
  {
    "text": "and so forth and so on.",
    "start": "770740",
    "end": "772125"
  },
  {
    "text": "The important thing is\nthat each token is now",
    "start": "772125",
    "end": "774000"
  },
  {
    "text": "represented by a vector and\nthat could be a powerful idea",
    "start": "774000",
    "end": "777720"
  },
  {
    "text": "because in representing each\nof these words as vectors,",
    "start": "777720",
    "end": "780449"
  },
  {
    "text": "we are now capturing\nthe relationships",
    "start": "780450",
    "end": "784110"
  },
  {
    "text": "between those tokens.",
    "start": "784110",
    "end": "785399"
  },
  {
    "text": "We might now have\na hope of seeing",
    "start": "785400",
    "end": "787080"
  },
  {
    "text": "that \"sofa\" and\n\"couch\" are actually",
    "start": "787080",
    "end": "789600"
  },
  {
    "text": "similar features\nin general and not",
    "start": "789600",
    "end": "791759"
  },
  {
    "text": "just with respect to the\nclass labels that we have.",
    "start": "791760",
    "end": "795240"
  },
  {
    "text": "So that's the idea why\nthis might be powerful.",
    "start": "795240",
    "end": "797157"
  },
  {
    "text": "So we take all those\nvectors and look them up.",
    "start": "797157",
    "end": "799073"
  },
  {
    "text": "However, for all these\nclassifier models,",
    "start": "799073",
    "end": "800860"
  },
  {
    "text": "we need a fixed\ndimensional representation",
    "start": "800860",
    "end": "802950"
  },
  {
    "text": "to feed into the\nactual classifier unit.",
    "start": "802950",
    "end": "805162"
  },
  {
    "text": "So we're going to have\nto combine those vectors",
    "start": "805162",
    "end": "807120"
  },
  {
    "text": "in some way, and the\nsimplest thing you could do",
    "start": "807120",
    "end": "809120"
  },
  {
    "text": "is combine them via some\nfunction like sum or mean,",
    "start": "809120",
    "end": "811987"
  },
  {
    "text": "right?",
    "start": "811987",
    "end": "813517"
  },
  {
    "text": "So take all these\nthings, for example,",
    "start": "813517",
    "end": "815100"
  },
  {
    "text": "and take their average and that\nwould give me another fixed",
    "start": "815100",
    "end": "817589"
  },
  {
    "text": "dimensional representation,\nno matter how many tokens are",
    "start": "817590",
    "end": "820530"
  },
  {
    "text": "in each one of the examples.",
    "start": "820530",
    "end": "822430"
  },
  {
    "text": "And that average vector would\nbe the input to the classifier.",
    "start": "822430",
    "end": "827160"
  },
  {
    "text": "So if each one of these vectors\nhas dimension 300, then so,",
    "start": "827160",
    "end": "830759"
  },
  {
    "text": "too, does the feature\nrepresentation",
    "start": "830760",
    "end": "832350"
  },
  {
    "text": "of my entire example.",
    "start": "832350",
    "end": "834060"
  },
  {
    "text": "And I now have a\nclassifier which",
    "start": "834060",
    "end": "836880"
  },
  {
    "text": "is processing feature\nrepresentations that",
    "start": "836880",
    "end": "838860"
  },
  {
    "text": "have 300 columns.",
    "start": "838860",
    "end": "840810"
  },
  {
    "text": "Each dimension in the\nunderlying embedding space",
    "start": "840810",
    "end": "843540"
  },
  {
    "text": "now corresponds to a feature.",
    "start": "843540",
    "end": "845639"
  },
  {
    "text": "And that's the basis\nfor optimization.",
    "start": "845640",
    "end": "847350"
  },
  {
    "text": "And I'd say an eye opening\nthing about this class of models",
    "start": "847350",
    "end": "849959"
  },
  {
    "text": "is despite them\nbeing very compact--",
    "start": "849960",
    "end": "852810"
  },
  {
    "text": "300 dimensions versus 20,000\nthat you might have from",
    "start": "852810",
    "end": "856290"
  },
  {
    "text": "a \"bag-of-words\" model--",
    "start": "856290",
    "end": "857639"
  },
  {
    "text": "they turn out to\nbe very powerful.",
    "start": "857640",
    "end": "860520"
  },
  {
    "text": "And this final slide\nhere just shows you",
    "start": "860520",
    "end": "862530"
  },
  {
    "text": "how to implement those using\ntools and other utilities",
    "start": "862530",
    "end": "865380"
  },
  {
    "text": "for our course.",
    "start": "865380",
    "end": "866603"
  },
  {
    "text": "So I'm going to\nuse GloVe, and I'm",
    "start": "866603",
    "end": "868020"
  },
  {
    "text": "going to use the 300 dimensional\nGloVe space which is included",
    "start": "868020",
    "end": "870960"
  },
  {
    "text": "in your data distribution.",
    "start": "870960",
    "end": "872910"
  },
  {
    "text": "In 4 and 5 here, we just\nwrite simple feature functions",
    "start": "872910",
    "end": "875517"
  },
  {
    "text": "and the hallmark of these\nis that they are simply",
    "start": "875518",
    "end": "877560"
  },
  {
    "text": "looking up words\nin the embedding",
    "start": "877560",
    "end": "879660"
  },
  {
    "text": "and then combining them\nvia whatever function they",
    "start": "879660",
    "end": "882180"
  },
  {
    "text": "use or specifies.",
    "start": "882180",
    "end": "883440"
  },
  {
    "text": "So the output of\nthis is directly",
    "start": "883440",
    "end": "885270"
  },
  {
    "text": "a vector representation\nof each example.",
    "start": "885270",
    "end": "888780"
  },
  {
    "text": "In cell 6, we set up a\nlogistic regression, as before.",
    "start": "888780",
    "end": "891878"
  },
  {
    "text": "Of course, it could be\na much fancier model",
    "start": "891878",
    "end": "893670"
  },
  {
    "text": "but logistic regression will do.",
    "start": "893670",
    "end": "895740"
  },
  {
    "text": "And then we use sst_experiment\nalmost exactly as before.",
    "start": "895740",
    "end": "899070"
  },
  {
    "text": "The one change we\nneed to remember",
    "start": "899070",
    "end": "900810"
  },
  {
    "text": "to make in operating\nin this mode is",
    "start": "900810",
    "end": "903060"
  },
  {
    "text": "to set the flag of\nvectorized equals false.",
    "start": "903060",
    "end": "905970"
  },
  {
    "text": "We already have each example\nrepresented as a vector,",
    "start": "905970",
    "end": "909600"
  },
  {
    "text": "so we do not need to pass it\nthrough that whole process",
    "start": "909600",
    "end": "912209"
  },
  {
    "text": "of using a DictVectorizer\nto turn count",
    "start": "912210",
    "end": "914850"
  },
  {
    "text": "dictionaries into vectors.",
    "start": "914850",
    "end": "917220"
  },
  {
    "text": "And as I said before,\nthese turn out",
    "start": "917220",
    "end": "919050"
  },
  {
    "text": "to be quite good models\ndespite their compactness.",
    "start": "919050",
    "end": "921510"
  },
  {
    "text": "And the final thing I'll\nsay is that this model",
    "start": "921510",
    "end": "924240"
  },
  {
    "text": "is a nice transition into\nthe recurrent neural networks",
    "start": "924240",
    "end": "927600"
  },
  {
    "text": "that we'll study in the final\nscreencast for this unit, which",
    "start": "927600",
    "end": "930569"
  },
  {
    "text": "essentially generalize\nthis idea by learning",
    "start": "930570",
    "end": "933540"
  },
  {
    "text": "an interesting combination\nfunction for all",
    "start": "933540",
    "end": "935850"
  },
  {
    "text": "the vectors for each of\nthe individual tokens.",
    "start": "935850",
    "end": "939050"
  },
  {
    "start": "939050",
    "end": "943019"
  }
]