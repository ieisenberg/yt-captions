[
  {
    "start": "0",
    "end": "4827"
  },
  {
    "text": "SPEAKER: Welcome back, everyone.",
    "start": "4827",
    "end": "6160"
  },
  {
    "text": "This is Part 4 in our series\non contextual representations.",
    "start": "6160",
    "end": "9389"
  },
  {
    "text": "We have come to what might be\nthe most famous transformer",
    "start": "9390",
    "end": "12840"
  },
  {
    "text": "based architecture,\nand that is GPT.",
    "start": "12840",
    "end": "16605"
  },
  {
    "text": "I thought I would\nstart this discussion",
    "start": "16605",
    "end": "18230"
  },
  {
    "text": "in a kind of technical place.",
    "start": "18230",
    "end": "19640"
  },
  {
    "text": "That is, the autoregressive\nloss function",
    "start": "19640",
    "end": "21740"
  },
  {
    "text": "that's usually used for\nneural language modeling.",
    "start": "21740",
    "end": "24590"
  },
  {
    "text": "And then I'm going to try to\nsupport that technical piece",
    "start": "24590",
    "end": "27080"
  },
  {
    "text": "with a bunch of illustrations.",
    "start": "27080",
    "end": "28920"
  },
  {
    "text": "So here is the\nfull loss function.",
    "start": "28920",
    "end": "30830"
  },
  {
    "text": "There are a lot of\nmathematical details here.",
    "start": "30830",
    "end": "32750"
  },
  {
    "text": "I think the smart thing to do\nis zoom in on the numerator.",
    "start": "32750",
    "end": "36470"
  },
  {
    "text": "What we're saying here\nis that at position T,",
    "start": "36470",
    "end": "39680"
  },
  {
    "text": "we're going to look up the\ntoken representation for t",
    "start": "39680",
    "end": "42740"
  },
  {
    "text": "in our embedding layer and do\na dot product of that vector",
    "start": "42740",
    "end": "46880"
  },
  {
    "text": "representation with a\nhidden representation",
    "start": "46880",
    "end": "49790"
  },
  {
    "text": "that we have built\nup in our model",
    "start": "49790",
    "end": "51800"
  },
  {
    "text": "to the timestep preceding\nthe one that is in focus here",
    "start": "51800",
    "end": "55170"
  },
  {
    "text": "at time step t.",
    "start": "55170",
    "end": "56420"
  },
  {
    "text": "The rest of this is kind\nof softmax normalization.",
    "start": "56420",
    "end": "59449"
  },
  {
    "text": "So we do that same\ncalculation for every item",
    "start": "59450",
    "end": "62180"
  },
  {
    "text": "in the vocabulary and then\nwe take the log of that,",
    "start": "62180",
    "end": "65157"
  },
  {
    "text": "and we are looking for\nparameters that will",
    "start": "65158",
    "end": "66950"
  },
  {
    "text": "maximize this log probability.",
    "start": "66950",
    "end": "69289"
  },
  {
    "text": "But again, the thing\nto keep an eye on",
    "start": "69290",
    "end": "71270"
  },
  {
    "text": "is that the scoring\nis based in the dot",
    "start": "71270",
    "end": "73759"
  },
  {
    "text": "product of the embedding\nrepresentation for the token we",
    "start": "73760",
    "end": "77400"
  },
  {
    "text": "want to predict and the\nhidden representation that we",
    "start": "77400",
    "end": "80340"
  },
  {
    "text": "have built up until the\ntime before that time step.",
    "start": "80340",
    "end": "84700"
  },
  {
    "text": "So here's that\nsame kind of thing",
    "start": "84700",
    "end": "86969"
  },
  {
    "text": "by way of an illustration.",
    "start": "86970",
    "end": "88590"
  },
  {
    "text": "Our sequence is\nthe, Rock, rules.",
    "start": "88590",
    "end": "90325"
  },
  {
    "text": "And for this kind of\nlanguage modeling,",
    "start": "90325",
    "end": "91950"
  },
  {
    "text": "I think it's good to keep track\nof start and end tokens here.",
    "start": "91950",
    "end": "95829"
  },
  {
    "text": "So we begin modeling with\nthat start token, which",
    "start": "95830",
    "end": "98550"
  },
  {
    "text": "is kind of given that\nis at position t1,",
    "start": "98550",
    "end": "101130"
  },
  {
    "text": "and we look up its\nembedding representation,",
    "start": "101130",
    "end": "103710"
  },
  {
    "text": "and then we form some kind\nof hidden representation H1.",
    "start": "103710",
    "end": "108010"
  },
  {
    "text": "Then to predict \"the,\"\nwhich is now at time step 2,",
    "start": "108010",
    "end": "112690"
  },
  {
    "text": "we are going to use h1 here dot\nproducted with the embedding",
    "start": "112690",
    "end": "118540"
  },
  {
    "text": "representation for \"the.\"",
    "start": "118540",
    "end": "119800"
  },
  {
    "text": "Remember that's the numerator\nin that scoring function.",
    "start": "119800",
    "end": "123280"
  },
  {
    "text": "At time step 2, we\nnow copy over \"the\",",
    "start": "123280",
    "end": "126100"
  },
  {
    "text": "and we continue, we get its\nembedding representation.",
    "start": "126100",
    "end": "128740"
  },
  {
    "text": "And here I'm depicting like\na recurrent neural network.",
    "start": "128740",
    "end": "132260"
  },
  {
    "text": "So we're traveling left to right\njust to keep things simple.",
    "start": "132260",
    "end": "135019"
  },
  {
    "text": "I'll talk about how GPT\nhandles this in a bit later.",
    "start": "135020",
    "end": "138530"
  },
  {
    "text": "So we traveled left\nto right and we",
    "start": "138530",
    "end": "139990"
  },
  {
    "text": "got a second hidden\nrepresentation H2,",
    "start": "139990",
    "end": "142270"
  },
  {
    "text": "and again the\nscoring is the same.",
    "start": "142270",
    "end": "143950"
  },
  {
    "text": "To predict the\nrock at position 3,",
    "start": "143950",
    "end": "146110"
  },
  {
    "text": "we use H2 and the\nembedding for Rock.",
    "start": "146110",
    "end": "149140"
  },
  {
    "text": "And then the sequence\nmodeling continues",
    "start": "149140",
    "end": "151210"
  },
  {
    "text": "in exactly that same way until\nwe predict our end token.",
    "start": "151210",
    "end": "156250"
  },
  {
    "text": "But for each one of\nthese time steps,",
    "start": "156250",
    "end": "157930"
  },
  {
    "text": "remember what we're doing is\ngetting a score for the Rock",
    "start": "157930",
    "end": "161170"
  },
  {
    "text": "that is proportional to the\ndot product of the embedding",
    "start": "161170",
    "end": "165250"
  },
  {
    "text": "for that token and the\nhidden representation",
    "start": "165250",
    "end": "168280"
  },
  {
    "text": "just prior to that point that\nwe're making the prediction at.",
    "start": "168280",
    "end": "171490"
  },
  {
    "text": "And then we exponentiate\nthat for the sake",
    "start": "171490",
    "end": "173410"
  },
  {
    "text": "of doing it softmax scoring.",
    "start": "173410",
    "end": "176530"
  },
  {
    "text": "When we move to GPT,\nwe're essentially",
    "start": "176530",
    "end": "178840"
  },
  {
    "text": "just doing this in the\ncontext of a transformer.",
    "start": "178840",
    "end": "181330"
  },
  {
    "text": "So kind of to depict that.",
    "start": "181330",
    "end": "183040"
  },
  {
    "text": "I've got at the bottom here a\ntraditional absolute encoding",
    "start": "183040",
    "end": "186129"
  },
  {
    "text": "scheme for positions.",
    "start": "186130",
    "end": "188200"
  },
  {
    "text": "We look up all of those\nstatic vector representations,",
    "start": "188200",
    "end": "191830"
  },
  {
    "text": "and we get our first\ncontextual representations",
    "start": "191830",
    "end": "194620"
  },
  {
    "text": "is in green as usual.",
    "start": "194620",
    "end": "196629"
  },
  {
    "text": "And then for GPT, we might stack\nlots and lots of transformer",
    "start": "196630",
    "end": "200320"
  },
  {
    "text": "blocks on top of each other.",
    "start": "200320",
    "end": "202780"
  },
  {
    "text": "Eventually, though, we will get\nsome output representations.",
    "start": "202780",
    "end": "206140"
  },
  {
    "text": "Those are the ones that\nI've depicted in green here.",
    "start": "206140",
    "end": "208690"
  },
  {
    "text": "And those will be the basis\nfor language modeling.",
    "start": "208690",
    "end": "211460"
  },
  {
    "text": "So we will add on top those\nsome language modeling specific",
    "start": "211460",
    "end": "215350"
  },
  {
    "text": "parameters which could\njust be the embedding",
    "start": "215350",
    "end": "217820"
  },
  {
    "text": "layer that comes from the\nword embeddings down here.",
    "start": "217820",
    "end": "221060"
  },
  {
    "text": "And that will be the basis for\npredicting the actual sequence.",
    "start": "221060",
    "end": "225300"
  },
  {
    "text": "And so we get an error\nsignal to the extent",
    "start": "225300",
    "end": "227360"
  },
  {
    "text": "that we're making predictions\ninto this space that",
    "start": "227360",
    "end": "230270"
  },
  {
    "text": "don't correspond to the\nactual one hot encoding",
    "start": "230270",
    "end": "233960"
  },
  {
    "text": "vectors that correspond\nto the sequence itself.",
    "start": "233960",
    "end": "237490"
  },
  {
    "text": "In essence, though,\nthis is just more",
    "start": "237490",
    "end": "240340"
  },
  {
    "text": "of that conditional language\nmodeling using the transformer",
    "start": "240340",
    "end": "243430"
  },
  {
    "text": "architecture.",
    "start": "243430",
    "end": "244510"
  },
  {
    "text": "Maybe the one thing\nto keep in mind",
    "start": "244510",
    "end": "246159"
  },
  {
    "text": "here is that because of\nthe nature of the attention",
    "start": "246160",
    "end": "248770"
  },
  {
    "text": "mechanisms, we need\nto do some masking",
    "start": "248770",
    "end": "251320"
  },
  {
    "text": "to make sure that we don't in\neffect look into the future.",
    "start": "251320",
    "end": "254690"
  },
  {
    "text": "So let's build that\nup a little bit.",
    "start": "254690",
    "end": "256778"
  },
  {
    "text": "We start with position A.\nAnd at that first position,",
    "start": "256779",
    "end": "261250"
  },
  {
    "text": "the only attending we\ncan do is to ourselves",
    "start": "261250",
    "end": "263860"
  },
  {
    "text": "because we can't\nlook into the future.",
    "start": "263860",
    "end": "265629"
  },
  {
    "text": "I haven't depicted that but\nwe could do self-attention.",
    "start": "265630",
    "end": "269020"
  },
  {
    "text": "When we move to\nposition B, we now",
    "start": "269020",
    "end": "271000"
  },
  {
    "text": "have the opportunity to\nlook back into position A",
    "start": "271000",
    "end": "274330"
  },
  {
    "text": "and get that dot product.",
    "start": "274330",
    "end": "275800"
  },
  {
    "text": "And we could self-attend,\nas I said before,",
    "start": "275800",
    "end": "278020"
  },
  {
    "text": "although I didn't depict that.",
    "start": "278020",
    "end": "279710"
  },
  {
    "text": "And then finally, when\nwe get to position C,",
    "start": "279710",
    "end": "281710"
  },
  {
    "text": "we can look back to the\nprevious two positions.",
    "start": "281710",
    "end": "284569"
  },
  {
    "text": "So the attention mask is\ngoing to have this look where",
    "start": "284570",
    "end": "287380"
  },
  {
    "text": "we go backwards but not\nforwards so that we don't end up",
    "start": "287380",
    "end": "290980"
  },
  {
    "text": "looking into the future, into\ntokens that we have not yet",
    "start": "290980",
    "end": "293710"
  },
  {
    "text": "generated.",
    "start": "293710",
    "end": "296289"
  },
  {
    "text": "So in a little\nmore detail, again,",
    "start": "296290",
    "end": "298080"
  },
  {
    "text": "I would kind of like\nto belabor these points",
    "start": "298080",
    "end": "299970"
  },
  {
    "text": "because there are a lot of\ntechnical details hiding here.",
    "start": "299970",
    "end": "302730"
  },
  {
    "text": "What I'm going to depict on\nthis slide is very specifically",
    "start": "302730",
    "end": "306480"
  },
  {
    "text": "training of a GPT style model\nwith what's called teacher",
    "start": "306480",
    "end": "310770"
  },
  {
    "text": "forcing, and this is going\nto mean that no matter",
    "start": "310770",
    "end": "313050"
  },
  {
    "text": "what token we predict\nat every time step,",
    "start": "313050",
    "end": "316389"
  },
  {
    "text": "we will use the actual\ntoken at the next time step.",
    "start": "316390",
    "end": "320380"
  },
  {
    "text": "So let's be really\npedantic about this.",
    "start": "320380",
    "end": "322440"
  },
  {
    "text": "At the bottom here, I have our\ninput sequence as represented",
    "start": "322440",
    "end": "327210"
  },
  {
    "text": "as a series of one hot vectors.",
    "start": "327210",
    "end": "329410"
  },
  {
    "text": "These are kind of\nlookup devices that",
    "start": "329410",
    "end": "331650"
  },
  {
    "text": "will give us back the embedding\nrepresentations for words",
    "start": "331650",
    "end": "334800"
  },
  {
    "text": "from our embedding space.",
    "start": "334800",
    "end": "337289"
  },
  {
    "text": "Here's our embedding\nspace in gray",
    "start": "337290",
    "end": "339270"
  },
  {
    "text": "and so we do those\nlookups, and that gives us",
    "start": "339270",
    "end": "341430"
  },
  {
    "text": "a bunch of vectors.",
    "start": "341430",
    "end": "342389"
  },
  {
    "text": "And as a kind of\nshorthand, I have",
    "start": "342390",
    "end": "344460"
  },
  {
    "text": "depicted the names of those\nvectors, B for beginning",
    "start": "344460",
    "end": "347789"
  },
  {
    "text": "of sequence, the Rock rules.",
    "start": "347790",
    "end": "350430"
  },
  {
    "text": "That's the sequence there.",
    "start": "350430",
    "end": "352360"
  },
  {
    "text": "Then we have a whole bunch\nof those transformer blocks,",
    "start": "352360",
    "end": "355840"
  },
  {
    "text": "and what I've done is\nsummarize them in green here.",
    "start": "355840",
    "end": "358570"
  },
  {
    "text": "And just reminder, I've\ngot all those arrows",
    "start": "358570",
    "end": "361660"
  },
  {
    "text": "showing you the\nattention patterns",
    "start": "361660",
    "end": "363340"
  },
  {
    "text": "so that we always\nlook into the past,",
    "start": "363340",
    "end": "365419"
  },
  {
    "text": "but never into the future.",
    "start": "365420",
    "end": "368120"
  },
  {
    "text": "Then on top of that, we're going\nto use our embedding parameters",
    "start": "368120",
    "end": "371870"
  },
  {
    "text": "again.",
    "start": "371870",
    "end": "372740"
  },
  {
    "text": "So these are the same\nparameters that I've",
    "start": "372740",
    "end": "375080"
  },
  {
    "text": "got depicted down here.",
    "start": "375080",
    "end": "377530"
  },
  {
    "text": "And now we are going\nto compare essentially",
    "start": "377530",
    "end": "380220"
  },
  {
    "text": "the scores that we predict\nin each one of those spaces",
    "start": "380220",
    "end": "383940"
  },
  {
    "text": "with the one hot vectors--",
    "start": "383940",
    "end": "385860"
  },
  {
    "text": "one hot vectors that actually\ncorrespond to the sequence",
    "start": "385860",
    "end": "388710"
  },
  {
    "text": "that we want to predict.",
    "start": "388710",
    "end": "390039"
  },
  {
    "text": "So this was the start token.",
    "start": "390040",
    "end": "391890"
  },
  {
    "text": "And conditional on that,\nwe want to predict \"the.\"",
    "start": "391890",
    "end": "394950"
  },
  {
    "text": "This is \"the\" down here and\nconditional on that we want",
    "start": "394950",
    "end": "397770"
  },
  {
    "text": "to predict Rock and so forth.",
    "start": "397770",
    "end": "399900"
  },
  {
    "text": "You do get this offset where the\ninput sequence and the output",
    "start": "399900",
    "end": "403320"
  },
  {
    "text": "sequence are staggered by\none, so that we're always",
    "start": "403320",
    "end": "406320"
  },
  {
    "text": "conditional on what we've seen\npredicting the next token.",
    "start": "406320",
    "end": "411660"
  },
  {
    "text": "Imagine that these are\nthe scores that we get out",
    "start": "411660",
    "end": "414330"
  },
  {
    "text": "of this final layer here.",
    "start": "414330",
    "end": "415560"
  },
  {
    "text": "I've depicted them as integers,\nbut they could be floats.",
    "start": "415560",
    "end": "418560"
  },
  {
    "text": "And the idea is that that\nis the comparison that we",
    "start": "418560",
    "end": "421800"
  },
  {
    "text": "make to get our error signal.",
    "start": "421800",
    "end": "423449"
  },
  {
    "text": "We can look at the difference\nbetween this vector",
    "start": "423450",
    "end": "425760"
  },
  {
    "text": "here and this\nvector here and use",
    "start": "425760",
    "end": "428760"
  },
  {
    "text": "that as a gradient\nsignal to update",
    "start": "428760",
    "end": "430860"
  },
  {
    "text": "the parameters of the model.",
    "start": "430860",
    "end": "432460"
  },
  {
    "text": "And that's how this actually\nhappens in practice.",
    "start": "432460",
    "end": "434850"
  },
  {
    "text": "We always think of language\nmodeling as predicting tokens,",
    "start": "434850",
    "end": "439270"
  },
  {
    "text": "but really and\ntruly, it predicts",
    "start": "439270",
    "end": "441520"
  },
  {
    "text": "scores over the\nentire vocabulary,",
    "start": "441520",
    "end": "444099"
  },
  {
    "text": "and then we make a choice\nabout which token was actually",
    "start": "444100",
    "end": "447460"
  },
  {
    "text": "predicted by, for example,\npicking the token that",
    "start": "447460",
    "end": "450880"
  },
  {
    "text": "had the largest score.",
    "start": "450880",
    "end": "453880"
  },
  {
    "text": "And this-- I've\nmentioned teacher",
    "start": "453880",
    "end": "455770"
  },
  {
    "text": "forcing in this context.",
    "start": "455770",
    "end": "456940"
  },
  {
    "text": "This is really important here.",
    "start": "456940",
    "end": "458680"
  },
  {
    "text": "I think we should imagine\nthat, at this time step here,",
    "start": "458680",
    "end": "462039"
  },
  {
    "text": "what the model did is give the\nhighest score to whatever word",
    "start": "462040",
    "end": "466000"
  },
  {
    "text": "was in the final position here,\nbut the actual token was rules.",
    "start": "466000",
    "end": "470590"
  },
  {
    "text": "So this will give\nus an error signal",
    "start": "470590",
    "end": "472449"
  },
  {
    "text": "because we have in effect made a\nmistake that we can learn from.",
    "start": "472450",
    "end": "476650"
  },
  {
    "text": "The teacher forcing\naspect of this",
    "start": "476650",
    "end": "478810"
  },
  {
    "text": "is that I do not use\nthe vector consisting",
    "start": "478810",
    "end": "481240"
  },
  {
    "text": "of all zeros and a one down\nhere at this time step,",
    "start": "481240",
    "end": "484389"
  },
  {
    "text": "but rather I use\nthe actual token.",
    "start": "484390",
    "end": "487570"
  },
  {
    "text": "If we back off from\nteacher forcing,",
    "start": "487570",
    "end": "490210"
  },
  {
    "text": "we could go into a mode where\nat least some of the time",
    "start": "490210",
    "end": "493240"
  },
  {
    "text": "we use the vector consisting\nof all zeros and a one",
    "start": "493240",
    "end": "496479"
  },
  {
    "text": "down here as the input\nat the next time step,",
    "start": "496480",
    "end": "499000"
  },
  {
    "text": "and that would\neffectively be using",
    "start": "499000",
    "end": "501250"
  },
  {
    "text": "the model's predictions rather\nthan the gold sequence as part",
    "start": "501250",
    "end": "505090"
  },
  {
    "text": "of training.",
    "start": "505090",
    "end": "505780"
  },
  {
    "text": "And that could introduce\nsome useful kind of diversity",
    "start": "505780",
    "end": "508720"
  },
  {
    "text": "into the learned\nrepresentations.",
    "start": "508720",
    "end": "510760"
  },
  {
    "text": "But usually, we do\nsomething like teacher",
    "start": "510760",
    "end": "513380"
  },
  {
    "text": "forcing where even though\nwe got an error signal here,",
    "start": "513380",
    "end": "516200"
  },
  {
    "text": "we use the actual thing that we\nwanted to have predicted down",
    "start": "516200",
    "end": "519740"
  },
  {
    "text": "at the next time step.",
    "start": "519740",
    "end": "521979"
  },
  {
    "text": "That is part of\ntraining the model,",
    "start": "521980",
    "end": "523928"
  },
  {
    "text": "and then when we\nmove to generation,",
    "start": "523929",
    "end": "525610"
  },
  {
    "text": "we do something that's very\nsimilar although with some",
    "start": "525610",
    "end": "528370"
  },
  {
    "text": "twists.",
    "start": "528370",
    "end": "529060"
  },
  {
    "text": "What I've depicted\non the slide here",
    "start": "529060",
    "end": "531010"
  },
  {
    "text": "is something like\nimagining that the user has",
    "start": "531010",
    "end": "533020"
  },
  {
    "text": "prompted the model with the\nsequence start token and the--",
    "start": "533020",
    "end": "538312"
  },
  {
    "text": "and the model has predicted\nRock as the next sequence.",
    "start": "538312",
    "end": "541750"
  },
  {
    "text": "So we copy that over--",
    "start": "541750",
    "end": "543580"
  },
  {
    "text": "that representation.",
    "start": "543580",
    "end": "545080"
  },
  {
    "text": "We process with the\ntransformer as usual,",
    "start": "545080",
    "end": "548140"
  },
  {
    "text": "and we get another prediction.",
    "start": "548140",
    "end": "549580"
  },
  {
    "text": "In this case, it was\nthe sequence rolls.",
    "start": "549580",
    "end": "551710"
  },
  {
    "text": "So now we have generated the\nRock rolls as our sequence.",
    "start": "551710",
    "end": "555850"
  },
  {
    "text": "We copy that over into\nthe next time step.",
    "start": "555850",
    "end": "559490"
  },
  {
    "text": "And then we get along\nas the next token.",
    "start": "559490",
    "end": "562270"
  },
  {
    "text": "Notice that you\nmight have expected",
    "start": "562270",
    "end": "564280"
  },
  {
    "text": "that this would\nbe the Rock rules,",
    "start": "564280",
    "end": "565900"
  },
  {
    "text": "and the model ended up\npredicting something",
    "start": "565900",
    "end": "567790"
  },
  {
    "text": "different that might\nbe in its nature,",
    "start": "567790",
    "end": "570040"
  },
  {
    "text": "maybe that was a mistake,\nmaybe it wasn't, but the point",
    "start": "570040",
    "end": "573130"
  },
  {
    "text": "is that in generation\nwe no longer",
    "start": "573130",
    "end": "575140"
  },
  {
    "text": "have the possibility of doing\nteacher forcing because we",
    "start": "575140",
    "end": "577930"
  },
  {
    "text": "are creating new tokens.",
    "start": "577930",
    "end": "580010"
  },
  {
    "text": "And so we have to use the\nscores that we got up here",
    "start": "580010",
    "end": "583460"
  },
  {
    "text": "to infer a next token,\ncopy that over, condition",
    "start": "583460",
    "end": "587540"
  },
  {
    "text": "the model on that, and have\nthe generation process repeat.",
    "start": "587540",
    "end": "591540"
  },
  {
    "text": "But throughout this entire\nprocess, again, a reminder,",
    "start": "591540",
    "end": "595320"
  },
  {
    "text": "the model does not\npredict tokens.",
    "start": "595320",
    "end": "597600"
  },
  {
    "text": "The model predicts scores\nover the vocabulary",
    "start": "597600",
    "end": "601230"
  },
  {
    "text": "as depicted here, and we do\nsome inferencing to figure out",
    "start": "601230",
    "end": "604860"
  },
  {
    "text": "which token that ought to be.",
    "start": "604860",
    "end": "606630"
  },
  {
    "text": "And as we'll discuss\nlater in the course,",
    "start": "606630",
    "end": "608470"
  },
  {
    "text": "there are lots of schemes for\ndoing that kind of sampling.",
    "start": "608470",
    "end": "611879"
  },
  {
    "text": "You could pick the\nmax scoring one,",
    "start": "611880",
    "end": "613680"
  },
  {
    "text": "but you could also roll out\nover a whole bunch of timesteps",
    "start": "613680",
    "end": "618210"
  },
  {
    "text": "looking at all the\ndifferent predictions",
    "start": "618210",
    "end": "620040"
  },
  {
    "text": "and predict and generate the\nsequence that kind of maximizes",
    "start": "620040",
    "end": "624509"
  },
  {
    "text": "that overall probability, that\nwould be more like beam search.",
    "start": "624510",
    "end": "628090"
  },
  {
    "text": "And that's very different from\nthe kind of maximum probability",
    "start": "628090",
    "end": "631950"
  },
  {
    "text": "step that I'm depicting here.",
    "start": "631950",
    "end": "633810"
  },
  {
    "text": "And that's a nice reminder that\nin generation what we're doing",
    "start": "633810",
    "end": "637950"
  },
  {
    "text": "is applying a decision rule\non top of the representations",
    "start": "637950",
    "end": "641730"
  },
  {
    "text": "that these models have created.",
    "start": "641730",
    "end": "643350"
  },
  {
    "text": "And it's kind of not so\nintrinsic to the models",
    "start": "643350",
    "end": "645750"
  },
  {
    "text": "themselves that they follow\nthat particular decision rule.",
    "start": "645750",
    "end": "649620"
  },
  {
    "text": "That's a complexity\nof generation.",
    "start": "649620",
    "end": "653970"
  },
  {
    "text": "Final step here when we\nthink about fine-tuning GPT,",
    "start": "653970",
    "end": "657480"
  },
  {
    "text": "the standard mode is\nto process a sequence",
    "start": "657480",
    "end": "660990"
  },
  {
    "text": "and then use the\nfinal output state",
    "start": "660990",
    "end": "663180"
  },
  {
    "text": "as the basis for some task\nspecific parameters that you",
    "start": "663180",
    "end": "666660"
  },
  {
    "text": "use to fine-tune on whatever\nsupervised learning task",
    "start": "666660",
    "end": "669959"
  },
  {
    "text": "you're focused on.",
    "start": "669960",
    "end": "671220"
  },
  {
    "text": "But of course, we're\nnot limited to do that.",
    "start": "671220",
    "end": "673170"
  },
  {
    "text": "We could also think about\nusing all of the output states",
    "start": "673170",
    "end": "676529"
  },
  {
    "text": "that the model has created,\nmaybe by doing some kind of max",
    "start": "676530",
    "end": "679680"
  },
  {
    "text": "or mean pooling over them\nto gather more information",
    "start": "679680",
    "end": "682890"
  },
  {
    "text": "from the sequence than is just\ncontained in that final output",
    "start": "682890",
    "end": "686460"
  },
  {
    "text": "state but like, for example,\nin the first GPT paper,",
    "start": "686460",
    "end": "689790"
  },
  {
    "text": "their fine-tuning is\nbased entirely, I believe,",
    "start": "689790",
    "end": "692519"
  },
  {
    "text": "on that final output state.",
    "start": "692520",
    "end": "694170"
  },
  {
    "start": "694170",
    "end": "700868"
  },
  {
    "text": "To round this out,\nI thought I'd just",
    "start": "700868",
    "end": "702410"
  },
  {
    "text": "show you some GPTs that\nhave been released along",
    "start": "702410",
    "end": "705410"
  },
  {
    "text": "with some information\nabout how they're",
    "start": "705410",
    "end": "707209"
  },
  {
    "text": "structured to the extent that\nwe know how they're structured.",
    "start": "707210",
    "end": "709950"
  },
  {
    "text": "So the first GPT had 12 layers\nand a model dimensionality",
    "start": "709950",
    "end": "714260"
  },
  {
    "text": "of 768 and a feed forward\ndimensionality of 3072,",
    "start": "714260",
    "end": "719000"
  },
  {
    "text": "that one point in the model\nwhere you can expand out",
    "start": "719000",
    "end": "721760"
  },
  {
    "text": "before collapsing back to dk\nInside the feed forward layers.",
    "start": "721760",
    "end": "726140"
  },
  {
    "text": "And that gave rise\nto a model that",
    "start": "726140",
    "end": "728090"
  },
  {
    "text": "had 117 million parameters.",
    "start": "728090",
    "end": "731550"
  },
  {
    "text": "GPT-2 scaled that\nup considerably",
    "start": "731550",
    "end": "734339"
  },
  {
    "text": "to 48 layers, 1,600\ndimensionality for the model,",
    "start": "734340",
    "end": "739020"
  },
  {
    "text": "and 1,600 for the\nfeedforward layer",
    "start": "739020",
    "end": "741690"
  },
  {
    "text": "for a total of about\n1.5 billion parameters.",
    "start": "741690",
    "end": "745770"
  },
  {
    "text": "And then GPT-3 had 96 layers and\na massive model dimensionality",
    "start": "745770",
    "end": "752340"
  },
  {
    "text": "over 12,000 for its\nmodel dimensionality.",
    "start": "752340",
    "end": "755640"
  },
  {
    "text": "As far as I know, we don't\nknow the dimensionality",
    "start": "755640",
    "end": "758220"
  },
  {
    "text": "of that inside feedforward layer\nbut it might also be 12,000.",
    "start": "758220",
    "end": "762870"
  },
  {
    "text": "And that gave rise\nto a model that",
    "start": "762870",
    "end": "764580"
  },
  {
    "text": "had 175 billion parameters.",
    "start": "764580",
    "end": "769080"
  },
  {
    "text": "And by the way, the\nGPT-3 paper reports",
    "start": "769080",
    "end": "771560"
  },
  {
    "text": "on models that are kind of\nintermediate in those sizes.",
    "start": "771560",
    "end": "774830"
  },
  {
    "text": "All of those models\nare from OpenAI.",
    "start": "774830",
    "end": "777170"
  },
  {
    "text": "If you want to think about\ntruly open alternatives,",
    "start": "777170",
    "end": "779779"
  },
  {
    "text": "here is a kind of fast\nsummary of the models",
    "start": "779780",
    "end": "782210"
  },
  {
    "text": "that I know about.",
    "start": "782210",
    "end": "783500"
  },
  {
    "text": "This table is probably\nalready hopelessly out of date",
    "start": "783500",
    "end": "786740"
  },
  {
    "text": "by the time you are viewing\nit, but it does give you",
    "start": "786740",
    "end": "789830"
  },
  {
    "text": "a sense for the kinds of\nthings that have happened",
    "start": "789830",
    "end": "792800"
  },
  {
    "text": "on the open source side.",
    "start": "792800",
    "end": "794180"
  },
  {
    "text": "And I would say that the\nhopeful aspect of this",
    "start": "794180",
    "end": "796279"
  },
  {
    "text": "is that there are a lot of these\nmodels now and some of them",
    "start": "796280",
    "end": "799280"
  },
  {
    "text": "are quite competitive in\nterms of their overall size.",
    "start": "799280",
    "end": "802130"
  },
  {
    "text": "For example, the\nBloom model there",
    "start": "802130",
    "end": "804200"
  },
  {
    "text": "has 176 billion parameters,\nand is truly gargantuan",
    "start": "804200",
    "end": "808820"
  },
  {
    "text": "in terms of its\ndimensionalities and so forth.",
    "start": "808820",
    "end": "811530"
  },
  {
    "text": "And there are other some--\nother smaller ones here",
    "start": "811530",
    "end": "813890"
  },
  {
    "text": "that are obviously very\nperformant, very powerful, very",
    "start": "813890",
    "end": "817220"
  },
  {
    "text": "interesting artifacts\nin the GPT mode.",
    "start": "817220",
    "end": "820959"
  },
  {
    "start": "820960",
    "end": "825000"
  }
]