[
  {
    "start": "0",
    "end": "910"
  },
  {
    "text": "So welcome back.",
    "start": "910",
    "end": "2630"
  },
  {
    "text": "So we've talked about a forward\nstepwise selection and backward",
    "start": "2630",
    "end": "5440"
  },
  {
    "text": "stepwise and all subsets,\nas well as some techniques",
    "start": "5440",
    "end": "7840"
  },
  {
    "text": "for choosing the model size.",
    "start": "7840",
    "end": "9530"
  },
  {
    "text": "All of those methods\nfit by least squares.",
    "start": "9530",
    "end": "11599"
  },
  {
    "text": "So whenever we consider\nmodel a subset,",
    "start": "11600",
    "end": "14470"
  },
  {
    "text": "we always fit the\ncoefficients by least squares.",
    "start": "14470",
    "end": "16640"
  },
  {
    "text": "Now we're going to talk about\na different method called",
    "start": "16640",
    "end": "19150"
  },
  {
    "text": "shrinkage, namely two\ntechniques, ridge regression",
    "start": "19150",
    "end": "22180"
  },
  {
    "text": "and lasso.",
    "start": "22180",
    "end": "23290"
  },
  {
    "text": "And as we'll see that\nthese methods do not",
    "start": "23290",
    "end": "26545"
  },
  {
    "text": "use full least squares to fit,\nbut rather a different criterion",
    "start": "26545",
    "end": "29170"
  },
  {
    "text": "that has a penalty\nthat will shrink",
    "start": "29170",
    "end": "30970"
  },
  {
    "text": "the coefficients\ntowards typically 0.",
    "start": "30970",
    "end": "33260"
  },
  {
    "text": "And we'll see these\nmethods are very powerful.",
    "start": "33260",
    "end": "35592"
  },
  {
    "text": "in particular, they can be\napplied to very large data sets,",
    "start": "35592",
    "end": "38050"
  },
  {
    "text": "like the kind that we see more\nand more often these days,",
    "start": "38050",
    "end": "40030"
  },
  {
    "text": "where the number of variables\nmight be in the thousands",
    "start": "40030",
    "end": "41680"
  },
  {
    "text": "or even millions.",
    "start": "41680",
    "end": "42880"
  },
  {
    "text": "And one thing that's\nworth mentioning",
    "start": "42880",
    "end": "46212"
  },
  {
    "text": "is that like ridge regression,\nlasso, and shrinkage methods",
    "start": "46212",
    "end": "48670"
  },
  {
    "text": "like this, are really\ncontemporary area of research",
    "start": "48670",
    "end": "51609"
  },
  {
    "text": "right now.",
    "start": "51610",
    "end": "52690"
  },
  {
    "text": "There's papers every day written\nby statisticians about variants",
    "start": "52690",
    "end": "55433"
  },
  {
    "text": "of these methods and\nhow to improve them",
    "start": "55433",
    "end": "57100"
  },
  {
    "text": "and that sort of\nthing, as opposed",
    "start": "57100",
    "end": "58719"
  },
  {
    "text": "to some of the other\nthings that we've",
    "start": "58720",
    "end": "60303"
  },
  {
    "text": "covered so far in this course,\nwhere the ideas have been around",
    "start": "60303",
    "end": "63080"
  },
  {
    "text": "for 30, 40 years.",
    "start": "63080",
    "end": "64339"
  },
  {
    "text": "Although actually\nfor ridge regression,",
    "start": "64340",
    "end": "66079"
  },
  {
    "text": "I think was invented in the\n'70s, but it is true that it",
    "start": "66080",
    "end": "69350"
  },
  {
    "text": "wasn't very popular\nfor many years,",
    "start": "69350",
    "end": "71040"
  },
  {
    "text": "there's only with the advent of\nfast computation in the last 10",
    "start": "71040",
    "end": "74960"
  },
  {
    "text": "years that it's become very\npopular along with the lasso.",
    "start": "74960",
    "end": "77760"
  },
  {
    "text": "So some of these methods\nare old, some are new,",
    "start": "77760",
    "end": "80390"
  },
  {
    "text": "but they're really quite hot\nnow in their applications.",
    "start": "80390",
    "end": "83600"
  },
  {
    "text": "So ridge regression.",
    "start": "83600",
    "end": "84882"
  },
  {
    "text": "Well, let's first of all,\njust remind ourselves",
    "start": "84882",
    "end": "86840"
  },
  {
    "text": "what least squares is,\nthe training error.",
    "start": "86840",
    "end": "91609"
  },
  {
    "text": "So RSS, or training error, is\nthe sum of squares deviation",
    "start": "91610",
    "end": "95660"
  },
  {
    "text": "between Y and the predictions.",
    "start": "95660",
    "end": "97340"
  },
  {
    "text": "So when we do least\nsquares, we simply",
    "start": "97340",
    "end": "99409"
  },
  {
    "text": "find the coefficients that\nmake this as small as possible.",
    "start": "99410",
    "end": "102750"
  },
  {
    "text": "Now with ridge\nregression, we're going",
    "start": "102750",
    "end": "104525"
  },
  {
    "text": "to do something a little\nbit different, we're going",
    "start": "104525",
    "end": "106650"
  },
  {
    "text": "to use training error, RSS.",
    "start": "106650",
    "end": "108160"
  },
  {
    "text": "So we're going to add a\npenalty to that, namely",
    "start": "108160",
    "end": "112460"
  },
  {
    "text": "a tuning parameter, which\nwe'll choose in some way,",
    "start": "112460",
    "end": "115170"
  },
  {
    "text": "and as you can guess, it'll\nbe by cross-validation,",
    "start": "115170",
    "end": "117710"
  },
  {
    "text": "times the sum of the\nsquares of the coefficients.",
    "start": "117710",
    "end": "120650"
  },
  {
    "text": "So we're going to try to make\nthis total quantity small, which",
    "start": "120650",
    "end": "125060"
  },
  {
    "text": "means we're going to try to\nmake the fit good by making",
    "start": "125060",
    "end": "128119"
  },
  {
    "text": "the RSS small.",
    "start": "128120",
    "end": "128845"
  },
  {
    "text": "But at the same\ntime, we're going",
    "start": "128845",
    "end": "130220"
  },
  {
    "text": "to have something which is going\nto push us the other direction.",
    "start": "130220",
    "end": "132450"
  },
  {
    "text": "This is going to\npenalize coefficients",
    "start": "132450",
    "end": "134420"
  },
  {
    "text": "which get too large.",
    "start": "134420",
    "end": "136760"
  },
  {
    "text": "The more non-zero coefficient\nis, the larger this is.",
    "start": "136760",
    "end": "140030"
  },
  {
    "text": "So we're going to pay a\nprice for being non-zero,",
    "start": "140030",
    "end": "142550"
  },
  {
    "text": "and we'll pay a bigger price\nthat the larger the coefficients",
    "start": "142550",
    "end": "146270"
  },
  {
    "text": "are.",
    "start": "146270",
    "end": "146930"
  },
  {
    "text": "So it's basically fit versus\nthe size of the coefficients.",
    "start": "146930",
    "end": "150959"
  },
  {
    "start": "150960",
    "end": "154880"
  },
  {
    "text": "And that penalty is called\na shrinkage penalty,",
    "start": "154880",
    "end": "158720"
  },
  {
    "text": "because it's going to\nencourage the parameters to be",
    "start": "158720",
    "end": "162260"
  },
  {
    "text": "shrunk towards 0.",
    "start": "162260",
    "end": "163700"
  },
  {
    "text": "And the amount by which\nthey're encouraged to be 0",
    "start": "163700",
    "end": "167120"
  },
  {
    "text": "will be determined by this\ntuning parameter, lambda.",
    "start": "167120",
    "end": "170599"
  },
  {
    "text": "If lambda is 0, let's\ngo back to this.",
    "start": "170600",
    "end": "172183"
  },
  {
    "text": "If lambda is 0, of\ncourse, we're just",
    "start": "172183",
    "end": "173725"
  },
  {
    "text": "doing least squares,\nbecause this term will be 0.",
    "start": "173725",
    "end": "175970"
  },
  {
    "text": "But the larger lambda is,\nthis tuning parameter,",
    "start": "175970",
    "end": "179310"
  },
  {
    "text": "the more and more\nof a price will",
    "start": "179310",
    "end": "181489"
  },
  {
    "text": "pay for making these\ncoefficients non-zero.",
    "start": "181490",
    "end": "183690"
  },
  {
    "text": "If we make lambda extremely\nlarge, then at some point",
    "start": "183690",
    "end": "186498"
  },
  {
    "text": "the coefficients are going\nto be very close to 0,",
    "start": "186498",
    "end": "188540"
  },
  {
    "text": "because they'll have\nto be close to 0",
    "start": "188540",
    "end": "190489"
  },
  {
    "text": "to make this term small enough.",
    "start": "190490",
    "end": "193410"
  },
  {
    "text": "No matter how good\nthey help to fit,",
    "start": "193410",
    "end": "195440"
  },
  {
    "text": "we're going to pay a large\nprice in the penalty term.",
    "start": "195440",
    "end": "202040"
  },
  {
    "text": "So in other words, this\nterm has the effect",
    "start": "202040",
    "end": "205519"
  },
  {
    "text": "of shrinking the\ncoefficients towards 0.",
    "start": "205520",
    "end": "207730"
  },
  {
    "text": "And why 0?",
    "start": "207730",
    "end": "208709"
  },
  {
    "text": "Well, 0 is a natural value.",
    "start": "208710",
    "end": "210250"
  },
  {
    "text": "Remember, zero, of course,\nif a coefficient is 0,",
    "start": "210250",
    "end": "212390"
  },
  {
    "text": "the feature's not\nappearing in the model.",
    "start": "212390",
    "end": "214420"
  },
  {
    "text": "So if you're going to\nshrink towards some value,",
    "start": "214420",
    "end": "217890"
  },
  {
    "text": "a 0 is a natural place\nto shrink towards.",
    "start": "217890",
    "end": "222690"
  },
  {
    "text": "And the size of the\ntuning parameter,",
    "start": "222690",
    "end": "225840"
  },
  {
    "text": "it trades off the fit versus\nthe size of the coefficients.",
    "start": "225840",
    "end": "229930"
  },
  {
    "text": "So as you can imagine, choosing\na good value for the tuning",
    "start": "229930",
    "end": "234180"
  },
  {
    "text": "parameter lambda is critical,\nand cross-validation is what",
    "start": "234180",
    "end": "237329"
  },
  {
    "text": "we're going to use for this.",
    "start": "237330",
    "end": "239700"
  },
  {
    "text": "So let's see what happens\nfor the credit data.",
    "start": "239700",
    "end": "244208"
  },
  {
    "text": "Well, let me just go back here.",
    "start": "244208",
    "end": "245500"
  },
  {
    "text": "So if we think of this for\na fixed value of lambda,",
    "start": "245500",
    "end": "250600"
  },
  {
    "text": "we have to find the smallest\nvalue of this criterion.",
    "start": "250600",
    "end": "254210"
  },
  {
    "text": "And again, it's just\nan optimization problem",
    "start": "254210",
    "end": "256570"
  },
  {
    "text": "with actually a very\nsimple solution.",
    "start": "256570",
    "end": "258579"
  },
  {
    "text": "And there are computer\nprograms for doing that.",
    "start": "258579",
    "end": "261290"
  },
  {
    "text": "So I've used such a\nprogram, and we'll",
    "start": "261290",
    "end": "263140"
  },
  {
    "text": "talk in the ActionR\nsession about it,",
    "start": "263140",
    "end": "265330"
  },
  {
    "text": "an R function for\ndoing ridge regression.",
    "start": "265330",
    "end": "267110"
  },
  {
    "text": "But let's see what it looks\nlike in the credit example.",
    "start": "267110",
    "end": "270129"
  },
  {
    "text": "So here we've plotted the\nstandardized coefficients",
    "start": "270130",
    "end": "274990"
  },
  {
    "text": "versus lambda for\nthe credit data.",
    "start": "274990",
    "end": "277900"
  },
  {
    "text": "And let's see what happens.",
    "start": "277900",
    "end": "279320"
  },
  {
    "text": "Well, first of all, on the left\nhand side, lambda is close to 0,",
    "start": "279320",
    "end": "283040"
  },
  {
    "text": "so there's almost no\nconstraint on the coefficients.",
    "start": "283040",
    "end": "285377"
  },
  {
    "text": "So here we're pretty much\ngetting the full least squares",
    "start": "285377",
    "end": "287710"
  },
  {
    "text": "estimates.",
    "start": "287710",
    "end": "288669"
  },
  {
    "text": "And now as lambda\ngets larger, it's",
    "start": "288670",
    "end": "290380"
  },
  {
    "text": "pushing the\ncoefficients towards 0,",
    "start": "290380",
    "end": "291850"
  },
  {
    "text": "because we're paying\nmore and more of a price",
    "start": "291850",
    "end": "293725"
  },
  {
    "text": "for being non-zero.",
    "start": "293725",
    "end": "295020"
  },
  {
    "text": "In the extreme over here,\nwhere lambda is maybe more than",
    "start": "295020",
    "end": "298710"
  },
  {
    "text": "10,000, the coefficients\nare all essentially 0.",
    "start": "298710",
    "end": "303110"
  },
  {
    "text": "In between they've shrunken\ntowards 0 as lambda gets larger,",
    "start": "303110",
    "end": "307969"
  },
  {
    "text": "although not uniformly\nso, the rating variable",
    "start": "307970",
    "end": "313820"
  },
  {
    "text": "actually gets\nbigger for a while,",
    "start": "313820",
    "end": "315530"
  },
  {
    "text": "and then shrinks\nback down to zero.",
    "start": "315530",
    "end": "317930"
  },
  {
    "text": "And I remember when I was a\nstudent, seeing this figure",
    "start": "317930",
    "end": "320449"
  },
  {
    "text": "again and again, actually,\nit was in a class",
    "start": "320450",
    "end": "322283"
  },
  {
    "text": "that Rob was teaching, and\nbeing totally confused for three",
    "start": "322283",
    "end": "325280"
  },
  {
    "text": "lectures.",
    "start": "325280",
    "end": "326040"
  },
  {
    "text": "So I just want to spare everyone\nthis confusion in case anyone",
    "start": "326040",
    "end": "328670"
  },
  {
    "text": "shares the confusion I had.",
    "start": "328670",
    "end": "330000"
  },
  {
    "text": "So if we look\nhere, this red line",
    "start": "330000",
    "end": "332570"
  },
  {
    "text": "indicates the spot at\nwhich lambda equals 100.",
    "start": "332570",
    "end": "335400"
  },
  {
    "text": "And at that spot, the\nincome coefficient",
    "start": "335400",
    "end": "337850"
  },
  {
    "text": "takes on a value\nof negative 100.",
    "start": "337850",
    "end": "340880"
  },
  {
    "text": "These other six coefficients\nhere are all around zero.",
    "start": "340880",
    "end": "344290"
  },
  {
    "text": "Student takes on a\nvalue of around 100,",
    "start": "344290",
    "end": "347090"
  },
  {
    "text": "limit and rating take\non values of around 250.",
    "start": "347090",
    "end": "349500"
  },
  {
    "text": "And so the point is,\nwhat we're plotting here",
    "start": "349500",
    "end": "351800"
  },
  {
    "text": "is a ton of different models for\na huge grid of lambda values,",
    "start": "351800",
    "end": "355103"
  },
  {
    "text": "and you just need to\nchoose a value of lambda,",
    "start": "355103",
    "end": "357020"
  },
  {
    "text": "and then look at that\nvertical cross-section.",
    "start": "357020",
    "end": "360050"
  },
  {
    "text": "And so, as Danielle said, if we\nchose the value of lambda about,",
    "start": "360050",
    "end": "363979"
  },
  {
    "text": "was that 100, then\nit would seems",
    "start": "363980",
    "end": "366680"
  },
  {
    "text": "like it chooses about three\nnon-zero coefficients,",
    "start": "366680",
    "end": "369259"
  },
  {
    "text": "the black, the blue,\nand the red, maybe four.",
    "start": "369260",
    "end": "372607"
  },
  {
    "text": "And then these guys here are\nbasically essentially zero,",
    "start": "372607",
    "end": "374940"
  },
  {
    "text": "the gray ones.",
    "start": "374940",
    "end": "375523"
  },
  {
    "text": "So they're not quite\n0, but they're small.",
    "start": "375523",
    "end": "377320"
  },
  {
    "text": "Right, exactly.",
    "start": "377320",
    "end": "378150"
  },
  {
    "text": "So an equivalent picture\non the right now,",
    "start": "378150",
    "end": "379910"
  },
  {
    "text": "we've plotted the\nstandardized coefficients",
    "start": "379910",
    "end": "383330"
  },
  {
    "text": "as a function of the L2 norm,\nthe sums of the squares,",
    "start": "383330",
    "end": "388909"
  },
  {
    "text": "the square root of the sum of\nthe squares of the coefficients,",
    "start": "388910",
    "end": "391730"
  },
  {
    "text": "divided by the L2 norm of the\nfull least squares coefficient.",
    "start": "391730",
    "end": "395460"
  },
  {
    "text": "So Rob, what's the L2 norm?",
    "start": "395460",
    "end": "396755"
  },
  {
    "start": "396755",
    "end": "400410"
  },
  {
    "text": "So the L2 norm of the\nbeta of the vector",
    "start": "400410",
    "end": "402410"
  },
  {
    "text": "beta 1 through beta p,\nit's written this way.",
    "start": "402410",
    "end": "408810"
  },
  {
    "text": "So the L2 norm, so it's a\nsquare root of the sum of beta j",
    "start": "408810",
    "end": "415520"
  },
  {
    "text": "squared.",
    "start": "415520",
    "end": "417080"
  },
  {
    "text": "So this is z.",
    "start": "417080",
    "end": "419081"
  },
  {
    "text": "That's the L2 norm.",
    "start": "419081",
    "end": "420310"
  },
  {
    "start": "420310",
    "end": "427400"
  },
  {
    "text": "So we see here when\nthe L2 norm is 0,",
    "start": "427400",
    "end": "431212"
  },
  {
    "text": "so when the\ncoefficients are all 0,",
    "start": "431212",
    "end": "432670"
  },
  {
    "text": "the L2 norm is zero,\nthat corresponds",
    "start": "432670",
    "end": "434980"
  },
  {
    "text": "to the right side\nof this picture.",
    "start": "434980",
    "end": "438470"
  },
  {
    "text": "So the lambda is\nvery large here,",
    "start": "438470",
    "end": "440260"
  },
  {
    "text": "it's large enough that it's\ndriven the L2 norm down almost",
    "start": "440260",
    "end": "442960"
  },
  {
    "text": "to 0, so the coefficients\nare basically 0.",
    "start": "442960",
    "end": "445720"
  },
  {
    "text": "And then on the right,\nthe lambda is very small,",
    "start": "445720",
    "end": "450520"
  },
  {
    "text": "and we get the full\nleast squares estimates.",
    "start": "450520",
    "end": "453310"
  },
  {
    "text": "And in between we get, again,\na shrunken coefficient.",
    "start": "453310",
    "end": "455900"
  },
  {
    "text": "So these two pictures\nare very the same,",
    "start": "455900",
    "end": "457567"
  },
  {
    "text": "but they've been flipped\nfrom left to right.",
    "start": "457567",
    "end": "459810"
  },
  {
    "text": "And the x-axis are parameterized\nin a different way.",
    "start": "459810",
    "end": "462360"
  },
  {
    "text": "So, Rob, why does this\nx-axis on the right hand side",
    "start": "462360",
    "end": "465539"
  },
  {
    "text": "go from 0 to 1, why\ndoes it end at one?",
    "start": "465540",
    "end": "469530"
  },
  {
    "text": "It ends at 1, because\nwe're plotting it",
    "start": "469530",
    "end": "471360"
  },
  {
    "text": "as a function of this\nstandardized L2 norm.",
    "start": "471360",
    "end": "474340"
  },
  {
    "text": "So at the right hand side, we\nhave the full least squares",
    "start": "474340",
    "end": "479940"
  },
  {
    "text": "estimates, so these numerator\nand denominator are the same.",
    "start": "479940",
    "end": "482470"
  },
  {
    "text": "So on the right-hand side\nhere, on this right hand plot,",
    "start": "482470",
    "end": "484803"
  },
  {
    "text": "lambda is zero.",
    "start": "484803",
    "end": "486210"
  },
  {
    "text": "And so your ridge\nregression estimate",
    "start": "486210",
    "end": "487788"
  },
  {
    "text": "is the same as your\nleast squares estimate.",
    "start": "487788",
    "end": "489580"
  },
  {
    "text": "And so that ratio is just one.",
    "start": "489580",
    "end": "492490"
  },
  {
    "text": "Exactly.",
    "start": "492490",
    "end": "492990"
  },
  {
    "start": "492990",
    "end": "495630"
  },
  {
    "text": "I think we've actually\njust said all this.",
    "start": "495630",
    "end": "498780"
  },
  {
    "text": "Thanks to the questions\nfrom Daniella.",
    "start": "498780",
    "end": "501600"
  },
  {
    "text": "So there's the L2 norm\ndefined, I wrote it",
    "start": "501600",
    "end": "504210"
  },
  {
    "text": "in the previous\nslide, and that's",
    "start": "504210",
    "end": "506639"
  },
  {
    "text": "what was used for\nthe plotting axis.",
    "start": "506640",
    "end": "509960"
  },
  {
    "text": "One important point\nwith ridge regression,",
    "start": "509960",
    "end": "511710"
  },
  {
    "text": "is that it matters whether you\nscale the variables or not.",
    "start": "511710",
    "end": "515459"
  },
  {
    "text": "Now, just to remind\nyou, if you just",
    "start": "515460",
    "end": "517710"
  },
  {
    "text": "do standard least squares,\nso standard least squares",
    "start": "517710",
    "end": "520059"
  },
  {
    "text": "is called scale invariant.",
    "start": "520059",
    "end": "521570"
  },
  {
    "text": "If I multiply a\nfeature by a constant,",
    "start": "521570",
    "end": "523780"
  },
  {
    "text": "it's not going to\nmatter, because I",
    "start": "523780",
    "end": "525410"
  },
  {
    "text": "can divide the coefficient\nby the same constant,",
    "start": "525410",
    "end": "527410"
  },
  {
    "text": "I get the same answer.",
    "start": "527410",
    "end": "528620"
  },
  {
    "text": "So in least squares, the scaling\nof the variable doesn't matter.",
    "start": "528620",
    "end": "534290"
  },
  {
    "text": "So whether I measure a\nlength in feet or inches,",
    "start": "534290",
    "end": "536380"
  },
  {
    "text": "it's not going to matter\nbecause, the coefficient can",
    "start": "536380",
    "end": "538588"
  },
  {
    "text": "just account for\nthe change in units.",
    "start": "538588",
    "end": "541360"
  },
  {
    "text": "But it's a little\nbit subtle here now,",
    "start": "541360",
    "end": "543010"
  },
  {
    "text": "for ridge regression\nand penalize methods,",
    "start": "543010",
    "end": "545200"
  },
  {
    "text": "like ridge regression,\nthe scaling does",
    "start": "545200",
    "end": "547300"
  },
  {
    "text": "matter in an important way.",
    "start": "547300",
    "end": "549339"
  },
  {
    "text": "And that's because,\nif you go back",
    "start": "549340",
    "end": "551470"
  },
  {
    "text": "to the definition\nof ridge regression,",
    "start": "551470",
    "end": "556420"
  },
  {
    "text": "these coefficients are all\nput in a penalty together,",
    "start": "556420",
    "end": "559089"
  },
  {
    "text": "and there's a sum of squares.",
    "start": "559090",
    "end": "561620"
  },
  {
    "text": "So if I change the\nunits of one variable,",
    "start": "561620",
    "end": "563355"
  },
  {
    "text": "it's going to change the\nscale of the coefficients.",
    "start": "563355",
    "end": "565480"
  },
  {
    "text": "Well, if I change the\nunits of one variable,",
    "start": "565480",
    "end": "568149"
  },
  {
    "text": "the coefficient has\nto change to try",
    "start": "568150",
    "end": "570353"
  },
  {
    "text": "to accommodate for\nthat change, but it's",
    "start": "570353",
    "end": "572020"
  },
  {
    "text": "competing against the\ncoefficients for other features.",
    "start": "572020",
    "end": "575420"
  },
  {
    "text": "So because they're all put\ntogether in a penalty in a sum,",
    "start": "575420",
    "end": "580480"
  },
  {
    "text": "the scale of the\nfeatures matters.",
    "start": "580480",
    "end": "584089"
  },
  {
    "text": "And as a result, it's\nimportant to standardize",
    "start": "584090",
    "end": "587390"
  },
  {
    "text": "the predictors in\nregression before applying",
    "start": "587390",
    "end": "589460"
  },
  {
    "text": "ridge regression.",
    "start": "589460",
    "end": "590370"
  },
  {
    "text": "So in most cases, before\nyou do ridge regression,",
    "start": "590370",
    "end": "593947"
  },
  {
    "text": "you want to standardize\nthe variables.",
    "start": "593947",
    "end": "595530"
  },
  {
    "text": "What does that mean to\nstandardize the variables.",
    "start": "595530",
    "end": "597572"
  },
  {
    "text": "Well, you take the\nvariable or feature",
    "start": "597572",
    "end": "599420"
  },
  {
    "text": "and you divide it by\nthe standard deviation",
    "start": "599420",
    "end": "601220"
  },
  {
    "text": "of that feature over\nall the observations.",
    "start": "601220",
    "end": "604040"
  },
  {
    "text": "And now the standard\ndeviation of this guy is one.",
    "start": "604040",
    "end": "607295"
  },
  {
    "text": "And you do that\nfor each feature,",
    "start": "607295",
    "end": "608670"
  },
  {
    "text": "and that makes the\nfeatures comparable",
    "start": "608670",
    "end": "611120"
  },
  {
    "text": "and makes their\ncoefficients comparable.",
    "start": "611120",
    "end": "614040"
  },
  {
    "text": "So that's an example\nof ridge regression",
    "start": "614040",
    "end": "616620"
  },
  {
    "text": "compared to least squares.",
    "start": "616620",
    "end": "617770"
  },
  {
    "text": "Here's a simulated example\nwith 50 observations and 45",
    "start": "617770",
    "end": "620490"
  },
  {
    "text": "predictors in which\nall of the predictors",
    "start": "620490",
    "end": "622560"
  },
  {
    "text": "have been given\nnon-zero coefficients.",
    "start": "622560",
    "end": "624870"
  },
  {
    "text": "So on the left, we see the bias\nin black, the variance in green,",
    "start": "624870",
    "end": "631320"
  },
  {
    "text": "and the test error in\npurple for ridge regression",
    "start": "631320",
    "end": "634200"
  },
  {
    "text": "as a function of lambda.",
    "start": "634200",
    "end": "635368"
  },
  {
    "text": "And the same thing on the\nright as a function now",
    "start": "635368",
    "end": "637410"
  },
  {
    "text": "of the standardized\ncoefficients of the 2",
    "start": "637410",
    "end": "640769"
  },
  {
    "text": "norm of ridge regression\ndivided by the 2 norm",
    "start": "640770",
    "end": "645060"
  },
  {
    "text": "of the full least squares.",
    "start": "645060",
    "end": "646150"
  },
  {
    "text": "So again, this is the\nsame pictures essentially,",
    "start": "646150",
    "end": "648150"
  },
  {
    "text": "but the x-axis\nhave been changed.",
    "start": "648150",
    "end": "650130"
  },
  {
    "text": "But let's look over here, we can\nsee that as we move, so fully",
    "start": "650130",
    "end": "658290"
  },
  {
    "text": "squares is over here on the\nleft, lambda is close to zero.",
    "start": "658290",
    "end": "661959"
  },
  {
    "text": "As we make lambda larger, the\nbias is pretty much unchanged,",
    "start": "661960",
    "end": "665370"
  },
  {
    "text": "but the variance drops.",
    "start": "665370",
    "end": "667500"
  },
  {
    "text": "So ridge regression, by\nshrinking the coefficient",
    "start": "667500",
    "end": "670740"
  },
  {
    "text": "towards 0 controls\nthe variance, it",
    "start": "670740",
    "end": "672570"
  },
  {
    "text": "doesn't allow the\ncoefficients to be too big.",
    "start": "672570",
    "end": "674860"
  },
  {
    "text": "And it gets rewarded, because\nthe mean square error,",
    "start": "674860",
    "end": "677339"
  },
  {
    "text": "which is the sum of\nthese two, goes down.",
    "start": "677340",
    "end": "679920"
  },
  {
    "text": "So here's the place where\nthe mean square is minimized,",
    "start": "679920",
    "end": "683070"
  },
  {
    "text": "and it's lower than that for\nthe fully squared estimate.",
    "start": "683070",
    "end": "686380"
  },
  {
    "text": "So in this example,\nridge regression",
    "start": "686380",
    "end": "688290"
  },
  {
    "text": "has improved the\nmean square error",
    "start": "688290",
    "end": "690329"
  },
  {
    "text": "of fully squares by\nshrinking the coefficients",
    "start": "690330",
    "end": "694740"
  },
  {
    "text": "by a certain amount.",
    "start": "694740",
    "end": "695950"
  },
  {
    "text": "And we see the same\nthing in this picture.",
    "start": "695950",
    "end": "697740"
  },
  {
    "text": "And actually this\nu-shaped curve that we",
    "start": "697740",
    "end": "699810"
  },
  {
    "text": "see for the mean squared\nerror in this figure in purple",
    "start": "699810",
    "end": "702900"
  },
  {
    "text": "comes up again and\nagain, where, when",
    "start": "702900",
    "end": "704760"
  },
  {
    "text": "we're considering a bunch\nof different models, that",
    "start": "704760",
    "end": "707130"
  },
  {
    "text": "have different levels of\nflexibility or complexity,",
    "start": "707130",
    "end": "709740"
  },
  {
    "text": "there's usually some sweet\nspot in the middle that",
    "start": "709740",
    "end": "711930"
  },
  {
    "text": "has the smallest test\nerror, and that's",
    "start": "711930",
    "end": "713640"
  },
  {
    "text": "really what we're going for.",
    "start": "713640",
    "end": "714870"
  },
  {
    "text": "So that's what's marked as\nan x in these two figures.",
    "start": "714870",
    "end": "719190"
  },
  {
    "text": "So I want to go back to\nthis picture of ridge.",
    "start": "719190",
    "end": "722345"
  },
  {
    "start": "722345",
    "end": "725829"
  },
  {
    "text": "And one thing you might have\nnoticed here, that we mentioned",
    "start": "725830",
    "end": "728330"
  },
  {
    "text": "is, that the\ncoefficients are never",
    "start": "728330",
    "end": "729320"
  },
  {
    "text": "exactly 0, they're close to 0,\nlike here, these gray guys are",
    "start": "729320",
    "end": "732140"
  },
  {
    "text": "all close here, but\nthey're never exactly 0.",
    "start": "732140",
    "end": "734850"
  },
  {
    "text": "Matter of fact, you\ncan show mathematically",
    "start": "734850",
    "end": "736699"
  },
  {
    "text": "that, unless you're\nextremely lucky,",
    "start": "736700",
    "end": "738605"
  },
  {
    "text": "you're not going to get a\ncoefficient exactly of 0.",
    "start": "738605",
    "end": "740730"
  },
  {
    "text": "So ridge regression\nshrinks things",
    "start": "740730",
    "end": "742555"
  },
  {
    "text": "in a continuous\nway towards zero,",
    "start": "742555",
    "end": "743930"
  },
  {
    "text": "but doesn't actually\nselect variables",
    "start": "743930",
    "end": "745880"
  },
  {
    "text": "by setting a coefficient\nequal to 0 exactly.",
    "start": "745880",
    "end": "748042"
  },
  {
    "text": "But it seems like a\npity in this example,",
    "start": "748042",
    "end": "749750"
  },
  {
    "text": "because all those gray\nvariables are so tiny,",
    "start": "749750",
    "end": "751720"
  },
  {
    "text": "it would just be more\nconvenient if they were 0.",
    "start": "751720",
    "end": "753720"
  },
  {
    "text": "Which is a great setup, where\nthe next method called the lasso",
    "start": "753720",
    "end": "758170"
  }
]