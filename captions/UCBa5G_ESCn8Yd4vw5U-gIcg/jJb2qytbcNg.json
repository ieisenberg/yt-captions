[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "Welcome back to the course.",
    "start": "0",
    "end": "2690"
  },
  {
    "text": "In this series of\nsessions, we're",
    "start": "2690",
    "end": "4880"
  },
  {
    "text": "going to be talking\nabout deep learning.",
    "start": "4880",
    "end": "6750"
  },
  {
    "text": "This is one of the new\nchapters in the second edition",
    "start": "6750",
    "end": "9350"
  },
  {
    "text": "of Introduction to\nStatistical Learning.",
    "start": "9350",
    "end": "11790"
  },
  {
    "text": "Deep learning is a new\nname for neural networks,",
    "start": "11790",
    "end": "14660"
  },
  {
    "text": "which became popular\nin the 1980s.",
    "start": "14660",
    "end": "17100"
  },
  {
    "text": "There were lots of successes,\nhype, and great conferences.",
    "start": "17100",
    "end": "20940"
  },
  {
    "text": "That's where the start of\nthe NeurIPS sequence were",
    "start": "20940",
    "end": "23450"
  },
  {
    "text": "and the Snowbird sequence.",
    "start": "23450",
    "end": "26030"
  },
  {
    "text": "And Rob, you and I went--",
    "start": "26030",
    "end": "28130"
  },
  {
    "text": "we were regular attendees\nof those conferences.",
    "start": "28130",
    "end": "30869"
  },
  {
    "text": "Yeah, from about '85\nto into the '90s.",
    "start": "30870",
    "end": "34130"
  },
  {
    "text": "Initially, one of the\nattractions, apart from all",
    "start": "34130",
    "end": "36620"
  },
  {
    "text": "the really interesting work, was\nthey were held in ski resorts.",
    "start": "36620",
    "end": "41430"
  },
  {
    "text": "So it was an opportunity to\nboth learn interesting new stuff",
    "start": "41430",
    "end": "44660"
  },
  {
    "text": "and get some skiing in.",
    "start": "44660",
    "end": "46520"
  },
  {
    "text": "Then along came support\nvector machines,",
    "start": "46520",
    "end": "48890"
  },
  {
    "text": "random forests and boosting,\nwhich you've learnt about",
    "start": "48890",
    "end": "51920"
  },
  {
    "text": "in this course or\nabout to learn about.",
    "start": "51920",
    "end": "54480"
  },
  {
    "text": "And the neural networks\ntook a bit of a back seat.",
    "start": "54480",
    "end": "57680"
  },
  {
    "text": "They reemerged around 2010\nwith a new name, deep learning.",
    "start": "57680",
    "end": "62220"
  },
  {
    "text": "And by the 2020s they became\nvery dominant and successful.",
    "start": "62220",
    "end": "67860"
  },
  {
    "text": "And they're used all\nover the place now.",
    "start": "67860",
    "end": "70620"
  },
  {
    "text": "So part of the success was\ndue to the vast improvements",
    "start": "70620",
    "end": "74700"
  },
  {
    "text": "in computing power, much bigger\ntraining sets and software.",
    "start": "74700",
    "end": "79399"
  },
  {
    "text": "The main software\nplatforms are TensorFlow,",
    "start": "79400",
    "end": "82270"
  },
  {
    "text": "which is a Google software\nand PyTorch, which",
    "start": "82270",
    "end": "86530"
  },
  {
    "text": "comes from Facebook.",
    "start": "86530",
    "end": "89280"
  },
  {
    "text": "But much of the credit\ngoes to three pioneers",
    "start": "89280",
    "end": "91979"
  },
  {
    "text": "and their students.",
    "start": "91980",
    "end": "93770"
  },
  {
    "text": "There's Yann LeCun, Geoffrey\nHinton, and Yoshua Bengio,",
    "start": "93770",
    "end": "97649"
  },
  {
    "text": "who received the 2019 ACM\nTuring Award for their work",
    "start": "97650",
    "end": "102300"
  },
  {
    "text": "in neural networks.",
    "start": "102300",
    "end": "103930"
  },
  {
    "text": "And you'll find on the\nwebsite for the course you do,",
    "start": "103930",
    "end": "107010"
  },
  {
    "text": "Trevor and I have an interview\nwith Jeff Hinton talking about",
    "start": "107010",
    "end": "110010"
  },
  {
    "text": "the history of neural networks\nand their development since",
    "start": "110010",
    "end": "113820"
  },
  {
    "text": "the '80s.",
    "start": "113820",
    "end": "114400"
  },
  {
    "text": "Yes.",
    "start": "114400",
    "end": "114900"
  },
  {
    "text": "And we've known these\nthree gentlemen pretty",
    "start": "114900",
    "end": "117720"
  },
  {
    "text": "much for their whole careers.",
    "start": "117720",
    "end": "121170"
  },
  {
    "text": "Yann LeCun was at Bell\nLabs in the '80s and '90s.",
    "start": "121170",
    "end": "126479"
  },
  {
    "text": "And I was there\nat the same time.",
    "start": "126480",
    "end": "128380"
  },
  {
    "text": "Yoshua Bengio was\na frequent visitor.",
    "start": "128380",
    "end": "132150"
  },
  {
    "text": "And Rob worked with Jeff Hinton\nat University of Toronto.",
    "start": "132150",
    "end": "138180"
  },
  {
    "start": "138000",
    "end": "138000"
  },
  {
    "text": "OK, so let's get started.",
    "start": "138180",
    "end": "140040"
  },
  {
    "text": "We'll start off with a really\nsimple single layer neural",
    "start": "140040",
    "end": "143519"
  },
  {
    "text": "network, sometimes called a\nfeedforward neural network.",
    "start": "143520",
    "end": "148140"
  },
  {
    "text": "And neural networks\nare often displayed",
    "start": "148140",
    "end": "150580"
  },
  {
    "text": "using what's known\nas network diagrams.",
    "start": "150580",
    "end": "153710"
  },
  {
    "text": "And here we see such\na network diagram.",
    "start": "153710",
    "end": "156760"
  },
  {
    "text": "In the orange, we've\ngot the input layer.",
    "start": "156760",
    "end": "159700"
  },
  {
    "text": "Here in this example\nwe've got four variables.",
    "start": "159700",
    "end": "162849"
  },
  {
    "text": "And then we have what's\nknown as a hidden layer.",
    "start": "162850",
    "end": "166270"
  },
  {
    "text": "And there's five units\nin the hidden layer here.",
    "start": "166270",
    "end": "170110"
  },
  {
    "text": "And then there's\nan output layer.",
    "start": "170110",
    "end": "172780"
  },
  {
    "text": "And the way you think\nof the hidden layer",
    "start": "172780",
    "end": "174880"
  },
  {
    "text": "is you think of them as\ntransformations of the inputs.",
    "start": "174880",
    "end": "180340"
  },
  {
    "text": "The A stands for activations.",
    "start": "180340",
    "end": "182170"
  },
  {
    "text": "And we'll get into the\ndetailed notation in a moment.",
    "start": "182170",
    "end": "185450"
  },
  {
    "text": "So where is the observed data?",
    "start": "185450",
    "end": "186700"
  },
  {
    "text": "Is it all observed?",
    "start": "186700",
    "end": "188650"
  },
  {
    "text": "Rob, this is observed.",
    "start": "188650",
    "end": "189849"
  },
  {
    "text": "The X's are observed.",
    "start": "189850",
    "end": "191920"
  },
  {
    "text": "And the output Y is observed.",
    "start": "191920",
    "end": "194780"
  },
  {
    "text": "That's a response.",
    "start": "194780",
    "end": "196510"
  },
  {
    "text": "But the A's aren't observed.",
    "start": "196510",
    "end": "198055"
  },
  {
    "text": "I actually knew the answer.",
    "start": "198055",
    "end": "199180"
  },
  {
    "text": "You knew the answer.",
    "start": "199180",
    "end": "200012"
  },
  {
    "text": "I thought it'd be\ngood to explain it.",
    "start": "200013",
    "end": "202060"
  },
  {
    "text": "And these activations\nare computed.",
    "start": "202060",
    "end": "204590"
  },
  {
    "text": "In fact, in this equation\nover here, they named hk of X.",
    "start": "204590",
    "end": "209900"
  },
  {
    "text": "So the k-th activation is a\ntransformation of the inputs X.",
    "start": "209900",
    "end": "214360"
  },
  {
    "text": "And these little arrows\nindicate linear combinations",
    "start": "214360",
    "end": "218140"
  },
  {
    "text": "of these X's that feed into\neach of these hidden units.",
    "start": "218140",
    "end": "222850"
  },
  {
    "text": "And here, hk of X is\nexpanded as a function, which",
    "start": "222850",
    "end": "228160"
  },
  {
    "text": "is going to be a\nnonlinear function,",
    "start": "228160",
    "end": "229750"
  },
  {
    "text": "of a linear combination\nof the inputs.",
    "start": "229750",
    "end": "232400"
  },
  {
    "text": "And each of those linear\ncombinations is different.",
    "start": "232400",
    "end": "235189"
  },
  {
    "text": "So the linear combination\nfeeding into A1",
    "start": "235190",
    "end": "237760"
  },
  {
    "text": "is different to the\none feeding into A2.",
    "start": "237760",
    "end": "240250"
  },
  {
    "text": "So what you can think of is\nthese are transformations that",
    "start": "240250",
    "end": "244390"
  },
  {
    "text": "are learned--",
    "start": "244390",
    "end": "245530"
  },
  {
    "text": "when you train the network,\nthese transformations",
    "start": "245530",
    "end": "248200"
  },
  {
    "text": "are learned on the fly.",
    "start": "248200",
    "end": "250870"
  },
  {
    "text": "So let's look in a\nlittle bit more detail.",
    "start": "250870",
    "end": "255040"
  },
  {
    "text": "The Ak, these are\nthe activations.",
    "start": "255040",
    "end": "257018"
  },
  {
    "text": "As I said, the hk of X.\nAnd we can think of it",
    "start": "257019",
    "end": "259898"
  },
  {
    "text": "as a nonlinear transformation\nof a linear function.",
    "start": "259899",
    "end": "263889"
  },
  {
    "text": "And they called, again,\nactivations in the hidden layer.",
    "start": "263890",
    "end": "267430"
  },
  {
    "text": "Now, these nonlinear\nfunctions, there's a choice.",
    "start": "267430",
    "end": "271350"
  },
  {
    "text": "And they're called\nactivation functions.",
    "start": "271350",
    "end": "273810"
  },
  {
    "text": "And two that are\npopular are the sigmoid",
    "start": "273810",
    "end": "276450"
  },
  {
    "text": "and the rectified linear.",
    "start": "276450",
    "end": "278250"
  },
  {
    "text": "So in the early neural networks,\nthe sigmoids were popular.",
    "start": "278250",
    "end": "281950"
  },
  {
    "text": "So you can see it's\na nonlinear function.",
    "start": "281950",
    "end": "284370"
  },
  {
    "text": "It takes a standardized range.",
    "start": "284370",
    "end": "287190"
  },
  {
    "text": "z is running from minus 4 to 4.",
    "start": "287190",
    "end": "289440"
  },
  {
    "text": "And it maps it to the interval\n0, 1 in a smooth transformation.",
    "start": "289440",
    "end": "294940"
  },
  {
    "text": "It's actually the same\ntransformation that's",
    "start": "294940",
    "end": "297030"
  },
  {
    "text": "used in logistic regression.",
    "start": "297030",
    "end": "298889"
  },
  {
    "text": "The black one is the one\nthat's more popular today.",
    "start": "298890",
    "end": "302500"
  },
  {
    "text": "It's called a ReLU\nfunction, which stands",
    "start": "302500",
    "end": "305070"
  },
  {
    "text": "for rectified linear unit.",
    "start": "305070",
    "end": "307590"
  },
  {
    "text": "And look what it does.",
    "start": "307590",
    "end": "309550"
  },
  {
    "text": "It's 0 up to--",
    "start": "309550",
    "end": "312389"
  },
  {
    "text": "if z is less than or\nequal to 0, it returns 0.",
    "start": "312390",
    "end": "316540"
  },
  {
    "text": "And after 0, it returns\na linear function.",
    "start": "316540",
    "end": "321650"
  },
  {
    "text": "So it basically truncates at 0.",
    "start": "321650",
    "end": "325280"
  },
  {
    "text": "And since there's\nan intercept in each",
    "start": "325280",
    "end": "328340"
  },
  {
    "text": "of these transformations,\nwhat 0 on the z scale",
    "start": "328340",
    "end": "333590"
  },
  {
    "text": "can move around because\nof that intercept.",
    "start": "333590",
    "end": "336260"
  },
  {
    "text": "So why nonlinear?",
    "start": "336260",
    "end": "338240"
  },
  {
    "text": "Well, if they weren't nonlinear,\nfor example, if the activation",
    "start": "338240",
    "end": "341810"
  },
  {
    "text": "function was linear,\nthen the whole network",
    "start": "341810",
    "end": "344990"
  },
  {
    "text": "would just be a linear\nmodel because then you",
    "start": "344990",
    "end": "347360"
  },
  {
    "text": "would have linear\ntransformations",
    "start": "347360",
    "end": "350030"
  },
  {
    "text": "of linear variables become--",
    "start": "350030",
    "end": "353210"
  },
  {
    "text": "you just get one\nbig linear model.",
    "start": "353210",
    "end": "355020"
  },
  {
    "text": "So if you didn't do\nnonlinear activations,",
    "start": "355020",
    "end": "358520"
  },
  {
    "text": "it would just be a\nbig linear model.",
    "start": "358520",
    "end": "361009"
  },
  {
    "text": "And so again, the activations\nare derived features",
    "start": "361010",
    "end": "365720"
  },
  {
    "text": "that we learn when\nwe train the model.",
    "start": "365720",
    "end": "369220"
  },
  {
    "text": "And then with all that in\nplace, you fit the model",
    "start": "369220",
    "end": "372280"
  },
  {
    "text": "by minimizing this--",
    "start": "372280",
    "end": "374230"
  },
  {
    "text": "say for regression and\nsquared error loss, you just",
    "start": "374230",
    "end": "377440"
  },
  {
    "text": "minimize the sum of squares\nlike you would for regression.",
    "start": "377440",
    "end": "380680"
  },
  {
    "text": "But this f-- so f was at the\nend of the pipeline here.",
    "start": "380680",
    "end": "386800"
  },
  {
    "text": "f encompasses a\nlot of parameters.",
    "start": "386800",
    "end": "389569"
  },
  {
    "text": "There's the parameters\ngoing from the hidden layer",
    "start": "389570",
    "end": "391720"
  },
  {
    "text": "to the output layer.",
    "start": "391720",
    "end": "393350"
  },
  {
    "text": "And there's the parameters\ngoing from the inputs",
    "start": "393350",
    "end": "395680"
  },
  {
    "text": "to all the hidden units.",
    "start": "395680",
    "end": "397580"
  },
  {
    "text": "So there's a lot of parameters.",
    "start": "397580",
    "end": "399430"
  },
  {
    "text": "And you'll see\nthat's something that",
    "start": "399430",
    "end": "401770"
  },
  {
    "text": "characterizes neural networks.",
    "start": "401770",
    "end": "403789"
  },
  {
    "text": "There can be lots of parameters.",
    "start": "403790",
    "end": "406210"
  },
  {
    "start": "404000",
    "end": "404000"
  },
  {
    "text": "So let's get on to a\nmore serious example.",
    "start": "406210",
    "end": "411400"
  },
  {
    "text": "So this is the MNIST digit data.",
    "start": "411400",
    "end": "415630"
  },
  {
    "text": "This digit data, this is like a\ntest bench for neural networks.",
    "start": "415630",
    "end": "419060"
  },
  {
    "text": "In fact, I would say neural\nnetworks really started",
    "start": "419060",
    "end": "421990"
  },
  {
    "text": "with handwritten\ndigit classification.",
    "start": "421990",
    "end": "425319"
  },
  {
    "text": "So you can see that\nthese handwritten digits",
    "start": "425320",
    "end": "428740"
  },
  {
    "text": "have been scanned in.",
    "start": "428740",
    "end": "430819"
  },
  {
    "text": "They've been written with a\npen or pencil, typically taken",
    "start": "430820",
    "end": "434720"
  },
  {
    "text": "from envelopes.",
    "start": "434720",
    "end": "436130"
  },
  {
    "text": "And they've been scanned in.",
    "start": "436130",
    "end": "437570"
  },
  {
    "text": "And they've been\nnormalized, in this case,",
    "start": "437570",
    "end": "439610"
  },
  {
    "text": "to 28 by 28 grayscale images.",
    "start": "439610",
    "end": "443240"
  },
  {
    "text": "What that means is-- here,\nwe've blown up some three",
    "start": "443240",
    "end": "446030"
  },
  {
    "text": "of the images, the 3s, 5s,\nand 8s for this example here.",
    "start": "446030",
    "end": "450139"
  },
  {
    "text": "And so there's 28\nrows and 28 columns.",
    "start": "450140",
    "end": "454540"
  },
  {
    "text": "And each pixel is\na grayscale value,",
    "start": "454540",
    "end": "457490"
  },
  {
    "text": "taking on values\nbetween 0 and 255.",
    "start": "457490",
    "end": "460970"
  },
  {
    "text": "And what that means is that\ngrayscale value basically",
    "start": "460970",
    "end": "465800"
  },
  {
    "text": "calculates how much of the\nink is in that little pixel.",
    "start": "465800",
    "end": "470039"
  },
  {
    "text": "If it's 0, there's nothing.",
    "start": "470040",
    "end": "472070"
  },
  {
    "text": "And then depending\non the amount of ink,",
    "start": "472070",
    "end": "474710"
  },
  {
    "text": "it can go all the way up to 255.",
    "start": "474710",
    "end": "477063"
  },
  {
    "text": "I guess one of the reasons you\nnormalize is that the input",
    "start": "477063",
    "end": "479479"
  },
  {
    "text": "features are the pixels here.",
    "start": "479480",
    "end": "480990"
  },
  {
    "text": "So we want somehow a\npixel in each image,",
    "start": "480990",
    "end": "483569"
  },
  {
    "text": "so each three to\nmean the same thing,",
    "start": "483570",
    "end": "485870"
  },
  {
    "text": "to mean the same place\nrelative to the three.",
    "start": "485870",
    "end": "488180"
  },
  {
    "text": "Same place.",
    "start": "488180",
    "end": "489380"
  },
  {
    "text": "Exactly.",
    "start": "489380",
    "end": "489980"
  },
  {
    "text": "So if you had the images all\ndifferent sizes and shapes,",
    "start": "489980",
    "end": "494540"
  },
  {
    "text": "that would be much\nharder to achieve.",
    "start": "494540",
    "end": "496530"
  },
  {
    "text": "But when you see the\nconvolutional neural network",
    "start": "496530",
    "end": "498680"
  },
  {
    "text": "coming up, it'll make that\ntake that even further.",
    "start": "498680",
    "end": "501630"
  },
  {
    "text": "It'll be invariant\nto the position",
    "start": "501630",
    "end": "504140"
  },
  {
    "text": "of the pixel in essence.",
    "start": "504140",
    "end": "506460"
  },
  {
    "text": "So along with this\nexample, you come",
    "start": "506460",
    "end": "509449"
  },
  {
    "text": "with a class label which, of\ncourse, are the digits 0 to 9.",
    "start": "509450",
    "end": "513020"
  },
  {
    "text": "So this is a multi-class\nclassification problem.",
    "start": "513020",
    "end": "516900"
  },
  {
    "text": "You've got 10 classes.",
    "start": "516900",
    "end": "518390"
  },
  {
    "text": "And the input\nfeatures are these 28",
    "start": "518390",
    "end": "522289"
  },
  {
    "text": "by 28, which is 784\npixels per image.",
    "start": "522289",
    "end": "531090"
  },
  {
    "text": "You can think of that just\nas a set of inputs per image.",
    "start": "531090",
    "end": "534540"
  },
  {
    "text": "And we need to use the\nvalues in those inputs",
    "start": "534540",
    "end": "537240"
  },
  {
    "text": "to classify the digits.",
    "start": "537240",
    "end": "540060"
  },
  {
    "text": "So in this case, we're going to\nbuild a two layer feedforward",
    "start": "540060",
    "end": "543420"
  },
  {
    "text": "neural network, which is just\na more complicated version",
    "start": "543420",
    "end": "547260"
  },
  {
    "text": "of the one we showed you before.",
    "start": "547260",
    "end": "549330"
  },
  {
    "text": "And it's going to have 256 units\nat the first layer and 128 units",
    "start": "549330",
    "end": "555510"
  },
  {
    "text": "at the second layer.",
    "start": "555510",
    "end": "556600"
  },
  {
    "text": "And of course, the output\nlayer has got 10 units.",
    "start": "556600",
    "end": "560430"
  },
  {
    "text": "And if you count up\nall the parameters,",
    "start": "560430",
    "end": "562950"
  },
  {
    "text": "there's 235,146\nparameters altogether.",
    "start": "562950",
    "end": "568300"
  },
  {
    "text": "And in this community,\nthese are known as weights.",
    "start": "568300",
    "end": "571222"
  },
  {
    "text": "I remember some\nof the first times",
    "start": "571223",
    "end": "572640"
  },
  {
    "text": "we saw these kinds of models,\nwe thought, how can you",
    "start": "572640",
    "end": "574848"
  },
  {
    "text": "have four times as many\nparameters as observations?",
    "start": "574848",
    "end": "577440"
  },
  {
    "text": "Exactly.",
    "start": "577440",
    "end": "577940"
  },
  {
    "text": "An overfit, right?",
    "start": "577940",
    "end": "578950"
  },
  {
    "text": "So we're going to talk\nquite a bit about how",
    "start": "578950",
    "end": "581100"
  },
  {
    "text": "we avoid overfitting in\nthese kinds of models.",
    "start": "581100",
    "end": "584550"
  },
  {
    "text": "I mean, look at that 60,000\ntraining observations",
    "start": "584550",
    "end": "587350"
  },
  {
    "text": "and 235,000 weights.",
    "start": "587350",
    "end": "590259"
  },
  {
    "text": "Sounds crazy.",
    "start": "590260",
    "end": "593140"
  },
  {
    "text": "So here's a network diagram.",
    "start": "593140",
    "end": "595840"
  },
  {
    "text": "So there's the inputs.",
    "start": "595840",
    "end": "597710"
  },
  {
    "text": "So of course, we've\ngot dot, dot, dot.",
    "start": "597710",
    "end": "599870"
  },
  {
    "text": "These are going up to all\nthe pixels in the image.",
    "start": "599870",
    "end": "603400"
  },
  {
    "text": "So that's the input layer.",
    "start": "603400",
    "end": "605060"
  },
  {
    "text": "There's the first hidden\nlayer with 256 hidden units.",
    "start": "605060",
    "end": "609970"
  },
  {
    "text": "And all these little arrows\nrepresent all the weights",
    "start": "609970",
    "end": "612610"
  },
  {
    "text": "going from every input\nto every hidden unit.",
    "start": "612610",
    "end": "619050"
  },
  {
    "text": "And then likewise, we have\na second hidden layer,",
    "start": "619050",
    "end": "621790"
  },
  {
    "text": "which takes, as\ninputs, the values",
    "start": "621790",
    "end": "623519"
  },
  {
    "text": "from the first hidden layer.",
    "start": "623520",
    "end": "624990"
  },
  {
    "text": "And, again, fully connected\nwith weights going from every--",
    "start": "624990",
    "end": "629890"
  },
  {
    "text": "they are one unit to\nevery layer, two units.",
    "start": "629890",
    "end": "633310"
  },
  {
    "text": "And then finally, we\nhave an output layer,",
    "start": "633310",
    "end": "635600"
  },
  {
    "text": "which has got 10 units\nrepresented in the digits 0, 1,",
    "start": "635600",
    "end": "638920"
  },
  {
    "text": "through up to 9.",
    "start": "638920",
    "end": "642790"
  },
  {
    "text": "And these actually\nestimate a probability",
    "start": "642790",
    "end": "645459"
  },
  {
    "text": "of being each of the digits.",
    "start": "645460",
    "end": "648200"
  },
  {
    "text": "And then those lead\nto a prediction.",
    "start": "648200",
    "end": "650390"
  },
  {
    "text": "And this are a\nhistorical comment.",
    "start": "650390",
    "end": "651840"
  },
  {
    "text": "This is an example\nof a deep net.",
    "start": "651840",
    "end": "653570"
  },
  {
    "text": "I mean, it's got\ntwo hidden layers.",
    "start": "653570",
    "end": "655760"
  },
  {
    "text": "In the '80s, the original neural\nnetworks typically had one",
    "start": "655760",
    "end": "658510"
  },
  {
    "text": "hidden layer.",
    "start": "658510",
    "end": "659420"
  },
  {
    "text": "And Tommy Poggio\nfrom MIT gave a talk",
    "start": "659420",
    "end": "662260"
  },
  {
    "text": "at Stanford a few years ago.",
    "start": "662260",
    "end": "663890"
  },
  {
    "text": "And he said, in the '80s, when\nthe neural nets came out with",
    "start": "663890",
    "end": "666400"
  },
  {
    "text": "single hidden layers, some\napproximation theorems were",
    "start": "666400",
    "end": "669330"
  },
  {
    "text": "proven by mathematicians showing\nthat if you make the hidden",
    "start": "669330",
    "end": "671830"
  },
  {
    "text": "layer, L1 here, have as many\narbitrary number of hidden",
    "start": "671830",
    "end": "676210"
  },
  {
    "text": "units, you can approximate\nany smooth function well.",
    "start": "676210",
    "end": "679390"
  },
  {
    "text": "So the conclusion\nfrom the community",
    "start": "679390",
    "end": "680890"
  },
  {
    "text": "was, we only ever\nneed one hidden layer.",
    "start": "680890",
    "end": "683170"
  },
  {
    "text": "And in Tommy's opinion, that\nset the field back by 30 years",
    "start": "683170",
    "end": "686829"
  },
  {
    "text": "because people just--",
    "start": "686830",
    "end": "688150"
  },
  {
    "text": "they didn't bother with trying--",
    "start": "688150",
    "end": "689990"
  },
  {
    "text": "with deep nets more\nthan the one layer.",
    "start": "689990",
    "end": "691970"
  },
  {
    "text": "They just used one layer.",
    "start": "691970",
    "end": "693560"
  },
  {
    "text": "And this is not to\ncriticize mathematicians",
    "start": "693560",
    "end": "695570"
  },
  {
    "text": "who were very clever\nin their proofs,",
    "start": "695570",
    "end": "697160"
  },
  {
    "text": "but it just makes the point that\nyou shouldn't take mathematics",
    "start": "697160",
    "end": "699800"
  },
  {
    "text": "too seriously.",
    "start": "699800",
    "end": "700620"
  },
  {
    "text": "And some times it\nmakes assumptions.",
    "start": "700620",
    "end": "703260"
  },
  {
    "text": "And as we'll see later in\nthis lecture, what's the--",
    "start": "703260",
    "end": "706790"
  },
  {
    "text": "We're going to show you a\nnetwork that's got 50 layers.",
    "start": "706790",
    "end": "710035"
  },
  {
    "text": "And there's a good\nreason for it.",
    "start": "710035",
    "end": "711410"
  },
  {
    "text": "And it works really well.",
    "start": "711410",
    "end": "712430"
  },
  {
    "text": "So we'll talk a little bit\nmore about the function",
    "start": "712430",
    "end": "713899"
  },
  {
    "text": "of these layers.",
    "start": "713900",
    "end": "714567"
  },
  {
    "text": "But the point is, with deep\nnetworks with multiple layers,",
    "start": "714567",
    "end": "717680"
  },
  {
    "text": "you can really get much\nbetter performance.",
    "start": "717680",
    "end": "720170"
  },
  {
    "start": "718000",
    "end": "718000"
  },
  {
    "text": "OK, let's talk a little\nbit about the output layer.",
    "start": "720170",
    "end": "722959"
  },
  {
    "text": "So what we have is\ncoming from the second--",
    "start": "722960",
    "end": "726780"
  },
  {
    "text": "so we've had to introduce\nsome notation now",
    "start": "726780",
    "end": "728870"
  },
  {
    "text": "because we've got\nmultiple layers.",
    "start": "728870",
    "end": "730529"
  },
  {
    "text": "So we go from the second hidden\nlayer to the output layer.",
    "start": "730530",
    "end": "734150"
  },
  {
    "text": "And let's suppose that\ncomputes some activation Zm.",
    "start": "734150",
    "end": "739310"
  },
  {
    "text": "Then the output activation\nfunction encodes what's",
    "start": "739310",
    "end": "742100"
  },
  {
    "text": "known as the softmax function.",
    "start": "742100",
    "end": "744120"
  },
  {
    "text": "This is the same\nfunction that's used",
    "start": "744120",
    "end": "746480"
  },
  {
    "text": "in multiclass\nlogistic regression.",
    "start": "746480",
    "end": "748889"
  },
  {
    "text": "So just takes these\n10 real numbers",
    "start": "748890",
    "end": "752420"
  },
  {
    "text": "and puts them through\nthis nonlinear",
    "start": "752420",
    "end": "754250"
  },
  {
    "text": "transformation, which turns them\ninto numbers between 0 and 1",
    "start": "754250",
    "end": "758690"
  },
  {
    "text": "that also sum to 1.",
    "start": "758690",
    "end": "760550"
  },
  {
    "text": "So this m here is going\nfrom class 0 up to class 9.",
    "start": "760550",
    "end": "765290"
  },
  {
    "text": "And if you sum\nthese quantities up,",
    "start": "765290",
    "end": "767810"
  },
  {
    "text": "you'll get a probability\nthat sum to 1.",
    "start": "767810",
    "end": "771600"
  },
  {
    "text": "And then also like in\nmulti-class logistic regression,",
    "start": "771600",
    "end": "775769"
  },
  {
    "text": "we fit the model by minimizing\nthe negative multinomial log",
    "start": "775770",
    "end": "779130"
  },
  {
    "text": "likelihood.",
    "start": "779130",
    "end": "780360"
  },
  {
    "text": "It's also known as\ncross-entropy in this field.",
    "start": "780360",
    "end": "782980"
  },
  {
    "text": "And here it is here it's just\nthat you've got your training",
    "start": "782980",
    "end": "787050"
  },
  {
    "text": "data with true labels.",
    "start": "787050",
    "end": "788670"
  },
  {
    "text": "And the true labels\nare represented",
    "start": "788670",
    "end": "791579"
  },
  {
    "text": "by this set of numbers Y. So\nY is a vector of 10 numbers.",
    "start": "791580",
    "end": "797490"
  },
  {
    "text": "And it will be 1 if the true\nclass for observation i is m,",
    "start": "797490",
    "end": "801149"
  },
  {
    "text": "otherwise it will be 0.",
    "start": "801150",
    "end": "803100"
  },
  {
    "text": "And in this field, that's\nknown as one-hot encoded.",
    "start": "803100",
    "end": "807990"
  },
  {
    "text": "What do we call that\nin statistics, Rob?",
    "start": "807990",
    "end": "809670"
  },
  {
    "text": "Dummy variables.",
    "start": "809670",
    "end": "810649"
  },
  {
    "text": "Dummy variables.",
    "start": "810650",
    "end": "811470"
  },
  {
    "text": "I think their marketing\ndepartment's definitely",
    "start": "811470",
    "end": "813428"
  },
  {
    "text": "better than ours.",
    "start": "813428",
    "end": "815880"
  },
  {
    "text": "So if you think about this now--",
    "start": "815880",
    "end": "818130"
  },
  {
    "text": "we've summed from m\ngoing from 0 to 9,",
    "start": "818130",
    "end": "820920"
  },
  {
    "text": "but only one of\nthese Y's is one.",
    "start": "820920",
    "end": "823110"
  },
  {
    "text": "And so that will pick up\nthe log of the probability",
    "start": "823110",
    "end": "826140"
  },
  {
    "text": "of the observed label.",
    "start": "826140",
    "end": "828450"
  },
  {
    "text": "And that's therefore\nthe log likelihood.",
    "start": "828450",
    "end": "830500"
  },
  {
    "text": "And we take the negative\nof the log likelihood.",
    "start": "830500",
    "end": "833500"
  },
  {
    "start": "832000",
    "end": "832000"
  },
  {
    "text": "So we're going to\ntell you about details",
    "start": "833500",
    "end": "835750"
  },
  {
    "text": "of fitting neural\nnetworks later.",
    "start": "835750",
    "end": "837350"
  },
  {
    "text": "But let's look at the\nresults on these data set.",
    "start": "837350",
    "end": "839980"
  },
  {
    "text": "So we show you two\nneural network results.",
    "start": "839980",
    "end": "843399"
  },
  {
    "text": "And the one gets 2.3% errors.",
    "start": "843400",
    "end": "846170"
  },
  {
    "text": "The other gets 1.8% errors.",
    "start": "846170",
    "end": "848980"
  },
  {
    "text": "If you just fit a multinomial\nlogistic regression,",
    "start": "848980",
    "end": "852339"
  },
  {
    "text": "you get 7.2% errors.",
    "start": "852340",
    "end": "855250"
  },
  {
    "text": "And if you fit linear\ndiscriminant analysis, which",
    "start": "855250",
    "end": "857890"
  },
  {
    "text": "is another way of fitting\nmulti-class classification",
    "start": "857890",
    "end": "861250"
  },
  {
    "text": "models, you get 12.7% errors.",
    "start": "861250",
    "end": "864040"
  },
  {
    "text": "So these are\nsubstantial improvements",
    "start": "864040",
    "end": "866560"
  },
  {
    "text": "over these pre-existing\nand more simple methods.",
    "start": "866560",
    "end": "871570"
  },
  {
    "text": "And again, this problem with the\ndigits spelled one of the early",
    "start": "871570",
    "end": "876910"
  },
  {
    "text": "successes for neural\nnetworks in the '90s.",
    "start": "876910",
    "end": "880180"
  },
  {
    "text": "So we mentioned two forms of\nridge regularization, which",
    "start": "880180",
    "end": "884020"
  },
  {
    "text": "you know about and dropout\nregularization, which",
    "start": "884020",
    "end": "887200"
  },
  {
    "text": "we will tell you about\na little bit later.",
    "start": "887200",
    "end": "889370"
  },
  {
    "text": "And these both are essential and\nhelp give this good performance.",
    "start": "889370",
    "end": "894339"
  },
  {
    "text": "And again, we'll give you some\nmore details of that later.",
    "start": "894340",
    "end": "897760"
  },
  {
    "text": "Now, we should\npoint out that this",
    "start": "897760",
    "end": "899530"
  },
  {
    "text": "is a very overworked problem.",
    "start": "899530",
    "end": "901470"
  },
  {
    "text": "These aren't the best\nrates you're going to see.",
    "start": "901470",
    "end": "904129"
  },
  {
    "text": "You can find reported rates\nthat are less than 0.5%",
    "start": "904130",
    "end": "908020"
  },
  {
    "text": "misclassification error.",
    "start": "908020",
    "end": "910090"
  },
  {
    "text": "For this particular data\nset, this test data set,",
    "start": "910090",
    "end": "913000"
  },
  {
    "text": "the human error rate is\nreported at around 0.2% or 1020",
    "start": "913000",
    "end": "918100"
  },
  {
    "text": "of the 10,000 test images.",
    "start": "918100",
    "end": "920420"
  },
  {
    "text": "But I think so many people have\ntried and fit models to this.",
    "start": "920420",
    "end": "925750"
  },
  {
    "text": "It could be a case that we\nover training on the test set.",
    "start": "925750",
    "end": "930720"
  }
]