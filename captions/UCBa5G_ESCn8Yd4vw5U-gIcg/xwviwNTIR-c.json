[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "thanks for having me and thanks for the very nice introduction and it's nice to be back at stanford after two years so",
    "start": "10960",
    "end": "18640"
  },
  {
    "text": "and this is actually my first academic travel ever since the pandemics right so that i feel very excited to talk to a",
    "start": "18640",
    "end": "24960"
  },
  {
    "text": "live card rather than just my computer screen so and today i'm going to talk about object skills and uh the quest for",
    "start": "24960",
    "end": "31920"
  },
  {
    "text": "compositional robot autonomy so as jeannette introduced i started my faculty at ut austin and i found a new",
    "start": "31920",
    "end": "38480"
  },
  {
    "text": "lab called rhubarb perception learning lab there at ut austin and so today what i'm presenting",
    "start": "38480",
    "end": "44079"
  },
  {
    "text": "i really the first batch of work from my own student right so i should start by first saying that uh i'm probably very",
    "start": "44079",
    "end": "49680"
  },
  {
    "text": "proud of my students work and especially during this very difficult time and uh because i did",
    "start": "49680",
    "end": "55520"
  },
  {
    "text": "this amazing work that i got the chance to hear talk about them all right so let's get started",
    "start": "55520",
    "end": "62480"
  },
  {
    "start": "60000",
    "end": "142000"
  },
  {
    "text": "uh one of the most exciting news i think in the robotics world recently comes actually from the outer space right so",
    "start": "62480",
    "end": "70880"
  },
  {
    "text": "maybe some of you already know that james webb space telescope is now successfully launched and the following",
    "start": "70880",
    "end": "77680"
  },
  {
    "text": "operation so to me this is really a remarkable progress of uh robotic system",
    "start": "77680",
    "end": "83439"
  },
  {
    "text": "engineering right so out of the thousands of different tasks to be done there are 300 of them a single point",
    "start": "83439",
    "end": "89600"
  },
  {
    "text": "failure that means that if any of these 300 times fails the entire telescope",
    "start": "89600",
    "end": "95360"
  },
  {
    "text": "will fail as whole and also because this is in outer speed there's no way that we can send a technician to go fix those",
    "start": "95360",
    "end": "101840"
  },
  {
    "text": "bugs but still this is a ginormous effort it takes over nine billion dollars of internal",
    "start": "101840",
    "end": "108720"
  },
  {
    "text": "money and a huge team of engineers and scientists working together for 20 years",
    "start": "108720",
    "end": "115040"
  },
  {
    "text": "but in the end it was a great success and it was successful",
    "start": "115040",
    "end": "120079"
  },
  {
    "text": "so just seeing this news kind of makes me wonder that apparently humankind has already figured",
    "start": "120079",
    "end": "126159"
  },
  {
    "text": "a way to be extremely complicated yet highly reliable systems but why",
    "start": "126159",
    "end": "132720"
  },
  {
    "text": "we see from the news and meet social media that uh autumn's driving cars and personal",
    "start": "132720",
    "end": "138640"
  },
  {
    "text": "robots are always five to ten years away well one of the key reasons is that we",
    "start": "138640",
    "end": "144879"
  },
  {
    "start": "142000",
    "end": "228000"
  },
  {
    "text": "need a generalist robot to sense and act in the real world and it is kind of difficult to program and design this",
    "start": "144879",
    "end": "151840"
  },
  {
    "text": "robotic system in the traditional form of system engineering and that's why our",
    "start": "151840",
    "end": "158160"
  },
  {
    "text": "research on robot learning come into picture right so it offers a new paradigm to design robotic behaviors",
    "start": "158160",
    "end": "164720"
  },
  {
    "text": "with ai and data driven techniques that are otherwise like hard to manually engineer",
    "start": "164720",
    "end": "170640"
  },
  {
    "text": "typically when we do group learning research the workflow is like this right so we have a small team of researchers and",
    "start": "170640",
    "end": "177599"
  },
  {
    "text": "faculties and work together bring some ideas and this is actually showing me doing research with some of my my",
    "start": "177599",
    "end": "184080"
  },
  {
    "text": "stanford colleagues when i was still here um and we come up with idea and then we kind of implement the modeling",
    "start": "184080",
    "end": "190480"
  },
  {
    "text": "our favorite framework let it be pytorch tensorflow jacks and then we try it on our robot platform if we get a decent",
    "start": "190480",
    "end": "197599"
  },
  {
    "text": "success see 70 80 success and beat all the baseline of",
    "start": "197599",
    "end": "203040"
  },
  {
    "text": "our choices we're pretty happy about it and then we write a paper and we get a publishing top conference and then we",
    "start": "203040",
    "end": "208720"
  },
  {
    "text": "move on and then when we move on to the next work it is very often that uh the next",
    "start": "208720",
    "end": "215120"
  },
  {
    "text": "paper will not build upon the prior ones right so there's no effort taken to get",
    "start": "215120",
    "end": "220319"
  },
  {
    "text": "that 70 success to the 99.99 reliability for real-world execution",
    "start": "220319",
    "end": "228080"
  },
  {
    "start": "228000",
    "end": "286000"
  },
  {
    "text": "so we see a dichotomy here on one side we know the drill of how to build really",
    "start": "228080",
    "end": "234799"
  },
  {
    "text": "complex it's reliable special purpose body system like the space telescope on",
    "start": "234799",
    "end": "240319"
  },
  {
    "text": "the other hand we see that uh ai powered robot learning has become a new paradigm to kind of design and",
    "start": "240319",
    "end": "248159"
  },
  {
    "text": "generate purpose robotic behaviors in the world um but the lack of reliability of this",
    "start": "248159",
    "end": "254799"
  },
  {
    "text": "learning methods is really preventing us from widespread deployments so naturally",
    "start": "254799",
    "end": "260079"
  },
  {
    "text": "we are asking can we combine the best of both worlds so on one hand we have this rigorous",
    "start": "260079",
    "end": "266960"
  },
  {
    "text": "engineering principles to design complex and the reliable system on the other hand we can have the robot learning and",
    "start": "266960",
    "end": "274639"
  },
  {
    "text": "techniques and they developed like recently to encourage like a general and robust behaviors and that's exactly what",
    "start": "274639",
    "end": "281600"
  },
  {
    "text": "argue as the key to achieve deploy vulnerable autonomy in the wild",
    "start": "281600",
    "end": "287360"
  },
  {
    "start": "286000",
    "end": "337000"
  },
  {
    "text": "so to think about what exactly other kind of engineering principles that we can learn as a robot learning",
    "start": "287360",
    "end": "292960"
  },
  {
    "text": "researchers i think there are two principles that inspired my research our research a lot that are",
    "start": "292960",
    "end": "299440"
  },
  {
    "text": "abstraction and the composition what is abstraction abstraction con",
    "start": "299440",
    "end": "304800"
  },
  {
    "text": "concerns about the division of labor if we give you a very complex system of",
    "start": "304800",
    "end": "310240"
  },
  {
    "text": "very hard problems how can we break that down into some simpler pieces so that we",
    "start": "310240",
    "end": "315520"
  },
  {
    "text": "can work on this individual sub problems in isolation and what about composition",
    "start": "315520",
    "end": "322000"
  },
  {
    "text": "is a one way piece together this individual modules right and how can we",
    "start": "322000",
    "end": "327039"
  },
  {
    "text": "make sure that this individual modules work in harmony in coordination with each other towards this",
    "start": "327039",
    "end": "334160"
  },
  {
    "text": "shared and overarching goal so our recent research has been looking",
    "start": "334160",
    "end": "339840"
  },
  {
    "start": "337000",
    "end": "421000"
  },
  {
    "text": "at how can we kind of embrace this engineering principles for building complex robotics systems in building",
    "start": "339840",
    "end": "346560"
  },
  {
    "text": "general purposes of autonomy in the world so if you take a system perspective to look at robotics",
    "start": "346560",
    "end": "353360"
  },
  {
    "text": "generalist robots that is a complex system that sends an act right so there's a sense into action",
    "start": "353360",
    "end": "361039"
  },
  {
    "text": "loop that the robot receives the perceptual information from the environment using its on-board sensors",
    "start": "361039",
    "end": "367440"
  },
  {
    "text": "and processing information to determine what the action to take and then the action in turns generate new sensory",
    "start": "367440",
    "end": "373840"
  },
  {
    "text": "stimuli for the next times this stream making and a straightforward way to model this",
    "start": "373840",
    "end": "380160"
  },
  {
    "text": "sensing action loop is direct mapping right so from sensory data to uh motor",
    "start": "380160",
    "end": "385759"
  },
  {
    "text": "actions if you take a computational perspective this is a very non-linear complex function that we can learn and",
    "start": "385759",
    "end": "392319"
  },
  {
    "text": "this is indeed what like this end-to-end deep learning approach has been",
    "start": "392319",
    "end": "397600"
  },
  {
    "text": "exploring sometimes we call it a pixel 2 torque that would learn a direct mapping from the raw sensory data to motor",
    "start": "397600",
    "end": "404080"
  },
  {
    "text": "combined and this has been achieving a lot of great progress and",
    "start": "404080",
    "end": "409360"
  },
  {
    "text": "impressive demos in recent years but more and more we start to see the shortcoming of this approach in the kind",
    "start": "409360",
    "end": "415759"
  },
  {
    "text": "of the high data requirement the high sample complexity as well as the british generalization",
    "start": "415759",
    "end": "422160"
  },
  {
    "start": "421000",
    "end": "492000"
  },
  {
    "text": "so instead of seeking this pixel to torque approach or",
    "start": "422160",
    "end": "427599"
  },
  {
    "text": "like a research investigator how can we embrace the principle of abstraction and composition in particular how can we",
    "start": "427599",
    "end": "435280"
  },
  {
    "text": "learn state abstractions from the raw sensory data how can we learn the action",
    "start": "435280",
    "end": "440479"
  },
  {
    "text": "abstraction from the low level motor control and once we establish these two abstractions we're able to build more",
    "start": "440479",
    "end": "447759"
  },
  {
    "text": "robust like a making modules that achieve a higher level of robustness and",
    "start": "447759",
    "end": "453039"
  },
  {
    "text": "generalization and i call this the compositional robot autonomy stack",
    "start": "453039",
    "end": "458319"
  },
  {
    "text": "so today's topic or talk about the kind of concrete research problem we've been looking at like in the overall theme of",
    "start": "458319",
    "end": "465680"
  },
  {
    "text": "a compositional robot autonomy stack in particular we look at how can we",
    "start": "465680",
    "end": "471199"
  },
  {
    "text": "build understand unstructured things as a composition of the objects present in",
    "start": "471199",
    "end": "476960"
  },
  {
    "text": "the scene and how can we understand the robotic behaviors as a set of",
    "start": "476960",
    "end": "484160"
  },
  {
    "text": "like a skills or reputable of skills such as grasping pushing reaching as a building blocks to synthesize long-term",
    "start": "484160",
    "end": "491039"
  },
  {
    "text": "robot interaction and as an early showcase of this compositional paradigm is our work on",
    "start": "491039",
    "end": "498960"
  },
  {
    "start": "492000",
    "end": "576000"
  },
  {
    "text": "neural task programming that i did a while ago with some of my stanford",
    "start": "498960",
    "end": "504160"
  },
  {
    "text": "collaborators here where the high-level idea is to interpret a long-term robot",
    "start": "504160",
    "end": "510479"
  },
  {
    "text": "execution like say a long-term demonstration sequence of a robotic task",
    "start": "510479",
    "end": "515599"
  },
  {
    "text": "into this hierarchical program such that a complex procedures like stacking a",
    "start": "515599",
    "end": "520800"
  },
  {
    "text": "bunch of color cubes can be recursively broken down into simpler pieces like a",
    "start": "520800",
    "end": "526640"
  },
  {
    "text": "stacking a bunch of color cubes can be breaking down to picking a bunch of pick and place actions and",
    "start": "526640",
    "end": "533760"
  },
  {
    "text": "then each peak and place can be further decomposed into picking and placing and then eventually the pick and place got",
    "start": "533760",
    "end": "540080"
  },
  {
    "text": "mapped to some lower level eps that can be directly executed on the robot and",
    "start": "540080",
    "end": "545600"
  },
  {
    "text": "with this the objects in these domains are the color cubes on the table",
    "start": "545600",
    "end": "551839"
  },
  {
    "text": "the skills are picking and placing and we show that with this object and skill",
    "start": "551839",
    "end": "557120"
  },
  {
    "text": "abstraction we can achieve a higher level of generalization that uh",
    "start": "557120",
    "end": "562880"
  },
  {
    "text": "once our model is trained we can apply this to generalize to unseen block",
    "start": "562880",
    "end": "568000"
  },
  {
    "text": "configurations that never appeared in the training set so that's much better than the end-to-end approach that we",
    "start": "568000",
    "end": "574480"
  },
  {
    "text": "demonstrated earlier but there's a catch here that for ntp the objects and skills are",
    "start": "574480",
    "end": "583360"
  },
  {
    "text": "clearly defined right so and very often the cases in empirical study",
    "start": "583360",
    "end": "589040"
  },
  {
    "text": "of the compositional approach we find that this type of composition modeling typically works really well if we have a",
    "start": "589040",
    "end": "596640"
  },
  {
    "text": "clear notion of a finite set of objects and and skills and to model so in the block",
    "start": "596640",
    "end": "603200"
  },
  {
    "text": "word and the if we're doing a bunch of picking and placing on color pubes it makes it so",
    "start": "603200",
    "end": "608880"
  },
  {
    "text": "easy once the compositional structure will emerge but really the question is how can we unblock the block work right",
    "start": "608880",
    "end": "615200"
  },
  {
    "text": "so pun intended here so when it comes to real world what are the kind of complete",
    "start": "615200",
    "end": "621040"
  },
  {
    "text": "set of objects and what are the completeness like set of skills to model in order to really operationalize this",
    "start": "621040",
    "end": "628480"
  },
  {
    "text": "compositional approach that is a kind of fundamental questions in many research",
    "start": "628480",
    "end": "634800"
  },
  {
    "text": "domains that we kind of sweep that under the rug and that we assume that the object and skills are given",
    "start": "634800",
    "end": "641120"
  },
  {
    "text": "um that makes it easy to build this compositional approach but really i think uh these are the kind of really",
    "start": "641120",
    "end": "647680"
  },
  {
    "text": "fundamental questions to really take advantage of this compositional modeling",
    "start": "647680",
    "end": "652720"
  },
  {
    "text": "so like what is the object what is the skill right so this seems uh a trivial",
    "start": "652720",
    "end": "658000"
  },
  {
    "text": "question from the first look but the more you think the more you research you you find that",
    "start": "658000",
    "end": "663360"
  },
  {
    "text": "the more you know the less you know so really the questions and the focus of today's talk is to really understand",
    "start": "663360",
    "end": "670800"
  },
  {
    "text": "what is the object and the skills to in unlock this type of compositional",
    "start": "670800",
    "end": "676480"
  },
  {
    "text": "modeling in the autonomy stack in particular i'm going to talk about got",
    "start": "676480",
    "end": "681839"
  },
  {
    "text": "two lines of work on learning object abstraction and skill abstraction so",
    "start": "681839",
    "end": "687519"
  },
  {
    "text": "instead of seeking a universal definition what's the true definition of a skill and object we're going to study",
    "start": "687519",
    "end": "693519"
  },
  {
    "text": "this in a pragmatic approach i'm going to study the operation operationally in",
    "start": "693519",
    "end": "699519"
  },
  {
    "text": "concrete problems in robot learning such that hopefully by studying this is what gives us good intuition what other",
    "start": "699519",
    "end": "706160"
  },
  {
    "text": "object and skill representation are useful and necessary for building general purpose robot",
    "start": "706160",
    "end": "712320"
  },
  {
    "text": "so for the first one i'm going to look at like a learning object abstraction from embodied interaction",
    "start": "712320",
    "end": "719440"
  },
  {
    "text": "and then we're going to move skill move to skill abstraction at the second part",
    "start": "719440",
    "end": "725600"
  },
  {
    "start": "724000",
    "end": "755000"
  },
  {
    "text": "okay so we study object representation first in the task of a robotic grasping",
    "start": "725680",
    "end": "733040"
  },
  {
    "text": "um why robotic grasping it's very important like a first like skill to learn to unlock a",
    "start": "733040",
    "end": "739600"
  },
  {
    "text": "lot of real-world applications like assembly logistics smart warehousing etc",
    "start": "739600",
    "end": "745440"
  },
  {
    "text": "where the input we're assuming is like partial point cloud of the entire thing and you want to determine the grass pose",
    "start": "745440",
    "end": "751760"
  },
  {
    "text": "and grasp orientations to execute the grasp our intuition uh data if you look at",
    "start": "751760",
    "end": "758959"
  },
  {
    "start": "755000",
    "end": "895000"
  },
  {
    "text": "objects how can we kind of characterize a real world object there's different perspectives like drawing inspiration",
    "start": "758959",
    "end": "765360"
  },
  {
    "text": "from human cognition we can describe an object like a mug what make a mark on",
    "start": "765360",
    "end": "770880"
  },
  {
    "text": "mug based on its visual attributes right so what makes a mug a mug is because it",
    "start": "770880",
    "end": "775920"
  },
  {
    "text": "looks like a mug you have an empty inside it has a curvy handle so when we recognize this visual appearance and we",
    "start": "775920",
    "end": "782639"
  },
  {
    "text": "we see this is a mug and an alternative way to look at how we characterize attribute objects is based",
    "start": "782639",
    "end": "790240"
  },
  {
    "text": "on their functional properties although no matter how the marks looks like and",
    "start": "790240",
    "end": "796639"
  },
  {
    "text": "the mark is the mark because we can use it to grasp it from the handle and also it can hold",
    "start": "796639",
    "end": "802399"
  },
  {
    "text": "water that we can drink from the mug so based on this different um",
    "start": "802399",
    "end": "808320"
  },
  {
    "text": "characterization of objects it requires different type of understanding of the object geometry versus object affordance",
    "start": "808320",
    "end": "815680"
  },
  {
    "text": "affordance means the functional properties of the objects and so our insight is that the object",
    "start": "815680",
    "end": "823199"
  },
  {
    "text": "affordance understanding and geometry understanding are not too isolated problems so when it comes to like",
    "start": "823199",
    "end": "830639"
  },
  {
    "text": "encoding object information and building object representation they're really the two sides of the same coin so just to",
    "start": "830639",
    "end": "837360"
  },
  {
    "text": "use this example to illustrate like the intuition if we think about how to grasp",
    "start": "837360",
    "end": "842399"
  },
  {
    "text": "this mug from this partially observed view you see that there's a handle that's like a partially",
    "start": "842399",
    "end": "848160"
  },
  {
    "text": "occluded if we want to grasp this mug from this view if a robot can",
    "start": "848160",
    "end": "853839"
  },
  {
    "text": "reconstruct this mug from this partial view it will help it determine the precise grasp location grasp orientation",
    "start": "853839",
    "end": "861760"
  },
  {
    "text": "to to to be able to grasp the mark successfully and once the was able to",
    "start": "861760",
    "end": "867839"
  },
  {
    "text": "pick up the mug and that actually confirms the 3d reconstruction right so",
    "start": "867839",
    "end": "873199"
  },
  {
    "text": "it tells us that there's indeed certain 3d geometry that exists in this occluded",
    "start": "873199",
    "end": "879760"
  },
  {
    "text": "area so it goes the other way so that means that there's some kind of",
    "start": "879760",
    "end": "884959"
  },
  {
    "text": "synergistic relationship between the understanding the geometry like 3d reconstruction and understanding the",
    "start": "884959",
    "end": "892079"
  },
  {
    "text": "functional properties being able to grasp the object so we built giga to capture this",
    "start": "892079",
    "end": "898240"
  },
  {
    "start": "895000",
    "end": "923000"
  },
  {
    "text": "intuition which stands for graphing by implicit geometry and affordance where",
    "start": "898240",
    "end": "904880"
  },
  {
    "text": "the idea is to learn a shared object representation through a 3d backbone and",
    "start": "904880",
    "end": "911279"
  },
  {
    "text": "the train for both types of object avoidance and the 3d reconstruction so",
    "start": "911279",
    "end": "916560"
  },
  {
    "text": "that the learn object representation encodes the object information from both",
    "start": "916560",
    "end": "921760"
  },
  {
    "text": "dimensions so the question first question uh we need to",
    "start": "921760",
    "end": "927040"
  },
  {
    "start": "923000",
    "end": "1066000"
  },
  {
    "text": "answer is what is a good representation for the object things that encodes afford and standard 3d geometry so",
    "start": "927040",
    "end": "934399"
  },
  {
    "text": "motivated by advances recently from 3d vision and graphics community we use the",
    "start": "934399",
    "end": "940399"
  },
  {
    "text": "newer fields for 3d sync what is a neural field right so it is a",
    "start": "940399",
    "end": "945680"
  },
  {
    "text": "coordinate based neural network that maps some input feature like a visual",
    "start": "945680",
    "end": "950720"
  },
  {
    "text": "feature of the object and also a chord coordinates in the cartesian space xyz",
    "start": "950720",
    "end": "956480"
  },
  {
    "text": "to some quantities at that location where the quantity can be the sign distance function or the occupancy at",
    "start": "956480",
    "end": "963199"
  },
  {
    "text": "that location why do we like about neural fields is because that uh by",
    "start": "963199",
    "end": "969360"
  },
  {
    "text": "encoding the 3d shape as part of neural network parameters it allows us to",
    "start": "969360",
    "end": "975199"
  },
  {
    "text": "develop this smooth and continuous representation of the of the things that",
    "start": "975199",
    "end": "980320"
  },
  {
    "text": "we can query the coordinates at arbitrary resolution and",
    "start": "980320",
    "end": "985600"
  },
  {
    "text": "also because this is a distributable layer so we can plug that in as part of a bigger network and train with a rich",
    "start": "985600",
    "end": "992160"
  },
  {
    "text": "source of supervision so that encourage learnings to like give rise to the",
    "start": "992160",
    "end": "998639"
  },
  {
    "text": "meaningful representations from different sources of training data so that's the key idea we have the",
    "start": "998639",
    "end": "1006800"
  },
  {
    "text": "neural fields as a backbone and to really operationalize this idea we",
    "start": "1006800",
    "end": "1012240"
  },
  {
    "text": "start with tsdf that's something we can get from the depth camera on the robot",
    "start": "1012240",
    "end": "1017759"
  },
  {
    "text": "and then we perform convolution to map this input tsdf into a structured",
    "start": "1017759",
    "end": "1023360"
  },
  {
    "text": "volumetric feature grid from this feature grid we can at each location we can get a local feature by",
    "start": "1023360",
    "end": "1030079"
  },
  {
    "text": "pulling from the grid and doing some interpolations to get that local feature at a particular point and based on the",
    "start": "1030079",
    "end": "1036959"
  },
  {
    "text": "local feature and graph center we can predict the the graph quality graph grip orientation",
    "start": "1036959",
    "end": "1043760"
  },
  {
    "text": "on in that like a grade print center and also we can query the 3d geometry like",
    "start": "1043760",
    "end": "1049760"
  },
  {
    "text": "the occupancy probability at the location and then by training both grasp",
    "start": "1049760",
    "end": "1054880"
  },
  {
    "text": "affordance and the 3d reconstruction with this shared structure feature grades it encourages information sharing",
    "start": "1054880",
    "end": "1061679"
  },
  {
    "text": "so to be able to encode the objects in both different properties",
    "start": "1061679",
    "end": "1067280"
  },
  {
    "start": "1066000",
    "end": "1097000"
  },
  {
    "text": "and to actually train this model we use a cell supervised procedure so the way",
    "start": "1067280",
    "end": "1072480"
  },
  {
    "text": "we do that is we dump a large rich set of like a 3d off objects into simulating",
    "start": "1072480",
    "end": "1079039"
  },
  {
    "text": "environments and we sample like a variety of our grants candidates as you can imagine that if we're doing random",
    "start": "1079039",
    "end": "1084799"
  },
  {
    "text": "sampling some of them will lead to success other for leads to failure right so that will give us a data set of",
    "start": "1084799",
    "end": "1090960"
  },
  {
    "text": "positive and negative data to train the affordance prediction without any human supervision",
    "start": "1090960",
    "end": "1097200"
  },
  {
    "start": "1097000",
    "end": "1683000"
  },
  {
    "text": "okay so to show how this works and uh kind of qualitatively on the robot and",
    "start": "1097200",
    "end": "1103200"
  },
  {
    "text": "this is achieving a standard performance on the task of six stop grasping in cultural environment and typically in my",
    "start": "1103200",
    "end": "1110160"
  },
  {
    "text": "lab when i asked my student to do real reward experiments i asked them to show me the",
    "start": "1110160",
    "end": "1116000"
  },
  {
    "text": "uncut video right so this is just showing you that a 10-minute continued execution of robot here is ethan he's",
    "start": "1116000",
    "end": "1123360"
  },
  {
    "text": "putting a bunch of objects into the scene and the running continuous for 10 minutes you can see that some of the",
    "start": "1123360",
    "end": "1129600"
  },
  {
    "text": "graphs like a fails occasionally but overall it it has like pretty consistent",
    "start": "1129600",
    "end": "1135440"
  },
  {
    "text": "performance in being able to picking up object from culture and then like drop them into the",
    "start": "1135440",
    "end": "1140640"
  },
  {
    "text": "beam okay so encouraged by this result so we try to",
    "start": "1140640",
    "end": "1146559"
  },
  {
    "text": "understand why exactly giga works and what exactly the object representation learns all right so because we are using",
    "start": "1146559",
    "end": "1153039"
  },
  {
    "text": "this implicit neural representation on your fields we can actually query the",
    "start": "1153039",
    "end": "1158080"
  },
  {
    "text": "affordance as arbitrary locations in the scene so by corey the location densely",
    "start": "1158080",
    "end": "1164320"
  },
  {
    "text": "it allows us to map the 3d thing into this kind of beautiful uh heat map and called",
    "start": "1164320",
    "end": "1171919"
  },
  {
    "text": "affordance landscape basically showing what is the grasp quality or like grass success likelihood at each individual",
    "start": "1171919",
    "end": "1178799"
  },
  {
    "text": "locations where the red one means higher affordance like more likely to have a",
    "start": "1178799",
    "end": "1183919"
  },
  {
    "text": "grass success the like the white one or gray ones are low affordance scores right we see that uh",
    "start": "1183919",
    "end": "1190640"
  },
  {
    "text": "compared to vision the former state of art and our method was able to generate more diverse graphs and especially in",
    "start": "1190640",
    "end": "1197440"
  },
  {
    "text": "those alkali occluded locations and to show you qualitatively how that comparison looks like vgn fails at two",
    "start": "1197440",
    "end": "1205840"
  },
  {
    "text": "consecutive trials on picking up the big object we see that a giga actually first like pick up the small object from",
    "start": "1205840",
    "end": "1212240"
  },
  {
    "text": "behind because they it knows that the p-cap this object actually is like less likely to incur collision",
    "start": "1212240",
    "end": "1219280"
  },
  {
    "text": "so it was able to declutter this thing successfully so um then we kind of gotta take a step",
    "start": "1219280",
    "end": "1226960"
  },
  {
    "text": "further to see what exactly is a 3d shape like a that's encoded in that representation so the way we investigate",
    "start": "1226960",
    "end": "1234480"
  },
  {
    "text": "that is we have a 3d reconstruction branch right so we can read all the 3d shape and you can query",
    "start": "1234480",
    "end": "1240880"
  },
  {
    "text": "that occupancy property at different locations from margin cube to extract mesh from it and we can actually",
    "start": "1240880",
    "end": "1246799"
  },
  {
    "text": "visualize the mesh and do comparison and we compare giga with two different variants one is only training with",
    "start": "1246799",
    "end": "1253200"
  },
  {
    "text": "affordance one is only trained with a grasp supervision and then our final model trends on both we see an",
    "start": "1253200",
    "end": "1260480"
  },
  {
    "text": "interesting result that uh the the giga representation that trend um the grasp affordance uh actually pay",
    "start": "1260480",
    "end": "1268640"
  },
  {
    "text": "more attention to the graspable part of the thing than the non-glass bubble part which can kind of make sense if you're",
    "start": "1268640",
    "end": "1275200"
  },
  {
    "text": "looking like the 3d reconstruction with only the 3d reconstruction uh supervision it sometimes meets some",
    "start": "1275200",
    "end": "1282799"
  },
  {
    "text": "geometric details like the handle of the mug because it's really a thin part of this thing but",
    "start": "1282799",
    "end": "1288480"
  },
  {
    "text": "when we train with the grasping supervision it acts try to reconstruct",
    "start": "1288480",
    "end": "1293679"
  },
  {
    "text": "something right so in the grasp of parts like a handle does not have to be super accurate but it knows that there are",
    "start": "1293679",
    "end": "1300000"
  },
  {
    "text": "some 3d geometry in the scene and on the other hand we see that the giga supervision actually ignores some of the",
    "start": "1300000",
    "end": "1307440"
  },
  {
    "text": "part of thing that's less relevant to the grasping test for example the bow before training 3d reconstruction it",
    "start": "1307440",
    "end": "1314720"
  },
  {
    "text": "learns to grasp the bottom of the bow but it is not a grasp of a part so once we see we trained the jointly it was not",
    "start": "1314720",
    "end": "1322159"
  },
  {
    "text": "uh like a grasping that bottom part of the the bow so",
    "start": "1322159",
    "end": "1327600"
  },
  {
    "text": "that's that's giga what we learned from it all right so we use this idea of a joint learning affordance and uh",
    "start": "1327600",
    "end": "1335760"
  },
  {
    "text": "and the 3d reconstruction to develop a steve art performance uh in sixth of",
    "start": "1335760",
    "end": "1341360"
  },
  {
    "text": "grasping and as object representation it has the following properties first",
    "start": "1341360",
    "end": "1347200"
  },
  {
    "text": "this is task dependent meaning that the operator representation is learned from a downstream task for optimizing the",
    "start": "1347200",
    "end": "1354320"
  },
  {
    "text": "graph success so as a result the learned representation actually dynamically",
    "start": "1354320",
    "end": "1359919"
  },
  {
    "text": "allocate its representative representation resource to the part of the things that's most relevant to the",
    "start": "1359919",
    "end": "1366320"
  },
  {
    "text": "task and we see from the 3d reconstruction result show you earlier it is multimodal it tried to capture",
    "start": "1366320",
    "end": "1372799"
  },
  {
    "text": "like a different aspect of the object properties i have 3d shape versus like affordance and",
    "start": "1372799",
    "end": "1379440"
  },
  {
    "text": "and it captured the synergies between these two right so learning affordance actually",
    "start": "1379440",
    "end": "1385280"
  },
  {
    "text": "improved the 3d reconstruction on the grass bubble part and conversely and doing the",
    "start": "1385280",
    "end": "1391360"
  },
  {
    "text": "3d reconstruction helps grasp in those like a partially occluded area",
    "start": "1391360",
    "end": "1397360"
  },
  {
    "text": "it is a structured we combine this conventional like explicit structure of",
    "start": "1397360",
    "end": "1402880"
  },
  {
    "text": "a volumetric feature grid with this implicit neural representation based on neural fields and the entire of the",
    "start": "1402880",
    "end": "1410320"
  },
  {
    "text": "representation is self-supervised meaning that it's learned from the embodied interaction of the robot by",
    "start": "1410320",
    "end": "1417200"
  },
  {
    "text": "doing this kind of trying to error with a grasp candidate without any human supervision",
    "start": "1417200",
    "end": "1424639"
  },
  {
    "text": "some questions um yeah so this is really interesting work i'm wondering um the affordances um are",
    "start": "1424720",
    "end": "1432640"
  },
  {
    "text": "they influenced by the context of the scene as well at all or or is that not or are these",
    "start": "1432640",
    "end": "1439120"
  },
  {
    "text": "objects basically treated individuals the question is like if the affordance are affected by the context of the thing yes because we are occurring this local",
    "start": "1439120",
    "end": "1445840"
  },
  {
    "text": "feature by pulling the representation from the neighboring feature grid it actually takes into account the context",
    "start": "1445840",
    "end": "1452000"
  },
  {
    "text": "information in the neighborhood right so that's why this example of the",
    "start": "1452000",
    "end": "1457600"
  },
  {
    "text": "small bottle for example being picked first in that initial video yeah and then i was wondering um",
    "start": "1457600",
    "end": "1465039"
  },
  {
    "text": "i guess there are lots of works out there that do",
    "start": "1465039",
    "end": "1470080"
  },
  {
    "text": "pick in place right from clutter and it's really hard to compare them uh because of maybe different data they",
    "start": "1470640",
    "end": "1478000"
  },
  {
    "text": "are using or just um how to reproduce them right locally",
    "start": "1478000",
    "end": "1483279"
  },
  {
    "text": "so how do you uh you you because you talked about state of the art and i'm wondering like",
    "start": "1483279",
    "end": "1488480"
  },
  {
    "text": "how do you kind of encompass all this uh landscape of work to good question how do we do like actual proper benchmarking",
    "start": "1488480",
    "end": "1495200"
  },
  {
    "text": "right so here uh most of the quantity results we report an actual based on simulation that we use the same",
    "start": "1495200",
    "end": "1502000"
  },
  {
    "text": "simulation as the prior work and so that we can make sure that we have the right the standardized like protocols yeah",
    "start": "1502000",
    "end": "1508720"
  },
  {
    "text": "doing real-world evaluation as you can imagine is a lot harder to standardize based on different settings hardware",
    "start": "1508720",
    "end": "1514960"
  },
  {
    "text": "objects and lighting conditions and all those and so you train on sim data",
    "start": "1514960",
    "end": "1522960"
  },
  {
    "text": "do you have any sense of the sim to real gap here uh",
    "start": "1522960",
    "end": "1529200"
  },
  {
    "text": "so when it comes to grasping symmetrical transfer is either when we are using dapps information actually we use psdf",
    "start": "1529200",
    "end": "1535840"
  },
  {
    "text": "it's the actual process that's information that really narrows like domain gap allows zero shot transfer",
    "start": "1535840",
    "end": "1541200"
  },
  {
    "text": "with some kind of a proper data augmentation noise injection if i'm doing semantic grasp with rgb",
    "start": "1541200",
    "end": "1547200"
  },
  {
    "text": "rendering will enlarge the domain gap right so that's what makes the transfer harder so i",
    "start": "1547200",
    "end": "1553039"
  },
  {
    "text": "think that's what requires new tools and better rendering better digital augmentation techniques to further close",
    "start": "1553039",
    "end": "1558240"
  },
  {
    "text": "the gap okay yeah thank you i was curious uh maybe a similar",
    "start": "1558240",
    "end": "1564080"
  },
  {
    "text": "question right now this is all with the parallel jaw gripper do you see",
    "start": "1564080",
    "end": "1569200"
  },
  {
    "text": "like this would easily extend to much more complicated grasping like hardware or would it be very hard because of the",
    "start": "1569200",
    "end": "1574799"
  },
  {
    "text": "simulation challenge yeah so um the cell supervises the procedures and allows as long as you can",
    "start": "1574799",
    "end": "1581840"
  },
  {
    "text": "like sample random grasp with like uh the other morphologies but as you can imagine if we're moving to dexter's hand",
    "start": "1581840",
    "end": "1588799"
  },
  {
    "text": "like so sampling valid grasp with like complex mythology for what dealing with like much higher dimensional action",
    "start": "1588799",
    "end": "1595679"
  },
  {
    "text": "space right so that's what could potentially create difficulties so one needs a better pliers to make sure that",
    "start": "1595679",
    "end": "1601679"
  },
  {
    "text": "we're actually simple valley grass with different robot hardware",
    "start": "1601679",
    "end": "1606720"
  },
  {
    "text": "right now the scenes are only composed of graspable and movable objects i'm wondering like what will happen if there",
    "start": "1607840",
    "end": "1612960"
  },
  {
    "text": "are immovable parts for example if there are is a shelf in the scene we haven't",
    "start": "1612960",
    "end": "1618320"
  },
  {
    "text": "configured that we assume that all the small objects are movable yeah",
    "start": "1618320",
    "end": "1624000"
  },
  {
    "text": "like 3d reconstruction as well as the affordance view well i can imagine if we really have",
    "start": "1624320",
    "end": "1629679"
  },
  {
    "text": "like environment fixtures and once you sample random grasp there it will not it will give up negative affordance labels",
    "start": "1629679",
    "end": "1635120"
  },
  {
    "text": "so the network might be able to figure that out but we haven't tried that this we assume that all the objects are",
    "start": "1635120",
    "end": "1640399"
  },
  {
    "text": "graspable from now get there do you like measure anything like",
    "start": "1640399",
    "end": "1646720"
  },
  {
    "text": "out of distribution um yeah so like a",
    "start": "1646720",
    "end": "1652000"
  },
  {
    "text": "good question i mean if you look at the results simulating deformable objects is really hard right so and though we are",
    "start": "1652000",
    "end": "1657679"
  },
  {
    "text": "using a rigid body simulation it's really expensive to simulate the deformation in general but you see that",
    "start": "1657679",
    "end": "1663039"
  },
  {
    "text": "the model was able to kind of grasp the deformable object and all the objects here are not in the training",
    "start": "1663039",
    "end": "1669039"
  },
  {
    "text": "that because there's some random objects that we find in the lab um yeah so i have a decent performance",
    "start": "1669039",
    "end": "1674880"
  },
  {
    "text": "on generalizing to unseen objects good question",
    "start": "1674880",
    "end": "1679440"
  },
  {
    "text": "okay so that's that's giga um",
    "start": "1680480",
    "end": "1687200"
  },
  {
    "start": "1683000",
    "end": "1728000"
  },
  {
    "text": "so it's kind of encouraging that we see that like this object representation can be learned from in body interaction",
    "start": "1687200",
    "end": "1694080"
  },
  {
    "text": "right so it kind of motivates us to take a step further to think can we actually use this interactions to",
    "start": "1694080",
    "end": "1701679"
  },
  {
    "text": "build models of objects that capture the environment properties that's in independent from the interactions see",
    "start": "1701679",
    "end": "1708240"
  },
  {
    "text": "look at this two objects and if i try to pull the door and imagine like how the",
    "start": "1708240",
    "end": "1713679"
  },
  {
    "text": "door will move all right you can sort of guess it and based on your plier but",
    "start": "1713679",
    "end": "1720480"
  },
  {
    "text": "it's only until like you pull them and you see that they are actually quite different kinematic structures right one",
    "start": "1720480",
    "end": "1725600"
  },
  {
    "text": "is the revolute joints and that is the prismatic joints so it's kind of interesting to see that whether we can",
    "start": "1725600",
    "end": "1732399"
  },
  {
    "start": "1728000",
    "end": "1760000"
  },
  {
    "text": "observe the physical interactions of the object in the real world and use us to create",
    "start": "1732399",
    "end": "1738480"
  },
  {
    "text": "evidence for us to learn objects and create digital twins by digital training i mean the",
    "start": "1738480",
    "end": "1744240"
  },
  {
    "text": "virtual copy of the the the same objects in the real world so that we can spawn",
    "start": "1744240",
    "end": "1749279"
  },
  {
    "text": "them in virtual world for same physical simulation and in the fashionable languages uploading those",
    "start": "1749279",
    "end": "1757120"
  },
  {
    "text": "physical objects into the metaverse right so that's kind of the settings very interesting um the concrete approach",
    "start": "1757120",
    "end": "1764320"
  },
  {
    "start": "1760000",
    "end": "1815000"
  },
  {
    "text": "similar to similar to the structure implicit representation we use in giga we use",
    "start": "1764320",
    "end": "1771120"
  },
  {
    "text": "that to process the input pawn club before and after the interactions and that we call this approach detail that",
    "start": "1771120",
    "end": "1777360"
  },
  {
    "text": "stands for digital training of articulate objects if you play pokemon you know what detail",
    "start": "1777360",
    "end": "1782480"
  },
  {
    "text": "is okay so it opens the uh part level geometry and the three illustrate and and also like a 3d",
    "start": "1782480",
    "end": "1789440"
  },
  {
    "text": "segmentation of the parts and also the articulation parameters so just to",
    "start": "1789440",
    "end": "1794720"
  },
  {
    "text": "illustrate how the workflow looks like so it start by taking the um like pawn",
    "start": "1794720",
    "end": "1800399"
  },
  {
    "text": "cloud observations from the iron hand camera and then it can interact either by human or by the robot and then look",
    "start": "1800399",
    "end": "1807600"
  },
  {
    "text": "at the thing again after the interaction this gave us the evidence to create this",
    "start": "1807600",
    "end": "1813679"
  },
  {
    "text": "interactable digital tree by interactive digital training we mean that you have",
    "start": "1813679",
    "end": "1818880"
  },
  {
    "start": "1815000",
    "end": "1842000"
  },
  {
    "text": "this object segmentation geometry and the articulation properties in place so that we can actually represent this",
    "start": "1818880",
    "end": "1826080"
  },
  {
    "text": "object in some standard 3d format that we can import in a physics engine for simulation and",
    "start": "1826080",
    "end": "1832480"
  },
  {
    "text": "for detail to work it does not assume that we know the object category a priori and also it works on two type of",
    "start": "1832480",
    "end": "1838799"
  },
  {
    "text": "joints like prismatic versus versus revolut so why do we care about digital twin it's",
    "start": "1838799",
    "end": "1845440"
  },
  {
    "text": "about is that this type of digital training create recreation actually paves the road for us to narrow the",
    "start": "1845440",
    "end": "1852559"
  },
  {
    "text": "reality gap so that we can observe the interactions of physical objects in the",
    "start": "1852559",
    "end": "1857840"
  },
  {
    "text": "real world and then use this to recreate this the same object in physical",
    "start": "1857840",
    "end": "1862880"
  },
  {
    "text": "simulation and the reason we like simulation is because we can leverage like a data driven text in general like",
    "start": "1862880",
    "end": "1869200"
  },
  {
    "text": "a very huge amount of cheap data to train robots to",
    "start": "1869200",
    "end": "1874240"
  },
  {
    "text": "learn manipulation behaviors with this object and here we are using a simulation framework that we developed",
    "start": "1874240",
    "end": "1879679"
  },
  {
    "text": "in in my lab called robot suite and then we can take the uh manipulation behavior back to the real",
    "start": "1879679",
    "end": "1886159"
  },
  {
    "text": "world for execution right and then this new manipulation behavior further like",
    "start": "1886159",
    "end": "1891200"
  },
  {
    "text": "generate new sensory stimuli for us to learn more knowledge about the object so overall the detail gives us a",
    "start": "1891200",
    "end": "1898720"
  },
  {
    "text": "a way to build interactive models to learn the environment property of",
    "start": "1898720",
    "end": "1903919"
  },
  {
    "text": "objects that form from invited interaction and we show again that uh being able to physically interact with",
    "start": "1903919",
    "end": "1910480"
  },
  {
    "text": "the object emit new information for us to better understand look at different",
    "start": "1910480",
    "end": "1915679"
  },
  {
    "text": "aspects of objects and again we show that uh this structured representation allows us to jointly encode different",
    "start": "1915679",
    "end": "1922640"
  },
  {
    "text": "object properties like their geometry and the articulation and uh again the idea of creating digital twin allows us",
    "start": "1922640",
    "end": "1930399"
  },
  {
    "text": "to kind of like bridging the simulation and the real world in a tight loop so that we can go from",
    "start": "1930399",
    "end": "1936720"
  },
  {
    "text": "real world to simulation and back to the real world uh through this digital twin creation",
    "start": "1936720",
    "end": "1942720"
  },
  {
    "text": "okay so that's the kind of the first part i thought i should probably stop stop here and see any questions",
    "start": "1942720",
    "end": "1950559"
  },
  {
    "start": "1943000",
    "end": "2030000"
  },
  {
    "text": "i think wait on my microphone um i yeah maybe i",
    "start": "1954080",
    "end": "1960159"
  },
  {
    "text": "blinked or maybe i missed it but where was the model how you uh did the digital",
    "start": "1960159",
    "end": "1966080"
  },
  {
    "text": "twin creation like i was just wondering about oh okay okay i haven't showed them",
    "start": "1966080",
    "end": "1971679"
  },
  {
    "text": "like i i guess i blanked yeah or maybe you didn't show up that's kind of a technical detail i didn't go over it",
    "start": "1971679",
    "end": "1977279"
  },
  {
    "text": "does some kind of a point cloud processing use some cross attention the key is really established",
    "start": "1977279",
    "end": "1983200"
  },
  {
    "text": "correspondence between before after interactions right it's how the pawn cloud moves okay and then we use this to",
    "start": "1983200",
    "end": "1989840"
  },
  {
    "text": "decode with this implicit neural field representations yeah okay and so the articulation decoder do",
    "start": "1989840",
    "end": "1996880"
  },
  {
    "text": "you how do you assume like a set of possible mechanisms like a",
    "start": "1996880",
    "end": "2002799"
  },
  {
    "text": "prismatic and revolutionary assume there's two and there's like first a classification to",
    "start": "2002799",
    "end": "2007840"
  },
  {
    "text": "determine the type versus like it's a binary classifier and then based on that we have a",
    "start": "2007840",
    "end": "2014080"
  },
  {
    "text": "another decoders to output the actual parameters under that articulation type",
    "start": "2014080",
    "end": "2019200"
  },
  {
    "text": "okay great thank you okay so",
    "start": "2019200",
    "end": "2024720"
  },
  {
    "text": "now i finish talk about object representation let's move on to to to scales all right so",
    "start": "2024720",
    "end": "2030720"
  },
  {
    "text": "what is skills so skill is highly ties to the notion of a",
    "start": "2030720",
    "end": "2036120"
  },
  {
    "text": "compositionality so compositionality is a multiple word but it's also a very",
    "start": "2036120",
    "end": "2041200"
  },
  {
    "text": "hot topic these days in machine learning and many people many machine learning researchers believe that capturing the",
    "start": "2041200",
    "end": "2047919"
  },
  {
    "text": "compositional structures of real world phenomena let it be language of videos",
    "start": "2047919",
    "end": "2053118"
  },
  {
    "text": "or robotics task execution is really the key to achieve systematic generalizations with modern machine",
    "start": "2053119",
    "end": "2059358"
  },
  {
    "text": "learning tool so a lot of the seminal working compositionality come from",
    "start": "2059359",
    "end": "2064480"
  },
  {
    "text": "language and logics and semantics which they investigate how the sentence of the",
    "start": "2064480",
    "end": "2071679"
  },
  {
    "text": "meaning of the sentence emerged as like a composition of a bunch of words in it and drawing a parallel we like to study",
    "start": "2071679",
    "end": "2079280"
  },
  {
    "text": "compositionality uh in the context of robotic execution think about a task like pre-print dinner how can we",
    "start": "2079280",
    "end": "2085358"
  },
  {
    "text": "gradually break that down into sub parts like washing dishes cooking food and then further decomposing into like a",
    "start": "2085359",
    "end": "2091520"
  },
  {
    "text": "more primitive actions like picking pushing grasping and we call this bottom up bottom level constituents of the",
    "start": "2091520",
    "end": "2098960"
  },
  {
    "text": "behavior as like sensory motor skills so these are grounded on raw perception that are temporarily extended so um with",
    "start": "2098960",
    "end": "2106880"
  },
  {
    "text": "temporal abstraction so if you look at compositionality and the philosophers investmentation",
    "start": "2106880",
    "end": "2113440"
  },
  {
    "text": "linguists have spent a lot of time like understanding what are the principles of a compositionality so looking at those",
    "start": "2113440",
    "end": "2119760"
  },
  {
    "text": "it really gives you lots of insight so overall there's kind of two ways of looking at this one is the context",
    "start": "2119760",
    "end": "2126320"
  },
  {
    "text": "principle saying that the really the meaning of the word hinges on the context of the other words like",
    "start": "2126320",
    "end": "2132960"
  },
  {
    "text": "surrounding the word in the in the same sentence right so the exact meaning of bark kind of like it depends on there's",
    "start": "2132960",
    "end": "2138720"
  },
  {
    "text": "a the yellow dog bark at the cat right so the yellow dog and the the cat can't",
    "start": "2138720",
    "end": "2144400"
  },
  {
    "text": "influence the meaning of the bark on the other hand there's like the compositionality principle seeing that",
    "start": "2144400",
    "end": "2150240"
  },
  {
    "text": "the meaning of this sentence is a function of this individual parts in that case like dog cat bark have their",
    "start": "2150240",
    "end": "2156720"
  },
  {
    "text": "own meaning and when we piece them together that can like give rise to the meaning of the sentence so",
    "start": "2156720",
    "end": "2161839"
  },
  {
    "text": "which one is correct with wrong is wrong this is still open debate right so we're not going to argue which one is the",
    "start": "2161839",
    "end": "2167880"
  },
  {
    "text": "compositionality principle uh is is truth but uh instead we're going to",
    "start": "2167880",
    "end": "2174320"
  },
  {
    "text": "leverage these two principles and investigates in the context of compositionality in the robotics context",
    "start": "2174320",
    "end": "2180560"
  },
  {
    "start": "2180000",
    "end": "2232000"
  },
  {
    "text": "so this leads to our work first bus which stands for uh bottom up discovery",
    "start": "2180560",
    "end": "2186400"
  },
  {
    "text": "of sensory motor skill that uh we start with a bunch of like unstructured",
    "start": "2186400",
    "end": "2192079"
  },
  {
    "text": "task demonstrations where the goal is to extract this words from the sentence and",
    "start": "2192079",
    "end": "2198400"
  },
  {
    "text": "the drawing analogy here is going from the task demonstration to a set of",
    "start": "2198400",
    "end": "2204079"
  },
  {
    "text": "sensory motor skills that we can use as building blocks to synthesize like a long-term robot interaction to solve new",
    "start": "2204079",
    "end": "2211200"
  },
  {
    "text": "tasks and once we have this library of skills we can compose them with a meta controller to",
    "start": "2211200",
    "end": "2217599"
  },
  {
    "text": "synthesize long-term interaction and we want this to be realistic and deployable to real hardware so we want to make sure",
    "start": "2217599",
    "end": "2224480"
  },
  {
    "text": "that this work is scalable we do not require human temporal annotations and",
    "start": "2224480",
    "end": "2229680"
  },
  {
    "text": "the skills operate on raw sensory data so concretely how about work is first by",
    "start": "2229680",
    "end": "2236400"
  },
  {
    "start": "2232000",
    "end": "2276000"
  },
  {
    "text": "building a hardcore representation of a task demonstration it goes from the bottom all the way to the top but",
    "start": "2236400",
    "end": "2243440"
  },
  {
    "text": "recursively group like a neighboring um temporal segments into one based on",
    "start": "2243440",
    "end": "2249520"
  },
  {
    "text": "their similarity where the similarity is measured by some multimodal or multisensory statistics so we go all the",
    "start": "2249520",
    "end": "2257119"
  },
  {
    "text": "way to the top and apply this algorithm clustering procedures and from this",
    "start": "2257119",
    "end": "2262400"
  },
  {
    "text": "hierarchical tree structure we can extract the recurring temporal segments so these are",
    "start": "2262400",
    "end": "2269599"
  },
  {
    "text": "the temporal segments that keep repeating in your data sets and then we use them as the training set of skills",
    "start": "2269599",
    "end": "2275839"
  },
  {
    "text": "so to illustrate the workflow we have the temporal tree structure that we",
    "start": "2275839",
    "end": "2281599"
  },
  {
    "text": "extract the temporal segmentations and then we compute some feature representation of those individual",
    "start": "2281599",
    "end": "2287680"
  },
  {
    "text": "segments and we put together like all the temporal segments from like the task",
    "start": "2287680",
    "end": "2293599"
  },
  {
    "text": "demonstrations in our in our data set and this demonstration can be collected for",
    "start": "2293599",
    "end": "2299440"
  },
  {
    "text": "like a variety of our task skills and this allows us to map the robotic",
    "start": "2299440",
    "end": "2304800"
  },
  {
    "text": "behavior into this latent space that we can do a clustering procedure to identify the different",
    "start": "2304800",
    "end": "2311920"
  },
  {
    "text": "partitions of the behaviors in that latent space from this just give us like a training",
    "start": "2311920",
    "end": "2317760"
  },
  {
    "text": "set for k different skills like if we are running a gaming class string and that partition the space by k copies and",
    "start": "2317760",
    "end": "2325440"
  },
  {
    "text": "then this gives the data sets for us to train for case sensory motor skills that we can actually use the data for um goal",
    "start": "2325440",
    "end": "2333359"
  },
  {
    "text": "condition imitation learning to train these skills and once we train that we can",
    "start": "2333359",
    "end": "2338880"
  },
  {
    "text": "compose this with a metacontroller to invoke the skill by like a",
    "start": "2338880",
    "end": "2344720"
  },
  {
    "text": "predicting the skill id like which skill to invoke as well like the setting the sub goal for",
    "start": "2344720",
    "end": "2350400"
  },
  {
    "text": "those skills to achieve that this way allows us to like uh invoke those skills",
    "start": "2350400",
    "end": "2356720"
  },
  {
    "text": "to solve tasks and when we train the metacontroller and again use some like a",
    "start": "2356720",
    "end": "2361760"
  },
  {
    "text": "supervised imitation learning objective that we can learn from a dataset without drawing error",
    "start": "2361760",
    "end": "2367440"
  },
  {
    "text": "so to show this actually gives like a quite capable like a um solutions to learn like a sensory motor",
    "start": "2367440",
    "end": "2374560"
  },
  {
    "text": "controllers to solve reasonable long horizon tasks that if you don't have the temporal hierarchy or modeling it's",
    "start": "2374560",
    "end": "2380480"
  },
  {
    "text": "really hard to struggle to to learn and the entire model is trained quite efficiently with handful of",
    "start": "2380480",
    "end": "2387040"
  },
  {
    "text": "demonstrations collecting certain minutes all the time and there's kind of interesting observations from this uh this",
    "start": "2387040",
    "end": "2394720"
  },
  {
    "text": "experiment first we showed that uh by grouping and building this hardcore",
    "start": "2394720",
    "end": "2399920"
  },
  {
    "text": "task structure based on multiple uh modality or like a multi-sensory cues",
    "start": "2399920",
    "end": "2405440"
  },
  {
    "text": "like multiple views of images and also the pro perception actually leads to better",
    "start": "2405440",
    "end": "2411200"
  },
  {
    "text": "tasks and skill segmentation right so this allows us to discover better skills",
    "start": "2411200",
    "end": "2416319"
  },
  {
    "text": "to be composed and for solving tasks so this kind of resonates some of the seminal study in",
    "start": "2416319",
    "end": "2422800"
  },
  {
    "text": "event perception a lot of work done here in stanford like by uh barbara trevetsky",
    "start": "2422800",
    "end": "2428079"
  },
  {
    "text": "that uh like so human actually leverage this multi-modal cues and to perceive the boundaries of events",
    "start": "2428079",
    "end": "2434400"
  },
  {
    "text": "and we show that uh by discovering and learning those skills from multi-task demonstration although those",
    "start": "2434400",
    "end": "2440480"
  },
  {
    "text": "demonstrations are not necessarily collected for the same task by kind of identifying these recurring patterns and",
    "start": "2440480",
    "end": "2447119"
  },
  {
    "text": "the training on those data they actually give rise to more reusable more",
    "start": "2447119",
    "end": "2452240"
  },
  {
    "text": "generalizable skills so that uh uh leads to uh higher task performance",
    "start": "2452240",
    "end": "2457680"
  },
  {
    "text": "compared to skills if are only like discovered from the task specific demonstration",
    "start": "2457680",
    "end": "2464480"
  },
  {
    "text": "and also uh because like the skills are discovered for multiple tasks and they",
    "start": "2464480",
    "end": "2469599"
  },
  {
    "text": "have the good versatility and generalization that uh we can reuse them for novel tasks by freezing the skills",
    "start": "2469599",
    "end": "2477359"
  },
  {
    "text": "as apis and only train a high-level meta controller to invoke them in a new",
    "start": "2477359",
    "end": "2483680"
  },
  {
    "text": "combination to solve new instances of tasks and to show you how this works as we",
    "start": "2483680",
    "end": "2489440"
  },
  {
    "text": "said that uh we want to put minimal assumptions so that it does not rely on human",
    "start": "2489440",
    "end": "2495040"
  },
  {
    "text": "manual segmentation and uh like in 30 minutes of demonstration were able to train the robots to perform this long",
    "start": "2495040",
    "end": "2502560"
  },
  {
    "text": "horizon task on the real robot so everything was done from growing up from images and to the motor control with",
    "start": "2502560",
    "end": "2509680"
  },
  {
    "text": "imitation learning approach so that concludes buzz right so it's a",
    "start": "2509680",
    "end": "2515119"
  },
  {
    "start": "2511000",
    "end": "2561000"
  },
  {
    "text": "bottom-up approach to discover a set of reusable skills from task demonstration",
    "start": "2515119",
    "end": "2520720"
  },
  {
    "text": "um we showed that uh leveraging multi-sensory cues and being able to discover the skills from multi-task",
    "start": "2520720",
    "end": "2527359"
  },
  {
    "text": "demonstrations and improve the diversity of data and as a result by training those improve the quality of the skill",
    "start": "2527359",
    "end": "2535280"
  },
  {
    "text": "and by exploiting this skill and the meta controller type of a hierarchical design it give rise to a sample",
    "start": "2535280",
    "end": "2542800"
  },
  {
    "text": "efficient learning algorithm to learn complex behaviors using handful of demonstrations",
    "start": "2542800",
    "end": "2549359"
  },
  {
    "text": "okay so take questions how did you do the clustering in the feature space or like were there a lot",
    "start": "2549359",
    "end": "2555760"
  },
  {
    "text": "of was there a lot of tuning required or was it easy to find very different clusters",
    "start": "2555760",
    "end": "2561040"
  },
  {
    "start": "2561000",
    "end": "2606000"
  },
  {
    "text": "um how how much does that have an effect on the end goal good question so for the classroom what",
    "start": "2561040",
    "end": "2567040"
  },
  {
    "text": "we do is for each of the temporal segments we compute a feature based on some multi-modal representation learning",
    "start": "2567040",
    "end": "2573520"
  },
  {
    "text": "actually that exactly work is what i did with ginette and there was michelle lee and jeanette in the icar 2019 on how to",
    "start": "2573520",
    "end": "2581040"
  },
  {
    "text": "learn like a cell supervised multi-modal representation we use this representation as some signature of the",
    "start": "2581040",
    "end": "2587599"
  },
  {
    "text": "temporal segmentation and then we group them so the grouping actually we're using a",
    "start": "2587599",
    "end": "2593200"
  },
  {
    "text": "classical like a clustering method called hierarchical algorithm clustering it's pretty simple method to find two",
    "start": "2593200",
    "end": "2600880"
  },
  {
    "text": "closest uh like a temporal segments in in the neighborhood and then combine them into one",
    "start": "2600880",
    "end": "2606880"
  },
  {
    "text": "but uh the exact algorithm that we use is kind of orthogonal right so",
    "start": "2606880",
    "end": "2612079"
  },
  {
    "text": "if we have a better clustering algorithm that we can plug that in in our framework and in the paper we actually",
    "start": "2612079",
    "end": "2617839"
  },
  {
    "text": "com compare with like different choices like k-means versus spectral clustering versus this and this is the one that is",
    "start": "2617839",
    "end": "2625359"
  },
  {
    "text": "the most effective in the setting",
    "start": "2625359",
    "end": "2628799"
  },
  {
    "text": "generally related like a follow-up um so you have this representation and you",
    "start": "2630560",
    "end": "2636079"
  },
  {
    "text": "do this unsupervised clustering i guess right but is there any what",
    "start": "2636079",
    "end": "2641440"
  },
  {
    "text": "what is the supervisory signal at the end to say like oh yeah that's a good",
    "start": "2641440",
    "end": "2646560"
  },
  {
    "text": "cluster or is that not that kind of supervision signal is that not there",
    "start": "2646560",
    "end": "2652079"
  },
  {
    "text": "yeah so there's no human supervision which means that the discovered there's a give and",
    "start": "2652079",
    "end": "2657839"
  },
  {
    "text": "take if you are doing unsupervised completely there's no guarantee that the discovered skill actually carry like",
    "start": "2657839",
    "end": "2664400"
  },
  {
    "text": "semantic meanings but the reason that it was able to produce sensible segments is",
    "start": "2664400",
    "end": "2670880"
  },
  {
    "text": "because that uh in a manipulation procedure by looking at like the multi-sensory statistics and",
    "start": "2670880",
    "end": "2678160"
  },
  {
    "text": "it can actually give some hints on the structures of the task if you're doing like so pack insertion before and after",
    "start": "2678160",
    "end": "2684800"
  },
  {
    "text": "you touch the packet right so the actually the correlation between the",
    "start": "2684800",
    "end": "2690480"
  },
  {
    "text": "like a multi-modal sensory data changes like it's like statistical pattern so by using this multi-sensory contingencies",
    "start": "2690480",
    "end": "2697760"
  },
  {
    "text": "and that is like the kind of supervisory signal that we are trying to harness to build this consistent house structure",
    "start": "2697760",
    "end": "2705359"
  },
  {
    "text": "okay yeah and is it how do you pick k",
    "start": "2705359",
    "end": "2711200"
  },
  {
    "text": "again a good question so uh that's a higher that's a hyper parameter we",
    "start": "2711200",
    "end": "2716240"
  },
  {
    "text": "choose right so somehow in the paper we said six is the best uh so we have a cross validation and it was",
    "start": "2716240",
    "end": "2723680"
  },
  {
    "text": "kind of a bell-shaped curve um this i think is like a problem dependent right based on what is like the",
    "start": "2723680",
    "end": "2729920"
  },
  {
    "text": "diversity of the task and also like the complexity and so forth so yeah that's a",
    "start": "2729920",
    "end": "2735680"
  },
  {
    "text": "that's a hyper problem to change okay so that's",
    "start": "2735680",
    "end": "2742240"
  },
  {
    "text": "the first part of the compositional modeling and back to uh this this slice on compositionality",
    "start": "2742240",
    "end": "2750640"
  },
  {
    "text": "in language and the behaviors right so what kind of boss was doing it kind of resonates with like the context",
    "start": "2750640",
    "end": "2757280"
  },
  {
    "start": "2753000",
    "end": "2789000"
  },
  {
    "text": "principle seeing that we first give you a bunch of tasks and i want to extract the skills as like the kind of the the",
    "start": "2757280",
    "end": "2764640"
  },
  {
    "text": "building blocks like the basic constituents of the tasks so you can imagine it's like drawing a parallel to",
    "start": "2764640",
    "end": "2770800"
  },
  {
    "text": "like it from sentences to words so now we are like looking at the opposite",
    "start": "2770800",
    "end": "2776160"
  },
  {
    "text": "direction if we are provided with a a set of like skills a priori uh like a",
    "start": "2776160",
    "end": "2781839"
  },
  {
    "text": "reaching grasping pushing can we use as the building blocks to scaffold like",
    "start": "2781839",
    "end": "2787119"
  },
  {
    "text": "long horizon and complex robotic tasks so that leads to maple which stands for",
    "start": "2787119",
    "end": "2794400"
  },
  {
    "start": "2789000",
    "end": "2807000"
  },
  {
    "text": "manipulation primitive augmented reinforcement learning so in maple we look at we already given a set of",
    "start": "2794400",
    "end": "2801599"
  },
  {
    "text": "heterogeneous uh library of skills the reason we call heterogeneous is because",
    "start": "2801599",
    "end": "2807119"
  },
  {
    "start": "2807000",
    "end": "2886000"
  },
  {
    "text": "this skill might operate at different time temporal resolution and take different arguments to instantiate their",
    "start": "2807119",
    "end": "2813040"
  },
  {
    "text": "execution for example grasping can be a skill right so the way we call grasping as functional apis we specify the graph",
    "start": "2813040",
    "end": "2819599"
  },
  {
    "text": "center and the grip orientation why this is a good idea to use like a skills well if i'm doing reinforcement",
    "start": "2819599",
    "end": "2826640"
  },
  {
    "text": "learning you know there's other word of robotics there has been been decades of research building special purpose",
    "start": "2826640",
    "end": "2832560"
  },
  {
    "text": "robotic behavior modules right so there's grasping there's a motion planning there's like a dynamic movement",
    "start": "2832560",
    "end": "2838960"
  },
  {
    "text": "like a primitive et cetera so can we actually take advantage of this decades of",
    "start": "2838960",
    "end": "2844640"
  },
  {
    "text": "research and in robotics and to use that as a building blocks for behavior and",
    "start": "2844640",
    "end": "2849680"
  },
  {
    "text": "this modules are typically very well understood they generalize well and they have like good performance guarantee",
    "start": "2849680",
    "end": "2857119"
  },
  {
    "text": "so that's the key idea but the one special thing that we did is that what",
    "start": "2857119",
    "end": "2862640"
  },
  {
    "text": "if the libraries that we built is incomplete it cannot express the entire",
    "start": "2862640",
    "end": "2868079"
  },
  {
    "text": "space of possible robotic behavior so we add a additional token or additional",
    "start": "2868079",
    "end": "2873760"
  },
  {
    "text": "primitive called autonomic actions that's typically what reinforcement learning algorithm use as their action",
    "start": "2873760",
    "end": "2879680"
  },
  {
    "text": "space right so to fill in the bank in case like some behavior cannot be expressed by the the rest of the library",
    "start": "2879680",
    "end": "2887119"
  },
  {
    "start": "2886000",
    "end": "2931000"
  },
  {
    "text": "so that's the key idea that uh we train reinforcement learning now we use this",
    "start": "2887119",
    "end": "2892160"
  },
  {
    "text": "uh library as the underlying action space so the policy actually outputs the primitive type and also the parameter",
    "start": "2892160",
    "end": "2899599"
  },
  {
    "text": "arguments to instantiate those primitives and then execute this primitives to interact with the",
    "start": "2899599",
    "end": "2904720"
  },
  {
    "text": "environment and then once the primitive finishes and it returns back",
    "start": "2904720",
    "end": "2909760"
  },
  {
    "text": "the observation and the rewards and so that we can move to the next stage of decision making",
    "start": "2909760",
    "end": "2915040"
  },
  {
    "text": "when we do it other way kind of implement a hierarchical policy that uh uh",
    "start": "2915040",
    "end": "2920319"
  },
  {
    "text": "first select the primitive and under that primitive and the way determines the the argument that the whole thing",
    "start": "2920319",
    "end": "2926319"
  },
  {
    "text": "can be trained end-to-end with some software to critique this method",
    "start": "2926319",
    "end": "2931440"
  },
  {
    "start": "2931000",
    "end": "2980000"
  },
  {
    "text": "so to just to kind of illustrate the idea we kind of compare maple with a bunch of",
    "start": "2931440",
    "end": "2937359"
  },
  {
    "text": "baselines and you can find more in the paper uh we first showed that atomic actually if we're just using the like a",
    "start": "2937359",
    "end": "2943680"
  },
  {
    "text": "bare metal low level control and it won't succeed there's really struggles for the exploration challenge",
    "start": "2943680",
    "end": "2949839"
  },
  {
    "text": "and uh compared to maple we show that it's also necessary to include those uh uh",
    "start": "2949839",
    "end": "2956800"
  },
  {
    "text": "primitive uh atomic actions because if i do impact insertion there's a certain",
    "start": "2956800",
    "end": "2961839"
  },
  {
    "text": "like phase of the task that requires some really contact reach interaction like the insertion phase right so it needs some very precise and dexterous",
    "start": "2961839",
    "end": "2969760"
  },
  {
    "text": "manipulation behavior it's really hard to model in the primitive that we have as limited so we see that it was able to",
    "start": "2969760",
    "end": "2976240"
  },
  {
    "text": "invoke those atomic primitives to fill in those gaps and actually we kind of by composing those",
    "start": "2976240",
    "end": "2982640"
  },
  {
    "start": "2980000",
    "end": "3095000"
  },
  {
    "text": "primitives and the maple unveils the innate compositional structure of the task so by analyzing the execution of",
    "start": "2982640",
    "end": "2990960"
  },
  {
    "text": "what the primitive is like each time it can actually be this kind of wordle like",
    "start": "2990960",
    "end": "2997040"
  },
  {
    "text": "task sketch any of you playing wordle so you know that uh there's a",
    "start": "2997040",
    "end": "3002160"
  },
  {
    "text": "squares means like the type of primitive it's like each time and then we can just try roll out the policy under different",
    "start": "3002160",
    "end": "3008960"
  },
  {
    "text": "initialization and then this kind of generate this what we call a task sketch as like the",
    "start": "3008960",
    "end": "3014880"
  },
  {
    "text": "scaffold as some kind of a signature representing the kind of the structures of the task so why are we",
    "start": "3014880",
    "end": "3022480"
  },
  {
    "text": "interested in that well for example like this pack insertion task we see that first it involves the grasp primitive to",
    "start": "3022480",
    "end": "3029440"
  },
  {
    "text": "firstly pick up the pack and then call reach to try to inch forward and towards",
    "start": "3029440",
    "end": "3034559"
  },
  {
    "text": "the hole and then in the insertion phase is use the atomic actions to actually does the insertion",
    "start": "3034559",
    "end": "3040400"
  },
  {
    "text": "so by analyzing the task sketch and under different instantiation different randomization we",
    "start": "3040400",
    "end": "3046800"
  },
  {
    "text": "can actually have some kind of quantitative sense of the compositionality of the task that's",
    "start": "3046800",
    "end": "3051839"
  },
  {
    "text": "actually what we did like for a variety of uh robotics manipulation from simple lifting stacking pick place to like more",
    "start": "3051839",
    "end": "3059920"
  },
  {
    "text": "contact-rich ones like assembly and an insertion that we see that uh",
    "start": "3059920",
    "end": "3065520"
  },
  {
    "text": "it makes intuitive sense like for for block word and the stacking picking",
    "start": "3065520",
    "end": "3070559"
  },
  {
    "text": "place and the often the cases only the high level primitive like picking placing and",
    "start": "3070559",
    "end": "3076319"
  },
  {
    "text": "reaching i invoke to fulfill the task and the task sketch has a very good",
    "start": "3076319",
    "end": "3081599"
  },
  {
    "text": "consistency across different randomization and and uh but uh for more contact reach and task that requires",
    "start": "3081599",
    "end": "3088160"
  },
  {
    "text": "physical interaction often the case we see that it's a lot more variable in terms of the task structure",
    "start": "3088160",
    "end": "3095599"
  },
  {
    "text": "and another benefit of using this primitive is that it can effectively abstract away a lot of look at the low",
    "start": "3095599",
    "end": "3102720"
  },
  {
    "text": "level dynamics mismatch between simulation and the real robot if we have a good implementation of a grasping",
    "start": "3102720",
    "end": "3108079"
  },
  {
    "text": "module or motion planning module right so we can effectively train our models in physical",
    "start": "3108079",
    "end": "3114400"
  },
  {
    "text": "simulation and then deploy that in the real world so this is in both domains we",
    "start": "3114400",
    "end": "3119680"
  },
  {
    "text": "are using a post estimator first to detect the object pose and empirically",
    "start": "3119680",
    "end": "3124720"
  },
  {
    "text": "often we find that transferring a post-estimation model from simulation to real world with aggressive randomization",
    "start": "3124720",
    "end": "3131200"
  },
  {
    "text": "is a lot easier than transferring into a policy right so by doing this and training the policy on that like a 3d",
    "start": "3131200",
    "end": "3137920"
  },
  {
    "text": "pose and it allows us to achieve this real robot transfer okay so to conclude",
    "start": "3137920",
    "end": "3143599"
  },
  {
    "start": "3142000",
    "end": "3185000"
  },
  {
    "text": "this we talked about how can we use a heterogeneous set of primitives to scaffold the lung horizon robot",
    "start": "3143599",
    "end": "3150640"
  },
  {
    "text": "manipulation and uh the idea is to develop develop a hierarchical policy",
    "start": "3150640",
    "end": "3155760"
  },
  {
    "text": "that selects the primitive from the library and instantiate its behaviors by",
    "start": "3155760",
    "end": "3160880"
  },
  {
    "text": "like specifying the arguments of the functional apis and we show that by doing this we can analyze the innate",
    "start": "3160880",
    "end": "3168160"
  },
  {
    "text": "structures of the composition composition structure of the task contingent on the primitive we have of",
    "start": "3168160",
    "end": "3175200"
  },
  {
    "text": "course the more primitive we have and it's likely to change our composition representation of the task depends on",
    "start": "3175200",
    "end": "3182800"
  },
  {
    "text": "the versatility and reusability of the task so um",
    "start": "3182800",
    "end": "3188319"
  },
  {
    "start": "3185000",
    "end": "3243000"
  },
  {
    "text": "these are the four works that i covered so far what we learned okay so we first",
    "start": "3188319",
    "end": "3193760"
  },
  {
    "text": "talked about what is the skill what are the object representation that we want to learn that i talked about giga and",
    "start": "3193760",
    "end": "3200319"
  },
  {
    "text": "detail showing you that the object representation we care for fulfilling robotic tasks are often context",
    "start": "3200319",
    "end": "3206880"
  },
  {
    "text": "dependent and task dependent so we can actually learn effective actionable",
    "start": "3206880",
    "end": "3212079"
  },
  {
    "text": "representation from downstream task supervision and it's often a good idea to capture the",
    "start": "3212079",
    "end": "3218240"
  },
  {
    "text": "multimodality of the uh the object properties and the multifaceted and like",
    "start": "3218240",
    "end": "3223599"
  },
  {
    "text": "very often we see synergistic relationship of understanding the object from different perspectives",
    "start": "3223599",
    "end": "3229920"
  },
  {
    "text": "and uh we show that uh in body interaction with the objects and is a powerful source of supervision that",
    "start": "3229920",
    "end": "3236079"
  },
  {
    "text": "allows us to learn this perceptually grounded action-informed information",
    "start": "3236079",
    "end": "3241119"
  },
  {
    "text": "from the interaction and what is a skill we investigate like the context principle versus",
    "start": "3241119",
    "end": "3247880"
  },
  {
    "start": "3243000",
    "end": "3284000"
  },
  {
    "text": "compositionality principle was going from task to skill another is the opposite direction from skill to task",
    "start": "3247880",
    "end": "3254720"
  },
  {
    "text": "and we one of the key like a property to build effective skill is",
    "start": "3254720",
    "end": "3261520"
  },
  {
    "text": "their expressiveness their versatility and also their reusability in different contexts in different tasks and of the",
    "start": "3261520",
    "end": "3268559"
  },
  {
    "text": "key here is to build temporal abstraction to reduce the challenge of modeling the low level actions and the",
    "start": "3268559",
    "end": "3276319"
  },
  {
    "text": "fine is the and also we want to make sure that the learning skills are perceptually grounded that can operate on raw sensory",
    "start": "3276319",
    "end": "3283119"
  },
  {
    "text": "data so to kind of put this in a bigger scope remember like at the beginning i",
    "start": "3283119",
    "end": "3289359"
  },
  {
    "start": "3284000",
    "end": "3350000"
  },
  {
    "text": "mentioned like so how do we combine the comp engineering principles and two modern rebound learning approach that uh",
    "start": "3289359",
    "end": "3296720"
  },
  {
    "text": "uh to achieve this i argue that we really need to combine like the methodologies from different views and",
    "start": "3296720",
    "end": "3303599"
  },
  {
    "text": "historically um building ai architecture for rural autonomy so in the classical",
    "start": "3303599",
    "end": "3309440"
  },
  {
    "text": "pipeline naturally we are embracing the principle of abstraction right so we divide this entire stack of pixel torque",
    "start": "3309440",
    "end": "3316400"
  },
  {
    "text": "into a separate separate problem of perception planning control there's",
    "start": "3316400",
    "end": "3321440"
  },
  {
    "text": "vision committee work on perception planning community work on planning control community work on control and that",
    "start": "3321440",
    "end": "3327680"
  },
  {
    "text": "didn't really talk to each other and what's the problem with this is it creates this inf information bottleneck",
    "start": "3327680",
    "end": "3334000"
  },
  {
    "text": "between the modules right so there's going to be compounding errors throughout the stack and deep learning",
    "start": "3334000",
    "end": "3339680"
  },
  {
    "text": "eventually throughout the stack and replace this with an end-to-end trainable neural network but we start to",
    "start": "3339680",
    "end": "3344799"
  },
  {
    "text": "see limitation on the high data requirements and also limited generalization",
    "start": "3344799",
    "end": "3350160"
  },
  {
    "start": "3350000",
    "end": "3385000"
  },
  {
    "text": "i think um a promising like new frontier is really kind of like combine the",
    "start": "3350160",
    "end": "3355440"
  },
  {
    "text": "complementary strengths of different ideas right so well we can still use the kind of",
    "start": "3355440",
    "end": "3361520"
  },
  {
    "text": "restructured priors and inductive bias in individual learning modules like the",
    "start": "3361520",
    "end": "3366799"
  },
  {
    "text": "perception planning control as a black as the as the individual function or",
    "start": "3366799",
    "end": "3372000"
  },
  {
    "text": "decomposition of the stack but at the same time we can couple them together with object representation the skill abs",
    "start": "3372000",
    "end": "3378799"
  },
  {
    "text": "representation as the state action abstractions so that we can enable full stack optimization",
    "start": "3378799",
    "end": "3385119"
  },
  {
    "start": "3385000",
    "end": "3599000"
  },
  {
    "text": "so that's the vision well going forward and i'd like to acknowledge my students who",
    "start": "3385119",
    "end": "3390160"
  },
  {
    "text": "are doing this amazing work for my talk and have to take more questions",
    "start": "3390160",
    "end": "3396560"
  },
  {
    "text": "[Applause] thank you yuki are there any questions",
    "start": "3396560",
    "end": "3402799"
  },
  {
    "text": "either in the room or in the zoom room as well if you're on zoom you can unmute",
    "start": "3402799",
    "end": "3408319"
  },
  {
    "text": "yourself and ask a question if you like",
    "start": "3408319",
    "end": "3412760"
  },
  {
    "text": "so something that was very exciting was in the second half of the talk the first part of that you showed with the buds",
    "start": "3420400",
    "end": "3428160"
  },
  {
    "text": "uh that you're getting like 70 percent success on that was really good and then in the next part we showed 100",
    "start": "3428160",
    "end": "3435280"
  },
  {
    "text": "success on the peg insertion cost so do you think that the i mean it's clear from the way you",
    "start": "3435280",
    "end": "3441359"
  },
  {
    "text": "presented that bringing these two things together should really improve cost success but is there anything missing or",
    "start": "3441359",
    "end": "3447040"
  },
  {
    "text": "is it just like maple plus buds and you're done it's maple and plus like about a buzzer",
    "start": "3447040",
    "end": "3452640"
  },
  {
    "text": "is done not really right so and i think there's definitely things that will",
    "start": "3452640",
    "end": "3458400"
  },
  {
    "text": "uh simplify a lot of internal object variations like you are seeing that we are evaluating maple on a lot of like",
    "start": "3458400",
    "end": "3464480"
  },
  {
    "text": "simulation environments the visual stimuli is still quite sim simplistic compared to the real world i",
    "start": "3464480",
    "end": "3471359"
  },
  {
    "text": "think like so when we move to actual real world settings so we need to bring better models to process like on the",
    "start": "3471359",
    "end": "3478319"
  },
  {
    "text": "vision part as well so definitely like just combining these two is not the end of the story and also how do we actually",
    "start": "3478319",
    "end": "3485200"
  },
  {
    "text": "build models that can continually interact with the environment discover new skills and need",
    "start": "3485200",
    "end": "3491280"
  },
  {
    "text": "like like a keep adding new primitives and new abilities to the library right it's",
    "start": "3491280",
    "end": "3497040"
  },
  {
    "text": "likely that it is not going to be a one-day job we can like this completes that we are done with that so we need",
    "start": "3497040",
    "end": "3503280"
  },
  {
    "text": "some continued learning never ending learning and to really kind of kick in the loop",
    "start": "3503280",
    "end": "3509119"
  },
  {
    "text": "more questions",
    "start": "3509760",
    "end": "3513000"
  },
  {
    "text": "okay i have one question oh sorry was there someone on zoom no no okay i have one question so in the",
    "start": "3518000",
    "end": "3524720"
  },
  {
    "text": "beginning you showed this engineering paradigm with like the telescope and hundreds of people",
    "start": "3524720",
    "end": "3530880"
  },
  {
    "text": "and then you showed this general robot with three people and you talked about combining those i'm",
    "start": "3530880",
    "end": "3536079"
  },
  {
    "text": "wondering what happens if you just add more people to this well in some sense like adding more",
    "start": "3536079",
    "end": "3542160"
  },
  {
    "text": "people has already tried right it's not that well lack of manpower in working on",
    "start": "3542160",
    "end": "3547520"
  },
  {
    "text": "those generalistic systems and there's also driving car companies and in the industry robotics companies has been",
    "start": "3547520",
    "end": "3554960"
  },
  {
    "text": "also have big teams of engineers and scientist working on that so well",
    "start": "3554960",
    "end": "3560240"
  },
  {
    "text": "people is one aspect of the the puzzle but i think so like a better methodologies and better",
    "start": "3560240",
    "end": "3567440"
  },
  {
    "text": "algorithms and being able to be open-minded and see that when",
    "start": "3567440",
    "end": "3573200"
  },
  {
    "text": "well as i like about learning researchers sometimes we hear engineering and the systems we kind of like that's something that would be",
    "start": "3573200",
    "end": "3578799"
  },
  {
    "text": "little right so that's something that we don't like that's a sign of incremental uh",
    "start": "3578799",
    "end": "3583920"
  },
  {
    "text": "improvement but i feel like being more uh open-minded and embrace those principles and really look at the",
    "start": "3583920",
    "end": "3590880"
  },
  {
    "text": "problem as from the system perspective is the way to go so yeah again it's not just a people problem but also um",
    "start": "3590880",
    "end": "3599520"
  },
  {
    "text": "new method and new insights and are needed to enable that next level take it to the next level",
    "start": "3599520",
    "end": "3606960"
  },
  {
    "text": "okay all right so thank you very much yuki for coming also in person to",
    "start": "3606960",
    "end": "3613920"
  },
  {
    "text": "thank you for being part of the invite yeah be part of this stanford robotics seminar after two years",
    "start": "3613920",
    "end": "3619520"
  },
  {
    "text": "so thank you everyone for coming and i see you next week on friday",
    "start": "3619520",
    "end": "3624810"
  },
  {
    "text": "[Applause]",
    "start": "3624810",
    "end": "3628900"
  },
  {
    "text": "you",
    "start": "3630720",
    "end": "3632799"
  }
]