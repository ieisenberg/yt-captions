[
  {
    "text": " OK.",
    "start": "0",
    "end": "5580"
  },
  {
    "text": "So today we're going to talk\nabout distributed computing with Spark.",
    "start": "5580",
    "end": "10630"
  },
  {
    "text": "And to put this in context, you\nspent a lot of time thinking about how to optimize the power\nperformance of single core",
    "start": "10630",
    "end": "20279"
  },
  {
    "text": "versus single chips\ncomposed of multiple cores with SIMD units using ISPC\nand thread-based programming.",
    "start": "20280",
    "end": "28509"
  },
  {
    "text": "You've thought about how to\nwrite data-parallel programs with hundreds of thousands\nof threads using CUDA.",
    "start": "28510",
    "end": "38730"
  },
  {
    "text": "We've also talked\nabout how you can use data-parallel\nprogramming ideas to generate",
    "start": "38730",
    "end": "47205"
  },
  {
    "text": "the large number of threads you\nneed to take advantage of a GPU. And today we want\nto think about how",
    "start": "47205",
    "end": "54150"
  },
  {
    "text": "you use data-parallel\nprogramming ideas to program a distributed computer.",
    "start": "54150",
    "end": "59870"
  },
  {
    "text": "So a computer composed of\nmultiple separate operating",
    "start": "59870",
    "end": "66280"
  },
  {
    "text": "system instances. And the main programming model\nwe're going to be talking about",
    "start": "66280",
    "end": "73270"
  },
  {
    "text": "is one called Spark. And I wish there was\na podium up here. I could raise this to be higher.",
    "start": "73270",
    "end": "78800"
  },
  {
    "text": "But anyway, I'll fix\nthat in future lectures. And so the way to\nthink about this",
    "start": "78800",
    "end": "85780"
  },
  {
    "text": "is how are we going to program\nhundreds of thousands of cores?",
    "start": "85780",
    "end": "91130"
  },
  {
    "text": "And so the question then\nis something might fail, and you want to make sure\nthat you don't lose data,",
    "start": "91130",
    "end": "96890"
  },
  {
    "text": "especially if you think about\nthe main use of distributed computing in this context\nis data processing.",
    "start": "96890",
    "end": "103159"
  },
  {
    "text": "And so you want to make\nsure you don't lose data. So we're going to\nrevisit this whole idea of data-parallel or functional\ndata-parallel primitives",
    "start": "103160",
    "end": "114835"
  },
  {
    "text": "as the way in which we program\nthese distributed computers.",
    "start": "114835",
    "end": "120430"
  },
  {
    "text": "So the question then is, given\nthe data-parallel programming model, we want to\nthink about, how",
    "start": "120430",
    "end": "125530"
  },
  {
    "text": "do we make it scale to\nhundreds of thousands of cores and do that efficiently?",
    "start": "125530",
    "end": "131370"
  },
  {
    "text": "And then how do we\nmake sure that we can handle faults or cases\nwhere parts of the system",
    "start": "131370",
    "end": "137890"
  },
  {
    "text": "fails and we can recover\nfrom that in a elegant way? And lastly, we want to make\nsure that we efficiently",
    "start": "137890",
    "end": "144340"
  },
  {
    "text": "use memory because\nof course that is going to be the key\ncomponent that determines the performance that we get.",
    "start": "144340",
    "end": "150940"
  },
  {
    "text": "So the main motivation\nthen is, why would you want to use a cluster\nof machines as opposed",
    "start": "150940",
    "end": "157930"
  },
  {
    "text": "to a single machine? And so the key\nthing, of course, is if you want to process huge\namounts of data, hundreds",
    "start": "157930",
    "end": "164170"
  },
  {
    "text": "of terabytes of data, for\ninstance, if you're looking at processing the log data from\na large website like Facebook,",
    "start": "164170",
    "end": "171950"
  },
  {
    "text": "so you could do it\nwith a single node, and your performance would\nbe limited by the I/O rate",
    "start": "171950",
    "end": "180820"
  },
  {
    "text": "that you could get the\nbandwidth to the disk, and if you did it\nwith a single node,",
    "start": "180820",
    "end": "188000"
  },
  {
    "text": "it would take you 23 days at the\nrate of 50 megabytes per second.",
    "start": "188000",
    "end": "193060"
  },
  {
    "text": "But if you had 1,000\nnodes, then of course, you have 1,000 fold speed up in\nthe bandwidth from your storage",
    "start": "193060",
    "end": "201909"
  },
  {
    "text": "system, and so that\ngoes down to 33 minutes. So this is the motivation. If you to process hundreds\nof terabytes of data,",
    "start": "201910",
    "end": "208970"
  },
  {
    "text": "then you need to do it\nacross multiple machines because you need the I/O\nbandwidth to solve the problem.",
    "start": "208970",
    "end": "216797"
  },
  {
    "text": "So this is something\nwe haven't really talked about up to this point. We've talked about compute,\nwe've talked about memory,",
    "start": "216797",
    "end": "223189"
  },
  {
    "text": "but we haven't really\ntalked about I/O. And so one of the really\nbig reasons to use a cluster",
    "start": "223190",
    "end": "228610"
  },
  {
    "text": "is to get I/O bandwidth. But the problem now\nis that you need to figure out how to program\nthese hundreds of thousands",
    "start": "228610",
    "end": "237115"
  },
  {
    "text": "of course, and so\nyou need to think about how to deal with the\nfact that things break.",
    "start": "237115",
    "end": "245840"
  },
  {
    "text": "So even if the mean time to\nfailure of a single server is 25 years, you put 1,000\nof them together,",
    "start": "245840",
    "end": "253040"
  },
  {
    "text": "and then something\nbreaks every hour. And so you want to make sure\nthat you can recover from that and you need it to\nbe, efficient, you",
    "start": "253040",
    "end": "260620"
  },
  {
    "text": "need it to be\nreliable, and you need it to be a usable framework, a\nframework that programmers can",
    "start": "260620",
    "end": "267220"
  },
  {
    "text": "after taking a course like this\ncan think about how to use. So the whole idea of\nclusters is actually",
    "start": "267220",
    "end": "275290"
  },
  {
    "text": "being elevated to what is\ncalled warehouse-sized computers",
    "start": "275290",
    "end": "280570"
  },
  {
    "text": "or warehouse-sized clusters. And so these are the\ncomputing infrastructure behind large websites like\nGoogle, Facebook, Amazon,",
    "start": "280570",
    "end": "291410"
  },
  {
    "text": "and, of course, there\nare huge data warehouses with racks and racks of\ncomputers networked together",
    "start": "291410",
    "end": "297100"
  },
  {
    "text": "in a way that gives you a\nsingle computing environment. And the pioneer of this idea\nwas a guy named Luiz Barroso.",
    "start": "297100",
    "end": "306220"
  },
  {
    "text": "And he came up with\nthis idea of thinking about the whole warehouse,\nwhich is composed",
    "start": "306220",
    "end": "315090"
  },
  {
    "text": "of hundreds of\nthousands of computers, as a single computer that\ncould be optimized together. So you think about\nthe networking,",
    "start": "315090",
    "end": "321540"
  },
  {
    "text": "you think about the\npower and the cooling, and the programming model. And I mentioned Luiz because\nhe's a great computer architect.",
    "start": "321540",
    "end": "329830"
  },
  {
    "text": "He recently passed away, and\nhe was a good friend of mine. So this is somebody-- if you\nwant to go look at his book,",
    "start": "329830",
    "end": "337150"
  },
  {
    "text": "it's called Datacenter\nas a Computer. It's a great book, a\ngreat primer on how",
    "start": "337150",
    "end": "342780"
  },
  {
    "text": "to design systems like this. So warehouse scale computers,\nwhat are they all about?",
    "start": "342780",
    "end": "349270"
  },
  {
    "text": "Well, they came out of\nthis idea of a cluster. So you take a commodity PC\nand you connect them together",
    "start": "349270",
    "end": "357270"
  },
  {
    "text": "with ethernet. And so the idea is now you've\ngot the scalable computer,",
    "start": "357270",
    "end": "363100"
  },
  {
    "text": "and it's fairly cheap because\nit's based on these commodity PC components, with China,\neverybody in the world,",
    "start": "363100",
    "end": "371110"
  },
  {
    "text": "especially when this idea\nwas kind of first invented in the early 2000, had a PC on\ntheir desk, and the idea is,",
    "start": "371110",
    "end": "378020"
  },
  {
    "text": "OK, well, if we network\nthem all together, we can get this large\nscalable computer.",
    "start": "378020",
    "end": "383210"
  },
  {
    "text": "And so the ethernet networks of\nthose times were not that fast. Today we're in the 10\nto 40 gigabits regime.",
    "start": "383210",
    "end": "394150"
  },
  {
    "text": "And the notion is\nthat now you could build this large-scale computer,\nand you could build it out",
    "start": "394150",
    "end": "399310"
  },
  {
    "text": "of these cheap components. And so the whole thing\nwas relatively cheap compared to the\nbig computers that",
    "start": "399310",
    "end": "407710"
  },
  {
    "text": "were used in\nscientific computing, high-performance computers. But it turns out that\nas people started",
    "start": "407710",
    "end": "414790"
  },
  {
    "text": "to think about programming\nthese computers and using these computers in\nplaces like Google and Yahoo",
    "start": "414790",
    "end": "422860"
  },
  {
    "text": "and Facebook, they found out\nthat really the big thing that differentiated the clusters\nfrom the supercomputers",
    "start": "422860",
    "end": "430360"
  },
  {
    "text": "was the network. And having a really powerful\nhigh-bandwidth network",
    "start": "430360",
    "end": "437080"
  },
  {
    "text": "could dramatically make the\ndevelopment of applications easier.",
    "start": "437080",
    "end": "442400"
  },
  {
    "text": "And so it turns out that\nthey actually kind of took a page from the\nsupercomputer systems",
    "start": "442400",
    "end": "448750"
  },
  {
    "text": "when they were building these\nwarehouse-scale computers and they decided to use a\ncustomized, expensive network,",
    "start": "448750",
    "end": "455420"
  },
  {
    "text": "and so now, of course, the\ncost of these warehouse, scale computers are\nkind of edging up",
    "start": "455420",
    "end": "463510"
  },
  {
    "text": "into the realm of high\nperformance computing supercomputers.",
    "start": "463510",
    "end": "468860"
  },
  {
    "text": "So the question then is,\ngiven this style of computer,",
    "start": "468860",
    "end": "474620"
  },
  {
    "text": "how do you organize computations\non this architecture and mask this issue of how do you balance\nthe load across 100,000 CPUs?",
    "start": "474620",
    "end": "484340"
  },
  {
    "text": "And then how do\nyou mask failures? So the first thing we need\nto understand in order to figure out how to do this\nis how these kinds of systems",
    "start": "484340",
    "end": "493300"
  },
  {
    "text": "are organized. So the way to think\nabout it, as I said, you've got hundreds of racks of\ncomputers, and each of the racks",
    "start": "493300",
    "end": "501610"
  },
  {
    "text": "is organized in\nthe following way. At the top of the rack-- sorry, the top of\nthe rack, you've got this top of\nrack switch, which",
    "start": "501610",
    "end": "507550"
  },
  {
    "text": "is the networking\ninterface to the rest of the racks in the system.",
    "start": "507550",
    "end": "513380"
  },
  {
    "text": "And then within the rack you've\ngot a stack of computer servers,",
    "start": "513380",
    "end": "519950"
  },
  {
    "text": "and you can have\n20 to 40 servers, and how many servers\nyou can fit in the rack",
    "start": "519950",
    "end": "526270"
  },
  {
    "text": "is really dependent\non the amount of power that you can actually\ndeliver to the rack,",
    "start": "526270",
    "end": "531530"
  },
  {
    "text": "and that will be dependent\non what infrastructure you've built in the data center. So that could range\nfrom 12 to 20 kilowatts,",
    "start": "531530",
    "end": "540620"
  },
  {
    "text": "and if these racks-- today, if you're\nin the ML regime and they have GPUs in\nthem, then of course,",
    "start": "540620",
    "end": "548870"
  },
  {
    "text": "the number of computers\nyou can fit in the rack is dramatically lower because\nGPUs take a lot more power.",
    "start": "548870",
    "end": "555860"
  },
  {
    "text": "So then the way to think\nabout it then is, OK, so within the rack, you've\ngot the nodes or servers",
    "start": "555860",
    "end": "564730"
  },
  {
    "text": "are connected by between\ntoday between 1 to 2 gigabytes",
    "start": "564730",
    "end": "572250"
  },
  {
    "text": "of bandwidth. And then between the racks,\nso outside the racks,",
    "start": "572250",
    "end": "579010"
  },
  {
    "text": "early on it was maybe a\n10th of a gigabyte, today, it might go up to 2 gigabytes.",
    "start": "579010",
    "end": "585020"
  },
  {
    "text": "So let's look at what an\nindividual node looks like. So you've seen this\npicture before in the sense",
    "start": "585020",
    "end": "593770"
  },
  {
    "text": "that you've got maybe\ntwo socket system, which means two CPU chips.",
    "start": "593770",
    "end": "599180"
  },
  {
    "text": "Each of those chips could\ncontain 16 to 32 CPU course.",
    "start": "599180",
    "end": "605730"
  },
  {
    "text": "And then they're connected\nto DDR chips, which provide the main memory, and you\nmight have 128 to 2 terabytes",
    "start": "605730",
    "end": "616290"
  },
  {
    "text": "of DRAM main memory. And the bandwidth\nchannel between that is roughly 100 to 200\ngigabytes per second.",
    "start": "616290",
    "end": "624640"
  },
  {
    "text": "And then from the\nI/O point of view, then you've got I/O which is to\nstorage, which is solid state",
    "start": "624640",
    "end": "631620"
  },
  {
    "text": "disk, which would give you 10\nto 30 terabytes of storage,",
    "start": "631620",
    "end": "637029"
  },
  {
    "text": "and then the other I/O\ninterface is to the network. And so you can imagine\nthen racking and stacking",
    "start": "637030",
    "end": "643620"
  },
  {
    "text": "these servers to create\nthese racks of computation. So any questions at this point?",
    "start": "643620",
    "end": "649205"
  },
  {
    "text": " Yeah. So I didn't understand\nwhat's the difference of node",
    "start": "649205",
    "end": "654899"
  },
  {
    "text": "and a server. Same. I use them interchangeably. You think about a node\nas a computer which",
    "start": "654900",
    "end": "665970"
  },
  {
    "text": "is running an operating system. So all of these nodes are\nrunning a separate operating",
    "start": "665970",
    "end": "672900"
  },
  {
    "text": "system. So it's important\nto understand that. So one thing that I\nshould point out then",
    "start": "672900",
    "end": "678210"
  },
  {
    "text": "is as you take a\nlook at this picture, so what are the new system\ncomponents that we've",
    "start": "678210",
    "end": "683820"
  },
  {
    "text": "mentioned that we haven't\nreally introduced before? ",
    "start": "683820",
    "end": "691020"
  },
  {
    "text": "Yeah. Network. Network, and-- Solid state. And solid-state storage.",
    "start": "691020",
    "end": "696839"
  },
  {
    "text": "So these two I/O components. So what's the other\nthing you notice about when you look\nat the bandwidths that",
    "start": "696840",
    "end": "703710"
  },
  {
    "text": "are shown in this diagram? What conclusions do you\nmake about the bandwidths",
    "start": "703710",
    "end": "711280"
  },
  {
    "text": "that we see in this picture? The main bottleneck is network. It's even slower, potentially\nthat writing to hard disk.",
    "start": "711280",
    "end": "719810"
  },
  {
    "text": "So what you see is\nthat the network, especially in the early days\nwhen you've got 0.1 gigabytes",
    "start": "719810",
    "end": "728620"
  },
  {
    "text": "of bandwidth between the\nracks, this is maybe a 10th",
    "start": "728620",
    "end": "733960"
  },
  {
    "text": "of the bandwidth you get from\ngoing to your local disk. So that's one thing to point\nout, that the network is",
    "start": "733960",
    "end": "742600"
  },
  {
    "text": "potentially-- has lower bandwidth than\naccessing your local disk.",
    "start": "742600",
    "end": "747750"
  },
  {
    "text": "What's the other point to make? I was just going to\nask, why is there a difference between\nthe bandwidth for the network and the nodes?",
    "start": "747750",
    "end": "754120"
  },
  {
    "text": "In other words, isn't\nthe communication between nodes\nwithin the network? Yes, but what you have is\nyou've got-- within the rack,",
    "start": "754120",
    "end": "763520"
  },
  {
    "text": "you have a higher\nbandwidth network than you have between the racks,\nat least to a first order.",
    "start": "763520",
    "end": "771250"
  },
  {
    "text": "But then we're going to make\nanother point in just a moment. So what other point before we\ntalk about the network, what",
    "start": "771250",
    "end": "778880"
  },
  {
    "text": "other things do you\nsee here when you're looking at the bandwidth? ",
    "start": "778880",
    "end": "786980"
  },
  {
    "text": "What's the relationship\nbetween the memory bandwidth and the network\nand disk bandwidth? ",
    "start": "786980",
    "end": "794310"
  },
  {
    "text": "Orders of magnitude. Yeah, your two\norders of magnitude. That's a big difference.",
    "start": "794310",
    "end": "800766"
  },
  {
    "text": " So that's the one point, but\nthe point about networks-- so I made this point about the\nfact that in the early days",
    "start": "800766",
    "end": "807865"
  },
  {
    "text": "it was kind of\ncommodity ethernet and then the warehouse\nscale computers.",
    "start": "807865",
    "end": "812970"
  },
  {
    "text": "Guys really got serious about\ntheir networking capability, and they started making the\nnetwork much higher bandwidth.",
    "start": "812970",
    "end": "821370"
  },
  {
    "text": "And so if you're in the\n2 gigabytes per second, then that's roughly\nthe bandwidth",
    "start": "821370",
    "end": "826610"
  },
  {
    "text": "that you're getting from\nthe solid-state disk. And so at this point--",
    "start": "826610",
    "end": "833330"
  },
  {
    "text": "so at this point, it may\nbe the same bandwidth to get data from\nanother nodes disks",
    "start": "833330",
    "end": "840935"
  },
  {
    "text": "as it is from your\nlocal nodes disk. So in the early days, that\nwas certainly not the case",
    "start": "840935",
    "end": "846890"
  },
  {
    "text": "when you're down at\n0.1, but when you get up to the 2 gigabytes\nper second, now",
    "start": "846890",
    "end": "853980"
  },
  {
    "text": "the picture is changing because\nnow you can get potentially",
    "start": "853980",
    "end": "859529"
  },
  {
    "text": "the same bandwidth to\na remote nodes disk as to your local disk.",
    "start": "859530",
    "end": "866140"
  },
  {
    "text": "So good point. Something to keep in mind. ",
    "start": "866140",
    "end": "873966"
  },
  {
    "text": "So I said that\nall of these nodes are running different\noperating systems.",
    "start": "873966",
    "end": "880730"
  },
  {
    "text": "So what does that mean about\nhow they can communicate? Can they share memory.",
    "start": "880730",
    "end": "886709"
  },
  {
    "text": "They can't share memory. They're not touching the\nsame address spaces at all. And so what we need to do\nis we need another mechanism",
    "start": "886710",
    "end": "895440"
  },
  {
    "text": "for communication,\nand that mechanism is called message passing, which\nis kind of sending a letter",
    "start": "895440",
    "end": "901020"
  },
  {
    "text": "or sending a note\nto your friend. And so the abstraction is I'm\na thread in some address space,",
    "start": "901020",
    "end": "910949"
  },
  {
    "text": "and I've got some variable X\nthat's in my address space, and I want it to send it\nto another thread that",
    "start": "910950",
    "end": "916710"
  },
  {
    "text": "doesn't share my address space. And so I issue a send\ncall, and that I give it",
    "start": "916710",
    "end": "925470"
  },
  {
    "text": "the address of my variable\nX in my address space.",
    "start": "925470",
    "end": "930629"
  },
  {
    "text": "I say which thread I\nwant to send it to, and I might give it\nsome message ID tag,",
    "start": "930630",
    "end": "937350"
  },
  {
    "text": "and that will be the send and\nthat we'll go over the network and land at the thread.",
    "start": "937350",
    "end": "947220"
  },
  {
    "text": "Of course, you can do message\npassing on the same node, but if you've got\nseparate address spaces,",
    "start": "947220",
    "end": "954490"
  },
  {
    "text": "but let's assume that it's\ngoing over the network. And then on the thread running\non a different node thread 2",
    "start": "954490",
    "end": "961200"
  },
  {
    "text": "does a receive-- and then receives the\nmessage and puts it",
    "start": "961200",
    "end": "967500"
  },
  {
    "text": "in its variable in its\nlocal address space Y.",
    "start": "967500",
    "end": "972780"
  },
  {
    "text": "So it receives the message. And so this communication then\nhappens with sends and receives,",
    "start": "972780",
    "end": "981100"
  },
  {
    "text": "and the question is, if\nyou've got message passing,",
    "start": "981100",
    "end": "987730"
  },
  {
    "text": "do you need synchronization? ",
    "start": "987730",
    "end": "992900"
  },
  {
    "text": "See somebody shaking\ntheir head yes, somebody shaking their head no. So what's the answer?",
    "start": "992900",
    "end": "999240"
  },
  {
    "text": "So someone who says\nyes, why do you think you need sync\nsynchronization? ",
    "start": "999240",
    "end": "1006090"
  },
  {
    "text": "Because if they will\nreceive [INAUDIBLE] before",
    "start": "1006090",
    "end": "1014700"
  },
  {
    "text": "you can carry on. Because if you don't do that,\nthen things can go haywire.",
    "start": "1014700",
    "end": "1020160"
  },
  {
    "text": "You could potentially\ndeadlock if you're waiting for a message that\nis never going to arrive,",
    "start": "1020160",
    "end": "1028030"
  },
  {
    "text": "but do you need explicit\nsynchronization? And you would say no. Not really.",
    "start": "1028030",
    "end": "1033709"
  },
  {
    "text": "No because the message-- the act of sending a message\nis the synchronization.",
    "start": "1033710",
    "end": "1039270"
  },
  {
    "text": "So you don't need-- you can get deadlocked. So that was the issue\nyou were thinking about. That's true.",
    "start": "1039270",
    "end": "1044709"
  },
  {
    "text": "You could deadlock,\nbut you don't need explicit synchronization.",
    "start": "1044710",
    "end": "1050120"
  },
  {
    "text": "So we've got send\nand receive, so we have a mechanism, an\nabstraction for communicating",
    "start": "1050120",
    "end": "1056410"
  },
  {
    "text": "between these separate address\nspaces that could potentially live on the same machine,\nbut in this large distributed",
    "start": "1056410",
    "end": "1063730"
  },
  {
    "text": "system are typically going\nto be on different nodes.",
    "start": "1063730",
    "end": "1068860"
  },
  {
    "text": "So first order of business. We're going to use this data-- this distributed computer system\nfor doing data processing.",
    "start": "1068860",
    "end": "1077690"
  },
  {
    "text": "And so the data has\ngot to live somewhere. And it's going to live\nsomewhere in the storage system. But we said, you've\ngot potentially",
    "start": "1077690",
    "end": "1085299"
  },
  {
    "text": "hundreds of thousands\nof components, and the components could break,\nand you could lose a node",
    "start": "1085300",
    "end": "1092950"
  },
  {
    "text": "or you could lose a whole\nrack because the networking at the top of rack breaks.",
    "start": "1092950",
    "end": "1098090"
  },
  {
    "text": "And now the question\nis, how do we ensure that we don't lose any data?",
    "start": "1098090",
    "end": "1103160"
  },
  {
    "text": "We're using this computer\nfor data processing, and if we lose data,\nthen it's kind of",
    "start": "1103160",
    "end": "1109490"
  },
  {
    "text": "failed in its first goal. So how can we store\nthe data persistently?",
    "start": "1109490",
    "end": "1116309"
  },
  {
    "text": "So the answer is, well, let's\nbuild a distributed file system. So any of you who've\nbeen introduced",
    "start": "1116310",
    "end": "1125450"
  },
  {
    "text": "to the idea of a\ndistributed file system? Yes, some of you. So we'll just talk\nbriefly about it here.",
    "start": "1125450",
    "end": "1133160"
  },
  {
    "text": "So the idea of a\ndistributed file system is that you've got some\nglobal file namespace,",
    "start": "1133160",
    "end": "1139960"
  },
  {
    "text": "and the idea was pioneered by\nGoogle with the Google File",
    "start": "1139960",
    "end": "1145630"
  },
  {
    "text": "System, GFS. The open source version was\nHadoop Distributed File System,",
    "start": "1145630",
    "end": "1151190"
  },
  {
    "text": "HDFS. And so it was predicated on\na particular usage model,",
    "start": "1151190",
    "end": "1156850"
  },
  {
    "text": "and the model is that you've\ngot large files, potentially",
    "start": "1156850",
    "end": "1162610"
  },
  {
    "text": "tens or hundreds of terabytes,\nand that the data that you write to--",
    "start": "1162610",
    "end": "1168710"
  },
  {
    "text": "the kind of access pattern\nthat you use with these files, is that you\nbasically just append",
    "start": "1168710",
    "end": "1174940"
  },
  {
    "text": "data to the files in terms\nof rights and then you read. So it's mostly read and\nappend, and very rarely",
    "start": "1174940",
    "end": "1182350"
  },
  {
    "text": "do you update the data in\nplace because a big use of this is for storage of log data.",
    "start": "1182350",
    "end": "1188660"
  },
  {
    "text": "So log data, you just-- essentially as the new entry\nfor the log gets generated,",
    "start": "1188660",
    "end": "1195330"
  },
  {
    "text": "it gets appended\nto the log file. So reads and appends\nthe dominant access mode",
    "start": "1195330",
    "end": "1204770"
  },
  {
    "text": "for this distributed\nfile system. So the idea then of a\ndistributed file system is to divide your huge file\nup into chunks or blocks,",
    "start": "1204770",
    "end": "1217410"
  },
  {
    "text": "and these are usually 64\nto 256 megabytes in size.",
    "start": "1217410",
    "end": "1226250"
  },
  {
    "text": "And then, in order to make\nsure that you don't lose data, you replicate the chunks.",
    "start": "1226250",
    "end": "1233190"
  },
  {
    "text": "And where would you-- what kind of replication\nscheme would you want to come up with here?",
    "start": "1233190",
    "end": "1240750"
  },
  {
    "text": "Would you put all of them-- all of the replicates\nin the same rack?",
    "start": "1240750",
    "end": "1246850"
  },
  {
    "text": "Probably not. So you want to distribute\nacross multiple racks so that if you lost\na top of rack switch,",
    "start": "1246850",
    "end": "1255020"
  },
  {
    "text": "you wouldn't lose the data. So then the idea then is\nyou've got a master node.",
    "start": "1255020",
    "end": "1260380"
  },
  {
    "text": "The master node\nis going to tell-- basically it's got the metadata,\nthe directory for all of where",
    "start": "1260380",
    "end": "1269760"
  },
  {
    "text": "all the replicas are stored. And so the idea then is\nyou've got a global namespace",
    "start": "1269760",
    "end": "1276990"
  },
  {
    "text": "across the whole\ndistributed file system. And the way that\nthe clients access",
    "start": "1276990",
    "end": "1282840"
  },
  {
    "text": "files is they talk\nto the master, they find out where the\nreplicas are, and then they go",
    "start": "1282840",
    "end": "1288000"
  },
  {
    "text": "read directly from the replicas. So this is shown in\nthis figure here. So you've got a name node or the\nmaster node, that has metadata,",
    "start": "1288000",
    "end": "1299950"
  },
  {
    "text": "and you've got a client\nthat wants to read. So first of all, it\ngoes to the name node, finds out where the replicas\nare, that's step one.",
    "start": "1299950",
    "end": "1307790"
  },
  {
    "text": "and then in step two, it goes to\nthe particular data nodes which",
    "start": "1307790",
    "end": "1313120"
  },
  {
    "text": "actually have the data,\nand it picks up the block",
    "start": "1313120",
    "end": "1318640"
  },
  {
    "text": "from one of the replicas. And then if that's client\nnumber one, client number",
    "start": "1318640",
    "end": "1325090"
  },
  {
    "text": "two wants to do a write,\nand so, of course, it has to update\nall the replicas because it got that information\nabout where the replicas exist",
    "start": "1325090",
    "end": "1332380"
  },
  {
    "text": "from the name node. Question. You suddenly [INAUDIBLE] were\nfacing a single computer file",
    "start": "1332380",
    "end": "1339760"
  },
  {
    "text": "system. You kind of offer the rights\nand make sure that hey, you have the same guy,\n[INAUDIBLE] same thing",
    "start": "1339760",
    "end": "1345309"
  },
  {
    "text": "as [INAUDIBLE]. I have some [INAUDIBLE] of that. How do you make\nsure [INAUDIBLE]? How do you make sure that\nthe multiple clients aren't--",
    "start": "1345310",
    "end": "1352070"
  },
  {
    "text": "You need to do synchronization\nacross all of the bags when you find some\nthings like that? Well, typically, you make sure\nthat of only one of the clients",
    "start": "1352070",
    "end": "1364779"
  },
  {
    "text": "is writing to-- you handle that at\nthe application level. ",
    "start": "1364780",
    "end": "1371326"
  },
  {
    "text": "You have the local master\nnode for [? drag ?] and each [INAUDIBLE].",
    "start": "1371326",
    "end": "1377682"
  },
  {
    "text": "No, there's a single master\nnode that may be replicated. Turns out that if you've\nonly got a single node,",
    "start": "1377682",
    "end": "1385460"
  },
  {
    "text": "the chances of failure\nare not that high. The issue is, of course, if\nyou've got thousands of nodes.",
    "start": "1385460",
    "end": "1391900"
  },
  {
    "text": "But, of course, you may\nwant some duplication. Maybe to handle the load.",
    "start": "1391900",
    "end": "1398700"
  },
  {
    "text": "And you can imagine that in a\nsystem with high enough usage",
    "start": "1398700",
    "end": "1408139"
  },
  {
    "text": "that you might need to duplicate\nto make sure that you can handle the load, and you might\nwant some duplication",
    "start": "1408140",
    "end": "1413840"
  },
  {
    "text": "to make sure that if a node\nfails, you still have a backup. So since you have a ticket\nin two to three times,",
    "start": "1413840",
    "end": "1419309"
  },
  {
    "text": "so does that really mean like\nif my entire cluster [INAUDIBLE]",
    "start": "1419310",
    "end": "1424670"
  },
  {
    "text": "50% of [INAUDIBLE]? Yeah. Storage. ",
    "start": "1424670",
    "end": "1432030"
  },
  {
    "text": "This looks very similar to the\nFile Address Table, FAT storage where you have a address\ntable for [INAUDIBLE] files,",
    "start": "1432030",
    "end": "1439920"
  },
  {
    "text": "and then you go look at it. Is it very similar, or are there\n[INAUDIBLE] FAT, File Address",
    "start": "1439920",
    "end": "1447419"
  },
  {
    "text": "Table storage? Yeah, it's similar. Yeah, but it's distributed. ",
    "start": "1447420",
    "end": "1456655"
  },
  {
    "text": "So now we have a way of\npersistently storing data such",
    "start": "1456655",
    "end": "1461780"
  },
  {
    "text": "that we never lose data\nif one of our components in our distributed system fails.",
    "start": "1461780",
    "end": "1468122"
  },
  {
    "text": "So now here's the problem\nwe're trying to solve. Suppose CS 149 gets even more\npopular than it is today.",
    "start": "1468122",
    "end": "1476370"
  },
  {
    "text": "Everybody who majors\nin CS and everybody outside wants to understand and\nlearn about parallel computing.",
    "start": "1476370",
    "end": "1484169"
  },
  {
    "text": "And so the website is\ngetting tons of page views.",
    "start": "1484170",
    "end": "1490320"
  },
  {
    "text": "So here's a log of page views\nfrom the course website,",
    "start": "1490320",
    "end": "1495380"
  },
  {
    "text": "and your goal is to\nunderstand something",
    "start": "1495380",
    "end": "1501510"
  },
  {
    "text": "about these page views. So you imagine that we're\ngoing to store these page views",
    "start": "1501510",
    "end": "1506929"
  },
  {
    "text": "on a cluster of four nodes. So our tiny cluster here,\nbut will give you an idea.",
    "start": "1506930",
    "end": "1514220"
  },
  {
    "text": "And so each of the nodes\nhas a 10-terabyte SSD, and you've got some number of\nblocks associated with each",
    "start": "1514220",
    "end": "1525389"
  },
  {
    "text": "of the nodes in the system. So you've divided the large log\nfile into 256 megabyte chunks",
    "start": "1525390",
    "end": "1540780"
  },
  {
    "text": "that we're going to call\nblocks, and then we're going to distribute\nthe blocks across all the nodes in our cluster.",
    "start": "1540780",
    "end": "1547210"
  },
  {
    "text": "In this case, four nodes. So we've divided the\nfile into eight blocks,",
    "start": "1547210",
    "end": "1554100"
  },
  {
    "text": "and distributed\nacross the four nodes. ",
    "start": "1554100",
    "end": "1561870"
  },
  {
    "text": "So what we want to know is\nwho's accessing the site",
    "start": "1561870",
    "end": "1567570"
  },
  {
    "text": "rather than who's accessing--\nwhat type of device are they using to\naccess the site. So what type of mobile\nphone are the students",
    "start": "1567570",
    "end": "1574830"
  },
  {
    "text": "using because, of course,\nwho accesses websites using anything else these days?",
    "start": "1574830",
    "end": "1582070"
  },
  {
    "text": "So you could potentially\nwrite a program to analyze the log file\nusing message passing.",
    "start": "1582070",
    "end": "1590890"
  },
  {
    "text": "So we're not going to talk\nabout it in this class, but you can program these\ndistributed computers using",
    "start": "1590890",
    "end": "1598590"
  },
  {
    "text": "message passing,\nusing send and receive and what's called\ncollective operations.",
    "start": "1598590",
    "end": "1603730"
  },
  {
    "text": "There's API called the Message\nPassing Interface, MPI.",
    "start": "1603730",
    "end": "1609250"
  },
  {
    "text": "I encourage you to go\nlook at it if you're interested, but believe\nme, it would be painful.",
    "start": "1609250",
    "end": "1614530"
  },
  {
    "text": "And furthermore, it\nwouldn't necessarily handle fault tolerance.",
    "start": "1614530",
    "end": "1619810"
  },
  {
    "text": "So there's one thing-- we've got this persistent\ndistributed file system,",
    "start": "1619810",
    "end": "1625270"
  },
  {
    "text": "which makes sure that you don't\nlose any data once you put data in the file system. But what happens if in the\nmiddle of your computation,",
    "start": "1625270",
    "end": "1633030"
  },
  {
    "text": "you lose one of the\nnodes, and the data that you have is in memory.",
    "start": "1633030",
    "end": "1639510"
  },
  {
    "text": "Then you've lost that. So MPI doesn't help you there. So you need some other way of\ndealing with fault tolerance",
    "start": "1639510",
    "end": "1647279"
  },
  {
    "text": "beyond just having a file\nsystem that doesn't lose data.",
    "start": "1647280",
    "end": "1653340"
  },
  {
    "text": "So MPI is an option, but\nit's not a very good one and it's not easily\nprogrammable.",
    "start": "1653340",
    "end": "1659640"
  },
  {
    "text": "So let's think about\nhow you might want to-- how you might want to\nthink about programming",
    "start": "1659640",
    "end": "1667700"
  },
  {
    "text": "the application. So let's go back to the idea\nof data-parallel operations, in particular, map and reduce.",
    "start": "1667700",
    "end": "1676920"
  },
  {
    "text": "So just to refresh\nyour memory right, so the map is going to take\na collection or a sequence",
    "start": "1676920",
    "end": "1683840"
  },
  {
    "text": "in this case of type A\nand create or produce",
    "start": "1683840",
    "end": "1690034"
  },
  {
    "text": "a sequence or\ncollection of type B, and the way that\nit's going to do that is going to take each\nelement of the input sequence",
    "start": "1690035",
    "end": "1699350"
  },
  {
    "text": "and convert it to an element\nof the output sequence. So what do we say?",
    "start": "1699350",
    "end": "1704549"
  },
  {
    "text": "There are two things we\nsaid about map that we could do that are important.",
    "start": "1704550",
    "end": "1709650"
  },
  {
    "text": " What's important about math. ",
    "start": "1709650",
    "end": "1718320"
  },
  {
    "text": "It's easily parallelizable. It's easily\nparallelizable, right. So because, of course, we\nknow the no dependencies",
    "start": "1718320",
    "end": "1724260"
  },
  {
    "text": "between the different elements,\nso we can do the elements--",
    "start": "1724260",
    "end": "1731810"
  },
  {
    "text": "we can create the output\nsequence in any order. What's the other\nimportant thing about map?",
    "start": "1731810",
    "end": "1738460"
  },
  {
    "text": " Something feels [INAUDIBLE].",
    "start": "1738460",
    "end": "1744450"
  },
  {
    "text": "Why is that? Well, the side effect\nfor you [INAUDIBLE]. Ah! Side effect for you.",
    "start": "1744450",
    "end": "1750263"
  },
  {
    "text": "So what does that mean? Which means you can try it\nagain and again and again on the same [INAUDIBLE]. And why is that?",
    "start": "1750263",
    "end": "1756290"
  },
  {
    "text": " Because it's bad, we can\naccess that piece of--",
    "start": "1756290",
    "end": "1762529"
  },
  {
    "text": "that recorded memory\nand [INAUDIBLE]. Because map does not\nmutate its input.",
    "start": "1762530",
    "end": "1768790"
  },
  {
    "text": "It doesn't change its input. So the input never changes. So you can run it as much as\noften as you like and generate",
    "start": "1768790",
    "end": "1776460"
  },
  {
    "text": "the output. Good. So that's an important\npoint to remember that these data-parallel\nfunctional operations.",
    "start": "1776460",
    "end": "1786550"
  },
  {
    "text": "The key thing about\nfunctional programming models",
    "start": "1786550",
    "end": "1793050"
  },
  {
    "text": "is they don't\nmutate their input. And because they don't\nmutate their input,",
    "start": "1793050",
    "end": "1798510"
  },
  {
    "text": "it has all these nice properties\nof side effects, freeness and which will have--",
    "start": "1798510",
    "end": "1805210"
  },
  {
    "text": "I think you're\nalluding to the fact that it has some fault tolerant\nbenefits that we'll definitely",
    "start": "1805210",
    "end": "1812710"
  },
  {
    "text": "talk about. All right. So the other\noperation is reduce.",
    "start": "1812710",
    "end": "1818960"
  },
  {
    "text": "So reduce will take\na sequence of type A",
    "start": "1818960",
    "end": "1825100"
  },
  {
    "text": "and produce a single\nelement of type B",
    "start": "1825100",
    "end": "1830350"
  },
  {
    "text": "by using some of\nreduction function. All right.",
    "start": "1830350",
    "end": "1835610"
  },
  {
    "text": "So now let's think about the\nMapReduce programming model and a way of counting\nthe page views",
    "start": "1835610",
    "end": "1843970"
  },
  {
    "text": "of a different-- of each type. So the key elements of the\nMapReduce programming model,",
    "start": "1843970",
    "end": "1849770"
  },
  {
    "text": "of course, are a mapper\nfunction, which is shown here.",
    "start": "1849770",
    "end": "1856280"
  },
  {
    "text": "Oops. ",
    "start": "1856280",
    "end": "1866150"
  },
  {
    "text": "And a reducer function,\nwhich is shown here. So the mapper function is called\nonce per line of the log file,",
    "start": "1866150",
    "end": "1877190"
  },
  {
    "text": "and it parses the\nline and figures out whether the line is from--",
    "start": "1877190",
    "end": "1886309"
  },
  {
    "text": "the entry is from\na mobile client. So it figures out whether\nit's from a mobile client,",
    "start": "1886310",
    "end": "1894350"
  },
  {
    "text": "and if it is, it records\na entry or updates",
    "start": "1894350",
    "end": "1901610"
  },
  {
    "text": "an entry in the result map.",
    "start": "1901610",
    "end": "1906929"
  },
  {
    "text": "So the input to mapper is a\nsingle line from the log file,",
    "start": "1906930",
    "end": "1913700"
  },
  {
    "text": "and the output is\na map of results.",
    "start": "1913700",
    "end": "1920269"
  },
  {
    "text": "And so it's going to add an\nentry to the map data structure.",
    "start": "1920270",
    "end": "1931550"
  },
  {
    "text": "And then the reducer\nfunction is going to be called for unique value--",
    "start": "1931550",
    "end": "1941590"
  },
  {
    "text": "unique keys. All the values associated\nwith unique key, and it's going to be called\nonce for each unique key.",
    "start": "1941590",
    "end": "1949409"
  },
  {
    "text": "And what it's\ngoing to do is it's going to take the\nstring of values and just add them up to create\na sum which is the result.",
    "start": "1949410",
    "end": "1963820"
  },
  {
    "text": "So the idea then is we\ngenerate these key value pairs",
    "start": "1963820",
    "end": "1969220"
  },
  {
    "text": "in the mapper\nfunction, and then we reduce these key value pairs\nin the reducer function.",
    "start": "1969220",
    "end": "1978370"
  },
  {
    "text": "And so the code at the bottom\nshows how things get called.",
    "start": "1978370",
    "end": "1985750"
  },
  {
    "text": "First of all, we get the input\nfrom our distributed file",
    "start": "1985750",
    "end": "1991240"
  },
  {
    "text": "system, and then write\nthe output again back",
    "start": "1991240",
    "end": "1998290"
  },
  {
    "text": "to the distributed file system. And that happens when\nwe run MapReduce job",
    "start": "1998290",
    "end": "2005039"
  },
  {
    "text": "with the mapper function, the\nreducer function, and the input and output.",
    "start": "2005040",
    "end": "2010880"
  },
  {
    "text": "[INAUDIBLE] previews here\nmutating its inputs because it's adding to the map and\nsetting the result,",
    "start": "2010880",
    "end": "2017200"
  },
  {
    "text": "which is [INAUDIBLE]. ",
    "start": "2017200",
    "end": "2022470"
  },
  {
    "text": "So the way to think about this\nis that the input to the map",
    "start": "2022470",
    "end": "2027600"
  },
  {
    "text": "is really the data that you\nget from the distributed file system.",
    "start": "2027600",
    "end": "2033060"
  },
  {
    "text": "So you can think about the-- in this case, you can think\nabout the results as internal",
    "start": "2033060",
    "end": "2040860"
  },
  {
    "text": "to MapReduce. And so the interface is really--",
    "start": "2040860",
    "end": "2048120"
  },
  {
    "text": "the API really has\nto do with the data that you give to the\nrun MapReduce job.",
    "start": "2048120",
    "end": "2056190"
  },
  {
    "text": "And so the input\ndoesn't get mutated. ",
    "start": "2056190",
    "end": "2065899"
  },
  {
    "text": "So let's think about actually\nimplementing run MapReduce job.",
    "start": "2065900",
    "end": "2071658"
  },
  {
    "text": "And so the way to\nthink about things is-- so I'm going to show you\nthe kind of classic MapReduce",
    "start": "2071659",
    "end": "2079600"
  },
  {
    "text": "101, which is word count. So the idea is that\nyou have one map task",
    "start": "2079600",
    "end": "2087219"
  },
  {
    "text": "per block of the input file. And so in this case,\nwe've got three blocks,",
    "start": "2087219",
    "end": "2092770"
  },
  {
    "text": "and so we've got\nthree map tasks. And the map task read\nlines from the input blocks",
    "start": "2092770",
    "end": "2105900"
  },
  {
    "text": "and they create this\nset key value pairs.",
    "start": "2105900",
    "end": "2112049"
  },
  {
    "text": "And so here we want to count\nthe number of word occurrences in the input file, and it\ncreates an entry, a key value",
    "start": "2112050",
    "end": "2120810"
  },
  {
    "text": "pair, which has a one\nassociated with all of the words that it found in\nthe input files.",
    "start": "2120810",
    "end": "2129619"
  },
  {
    "text": "And so you've got three\nsets of three maps--",
    "start": "2129620",
    "end": "2137000"
  },
  {
    "text": "three instances\nof key value pairs associated with each\nof the mapper tasks.",
    "start": "2137000",
    "end": "2145050"
  },
  {
    "text": "So now in this case, we've\ngot two reduced tasks.",
    "start": "2145050",
    "end": "2152700"
  },
  {
    "text": "And so the question--\nso the mapper tasks, the parallelism is obvious. It's associated with\neach of the input blocks.",
    "start": "2152700",
    "end": "2160090"
  },
  {
    "text": "How about the reducer tasks? Where do we get the parallelism\nfrom for the reducer tasks?",
    "start": "2160090",
    "end": "2165460"
  },
  {
    "start": "2165460",
    "end": "2170760"
  },
  {
    "text": "[INAUDIBLE] Right. So you're going to do-- you're going to associate\nsome number of keys",
    "start": "2170760",
    "end": "2178820"
  },
  {
    "text": "to each of the reducer tasks. And so that's where\nthe parallelism is going to come from.",
    "start": "2178820",
    "end": "2184260"
  },
  {
    "text": "But what happens\nwhen you do that? How do we know the Reduce\ntask is associated?",
    "start": "2184260",
    "end": "2191210"
  },
  {
    "text": "If we start giving some\nrandom Reduce function, how do we know that we\ncan actually split it up and get the same thing?",
    "start": "2191210",
    "end": "2196970"
  },
  {
    "text": "So, I mean, it won't work unless\nyou have an associative reducer. Yeah, that's right.",
    "start": "2196970",
    "end": "2203800"
  },
  {
    "text": "So that's kind of part of\nthe limitation of MapReduce.",
    "start": "2203800",
    "end": "2209260"
  },
  {
    "text": "If you want to\nparalyze it at least. So but what happens\nonce you do this?",
    "start": "2209260",
    "end": "2214455"
  },
  {
    "text": " You said we want to\nget the parallelism",
    "start": "2214455",
    "end": "2221450"
  },
  {
    "text": "by having multiple keys, being\noperated at the same time by different reduced tasks.",
    "start": "2221450",
    "end": "2227579"
  },
  {
    "text": "So what do we have to do in\norder to make that happen? Yeah somebody said something.",
    "start": "2227580",
    "end": "2233359"
  },
  {
    "text": "Synchronization. Synchronization of what sort? Somebody back there.",
    "start": "2233360",
    "end": "2239869"
  },
  {
    "text": "What's the answer to two? If we've got to\nparallelize across keys,",
    "start": "2239870",
    "end": "2246300"
  },
  {
    "text": "what do we have to\nmake sure happens in order for the\nreduction to be correct? All keys will be the same.",
    "start": "2246300",
    "end": "2252020"
  },
  {
    "text": "All the keys of the same\ntype or the same value have to go to the\nsame reducer task.",
    "start": "2252020",
    "end": "2260570"
  },
  {
    "text": "And so the way to\nthink about this is MapReduce is really\nmap group by key reduce.",
    "start": "2260570",
    "end": "2270625"
  },
  {
    "text": "Or there's this\nbig soul or shuffle in the middle to make sure all\nthe keys with the same value",
    "start": "2270625",
    "end": "2278059"
  },
  {
    "text": "go to the same reduce task. And so that's the way to\nthink about MapReduce.",
    "start": "2278060",
    "end": "2286130"
  },
  {
    "text": "It's really map\ngroup by key reduce",
    "start": "2286130",
    "end": "2291920"
  },
  {
    "text": "or some people say\nsword or shuffle right in the middle in which you've\ngot a lot of communication that",
    "start": "2291920",
    "end": "2298790"
  },
  {
    "text": "is potentially going to\nhappen between the mapper tasks and the reducer tasks.",
    "start": "2298790",
    "end": "2303960"
  },
  {
    "text": "And then we're going to talk\nmore specifically about how you make sure the keys of\nthe same value go to the same",
    "start": "2303960",
    "end": "2314330"
  },
  {
    "text": "reduce task. So let's run the\nmapper function. So here's our\nMapReduce code, and we",
    "start": "2314330",
    "end": "2322940"
  },
  {
    "text": "want to run the MapReduce job. And so the first\nquestion is we've",
    "start": "2322940",
    "end": "2330170"
  },
  {
    "text": "got these mapper tasks\nassociated with each block, how should we run them?",
    "start": "2330170",
    "end": "2337820"
  },
  {
    "text": "Anybody have ideas about\nhow we should run them? Yeah. I think on the same\nnode as the block.",
    "start": "2337820",
    "end": "2344840"
  },
  {
    "text": "That's one idea. We could run the same-- how do we make sure\nthings get load balanced? ",
    "start": "2344840",
    "end": "2352500"
  },
  {
    "text": "What if we're really\nconcerned about load balance? What would we do? ",
    "start": "2352500",
    "end": "2364050"
  },
  {
    "text": "Yeah. You could do something\nlike the thread pool. Like, make one node a master\nnod and have all the tasks",
    "start": "2364050",
    "end": "2372703"
  },
  {
    "text": "queued up there, and\nthen the other ones keep sending messages to\ngive them the next task. Right.",
    "start": "2372703",
    "end": "2377730"
  },
  {
    "text": "Use a work queue or some\nof distributed work queue to make sure that\nwe had extreme--",
    "start": "2377730",
    "end": "2383160"
  },
  {
    "text": "make sure that we had\nextreme load balance. And this might work,\nbut, well, in the case",
    "start": "2383160",
    "end": "2390390"
  },
  {
    "text": "where we had a really\npowerful network. That second case I talked about\nhaving bandwidth in the network",
    "start": "2390390",
    "end": "2398609"
  },
  {
    "text": "equal to what you might get\nto your local hard disk. But what if in the early days of\nMapReduce when the network was",
    "start": "2398610",
    "end": "2407820"
  },
  {
    "text": "much had much less\nbandwidth than the bandwidth you get to your local disk?",
    "start": "2407820",
    "end": "2413500"
  },
  {
    "text": "Then we had another idea\nwhich was proposed first, which was what? Can you do the same.",
    "start": "2413500",
    "end": "2419309"
  },
  {
    "text": "Yeah, run the task associated\nwith the block on the CPU",
    "start": "2419310",
    "end": "2426120"
  },
  {
    "text": "that actually contains--\non the node that actually contains the block. So this idea is sort\nof data distribution",
    "start": "2426120",
    "end": "2434759"
  },
  {
    "text": "or task distribution. Each processing node\nprocesses lines or blocks",
    "start": "2434760",
    "end": "2441110"
  },
  {
    "text": "in the input file that\nare stored locally. ",
    "start": "2441110",
    "end": "2447940"
  },
  {
    "text": "All right. So now we-- so now we've\ngot all the mapper tasks,",
    "start": "2447940",
    "end": "2455740"
  },
  {
    "text": "and we've created all these\nkey value pairs associated with the different\nmobile clients,",
    "start": "2455740",
    "end": "2461660"
  },
  {
    "text": "and now we have to\ndo the reductions. So we've got all\nthese keys which have been generated by\nthe mapper function,",
    "start": "2461660",
    "end": "2470109"
  },
  {
    "text": "and so now we have to make sure\nthat we can do the reduction.",
    "start": "2470110",
    "end": "2475910"
  },
  {
    "text": "So there's two questions. How do we assign\nthe reducer tasks?",
    "start": "2475910",
    "end": "2482000"
  },
  {
    "text": "And then how do we make sure we\nget all the data for a key value",
    "start": "2482000",
    "end": "2488070"
  },
  {
    "text": "to the correct worker node? So who has ideas about what we\nshould do to assign the reducer",
    "start": "2488070",
    "end": "2498940"
  },
  {
    "text": "tasks to nodes to run? ",
    "start": "2498940",
    "end": "2504600"
  },
  {
    "text": "Yeah. Use the hash function, and\nthen group by the hash value.",
    "start": "2504600",
    "end": "2509850"
  },
  {
    "text": "I think that's answering\nthe second question. What about the first\nquestion about where",
    "start": "2509850",
    "end": "2516750"
  },
  {
    "text": "the tasks are going to run? Yeah. See what threads are spinning\nwith no work to be done",
    "start": "2516750",
    "end": "2522480"
  },
  {
    "text": "and are available, or I\nguess use in this case and assign it to them. Right. So some of scheduler\nwhich is looking at the--",
    "start": "2522480",
    "end": "2533442"
  },
  {
    "text": "you have some set of nodes which\nare going to be reducer nodes, and you have a scheduler,\nwhich is going to assign tasks.",
    "start": "2533442",
    "end": "2541210"
  },
  {
    "text": "But the question then still\nis who is actually going to--",
    "start": "2541210",
    "end": "2547020"
  },
  {
    "text": "how are you going to make\nsure that the data gets to the right node? ",
    "start": "2547020",
    "end": "2555800"
  },
  {
    "text": "Somebody had-- you\nhad an idea for that, which was use some hash\nfunction based on the key value,",
    "start": "2555800",
    "end": "2563420"
  },
  {
    "text": "and then use that as\nthe partitioner to-- or use that as the information\ngiven to the mapping task nodes,",
    "start": "2563420",
    "end": "2572700"
  },
  {
    "text": "and that will indicate\nwhere those nodes should send the key value data to be\nreduced depending on the key.",
    "start": "2572700",
    "end": "2583330"
  },
  {
    "text": "So example is a sign\nSafari iOS to node 0,",
    "start": "2583330",
    "end": "2590510"
  },
  {
    "text": "and then we know where\nto send the data to, and where the task\nis going to run,",
    "start": "2590510",
    "end": "2596070"
  },
  {
    "text": "where the reducer\ntask is going to run. So in different\nsystems work different.",
    "start": "2596070",
    "end": "2601650"
  },
  {
    "text": "Some ways decide\nup front, and so the mapper declares what\nkey value pairs it has,",
    "start": "2601650",
    "end": "2609620"
  },
  {
    "text": "but it knows where to\nsend them ahead of time, and others might decide\nonce all of the mapper tasks",
    "start": "2609620",
    "end": "2616930"
  },
  {
    "text": "are complete. Because you can't do any\nreduction until all the mapper",
    "start": "2616930",
    "end": "2622570"
  },
  {
    "text": "tasks are complete,\nso there has to be a barrier between the mapper\ntasks and the reducer tasks.",
    "start": "2622570",
    "end": "2632190"
  },
  {
    "text": "Would this system be aware\nof-- so in this case, Safari values are all equally\ndistributed among the four",
    "start": "2632190",
    "end": "2637780"
  },
  {
    "text": "nodes, but say like three\nof the Safari values were node 1 and one\nof them was a node 2.",
    "start": "2637780",
    "end": "2643970"
  },
  {
    "text": "It would be more\nefficient to get it. Yes, you could imagine a\nlocality aware-scheduling mechanism that took\nthat into account.",
    "start": "2643970",
    "end": "2650934"
  },
  {
    "text": "Good point.  all right.",
    "start": "2650935",
    "end": "2655970"
  },
  {
    "text": "So we have a mechanism\nfor deciding. You could use the\nscheme that was just suggested where we take--",
    "start": "2655970",
    "end": "2661930"
  },
  {
    "text": "we try and group the reducer-- we put the reducer\ntask on the place",
    "start": "2661930",
    "end": "2667120"
  },
  {
    "text": "where most of the\nkey value pairs already exist in order to\nminimize data movement,",
    "start": "2667120",
    "end": "2674690"
  },
  {
    "text": "or we could in the\ninstance of hey,",
    "start": "2674690",
    "end": "2680335"
  },
  {
    "text": "I've got a really powerful\nnetwork, it may not matter. ",
    "start": "2680335",
    "end": "2686070"
  },
  {
    "text": "So we've solved the key\nproblem of executing the mapper",
    "start": "2686070",
    "end": "2695090"
  },
  {
    "text": "and reducer function using\nthese different tasks associated with either different\nblocks of the input",
    "start": "2695090",
    "end": "2704750"
  },
  {
    "text": "file or different keys. But what if I've got\nthousands of nodes?",
    "start": "2704750",
    "end": "2711150"
  },
  {
    "text": "There are a couple of\nissues that can crop up. One is that some of the nodes\nmay fail, as we've talked about,",
    "start": "2711150",
    "end": "2718670"
  },
  {
    "text": "during the computation. So we're not going to lose any\ndata because we have made sure",
    "start": "2718670",
    "end": "2727940"
  },
  {
    "text": "that we've got a file system\nthat is fault tolerant,",
    "start": "2727940",
    "end": "2734220"
  },
  {
    "text": "but we need some\nway of making sure that we can recover\nthe computation",
    "start": "2734220",
    "end": "2739880"
  },
  {
    "text": "and make sure that the\ncomputation will complete successfully. So that's one issue. The other issue is that if\nI've got a huge data center,",
    "start": "2739880",
    "end": "2748870"
  },
  {
    "text": "I'm not going to have all\nthe nodes in the data center be of the same vintage.",
    "start": "2748870",
    "end": "2754250"
  },
  {
    "text": "You may refresh the\ndata center over time, over the period of maybe\nthree to five years.",
    "start": "2754250",
    "end": "2759760"
  },
  {
    "text": "And so I'm going to have\nsome older nodes, maybe with fewer cores and\nmaybe a lower clock rate,",
    "start": "2759760",
    "end": "2765960"
  },
  {
    "text": "and some newer nodes with more\ncores and maybe a higher clock rate. And so some of those nodes may\nfinish faster than other nodes.",
    "start": "2765960",
    "end": "2775510"
  },
  {
    "text": "And so how do I make sure that\nI can deal with this problem?",
    "start": "2775510",
    "end": "2781290"
  },
  {
    "text": "And so the key thing then is\nto have a job scheduler that",
    "start": "2781290",
    "end": "2786930"
  },
  {
    "text": "handles these issues. So exploits data\nlocality, runs mapper jobs",
    "start": "2786930",
    "end": "2793380"
  },
  {
    "text": "close to two input blocks, runs\nreducer blocks or reducer jobs or tasks, as was suggested\nclose to the place",
    "start": "2793380",
    "end": "2801390"
  },
  {
    "text": "where you've got\nmost of the data. And in particular, it\nhandles node failures. So basically you've\ngot a heartbeat, which",
    "start": "2801390",
    "end": "2810450"
  },
  {
    "text": "is each of the nodes regularly\nsends a message saying,",
    "start": "2810450",
    "end": "2816609"
  },
  {
    "text": "hey, I'm alive to\nthe master node, and the node is doing\nthe job scheduling.",
    "start": "2816610",
    "end": "2822730"
  },
  {
    "text": "At some point, if that\nheartbeat goes away, then the node is\ndeclared to be dead,",
    "start": "2822730",
    "end": "2828119"
  },
  {
    "text": "and the scheduler has to\nfigure out what to do. So what can you do?",
    "start": "2828120",
    "end": "2835630"
  },
  {
    "text": "Well, you once you detect\nthe failure of a mapper node, what should you do?",
    "start": "2835630",
    "end": "2840785"
  },
  {
    "text": " Yeah. You go to another one\nof the computers that",
    "start": "2840785",
    "end": "2848650"
  },
  {
    "text": "has the copies of the\nstorage [INAUDIBLE]. So you go somewhere\nelse, and you fire up",
    "start": "2848650",
    "end": "2855430"
  },
  {
    "text": "the tasks that were running\non that mapper node. And the data will come\nfrom blocks that have",
    "start": "2855430",
    "end": "2862510"
  },
  {
    "text": "been replicated in the system. And the fact that you've got the\nfunctional data-parallel model",
    "start": "2862510",
    "end": "2872190"
  },
  {
    "text": "in which the inputs\nare not mutated guarantees that that's\na safe thing to do. And so now you're going to\ncreate a new set key value",
    "start": "2872190",
    "end": "2879960"
  },
  {
    "text": "pairs that can be reduced.  So what happens if\na reducer fails?",
    "start": "2879960",
    "end": "2887165"
  },
  {
    "start": "2887165",
    "end": "2892920"
  },
  {
    "text": "Yeah. We need to send all that\ndata to a new reducer. What if it fails\nafter it's done?",
    "start": "2892920",
    "end": "2898230"
  },
  {
    "text": " Doesn't matter?",
    "start": "2898230",
    "end": "2903460"
  },
  {
    "text": "If the reduce\ntasks are complete, then it doesn't matter.",
    "start": "2903460",
    "end": "2908860"
  },
  {
    "text": "But if they're in the middle,\nthen you've got to restart them, and the data has to be either--",
    "start": "2908860",
    "end": "2916119"
  },
  {
    "text": "it's got to go get the data\nagain from the file system that where the key\nvalue pairs were stored.",
    "start": "2916120",
    "end": "2923040"
  },
  {
    "text": " So how do you handle\nslow machines. Well, the scheduler can just\nreplicate multiple reducer",
    "start": "2923040",
    "end": "2934310"
  },
  {
    "text": "or mapper jobs. If the machine is taking\ntoo long, it will say,",
    "start": "2934310",
    "end": "2939900"
  },
  {
    "text": "I'll just fire off a new job,\nand then it'll be a race. So if the slow computer\nfinishes first,",
    "start": "2939900",
    "end": "2947460"
  },
  {
    "text": "then that result will be\ntaken and the replicated job",
    "start": "2947460",
    "end": "2952970"
  },
  {
    "text": "will be killed. Otherwise, if the replicated\njob finishes first, then you can kill\nthe slow machine.",
    "start": "2952970",
    "end": "2960619"
  },
  {
    "text": "So duplicate and handle\nmultiple machines, and this is all possible\nbecause of this kind of data",
    "start": "2960620",
    "end": "2968540"
  },
  {
    "text": "parallel functional\nprogramming model. Yeah. Could we have some sort of\na metadata or heuristics",
    "start": "2968540",
    "end": "2974540"
  },
  {
    "text": "to avoid wasteful computation? Like, in this case, we\nare killing a node, which is not doing any useful work.",
    "start": "2974540",
    "end": "2982160"
  },
  {
    "text": "If you had metadata that this\nmachine is generally posting, this machine [INAUDIBLE]\nto schedule better?",
    "start": "2982160",
    "end": "2988619"
  },
  {
    "text": " I suppose you could, but\nthen extra complication",
    "start": "2988620",
    "end": "2997000"
  },
  {
    "text": "in your scheduler. And what happens when\nthe machine-- and it's got to keep track of what\nmachines are of what vintage,",
    "start": "2997000",
    "end": "3005940"
  },
  {
    "text": "and so on and so forth,\nand how loaded they are. That becomes more of\na scheduling issue.",
    "start": "3005940",
    "end": "3012070"
  },
  {
    "text": "Yes. What if your job\nscheduler fails? The job scheduler fails,\nthen you're out of luck. Your whole-- well, I\nmean, so as I said,",
    "start": "3012070",
    "end": "3021240"
  },
  {
    "text": "you've got a single no, the\nchances of that failing is not that high. But you might duplicate it.",
    "start": "3021240",
    "end": "3027190"
  },
  {
    "start": "3027190",
    "end": "3033180"
  },
  {
    "text": "All right. So the advantage of\nMapReduce then, of course, is that it gives you this\nnice data-parallel programming",
    "start": "3033180",
    "end": "3041970"
  },
  {
    "text": "model, MapReduce. It's fairly easy to understand. I can explain MapReduce\nto most CS undergrads",
    "start": "3041970",
    "end": "3050855"
  },
  {
    "text": "and they would\nget it right away. Whereas when I introduced\nthem to message passing, it would be a difficult process.",
    "start": "3050855",
    "end": "3057960"
  },
  {
    "text": "So you've got this automatic\ndivision of the job into tasks. See the mapper tasks\nor reducer tasks.",
    "start": "3057960",
    "end": "3065940"
  },
  {
    "text": "You've got this\nload balancing that happens because you've got many\ntasks and many reducer tasks.",
    "start": "3065940",
    "end": "3073930"
  },
  {
    "text": "You've got locality\naware scheduling, and you've got this idea\nof being able to recover",
    "start": "3073930",
    "end": "3080580"
  },
  {
    "text": "from failures and stragglers. So it's a pretty nice\nprogramming model, and it made it possible\nfor many models",
    "start": "3080580",
    "end": "3089670"
  },
  {
    "text": "to program hundreds\nof thousands of CPUs for doing these data\nprocessing tasks.",
    "start": "3089670",
    "end": "3095619"
  },
  {
    "text": "So it really took off. And it became an\nidea that actually had widespread use, even\nbeyond distributed computing.",
    "start": "3095620",
    "end": "3104350"
  },
  {
    "text": "We've already seen it in\nthe context of GPU computing and it's being used\nin other areas too.",
    "start": "3104350",
    "end": "3112770"
  },
  {
    "text": "However, it does\nhave some issues. And the first issue being\nthe programming model",
    "start": "3112770",
    "end": "3118530"
  },
  {
    "text": "is pretty simplistic. The only thing you can do is\nhave a map followed by reduce,",
    "start": "3118530",
    "end": "3123825"
  },
  {
    "text": "followed by a map, followed\nby a reduce, and so on. So it's a fairly kind of this\nlinear arrangement of map",
    "start": "3123825",
    "end": "3129599"
  },
  {
    "text": "and reduce functions, which\nkind of limits what kinds",
    "start": "3129600",
    "end": "3135690"
  },
  {
    "text": "of applications you can write. And so there was extension\nto a directed acyclic graph.",
    "start": "3135690",
    "end": "3144270"
  },
  {
    "text": "The work was called\nDryadLINQ, and it certainly had some academic impact, but\nit didn't get much adoption",
    "start": "3144270",
    "end": "3156280"
  },
  {
    "text": "as far as I can tell. But it's an interesting idea. If you're interested, go and\nlook up the DryadLINQ paper.",
    "start": "3156280",
    "end": "3163279"
  },
  {
    "text": "It's a pretty interesting paper. And how about\niterative program-- iterative algorithms?",
    "start": "3163280",
    "end": "3168920"
  },
  {
    "text": "So PageRank, everybody\nhas heard of PageRank. You want to understand\nthe importance of a particular website,\nthen you compute.",
    "start": "3168920",
    "end": "3176830"
  },
  {
    "text": "PageRank was originally\ninvented by Larry Page, so one",
    "start": "3176830",
    "end": "3182290"
  },
  {
    "text": "of the founders of Google. And the idea is it's\nan iterative algorithm.",
    "start": "3182290",
    "end": "3189770"
  },
  {
    "text": "And if you implement it\nusing the MapReduce model, then every iteration requires\na distributed file system",
    "start": "3189770",
    "end": "3198790"
  },
  {
    "text": "read followed by a\ndistributed file system.",
    "start": "3198790",
    "end": "3204760"
  },
  {
    "text": "And this could go on\nfor many iterations, and so it becomes fairly\ninefficient to run",
    "start": "3204760",
    "end": "3213970"
  },
  {
    "text": "this algorithm using MapReduce. Another area where MapReduce\ndoesn't work so well",
    "start": "3213970",
    "end": "3221230"
  },
  {
    "text": "is if you've got a set\nof data and that you want",
    "start": "3221230",
    "end": "3227080"
  },
  {
    "text": "to query in some of ad hoc way. And this often happens in\ninteractive data processing",
    "start": "3227080",
    "end": "3238180"
  },
  {
    "text": "applications. So you've got some data that\nexists in the file system,",
    "start": "3238180",
    "end": "3244390"
  },
  {
    "text": "and you've got a bunch of ad\nhoc queries, and each of them requires this access to\nthe file system, which",
    "start": "3244390",
    "end": "3252790"
  },
  {
    "text": "is not very inefficient. So even though\nthen this MapReduce has all these nice\nproperties and as I said,",
    "start": "3252790",
    "end": "3259520"
  },
  {
    "text": "had a huge impact in the\nability for programmers",
    "start": "3259520",
    "end": "3264970"
  },
  {
    "text": "to develop these\ndistributed applications, these kind of\nlimitations led to people",
    "start": "3264970",
    "end": "3275070"
  },
  {
    "text": "thinking about more efficient\nways of using the computation. And sorry for the font.",
    "start": "3275070",
    "end": "3281593"
  },
  {
    "text": " The way the fonts\nlook on the slide,",
    "start": "3281593",
    "end": "3288440"
  },
  {
    "text": "but the key point\nis to remember what we said about the relative\nbandwidth between the memory",
    "start": "3288440",
    "end": "3297070"
  },
  {
    "text": "versus the network and\nthe storage devices. And so what we want\nto do is to say--",
    "start": "3297070",
    "end": "3304090"
  },
  {
    "text": "is to think about how we can\nmake much more intensive use of the highest bandwidth\ninterface on this picture, which",
    "start": "3304090",
    "end": "3311575"
  },
  {
    "text": "is the memory. So what is the problem? Well, before we talk\nabout the problem,",
    "start": "3311575",
    "end": "3318720"
  },
  {
    "text": "let's talk about further\nmotivation for why we potentially could use\nthe memory more intensively.",
    "start": "3318720",
    "end": "3325049"
  },
  {
    "text": "So this is a table from a\npaper called \"Disk-Locality",
    "start": "3325050",
    "end": "3332900"
  },
  {
    "text": "in Datacenter Computing\nConsidered Irrelevant.\" And that's from a paper\npublished in 2011.",
    "start": "3332900",
    "end": "3340710"
  },
  {
    "text": "So part of the reason that\nit was considered irrelevant was the point that\nwe've already made, which is the networks\nwere getting much more",
    "start": "3340710",
    "end": "3348470"
  },
  {
    "text": "bandwidth-capable. But the other reason is\nshown in this table, which",
    "start": "3348470",
    "end": "3355839"
  },
  {
    "text": "is data from 2009, and\nit's looking at the working",
    "start": "3355840",
    "end": "3361310"
  },
  {
    "text": "set of the big data applications\nat Facebook, Microsoft, and Yahoo, and it's showing that\nwith 64 gigabytes of memory,",
    "start": "3361310",
    "end": "3372840"
  },
  {
    "text": "97% of the working sets\nof data at Facebook can be contained in the memory,\n98% at Microsoft, and 99.5%",
    "start": "3372840",
    "end": "3384630"
  },
  {
    "text": "at Yahoo. So the point here\nis that you really can keep most of your\ndata in the memory.",
    "start": "3384630",
    "end": "3392289"
  },
  {
    "text": "The memory is big enough,\nbut the programming model forces you to shuttle\nthis data back and forth",
    "start": "3392290",
    "end": "3398820"
  },
  {
    "text": "between the storage\ndevices and the memory. So the question is,\ncould we come up",
    "start": "3398820",
    "end": "3405480"
  },
  {
    "text": "with a programming model, which\nis just as nice or even better than MapReduce, but allows you\nto use the memory much more",
    "start": "3405480",
    "end": "3414600"
  },
  {
    "text": "intensively? What would be the-- yeah. You're saying that all the\ndata fits in single nodes disk,",
    "start": "3414600",
    "end": "3420780"
  },
  {
    "text": "or that the data fits\nin all the nodes disk? All the nodes disk. Yeah.",
    "start": "3420780",
    "end": "3426090"
  },
  {
    "text": "So all the nodes\nthat you're running, the data will fit in the\nlocal memory, not disk.",
    "start": "3426090",
    "end": "3432185"
  },
  {
    "text": " Including the replicas?",
    "start": "3432185",
    "end": "3437549"
  },
  {
    "text": "The replicas have to do\nwith the file system. We're talking about you're\nactually doing the computation.",
    "start": "3437550",
    "end": "3443160"
  },
  {
    "text": "In the middle of\nthe computation, will the data fit\nin your local-- in the memory, or will you\nhave to make use of the disk?",
    "start": "3443160",
    "end": "3453720"
  },
  {
    "text": "It turns out you can\nactually do the computation by keeping the data in memory.",
    "start": "3453720",
    "end": "3462030"
  },
  {
    "text": "I guess your computation\nstill depends on the memory of [INAUDIBLE]\ndata and other nodes memory.",
    "start": "3462030",
    "end": "3468339"
  },
  {
    "text": "You still have to\ncommunicate between them. Yeah, I mean, you still-- I mean, so this is talking\nabout working set, which",
    "start": "3468340",
    "end": "3474390"
  },
  {
    "text": "is most of the accesses are\nto data that lives in memory.",
    "start": "3474390",
    "end": "3480299"
  },
  {
    "text": "There's not all the accesses. So you still have to move some\nof the data from other places. The question is, we talked about\nlocality, so you've got reuse,",
    "start": "3480300",
    "end": "3489250"
  },
  {
    "text": "you've got spatial locality. So this is just talking about\nworking set, which is a concept",
    "start": "3489250",
    "end": "3495640"
  },
  {
    "text": "that you should have-- we should have talked about\nin an operating systems class. There's a notion of\nwhat is a set of data",
    "start": "3495640",
    "end": "3502180"
  },
  {
    "text": "that you're actively using? And that is what they're\ntrying to measure here.",
    "start": "3502180",
    "end": "3507890"
  },
  {
    "text": " So what would be the problem\nwith using memory instead",
    "start": "3507890",
    "end": "3516970"
  },
  {
    "text": "of the storage system to\nhold the intermediate data in your computation?",
    "start": "3516970",
    "end": "3522290"
  },
  {
    "text": "Yeah. If you lose power, you're\nspirit, if you lose power, you're screwed. You're going to lose\nyour computation.",
    "start": "3522290",
    "end": "3529069"
  },
  {
    "text": "So the whole idea of spark\nwas in memory fault tolerant",
    "start": "3529070",
    "end": "3534130"
  },
  {
    "text": "distributed computing, with\nemphasis on the fault tolerant peace thing.",
    "start": "3534130",
    "end": "3539237"
  },
  {
    "text": "Once you lose power, you\ndon't want to be screwed.  So the goals were can you have\nsupport for iterative machine",
    "start": "3539237",
    "end": "3549579"
  },
  {
    "text": "learning algorithms, data\nmining algorithms in which you",
    "start": "3549580",
    "end": "3555250"
  },
  {
    "text": "keep your data in memory. So you don't want to incur\nthe inefficiencies of writing",
    "start": "3555250",
    "end": "3563859"
  },
  {
    "text": "the intermediate\ndata to the disk because we said that's a low\nbandwidth, low performance data",
    "start": "3563860",
    "end": "3569980"
  },
  {
    "text": "path, and what you want to\nmake sure is intensively use the high-performance,\nhigh-bandwidth data",
    "start": "3569980",
    "end": "3575770"
  },
  {
    "text": "path between the CPU and memory. So the challenge\nthen, of course,",
    "start": "3575770",
    "end": "3582030"
  },
  {
    "text": "is how do we make sure that\nwe can do this efficiently and we can efficiently\nimplement fault",
    "start": "3582030",
    "end": "3588500"
  },
  {
    "text": "tolerance for this large-scale\ndistributed memory? So the solution that\nwe've come up so far",
    "start": "3588500",
    "end": "3594620"
  },
  {
    "text": "is we know how to do a fault\ntolerant storage system, so let's just use that.",
    "start": "3594620",
    "end": "3600539"
  },
  {
    "text": "That's the MapReduce solution. But that's a\nlow-performance solution. So what we want is a\nhigher-performance solution",
    "start": "3600540",
    "end": "3606740"
  },
  {
    "text": "that relies on memory. And so that's Spark.",
    "start": "3606740",
    "end": "3613050"
  },
  {
    "text": "So we talked about could kind\nof checkpoint and rollback,",
    "start": "3613050",
    "end": "3619800"
  },
  {
    "text": "but then the question\nis, how do you do that and how do you make\nsure that you distribute your data to different racks?",
    "start": "3619800",
    "end": "3627240"
  },
  {
    "text": "So it could be a\nnetwork intensive, you can maintain\na log of updates,",
    "start": "3627240",
    "end": "3634339"
  },
  {
    "text": "but it could be a high overhead\nto maintain these logs, so that probably is not\ngoing to work so well.",
    "start": "3634340",
    "end": "3641724"
  },
  {
    "text": "And then you could go to\na low-performance solution like we've talked about\nin the case of MapReduce.",
    "start": "3641725",
    "end": "3649880"
  },
  {
    "text": "So in this case, we're going to\ncheckpoint after each MapReduce by writing results\nto the file system,",
    "start": "3649880",
    "end": "3656579"
  },
  {
    "text": "and then the scheduler\nis going to make sure that it's keeping a list\nof what needs to complete,",
    "start": "3656580",
    "end": "3664290"
  },
  {
    "text": "and it will rerun\nthings that fail. And the functional\nstructure of programs",
    "start": "3664290",
    "end": "3671299"
  },
  {
    "text": "allows for the restart at\nthe granularity of the map",
    "start": "3671300",
    "end": "3676895"
  },
  {
    "text": "or reduce task. All right. So how does Spark\nsolve the problem?",
    "start": "3676895",
    "end": "3684440"
  },
  {
    "text": "So the first thing Spark\ndoes is introduce this idea, which is central to\nthe approach, which",
    "start": "3684440",
    "end": "3692800"
  },
  {
    "text": "is the idea of a Resilient\nDistributed Dataset, RDD. So this is Spark's key\nprogramming abstraction.",
    "start": "3692800",
    "end": "3701680"
  },
  {
    "text": "And what is an RDD? It's a read-only ordered\ncollection of records.",
    "start": "3701680",
    "end": "3709610"
  },
  {
    "text": "So it's immutable. So this idea of the RDD\nis a functional in nature.",
    "start": "3709610",
    "end": "3718370"
  },
  {
    "text": "So once you think about\nfunctional programming, RDD is a fairly\nnatural construct.",
    "start": "3718370",
    "end": "3726590"
  },
  {
    "text": "And so RDDs can only be\ncreated by the transformations",
    "start": "3726590",
    "end": "3732040"
  },
  {
    "text": "from other RDDs or from\npersistent storage.",
    "start": "3732040",
    "end": "3738170"
  },
  {
    "text": "So for instance, if I start\nwith our CS149 log, which",
    "start": "3738170",
    "end": "3745480"
  },
  {
    "text": "is a text file, which is stored\nin our distributed file system, I can get the lines\nfrom that file",
    "start": "3745480",
    "end": "3753310"
  },
  {
    "text": "to create an RDD called lines. So that was a transformation\nfrom the file system",
    "start": "3753310",
    "end": "3760720"
  },
  {
    "text": "to an RDD called lines. Then I can use a\nfilter transformation.",
    "start": "3760720",
    "end": "3769460"
  },
  {
    "text": "So I have the lines\nRDD, and I'm going to apply a filter\ntransformation on to see",
    "start": "3769460",
    "end": "3777740"
  },
  {
    "text": "whether this is a\nmobile client, and then",
    "start": "3777740",
    "end": "3784404"
  },
  {
    "text": "what I'm going to\nget is mobile views, lines that correspond\nto mobile views.",
    "start": "3784404",
    "end": "3790220"
  },
  {
    "text": "And so this is another RDD.  And then I'm going to\ntake the mobile views RDD,",
    "start": "3790220",
    "end": "3801290"
  },
  {
    "text": "and I'm going to\nfilter it again to see whether the string\ncontains Safari.",
    "start": "3801290",
    "end": "3808370"
  },
  {
    "text": "And if that is true,\nI'm going to create-- it's going to be entered into\nthis new RDD called SafariViews.",
    "start": "3808370",
    "end": "3817680"
  },
  {
    "text": "And then finally, I'm\ngoing to take SafariViews. I'm going to apply action\nwhich will essentially",
    "start": "3817680",
    "end": "3826220"
  },
  {
    "text": "give us a count. It's a reduction\nwhich of course, it's going to create this\nint, which is not an RDD.",
    "start": "3826220",
    "end": "3832670"
  },
  {
    "text": "But the RDDs are lines,\nmobile views and SafariViews, and the sequence of\noperations that create",
    "start": "3832670",
    "end": "3842780"
  },
  {
    "text": "the RDDs is called a lineage. And we'll come back\nand revisit this idea.",
    "start": "3842780",
    "end": "3852101"
  },
  {
    "text": "So this is the main way that\nyou program using Spark.",
    "start": "3852101",
    "end": "3858150"
  },
  {
    "text": "You start with some\ndata in the file system, and then you apply\ntransformations to that data",
    "start": "3858150",
    "end": "3865260"
  },
  {
    "text": "to create different\nRDDs to encode the logic of your program.",
    "start": "3865260",
    "end": "3871390"
  },
  {
    "text": "And each of the RDDs is,\nof course, read only,",
    "start": "3871390",
    "end": "3876510"
  },
  {
    "text": "and it's ordered,\nand it's immutable, and these turn out to be\nvery useful properties when",
    "start": "3876510",
    "end": "3883020"
  },
  {
    "text": "you're trying to build\na fault tolerant system. ",
    "start": "3883020",
    "end": "3888530"
  },
  {
    "text": "All right. So we talked about\nthe fact that you-- so you could write\nin using Scala,",
    "start": "3888530",
    "end": "3897645"
  },
  {
    "text": "you can write all this in\na fairly functional way, and you can create--",
    "start": "3897645",
    "end": "3903690"
  },
  {
    "text": "in this case, you create an\nRDD from the file system,",
    "start": "3903690",
    "end": "3908760"
  },
  {
    "text": "and you don't have to\nexplicitly call out the RDDs,",
    "start": "3908760",
    "end": "3915330"
  },
  {
    "text": "and the sequence of\ntransformations then, as I said,",
    "start": "3915330",
    "end": "3920790"
  },
  {
    "text": "cold lineage.  All right.",
    "start": "3920790",
    "end": "3926080"
  },
  {
    "text": "So in this case, you do-- your operations are filter,\nmap, reduceBy, and collect.",
    "start": "3926080",
    "end": "3934560"
  },
  {
    "text": "So reduceBy. Collect is an action. It's not a transformation.",
    "start": "3934560",
    "end": "3941770"
  },
  {
    "text": " Any questions? So you've got this concept\nof the RDD, the Resilient",
    "start": "3941770",
    "end": "3955320"
  },
  {
    "text": "Distributed Dataset. You've got this\nconcept of a lineage. ",
    "start": "3955320",
    "end": "3961530"
  },
  {
    "text": "So how can we optimize\nthings with Spark? Well, we have this\nnotion of persist, which",
    "start": "3961530",
    "end": "3971160"
  },
  {
    "text": "says keep the RDD in memory. And so in this case, we\ncreate a mobile views RDD,",
    "start": "3971160",
    "end": "3982569"
  },
  {
    "text": "and then we're going\nto use it twice. Once we're going to filter\nfor Chrome, and then collect--",
    "start": "3982570",
    "end": "3994109"
  },
  {
    "text": "take the lines, the\nstrings that are in the RDD",
    "start": "3994110",
    "end": "4003320"
  },
  {
    "text": "created from the filter,\nand extract time counts-- time stamps, I should say.",
    "start": "4003320",
    "end": "4009920"
  },
  {
    "text": "And then the other case, we're\ngoing to filter based on Safari, and figure out the\nnumber of page views",
    "start": "4009920",
    "end": "4017990"
  },
  {
    "text": "that came from a Safari client. ",
    "start": "4017990",
    "end": "4024180"
  },
  {
    "text": "So here are the set\nof transformations that you can use in Spark.",
    "start": "4024180",
    "end": "4029670"
  },
  {
    "text": "So you've got map, filter,\nflatMap, sample, reduceByKey,",
    "start": "4029670",
    "end": "4035520"
  },
  {
    "text": "join, sort, partitionBy,\nwhich we're going to talk about in just a moment.",
    "start": "4035520",
    "end": "4042450"
  },
  {
    "text": "And then you've got actions,\nwhich, of course, create data back to the host like\ncount, collect, reduce, lookup,",
    "start": "4042450",
    "end": "4050730"
  },
  {
    "text": "save. All right. So with these operations, you\ncan write very sophisticated",
    "start": "4050730",
    "end": "4057559"
  },
  {
    "text": "applications. So let's think about now--",
    "start": "4057560",
    "end": "4062630"
  },
  {
    "text": "any questions on the programming\nmodel and abstraction that Spark presents.",
    "start": "4062630",
    "end": "4069230"
  },
  {
    "text": "So this is a notion of the RDD,\nand then these transformations and actions. Yeah. By default, every\ntransformation consumes",
    "start": "4069230",
    "end": "4075898"
  },
  {
    "text": "the previous [INAUDIBLE]. It will finish the [INAUDIBLE]. Why do we need an\nexplicit persist?",
    "start": "4075898",
    "end": "4083480"
  },
  {
    "text": "So persist just says,\nkeep the RDD in memory. ",
    "start": "4083480",
    "end": "4089780"
  },
  {
    "text": "It will be\n[? elevated, ?] right? Well, it may not\nkeep it in memory.",
    "start": "4089780",
    "end": "4095305"
  },
  {
    "text": " It's not going to-- I mean, the RDD will exist\npotentially in the file system.",
    "start": "4095305",
    "end": "4109949"
  },
  {
    "text": "The question is, will\nit stay in memory?",
    "start": "4109950",
    "end": "4116183"
  },
  {
    "text": "And that's what\npersists-- persist says, keep the RDD in memory.",
    "start": "4116183",
    "end": "4121977"
  },
  {
    "text": "How do we decide which RDDs\nget persistent to the file? Well, you can say explicitly.",
    "start": "4121977",
    "end": "4128089"
  },
  {
    "text": "The scheduler can\nmake some decisions or you can say explicitly. One question is, if\nyou don't say persist",
    "start": "4128090",
    "end": "4135439"
  },
  {
    "text": "and you ask for it again,\nisn't that computer [INAUDIBLE] or is it [INAUDIBLE]? ",
    "start": "4135439",
    "end": "4141560"
  },
  {
    "text": "Yeah, I'll probably save it\nto storage and reload it. So it will be inefficient.",
    "start": "4141560",
    "end": "4147439"
  },
  {
    "text": "So in the case of the\nexample that we showed here,",
    "start": "4147439",
    "end": "4155560"
  },
  {
    "text": "if you didn't\npersist mobile views, then mobile views would get\nwritten to the file system,",
    "start": "4155560",
    "end": "4165250"
  },
  {
    "text": "and you would have to go\nfetch it again in order to do. Let's suppose you did\nthe left-hand side first,",
    "start": "4165250",
    "end": "4171500"
  },
  {
    "text": "then to do the right-hand side,\nyou'd have to fetch it again. One more question\nis, when you're hitting the collect there,\nit's kind of clear that you've",
    "start": "4171500",
    "end": "4179799"
  },
  {
    "text": "[INAUDIBLE] it in both cases. If you have a collect instead\nof account or something-- like if you make [INAUDIBLE]\nsome sort of lazy [INAUDIBLE],",
    "start": "4179800",
    "end": "4187299"
  },
  {
    "text": "that Spark will do the\noptimization for you? Do what optimization?",
    "start": "4187300",
    "end": "4193611"
  },
  {
    "text": "If Spark knew that mobile views\nis used by two operations, then can it not just leave\nit in memory, do it both",
    "start": "4193612",
    "end": "4201160"
  },
  {
    "text": "and do that optimization? Yeah.",
    "start": "4201160",
    "end": "4206300"
  },
  {
    "text": "Sometimes you've got\nto help the system, but other times the\nsystem can analyze.",
    "start": "4206300",
    "end": "4211770"
  },
  {
    "text": "And as I'll describe,\nthe Spark-run system does a bunch of analysis and\nwe'll do optimizations for you",
    "start": "4211770",
    "end": "4218750"
  },
  {
    "text": "and we'll talk about\none in just a moment. ",
    "start": "4218750",
    "end": "4224780"
  },
  {
    "text": "So let's think about how\nwe can implement RDDs.",
    "start": "4224780",
    "end": "4230989"
  },
  {
    "text": "So imagine that\nhere are the RDDs. We've got lines,\nlower mobile views,",
    "start": "4230990",
    "end": "4237969"
  },
  {
    "text": "and how many is not an RDD, but\nlines lower and mobile views.",
    "start": "4237970",
    "end": "4246640"
  },
  {
    "text": "So if you think about the fact\nthat this is going to be part-- these are going to be\npartitioned across the nodes,",
    "start": "4246640",
    "end": "4254240"
  },
  {
    "text": "so let's assume that there are\ntwo partitions on every node,",
    "start": "4254240",
    "end": "4262850"
  },
  {
    "text": "so lines partition creates--",
    "start": "4262850",
    "end": "4269800"
  },
  {
    "text": "is used to create the\nlower partition, which is used to create the\nmobile views partition.",
    "start": "4269800",
    "end": "4277270"
  },
  {
    "text": "So one way would\nbe we could think of these as arrays that\nget duplicated in memory,",
    "start": "4277270",
    "end": "4287620"
  },
  {
    "text": "but that would lead to\na lot of memory use.",
    "start": "4287620",
    "end": "4292780"
  },
  {
    "text": "And so the question is, how can\nwe implement this data parallel",
    "start": "4292780",
    "end": "4298360"
  },
  {
    "text": "RDD abstraction in\nan efficient way such that we minimize\nthe use of memory,",
    "start": "4298360",
    "end": "4306590"
  },
  {
    "text": "but we still keep all of the\nfault tolerant capabilities that we want?",
    "start": "4306590",
    "end": "4313060"
  },
  {
    "text": "We don't keep the partitions\nthat were done with. Like if we finish--",
    "start": "4313060",
    "end": "4318070"
  },
  {
    "text": "if we're computing lower or\nwe finish computing lower, we don't need to then also\nkeep the data from lines",
    "start": "4318070",
    "end": "4325030"
  },
  {
    "text": "because we already have the\nRDD that comes after that, and so we're already like, yeah.",
    "start": "4325030",
    "end": "4330550"
  },
  {
    "text": "Yeah, that would be\na good first step, but we may be able to do\neven better than that.",
    "start": "4330550",
    "end": "4338980"
  },
  {
    "text": "Instead of duplicating\nthe array each time you're doing a filter\nor any function,",
    "start": "4338980",
    "end": "4345260"
  },
  {
    "text": "you can just have\none general array and change the references to\nthe elements in it each time",
    "start": "4345260",
    "end": "4350900"
  },
  {
    "text": "you [INAUDIBLE] it. So that way you don't have to\nduplicate the array each time. ",
    "start": "4350900",
    "end": "4363570"
  },
  {
    "text": "In which case? I mean, so in terms\nof efficiency, you probably don't\nwant references",
    "start": "4363570",
    "end": "4370560"
  },
  {
    "text": "going all over the place. I mean, you want things\nto be concatenated",
    "start": "4370560",
    "end": "4378210"
  },
  {
    "text": "and you want things to be dense. So having references is probably\nnot a very efficient way",
    "start": "4378210",
    "end": "4384960"
  },
  {
    "text": "of doing things. Other questions? ",
    "start": "4384960",
    "end": "4392270"
  },
  {
    "text": "Where are we? Running out of time. All right. So let's figure out how we\ncan do things efficiently.",
    "start": "4392270",
    "end": "4403340"
  },
  {
    "text": "So one of the things we could do\nis think about the dependencies.",
    "start": "4403340",
    "end": "4410360"
  },
  {
    "text": "And the dependencies go from\nthe data in the file system",
    "start": "4410360",
    "end": "4418580"
  },
  {
    "text": "through the different partitions\nof the different RDDs from lines",
    "start": "4418580",
    "end": "4428600"
  },
  {
    "text": "to lower to mobile views. So the question is,\nthinking about the fact",
    "start": "4428600",
    "end": "4436670"
  },
  {
    "text": "that we've got\nthese dependencies, can we optimize the\nimplementation of RDDs?",
    "start": "4436670",
    "end": "4443940"
  },
  {
    "text": "And the way to think\nabout it is think back to some of the optimizations\nthat we've talked about so far.",
    "start": "4443940",
    "end": "4449540"
  },
  {
    "text": "So we've talked about the\nfact that this program",
    "start": "4449540",
    "end": "4455410"
  },
  {
    "text": "is better than that program. And why is that? [INAUDIBLE] to memory.",
    "start": "4455410",
    "end": "4461733"
  },
  {
    "text": "That's the reason why. We've done fusion\nof these loops, and we've reduced\nthe rise of memory,",
    "start": "4461733",
    "end": "4471540"
  },
  {
    "text": "and so improved the\narithmetic intensity. And so we also talked\nabout this optimization",
    "start": "4471540",
    "end": "4481770"
  },
  {
    "text": "where we use tiling to\noptimize the use of memory,",
    "start": "4481770",
    "end": "4490440"
  },
  {
    "text": "such that we reduce\nthe amount of memory we need to support the blurring.",
    "start": "4490440",
    "end": "4501420"
  },
  {
    "text": "And so these two ideas of fusion\nand tiling are pretty important,",
    "start": "4501420",
    "end": "4508130"
  },
  {
    "text": "and they can be implemented\nin the Spark runtime system.",
    "start": "4508130",
    "end": "4513822"
  },
  {
    "text": "So the question is, when can\nyou apply these transformations. Well, they fail--\nif you try and do",
    "start": "4513822",
    "end": "4520460"
  },
  {
    "text": "these transformations\non arbitrary C programs, they're difficult to do.",
    "start": "4520460",
    "end": "4525739"
  },
  {
    "text": "Usually you as the programmer\nhave to implement them. However, if you start from\na high level representation",
    "start": "4525740",
    "end": "4533210"
  },
  {
    "text": "like Spark or PyTorch or\nsomething that gives you more semantic information about\nwhat is actually going on,",
    "start": "4533210",
    "end": "4542489"
  },
  {
    "text": "then you can do some of these\ntransformations automatically. So fusion with RDDs is\npossible, however, you",
    "start": "4542490",
    "end": "4552500"
  },
  {
    "text": "need to know what\nthe dependencies are between the different RDDs.",
    "start": "4552500",
    "end": "4557700"
  },
  {
    "text": "So we have this notion\nof narrow dependencies where one RDD only depends\non a one other partition.",
    "start": "4557700",
    "end": "4568510"
  },
  {
    "text": "So these are called\nnarrow dependencies. So partition 0 of\nmobile views is",
    "start": "4568510",
    "end": "4573719"
  },
  {
    "text": "dependent on\npartition 0 of lower, which is dependent on\npartition 0 of lines,",
    "start": "4573720",
    "end": "4580930"
  },
  {
    "text": "which is dependent on block 0. So these are narrow\ndependencies in that RDD only",
    "start": "4580930",
    "end": "4586889"
  },
  {
    "text": "depends on one other\npartition of an RDD.",
    "start": "4586890",
    "end": "4592290"
  },
  {
    "text": "Why dependencies\nare a case where you've got an RDD which\ndepends on multiple RDDs,",
    "start": "4592290",
    "end": "4602940"
  },
  {
    "text": "so in this case, if\nI do a group by key,",
    "start": "4602940",
    "end": "4608719"
  },
  {
    "text": "I may have to get\nelements of the partition",
    "start": "4608720",
    "end": "4613790"
  },
  {
    "text": "from multiple places. And so if I have\nnarrow partitions, then",
    "start": "4613790",
    "end": "4620090"
  },
  {
    "text": "potentially I can have\nthe system automatically do the fusion.",
    "start": "4620090",
    "end": "4625820"
  },
  {
    "text": "At this point, we'll run out\nof time, so what we'll do",
    "start": "4625820",
    "end": "4630920"
  },
  {
    "text": "is after probably\nnext Tuesday, I",
    "start": "4630920",
    "end": "4636520"
  },
  {
    "text": "will finish up this\ndiscussion of Spark and move on to cache coherency.",
    "start": "4636520",
    "end": "4643219"
  },
  {
    "text": "And on Thursday,\nKayvon will be back to talk about efficient\nimplementation of DNNs, right?",
    "start": "4643220",
    "end": "4653490"
  },
  {
    "text": "No, this Thursday. This Thursday. Yeah, right. And then on Tuesday, I'll come\nback and we'll wrap up Spark,",
    "start": "4653490",
    "end": "4660460"
  },
  {
    "text": "and we'll start talking\nabout cache coherency. So sorry for going\nover time, but it's",
    "start": "4660460",
    "end": "4667270"
  },
  {
    "text": "really interesting stuff. ",
    "start": "4667270",
    "end": "4674000"
  }
]