[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "start": "0",
    "end": "5420"
  },
  {
    "text": "Hi.",
    "start": "5420",
    "end": "5920"
  },
  {
    "text": "In this lecture, I'm going to\ntalk about stochastic gradient",
    "start": "5920",
    "end": "9520"
  },
  {
    "text": "descent.",
    "start": "9520",
    "end": "11360"
  },
  {
    "text": "So recall gradient descent,\nwhich was the optimization",
    "start": "11360",
    "end": "14799"
  },
  {
    "text": "algorithm that we decided on\nfor optimizing all our training",
    "start": "14800",
    "end": "18970"
  },
  {
    "text": "losses for classification\nand regression.",
    "start": "18970",
    "end": "23119"
  },
  {
    "text": "So recall that the\ntraining loss is",
    "start": "23120",
    "end": "24880"
  },
  {
    "text": "an average over all the examples\nin the training set of the per",
    "start": "24880",
    "end": "30579"
  },
  {
    "text": "example losses.",
    "start": "30580",
    "end": "33360"
  },
  {
    "text": "So gradient descent\nworks as follows.",
    "start": "33360",
    "end": "36630"
  },
  {
    "text": "We're going to initialize\nthe weight vector to 0,",
    "start": "36630",
    "end": "39260"
  },
  {
    "text": "and then we're going\nto repeat T times",
    "start": "39260",
    "end": "42289"
  },
  {
    "text": "and do the following update.",
    "start": "42290",
    "end": "44780"
  },
  {
    "text": "We're going to take\nthe old weight vector",
    "start": "44780",
    "end": "47180"
  },
  {
    "text": "and subtract out the step\nsize times the gradient",
    "start": "47180",
    "end": "50540"
  },
  {
    "text": "over the training loss.",
    "start": "50540",
    "end": "53480"
  },
  {
    "text": "And now this looks very simple.",
    "start": "53480",
    "end": "56190"
  },
  {
    "text": "But if you unpack\nwhat this gradient is,",
    "start": "56190",
    "end": "59390"
  },
  {
    "text": "it's actually the average\nover the gradients",
    "start": "59390",
    "end": "62149"
  },
  {
    "text": "of the per example losses.",
    "start": "62150",
    "end": "65370"
  },
  {
    "text": "So now imagine you have a data\nset with a million examples.",
    "start": "65370",
    "end": "68840"
  },
  {
    "text": "Computing the single\ngradient is going",
    "start": "68840",
    "end": "72799"
  },
  {
    "text": "to involve looping over all\nthe million examples just",
    "start": "72800",
    "end": "76400"
  },
  {
    "text": "to get a single update,\nand then you take a step",
    "start": "76400",
    "end": "81050"
  },
  {
    "text": "and then you have to\ndo it all over again.",
    "start": "81050",
    "end": "83730"
  },
  {
    "text": "So this is why gradient\ndescent is slow,",
    "start": "83730",
    "end": "86167"
  },
  {
    "text": "because it requires going\nthrough all the training",
    "start": "86167",
    "end": "88250"
  },
  {
    "text": "examples just to\nmake one update.",
    "start": "88250",
    "end": "91860"
  },
  {
    "text": "So what can we do about this?",
    "start": "91860",
    "end": "95180"
  },
  {
    "start": "95000",
    "end": "830000"
  },
  {
    "text": "So the answer is stochastic\ngradient descent.",
    "start": "95180",
    "end": "98360"
  },
  {
    "text": "So here is the same\ntraining loss function,",
    "start": "98360",
    "end": "101080"
  },
  {
    "text": "and stochastic descent is\ngoing to work as follows.",
    "start": "101080",
    "end": "104390"
  },
  {
    "text": "So we initialize the\nweight vectors to 0,",
    "start": "104390",
    "end": "107140"
  },
  {
    "text": "and then we iterate T times.",
    "start": "107140",
    "end": "110229"
  },
  {
    "text": "And now on each\nepoch, we're going",
    "start": "110230",
    "end": "114460"
  },
  {
    "text": "to loop over the\ntraining examples",
    "start": "114460",
    "end": "120700"
  },
  {
    "text": "and then perform an update\non the individual losses.",
    "start": "120700",
    "end": "126906"
  },
  {
    "text": "OK, so here instead of going\nthrough the training set",
    "start": "126906",
    "end": "132280"
  },
  {
    "text": "and performing one\nupdate, we're going",
    "start": "132280",
    "end": "135850"
  },
  {
    "text": "to go through the training set.",
    "start": "135850",
    "end": "137230"
  },
  {
    "text": "And after each example, we're\ngoing to perform an update.",
    "start": "137230",
    "end": "141580"
  },
  {
    "text": "And this is going to be a\nlot faster in terms of having",
    "start": "141580",
    "end": "146110"
  },
  {
    "text": "the number of updates be large.",
    "start": "146110",
    "end": "148870"
  },
  {
    "text": "Of course, there is a\ntrade-off because each",
    "start": "148870",
    "end": "151330"
  },
  {
    "text": "update itself is not going to be\nas high quality because it only",
    "start": "151330",
    "end": "156370"
  },
  {
    "text": "consists of one example as\nopposed to all the examples.",
    "start": "156370",
    "end": "161440"
  },
  {
    "text": "And that's it for\nstochastic gradient descent.",
    "start": "161440",
    "end": "164200"
  },
  {
    "text": "I want to talk about one small\nnote, which is the step size.",
    "start": "164200",
    "end": "169420"
  },
  {
    "text": "So recall that update\nincludes a step size which",
    "start": "169420",
    "end": "173650"
  },
  {
    "text": "determines how far in the\ndirection of the gradient",
    "start": "173650",
    "end": "177700"
  },
  {
    "text": "or away from the gradient\ndo you want to move.",
    "start": "177700",
    "end": "181450"
  },
  {
    "text": "OK, so what should eta be?",
    "start": "181450",
    "end": "183180"
  },
  {
    "text": "And in general, there's not\nreally a one satisfying answer",
    "start": "183180",
    "end": "186659"
  },
  {
    "text": "to this.",
    "start": "186660",
    "end": "187290"
  },
  {
    "text": "And it's usually\na hyperparameter",
    "start": "187290",
    "end": "188939"
  },
  {
    "text": "that has to be tuned\nvia trial and error.",
    "start": "188940",
    "end": "192070"
  },
  {
    "text": "But here are some\ngeneral guidance here.",
    "start": "192070",
    "end": "193990"
  },
  {
    "text": "So the step size has to\nbe greater or equal to 0.",
    "start": "193990",
    "end": "197460"
  },
  {
    "text": "And if it is small,\nthat means you're",
    "start": "197460",
    "end": "199950"
  },
  {
    "text": "taking little, little steps.",
    "start": "199950",
    "end": "201660"
  },
  {
    "text": "But that means your\nalgorithm is going",
    "start": "201660",
    "end": "203580"
  },
  {
    "text": "to be more stable and less\nlikely to bounce around.",
    "start": "203580",
    "end": "206880"
  },
  {
    "text": "And as you increase\neta larger and larger,",
    "start": "206880",
    "end": "210030"
  },
  {
    "text": "then you're taking\nmore aggressive steps",
    "start": "210030",
    "end": "212280"
  },
  {
    "text": "so you can move faster but\nperhaps at the risk of being",
    "start": "212280",
    "end": "216630"
  },
  {
    "text": "a bit more unstable.",
    "start": "216630",
    "end": "219060"
  },
  {
    "text": "So two typical strategies\nfor setting a step size.",
    "start": "219060",
    "end": "221760"
  },
  {
    "text": "One is using just a\nconstant step size.",
    "start": "221760",
    "end": "224280"
  },
  {
    "text": "We've used, so far,\neta equals 0.1--",
    "start": "224280",
    "end": "227400"
  },
  {
    "text": "kind of an arbitrary number--",
    "start": "227400",
    "end": "230010"
  },
  {
    "text": "or you can do a\ndecreasing step size",
    "start": "230010",
    "end": "232980"
  },
  {
    "text": "rate, where eta is 1 over\nthe number of updates",
    "start": "232980",
    "end": "237540"
  },
  {
    "text": "that you've made.",
    "start": "237540",
    "end": "239099"
  },
  {
    "text": "And the intuition here is\nthat, in the beginning,",
    "start": "239100",
    "end": "242380"
  },
  {
    "text": "you're far away\nfrom the optimum,",
    "start": "242380",
    "end": "244360"
  },
  {
    "text": "so you're going to move quickly.",
    "start": "244360",
    "end": "246128"
  },
  {
    "text": "You want to move quickly.",
    "start": "246128",
    "end": "247170"
  },
  {
    "text": "But as soon as you start\ngetting close to the optimum,",
    "start": "247170",
    "end": "250770"
  },
  {
    "text": "you want to slow down.",
    "start": "250770",
    "end": "251880"
  },
  {
    "start": "251880",
    "end": "255630"
  },
  {
    "text": "So now let us explore stochastic\ngradient descent in Python.",
    "start": "255630",
    "end": "261540"
  },
  {
    "text": "I'm going to code it up\nand see what happens, OK?",
    "start": "261540",
    "end": "268120"
  },
  {
    "text": "So remember last time,\nwe did gradient descent.",
    "start": "268120",
    "end": "270940"
  },
  {
    "text": "So I'm going to\ncopy this code over.",
    "start": "270940",
    "end": "274930"
  },
  {
    "text": "DescentHinge-- sorry,\nstochasticGradientDescent.",
    "start": "274930",
    "end": "282009"
  },
  {
    "text": "And what we're going to\ndo is modify this code",
    "start": "282010",
    "end": "285790"
  },
  {
    "text": "to make it do stochastic\ngradient descent.",
    "start": "285790",
    "end": "290640"
  },
  {
    "text": "OK, so just recall,\nlast time, we",
    "start": "290640",
    "end": "293130"
  },
  {
    "text": "set up some training examples.",
    "start": "293130",
    "end": "295290"
  },
  {
    "text": "We defined the loss\nfunction, and then we",
    "start": "295290",
    "end": "297450"
  },
  {
    "text": "had this generic\noptimization algorithm.",
    "start": "297450",
    "end": "300980"
  },
  {
    "text": "So now to really\ntell the difference",
    "start": "300980",
    "end": "302860"
  },
  {
    "text": "between gradient descent and\nstochastic gradient descent,",
    "start": "302860",
    "end": "305560"
  },
  {
    "text": "I'm going to make\na larger data set.",
    "start": "305560",
    "end": "308260"
  },
  {
    "text": "And I'm going to do it in\na way so that it's large",
    "start": "308260",
    "end": "310810"
  },
  {
    "text": "but it's structured so we\nknow what the right answer is.",
    "start": "310810",
    "end": "313600"
  },
  {
    "text": "Because, otherwise, how can we\nverify it did the right thing?",
    "start": "313600",
    "end": "317230"
  },
  {
    "text": "To do this-- this is kind\nof just a general trick--",
    "start": "317230",
    "end": "320560"
  },
  {
    "text": "is that you kind of generate\nsynthetic data from kind",
    "start": "320560",
    "end": "325012"
  },
  {
    "text": "of a ground truth,\nand then you try",
    "start": "325012",
    "end": "326470"
  },
  {
    "text": "to recover that ground truth.",
    "start": "326470",
    "end": "328170"
  },
  {
    "text": "So suppose we had some\ntrue weight vector.",
    "start": "328170",
    "end": "331930"
  },
  {
    "text": "This is our secret code, which\nis unknown to the learning",
    "start": "331930",
    "end": "335080"
  },
  {
    "text": "algorithm, but we hope that\nlearning algorithm will recover",
    "start": "335080",
    "end": "337900"
  },
  {
    "text": "this, and then we're going\nto define a function called",
    "start": "337900",
    "end": "341530"
  },
  {
    "text": "generate which uses this\ntrueW to generate an example.",
    "start": "341530",
    "end": "346400"
  },
  {
    "text": "So here I'm going to generate\nx, and I'm going to just sample",
    "start": "346400",
    "end": "351550"
  },
  {
    "text": "randomly a five-dimensional\nweight vector--",
    "start": "351550",
    "end": "357379"
  },
  {
    "text": "oh, sorry, an input point, and\nthen I'm going to set y to be",
    "start": "357380",
    "end": "363770"
  },
  {
    "text": "trueW.dot(x).",
    "start": "363770",
    "end": "366620"
  },
  {
    "text": "So the examples I'm\ngoing to generate",
    "start": "366620",
    "end": "369919"
  },
  {
    "text": "are generated from the\ntrue weight vector,",
    "start": "369920",
    "end": "372140"
  },
  {
    "text": "and then I'm just going\nto add some noise.",
    "start": "372140",
    "end": "375755"
  },
  {
    "text": "Do randn, OK?",
    "start": "375755",
    "end": "376370"
  },
  {
    "start": "376370",
    "end": "380110"
  },
  {
    "text": "And then I'm going to set the\ntraining examples to be just",
    "start": "380110",
    "end": "384610"
  },
  {
    "text": "generate for, let's say, one--",
    "start": "384610",
    "end": "389770"
  },
  {
    "text": "let's do 1 million examples.",
    "start": "389770",
    "end": "391780"
  },
  {
    "text": "That's a lot of examples.",
    "start": "391780",
    "end": "394600"
  },
  {
    "text": "All right, so let's see\nwhat this data looks like.",
    "start": "394600",
    "end": "398120"
  },
  {
    "text": "So I'm going to\nprint out x and y",
    "start": "398120",
    "end": "403160"
  },
  {
    "text": "and just to see\nwhat is coming out--",
    "start": "403160",
    "end": "405665"
  },
  {
    "start": "405665",
    "end": "408730"
  },
  {
    "text": "oops, I had a typo here.",
    "start": "408730",
    "end": "413180"
  },
  {
    "text": "OK, so here is the data set\nthat we are going to train on.",
    "start": "413180",
    "end": "418160"
  },
  {
    "text": "So, example, x is a\nfive-dimensional vector,",
    "start": "418160",
    "end": "422600"
  },
  {
    "text": "and the output is a scalar.",
    "start": "422600",
    "end": "425600"
  },
  {
    "text": "A lot of examples here.",
    "start": "425600",
    "end": "427010"
  },
  {
    "start": "427010",
    "end": "430410"
  },
  {
    "text": "All right, so I need to update\nthe feature vector to be just",
    "start": "430410",
    "end": "435180"
  },
  {
    "text": "x, the identity, and here I'm\ngoing to-- the initial weight",
    "start": "435180",
    "end": "440130"
  },
  {
    "text": "vector has to match\nthe dimensionality",
    "start": "440130",
    "end": "442440"
  },
  {
    "text": "of the true weight vector,\nand then everything",
    "start": "442440",
    "end": "445230"
  },
  {
    "text": "else, the training\nloss and gradient,",
    "start": "445230",
    "end": "446962"
  },
  {
    "text": "I'm going to leave alone, OK?",
    "start": "446962",
    "end": "448170"
  },
  {
    "start": "448170",
    "end": "451610"
  },
  {
    "text": "So now let's uncomment this line\nand let's run gradient descent.",
    "start": "451610",
    "end": "458659"
  },
  {
    "text": "Let's see what happens here.",
    "start": "458660",
    "end": "462340"
  },
  {
    "text": "OK, so it's going to\ngenerate the data.",
    "start": "462340",
    "end": "465710"
  },
  {
    "text": "And now to compute\na single gradient,",
    "start": "465710",
    "end": "468259"
  },
  {
    "text": "it has to enumerate over\none million examples.",
    "start": "468260",
    "end": "473370"
  },
  {
    "text": "So this is going\nto be quite slow.",
    "start": "473370",
    "end": "475669"
  },
  {
    "text": "I'll finish the first epoch,\nand it has some values.",
    "start": "475670",
    "end": "482980"
  },
  {
    "text": "And then the second\nepoch, and it seems",
    "start": "482980",
    "end": "486700"
  },
  {
    "text": "like it's making some progress.",
    "start": "486700",
    "end": "489130"
  },
  {
    "text": "Remember, we want to see if\nthis can hit 1, 2, 3, 4, 5.",
    "start": "489130",
    "end": "493330"
  },
  {
    "text": "The loss is going\ndown, which is good,",
    "start": "493330",
    "end": "496310"
  },
  {
    "text": "and it seems like it's moving\nin the right direction.",
    "start": "496310",
    "end": "498700"
  },
  {
    "text": "But it's pretty\nslow, and I'm here",
    "start": "498700",
    "end": "501937"
  },
  {
    "text": "so I'm just going to\nstop it there because I",
    "start": "501937",
    "end": "503770"
  },
  {
    "text": "don't want to wait forever.",
    "start": "503770",
    "end": "506180"
  },
  {
    "text": "OK, so now let's do\nstochastic gradient descent.",
    "start": "506180",
    "end": "509759"
  },
  {
    "text": "So first, I need to change\nthe interface because gradient",
    "start": "509760",
    "end": "515210"
  },
  {
    "text": "descent only had access to\nF and the gradient of F.",
    "start": "515210",
    "end": "518900"
  },
  {
    "text": "And now stochastic\ngradient descent",
    "start": "518900",
    "end": "520520"
  },
  {
    "text": "needs to assess\nindividual losses.",
    "start": "520520",
    "end": "523860"
  },
  {
    "text": "So what I'm going to do is I'm\ngoing to define a stochastic--",
    "start": "523860",
    "end": "529160"
  },
  {
    "text": "or actually I just call\nthis the loss of w.",
    "start": "529160",
    "end": "533959"
  },
  {
    "text": "I'm going to use i here to\ndenote an index into one",
    "start": "533960",
    "end": "540500"
  },
  {
    "text": "of these terms and the sum.",
    "start": "540500",
    "end": "542480"
  },
  {
    "text": "So what the loss\nis going to be is",
    "start": "542480",
    "end": "545360"
  },
  {
    "text": "it's just going to be\none of these terms.",
    "start": "545360",
    "end": "549950"
  },
  {
    "text": "And the term I'm\ngoing to select out",
    "start": "549950",
    "end": "552050"
  },
  {
    "text": "is just the ith data point, OK?",
    "start": "552050",
    "end": "556500"
  },
  {
    "text": "And similarly, the\ngradient of the loss",
    "start": "556500",
    "end": "561630"
  },
  {
    "text": "is going to be just the gradient\nbut for the ith data point.",
    "start": "561630",
    "end": "572980"
  },
  {
    "text": "And this also takes\nin the index i.",
    "start": "572980",
    "end": "577440"
  },
  {
    "text": "So now if I feed in\ni for various values,",
    "start": "577440",
    "end": "580050"
  },
  {
    "text": "I can access the loss and\nthe gradient of that loss",
    "start": "580050",
    "end": "583140"
  },
  {
    "text": "function for any\ngiven point vector.",
    "start": "583140",
    "end": "586660"
  },
  {
    "text": "All right, so now let's go over\nto the optimization algorithm",
    "start": "586660",
    "end": "591759"
  },
  {
    "text": "and let me do stochastic\ngradient descent.",
    "start": "591760",
    "end": "595870"
  },
  {
    "text": "OK, so I'm going to call this\nstochasticGradientDescent,",
    "start": "595870",
    "end": "601750"
  },
  {
    "text": "and I'm going to call this--",
    "start": "601750",
    "end": "603178"
  },
  {
    "text": "just to distinguish\nthings, I'm going",
    "start": "603178",
    "end": "604720"
  },
  {
    "text": "to use lowercase f for\nindividual components",
    "start": "604720",
    "end": "608680"
  },
  {
    "text": "of an objective function.",
    "start": "608680",
    "end": "611120"
  },
  {
    "text": "OK, so I'm going to\ninitialize the weight vector.",
    "start": "611120",
    "end": "614240"
  },
  {
    "text": "I'm going to use a different\nstep size here, just for fun.",
    "start": "614240",
    "end": "620610"
  },
  {
    "text": "I'm going to\ninitialize with 1.0--",
    "start": "620610",
    "end": "623015"
  },
  {
    "start": "623015",
    "end": "626555"
  },
  {
    "text": "actually, let me\ndo this instead.",
    "start": "626555",
    "end": "629610"
  },
  {
    "text": "I'm going to set\nthe step size to be",
    "start": "629610",
    "end": "632579"
  },
  {
    "text": "1 over the square root\nof number of updates.",
    "start": "632580",
    "end": "636600"
  },
  {
    "text": "And each time I\ndo an update, I'm",
    "start": "636600",
    "end": "639449"
  },
  {
    "text": "going to increase the\nnumber of updates.",
    "start": "639450",
    "end": "643260"
  },
  {
    "text": "So, actually, let me\ndo it in this order.",
    "start": "643260",
    "end": "646260"
  },
  {
    "text": "OK, so the number of\nupdates starts at 0.",
    "start": "646260",
    "end": "650520"
  },
  {
    "text": "And then remember in\nstochastic gradient descent,",
    "start": "650520",
    "end": "654660"
  },
  {
    "text": "I'm going to loop over\nthe number of components",
    "start": "654660",
    "end": "662440"
  },
  {
    "text": "of the objective function.",
    "start": "662440",
    "end": "665596"
  },
  {
    "text": "So 1-- 0 to n minus 1.",
    "start": "665596",
    "end": "669363"
  },
  {
    "text": "So another thing\nI'm going to have",
    "start": "669363",
    "end": "670779"
  },
  {
    "text": "to pass in is the\nnumber of components",
    "start": "670780",
    "end": "674860"
  },
  {
    "text": "that I'm going to\nuse to index into F.",
    "start": "674860",
    "end": "677980"
  },
  {
    "text": "And so now this is f of\nw, 1, gradient f of w, i,",
    "start": "677980",
    "end": "683949"
  },
  {
    "text": "and then I'm going to\nmove everything inward.",
    "start": "683950",
    "end": "688220"
  },
  {
    "text": "OK, so now to call\nthis function,",
    "start": "688220",
    "end": "694589"
  },
  {
    "text": "I'm going to run stochastic\ngradient descent.",
    "start": "694590",
    "end": "697560"
  },
  {
    "text": "And with just the loss and\nthe gradient of the loss,",
    "start": "697560",
    "end": "707130"
  },
  {
    "text": "I'm going to pass in n, which\nis a number of training examples",
    "start": "707130",
    "end": "711120"
  },
  {
    "text": "and initial weight vector, OK?",
    "start": "711120",
    "end": "714000"
  },
  {
    "text": "So let's just review\nwhat's going on here.",
    "start": "714000",
    "end": "715750"
  },
  {
    "text": "So stochastic gradient\ndescent takes a function",
    "start": "715750",
    "end": "719370"
  },
  {
    "text": "which can access individual\ncomponents of the objective,",
    "start": "719370",
    "end": "723600"
  },
  {
    "text": "initialize the weights, and then\niterate some number of times.",
    "start": "723600",
    "end": "729339"
  },
  {
    "text": "And in each epoch, it's going\nto loop over all the examples,",
    "start": "729340",
    "end": "736650"
  },
  {
    "text": "compute the value,\ncompute the gradient,",
    "start": "736650",
    "end": "739560"
  },
  {
    "text": "and then it's going to\ndo a gradient update.",
    "start": "739560",
    "end": "745050"
  },
  {
    "text": "And here I'm using\na step size, which",
    "start": "745050",
    "end": "747149"
  },
  {
    "text": "is 1 over the number of\nupdates that I've made so far.",
    "start": "747150",
    "end": "752600"
  },
  {
    "text": "OK, so let's see stochastic\ngradient descent in action now.",
    "start": "752600",
    "end": "759300"
  },
  {
    "text": "I have two returns here,\nso there's a syntax error.",
    "start": "759300",
    "end": "763570"
  },
  {
    "text": "Let me fix that.",
    "start": "763570",
    "end": "764460"
  },
  {
    "start": "764460",
    "end": "767270"
  },
  {
    "text": "So now it's going through\n1 million examples-- oh, I",
    "start": "767270",
    "end": "770500"
  },
  {
    "text": "need to import math as well.",
    "start": "770500",
    "end": "772675"
  },
  {
    "start": "772675",
    "end": "776519"
  },
  {
    "text": "So it's going to loop\nover 1 million examples,",
    "start": "776520",
    "end": "780530"
  },
  {
    "text": "but each example it's\ngoing to perform an update.",
    "start": "780530",
    "end": "783530"
  },
  {
    "text": "And so when it prints\nout, it's going",
    "start": "783530",
    "end": "785330"
  },
  {
    "text": "to have already taken 1 million\nsteps of stochastic gradient",
    "start": "785330",
    "end": "790190"
  },
  {
    "text": "descent.",
    "start": "790190",
    "end": "791430"
  },
  {
    "text": "And look at what happened here.",
    "start": "791430",
    "end": "794690"
  },
  {
    "text": "So after the first\nstep, it's already",
    "start": "794690",
    "end": "797710"
  },
  {
    "text": "quite close to 1, 2, 3, 4, 5.",
    "start": "797710",
    "end": "800170"
  },
  {
    "text": "And objective in the--",
    "start": "800170",
    "end": "803647"
  },
  {
    "text": "I guess the function\nof value doesn't really",
    "start": "803647",
    "end": "805480"
  },
  {
    "text": "mean as much because it's\nonly of an individual point.",
    "start": "805480",
    "end": "808250"
  },
  {
    "text": "But you can see that\nthe weight vector",
    "start": "808250",
    "end": "811030"
  },
  {
    "text": "is converging quite nicely.",
    "start": "811030",
    "end": "814960"
  },
  {
    "text": "And this shows that stochastic\ngradient descent, just even",
    "start": "814960",
    "end": "818610"
  },
  {
    "text": "sometimes with one pass\nof a training data,",
    "start": "818610",
    "end": "820680"
  },
  {
    "text": "can get much closer\nto the optimum",
    "start": "820680",
    "end": "823649"
  },
  {
    "text": "than if you were to\ndo many, many rounds",
    "start": "823650",
    "end": "826740"
  },
  {
    "text": "of gradient descent.",
    "start": "826740",
    "end": "827700"
  },
  {
    "start": "827700",
    "end": "831110"
  },
  {
    "start": "830000",
    "end": "904000"
  },
  {
    "text": "OK, so that was stochastic\ngradient descent in Python.",
    "start": "831110",
    "end": "836700"
  },
  {
    "text": "So let's summarize here.",
    "start": "836700",
    "end": "838660"
  },
  {
    "text": "So we want to optimize\nthis training loss, which",
    "start": "838660",
    "end": "842310"
  },
  {
    "text": "is an average over the\nper example losses,",
    "start": "842310",
    "end": "846190"
  },
  {
    "text": "and we looked at\ngradient descent,",
    "start": "846190",
    "end": "849070"
  },
  {
    "text": "which takes a step on the\ngradient of the training loss.",
    "start": "849070",
    "end": "854790"
  },
  {
    "text": "And we also looked at\nstochastic gradient descent,",
    "start": "854790",
    "end": "857550"
  },
  {
    "text": "which picked up individual\nexamples and updated",
    "start": "857550",
    "end": "861149"
  },
  {
    "text": "on computing-- after\ncomputing the gradient",
    "start": "861150",
    "end": "863490"
  },
  {
    "text": "of individual examples.",
    "start": "863490",
    "end": "866470"
  },
  {
    "text": "And now on this example, we've\nshown that stochastic gradient",
    "start": "866470",
    "end": "869740"
  },
  {
    "text": "descents wins.",
    "start": "869740",
    "end": "870940"
  },
  {
    "text": "And the key idea behind\nstochastic updates",
    "start": "870940",
    "end": "873220"
  },
  {
    "text": "is that it's not about quality.",
    "start": "873220",
    "end": "875230"
  },
  {
    "text": "It's about quantity.",
    "start": "875230",
    "end": "877240"
  },
  {
    "text": "So maybe not a\ngeneral life lesson,",
    "start": "877240",
    "end": "879640"
  },
  {
    "text": "but it seems like, in this case,\nit is more wise to keep in mind",
    "start": "879640",
    "end": "884710"
  },
  {
    "text": "what you're trying to do, which\nis optimize this objective,",
    "start": "884710",
    "end": "888930"
  },
  {
    "text": "rather than compute\nthe gradient, which",
    "start": "888930",
    "end": "892050"
  },
  {
    "text": "is only a means to an end.",
    "start": "892050",
    "end": "895190"
  },
  {
    "text": "OK, so that concludes the module\non stochastic gradient descent.",
    "start": "895190",
    "end": "898310"
  },
  {
    "text": "Thanks for listening.",
    "start": "898310",
    "end": "900550"
  },
  {
    "start": "900550",
    "end": "904000"
  }
]