[
  {
    "start": "0",
    "end": "5270"
  },
  {
    "text": "Hello. Thank you for joining\nCS 25 Transformers. Today's the last class.",
    "start": "5270",
    "end": "10750"
  },
  {
    "text": "Today, we have Loubna, who is\na machine learning engineer in the science team\nat Hugging Face,",
    "start": "10750",
    "end": "16290"
  },
  {
    "text": "working on large language models\nfor code and synthetic data generation.",
    "start": "16290",
    "end": "21660"
  },
  {
    "text": "She is part of the core\nteam at the BigCode project and has co-authored the Stack\ndataset and the StarCoder models",
    "start": "21660",
    "end": "30210"
  },
  {
    "text": "for code generation. Thank you so much for\ncoming to our talk today. And as always, the attendance\nlink and the Slido questions",
    "start": "30210",
    "end": "39270"
  },
  {
    "text": "are on our website and we'll be\ntaking questions after the talk. Thank you, and you\ncan take it off now.",
    "start": "39270",
    "end": "47130"
  },
  {
    "text": "Hi. Thank you for the introduction. So I'm Loubna. I'm a machine learning engineer\nat Hugging Face in the science",
    "start": "47130",
    "end": "53680"
  },
  {
    "text": "team and today, I'll\ntell you about the behind the scenes for training\nlarge language models.",
    "start": "53680",
    "end": "59690"
  },
  {
    "text": "And I will use the\nStarCoder model that our team has\ntrained as a use case.",
    "start": "59690",
    "end": "64874"
  },
  {
    "text": " So today's plan is very simple.",
    "start": "64874",
    "end": "69900"
  },
  {
    "text": "We're going to try to\nanswer this question. What does it take\nto train a good LLM? So it's one question,\nbut it's very loaded",
    "start": "69900",
    "end": "76670"
  },
  {
    "text": "and it has a lot of follow\nups. And as you will see, my slides will be a series\nof questions and answers.",
    "start": "76670",
    "end": "82045"
  },
  {
    "text": " So a few years ago,\na lot of people",
    "start": "82045",
    "end": "87130"
  },
  {
    "text": "thought that there was\nsome molten secret sauce to the song closed\nmodels like GPT-4",
    "start": "87130",
    "end": "92980"
  },
  {
    "text": "and that it will take the open\nsource community a lot of time to catch up because the open\nsource models that we had back",
    "start": "92980",
    "end": "98920"
  },
  {
    "text": "then were much smaller\nand less performance. But now, it seems that\nthe community figured out",
    "start": "98920",
    "end": "105100"
  },
  {
    "text": "most of the pieces for getting\nstrong LLMs as it was predicted in this Google memo that\nwas leaked and released",
    "start": "105100",
    "end": "112120"
  },
  {
    "text": "on semi analysis. For example, today we\nhave Llama 370b instruct,",
    "start": "112120",
    "end": "118690"
  },
  {
    "text": "which has almost the same\nperformance as GPT-4 but it unlocked so many use cases\nbecause the model weights are",
    "start": "118690",
    "end": "125320"
  },
  {
    "text": "open, the model can be quantized\nand can even run on a consumer desktop. It also allows the community to\nbuild very cool use cases on top",
    "start": "125320",
    "end": "133750"
  },
  {
    "text": "through fine tuning. So we've made a lot of\nprogress in the open field and this is not the only\nmodel that's out there.",
    "start": "133750",
    "end": "142060"
  },
  {
    "text": "We're now observing a rise\nof open LLMs in the company. More and more companies are\nembracing releasing models.",
    "start": "142060",
    "end": "150060"
  },
  {
    "text": "That was the case, for example,\nwith DeepMind's Gemma models and with Mistral's models\nand also other models",
    "start": "150060",
    "end": "157400"
  },
  {
    "text": "from GPT [INAUDIBLE]. Here, I put a plot\nfrom the LMSYS Arena,",
    "start": "157400",
    "end": "163760"
  },
  {
    "text": "which is the go to\nleaderboard for comparing abstract models nowadays.",
    "start": "163760",
    "end": "170239"
  },
  {
    "text": "It uses human evaluation. And you can see in this plot\nthat as we went from 2023 to May",
    "start": "170240",
    "end": "178740"
  },
  {
    "text": "24, the gap in performance\nbetween the closed models and the open models is\nshrinking and becoming smaller,",
    "start": "178740",
    "end": "186080"
  },
  {
    "text": "which is very promising. So we're on a very great\npath but there are still",
    "start": "186080",
    "end": "193569"
  },
  {
    "text": "a lot of limitations for this. And this is mainly due\nto releases missing out",
    "start": "193570",
    "end": "200859"
  },
  {
    "text": "important details about\nhow the data was processed and how the models were trained.",
    "start": "200860",
    "end": "206380"
  },
  {
    "text": "And this is usually the\ncase for two main reasons. The first one is to\navoid legal scrutiny,",
    "start": "206380",
    "end": "213040"
  },
  {
    "text": "because when companies\npublicly disclose the training data, if the training\nwas not done properly",
    "start": "213040",
    "end": "219280"
  },
  {
    "text": "and the copyrighted\nwere not respected, they risk facing\nlegal investigation.",
    "start": "219280",
    "end": "225160"
  },
  {
    "text": "The other reason for not\ndisclosing the details can be to maintain\na competitive edge.",
    "start": "225160",
    "end": "231920"
  },
  {
    "text": "So some companies want to\nbe the best at training LLMs so they don't want to give all\nthe details for their training.",
    "start": "231920",
    "end": "239140"
  },
  {
    "text": "Nevertheless, because we\nhave a lot of releases, I think we can still answer this\nquestion and put a lot of pieces",
    "start": "239140",
    "end": "244780"
  },
  {
    "text": "together. So what do we need\nto train a good LLM.",
    "start": "244780",
    "end": "249850"
  },
  {
    "text": "The first thing is\nprobably the model. You need to have a\ngood architecture.",
    "start": "249850",
    "end": "255050"
  },
  {
    "text": "And I think now,\ntransformers are the default, but there are also other\ninteresting architectures",
    "start": "255050",
    "end": "261160"
  },
  {
    "text": "like Mamba, which is\na state space model, or you can use mixture\nof experts, which can",
    "start": "261160",
    "end": "266350"
  },
  {
    "text": "be multiple transformer models. But I'm not going to\nspend a lot of time in this lecture on\nmodels because I think",
    "start": "266350",
    "end": "274090"
  },
  {
    "text": "it's a topic that's\nalready thoroughly explored and there are other aspects\nthat maybe deserve a little bit",
    "start": "274090",
    "end": "280870"
  },
  {
    "text": "more attention. So that was it for models. Then for GPUs, I\ndon't think there's",
    "start": "280870",
    "end": "286580"
  },
  {
    "text": "much I can tell you about that,\nexcept maybe I ask Jensen. But the part that they\nwere the most interested of",
    "start": "286580",
    "end": "294710"
  },
  {
    "text": "is the data, which I think\nis the backbone of LLMs because now, almost\neveryone is using",
    "start": "294710",
    "end": "301700"
  },
  {
    "text": "the same architecture and\nthe same training techniques. And for a given budget, data is\nwhat makes some models better",
    "start": "301700",
    "end": "309110"
  },
  {
    "text": "than the others. So it's really worth spending\ntime exploring this data and understanding how to\nget higher quality samples.",
    "start": "309110",
    "end": "317120"
  },
  {
    "text": "So now, we're going to try to\nanswer our previous question of how to train a good LLM by\nhow do we get good training",
    "start": "317120",
    "end": "323705"
  },
  {
    "text": "data?  And I think the answer\nto this is threefold.",
    "start": "323705",
    "end": "330430"
  },
  {
    "text": "First, we need to understand\nhow much data do we need? And then once we figured\nout the size of the data",
    "start": "330430",
    "end": "336690"
  },
  {
    "text": "that we need, where can we\nget this data and to clean it, which filtering\ntechniques make more sense",
    "start": "336690",
    "end": "342840"
  },
  {
    "text": "and will give us the\nbest performance. So to answer the first\none, the answer to that",
    "start": "342840",
    "end": "350130"
  },
  {
    "text": "is the scaling laws. You want to know which\ndata, how much data you want to train your model\non, but also what is",
    "start": "350130",
    "end": "356669"
  },
  {
    "text": "the optimal size of the model. And the scaling\nlaws try to study",
    "start": "356670",
    "end": "361710"
  },
  {
    "text": "the allocation of a\ncompute budget between data size and model size. This means should you\ntake a smaller model",
    "start": "361710",
    "end": "368370"
  },
  {
    "text": "and train it longer\nor take a larger model and train it on less data. And I'm going to present a brief\nhistory of the scaling laws",
    "start": "368370",
    "end": "379199"
  },
  {
    "text": "because I think it's really\ninteresting to see how the sizes of the models\nprogress through time and also",
    "start": "379200",
    "end": "385290"
  },
  {
    "text": "how the size of the data sets\nand the number of tokens we train on them have changed\nbecause there were really",
    "start": "385290",
    "end": "390879"
  },
  {
    "text": "some drastic changes in that. I think the first to\nestablish the scaling",
    "start": "390880",
    "end": "397510"
  },
  {
    "text": "laws were Kaplan from OpenAI. And they tried to fit the\nlaw as a function of--",
    "start": "397510",
    "end": "405670"
  },
  {
    "text": "the laws as a function of\nthe data size and model size. And they found that\nif you have a 10 times",
    "start": "405670",
    "end": "412390"
  },
  {
    "text": "increase in your compute, you\nshould increase your parameter count by 5.5, but\nyour training tokens,",
    "start": "412390",
    "end": "419170"
  },
  {
    "text": "you should only\nincrease them by 1.8. This means that if you have more\nresources to train your models,",
    "start": "419170",
    "end": "425259"
  },
  {
    "text": "you should make the model much\nlarger but the data is fine. You shouldn't\nincrease it that much.",
    "start": "425260",
    "end": "430990"
  },
  {
    "text": "And this is what led to\nmodels like GPT-3, which is 175 billion\nparameters, but it",
    "start": "430990",
    "end": "437770"
  },
  {
    "text": "was only trained on $300 billion\ntokens, which, if we think about it now, is really small.",
    "start": "437770",
    "end": "444400"
  },
  {
    "text": "Other models also followed\nthis, for example, like OPT, which was\nthe same size as GPT-3",
    "start": "444400",
    "end": "449650"
  },
  {
    "text": "and trained on a\nsimilar amount of data. There was also BLOOM. So all these models are\nactually very under trained.",
    "start": "449650",
    "end": "458020"
  },
  {
    "text": "Then the Chinchilla\nscaling laws came after and they revisited\nthe scaling laws.",
    "start": "458020",
    "end": "464440"
  },
  {
    "text": "And they found that\nthe reason Kaplan thought that data\nshould not be as scaled",
    "start": "464440",
    "end": "470590"
  },
  {
    "text": "as model size is because they\nused a fixed cosine scheduler for all their experiments.",
    "start": "470590",
    "end": "476300"
  },
  {
    "text": "So although they were\nchanging the data size, the cosine scheduler was fixed. This meant that for\nsome models, they",
    "start": "476300",
    "end": "482919"
  },
  {
    "text": "were underestimated\nbecause they were not using the correct cosine that\ncorresponded to the data size.",
    "start": "482920",
    "end": "489970"
  },
  {
    "text": "This led to false conclusions\nand the Chinchilla can now give us new\nscaling laws that",
    "start": "489970",
    "end": "496810"
  },
  {
    "text": "say that you should\nscale your data and your model size equally. And in their paper,\nthey train a 65 billion",
    "start": "496810",
    "end": "505389"
  },
  {
    "text": "model on 1.6 trillion tokens,\nwhich is the Chinchilla optimal point.",
    "start": "505390",
    "end": "510590"
  },
  {
    "text": "And they outperform much larger\nmodels like GPT-3 and Gopher, which was over 200\nbillion parameters.",
    "start": "510590",
    "end": "519909"
  },
  {
    "text": "So here, for example,\nI have a plot which shows what the\nscaling laws try to do. For example, here you\nhave IsoFLOP curves, which",
    "start": "519909",
    "end": "527470"
  },
  {
    "text": "each curve uses a fixed budget. And then you try to find\nthe sweet spot, which is the optimal for\nyour budget allocation",
    "start": "527470",
    "end": "534730"
  },
  {
    "text": "and it tells you what\nyour model size should be and what your data\nsize should be.",
    "start": "534730",
    "end": "540610"
  },
  {
    "text": "And as you can see here,\nif we try to fit the loss, we can see that there's a linear\nincrease for data and also model",
    "start": "540610",
    "end": "547900"
  },
  {
    "text": "size.  In this scheme, I\ntried to show how",
    "start": "547900",
    "end": "555190"
  },
  {
    "text": "we've moved from the Chinchilla\nscaling laws to today's models. And you can see that, for\nexample, the Chinchilla model,",
    "start": "555190",
    "end": "562870"
  },
  {
    "text": "which is 60 billion\nparameters, was trained on less than 2 trillion tokens. But then after\nthat, we have Llama,",
    "start": "562870",
    "end": "569230"
  },
  {
    "text": "which was released last year,\nand it was just a 7B model and it was trained on as much\ndata as the Chinchilla model.",
    "start": "569230",
    "end": "576399"
  },
  {
    "text": "So it was trained way past\nthe Chinchilla optimal point. And we might be wondering,\nwhy is that the case?",
    "start": "576400",
    "end": "584920"
  },
  {
    "text": "Did Meta not use their compute\nbudgets in an optimal way? And the answer to that is that\ncompute optimal is not always",
    "start": "584920",
    "end": "593620"
  },
  {
    "text": "optimal because when\nyou train a model, you don't only care\nabout what you're",
    "start": "593620",
    "end": "598750"
  },
  {
    "text": "going to spend in training, but\nyou also care about inference. And the model is\ntrained one time",
    "start": "598750",
    "end": "604280"
  },
  {
    "text": "but inference is for more the\nmodel is going to be served. So you want to save\nsome cost in that. This makes it that people prefer\ntraining smaller models longer",
    "start": "604280",
    "end": "612980"
  },
  {
    "text": "than actually using\nmuch larger models that are trained on less data. So this was the\ncase for Llama 1,",
    "start": "612980",
    "end": "618380"
  },
  {
    "text": "for other models like\nMistral, but also for Llama 3 which went even further and\ntrained not on 1 trillion tokens",
    "start": "618380",
    "end": "625069"
  },
  {
    "text": "but on 15 trillion tokens. And if you check\nthe archive paper, the loss kept going down and\nalso the downstream evaluations",
    "start": "625070",
    "end": "634250"
  },
  {
    "text": "as the model training\nis kept improving. I think this is\nreally interesting",
    "start": "634250",
    "end": "639380"
  },
  {
    "text": "because some people\nmisunderstood the Chinchilla scaling laws as compute\noptimal, optimal but that's not the case because\nthe inference cost is not",
    "start": "639380",
    "end": "648200"
  },
  {
    "text": "considered. So for example, this is the\ncost for training GPT-4.",
    "start": "648200",
    "end": "653240"
  },
  {
    "text": "It is said that it's estimated\nthat $100 million, but also the inference is very\nexpensive and the larger",
    "start": "653240",
    "end": "660230"
  },
  {
    "text": "the model becomes, the more\ntime it takes to process tokens. So inference, the scaling laws\ndon't take the inference costs",
    "start": "660230",
    "end": "668540"
  },
  {
    "text": "into consideration. And if we do take the\ninference cost, which is the case for most\npeople because they want",
    "start": "668540",
    "end": "675950"
  },
  {
    "text": "to use these models\nin inference, you might prefer using smaller\nmodels and training them longer.",
    "start": "675950",
    "end": "681620"
  },
  {
    "text": "And we do that, we're not\nrespecting the Chinchilla scaling laws, so we're\nchoosing to pay what",
    "start": "681620",
    "end": "686900"
  },
  {
    "text": "we call a compute overhead. It's a sacrifice that you\ndo during the training.",
    "start": "686900",
    "end": "692130"
  },
  {
    "text": "You choose to pay more, but\nthis will have a benefit during inference\nbecause you will save a lot of cost and money.",
    "start": "692130",
    "end": "699899"
  },
  {
    "text": "And there's this very\ninteresting blog post about Hartman's law, which tries\nto measure the compute overhead",
    "start": "699900",
    "end": "709740"
  },
  {
    "text": "that you will be paying when you\nchoose to train a small model. For example, here, there's\nthe space on Hugging",
    "start": "709740",
    "end": "715440"
  },
  {
    "text": "Face where you can\ninput the model size and what data sets\nyou want to train on, and it will show you where you\nare regarding the Chinchilla",
    "start": "715440",
    "end": "724019"
  },
  {
    "text": "optimal point. So, for example, if we take\n7B model and we train it on 1 billion tokens, you can\nsee that we are here.",
    "start": "724020",
    "end": "730810"
  },
  {
    "text": "It's the red dots and it's\nbefore the Chinchilla optimal model. And this gives approximately,\nI think, a 40% overhead.",
    "start": "730810",
    "end": "740480"
  },
  {
    "text": "But then during inference, as\nit shows here in the table-- sorry, it was 13% overhead, but\nthere's almost 50% saving costs.",
    "start": "740480",
    "end": "748980"
  },
  {
    "text": "So that's something that's\nalmost everyone is doing now, which is why we see models\nthat are much, much smaller",
    "start": "748980",
    "end": "754590"
  },
  {
    "text": "than one or two years ago.  For further reading, there are\nsome very interesting papers",
    "start": "754590",
    "end": "762610"
  },
  {
    "text": "about scaling laws. For example, this paper called\nScaling Data-Constrained Language Models, which shows\nthat if you are limited",
    "start": "762610",
    "end": "771850"
  },
  {
    "text": "in your data size,\nlet's say, for example, you wanted to finish 7B on\n10 trillion tokens but you",
    "start": "771850",
    "end": "777940"
  },
  {
    "text": "don't have these\n10 trillion tokens, this paper says that you can\nbasically repeat your data up",
    "start": "777940",
    "end": "783100"
  },
  {
    "text": "to four times, so four epochs,\nand you will get similar performance as if you\nuse the unique tokens.",
    "start": "783100",
    "end": "788840"
  },
  {
    "text": "So for example, instead of\nusing a trillion tokens unique, you could use just 2 and\nrepeat them four times",
    "start": "788840",
    "end": "795190"
  },
  {
    "text": "and you get almost\nthe same performance as if these tokens were unique. And this is especially\nuseful for some domains",
    "start": "795190",
    "end": "802990"
  },
  {
    "text": "where we almost exhaust all the\ndata that's publicly available.",
    "start": "802990",
    "end": "808000"
  },
  {
    "text": "As I will show you later,\nthe Stack v2, which is a code data set\nthat we released, I think it has almost all the\ncode that is available publicly",
    "start": "808000",
    "end": "816740"
  },
  {
    "text": "so it's going to be very hard\nto scrape and get more code. And if you want to\ntrain models longer,",
    "start": "816740",
    "end": "822800"
  },
  {
    "text": "the only option is to actually\nrepeat the data during training. And this is good news\nbecause repeating the data up",
    "start": "822800",
    "end": "829670"
  },
  {
    "text": "to four times is\nactually significant. Another paper that I\nthink is interesting",
    "start": "829670",
    "end": "836089"
  },
  {
    "text": "when it comes to scaling\nlaws is the DeepSeek LLM.",
    "start": "836090",
    "end": "841370"
  },
  {
    "text": "They try to establish\nnew scaling laws that are suited for the data because\nthey found that the scaling",
    "start": "841370",
    "end": "849410"
  },
  {
    "text": "behavior is highly dependent\non the data quality. So they tried different data\nsubsets, different filterings,",
    "start": "849410",
    "end": "856399"
  },
  {
    "text": "and they found that the\nscaling laws were changing. So this is very important\nbecause up until now,",
    "start": "856400",
    "end": "861870"
  },
  {
    "text": "we were using the Chinchilla,\nbut the Chinchilla was using fixed data sets. They are not necessarily the\nones that we are using now,",
    "start": "861870",
    "end": "868550"
  },
  {
    "text": "so it's really important\nto be aware of that. And this is why DeepSeek\ntried to come up with their own scaling laws\nthat work for their data sets.",
    "start": "868550",
    "end": "876660"
  },
  {
    "text": "And they also conclude that\nwhen you have higher quality data sets, maybe more compute\nshould be allocated to the model",
    "start": "876660",
    "end": "884360"
  },
  {
    "text": "size and not to the data size. So these are interesting\nthings to keep in mind",
    "start": "884360",
    "end": "889760"
  },
  {
    "text": "when it comes to scaling LLMs. ",
    "start": "889760",
    "end": "894779"
  },
  {
    "text": "So we have answered the\nfirst question, I hope, how much data to train LLMs.",
    "start": "894780",
    "end": "901290"
  },
  {
    "text": "So let's say now you have\nyour compute budget, a fixed number of GPUs for a\ncertain amount of days,",
    "start": "901290",
    "end": "907890"
  },
  {
    "text": "and you also know approximately\nhow much data you want to use. The question is that, where\ndo you find this type of data?",
    "start": "907890",
    "end": "917220"
  },
  {
    "text": "For example, Llama 3 was\ntrained on 15 trillion tokens, but where do you get\n15 trillion tokens?",
    "start": "917220",
    "end": "922660"
  },
  {
    "text": "That's a huge number. To get this data,\nthe two main sources",
    "start": "922660",
    "end": "929700"
  },
  {
    "text": "where you can actually get\nvery large volume of data are the web and\nthen GitHub code.",
    "start": "929700",
    "end": "936720"
  },
  {
    "text": "There are some other curated\nsources that are of high quality but are much smaller, like\nWikipedia books, archive,",
    "start": "936720",
    "end": "943860"
  },
  {
    "text": "or StackExchange. You can also get\ndata, a new type that's been very\ntrendy recently,",
    "start": "943860",
    "end": "951210"
  },
  {
    "text": "which is synthetic data. But let's first start\nwith the sources where",
    "start": "951210",
    "end": "956680"
  },
  {
    "text": "you can get very large volumes. The first one is web data.",
    "start": "956680",
    "end": "962130"
  },
  {
    "text": "So that's basically web pages. And usually, people\nto create these data",
    "start": "962130",
    "end": "968280"
  },
  {
    "text": "sets, they start\nfrom CommonCrawl, which is a public repository\nof crawled web pages.",
    "start": "968280",
    "end": "975029"
  },
  {
    "text": "CommonCrawl crawls\npages regularly and they publish dumps\nevery few months.",
    "start": "975030",
    "end": "980928"
  },
  {
    "text": "But if you start\nfrom there, you will need to do some heavy filtering\nat a very large scale.",
    "start": "980928",
    "end": "986050"
  },
  {
    "text": "For example, just the latest\ndump has over 400 terabytes, and they have almost 95 dumps.",
    "start": "986050",
    "end": "992730"
  },
  {
    "text": "So that's not a very\neasy task, and you will need to have a lot\nof resources and a team",
    "start": "992730",
    "end": "1000350"
  },
  {
    "text": "to be able to do that crawling. The other option is to use\nan existing filtered web",
    "start": "1000350",
    "end": "1007279"
  },
  {
    "text": "data set where researchers\nalready filtered the CommonCrawl and released them.",
    "start": "1007280",
    "end": "1012360"
  },
  {
    "text": "And luckily, we\ndo have data sets that are very large\nand well filtered. One of them is the web data\nFineWeb, that was recently",
    "start": "1012360",
    "end": "1021509"
  },
  {
    "text": "released by Hugging\nFace, and it has 15 trillion tokens of web data.",
    "start": "1021510",
    "end": "1027199"
  },
  {
    "text": " It's not just a large\ndata set, but it also",
    "start": "1027199",
    "end": "1032929"
  },
  {
    "text": "has the best performance among\nthe publicly available data sets. And here, for example,\nit shows the performance,",
    "start": "1032930",
    "end": "1040400"
  },
  {
    "text": "which is an aggregation over\nmultiple popular benchmarks for NLP like HellaSwag,\nMMLU, and others.",
    "start": "1040400",
    "end": "1049170"
  },
  {
    "text": "And it's averages them and\ncompares to other data sets like C4, RefinedWeb,\nSlimPajama, and The Pile.",
    "start": "1049170",
    "end": "1055409"
  },
  {
    "text": " So that was for web. So you can get 15\ntrillion tokens easily.",
    "start": "1055410",
    "end": "1062440"
  },
  {
    "text": "And then for code data, we\nhave released the stack data set, which is the largest\ndata set of open source code.",
    "start": "1062440",
    "end": "1072330"
  },
  {
    "text": "This data set comes\nin two versions. Version one consisted of six\nterabytes of permissive code.",
    "start": "1072330",
    "end": "1078760"
  },
  {
    "text": "And how we build\nthis data set is that we first cloned all the\npublic repositories on GitHub.",
    "start": "1078760",
    "end": "1085540"
  },
  {
    "text": "So this gave us over\n130 repositories and 100 terabytes of data.",
    "start": "1085540",
    "end": "1091440"
  },
  {
    "text": "But we don't want all of\nthat data because a lot of it can be configs or extensions\nthat we don't need",
    "start": "1091440",
    "end": "1097590"
  },
  {
    "text": "or languages that are\nno longer maintained. So we did some file\nextension filtering",
    "start": "1097590",
    "end": "1103290"
  },
  {
    "text": "and we ended up with almost\n90 terabytes of data. After that, we\nfiltered repositories",
    "start": "1103290",
    "end": "1109409"
  },
  {
    "text": "based on their licenses so we\ncan have permissive licenses like Apache 2 or MIT.",
    "start": "1109410",
    "end": "1115620"
  },
  {
    "text": "You can have more restrictive\nlicenses like GPL. So we filtered all the\nrepositories that did not",
    "start": "1115620",
    "end": "1121780"
  },
  {
    "text": "have a permissive license. And after that, we\ndid a deduplication to remove files\nthat are similar.",
    "start": "1121780",
    "end": "1128210"
  },
  {
    "text": "So we ended up with almost 3\nterabytes of deduplicated data. ",
    "start": "1128210",
    "end": "1134370"
  },
  {
    "text": "The stack comes also with a\nvery cool tool for opt out. This tool is basically a\nspace where you can go.",
    "start": "1134370",
    "end": "1142570"
  },
  {
    "text": "You can type your\nGitHub username and it tells you if you have\nany of your GitHub repositories in the data set.",
    "start": "1142570",
    "end": "1149130"
  },
  {
    "text": "And if that's the\ncase, there's also an option to fill\na form and request to be removed from all the\nfuture trainings of BigCode.",
    "start": "1149130",
    "end": "1157350"
  },
  {
    "text": "So we did that for the second\none, but also for the stack v2. In the v2 is a much\nlarger and enhanced",
    "start": "1157350",
    "end": "1164790"
  },
  {
    "text": "data set compared to the v1. This time, instead of\ncloning GitHub repositories,",
    "start": "1164790",
    "end": "1171270"
  },
  {
    "text": "we went through\nSoftware Heritage, which is an archive of code. They already did the scraping\nand we just extracted the data",
    "start": "1171270",
    "end": "1178799"
  },
  {
    "text": "from their archive. And we ended up, after\nall the filtering,",
    "start": "1178800",
    "end": "1183840"
  },
  {
    "text": "with almost 1\ntrillion tokens, which is a lot compared to the\nv1 where we got around 200",
    "start": "1183840",
    "end": "1190450"
  },
  {
    "text": "billion tokens at the end. We also added some high quality\nresources like GitHub issues,",
    "start": "1190450",
    "end": "1197200"
  },
  {
    "text": "math and coding data\nsets, and pull requests. So these data sets, the\nstack v1, the stack v2",
    "start": "1197200",
    "end": "1203170"
  },
  {
    "text": "can be used to train LLMs on\ncode or to train general LLMs and include code as a subset\nof the general web data.",
    "start": "1203170",
    "end": "1212820"
  },
  {
    "text": "This shows how the stack\nv2 compares to the v1. And you can see that\nbefore filtering, it's almost 10 times larger\nand after filtering, it's",
    "start": "1212820",
    "end": "1220560"
  },
  {
    "text": "four or five times larger. So I talk about how to get web\ndata, how to get code data,",
    "start": "1220560",
    "end": "1227970"
  },
  {
    "text": "and then I also\nmentioned synthetic data. And it's this year and last year\nthat synthetic data became very",
    "start": "1227970",
    "end": "1234960"
  },
  {
    "text": "important for LLM training. And I think that in\nthe next few years, it will become even\nmore important.",
    "start": "1234960",
    "end": "1241169"
  },
  {
    "text": "And I think this was\nmainly sparked by the five series of models by Microsoft.",
    "start": "1241170",
    "end": "1247140"
  },
  {
    "text": "The first paper was called\ntextbooks are all you need. And they basically generated\nsynthetic textbooks using",
    "start": "1247140",
    "end": "1255059"
  },
  {
    "text": "GPT-3.5 and GPT-4, and\nthey tried to build a new pre-training corpus that is\nsynthetic and they were able",
    "start": "1255060",
    "end": "1263280"
  },
  {
    "text": "to match and outperform models\nthat are trained on web data sets.",
    "start": "1263280",
    "end": "1268679"
  },
  {
    "text": "So this model was trained on\nalmost entirely synthetic data. But now, some of the\nvery popular LLMs",
    "start": "1268680",
    "end": "1275049"
  },
  {
    "text": "are using synthetic data as\npart of their pre-training mix. For example, Claude\n3, in the model car,",
    "start": "1275050",
    "end": "1282190"
  },
  {
    "text": "they say that they\ngenerate data internally and they include it\nin the pre-training.",
    "start": "1282190",
    "end": "1287320"
  },
  {
    "text": "This is also the\ncase for Llama 3, where they use the LLMs\nto build classifiers",
    "start": "1287320",
    "end": "1292690"
  },
  {
    "text": "that would annotate samples and\nonly keep the high quality ones. But they also generated\nsynthetic content",
    "start": "1292690",
    "end": "1299290"
  },
  {
    "text": "to improve performance on coding\nand reasoning and long contexts.",
    "start": "1299290",
    "end": "1305380"
  },
  {
    "text": "So synthetic data\nis a very new topic, but it seems really interesting.",
    "start": "1305380",
    "end": "1311380"
  },
  {
    "text": "I'm personally working also\non that as Hugging Face. We recently released a data\nset called Cosmopedia, which",
    "start": "1311380",
    "end": "1317830"
  },
  {
    "text": "was the largest data\nset of synthetic texts, and it had almost\n25 billion tokens.",
    "start": "1317830",
    "end": "1325250"
  },
  {
    "text": "And instead of using\nclosed models like GPT-4, it used an open source\nmodel, which is Mixtral-8x7B.",
    "start": "1325250",
    "end": "1332910"
  },
  {
    "text": "And we also released\na blog post that explains how we\ncreated this data set",
    "start": "1332910",
    "end": "1338970"
  },
  {
    "text": "because it can be very tricky\nto get very diverse samples. So we used an\napproach where we had",
    "start": "1338970",
    "end": "1346620"
  },
  {
    "text": "80% of the data that\ncomes from the web, and then we try to use\nthese web samples to build",
    "start": "1346620",
    "end": "1352500"
  },
  {
    "text": "new prompts that ask models\nto generate text books that are related to\nthese web samples, but while giving\nthem more context",
    "start": "1352500",
    "end": "1360039"
  },
  {
    "text": "so we can limit the generations. For example, we can have a\ntopic that is a mathematics,",
    "start": "1360040",
    "end": "1367680"
  },
  {
    "text": "and then we have web samples\nthat are related to mathematics. And each time we give the\nmodel a prompt, generate",
    "start": "1367680",
    "end": "1373379"
  },
  {
    "text": "a textbook in the field\nof mathematics that is related to this web sample. And the more web samples we\nadd, the more diversity we add.",
    "start": "1373380",
    "end": "1381840"
  },
  {
    "text": "We also use some curated\nsources like Stanford courses and wikiHow, where we use\nextracts from these pages",
    "start": "1381840",
    "end": "1388710"
  },
  {
    "text": "to ask the models to generate\ncontent that is related to them. You can find more details\nin the Cosmopedia blog post.",
    "start": "1388710",
    "end": "1395799"
  },
  {
    "text": " So I guess now, we\nalso have the answer for our second question, which\nwas where to find the data.",
    "start": "1395800",
    "end": "1404350"
  },
  {
    "text": "And if you're following,\nwe have one question left, which is, how can\nwe filter this data?",
    "start": "1404350",
    "end": "1410480"
  },
  {
    "text": "Because for example, if you\nuse CommonCrawl and you need to filter it, and even\nif you use the stack,",
    "start": "1410480",
    "end": "1416350"
  },
  {
    "text": "we did not train our models\non the stack directly. We did a lot of filtering to\nget a data set that is smaller",
    "start": "1416350",
    "end": "1422380"
  },
  {
    "text": "but has a higher quality. And for this data set,\nI will cite this slide",
    "start": "1422380",
    "end": "1429520"
  },
  {
    "text": "from Thomas Wolfe's\npresentation. This lecture is very\ninteresting, by the way. You can find it here.",
    "start": "1429520",
    "end": "1436269"
  },
  {
    "text": "And this is from\nthe Yi paper, where they state that a\nhigh quality data",
    "start": "1436270",
    "end": "1442090"
  },
  {
    "text": "set might exhibit very\nadvanced capabilities for a standard architecture.",
    "start": "1442090",
    "end": "1447580"
  },
  {
    "text": "And this is actually the\nfocus of many recent papers",
    "start": "1447580",
    "end": "1452980"
  },
  {
    "text": "and we can see that\nin model releases, the sections about data sets\nare becoming smaller and smaller",
    "start": "1452980",
    "end": "1458470"
  },
  {
    "text": "because people are realizing\nthat the data set is actually the backbone. It is the one that is making\nsome models much better",
    "start": "1458470",
    "end": "1465789"
  },
  {
    "text": "than others. So it's really\nimportant to spend a lot of time curating\nthese data sets",
    "start": "1465790",
    "end": "1472480"
  },
  {
    "text": "and trying to remove all\nthe outliers and data sets that can hurt the\nmodel during the training.",
    "start": "1472480",
    "end": "1477794"
  },
  {
    "text": " This is the pipeline from\nthe Yi paper for filtering",
    "start": "1477795",
    "end": "1485300"
  },
  {
    "text": "their crawled web data sets. So first, they do\nlanguage filtering.",
    "start": "1485300",
    "end": "1490570"
  },
  {
    "text": "So I guess in Yi's\ncase, they kept English and some Asian languages. Then they apply some\nfiltering techniques",
    "start": "1490570",
    "end": "1497380"
  },
  {
    "text": "to remove low quality samples. For example, there\nare some metrics, you look for files that\nhave a lot of lines repeated",
    "start": "1497380",
    "end": "1505360"
  },
  {
    "text": "and then you remove them. There's also\nrule-based collection. You also can use\nperplexity filtering",
    "start": "1505360",
    "end": "1510640"
  },
  {
    "text": "where you compute\nsomething like loss and remove samples that\nhave a very high one.",
    "start": "1510640",
    "end": "1515660"
  },
  {
    "text": "Then after that,\nthey also did a step which is very important,\ndeduplication,",
    "start": "1515660",
    "end": "1522190"
  },
  {
    "text": "because there are a\nlot of papers that study the effect of\nduplicates on training and they find that skipping\nduplicates in the training data",
    "start": "1522190",
    "end": "1529960"
  },
  {
    "text": "can cause models to memorize\nand they have less space to be creative.",
    "start": "1529960",
    "end": "1535960"
  },
  {
    "text": "So this hurts the\nperformance of models and it's always advise\nto remove duplicates.",
    "start": "1535960",
    "end": "1542030"
  },
  {
    "text": "Using exact\nduplication, to remove files that are exactly\nidentical, but also",
    "start": "1542030",
    "end": "1548120"
  },
  {
    "text": "near deduplication to remove\nfiles that are similar. And this uses techniques\nlike MinHash deduplication.",
    "start": "1548120",
    "end": "1555500"
  },
  {
    "text": "For Yi, after that, they also\ndid more filtering on top like semantic and\ntopic filtering.",
    "start": "1555500",
    "end": "1561200"
  },
  {
    "text": "But usually, you can do\nthe classic filtering and duplication and then be more\ncreative for the other filters.",
    "start": "1561200",
    "end": "1569590"
  },
  {
    "text": "This was also the\ncase for FineWeb. The reason it is better\nthan other data sets is",
    "start": "1569590",
    "end": "1574840"
  },
  {
    "text": "that because they spent\na lot of time trying to come up with better filters\nand also deduplicate the data",
    "start": "1574840",
    "end": "1581680"
  },
  {
    "text": "sets well.  Now the question is, OK,\nwe can do the duplication.",
    "start": "1581680",
    "end": "1588860"
  },
  {
    "text": "I think we have methods that\nare established to do that. We can also do\nlanguage filtering.",
    "start": "1588860",
    "end": "1595120"
  },
  {
    "text": "But then if you want to filter\nthe data to remove garbage and lower quality files, how do\nyou come up with good filters?",
    "start": "1595120",
    "end": "1603100"
  },
  {
    "text": "You can for sure find some\nfilters in the literature, but if you want to really\nbuild a data set that",
    "start": "1603100",
    "end": "1609280"
  },
  {
    "text": "is better than what\nexists, you need to invest some time trying\nto find more techniques that work better for your case.",
    "start": "1609280",
    "end": "1615820"
  },
  {
    "text": "This can be done with manual\ninspection, which is always",
    "start": "1615820",
    "end": "1621039"
  },
  {
    "text": "a good idea to look at the\ndata and see what it actually looks like. And you can come up with filters\nto help you during the training.",
    "start": "1621040",
    "end": "1628480"
  },
  {
    "text": "But that is usually\nnot enough because you might have an intuition for\na filtering that works better",
    "start": "1628480",
    "end": "1634840"
  },
  {
    "text": "for your model. But then when you train,\nactually, this filtering doesn't help. And for example, for us, when\nwe were developing the StarCoder",
    "start": "1634840",
    "end": "1643040"
  },
  {
    "text": "series of models, we\nwere thinking, OK, what are the best ways\nfor us to filter code?",
    "start": "1643040",
    "end": "1648900"
  },
  {
    "text": "So we use some standard\nfilters, for example, to remove auto\ngenerated content, but we try to come\nup with a little bit",
    "start": "1648900",
    "end": "1655220"
  },
  {
    "text": "more complex filtering\nthat could help us. Looking for files that\nhave a lot of comments",
    "start": "1655220",
    "end": "1660890"
  },
  {
    "text": "because code that is\nusually well documented is probably of a higher quality\nthan another code file that",
    "start": "1660890",
    "end": "1667700"
  },
  {
    "text": "doesn't have any comments. So we implemented\nthis filter that looks for files that\nhave almost no comments,",
    "start": "1667700",
    "end": "1673970"
  },
  {
    "text": "and then removes them. And we train the model\non that and turned out the performance improvement\nwas really negligible.",
    "start": "1673970",
    "end": "1681300"
  },
  {
    "text": "It was not as much\nas we thought. We also tried to use\nanother filter, which is using the stars\nof a repository",
    "start": "1681300",
    "end": "1688580"
  },
  {
    "text": "as an indicator of quality. So we tried removing all\nthe files from repository",
    "start": "1688580",
    "end": "1693860"
  },
  {
    "text": "that have less than five stars,\nand this ended up removing over 70% of the data sets.",
    "start": "1693860",
    "end": "1699510"
  },
  {
    "text": "And then when we\ntrained on it, the model was the worst model that we\ntrained in all our ablation",
    "start": "1699510",
    "end": "1704630"
  },
  {
    "text": "experiments, simply because\nit removed too much data. It was not worth using\nthis filtering technique.",
    "start": "1704630",
    "end": "1711690"
  },
  {
    "text": "This is why it's very important\nthat when you have a filter, you should run what we\ncall an ablation model.",
    "start": "1711690",
    "end": "1717890"
  },
  {
    "text": "The ablation is basically,\nyou take a subset of your data set after you\napplied the filtering",
    "start": "1717890",
    "end": "1723050"
  },
  {
    "text": "and you train a\nsmall model on it and see how it behaves with\nand without the filtering.",
    "start": "1723050",
    "end": "1728510"
  },
  {
    "text": "And you might be wondering,\nOK, if I use a small model, but does it really\nextrapolate to larger models?",
    "start": "1728510",
    "end": "1735210"
  },
  {
    "text": "I think that's a good question. But generally, from\nour experience, we found that this\ndoes extrapolate",
    "start": "1735210",
    "end": "1740990"
  },
  {
    "text": "for most of data ablations. When you're doing\nthese ablations,",
    "start": "1740990",
    "end": "1746240"
  },
  {
    "text": "you should select a set\nof high signal benchmarks that could give you\nsome conclusions",
    "start": "1746240",
    "end": "1754760"
  },
  {
    "text": "about the effect of your\nfilter early in the training. This can be some of the popular\nNLP benchmarks for LLMs.",
    "start": "1754760",
    "end": "1761960"
  },
  {
    "text": "For example, HellaSwag or MMLU. And you should also-- sorry, here it's not tran,\nit's train with different seeds",
    "start": "1761960",
    "end": "1769750"
  },
  {
    "text": "to reduce the noise\nbecause sometimes you can have filtering techniques\nthat don't give you a very big difference.",
    "start": "1769750",
    "end": "1775430"
  },
  {
    "text": "But if you train with just one\nseed, you must draw conclusions but they're actually just noise.",
    "start": "1775430",
    "end": "1780980"
  },
  {
    "text": "So if you can and\nyou have the compute, it's always better to run\nthe same experiment with two",
    "start": "1780980",
    "end": "1786530"
  },
  {
    "text": "or three different\nseeds, and then maybe do something\nlike the averaging so that you reduce\nthe noise and you",
    "start": "1786530",
    "end": "1792160"
  },
  {
    "text": "have more robust conclusions\nabout the effects of your filtering.",
    "start": "1792160",
    "end": "1797920"
  },
  {
    "text": "For example, for the\nfine web data set, the authors run over\n200 plus ablations.",
    "start": "1797920",
    "end": "1805040"
  },
  {
    "text": "These were 1 billion\nmodels trained on, I think, 30 billion tokens. And this is how they were able\nto find filterings that worked",
    "start": "1805040",
    "end": "1813490"
  },
  {
    "text": "better for their data sets.  Now let's go back to\nour StarCoder use case",
    "start": "1813490",
    "end": "1822020"
  },
  {
    "text": "and I will tell you about how\nwe filtered the stack data sets. So for the version\none, if you remember,",
    "start": "1822020",
    "end": "1829280"
  },
  {
    "text": "we had 6 terabytes\nof source code. But when we trained\nStarCoder, we only",
    "start": "1829280",
    "end": "1834950"
  },
  {
    "text": "used 800 gigabytes\nof these 6 terabytes. So a lot of this data was\nfiltered out after our filtering",
    "start": "1834950",
    "end": "1844460"
  },
  {
    "text": "or curation. The same happened for the\nstack V2, where this time we started from 32 in 600\nprogramming languages,",
    "start": "1844460",
    "end": "1852710"
  },
  {
    "text": "and after the\nfiltering we ended up with only 6.3 terabytes of code.",
    "start": "1852710",
    "end": "1858750"
  },
  {
    "text": "And for filtering\ncode, the approach is a bit similar to\njust filtering web data,",
    "start": "1858750",
    "end": "1865650"
  },
  {
    "text": "but the filtering techniques\nare a bit different. So first, we wanted to include\na lot of programming languages",
    "start": "1865650",
    "end": "1873300"
  },
  {
    "text": "and we looked at them and\nwe didn't keep all of them. We only kept the popular\nones and excluded,",
    "start": "1873300",
    "end": "1879660"
  },
  {
    "text": "for example, configs\nand languages that are no longer maintained,\nso this was what V1. For StarCoder 2, we included\nmore languages over 600,",
    "start": "1879660",
    "end": "1889200"
  },
  {
    "text": "and then we added\nsome other sources that could be interesting for\na code model to learn from,",
    "start": "1889200",
    "end": "1895380"
  },
  {
    "text": "which are GitHub issues, Git\ncommits, and Jupyter Notebooks. We also added for the v2,\nKaggle, notebooks, and pull",
    "start": "1895380",
    "end": "1904139"
  },
  {
    "text": "requests. Our second step\nafter we selected the languages we wanted to train\non was data quality inspection.",
    "start": "1904140",
    "end": "1913170"
  },
  {
    "text": "So basically, as\nI told you, we had some filters to remove\nlow quality files and autogenerated content.",
    "start": "1913170",
    "end": "1920620"
  },
  {
    "text": "An example is the\naverage line length. So if we have an average\nline length that is too high,",
    "start": "1920620",
    "end": "1926552"
  },
  {
    "text": "there's probably something\nwrong with this file or it's probably auto generated. But since we had almost\n100 programming languages,",
    "start": "1926552",
    "end": "1934840"
  },
  {
    "text": "we should not use\nthe same threshold for all the languages\nfor this filter because some programming\nlanguages just",
    "start": "1934840",
    "end": "1940690"
  },
  {
    "text": "have longer lines. So it's important to\ndo some inspection and look at some samples\nfrom these languages.",
    "start": "1940690",
    "end": "1946789"
  },
  {
    "text": "In our case, we had\nthe BigCode community which helps us look at\n100 samples per extension",
    "start": "1946790",
    "end": "1953440"
  },
  {
    "text": "and derive the appropriate\nthresholds and filtering heuristics. ",
    "start": "1953440",
    "end": "1959970"
  },
  {
    "text": "The third deduplicate--\nsorry, filtering step was near-deduplication.",
    "start": "1959970",
    "end": "1966720"
  },
  {
    "text": "We found that near-deduplication\nwas the filtering that gave us the most performance boost.",
    "start": "1966720",
    "end": "1972190"
  },
  {
    "text": "It's also very easy to apply\nbecause it's language agnostic. Even though we have 86\nprogramming languages,",
    "start": "1972190",
    "end": "1978480"
  },
  {
    "text": "we don't need to change the\nduplication for each language. We can just apply\nit to the whole sub,",
    "start": "1978480",
    "end": "1983550"
  },
  {
    "text": "to the whole data set. And here I show you some\nresults of the effects",
    "start": "1983550",
    "end": "1988920"
  },
  {
    "text": "of the duplication. For example, here you can see\nthis model Python all licensed. If the filtering is\ndone, you get a pass",
    "start": "1988920",
    "end": "1996120"
  },
  {
    "text": "at one, which is our\ncode metric of 13. But if you apply no duplication,\nyou go from 13 to 17.",
    "start": "1996120",
    "end": "2002930"
  },
  {
    "text": "That's a very big\nperformance bump. The same goes for other subsets\nlike permissive license.",
    "start": "2002930",
    "end": "2008779"
  },
  {
    "text": "So we decided to use\ndeduplication for our data set and to use strong\ndeduplication to really remove",
    "start": "2008780",
    "end": "2016310"
  },
  {
    "text": "all the files that\ncould be similar. Another step is\nin our pipeline is",
    "start": "2016310",
    "end": "2022580"
  },
  {
    "text": "to remove personal\nidentifiable information. So this could be names,\nemails, or keys, or passwords",
    "start": "2022580",
    "end": "2029990"
  },
  {
    "text": "because we scraped\ncode from GitHub. And although GitHub\nhas some tools",
    "start": "2029990",
    "end": "2035120"
  },
  {
    "text": "to detect secrets and\nprompt users to remove them, that's not always the case. And we found that\nthere was still",
    "start": "2035120",
    "end": "2041330"
  },
  {
    "text": "a lot of secrets\nin the data sets. And we train a\nmodel, you don't want it to be trained on that because\nin inference, it might generate",
    "start": "2041330",
    "end": "2048949"
  },
  {
    "text": "sensitive or personal data. So our approach to removing it\nwas to first annotate the data",
    "start": "2048949",
    "end": "2055429"
  },
  {
    "text": "sets for PII. We collaborated with\nan annotation company",
    "start": "2055429",
    "end": "2061340"
  },
  {
    "text": "to annotate some samples. So the annotators were\ntasked with the labeling PII",
    "start": "2061340",
    "end": "2067879"
  },
  {
    "text": "when they found it. For example, if they find the\nname, they give it a class name. If you find an email, they\nalso label it as an email.",
    "start": "2067880",
    "end": "2074190"
  },
  {
    "text": "So it was a named\nentity recognition task. And then we trained\nthe StarPII, which",
    "start": "2074190",
    "end": "2079879"
  },
  {
    "text": "is our NER model\nto detect this PII, and then we run it on the\nwhole StarCoder training data.",
    "start": "2079880",
    "end": "2088638"
  },
  {
    "text": "This took almost 800 GPU\nhours because it's a more--",
    "start": "2088639",
    "end": "2093980"
  },
  {
    "text": "it's a neural network and\nit needs to run on GPUs. The last step in our filtering\nwas data decontamination",
    "start": "2093980",
    "end": "2102620"
  },
  {
    "text": "because you should make sure to\nremove the benchmarks and test sets from your training data.",
    "start": "2102620",
    "end": "2108090"
  },
  {
    "text": "Otherwise, your evaluation\nnumbers will just be inflated. So we made sure to\nremove the benchmarks",
    "start": "2108090",
    "end": "2114260"
  },
  {
    "text": "that we use for evaluation\nfrom our training sets. ",
    "start": "2114260",
    "end": "2119810"
  },
  {
    "text": "The last step in the data\ncuration of the stack was to format the data.",
    "start": "2119810",
    "end": "2125549"
  },
  {
    "text": "So now that the data is\nfiltered and because code is different from\ntext, we can allow",
    "start": "2125550",
    "end": "2131540"
  },
  {
    "text": "ourselves to apply some\nnice formatting that could help us during inference.",
    "start": "2131540",
    "end": "2137240"
  },
  {
    "text": "For example, for StarCoder,\nwe had the code file, but before the code,\nwe added some tokens",
    "start": "2137240",
    "end": "2143780"
  },
  {
    "text": "that indicate that this\nis the repository name and another token file name\nthat indicates the file",
    "start": "2143780",
    "end": "2149180"
  },
  {
    "text": "name and another one for stars. And this is interesting because\nthis model resembles StarCoder",
    "start": "2149180",
    "end": "2155750"
  },
  {
    "text": "and other code models. I guess their main use case\nis to be plugged in an IDE, for example, VS Code.",
    "start": "2155750",
    "end": "2162210"
  },
  {
    "text": "And when you're\nusing them, it could be interesting to\nappend the code",
    "start": "2162210",
    "end": "2167390"
  },
  {
    "text": "file with the name of\nthe file, for example, I don't know, file.py\nso that the model would",
    "start": "2167390",
    "end": "2172819"
  },
  {
    "text": "know this is the Python file. If it's in another language,\nwhen you add the file name and you have the\nextension, it could",
    "start": "2172820",
    "end": "2178620"
  },
  {
    "text": "know that this is detect\nthe language that it should generate code in. We also added GitHub stars\ntoken and we try to play with it",
    "start": "2178620",
    "end": "2186930"
  },
  {
    "text": "to say, \"This file\nhas the 100 stars.\" And see if the model would\ngenerate higher quality",
    "start": "2186930",
    "end": "2192570"
  },
  {
    "text": "code than if it were to\ngenerate for zero stars. We didn't find any differences\nreally during inference,",
    "start": "2192570",
    "end": "2198990"
  },
  {
    "text": "but it was fun to add\nall this formatting. For StarCoder2, one\nof the improvements",
    "start": "2198990",
    "end": "2206130"
  },
  {
    "text": "was that StarCoder2\nwas repository aware. Because when we have GitHub\nrepositories, it's repository.",
    "start": "2206130",
    "end": "2213870"
  },
  {
    "text": "So we have some files that are\nin the same repository that are related to each other. But when we built\nthe stack V1, we just",
    "start": "2213870",
    "end": "2221369"
  },
  {
    "text": "shuffled files so\nwe kept-- we didn't keep this repository structure. And we trained the model,\nwe just shuffled them",
    "start": "2221370",
    "end": "2227950"
  },
  {
    "text": "and the model did not\nknow if two files belong to the same repository. But when we did\nStarCoder2, we try",
    "start": "2227950",
    "end": "2234690"
  },
  {
    "text": "to keep files that are\nin the same repository next to each other. And how we did that is\nby concatenating them",
    "start": "2234690",
    "end": "2240870"
  },
  {
    "text": "with some special tokens like\nfile SEP, which basically separates files. And this way, the\nmodel can know which",
    "start": "2240870",
    "end": "2249150"
  },
  {
    "text": "files need the same\nrepository and try to find links between them. 3D parallelism, and then you\nhave also LightEval for doing",
    "start": "2249150",
    "end": "2256650"
  },
  {
    "text": "the evaluation. So this is a stack to be able\nto run your full trainings, but also your ablation models.",
    "start": "2256650",
    "end": "2263559"
  },
  {
    "text": "You can apply a\nfilter from datatrove and then train with nanotron\nand evaluate with LightEval, and they're well\nintegrated together",
    "start": "2263560",
    "end": "2270390"
  },
  {
    "text": "and they make one ecosystem. So that's for general LLMs. For code LLMs, we\nalso released the code",
    "start": "2270390",
    "end": "2278250"
  },
  {
    "text": "we used for both the stack\nand StarCoder models, and our BigCode\nrepository on GitHub.",
    "start": "2278250",
    "end": "2284180"
  },
  {
    "text": " And I think we just answered\nour third question, which",
    "start": "2284180",
    "end": "2290950"
  },
  {
    "text": "was how to filter the data. So now, you know how to--\nfirst, how much data you need,",
    "start": "2290950",
    "end": "2296200"
  },
  {
    "text": "and then where you can get\nthis data, both web, and code, and synthetic, and curated.",
    "start": "2296200",
    "end": "2301960"
  },
  {
    "text": "And you also know how you\ncan properly filter the data and you can test the\nfiltering techniques",
    "start": "2301960",
    "end": "2307990"
  },
  {
    "text": "that you have in mind. So now, let me tell you a\nlittle bit more about code LLMs",
    "start": "2307990",
    "end": "2314980"
  },
  {
    "text": "because that's what\nI'm working on. And I'm trying to give you\na little bit of an overview",
    "start": "2314980",
    "end": "2321280"
  },
  {
    "text": "about these models so that you\nknow how to train good LLMs, but you also know how to build\nvery cool code assistants",
    "start": "2321280",
    "end": "2328089"
  },
  {
    "text": "and completion models. So how all of this\nstarted was when",
    "start": "2328090",
    "end": "2334540"
  },
  {
    "text": "GitHub Copilot was released. And it was very interesting\nbecause it was so much better",
    "start": "2334540",
    "end": "2340060"
  },
  {
    "text": "than all the other code\ncompletion models that were before it, which were very\nsmall and much less performance.",
    "start": "2340060",
    "end": "2346770"
  },
  {
    "text": "And GitHub Copilot was using\nthe Codex model by OpenAI.",
    "start": "2346770",
    "end": "2351950"
  },
  {
    "text": "And they just showed that\nyou can train or code LLM in the same way that you train\nan for English, for example.",
    "start": "2351950",
    "end": "2358970"
  },
  {
    "text": "You can just take a\nlarge transformer model and give it a lot of code data\nand it will learn this code.",
    "start": "2358970",
    "end": "2365600"
  },
  {
    "text": "Because before, a\nlot of people were trying to treat code very\ndifferently, for example, by using abstract syntax trees.",
    "start": "2365600",
    "end": "2372950"
  },
  {
    "text": "But what Codex model\nshowed is that you can treat code like text. And if you want to\npredict the next line,",
    "start": "2372950",
    "end": "2378740"
  },
  {
    "text": "you can predict the\nnext text, you just do next token prediction\nand you get your code.",
    "start": "2378740",
    "end": "2384520"
  },
  {
    "text": "It works very well, much better\ncompared to the more feature engineering techniques.",
    "start": "2384520",
    "end": "2390580"
  },
  {
    "text": "And that was over two\nyears ago and we didn't have any good open code models.",
    "start": "2390580",
    "end": "2397330"
  },
  {
    "text": "But today, if you go to the hub,\nyou can find that we have over 1,700 models that\nare trained on code.",
    "start": "2397330",
    "end": "2404380"
  },
  {
    "text": "So these are models that\nare either trained only on code or LLMs that included\ncode as part of their training.",
    "start": "2404380",
    "end": "2412840"
  },
  {
    "text": "So you can see that we've\nmade a lot of progress in this code generation\nfield, which is amazing.",
    "start": "2412840",
    "end": "2421060"
  },
  {
    "text": "And this is the word-- this is\nthe result of the committee's",
    "start": "2421060",
    "end": "2426160"
  },
  {
    "text": "work to build very good\ninstruction-tuned models and base models. For example, here, as you\ncan see in the leaderboard,",
    "start": "2426160",
    "end": "2433539"
  },
  {
    "text": "we have some very strong\nmodels that score almost 80% on the code evaluation\nbenchmark, which",
    "start": "2433540",
    "end": "2439750"
  },
  {
    "text": "is human eval, which\nmeans they get almost 80% of the problems right, which\nis a very large number.",
    "start": "2439750",
    "end": "2447109"
  },
  {
    "text": "And when talking about the\nlandscape of open code LLMs, in BigCode, we have\nreleased the stack data set,",
    "start": "2447110",
    "end": "2454250"
  },
  {
    "text": "which is now the default\ndata set for training on code, and also StarCoder1 and\nStarCoder2 two family of models,",
    "start": "2454250",
    "end": "2460790"
  },
  {
    "text": "and other instruction-tuned\nmodels with the H2 team like StarChat2. Meta also released some very\ngood code models which are",
    "start": "2460790",
    "end": "2469250"
  },
  {
    "text": "the code Llama series of\nmodels that go from 7B to 70B. There are also the deep C models\nwhich are also very strong,",
    "start": "2469250",
    "end": "2477920"
  },
  {
    "text": "and they have also other models\nlike the recent Granite models from IBM, CodeQwen,\nCodeGen, and StableCode.",
    "start": "2477920",
    "end": "2485250"
  },
  {
    "text": "So there are different\nproviders for code LLMs and also for data sets for code.",
    "start": "2485250",
    "end": "2492750"
  },
  {
    "text": "And the main reason we started\nthe BigCode collaboration and to train our models was\nto have a collaboration where",
    "start": "2492750",
    "end": "2500609"
  },
  {
    "text": "we have full data transparency. We released all the\ndetails about the training,",
    "start": "2500610",
    "end": "2505920"
  },
  {
    "text": "but also the data is\npublic so that people can inspect it and use it. And we also have the code for\nthe processing and the model",
    "start": "2505920",
    "end": "2513569"
  },
  {
    "text": "weights. And the collaboration was open. We had over 1,000 researchers\njoin our Slack and following",
    "start": "2513570",
    "end": "2519750"
  },
  {
    "text": "the journey with us. And this created a\nBigCode ecosystem",
    "start": "2519750",
    "end": "2525349"
  },
  {
    "text": "where the stack was\nused in the pre-training of a lot of\nprominent code models like CodeGen and StableCode.",
    "start": "2525350",
    "end": "2533070"
  },
  {
    "text": "And the StarCoder\nmodels were used as basis for a lot of\ncommunity fine tuning.",
    "start": "2533070",
    "end": "2538240"
  },
  {
    "text": " And I think it's\nvery important to be",
    "start": "2538240",
    "end": "2543940"
  },
  {
    "text": "aware of what makes\na release of an LLM, whether it be a code\nLLM or general LLM",
    "start": "2543940",
    "end": "2550660"
  },
  {
    "text": "open and responsible. And I think this is four-fold. First, it's really\ngood for the community",
    "start": "2550660",
    "end": "2558460"
  },
  {
    "text": "and for research\nin AI in general. If you can make open access\ndata sets, this will help.",
    "start": "2558460",
    "end": "2566170"
  },
  {
    "text": "This will mean having\ndata inspection tools, but also opt out tools to\nrespect people's wishes",
    "start": "2566170",
    "end": "2573160"
  },
  {
    "text": "regarding their data sets. For example, if they don't want\nto be included in the trainings, they should be able to opt out.",
    "start": "2573160",
    "end": "2580060"
  },
  {
    "text": "It's also important to\nremove personal identifiable information. So an open release does not mean\njust releasing model weights",
    "start": "2580060",
    "end": "2587890"
  },
  {
    "text": "and stopping there, but also\nmaking your work reproducible by fully documenting the\npipeline for using these models",
    "start": "2587890",
    "end": "2595270"
  },
  {
    "text": "and also releasing tools for\nevaluation and technical reports",
    "start": "2595270",
    "end": "2600590"
  },
  {
    "text": "that documents the\nwhole pipeline. And for us in BigCode,\nwe went from SantaCoder,",
    "start": "2600590",
    "end": "2608210"
  },
  {
    "text": "which was part of our\nablations to understand how to filter the stack data sets.",
    "start": "2608210",
    "end": "2613820"
  },
  {
    "text": "And then we went to StarCoder\nwhich was released last year, a 15 billion code\ngeneration model.",
    "start": "2613820",
    "end": "2619290"
  },
  {
    "text": "And then this year,\nwe released StarCoder2 which was trained on much\nmore programming languages",
    "start": "2619290",
    "end": "2625340"
  },
  {
    "text": "and had a much higher\nevaluation score.",
    "start": "2625340",
    "end": "2631080"
  },
  {
    "text": "And StarCoder was also rated\nas the most transparent model by the Stanford Foundation\nmodel transparency",
    "start": "2631080",
    "end": "2638730"
  },
  {
    "text": "index, which is really\nheartwarming given the efforts that we put\ninto data governance",
    "start": "2638730",
    "end": "2644190"
  },
  {
    "text": "and into making the\nmodel release as transparent as possible. ",
    "start": "2644190",
    "end": "2650950"
  },
  {
    "text": "Regarding evaluation, so\nfor example, StarCoder 15B, when it was released, it was\nthe state of the art code model.",
    "start": "2650950",
    "end": "2659000"
  },
  {
    "text": "And this was also the case for\nStarCoder 215B among other 15B models.",
    "start": "2659000",
    "end": "2664340"
  },
  {
    "text": "And it was even close or\nbetter than larger models. I think I don't have the plot\nhere, but it was better than--",
    "start": "2664340",
    "end": "2671770"
  },
  {
    "text": "it was matching code Llama 34B,\nand it was close to DeepSeek 33B",
    "start": "2671770",
    "end": "2677230"
  },
  {
    "text": "on some benchmarks. And here, for example,\nyou can see the results on different benchmarks because\nwhen releasing our model,",
    "start": "2677230",
    "end": "2685059"
  },
  {
    "text": "it's really important\nthat you don't just evaluate on one benchmark,\nbut you should add",
    "start": "2685060",
    "end": "2690400"
  },
  {
    "text": "as many benchmarks as you want. In case you had\ncontamination, although we try to avoid this one benchmark,\nthere's a very low chance",
    "start": "2690400",
    "end": "2697570"
  },
  {
    "text": "that you also have contamination\non other benchmarks. And it also allows you\nto fully understand",
    "start": "2697570",
    "end": "2703150"
  },
  {
    "text": "how your model behaves if you\nadd more evaluation benchmarks.",
    "start": "2703150",
    "end": "2708680"
  },
  {
    "text": "And I think that's just a good\npractice that everyone should be doing with their releases. ",
    "start": "2708680",
    "end": "2715670"
  },
  {
    "text": "So with the StarCoder models,\nwe also released some tooling like VS Code implementation,\nwhich also has a membership",
    "start": "2715670",
    "end": "2724849"
  },
  {
    "text": "test that tries to see\nif the generated code was in the training data and\nhighlight that to the author.",
    "start": "2724850",
    "end": "2732240"
  },
  {
    "text": "So that's part of\nour code attribution efforts for these code models.",
    "start": "2732240",
    "end": "2740730"
  },
  {
    "text": "Maybe you're interested in\nusing these models to build your own personal\nCopilot and fine tuning",
    "start": "2740730",
    "end": "2747060"
  },
  {
    "text": "StarCoder or Code Llama or other\nmodels on your personal code bases. To do that, there's a very nice\nblog post by Sourab and Sayak,",
    "start": "2747060",
    "end": "2755849"
  },
  {
    "text": "where they try to\ntake a code model and train it on the Hugging\nFace internal libraries,",
    "start": "2755850",
    "end": "2762630"
  },
  {
    "text": "and then deploy it in old Llama\nand have a local code assistant. And the pipeline is very similar\nto what we did in pre-training.",
    "start": "2762630",
    "end": "2770470"
  },
  {
    "text": "First, you take\nyour data set, you try to filter out the things\nyou don't want to keep, and then you do deduplication,\nand you train your model.",
    "start": "2770470",
    "end": "2778010"
  },
  {
    "text": "So in this case, it will\nbe just a fine tuning, so it will be much quicker. You can use libraries\nlike PEFT which",
    "start": "2778010",
    "end": "2784470"
  },
  {
    "text": "do parameter efficient fine\ntuning where you don't need to train all the\nparameters of your models, but you only inject few\ntrainable parameters.",
    "start": "2784470",
    "end": "2792870"
  },
  {
    "text": "This makes the\ntraining much faster. For example, 7B model can be\ntrained in a Google Colab.",
    "start": "2792870",
    "end": "2799136"
  },
  {
    "text": " Now, let's go back\nto evaluation. So, for example,\nfor LLMs, there's",
    "start": "2799136",
    "end": "2806260"
  },
  {
    "text": "the open LLM leaderboard\nthat evaluates models. There's also the LLMs arena,\nwhich compares instructs models",
    "start": "2806260",
    "end": "2814059"
  },
  {
    "text": "and uses human evaluation. For code models, one of\nthe most popular benchmarks",
    "start": "2814060",
    "end": "2819610"
  },
  {
    "text": "is human eval. And it's basically\na benchmark where you have a function that the\nmodel has to autocomplete.",
    "start": "2819610",
    "end": "2828799"
  },
  {
    "text": "And then when the\nfunction is completed, you take this solution and then\nyou run it against multiple unit",
    "start": "2828800",
    "end": "2835450"
  },
  {
    "text": "tests and you count\nhow many solutions pass and how many solutions fail. And then you count what we a\nmetric that we call pass at one.",
    "start": "2835450",
    "end": "2843800"
  },
  {
    "text": "For example, this\nis the one that's been reported in\nthis leaderboard and this gives you\nthe human eval score.",
    "start": "2843800",
    "end": "2850330"
  },
  {
    "text": "There's also a translation\nof this benchmark to 18 other languages. Here, I show Java and\nJavaScript and C+ plus.",
    "start": "2850330",
    "end": "2858280"
  },
  {
    "text": "And this benchmark\nis called MultiPL-E. So it allows you to see\nhow well each model does",
    "start": "2858280",
    "end": "2864950"
  },
  {
    "text": "on which programming language\nand choose the one that's the most interesting for you.",
    "start": "2864950",
    "end": "2871630"
  },
  {
    "text": "But these benchmarks\nusually have an issue of contamination and\noverfitting, especially",
    "start": "2871630",
    "end": "2880180"
  },
  {
    "text": "instruction-tuned models. I don't know if you've already\nchecked what these data sets look like, but\nusually for code, they",
    "start": "2880180",
    "end": "2887440"
  },
  {
    "text": "are an instruction that asks the\nmodel to generate an exercise. And often, if you\nlook at them, they",
    "start": "2887440",
    "end": "2893680"
  },
  {
    "text": "look really similar\nto human eval which is function implementations. So there's a very high chance\nof having contamination,",
    "start": "2893680",
    "end": "2902230"
  },
  {
    "text": "which means having\nsome files that look like human eval exercises\nin your instruction tuning data",
    "start": "2902230",
    "end": "2910610"
  },
  {
    "text": "set. So here, for example, this\nplot is from the LiveCodeBench",
    "start": "2910610",
    "end": "2915760"
  },
  {
    "text": "leaderboard, and they find\nthat some benchmarks may be overfitting on human eval.",
    "start": "2915760",
    "end": "2922960"
  },
  {
    "text": "And so their solution was\nto have a leaderboard called LiveCodeBench where\nthey regularly scrape",
    "start": "2922960",
    "end": "2931280"
  },
  {
    "text": "new problems from platforms like\ncode contests and leaked code.",
    "start": "2931280",
    "end": "2936780"
  },
  {
    "text": "And they evaluate\nthe models only on the problems that were\nreleased after the model release",
    "start": "2936780",
    "end": "2942210"
  },
  {
    "text": "date. This way, they are sure that\nthere is no contamination. And for example, that\nwas the case here.",
    "start": "2942210",
    "end": "2949260"
  },
  {
    "text": "They tried to evaluate these\nmodels on all the data they have, and then they can\ncompare the performance",
    "start": "2949260",
    "end": "2955280"
  },
  {
    "text": "to the data that was only\nreleased after the model release. And they found that\nsome models were not",
    "start": "2955280",
    "end": "2961070"
  },
  {
    "text": "consistent in their results. So that's one interesting\nthing to keep in mind.",
    "start": "2961070",
    "end": "2967020"
  },
  {
    "text": "And this is also\nanother leader for this will be interesting to compare\nnot just open models, but also",
    "start": "2967020",
    "end": "2974900"
  },
  {
    "text": "closed models like GPT-4 and\nsee where the open source community is standing\ncompared to these code models.",
    "start": "2974900",
    "end": "2982900"
  },
  {
    "text": "So that was my presentation. Thank you very much\nfor your attention. And if you have any\nquestions, I can answer them.",
    "start": "2982900",
    "end": "2992230"
  },
  {
    "text": "Yes, thank you very much for\nthe great insightful talk. So we have some\nquestions here on Slido.",
    "start": "2992230",
    "end": "2999119"
  },
  {
    "text": "So I'm not sure if there are\nany in-person questions or else I will get started with\nthe Slido question.",
    "start": "2999120",
    "end": "3005740"
  },
  {
    "text": " Sure.",
    "start": "3005740",
    "end": "3011980"
  },
  {
    "text": "OK, guess not. So I'll ask some of\nthe questions online. I think I had submitted\nsome of these as well.",
    "start": "3011980",
    "end": "3020339"
  },
  {
    "text": "It seems like there's some\nquestions about synthetic data. Let me see.",
    "start": "3020340",
    "end": "3025650"
  },
  {
    "text": "I was also wondering about this. So someone's asking what\nare the consequences of training AI models on\nAI-generated synthetic data?",
    "start": "3025650",
    "end": "3035349"
  },
  {
    "text": "Do you foresee any\nproblems with this? And there's a related question. Does synthetic data\nclosely represent",
    "start": "3035350",
    "end": "3042840"
  },
  {
    "text": "the natural distribution\nof language. I assume some low\nquality data from humans is necessary for things\nlike learning robustness",
    "start": "3042840",
    "end": "3050460"
  },
  {
    "text": "and so forth. Yeah, sure. These are very great questions. So about the consequences\nof training models",
    "start": "3050460",
    "end": "3058680"
  },
  {
    "text": "on AI-generated data, I\ncan think of two main ones. First is enforcing some\nbiases because models already",
    "start": "3058680",
    "end": "3066330"
  },
  {
    "text": "have some biases. And if we train on data\nthat is generated by them, we might be enforcing\nit even more.",
    "start": "3066330",
    "end": "3072480"
  },
  {
    "text": "The other thing is, for\nexample, contamination. These models might be trained--",
    "start": "3072480",
    "end": "3077730"
  },
  {
    "text": "might generate content that\nlooks like the evaluation benchmarks. And when you train on\nthat, you will have contamination in your data.",
    "start": "3077730",
    "end": "3084360"
  },
  {
    "text": "So, for example, one of the\ncritiques of the five model is that people, because they\ndid not see the synthetic data",
    "start": "3084360",
    "end": "3089970"
  },
  {
    "text": "and the models were very\ngood on the benchmarks, they were very skeptical. Are these models really good\nor are they just overfitting",
    "start": "3089970",
    "end": "3096030"
  },
  {
    "text": "on the benchmarks? So I think contamination\nand enforcing biases are one of the main\nthings to keep in mind.",
    "start": "3096030",
    "end": "3103590"
  },
  {
    "text": "And regarding synthetic\ndata not being the same as a web\ndistribution, I",
    "start": "3103590",
    "end": "3110550"
  },
  {
    "text": "think that's a very good point. And for example, when we\nwere developing Cosmopedia,",
    "start": "3110550",
    "end": "3116290"
  },
  {
    "text": "first, we found that it\nwas worse than the web, and which was surprising because\nwe spent a lot of time trying",
    "start": "3116290",
    "end": "3123580"
  },
  {
    "text": "to curate this data\nset which looks so much cleaner than the web. And then adding some\nweb data and trying",
    "start": "3123580",
    "end": "3129190"
  },
  {
    "text": "to add like more\ntopics was able to help us compensate some of the gaps.",
    "start": "3129190",
    "end": "3134480"
  },
  {
    "text": "But adding some web always\ngives you a performance boost. So yes, there is some noise\nand some specific patterns",
    "start": "3134480",
    "end": "3141550"
  },
  {
    "text": "in web data that\nwill probably need to be included in\nthe training mix to keep a whole coverage of\nwhat's natural distributions",
    "start": "3141550",
    "end": "3151510"
  },
  {
    "text": "look like. So it sounds like you're\nsaying a good training set would have a\nmix, potentially,",
    "start": "3151510",
    "end": "3158410"
  },
  {
    "text": "of synthetic and natural data. Is that correct? Yeah, I think so. Some experiments were on show\nthat that's the case because you",
    "start": "3158410",
    "end": "3167200"
  },
  {
    "text": "can try to spend some time to\ncarefully curate the topics, but you'll probably be\nmissing out on some things.",
    "start": "3167200",
    "end": "3173320"
  },
  {
    "text": "And the human\nintuition that we have is not always what works\nfor training models. It seems that keeping\nsome filtered web helps.",
    "start": "3173320",
    "end": "3180200"
  },
  {
    "text": "And also, if you see the\ntechnical reports, for example, in [INAUDIBLE], they insist\na lot on filtering the web",
    "start": "3180200",
    "end": "3185890"
  },
  {
    "text": "and including it in\nthe pre-training. And I think that now seems\nlike maybe the best way to go.",
    "start": "3185890",
    "end": "3193520"
  },
  {
    "text": "But that makes sense. Great. Another question is, is\nRLHF type preference data",
    "start": "3193520",
    "end": "3199309"
  },
  {
    "text": "more important than\nunsupervised pre-training data? Should we spend more\nresources on RLHF data?",
    "start": "3199310",
    "end": "3207470"
  },
  {
    "text": "Yeah, that's a good question. So for example, the\nunsupervised pre-training is mainly to get base\nmodels, but then you",
    "start": "3207470",
    "end": "3214339"
  },
  {
    "text": "can't use these base\nmodels as chat assistants. You need to do another step. So you can either do\nRLHF, but nowadays, people",
    "start": "3214340",
    "end": "3222440"
  },
  {
    "text": "are just doing\ninstruction tuning without needing to go\nthrough RL where you just train the model on pairs of\ninstructions and solutions,",
    "start": "3222440",
    "end": "3229849"
  },
  {
    "text": "and that seems to\nwork very well. And there are now\nsome methods that don't use reinforcement learning\nbut work as well, for example,",
    "start": "3229850",
    "end": "3236660"
  },
  {
    "text": "DPO or ORPO. So I think if you want\nto chat assistant, you definitely need to run\na supervised training on top",
    "start": "3236660",
    "end": "3244280"
  },
  {
    "text": "of the unsupervised one,\nbut it doesn't necessarily have to be RLHF. There are some other\nalgorithms now.",
    "start": "3244280",
    "end": "3251770"
  },
  {
    "text": "Great. And here's a\nmultimodal question. Does multimodal\ngrounding, for example,",
    "start": "3251770",
    "end": "3257450"
  },
  {
    "text": "including images and\nvideos along with the text, reduce the need for so\nmuch text-only data?",
    "start": "3257450",
    "end": "3263065"
  },
  {
    "text": " Yeah what do you mean? I'm sorry.",
    "start": "3263065",
    "end": "3269200"
  },
  {
    "text": "Well, the question is asking,\ndoes multimodal grounding help, basically? If you have images and\nvideos along with the text,",
    "start": "3269200",
    "end": "3276400"
  },
  {
    "text": "does this reduce the amount\nof text-only data required to train models?",
    "start": "3276400",
    "end": "3282640"
  },
  {
    "text": "So I can't probably answer\nthat because I haven't tried, but I guess, all for example,\nthe multimodal models,",
    "start": "3282640",
    "end": "3288580"
  },
  {
    "text": "for example, that were\nrecently released, there's always a\nsignificant text portion.",
    "start": "3288580",
    "end": "3294339"
  },
  {
    "text": "That seems the case for\nmost vision language models. But yeah, I don't know, really,\nabout the percentages for each.",
    "start": "3294340",
    "end": "3302900"
  },
  {
    "text": "Right, OK.  A more general question. You probably touched\nupon some of this,",
    "start": "3302900",
    "end": "3308975"
  },
  {
    "text": "but are there any\nmajor differences between training text\nversus code models other than the training\ndata being different?",
    "start": "3308975",
    "end": "3318020"
  },
  {
    "text": "Yes, that's a good question. So the training\ndata is different. Regarding the training itself,\nwe use a similar architecture.",
    "start": "3318020",
    "end": "3325650"
  },
  {
    "text": "For example, StarCoder, it was a\nLlama or a Mixtral architecture. I think one thing\nthat you probably",
    "start": "3325650",
    "end": "3331550"
  },
  {
    "text": "want is long context\nbecause if you want to use these models,\nfor example, in VS code,",
    "start": "3331550",
    "end": "3336950"
  },
  {
    "text": "then you want to add all\nthe neighboring files in the context. You should be able to\nfit a very large context.",
    "start": "3336950",
    "end": "3342570"
  },
  {
    "text": "So we try to do some\nlong context extension. But again, people\nalso do this for LLMs.",
    "start": "3342570",
    "end": "3349310"
  },
  {
    "text": "We also care a lot\nabout inference. So we use first, MQA and then\nGQA to have faster inference.",
    "start": "3349310",
    "end": "3356280"
  },
  {
    "text": "But these are also techniques\nthat are implemented for LLMs. So I'd say overall,\nit's very similar.",
    "start": "3356280",
    "end": "3362340"
  },
  {
    "text": "But yeah, maybe we should\nprioritize some things like having a smaller\nmodel that can",
    "start": "3362340",
    "end": "3367640"
  },
  {
    "text": "be used for, for example,\nIDE is faster than actually, a much larger model that\nwould need more deployment.",
    "start": "3367640",
    "end": "3375350"
  },
  {
    "text": "Yeah. All right, great. Here's also a general question. I guess they're\nasking for advice.",
    "start": "3375350",
    "end": "3381780"
  },
  {
    "text": "So if you have a very\ntiny compute budget, for example, a single\nGPU, what would you",
    "start": "3381780",
    "end": "3387170"
  },
  {
    "text": "recommend prioritizing? Let's assume you're\nfine tuning a model.",
    "start": "3387170",
    "end": "3392789"
  },
  {
    "text": "Yeah, so I think,\nfor example, now there are some great solutions\nfor on-device deployment",
    "start": "3392790",
    "end": "3399630"
  },
  {
    "text": "and fine tuning. For example, you can\nrun quantized models with Llama CPP or\nother frameworks.",
    "start": "3399630",
    "end": "3406860"
  },
  {
    "text": "And with the\ntechniques like PEFT, you don't need to do full model\nfine tuning and you should be",
    "start": "3406860",
    "end": "3412530"
  },
  {
    "text": "able to run this on one\nGPU even in 7B model. So I think you should just\nfind a very well curated data",
    "start": "3412530",
    "end": "3419370"
  },
  {
    "text": "sets because quality is more\nimportant than quantity, and then use one of these\ntechniques for easy fine tuning",
    "start": "3419370",
    "end": "3426730"
  },
  {
    "text": "and that should work. All right, great. ",
    "start": "3426730",
    "end": "3433000"
  },
  {
    "text": "Here's a question asking-- I guess, different\nfrom pre-training, but they're saying,\nI'm guessing,",
    "start": "3433000",
    "end": "3439840"
  },
  {
    "text": "the optimal amount\nof training data depends heavily on the domain as\nwell as the task at hand, right?",
    "start": "3439840",
    "end": "3447255"
  },
  {
    "text": "Yes, probably. Now, we're following the\nChinchilla scaling laws. I think they tried to\ncompare English to code,",
    "start": "3447256",
    "end": "3455230"
  },
  {
    "text": "and they found that the\nfindings still hold. But maybe if you go\nto another domain, I don't know, like medical,\nthings could change.",
    "start": "3455230",
    "end": "3462175"
  },
  {
    "text": "And that's why I mentioned\nthe DeepSeek paper where they mentioned\nthat it's really heavily dependent on data.",
    "start": "3462175",
    "end": "3467468"
  },
  {
    "text": "And for them, it\nwas the same domain. They just changed data sets,\ngoing from one generic data set",
    "start": "3467468",
    "end": "3472600"
  },
  {
    "text": "to another, well curated one,\nand things started changing. So I think that's\nprobably the case.",
    "start": "3472600",
    "end": "3478579"
  },
  {
    "text": "But it's underexplored\nhow these scaling laws change depending on domains. So it's good to be\naware of that when",
    "start": "3478580",
    "end": "3485109"
  },
  {
    "text": "developing models for domains\nthat are not explored by these-- Speaking of different\ndomains, code versus text,",
    "start": "3485110",
    "end": "3491080"
  },
  {
    "text": "someone is asking, what are some\nof the interesting differences between tokenizing for\ngeneral purpose text",
    "start": "3491080",
    "end": "3499730"
  },
  {
    "text": "versus for code generation? Yeah, so when we were\ntraining the tokenizer,",
    "start": "3499730",
    "end": "3507227"
  },
  {
    "text": "I think one thing that\nwas important to keep was number splitting, and\nwe use the standard bpe.",
    "start": "3507227",
    "end": "3513680"
  },
  {
    "text": "And we were training it. We trained on our data set that\nwe were using for this training",
    "start": "3513680",
    "end": "3518930"
  },
  {
    "text": "data, so our code mixture. And we did some\nanalysis to see if there are any outliers,\nany tokens that",
    "start": "3518930",
    "end": "3527060"
  },
  {
    "text": "were underrepresented\nor overrepresented as sanity checks. But overall, it's very\nclose to the text training.",
    "start": "3527060",
    "end": "3537150"
  },
  {
    "text": "And now, most LLMs have a\nsignificant code portion in their tokenizers so they're\nalso trained on a lot of code.",
    "start": "3537150",
    "end": "3544590"
  },
  {
    "text": "And at the end, you can use\neither one tokenizer for LLMs or for code or the other way.",
    "start": "3544590",
    "end": "3550880"
  },
  {
    "text": "Because even on code, you\nhave a lot of markdown, so there's a lot of\nEnglish, so you end up representing all the\nEnglish tokens, for example,",
    "start": "3550880",
    "end": "3557660"
  },
  {
    "text": "in your code tokenizer. All right, great. And here's a question\nabout fine tuning, I guess,",
    "start": "3557660",
    "end": "3563372"
  },
  {
    "text": "compared to pre-training. So they're asking, do the same\nprinciples apply for fine tuning",
    "start": "3563372",
    "end": "3570120"
  },
  {
    "text": "or would you make a different\nor additional recommendation? ",
    "start": "3570120",
    "end": "3576620"
  },
  {
    "text": "So yeah, for fine\ntuning, I think when you're preparing the data,\nit's probably a different thing.",
    "start": "3576620",
    "end": "3582210"
  },
  {
    "text": "You're not going to train\non all of the stack. You probably want to continue\ntraining on specific language.",
    "start": "3582210",
    "end": "3588260"
  },
  {
    "text": "So maybe you could\ninvest more time to even heavily filter\nbecause for fine tuning, you don't need as much\ndata as for pre-training.",
    "start": "3588260",
    "end": "3595040"
  },
  {
    "text": "For example, for\nus, the filtering we tried, for\nexample, stars didn't work because they\nremoved a lot of data and we did not have enough\nfor our pre-training.",
    "start": "3595040",
    "end": "3601970"
  },
  {
    "text": "Well, for fine tuning, for\nexample, for instruction tuning, there was the Lima paper where\nthey instruction tuned only",
    "start": "3601970",
    "end": "3607880"
  },
  {
    "text": "on 1,000 instructions and they\nhad a model that was much better than train on\nmillions of samples. So I think that situation is\neven much more important when",
    "start": "3607880",
    "end": "3615680"
  },
  {
    "text": "it comes to fine tuning.  Great. One last question, I guess.",
    "start": "3615680",
    "end": "3622359"
  },
  {
    "text": "So you might have also\ntouched upon this briefly, but what are some considerations\nto make when publishing",
    "start": "3622360",
    "end": "3627940"
  },
  {
    "text": "very large data sets, and more\nnuanced or less known things to be aware of?",
    "start": "3627940",
    "end": "3635190"
  },
  {
    "text": "Yeah, so maybe on\nthe technical side,",
    "start": "3635190",
    "end": "3640869"
  },
  {
    "text": "releasing tools also for\nfiltering and documentation. That's what we tried\nto do with the stack.",
    "start": "3640870",
    "end": "3646619"
  },
  {
    "text": "And maybe more on\nthe governance side, be aware of where the\nlicenses are respected, where",
    "start": "3646620",
    "end": "3653130"
  },
  {
    "text": "the copyright is respected. Do you have an opt out\ntool for your data sets? And maybe try to\nrelease it on the hub",
    "start": "3653130",
    "end": "3659400"
  },
  {
    "text": "to make it easily\naccessible for people. If there are some concerns,\nyou could try to add a gate. For example, for us,\nwe release the data set",
    "start": "3659400",
    "end": "3666210"
  },
  {
    "text": "that we used for PII detection,\nbut we add some gating mechanism because it was\nsensitive information.",
    "start": "3666210",
    "end": "3671950"
  },
  {
    "text": "So it's good to think of\nthis things in advance before releasing the data sets.",
    "start": "3671950",
    "end": "3677490"
  },
  {
    "text": "But yeah, in general,\nthese are my advice.  Great.",
    "start": "3677490",
    "end": "3684060"
  },
  {
    "text": "Do we have any\nin-person questions? If not, then we can\nprobably conclude.",
    "start": "3684060",
    "end": "3690859"
  },
  {
    "start": "3690860",
    "end": "3696000"
  }
]