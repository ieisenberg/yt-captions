[
  {
    "text": "hello and welcome to section 5 features",
    "start": "6720",
    "end": "15920"
  },
  {
    "text": "so we now finish talking about what a predictor is \nand how one evaluates predictors and we can start  ",
    "start": "22000",
    "end": "31600"
  },
  {
    "text": "to move on by talking about how to construct \npredictors and the first step in the process  ",
    "start": "31600",
    "end": "38000"
  },
  {
    "text": "is taking the data in its raw form and \nembedding it into vectors and i'm going  ",
    "start": "38560",
    "end": "48640"
  },
  {
    "text": "to establish a little bit more with a little \nbit more formality right now exactly how we do   that and talk about a variety of different ways in \nwhich we embed data in vector space so that we can  ",
    "start": "48640",
    "end": "61760"
  },
  {
    "text": "use mathematical techniques to construct \npredictors we have raw data pairs which we  ",
    "start": "62400",
    "end": "69120"
  },
  {
    "text": "will denote by u and v where u is in some script u \nwhich is the set of possible u's and v is in some  ",
    "start": "69120",
    "end": "76960"
  },
  {
    "text": "script with v the set of possible v's and here \nscript u is the set of all possible input values  ",
    "start": "76960",
    "end": "84080"
  },
  {
    "text": "and script v is the set of all possible output \nvalues and ideally our predictor is going to take  ",
    "start": "84960",
    "end": "91040"
  },
  {
    "text": "an element of script u and return \nfor us an element of script v",
    "start": "92160",
    "end": "96080"
  },
  {
    "text": "the elements of u are called records and \ntypically they're a list of attributes  ",
    "start": "98240",
    "end": "105600"
  },
  {
    "text": "of the data um these attributes these \nwhich will index the uis are called  ",
    "start": "106320",
    "end": "114400"
  },
  {
    "text": "fields or components of the data and notice \nhere the i here is a subscript on you when  ",
    "start": "114400",
    "end": "120880"
  },
  {
    "text": "we use a superscript that means the ice record \nwhen we use a subscript that means the ice field  ",
    "start": "120880",
    "end": "128720"
  },
  {
    "text": "of a record and each of these fields has a type \nit might be a real number it might be a boolean  ",
    "start": "128720",
    "end": "136000"
  },
  {
    "text": "it could be one of several different \ntypes it could be a piece of text   could be audio it could be an image it could \nbe a parse tree an output of a compiler",
    "start": "136880",
    "end": "147760"
  },
  {
    "text": "there are also certain generic \ntypes that we talk about categorical  ",
    "start": "150080",
    "end": "153520"
  },
  {
    "text": "in ordinal types and i will tell you what those \nare shortly um so for example you might have a um  ",
    "start": "155120",
    "end": "165280"
  },
  {
    "text": "the set u be a set of houses and uh",
    "start": "166640",
    "end": "172080"
  },
  {
    "text": "and then a record would record the properties \nof um a house so it might give us its address a  ",
    "start": "174320",
    "end": "184160"
  },
  {
    "text": "photo of the house perhaps some photos of inside \nthe house a description which is a piece of text  ",
    "start": "184160",
    "end": "190960"
  },
  {
    "text": "a categorization of it as either a \nhouse or an apartment the size of a lot  ",
    "start": "192800",
    "end": "200480"
  },
  {
    "text": "the number of bedrooms and so on the list goes on \num our goal with all of these things is that we're  ",
    "start": "201120",
    "end": "209440"
  },
  {
    "text": "given all of this data and we're trying to predict \nsay the price how much did the house sell for  ",
    "start": "209440",
    "end": "215520"
  },
  {
    "text": "should the house sell for and notice that each \nof these fields of the description of a house  ",
    "start": "215520",
    "end": "223600"
  },
  {
    "text": "has a type so address well that might \nbe text or we might be a little bit more  ",
    "start": "223600",
    "end": "230880"
  },
  {
    "text": "uh precise about what an address is it might be \na have a street number and a street name and a  ",
    "start": "231680",
    "end": "240319"
  },
  {
    "text": "city and a county we might have gps \ncoordinates for the location of the house",
    "start": "240320",
    "end": "247200"
  },
  {
    "text": "the description is is text but even there we \nmight be able to be a little bit more refined  ",
    "start": "250480",
    "end": "259120"
  },
  {
    "text": "about that we might have description \nof various parts of the house we might   have standard forms for the description of \nthe house and so how we take this data and  ",
    "start": "259120",
    "end": "268720"
  },
  {
    "text": "embed it in vector space is going to affect a lot \nour ability to make useful predictions using it",
    "start": "269840",
    "end": "281840"
  },
  {
    "text": "so abstractly the uh the functions that we use to \ndo this embedding are called the feature maps and  ",
    "start": "285360",
    "end": "295280"
  },
  {
    "text": "we have two of them one of them takes u and embeds \nit in d-dimensional vector space and gives us  ",
    "start": "295280",
    "end": "302639"
  },
  {
    "text": "x one of them the other one takes v and \nembeds that in m-dimensional vector space  ",
    "start": "302640",
    "end": "309440"
  },
  {
    "text": "and gives us y and learning algorithms are \napplied to x y pairs which are just vectors  ",
    "start": "309440",
    "end": "314880"
  },
  {
    "text": "and the functions are called phi and psi so phi \ntakes a u and gives us an x and psi takes a v  ",
    "start": "316320",
    "end": "323120"
  },
  {
    "text": "and gives us a y and these are the feature \nmaps they transform data records into vectors",
    "start": "323120",
    "end": "331280"
  },
  {
    "text": "usually feature maps work on the separate \nfields so we'll have a feature map that maps the  ",
    "start": "334320",
    "end": "340080"
  },
  {
    "text": "photograph into a vector space we have a feeder \nfeature map that maps the address into a vector   space and so we've really got a different \nfeature map for each of the different fields  ",
    "start": "340640",
    "end": "352320"
  },
  {
    "text": "of a particular record and um and then \nwe'll construct one feature map which takes  ",
    "start": "352880",
    "end": "362160"
  },
  {
    "text": "uh the entire record and maps that into a \nvector space and the way it does it is by  ",
    "start": "362800",
    "end": "368960"
  },
  {
    "text": "mapping each of the fields of the record into \neach of their component vector spaces and then  ",
    "start": "368960",
    "end": "375120"
  },
  {
    "text": "joining up all the vectors into one long \nvector so phi subscript i is an embedding  ",
    "start": "375760",
    "end": "383200"
  },
  {
    "text": "it's if it's uh it's a map which takes field \ni into a vector and the overall function phi  ",
    "start": "383200",
    "end": "390640"
  },
  {
    "text": "is the feature map that combines all these \nembeddings and takes a record into a vector",
    "start": "390640",
    "end": "396880"
  },
  {
    "text": "the the reason we do this is so that we \ncan put all the different field types   on an equal footing they're all vectors",
    "start": "403600",
    "end": "411200"
  },
  {
    "text": "and some some of these embeddings are very very \nsimple so if the raw data is actually just a  ",
    "start": "413680",
    "end": "420720"
  },
  {
    "text": "real number such as the uh the lot size of the \nhouse then we might just embed it by using the  ",
    "start": "420720",
    "end": "429200"
  },
  {
    "text": "identity map we'll just embed it according \nto x it's equal to u um uh sometimes uh uh",
    "start": "429200",
    "end": "439280"
  },
  {
    "text": "a field may be boolean um and so for example uh  ",
    "start": "442000",
    "end": "448000"
  },
  {
    "text": "uh it might be uh uh for a house it might be \nhas a pool and the answer could be yes or no  ",
    "start": "449600",
    "end": "457920"
  },
  {
    "text": "and uh in which case we would map that to a real \nnumber in a vector space by saying 1 if the answer  ",
    "start": "458720",
    "end": "467600"
  },
  {
    "text": "is yes and minus 1 if the answer is no or we \nmight map it according to 1 if yes and 0 if no",
    "start": "467600",
    "end": "474160"
  },
  {
    "text": "uh color um we might map to uh three numbers \nuh the amount of red the amount of green  ",
    "start": "476960",
    "end": "485039"
  },
  {
    "text": "uh the amount of blue and uh that's a very typical \nencoding for color which we use a lot for images  ",
    "start": "485040",
    "end": "492560"
  },
  {
    "text": "but it's by no means the only one and uh there \nare quite a few different standard color encodings",
    "start": "492560",
    "end": "499680"
  },
  {
    "text": "um there are also much more sophisticated \nencodings which we will have a   chance to talk about later",
    "start": "501840",
    "end": "507600"
  },
  {
    "text": "so one is this one has a text \ndocument and one wants to   embed that as a vector then there's a map \ncalled the tfidf map that stands for term  ",
    "start": "509920",
    "end": "521680"
  },
  {
    "text": "frequency inverse document frequency and we'll \nsee exactly what that is and why it's useful for",
    "start": "521680",
    "end": "527279"
  },
  {
    "text": "learning properties of text building predictors \nfor text another one for text is called  ",
    "start": "529600",
    "end": "537279"
  },
  {
    "text": "word to back we'll see that in just a second that \nmaps individual words into vectors rather than  ",
    "start": "537280",
    "end": "543520"
  },
  {
    "text": "mapping hold documents into vectors um another \none is that instead of thinking about an image  ",
    "start": "543520",
    "end": "551040"
  },
  {
    "text": "as a collection of pixels we use a particular \nfeature map to map the image into vectors  ",
    "start": "551040",
    "end": "560320"
  },
  {
    "text": "and these are pre-trained neural networks and \nwe will see uh an example of this for shortly",
    "start": "560320",
    "end": "567280"
  },
  {
    "text": "one of the things we want out of \nan embedding is that it should be   faithful and what that means is that it should \nsomehow preserve similarity so if you and utilde  ",
    "start": "573680",
    "end": "588800"
  },
  {
    "text": "are two different records but somehow they're \nsimilar they correspond to two similar houses  ",
    "start": "588800",
    "end": "594399"
  },
  {
    "text": "then when we embed them we get a phi of u and \nwe get a phi of utilda and those two vectors  ",
    "start": "595520",
    "end": "601760"
  },
  {
    "text": "should be close to each other in the vector space and conversely if phi of u is not near 5u tilde \nwell then u and utilda should be dissimilar  ",
    "start": "602400",
    "end": "615280"
  },
  {
    "text": "houses and so the the geometry and the distances \nwithin our vector space should somehow correspond  ",
    "start": "615280",
    "end": "625920"
  },
  {
    "text": "to our intuitive notions of similarity for our \nunderlying raw data and this particular notion  ",
    "start": "626480",
    "end": "638480"
  },
  {
    "text": "of similarity well it depends on the application \nthat we have in mind it depends on the field type  ",
    "start": "638480",
    "end": "644079"
  },
  {
    "text": "that we're looking at for houses we have some idea \nof what that means they should be similar size  ",
    "start": "644080",
    "end": "650240"
  },
  {
    "text": "they should be similar quality of construction \nthey should be similar types of houses similar  ",
    "start": "650800",
    "end": "657279"
  },
  {
    "text": "numbers of bedrooms we have some idea of what \nthat means and for many other things it won't  ",
    "start": "657280",
    "end": "663120"
  },
  {
    "text": "be so obvious as to what it means in any \nnumerical sense but it might be obvious  ",
    "start": "663120",
    "end": "668720"
  },
  {
    "text": "in an intuitive sense we know that for example two \nimages that are both images of cats are similar to  ",
    "start": "669360",
    "end": "675839"
  },
  {
    "text": "each other but they're distinct from an image of a \ndog and that's something that we're going to have  ",
    "start": "675840",
    "end": "683520"
  },
  {
    "text": "trouble quantifying we'll see there are \ntexts there are ways of um embedding in uh  ",
    "start": "683520",
    "end": "690960"
  },
  {
    "text": "in such a way as to have a faithful representation \nof such things and we will get to those",
    "start": "690960",
    "end": "695360"
  },
  {
    "text": "so there are lots of interesting examples one \nmight think about where it's not so obvious   how to construct a faithful embedding \nwe can think about names of people  ",
    "start": "698240",
    "end": "706240"
  },
  {
    "text": "professions companies countries \nlanguages zip codes cities songs movies",
    "start": "707200",
    "end": "716560"
  },
  {
    "text": "and we're going to see several of these and \nexamples and we'll also see some generic   general techniques for constructing",
    "start": "718960",
    "end": "725519"
  },
  {
    "text": "embeddings here's one suppose you have uh \nlocation data you have the position of something  ",
    "start": "725520",
    "end": "736320"
  },
  {
    "text": "on the world um so you might describe that \nby two numbers the latitude and longitude  ",
    "start": "737200",
    "end": "744160"
  },
  {
    "text": "that would give us a two-dimensional vector in r2   or you might say really this should be embedded \nin r3 we think about the the world as a  ",
    "start": "744160",
    "end": "756160"
  },
  {
    "text": "globe and the globe is has a natural \nembedding in three-dimensional space and so  ",
    "start": "756160",
    "end": "762480"
  },
  {
    "text": "we can embed our position in on the surface of \nthe globe as a position on the sphere in our sv  ",
    "start": "763680",
    "end": "771839"
  },
  {
    "text": "maybe that makes more sense if your \ndata points are spread over the planet",
    "start": "772960",
    "end": "776880"
  },
  {
    "text": "here's another example the day of the week um so \nwe have monday tuesday wednesday thursday friday  ",
    "start": "781040",
    "end": "788880"
  },
  {
    "text": "saturday sunday we could just enable label them \none through seven and that would be a an embedding  ",
    "start": "788880",
    "end": "794960"
  },
  {
    "text": "we should embed them uh by the day of the \nweek into a one-dimensional vector space  ",
    "start": "794960",
    "end": "800960"
  },
  {
    "text": "also on this slide we see a two-dimensional vector \nspace this is interesting this is useful because  ",
    "start": "802720",
    "end": "810160"
  },
  {
    "text": "for many types of data the similarity between days \nis that a day is similar to the day before and  ",
    "start": "810160",
    "end": "819040"
  },
  {
    "text": "a day after and um if we were to embed monday as \none and tuesday wednesday thursday friday saturday  ",
    "start": "819040",
    "end": "827120"
  },
  {
    "text": "sunday is seven then one and seven would be quite \nfar apart and so monday and sunday would be quite  ",
    "start": "827120",
    "end": "832720"
  },
  {
    "text": "far apart if we embed them like this so here \nwe've embedded them as seven points in our r2  ",
    "start": "832720",
    "end": "841360"
  },
  {
    "text": "well then we can see that the distance between \nmonday and tuesday and the distance between   monday and sunday those two distances are the \nsame and indeed the distance between any two  ",
    "start": "842960",
    "end": "853680"
  },
  {
    "text": "consecutive days is uh is the same and so \nthis is an embedding that more faithfully  ",
    "start": "854720",
    "end": "861680"
  },
  {
    "text": "captures that idea and we can do that for lots \nof different notions of time we can do that for  ",
    "start": "861680",
    "end": "866960"
  },
  {
    "text": "um months of the year which you might \nembed as 12 points we can do that for hours  ",
    "start": "867680",
    "end": "875279"
  },
  {
    "text": "of the day which we might embed around the \ncircle that's 12 points around the circle",
    "start": "875280",
    "end": "883840"
  },
  {
    "text": "now here's a more sophisticated example this \nis a word to beg and that's a mapping on words  ",
    "start": "887920",
    "end": "896639"
  },
  {
    "text": "so you give the here the field type is a \nword a word in the english language and um  ",
    "start": "896640",
    "end": "905120"
  },
  {
    "text": "words to vec for each word in the english \nlanguage you map it will map it to a vector  ",
    "start": "906960",
    "end": "913600"
  },
  {
    "text": "and that vector is a 300 dimensional vector um and so where to beg actually will work on \nwords and short phrases it doesn't just include  ",
    "start": "913600",
    "end": "924640"
  },
  {
    "text": "dictionary words but it also includes names \nof people and names of places and many other  ",
    "start": "926560",
    "end": "932160"
  },
  {
    "text": "things and it's developed from a data set which \ncomes from google news which contains 100 billion  ",
    "start": "932160",
    "end": "938720"
  },
  {
    "text": "words and so this is not a hand constructed \nembedding or hand constructed feature map this is  ",
    "start": "938720",
    "end": "945360"
  },
  {
    "text": "an automatically constructed embedding and we will \nsay more about how that is done later in the class",
    "start": "945360",
    "end": "953839"
  },
  {
    "text": "the principal idea here is that words that \nfrequently appear near each other should get  ",
    "start": "956080",
    "end": "962720"
  },
  {
    "text": "nearby vectors and um ah so we might look at that \nnow of course it's hard to look at the specific  ",
    "start": "962720",
    "end": "973600"
  },
  {
    "text": "vectors that correspond to where to vect because \num they're 300 dimensional so we can't plot them  ",
    "start": "973600",
    "end": "980959"
  },
  {
    "text": "but of course we can use them for all sorts \nof predictor construction however here we   can we can just plot some of their components \nso here are two of their components x1 and x2  ",
    "start": "981840",
    "end": "990640"
  },
  {
    "text": "of a bunch of different words \nlet's zoom in a bit here",
    "start": "991600",
    "end": "995839"
  },
  {
    "text": "and so you can see even when you look at a \ntwo-dimensional projection of this 300 dimensional  ",
    "start": "1001920",
    "end": "1007519"
  },
  {
    "text": "vector space you can see that words that sort of \nmean similar things are kind of close together  ",
    "start": "1007520",
    "end": "1014560"
  },
  {
    "text": "uh let's look here we've got uh optimistic \nlet's see if i can make that a bit smaller  ",
    "start": "1015440",
    "end": "1023840"
  },
  {
    "text": "we've got optimistic proud enthusiastic happy \nsatisfied eager interested amazed these are all  ",
    "start": "1025120",
    "end": "1036400"
  },
  {
    "text": "kind of next to each other down here uh we've also \ngot uh melancholy sorrow hostile bitter awkward  ",
    "start": "1036400",
    "end": "1048480"
  },
  {
    "text": "guilty anguished hopeless vengeful lonely what \nkind of negative words sitting at the top there",
    "start": "1048480",
    "end": "1056880"
  },
  {
    "text": "we've got some others we've got some uh irritated \nexasperated angry embarrassed frustrated scared  ",
    "start": "1063200",
    "end": "1075039"
  },
  {
    "text": "all of these kind of similar words over there and \nso here are these are just a selection of words  ",
    "start": "1075760",
    "end": "1082240"
  },
  {
    "text": "these are the emotion words of course there are uh \nthree million words in the database um and this is  ",
    "start": "1082240",
    "end": "1090240"
  },
  {
    "text": "just a two-dimensional projection but nonetheless \none can see the what it does in terms of grouping  ",
    "start": "1090240",
    "end": "1095280"
  },
  {
    "text": "there are also some interesting properties of word \nto vec some nice other types of faithfulness of  ",
    "start": "1096320",
    "end": "1103440"
  },
  {
    "text": "the data that comes out for example if i take the \nvector for king and subtract the vector for man",
    "start": "1103440",
    "end": "1114559"
  },
  {
    "text": "and then i add the vector for woman then the closest vector that i get to the result \nis the vector for queen similarly if i take",
    "start": "1116640",
    "end": "1130960"
  },
  {
    "text": "rome and subtract italy and add france \nthen the closest vector i get is paris  ",
    "start": "1133600",
    "end": "1139200"
  },
  {
    "text": "and so these kinds of things are somewhat \nsurprising properties of the embedding that  ",
    "start": "1140320",
    "end": "1146240"
  },
  {
    "text": "are quite magical but illustrate the point that   somehow the embedding is capturing the \nstructure of the relationship between words  ",
    "start": "1147280",
    "end": "1157840"
  },
  {
    "text": "um which is uh what's going to enable it to \nbe useful when we actually use it for learning",
    "start": "1158800",
    "end": "1163840"
  },
  {
    "text": "let's turn now to a sophisticated method of \nembedding image data and uh this is uh called  ",
    "start": "1173920",
    "end": "1185440"
  },
  {
    "text": "vgg16 so imagenet is an open database of images \nit contains about 14 million labeled images  ",
    "start": "1185440",
    "end": "1198000"
  },
  {
    "text": "in about a thousand different classes \nand they range over all sorts of   different things they include many images \nof animals of vehicles of everyday objects",
    "start": "1198960",
    "end": "1211040"
  },
  {
    "text": "shoes that's a very broad collection of images \nbut they've all been labeled and so",
    "start": "1213200",
    "end": "1223679"
  },
  {
    "text": "we know exactly what is in each of those images \nso vgg16 is an embedding which maps images um  ",
    "start": "1225760",
    "end": "1237840"
  },
  {
    "text": "to a vector in rd and here d is 4096. so we \nget a 4096 long vector and the images that we  ",
    "start": "1237840",
    "end": "1250000"
  },
  {
    "text": "start with are all cropped down to 224 by 224 \npixels their color so they have rgb components  ",
    "start": "1250000",
    "end": "1257760"
  },
  {
    "text": "and that is uh some uh 150 160 odd thousand \ncomponents 160 odd uh components of each  ",
    "start": "1258720",
    "end": "1271760"
  },
  {
    "text": "image and we have three numbers per pixel and so \nwe're going from what we might think of as a as a  ",
    "start": "1271760",
    "end": "1279200"
  },
  {
    "text": "vector in 160 000 dimensions and we're mapping \nit down to a vector in 40 96 dimensions  ",
    "start": "1280080",
    "end": "1288159"
  },
  {
    "text": "and vgg16 it's a neural network \nand it was originally developed to   classify the images and so it learned from the \nimagenet database the uh a way of mapping images  ",
    "start": "1289360",
    "end": "1304240"
  },
  {
    "text": "to labels but it's also been repurposed as \na general method of constructing a feature  ",
    "start": "1304240",
    "end": "1312400"
  },
  {
    "text": "map it's a general method of embedding images \nin vector space now this particular neural  ",
    "start": "1312400",
    "end": "1319120"
  },
  {
    "text": "network has 16 layers and its input is an \nimage and its output is the vector in r4096",
    "start": "1319120",
    "end": "1327120"
  },
  {
    "text": "let's look at what it looks like so here are \nsix different uh images they're each 224 by 224",
    "start": "1329520",
    "end": "1340800"
  },
  {
    "text": "image one is simply the ubuntu mug 2 and \n3 are some guy image 4 is professor steven  ",
    "start": "1344080",
    "end": "1356320"
  },
  {
    "text": "boyd image 5 is a car it happens to be a a mini \ntraveler edition from the 1960s and the image  ",
    "start": "1356320",
    "end": "1366240"
  },
  {
    "text": "6 is a london bus for those of you who haven't \nyet figured out for my accent i am english and  ",
    "start": "1366240",
    "end": "1375280"
  },
  {
    "text": "this particular car is actually exactly the same \ntype of car my family drove about in when i was",
    "start": "1376400",
    "end": "1383920"
  },
  {
    "text": "10 years old and this is exactly the \nkind of london bus that i would have  ",
    "start": "1386160",
    "end": "1392480"
  },
  {
    "text": "ridden on back in those days \nalthough the car that we had was   blue not green um so let's take these \nimages um and embed them under the vgg16  ",
    "start": "1393120",
    "end": "1408080"
  },
  {
    "text": "map and for each one of them we're going to get \na 4096 long vector and what we've done here is uh  ",
    "start": "1408800",
    "end": "1416320"
  },
  {
    "text": "com looked at it looked at those images in pairs \nand for each pair of images computed the distance  ",
    "start": "1416320",
    "end": "1424000"
  },
  {
    "text": "in terms of the euclidean norm between \nthe two corresponding embedded vectors",
    "start": "1424960",
    "end": "1432080"
  },
  {
    "text": "so let's look at this embedding the matrix \nhere is a matrix of the pairwise distances  ",
    "start": "1435440",
    "end": "1442879"
  },
  {
    "text": "between the embedded vectors of each of the \nimages so for example this number right here  ",
    "start": "1442880",
    "end": "1450400"
  },
  {
    "text": "is the 2 3 entry of the matrix and so that's \nthe distance between image 2 and image 3.  ",
    "start": "1450400",
    "end": "1460640"
  },
  {
    "text": "both of which are images of me and so we expect \nthem to be close if this is a faithful embedding",
    "start": "1461440",
    "end": "1466399"
  },
  {
    "text": "if we compare that distance between with the \ndistance between image two and image five well",
    "start": "1469360",
    "end": "1479040"
  },
  {
    "text": "then suddenly we're looking at the a number of 109 \ncompared to 63 which is substantially different  ",
    "start": "1481280",
    "end": "1487840"
  },
  {
    "text": "and we can see that the two images of me are \nrather close to each other whereas image 2 is  ",
    "start": "1488880",
    "end": "1495120"
  },
  {
    "text": "quite far from an image of a mini and same for \nthe distance between an image of me and uh the  ",
    "start": "1495120",
    "end": "1505360"
  },
  {
    "text": "image of the bus those are also quite far apart \nand the same is true for the other images of me  ",
    "start": "1505360",
    "end": "1511040"
  },
  {
    "text": "image three to image five and image three \nto image six are also quite far apart  ",
    "start": "1512560",
    "end": "1518000"
  },
  {
    "text": "uh and if we compare Stephen to the the bus and the \ncar he's also quite distinct from a bus and a car",
    "start": "1519440",
    "end": "1527840"
  },
  {
    "text": "however the distance between image 2 and image 4",
    "start": "1530000",
    "end": "1536080"
  },
  {
    "text": "we're both people those are both head shots we \ncan see that those numbers are quite a lot smaller  ",
    "start": "1538400",
    "end": "1543680"
  },
  {
    "text": "than the corresponding distances between \nhumans and people if we compare image  ",
    "start": "1543680",
    "end": "1550000"
  },
  {
    "text": "three and four that's the other image of \nme and Stephen they're also quite close  ",
    "start": "1550000",
    "end": "1555840"
  },
  {
    "text": "well there's one more interesting \npair that's comparing the vehicles   so that's this number right here that's the \ndistance between image five and image six  ",
    "start": "1557840",
    "end": "1566320"
  },
  {
    "text": "so that's this distance right there and we can see \nthat the two vehicles are considered much closer  ",
    "start": "1567360",
    "end": "1574960"
  },
  {
    "text": "at 86 than a vehicle in any person \nwhich are all about a hundred",
    "start": "1574960",
    "end": "1579520"
  },
  {
    "text": "so this image this embedding is \nfaithful uh we can also look at the um",
    "start": "1581840",
    "end": "1586240"
  },
  {
    "text": "the uh the images of the mug you \ncan see that the images of the mug",
    "start": "1590080",
    "end": "1594159"
  },
  {
    "text": "are all quite far from all the other images in \nterms of euclidean norm distance",
    "start": "1597120",
    "end": "1603840"
  },
  {
    "text": "and so we can say well this is a reasonably \nfaithful embedding it's it's it's surprisingly  ",
    "start": "1606720",
    "end": "1614000"
  },
  {
    "text": "effective at figuring out that three \nfaces are kind of similar to each other  ",
    "start": "1614000",
    "end": "1620160"
  },
  {
    "text": "two vehicles are kind of similar \nto each other but faces are really   different from vehicles and mugs are different \nfrom faces and mugs are different from vehicles",
    "start": "1620720",
    "end": "1630960"
  },
  {
    "text": "notice here that we didn't give it data that it \nhad been developed for we i picked these images  ",
    "start": "1636240",
    "end": "1642880"
  },
  {
    "text": "from uh some other source none of these images are \nas far as i know on image on imagenet and so here  ",
    "start": "1643760",
    "end": "1652400"
  },
  {
    "text": "we're repurposing uh a trained set of features \nand we're using effectively different images  ",
    "start": "1652400",
    "end": "1660560"
  },
  {
    "text": "and that sort of idea where one takes \na model that has been developed using a  ",
    "start": "1662000",
    "end": "1669760"
  },
  {
    "text": "particular data set and then uses it for a \ndifferent learning task is called transfer",
    "start": "1669760",
    "end": "1676000"
  },
  {
    "text": "learning",
    "start": "1676000",
    "end": "1681840"
  },
  {
    "text": "now we usually assume that an embedding is \nstandardized that's not always the case but  ",
    "start": "1684640",
    "end": "1691040"
  },
  {
    "text": "it is very often the case and certainly when we \nconstruct embeddings by hand we go out of our  ",
    "start": "1691040",
    "end": "1696160"
  },
  {
    "text": "way to make sure they're standardized what \nthat means is that they're centered around   zero and they have an rms value around one and \nof course that means for a particular data set  ",
    "start": "1696160",
    "end": "1706400"
  },
  {
    "text": "we take all of the records u in a data set and \nuh we look at the we take each of them and embed  ",
    "start": "1706960",
    "end": "1717360"
  },
  {
    "text": "them under phi so we get a whole bunch of phi \nu's which is a bunch of vectors and that set  ",
    "start": "1717360",
    "end": "1722880"
  },
  {
    "text": "of vectors should be centered around zero and that \nset of vectors should have an rms value around one  ",
    "start": "1722880",
    "end": "1728720"
  },
  {
    "text": "um and so that means that roughly the entries of \nfive u range between plus and minus 1 very roughly",
    "start": "1730080",
    "end": "1738880"
  },
  {
    "text": "and with standard embeddings that means \nthat if i've embedded each of these features  ",
    "start": "1741840",
    "end": "1747679"
  },
  {
    "text": "phi 1 up to phi r here into to have this \nproperty that they're all centered around 0  ",
    "start": "1749520",
    "end": "1756480"
  },
  {
    "text": "and have rms value around 1 that means \nthey're all comparable a large value of  ",
    "start": "1757040",
    "end": "1763600"
  },
  {
    "text": "x1 which is phi 1 of u1 and a large value of x2 \nphi 2 of u2 will both be around 1 or 2 and a small  ",
    "start": "1764800",
    "end": "1776880"
  },
  {
    "text": "value will both be around minus 1 or minus two \num and it means that we can compare them we don't  ",
    "start": "1776880",
    "end": "1783200"
  },
  {
    "text": "need to worry about what units the original data \nu1 and u2 was in or what magnitude they naturally  ",
    "start": "1783200",
    "end": "1790320"
  },
  {
    "text": "have we've normalized and we might measure the \ndistance between two embedded vectors using the  ",
    "start": "1790320",
    "end": "1800080"
  },
  {
    "text": "rms and so we take the rms of the vector 5u minus \nphi of utilda that's a reasonable measure of how  ",
    "start": "1800080",
    "end": "1807120"
  },
  {
    "text": "close records you and you today are that's again \na consequence of the faithfulness of the embedding",
    "start": "1807120",
    "end": "1813840"
  },
  {
    "text": "and so the way we ensure that all of our data has \nthis property that is centered at zero and has  ",
    "start": "1817200",
    "end": "1826559"
  },
  {
    "text": "uh values that range between plus and minus \none you do what's called standardization   it's also called z scoring um so suppose \nwe've got real numbers in the the field type  ",
    "start": "1827360",
    "end": "1840000"
  },
  {
    "text": "and then we've got u1 through \nun which are real numbers  ",
    "start": "1840960",
    "end": "1844720"
  },
  {
    "text": "uh then what we might do is we \nmight define u bar to be the",
    "start": "1846720",
    "end": "1853600"
  },
  {
    "text": "average of the uis and stood u to be the standard \ndeviation of the use and then in order to  ",
    "start": "1856640",
    "end": "1868240"
  },
  {
    "text": "construct uh a vector or in this case a scalar x \nwhich is an embedding of u which is normalized we  ",
    "start": "1868240",
    "end": "1881040"
  },
  {
    "text": "do what's called the z-score transformation where \nwe take u subtract it from subtract from it u bar",
    "start": "1881040",
    "end": "1889040"
  },
  {
    "text": "and then divide by the standard deviation",
    "start": "1891520",
    "end": "1894560"
  },
  {
    "text": "and that ensures that first of all \nthat the average of the x's is zero",
    "start": "1896720",
    "end": "1903120"
  },
  {
    "text": "and the standard deviation of the x's is one",
    "start": "1905600",
    "end": "1908960"
  },
  {
    "text": "so these are very easy to \ninterpret all of a sudden   if i've got uh z scored features and i \nlook at x when i see an x which is 1.3  ",
    "start": "1911840",
    "end": "1921920"
  },
  {
    "text": "well that means that the corresponding u is one \npoint three standard deviations above the mean",
    "start": "1922480",
    "end": "1933840"
  },
  {
    "text": "now another transformation we might do be \nuh before z-scoring is taking the log now  ",
    "start": "1936240",
    "end": "1943600"
  },
  {
    "text": "this is a very standard rule of thumb if \nyou've got a field u which is a positive  ",
    "start": "1943600",
    "end": "1950000"
  },
  {
    "text": "number but it ranges over a very wide scale \nthen you embed it as phi of u is equal to log  ",
    "start": "1950000",
    "end": "1957360"
  },
  {
    "text": "of u sometimes you'll use log of one plus u if u \nis allowed to be zero and then your standardized",
    "start": "1957360",
    "end": "1965760"
  },
  {
    "text": "so for example if u is the amount of traffic \nthat a particular website receives well then  ",
    "start": "1968240",
    "end": "1976480"
  },
  {
    "text": "you've got a bunch of different websites \nand each one has a different amount of   traffic and there is a huge difference \nbetween the amount of traffic that the  ",
    "start": "1976480",
    "end": "1985840"
  },
  {
    "text": "big websites like amazon and google \nreceive and then rather smaller websites  ",
    "start": "1987120",
    "end": "1991680"
  },
  {
    "text": "such as my own website receives and these \nare many many orders of magnitude different",
    "start": "1992880",
    "end": "2000080"
  },
  {
    "text": "and so uh with this kind of thing it's much \nbetter to instead of trying to uh look at  ",
    "start": "2002400",
    "end": "2010800"
  },
  {
    "text": "numbers over such a wide range of scales to take \nthe logs and then we might conclude that well  ",
    "start": "2010800",
    "end": "2018560"
  },
  {
    "text": "a thousand and eleven hundred are very \nsimilar 20 and 22 are also very similar  ",
    "start": "2020240",
    "end": "2026400"
  },
  {
    "text": "um but 20 and 120 are not similar if we were to \nmeasure those in absolute terms we might say well  ",
    "start": "2027360",
    "end": "2034880"
  },
  {
    "text": "20 and 120 are a hundred apart and a thousand \nand eleven hundred are a hundred apart and so we  ",
    "start": "2034880",
    "end": "2042640"
  },
  {
    "text": "should consider them both equally similar and 20 \nand 22 are only two apart so they're very similar  ",
    "start": "2042640",
    "end": "2048560"
  },
  {
    "text": "but actually if we look at it in a log scale \nwe'll find that 20 and 22 and a thousand and 1100  ",
    "start": "2049680",
    "end": "2057520"
  },
  {
    "text": "are both equally different or equally \nsimilar and 20 and 120 are much further apart  ",
    "start": "2058320",
    "end": "2063920"
  },
  {
    "text": "and because we're taking logs what \nwe end up measuring is the relative   difference between the values rather than \nthe absolute difference between the values",
    "start": "2065360",
    "end": "2074720"
  },
  {
    "text": "another way to say it is that 22 is only 10 more \nthan 20. 1100 is only 10 more than a thousand  ",
    "start": "2078880",
    "end": "2087839"
  },
  {
    "text": "and here's an example for house price prediction \nwe want to predict uh the house selling price v  ",
    "start": "2089600",
    "end": "2096080"
  },
  {
    "text": "we've got a record which in this \ncase just has two fields u1 and u2   u1 is the area in square feet \nu2 is the number of bedrooms",
    "start": "2096960",
    "end": "2106480"
  },
  {
    "text": "we care about relative errors in price so we're \nmuch more we can say if two house prices differ by  ",
    "start": "2108640",
    "end": "2116000"
  },
  {
    "text": "ten percent we will consider them as \nsimilar um and uh that allows us to consider  ",
    "start": "2116000",
    "end": "2123360"
  },
  {
    "text": "uh a million dollars and one point \none million dollars is just as similar   as a hundred thousand dollars and a \nhundred and ten thousand dollars um",
    "start": "2123360",
    "end": "2131920"
  },
  {
    "text": "and so we'll embed the price v as using psi \nof v as log v and then we'll standardize  ",
    "start": "2134800",
    "end": "2143840"
  },
  {
    "text": "we also standardize the records u in particular \nwe stand aside standardize the fields u1 and u2  ",
    "start": "2144880",
    "end": "2153839"
  },
  {
    "text": "and we do that by subtracting the corresponding \nmeans and dividing by the standard deviations  ",
    "start": "2154640",
    "end": "2159680"
  },
  {
    "text": "and the reason we do this is easy to see if \nwe consider having two different records so  ",
    "start": "2160800",
    "end": "2167200"
  },
  {
    "text": "we suppose i have a record u and a record \nu-tilde and then i'd like to say well how  ",
    "start": "2167200",
    "end": "2172400"
  },
  {
    "text": "close are you in utila well let's just write that \ndown well then u minus u tilde the 2 norm squared",
    "start": "2172400",
    "end": "2182480"
  },
  {
    "text": "that's going to be equal to u 1 minus u 2 \n1 squared plus u 2 minus u tilde 2 squared  ",
    "start": "2184640",
    "end": "2194799"
  },
  {
    "text": "and so the first term in this is the \ndifference in the number of bedrooms  ",
    "start": "2196480",
    "end": "2204160"
  },
  {
    "text": "of house you and house utilde and the \nsecond is the difference in the price  ",
    "start": "2204960",
    "end": "2213119"
  },
  {
    "text": "of house you and house utilita and both of \nthose quantities are squared and added up  ",
    "start": "2213120",
    "end": "2217920"
  },
  {
    "text": "of course the trouble is is that even a \nvery small change in price is going to swamp",
    "start": "2218960",
    "end": "2225280"
  },
  {
    "text": "a change in number of bedrooms and yet a change \nin the number of bedrooms from three to four  ",
    "start": "2227520",
    "end": "2232640"
  },
  {
    "text": "could be hugely significant much more so than \na change in the price from 200 000 to 250 000.  ",
    "start": "2232640",
    "end": "2241839"
  },
  {
    "text": "so standardizing puts these two different fields \non equal footing within our embedded vector space",
    "start": "2242960",
    "end": "2255839"
  },
  {
    "text": "so then we're going to predict why y is log v \nthe log of the price from the standardized area  ",
    "start": "2258560",
    "end": "2266320"
  },
  {
    "text": "in the standardized number of bedrooms we'll \nhave a linear predictor which will be y hat  ",
    "start": "2266320",
    "end": "2272240"
  },
  {
    "text": "is theta 1 plus theta 2 x 1 plus theta 2 \ntimes x 2 plus theta 3 times x 2. i'm sorry  ",
    "start": "2272240",
    "end": "2280640"
  },
  {
    "text": "and so in terms of the original data we're going \nto uh so how does this work we start off with a  ",
    "start": "2282640",
    "end": "2289599"
  },
  {
    "text": "a bunch of data which we use \nin order to determine theta 1   theta 2 and theta 3. and then we get given a new \nhouse that we've never seen before a new record  ",
    "start": "2290320",
    "end": "2300720"
  },
  {
    "text": "and that gives us a u1 and a u2 a number of \nbedrooms and a square footage we take that number  ",
    "start": "2301600",
    "end": "2309920"
  },
  {
    "text": "of bedrooms and number of square footage and we \nstandardize them to get an x one and next two  ",
    "start": "2309920",
    "end": "2315200"
  },
  {
    "text": "we then apply our predictor y hat is theta one \nplus theta two x one plus theta three x two to  ",
    "start": "2316800",
    "end": "2323600"
  },
  {
    "text": "get a y hat which is a prediction of the log \nof the price and so our predicted price is  ",
    "start": "2323600",
    "end": "2332640"
  },
  {
    "text": "constructed by inverting that embedding y is log v \nby to using v hat is the exponential of y so v hat  ",
    "start": "2332640",
    "end": "2342000"
  },
  {
    "text": "is going to be x of theta 1 plus theta 2 times u 1 \nminus mu 1 on sigma 1 plus theta 3 times u 2 minus  ",
    "start": "2342000",
    "end": "2351520"
  },
  {
    "text": "mu 2 and sigma 2. and this is very interpretable \num so for example if here theta 2 is 0.7  ",
    "start": "2351520",
    "end": "2361760"
  },
  {
    "text": "what does that mean well it means that every \ntime u1 increases by one standard deviation",
    "start": "2362560",
    "end": "2374080"
  },
  {
    "text": "that means the price of the \nhouse is going to get multiplied   by the exponential of 0.7 which \nis about 2 so every change in",
    "start": "2376400",
    "end": "2392640"
  },
  {
    "text": "u1 which i think is area yeah every change in area \nby one standard deviation doubles the house price",
    "start": "2395760",
    "end": "2409840"
  },
  {
    "text": "let's look at some more complicated embeddings   where we take an individual \nfield and embed it as a vector",
    "start": "2412720",
    "end": "2419040"
  },
  {
    "text": "so we can embed a field into a vector xs5 view \nand now x here lives in rk that's the dimension  ",
    "start": "2422800",
    "end": "2433600"
  },
  {
    "text": "in which that particular field is embedded and \nit's useful to have a k which is more than one  ",
    "start": "2433600",
    "end": "2441360"
  },
  {
    "text": "even when the quantity that you're embedding the \nraw data you is just a raw it's just a real scalar  ",
    "start": "2441360",
    "end": "2447200"
  },
  {
    "text": "we've already seen that with the polynomial \nembedding where phi of u is one u u squared  ",
    "start": "2448560",
    "end": "2454720"
  },
  {
    "text": "up to u to the d and that's useful because it \nallows us to use a linear predictor to construct",
    "start": "2454720",
    "end": "2462400"
  },
  {
    "text": "a predictor which is a non-linear function of u   even though the predictor \nis a linear function of x",
    "start": "2465200",
    "end": "2472080"
  },
  {
    "text": "and there are other things you can do you can \nsimilarly use other nonlinear functions you might   embed it as 1 u minus u plus it's worth drawing \nwhat these functions are so let me try and do that",
    "start": "2474240",
    "end": "2487760"
  },
  {
    "text": "so this is going to be u and then",
    "start": "2493200",
    "end": "2496640"
  },
  {
    "text": "i'll plot the function use there \nand i will also plot the function",
    "start": "2499280",
    "end": "2507840"
  },
  {
    "text": "0 which just sits right there and then u subscript \nminus is the minimum of these two functions  ",
    "start": "2510400",
    "end": "2518160"
  },
  {
    "text": "which means that there is u subscript minus",
    "start": "2518880",
    "end": "2525519"
  },
  {
    "text": "and u subscript plus is the \nmaximum of these two functions",
    "start": "2530160",
    "end": "2535839"
  },
  {
    "text": "and so that means that this here",
    "start": "2539600",
    "end": "2543600"
  },
  {
    "text": "is u plus so i've got two functions we \nthink about u plus as being the positive  ",
    "start": "2548080",
    "end": "2556720"
  },
  {
    "text": "part of u and u minus as being the negative \npart of u now what that means if i'm trying  ",
    "start": "2556720",
    "end": "2566560"
  },
  {
    "text": "going to construct a linear predictor with these \ntwo features it means well let's write that down",
    "start": "2566560",
    "end": "2577840"
  },
  {
    "text": "uh let us so i'm going to have a linear predictor \nwhich is a function of these two features  ",
    "start": "2582960",
    "end": "2590320"
  },
  {
    "text": "i'm going to have y hat which is going to be \ntheta 1 plus theta 2 over u plus plus theta 3",
    "start": "2590320",
    "end": "2602720"
  },
  {
    "text": "of u minus and uh so this is a constant this is a \nlinear function of u on the positive part of when  ",
    "start": "2605360",
    "end": "2618720"
  },
  {
    "text": "u is greater than zero and it's a constant when \nu is negative and this is a linear function of u  ",
    "start": "2618720",
    "end": "2626480"
  },
  {
    "text": "for when u is negative and it's a constant when \nu is positive what that means is that my function",
    "start": "2627760",
    "end": "2634640"
  },
  {
    "text": "y hat is a function of u i can plot \nit and it will look something",
    "start": "2639040",
    "end": "2647840"
  },
  {
    "text": "like this it has a particular slope here a \nparticular slope here and a particular intercept  ",
    "start": "2651120",
    "end": "2658800"
  },
  {
    "text": "there and those three quantities are determined by \ntheta one theta two and theta three in particular  ",
    "start": "2658800",
    "end": "2668400"
  },
  {
    "text": "the intercept is theta one the \nslope of this part of the line  ",
    "start": "2669680",
    "end": "2676240"
  },
  {
    "text": "is theta 2 and the slope of this part of the \nline is theta 3. and so i have a piecewise",
    "start": "2677040",
    "end": "2685200"
  },
  {
    "text": "linear function",
    "start": "2689280",
    "end": "2692400"
  },
  {
    "text": "now it's also worth being aware that when one \ntalks about piecewise linear functions often one  ",
    "start": "2694640",
    "end": "2700799"
  },
  {
    "text": "thinks about that meaning a function where the \njoint it is composed of linear segments and the  ",
    "start": "2700800",
    "end": "2709280"
  },
  {
    "text": "joins between the linear segments may be anywhere \nand i may have any number of segments here  ",
    "start": "2709280",
    "end": "2715840"
  },
  {
    "text": "we're choosing predictors which are piecewise \nlinear functions but they're very specific   piecewise linear functions they only have two  ",
    "start": "2715840",
    "end": "2723920"
  },
  {
    "text": "linear pieces and the join has to \nbe at the origin join has to be",
    "start": "2724720",
    "end": "2731840"
  },
  {
    "text": "somewhere there",
    "start": "2734160",
    "end": "2738100"
  },
  {
    "text": "this particular quantity the   the origin in this case is called a not for \nthis piecewise linear function that's a knot",
    "start": "2741840",
    "end": "2752640"
  },
  {
    "text": "and you might have more complicated piecewise \nlinear functions which you could construct  ",
    "start": "2757200",
    "end": "2762720"
  },
  {
    "text": "using a different embedding so for example if my \nembedding was 1 u plus u minus and u minus 1 plus  ",
    "start": "2762720",
    "end": "2780240"
  },
  {
    "text": "well then there'd be an additional not out here at \n1 when i'd be able to pick a different slope and  ",
    "start": "2780880",
    "end": "2788400"
  },
  {
    "text": "then slope would be determined by these additional \nparameters well i've got four parameters theta  ",
    "start": "2788400",
    "end": "2796160"
  },
  {
    "text": "one theta two theta three and theta four it's \nworth noticing also that the slope of this line  ",
    "start": "2796160",
    "end": "2801839"
  },
  {
    "text": "will not just be the coefficient in front \nof u minus 1 plus but it will in fact be  ",
    "start": "2802400",
    "end": "2808400"
  },
  {
    "text": "the coefficient the sum of the coefficient of \nu minus 1 plus and the coefficient of u plus",
    "start": "2808400",
    "end": "2815839"
  },
  {
    "text": "now then a particular type of data field is called \ncategorical and what that means is it only takes  ",
    "start": "2827280",
    "end": "2833680"
  },
  {
    "text": "a finite number of possible values in other words \nthe set script u is a finite set and we've called  ",
    "start": "2833680",
    "end": "2842960"
  },
  {
    "text": "the entries in that set alpha 1 up to alpha \nk those are called the category labels  ",
    "start": "2842960",
    "end": "2850960"
  },
  {
    "text": "sometimes we'll use category labels 1 \nthrough k and we'll refer to category i   sometimes will be explicit about alpha 1 \nto alpha k and refer to category alpha i  ",
    "start": "2852640",
    "end": "2861840"
  },
  {
    "text": "depending on what works best at that particular \nnotational point lots of things are categorical  ",
    "start": "2862640",
    "end": "2870559"
  },
  {
    "text": "variables the most common is the boolean true \nor false there's only two possible values  ",
    "start": "2870560",
    "end": "2875840"
  },
  {
    "text": "we might be trying to identify fruit in which case",
    "start": "2877520",
    "end": "2882240"
  },
  {
    "text": "we might have apple orange and banana we might \nhave a field which is the day of the week  ",
    "start": "2884480",
    "end": "2890240"
  },
  {
    "text": "in which case there are seven possible values \nmonday through sunday we might have zip codes  ",
    "start": "2890800",
    "end": "2896720"
  },
  {
    "text": "that's 40 000 possible values roughly um you \ncould just say well embed that as a real number  ",
    "start": "2897600",
    "end": "2904880"
  },
  {
    "text": "but then we'd ask the question well is that \nfaithful and we might conclude well there's  ",
    "start": "2905840",
    "end": "2912560"
  },
  {
    "text": "no particular reason why two zip codes that are \nnumerically close should be physically close  ",
    "start": "2912560",
    "end": "2918000"
  },
  {
    "text": "sometimes they are and sometimes they are not uh countries of course they're not numerical \nat all and so there are maybe about 180 or so  ",
    "start": "2919040",
    "end": "2929440"
  },
  {
    "text": "different countries that we might list \nin just a bunch of different categories  ",
    "start": "2929440",
    "end": "2933839"
  },
  {
    "text": "uh languages there are several \nthousand different languages   um at least those which are \nspoken by a large number of people",
    "start": "2935680",
    "end": "2942960"
  },
  {
    "text": "now there's a particular way of embedding \ncategoricals which is very common and very useful   and that's called the one hot embedding so \nlet's just refer to our categories now as  ",
    "start": "2950880",
    "end": "2960480"
  },
  {
    "text": "one through k so that script u is has k positive \nit consists of the numbers one two k any the value  ",
    "start": "2960480",
    "end": "2969119"
  },
  {
    "text": "of the field is either going to be 1 or 2 or up \nto k so the one hot embedding says if you see",
    "start": "2969120",
    "end": "2976800"
  },
  {
    "text": "u is equal to i then embed that as e subscript \ni remember what e subscript i is so in  ",
    "start": "2981440",
    "end": "2990800"
  },
  {
    "text": "a vector space e 1 is just the vector \n1 0 0 e 2 is just the vector 0 1 0 0  ",
    "start": "2992320",
    "end": "3005920"
  },
  {
    "text": "e 3 is the vector 0 0 one zero and so on people \ncall it the e i the eighth canonical basis vector  ",
    "start": "3007520",
    "end": "3018080"
  },
  {
    "text": "in a vector space and so in a k dimensional \nvector space there are k different canonical basis  ",
    "start": "3018960",
    "end": "3025040"
  },
  {
    "text": "vectors and so we can embed the categories one \nthrough k as the k different basis vectors in rk",
    "start": "3025040",
    "end": "3034160"
  },
  {
    "text": "so in particular we might \nembed apple as one zero zero   orange as zero one zero \nand banana as zero zero one  ",
    "start": "3036560",
    "end": "3044559"
  },
  {
    "text": "we might embed true as one zero and false as \nzero one and this is a different embedding of the  ",
    "start": "3045760",
    "end": "3052480"
  },
  {
    "text": "booleans into r2 this time as opposed to the minus \none one embedding we saw earlier we might embed  ",
    "start": "3052480",
    "end": "3060560"
  },
  {
    "text": "languages there's a bunch of different canonical \nbasis vectors uh all the way up to 185 of them  ",
    "start": "3060560",
    "end": "3070400"
  },
  {
    "text": "um and if we uh standardize these features well \nthat allows us to handle unbalanced data because  ",
    "start": "3072160",
    "end": "3083200"
  },
  {
    "text": "when we standardize these features they \nwon't remain at one zero zero zero one   zero zero zero one they will get shifted by \nthe mean and scaled by the standard deviation  ",
    "start": "3084400",
    "end": "3097520"
  },
  {
    "text": "and in particular the mean and the \nstandard deviation of the data set   will be affected by the proportions of the \ndifferent categories that show up in the data set  ",
    "start": "3098400",
    "end": "3109840"
  },
  {
    "text": "and that's a nice way of \nhandling unbalanced data sets   and we will see some of the problems that show \nup with unbalanced data sets later in the class  ",
    "start": "3110560",
    "end": "3123840"
  },
  {
    "text": "there's also another way of doing uh one \nhot embedding which is called reduced   one hot embedding and here the idea is is that \ninstead of it mapping the categories 1 through k  ",
    "start": "3125840",
    "end": "3138240"
  },
  {
    "text": "to k dimensional space we map them \nto k minus 1 dimensional space",
    "start": "3138800",
    "end": "3143600"
  },
  {
    "text": "where we do this is we choose one \nof the categories say the last one   category k as the default value the nominal \nvalue and we embed that at the origin",
    "start": "3145680",
    "end": "3156640"
  },
  {
    "text": "all of the other categories 1 through k \nminus 1 we embed as the first k minus 1  ",
    "start": "3158880",
    "end": "3165440"
  },
  {
    "text": "canonical basis vectors e \n1 to e k minus 1 as before",
    "start": "3165440",
    "end": "3169359"
  },
  {
    "text": "if we do that for the booleans then we would \nembed true as 1 and false as 0 because here k  ",
    "start": "3172080",
    "end": "3179840"
  },
  {
    "text": "is two there's only two categories and so r \nk minus one is simply the real numbers and uh  ",
    "start": "3179840",
    "end": "3186560"
  },
  {
    "text": "so we're embedding booleans as one and zero and \nthis is a very common embedding of the booleans",
    "start": "3187120",
    "end": "3193840"
  },
  {
    "text": "there's another type of data a specific type \nof categorical data and that's categorical data  ",
    "start": "3202080",
    "end": "3209600"
  },
  {
    "text": "that's ordered now apple pear banana are very much \nnot ordered quantities there's no natural order  ",
    "start": "3209600",
    "end": "3218720"
  },
  {
    "text": "to put the fruit in but very often you do have \nuh orderings so for example when you uh uh give  ",
    "start": "3218720",
    "end": "3228960"
  },
  {
    "text": "something a rating on amazon and uh you're either \ngonna give it one two three four or five stars and  ",
    "start": "3228960",
    "end": "3237520"
  },
  {
    "text": "yes there's only five possible categories that you \ncan put it in but those definitely have an order",
    "start": "3238320",
    "end": "3244400"
  },
  {
    "text": "such a scale where there's \nuh an order to the categories  ",
    "start": "3247360",
    "end": "3254560"
  },
  {
    "text": "it is uh and it's used to indicate a preference is \ncalled a leicard scale l-i-k-e-r-t um yeah so for  ",
    "start": "3255280",
    "end": "3267120"
  },
  {
    "text": "example uh the original leica scale was strongly \ndisagree disagree neutral agree or strongly agree  ",
    "start": "3267120",
    "end": "3275040"
  },
  {
    "text": "you know leica was actually a person it's not just \na scale it measures how much you like something",
    "start": "3276000",
    "end": "3281200"
  },
  {
    "text": "like was a psychologist an american \npsychologist who lived in michigan  ",
    "start": "3283360",
    "end": "3291840"
  },
  {
    "text": "he lived for most of the 20th century the likert \nscale can be embedded into the real numbers  ",
    "start": "3293200",
    "end": "3303280"
  },
  {
    "text": "with values minus two minus one zero one or \ntwo and it's nice it's a faithful embedding  ",
    "start": "3303840",
    "end": "3310800"
  },
  {
    "text": "in the sense that it preserves the \nordering of the different categories  ",
    "start": "3310800",
    "end": "3316880"
  },
  {
    "text": "it's also quite reasonable to treat the lycra \nscale as a categorical with one heart embedding  ",
    "start": "3318320",
    "end": "3323840"
  },
  {
    "text": "into our five um that way is perhaps a little \nless faithful but both ways are very common",
    "start": "3324720",
    "end": "3333840"
  },
  {
    "text": "you can do the same thing with other \ncategoricals as well for example the   number of bedrooms in a house you could treat \nit as a real number or you could treat it as an  ",
    "start": "3338080",
    "end": "3347599"
  },
  {
    "text": "ordinal with values between one and six \nand both would be completely reasonable",
    "start": "3347600",
    "end": "3353840"
  },
  {
    "text": "so i want to talk right now about feature \nengineering this is the next stage after embedding  ",
    "start": "3358640",
    "end": "3365599"
  },
  {
    "text": "so in embedding we're starting with \nraw data and we are putting that data  ",
    "start": "3366240",
    "end": "3373600"
  },
  {
    "text": "into a vector space so that we can use \nmathematics to develop machine learning algorithms  ",
    "start": "3373600",
    "end": "3379600"
  },
  {
    "text": "the next stage is to think about feature \nengineering and in feature engineering the   basic idea is that we start with some features \nand then we process or transform them to make new  ",
    "start": "3380800",
    "end": "3392880"
  },
  {
    "text": "engineered features and instead of just having a \npredictor which is dependent of on the original  ",
    "start": "3392880",
    "end": "3401680"
  },
  {
    "text": "embedding we have a predictor which is in \nbit which is dependent on these new features  ",
    "start": "3402320",
    "end": "3406640"
  },
  {
    "text": "uh we can do this in lots of different ways and \nwe've seen some ways already so one example of  ",
    "start": "3408480",
    "end": "3414640"
  },
  {
    "text": "this is where we used a polynomial embedding of a \nreal raw data we embedded it as one u u squared u  ",
    "start": "3414640",
    "end": "3425359"
  },
  {
    "text": "cubed and so on and you could either think about \nthat as an embedding or as feature engineering  ",
    "start": "3425360",
    "end": "3431040"
  },
  {
    "text": "where we've chosen features in a particular way \nin the hope that it will improve our predictor  ",
    "start": "3431600",
    "end": "3437360"
  },
  {
    "text": "in that case what it does is it allows our \nlinear predictors to become polynomial predictors",
    "start": "3437920",
    "end": "3442480"
  },
  {
    "text": "now fundamentally the question you have to ask \nyourself is what feature engineering should you  ",
    "start": "3445040",
    "end": "3451840"
  },
  {
    "text": "do does it improve the predictor um and \nthere's an answer to this question that  ",
    "start": "3451840",
    "end": "3458720"
  },
  {
    "text": "is worth repeating to yourself throughout this \nclass and and that is is that the way you tell  ",
    "start": "3459360",
    "end": "3468080"
  },
  {
    "text": "whether one predictor is better than another \nis by validating the performance so if you want  ",
    "start": "3468080",
    "end": "3476400"
  },
  {
    "text": "to know whether these new features that you're \nproducing using feature engineering are helpful  ",
    "start": "3476400",
    "end": "3483119"
  },
  {
    "text": "are better than the original features then \ndevelop a predictor using the original features  ",
    "start": "3483680",
    "end": "3489599"
  },
  {
    "text": "developer predictor using the new features in both \ncases validate the performance against unseen data  ",
    "start": "3490560",
    "end": "3496800"
  },
  {
    "text": "if the performance with the new features is better \nwell then your feature engineering is actually   helpful and if it wasn't then you don't need to do \nthat particular piece of feature engineering and  ",
    "start": "3498400",
    "end": "3508400"
  },
  {
    "text": "look at something else the point is is that \nthere is no answer to this question there is  ",
    "start": "3508400",
    "end": "3516319"
  },
  {
    "text": "no answer to how one should embed a particular \ntype of data for a particular type of problem  ",
    "start": "3516320",
    "end": "3523360"
  },
  {
    "text": "the answer comes about through validation through \nmeasuring the performance against a data set in  ",
    "start": "3523920",
    "end": "3531359"
  },
  {
    "text": "determining whether the performance is better with \nthat sort of features after the fact sometimes you  ",
    "start": "3531360",
    "end": "3537760"
  },
  {
    "text": "can come up with some post hoc explanation for \nwhy certain features were better you can say  ",
    "start": "3537760",
    "end": "3545360"
  },
  {
    "text": "things like well obviously for numbers of bedrooms \none should use uh one hot embedding rather than uh",
    "start": "3545360",
    "end": "3553920"
  },
  {
    "text": "ordinal embedding into the reels but there's \nno truth to such post-hoc explanations those  ",
    "start": "3556160",
    "end": "3567760"
  },
  {
    "text": "are just justifications for uh what happened with \nthe outcome that happened to turn out that way the  ",
    "start": "3567760",
    "end": "3576080"
  },
  {
    "text": "only test the only way of knowing whether one set \nof features is better than another is to validate",
    "start": "3576080",
    "end": "3585840"
  },
  {
    "text": "there are several different types of feature   transforms one is to modify particular features \nthat you have so you've got a feature x i and you  ",
    "start": "3588160",
    "end": "3602000"
  },
  {
    "text": "replace it with an with a transformed feature of \nx i new so you might standardize for example um  ",
    "start": "3602000",
    "end": "3610320"
  },
  {
    "text": "another thing you can do is you can say well \ni've got one feature and i'm going to take that  ",
    "start": "3612320",
    "end": "3617360"
  },
  {
    "text": "one feature and can create multiple features \nfrom it that's the power as example we take  ",
    "start": "3617360",
    "end": "3624480"
  },
  {
    "text": "x i we replace it with x i x i squared x i cubed \nand so on another thing you can do is you can take  ",
    "start": "3625120",
    "end": "3634560"
  },
  {
    "text": "more than one feature and combine them together \nto create new features so for example you can  ",
    "start": "3634560",
    "end": "3640400"
  },
  {
    "text": "take two features say x1 and x2 and create a new \nfeature which will be the product of x1 and x2",
    "start": "3640400",
    "end": "3646480"
  },
  {
    "text": "here's a feature that's very commonly used this \nis the gamma transform you take x and you replace  ",
    "start": "3653040",
    "end": "3659360"
  },
  {
    "text": "it with sine of x multiplied by the absolute \nvalue of x to the power of some constant gamma  ",
    "start": "3659360",
    "end": "3664560"
  },
  {
    "text": "and that produces curves that are shown in this \nplot right here if gamma is a half for example",
    "start": "3665280",
    "end": "3671760"
  },
  {
    "text": "it produces a curve that looks like that \nthat's a case where we might believe for  ",
    "start": "3677840",
    "end": "3685040"
  },
  {
    "text": "example that a feature has diminishing \nimportance as its magnitude grows and so  ",
    "start": "3685040",
    "end": "3690880"
  },
  {
    "text": "we might replace it um we might replace x by \nthe absolute value of x to the power of a half  ",
    "start": "3690880",
    "end": "3698400"
  },
  {
    "text": "um again that's a justification but it doesn't \nnecessarily mean that if a feature has diminishing  ",
    "start": "3699360",
    "end": "3712000"
  },
  {
    "text": "importance then you should use the gamma transform \nit's something reasonable to try particularly if  ",
    "start": "3712000",
    "end": "3717200"
  },
  {
    "text": "you're using a linear predictor but the only way \nto know whether it's better or not is to validate",
    "start": "3717200",
    "end": "3725839"
  },
  {
    "text": "another thing you can do is clip this is also \ncalled saturation or winds arising there's a nice  ",
    "start": "3728240",
    "end": "3735040"
  },
  {
    "text": "little formula for it here the new x is equal to \nx if x is between l and u or it's equal to u when  ",
    "start": "3735040",
    "end": "3741920"
  },
  {
    "text": "x is greater than u or it's equal to l when x is \nless than l and that saturized saturates like this",
    "start": "3741920",
    "end": "3748160"
  },
  {
    "text": "and so values larger than in this \ncase than one i just treat it as if  ",
    "start": "3752000",
    "end": "3757120"
  },
  {
    "text": "they're one values less than minus one \ni just treat it as if they're minus one",
    "start": "3757120",
    "end": "3761760"
  },
  {
    "text": "we might believe that for example our data \ncontains anomalies very large data values  ",
    "start": "3764080",
    "end": "3771040"
  },
  {
    "text": "that are really a an artifact of the way \nthe data was measured and we know that  ",
    "start": "3771600",
    "end": "3777200"
  },
  {
    "text": "the physical quantity that's being \nmeasured can never exceed values   of 1 or minus one in which case windsor \nrising would be a reasonable thing to do",
    "start": "3777200",
    "end": "3785200"
  },
  {
    "text": "here's powers we've seen these already \nthis we know that this is u u squared",
    "start": "3790960",
    "end": "3798000"
  },
  {
    "text": "and u cubed",
    "start": "3801040",
    "end": "3802320"
  },
  {
    "text": "and here's one we've seen already as well u \nplus and u minus these are splitting x into  ",
    "start": "3806960",
    "end": "3814240"
  },
  {
    "text": "uh positive and negative parts there's an \nadditional one here which is worth looking at  ",
    "start": "3815040",
    "end": "3820960"
  },
  {
    "text": "this is the saturation function sat of x sat of x \nis the minimum of 1 the maximum of x and minus 1.  ",
    "start": "3821840",
    "end": "3831440"
  },
  {
    "text": "we can plot that let's try and do that  ",
    "start": "3832240",
    "end": "3838320"
  },
  {
    "text": "the maximum of x and minus 1 well this is the \nminus 1 function and this is the x function",
    "start": "3838320",
    "end": "3846800"
  },
  {
    "text": "so the maximum of x and minus 1 is",
    "start": "3849120",
    "end": "3852400"
  },
  {
    "text": "that function right there and then we take the \nminimum of that function and one and the one  ",
    "start": "3856240",
    "end": "3862800"
  },
  {
    "text": "function is right here and so the minimum \nof those two functions is let me draw it",
    "start": "3862800",
    "end": "3873200"
  },
  {
    "text": "right here that's the sat function it's the \nwindsorization it's another way of expressing  ",
    "start": "3876000",
    "end": "3881600"
  },
  {
    "text": "the windsorization or the saturation \nfunction that we saw over here  ",
    "start": "3883600",
    "end": "3893840"
  },
  {
    "text": "another thing one can do is create \nnew features for multiple features   so we can model interactions among features so \nfor example um you might create all products  ",
    "start": "3895440",
    "end": "3908560"
  },
  {
    "text": "x i x j or you might create the \nmaxima maximum of x i and x j",
    "start": "3909120",
    "end": "3916560"
  },
  {
    "text": "you can also create all the monomials of \ndegree 3 or less x1 x2 x1 squared x1 x2  ",
    "start": "3918800",
    "end": "3925440"
  },
  {
    "text": "and so on and that would give us an \narbitrary polynomial of degree 3.",
    "start": "3926080",
    "end": "3930160"
  },
  {
    "text": "products can be thought about as a nice way \nof modeling interactions and so can maxima so  ",
    "start": "3934320",
    "end": "3940800"
  },
  {
    "text": "for example suppose x i are boolean so and \nwe've embedded them to take values 0 or 1.  ",
    "start": "3940800",
    "end": "3948240"
  },
  {
    "text": "they might represent say patient symptoms then we \ncreate interaction features which are the products  ",
    "start": "3949600",
    "end": "3956720"
  },
  {
    "text": "of x i x j and of course we only need to \nconsider the terms which i is less than j  ",
    "start": "3956720",
    "end": "3961760"
  },
  {
    "text": "because x i x j is equal to x j x i \nso there are going to be if we have  ",
    "start": "3961760",
    "end": "3967040"
  },
  {
    "text": "d original features we're going to add d times \nd minus one divided by two additional features",
    "start": "3967040",
    "end": "3972400"
  },
  {
    "text": "then if we do linear regression \nsay when d is 3 we're going to get",
    "start": "3975440",
    "end": "3978400"
  },
  {
    "text": "an expression which looks like theta 1 \nx 1 plus theta 2 x 2 plus theta 3 x 3  ",
    "start": "3980640",
    "end": "3986240"
  },
  {
    "text": "plus theta 1 2 x 1 x 2 plus theta 1 3 x 1 \nx 3 plus theta 2 3 x 2 x 3 those would be  ",
    "start": "3986240",
    "end": "3994400"
  },
  {
    "text": "that would be the form of a linear \npredictor with those new features",
    "start": "3995840",
    "end": "4001600"
  },
  {
    "text": "now these are very nicely interpretable these \ncoefficients because theta one is the amount   the prediction goes up when x one is one theta \ntwo is the amount of production goes up when  ",
    "start": "4004720",
    "end": "4013840"
  },
  {
    "text": "x three is one and c two one three is the amount \nthe prediction goes up when both x1 and x3 are one  ",
    "start": "4013840",
    "end": "4023120"
  },
  {
    "text": "in addition to c to one and theta \nthree you get this additional boost   in your prediction and so this is \nsaying that when two symptoms both occur  ",
    "start": "4025120",
    "end": "4035359"
  },
  {
    "text": "then we believe our predicted value of whatever \nit is we're predicting should have an additional  ",
    "start": "4036320",
    "end": "4042880"
  },
  {
    "text": "amount added to it c to one three another \nway to say is that if theta 1 3 is large then  ",
    "start": "4042880",
    "end": "4051759"
  },
  {
    "text": "if you've got both symptoms simultaneously \npresent then we increase our estimate a lot",
    "start": "4051760",
    "end": "4061840"
  },
  {
    "text": "another thing one can do in future \nengineering is quantization we specify  ",
    "start": "4065840",
    "end": "4072480"
  },
  {
    "text": "bin boundaries b1 to bk and we partition",
    "start": "4072480",
    "end": "4076960"
  },
  {
    "text": "the real numbers into buckets the interval \nfrom b1 to b2 from b2 to b3 and so on  ",
    "start": "4080240",
    "end": "4090160"
  },
  {
    "text": "and then we replace x by the following we \nembed it as e one through e k plus 1 where  ",
    "start": "4090160",
    "end": "4105440"
  },
  {
    "text": "the embedding returns e1 if \nx1 is less than equal to b1   it returns e2 if x1 is in the next bin between b1 \nand b2 and so on so x maps to e i if x is in bin i  ",
    "start": "4105440",
    "end": "4122079"
  },
  {
    "text": "this is a one hot embedding of the ordinal that \nresults from figuring out which bin x is in",
    "start": "4124080",
    "end": "4131839"
  },
  {
    "text": "and you might say well why would you ever do \nthat and the answer to that kind of question  ",
    "start": "4134560",
    "end": "4140000"
  },
  {
    "text": "is always the same is that this is the kind \nof thing you try and see whether it works  ",
    "start": "4140000",
    "end": "4147200"
  },
  {
    "text": "the only way to know whether or not it's better is \nto try it and validate sometimes you may believe  ",
    "start": "4149120",
    "end": "4156720"
  },
  {
    "text": "that this the case where x is in these different \nbins those are really different situations and so  ",
    "start": "4156720",
    "end": "4163520"
  },
  {
    "text": "the predictor should treat them differently and \nintroducing these kinds of features is one way to  ",
    "start": "4163520",
    "end": "4169440"
  },
  {
    "text": "develop a predictor that treats these different \ncases differently that would depend on  ",
    "start": "4170480",
    "end": "4177040"
  },
  {
    "text": "what type of predictor you're using and uh so \nit's a it's an intuitive motivation rather than a  ",
    "start": "4177600",
    "end": "4185520"
  },
  {
    "text": "rigorous motivation ultimately one validates to \ndetermine whether or not such a prediction such a  ",
    "start": "4186480",
    "end": "4193200"
  },
  {
    "text": "piece of feature engineering is a good idea or \nnot now you can do these many times you don't  ",
    "start": "4193200",
    "end": "4204240"
  },
  {
    "text": "have to do embedding and then one round of feature \nengineering you can compose them with each other  ",
    "start": "4204240",
    "end": "4210000"
  },
  {
    "text": "you start by embedding the original record u \ninto a feature vector which we'll call x0 and  ",
    "start": "4210640",
    "end": "4217200"
  },
  {
    "text": "then you transform x0 using a feature engineering \ntransform to get x1 that might be standardized  ",
    "start": "4217200",
    "end": "4223040"
  },
  {
    "text": "and then you repeat again you may do a polynomial \nembedding or you might do binning or you might do  ",
    "start": "4224240",
    "end": "4230000"
  },
  {
    "text": "windsorization then you repeat this m times and \nyou've got a composition of different feature maps  ",
    "start": "4230000",
    "end": "4236000"
  },
  {
    "text": "and that gives you the final embedding this \nis called the feature engineering pipeline",
    "start": "4237680",
    "end": "4243840"
  },
  {
    "text": "now there's one more topic that i want to mention   in this section and that's \nautomatic feature generation",
    "start": "4250880",
    "end": "4257840"
  },
  {
    "text": "so the features that we've seen so far are \ngenerally done by hand using experience",
    "start": "4262080",
    "end": "4267680"
  },
  {
    "text": "and that's hard to do essentially it's done \nwhen one's trying these kinds of things there  ",
    "start": "4269840",
    "end": "4276159"
  },
  {
    "text": "are things that one knows is are generally a good \nidea standardization logarithms one hot embeddings  ",
    "start": "4276160",
    "end": "4283680"
  },
  {
    "text": "and there are things that one has experience of \nin certain domains certain embeddings work well  ",
    "start": "4284640",
    "end": "4291840"
  },
  {
    "text": "in certain domains products for symptoms \nfor example is a very good thing to do  ",
    "start": "4291840",
    "end": "4297199"
  },
  {
    "text": "um and then there is the rest of \nit which is largely trial and error  ",
    "start": "4298160",
    "end": "4304000"
  },
  {
    "text": "one can try a bunch of different \nfeature engineering tricks   and hope that they improve your validation \nresults and because it's trial and error it  ",
    "start": "4304960",
    "end": "4315360"
  },
  {
    "text": "would be very nice to be able to develop feature \nmappings automatically directly from the data  ",
    "start": "4315360",
    "end": "4320480"
  },
  {
    "text": "and this is certainly possible we've \nalready seen two examples one is word to vec   where words were mapped to vectors another is \nvgg16 where images were mapped to vectors and  ",
    "start": "4321760",
    "end": "4335760"
  },
  {
    "text": "those were learned and they would do so they were \ndeveloped automatically from very large data sets  ",
    "start": "4335760",
    "end": "4341840"
  },
  {
    "text": "and those are very important methods they have \nmade a huge difference to our ability to do  ",
    "start": "4342880",
    "end": "4348560"
  },
  {
    "text": "effective machine learning and we will see later \nthose methods in some detail and we'll talk about  ",
    "start": "4348560",
    "end": "4356960"
  },
  {
    "text": "some specific methods for constructing \nthese things pca is an example that stands   for principle component analysis \nneural networks is another example",
    "start": "4356960",
    "end": "4369840"
  },
  {
    "text": "so to summarize all features are mapped to \nvectors that's embedding it's the feature map",
    "start": "4373440",
    "end": "4380640"
  },
  {
    "text": "we subsequently process those vectors \nin our predictor we may do extensive  ",
    "start": "4383760",
    "end": "4391920"
  },
  {
    "text": "feature engineering to construct more and more \ncomplex ways of mapping features to vectors",
    "start": "4391920",
    "end": "4398960"
  },
  {
    "text": "only validation can choose between \ndifferent candidate feature maps",
    "start": "4401920",
    "end": "4405600"
  },
  {
    "text": "and we're going to see later how feature mappings \ncan be derived from data as opposed to by hand",
    "start": "4408320",
    "end": "4415167"
  }
]