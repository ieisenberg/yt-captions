[
  {
    "start": "0",
    "end": "10870"
  },
  {
    "text": "Hello, everyone. Really nice to see\nyou guys today. So today I'm going to talk a\nlittle bit about robot skill",
    "start": "10870",
    "end": "17560"
  },
  {
    "text": "acquisition, and I'll\ntalk a little bit about policy representation. But majority of the\ntime I will spend on talking about data\ngeneration or how",
    "start": "17560",
    "end": "24310"
  },
  {
    "text": "to get the data that we\ncan train robot policies. So many of you probably know me.",
    "start": "24310",
    "end": "31359"
  },
  {
    "text": "So I work on robot perception\nand also manipulation. And I think in our\nwork, what we're really",
    "start": "31360",
    "end": "38079"
  },
  {
    "text": "trying to do is try to push\nthe boundaries for robot capabilities. So for example, basically\nmaking robots to do things",
    "start": "38080",
    "end": "44620"
  },
  {
    "text": "that was not possible before. So for example, cutting\navocados, tossing objects,",
    "start": "44620",
    "end": "49810"
  },
  {
    "text": "or blowing leaves. So throughout all these\nprojects in the past, one thing that our group starts\nto get quite familiar and also",
    "start": "49810",
    "end": "58270"
  },
  {
    "text": "kind of good at is to design\nthose very task-specific action primitives, right?",
    "start": "58270",
    "end": "63730"
  },
  {
    "text": "So for example, for many of the\nprojects that I just show you here, it typically works with\ngeneral workflows like this,",
    "start": "63730",
    "end": "72240"
  },
  {
    "text": "right? So first, you cannot start by\nthinking about the task that you want to work on. So for example, if\nyou want you to make",
    "start": "72240",
    "end": "79100"
  },
  {
    "text": "your robot able to toss\nobjects or unfolding cloth, the first step that\nwe're going to do",
    "start": "79100",
    "end": "84770"
  },
  {
    "text": "is to sit down and\nthen think about what are the action primitives,\nwhat are the type of actions we need the robot to learn,\nand then design-- and carefully",
    "start": "84770",
    "end": "93560"
  },
  {
    "text": "design those action primitives\nlike those tossing action, flinging action,\nand swing actions. So these steps of action\nprimitive design is actually",
    "start": "93560",
    "end": "102380"
  },
  {
    "text": "very critical in many of the\nproject because fundamentally what it allows us to do is to\nmake those very complex action",
    "start": "102380",
    "end": "109760"
  },
  {
    "text": "sequences-- for example,\nthose tossing action-- into just a few\nlearnable parameters that can critically\ndetermine the action outcomes",
    "start": "109760",
    "end": "117350"
  },
  {
    "text": "for different robots, right? So for example, to make\nthe robot able to toss",
    "start": "117350",
    "end": "122479"
  },
  {
    "text": "very precisely, we\nbasically code up the whole tossing trajectory. The whole trajectory is fixed.",
    "start": "122480",
    "end": "128060"
  },
  {
    "text": "And the only parameter\nthe system needs to learn is the releasing velocity\nfor tossing, right? It's only one parameter\nthat is learned.",
    "start": "128060",
    "end": "134659"
  },
  {
    "text": "And then that\nparameter is actually enough to actually\ngive the robot the control of like\nhow to-- like where",
    "start": "134660",
    "end": "140550"
  },
  {
    "text": "the objects will be landing. And then similarly for\nthe cloth unfolding task,",
    "start": "140550",
    "end": "146180"
  },
  {
    "text": "we use the fling action\nprimitives basically as a starting point. And then the only\nthing that the system",
    "start": "146180",
    "end": "152275"
  },
  {
    "text": "needs to learn is the grasping\nlocations on the cloth, right? So with that, we basically\nfront-load all the complexities",
    "start": "152275",
    "end": "159439"
  },
  {
    "text": "instead of learning\nhow to fling, instead of just\nthe robot only need to learn how to grasp the cloth\nso that it can execute a fling",
    "start": "159440",
    "end": "165680"
  },
  {
    "text": "action. And then once we have those\nwell-designed action primitives, the latest step\nbecomes quite easy.",
    "start": "165680",
    "end": "172550"
  },
  {
    "text": "We just need to collect\na lot of robot data overnight, for example, tossing\ndata like the robot tossed overnight.",
    "start": "172550",
    "end": "178200"
  },
  {
    "text": "So it's a kind of a\ntrend error process. And then the robot can\nlearn a few parameters",
    "start": "178200",
    "end": "184250"
  },
  {
    "text": "that we defined through\nthis with all this data set. And then finally, you\nwill have a working demo.",
    "start": "184250",
    "end": "191450"
  },
  {
    "text": "So this pipeline actually has\nbeen working pretty well for us for a few years, and I\nthink it particularly",
    "start": "191450",
    "end": "197780"
  },
  {
    "text": "works well under this\nkind of low-data regime. It means that when you don't\nhave a lot of robot data to train the different\naction primitives.",
    "start": "197780",
    "end": "204830"
  },
  {
    "text": "So what this framework\nreally allows us to do is to make robot to\nkind of try to focus on learning the most\ncritical parameters in action",
    "start": "204830",
    "end": "212840"
  },
  {
    "text": "with less amount of data\ninstead of learning everything from scratch.",
    "start": "212840",
    "end": "217844"
  },
  {
    "text": "However, there is also a few\ndownsides about this framework, right? So first of all, designing\nthose action primitives actually",
    "start": "217845",
    "end": "223510"
  },
  {
    "text": "requires a lot of engineering\neffort and also insights. So it actually spend-- we\nneed to spend a lot of time",
    "start": "223510",
    "end": "228670"
  },
  {
    "text": "to think about what is\nthe right prioritization for different action primitives. And I think more importantly\nis that those action primitives",
    "start": "228670",
    "end": "236590"
  },
  {
    "text": "sometimes is just\nnot general enough to represent all possible robot\nactions, especially those robot",
    "start": "236590",
    "end": "243159"
  },
  {
    "text": "actions that requires high-rate\nand reactive behaviors, which means that oftentimes you\nneed to redesign your action",
    "start": "243160",
    "end": "249640"
  },
  {
    "text": "primitives for new tasks. So slowly, we start to realize\nthat this type of action",
    "start": "249640",
    "end": "256140"
  },
  {
    "text": "primitive design start\nto become the bottleneck for the whole framework.",
    "start": "256140",
    "end": "261588"
  },
  {
    "text": "So recently, I think\nwith our whole group started kind of changing\nthe workflow a little bit,",
    "start": "261589",
    "end": "267140"
  },
  {
    "text": "especially after this\nwork diffusion policy. So what we learned\nis that instead",
    "start": "267140",
    "end": "272930"
  },
  {
    "text": "of trying to always carefully\ndesign the action primitive, we start to see a lot\nof strong evidence",
    "start": "272930",
    "end": "278780"
  },
  {
    "text": "that it's possible for the\nrobots to kind of directly learn those complex\nmanipulation skills directly",
    "start": "278780",
    "end": "284870"
  },
  {
    "text": "from human demonstration\ndata, right? So for example, here\nis the diffusion policy",
    "start": "284870",
    "end": "291199"
  },
  {
    "text": "trained for a variety of tasks\nall using the same network architecture. We don't need to change\nnetwork architecture design.",
    "start": "291200",
    "end": "297626"
  },
  {
    "text": "And we just need to\nswap out different data. And you can do a lot\nof different tasks. And if you pay attention\nto this corner task that's",
    "start": "297627",
    "end": "305248"
  },
  {
    "text": "trying to do the\n[INAUDIBLE] flipping task, you can see that the\nrobot is actually kind of able to\norganically switching",
    "start": "305248",
    "end": "310778"
  },
  {
    "text": "between different action\nprimitives like pick and place and pushing and also\nable to robustly recover",
    "start": "310778",
    "end": "316100"
  },
  {
    "text": "from like failures\nlike pickup failures. So all those things\nare something that's really hard to code up\nwith just action primitives,",
    "start": "316100",
    "end": "323940"
  },
  {
    "text": "but the robot is\nable to learn that all from data or all from\nhuman demonstration data now. ",
    "start": "323940",
    "end": "331879"
  },
  {
    "text": "So at this point, probably\none question in your head is that the idea of learning\nfrom human demonstration",
    "start": "331880",
    "end": "339080"
  },
  {
    "text": "isn't that pretty\nstraightforward, like isn't that pretty trivial. Well, we haven't done\nit before, right?",
    "start": "339080",
    "end": "344510"
  },
  {
    "text": "So the quick answer\nis that for a while, learning from demonstration\nor behavioral cloning actually",
    "start": "344510",
    "end": "350270"
  },
  {
    "text": "is pretty hard to make\nit work well in reality. And in fact, if you ever take\na class in robot learning,",
    "start": "350270",
    "end": "358100"
  },
  {
    "text": "the first few lectures\nwill basically talk about behavioral cloning\nand all its limitations, right?",
    "start": "358100",
    "end": "364100"
  },
  {
    "text": "So for example,\npeople often worry about when he encounters\nissues like how to really train",
    "start": "364100",
    "end": "369260"
  },
  {
    "text": "a policy that's able to model\ncomplex action distributions or how to collect the right\ndemonstration data for the robot",
    "start": "369260",
    "end": "375830"
  },
  {
    "text": "to learn effectively. And finally, people also\noftentimes criticize or has concerns about whether\nbehavioral cloning or learning",
    "start": "375830",
    "end": "382910"
  },
  {
    "text": "from demonstration is\nable to really allow the robot to generalize, right? I think all those are\nvery valid concerns.",
    "start": "382910",
    "end": "388610"
  },
  {
    "text": "And in today's talk,\nwe're going to kind of get to a little bit of all\nthese three problems and see how we can potentially\nbypass or kind of partially",
    "start": "388610",
    "end": "397139"
  },
  {
    "text": "address some of\nthose challenges. So first challenge is about\nhow to model this very complex",
    "start": "397140",
    "end": "404729"
  },
  {
    "text": "action distribution, right? So there's actually\nmany different aspects about how like the--",
    "start": "404730",
    "end": "410790"
  },
  {
    "text": "why is the action\ndistribution is complex? But one very concrete\nexample is that if you want to learn from\nhuman demonstration,",
    "start": "410790",
    "end": "417150"
  },
  {
    "text": "you really need to be\nable to kind of model this kind of action\nmultimodality. So what action\nmultimodality means",
    "start": "417150",
    "end": "423750"
  },
  {
    "text": "is that given the same state\nor same visual observation from the robot, there may be\nmultiple valid actions that",
    "start": "423750",
    "end": "431440"
  },
  {
    "text": "can possibly complete\nthe task, right? So for example, if\nyour task is trying",
    "start": "431440",
    "end": "437250"
  },
  {
    "text": "to push this T to\nthe target location like this green location-- and then the blue dot\nis your end effector.",
    "start": "437250",
    "end": "444600"
  },
  {
    "text": "You can actually\neither go from the left or go from the right to\ncomplete this task, right? And this kind of\naction multimodality",
    "start": "444600",
    "end": "451530"
  },
  {
    "text": "actually appears quite often\nin human demonstration data because there are\njust intrinsically many different ways to\ncomplete the same task.",
    "start": "451530",
    "end": "459240"
  },
  {
    "text": "So the problem\nhere is that if you try to make the policy\nlearn from this kind",
    "start": "459240",
    "end": "464290"
  },
  {
    "text": "of complex distribution, if you\njust try to minimize its output action, like the distance\nbetween the output action",
    "start": "464290",
    "end": "471160"
  },
  {
    "text": "with respect to both\nof the distributions, you easily get an action\nthat is not valid.",
    "start": "471160",
    "end": "476410"
  },
  {
    "text": "So for example, going\ninto the middle, right? So if your policy are not\nable to kind of handle",
    "start": "476410",
    "end": "482380"
  },
  {
    "text": "this kind of action\nmultimodality issue, your policy can\neasily get stuck. That is a typical failure mode\nfor learning from demonstration.",
    "start": "482380",
    "end": "491510"
  },
  {
    "text": "So how diffusion policy is able\nto address that-- so first, maybe let me talk about\nwhat is diffusion policy.",
    "start": "491510",
    "end": "497449"
  },
  {
    "text": "So I guess many of\nyou guys probably are very familiar with the\nidea of diffusion model. Oftentimes, you\nprobably see that used",
    "start": "497450",
    "end": "503960"
  },
  {
    "text": "for image generation\nlike a DALL.E. So the idea is\nthat you can learn general distribution of images\nfrom a large collection of image",
    "start": "503960",
    "end": "511669"
  },
  {
    "text": "data sets. And similarly here we are\nusing very similar idea. We're trying to train\na diffusion model.",
    "start": "511670",
    "end": "516979"
  },
  {
    "text": "But instead of\ngenerating images, we are trying to generate\nrobot actions, right? So here, a concrete example\nis that in the robot action",
    "start": "516980",
    "end": "526348"
  },
  {
    "text": "in this particular\nexample, we represent it as a 2D and the vector\ntrajectories, right? Those dots are the actions\nthat we're trying to generate.",
    "start": "526348",
    "end": "533029"
  },
  {
    "text": "And then like the\nimage generation, you go from complete noise\nto a high-resolution image.",
    "start": "533030",
    "end": "538190"
  },
  {
    "text": "We are trying to generate-- try to denoise the random\naction into a clean action",
    "start": "538190",
    "end": "543200"
  },
  {
    "text": "that the robot can execute. So what make diffusion\npolicy different",
    "start": "543200",
    "end": "549780"
  },
  {
    "text": "is this iterative\ndenoising process, which turns out to\nbe an excellent way to help the system to model this\nkind of multimodal distribution",
    "start": "549780",
    "end": "558720"
  },
  {
    "text": "in the robot action space,\nwhich is something just a lot of [INAUDIBLE]\nkind of struggled with.",
    "start": "558720",
    "end": "566640"
  },
  {
    "text": "So in addition to able\nto model this kind of multimodal distribution\nis the resulting prediction",
    "start": "566640",
    "end": "572370"
  },
  {
    "text": "is also very precise, meaning\nthat it's not just approximation of the action distribution,\nwhich we will probably",
    "start": "572370",
    "end": "579480"
  },
  {
    "text": "see this in the kind of quantize\nthe classification or action representation.",
    "start": "579480",
    "end": "584850"
  },
  {
    "text": "So here is a slice I'm trying\nto use to kind of give you a little bit intuition about\nwhy diffusion policy is",
    "start": "584850",
    "end": "591870"
  },
  {
    "text": "able to capture\nmultimodalities, right? So one way to think about\nthis diffusion process",
    "start": "591870",
    "end": "597000"
  },
  {
    "text": "is basically thinking about this\nas a gradient descent process on the action space, where\nthe predicted gradient",
    "start": "597000",
    "end": "603690"
  },
  {
    "text": "field can easily have any\nnumbers of local minimas. And if you use\ngradient descent, it's very likely you can stuck\nin certain local minimas.",
    "start": "603690",
    "end": "611138"
  },
  {
    "text": "And in this case, actually,\nstuck in local minima is a good thing because\neach of the local minima can help you to capture a mode.",
    "start": "611138",
    "end": "617380"
  },
  {
    "text": "And also, when the output is\nget to a particular local minima is also very precise. It's not approximation\naround that mode.",
    "start": "617380",
    "end": "624430"
  },
  {
    "text": "So that is like\nthe intuition why diffusion policy is able\nto capture multimodalities.",
    "start": "624430",
    "end": "630150"
  },
  {
    "text": "And then actually,\nin addition to just the ability to model\naction multimodality,",
    "start": "630150",
    "end": "635430"
  },
  {
    "text": "diffusion policy also provides\na number of other advantages, right? So for example, it turned\nout to be scaled really",
    "start": "635430",
    "end": "641819"
  },
  {
    "text": "well with respect to\naction dimensions. So it means that you can\nactually now make your robot",
    "start": "641820",
    "end": "647220"
  },
  {
    "text": "to predict a trajectory of\naction instead of a single step action in the future. And then also, diffusion\npolicy is much stable",
    "start": "647220",
    "end": "655350"
  },
  {
    "text": "to train compared to\nother generative models. So all these benefits\nor advantages",
    "start": "655350",
    "end": "661230"
  },
  {
    "text": "is what make diffusion policy\na really practical framework for learning any robot behaviors\nas long as you have the data.",
    "start": "661230",
    "end": "667650"
  },
  {
    "text": " So in our paper, we\nactually continue",
    "start": "667650",
    "end": "672810"
  },
  {
    "text": "to test our diffusion policy\non more and more tasks, right? So what really surprised\nus is that we can always",
    "start": "672810",
    "end": "681480"
  },
  {
    "text": "observe like a consistent\nperformance boost. Sometimes it's a large boost. Sometimes it's\nsmaller improvement.",
    "start": "681480",
    "end": "687180"
  },
  {
    "text": "But it's always\nconsistently able to improve the performance\nregardless on the tasks that we test out, right?",
    "start": "687180",
    "end": "692670"
  },
  {
    "text": "So that is actually\npretty surprising. And also, at some point,\nI think my student, Cheng,",
    "start": "692670",
    "end": "698070"
  },
  {
    "text": "start to just get a feeling\nthat if we can collect the data, we can make the robot\nto do any task we want.",
    "start": "698070",
    "end": "704410"
  },
  {
    "text": "Question? [INAUDIBLE] On the previous\nslide, what was the blue, and what was the gray?",
    "start": "704410",
    "end": "709480"
  },
  {
    "text": "Yeah. So the blue is the\nperformance of our method, and the gray is the best\nbaseline on that benchmark.",
    "start": "709480",
    "end": "715690"
  },
  {
    "text": "Yeah. So it's always slightly\nbetter or better. Yeah. ",
    "start": "715690",
    "end": "722850"
  },
  {
    "text": "Yeah. So at some point,\nwe actually test on multiple\ndifferent benchmarks. It's not just one benchmark. And different\nbenchmarks oftentimes",
    "start": "722850",
    "end": "728738"
  },
  {
    "text": "are designed for different\ntasks or highlight different properties\nof the algorithm. And then at some point, Cheng\nstart to feel like, yeah,",
    "start": "728738",
    "end": "735949"
  },
  {
    "text": "maybe we're at the stage that if\nwe can collect the right robot data, we can make the robot\nto do whatever task we want.",
    "start": "735950",
    "end": "743570"
  },
  {
    "text": "So I know that this [INAUDIBLE]\nsounds a little bit crazy, but I think, in some sense,\nit's kind of true because this",
    "start": "743570",
    "end": "750470"
  },
  {
    "text": "is actually what our\ncollaborators in TRI is able to achieve by\nbasically scaling up",
    "start": "750470",
    "end": "755570"
  },
  {
    "text": "this simple framework\nof diffusion policy. Many of the tasks here\nis actually something",
    "start": "755570",
    "end": "761300"
  },
  {
    "text": "that I thought was\nimpossible before, like flipping pancakes\nor rolling doughs.",
    "start": "761300",
    "end": "766790"
  },
  {
    "text": "So the question now is actually,\nshould we just keep doing it",
    "start": "766790",
    "end": "772519"
  },
  {
    "text": "until the robotics\nwill be solved? Well, I really hope so, but the\nanswer is clearly no, right?",
    "start": "772520",
    "end": "778460"
  },
  {
    "text": "I just started my talk. We still have a lot to go. So I think the issue is that we\ncannot really ignore this really",
    "start": "778460",
    "end": "786500"
  },
  {
    "text": "big if in this statement. That is if we can\ncollect the data, right? And in fact, it is not any\nkind of data, a big if.",
    "start": "786500",
    "end": "796415"
  },
  {
    "text": "There is actually a lot of\neffort and intelligence needed to capture what gets the\nright type of data, right?",
    "start": "796415",
    "end": "802460"
  },
  {
    "text": "So for example, here is\na secret recipe for you to get the right data. The first step for you to\nget a high-quality robot data",
    "start": "802460",
    "end": "810889"
  },
  {
    "text": "to train behavioral cloning\nis to really train yourself so that you can think\nand behave like a robot.",
    "start": "810890",
    "end": "816637"
  },
  {
    "text": "So for example, for the\n[INAUDIBLE] flipping case, I didn't show you the\ndemonstration phase. We actually use this\nkind of SpaceMouse",
    "start": "816637",
    "end": "823190"
  },
  {
    "text": "to collect the\nrobot demonstration, and it's actually really\nnot intuitive interface. And then it kind\nof-- at some point,",
    "start": "823190",
    "end": "829529"
  },
  {
    "text": "you need to mentally\ncompute IK in order to know whether you\nare reaching the limit. So so far, I think\nin our team, only",
    "start": "829530",
    "end": "835860"
  },
  {
    "text": "Siyuan is able to collect data\nfor this task and no one else, right? So you either need to\ntrain yourself really hard",
    "start": "835860",
    "end": "841680"
  },
  {
    "text": "or hire a student\nfor data collection. Oh. And then the second step-- oops.",
    "start": "841680",
    "end": "846940"
  },
  {
    "text": "Second step is to make sure\nthat you collect a lot of data to cover your task space\nso that in the test time,",
    "start": "846940",
    "end": "854079"
  },
  {
    "text": "your generalization just become\ninterpolation in your training data.",
    "start": "854080",
    "end": "859110"
  },
  {
    "text": "And then the third\nstep is actually also very important but\na little bit subtle is that you need to anticipate\nthe robot failures, knowing when",
    "start": "859110",
    "end": "865830"
  },
  {
    "text": "the robot will fail and also\nexplicitly collect some failure recovery behaviors, so that the\nend policy will be more robust.",
    "start": "865830",
    "end": "874710"
  },
  {
    "text": "And finally, also,\nmost importantly, you need to protect\nthe environment so that your colleagues don't\ntouch your camera before demo,",
    "start": "874710",
    "end": "880990"
  },
  {
    "text": "right? So actually, it's interesting\nthat learning from demonstration or behavioral cloning\ncan easily give you",
    "start": "880990",
    "end": "886170"
  },
  {
    "text": "a demo like working\ndemo on the real robots, but it also has a\nvery short shelf life,",
    "start": "886170",
    "end": "891450"
  },
  {
    "text": "meaning that if you shift\nthe camera a little bit, you need to\nrecollect data again. So all this are making like this\nframework not very scalable.",
    "start": "891450",
    "end": "901350"
  },
  {
    "text": "And then another way I was-- I always like to think that all\nthese issues is actually just really reflect the fact\nthat we are not yet",
    "start": "901350",
    "end": "908880"
  },
  {
    "text": "having the right data for\nrobot learning, right? In particular, I'd\nlike to highlight there are three aspects\nof data that we really",
    "start": "908880",
    "end": "915480"
  },
  {
    "text": "need to-- we really need. That is data that is\nscalable, reusable, and the robot-complete.",
    "start": "915480",
    "end": "921839"
  },
  {
    "text": "So why are all these different\nthree aspects important? So I think in today's\ncontext of machine learning",
    "start": "921840",
    "end": "930130"
  },
  {
    "text": "or big data oftentimes found\nthat the scale is the only thing that people emphasize. However, I think in\nrobotics, the story",
    "start": "930130",
    "end": "937480"
  },
  {
    "text": "is quite different, at\nleast slightly different, that we're having the\ndata that is reusable. And the robot-complete is\noftentimes even more important",
    "start": "937480",
    "end": "945220"
  },
  {
    "text": "than just the scale. And I think maybe-- at least I think-- that the lack of careful thought\non this later tool requirement",
    "start": "945220",
    "end": "953440"
  },
  {
    "text": "is maybe the true\nreason why we haven't put our robots on\nthe same scaling trend as other ML fields.",
    "start": "953440",
    "end": "959779"
  },
  {
    "text": "So today I'm going to\ndive a little bit deeper into these two requirements\nand how we can potentially get there.",
    "start": "959780",
    "end": "965980"
  },
  {
    "text": "So over the years,\nwe actually have seen a lot of similarly\nscalable solutions but, in reality,\ninsufficient, right?",
    "start": "965980",
    "end": "972638"
  },
  {
    "text": "So for example, I'm just\ngoing to use my own work as an example. Like earlier, I showed\nthis TossingBot's case.",
    "start": "972638",
    "end": "978190"
  },
  {
    "text": "This robot actually able to\ncollect its own training data 24/7 by resetting its own state.",
    "start": "978190",
    "end": "983530"
  },
  {
    "text": "So in some sense, it is a system\nthat's extremely scalable. It can collect tons of data\nwithout human supervision.",
    "start": "983530",
    "end": "990620"
  },
  {
    "text": "However, it has a lot of-- so the hidden cost\nhere is that you need to carefully\nengineering the environment",
    "start": "990620",
    "end": "996740"
  },
  {
    "text": "for a particular task. And then as a result,\nthe data you're collected with this\nframework is not",
    "start": "996740",
    "end": "1002290"
  },
  {
    "text": "reusable for different setup,\ndifferent cost, different tasks, or different robot setup, right?",
    "start": "1002290",
    "end": "1007810"
  },
  {
    "text": "So that is what makes this\nkind of particular framework although scalable,\nbut it's not reusable.",
    "start": "1007810",
    "end": "1016280"
  },
  {
    "text": "And similarly, for\nthe internet data, I think you can download\na lot of images and videos",
    "start": "1016280",
    "end": "1021680"
  },
  {
    "text": "from the internet. In some sense, very scalable. And also, you can reuse\nit for different projects. However, I think, unfortunately,\nmost of the internet",
    "start": "1021680",
    "end": "1029720"
  },
  {
    "text": "data are missing very critical\ninformation for robot learning. So for example, you cannot\nget those embodied actions",
    "start": "1029720",
    "end": "1035959"
  },
  {
    "text": "or observations. So as a result, we call\nit incomplete for learning robot policies.",
    "start": "1035960",
    "end": "1041270"
  },
  {
    "text": "So as a result, it's\nnot saying that they are irrelevant for robot learning. They are still very important.",
    "start": "1041270",
    "end": "1046579"
  },
  {
    "text": "But oftentimes, you do\nneed to design much more sophisticated\nalgorithms to make use of those data in\nan indirect way.",
    "start": "1046579",
    "end": "1054310"
  },
  {
    "text": "So how we can move\nforward, right? So if you think about it\nkind of a very high level,",
    "start": "1054310",
    "end": "1060039"
  },
  {
    "text": "I think there's really just\ntwo paths around it, right? So one is that you can simil-- kind of scale up\nyour data collection",
    "start": "1060040",
    "end": "1066940"
  },
  {
    "text": "in simulation environment or\nscaling up your robot data in real world. And today, my goal is not\ntrying to kind of argue",
    "start": "1066940",
    "end": "1074500"
  },
  {
    "text": "which path is better,\nbut instead, I try to give both paths a fair\nchance by carefully thinking",
    "start": "1074500",
    "end": "1080290"
  },
  {
    "text": "about what are the\npros and cons and also more importantly what are the\nbottlenecks and the hidden cost along each of the paths.",
    "start": "1080290",
    "end": "1086470"
  },
  {
    "text": "Like, what's the true\nbottlenecks along each path? And are we possibly--\nlike, is there possible",
    "start": "1086470",
    "end": "1091990"
  },
  {
    "text": "ways to bypass them? So first, let's look\nat a simulation. I think simulators are\nsomething like really reusable.",
    "start": "1091990",
    "end": "1099440"
  },
  {
    "text": "I think it can easily\nshared across different labs and projects, and\nit's also something",
    "start": "1099440",
    "end": "1104809"
  },
  {
    "text": "that we typically use for\nbenchmarking in robotics today. So what is the bottleneck\nfor a simulation environment",
    "start": "1104810",
    "end": "1110720"
  },
  {
    "text": "or scaling up in simulation? I think for-- I\nthink maybe slightly different from the\npopular opinions,",
    "start": "1110720",
    "end": "1116510"
  },
  {
    "text": "I'm actually less concerned\nabout accuracy in simulation because I feel that's\nsomething that clearly",
    "start": "1116510",
    "end": "1121760"
  },
  {
    "text": "can be improved with more\ncompute and more accurate models. But instead, what\nreally worries me",
    "start": "1121760",
    "end": "1127549"
  },
  {
    "text": "the most is the setup cost in\nsimulation for a new task, which is often overlooked\nby people when",
    "start": "1127550",
    "end": "1133560"
  },
  {
    "text": "we're thinking about simulation\nfor data generation, right? So for example, even giving\nan existing environment,",
    "start": "1133560",
    "end": "1139370"
  },
  {
    "text": "a simulation environment, there\nis still a nontrivial amount of effort that's needed\nto put in in order to get the robot\ncomplete data that's",
    "start": "1139370",
    "end": "1145790"
  },
  {
    "text": "ready for training a policy. That includes the embodied\naction, observation, and most importantly,\nnonzero success rate",
    "start": "1145790",
    "end": "1152030"
  },
  {
    "text": "for the tasks you care about. So oftentimes, in order\nto get those kind of data,",
    "start": "1152030",
    "end": "1157250"
  },
  {
    "text": "you either need to kind of tally\nup your robot in simulation or craft your reward function\nor policy for exploration.",
    "start": "1157250",
    "end": "1163940"
  },
  {
    "text": "So none of them are easy. So as a result,\noftentimes, we can see that simulation is easy\nto scale up for one task,",
    "start": "1163940",
    "end": "1171440"
  },
  {
    "text": "but it's really hard to\nscale up for many tasks. So one of our recent\nprojects really",
    "start": "1171440",
    "end": "1176990"
  },
  {
    "text": "tried to address this\nproblem a little bit by automate a large portion\nof those engineering efforts",
    "start": "1176990",
    "end": "1182539"
  },
  {
    "text": "with large language models. So this is a project called\nscaling up and distill down.",
    "start": "1182540",
    "end": "1188480"
  },
  {
    "text": "And this is led by my student,\nHuy, and my collaborator, Pete Florence, from Google. 1 for this project, we\nbasically have two parts to it.",
    "start": "1188480",
    "end": "1196670"
  },
  {
    "text": "So in the first\npart, we're trying to scale up the data\ncollection process by using large language models\nand the simulation state.",
    "start": "1196670",
    "end": "1203780"
  },
  {
    "text": "So what we do is that\ngiven any new task that you want the robot to do like\nsending the packages for return,",
    "start": "1203780",
    "end": "1210440"
  },
  {
    "text": "we'll first use large language\nmodel to recursively break down the task into smaller subtasks.",
    "start": "1210440",
    "end": "1216150"
  },
  {
    "text": "That's very similar to\na lot of other works that's using LMS for planning. But the difference here is that\nwe also allow the large language",
    "start": "1216150",
    "end": "1223350"
  },
  {
    "text": "model to call those low-level\nsampling-based utility functions to really try\ndifferent low-level strategies",
    "start": "1223350",
    "end": "1230910"
  },
  {
    "text": "to complete the task, right? So for example, the\nlarge language model can call like different motion\nplanner and a grass sampler",
    "start": "1230910",
    "end": "1239460"
  },
  {
    "text": "and [INAUDIBLE] sampler. So basically, what\nwe can achieve is by using those sampling-based\nutility functions,",
    "start": "1239460",
    "end": "1247590"
  },
  {
    "text": "we introduce some\nrandomness for exploration in those low-level\nmanipulation strategies.",
    "start": "1247590",
    "end": "1252870"
  },
  {
    "text": "But at the same time, we also\nuse large language models commonsense knowledge to\nhelp us break down the task",
    "start": "1252870",
    "end": "1258750"
  },
  {
    "text": "and also narrow down\nthe search space so that the exploration\nis more efficient.",
    "start": "1258750",
    "end": "1264510"
  },
  {
    "text": "And then we also use\na large language model to help us to generate a\nreward function or checking function for each\nof the subtasks",
    "start": "1264510",
    "end": "1270690"
  },
  {
    "text": "to verify whether\neach of the subtasks is successful or not by using\nsimulation state, right? So this reward\nfunction is actually",
    "start": "1270690",
    "end": "1277530"
  },
  {
    "text": "directly using the\nsimulation state to check whether each of those\nsubtasks is successful or not.",
    "start": "1277530",
    "end": "1283019"
  },
  {
    "text": "And if the subtask\nis not successful, the system will\nbasically keep trying on the low level on the\nsubtask before popping out",
    "start": "1283020",
    "end": "1290789"
  },
  {
    "text": "to a higher level. And this step actually\nis quite critical because it allows the system\nto self-correct its mistakes",
    "start": "1290790",
    "end": "1299580"
  },
  {
    "text": "and also record those\nretry behaviors. So remember in the earlier\nstrategy, one of the step",
    "start": "1299580",
    "end": "1306090"
  },
  {
    "text": "is actually to anticipate\nthe robot failure and also record the\nfailure recovery behaviors. So now we basically can get\nthose kind of data for free.",
    "start": "1306090",
    "end": "1315410"
  },
  {
    "text": "And then at this step,\nwhat we have is like-- by the end of this process,\nwhat we'll get is a multitask",
    "start": "1315410",
    "end": "1321740"
  },
  {
    "text": "language-labeled robot\ndata set that contains a lot of robot experience\nto complete a large number",
    "start": "1321740",
    "end": "1326750"
  },
  {
    "text": "of different tasks. And then the second phase is\nactually quite straightforward. We basically try to distill\ndown all those robot experience",
    "start": "1326750",
    "end": "1334100"
  },
  {
    "text": "into a visuomotor policy\nthat can be directly applied in, for example, real\nworld by inferring actions",
    "start": "1334100",
    "end": "1340549"
  },
  {
    "text": "from raw sensory inputs, right? So by doing this\ndistillation step, we no longer need to\nrely on simulation state",
    "start": "1340550",
    "end": "1346610"
  },
  {
    "text": "for executing action. The policy is just taking raw\nimage input and inferred action. And also, very importantly is\nthat by doing this distillation",
    "start": "1346610",
    "end": "1354470"
  },
  {
    "text": "step, the system actually has a\nchance to continuously improve its performance with\nmore and more experience",
    "start": "1354470",
    "end": "1360710"
  },
  {
    "text": "without really the need of\nfine-tuning the large language model itself, which is\noftentimes a lot more expensive.",
    "start": "1360710",
    "end": "1367340"
  },
  {
    "text": " So in the annual\npaper, we actually",
    "start": "1367340",
    "end": "1372820"
  },
  {
    "text": "test out a variety\nof different tasks. All requires 6DoF\nclosed-loop actions.",
    "start": "1372820",
    "end": "1378190"
  },
  {
    "text": "And many of the tasks actually\nrequires nontrivial commonsense reasoning which is enabled\nby the large language model.",
    "start": "1378190",
    "end": "1385309"
  },
  {
    "text": "And in the quantitative\nevaluation, I want to just highlight\none particular result that is kind of trying to demonstrate\nthe effect of the retry data.",
    "start": "1385310",
    "end": "1393590"
  },
  {
    "text": "So in the last two\nperformance bar, they basically are showing the\nsame policy, the same diffusion",
    "start": "1393590",
    "end": "1398750"
  },
  {
    "text": "policy, trained on\nthe simulation data. However, one is trained with\nand without the retried data,",
    "start": "1398750",
    "end": "1406610"
  },
  {
    "text": "meaning that whether\nyou have the-- the data that's using the large\nlanguage model to verify that",
    "start": "1406610",
    "end": "1412880"
  },
  {
    "text": "is not successful [INAUDIBLE]\nits behavior, right? So you can see that there\nis a really big performance",
    "start": "1412880",
    "end": "1418130"
  },
  {
    "text": "improvement without\nchanging the policy at all but just changing\nthe data, right? So I think the\nimportant takeaway here",
    "start": "1418130",
    "end": "1424520"
  },
  {
    "text": "is that it's actually\ncritical to have some suboptimal data\nin your training data especially if you're trying\nto learn from demonstration so",
    "start": "1424520",
    "end": "1431420"
  },
  {
    "text": "that the robot can learn how\nto recover from its own failure as well. OK.",
    "start": "1431420",
    "end": "1436650"
  },
  {
    "text": "So as you can imagine,\nthis framework of scaling up and distilling\ndown is quite general. You can use a large\nlanguage model",
    "start": "1436650",
    "end": "1442507"
  },
  {
    "text": "to generate training\ndata for almost any task that you want as long as you\nhave the simulation environment.",
    "start": "1442507",
    "end": "1448680"
  },
  {
    "text": "However, if you want to take\nthis idea one step further and make it really useful for\nreal-world robot deployment,",
    "start": "1448680",
    "end": "1455235"
  },
  {
    "text": "there's still a lot of\nthings missing, right? So for example, we\ndidn't talk about how to generate that environment\nat the first place,",
    "start": "1455235",
    "end": "1462030"
  },
  {
    "text": "like how to get those CAD models\nand loading all the objects. So that part now\nis still manual. And the second is that we\nneed to have way much better",
    "start": "1462030",
    "end": "1470100"
  },
  {
    "text": "way to do sim-to-real transfer. The render image still\ndoesn't look like realistic. However, personally, I\nfeel like this tool problem",
    "start": "1470100",
    "end": "1476790"
  },
  {
    "text": "with all the advancements with\ngenerative AI and everything, we can actually see\na path that they",
    "start": "1476790",
    "end": "1482460"
  },
  {
    "text": "can be potentially improved\nand addressed in the future. However, that's required\nto wait a little bit,",
    "start": "1482460",
    "end": "1489850"
  },
  {
    "text": "like wait for the simulator\nto get better or better sim-to-real techniques. So what if you're\nnot patient enough?",
    "start": "1489850",
    "end": "1495910"
  },
  {
    "text": "What you can do with the\nreal-world data today, right? So again, let's look\nat the different paths",
    "start": "1495910",
    "end": "1501669"
  },
  {
    "text": "and think about what is the\ntrue bottlenecks for scaling up the real-world data for robots.",
    "start": "1501670",
    "end": "1507040"
  },
  {
    "text": " Again, maybe slightly different\nfrom the popular opinions,",
    "start": "1507040",
    "end": "1513570"
  },
  {
    "text": "I think the need for--\nactually, the need of using human as a demonstrator\nmay not be the true bottleneck.",
    "start": "1513570",
    "end": "1519990"
  },
  {
    "text": "But I think it's that the lack\nof an intuitive and standardized interface might be\nthe real bottleneck.",
    "start": "1519990",
    "end": "1526860"
  },
  {
    "text": "So why is that or\nwhy I think so? So if you look at the internet\ndata that we have today,",
    "start": "1526860",
    "end": "1533670"
  },
  {
    "text": "actually, all this\ndata is collected by human, collected by people. However, we don't\nfind this really hard",
    "start": "1533670",
    "end": "1539790"
  },
  {
    "text": "because we have a very\nintuitive or existing interface to allow people to collect those\ndata or to generate those data.",
    "start": "1539790",
    "end": "1545490"
  },
  {
    "text": "You can use keyboard\nto type text. You can use cameras to\ncapture images, right? So those are the\nhardware interface that",
    "start": "1545490",
    "end": "1552150"
  },
  {
    "text": "really enables this data set. And similarly, if you\nstill don't believe me,",
    "start": "1552150",
    "end": "1557550"
  },
  {
    "text": "if you think about the\nself-driving car scenario-- I think the reason why a\nself-driving car can actually",
    "start": "1557550",
    "end": "1564410"
  },
  {
    "text": "get a head start on\nlearning-based method is because we already have\nan existing interface that",
    "start": "1564410",
    "end": "1569750"
  },
  {
    "text": "allows anyone who can\ndrive help to collect data. And the data is actually\nimmediately robot-complete",
    "start": "1569750",
    "end": "1575300"
  },
  {
    "text": "because it contains the\nred sensory input and also the red robot action.",
    "start": "1575300",
    "end": "1580770"
  },
  {
    "text": "Well, driving is not\nonly collecting the data from the car. You need to collect the data\nfrom the eyes of the human",
    "start": "1580770",
    "end": "1588960"
  },
  {
    "text": "as it's interact with others. There is so much cognition going\nin the brain about the driving",
    "start": "1588960",
    "end": "1595830"
  },
  {
    "text": "that we cannot collect directly. Otherwise, we would\nhave solved it. That's true. Yeah.",
    "start": "1595830",
    "end": "1601070"
  },
  {
    "text": "There is still not\ncomplete, but I will say that another\nway to think about it is that you can collect a lot\nmore data than what a human can",
    "start": "1601070",
    "end": "1608070"
  },
  {
    "text": "observe as well, right? So you actually can go beyond\nthe human sensing capabilities. You can have a much\nwider field of view.",
    "start": "1608070",
    "end": "1614190"
  },
  {
    "text": "You can add more cameras. But the key here is that we\ndon't need to train an expert",
    "start": "1614190",
    "end": "1619470"
  },
  {
    "text": "or we don't need to train a few\nexperts to collect this data. It's pretty easy to scale up.",
    "start": "1619470",
    "end": "1624950"
  },
  {
    "text": "We need to talk\nabout [INAUDIBLE] That's true as well. Yeah, maybe it's useful. Yeah.",
    "start": "1624950",
    "end": "1631670"
  },
  {
    "text": "OK. So the question here is, can\nwe create a similar interface for robot manipulation that's\nable to allow anyone that is not",
    "start": "1631670",
    "end": "1640179"
  },
  {
    "text": "trained roboticists to collect\ndata for any task they care about in any environment they\nwant the robot to be deployed?",
    "start": "1640180",
    "end": "1648970"
  },
  {
    "text": "So this question\nactually is something that motivates a number\nof projects in our group. And one of the earlier\nones that's-- actually,",
    "start": "1648970",
    "end": "1655780"
  },
  {
    "text": "before I started in Columbia\nis the project called Grasping in the Wild. So the idea here is that we can\nuse a grabbing tool that people",
    "start": "1655780",
    "end": "1663980"
  },
  {
    "text": "are already using in their\ndaily life to collect the trash or grabbing objects as an\ninterface for data collection.",
    "start": "1663980",
    "end": "1670480"
  },
  {
    "text": "So basically, what you do is\nadd an RGB-D camera to record the sensory observations. And you can track\nthe camera motion",
    "start": "1670480",
    "end": "1676720"
  },
  {
    "text": "to recover the robot actions. And then as a result, you\ncan-- with this device, you can get into any environment\nto collect robot complete data",
    "start": "1676720",
    "end": "1684350"
  },
  {
    "text": "for manipulation. And the one way to think\nabout this kind of Grasping",
    "start": "1684350",
    "end": "1690640"
  },
  {
    "text": "in the Wild interface\nis that it really provides us a very nice middle\nground between this kind",
    "start": "1690640",
    "end": "1696150"
  },
  {
    "text": "of in-the-lab\nteleoperation data and also the in-the-wild human videos\nbecause it can simultaneously",
    "start": "1696150",
    "end": "1703379"
  },
  {
    "text": "minimize the embodiment\ngap and also remained intuitive and flexible.",
    "start": "1703380",
    "end": "1709289"
  },
  {
    "text": "However, while the\nidea is quite exciting, I think, at that time, limited\nby my own engineering skills,",
    "start": "1709290",
    "end": "1714780"
  },
  {
    "text": "we still didn't really fully\nachieve the full potential for such device or\nsuch idea, right?",
    "start": "1714780",
    "end": "1720370"
  },
  {
    "text": "So the reality is that although\ntheoretically the user can use such interface to collect\na very diverse set of actions",
    "start": "1720370",
    "end": "1727740"
  },
  {
    "text": "to perform different\ntasks, but we found out is that not all the data is\ntransferable to a robot policy.",
    "start": "1727740",
    "end": "1735510"
  },
  {
    "text": "And then why that's the case? Let's look at like the\nactual low-level details of what the issue is, right?",
    "start": "1735510",
    "end": "1741690"
  },
  {
    "text": "So the first problem is the\nselection, the choice of camera. So although having a\nwrist-mounted camera",
    "start": "1741690",
    "end": "1747210"
  },
  {
    "text": "is actually very critical to\nmake the system portable so that you don't need to\nhave external camera. However, having a camera so\nclose to your manipulator",
    "start": "1747210",
    "end": "1755730"
  },
  {
    "text": "means that you have very\nrestricted visual coverage and you also\noftentimes don't have",
    "start": "1755730",
    "end": "1761760"
  },
  {
    "text": "sufficient visual context for\nthe robot to infer to action.",
    "start": "1761760",
    "end": "1767310"
  },
  {
    "text": "And also, when the robot\nactually moving very fast, you observe very\nfast camera motions.",
    "start": "1767310",
    "end": "1773520"
  },
  {
    "text": "And as a result,\ntracking the action or tracking the\ncamera, very difficult. And if you care\nabout manipulation,",
    "start": "1773520",
    "end": "1779460"
  },
  {
    "text": "it's very important to have\nhigh-quality or high-precision tracking. Otherwise, you'll\nmiss the object.",
    "start": "1779460",
    "end": "1785580"
  },
  {
    "text": "And then last one is also\npretty subtle but very important is the latency discrepancies\nbetween the data collection",
    "start": "1785580",
    "end": "1791340"
  },
  {
    "text": "and the robot deployment, right? When you collect\nthe data, there's almost no latencies between\nthe visual observation",
    "start": "1791340",
    "end": "1796830"
  },
  {
    "text": "and the action. However, when you train\na policy on your robot, you actually will observe very--\nlike many different sensor",
    "start": "1796830",
    "end": "1803640"
  },
  {
    "text": "latencies or inference\nlatencies or execution latencies depends on your hardware. And those different latencies\nwill actually easily",
    "start": "1803640",
    "end": "1811110"
  },
  {
    "text": "result in out-of-distribution\nobservations and lead to out-of-sync actions. So all those are subtle\nbut very important",
    "start": "1811110",
    "end": "1818760"
  },
  {
    "text": "issues that prevent\nus to really unlock the full potential\nof this framework.",
    "start": "1818760",
    "end": "1824620"
  },
  {
    "text": "So as a result, if you look\nat a lot of work in this area that's also including\nlike people from NIU--",
    "start": "1824620",
    "end": "1831040"
  },
  {
    "text": "they also try similar ideas-- the resulting\nsystem-- although they are very easy to achieve very\nimpressive visual diversity--",
    "start": "1831040",
    "end": "1837790"
  },
  {
    "text": "means that you can\ntake those device into many different\nenvironments. However, the resulting\nsystem is oftentimes",
    "start": "1837790",
    "end": "1844150"
  },
  {
    "text": "very low in action diversity. Means that they also\nonly do simple actions like grasping or pick and\nplace, which is a little bit",
    "start": "1844150",
    "end": "1850990"
  },
  {
    "text": "unsatisfying, because what we\nreally care about in robotics is the action diversity, right?",
    "start": "1850990",
    "end": "1856060"
  },
  {
    "text": "And that is exactly something\nthat we cannot get from internet data. But now we cannot get it either.",
    "start": "1856060",
    "end": "1862210"
  },
  {
    "text": "So how to address those issues? I think the most\nfundamental question",
    "start": "1862210",
    "end": "1868590"
  },
  {
    "text": "that I kind of asked\nmyself or again is that, is the wrist-mount\ncamera ever going",
    "start": "1868590",
    "end": "1874620"
  },
  {
    "text": "to be enough to enable large\nvariety of manipulation tasks, right? Do we really need to rely on our\nexternal camera or wrist-mount",
    "start": "1874620",
    "end": "1882540"
  },
  {
    "text": "camera is actually OK? So I think my current\nanswer is that yes, if we try to make a few\nimportant modifications.",
    "start": "1882540",
    "end": "1890040"
  },
  {
    "text": "And all this modification\nand the idea, I really need to give credit\nto Cheng and Zhenjiang, who are the students in my group\nthat make all this system work.",
    "start": "1890040",
    "end": "1899880"
  },
  {
    "text": "So the first improvement or\nupgrade is on the camera. So we actually switched from\nthe typical camera field of view",
    "start": "1899880",
    "end": "1906960"
  },
  {
    "text": "into a fisheye lens that give\nus a much wider ultrawide field of view that\nallows the robot",
    "start": "1906960",
    "end": "1913110"
  },
  {
    "text": "to see much bigger\nvisual context in order to infer the action. This is actually\nvery, very critical.",
    "start": "1913110",
    "end": "1918120"
  },
  {
    "text": "And the second is that\nwith bigger field of view, you also has more visual\nfeature for tracking.",
    "start": "1918120",
    "end": "1923790"
  },
  {
    "text": "So it means that even\nduring manipulation or when you are very close\nto the object, you still have enough\nfeatures to track.",
    "start": "1923790",
    "end": "1931210"
  },
  {
    "text": "And when you combine it with\nthe IMU sensor that is already in the GoPro camera,\nwe can actually",
    "start": "1931210",
    "end": "1936940"
  },
  {
    "text": "achieve pretty precise tracking\neven on the very fast movement. And then another\nsmart design that",
    "start": "1936940",
    "end": "1943500"
  },
  {
    "text": "is challenging that I come up\nwith is using those kind of side mirrors to help us to\ngive the robot a slightly",
    "start": "1943500",
    "end": "1951350"
  },
  {
    "text": "different perspective to help\nthem estimate steps, right? So if you only have one\ncamera, you actually don't know the\ndepth information.",
    "start": "1951350",
    "end": "1957015"
  },
  {
    "text": "But if you add two small\nmirror on the side, they actually give you\na different perspective. You can kind of run\nstereos on that.",
    "start": "1957015",
    "end": "1963029"
  },
  {
    "text": "So we call that implicit stereo. And then other design\ndecisions like-- for example,",
    "start": "1963030",
    "end": "1969799"
  },
  {
    "text": "we also add the [INAUDIBLE]\ntag on each of the fingers so that you can continuously\ntrack the gripper width, which",
    "start": "1969800",
    "end": "1976293"
  },
  {
    "text": "is actually quite\nimportant, because you want to recover\nthose precise gripper actions like timing and force.",
    "start": "1976293",
    "end": "1981980"
  },
  {
    "text": "And then by using a\ndeformable gripper or kind of like a soft finger,\nwe are able to get",
    "start": "1981980",
    "end": "1987919"
  },
  {
    "text": "a pretty nice compliance\nwhen you're interacting with the environment. And you can also kind\nof get like a little bit",
    "start": "1987920",
    "end": "1994070"
  },
  {
    "text": "of implicit force\nmeasurement by observing the deformation itself.",
    "start": "1994070",
    "end": "1999480"
  },
  {
    "text": "So this is actually an example\nof this soft finger grasping egg.",
    "start": "1999480",
    "end": "2005350"
  },
  {
    "text": "And then beyond the\nvisual features, we also find out\nthat you can actually add different sensory\nmodalities, for example,",
    "start": "2005350",
    "end": "2012230"
  },
  {
    "text": "a microphone, onto\nthe fingertip. And so in particular,\nin this case, we use a contact microphone\nthat can help you to pick up",
    "start": "2012230",
    "end": "2018640"
  },
  {
    "text": "those contact information\nlike whether you're in contact with an object\nor what kind of surface you are sliding on. And then this contact\nmicrophone can",
    "start": "2018640",
    "end": "2024970"
  },
  {
    "text": "be directly plugged in\ninto the GoPro camera, so you don't need to have\nany additional hardware",
    "start": "2024970",
    "end": "2030309"
  },
  {
    "text": "to record these new\nsensor modalities.  And then most\nimportantly, we also",
    "start": "2030310",
    "end": "2036580"
  },
  {
    "text": "make sure that this\ndevice is actually compatible with different robot\nplatforms like UR5 or Franka.",
    "start": "2036580",
    "end": "2042789"
  },
  {
    "text": "And then as a\nresult, the same data collected with this\nhandheld gripper can be used for training\npolicies for different robot",
    "start": "2042790",
    "end": "2050199"
  },
  {
    "text": "embodiments. So in the end of\nthe day, we only-- the end product of all\nthe data collection",
    "start": "2050199",
    "end": "2057149"
  },
  {
    "text": "is just a single mp4\nfile that contains all the information needed for\ntraining an effective policy,",
    "start": "2057150",
    "end": "2063359"
  },
  {
    "text": "for example, those kind of\nmultisensory observations or detailed robot actions. And we really hope\nthat this data format",
    "start": "2063360",
    "end": "2070589"
  },
  {
    "text": "can be like a standard\nand shareable data that for robot\nmanipulation, that can be shared across\ndifferent labs and projects.",
    "start": "2070590",
    "end": "2077730"
  },
  {
    "text": "So at this point,\nyou're probably very curious about what we can\ndo with a device like this, right? So let's look at some demos on\nsome hard manipulation tasks.",
    "start": "2077730",
    "end": "2087119"
  },
  {
    "text": "So the first example\nis tossing, right? We talked about that. Tossing is actually\nsomething really hard to teleop with like a\nslow interface like SpaceMouse.",
    "start": "2087120",
    "end": "2096060"
  },
  {
    "text": "And it also requires a\nvery accurate tracking on their fast motion. And it also requires\nvery precise timing",
    "start": "2096060",
    "end": "2101790"
  },
  {
    "text": "for the gripper release action. So that's why-- oops.",
    "start": "2101790",
    "end": "2107370"
  },
  {
    "text": "That's why a lot of our projects\nbefore we try to design this, carefully designed\ntossing primitives.",
    "start": "2107370",
    "end": "2112810"
  },
  {
    "text": "But here, you can just take the\ngripper and able to collect data yourself and teach the\nrobot this new skill.",
    "start": "2112810",
    "end": "2119620"
  },
  {
    "text": "And here's the\nrobot policy rollout trained on 200 demonstrations. And you can see the\nrobot is actually",
    "start": "2119620",
    "end": "2125860"
  },
  {
    "text": "sorting different objects,\nround objects, round bin; square object to the square bin. And it's able to achieve\naround like 80% or almost 90%",
    "start": "2125860",
    "end": "2134350"
  },
  {
    "text": "tossing success rate.  And then one thing that we\nnoticed also in this task",
    "start": "2134350",
    "end": "2141190"
  },
  {
    "text": "is that handling\nlatency or-- make sure that the latency matching\nis correct is very critical.",
    "start": "2141190",
    "end": "2146800"
  },
  {
    "text": "So in today's talk, I\nwon't get into details of how we actually able to match\nor compensate different latency.",
    "start": "2146800",
    "end": "2153100"
  },
  {
    "text": "But the short\nversion of the story is that if you don't handle\nthe latency well, then you are very likely to observe\nthis kind of jerky motion",
    "start": "2153100",
    "end": "2160059"
  },
  {
    "text": "on your robot\nbecause observation is out-of-distribution. And then this kind\nof jerky motion is actually OK if you only care\nabout quality static pick up",
    "start": "2160060",
    "end": "2167230"
  },
  {
    "text": "and place task. But for tossing, it's very\nobviously not good idea because it can destroy all the\nmomentums that the robot trying",
    "start": "2167230",
    "end": "2173470"
  },
  {
    "text": "to build up. And as a result, most of the\nrobot cannot lend into the bins because the velocity\nis not sufficient.",
    "start": "2173470",
    "end": "2180220"
  },
  {
    "text": " And then another task we\ntest on is bimanual folding.",
    "start": "2180220",
    "end": "2187830"
  },
  {
    "text": "So this is again\na task as we have done before but with a\nlot of carefully designed",
    "start": "2187830",
    "end": "2193260"
  },
  {
    "text": "primitives and state machines. And we are able to\ndesign a robot that's able to follow the\nclass, but it's",
    "start": "2193260",
    "end": "2198360"
  },
  {
    "text": "very, very complicated and\nreally fragile, really easy to break. But now you can just have\ntwo handheld gripper,",
    "start": "2198360",
    "end": "2204445"
  },
  {
    "text": "and you can demonstrate\na large variety of tasks that's required\nby manual manipulation. And I don't know\nwhether you catch it,",
    "start": "2204445",
    "end": "2210360"
  },
  {
    "text": "but actually, the start and\na stop for the data capturing is voice-controlled because at\nthis point, both of your hand",
    "start": "2210360",
    "end": "2217230"
  },
  {
    "text": "is occupied. You don't know how to\nturn it on and off, so you can voice control it.",
    "start": "2217230",
    "end": "2222319"
  },
  {
    "text": "OK. So this is how the robot is able\nto execute this task after again demonstrated 200 demonstrations.",
    "start": "2222320",
    "end": "2229000"
  },
  {
    "text": "And in this task, actually it's\nvery important to kind of-- it kind of tests the coordination\nand synchronization",
    "start": "2229000",
    "end": "2234280"
  },
  {
    "text": "between these two arm, right? So for example,\nwhen the robot arm trying to pick up this\npart of the cloth,",
    "start": "2234280",
    "end": "2241270"
  },
  {
    "text": "they need to\nsynchronize very well. If one of the robot\narm is slightly slower, it will fail on doing this task.",
    "start": "2241270",
    "end": "2248829"
  },
  {
    "text": "So to address this\nproblem, we actually introduced this what we call\nthe inter-gripper pose input",
    "start": "2248830",
    "end": "2254080"
  },
  {
    "text": "as an additional\ninput to the policy. You can read on the paper\nto figure out the details. But the key here\nI want to deliver",
    "start": "2254080",
    "end": "2260020"
  },
  {
    "text": "is that it's important to\nconsider the synchronization. ",
    "start": "2260020",
    "end": "2266160"
  },
  {
    "text": "And finally, the\nlast task we test on is something that we have\nnever attempted before,",
    "start": "2266160",
    "end": "2271290"
  },
  {
    "text": "which is this\ndishwashing task, right? So if you ask me,\nI don't even know how I should like start by\nlike writing out the primitives",
    "start": "2271290",
    "end": "2277290"
  },
  {
    "text": "and design the state\nmachines for this task because it is an\nultralong horizon task that each step success\ndepends on the previous one.",
    "start": "2277290",
    "end": "2284610"
  },
  {
    "text": "And also, the robot needs\nto kind of deal with-- kind of perceive and manipulate\nthe very complex objects",
    "start": "2284610",
    "end": "2289650"
  },
  {
    "text": "or fluid objects like the\nwater and the ketchup. And then it also needs\nto kind of manipulate",
    "start": "2289650",
    "end": "2295020"
  },
  {
    "text": "those constrained\narticulated objects which requires mechanical\ncompliance that is provided by our soft finger.",
    "start": "2295020",
    "end": "2302280"
  },
  {
    "text": "And finally, it\nneed to knows how to use deformable\ntools like the sponge. And then the whole\nsystem also needs",
    "start": "2302280",
    "end": "2309000"
  },
  {
    "text": "to be robust to this semantic\nconcept of cleanliness because we want the system\nto know when to stop once it",
    "start": "2309000",
    "end": "2316559"
  },
  {
    "text": "actually finish this wiping. And also, if you add more\nketchup onto the plate, the robot should learn,\nlike to continue to wash it.",
    "start": "2316560",
    "end": "2324510"
  },
  {
    "text": "So this is a very complex task. And then here is how it works.",
    "start": "2324510",
    "end": "2329550"
  },
  {
    "text": "It turned on the faucet, pick up\nthe plate, pick up the sponge, wipe the ketchup.",
    "start": "2329550",
    "end": "2336270"
  },
  {
    "text": "And if you add more\nketchup, it will come back to continue to wash it. Add more and\ncontinue to wash it.",
    "start": "2336270",
    "end": "2342780"
  },
  {
    "text": "And now you try to put back\neverything, close the faucet. If you messed up, it actually\nlearns how to correct it, right?",
    "start": "2342780",
    "end": "2350009"
  },
  {
    "text": "So it is actually pretty robust,\nat least seeing in this video. I think this is actually\npretty surprising to us",
    "start": "2350010",
    "end": "2355830"
  },
  {
    "text": "that it actually can work. So just to make sure that we're\nnot cherry-picking the results,",
    "start": "2355830",
    "end": "2362802"
  },
  {
    "text": "here it's actually\nall the rollouts that we did in the\nStanford Robotics Center. And indeed, it's not\nalways successful, right?",
    "start": "2362802",
    "end": "2369810"
  },
  {
    "text": "I think the average success\nrate is 70% for this task. But we see a sign of life.",
    "start": "2369810",
    "end": "2377079"
  },
  {
    "text": "OK. So another thing I want\nto highlight a little bit, you probably already see\nall the capabilities.",
    "start": "2377080",
    "end": "2382620"
  },
  {
    "text": "But in addition to that\nenabling new capabilities, I also want to address this\nquestion of generalization",
    "start": "2382620",
    "end": "2388170"
  },
  {
    "text": "because we are still training\na behavioral cloning policy. And most people will\nreally worry about whether your policy\nis able to generalize.",
    "start": "2388170",
    "end": "2395440"
  },
  {
    "text": "I think indeed for\nmost of the tasks that I'm showing\nyou today, what we",
    "start": "2395440",
    "end": "2400510"
  },
  {
    "text": "are doing is what we call\nnarrow-domain evaluation. Means that you are\ncollecting the training data in the same environment\nas where the robot going",
    "start": "2400510",
    "end": "2407380"
  },
  {
    "text": "to deploy, meaning that\nif you want the robot to wash dishes in your home, you\nneed to collect data yourself.",
    "start": "2407380",
    "end": "2413200"
  },
  {
    "text": "And then we do validate\nthe initial configuration of the robots and its objects.",
    "start": "2413200",
    "end": "2418450"
  },
  {
    "text": "And that's something that\nwe're showing overlay here. But now I think this kind\nof narrow-domain evaluation",
    "start": "2418450",
    "end": "2425349"
  },
  {
    "text": "is pretty standard in behavioral\ncloning paper evaluation, but it's not very\nideal if you want",
    "start": "2425350",
    "end": "2431200"
  },
  {
    "text": "to actually make the robot\na product and able to kind of deploy it anywhere. So the difference now is that\nwith a [INAUDIBLE] gripper--",
    "start": "2431200",
    "end": "2439440"
  },
  {
    "text": "and we're probably\nable to collect a lot of demonstration data on\na large variety of environments. So the question\nnow is that if we",
    "start": "2439440",
    "end": "2446289"
  },
  {
    "text": "have a lot of diverse\ntraining data, can our robot now able to\nlearn from those diverse data",
    "start": "2446290",
    "end": "2451990"
  },
  {
    "text": "and immediately generalize to\nout-of-domain environments? I perform this kind of\nout-of-domain evaluation.",
    "start": "2451990",
    "end": "2458660"
  },
  {
    "text": "Right. So that is the question\nthat we want to answer. And in this\ngeneralization experiment,",
    "start": "2458660",
    "end": "2463750"
  },
  {
    "text": "we actually study a\nrelatively simple task because we want\nto really scale up the data collection in many\ndifferent environments.",
    "start": "2463750",
    "end": "2470589"
  },
  {
    "text": "So this is the cup\nrearrangement task that we want to rearrange\nthe cup by rotating the cup",
    "start": "2470590",
    "end": "2477760"
  },
  {
    "text": "and then put-- and\nperform a pick and place. So this task, although\nit looks much simpler than the dishwashing\ntask, is still not trivial",
    "start": "2477760",
    "end": "2484960"
  },
  {
    "text": "because it needs to combine\ndifferent action space and also like--\nlike, for example, the underactuated pushing and\nalso precise pick and place,",
    "start": "2484960",
    "end": "2491960"
  },
  {
    "text": "right? So for this task, our students\nactually go around the campus",
    "start": "2491960",
    "end": "2497200"
  },
  {
    "text": "and collect a lot of\nhuman demonstration data in many different environments.",
    "start": "2497200",
    "end": "2502750"
  },
  {
    "text": "And we're going to train one\ndiffusion policy with all this data that we collected.",
    "start": "2502750",
    "end": "2508519"
  },
  {
    "text": "And then after that, we'll\nbasically pick a sunny day, roll out our robot\ninto an environment",
    "start": "2508520",
    "end": "2514099"
  },
  {
    "text": "that we never\ncollect the data in, and then put down some\nnew objects that we have-- new cups that the robot\nnever seen before.",
    "start": "2514100",
    "end": "2522280"
  },
  {
    "text": "And then we'll just let the\nrobot to do the rollout. So in this particular\nexample, the robot is able to kind of\njust immediately",
    "start": "2522280",
    "end": "2529900"
  },
  {
    "text": "generalize to this unseen\nobject and unseen environment without any further fine-tuning\nin this particular environment.",
    "start": "2529900",
    "end": "2536740"
  },
  {
    "text": "And that is because of\nthe diverse training data that we used. And then also because\nthe whole system",
    "start": "2536740",
    "end": "2544329"
  },
  {
    "text": "only rely on wrist-mount\ncamera, no environment camera. And also, we predict\nrelative action trajectory.",
    "start": "2544330",
    "end": "2551170"
  },
  {
    "text": "The system is actually\nrobust to based movement. So we can move the robot\nbased around is still able to perform the task.",
    "start": "2551170",
    "end": "2556570"
  },
  {
    "text": "So that actually makes it\na really nice framework for mobile manipulation systems.",
    "start": "2556570",
    "end": "2563560"
  },
  {
    "text": "And then we try to stress test\nthe system by basically taking our robots out for a walk\nanywhere around the campus",
    "start": "2563560",
    "end": "2570100"
  },
  {
    "text": "and then try to test the\nalgorithm performance, right? So the average success rate\nis about 75 success rate.",
    "start": "2570100",
    "end": "2577599"
  },
  {
    "text": "Sometimes it's still not\nable to perform the task. But for the majority\nof the time, it's able to do the\ntask pretty well.",
    "start": "2577600",
    "end": "2583210"
  },
  {
    "text": "Like there is-- like\nyou probably recognize all the spots in campus. Yeah. And this one is particularly\nlike surprising to us",
    "start": "2583210",
    "end": "2589420"
  },
  {
    "text": "that you can put the robot\non a fountain that's not even a table. And the robot is\nable to do the task.",
    "start": "2589420",
    "end": "2596210"
  },
  {
    "text": "OK. So now you can see that\nthe policy potentially can generalize, like can\ngeneralize to new environment.",
    "start": "2596210",
    "end": "2602200"
  },
  {
    "text": "But we also want to just ask-- kind of dig one layer deeper\nand ask the question of,",
    "start": "2602200",
    "end": "2608350"
  },
  {
    "text": "where does this generalization\ncapability really come from, right? So I think one of the subtle\ndetails I haven't mentioned",
    "start": "2608350",
    "end": "2615070"
  },
  {
    "text": "is that all the policy\nthat I show you today is actually started by using a\npretrained visual encoder that",
    "start": "2615070",
    "end": "2621940"
  },
  {
    "text": "is CLIP, a visual\nencoder that is already trained on internet data. So it is possible\nor at least some",
    "start": "2621940",
    "end": "2629500"
  },
  {
    "text": "have a hypothesis that maybe\nthe visual encoder itself that's trained on\nthe internet data already provide this\ngeneralization capability.",
    "start": "2629500",
    "end": "2636550"
  },
  {
    "text": "And we actually don't need\nthis In-the-Wild action data. So ourself is also very\ncurious about this question,",
    "start": "2636550",
    "end": "2643360"
  },
  {
    "text": "so that's why we conduct\neven more experiments. So in this particular\nexperiment, we still keep the visual\nencoder that is pretrained",
    "start": "2643360",
    "end": "2651910"
  },
  {
    "text": "with CLIP on the internet data. But now instead of using\nthis In-the-Wild data set, we actually collect a\nnarrow-domain action",
    "start": "2651910",
    "end": "2658660"
  },
  {
    "text": "data set that's doing this\ntask in the lab environment. So the question now is\nthat with this combination",
    "start": "2658660",
    "end": "2665560"
  },
  {
    "text": "whether the robot\npolicy is still able to do auto\ndomain generalization.",
    "start": "2665560",
    "end": "2671680"
  },
  {
    "text": "So the short answer is\nthat not really, right? So actually, to our surprise\nis not just slightly worse.",
    "start": "2671680",
    "end": "2678250"
  },
  {
    "text": "It actually completely\ndoesn't work. So this robot actually\ndoes not even go to the cup and just not-- don't\nreally know how",
    "start": "2678250",
    "end": "2684830"
  },
  {
    "text": "to do this task in this\nout-of-distribution environment. However, the same\npolicy actually",
    "start": "2684830",
    "end": "2690440"
  },
  {
    "text": "can work really well\nin the lab environment, in this narrow domain\nthat we test on,",
    "start": "2690440",
    "end": "2696453"
  },
  {
    "text": "in the narrow domain\nthat we trained on. So here is just a quick\ncomparison of all our policy that is actually trained\nwith In-the-Wild action data.",
    "start": "2696453",
    "end": "2705950"
  },
  {
    "text": "So I think the\nquick takeaway here is that at least for today--",
    "start": "2705950",
    "end": "2711920"
  },
  {
    "text": "you draw the conclusion today-- I think fine-tuning\nlarge pretrained vision model with a\nnarrow-domain robot data",
    "start": "2711920",
    "end": "2718280"
  },
  {
    "text": "is still insufficient\nfor generalization. So what's the conclusion\nmade from here is that we still need\ndiverse robot action data.",
    "start": "2718280",
    "end": "2726619"
  },
  {
    "text": "So I think this really\nhelp us to kind of put it into perspective that\nmaybe a lot of people",
    "start": "2726620",
    "end": "2732050"
  },
  {
    "text": "think that if we have\ninternet-scale data, can we just make a smart\nuse of this internet data",
    "start": "2732050",
    "end": "2737059"
  },
  {
    "text": "so that robot can work? I think maybe yes in the\nfuture, but right now, still, we cannot do that.",
    "start": "2737060",
    "end": "2742355"
  },
  {
    "text": " OK. So to take a step back and\nquickly summarize this UMI",
    "start": "2742355",
    "end": "2749570"
  },
  {
    "text": "project and think\nabout how it actually able to address each of those\ndata collection issues, right?",
    "start": "2749570",
    "end": "2754580"
  },
  {
    "text": "So for the first one,\nbecause the interface is so easy and intuitive\nto use, you no longer",
    "start": "2754580",
    "end": "2759859"
  },
  {
    "text": "need to train yourself in\norder to help the robot to collect data. And then because the setup is\nvery portable and low-cost,",
    "start": "2759860",
    "end": "2768080"
  },
  {
    "text": "you can easily\npass this hardware to your friends to get\ninto different environments in order to cover the large\nand diverse task space.",
    "start": "2768080",
    "end": "2775712"
  },
  {
    "text": "And also, you can\ntell your friends to help you collect data. Don't be afraid of making\nmistakes during demonstration because those error\ncorrection data is actually",
    "start": "2775712",
    "end": "2782930"
  },
  {
    "text": "very important and useful for\nthe downstream policy training. And then for the last part,\nprotect the environment,",
    "start": "2782930",
    "end": "2790340"
  },
  {
    "text": "now because we use only\nwrist-mount camera-- we don't need to calibrate the\ncamera at all-- you actually don't really worry\nabout anyone touching",
    "start": "2790340",
    "end": "2797210"
  },
  {
    "text": "your setup or your camera. You actually can achieve very\neasy deployment for your robots.",
    "start": "2797210",
    "end": "2802820"
  },
  {
    "text": "Drag it out and it can work. The system is a lot\nmore robust now, and our engineers are\nmuch-- or our students",
    "start": "2802820",
    "end": "2808828"
  },
  {
    "text": "are much happier because\nthey don't need to worry about camera calibration. I will say this is a\nbig deal if you really",
    "start": "2808828",
    "end": "2815309"
  },
  {
    "text": "care about robot deployment. I think this is a\nquality-of-life improvement. ",
    "start": "2815310",
    "end": "2821990"
  },
  {
    "text": "So if you are hopefully--\nat this point, I pick your interest, and\nyou want to try it out. So all the code base\nand hardware guidelines",
    "start": "2821990",
    "end": "2829520"
  },
  {
    "text": "of how to build UMI's data\nset just got released today, so it's all like\nfresh off the oven.",
    "start": "2829520",
    "end": "2835310"
  },
  {
    "text": "And also, we have the\ndiffusion policy's code base. So for any of those part\nlike code or the hardware,",
    "start": "2835310",
    "end": "2841610"
  },
  {
    "text": "if you have suggestions\nor questions, feel free to reach out to\nus, and we are constantly improving it.",
    "start": "2841610",
    "end": "2846890"
  },
  {
    "text": "We really hope that\nwe can maintain a nice like open-source\nenvironment that everybody can contribute to.",
    "start": "2846890",
    "end": "2852590"
  },
  {
    "text": " OK, so I will--",
    "start": "2852590",
    "end": "2859150"
  },
  {
    "text": "I will conclude\nmy talk by trying to share a few thoughts about\nrobotics and large-scale data.",
    "start": "2859150",
    "end": "2866849"
  },
  {
    "text": "So this is mostly\nmy personal opinion, but we all know that data is\nvery important for learning.",
    "start": "2866850",
    "end": "2872700"
  },
  {
    "text": "The more, the better. But I think, unfortunately,\nas roboticists, we may not have the\nluxury to just sit",
    "start": "2872700",
    "end": "2879119"
  },
  {
    "text": "there and wait for the perfect\ndata to emerge on the internet. But also, at the same\ntime, as roboticists,",
    "start": "2879120",
    "end": "2886500"
  },
  {
    "text": "we do have a lot of\npowerful tools and knowledge at our disposal to create the\nright data for robot learning.",
    "start": "2886500",
    "end": "2893400"
  },
  {
    "text": "So for example, we have\nhigh-quality physics simulators, or we can design our\nnew hardware or sensors",
    "start": "2893400",
    "end": "2899880"
  },
  {
    "text": "to help us to scale up the\ndata collection for robots. Those tools and\nknowledge are something that most of ML\nresearchers don't have.",
    "start": "2899880",
    "end": "2907350"
  },
  {
    "text": "That's our advantage. And I really believe\nthat the data that were driven by physical\nrobots in the physical world",
    "start": "2907350",
    "end": "2913800"
  },
  {
    "text": "will eventually go beyond\nthe internet data we have today in terms of both\nthe scale and the richness.",
    "start": "2913800",
    "end": "2919510"
  },
  {
    "text": "And then I think what I really\nhope for my today's talk is to encourage you as current\nand future roboticists,",
    "start": "2919510",
    "end": "2926920"
  },
  {
    "text": "really think about how you can\nmake use of your unique skill set and knowledge to help us\nto shape the next generation",
    "start": "2926920",
    "end": "2934660"
  },
  {
    "text": "of big data. So I think, for\nexample, there are a lot of existing exciting\nwork in the community that",
    "start": "2934660",
    "end": "2942360"
  },
  {
    "text": "are trying to approach\nthis data problem from many different directions. Like, for example, ALOHA is\nsomething very quite famous.",
    "start": "2942360",
    "end": "2949220"
  },
  {
    "text": "I think this is a very\nnice framework as well, like hardware\ndesign that helps us to reduce the cost\nof data collection",
    "start": "2949220",
    "end": "2954710"
  },
  {
    "text": "for very dexterous\nmanipulation tasks. There's Open\nX-Embodiment data sets. All this work, I think,\nis very exciting,",
    "start": "2954710",
    "end": "2961040"
  },
  {
    "text": "and I highly recommend\nyou to check it out. And also very interesting\nto hear your thoughts.",
    "start": "2961040",
    "end": "2966290"
  },
  {
    "text": "So if you have\nideas or thoughts, I'm happy to hear them. And please email to\nme your ideas as well.",
    "start": "2966290",
    "end": "2975010"
  },
  {
    "text": "And with that, I want to just\nthank all my collaborators and funding agencies\nand, most importantly,",
    "start": "2975010",
    "end": "2981000"
  },
  {
    "text": "my students who make all\nthese projects possible. And I'm happy to take questions. [APPLAUSE]",
    "start": "2981000",
    "end": "2988940"
  },
  {
    "text": " And if I may, I want to\njust introduce this picture.",
    "start": "2988940",
    "end": "2994210"
  },
  {
    "text": "So this is actually a picture\nthat every student makes a little toy for themselves. And you can see that this little\nrobot actually has this gripper.",
    "start": "2994210",
    "end": "3001340"
  },
  {
    "text": "So this is [INAUDIBLE]. And then if you look carefully\nand you can-- for example, this is LLM planner\ndiffusion policy.",
    "start": "3001340",
    "end": "3009049"
  },
  {
    "text": "And then they actually\nmade different robots for their own research. So that's like a\nvery cute photo.",
    "start": "3009050",
    "end": "3017000"
  },
  {
    "text": "Question? Beautiful talk. Thank you so much. I was curious. So I think today\nyou demonstrated",
    "start": "3017000",
    "end": "3023420"
  },
  {
    "text": "how with enough data\nyou can generalize to changing environments\nwith the same hardware. I was curious whether\na similar approach",
    "start": "3023420",
    "end": "3030980"
  },
  {
    "text": "can be used to generalize among\ndifferent hardware platforms. So for instance, you might\nhave rigid fingers, two fingers",
    "start": "3030980",
    "end": "3037280"
  },
  {
    "text": "instead of [INAUDIBLE] or maybe\nyou have more fingers or yeah. Yeah, that is still hard.",
    "start": "3037280",
    "end": "3043645"
  },
  {
    "text": "But I can tell you\nwhat we can do already with the current setup. We can generalize\ndifferent robot embodiments",
    "start": "3043645",
    "end": "3050519"
  },
  {
    "text": "that has different--\nlike for your arms. Like if the hand is the same,\nyou switch the same hand. We can actually deploy the\nsame policy on Franka, on UR5",
    "start": "3050520",
    "end": "3058755"
  },
  {
    "text": "or on other different\nrobot arms because we only care about the observation\nbelow the hand, right?",
    "start": "3058755",
    "end": "3066270"
  },
  {
    "text": "If you want the same\ndata or the policy to directly generalize\ndifferentials like hand,",
    "start": "3066270",
    "end": "3072069"
  },
  {
    "text": "I think that probably requires\nmore involved like engineering. Like maybe you need a different\nway to train the policy. So instead of training directly\nfrom a visual motor policy that",
    "start": "3072070",
    "end": "3080410"
  },
  {
    "text": "come from image to\naction, maybe instead you want to train a dynamics\nmodel that kind of just learned like general\ndynamics how action",
    "start": "3080410",
    "end": "3088210"
  },
  {
    "text": "will change the environment\nand then use that dynamics model to kind of\ninfer the action or have a separate like\ninverse model for your robots.",
    "start": "3088210",
    "end": "3094810"
  },
  {
    "text": "So that is not yet done. Yeah. Thank you. ",
    "start": "3094810",
    "end": "3102569"
  },
  {
    "text": "Any other questions?  Would it be possible\nto get UMI out",
    "start": "3102570",
    "end": "3109470"
  },
  {
    "text": "in the wild to\nthe general public and just like instead\nof skipping it in a day to just like tell them\nall to gather data for you?",
    "start": "3109470",
    "end": "3115799"
  },
  {
    "text": "Yeah. I think it's possible. It does require us to think\ncarefully about the incentives.",
    "start": "3115800",
    "end": "3121260"
  },
  {
    "text": "Or you can obviously\nhire people to do it. But you kind of need to be\na big company to do that.",
    "start": "3121260",
    "end": "3128310"
  },
  {
    "text": "Or if you-- like I think\nself-driving car is a nice example is that it not\nonly has a good interface. It's also something\nthat people have",
    "start": "3128310",
    "end": "3134280"
  },
  {
    "text": "to do on a day-to-day basis. They need to drive. So right now, I don't think\nwe have a nice business model",
    "start": "3134280",
    "end": "3141450"
  },
  {
    "text": "or scale-up model that\nalso provides the user incentive to use this hardware. But at least, we show that\nit's not hard, it's easy,",
    "start": "3141450",
    "end": "3148770"
  },
  {
    "text": "and it's doable. Yeah. Were there any\nspecial type of tasks",
    "start": "3148770",
    "end": "3153839"
  },
  {
    "text": "that you found particularly\nhard to do even with this very beautiful\nframework, like working",
    "start": "3153840",
    "end": "3159869"
  },
  {
    "text": "with soft deformable objects?  Don't beat her.",
    "start": "3159870",
    "end": "3165480"
  },
  {
    "text": "Let's see what she says. [CHUCKLES] Yeah. Some deformable\nobjects is not hard.",
    "start": "3165480",
    "end": "3170819"
  },
  {
    "text": "It's actually pretty\neasy because the object itself is compliant. Grasping is not hard.",
    "start": "3170820",
    "end": "3177000"
  },
  {
    "text": "There are things that are hard. I think it's quite-- I think the biggest\nlimiting factor is the form",
    "start": "3177000",
    "end": "3182700"
  },
  {
    "text": "factor for the finger itself. Right now, you can\nsee it's quite big. And then the reason\nthat it's big is because we want\nit to directly apply",
    "start": "3182700",
    "end": "3189390"
  },
  {
    "text": "for like UR5 types of robots. As a result, it cannot really\ndo very fine-grained detailed manipulation tasks.",
    "start": "3189390",
    "end": "3195267"
  },
  {
    "text": "For example, the ones that ALOHA\nhas pretty good [INAUDIBLE],, like those like putting\non contact, I think,",
    "start": "3195267",
    "end": "3200512"
  },
  {
    "text": "right now is not good enough. And I think that is limited by\nlike the size of the gripper. But if you just shrink down\neverything, make it smaller,",
    "start": "3200512",
    "end": "3207088"
  },
  {
    "text": "I think it's possible. ",
    "start": "3207088",
    "end": "3212191"
  },
  {
    "text": "OK. [CHUCKLES] So most\nmanipulation task",
    "start": "3212191",
    "end": "3218059"
  },
  {
    "text": "involves contact and being able\nto apply specific forces when",
    "start": "3218060",
    "end": "3223670"
  },
  {
    "text": "we go to more complex tasks. And it seems like most of the\ndata we are collecting today",
    "start": "3223670",
    "end": "3232676"
  },
  {
    "text": "is aimed at finding the\ntrajectories, finding positions.",
    "start": "3232676",
    "end": "3238100"
  },
  {
    "text": "And in fact, I don't think\nthis is a difficult thing. I mean, if you know where\nyou want to go today,",
    "start": "3238100",
    "end": "3247069"
  },
  {
    "text": "it's very easy to\nsend the robot there. I mean, this is not\nreally the problem that you're solving\nmany different problems",
    "start": "3247070",
    "end": "3253670"
  },
  {
    "text": "at the same time. You're solving the\ngrasping configuration. You're solving what speed\nat which you are moving,",
    "start": "3253670",
    "end": "3262250"
  },
  {
    "text": "what contact-- I mean, grasping force. But I think missing the forces\nand moment being applied",
    "start": "3262250",
    "end": "3273890"
  },
  {
    "text": "is really, really a major\nlimitation for many of the data we are collecting today.",
    "start": "3273890",
    "end": "3279680"
  },
  {
    "text": "That's true. I think that clearly we are\nmissing those information like for a large part.",
    "start": "3279680",
    "end": "3286010"
  },
  {
    "text": "But maybe I can address\nit from two angles. First is that I think\nwith the current setup, we can observe some\nforce information, right?",
    "start": "3286010",
    "end": "3293750"
  },
  {
    "text": "So I emphasize a few times\nthat the finger is deformable. We actually can kind of\nobserve like how much",
    "start": "3293750",
    "end": "3298790"
  },
  {
    "text": "force you're applying\nthrough the deformation. We have contact microphone\ntell you the contact events.",
    "start": "3298790",
    "end": "3304160"
  },
  {
    "text": "And then the camera\nalso can tell you like the contact\nforce actually a little bit, even\nthe contact position.",
    "start": "3304160",
    "end": "3311120"
  },
  {
    "text": "So we have some of\nthose information. It's not completely in the dark. And then second is that\nfor a lot of tasks,",
    "start": "3311120",
    "end": "3319250"
  },
  {
    "text": "actually, surprisingly,\nwith force, we'll probably be\nable to do it better. But with a very high\nrate closed-loop policy,",
    "start": "3319250",
    "end": "3326810"
  },
  {
    "text": "you can already do\nthe task pretty well. I think that's something\nthat like surprised to--",
    "start": "3326810",
    "end": "3333420"
  },
  {
    "text": "kind of surprised\nto a lot of people that if you just train a policy\nto do closed-loop control and you have fast\nreaction rate with respect",
    "start": "3333420",
    "end": "3340730"
  },
  {
    "text": "to your visual data,\nyou can kind of get away without a lot of like\nhigh-quality force feedback.",
    "start": "3340730",
    "end": "3346900"
  },
  {
    "start": "3346900",
    "end": "3351000"
  }
]