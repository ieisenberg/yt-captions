[
  {
    "start": "0",
    "end": "73000"
  },
  {
    "text": "well welcome everyone is it ok yeah better holiday we've been so far good",
    "start": "50",
    "end": "7200"
  },
  {
    "text": "awesome thank you well so in this session we're going to talk about is the",
    "start": "7200",
    "end": "14429"
  },
  {
    "text": "to announcement which is saw in keynote today which is Amazon ST select and Amazon Glacius select and then we're",
    "start": "14429",
    "end": "21359"
  },
  {
    "text": "going to talk about that you listen to Andy talk about in the keynote that how these features can help you run make a",
    "start": "21359",
    "end": "27630"
  },
  {
    "text": "data Lake effect in a more effective manner based on the existing technologies you need get the",
    "start": "27630",
    "end": "32880"
  },
  {
    "text": "performance improvements so what we're going to talk about in this session is first introduce you to these two",
    "start": "32880",
    "end": "39210"
  },
  {
    "text": "features which we announced today in the keynote Amazon ST select an Amazon",
    "start": "39210",
    "end": "44430"
  },
  {
    "text": "glacier select in a very brief describe what they do how they work or data operate and then do a deep dive into",
    "start": "44430",
    "end": "51960"
  },
  {
    "text": "some of the use cases in which these these features apply and how they work",
    "start": "51960",
    "end": "57870"
  },
  {
    "text": "so we're gonna do also do demo in all of the most of the use cases say that this is the use case and how these demos or",
    "start": "57870",
    "end": "63629"
  },
  {
    "text": "how this technology actually operate so with that what you heard Andy talk about",
    "start": "63629",
    "end": "71610"
  },
  {
    "text": "in the keynote today is that many customers today are choosing to use s3",
    "start": "71610",
    "end": "77310"
  },
  {
    "start": "73000",
    "end": "337000"
  },
  {
    "text": "as a data Lake they are choosing s to be use at a data Lake because of the",
    "start": "77310",
    "end": "83009"
  },
  {
    "text": "feature it provides for you to effectively manage operate and store the data so if you think about it for",
    "start": "83009",
    "end": "90720"
  },
  {
    "text": "example with in s3 when you put data we store the data in a minimum of three",
    "start": "90720",
    "end": "96900"
  },
  {
    "text": "facilities or three Easy's which means that you get eleven nines of durability",
    "start": "96900",
    "end": "101909"
  },
  {
    "text": "from your data set not only that your data sort of yours you get the",
    "start": "101909",
    "end": "107399"
  },
  {
    "text": "throughput we're depending on irrespective of how much data you're storing but you can use the same data",
    "start": "107399",
    "end": "113340"
  },
  {
    "text": "set you can use the same construct within s3 to query the data using a ten node cluster 100 node cluster a thousand",
    "start": "113340",
    "end": "120840"
  },
  {
    "text": "node cluster you can operate the same data set at a terabyte level para byte",
    "start": "120840",
    "end": "126270"
  },
  {
    "text": "level or exabyte level without having to do any sort of management any change in architecture or any sort of like",
    "start": "126270",
    "end": "131879"
  },
  {
    "text": "external steps and but if you don't think about that from a encryption perspective for example you",
    "start": "131879",
    "end": "139110"
  },
  {
    "text": "have a choice of three encryptions you could choose to encrypt data you have rs3 he do the encryption for you you",
    "start": "139110",
    "end": "145350"
  },
  {
    "text": "could choose to have the key encryption key being stored in AWS kms and Esty used as keys to encrypt your data which",
    "start": "145350",
    "end": "152190"
  },
  {
    "text": "means that you are in control of the keys or in cases you can also choose to encrypt the data on the client side we're putting in s3 and all of these",
    "start": "152190",
    "end": "159960"
  },
  {
    "text": "work with s3 or if you think about auditing for instance that you can use",
    "start": "159960",
    "end": "165480"
  },
  {
    "text": "cloud real data events to figure out who is accessing your data when they're accessing it how they access the data",
    "start": "165480",
    "end": "170970"
  },
  {
    "text": "sets or think about Amazon Maisie which allows you to protect discover and find",
    "start": "170970",
    "end": "178280"
  },
  {
    "text": "sensitive information sitting in s3 but not only that Amazon may see also allows",
    "start": "178280",
    "end": "184380"
  },
  {
    "text": "you to figure out that has there been any anomalies in your object access",
    "start": "184380",
    "end": "189450"
  },
  {
    "text": "patterns it uses the cloud trail data events and machine learning to understand that this was your normal",
    "start": "189450",
    "end": "195360"
  },
  {
    "text": "access patterns but now they see a change in behavior and it can alert based on that which means you can build",
    "start": "195360",
    "end": "200700"
  },
  {
    "text": "complete set of auditing capabilities on top of that or for instance tags using",
    "start": "200700",
    "end": "205950"
  },
  {
    "text": "tags you can control lifecycle policies you can say that which objects get moved to which storage class using tags you",
    "start": "205950",
    "end": "212880"
  },
  {
    "text": "can even control access policies to say that this application can only set connect can only access these data set",
    "start": "212880",
    "end": "219780"
  },
  {
    "text": "which are tagged using these objects and the tags are mutable you can change the tags you can have the tags of prey",
    "start": "219780",
    "end": "225030"
  },
  {
    "text": "change this on your house needs changes or if you think about the business insights what you can get for example",
    "start": "225030",
    "end": "232100"
  },
  {
    "text": "storage insights can recommend that based on your lifecycle based on your object in access patterns that you can",
    "start": "232100",
    "end": "238260"
  },
  {
    "text": "move this data from one system from one as the standard storage class for example into stand an infrequent storage",
    "start": "238260",
    "end": "244980"
  },
  {
    "text": "class or even to glacier for I archival purposes or if you think about it within",
    "start": "244980",
    "end": "250950"
  },
  {
    "text": "a data Lake it's most important most critical for figuring out many ways for you to bring the data because data comes",
    "start": "250950",
    "end": "257549"
  },
  {
    "text": "in wide variety of formats wide variety of sources for example you can use no mobile snowball to move petabytes or",
    "start": "257549",
    "end": "264479"
  },
  {
    "text": "exabytes of data within s3 very fish very effortlessly or you can use s3 transfer acceleration which allows you",
    "start": "264479",
    "end": "271740"
  },
  {
    "text": "to move data from remote devices or from other locations and a very high throughput into s3 in a very reliable",
    "start": "271740",
    "end": "278759"
  },
  {
    "text": "manner or if you think about Canisius firehose for example which allows you to capture data in real time so clickstream",
    "start": "278759",
    "end": "285780"
  },
  {
    "text": "data sets he went screen event data sets all of that can land up in s3 much easily and",
    "start": "285780",
    "end": "292669"
  },
  {
    "text": "s3 has a storage has twice as many partner integrations as any other cloud",
    "start": "292669",
    "end": "298530"
  },
  {
    "text": "storage vendor out there if you think about a doubles marketplace today has over 3500 software's available which",
    "start": "298530",
    "end": "306210"
  },
  {
    "text": "work with s3 these are offered by 1100 over 1100 software partners who have put",
    "start": "306210",
    "end": "311610"
  },
  {
    "text": "the solutions in the idris marketplace and all of these features which comes",
    "start": "311610",
    "end": "317129"
  },
  {
    "text": "with nest 3 these are what is good it's almost fundamentally impossible for you",
    "start": "317129",
    "end": "323849"
  },
  {
    "text": "to build them using your own storage but you get this in a way so you can build your data leak you can manage your data",
    "start": "323849",
    "end": "329969"
  },
  {
    "text": "leak but with building managing or storing your data in data like is just not enough if you really think about it one of the",
    "start": "329969",
    "end": "337409"
  },
  {
    "start": "337000",
    "end": "487000"
  },
  {
    "text": "biggest reason like if you look back in time why people started using s3 as a",
    "start": "337409",
    "end": "342960"
  },
  {
    "text": "data leak or as a way to store the data and process analyze the data was because it gave you the ability to use the data",
    "start": "342960",
    "end": "350370"
  },
  {
    "text": "in place without having to move the data from one system to another system we all",
    "start": "350370",
    "end": "355500"
  },
  {
    "text": "have been through the journey where we had a transactional data base very storing all of our data sets and then",
    "start": "355500",
    "end": "361259"
  },
  {
    "text": "moving them into a data warehouse after doing ETL or processing just to be able to create the data set but with s3 over",
    "start": "361259",
    "end": "368669"
  },
  {
    "text": "the same set of data now you can run sequel queries using a teener using Idina you can just start with a few",
    "start": "368669",
    "end": "374520"
  },
  {
    "text": "clicks in a console define a table and start running you just sequel queries on ad hoc lee with richard spectrum for",
    "start": "374520",
    "end": "380430"
  },
  {
    "text": "example the redshift queries can span over your data in redshift or in s3 it",
    "start": "380430",
    "end": "386520"
  },
  {
    "text": "runs the same query seamlessly across either of these data stores so s3 becomes a fundamental extension of your",
    "start": "386520",
    "end": "392639"
  },
  {
    "text": "data warehouse or with Amazon EMR which has been available for many years now which provider manage Hadoop framework",
    "start": "392639",
    "end": "398789"
  },
  {
    "text": "so you can run job like or frameworks such as high spark big MapReduce all of these technologies",
    "start": "398789",
    "end": "406960"
  },
  {
    "text": "they operate by directly working with the data sitting in s3 you never had to",
    "start": "406960",
    "end": "412480"
  },
  {
    "text": "sort of copy data out of s3 into one of the system just to be able to use it if you ever thought about it",
    "start": "412480",
    "end": "418930"
  },
  {
    "text": "for example when you go to Athena you just define a table saying that this is the location of the data in s3 and you just run queries on top of it or blue",
    "start": "418930",
    "end": "426700"
  },
  {
    "text": "for example glue allows you to catalog your data it also have crawlers which",
    "start": "426700",
    "end": "431890"
  },
  {
    "text": "can discover your data and automatically catalog them in the data catalog even for our transformation perspective what",
    "start": "431890",
    "end": "437950"
  },
  {
    "text": "blue allows you to do is it takes your ETL workflow generates python script for the you and these Python scripts glue",
    "start": "437950",
    "end": "444670"
  },
  {
    "text": "runs them on a scale outs SPARC servers but you don't have to manage any servers you just write to a deal generator with",
    "start": "444670",
    "end": "450880"
  },
  {
    "text": "your workflow it converts them into Python code which runs on a spark servers but all of these to us together",
    "start": "450880",
    "end": "456610"
  },
  {
    "text": "for example the catalog you create using glue you can quit you can use the capital to query the data from Athena",
    "start": "456610",
    "end": "462580"
  },
  {
    "text": "from a spectrum or even from Amazon EMR in all of the systems what is happening",
    "start": "462580",
    "end": "468010"
  },
  {
    "text": "is that you're just moving taking the data Orbeez are reading the data from s3 processing it and giving it the result",
    "start": "468010",
    "end": "473620"
  },
  {
    "text": "set but have you ever thought about what really happens in the scenarios deep",
    "start": "473620",
    "end": "479080"
  },
  {
    "text": "inside when we run this query saying that this is my table in s3 and this is my query what happens is the query for",
    "start": "479080",
    "end": "484960"
  },
  {
    "text": "example let's think about it today we are a small company we're a startup we",
    "start": "484960",
    "end": "491230"
  },
  {
    "start": "487000",
    "end": "589000"
  },
  {
    "text": "have 100 customers we are storing all of the data set in let's say one object my",
    "start": "491230",
    "end": "496720"
  },
  {
    "text": "object has one data for all all the events generated by those hundred customers I want today to get data for",
    "start": "496720",
    "end": "503470"
  },
  {
    "text": "one customer so I will just log into a single node cluster somewhere and I will say select star from that object or",
    "start": "503470",
    "end": "509440"
  },
  {
    "text": "select star from that file where customer ID equal to this that's great tomorrow let's say that we become a big",
    "start": "509440",
    "end": "517539"
  },
  {
    "text": "we are very successful become a big enterprise now we have a million customers and I storing the same million",
    "start": "517540",
    "end": "523030"
  },
  {
    "text": "customers in an object so if I run a query today select star from that object where customer ID equal to this in that",
    "start": "523030",
    "end": "530440"
  },
  {
    "text": "case what's going to happen is that query is gonna retrieve the data for all a million customers and then find one",
    "start": "530440",
    "end": "537680"
  },
  {
    "text": "single customer autofit what really happen in this case is your compute now",
    "start": "537680",
    "end": "543380"
  },
  {
    "text": "will no longer be able to run on a single node cluster you probably have to deploy a cluster of hundred nodes maybe",
    "start": "543380",
    "end": "548620"
  },
  {
    "text": "something bigger than that and in this case what you're doing is that all of the compute is just reading is scanning",
    "start": "548620",
    "end": "556130"
  },
  {
    "text": "filtering data from s3 in most of the time when you run a queries we don't think in that manner right but this is",
    "start": "556130",
    "end": "563030"
  },
  {
    "text": "what essentially happens if you look deep down and a resultant of which is that your compute scales based on the",
    "start": "563030",
    "end": "569870"
  },
  {
    "text": "object size it does not need to it does not scales today based on the amount of data you actually want to use for a",
    "start": "569870",
    "end": "576110"
  },
  {
    "text": "query because in both of the cases of the query I wanted to query one customer but in one case I to use a single node",
    "start": "576110",
    "end": "582380"
  },
  {
    "text": "in other case I'd use 100 node cluster just to keep the performance same so",
    "start": "582380",
    "end": "588830"
  },
  {
    "text": "what it really means is that the whole filtering the scanning retrieval which is in Bill part of any framework any",
    "start": "588830",
    "end": "595790"
  },
  {
    "text": "application which works on top of s3 it's there by design but we never look",
    "start": "595790",
    "end": "601760"
  },
  {
    "text": "into that that compute part of application all it's doing is just",
    "start": "601760",
    "end": "607010"
  },
  {
    "text": "reading data from s3 to find the bytes you need or to find the data you need and which means that a whole lot of",
    "start": "607010",
    "end": "612890"
  },
  {
    "text": "applications every application in fact is doing this heavy lifting in trying to scan find and retrieve data from s3 in",
    "start": "612890",
    "end": "620210"
  },
  {
    "text": "another case for example think about that what if I would have archived that using life cycle policies back from s3",
    "start": "620210",
    "end": "625490"
  },
  {
    "text": "to glacier and now didn't do to some reason I want to find that single customer again today I have to retrieve",
    "start": "625490",
    "end": "632750"
  },
  {
    "text": "the entire object I have to restore the entire object back to s3 and then be able to use it so while technologies",
    "start": "632750",
    "end": "639620"
  },
  {
    "text": "have allowed you to use data in s3 in a very effective in a many different variety of ways yet if you look at it",
    "start": "639620",
    "end": "646280"
  },
  {
    "text": "there are situations where the data like concept allows a decoupled storage and",
    "start": "646280",
    "end": "651380"
  },
  {
    "text": "compute but yet both of them leaving and he grows as your data grows your compute",
    "start": "651380",
    "end": "656600"
  },
  {
    "text": "also grows this is the feedback we constantly heard from customers saying",
    "start": "656600",
    "end": "661670"
  },
  {
    "text": "that while the data Lakers are growing the same time the computer is also going either the query performance are getting",
    "start": "661670",
    "end": "667400"
  },
  {
    "text": "slow or their processing performance I'm getting slow or they have to invest grow in a larger cluster so sorry I think I",
    "start": "667400",
    "end": "677000"
  },
  {
    "text": "forgot to change this way but with that we announce today Amazon St select and Amazon glacier",
    "start": "677000",
    "end": "684680"
  },
  {
    "start": "679000",
    "end": "702000"
  },
  {
    "text": "select both of these features allow you to retrieve the needed it the what data",
    "start": "684680",
    "end": "690920"
  },
  {
    "text": "you want of the data your workflow needs based on a simple signal expression from",
    "start": "690920",
    "end": "696290"
  },
  {
    "text": "s3 from the object in s3 or from an archive in glacier the way they work is",
    "start": "696290",
    "end": "703240"
  },
  {
    "start": "702000",
    "end": "799000"
  },
  {
    "text": "the both are available as API they work on a single object or a single archive",
    "start": "703240",
    "end": "710120"
  },
  {
    "text": "for similar to what you do it get what you can say is that from a single object or from a single archive get my data",
    "start": "710120",
    "end": "718360"
  },
  {
    "text": "second because of the way we run it we run when you call this API to say that",
    "start": "718360",
    "end": "723950"
  },
  {
    "text": "give me the data you want we process the data as close as we can to the storage which means that we are avoiding all the",
    "start": "723950",
    "end": "730850"
  },
  {
    "text": "network hop of for example get a data out of s3 back into some compute layer and then processing it we are directly",
    "start": "730850",
    "end": "736790"
  },
  {
    "text": "reading data from the storage which means that we can run this processing faster than what you can do outside of",
    "start": "736790",
    "end": "743030"
  },
  {
    "text": "s3 in every single case not only that by",
    "start": "743030",
    "end": "749000"
  },
  {
    "text": "retrieving less data or by allowing you to use SD select by to retrieve the data you only need from an object what",
    "start": "749000",
    "end": "756200"
  },
  {
    "text": "happens is that first you save money on the computer of course and we look take a look into that how but second part in",
    "start": "756200",
    "end": "762620"
  },
  {
    "text": "certain situations for example like glacier when you're trying to restore the dataset you also save on the tree will cost the less amount of data you",
    "start": "762620",
    "end": "769730"
  },
  {
    "text": "retrieve the more you save on both compute and retrieval cost my colleague",
    "start": "769730",
    "end": "775040"
  },
  {
    "text": "regime who is working with glacier select we will talk about that how does this work with glacier in terms of retrieval for example so that's just a",
    "start": "775040",
    "end": "783530"
  },
  {
    "text": "general introduction I mean I think on how these features work what are the features intended for with that said now",
    "start": "783530",
    "end": "790430"
  },
  {
    "text": "we're going to take a look into each of these features on individual basis so let's start with Amazon s3 select first",
    "start": "790430",
    "end": "797980"
  },
  {
    "text": "so as we mentioned that Amazon ST select works exactly like",
    "start": "797980",
    "end": "804230"
  },
  {
    "start": "799000",
    "end": "907000"
  },
  {
    "text": "a get request if you know a get request is a API very easy using which you can",
    "start": "804230",
    "end": "810170"
  },
  {
    "text": "read your data it's like saying that give me that object or give me the contents of the object so you call it",
    "start": "810170",
    "end": "815690"
  },
  {
    "text": "yet you give it object name and it restreams the entire byte of the objects back to you in a sort of synchronous",
    "start": "815690",
    "end": "820820"
  },
  {
    "text": "response select works exactly in the same manner but the difference being",
    "start": "820820",
    "end": "826160"
  },
  {
    "text": "that is still saying that give me the object what you say is that give me the data from the object which",
    "start": "826160",
    "end": "833540"
  },
  {
    "text": "matches this criteria the criteria what you specify in this case is a sequel",
    "start": "833540",
    "end": "838850"
  },
  {
    "text": "expression and we'll talk about that a bit further in addition we also have a",
    "start": "838850",
    "end": "846100"
  },
  {
    "text": "select available from Java and Python SDK Python SDK because a lot of you guys",
    "start": "846100",
    "end": "852200"
  },
  {
    "text": "like lamda and we will talk about the lamda use case and giy SDK just because a lot of these frameworks for example",
    "start": "852200",
    "end": "858950"
  },
  {
    "text": "press to high MapReduce all they operate on the Java realm and other applications",
    "start": "858950",
    "end": "864320"
  },
  {
    "text": "too so we talked about the criteria so you say get with the criteria and the criteria specified using sequel the",
    "start": "864320",
    "end": "872570"
  },
  {
    "text": "reason we chose sequel is because like many of firsts if anybody who's worked in data processing data analytics sequel",
    "start": "872570",
    "end": "879440"
  },
  {
    "text": "is something we all understand sequel is something we all know so we think there are applications of these features",
    "start": "879440",
    "end": "885620"
  },
  {
    "text": "beyond what we can imagine and I'm fairly sure if you look back in history we never imagined that how I stay will",
    "start": "885620",
    "end": "891140"
  },
  {
    "text": "be used with for example Hadoop kind of use cases for big data processing and we hope that some of these cases were like",
    "start": "891140",
    "end": "896930"
  },
  {
    "text": "something you guys or you as a customer will think about it and let us know that this is what it can enable this we can",
    "start": "896930",
    "end": "902270"
  },
  {
    "text": "help you do so if you take a look back into the sequel the API you specify",
    "start": "902270",
    "end": "911210"
  },
  {
    "start": "907000",
    "end": "1300000"
  },
  {
    "text": "three things the first thing is input civilization what the input selection",
    "start": "911210",
    "end": "916250"
  },
  {
    "text": "does is it helps us pass the object content into sort of four records or",
    "start": "916250",
    "end": "921920"
  },
  {
    "text": "fields you can think of it as a table in a database so based on that if you say that your data is CSV we will take the",
    "start": "921920",
    "end": "928640"
  },
  {
    "text": "CSV pass that into records and fields if you say there is a JSON based on the whatever JSON expression you have",
    "start": "928640",
    "end": "935180"
  },
  {
    "text": "for example saying that's a Lockheed or value we'll pass that into again a tabular format which is recording fields",
    "start": "935180",
    "end": "941170"
  },
  {
    "text": "we do support gzip compression and the ellipses here denote that we are in",
    "start": "941170",
    "end": "948290"
  },
  {
    "text": "preview as of today and the ellipses denote that we are gonna support more features and more compressions as we go",
    "start": "948290",
    "end": "953720"
  },
  {
    "text": "along from now and then you can also specify output specification and the",
    "start": "953720",
    "end": "960290"
  },
  {
    "text": "output specification says that how do you want data back and if you really think about it for a moment what it",
    "start": "960290",
    "end": "966290"
  },
  {
    "text": "allows you to do using the Select API you can provide a single common way to",
    "start": "966290",
    "end": "972140"
  },
  {
    "text": "retrieve data set in a common format irrespective of how you store your data you could store your data in CSV JSON",
    "start": "972140",
    "end": "979820"
  },
  {
    "text": "could be any log format could be parque or c overall yet you can construct then",
    "start": "979820",
    "end": "985610"
  },
  {
    "text": "or you can extract the data in a common format which means every single downstream application from s3 now can",
    "start": "985610",
    "end": "991130"
  },
  {
    "text": "be very efficiently optimized to use that format you don't have to write applications ashrak as many formats for",
    "start": "991130",
    "end": "996530"
  },
  {
    "text": "instance so in between there is sequel so you get the input you apply the",
    "start": "996530",
    "end": "1002800"
  },
  {
    "text": "sequel expression on top of it and you deliver the results back as a byte stream which is the sequel in sequel we",
    "start": "1002800",
    "end": "1010030"
  },
  {
    "text": "support select which is used for projection so imagine that you have a delimited file a CSV so we can say that",
    "start": "1010030",
    "end": "1017160"
  },
  {
    "text": "select column 1 or column 2 not only that you could do things like imagining",
    "start": "1017160",
    "end": "1023470"
  },
  {
    "text": "first column 1 is first name and column second is last name so now I can say select full name which is the",
    "start": "1023470",
    "end": "1029380"
  },
  {
    "text": "concatenation of first name plus last name so I can say that select full name from my object the where clause which is",
    "start": "1029380",
    "end": "1038350"
  },
  {
    "text": "basically used for the predicate so now you can for example if you want the name of the customers from the state",
    "start": "1038350",
    "end": "1044560"
  },
  {
    "text": "California so you can say that select full name from this object where state",
    "start": "1044560",
    "end": "1051610"
  },
  {
    "text": "equal to California the from clause is basically a filler in this case in the",
    "start": "1051610",
    "end": "1058780"
  },
  {
    "text": "sequel expression actually you always write from s3 object and the reason for",
    "start": "1058780",
    "end": "1064300"
  },
  {
    "text": "that is that because the compatibility with the gate API the actual name of the object you pass in with the parameter",
    "start": "1064300",
    "end": "1070900"
  },
  {
    "text": "another parameter called key in the key you actually specify the object name right but within the sequel expression",
    "start": "1070900",
    "end": "1077620"
  },
  {
    "text": "you always say from s3 object now we",
    "start": "1077620",
    "end": "1083200"
  },
  {
    "text": "also do support some fundamental datatypes string boolean decimal integer",
    "start": "1083200",
    "end": "1088900"
  },
  {
    "text": "float and so on in many of these cases for example in CSV there's no concept of",
    "start": "1088900",
    "end": "1096160"
  },
  {
    "text": "a data type you never say that in CSV my field one is an integer or something like that it just stored as a string so",
    "start": "1096160",
    "end": "1103750"
  },
  {
    "text": "by default we think everything is a string in CSV it's not that case with JSON because Jason does have some",
    "start": "1103750",
    "end": "1109929"
  },
  {
    "text": "concept of a data type we do use the same concept as a standard JSON so for",
    "start": "1109929",
    "end": "1115000"
  },
  {
    "text": "example what if you wanted to say that give me the name of the customers where a zip code equal to 9 8 1 1 5 so you",
    "start": "1115000",
    "end": "1122200"
  },
  {
    "text": "cannot say that from the CSV file select full name from CSV or from s3 object",
    "start": "1122200",
    "end": "1128679"
  },
  {
    "text": "where zip code equal to 9 8 1 or 9 because in one hand zip code is assumed",
    "start": "1128679",
    "end": "1134260"
  },
  {
    "text": "to be a string but you're giving an integer value to it to compare but what you could do in that case is you could",
    "start": "1134260",
    "end": "1140350"
  },
  {
    "text": "say select full name from this s3 object where caste zip code as integer equal to",
    "start": "1140350",
    "end": "1147490"
  },
  {
    "text": "9 8 1 0 9 in that case what it means is that where there's a lack of data type everything is assumed to be a string but",
    "start": "1147490",
    "end": "1154690"
  },
  {
    "text": "you can apply the cost function to convert into the appropriate data type from operators perspective we support",
    "start": "1154690",
    "end": "1163650"
  },
  {
    "text": "conditional operators so we talked about some of them already like equal less than greater than not equal and so on",
    "start": "1163650",
    "end": "1169540"
  },
  {
    "text": "and so forth math operators so we can do addition subtraction modulo division multiplication et Cie from logical",
    "start": "1169540",
    "end": "1177490"
  },
  {
    "text": "operators we can combine multiple conditions in the where clause so you can say that we are state equal to",
    "start": "1177490",
    "end": "1183190"
  },
  {
    "text": "California and zip code equal to 9 8 1 1 5 or 9 5 something and or basically the",
    "start": "1183190",
    "end": "1190330"
  },
  {
    "text": "same concept of boolean expressions and or not and within the string you can do substring matching or you can do partial",
    "start": "1190330",
    "end": "1196840"
  },
  {
    "text": "matches for example you can say that select full name there last name begins with a or something of",
    "start": "1196840",
    "end": "1204119"
  },
  {
    "text": "that sort we talk about some of the functions already for example the cost function within a string you can do",
    "start": "1204119",
    "end": "1210389"
  },
  {
    "text": "substring upper/lower and all of that we can also do some map functions for",
    "start": "1210389",
    "end": "1215399"
  },
  {
    "text": "example floor ceiling absolute and stuff and then you can do aggregate such as",
    "start": "1215399",
    "end": "1222029"
  },
  {
    "text": "some count min max EDC so if you guys are I mean all of us I think in this",
    "start": "1222029",
    "end": "1228480"
  },
  {
    "text": "room probably our family of its equal there are some big things you to understand of missing here like group by",
    "start": "1228480",
    "end": "1236360"
  },
  {
    "text": "join what was that yeah count",
    "start": "1236360",
    "end": "1241769"
  },
  {
    "text": "well count is there in aggregate I didn't mention it goodbye and joints and analytical functions like rank window",
    "start": "1241769",
    "end": "1250200"
  },
  {
    "text": "functions and the reason for that is this is not a sequel syntax which is",
    "start": "1250200",
    "end": "1256080"
  },
  {
    "text": "used for analytical queries we are using sequel as a way for one for you to",
    "start": "1256080",
    "end": "1262230"
  },
  {
    "text": "express what data set you want out of an object the intention is very different",
    "start": "1262230",
    "end": "1268019"
  },
  {
    "text": "the intention of its sequel in a teener spectrum is to provide your full spectrum or full wide variety of all the",
    "start": "1268019",
    "end": "1275460"
  },
  {
    "text": "sequel capabilities so you can run your analytical queries but in this case our intention is for you to express what",
    "start": "1275460",
    "end": "1283019"
  },
  {
    "text": "data you want to retrieve out of an object and this is per object this is",
    "start": "1283019",
    "end": "1288269"
  },
  {
    "text": "not across a group of objects it's not across a prefix not across a bucket every request runs on a single object in",
    "start": "1288269",
    "end": "1295889"
  },
  {
    "text": "there so let's think about what we can do with it for example let's say that we",
    "start": "1295889",
    "end": "1302909"
  },
  {
    "text": "have a log file and in that log file I want to find I want to print the first column if the value of both column equal",
    "start": "1302909",
    "end": "1310619"
  },
  {
    "text": "to some value let's say X in general how would we do it I mean we'll just get the object will say get object from s3 and",
    "start": "1310619",
    "end": "1316919"
  },
  {
    "text": "then we'll run some tool which is like all grab or something similar on that to filter the data maybe a script if there",
    "start": "1316919",
    "end": "1324869"
  },
  {
    "text": "are a lot of objects we're probably gonna use a cluster maybe run it using MapReduce streaming MapReduce job maybe",
    "start": "1324869",
    "end": "1331619"
  },
  {
    "text": "run it using spark maybe run it something else but with SD select now you can just convert the same the work which is done",
    "start": "1331619",
    "end": "1339120"
  },
  {
    "text": "by OGG into something a select request so now you can say that select object",
    "start": "1339120",
    "end": "1344640"
  },
  {
    "text": "one where object for value equal to X and even if you have 10,000 requests in",
    "start": "1344640",
    "end": "1350190"
  },
  {
    "text": "this case what you could do is you could run those 10,000 requests just right from your laptop you can run 10,000",
    "start": "1350190",
    "end": "1358500"
  },
  {
    "text": "parallel s3 select requests in parallel which will basically get the result back to you without having the need for the",
    "start": "1358500",
    "end": "1363750"
  },
  {
    "text": "cluster because in this case all you're doing is we're just showing as the Select request versus in the previous",
    "start": "1363750",
    "end": "1370230"
  },
  {
    "text": "case you actually are downloading all the data and doing all the processing so let's just take a quick example or quick",
    "start": "1370230",
    "end": "1377220"
  },
  {
    "text": "demo",
    "start": "1377220",
    "end": "1379490"
  },
  {
    "text": "okay so if you focus on the first part",
    "start": "1383610",
    "end": "1389790"
  },
  {
    "text": "of the demo which is before the with SD select in grey in the first part of the script this is a simple Python script",
    "start": "1389790",
    "end": "1395970"
  },
  {
    "text": "what is doing is it's downloading a file called airport courts dot CSV it's basically a CSV file which has first",
    "start": "1395970",
    "end": "1402300"
  },
  {
    "text": "column as the airport code which is like for example alias second column as the name of the airport like McCarran",
    "start": "1402300",
    "end": "1407520"
  },
  {
    "text": "International followed by latitude longitude and and so on so in the first",
    "start": "1407520",
    "end": "1413340"
  },
  {
    "text": "section what is doing is that it's getting the data CSV the entire data from the CSV object is splitting that",
    "start": "1413340",
    "end": "1419160"
  },
  {
    "text": "object by new line into like lines into records is splitting each record by comma because as a CSV file and then it",
    "start": "1419160",
    "end": "1426480"
  },
  {
    "text": "has basically a data in a format and then is saying that if data 0 equal to LS which is that first field equal to LS",
    "start": "1426480",
    "end": "1432840"
  },
  {
    "text": "then print the name of the airport which is the next field and if you look down",
    "start": "1432840",
    "end": "1438600"
  },
  {
    "text": "below which is the width st select part in that what is doing is that it's saying that now instead of doing all of",
    "start": "1438600",
    "end": "1444420"
  },
  {
    "text": "that work just run this query on that object or on this equal expression and the expression here is basically saying",
    "start": "1444420",
    "end": "1451380"
  },
  {
    "text": "that select object to where object equal to elias so that's the code I'm gonna I",
    "start": "1451380",
    "end": "1458270"
  },
  {
    "text": "already have branch so I'm just going to try to show you that",
    "start": "1458270",
    "end": "1463490"
  },
  {
    "text": "and I'm a little hard to see at the back but I mean at least you could see the code and you probably got the idea so if",
    "start": "1470520",
    "end": "1475830"
  },
  {
    "text": "you look at it in this query it's not the same code which I showed you but in this query what I'm trying to do is print all the first and the second",
    "start": "1475830",
    "end": "1481500"
  },
  {
    "text": "columns without a filter and if you look at it at the bottom it says with st select it took point 32 seconds and",
    "start": "1481500",
    "end": "1488640"
  },
  {
    "text": "without st select it took i think one point eight nine seconds so just by a",
    "start": "1488640",
    "end": "1494670"
  },
  {
    "text": "rough calculation it's almost six times faster so running the same same sort of",
    "start": "1494670",
    "end": "1500850"
  },
  {
    "text": "processing run the same sort of query with s is select just right from a laptop I wasn't able to run this 6 X",
    "start": "1500850",
    "end": "1506910"
  },
  {
    "text": "faster which is almost 500 percent faster let's look at the second part for",
    "start": "1506910",
    "end": "1511950"
  },
  {
    "text": "example and in this case I'm actually",
    "start": "1511950",
    "end": "1518280"
  },
  {
    "text": "running the query or example I showed you which is this one that get the data",
    "start": "1518280",
    "end": "1524190"
  },
  {
    "text": "and print the object print the second column where the first column value equal to Elias so if I run this and now",
    "start": "1524190",
    "end": "1539730"
  },
  {
    "text": "you can see both the queries are both the X both the scripts printed the same value which is McCarran International Airport which is the name of the Elias a",
    "start": "1539730",
    "end": "1545970"
  },
  {
    "text": "report and with St select it ran in point to 7 seconds and without SD select",
    "start": "1545970",
    "end": "1552720"
  },
  {
    "text": "it took over 2 seconds that's almost improvement of 10 times now a question",
    "start": "1552720",
    "end": "1559770"
  },
  {
    "text": "why did it run only 5 to 6 times faster in the previous case why 10 times in the next case because if you think about it",
    "start": "1559770",
    "end": "1567360"
  },
  {
    "text": "in the first case I am retrieving two columns for every single row which means",
    "start": "1567360",
    "end": "1573179"
  },
  {
    "text": "that my file almost I think has four to five columns I don't remember exactly but if you are retrieving 50% of the",
    "start": "1573179",
    "end": "1579840"
  },
  {
    "text": "data in the second query I am retrieving only few very few data",
    "start": "1579840",
    "end": "1584880"
  },
  {
    "text": "hardly a name of the airport I'm retrieving less than a percent of the data which means that the less data you",
    "start": "1584880",
    "end": "1591360"
  },
  {
    "text": "retrieve the more faster it becomes and if you think about the applications of",
    "start": "1591360",
    "end": "1596490"
  },
  {
    "text": "this I'll go back here if you really think",
    "start": "1596490",
    "end": "1603850"
  },
  {
    "text": "about the applications of this one of the most important thing which comes to our mind is lambda several applications",
    "start": "1603850",
    "end": "1610860"
  },
  {
    "text": "why because a server less application a single lambda compute function provides",
    "start": "1610860",
    "end": "1616539"
  },
  {
    "text": "you with limited compute and you wanna optimize that compute to do the most",
    "start": "1616539",
    "end": "1621610"
  },
  {
    "text": "important thing do I really want to use a computer do all of this processing or lifting the data entire object from s3",
    "start": "1621610",
    "end": "1627730"
  },
  {
    "text": "processor transform it and then find what I need what if for example you",
    "start": "1627730",
    "end": "1632890"
  },
  {
    "text": "wanted to create a notification saying that if a count of errors in my log file",
    "start": "1632890",
    "end": "1638380"
  },
  {
    "text": "becomes higher than X then trigger a notification a typical way we will do",
    "start": "1638380",
    "end": "1643600"
  },
  {
    "text": "that with lambda is that will say retrieve the entire object process it split it in two lines do a line count",
    "start": "1643600",
    "end": "1650770"
  },
  {
    "text": "where line has error field or something like that which means that your lambda in certain cases might not even be able",
    "start": "1650770",
    "end": "1657970"
  },
  {
    "text": "to press the entire data then you ever think about how to do distributed lambda how to use some other application but",
    "start": "1657970",
    "end": "1664090"
  },
  {
    "text": "with DES this select you can just return has run a single select request and as we mentioned that we run the same select",
    "start": "1664090",
    "end": "1669669"
  },
  {
    "text": "request within s3 which means it will run faster cheaper and more efficiently from a practical perspective you really",
    "start": "1669669",
    "end": "1676480"
  },
  {
    "text": "think about it some of you guys might have seen this example which is a",
    "start": "1676480",
    "end": "1682480"
  },
  {
    "text": "several s MapReduce using lambda well this is a very engine in case the author",
    "start": "1682480",
    "end": "1687610"
  },
  {
    "text": "who's a colleague of ours what he has done is he's basically created a distributive framework in which you're",
    "start": "1687610",
    "end": "1693580"
  },
  {
    "text": "running map and reduced function as lambda functions and he's coordinating them by writing like they stayed back in",
    "start": "1693580",
    "end": "1699309"
  },
  {
    "text": "s3 and coordinating the work around there so what this function is doing on the left hand side is from the wiki",
    "start": "1699309",
    "end": "1707289"
  },
  {
    "text": "stash data file it's saying that for the IP addresses tell me the amount of revenue I generated so what is doing is",
    "start": "1707289",
    "end": "1713770"
  },
  {
    "text": "reading the entire object from s3 again splitting them by new line again splitting them based on a comma finding",
    "start": "1713770",
    "end": "1720760"
  },
  {
    "text": "the IP address which I think is the first feel and then doing a substring on that which is the last line you can see in red source IP equal to data 0 of 8",
    "start": "1720760",
    "end": "1728380"
  },
  {
    "text": "what it just means is saying that take the first eight characters of that field and then it's doing the all the",
    "start": "1728380",
    "end": "1734600"
  },
  {
    "text": "calculation back of it but on the other side if you look at it all the red part which is the main processing or fade it",
    "start": "1734600",
    "end": "1740450"
  },
  {
    "text": "can be just replaced with SD select query or expression in this case which says that select substring of this",
    "start": "1740450",
    "end": "1746029"
  },
  {
    "text": "object which is the fluent to 8 characters and the second field which is the revenue by just doing this change in",
    "start": "1746029",
    "end": "1753200"
  },
  {
    "text": "the lambda in this server lab server les MapReduce functions we saw that there",
    "start": "1753200",
    "end": "1758990"
  },
  {
    "text": "was an improvement of 2x but the most important part was that the cost went down by 80% which means that this lambda",
    "start": "1758990",
    "end": "1767570"
  },
  {
    "text": "function by just replacing this was able to run 100% faster at 20% of the cost so",
    "start": "1767570",
    "end": "1774080"
  },
  {
    "text": "you were able to save 80% of the cost just by doing the small change but really if you think about it with big",
    "start": "1774080",
    "end": "1781730"
  },
  {
    "text": "data applications this again quantifies or this magnifies we were talking about",
    "start": "1781730",
    "end": "1787610"
  },
  {
    "text": "a small lambda example but think about you're running a query on a large very large data set a petabyte a terabyte",
    "start": "1787610",
    "end": "1792649"
  },
  {
    "text": "data set what happens in that case as you mentioned earlier that hole filtering that hole scanning happens",
    "start": "1792649",
    "end": "1798830"
  },
  {
    "text": "within your application with St select that filtering just becomes a pass-through which means not only the",
    "start": "1798830",
    "end": "1804590"
  },
  {
    "text": "compute footprint the compute required by your application reduces but it becomes simpler easier to operate so if",
    "start": "1804590",
    "end": "1811760"
  },
  {
    "text": "you think about it you can get up to 400 percent faster and up to 80 percent cheaper cost on your compute just by",
    "start": "1811760",
    "end": "1818659"
  },
  {
    "text": "using a stay select for these queries and just to make it easier because many",
    "start": "1818659",
    "end": "1827510"
  },
  {
    "start": "1825000",
    "end": "1911000"
  },
  {
    "text": "of you guys might be using presto I don't know can I see how many of you guys use pressed row lot made this side",
    "start": "1827510",
    "end": "1834470"
  },
  {
    "text": "you guys like sort of con grating and presto site or something no okay and many figures use press to query data",
    "start": "1834470",
    "end": "1841190"
  },
  {
    "text": "from Esther it's a quite popular tool in fact Amazon Athena is based on this Amazon Athena is in Traktor managed",
    "start": "1841190",
    "end": "1846679"
  },
  {
    "text": "presto so what in this case happens is we created a connector so that connector",
    "start": "1846679",
    "end": "1853909"
  },
  {
    "text": "what it does is it works with existing queries it works with existing high meta store which means you don't have to change anything don't have to remove",
    "start": "1853909",
    "end": "1860000"
  },
  {
    "text": "anything it just works as is all you do is just run a presto query once you install the connector you",
    "start": "1860000",
    "end": "1865880"
  },
  {
    "text": "provide a bootstrap fraction for Amazon EMR so folks who are using Amazon EMR they can quickly bootstrap this",
    "start": "1865880",
    "end": "1871460"
  },
  {
    "text": "connector on the Presto cluster and after that you don't have to change anything in a query you don't have to",
    "start": "1871460",
    "end": "1876590"
  },
  {
    "text": "redefine your meta store you don't have to do anything you just run your queries and everything works exactly as this what the connector does is one the",
    "start": "1876590",
    "end": "1883910"
  },
  {
    "text": "Prestel once you run a query Presto generates a plan it looks at the plan figure out what are the predicates and",
    "start": "1883910",
    "end": "1889640"
  },
  {
    "text": "converts them into st select requests and transparently uses as they select for the predicate evaluation so if you",
    "start": "1889640",
    "end": "1897350"
  },
  {
    "text": "quickly look let's quickly look at a demo let me make sure so I am just",
    "start": "1897350",
    "end": "1908630"
  },
  {
    "text": "logging inside and Amazon here mark luster and ok cool so I have this is",
    "start": "1908630",
    "end": "1914300"
  },
  {
    "start": "1911000",
    "end": "1935000"
  },
  {
    "text": "Amazonia my cluster which I've been running and this is already been bootstrap with the Presto the connector",
    "start": "1914300",
    "end": "1919640"
  },
  {
    "text": "we created for presto and the query which I'm gonna run in this case is if I",
    "start": "1919640",
    "end": "1926270"
  },
  {
    "text": "just do a cat this is the query I'm running and for readability let me",
    "start": "1926270",
    "end": "1933800"
  },
  {
    "text": "quickly find this is the query which I'm running now if you actually remember in",
    "start": "1933800",
    "end": "1940310"
  },
  {
    "start": "1935000",
    "end": "1982000"
  },
  {
    "text": "Andy's keynote and he talked about we are running a complex presto query which",
    "start": "1940310",
    "end": "1945470"
  },
  {
    "text": "is caning the largest table over a standard TPC des data set this is the",
    "start": "1945470",
    "end": "1951290"
  },
  {
    "text": "query which is running the largest table in the TPC das data set it's a pretty complicated query it has six sub queries",
    "start": "1951290",
    "end": "1958340"
  },
  {
    "text": "it has aggregation it has very Clause it has all the filtering the reason we",
    "start": "1958340",
    "end": "1964250"
  },
  {
    "text": "chose this query is because this query grants over the largest table yet only",
    "start": "1964250",
    "end": "1969530"
  },
  {
    "text": "uses less than one percent of the data so you can understand that you can see that how this works and what impact does",
    "start": "1969530",
    "end": "1975950"
  },
  {
    "text": "it has to a presto kind of use case or any sort of analytical kind of use case so",
    "start": "1975950",
    "end": "1983290"
  },
  {
    "start": "1982000",
    "end": "2070000"
  },
  {
    "text": "for the guys who cannot see in the back what I'm doing is I'm just running the same query which I showed you but one",
    "start": "1986970",
    "end": "1992610"
  },
  {
    "text": "other thing when you install a presto connector on the Amazon EMR cluster it enables a session property in presto",
    "start": "1992610",
    "end": "2000260"
  },
  {
    "text": "which is basically s3 optimized select enable by setting it to true you're",
    "start": "2000260",
    "end": "2005960"
  },
  {
    "text": "making the connector use as the select by setting it to false you'll make it not users deselect and just use the",
    "start": "2005960",
    "end": "2012890"
  },
  {
    "text": "traditional method in both the cases whether you want to use as its select or not user to select you don't have to",
    "start": "2012890",
    "end": "2019460"
  },
  {
    "text": "change anything you don't have to restart your cluster you don't have to install remove anything all you can do",
    "start": "2019460",
    "end": "2024890"
  },
  {
    "text": "is just set this flag to true or false and accordingly we can change whether it's running with s3 or without st selector so when i run it with testa",
    "start": "2024890",
    "end": "2033350"
  },
  {
    "text": "select what really happens in this case is that the query the plan gets converted into says this alec request",
    "start": "2033350",
    "end": "2039530"
  },
  {
    "text": "and it comes back in somewhere around six seconds on the elapsed time on my laptop which says that it took seconds",
    "start": "2039530",
    "end": "2045170"
  },
  {
    "text": "six some seconds and to end when i run it without st select same query nothing",
    "start": "2045170",
    "end": "2053090"
  },
  {
    "text": "changes exactly the same exactly the same environment exactly the same setup all i've done is change the flag to say",
    "start": "2053090",
    "end": "2060500"
  },
  {
    "text": "that don't use se select and let's",
    "start": "2060500",
    "end": "2066830"
  },
  {
    "text": "quickly go back to the presto console you probably can see that running on",
    "start": "2066830",
    "end": "2072740"
  },
  {
    "start": "2070000",
    "end": "2180000"
  },
  {
    "text": "that side so you can see the first query which is there which took around 6.1 second and the second one is still",
    "start": "2072740",
    "end": "2078408"
  },
  {
    "text": "running because in this case is trying to retrieve all that data set from is from from s3 and then doing all the",
    "start": "2078409",
    "end": "2085158"
  },
  {
    "text": "scanning filtering and processing it does take a little while not that much promise it's less than like 35 seconds",
    "start": "2085159",
    "end": "2092148"
  },
  {
    "text": "or something like that but you get the idea right",
    "start": "2092149",
    "end": "2098450"
  },
  {
    "text": "you should you can use this query or you can use ssl a connector to run query from any scale okay so it came back so",
    "start": "2098450",
    "end": "2105950"
  },
  {
    "text": "on the top you can see it took 34 seconds and the bottom one you can see it took 6.10 seconds but very",
    "start": "2105950",
    "end": "2113270"
  },
  {
    "text": "interesting if you look on the other side some of you guys who know presto",
    "start": "2113270",
    "end": "2118890"
  },
  {
    "text": "on the same row where it says 34 second you can sort of see a icon on the towards the end which says 3.8 five",
    "start": "2118890",
    "end": "2124230"
  },
  {
    "text": "minute and on the bottom you can say 6.80 second and what it means or what",
    "start": "2124230",
    "end": "2129750"
  },
  {
    "text": "press was trying to say in this case that the query without st select consumed three point eight five minutes",
    "start": "2129750",
    "end": "2136109"
  },
  {
    "text": "of the cpu time the query with st select consumed six point eight zero seconds of",
    "start": "2136109",
    "end": "2142740"
  },
  {
    "text": "t cpu time so we went from minutes to seconds look at the memory footprint",
    "start": "2142740",
    "end": "2148079"
  },
  {
    "text": "which is just below that column which says the query without st select took",
    "start": "2148079",
    "end": "2153150"
  },
  {
    "text": "one point almost a terabyte Oh does it say para wait something wrong",
    "start": "2153150",
    "end": "2160890"
  },
  {
    "text": "alright nevermind I will have to look at it again sorry that maybe something",
    "start": "2160890",
    "end": "2169680"
  },
  {
    "text": "is not right here we need to work on that so let me get back to presentation it always happens",
    "start": "2169680",
    "end": "2178349"
  },
  {
    "text": "in them was right and that's why I had this slide pre because I knew that it's",
    "start": "2178349",
    "end": "2184079"
  },
  {
    "start": "2180000",
    "end": "2229000"
  },
  {
    "text": "gonna happen so you can look at that in",
    "start": "2184079",
    "end": "2190349"
  },
  {
    "text": "the other case the memory footprint actually goes down from a terabyte to a",
    "start": "2190349",
    "end": "2195380"
  },
  {
    "text": "to a gigabyte so I'm not sure what's happening they will check on that for sure but I can assure it that's not the",
    "start": "2195380",
    "end": "2201660"
  },
  {
    "text": "case that's not what's gonna happen but you get the idea right so that you were",
    "start": "2201660",
    "end": "2208049"
  },
  {
    "text": "able to run the query four times faster at 1:48 of the CPU requirements roughly right so you can see how it's gonna help",
    "start": "2208049",
    "end": "2214440"
  },
  {
    "text": "you in the same so we talked about lambda we're able to show example it shows you 80% of the cost and here we're",
    "start": "2214440",
    "end": "2220559"
  },
  {
    "text": "talking about example which shows you that it's going to run a 400% faster all these applications can run at that speed",
    "start": "2220559",
    "end": "2226940"
  },
  {
    "text": "so in fact many of you I mean as you asked that how many used presto I think",
    "start": "2226940",
    "end": "2233160"
  },
  {
    "text": "roughly 20% the rest of T of 80% of you use any of one of these tools available or probably something different so our",
    "start": "2233160",
    "end": "2240210"
  },
  {
    "text": "intention is to work so that Amazon Athena Amazon EMR Amazon Rich's spectrum",
    "start": "2240210",
    "end": "2246660"
  },
  {
    "text": "they all support ST select and not only that partner such a cloud data breaks and Haughton",
    "start": "2246660",
    "end": "2252290"
  },
  {
    "text": "they also will support SD select so you can run the same workload from impalas park however many other tools and get",
    "start": "2252290",
    "end": "2258620"
  },
  {
    "text": "the efficiency out of it so just to recap and while I'm recapping",
    "start": "2258620",
    "end": "2263780"
  },
  {
    "start": "2262000",
    "end": "2352000"
  },
  {
    "text": "it you actually guys can go and apply the preview right now if you want to we intend to whitelist users today I start",
    "start": "2263780",
    "end": "2270350"
  },
  {
    "text": "white listing them today so if you think about to be support again to recap we",
    "start": "2270350",
    "end": "2275690"
  },
  {
    "text": "support CSV in JSON format with more to come later we support DZ compression in our preview",
    "start": "2275690",
    "end": "2281720"
  },
  {
    "text": "we do not support encryption at rest but rest assured we'll support encryption during our GA we do support utf-8",
    "start": "2281720",
    "end": "2289370"
  },
  {
    "text": "encoding we have Python Java SDK and presto connector available with more to",
    "start": "2289370",
    "end": "2294770"
  },
  {
    "text": "come again and then we are available in our part of preview in Virginia Ohio",
    "start": "2294770",
    "end": "2300110"
  },
  {
    "text": "Oregon Dublin and Singapore so with that I would like to invite my colleague",
    "start": "2300110",
    "end": "2306380"
  },
  {
    "text": "Russian to talk about glacier select thank you [Applause]",
    "start": "2306380",
    "end": "2319319"
  },
  {
    "text": "I'm gonna first make sure my laptop gets connected and then I'll start okay great",
    "start": "2323600",
    "end": "2343370"
  },
  {
    "text": "so hi everybody my name is Russian Gupta I'm a PM for Amazon t-shirt and I'm here",
    "start": "2343370",
    "end": "2349590"
  },
  {
    "text": "to talk about leisure select so how many of you use glacier today so I see 5 to",
    "start": "2349590",
    "end": "2358080"
  },
  {
    "start": "2352000",
    "end": "2480000"
  },
  {
    "text": "10% so I'm gonna spend a couple of slides on leisure first and then we can talk about what leisure select is so",
    "start": "2358080",
    "end": "2363900"
  },
  {
    "text": "Amazon leisure is an extremely low cost archival storage service but pricing starting from point four cents per GB",
    "start": "2363900",
    "end": "2369540"
  },
  {
    "text": "per month last year at reinvent we introduced two new retrieval tiers which",
    "start": "2369540",
    "end": "2376110"
  },
  {
    "text": "allowed you to not just stow your data in glacier but retrieve it within a few minutes and this was a game changer",
    "start": "2376110",
    "end": "2382860"
  },
  {
    "text": "because if you are traditionally using tapes you know it can take you from days to weeks to retrieve your data but with",
    "start": "2382860",
    "end": "2389310"
  },
  {
    "text": "this with the new retrieval features you can now get your data in case of urgent",
    "start": "2389310",
    "end": "2395130"
  },
  {
    "text": "need in a few minutes and glacier select actually uses it so I'm going to spend some more slides on that in terms of",
    "start": "2395130",
    "end": "2402420"
  },
  {
    "text": "durablity glacier has 11 nines of durability similar to s3 select what",
    "start": "2402420",
    "end": "2407550"
  },
  {
    "text": "that means mathematically is if you have 10,000 files stored in glacier the probability of you losing probability of",
    "start": "2407550",
    "end": "2414660"
  },
  {
    "text": "glacier losing one will is after 10 million years so it's super super low",
    "start": "2414660",
    "end": "2420000"
  },
  {
    "text": "and even if you for those of you again using tape the common paradigm is to use",
    "start": "2420000",
    "end": "2425460"
  },
  {
    "text": "two tapes in separate locations and we actually worked with a customer to see to model out the durability and the best",
    "start": "2425460",
    "end": "2432960"
  },
  {
    "text": "durability you can get the two tapes in separate locations is five nines so glacier offers you five magnitudes of",
    "start": "2432960",
    "end": "2439860"
  },
  {
    "text": "higher durability when you store data with little assured coming to encryption",
    "start": "2439860",
    "end": "2445950"
  },
  {
    "text": "all data is encrypted at resting glacier even if you encrypt your data we will encrypt it again to make sure it's",
    "start": "2445950",
    "end": "2451740"
  },
  {
    "text": "secure and finally if you look at the features glazier offers compliance data",
    "start": "2451740",
    "end": "2456869"
  },
  {
    "text": "management cost management audit logging and all these features make it easy for you to see how storages is being used",
    "start": "2456869",
    "end": "2464430"
  },
  {
    "text": "and also for cost auditing so the key takeaway here is glacier is not just low",
    "start": "2464430",
    "end": "2471240"
  },
  {
    "text": "cost archival storage service but can meet a variety of archival use cases and",
    "start": "2471240",
    "end": "2476670"
  },
  {
    "text": "richer select is adding more to that so if you look at the AWS Storage classes",
    "start": "2476670",
    "end": "2485010"
  },
  {
    "start": "2480000",
    "end": "2554000"
  },
  {
    "text": "we give you a variety of choices starting from Amazon s3 standard which is used for active data to Amazon s3",
    "start": "2485010",
    "end": "2491580"
  },
  {
    "text": "standard infrequent axis which is for more in frequently accessed data and",
    "start": "2491580",
    "end": "2497460"
  },
  {
    "text": "finally glacier which is for archival data one of the common patterns we see from customers is they bring their data",
    "start": "2497460",
    "end": "2502980"
  },
  {
    "text": "to s3 keep it there for a certain period of time weeks months and then as the data",
    "start": "2502980",
    "end": "2508950"
  },
  {
    "text": "is used less and less they move it to across these different storage classes",
    "start": "2508950",
    "end": "2514890"
  },
  {
    "text": "to save on costs so glacier so AWS offers a lifecycle policy management",
    "start": "2514890",
    "end": "2521490"
  },
  {
    "text": "where you can actually define a template of how often you want this to happen automatically so your data comes into s3",
    "start": "2521490",
    "end": "2529560"
  },
  {
    "text": "and you say after a month move it - si a and after maybe six months move it to glacier and all this",
    "start": "2529560",
    "end": "2535410"
  },
  {
    "text": "will happen automatically so the the take over here is the data that's coming",
    "start": "2535410",
    "end": "2542010"
  },
  {
    "text": "to glacier can either come directly if you upload it to glacier or it can come",
    "start": "2542010",
    "end": "2547020"
  },
  {
    "text": "if through s3 through lifecycle policy management so in either of the two cases you're able to use glacier I did talk",
    "start": "2547020",
    "end": "2555599"
  },
  {
    "start": "2554000",
    "end": "2589000"
  },
  {
    "text": "about these retrieval tiers as I mentioned we have three retrieval trees in Glacier expedited standard and bulk",
    "start": "2555599",
    "end": "2562470"
  },
  {
    "text": "so if you have a need of retrieving your data you can choose one of these three tiers and depending on how soon you want",
    "start": "2562470",
    "end": "2569849"
  },
  {
    "text": "it you can get back your data this expedited and bulk were introduced last",
    "start": "2569849",
    "end": "2575730"
  },
  {
    "text": "year and they're all three tiers are quite popular customers love being able",
    "start": "2575730",
    "end": "2580740"
  },
  {
    "text": "to retrieve their data especially if you have an original you can retrieve it in one to five minutes",
    "start": "2580740",
    "end": "2588500"
  },
  {
    "text": "okay now I think that was a good intro of leisure let's talk about glacier select so what is glacier select they",
    "start": "2588940",
    "end": "2595700"
  },
  {
    "start": "2589000",
    "end": "2652000"
  },
  {
    "text": "should select is a new feature that was launched today which allows you to select your relevant contents from a",
    "start": "2595700",
    "end": "2600830"
  },
  {
    "text": "glacier archive rather than restoring the entire object so so similar to s3",
    "start": "2600830",
    "end": "2607070"
  },
  {
    "text": "select we use a sequel expression where you provide a select clause to tell us",
    "start": "2607070",
    "end": "2613310"
  },
  {
    "text": "which columns you want and the where clause which provides the filter filtering condition we also use the same",
    "start": "2613310",
    "end": "2621410"
  },
  {
    "text": "familiar semantics that we have for restore so if you are have been using restoring glacier already play sure",
    "start": "2621410",
    "end": "2628910"
  },
  {
    "text": "select is just extending that API to add more arguments to it to make it pretty simple for you to start using it and I'm",
    "start": "2628910",
    "end": "2635480"
  },
  {
    "text": "going to show that example in a few slides it's also integrated with AWS SDK and CLI one thing to notice there is no",
    "start": "2635480",
    "end": "2642710"
  },
  {
    "text": "console access yet it is supposed to be used programmatically but all the tools",
    "start": "2642710",
    "end": "2648109"
  },
  {
    "text": "you use today for glacier will work with literal select so how do you use glacier",
    "start": "2648109",
    "end": "2654260"
  },
  {
    "start": "2652000",
    "end": "2693000"
  },
  {
    "text": "select like up which API as I mentioned a few slides before there are two ways to load your data into glacier you can",
    "start": "2654260",
    "end": "2660800"
  },
  {
    "text": "either load your glish data directly into glacier using the glacier API and that's shown on the left or you can",
    "start": "2660800",
    "end": "2667220"
  },
  {
    "text": "upload your data to s3 and have lifecycle policy management move it to play sure so and that from that point on",
    "start": "2667220",
    "end": "2674960"
  },
  {
    "text": "if you want to access your data stored in glacier through lifecycle policy management you'll use s3 API so",
    "start": "2674960",
    "end": "2681650"
  },
  {
    "text": "depending on how your data landed into glacier you use either the glacier API or s3 API and glacier select works with",
    "start": "2681650",
    "end": "2688130"
  },
  {
    "text": "both of these api's depending on how you brought in your data so I did mention",
    "start": "2688130",
    "end": "2695900"
  },
  {
    "start": "2693000",
    "end": "2809000"
  },
  {
    "text": "briefly that we've extended the current restore object API argument so if you",
    "start": "2695900",
    "end": "2701990"
  },
  {
    "text": "look at the top row here today if you were to restore an object from glacier",
    "start": "2701990",
    "end": "2707540"
  },
  {
    "text": "you would provide us two arguments you would provide an object ID which you want to restore and you would provide us",
    "start": "2707540",
    "end": "2714830"
  },
  {
    "text": "a tear like I mentioned that here can be bulk standard or expedited but these two",
    "start": "2714830",
    "end": "2720020"
  },
  {
    "text": "are humans we can get the object read the object in Glacier and give it back to",
    "start": "2720020",
    "end": "2726390"
  },
  {
    "text": "you four glaciers select we added three more optional arguments if you do not",
    "start": "2726390",
    "end": "2733260"
  },
  {
    "text": "pro and these are shown at the in the at the bottom in the second row if you do not provide these it's a restore job if",
    "start": "2733260",
    "end": "2740460"
  },
  {
    "text": "you provide these it's a glacier select job the three optional arguments that are needed for glacier select are",
    "start": "2740460",
    "end": "2746790"
  },
  {
    "text": "firstly the sequel query which actually describes what you want to filter the",
    "start": "2746790",
    "end": "2752820"
  },
  {
    "text": "output s3 location which is a location of an s3 bucket that you on and have",
    "start": "2752820",
    "end": "2759030"
  },
  {
    "text": "permissions to and where the output will be written to and finally an SNS topic which you can subscribe so that when a",
    "start": "2759030",
    "end": "2765870"
  },
  {
    "text": "job completes you you are notified the SNS topic and output s3 location are",
    "start": "2765870",
    "end": "2772680"
  },
  {
    "text": "actually quite powerful which are not available in restore today if you didn't have the SNF this topic you have to keep",
    "start": "2772680",
    "end": "2778890"
  },
  {
    "text": "holing to checking if the results are there or not so and that something customers do they have to do with restore but the delicious select you do",
    "start": "2778890",
    "end": "2785640"
  },
  {
    "text": "not have to do it you can just subscribe to the topic and you'll be notified so it saves a lot of cycles on your",
    "start": "2785640",
    "end": "2791190"
  },
  {
    "text": "application also with the output has three location this is now bucket you own and so you it gives you much more",
    "start": "2791190",
    "end": "2797670"
  },
  {
    "text": "control of the output how you want to use it if you want to integrate it with other applications define more fine",
    "start": "2797670",
    "end": "2804000"
  },
  {
    "text": "granulated access control all that is possible so this is an example of how",
    "start": "2804000",
    "end": "2812400"
  },
  {
    "start": "2809000",
    "end": "2879000"
  },
  {
    "text": "the steps you would have to take with and without glacier select so this was similar to the example that my colleague",
    "start": "2812400",
    "end": "2818730"
  },
  {
    "text": "rahul showed if you have a log file where you want to print the first column where the fourth column matches ID for",
    "start": "2818730",
    "end": "2827280"
  },
  {
    "text": "example if you if this was your task and the archive was in glacier if you didn't",
    "start": "2827280",
    "end": "2834060"
  },
  {
    "text": "have they should select you would first have to restore this object into s3 you",
    "start": "2834060",
    "end": "2839070"
  },
  {
    "text": "would then have to call a get request get object in s3 and loaded into like an",
    "start": "2839070",
    "end": "2845160"
  },
  {
    "text": "ec2 class ec2 or any or more cluster or something run it there and and get your results",
    "start": "2845160",
    "end": "2850980"
  },
  {
    "text": "back so it's a lot of steps and a lot of cost you have to pay for restore cost you have to pay for the",
    "start": "2850980",
    "end": "2856110"
  },
  {
    "text": "compute cost now that we have glaciers select you can just call the restore",
    "start": "2856110",
    "end": "2861150"
  },
  {
    "text": "object API and pass in the sequel optional sequel query which will filter the data and it will reduce the amount",
    "start": "2861150",
    "end": "2868110"
  },
  {
    "text": "of data that comes back it's written to s3 and then you can just call get object and get your result so it's much simpler",
    "start": "2868110",
    "end": "2874800"
  },
  {
    "text": "workflow and it's much cheaper this is",
    "start": "2874800",
    "end": "2881280"
  },
  {
    "start": "2879000",
    "end": "2940000"
  },
  {
    "text": "also this tag this slide shows you the flow of messages between your",
    "start": "2881280",
    "end": "2886950"
  },
  {
    "text": "application glacier and s3 so your application can call the glacier select ap ice with an archive ID the sequel",
    "start": "2886950",
    "end": "2895050"
  },
  {
    "text": "query and the tier and also an s3 bucket to write the output once this is",
    "start": "2895050",
    "end": "2901770"
  },
  {
    "text": "received by glacier the first thing glacier does is it checks that you have actually the sequel query is well-formed",
    "start": "2901770",
    "end": "2909080"
  },
  {
    "text": "secondly you actually have permissions to read the data from layer and you also have permissions to write it into the",
    "start": "2909080",
    "end": "2915510"
  },
  {
    "text": "output bucket if any of these failed the query is fails otherwise it sends a 200 ok from that point on depending on the",
    "start": "2915510",
    "end": "2922470"
  },
  {
    "text": "tier you chose glacier will schedule the reading and the filtering and once the",
    "start": "2922470",
    "end": "2930030"
  },
  {
    "text": "job is complete we write the output to s3 and then notify you using SNS from that point on you can read the data from",
    "start": "2930030",
    "end": "2937800"
  },
  {
    "text": "s3 so some glacier select use cases are",
    "start": "2937800",
    "end": "2943950"
  },
  {
    "start": "2940000",
    "end": "3052000"
  },
  {
    "text": "listed here I'm sure there are more but we have a lot of customers in ad Tech and they use their click stream logs to",
    "start": "2943950",
    "end": "2952260"
  },
  {
    "text": "measure to collect you know calculate billing for their customers and what",
    "start": "2952260",
    "end": "2958590"
  },
  {
    "text": "they end up doing is again they have for a few months initially they load their data in s3 as the data gets older the",
    "start": "2958590",
    "end": "2964500"
  },
  {
    "text": "movie to glacier to cut costs so now with with this feature before glacier",
    "start": "2964500",
    "end": "2970110"
  },
  {
    "text": "select if they had like billing and queries they would actually have to restore the entire click stream logs and",
    "start": "2970110",
    "end": "2975300"
  },
  {
    "text": "search for the particular users particular customer but this feature now they can just run a glacier select query",
    "start": "2975300",
    "end": "2981030"
  },
  {
    "text": "get the relevant records and provide the inquiry back to the cust",
    "start": "2981030",
    "end": "2987710"
  },
  {
    "text": "we also have customers in financial space who use glacier a lot we actually",
    "start": "2987710",
    "end": "2993990"
  },
  {
    "text": "have a feature called Walt log which enables them for write only read many which is a requirement in financial",
    "start": "2993990",
    "end": "3000710"
  },
  {
    "text": "regulations and for them if they can now use this feature if they have any audits",
    "start": "3000710",
    "end": "3005930"
  },
  {
    "text": "that they need to perform so they can use glacier select to quickly get the answers if required and finally we have",
    "start": "3005930",
    "end": "3012530"
  },
  {
    "text": "a lot of customers who are using building using s3 and writing their own application to build like training",
    "start": "3012530",
    "end": "3018799"
  },
  {
    "text": "models for machine learning but as these training model get bigger and bigger we",
    "start": "3018799",
    "end": "3023930"
  },
  {
    "text": "see some of them moving this these training models to play sure because you usually run these training models once",
    "start": "3023930",
    "end": "3029270"
  },
  {
    "text": "every three months or once every six months and in this particular scenario glacier select will be quite useful because you don't have to again get the",
    "start": "3029270",
    "end": "3036200"
  },
  {
    "text": "entire set of training models to s3 and run the Savi weightlifting you can get only the relevant set and run it again",
    "start": "3036200",
    "end": "3043670"
  },
  {
    "text": "we expect I'm sure there are more use cases but these are the ones that customers have told us they would they",
    "start": "3043670",
    "end": "3049460"
  },
  {
    "text": "would like to use licious like for going",
    "start": "3049460",
    "end": "3054859"
  },
  {
    "text": "into the features supported the support on the input side we support anything",
    "start": "3054859",
    "end": "3061010"
  },
  {
    "text": "delimited so CSV is TSV psv anything you can provide any kind of a delimiter and",
    "start": "3061010",
    "end": "3067430"
  },
  {
    "text": "on the output side it's also any any delimited supported text the input",
    "start": "3067430",
    "end": "3075109"
  },
  {
    "text": "delimiter can be different than the output and this is again goes back to",
    "start": "3075109",
    "end": "3080119"
  },
  {
    "text": "what Rahul was talking about living the derelict dream because you can have different formats on the input and have",
    "start": "3080119",
    "end": "3085369"
  },
  {
    "text": "the same output format when you're reading data in terms of encryption we",
    "start": "3085369",
    "end": "3092089"
  },
  {
    "text": "support as a server-side encryption specifically as the C kms and SSE s 3 and then sequel philosophy supports",
    "start": "3092089",
    "end": "3099290"
  },
  {
    "text": "select for selecting the columns from you always write from archive in glacier",
    "start": "3099290",
    "end": "3105500"
  },
  {
    "text": "and the archive refers to the archive ID you've specified I'll show you that in an example and the where is the",
    "start": "3105500",
    "end": "3112280"
  },
  {
    "text": "filtering conditions data types pretty we support pretty much all the fundamental data types default is string",
    "start": "3112280",
    "end": "3120859"
  },
  {
    "text": "operator we support conditional math logical and string operators and you can also cast",
    "start": "3120859",
    "end": "3126910"
  },
  {
    "text": "we use provide functions like cast in string to do sequel to provide cast",
    "start": "3126910",
    "end": "3134690"
  },
  {
    "text": "functions I would want to call her again similar to a three select we do not support join run support group by",
    "start": "3134690",
    "end": "3140000"
  },
  {
    "text": "because the goal of this feature is to do filtering and provide you relevant data not it's not a replacement for",
    "start": "3140000",
    "end": "3145660"
  },
  {
    "text": "analytical engines so at this point I'm going to do a quick demo and hopefully",
    "start": "3145660",
    "end": "3155600"
  },
  {
    "start": "3149000",
    "end": "3354000"
  },
  {
    "text": "it works little scared from the for okay",
    "start": "3155600",
    "end": "3162680"
  },
  {
    "text": "so what I want to show and I'm going to first run it because this is glacier it takes a few minutes it's not instant but",
    "start": "3162680",
    "end": "3169610"
  },
  {
    "text": "what I'm so that I'm running and I'll explain to you what I'm running so this is my input data okay it's not showing",
    "start": "3169610",
    "end": "3176360"
  },
  {
    "text": "up sorry so what I did is I ran the query here first just just so we have a",
    "start": "3176360",
    "end": "3183050"
  },
  {
    "text": "head start on that and this is my input data it's an air puts it's just the same",
    "start": "3183050",
    "end": "3190220"
  },
  {
    "text": "data said that Rahul was showing you which is a list of all the airports all the airports in a region how many",
    "start": "3190220",
    "end": "3196820"
  },
  {
    "text": "airports are there I initially thought Las Vegas only had one but apparently they're more and let's use glacier to",
    "start": "3196820",
    "end": "3202190"
  },
  {
    "text": "like to find out how many so this this archive has or file has 52,000 records",
    "start": "3202190",
    "end": "3208640"
  },
  {
    "text": "right so they're 52,000 records this is stored in glacier what I want to do is run a query in glacier and say give me",
    "start": "3208640",
    "end": "3215870"
  },
  {
    "text": "all the airports or helipads or or what-have-you in the Las Vegas region so",
    "start": "3215870",
    "end": "3221480"
  },
  {
    "text": "I this this is what I did is I used D I wrote a quick Python script which is",
    "start": "3221480",
    "end": "3228280"
  },
  {
    "text": "extending the restore restore our API to use glacier select so what I define here",
    "start": "3228280",
    "end": "3234800"
  },
  {
    "text": "is I say the type is select I provide the archive ID which is the archive ID",
    "start": "3234800",
    "end": "3239810"
  },
  {
    "text": "of the actual object stored in glacier I said here is expedited expedite it again",
    "start": "3239810",
    "end": "3245210"
  },
  {
    "text": "which should respond in under five minutes I can also do standard in bulk and then I say the inputs utilization is",
    "start": "3245210",
    "end": "3252770"
  },
  {
    "text": "CSV I provide my sequel plus a secret expression here if you",
    "start": "3252770",
    "end": "3258080"
  },
  {
    "text": "notice it says select star from archive and this this will always be archived but it's referring to this archived ID",
    "start": "3258080",
    "end": "3263320"
  },
  {
    "text": "where municipality is Las Vegas I was able to give this column name header",
    "start": "3263320",
    "end": "3269210"
  },
  {
    "text": "name because I used another field here which says file header info use what this is telling glacier is to use the",
    "start": "3269210",
    "end": "3274970"
  },
  {
    "text": "first row as the name of the header if I did not have it I would actually have to say underscore column number like",
    "start": "3274970",
    "end": "3281390"
  },
  {
    "text": "underscore 10 and then the output serialization I say a CSV it could be",
    "start": "3281390",
    "end": "3286700"
  },
  {
    "text": "any other TSV PSV I also provide an output location this is a bucket my",
    "start": "3286700",
    "end": "3292850"
  },
  {
    "text": "pocket and the prefix so I ran this when I run this I get a job ID you can see",
    "start": "3292850",
    "end": "3299330"
  },
  {
    "text": "it's easy Rho J let's just remember those three and now what I'm going to do is I'm going to go to my s3 bucket which",
    "start": "3299330",
    "end": "3307850"
  },
  {
    "text": "I provided in the job and look for this folder ez Rho J and it it's there and I",
    "start": "3307850",
    "end": "3314210"
  },
  {
    "text": "see that this job started at 7:53 so about three minutes ago and I already have a results folder if I go to the",
    "start": "3314210",
    "end": "3320240"
  },
  {
    "text": "results folder and I download it let me do that so within three minutes this job",
    "start": "3320240",
    "end": "3326240"
  },
  {
    "text": "completed which is good and I'm going to",
    "start": "3326240",
    "end": "3332390"
  },
  {
    "text": "open it and see the results so let's see how many airports are there are does",
    "start": "3332390",
    "end": "3337820"
  },
  {
    "text": "anybody want to take a guess while I'm opening this twenty-three so if you",
    "start": "3337820",
    "end": "3344750"
  },
  {
    "text": "cannot get a flight out tomorrow or Friday you have 22 other options so I",
    "start": "3344750",
    "end": "3349960"
  },
  {
    "text": "can share this data set with you if you'd like to know where to go okay so",
    "start": "3349960",
    "end": "3355330"
  },
  {
    "start": "3354000",
    "end": "3433000"
  },
  {
    "text": "with that let's talk about pricing the way we price this is we have we charge",
    "start": "3355330",
    "end": "3362510"
  },
  {
    "text": "you on data scanned on data return and a request cost so what this means is when you send a glacier select request you",
    "start": "3362510",
    "end": "3369260"
  },
  {
    "text": "are charged for the total amount of data right scanned and then the amount of data that was returned to you so if you",
    "start": "3369260",
    "end": "3376640"
  },
  {
    "text": "think about it if you had hypothetically if you had a one gigabyte object and you",
    "start": "3376640",
    "end": "3382760"
  },
  {
    "text": "only returned one megabyte if you were to restore it that one gigabyte would cost you three cents in expedited",
    "start": "3382760",
    "end": "3389830"
  },
  {
    "text": "and then you would have to pay for an ec2 cluster what have you to run the",
    "start": "3389830",
    "end": "3395320"
  },
  {
    "text": "analysis with glaciers select if you do the same thing we will charge you two cents for the scanning and since you're",
    "start": "3395320",
    "end": "3403090"
  },
  {
    "text": "only returning one megabyte it's minimal cost so essentially already using glacier",
    "start": "3403090",
    "end": "3409210"
  },
  {
    "text": "select is cheaper than restoring the entire object so it's a 33% savings right there",
    "start": "3409210",
    "end": "3414580"
  },
  {
    "text": "in spite of us doing the compute for you in addition like like Rahul mentioned you can save up to 80% in compute costs",
    "start": "3414580",
    "end": "3422950"
  },
  {
    "text": "but you don't have to do any more so it's it's a lot of savings for you so not only it saves you steps it's a lot",
    "start": "3422950",
    "end": "3428380"
  },
  {
    "text": "of convenience it's cheap so that's so that's how it's priced another",
    "start": "3428380",
    "end": "3434500"
  },
  {
    "start": "3433000",
    "end": "3451000"
  },
  {
    "text": "announcement is in 2018 Amazon Athena will integrate with laser select and with this you will be able to query",
    "start": "3434500",
    "end": "3440410"
  },
  {
    "text": "click a Malaysia directly glacier data directly in Athena you will also be able",
    "start": "3440410",
    "end": "3445690"
  },
  {
    "text": "to use plasure as a data source in the sequel queries that you can write an Athena so so glacier select is in GA",
    "start": "3445690",
    "end": "3455680"
  },
  {
    "start": "3451000",
    "end": "3476000"
  },
  {
    "text": "starting today we support for all comas all comma separated any delimited",
    "start": "3455680",
    "end": "3461860"
  },
  {
    "text": "limiter separated files we support SSE encryption utf-8 encoding and we work",
    "start": "3461860",
    "end": "3468280"
  },
  {
    "text": "with SDK and CLI attina will come next year it's available in all commercial regions where glacier exists today and",
    "start": "3468280",
    "end": "3476010"
  },
  {
    "start": "3476000",
    "end": "3515000"
  },
  {
    "text": "with this I want to go back to Andy's keynote announcement this morning AWS is",
    "start": "3476010",
    "end": "3483130"
  },
  {
    "text": "making your data Lake simpler faster and cheaper with the announcements today of",
    "start": "3483130",
    "end": "3488560"
  },
  {
    "text": "Amazon glacier select and s3 select you can use that you'll be able to use them with a lot more engines and applications",
    "start": "3488560",
    "end": "3495070"
  },
  {
    "text": "you can already use a Tina redshift spectrum EMR and lambda the test 3 but",
    "start": "3495070",
    "end": "3500110"
  },
  {
    "text": "the test 3 select it just becomes faster and cheaper we're also seeing more and more of these engines work with we'll be",
    "start": "3500110",
    "end": "3506500"
  },
  {
    "text": "integrating with less your select over time you'll also see third-party ice fees and custom applications which will",
    "start": "3506500",
    "end": "3512170"
  },
  {
    "text": "build to leverage these features these there are two sessions that already",
    "start": "3512170",
    "end": "3518770"
  },
  {
    "start": "3515000",
    "end": "3600000"
  },
  {
    "text": "happened this afternoon no but you might find it more useful if you you can go back and check on you tube or",
    "start": "3518770",
    "end": "3524080"
  },
  {
    "text": "and I I can I don't know if there was I think there might be more sessions we can follow up they both are tomorrow at",
    "start": "3524080",
    "end": "3531340"
  },
  {
    "text": "the time mention they both are tomorrow these two okay yeah okay at this point",
    "start": "3531340",
    "end": "3538240"
  },
  {
    "text": "any questions phase three selector glacier select yeah yeah there we go",
    "start": "3538240",
    "end": "3548200"
  },
  {
    "text": "just wanted to validate no support for JSON and glacier glacier selected that's",
    "start": "3548200",
    "end": "3553660"
  },
  {
    "text": "correct that's high on our list but not right now so two questions the first of",
    "start": "3553660",
    "end": "3560740"
  },
  {
    "text": "the first one was about JSON data so do you support her article queries in any way what was that like do you support",
    "start": "3560740",
    "end": "3567700"
  },
  {
    "text": "queries on here article data like arrays or nested records things like that no I",
    "start": "3567700",
    "end": "3572800"
  },
  {
    "text": "at this moment I think we support only the first level of JSON what you could",
    "start": "3572800",
    "end": "3578110"
  },
  {
    "text": "there is a way you could sort of like project a nested scheme out of JSON and",
    "start": "3578110",
    "end": "3583180"
  },
  {
    "text": "then run first of all queries on that I think our API specification which is public let's talk about how you can do",
    "start": "3583180",
    "end": "3589960"
  },
  {
    "text": "that right and is there any plan to support like Avro or presto in the future oh yes yeah we do have plans to",
    "start": "3589960",
    "end": "3597040"
  },
  {
    "text": "support every arcane or see if you that's a request feel free to send your feedback to us and we can definitely work on that okay and also you have I",
    "start": "3597040",
    "end": "3605170"
  },
  {
    "text": "saw that you read compression so gzip I assume snappy probably at some point in the future hold compression more for",
    "start": "3605170",
    "end": "3612310"
  },
  {
    "text": "more compression codecs and more formats definitely yes so so that's on the input",
    "start": "3612310",
    "end": "3617410"
  },
  {
    "text": "but then I guess the data that you're sending me is already uncompressed are you ever going to support compressing the data when you send it to me yes we",
    "start": "3617410",
    "end": "3624850"
  },
  {
    "text": "are I mean and that's the reason why we didn't preview because you wanna get the feedback from you and what's more important what's more requiring for your",
    "start": "3624850",
    "end": "3630790"
  },
  {
    "text": "customer use cases so we can collect that and build that add support and this is just a start that you'll see more more elevation on these features thanks",
    "start": "3630790",
    "end": "3637990"
  },
  {
    "text": "a lot thank you Hey so I get that it's still in preview but you kind of gave",
    "start": "3637990",
    "end": "3643480"
  },
  {
    "text": "some hints and so what's coming up yeah so for the current EMR support just to",
    "start": "3643480",
    "end": "3649330"
  },
  {
    "text": "clarify it it's just presto in preview yesterday just just presto and when you say there's upcoming",
    "start": "3649330",
    "end": "3657779"
  },
  {
    "text": "support from other distributions does that imply like MapReduce tez Impala",
    "start": "3657779",
    "end": "3663089"
  },
  {
    "text": "spark what does that mean that the other distribution will support it it's sort",
    "start": "3663089",
    "end": "3668699"
  },
  {
    "text": "of a question which we I don't think I can answer right now but you can definitely follow up on that",
    "start": "3668699",
    "end": "3674029"
  },
  {
    "text": "what we would think that the support will be limited to the most of the medical queries and other applications",
    "start": "3674029",
    "end": "3679650"
  },
  {
    "text": "also but that's what we would think that we can definitely follow up and confirm on that okay and then the the the",
    "start": "3679650",
    "end": "3686249"
  },
  {
    "text": "current presto connector support partition tables already yes okay yeah it was it partition table in fact can",
    "start": "3686249",
    "end": "3692789"
  },
  {
    "text": "run a query which is for example an on separate format let's say today like Parque and one other one is a CSV or",
    "start": "3692789",
    "end": "3698369"
  },
  {
    "text": "JSON you can run a joint across that partition on partition doesn't matter so yeah thank you sure and by the way just a like if you",
    "start": "3698369",
    "end": "3707579"
  },
  {
    "text": "apply for preview you also will get opportunity to pass participate in a private forum and you'll get access to the feedback email where you can send",
    "start": "3707579",
    "end": "3713819"
  },
  {
    "text": "all these requests of any other thing you have so alright so building on the presto query yeah",
    "start": "3713819",
    "end": "3719999"
  },
  {
    "text": "we have presto up and running in production today so how does this s3",
    "start": "3719999",
    "end": "3725190"
  },
  {
    "text": "select automatically get into our production what you'll need to do for that is you will need to take the",
    "start": "3725190",
    "end": "3730709"
  },
  {
    "text": "connector what we have which is our level as a part of the preview only to install that kind of connector I put the",
    "start": "3730709",
    "end": "3736440"
  },
  {
    "text": "connector on the restroom so if you what if you know what pressure it works on a plug-in interface should we just me",
    "start": "3736440",
    "end": "3741930"
  },
  {
    "text": "install a new hive connector plug-in and then after that you can just get the same thing so I think we're gonna take",
    "start": "3741930",
    "end": "3748709"
  },
  {
    "text": "questions right here at the podium because we have to allow two mics and stuff",
    "start": "3748709",
    "end": "3754849"
  }
]