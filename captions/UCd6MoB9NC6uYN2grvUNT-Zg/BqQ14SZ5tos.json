[
  {
    "text": "- Hi, folks. My name's Emily Webber. I'm a Machine Learning Specialist\nat Amazon Web Services. And today, we're gonna\nlearn about Hugging Face",
    "start": "610",
    "end": "7350"
  },
  {
    "text": "on SageMaker. This is your deep dive.",
    "start": "7350",
    "end": "9803"
  },
  {
    "text": "And this is the second part\nof a three video series. Our first video is about an introduction",
    "start": "13730",
    "end": "19970"
  },
  {
    "text": "to the Hugging Face transformer's SDK and about all the ways\nto use it on SageMaker. In this video, we are gonna\ndeep dive on training.",
    "start": "19970",
    "end": "27130"
  },
  {
    "text": "We're gonna learn all about distributing across multiple GPUs,\nmultiple server nodes. We're gonna look at\nSageMaker training compiler.",
    "start": "27130",
    "end": "33640"
  },
  {
    "text": "We're gonna understand\nthe hyper parameters and the model selection that is most optimal\nfor different use cases",
    "start": "33640",
    "end": "39329"
  },
  {
    "text": "you're interested in. And then the third video is about hosting. So let's get started.",
    "start": "39330",
    "end": "44183"
  },
  {
    "text": "All right, so there are three modes of Hugging Face models on SageMaker.",
    "start": "46130",
    "end": "52120"
  },
  {
    "text": "You can fine tune your model, you can scale your model, and you can train from scratch.",
    "start": "52120",
    "end": "57850"
  },
  {
    "text": "And you can do all of those using this Hugging Face\ndeep learning container. So you remember,",
    "start": "57850",
    "end": "63440"
  },
  {
    "text": "we learned about this in that first video where essentially, again, there's a pre-built\ndeep learning container",
    "start": "63440",
    "end": "69890"
  },
  {
    "text": "that is available to SageMaker customers, and that uses the Hugging Face base images",
    "start": "69890",
    "end": "76580"
  },
  {
    "text": "and the base content. Now, we're gonna pass in a\nhandful of hyper parameters to that container.",
    "start": "76580",
    "end": "83329"
  },
  {
    "text": "Now, the hyper parameters are\ngenerally pretty consistent. You wanna pass in the name of the model",
    "start": "83330",
    "end": "88600"
  },
  {
    "text": "that you're interested in. You need to pass in the\ndirectory for that model.",
    "start": "88600",
    "end": "93810"
  },
  {
    "text": "Now, remember, in SageMaker training, we pass a lot through that opt ML. That's actually an indicator for the path",
    "start": "93810",
    "end": "100530"
  },
  {
    "text": "within the training container itself. You'll see this do train. Parameter pro tip,",
    "start": "100530",
    "end": "106240"
  },
  {
    "text": "this is how you tell your\nmodel to train or not. If you don't have it here, odds are it's not gonna be training.",
    "start": "106240",
    "end": "112820"
  },
  {
    "text": "And then the dataset name. So the dataset name is especially relevant",
    "start": "112820",
    "end": "118130"
  },
  {
    "text": "when you are using a dataset\nthat's coming from the hub. So remember, the Hugging Face\nhub has pre-trained models,",
    "start": "118130",
    "end": "125400"
  },
  {
    "text": "has tokenizers, and has data sets, all of which you can easily download. If you are using one of those",
    "start": "125400",
    "end": "131910"
  },
  {
    "text": "and you pass it here as a hyper parameter, the SageMaker training\ncontainer will download it",
    "start": "131910",
    "end": "137740"
  },
  {
    "text": "from the hub on your\nbehalf during training. And so it executes again\nduring the training config.",
    "start": "137740",
    "end": "144470"
  },
  {
    "text": "When you are bringing your own data. So say, you're bringing your own support ticket classification data,",
    "start": "144470",
    "end": "150379"
  },
  {
    "text": "or your own customer search data, or your own documentation data, all of that you can bring.",
    "start": "150380",
    "end": "156530"
  },
  {
    "text": "And then again, fine tune that base model. Next, we have this very handy\nparameter called a git config.",
    "start": "156530",
    "end": "164730"
  },
  {
    "text": "So this git config parameter is pointing to the\nHugging Face repository. So again, it's pointing to\nthe Hugging Face transformers",
    "start": "164730",
    "end": "174300"
  },
  {
    "text": "GitHub repository\ndirectly, and the branch. So you know the version\nof the transformer's SDK",
    "start": "174300",
    "end": "181460"
  },
  {
    "text": "that we're using, and you'll see 4.6.1 pretty commonly. So again, we're pointing\nto that base git config.",
    "start": "181460",
    "end": "190150"
  },
  {
    "text": "And then in my estimator, I'm pointing to the name of a script",
    "start": "191160",
    "end": "196690"
  },
  {
    "text": "that is sitting in GitHub. I do not have the script locally. The script is not sitting in a local file.",
    "start": "196690",
    "end": "203980"
  },
  {
    "text": "It's sitting on GitHub. And then in the source directory file, our parameter, I'm\npassing in the directory.",
    "start": "203980",
    "end": "212784"
  },
  {
    "text": "So the path of this\nscript in that repository. And this does not just\nwork with Hugging Face.",
    "start": "212784",
    "end": "218290"
  },
  {
    "text": "This works with any git that\nSageMaker has access to. And so you can just pass\nin git configs to train",
    "start": "218290",
    "end": "226370"
  },
  {
    "text": "rather than strictly training scripts. But in any case, we'll do this with Hugging Face right now.",
    "start": "226370",
    "end": "231840"
  },
  {
    "text": "Next, I'm using this is a p3.16xl, so this is a healthy machine\nwith eight NVIDIA GPUs.",
    "start": "231840",
    "end": "239610"
  },
  {
    "text": "So definitely good for scaling. We're using one machine here. Got my SageMaker role,\nconfig, different versions.",
    "start": "239610",
    "end": "246810"
  },
  {
    "text": "And then again, I call model dot fit. And so we're gonna fit this model. And again, we're gonna\nturn on this new instance",
    "start": "246810",
    "end": "253730"
  },
  {
    "text": "in order to download that data set. If you are thinking,",
    "start": "253730",
    "end": "258829"
  },
  {
    "text": "well, that sounds like a lot of work. How do I get that code? You can generate the code\nconfigs for SageMaker",
    "start": "258830",
    "end": "266290"
  },
  {
    "text": "on Hugging Face. So in this little snippet here, I'm going onto the\nHugging Face model site.",
    "start": "266290",
    "end": "271790"
  },
  {
    "text": "And then again, I picked\ndistill (indistinct) GPT-2. And then you're selecting the task. So token classification,\ntext classification,",
    "start": "271790",
    "end": "279610"
  },
  {
    "text": "whichever you are looking for. And then you just determine if you're training from\nyour local machines.",
    "start": "279610",
    "end": "285130"
  },
  {
    "text": "So that means basically\nbeing on your laptop or even on studio like in local\nmode for notebook instances.",
    "start": "285130",
    "end": "292197"
  },
  {
    "text": "And so here's another repeat\non the same thing, right? So you go to the model, you pick train, you pick SageMaker,",
    "start": "292197",
    "end": "299660"
  },
  {
    "text": "and then you pick the task\nthat you're most interested in. It really doesn't matter, whichever one works best for you.",
    "start": "299660",
    "end": "305600"
  },
  {
    "text": "And then the config for that. And so pick AWS and then\njust go ahead and hit copy.",
    "start": "305600",
    "end": "312720"
  },
  {
    "text": "And then when you hit copy again, you can just paste that directly\nto your studio notebook.",
    "start": "312720",
    "end": "320453"
  },
  {
    "text": "And so another SageMaker\nfeature you should know about is the training compiler.",
    "start": "321360",
    "end": "326660"
  },
  {
    "text": "So we launched training\ncompiler at re:Invent last year. That was 2021. And the training compiler",
    "start": "326660",
    "end": "333099"
  },
  {
    "text": "is gonna compile your Hugging Face models. So for supported NLP models\nwithin the Hugging Face family,",
    "start": "333100",
    "end": "340090"
  },
  {
    "text": "we have about 50 models\nthat are supported to date. And essentially, when you\nuse the base trainer API,",
    "start": "340090",
    "end": "348319"
  },
  {
    "text": "so when you're using that\ntrainer API from Hugging Face, there is zero code changes required. The only code change that we ask",
    "start": "348320",
    "end": "354960"
  },
  {
    "text": "is that you pass in this\ntraining compiler config. So essentially, in your estimator,",
    "start": "354960",
    "end": "361700"
  },
  {
    "text": "pass in this training compiler config, and we will take your model and we will literally make\nit smaller essentially.",
    "start": "361700",
    "end": "367800"
  },
  {
    "text": "There are a number of\ndifferent compilation methods, and essentially what is gonna happen",
    "start": "367800",
    "end": "373229"
  },
  {
    "text": "is that we're looking at\nthe profile of your model and then we're placing\ndifferent pieces of your model",
    "start": "373230",
    "end": "378290"
  },
  {
    "text": "on your hardware in a\nmore optimized fashion, just to utilize less and less memory",
    "start": "378290",
    "end": "384200"
  },
  {
    "text": "to give you more memory\nfor larger batch sizes and faster training. So SageMaker training compiler",
    "start": "384200",
    "end": "389620"
  },
  {
    "text": "delivers up to 50%\nfaster overall training. We can also distribute\nour Hugging Face models",
    "start": "389620",
    "end": "397330"
  },
  {
    "text": "across many, many, many, many, many GPUs. So it's a really exciting\ntime where we see,",
    "start": "397330",
    "end": "404170"
  },
  {
    "text": "obviously, there's so many\nNLP startups out there, and there's so many models that are running on thousands of GPUs,",
    "start": "404170",
    "end": "411009"
  },
  {
    "text": "tens of thousands of GPUs. You'll see very aggressive\ntraining schedules. And so we recently completed a task",
    "start": "411010",
    "end": "419270"
  },
  {
    "text": "with a 175 billion parameter\nNLP model on SageMaker. We scaled this on 125 machines.",
    "start": "419270",
    "end": "427170"
  },
  {
    "text": "We saw 32 samples per second, so we were able to deliver that\nfor 175 billion parameters.",
    "start": "427170",
    "end": "433280"
  },
  {
    "text": "And we projected if we scaled\nthat up to 240 P4 instances, that would take 25 days to train.",
    "start": "433280",
    "end": "440750"
  },
  {
    "text": "And the good news is\nthat distributing this with Hugging Face and\nSageMaker is really easy.",
    "start": "440750",
    "end": "446310"
  },
  {
    "text": "So on the right hand side, we'll look at, this time we're using a PyTorch container.",
    "start": "446310",
    "end": "451449"
  },
  {
    "text": "So based on the different versions, you may use PyTorch container or you may use the Hugging Face container.",
    "start": "451450",
    "end": "458285"
  },
  {
    "text": "At this point in time, I\nneed the PyTorch containers. So we've got the PyTorch container loaded. Here, I'm gonna train GTPJ.",
    "start": "458285",
    "end": "464919"
  },
  {
    "text": "So I have a GPTJ script using\nSageMaker model parallel. And we're gonna run\nthat on a training job.",
    "start": "464920",
    "end": "473900"
  },
  {
    "text": "And so that's the estimator view. So not much change. But then in the SageMaker distributed,",
    "start": "473900",
    "end": "480210"
  },
  {
    "text": "you'll see I have quite\na few hyper parameters. So let's try and unpack that. So with model parallel,",
    "start": "480210",
    "end": "486000"
  },
  {
    "text": "so model parallel means\nI'm gonna take my model and I'm gonna break it up, literally splitting my\nmodel in half or in thirds.",
    "start": "486000",
    "end": "493550"
  },
  {
    "text": "I'm cutting it up into pieces and then I'm placing\ndifferent pieces of that model on different GPUs.",
    "start": "493550",
    "end": "500030"
  },
  {
    "text": "So one overall model\nthat's using multiple GPUs and that's good for both single servers",
    "start": "500030",
    "end": "506230"
  },
  {
    "text": "and multiple servers. So if I have one p3.16xl,\nthat's eight GPUs.",
    "start": "506230",
    "end": "512320"
  },
  {
    "text": "If I have two, that's 16 GPUs. What I would do is set partitions to 16.",
    "start": "513070",
    "end": "520979"
  },
  {
    "text": "And then what's happening\nis SageMaker model parallel, we're gonna profile again\nthe algorithm that you bring,",
    "start": "520980",
    "end": "526930"
  },
  {
    "text": "in this case, GTPJ. We're gonna break it\nup into multiple parts and place those parts literally\non the different devices.",
    "start": "526930",
    "end": "534540"
  },
  {
    "text": "So that is model parallel. That is a sister product to\nwhat's called data parallel,",
    "start": "534540",
    "end": "541510"
  },
  {
    "text": "SageMaker data parallel, which actually makes copies of your model. So data parallel is useful\nwhen you have small models.",
    "start": "541510",
    "end": "550209"
  },
  {
    "text": "If you have models that are typically 1.5\nbillion parameters or under,",
    "start": "550210",
    "end": "556240"
  },
  {
    "text": "data parallel is an appropriate approach where you can scale out\ncopies of that model onto multiple GPUs and multiple server",
    "start": "556240",
    "end": "564222"
  },
  {
    "text": "in your overall training\njob versus, again,",
    "start": "564222",
    "end": "569222"
  },
  {
    "text": "SageMaker model parallel, we're just doing that at the GPU level. So both of those are easy\nto set up with Hugging Face",
    "start": "569310",
    "end": "578220"
  },
  {
    "text": "because Hugging Face actually built in their base trainer API, they built an integration\nto SageMaker model parallel",
    "start": "578440",
    "end": "585250"
  },
  {
    "text": "and data parallel. So commonly, you'll just set\nmodel parallel as enabled,",
    "start": "585250",
    "end": "590320"
  },
  {
    "text": "and then they'll pick\nit up from the back end. And with that, let's\ntake a look at a demo.",
    "start": "590320",
    "end": "595203"
  },
  {
    "text": "All right, so here we are. So we're still in SageMaker Studio. We're sitting here in (indistinct).",
    "start": "597202",
    "end": "603220"
  },
  {
    "text": "The notebook I am using\nis a public notebook. So this notebook is coming directly",
    "start": "603220",
    "end": "608360"
  },
  {
    "text": "from the SageMaker examples. And we'll just show that out here.",
    "start": "608360",
    "end": "613600"
  },
  {
    "text": "So again, we're using\nthe SageMaker examples. And then we'll cruise down here\nand we'll click on training.",
    "start": "613600",
    "end": "620160"
  },
  {
    "text": "So that training folder,\ndistributed training, PyTorch, model parallel, GPT-2.",
    "start": "622000",
    "end": "631020"
  },
  {
    "text": "So that's what I'm running right now. And let's just zoom in here. Actually, it's pretty good.",
    "start": "631130",
    "end": "637990"
  },
  {
    "text": "So, great. Let's check this out. So in this notebook, we're gonna do similar\nsteps as you've seen before.",
    "start": "637990",
    "end": "645390"
  },
  {
    "text": "We're installing SageMaker, SageMaker experiments right here. And then we're getting the config set up.",
    "start": "645390",
    "end": "651860"
  },
  {
    "text": "So we've got our boto3\nclient, our session, our region, SageMaker\nsession, default bucket,",
    "start": "651860",
    "end": "660090"
  },
  {
    "text": "all the normal pieces. And then we're gonna point to S3. So this notebook,",
    "start": "660090",
    "end": "664963"
  },
  {
    "text": "we have two modes for\nrunning this notebook. One mode of running this notebook is on a very small sample data set,",
    "start": "666170",
    "end": "673120"
  },
  {
    "text": "which is not intended to train\nwhat we call to convergence. So it's not gonna train your entire model.",
    "start": "673120",
    "end": "679420"
  },
  {
    "text": "It's just to show you that things work. You can indeed distribute your model. You can indeed get this piece to run.",
    "start": "679420",
    "end": "687410"
  },
  {
    "text": "But if you wanna run it to\nconvergence essentially, you'll switch out some of these parameters",
    "start": "687410",
    "end": "692620"
  },
  {
    "text": "and then actually run a script",
    "start": "692620",
    "end": "696230"
  },
  {
    "text": "to download a very sizeable data set and do some local processing, and then land that in S3.",
    "start": "698951",
    "end": "705060"
  },
  {
    "text": "So that's the other option. And also, you can point to FSx. So remember, FSx is an\noptimized file store",
    "start": "705060",
    "end": "712640"
  },
  {
    "text": "for AWS generally. FSx for Lustre. In SageMaker, we can\ntrain on FSx actually.",
    "start": "712640",
    "end": "718380"
  },
  {
    "text": "So when you're working\nwith larger data sets, really anything beyond 50 gigabytes,",
    "start": "718380",
    "end": "725540"
  },
  {
    "text": "I would look at trying to put that into a SageMaker FSx for\nLustre rather file store.",
    "start": "725540",
    "end": "731430"
  },
  {
    "text": "That's because two reasons. Number one, the data download\nis just so much faster.",
    "start": "731430",
    "end": "737420"
  },
  {
    "text": "Like part of your job wait time actually is just waiting for the\ndata set to be downloaded",
    "start": "737420",
    "end": "742570"
  },
  {
    "text": "from S3 to your SageMaker\ntraining cluster. So when you use FSx for Lustre, that wait time is gone because\nyou have this file store.",
    "start": "742570",
    "end": "751360"
  },
  {
    "text": "So there's an offline creation where you create the FSx\nfor Lustre file store off of what's sitting in S3.",
    "start": "751360",
    "end": "757940"
  },
  {
    "text": "And so in this case,\nwe're not gonna do that, but you can again create\nthat FSx for Lustre.",
    "start": "757940",
    "end": "764230"
  },
  {
    "text": "And then your training\nis just gonna be faster because essentially, it's just a much better\niterative experience.",
    "start": "764230",
    "end": "770110"
  },
  {
    "text": "Like, your instances come on and they just attach to\nyour FSx for Lustre volume rather than waiting for\nthat copy in from S3.",
    "start": "770110",
    "end": "776860"
  },
  {
    "text": "So that's an option. Yeah, there we go. Great. All right.",
    "start": "776860",
    "end": "782339"
  },
  {
    "text": "So now, let's take a look\nat these hyper parameters. So there are a lot of\nhyper parameters here.",
    "start": "782340",
    "end": "788060"
  },
  {
    "text": "What's going on? So actually, if you cruise down, you can see the model config, which helps fix it a\nlittle bit more clearly.",
    "start": "788060",
    "end": "795840"
  },
  {
    "text": "So there are three different\nmodes for running this. So GPT-2 small, GPT-2 30\nbillion parameters, and then xl.",
    "start": "795840",
    "end": "803550"
  },
  {
    "text": "So these three different model configs. For each of those model configs,",
    "start": "806270",
    "end": "811370"
  },
  {
    "text": "we strongly recommend\nchanging the hyper parameters. And so as you are working on this at home,",
    "start": "811370",
    "end": "817030"
  },
  {
    "text": "as you have your own Hugging\nFace text classifier, or translation algorithm,",
    "start": "817030",
    "end": "823580"
  },
  {
    "text": "or question answering solution\nthat you're working on, as you scale up that model,",
    "start": "823580",
    "end": "828810"
  },
  {
    "text": "I need you to change\nthe parameters actually. I need you to change out the\noverall size of that model",
    "start": "828810",
    "end": "835490"
  },
  {
    "text": "and how you are defining that using SageMaker distributed frameworks.",
    "start": "835490",
    "end": "841889"
  },
  {
    "text": "So tensor parallel, pipeline parallel, all of those, you need to modify those",
    "start": "841890",
    "end": "847140"
  },
  {
    "text": "as you increase the size\nof that training job.",
    "start": "847140",
    "end": "851293"
  },
  {
    "text": "And we have notes about doing\nthat in our documentation, so fear not. It's all very well documented. But so now that we know",
    "start": "853140",
    "end": "858810"
  },
  {
    "text": "we've got three different models here. So these hyper parameters\nare consistent as a base",
    "start": "858810",
    "end": "864079"
  },
  {
    "text": "for all of them. So essentially, the logging frequency,",
    "start": "864080",
    "end": "869570"
  },
  {
    "text": "how frequently we wanna\ndo our check pointing, some of our learning rate specifications,",
    "start": "869570",
    "end": "874730"
  },
  {
    "text": "total number of steps, using\nWikipedia data, et cetera. And then the MPI options",
    "start": "874730",
    "end": "881930"
  },
  {
    "text": "are specific to SageMaker model parallel. They essentially optimize\nit more efficiently.",
    "start": "881930",
    "end": "887829"
  },
  {
    "text": "And then great, yeah,\nso our model configs. So again, 30 billion, which is very large,",
    "start": "887830",
    "end": "894740"
  },
  {
    "text": "like the 30 billion is not a small model. If you were doing this, I would say, and again, it's great\nthat we start with this.",
    "start": "894740",
    "end": "901480"
  },
  {
    "text": "So start with GPT-2 small. Then increase to GPT-2 xl.",
    "start": "901480",
    "end": "908480"
  },
  {
    "text": "And again, that's 1.5 billion. And if and only if that seems promising, so after you've proven out\nsome value at GPT-2 xl scales,",
    "start": "908480",
    "end": "918080"
  },
  {
    "text": "then and only then increase your size. And honestly, that's a pretty big jump going from 1.5 to 30.",
    "start": "918630",
    "end": "925509"
  },
  {
    "text": "Like I might consider 1.5 to\nmaybe 13 billion parameters",
    "start": "925510",
    "end": "930510"
  },
  {
    "text": "is a more common sort of middle step. So definitely increase\nthe size of your generator",
    "start": "930920",
    "end": "936922"
  },
  {
    "text": "as you get fruit from your experiments, as they're shown value.",
    "start": "936922",
    "end": "942540"
  },
  {
    "text": "So that's that. So we're gonna set up studio, just create this experiment.",
    "start": "942540",
    "end": "949420"
  },
  {
    "text": "And then here we go. Yeah, so this is one p3.16xl.",
    "start": "949420",
    "end": "954699"
  },
  {
    "text": "And so you can increase again,\nthe number of instances. And then the volume size, in this case,",
    "start": "954700",
    "end": "961990"
  },
  {
    "text": "does need to be pretty sizable. And then we'll create this estimator. So here we go.",
    "start": "961990",
    "end": "969020"
  },
  {
    "text": "So in this case, I am\nusing a local script. So in that first video,\nwe pointed to GitHub.",
    "start": "969020",
    "end": "976017"
  },
  {
    "text": "In this video, I have a local script and we'll look at that\nin just a little bit. So I've got my local script here.",
    "start": "976017",
    "end": "981839"
  },
  {
    "text": "I've got my p3.16xl right here. The size of my volume. So that's the size of the EBS volume",
    "start": "981840",
    "end": "989579"
  },
  {
    "text": "on my SageMaker training container. The number of them. And then here are all those\ndistribution parameters.",
    "start": "989580",
    "end": "997320"
  },
  {
    "text": "So if you're using model parallel, you're gonna need to have MPI enabled.",
    "start": "997320",
    "end": "1003120"
  },
  {
    "text": "Processes per host, et cetera. Yeah, okay, tensor parallel. So tenor parallel is an addition",
    "start": "1003120",
    "end": "1011180"
  },
  {
    "text": "to model parallel essentially. So remember, model\nparallel takes your model, then breaks it up across GPUs.",
    "start": "1011180",
    "end": "1017459"
  },
  {
    "text": "Tensor parallel takes specific\nlayers within that model and breaks those up and\nplaces them on multiple GPUs.",
    "start": "1017460",
    "end": "1024569"
  },
  {
    "text": "So if you have one or even a few extremely large\ntensors within your model,",
    "start": "1024570",
    "end": "1030890"
  },
  {
    "text": "tensor parallel will break those up and place them more efficiently. So that's your tensor parallel degree.",
    "start": "1030890",
    "end": "1037470"
  },
  {
    "text": "Then we have partitions,\noptimizer state, et cetera. Those are all defined here.",
    "start": "1037470",
    "end": "1042730"
  },
  {
    "text": "And again, with the hyper parameters, the most common thing to do, number one,",
    "start": "1042730",
    "end": "1048930"
  },
  {
    "text": "is to look at the research\npaper for that model and just reuse the same hyper parameters.",
    "start": "1048930",
    "end": "1054460"
  },
  {
    "text": "Like, maybe do some small modifications to get it more optimized\nfor your training job and your data set.",
    "start": "1054460",
    "end": "1060380"
  },
  {
    "text": "But on average, most people just reuse\nthe hyper parameters from the research data\nset and from the paper,",
    "start": "1060380",
    "end": "1066450"
  },
  {
    "text": "and also use our documentation. Like heavily, heavily use\nthe SageMaker documentation",
    "start": "1066450",
    "end": "1073580"
  },
  {
    "text": "for distributed training. It is excellent. They've really, they've done an excellent\njob on our documentation",
    "start": "1073580",
    "end": "1080650"
  },
  {
    "text": "for distributed training. So look at that, spend\ntime thinking about it, cut tickets there if\nyou have any questions.",
    "start": "1080650",
    "end": "1086670"
  },
  {
    "text": "It's really, really useful. So in our estimator here, so again, we're specifying\nour framework version,",
    "start": "1086670",
    "end": "1094179"
  },
  {
    "text": "the model of Python or the\nversion of Python rather, our checkpoints, and onward ho.",
    "start": "1094180",
    "end": "1101740"
  },
  {
    "text": "So then we're gonna call model.fit. So we call model.fit.",
    "start": "1101740",
    "end": "1106723"
  },
  {
    "text": "And then this should look\npretty similar, right? We're installing the requirements and then we're downloading\nall of our SDKs.",
    "start": "1107850",
    "end": "1116850"
  },
  {
    "text": "Things look normal. Got our p3.16xl in there. Then reprint on the hyper parameters.",
    "start": "1120190",
    "end": "1126803"
  },
  {
    "text": "And I'm gonna try and zoom down 'cause I wanna show you the ranks.",
    "start": "1127640",
    "end": "1134610"
  },
  {
    "text": "So the ranks are your GPUs actually. They're the different devices",
    "start": "1134610",
    "end": "1141120"
  },
  {
    "text": "that are available in your clusters. So remember, we have one p3.16xl,",
    "start": "1141120",
    "end": "1146970"
  },
  {
    "text": "which means I have a\nsum total of eight GPUs. And so if I start at zero,\nI'll make it up to seven.",
    "start": "1146970",
    "end": "1153640"
  },
  {
    "text": "So those are my eight GPUs. And so what model parallel is gonna do,",
    "start": "1153640",
    "end": "1160600"
  },
  {
    "text": "remember, it's taking that GPT-2 and this is a small GPT-2, but we're gonna take it",
    "start": "1160600",
    "end": "1166590"
  },
  {
    "text": "and we're gonna break\nit onto the partition. So I have 12 layers\nassigned to partition zero,",
    "start": "1166590",
    "end": "1172913"
  },
  {
    "text": "and then essentially\npieces are gonna execute on different ranks.",
    "start": "1174400",
    "end": "1180550"
  },
  {
    "text": "So there's my partition zero. And we're just gonna zoom ahead here.",
    "start": "1180550",
    "end": "1186770"
  },
  {
    "text": "Okay. And so we've got some action happening. So that's our loss, that's\nour speed per batch.",
    "start": "1186770",
    "end": "1194683"
  },
  {
    "text": "Here we go. And then we're storing the ranks again. We're storing the gradients",
    "start": "1197770",
    "end": "1204240"
  },
  {
    "text": "and the values for each\nof those different ranks. So again, SageMaker model parallel is managing that distribution for you.",
    "start": "1204240",
    "end": "1213240"
  },
  {
    "text": "And how does it do that, you ask? Let's check it out. So we're gonna go into that\ntrain GPT simple script,",
    "start": "1213240",
    "end": "1220940"
  },
  {
    "text": "which is sitting right out here. And let's just see if we can\nunpack this a little bit. So remember, this script\nis available on GitHub.",
    "start": "1220940",
    "end": "1226970"
  },
  {
    "text": "It's just in the SageMaker examples. Anyone and anyone can access this, but let's see if we can unpack this.",
    "start": "1226970",
    "end": "1232570"
  },
  {
    "text": "And we're gonna start at the bottom since that is far and\naway the easiest thing. Okay.",
    "start": "1232570",
    "end": "1238213"
  },
  {
    "text": "So we've got a main function. Let's see if we can\nfind the main function. So the main function is,",
    "start": "1238213",
    "end": "1245653"
  },
  {
    "text": "should be, yeah, here we go right here. So our main function. So we're gonna start with\nparsing those arguments.",
    "start": "1247320",
    "end": "1253720"
  },
  {
    "text": "So that's nothing unusual. And then that SageMaker model\nparallel config is right here.",
    "start": "1253720",
    "end": "1260760"
  },
  {
    "text": "So that's using distributed data parallel, which is right here. And then basically just applying",
    "start": "1261140",
    "end": "1267180"
  },
  {
    "text": "those hyper parameters, remember. So everything you loaded in through ARGs, the micro batches.",
    "start": "1267180",
    "end": "1272930"
  },
  {
    "text": "So the micro batches are splitting. So one batch is a part of your epoch.",
    "start": "1272930",
    "end": "1279570"
  },
  {
    "text": "So one part of your overall data set that's passed onto the model.",
    "start": "1279570",
    "end": "1285680"
  },
  {
    "text": "A micro batch is a split\nin your batch actually. So a micro batch is\ntaking your overall batch",
    "start": "1285680",
    "end": "1292059"
  },
  {
    "text": "and then sharding that out\nto your different ranks. So in this case,",
    "start": "1292060",
    "end": "1297450"
  },
  {
    "text": "the number of micro batches is basically your overall batch size divided by the number of\ndevices that you have.",
    "start": "1297450",
    "end": "1304910"
  },
  {
    "text": "So your batch size divided by eight for an eight device machine.",
    "start": "1304910",
    "end": "1309423"
  },
  {
    "text": "Checkpoint activations, a couple other new SageMaker\nmodel parallel features,",
    "start": "1310260",
    "end": "1314773"
  },
  {
    "text": "and onward. So we're gonna initialize SageMaker model parallel\nright here, smp.omit.",
    "start": "1315690",
    "end": "1323430"
  },
  {
    "text": "And then if we're on the main rank, make sure that's where we do our saving.",
    "start": "1323520",
    "end": "1327970"
  },
  {
    "text": "Yeah. If local rank is this guy. And then we're gonna load\nin that GPT-2 config.",
    "start": "1329380",
    "end": "1335070"
  },
  {
    "text": "Where does this come, you ask? It comes from Hugging Face. Hugging Face has a GPT-2 config,",
    "start": "1335070",
    "end": "1342230"
  },
  {
    "text": "so we're using the Hugging Face SDK. And if you want proof, let's\ngo look for that GPT-2.",
    "start": "1342230",
    "end": "1349150"
  },
  {
    "text": "Oh, perfect. That's what I was already searching for. So where does that come in from?",
    "start": "1349150",
    "end": "1354850"
  },
  {
    "text": "From transformers, remember? So transformers is that Hugging Face SDK.",
    "start": "1354850",
    "end": "1360450"
  },
  {
    "text": "And we're importing the auto model for causal language modeling.",
    "start": "1360450",
    "end": "1366140"
  },
  {
    "text": "That's the objective\nthat we use for GPT-2. It uses the previous tokens\nto generate the future tokens.",
    "start": "1366140",
    "end": "1372169"
  },
  {
    "text": "That's why they call it causal versus mass language modeling, which is bidirectional. And that's what you've\nseen for (indistinct).",
    "start": "1372170",
    "end": "1378363"
  },
  {
    "text": "And so in any case, we've got our GP2 config set there.",
    "start": "1378363",
    "end": "1382573"
  },
  {
    "text": "And so in the script,\nso we load our config. So here, they're hard\ncoding in the vocab size.",
    "start": "1383520",
    "end": "1390860"
  },
  {
    "text": "So protip to modify this,\nto parameterize that guy. But in any case,",
    "start": "1390860",
    "end": "1396720"
  },
  {
    "text": "that's the vocab size that they're using. And then, yeah.",
    "start": "1396720",
    "end": "1401740"
  },
  {
    "text": "So then we're gonna pass\nin our different arguments for the context with number of positions, number of embeddings, number of layers.",
    "start": "1401740",
    "end": "1409020"
  },
  {
    "text": "So again, we're actually\nchanging the architecture of this model based on the parameters.",
    "start": "1409020",
    "end": "1414523"
  },
  {
    "text": "The activation function\nis defined right here, the embeddings, the attention, et cetera.",
    "start": "1415520",
    "end": "1423250"
  },
  {
    "text": "Yeah. So that's our GPT-2 config is right there. And then we're gonna load that in.",
    "start": "1423270",
    "end": "1431683"
  },
  {
    "text": "So set, yeah, okay. So then we're just gonna set our seed. We're gonna turn on tensor\nparallel right here,",
    "start": "1433820",
    "end": "1440710"
  },
  {
    "text": "so with smp dot tensor parallel. Get that turned on.",
    "start": "1440710",
    "end": "1445730"
  },
  {
    "text": "And then again, if it's the rank, just print out the total parameters.",
    "start": "1445730",
    "end": "1451049"
  },
  {
    "text": "We're gonna set our device right here. So you'll see this quite a lot",
    "start": "1451050",
    "end": "1456880"
  },
  {
    "text": "in the SageMaker model parallel framework, which is you take your model and then you're gonna load\nthat as an argument to SMP.",
    "start": "1456880",
    "end": "1465570"
  },
  {
    "text": "So again, SMP is the\nSageMaker model parallel SDK. And so we're gonna load\nthat base GPT-2 model",
    "start": "1465570",
    "end": "1473860"
  },
  {
    "text": "into this framework here. And then we'll go a\nlittle bit further down.",
    "start": "1473860",
    "end": "1480002"
  },
  {
    "text": "And so we'll keep determining, so we're setting our optimizer here,",
    "start": "1481390",
    "end": "1488639"
  },
  {
    "text": "turning on activation checkpointing, all of which again,\nbeautifully documented.",
    "start": "1488640",
    "end": "1493013"
  },
  {
    "text": "And then we've got our\noptimizer determined here, and then we're gonna load\nthe optimizer as well",
    "start": "1493960",
    "end": "1499770"
  },
  {
    "text": "into that model parallel. So again, SMP dot distributed optimizer will load that optimizer in here",
    "start": "1499770",
    "end": "1506700"
  },
  {
    "text": "to find our learning rate schedule. And then we'll load those in.",
    "start": "1506700",
    "end": "1512132"
  },
  {
    "text": "So we've got our function,\nload model, and optimizer, and then essentially, we're gonna define all these\npieces and then call train.",
    "start": "1513030",
    "end": "1521169"
  },
  {
    "text": "So then there's this train function. And then, that train function\nincludes the logging.",
    "start": "1523160",
    "end": "1530530"
  },
  {
    "text": "So we get to see all of the action that's happening per step. And then once that has completed,",
    "start": "1530530",
    "end": "1538400"
  },
  {
    "text": "we'll save the final model\nagain just on that first rank.",
    "start": "1538400",
    "end": "1542533"
  },
  {
    "text": "And we'll save that and that's our script. And so that is what's running again",
    "start": "1543610",
    "end": "1550020"
  },
  {
    "text": "on the SageMaker training clusters. And if you wanna see it\nin the console again,",
    "start": "1550020",
    "end": "1558500"
  },
  {
    "text": "that's gonna be right out here. So we'll look at our\nSageMaker training jobs. This is my SMP GPT-2 small, small model.",
    "start": "1558500",
    "end": "1567530"
  },
  {
    "text": "And again, that's on a p316.xl. And I can see my training image.",
    "start": "1567610",
    "end": "1573320"
  },
  {
    "text": "I can see my training data sets. So there's that train S3 path",
    "start": "1573320",
    "end": "1578370"
  },
  {
    "text": "and then my test S3 path right out here. My model finished and\nthen was loaded into S3.",
    "start": "1578370",
    "end": "1586430"
  },
  {
    "text": "And I can view my hyper parameters. Remember, those are all stored by default.",
    "start": "1586758",
    "end": "1593260"
  },
  {
    "text": "And then my monitoring, again, this a very small job that we ran.",
    "start": "1593260",
    "end": "1599242"
  },
  {
    "text": "And you know what, let's\nactually just look at the logs. So when you're\ntroubleshooting on SageMaker",
    "start": "1600330",
    "end": "1605990"
  },
  {
    "text": "and when you're trying to figure out, you know, why did my\ntraining job do X, Y, Z? Definitely, a really helpful step",
    "start": "1605990",
    "end": "1611480"
  },
  {
    "text": "is just to open up CloudWatch. So from that log readout or from the metadata on your training job,",
    "start": "1611480",
    "end": "1619420"
  },
  {
    "text": "you can click into CloudWatch. And then, you can see the action.",
    "start": "1619420",
    "end": "1626970"
  },
  {
    "text": "Basically, we can look at the\nlogs for that specific job.",
    "start": "1626970",
    "end": "1631970"
  },
  {
    "text": "And so we can load again, all the logs from the\nbeginning of the job.",
    "start": "1634010",
    "end": "1639240"
  },
  {
    "text": "And then you'll see we're\ninstalling the requirements here. We're downloading them.",
    "start": "1639240",
    "end": "1645010"
  },
  {
    "text": "We are again running the\nenvironment variables and just printing out\neverything that's happening",
    "start": "1645010",
    "end": "1650350"
  },
  {
    "text": "while that model is training. And that's your train demo.",
    "start": "1650350",
    "end": "1656730"
  },
  {
    "text": "All right. And so just to close this\nout with some pro tips. So the first pro tip",
    "start": "1656730",
    "end": "1662600"
  },
  {
    "text": "is to absolutely test your model locally. It is so efficient to take\na sample of your data set,",
    "start": "1662600",
    "end": "1670440"
  },
  {
    "text": "take 10% of your data set,\ntake 15% of your data set, definitely a smaller batch",
    "start": "1670440",
    "end": "1676070"
  },
  {
    "text": "and make sure that you can\nget it to work locally. Whether you're doing that on Studio,",
    "start": "1676070",
    "end": "1681510"
  },
  {
    "text": "or if you're doing that\non a notebook instance, or if you're doing it on your laptop, do it wherever it's most\nefficient for you to do this,",
    "start": "1681510",
    "end": "1687659"
  },
  {
    "text": "but definitely test it locally. Make sure that the model config works, that you can load it,",
    "start": "1687660",
    "end": "1693550"
  },
  {
    "text": "that you have the right tokenizer, that you have some of\nthe hyper parameters set.",
    "start": "1693550",
    "end": "1698990"
  },
  {
    "text": "Run inferencing locally. Remember in that first video, we looked at fine tuning the model",
    "start": "1698990",
    "end": "1704610"
  },
  {
    "text": "and then running inference locally. You can do that, can and should do that\nagain before you scale.",
    "start": "1704610",
    "end": "1710480"
  },
  {
    "text": "So before you even turn on\na SageMaker training job, I really want you to\njust test out that model",
    "start": "1710480",
    "end": "1715779"
  },
  {
    "text": "just on your laptop or on your notebook. I'm gonna say this one, so I'm gonna say this a million times,",
    "start": "1715780",
    "end": "1722170"
  },
  {
    "text": "crawl before you walk and\nthen walk before you run. It is so easy to turn on too many machines",
    "start": "1722170",
    "end": "1728970"
  },
  {
    "text": "and then just leave them running. And we do not want to do that. What we want to happen\nis we wanna start again",
    "start": "1728970",
    "end": "1735130"
  },
  {
    "text": "with success on our notebooks, then move into a small training job, move into a p3.16xl,",
    "start": "1735130",
    "end": "1742420"
  },
  {
    "text": "just one of those, just one epochs. Once that works, move\nit to maybe 30 epochs,",
    "start": "1742420",
    "end": "1749370"
  },
  {
    "text": "maybe 40 epochs. Maybe add in SageMaker training compiler to improve the performance.",
    "start": "1749370",
    "end": "1754940"
  },
  {
    "text": "Once that is operational, add your second instance. Move out to two instances.",
    "start": "1754940",
    "end": "1760890"
  },
  {
    "text": "So now, you have 16 devices to play with. So you can change some\nof your hyper parameters,",
    "start": "1760890",
    "end": "1766610"
  },
  {
    "text": "change to the tensor parallel degree if you have a large model. Maybe you add in more models\nbecause you have more devices.",
    "start": "1766610",
    "end": "1772800"
  },
  {
    "text": "And then once that works\nat a two node scale, then maybe you scale out to eight,",
    "start": "1772800",
    "end": "1777980"
  },
  {
    "text": "and then you go to 16,\nand then you go to 32, and then you go to 64, and then you can really have some fun",
    "start": "1777980",
    "end": "1784450"
  },
  {
    "text": "with quite a few machines on the cloud, but remember, just start small",
    "start": "1784450",
    "end": "1790540"
  },
  {
    "text": "and just prove success at every scale. And the last pro tip is\nget your volume discounts.",
    "start": "1790540",
    "end": "1796700"
  },
  {
    "text": "Like, definitely every cloud\nprovider has volume discounts. Absolutely make sure",
    "start": "1796700",
    "end": "1802770"
  },
  {
    "text": "that when you look at your\noverall demand schedule of what types of training\njobs you wanna run,",
    "start": "1802770",
    "end": "1809460"
  },
  {
    "text": "what types of GPUs you wanna utilize, make sure that you are talking\nto your AWS account team,",
    "start": "1809460",
    "end": "1815879"
  },
  {
    "text": "your account manager,\nyour solution architect, your technical account\nmanager, your specialists,",
    "start": "1815880",
    "end": "1821200"
  },
  {
    "text": "and ask them for\nsuggestions on saving money. Ask them to help you use spot instances,",
    "start": "1821200",
    "end": "1829433"
  },
  {
    "text": "which actually is not ideal\nfor really large scale training just because of capacity. So it's preferred to use spot\nfor smaller scale testing,",
    "start": "1830610",
    "end": "1839040"
  },
  {
    "text": "but in any case, make sure you're very cognizant about getting all the\nsavings that you can.",
    "start": "1839040",
    "end": "1845320"
  },
  {
    "text": "Definitely, that's that's my pro tip. And so with that, I hope you enjoyed your\ntraining deep dive.",
    "start": "1845320",
    "end": "1852260"
  },
  {
    "text": "Check out our next video, which is a deep dive on\nhosting with Hugging Face. Thanks.",
    "start": "1852260",
    "end": "1856613"
  }
]