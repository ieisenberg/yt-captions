[
  {
    "start": "0",
    "end": "48000"
  },
  {
    "text": "wonderful well welcome back thank you for joining me again thank you for everybody who's joined online my name is",
    "start": "0",
    "end": "6870"
  },
  {
    "text": "Ian Robinson I'm a specialist Solutions Architect for data analytics at AWS this",
    "start": "6870",
    "end": "12570"
  },
  {
    "text": "is the second session I've been running today so many of you may have joined for the previous session either here on-site or online where we were looking at",
    "start": "12570",
    "end": "19170"
  },
  {
    "text": "service analytics with Athena glue and quick site and this session we're going",
    "start": "19170",
    "end": "25470"
  },
  {
    "text": "to dive in more detail into AWS glue which is our managed data catalog and",
    "start": "25470",
    "end": "32160"
  },
  {
    "text": "ETL service so any questions perhaps questions that you had in that first",
    "start": "32160",
    "end": "37920"
  },
  {
    "text": "session around glue please feel free to ask them now or over the course of this session but hopefully we'll get into a",
    "start": "37920",
    "end": "45780"
  },
  {
    "text": "bit more detail all right so what is gluten well glue is our managed data catalog",
    "start": "45780",
    "end": "53340"
  },
  {
    "start": "48000",
    "end": "48000"
  },
  {
    "text": "and managed ETL service the goal here is effectively to take away the burden of",
    "start": "53340",
    "end": "59879"
  },
  {
    "text": "your having to do a lot of the undifferentiated heavy lifting around ETL around cataloging your data and",
    "start": "59879",
    "end": "66270"
  },
  {
    "text": "transforming your data we want to give you the ability to be able to discover your data and transform your data and to",
    "start": "66270",
    "end": "72180"
  },
  {
    "text": "focus on those transformations focus on the sources and the targets and the kinds of logic that you want to employ",
    "start": "72180",
    "end": "78210"
  },
  {
    "text": "in order to convert or transform your data we don't want you to have to spend time worrying about standing up",
    "start": "78210",
    "end": "84060"
  },
  {
    "text": "infrastructure provisioning infrastructure or procuring it configuring it monitoring it and so on",
    "start": "84060",
    "end": "89430"
  },
  {
    "text": "that's something that we can do very easily very happening on your behalf so",
    "start": "89430",
    "end": "95060"
  },
  {
    "start": "94000",
    "end": "94000"
  },
  {
    "text": "glue has three main components three core capabilities data catalog job",
    "start": "95119",
    "end": "102540"
  },
  {
    "text": "authoring and job execution so the data catalog is a highly available hive meta",
    "start": "102540",
    "end": "109470"
  },
  {
    "text": "store compatible that metadata repository right so it allows you to",
    "start": "109470",
    "end": "115259"
  },
  {
    "text": "capture metadata describing all your sources of data within AWS so as I say",
    "start": "115259",
    "end": "121469"
  },
  {
    "text": "it's hive meta store compatible but we've added some additional features such as search such as the ability to be",
    "start": "121469",
    "end": "127259"
  },
  {
    "text": "able to tag data sets and the ability to be able to provision and running crawlers that can actually",
    "start": "127259",
    "end": "132660"
  },
  {
    "text": "go searching for sources of data within AWS so that's one core capability and",
    "start": "132660",
    "end": "139230"
  },
  {
    "text": "you can use that data catalog even if you don't use anything else in Glu even if you don't use glue for ETL you may",
    "start": "139230",
    "end": "145170"
  },
  {
    "text": "very well end up using the data catalog because that catalog can be used by a number of other different services and",
    "start": "145170",
    "end": "150660"
  },
  {
    "text": "in the previous session we sort of being used by Athena it can be used by Glu zone ETL it can be used by hive and",
    "start": "150660",
    "end": "156330"
  },
  {
    "text": "spark on EMR and it can even be used by redshift spectrum for querying against data in s3 but the other two",
    "start": "156330",
    "end": "164370"
  },
  {
    "text": "capabilities here our job authoring and job execution so Glu allows you to create or describe transformations given",
    "start": "164370",
    "end": "172290"
  },
  {
    "text": "this source and this target from the data catalog create from your transformation and glue or create a",
    "start": "172290",
    "end": "178770"
  },
  {
    "text": "transformation take a date from the source transform it push it into the target and in fact we will create for you some coats and pice barcodes and",
    "start": "178770",
    "end": "185610"
  },
  {
    "text": "pythons code that addresses the sparklin time so in that sense glue is a very",
    "start": "185610",
    "end": "191670"
  },
  {
    "text": "developer centric tool actually in the world of writing Python code you can open up that code in your own browser or",
    "start": "191670",
    "end": "198090"
  },
  {
    "text": "in the in your own IDE or in a notebook and further modify it may be that you",
    "start": "198090",
    "end": "203430"
  },
  {
    "text": "want to draw in some additional sources of data maybe you want to apply some additional computations or calculations",
    "start": "203430",
    "end": "209970"
  },
  {
    "text": "or transformations before pushing into a target so you can always modify that code you can also bring your own code to",
    "start": "209970",
    "end": "217620"
  },
  {
    "text": "glue if you have existing PI spark code existing PI spark transformations you",
    "start": "217620",
    "end": "222990"
  },
  {
    "text": "can store those scripts in s3 and then use glue to execute them and this is the",
    "start": "222990",
    "end": "228630"
  },
  {
    "text": "third core capability of glue the ability to schedule and execute transformations so you can schedule them",
    "start": "228630",
    "end": "235650"
  },
  {
    "text": "on a periodic basis but you can also schedule them based upon specific events you can say I want this transformation",
    "start": "235650",
    "end": "242100"
  },
  {
    "text": "over here to execute when that one over there completes when that job completes you can also use lamda to kick off a job",
    "start": "242100",
    "end": "249960"
  },
  {
    "text": "so maybe the you land a twin s3 and every time a new large file lands in s3 you would use that as a trigger or you",
    "start": "249960",
    "end": "257760"
  },
  {
    "text": "used a lambda to trigger the execution of a job",
    "start": "257760",
    "end": "262430"
  },
  {
    "text": "so there's several different use cases that Glu's intended to address the first",
    "start": "264670",
    "end": "270020"
  },
  {
    "text": "is this core use of the catalog the ability to be able to scour your AWS estate and discover sources of data in",
    "start": "270020",
    "end": "277100"
  },
  {
    "start": "271000",
    "end": "271000"
  },
  {
    "text": "s3 and at the end of a jade we same point so it might be relational data in RDS it might be data sitting in redshift",
    "start": "277100",
    "end": "283940"
  },
  {
    "text": "it might be CSV TSV Jason Parker oh I'll see data sitting at rest in s three we",
    "start": "283940",
    "end": "290990"
  },
  {
    "text": "can use the data catalog and those crawlers to discover those sources of data pull all of that data into the",
    "start": "290990",
    "end": "296930"
  },
  {
    "text": "catalog and then you can use the catalog search capabilities and its metadata API",
    "start": "296930",
    "end": "303620"
  },
  {
    "text": "is in order to consume those definitions and reuse those definitions in a number of other tools as we saw in the previous",
    "start": "303620",
    "end": "314570"
  },
  {
    "start": "311000",
    "end": "311000"
  },
  {
    "text": "session using that catalog and using the ability to create more query optimized",
    "start": "314570",
    "end": "319880"
  },
  {
    "text": "data format allows us to build out an analytics capability that's based in",
    "start": "319880",
    "end": "326150"
  },
  {
    "text": "large part a round data sitting at rest in s 3 effectively a data lake-like architecture where we're taking taking",
    "start": "326150",
    "end": "333410"
  },
  {
    "text": "raw sources of data from multiple different environments bring them together landing that data in s3",
    "start": "333410",
    "end": "339550"
  },
  {
    "text": "applying transformations building out potentially several different models",
    "start": "339550",
    "end": "345530"
  },
  {
    "text": "that represent trusted sources of data for consumption by a number of different tools and they could be service tools",
    "start": "345530",
    "end": "352970"
  },
  {
    "text": "such as a thinner and quick site as we saw in the last session it could be bringing that data into redshift for",
    "start": "352970",
    "end": "358160"
  },
  {
    "text": "structured data warehousing like analysis it could be doing further complex computations building models",
    "start": "358160",
    "end": "364820"
  },
  {
    "text": "apply machine learning in EMR or MX net",
    "start": "364820",
    "end": "369700"
  },
  {
    "text": "and in fact building up your enterprise data warehouse is another canonical use",
    "start": "371050",
    "end": "376910"
  },
  {
    "text": "case for Glu and that's partly the kind of demo we're going to go through today",
    "start": "376910",
    "end": "382130"
  },
  {
    "text": "where we're going to take some online transactional row level data and ETL it into a star schema in red shed",
    "start": "382130",
    "end": "391689"
  },
  {
    "start": "391000",
    "end": "391000"
  },
  {
    "text": "as I mentioned earlier the jobs that we're executing glue we can execute them",
    "start": "392629",
    "end": "397919"
  },
  {
    "text": "simply on a periodic basis but we can also execute them based upon specific events based upon prior jobs having",
    "start": "397919",
    "end": "404520"
  },
  {
    "text": "completed or we can trigger them from London because we exposed an SDK and a",
    "start": "404520",
    "end": "411449"
  },
  {
    "text": "command-line tool it's very easy for you to write lambda functions that react to events within your aid of your sis state",
    "start": "411449",
    "end": "417360"
  },
  {
    "text": "and use those events to trigger glue jobs so we can build out an event-driven",
    "start": "417360",
    "end": "422669"
  },
  {
    "text": "data pipeline using lambda and glue and potentially step functions for orchestrating long-running processes",
    "start": "422669",
    "end": "430610"
  },
  {
    "text": "okay so over the course of the rest of this session we're going to look at the data catalog about creating connections",
    "start": "431360",
    "end": "438270"
  },
  {
    "text": "creating crawlers crawling our data then we're going to create a new job modify",
    "start": "438270",
    "end": "445110"
  },
  {
    "text": "it a little and execute that job and we're going to do that in the context of this example now apologies this is all",
    "start": "445110",
    "end": "452190"
  },
  {
    "text": "very blurred but effectively what going to do is to take some online transactional data in my sequel database",
    "start": "452190",
    "end": "458779"
  },
  {
    "text": "data related around customers and the things that they've ordered and we're going to transform that into a star",
    "start": "458779",
    "end": "465210"
  },
  {
    "text": "schema and push that into red shirt for doing data warehousing like queries over the same data so we're going to create",
    "start": "465210",
    "end": "472199"
  },
  {
    "text": "dimensions and a single factor table that we can join to within Richard",
    "start": "472199",
    "end": "479389"
  },
  {
    "text": "okay so let's start by looking in that catalog in a bit more detail as I said",
    "start": "487560",
    "end": "494200"
  },
  {
    "text": "the data catalog is a highly available account wide hive meta store compatible",
    "start": "494200",
    "end": "499810"
  },
  {
    "text": "metadata repository right so it allows you to store metadata describing all",
    "start": "499810",
    "end": "505840"
  },
  {
    "text": "your different sources of data all today sources of data in s3 and potentially data stored in relational databases and",
    "start": "505840",
    "end": "512710"
  },
  {
    "text": "in Richard we've added search capabilities we also store connection",
    "start": "512710",
    "end": "518140"
  },
  {
    "text": "info for all those relational endpoints and we also version the metadata in the",
    "start": "518140",
    "end": "524020"
  },
  {
    "text": "catalog so this is a very important capability that we've added that if you're crawling a specific key space",
    "start": "524020",
    "end": "531160"
  },
  {
    "text": "with an s3 if you create a crawl I call that key space you discover sources of data that crawler can upload the",
    "start": "531160",
    "end": "538000"
  },
  {
    "text": "metadata into the catalog if at a later point in dying weari execute that crawler",
    "start": "538000",
    "end": "543870"
  },
  {
    "text": "perhaps you executing it on a weekly basis if it finds new sources of data in",
    "start": "543870",
    "end": "549970"
  },
  {
    "text": "the same space and some of that data has changed perhaps the schema has changed slightly perhaps you're using a semi structured",
    "start": "549970",
    "end": "556060"
  },
  {
    "text": "format such as Jason you've added additional fields and remove fields and so on if the crawler finds a slightly",
    "start": "556060",
    "end": "562630"
  },
  {
    "text": "modified schema but it believes that it's still on the whole similar to the existing definition table definition",
    "start": "562630",
    "end": "569560"
  },
  {
    "text": "then it will version your schema in the catalog so you'll get versioned",
    "start": "569560",
    "end": "575140"
  },
  {
    "text": "metadata in the catalog if the data has changed significantly then it may be that we recognize this is so dissimilar",
    "start": "575140",
    "end": "582580"
  },
  {
    "text": "we're going to create a new table definition for this new source of data",
    "start": "582580",
    "end": "588329"
  },
  {
    "text": "we can create these table definitions using the crawlers so that's pretty much the automated fashion but we can create",
    "start": "590060",
    "end": "597560"
  },
  {
    "text": "hive DDL we can use Athena in order to create our own external table table",
    "start": "597560",
    "end": "602930"
  },
  {
    "text": "definitions and submit them we also exposed through online tools and the API is there also allow us programmatically",
    "start": "602930",
    "end": "609470"
  },
  {
    "text": "to create table and partition definitions within the catalog so lots and lots of different ways of actually",
    "start": "609470",
    "end": "615170"
  },
  {
    "text": "creating those entries and we can do it through the console as well",
    "start": "615170",
    "end": "619870"
  },
  {
    "start": "622000",
    "end": "622000"
  },
  {
    "text": "I've pretty much described what the quarters do the crawlers are an additional feature of the catalog you",
    "start": "622570",
    "end": "628880"
  },
  {
    "text": "create a crawler in order to discover a source of data inference schema and upload that metadata into the catalog",
    "start": "628880",
    "end": "636370"
  },
  {
    "text": "when we find a source of data we'll try and interpret the file format if we recognize the file format and we've got",
    "start": "636370",
    "end": "642740"
  },
  {
    "text": "a classifier we will apply the classifier so out of the box glue comes with a number of different classifiers the CSV TSV Jason Park AOR C and so on",
    "start": "642740",
    "end": "654580"
  },
  {
    "text": "if we recognize the file format and we can apply a classifier we'll use that to try and infer the schema and upload that",
    "start": "654580",
    "end": "660950"
  },
  {
    "text": "metadata there's also an extensibility mechanism here again we support grok",
    "start": "660950",
    "end": "667100"
  },
  {
    "text": "expressions effectively composing regular expressions in order to create",
    "start": "667100",
    "end": "672290"
  },
  {
    "text": "schema on behalf of your own proprietary text-based file formats over time we'll",
    "start": "672290",
    "end": "678890"
  },
  {
    "text": "be extending those capabilities within glue we'll be adding more out-of-the-box classifiers we'll be adding additional",
    "start": "678890",
    "end": "684590"
  },
  {
    "text": "classifier and logic so that you can make kind of decisions as you're calling",
    "start": "684590",
    "end": "690560"
  },
  {
    "text": "the data and effectively create high fidelity schema with using those",
    "start": "690560",
    "end": "696590"
  },
  {
    "text": "extensibility mechanisms that's coming",
    "start": "696590",
    "end": "701500"
  },
  {
    "text": "so catalog allows us to bring in the metadata when we start reviewing a table",
    "start": "704740",
    "end": "710779"
  },
  {
    "start": "708000",
    "end": "708000"
  },
  {
    "text": "definition we'll see it broken down like this will see a screen describing some table properties some information about",
    "start": "710779",
    "end": "717199"
  },
  {
    "text": "where the underlying data resides and some of the metadata that we've captured",
    "start": "717199",
    "end": "722350"
  },
  {
    "text": "regarding that you know it's file format and serializers and the d c-- realises that we expect to use when we're",
    "start": "722350",
    "end": "728540"
  },
  {
    "text": "interrogating that data and also details of the schema that we've inferred on",
    "start": "728540",
    "end": "733759"
  },
  {
    "text": "behalf of that data source now the important point to mention here is this is all metadata the only thing that the",
    "start": "733759",
    "end": "740089"
  },
  {
    "text": "catalog is storing is metadata so we can create table definitions at a later point in time it may be that we want to",
    "start": "740089",
    "end": "746389"
  },
  {
    "text": "delete a database or delete a table definition within a database all we're",
    "start": "746389",
    "end": "751790"
  },
  {
    "text": "doing is deleting the metadata the underlying data whether it's data an s3 or data in a relational database that",
    "start": "751790",
    "end": "759199"
  },
  {
    "text": "stays as is okay so the catalog is only about metadata and we're only",
    "start": "759199",
    "end": "764269"
  },
  {
    "text": "manipulating and consuming the method data via the catalog this screenshot",
    "start": "764269",
    "end": "771139"
  },
  {
    "start": "768000",
    "end": "768000"
  },
  {
    "text": "here shows the kind of versioning that we support in glue and in those table",
    "start": "771139",
    "end": "777230"
  },
  {
    "text": "definitions so as I say if we've actually inferred that table definition has changed but it still constitutes",
    "start": "777230",
    "end": "783680"
  },
  {
    "text": "part of the same table or same table definition then we'll version it within the catalog and you can compare two",
    "start": "783680",
    "end": "789470"
  },
  {
    "text": "versions side by side to see what's changed what's been added deleted what data types have changed the catalog and",
    "start": "789470",
    "end": "800059"
  },
  {
    "start": "797000",
    "end": "797000"
  },
  {
    "text": "the crawlers also support this concept of partitioning so I discussed this",
    "start": "800059",
    "end": "805100"
  },
  {
    "text": "briefly in the last talk as well but a lot of analytics tooling works best with",
    "start": "805100",
    "end": "813079"
  },
  {
    "text": "very very large bodies of data that have been partitioned so it may be that if you're storing data in s3 you might",
    "start": "813079",
    "end": "819470"
  },
  {
    "text": "partition it by week or month you've got time series data and large volumes of time series data you might partition",
    "start": "819470",
    "end": "825259"
  },
  {
    "text": "that data by day or by week or by month okay so you'd store all of this week's",
    "start": "825259",
    "end": "830929"
  },
  {
    "text": "data in one folder or directory last weeks in another weeks in another okay if we can then",
    "start": "830929",
    "end": "838319"
  },
  {
    "text": "reflect that directory structure in our table definition if we can effectively",
    "start": "838319",
    "end": "843680"
  },
  {
    "text": "capture that directory structure as a series of partition keys then we can reuse those partition keys in our",
    "start": "843680",
    "end": "850019"
  },
  {
    "text": "queries in order to narrow down the amount of data that we have to scan when we're computing a result if you've",
    "start": "850019",
    "end": "857759"
  },
  {
    "text": "stored your data in a kind of hierarchical format in s3 then",
    "start": "857759",
    "end": "863579"
  },
  {
    "text": "the crawlers will try and infer a partitioning scheme now there's a",
    "start": "863579",
    "end": "869579"
  },
  {
    "text": "specific partitioning scheme that's hive supports which is kind of key value pairs date equals 15 month equals",
    "start": "869579",
    "end": "877800"
  },
  {
    "text": "November if you've actually named your directory using this hive partitioning scheme that makes it very easy for glue",
    "start": "877800",
    "end": "884939"
  },
  {
    "text": "to apply exactly the same partitioning scheme but you don't have to use highest partitioning scheme but you can still",
    "start": "884939",
    "end": "891389"
  },
  {
    "text": "use a hierarchical structure and if you do and if glue crawls that hierarchical",
    "start": "891389",
    "end": "896490"
  },
  {
    "text": "structure and thinks that the files in all of those directories are similar then it will assume that what you have",
    "start": "896490",
    "end": "903480"
  },
  {
    "text": "in place is a partitioning scheme and it will actually create a table definition with additional columns that represent",
    "start": "903480",
    "end": "910379"
  },
  {
    "text": "the different levels within your hierarchy okay and those columns will be marked as being partitions so you add",
    "start": "910379",
    "end": "918120"
  },
  {
    "text": "new partitions to your underlying data set and when we write queries query against these external table definitions",
    "start": "918120",
    "end": "924420"
  },
  {
    "text": "we can take advantage of those those partition columns in our where clauses",
    "start": "924420",
    "end": "931439"
  },
  {
    "text": "and that can hugely restrict the amount of data that we have to scan",
    "start": "931439",
    "end": "936620"
  },
  {
    "text": "so yeah automatic part partition recognition and actually we also allow",
    "start": "937870",
    "end": "943430"
  },
  {
    "start": "942000",
    "end": "942000"
  },
  {
    "text": "you to to drill into those partitions and in and inspect some of the data that resides in each of them so that's for",
    "start": "943430",
    "end": "954020"
  },
  {
    "text": "example remember the example that we've got here is I've got a my sequel database with online transactional data",
    "start": "954020",
    "end": "961730"
  },
  {
    "text": "every time somebody places an order we'll create a new row in one of those tables and what I'm wanting to do in my",
    "start": "961730",
    "end": "967970"
  },
  {
    "text": "example is detail that into redshift I'm going to do this just as a one-off operation in this example but we can",
    "start": "967970",
    "end": "974660"
  },
  {
    "text": "support ongoing operations where we're continuing updating fat tables with",
    "start": "974660",
    "end": "980780"
  },
  {
    "text": "batches of records source from the source what we want to do is to take",
    "start": "980780",
    "end": "986090"
  },
  {
    "text": "that source data in my sequel transform it to a star schema and push it into reg",
    "start": "986090",
    "end": "991430"
  },
  {
    "text": "note I've already done some of this I've actually pushed I've set up my sequel",
    "start": "991430",
    "end": "996920"
  },
  {
    "text": "populated it I've already set up my redshift instance as well and I've populated all the dimension tables but",
    "start": "996920",
    "end": "1006329"
  },
  {
    "text": "and again so I just need to mirror my screens",
    "start": "1013080",
    "end": "1017600"
  },
  {
    "text": "I have no idea how to make that text a bit bigger because that's tiny sorry this is very simple query against my",
    "start": "1035409",
    "end": "1041699"
  },
  {
    "text": "fact table in redshift I'm just selecting rows from that factor table",
    "start": "1041699",
    "end": "1047918"
  },
  {
    "text": "and in the results pane here we can see it's empty alright so I want to populate them and",
    "start": "1047919",
    "end": "1055779"
  },
  {
    "text": "I'm going to do that via glue what I need to do is to create a connection to",
    "start": "1055779",
    "end": "1064409"
  },
  {
    "text": "just review my instructions where am i creating the connection to I can create a connection to redshift and I'm going",
    "start": "1064409",
    "end": "1072879"
  },
  {
    "text": "to create a crawler that can crawl redshift so as to create a new table definition or new database definition",
    "start": "1072879",
    "end": "1079360"
  },
  {
    "text": "inside my catalogue so in setting up Matthias set up my sequel and redshift and done some of the population I",
    "start": "1079360",
    "end": "1085240"
  },
  {
    "text": "actually deleted some of the metadata from the catalogue so we can reconstruct it so in the data catalog part of glue",
    "start": "1085240",
    "end": "1091389"
  },
  {
    "text": "I'm going to create a new connection so I've already got a connection to my sequel database I'm going to create one",
    "start": "1091389",
    "end": "1100350"
  },
  {
    "text": "named glue demo redshift connection we",
    "start": "1100590",
    "end": "1106240"
  },
  {
    "text": "see that okay it black it gives me the option here do I want to connect",
    "start": "1106240",
    "end": "1112000"
  },
  {
    "text": "something in RDS do I have JDBC connection perhaps something I'm hosting on ec2 or don't want to connect to",
    "start": "1112000",
    "end": "1118419"
  },
  {
    "text": "Reginald connect to rift Glu's",
    "start": "1118419",
    "end": "1123879"
  },
  {
    "text": "intelligent enough to try and identify existing redshift instances within my V",
    "start": "1123879",
    "end": "1130960"
  },
  {
    "text": "PC it's found the one that I'm running it's identified a database and the user I can type in a password for that user",
    "start": "1130960",
    "end": "1139360"
  },
  {
    "text": "that we're going to use to access the database I don't want to save that all",
    "start": "1139360",
    "end": "1146710"
  },
  {
    "text": "right that's created for me a connection we can test it just to ensure that",
    "start": "1146710",
    "end": "1153410"
  },
  {
    "text": "everything is okay and again I have to supply an iam role in order to give glue",
    "start": "1153410",
    "end": "1158660"
  },
  {
    "text": "permission or allow it to access resources within my V PC within my",
    "start": "1158660",
    "end": "1164000"
  },
  {
    "text": "account so this can take a minute or two",
    "start": "1164000",
    "end": "1169040"
  },
  {
    "text": "to establish that connection and just check that it's running I'm going to assume actually that it is working and",
    "start": "1169040",
    "end": "1175160"
  },
  {
    "text": "move on and actually create a crawler that takes advantage of that connection so create a connection so now I can",
    "start": "1175160",
    "end": "1180320"
  },
  {
    "text": "connect by a glue is aware of being able to connect it to my redshift database",
    "start": "1180320",
    "end": "1185440"
  },
  {
    "text": "I'm now going to create a crawler that can connect to that redshift database and pull down the metadata",
    "start": "1185440",
    "end": "1191990"
  },
  {
    "text": "describing all of the tables so my",
    "start": "1191990",
    "end": "1198620"
  },
  {
    "text": "crawler glued mo rich of crawler",
    "start": "1198620",
    "end": "1205929"
  },
  {
    "text": "and again supply the service rule now when I'm configuring crawlers I can",
    "start": "1207930",
    "end": "1213600"
  },
  {
    "text": "configure them to crawl s3 or I can take advantage of what the connections I specified earlier if I'm calling s3 it",
    "start": "1213600",
    "end": "1222480"
  },
  {
    "text": "may be that I also want to specify specific classifiers plus I've created use that extensibility mechanism to",
    "start": "1222480",
    "end": "1228990"
  },
  {
    "text": "create some custom classifiers that recognize my proprietary file formats if",
    "start": "1228990",
    "end": "1234990"
  },
  {
    "text": "that's the case this is where I would specify those additional custom classifiers but I don't need to do any",
    "start": "1234990",
    "end": "1241110"
  },
  {
    "text": "of that I'm not looking for date or an s3 I'm looking for data at the end of a",
    "start": "1241110",
    "end": "1246240"
  },
  {
    "text": "JDBC endpoint I'm going to select it select an existing connection although I gained through that the wizard here I",
    "start": "1246240",
    "end": "1252510"
  },
  {
    "text": "could create a connection and then I'm going to tell Glu about the path within",
    "start": "1252510",
    "end": "1259170"
  },
  {
    "text": "that redshift cluster that I want to search on a search a specific database",
    "start": "1259170",
    "end": "1264500"
  },
  {
    "text": "it's the dev database it's the public schema and I want to capture details",
    "start": "1264500",
    "end": "1270210"
  },
  {
    "text": "around all of the tables there so actually I could also include some exclude paths here so I can actually",
    "start": "1270210",
    "end": "1276390"
  },
  {
    "text": "even interrogate relational endpoints and pick and choose the metadata that I",
    "start": "1276390",
    "end": "1281430"
  },
  {
    "text": "want to bring into the catalog it's not an all-or-nothing affair I can specify specific paths into databases and",
    "start": "1281430",
    "end": "1287760"
  },
  {
    "text": "schemas and I can exclude specific tables if or so on don't want to add any",
    "start": "1287760",
    "end": "1294540"
  },
  {
    "text": "additional data sources all right so crawlers I've said we can schedule them",
    "start": "1294540",
    "end": "1299660"
  },
  {
    "text": "on a periodic basis or we can simply specify them as being run on demand where you would invoke it from the",
    "start": "1299660",
    "end": "1306900"
  },
  {
    "text": "console or from the command line or via an API I just want one that's run on demand and I also need to tell glue you",
    "start": "1306900",
    "end": "1315990"
  },
  {
    "text": "know given that I've found this new data where am I going to put it and am I going to create a new database",
    "start": "1315990",
    "end": "1321330"
  },
  {
    "text": "definition in the catalog or I am I going to add this definition to an existing catalog entry I'm going to",
    "start": "1321330",
    "end": "1330780"
  },
  {
    "text": "create a new one I think yes this is the important bit",
    "start": "1330780",
    "end": "1340250"
  },
  {
    "text": "forget this one",
    "start": "1340250",
    "end": "1343240"
  },
  {
    "text": "forget the name of the database wrong",
    "start": "1348669",
    "end": "1352080"
  },
  {
    "text": "that's the name of the database that my script is expecting so let's add a database give it this name glue demo",
    "start": "1356290",
    "end": "1363400"
  },
  {
    "text": "redshift not great naming conventions but I can you know so I'm going to",
    "start": "1363400",
    "end": "1369250"
  },
  {
    "text": "create some metadata definitions in the catalog table definitions I can if I want add some Preferences to them I'm",
    "start": "1369250",
    "end": "1374380"
  },
  {
    "text": "not going to bother doing that correct",
    "start": "1374380",
    "end": "1381120"
  },
  {
    "text": "so we can see that our connection that we were testing earlier has successfully connected to redshift I've now created a",
    "start": "1381120",
    "end": "1387730"
  },
  {
    "text": "crawler do you want to run it now yes I do all right this should only take a",
    "start": "1387730",
    "end": "1392860"
  },
  {
    "text": "minute or so in order to connect a redshift pulldown those table definitions and push them into the",
    "start": "1392860",
    "end": "1398800"
  },
  {
    "text": "catalog what we're waiting for that any questions",
    "start": "1398800",
    "end": "1404850"
  },
  {
    "text": "so the question here is we've got product Kinesis fire hose or Kinesis stream and we're effectively pushing",
    "start": "1427450",
    "end": "1433820"
  },
  {
    "text": "data into s3 on a periodic basis we've compressed and encrypted that data can",
    "start": "1433820",
    "end": "1440960"
  },
  {
    "text": "glue uncompressed and decrypt can't",
    "start": "1440960",
    "end": "1447260"
  },
  {
    "text": "remember the answer off the top of their head oh I thought it could read compressed and encrypted data I need to",
    "start": "1447260",
    "end": "1453350"
  },
  {
    "text": "go check",
    "start": "1453350",
    "end": "1455650"
  },
  {
    "text": "so while we're waiting for that table definition to be uploaded we'll click",
    "start": "1475840",
    "end": "1481490"
  },
  {
    "text": "back to the presentation and walk away forwards and then we'll come back to glue in a second all right all right so",
    "start": "1481490",
    "end": "1492470"
  },
  {
    "text": "that was all about the catalog about acquiring metadata in the first place describing all those different sources",
    "start": "1492470",
    "end": "1497780"
  },
  {
    "text": "of data the second capability is actually being able to author jobs within glue so today you've got several",
    "start": "1497780",
    "end": "1507110"
  },
  {
    "text": "options I mean it's all very developer centric and we're effectively working in Python land here but we can have glue",
    "start": "1507110",
    "end": "1512780"
  },
  {
    "start": "1508000",
    "end": "1508000"
  },
  {
    "text": "create for us some Python scripts and we'll we'll do that in a moment we effectively take a source and a",
    "start": "1512780",
    "end": "1518510"
  },
  {
    "text": "target from the catalog and glue will create for us a transformation that will",
    "start": "1518510",
    "end": "1523610"
  },
  {
    "text": "pull the data and push it into the target we can bring our own scripts that",
    "start": "1523610",
    "end": "1528710"
  },
  {
    "text": "we've authored outside of glue we can host them in s3 and we can configure a",
    "start": "1528710",
    "end": "1533929"
  },
  {
    "text": "job to take advantage of those scripts we can also iteratively and interactively develop this script taking",
    "start": "1533929",
    "end": "1541610"
  },
  {
    "text": "advantage of glue by connecting a notebook to glue so glue has this concept of developer endpoints that you",
    "start": "1541610",
    "end": "1548210"
  },
  {
    "text": "can configure and that will expose an endpoint that allows you to spin up an",
    "start": "1548210",
    "end": "1554330"
  },
  {
    "text": "instance of Zepplin and we provide all the cloud formation templates that will do this on your behalf once you have",
    "start": "1554330",
    "end": "1560900"
  },
  {
    "text": "that notebook up and running you can connect it to that endpoint and you can start developing an ETL script running",
    "start": "1560900",
    "end": "1569540"
  },
  {
    "text": "it re running it seeing where it succeeds where it fails take advantage of all of the additional code elements",
    "start": "1569540",
    "end": "1576950"
  },
  {
    "text": "that we've added to PI spark once you're happy with the script you can then",
    "start": "1576950",
    "end": "1582650"
  },
  {
    "text": "there's a little bit of boilerplate we probably add need to add at the beginning and end but you can effectively save that as a script again",
    "start": "1582650",
    "end": "1588110"
  },
  {
    "text": "in s3 and reuse that have glue execute that",
    "start": "1588110",
    "end": "1593500"
  },
  {
    "start": "1594000",
    "end": "1594000"
  },
  {
    "text": "so the automatic code generation and we'll see this in a moment will point to",
    "start": "1595170",
    "end": "1600840"
  },
  {
    "text": "a source and a target glue will actually give us a pictorial representation of the transformation that it's",
    "start": "1600840",
    "end": "1606480"
  },
  {
    "text": "anticipating making you know show us how it's trying to map from one column to another this at this point in time gives",
    "start": "1606480",
    "end": "1613230"
  },
  {
    "text": "us the ability to modify column names or modify data types in the target what we",
    "start": "1613230",
    "end": "1622680"
  },
  {
    "text": "end up with is some PI spark code that is heavily annotated and those annotations are used to generate a",
    "start": "1622680",
    "end": "1629310"
  },
  {
    "text": "visual representation of the several steps in that transformation okay so what we'll get in the browser is some",
    "start": "1629310",
    "end": "1636450"
  },
  {
    "text": "code plus a visual representation of the steps the glue is going to execute",
    "start": "1636450",
    "end": "1642770"
  },
  {
    "start": "1643000",
    "end": "1643000"
  },
  {
    "text": "as I say that code is very heavily annotated this is for the benefit not",
    "start": "1644790",
    "end": "1650590"
  },
  {
    "text": "only of creating those pictorial representations but it's also to help you understand what's going on in this",
    "start": "1650590",
    "end": "1656410"
  },
  {
    "text": "auto-generated code all right so it's tried to be as human readable as possible so you can work your way",
    "start": "1656410",
    "end": "1663130"
  },
  {
    "text": "through that code understand where you might need to intervene modify it or enrich it all right so you can use just",
    "start": "1663130",
    "end": "1673179"
  },
  {
    "start": "1670000",
    "end": "1670000"
  },
  {
    "text": "regular high spark code and all of the stuff that you get if you were just spinning up vanilla spark however we've",
    "start": "1673179",
    "end": "1681610"
  },
  {
    "text": "also added some additional libraries here and some additional constructs that make it easier to work with semi",
    "start": "1681610",
    "end": "1688510"
  },
  {
    "text": "structured data with data where we need to infer schema on the fly so the most",
    "start": "1688510",
    "end": "1693760"
  },
  {
    "text": "important construct here is the dynamic frame all right so this effectively",
    "start": "1693760",
    "end": "1699309"
  },
  {
    "text": "wraps a data frame but instead of having to supply a schemer upfront a dynamic",
    "start": "1699309",
    "end": "1704350"
  },
  {
    "text": "frame can infer schema or can draw schema from the catalog and use that to make sense in the data that we've",
    "start": "1704350",
    "end": "1710200"
  },
  {
    "text": "brought into memory we build on top of that dynamic frame with a number of the",
    "start": "1710200",
    "end": "1718799"
  },
  {
    "text": "the transforms that we include in our additional code constructs we take advantage of some of those dynamic frame",
    "start": "1718799",
    "end": "1725290"
  },
  {
    "text": "elements in order to be able to move data from say a JSON structure into a relational model you can always",
    "start": "1725290",
    "end": "1733419"
  },
  {
    "text": "effectively downcast you could convert a dynamic frame into a data frame if you",
    "start": "1733419",
    "end": "1738580"
  },
  {
    "text": "want to write some spark sequel you can do that and then convert it back to a dynamic frame potentially for output",
    "start": "1738580",
    "end": "1745090"
  },
  {
    "text": "into the source or for doing some more semi structured or schema let's work against that data so the dynamic frame",
    "start": "1745090",
    "end": "1754900"
  },
  {
    "start": "1751000",
    "end": "1751000"
  },
  {
    "text": "and then we have a library of transforms and we're constantly adding to this library but we were released with about",
    "start": "1754900",
    "end": "1761770"
  },
  {
    "text": "eight or ten different transforms I think when we first make them available so their transforms such as resolved",
    "start": "1761770",
    "end": "1767890"
  },
  {
    "text": "choice so you're working your way through a large data set and it may be",
    "start": "1767890",
    "end": "1773440"
  },
  {
    "text": "that there are some column values which yeah most of those column values are integers and a few of them are",
    "start": "1773440",
    "end": "1780230"
  },
  {
    "text": "strings what you want to do there do you just want to treat the strings as erroneous data or dynamic frame would",
    "start": "1780230",
    "end": "1787549"
  },
  {
    "text": "actually allow you to push those erroneous rows into a separate set of triples and store them in s3 and",
    "start": "1787549",
    "end": "1793070"
  },
  {
    "text": "continue running the job or do you want to try and apply some logic you want to",
    "start": "1793070",
    "end": "1800210"
  },
  {
    "text": "take that string representation to think laterally I could make some sense about I could perhaps cast it to an integer or look it up against the lookup table and",
    "start": "1800210",
    "end": "1807440"
  },
  {
    "text": "use that to create the integer integer representation of that value the choice transform or the resolve choice",
    "start": "1807440",
    "end": "1813259"
  },
  {
    "text": "transform allows you to do exactly that so this is helping you deal with messy data in your source data where you know",
    "start": "1813259",
    "end": "1819980"
  },
  {
    "text": "that a column has a mixture of types and you want to be able to work with those mixture of types we have an apply",
    "start": "1819980",
    "end": "1827870"
  },
  {
    "text": "mapping transform hello",
    "start": "1827870",
    "end": "1831399"
  },
  {
    "text": "so the question here is can we do schema catalog of validation against our data",
    "start": "1839789",
    "end": "1845649"
  },
  {
    "text": "using resolved choice or using some other transform today notice that",
    "start": "1845649",
    "end": "1851740"
  },
  {
    "text": "there's no explicit function for validating schema I think some of that",
    "start": "1851740",
    "end": "1856990"
  },
  {
    "text": "is forthcoming the the definitions that we create are based on sampling the data",
    "start": "1856990",
    "end": "1862990"
  },
  {
    "text": "and they try to be as generous as possible so potentially if we were seeing a mixture of integers and strings",
    "start": "1862990",
    "end": "1868120"
  },
  {
    "text": "in a column we would decide well the only thing that could accommodate that is going to be a string datatype but you",
    "start": "1868120",
    "end": "1874929"
  },
  {
    "text": "know that your target data type is going to be an integer and you can introduce this resolved element within the Pais",
    "start": "1874929",
    "end": "1882010"
  },
  {
    "text": "bar code in order to deal with a missing data so apply mapping apply mapping is",
    "start": "1882010",
    "end": "1890889"
  },
  {
    "text": "very useful for dealing with hierarchical data structures and for flattening them or taking a flat",
    "start": "1890889",
    "end": "1895990"
  },
  {
    "text": "structure and building up and hierarchical structure and then we effectively have an apply mapping",
    "start": "1895990",
    "end": "1901870"
  },
  {
    "text": "transform on steroids called relation eyes and this will take something like",
    "start": "1901870",
    "end": "1907570"
  },
  {
    "text": "semi-structured jason that's deeply nested jason in your source and it will",
    "start": "1907570",
    "end": "1914260"
  },
  {
    "text": "create relational schema at the back so this is ideal for taking relational data and pushing it sorry taking JSON data",
    "start": "1914260",
    "end": "1921970"
  },
  {
    "text": "and pushing it into a relational database perhaps you had a question in the break earlier a similar use case but",
    "start": "1921970",
    "end": "1929320"
  },
  {
    "text": "dealing with XML data today the the types we support out the box we don't have explicit support for XML I",
    "start": "1929320",
    "end": "1935139"
  },
  {
    "text": "understand there's a feature request for that but were we to have that would be able to do something very similar to",
    "start": "1935139",
    "end": "1940899"
  },
  {
    "text": "take XML source data whether it's an XML document or an XML field and relation",
    "start": "1940899",
    "end": "1947799"
  },
  {
    "text": "eyes it and push that into a relational schema okay so I mentioned that out of",
    "start": "1947799",
    "end": "1955179"
  },
  {
    "start": "1952000",
    "end": "1952000"
  },
  {
    "text": "the box we come with a number of different transforms this is the list I think that we we had when we we went",
    "start": "1955179",
    "end": "1960940"
  },
  {
    "text": "live and we're constantly wanting to add more so overtime again",
    "start": "1960940",
    "end": "1966480"
  },
  {
    "text": "because this is a managed service whenever we have these new features if we add new relational transforms then we",
    "start": "1966480",
    "end": "1973650"
  },
  {
    "text": "will out so if we add new transformations then they will immediately become available to you as a",
    "start": "1973650",
    "end": "1979799"
  },
  {
    "text": "user of that managed service all right",
    "start": "1979799",
    "end": "1984830"
  },
  {
    "start": "1983000",
    "end": "1983000"
  },
  {
    "text": "we're going to create transformation in a moment that's going to take some of our row level sales data and turn it",
    "start": "1985320",
    "end": "1992670"
  },
  {
    "text": "into a fact table I'll show you that in a second but the only other thing I wanted to mention here are these",
    "start": "1992670",
    "end": "1997679"
  },
  {
    "text": "developer endpoints through the glue console through the API you can create these developer endpoints provision them",
    "start": "1997679",
    "end": "2004610"
  },
  {
    "text": "and then using a cloud formation template which again you can access through the console you can create a",
    "start": "2004610",
    "end": "2010730"
  },
  {
    "text": "Zeppelin notebook that's hosted in ec2 and that will allow you to collect your developer endpoint and now you have a",
    "start": "2010730",
    "end": "2017090"
  },
  {
    "text": "nice repple like environment in which to develop your ETL scripts and this makes",
    "start": "2017090",
    "end": "2023600"
  },
  {
    "text": "it much faster for for working on custom transformations working on custom",
    "start": "2023600",
    "end": "2028940"
  },
  {
    "text": "scripts and when I started working with glue I was writing scripts in the",
    "start": "2028940",
    "end": "2034040"
  },
  {
    "text": "browser saving them and executing them through job glues job execution engine and because there was a certain amount",
    "start": "2034040",
    "end": "2040760"
  },
  {
    "text": "of time spent spinning up resource I would have to wait minutes for feedback with the notebook and the developer",
    "start": "2040760",
    "end": "2047690"
  },
  {
    "text": "endpoints you can get feedback typically in seconds it makes it a lot easier to develop in an iterative and incremental",
    "start": "2047690",
    "end": "2053210"
  },
  {
    "text": "manner I think there's another question that I had earlier as well given that",
    "start": "2053210",
    "end": "2061128"
  },
  {
    "start": "2055000",
    "end": "2055000"
  },
  {
    "text": "we've created dynamic frames and all these different transformations perhaps you want to develop these things",
    "start": "2061129",
    "end": "2067700"
  },
  {
    "text": "completely outside of the glue environment and do we make these things available to you and the answer is yes",
    "start": "2067700",
    "end": "2072878"
  },
  {
    "text": "so you can get these slides after the talk there's a link here to our github account where we have the source code",
    "start": "2072879",
    "end": "2079638"
  },
  {
    "text": "for all of those additional high spark constructs there's also a link for",
    "start": "2079639",
    "end": "2085429"
  },
  {
    "text": "another repository with some detailed examples of using glue of using things like resolve choice and so on and a nice",
    "start": "2085429",
    "end": "2092658"
  },
  {
    "text": "FAQ Q&A kind of stuff all right",
    "start": "2092659",
    "end": "2097579"
  },
  {
    "text": "so switching back to my browser alright",
    "start": "2104210",
    "end": "2112549"
  },
  {
    "text": "so I created a crawler I'd pointed it at that red shift endpoint it's now gone",
    "start": "2112549",
    "end": "2117740"
  },
  {
    "text": "and crawled that endpoint and we can see that it pulled down the definitions for all of the tables both of dimensions and",
    "start": "2117740",
    "end": "2122869"
  },
  {
    "text": "the fact tables so now I'm in a position having some source metadata in my sequel",
    "start": "2122869",
    "end": "2129109"
  },
  {
    "text": "and some target metadata my redshift metadata all of that in the catalog I can now create a job so you'll notice",
    "start": "2129109",
    "end": "2137510"
  },
  {
    "text": "here I've already got some jobs for loading the dimension data suppliers customers and products and so on I want",
    "start": "2137510",
    "end": "2144049"
  },
  {
    "text": "to create a job that is going to load all of the fact data all of the",
    "start": "2144049",
    "end": "2150950"
  },
  {
    "text": "individual sales rose so let's call it",
    "start": "2150950",
    "end": "2158869"
  },
  {
    "text": "load factor sales order supply the service role again it could be that I",
    "start": "2158869",
    "end": "2166339"
  },
  {
    "text": "want to supply an existing script it maybe I've got a script I developed earlier and I store it in s3 I could point at that again I'm going to have",
    "start": "2166339",
    "end": "2173630"
  },
  {
    "text": "glue create from your script although I'm getting got to modify it a little bit so you can see here this is where my",
    "start": "2173630",
    "end": "2179150"
  },
  {
    "text": "script is going to be stored I need to supply a temporary directory now the",
    "start": "2179150",
    "end": "2185930"
  },
  {
    "text": "temporary directory is really useful here because I'm going to be pushing data into redshift the ideal load path",
    "start": "2185930",
    "end": "2191240"
  },
  {
    "text": "for redshift is to have multiple files sitting at rest in s 3 and to load them in parallel into regice parallel",
    "start": "2191240",
    "end": "2198500"
  },
  {
    "text": "architecture ok so that's exactly what my glue job is going to do it's going to pull data from my sequel it's",
    "start": "2198500",
    "end": "2205220"
  },
  {
    "text": "temporarily going to create some files some CSV files in s3 and then it's going",
    "start": "2205220",
    "end": "2210230"
  },
  {
    "text": "to invoke a copy command that pulls all of that data into redshift so I need to supply with the location of a temporary",
    "start": "2210230",
    "end": "2216079"
  },
  {
    "text": "directory where those files are going to reside glue will clean up after the fact",
    "start": "2216079",
    "end": "2222190"
  },
  {
    "text": "all right so what's my datasource it's",
    "start": "2223280",
    "end": "2231850"
  },
  {
    "text": "it's this table here in my sequel and my",
    "start": "2231850",
    "end": "2238520"
  },
  {
    "text": "target is my fact table sales order fact table in regiment all right",
    "start": "2238520",
    "end": "2246620"
  },
  {
    "text": "actually in the relational schema we've got a highly normalized schema so when",
    "start": "2246620",
    "end": "2252320"
  },
  {
    "text": "we create details around an individual sale we're actually joining across or storing that detail in I think two or three",
    "start": "2252320",
    "end": "2258440"
  },
  {
    "text": "different tables so I've just pointed at one of those tables in order to create a kind of skeleton transformation here so",
    "start": "2258440",
    "end": "2265520"
  },
  {
    "text": "you can see that we're just pulling across four different columns from the source and pushing them into the fact table the model modification I want to",
    "start": "2265520",
    "end": "2273470"
  },
  {
    "text": "make in my code is to actually in the body of the code is to also source data from that other my sequel date table and",
    "start": "2273470",
    "end": "2281810"
  },
  {
    "text": "to join it in memory and then create the fact table that I'm going to push in to",
    "start": "2281810",
    "end": "2287570"
  },
  {
    "text": "redshift",
    "start": "2287570",
    "end": "2289990"
  },
  {
    "text": "so you can see here glue has created for me some place barcode because of the",
    "start": "2293950",
    "end": "2300549"
  },
  {
    "text": "annotations in the code it's also created a diagram of all the steps that it intends to apply I'm actually going",
    "start": "2300549",
    "end": "2309730"
  },
  {
    "text": "to modify it and I'm going to modify it using in the developers best friend which is copy and paste okay all right",
    "start": "2309730",
    "end": "2321099"
  },
  {
    "text": "um let me just try and grow this a",
    "start": "2321099",
    "end": "2327849"
  },
  {
    "text": "little what I'm really doing here I've taken you know so this is the original source",
    "start": "2327849",
    "end": "2336579"
  },
  {
    "text": "that that transformation or glue had created for me on my behalf but that was",
    "start": "2336579",
    "end": "2342520"
  },
  {
    "text": "only one table I wanted to join two tables in memory in order to create my factor table so I've also added these",
    "start": "2342520",
    "end": "2349359"
  },
  {
    "text": "rows here or these lines here in the code and again I'm consuming table definition information that's stored in",
    "start": "2349359",
    "end": "2355030"
  },
  {
    "text": "the catalog and then joining those two",
    "start": "2355030",
    "end": "2360280"
  },
  {
    "text": "tables based upon the order ID we've",
    "start": "2360280",
    "end": "2365950"
  },
  {
    "text": "also created a user-defined function which can create some derived values or some calculated values we're adding that",
    "start": "2365950",
    "end": "2372040"
  },
  {
    "text": "as well into our fact table so we're doing some quite complex stuff here we're joining two tables we're creating",
    "start": "2372040",
    "end": "2379780"
  },
  {
    "text": "some derived values applying some calculations and then we're going to push all of that into redshift and again",
    "start": "2379780",
    "end": "2386950"
  },
  {
    "text": "this effectively at the very end of the script here is code that gluer created for me on my behalf",
    "start": "2386950",
    "end": "2393309"
  },
  {
    "text": "taking some data that we've transformed and pushing it into redshift I'll save",
    "start": "2393309",
    "end": "2399460"
  },
  {
    "text": "that and then I now have a list of all",
    "start": "2399460",
    "end": "2405280"
  },
  {
    "text": "the different jobs now I could run the job directly or I could create a trigger for this job if I were to create a",
    "start": "2405280",
    "end": "2412240"
  },
  {
    "text": "trigger I might create a trigger called",
    "start": "2412240",
    "end": "2418650"
  },
  {
    "text": "facts and it may be that I want to load all of my facts only after I've loaded",
    "start": "2418890",
    "end": "2425860"
  },
  {
    "text": "all of my dimension data so maybe that I load all of my dimension data products suppliers otherwise I load all of that",
    "start": "2425860",
    "end": "2433270"
  },
  {
    "text": "in parallel and when one of those jobs completes that's enough to kick off this one so I could specify a trigger here",
    "start": "2433270",
    "end": "2440230"
  },
  {
    "text": "that will activate when a prior job has completed this is a way of building up into kind of dependency tree of jobs",
    "start": "2440230",
    "end": "2446920"
  },
  {
    "text": "that get scheduled and executed so I could say that whenever I've loaded that",
    "start": "2446920",
    "end": "2452980"
  },
  {
    "text": "customer dimension so I'm watching this job load dimension customer data I want",
    "start": "2452980",
    "end": "2459490"
  },
  {
    "text": "to trigger the job that we've just added which is the one that creates the fact table if I create a a trigger that's",
    "start": "2459490",
    "end": "2473110"
  },
  {
    "text": "activated based upon that kind of event then I also have to enable it so if i",
    "start": "2473110",
    "end": "2481930"
  },
  {
    "text": "were rerunning this entire example from beginning to end with an entirely empty redshift database then i will populate",
    "start": "2481930",
    "end": "2488800"
  },
  {
    "text": "all the dimensions in parallel when one of them has been completed that will",
    "start": "2488800",
    "end": "2493930"
  },
  {
    "text": "trigger our loading the fact table actually what I'm going to do is just simply run our our fact job on-demand as",
    "start": "2493930",
    "end": "2503020"
  },
  {
    "text": "it were so I'm going to run the job here",
    "start": "2503020",
    "end": "2507600"
  },
  {
    "text": "and we can see as it's running we get some feedback in the console we can also",
    "start": "2514040",
    "end": "2519920"
  },
  {
    "text": "use the command line tools the API is to ask glue for the current state refers to",
    "start": "2519920",
    "end": "2525080"
  },
  {
    "text": "statuses of a specific job or a list of all the jobs that are currently executing and then we can again use the",
    "start": "2525080",
    "end": "2531470"
  },
  {
    "text": "API to query for the status as we're executing those jobs we're generating lots of logs spark create lots and lots",
    "start": "2531470",
    "end": "2538130"
  },
  {
    "text": "of logs so we're storing all of that information in cloud watch and we're providing links here in the console that",
    "start": "2538130",
    "end": "2545240"
  },
  {
    "text": "allow you to get hold of all of those log entries within cloud watch so again before we introduce the developer",
    "start": "2545240",
    "end": "2551450"
  },
  {
    "text": "endpoints and the only way that I had of working with glue jobs a couple of months ago was to run them here if they",
    "start": "2551450",
    "end": "2558140"
  },
  {
    "text": "failed I then go look at the logs in cloud watch and work out what had gone wrong now I can actually develop these",
    "start": "2558140",
    "end": "2563300"
  },
  {
    "text": "things far more intuitively and interactively using a notebook and a developer endpoint alright that is",
    "start": "2563300",
    "end": "2571160"
  },
  {
    "text": "probably going to take five or six minutes to run I'll flick back",
    "start": "2571160",
    "end": "2579760"
  },
  {
    "text": "here okay so we've talked I mean so",
    "start": "2586160",
    "end": "2592340"
  },
  {
    "start": "2590000",
    "end": "2590000"
  },
  {
    "text": "we've talked about creating the jobs and we've seen an example of that and we've also effectively reviewed a lot of the",
    "start": "2592340",
    "end": "2599030"
  },
  {
    "text": "scheduling mechanisms as well you'll be able to look at these slides in more detail but effectively we're describing the fact that we can execute jobs on a",
    "start": "2599030",
    "end": "2606530"
  },
  {
    "text": "periodic basis effectively a cron job or we can supply specific events today we",
    "start": "2606530",
    "end": "2612110"
  },
  {
    "text": "support other jobs haven't completed that's an event that we can use to trigger a new job in the future we'll",
    "start": "2612110",
    "end": "2618560"
  },
  {
    "text": "add other events we'll add support for data catalog based events which is",
    "start": "2618560",
    "end": "2624410"
  },
  {
    "text": "really interesting you know okay so a crawler crates identifies a new database a new source of data in s3 identifies a",
    "start": "2624410",
    "end": "2632780"
  },
  {
    "text": "schema potentially that could be a trigger for some kind of job that you run so those kind of events in the",
    "start": "2632780",
    "end": "2639140"
  },
  {
    "text": "future will also be used to execute glue jobs we can use s3 notifications or",
    "start": "2639140",
    "end": "2644360"
  },
  {
    "text": "again we'll be able to use s3 notifications in the near future today you could use s3 notifications and",
    "start": "2644360",
    "end": "2649490"
  },
  {
    "text": "lambdas to kick off a glue job use the API is to kick off a glue job jobs also",
    "start": "2649490",
    "end": "2656450"
  },
  {
    "start": "2654000",
    "end": "2654000"
  },
  {
    "text": "have this concept of bookmarks so we maintain some state regarding the data",
    "start": "2656450",
    "end": "2661490"
  },
  {
    "text": "that we've previously processed so if you're running a job on a periodic basis against data that's landing in s3 we",
    "start": "2661490",
    "end": "2667700"
  },
  {
    "text": "don't want to rerun it over the entirety of that s3 data we only want to process the new files that have landed in s3",
    "start": "2667700",
    "end": "2673550"
  },
  {
    "text": "that's exactly what bookmarks allow you can enable bookmarks you can disable",
    "start": "2673550",
    "end": "2678770"
  },
  {
    "text": "them and you can pause them so if you've got an enabled bookmark but a job fails you may want to pause the bookmark so",
    "start": "2678770",
    "end": "2686720"
  },
  {
    "text": "you can rerun the job over the data that previously is failed maybe you modify",
    "start": "2686720",
    "end": "2692390"
  },
  {
    "text": "that source data then they've run a job and then you can re-enable the bookmark and every time we execute after that",
    "start": "2692390",
    "end": "2698360"
  },
  {
    "text": "we'll only be making forward progress",
    "start": "2698360",
    "end": "2701860"
  },
  {
    "start": "2701000",
    "end": "2701000"
  },
  {
    "text": "and to wrap up I mean the key point here is all of this is surplus we're trying",
    "start": "2703630",
    "end": "2709219"
  },
  {
    "text": "to remove a lot of the you know the undifferentiated heavy lifting around creating and managing ETL jobs we want",
    "start": "2709219",
    "end": "2716089"
  },
  {
    "text": "you to be able to focus on describing those transformations on discovering sources of data using the crawler using",
    "start": "2716089",
    "end": "2721789"
  },
  {
    "text": "the catalogue and execute scheduling and executing those transformations will take care of all the low-level",
    "start": "2721789",
    "end": "2727699"
  },
  {
    "text": "plumbing an infrastructure I've got a couple of minutes left so I'm very happy",
    "start": "2727699",
    "end": "2732979"
  },
  {
    "text": "to take questions",
    "start": "2732979",
    "end": "2735849"
  },
  {
    "text": "so the question here is around a hive catalog that may already have within my",
    "start": "2748780",
    "end": "2755210"
  },
  {
    "text": "AWS estate a long-lived hive catalog running off the back of odd yes so this",
    "start": "2755210",
    "end": "2761030"
  },
  {
    "text": "is something that we've had for many years where we said effectively you know when you spin up an EMR cluster you get a local hive meta store when you turn",
    "start": "2761030",
    "end": "2767630"
  },
  {
    "text": "the cluster off that clip that meta store disappears you can externalize it and run it off the back of RDS you can",
    "start": "2767630",
    "end": "2774680"
  },
  {
    "text": "continue using that today and in fact in the github repository that you grabbed the details here we",
    "start": "2774680",
    "end": "2782030"
  },
  {
    "text": "include an import/export tool that will allow you to take existing metadata in your hive meta store and push it into",
    "start": "2782030",
    "end": "2787850"
  },
  {
    "text": "the catalog or export data from the catalog and push it back into your hive meta store I think over time we're going",
    "start": "2787850",
    "end": "2795020"
  },
  {
    "text": "to see glues data catalog becoming you know go-to option for many many customers because it provides high",
    "start": "2795020",
    "end": "2802070"
  },
  {
    "text": "availability it's immediately reusable from rich spectrum from glues ETL jobs",
    "start": "2802070",
    "end": "2807290"
  },
  {
    "text": "from Athena you can use it from hive and spark on EMR today so already it's kind",
    "start": "2807290",
    "end": "2817310"
  },
  {
    "text": "of reusable across the board thank you take one more question and then that's",
    "start": "2817310",
    "end": "2822380"
  },
  {
    "text": "after we'll",
    "start": "2822380",
    "end": "2824829"
  },
  {
    "text": "yeah okay so several questions there",
    "start": "2831320",
    "end": "2843390"
  },
  {
    "text": "I've got some libraries some Python libraries that I've developed how can I keep how can I incorporate them in the jobs can I specify the Python versions can I",
    "start": "2843390",
    "end": "2850350"
  },
  {
    "text": "specify the SPARC motions you can incorporate your own libraries put them in s3 and then when you're creating or",
    "start": "2850350",
    "end": "2856890"
  },
  {
    "text": "scripting a job you can also supply some parameters saying here are some additional libraries that I want to pull",
    "start": "2856890",
    "end": "2862380"
  },
  {
    "text": "down okay so you're effectively just telling it be aware of these libraries pull them into the environment I don't",
    "start": "2862380",
    "end": "2869520"
  },
  {
    "text": "believe you can modify the version of spark I think we're on to one today and",
    "start": "2869520",
    "end": "2876090"
  },
  {
    "text": "I don't think you can affect the version of Python that we're running either at the moment so yeah some pros and cons",
    "start": "2876090",
    "end": "2884760"
  },
  {
    "text": "there you can tune or tweak the amount of processing then we dedicate to a job",
    "start": "2884760",
    "end": "2890070"
  },
  {
    "text": "up front we have this concept of data processing units which is a certain amount of memory and a certain number of",
    "start": "2890070",
    "end": "2896430"
  },
  {
    "text": "cores you can dial that number up or down on a per job basis we don't",
    "start": "2896430",
    "end": "2902670"
  },
  {
    "text": "currently have dynamic auto scaling that may come and that's it so thank you very",
    "start": "2902670",
    "end": "2908520"
  },
  {
    "text": "much [Applause] [Music]",
    "start": "2908520",
    "end": "2912699"
  }
]