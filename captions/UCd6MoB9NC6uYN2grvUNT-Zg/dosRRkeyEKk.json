[
  {
    "text": "hello everyone thank you for coming I can't believe we're all here today that",
    "start": "2210",
    "end": "8610"
  },
  {
    "text": "traffic right it took me forever to get here from the Venetian but I'm glad",
    "start": "8610",
    "end": "14130"
  },
  {
    "text": "you're all here and I think we're all here today because we're building distributed applications we want them to",
    "start": "14130",
    "end": "21150"
  },
  {
    "text": "be reliable and highly available we don't want our customers to be the first ones to let us know when something's",
    "start": "21150",
    "end": "27720"
  },
  {
    "text": "wrong with our service we want to be able to automatically detect and correct errors before customers notice damn time",
    "start": "27720",
    "end": "36530"
  },
  {
    "text": "so I'm not height up at then I'm a senior software engineer with AWS and",
    "start": "36530",
    "end": "41550"
  },
  {
    "text": "I'm going to be presenting with Kevin French Owen co-founder of segment who'll come up later on and I'm going to talk",
    "start": "41550",
    "end": "49140"
  },
  {
    "text": "about some tools and best practices for monitoring and debugging your containerized applications and Calvin",
    "start": "49140",
    "end": "55770"
  },
  {
    "text": "will talk about how they're actually monitoring their ECS services so micro",
    "start": "55770",
    "end": "62789"
  },
  {
    "text": "services are a great way to build resilient and scalable applications however it also makes it really hard to",
    "start": "62789",
    "end": "70140"
  },
  {
    "text": "monitor them because there are so many different interacting components now",
    "start": "70140",
    "end": "76619"
  },
  {
    "text": "suddenly you have all of these services you have load balancers you have instances clusters tasks and when",
    "start": "76619",
    "end": "83520"
  },
  {
    "text": "something happens you have to know where the failure is happening to be able to debug it so you need visibility into all",
    "start": "83520",
    "end": "90780"
  },
  {
    "text": "the different parts containers are also short-lived and transient in nature",
    "start": "90780",
    "end": "96030"
  },
  {
    "text": "which is great for development however it makes it hard to monitor them because",
    "start": "96030",
    "end": "101220"
  },
  {
    "text": "the monitoring tools need to be able to automatically detect new containers as they come up and also grab logs before",
    "start": "101220",
    "end": "108750"
  },
  {
    "text": "they go away so let's say you have a mobile banking",
    "start": "108750",
    "end": "114180"
  },
  {
    "text": "application how do you know when it's healthy or how do you know when",
    "start": "114180",
    "end": "119250"
  },
  {
    "text": "something's failing and how do you figure out where it's failing and when",
    "start": "119250",
    "end": "124290"
  },
  {
    "text": "everything is running successfully how do you know when you need to scale it so your customers don't notice a lag if",
    "start": "124290",
    "end": "130170"
  },
  {
    "text": "there's a sudden spike in call volume so these are some of the questions that",
    "start": "130170",
    "end": "135900"
  },
  {
    "text": "you want to be able to answer to successfully run and operate a production distributed service so let's",
    "start": "135900",
    "end": "144569"
  },
  {
    "text": "talk about some of the key metrics that you want to track so as we talked about",
    "start": "144569",
    "end": "149700"
  },
  {
    "text": "you want to have visibility into all the different parts of your application starting from infrastructure and into",
    "start": "149700",
    "end": "156780"
  },
  {
    "text": "the application level so on the infrastructure level you want to have visibility into your CPU and memory",
    "start": "156780",
    "end": "163080"
  },
  {
    "text": "usage you want to have visibility into your load balancers so for example you",
    "start": "163080",
    "end": "168810"
  },
  {
    "text": "want to know if there are lots of requests being backed up and your application is not able to handle it you",
    "start": "168810",
    "end": "175200"
  },
  {
    "text": "also want to see disk space so for example are your logs eating up all of your disk space and not leaving enough",
    "start": "175200",
    "end": "181290"
  },
  {
    "text": "for your application to run or things like database contention and on the",
    "start": "181290",
    "end": "187530"
  },
  {
    "text": "application layer you want to see if you have error rates and which api's are airing out or you want to see request",
    "start": "187530",
    "end": "194760"
  },
  {
    "text": "volumes per API and you want to be able to break it down per customer so you want to know if your customer one of",
    "start": "194760",
    "end": "201030"
  },
  {
    "text": "your customers is suddenly calling you a lot so you can figure out why you also",
    "start": "201030",
    "end": "206370"
  },
  {
    "text": "want to see call agencies so how long is it call taking and is there a sudden",
    "start": "206370",
    "end": "211799"
  },
  {
    "text": "spike or a drop and that will give you insight into how your application is behaving so fortunately ACS is",
    "start": "211799",
    "end": "220290"
  },
  {
    "text": "integrated by default with Claude watch which is a login metrics dashboarding an alarming system in AWS so if you have",
    "start": "220290",
    "end": "229049"
  },
  {
    "text": "the latest agent version on ECS we automatically send CPU memory",
    "start": "229049",
    "end": "234120"
  },
  {
    "text": "reservation and utilization metrics to cloud watch or you at least have to have agent version one point 4.0 or above and",
    "start": "234120",
    "end": "242870"
  },
  {
    "text": "this is what it looks like so if you go to your ECS console on the metrics tab",
    "start": "242870",
    "end": "249209"
  },
  {
    "text": "you can see CPU and memory reservation and utilization metrics so for example",
    "start": "249209",
    "end": "254910"
  },
  {
    "text": "here you can see the utilization went up and then suddenly you went down and you will have to figure out what's going on",
    "start": "254910",
    "end": "260640"
  },
  {
    "text": "so what these metrics mean is the reservation metric is what you provide",
    "start": "260640",
    "end": "266070"
  },
  {
    "text": "in your task Phoenicia when you say you want this menu cpu and memory shares for your task and the utilization metric shows how",
    "start": "266070",
    "end": "273600"
  },
  {
    "text": "much you're actually using we also break it down by service so this will show you",
    "start": "273600",
    "end": "278750"
  },
  {
    "text": "across your cluster all of your tasks well how much memory and CPU it's using",
    "start": "278750",
    "end": "284220"
  },
  {
    "text": "but what if you want to know for example how much service a is using compared to service B so we also break it down by",
    "start": "284220",
    "end": "291270"
  },
  {
    "text": "service so if you go into the service tab you will see metrics for that and if",
    "start": "291270",
    "end": "296340"
  },
  {
    "text": "you go into the cloud watch console you can actually see more metrics and you",
    "start": "296340",
    "end": "301770"
  },
  {
    "text": "can overlay them and graph them differently or you can zoom into your graphs so you just have more ways to",
    "start": "301770",
    "end": "308340"
  },
  {
    "text": "look at your metrics and figure out what's happening with your service so by",
    "start": "308340",
    "end": "313500"
  },
  {
    "text": "default it's es provides one minute granularity metrics so what that means",
    "start": "313500",
    "end": "318600"
  },
  {
    "text": "is that we collect metrics across all of your cluster instances multiple times",
    "start": "318600",
    "end": "324300"
  },
  {
    "text": "per minute but we only sent it to cloud watch once per minute so if your memory set only size spikes up for 10 seconds",
    "start": "324300",
    "end": "331500"
  },
  {
    "text": "and then it goes down you actually want see it on your graphs so what you might want to do cloud watch now provides put",
    "start": "331500",
    "end": "338310"
  },
  {
    "text": "metric data api's and you can put metric and custom metric data and high resolution metrics into cloud watch so",
    "start": "338310",
    "end": "345330"
  },
  {
    "text": "you can track your application more accurately one cool thing with cloud",
    "start": "345330",
    "end": "351090"
  },
  {
    "text": "watch is that you can create dashboards so before to see metrics from your",
    "start": "351090",
    "end": "358080"
  },
  {
    "text": "different services like ec2 load balancers ECS you would have to go to different pages in cloud watch and see",
    "start": "358080",
    "end": "364890"
  },
  {
    "text": "their metrics on different pages but that makes it really hard to correlate errors and failures so if something's",
    "start": "364890",
    "end": "371670"
  },
  {
    "text": "happening with your CPU utilization you don't actually know if it's your load balancer causing problems or if it's",
    "start": "371670",
    "end": "378060"
  },
  {
    "text": "your application and with dashboards you can put it all in one page and you can easily correlate them another cool thing",
    "start": "378060",
    "end": "385860"
  },
  {
    "text": "that you can do is you can actually overlay these crafts so if you look at the CPU reservation and utilization",
    "start": "385860",
    "end": "392550"
  },
  {
    "text": "metrics the graph shows it to you on the same graph so you can see if utilization",
    "start": "392550",
    "end": "398970"
  },
  {
    "text": "is getting close to the reservation it's much easier to see that if rather than if they were on two separate graphs",
    "start": "398970",
    "end": "404910"
  },
  {
    "text": "or on two separate pages you can also",
    "start": "404910",
    "end": "410660"
  },
  {
    "text": "monitor with our awesome partners like data dog so what data dot provides you is if you",
    "start": "410660",
    "end": "418410"
  },
  {
    "text": "install the agent on your UCS cluster it automatically gives you CPU memory",
    "start": "418410",
    "end": "424940"
  },
  {
    "text": "metrics including usage swamp you get IO reads and writes you get Network metrics you",
    "start": "424940",
    "end": "432090"
  },
  {
    "text": "automatically get metrics around your pending running stop tasks and everything just starts showing up on",
    "start": "432090",
    "end": "438000"
  },
  {
    "text": "your theater dog dashboard another partner that we have is the stick cloud",
    "start": "438000",
    "end": "444440"
  },
  {
    "text": "what's really awesome with Cystic is that you can see your application from an application level so what that means",
    "start": "444440",
    "end": "451260"
  },
  {
    "text": "is that for example with containers if you see CPU and memory metrics that's",
    "start": "451260",
    "end": "457140"
  },
  {
    "text": "not very useful because it's not a one-to-one to an instance anymore you might be running multiple containers on",
    "start": "457140",
    "end": "463020"
  },
  {
    "text": "the same instance so you don't see a breakdown and with Cystic you can actually see a breakdown by task",
    "start": "463020",
    "end": "468750"
  },
  {
    "text": "definition so each task is using this much CPU and this much memory and they also provide you application level",
    "start": "468750",
    "end": "475980"
  },
  {
    "text": "metrics so if you remember we talked about some of the metrics that you want to track like call volumes and error",
    "start": "475980",
    "end": "482310"
  },
  {
    "text": "rates and on the graph here it shows you all of these metrics that says the traps automatically you don't actually have to",
    "start": "482310",
    "end": "488730"
  },
  {
    "text": "do anything and you see top URLs for example or average and max request times",
    "start": "488730",
    "end": "494490"
  },
  {
    "text": "error rates and so it makes it really easy for you to see what's going on in",
    "start": "494490",
    "end": "499830"
  },
  {
    "text": "your application so great now you have metrics and you see that your call",
    "start": "499830",
    "end": "506430"
  },
  {
    "text": "volume went up and you want to know which customer is causing that or your error rate spiked which API is it how do",
    "start": "506430",
    "end": "514440"
  },
  {
    "text": "you figure out what's going on what you want to do is you want to have application logs but one it's awesome",
    "start": "514440",
    "end": "524250"
  },
  {
    "text": "having micro services like we talked about but what's what's really hard with them is now you have your application",
    "start": "524250",
    "end": "529440"
  },
  {
    "text": "scattered across instances and containers you don't want to have to SSH into every instance and grab logs",
    "start": "529440",
    "end": "537240"
  },
  {
    "text": "and try to see where all the errors are happening across instances what you want",
    "start": "537240",
    "end": "542610"
  },
  {
    "text": "to have is you want all of your logs being forwarded to a logging framework or service and just see all of your logs",
    "start": "542610",
    "end": "548640"
  },
  {
    "text": "and once and for central place and be able to search and filter them there so",
    "start": "548640",
    "end": "554040"
  },
  {
    "text": "easier supports multiple log drivers and you could easily configure that in the task definition and I'll show you how so",
    "start": "554040",
    "end": "562380"
  },
  {
    "text": "this is a screenshot of your task definition from the ACS console and under the log configuration you can see",
    "start": "562380",
    "end": "569670"
  },
  {
    "text": "all the different log drivers that ECS currently supports and it's really easy",
    "start": "569670",
    "end": "574710"
  },
  {
    "text": "to configure you just click on a log driver and provide the options to log in",
    "start": "574710",
    "end": "579900"
  },
  {
    "text": "into your logging framework and just forward to your logs there so let's talk about quad watch logs you can forward",
    "start": "579900",
    "end": "588540"
  },
  {
    "text": "your logs to quad watch what the AWS logs driver so cloud watch provider",
    "start": "588540",
    "end": "595260"
  },
  {
    "text": "groups logs into blob groups and that just gives you a nice grouping where you",
    "start": "595260",
    "end": "600960"
  },
  {
    "text": "can have access and retention metrics and search and filter metrics in one place where you just defined it for that",
    "start": "600960",
    "end": "606930"
  },
  {
    "text": "log group and then for different applications you would use different ones potentially depends on your use",
    "start": "606930",
    "end": "613800"
  },
  {
    "text": "case so I mean you have to provide the law group here in the AWS logs group",
    "start": "613800",
    "end": "620700"
  },
  {
    "text": "option that's not created by the log driver for you so you have to go into cloud watch and create that log group",
    "start": "620700",
    "end": "627560"
  },
  {
    "text": "the next option is the region so one thing that you can do is you can forward",
    "start": "627560",
    "end": "633810"
  },
  {
    "text": "all of your logs from different regions into the same cloud watch region so you can see them in one place and just be",
    "start": "633810",
    "end": "640410"
  },
  {
    "text": "able to search and filter and look at your logs in one region and cloud watch or you can decide to send them from us",
    "start": "640410",
    "end": "647580"
  },
  {
    "text": "was to you to us was to include watch or us east one to US east one so that's",
    "start": "647580",
    "end": "652590"
  },
  {
    "text": "what you provide there one really important option is the Stream prefix so",
    "start": "652590",
    "end": "658890"
  },
  {
    "text": "what this is if you don't provide anything by default it just sends it",
    "start": "658890",
    "end": "664560"
  },
  {
    "text": "sends the stream to a container ID stream in your log group so Claude watch groups these logs by",
    "start": "664560",
    "end": "671600"
  },
  {
    "text": "streams so in the law group as you can see here it's just different streams coming into your log group and it's not",
    "start": "671600",
    "end": "677240"
  },
  {
    "text": "very useful because now all of a sudden you have all of these random IDs and you can't correlate which logs that are",
    "start": "677240",
    "end": "683390"
  },
  {
    "text": "coming from where and for example if you have an EC s service and if your container goes down and then comes back",
    "start": "683390",
    "end": "690110"
  },
  {
    "text": "up that's that only it's going to show up into a different stream prefix and you can't really correlate it easily so",
    "start": "690110",
    "end": "696980"
  },
  {
    "text": "what you might want to do is just use the service name as the stream prefix and then everything will just show up",
    "start": "696980",
    "end": "703070"
  },
  {
    "text": "under the same stream so now we have",
    "start": "703070",
    "end": "710840"
  },
  {
    "text": "metrics and we have logs it's great now we can track our application and know",
    "start": "710840",
    "end": "716030"
  },
  {
    "text": "what's going on however you don't want to have someone sitting there and monitoring this 24/7 you want to be able",
    "start": "716030",
    "end": "724220"
  },
  {
    "text": "to just be alarmed you want to page yourself when something's wrong well",
    "start": "724220",
    "end": "730040"
  },
  {
    "text": "maybe you don't want to page yourself but your company will thank you for it",
    "start": "730040",
    "end": "735130"
  },
  {
    "text": "so let's talk about how to do that so in",
    "start": "735130",
    "end": "740210"
  },
  {
    "text": "cloud watch for example you want to alarm when your error rate is high so",
    "start": "740210",
    "end": "745550"
  },
  {
    "text": "you want to say when I have two errors in one minute page me well you can do",
    "start": "745550",
    "end": "751400"
  },
  {
    "text": "that and what you do is you have to create a metric filter and Claude watch logs what that is it's just that some",
    "start": "751400",
    "end": "759260"
  },
  {
    "text": "filter it's a pattern or a word that you provide in your log group and you say here for example I'm looking for access",
    "start": "759260",
    "end": "766400"
  },
  {
    "text": "tonight and if this happens n times per minute please alarm me I want to know about it that's just configured in the",
    "start": "766400",
    "end": "773510"
  },
  {
    "text": "cloud watch logs console you can also auto scale your service and instances",
    "start": "773510",
    "end": "780860"
  },
  {
    "text": "using alarms so what that is is you can define memory and CPU alarms in Cod",
    "start": "780860",
    "end": "786980"
  },
  {
    "text": "watch so you say if my memory went up to 75 percent utilization please alarm me",
    "start": "786980",
    "end": "793430"
  },
  {
    "text": "and also scale my service by this many tasks so now I have more",
    "start": "793430",
    "end": "799250"
  },
  {
    "text": "services available no more instances of my service and the load is more balanced",
    "start": "799250",
    "end": "805130"
  },
  {
    "text": "you can provide this alarm to auto-scaling groups as well and it will",
    "start": "805130",
    "end": "810870"
  },
  {
    "text": "launch a new instance into your auto scaling group into your cluster so now",
    "start": "810870",
    "end": "817260"
  },
  {
    "text": "we have metrics we have logs alarms our service is running great in production",
    "start": "817260",
    "end": "822740"
  },
  {
    "text": "what do we do if we have a new deployment and we a page because it's",
    "start": "822740",
    "end": "828960"
  },
  {
    "text": "failing so your service is now launching tasks or your tasks go into pending and",
    "start": "828960",
    "end": "834690"
  },
  {
    "text": "then they disappear they never go into running how do we figure out what's wrong so the first thing that you want",
    "start": "834690",
    "end": "842880"
  },
  {
    "text": "to do when there's something wrong with your service is you go to the service tab in the ucs console and look under",
    "start": "842880",
    "end": "850110"
  },
  {
    "text": "the events here you can see that the service was not able to launch a task",
    "start": "850110",
    "end": "855380"
  },
  {
    "text": "because there was not enough memory available on your instances so if you had that alarm that automatically",
    "start": "855380",
    "end": "862770"
  },
  {
    "text": "launches a new instance into your cluster your service would be successful but at least you can debug it this way",
    "start": "862770",
    "end": "868820"
  },
  {
    "text": "and for tasks failures if your tasks go from pending to stopped you can go into",
    "start": "868820",
    "end": "875520"
  },
  {
    "text": "the stopped tab under tasks and we provide you with more details around why",
    "start": "875520",
    "end": "880590"
  },
  {
    "text": "the stop the tasks stopped so here you can see that the last status was stopped and the stop reason was that the central",
    "start": "880590",
    "end": "887760"
  },
  {
    "text": "container in task exited that means there's something wrong with the container and if you scroll down under",
    "start": "887760",
    "end": "895160"
  },
  {
    "text": "containers you can see more details around why the container stopped so for",
    "start": "895160",
    "end": "902910"
  },
  {
    "text": "example the image was not able to get pulled from dr. hub or easier and that's",
    "start": "902910",
    "end": "908880"
  },
  {
    "text": "why your container was not able to start so this is great you can see this on the",
    "start": "908880",
    "end": "915300"
  },
  {
    "text": "console but what if you want to alarm on it or you want to search and filter in",
    "start": "915300",
    "end": "920550"
  },
  {
    "text": "one place and not have to go into a console every time there's something wrong well you can do that too",
    "start": "920550",
    "end": "926940"
  },
  {
    "text": "what you need to do is just have the cloud watch logs agent on your ECS instances",
    "start": "926940",
    "end": "932270"
  },
  {
    "text": "and one easy way to install these agents is just do it in the user data in the",
    "start": "932270",
    "end": "937880"
  },
  {
    "text": "ec2 launch configuration so every time a new instance comes up it automatically",
    "start": "937880",
    "end": "942890"
  },
  {
    "text": "launches the agent so you don't have to go and install it every time you have a new instance and then all you have to do",
    "start": "942890",
    "end": "949790"
  },
  {
    "text": "is just configure which logs you want forwarded to cloud watch so you can have instance you can have dr. daemon you can",
    "start": "949790",
    "end": "956480"
  },
  {
    "text": "have ECS agent and ECS init logs and these logs will have more details around your tasks failures and this is what it",
    "start": "956480",
    "end": "964880"
  },
  {
    "text": "will look like in cloud watch once you have that configured it just has your instance doctor and ECS longs so now",
    "start": "964880",
    "end": "974150"
  },
  {
    "text": "let's say we're going back to our mobile banking application everything is running successfully but in the",
    "start": "974150",
    "end": "980690"
  },
  {
    "text": "beginning of the month you notice a huge call spike what's going on so you have",
    "start": "980690",
    "end": "986450"
  },
  {
    "text": "your application logs so you go look into them and you see that people are looking at their pay stubs they want to",
    "start": "986450",
    "end": "992630"
  },
  {
    "text": "see if they got paged not paged paid that's a better one so now you know that",
    "start": "992630",
    "end": "1000310"
  },
  {
    "text": "you need to pre scale so your customers don't notice a lag when they're looking at their pay stubs however you're",
    "start": "1000310",
    "end": "1009340"
  },
  {
    "text": "generating all of this data all of these billions of logs and lines and you don't want to have to manually sort through",
    "start": "1009340",
    "end": "1015790"
  },
  {
    "text": "them that's very tiring and it's not easy to do and you might not notice trends what you can do is you can use",
    "start": "1015790",
    "end": "1024490"
  },
  {
    "text": "search and analytics tools so with cloud watch you can forward your logs to lamda",
    "start": "1024490",
    "end": "1030370"
  },
  {
    "text": "Kinesis and to other applications from there or you can even use elastic search which is a search and filtering tool and",
    "start": "1030370",
    "end": "1038170"
  },
  {
    "text": "you can integrate it with cabana which is a visualization tool that will",
    "start": "1038170",
    "end": "1043540"
  },
  {
    "text": "show you all of these logs and nice graphs so you can easily see what's going on and you can detect trends from",
    "start": "1043540",
    "end": "1052210"
  },
  {
    "text": "there you can also do this with our partners the ones that we talked about",
    "start": "1052210",
    "end": "1057370"
  },
  {
    "text": "or some other partners that we have as they provide lots of analytics and search and filter tools as well so",
    "start": "1057370",
    "end": "1065170"
  },
  {
    "text": "Amazon actually most elasticsearch you don't have to manage it yourself so it's just the open-source tool but we hosted on AWS",
    "start": "1065170",
    "end": "1072310"
  },
  {
    "text": "and from cloud watch logs you just go into the year log group and you say",
    "start": "1072310",
    "end": "1077670"
  },
  {
    "text": "forward my logs to elastic search and I can search on filter from there so what",
    "start": "1077670",
    "end": "1086680"
  },
  {
    "text": "did we learn today for the health of your application it's critical that you",
    "start": "1086680",
    "end": "1092470"
  },
  {
    "text": "have metrics and lungs and you want to have visibility into all the different layers starting from your infrastructure",
    "start": "1092470",
    "end": "1099490"
  },
  {
    "text": "into your application you want to have application logs and alarms you also",
    "start": "1099490",
    "end": "1106150"
  },
  {
    "text": "want to utilize all of this data that your application is generating to detect trends anomalies and patterns and there",
    "start": "1106150",
    "end": "1113620"
  },
  {
    "text": "are different tools that you can use for that thank you I'm going to hand it over",
    "start": "1113620",
    "end": "1119200"
  },
  {
    "text": "to Kelvin to talk about how they're using this yes in segments",
    "start": "1119200",
    "end": "1124570"
  },
  {
    "text": "[Applause]",
    "start": "1124570",
    "end": "1133380"
  },
  {
    "text": "cool is this mic on around here me cool hi everyone i'm calvin co-founder and",
    "start": "1133380",
    "end": "1141130"
  },
  {
    "text": "CTO of segment and for today's talk i was actually inspired by this great",
    "start": "1141130",
    "end": "1148120"
  },
  {
    "text": "Brendan Gregg blog post where he talks about basically his first 60 seconds",
    "start": "1148120",
    "end": "1153280"
  },
  {
    "text": "whenever he logs on to a Linux server and it's not prescriptive anyway it's not telling you what you should do but",
    "start": "1153280",
    "end": "1159580"
  },
  {
    "text": "it shares a lot of the techniques that someone who's a real pro uses and I",
    "start": "1159580",
    "end": "1166059"
  },
  {
    "text": "wanted to take that same idea of sharing like a tool set from someone who's really experienced to answer the",
    "start": "1166059",
    "end": "1172240"
  },
  {
    "text": "question hey if it's 2:00 a.m. I'm getting paged what do I do and share",
    "start": "1172240",
    "end": "1179289"
  },
  {
    "text": "some of the tools that we've found really really useful at segments over the course of our two-and-a-half year",
    "start": "1179289",
    "end": "1185890"
  },
  {
    "text": "history with ECS now can I get a quick show of hands how many of you are using",
    "start": "1185890",
    "end": "1191860"
  },
  {
    "text": "ECS in production today okay it looks like about 30 percent or so that's",
    "start": "1191860",
    "end": "1199360"
  },
  {
    "text": "awesome at segments we are currently processing around 140 billion events",
    "start": "1199360",
    "end": "1204700"
  },
  {
    "text": "every month these are just tracking data that comes in via various web pages for",
    "start": "1204700",
    "end": "1210150"
  },
  {
    "text": "thousands of customers and different web sites across the web we're currently processing about 160,000 events through",
    "start": "1210150",
    "end": "1217809"
  },
  {
    "text": "our Kafka and streaming infrastructure at peak and to do that we're powered by",
    "start": "1217809",
    "end": "1223409"
  },
  {
    "text": "16,000 containers running across 350 CCS services and so by now we've actually",
    "start": "1223409",
    "end": "1232059"
  },
  {
    "text": "gotten to the point where we have a fair amount of sophistication and a fair amount of variety in terms of what",
    "start": "1232059",
    "end": "1237640"
  },
  {
    "text": "services were actually running so when something goes wrong there's a lot of",
    "start": "1237640",
    "end": "1243159"
  },
  {
    "text": "places where you can look and in particular I find there's kind of three major findings or major ideas that we've",
    "start": "1243159",
    "end": "1249730"
  },
  {
    "text": "had when something goes wrong and you're on coal the first is that you want to build your mental model just understand",
    "start": "1249730",
    "end": "1256210"
  },
  {
    "text": "what's going on the second is that you actually want to dig in to the",
    "start": "1256210",
    "end": "1261370"
  },
  {
    "text": "that you're finding look at an individual process level or service level or host level and really",
    "start": "1261370",
    "end": "1267309"
  },
  {
    "text": "understand where things are going wrong and the third is it possible you want to solve the problem and for that we found",
    "start": "1267309",
    "end": "1274210"
  },
  {
    "text": "a variety of ways to really just lean into the cloud and I'll talk more about",
    "start": "1274210",
    "end": "1279250"
  },
  {
    "text": "that shortly but first building your mental model if you're encountering a",
    "start": "1279250",
    "end": "1285640"
  },
  {
    "text": "production issue the first thing that you want to be able to understand whether it's a service you wrote or a",
    "start": "1285640",
    "end": "1291070"
  },
  {
    "text": "service you didn't write or a service that maybe you've never even seen before but somehow you're getting page four you",
    "start": "1291070",
    "end": "1297250"
  },
  {
    "text": "want to know how does this system fit together what's going on how is it configured the second piece is you want",
    "start": "1297250",
    "end": "1304900"
  },
  {
    "text": "to know okay given that broad system how does this individual service fit within that how is it configured and then",
    "start": "1304900",
    "end": "1312790"
  },
  {
    "text": "finally what even should I be looking at what are the major areas so to",
    "start": "1312790",
    "end": "1318160"
  },
  {
    "text": "understand how the system fits together we actually wrote our own tool which sits atop various ECS api's and we call",
    "start": "1318160",
    "end": "1324640"
  },
  {
    "text": "it specs it's open source and put out there for you today so you can just grab it off github or docker hub and just",
    "start": "1324640",
    "end": "1331000"
  },
  {
    "text": "spin up the docker container to run as you choose what specs gives you is a",
    "start": "1331000",
    "end": "1336820"
  },
  {
    "text": "dashboard similar to what you'd find with the ECS dashboard but instead it's",
    "start": "1336820",
    "end": "1341920"
  },
  {
    "text": "maybe a little bit more user-friendly it contains a list of all the services that you're running split up under each",
    "start": "1341920",
    "end": "1348460"
  },
  {
    "text": "individual cluster and for each service it's great because you don't need an AWS",
    "start": "1348460",
    "end": "1354070"
  },
  {
    "text": "account if you're one of the members of that dev team who is in dealing as much with the core infrastructure and second",
    "start": "1354070",
    "end": "1360550"
  },
  {
    "text": "you don't have to page through hundreds or thousands potentially pages when you're looking for a particular services",
    "start": "1360550",
    "end": "1366370"
  },
  {
    "text": "in your account for each spec service you can actually click on it and pull",
    "start": "1366370",
    "end": "1371740"
  },
  {
    "text": "out a tab and understand how that service itself is configured everything down from the CLI or at the command like",
    "start": "1371740",
    "end": "1377650"
  },
  {
    "text": "arguments to the environment variables and for each one you can understand how many copies are running you can see the",
    "start": "1377650",
    "end": "1384370"
  },
  {
    "text": "list of events that NAR I mentioned earlier and you can actually drill down into individual tasks which link back to",
    "start": "1384370",
    "end": "1390550"
  },
  {
    "text": "the actual Amazon dashboard if you'd like to get a sense of what individual tasks are running like I",
    "start": "1390550",
    "end": "1397279"
  },
  {
    "text": "said you can actually just grab this today docker run segment specs or you can check it out on github if you'd like",
    "start": "1397279",
    "end": "1403840"
  },
  {
    "text": "it basically requires one IM role that's able to read from the e CS api and then",
    "start": "1403840",
    "end": "1410600"
  },
  {
    "text": "a handful permissions and you're kind of off to the races and you can run this as an internal tool or elsewhere depending",
    "start": "1410600",
    "end": "1416240"
  },
  {
    "text": "on what you need so if I'm looking at to understand how my service is configured",
    "start": "1416240",
    "end": "1421640"
  },
  {
    "text": "I can use specs but a lot of times that configuration changes over time and",
    "start": "1421640",
    "end": "1427000"
  },
  {
    "text": "actually in my experience on call the biggest the single biggest question that",
    "start": "1427000",
    "end": "1432980"
  },
  {
    "text": "I end up asking whenever something goes wrong is what changed recently and so",
    "start": "1432980",
    "end": "1439850"
  },
  {
    "text": "for that we use terraform now for those of you who are unfamiliar terraform",
    "start": "1439850",
    "end": "1445460"
  },
  {
    "text": "gives you this configuration configuration language that's fairly similar to cloud formation it allows you",
    "start": "1445460",
    "end": "1451399"
  },
  {
    "text": "to build individual modules to boot up a bunch of different pieces of infrastructure underneath so we use",
    "start": "1451399",
    "end": "1457850"
  },
  {
    "text": "terraform to create all of our services here you can see this is actually the terraform for specs in kind of a meadow",
    "start": "1457850",
    "end": "1464090"
  },
  {
    "text": "way and what this does under the hood is actually generate this de giant dependency graph of load balancers",
    "start": "1464090",
    "end": "1471549"
  },
  {
    "text": "services auto scaling rules network security groups",
    "start": "1471549",
    "end": "1476690"
  },
  {
    "text": "I am rolls it creates it all for you but as a developer you don't even have to",
    "start": "1476690",
    "end": "1483110"
  },
  {
    "text": "think about it you get this like 15 line configuration file and then you can say hey terraform I want you to create my",
    "start": "1483110",
    "end": "1489679"
  },
  {
    "text": "service for me just let me give you the image in the version I don't want to run",
    "start": "1489679",
    "end": "1496120"
  },
  {
    "text": "and in terms of actually interacting with that we keep terraform all entirely",
    "start": "1496330",
    "end": "1501710"
  },
  {
    "text": "version control so here you can see my teammate Jeremy actually swapping out the usage of our various TCS instances",
    "start": "1501710",
    "end": "1508909"
  },
  {
    "text": "we're testing out some of the new c5 instances the beauty of this is it actually just requires a two line change",
    "start": "1508909",
    "end": "1516019"
  },
  {
    "text": "and for in order for us to actually start testing this new instance type in production we attested in stage first",
    "start": "1516019",
    "end": "1522950"
  },
  {
    "text": "which we keep an environment for then roll it out to production it requires no manual scripts on our",
    "start": "1522950",
    "end": "1528430"
  },
  {
    "text": "side no craziness in terms of reprovision just this two line change and suddenly",
    "start": "1528430",
    "end": "1534940"
  },
  {
    "text": "we're able to start changing vast parts of our infrastructure without any work",
    "start": "1534940",
    "end": "1540630"
  },
  {
    "text": "to actually execute these plans you end up merging into github into master and",
    "start": "1540630",
    "end": "1546790"
  },
  {
    "text": "then you run terraform apply through terraform Enterprise which is a hashey core product the reason we decided to",
    "start": "1546790",
    "end": "1553390"
  },
  {
    "text": "use terraform Enterprise is so that we can actually see when changes were made and out at them so if for instance we",
    "start": "1553390",
    "end": "1561160"
  },
  {
    "text": "change to a new instance type or deploy a new version of the service we know exactly when that change went out who",
    "start": "1561160",
    "end": "1567760"
  },
  {
    "text": "confirmed and applied it and why that change was made in a general having this",
    "start": "1567760",
    "end": "1573520"
  },
  {
    "text": "version history along with a set of commit messages associated with it has allowed us to dig back over months or",
    "start": "1573520",
    "end": "1580330"
  },
  {
    "text": "even years to understand why a given change was made and finally what even should I be",
    "start": "1580330",
    "end": "1588670"
  },
  {
    "text": "looking at we understand our system at a high level we understand how little pieces of it are configured for that we",
    "start": "1588670",
    "end": "1595960"
  },
  {
    "text": "use Amazon's Cloud wash and data dog which I'll actually touch on a little bit later in this talk as well and for",
    "start": "1595960",
    "end": "1602440"
  },
  {
    "text": "data dog this is kind of our central reporting interface for all of our metrics each individual service gets its",
    "start": "1602440",
    "end": "1609610"
  },
  {
    "text": "own dashboard that dashboard combines CloudWatch metrics along with stats D metrics that were actually piping into",
    "start": "1609610",
    "end": "1615700"
  },
  {
    "text": "the service itself and we're using it to monitor both the AWS resources things",
    "start": "1615700",
    "end": "1622300"
  },
  {
    "text": "like RDS instances cloud watch logs s3",
    "start": "1622300",
    "end": "1627340"
  },
  {
    "text": "buckets etc and tie those metrics with our actual active plication specific metrics which are coming from stats D",
    "start": "1627340",
    "end": "1634770"
  },
  {
    "text": "and what's great about this tool is actually it will also paying into things like slack so if you have a member of",
    "start": "1634770",
    "end": "1641140"
  },
  {
    "text": "your team in this case Julien who's on call you can get kind of this conversation going where people are",
    "start": "1641140",
    "end": "1647440"
  },
  {
    "text": "responding to alerts in real time but able to discuss them in the tool they're already using slack",
    "start": "1647440",
    "end": "1654180"
  },
  {
    "text": "so we've got our mental model all set up now the next step is we actually have to",
    "start": "1655270",
    "end": "1661270"
  },
  {
    "text": "dig in and understand what's going on with each of these services and for that",
    "start": "1661270",
    "end": "1666670"
  },
  {
    "text": "there are basically three pieces that we need we need to make sure that we have good stats where the service is",
    "start": "1666670",
    "end": "1672010"
  },
  {
    "text": "reporting what it's actually doing where it's spending time we need to have good logging to understand a more human",
    "start": "1672010",
    "end": "1679150"
  },
  {
    "text": "readable level kind of the progression or execution of various events that the service is completing and finally we",
    "start": "1679150",
    "end": "1686080"
  },
  {
    "text": "need to have a good sense of the tracing for the service where is it actually spending time right now for the first",
    "start": "1686080",
    "end": "1694330"
  },
  {
    "text": "one we use cloud watch and a combination of stats D in particular this open",
    "start": "1694330",
    "end": "1699970"
  },
  {
    "text": "source program written by stripe called Vanir Vanir is essentially a stats",
    "start": "1699970",
    "end": "1705970"
  },
  {
    "text": "desync that's fully compatible with the data dog format so if you're already recording stats D metrics from your",
    "start": "1705970",
    "end": "1712300"
  },
  {
    "text": "problem from your program you can simply run veneer on the machine and then pass in it's addressed as configuration how",
    "start": "1712300",
    "end": "1720160"
  },
  {
    "text": "this looks for us is we run veneer via system D basically we just run this",
    "start": "1720160",
    "end": "1725260"
  },
  {
    "text": "simple go binary we don't have to do any sort of interpretation or compiling or anything on the box and for our programs",
    "start": "1725260",
    "end": "1733179"
  },
  {
    "text": "we'll actually specify and say hey I want to pass in this stats D or this data dog actress as the address of the",
    "start": "1733179",
    "end": "1740140"
  },
  {
    "text": "new nurse server when proxy through the doc docker Damon and kind of the beauty",
    "start": "1740140",
    "end": "1745840"
  },
  {
    "text": "that this gives us is allows each of our individual services to send their stats and metrics to veneer or be passively",
    "start": "1745840",
    "end": "1753490"
  },
  {
    "text": "collected by the data agent which is recording metrics on the host and from there we're actually able to combine",
    "start": "1753490",
    "end": "1759040"
  },
  {
    "text": "them with all of these metrics that we're getting absolutely for free for things like load balancers RDS etc from",
    "start": "1759040",
    "end": "1766630"
  },
  {
    "text": "cloud watch and combine those in data dog what it means is that if I'm a",
    "start": "1766630",
    "end": "1772870"
  },
  {
    "text": "developer I can record really custom metrics that are important to my particular programs execution but I can",
    "start": "1772870",
    "end": "1781600"
  },
  {
    "text": "also combine it with all of the things that AWS automatically gives me",
    "start": "1781600",
    "end": "1787540"
  },
  {
    "text": "and if I want to I can grab things like not only system load off the agents but any of the AWS automatically reported",
    "start": "1787540",
    "end": "1794500"
  },
  {
    "text": "metrics so we've got stats cloud watch",
    "start": "1794500",
    "end": "1800020"
  },
  {
    "text": "stats D data dog the second piece is logging and for that we actually use a",
    "start": "1800020",
    "end": "1807070"
  },
  {
    "text": "reasonably complicated setup of cloud watch this program called ECS logs and CW logs now for a high-level look at how",
    "start": "1807070",
    "end": "1816610"
  },
  {
    "text": "our logging works basically and I think this is essentially how it worked maybe three or four months ago we have a bunch",
    "start": "1816610",
    "end": "1823480"
  },
  {
    "text": "of containers all which are logging via standard ad those containers send their",
    "start": "1823480",
    "end": "1829060"
  },
  {
    "text": "logs directly to the docker daemon which is configured to send to journal D journal v itself is something that we",
    "start": "1829060",
    "end": "1835780"
  },
  {
    "text": "prefer because it allows us to jump on to an instance and actually look at both the system logs which are piping the",
    "start": "1835780",
    "end": "1841420"
  },
  {
    "text": "journal d as well as our individual docker logs we're not quite at the point",
    "start": "1841420",
    "end": "1847000"
  },
  {
    "text": "of sophistication we're jumping on to an instance is something that's never useful so for us it's helpful to have",
    "start": "1847000",
    "end": "1853300"
  },
  {
    "text": "all of those jobs all of those logs be accessible will be a common interface",
    "start": "1853300",
    "end": "1858840"
  },
  {
    "text": "from there we actually pipe journal the data out into this proxy or sorry not",
    "start": "1858840",
    "end": "1865870"
  },
  {
    "text": "proxy this adapter called ECS logs and ECS logs is just a go binary which",
    "start": "1865870",
    "end": "1871780"
  },
  {
    "text": "basically tails journal D and it looks for individual journal d entries which are tagged with a container ID for ECS",
    "start": "1871780",
    "end": "1878890"
  },
  {
    "text": "logs from there it will send those to any of the supported adaptors log Li Cloud watch log DNA you name it and it",
    "start": "1878890",
    "end": "1888070"
  },
  {
    "text": "allows us to disintermediate the actual logging step of the program is that the application is doing from the places",
    "start": "1888070",
    "end": "1894730"
  },
  {
    "text": "where the logs are actually used and read and this works great for a while",
    "start": "1894730",
    "end": "1900300"
  },
  {
    "text": "but after some time we actually ran into a little bit of problem with journal v and in particular it comes down to how",
    "start": "1900300",
    "end": "1908080"
  },
  {
    "text": "general D handles rate limits for individual processes and individual containers in particular you can specify",
    "start": "1908080",
    "end": "1915550"
  },
  {
    "text": "in your journal D file up as you go to rate lemon in this case we can say hey",
    "start": "1915550",
    "end": "1920929"
  },
  {
    "text": "every 200,000 loglines that happen once a minute stop blogging after that you're",
    "start": "1920929",
    "end": "1926599"
  },
  {
    "text": "done but what happens is Journal D actually treats that limit on a per",
    "start": "1926599",
    "end": "1934669"
  },
  {
    "text": "process basis and in terms of logging it's all coming from the dr. demian so",
    "start": "1934669",
    "end": "1941359"
  },
  {
    "text": "what happens is you have one really hot container which is just spewing logs for some reason maybe it's crashing maybe",
    "start": "1941359",
    "end": "1947809"
  },
  {
    "text": "it's trying to connect to a database that's not there whatever it is and that actually uses up the overall rate limit",
    "start": "1947809",
    "end": "1955039"
  },
  {
    "text": "for all of your containers on ECS not great so what we did is we introduced",
    "start": "1955039",
    "end": "1963909"
  },
  {
    "text": "something that's effectively a syslog proxy or a syslog server which sits in",
    "start": "1963909",
    "end": "1970700"
  },
  {
    "text": "front of journal D we configure the docker daemon to forward its logs to",
    "start": "1970700",
    "end": "1975889"
  },
  {
    "text": "what we call this rate limiting rot log proxy it's maybe not the most imaginative name but it is accurate and",
    "start": "1975889",
    "end": "1982749"
  },
  {
    "text": "what this does is actually treats each eight containers individual stream differently so what it can do is it can",
    "start": "1982749",
    "end": "1989839"
  },
  {
    "text": "rate limit only a single container at a time based upon the tags that are being passed from the docker daemon in",
    "start": "1989839",
    "end": "1998960"
  },
  {
    "text": "addition we said okay we still really like cloud watch why don't we make it easier to use cloud watch logs in a",
    "start": "1998960",
    "end": "2007509"
  },
  {
    "text": "particular we built this tool called CW logs which you can see up here what this",
    "start": "2007509",
    "end": "2012879"
  },
  {
    "text": "does is it groups are individual log streams based upon the easy ECS service so we create a group still per service",
    "start": "2012879",
    "end": "2020049"
  },
  {
    "text": "and then we have a list of tasks which we can access all of you in the command line if we'd like to we can fit fetched",
    "start": "2020049",
    "end": "2027940"
  },
  {
    "text": "logs for all of the tasks within that service so here we're grabbing all of",
    "start": "2027940",
    "end": "2032950"
  },
  {
    "text": "the logs for dynamo writer you can see there nicely outputs and they're prefixed with kind of a shorter version",
    "start": "2032950",
    "end": "2039639"
  },
  {
    "text": "of each of their individual task errands but if we'd like we can only look at a",
    "start": "2039639",
    "end": "2046899"
  },
  {
    "text": "single stream as well bypassing the stash te flag which actually allows us to grab only a single tasks out",
    "start": "2046899",
    "end": "2053679"
  },
  {
    "text": "here we can see for this given container exactly what it's writing in",
    "start": "2053679",
    "end": "2059840"
  },
  {
    "text": "additionally we can also output this data as JSON so if we'd like to we could pass it to JQ or some other formatter if",
    "start": "2059840",
    "end": "2067129"
  },
  {
    "text": "we'd like to programmatically access those logs or if we'd like to programmatically filter them somehow",
    "start": "2067130",
    "end": "2074830"
  },
  {
    "text": "like I said at the beginning of the talk all of these pieces are open-source and",
    "start": "2074830",
    "end": "2080389"
  },
  {
    "text": "up on github today we use our ECS logs proxy to actually fan out that docker",
    "start": "2080390",
    "end": "2086330"
  },
  {
    "text": "data and send it or start fan-out that log data and send it to cloud watch log",
    "start": "2086330",
    "end": "2091550"
  },
  {
    "text": "lea log DNA wherever we need it to go we use our go in JavaScript libraries to",
    "start": "2091550",
    "end": "2097129"
  },
  {
    "text": "actually create those logs in a way that's machine readable and nicely formatted we use our rate limiting log",
    "start": "2097130",
    "end": "2104570"
  },
  {
    "text": "proxy which you can just download and run as a docker container which will automatically limit for logs which will",
    "start": "2104570",
    "end": "2112490"
  },
  {
    "text": "automatically limit containers that are sent to journal D and finally we use CW logs as our sort of Swiss Army knife CLI",
    "start": "2112490",
    "end": "2119450"
  },
  {
    "text": "for accessing these logs and I'd say in general with CW logs and CLI is in",
    "start": "2119450",
    "end": "2124610"
  },
  {
    "text": "general no matter how fancy your nice your UI gets as programmers we've kind",
    "start": "2124610",
    "end": "2130160"
  },
  {
    "text": "of found no better substitute for dealing with a large amount of log data then your command-line tools stuff like",
    "start": "2130160",
    "end": "2137450"
  },
  {
    "text": "grep said och whatever it is being able to use those logs from the command line",
    "start": "2137450",
    "end": "2143180"
  },
  {
    "text": "is immensely powerful but getting the cheap audit logging and cheap tailing",
    "start": "2143180",
    "end": "2148520"
  },
  {
    "text": "cases from cloud watch is it just a great combination in terms of offloading",
    "start": "2148520",
    "end": "2154340"
  },
  {
    "text": "the infrastructure that we have to manage so finally in terms of tracing",
    "start": "2154340",
    "end": "2160660"
  },
  {
    "text": "the two tools that we use most often here are BC C and P prop server now BCC",
    "start": "2160660",
    "end": "2169130"
  },
  {
    "text": "itself stands for BPF compiler collection and BP f stands for Berkley",
    "start": "2169130",
    "end": "2176180"
  },
  {
    "text": "packet filter it's a little bit of this weirdly named kernel feature which was",
    "start": "2176180",
    "end": "2181970"
  },
  {
    "text": "recently added to recent Abood to kernels and recent Linux kernels where you can think",
    "start": "2181970",
    "end": "2187520"
  },
  {
    "text": "it is almost a VM that works inside the kernel so if you'd like to you can",
    "start": "2187520",
    "end": "2193460"
  },
  {
    "text": "actually instrument and understand what very assist calls are being made you can understand disk usage you can understand",
    "start": "2193460",
    "end": "2200600"
  },
  {
    "text": "a lot of parts of how your program is running passively without you needing to",
    "start": "2200600",
    "end": "2206330"
  },
  {
    "text": "add any instrument instrumentation to your program and a particular these",
    "start": "2206330",
    "end": "2212090"
  },
  {
    "text": "tools are all kind of put up at this IO visor BCC github repo where they kind of",
    "start": "2212090",
    "end": "2219830"
  },
  {
    "text": "come prepackaged so as long as you download them and run them on your machine you don't have to do anything",
    "start": "2219830",
    "end": "2225860"
  },
  {
    "text": "you just get these metrics for free to understand what's going on so as a quick",
    "start": "2225860",
    "end": "2231920"
  },
  {
    "text": "example we can see here from one of our Kafka boxes which is basically a bunch of Java processes which are writing to a",
    "start": "2231920",
    "end": "2238550"
  },
  {
    "text": "rated XFS drive we can see that here all these Kafka processes are doing a ton of",
    "start": "2238550",
    "end": "2245480"
  },
  {
    "text": "writes at any given time and we can know exactly on a per process basis kind of",
    "start": "2245480",
    "end": "2250760"
  },
  {
    "text": "how much time is spent writing how much time is spent reading exactly what that process is doing even though we didn't",
    "start": "2250760",
    "end": "2258410"
  },
  {
    "text": "write any part of Kafka we have an instrumented it in anyway we're just using these metrics that are given to us",
    "start": "2258410",
    "end": "2263960"
  },
  {
    "text": "for free by the kernel similarly we can actually",
    "start": "2263960",
    "end": "2269900"
  },
  {
    "text": "check the read time and write time for this X of s Drive and understand kind of a histogram showing the distribution of",
    "start": "2269900",
    "end": "2276950"
  },
  {
    "text": "how long it's actually taking us to read or write if for some reason we start shifting this write load down into maybe",
    "start": "2276950",
    "end": "2285230"
  },
  {
    "text": "bigger and bigger amounts of time we don't understand that okay we're actually IO or disk bound and it's not a",
    "start": "2285230",
    "end": "2291080"
  },
  {
    "text": "fault of the program itself on the theme of tracing without having to add",
    "start": "2291080",
    "end": "2297740"
  },
  {
    "text": "anything to your code the other big peep tool that we use often at segments is",
    "start": "2297740",
    "end": "2303140"
  },
  {
    "text": "what we call p prof server and this is actually one of my favorite tools that my coworker ashole wrote that i've been",
    "start": "2303140",
    "end": "2311090"
  },
  {
    "text": "using over the past two months or so now for those of you who use go you might",
    "start": "2311090",
    "end": "2316130"
  },
  {
    "text": "already familiar with p prof it's essentially this library that you can import",
    "start": "2316130",
    "end": "2321250"
  },
  {
    "text": "you just add this kind of single line to your go program saying hey I want to listen on the server and expose these",
    "start": "2321250",
    "end": "2326440"
  },
  {
    "text": "metrics it'll actually automatically give you stuff like profiling heap dumps",
    "start": "2326440",
    "end": "2333280"
  },
  {
    "text": "of where various go routines are it will tell you where various memories being spent where there's contention on",
    "start": "2333280",
    "end": "2339970"
  },
  {
    "text": "various mutexes etc and so we thought wow there's this amazing tool which is",
    "start": "2339970",
    "end": "2345940"
  },
  {
    "text": "just given to you basically out of the box you added a single line to your go program what if we made it easy for",
    "start": "2345940",
    "end": "2351910"
  },
  {
    "text": "developers to actually access that data in real time for a production account",
    "start": "2351910",
    "end": "2357520"
  },
  {
    "text": "and that's what P prop server does basically each service when it comes up",
    "start": "2357520",
    "end": "2364260"
  },
  {
    "text": "registers itself within console which we use this kind of our shared service discovery platform and when it comes up",
    "start": "2364260",
    "end": "2372580"
  },
  {
    "text": "in console will expose the URL as well as a set of links that you can just",
    "start": "2372580",
    "end": "2378340"
  },
  {
    "text": "click through and it can tell you in real time exactly where time in your",
    "start": "2378340",
    "end": "2384370"
  },
  {
    "text": "program is being spent it generates this nice SVG graphic telling you hey you've",
    "start": "2384370",
    "end": "2389830"
  },
  {
    "text": "got a certain amount of time being spent here of that there's maybe one second being sent here one millisecond being",
    "start": "2389830",
    "end": "2396010"
  },
  {
    "text": "sent here it allows you to get really fine-grained observability into what",
    "start": "2396010",
    "end": "2401590"
  },
  {
    "text": "exactly your program is doing like I said earlier you can get heap dumps for",
    "start": "2401590",
    "end": "2407080"
  },
  {
    "text": "it understand where your memory is being allocated and this tool has essentially",
    "start": "2407080",
    "end": "2412870"
  },
  {
    "text": "been invaluable for us when it comes to hunting down contention of various locks or mutexes where one single bottleneck",
    "start": "2412870",
    "end": "2421030"
  },
  {
    "text": "of your program is essentially slowing everything down just like specs just",
    "start": "2421030",
    "end": "2426850"
  },
  {
    "text": "like the log proxy you can run this yourself the only thing that you need to",
    "start": "2426850",
    "end": "2431860"
  },
  {
    "text": "pass it is the registry of the console address and from there you're off to the races you just connect to this service",
    "start": "2431860",
    "end": "2438670"
  },
  {
    "text": "and you can actually start understanding where your programs are spending time in real time so those were the three of",
    "start": "2438670",
    "end": "2448720"
  },
  {
    "text": "really digging in and understanding what's going on with an individual program we've got our host level metrics",
    "start": "2448720",
    "end": "2454270"
  },
  {
    "text": "that working from data dog we've got our individual process metrics that we're collecting",
    "start": "2454270",
    "end": "2459339"
  },
  {
    "text": "from stats D we've got our automatically defined kernel metrics that we're",
    "start": "2459339",
    "end": "2465819"
  },
  {
    "text": "getting from BCC we've got our going metrics that we're getting from P prof",
    "start": "2465819",
    "end": "2471119"
  },
  {
    "text": "the last step is actually fixing the problem and the best luck we've had",
    "start": "2471119",
    "end": "2476709"
  },
  {
    "text": "there at segment is really leaning into the cloud now what do I mean by that",
    "start": "2476709",
    "end": "2483359"
  },
  {
    "text": "well Adrienne Cockroft has this really famous saying that you want to treat",
    "start": "2483359",
    "end": "2489130"
  },
  {
    "text": "your servers like cattle and not pets right you want to be able to just take one down and make sure that another can",
    "start": "2489130",
    "end": "2496479"
  },
  {
    "text": "come up in its place there's nothing really special about it and in our case that comes down to a few",
    "start": "2496479",
    "end": "2504299"
  },
  {
    "text": "big tenants but I'd say the biggest ones by far are one kind of reproducible",
    "start": "2504299",
    "end": "2512229"
  },
  {
    "text": "machine images the ability to boot up a em eyes and be 100% confident that your",
    "start": "2512229",
    "end": "2518799"
  },
  {
    "text": "machine is going to have everything it needs on it pre-baked and it won't have to go out to the network to grab a bunch",
    "start": "2518799",
    "end": "2525069"
  },
  {
    "text": "of resources and won't have to install a bunch of things when it comes up that ami has everything it needs for that we",
    "start": "2525069",
    "end": "2535419"
  },
  {
    "text": "use Packer which is another hashing core product and what it lets us do is actually provision that machine via JSON",
    "start": "2535419",
    "end": "2543759"
  },
  {
    "text": "config file and then it's run via system D autocorrect kind of got me here and",
    "start": "2543759",
    "end": "2550479"
  },
  {
    "text": "the second of those is actually using Auto the out-of-the-box auto scaling we want to make sure that for every",
    "start": "2550479",
    "end": "2556449"
  },
  {
    "text": "instance it's part of an auto scaling group if one machine goes bad we don't",
    "start": "2556449",
    "end": "2561640"
  },
  {
    "text": "care we just terminate that machine and we're confident that other containers will come up with that machine another",
    "start": "2561640",
    "end": "2568089"
  },
  {
    "text": "machine will boot in its place it kind of takes care of all the hassle for us well this actually looks like in",
    "start": "2568089",
    "end": "2574839"
  },
  {
    "text": "practice is we keep a github repo that we call ec2 images and here you can see",
    "start": "2574839",
    "end": "2580659"
  },
  {
    "text": "all of the different a.m. is that we're actually using depending on which cluster we're booting in and the reason",
    "start": "2580659",
    "end": "2587319"
  },
  {
    "text": "we pick differ am i spur coaster is actually because some clusters have slightly different needs some of them are running stateless",
    "start": "2587319",
    "end": "2595630"
  },
  {
    "text": "services just kind of this big one that we call mega pool just sort of the general place where you're booting a",
    "start": "2595630",
    "end": "2601390"
  },
  {
    "text": "service you put your service definition some of them are running Kafka with like really heavy right workloads where",
    "start": "2601390",
    "end": "2608019"
  },
  {
    "text": "they're totally stateful some of them are maybe somewhere in between where maybe they're running one-off tasks",
    "start": "2608019",
    "end": "2613150"
  },
  {
    "text": "whatever it is if you look at the Packer file what this does is actually provides",
    "start": "2613150",
    "end": "2619630"
  },
  {
    "text": "you a programmatic way as well to provision your service so you can say hey I want to give this",
    "start": "2619630",
    "end": "2626109"
  },
  {
    "text": "service create this instance with this set of configuration I want to create it with this set of scripts and it will",
    "start": "2626109",
    "end": "2632680"
  },
  {
    "text": "actually boot up an instance for you run all those scripts create the ami out of",
    "start": "2632680",
    "end": "2638680"
  },
  {
    "text": "it and then output that little ami ID that you can use elsewhere when we look",
    "start": "2638680",
    "end": "2646059"
  },
  {
    "text": "at what these scripts look like we basically keep a single one per each of the programs that are automatically",
    "start": "2646059",
    "end": "2651309"
  },
  {
    "text": "installed on the machine and we just run through them sequentially that way we",
    "start": "2651309",
    "end": "2656739"
  },
  {
    "text": "know when the instance actually boots up we're not getting some other version of a service that we specified or some",
    "start": "2656739",
    "end": "2663309"
  },
  {
    "text": "other piece of configuration we're getting exactly what we asked for when we built the image reproducible builds",
    "start": "2663309",
    "end": "2673150"
  },
  {
    "text": "are one thing the other thing is having auto scaling everywhere as I mentioned",
    "start": "2673150",
    "end": "2679029"
  },
  {
    "text": "before auto scaling for us comes by default with every service when you",
    "start": "2679029",
    "end": "2684519"
  },
  {
    "text": "specify that little piece of terraform configuration you automatically get auto scaling rules in response to CPU and",
    "start": "2684519",
    "end": "2691599"
  },
  {
    "text": "memory and in places where we don't have that things like dynamo we put it into",
    "start": "2691599",
    "end": "2697480"
  },
  {
    "text": "place to make sure that we're not having production incidents all the time I'd say the general rule that we've kind of",
    "start": "2697480",
    "end": "2703180"
  },
  {
    "text": "come to accept over the course of building these systems is that if you don't have auto scaling in place",
    "start": "2703180",
    "end": "2709239"
  },
  {
    "text": "you're really not ready for prod auto scaling allows you as the developer to",
    "start": "2709239",
    "end": "2715119"
  },
  {
    "text": "not even really think about stuff like how much CPU or memory your program is using because you can be assured that",
    "start": "2715119",
    "end": "2722270"
  },
  {
    "text": "if say something happens where we get a big spike in terms of load you'll just create more and more of that set of",
    "start": "2722270",
    "end": "2728450"
  },
  {
    "text": "programs and you don't have to worry at all because your service will scale horizontally what this looks like",
    "start": "2728450",
    "end": "2737240"
  },
  {
    "text": "concretely as we keep rules around for scaling in regards to CPU and memory if",
    "start": "2737240",
    "end": "2743320"
  },
  {
    "text": "CPU goes up by a certain percentage scale up if memory goes up by a certain",
    "start": "2743320",
    "end": "2748760"
  },
  {
    "text": "percentage scale up in general what we found is this works really well so long",
    "start": "2748760",
    "end": "2755270"
  },
  {
    "text": "as you don't have contention as I mentioned earlier if you have some place where you have a shared mutex or like",
    "start": "2755270",
    "end": "2760820"
  },
  {
    "text": "someplace where you're not actually taking full advantage of the CPU these rules won't work for you so it's worth",
    "start": "2760820",
    "end": "2767990"
  },
  {
    "text": "doing a bunch of those other things first before you actually start scaling via CPU though we've also talked about",
    "start": "2767990",
    "end": "2773839"
  },
  {
    "text": "things like scaling and response to queue depth requests load etc so in",
    "start": "2773839",
    "end": "2780800"
  },
  {
    "text": "closing a few of the lessons we've learned you want to build tools to surface the actual information or",
    "start": "2780800",
    "end": "2787609"
  },
  {
    "text": "actionable information first if you've got a bunch of noise that's sitting around or a bunch of alerts that aren't",
    "start": "2787609",
    "end": "2793700"
  },
  {
    "text": "really actionable or aren't really doing anything you're just causing more of this kind of sea of information to crop",
    "start": "2793700",
    "end": "2799700"
  },
  {
    "text": "up which people then have to wade through and understand auto-scaling is a",
    "start": "2799700",
    "end": "2806570"
  },
  {
    "text": "huge win before we would manually scale up or over provision and it lasts us",
    "start": "2806570",
    "end": "2812000"
  },
  {
    "text": "about six months and tell if we can hit our next inflection point and then we have to scale up again by auto scaling",
    "start": "2812000",
    "end": "2818119"
  },
  {
    "text": "we've been able to boot more services earlier because we don't have to worry",
    "start": "2818119",
    "end": "2823250"
  },
  {
    "text": "about a new service failing to scale and then bringing down the entire pipeline on that note give developers learning",
    "start": "2823250",
    "end": "2831349"
  },
  {
    "text": "and scaling policies out of the box don't make it so you have to pass a bunch of configuration every time you",
    "start": "2831349",
    "end": "2837410"
  },
  {
    "text": "want to boot a new service just make say in defaults there for everyone and from",
    "start": "2837410",
    "end": "2843349"
  },
  {
    "text": "there you can actually allow customizable tweaks and then finally passive tools are some of the easiest",
    "start": "2843349",
    "end": "2850280"
  },
  {
    "text": "that we found to builds adoption around if you don't have to import other libraries",
    "start": "2850280",
    "end": "2856020"
  },
  {
    "text": "you don't have to add a bunch of configuration you don't have to do things like add tracing points to all of",
    "start": "2856020",
    "end": "2862980"
  },
  {
    "text": "your code if you get all that for free from tools like BCC or tools like specs",
    "start": "2862980",
    "end": "2869310"
  },
  {
    "text": "or tools like our login configuration then nine times out of ten you're just",
    "start": "2869310",
    "end": "2876120"
  },
  {
    "text": "going to do the easy thing which is also the right thing and you're going to use these tools out of the box and make sure",
    "start": "2876120",
    "end": "2883350"
  },
  {
    "text": "that they actually just work for you and with that I'd like to thank you for your",
    "start": "2883350",
    "end": "2889440"
  },
  {
    "text": "time and thank you now right thanks [Applause]",
    "start": "2889440",
    "end": "2896219"
  }
]