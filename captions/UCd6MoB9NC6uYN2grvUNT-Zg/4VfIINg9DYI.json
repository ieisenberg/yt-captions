[
  {
    "start": "0",
    "end": "76000"
  },
  {
    "text": "good morning everyone welcome so we're",
    "start": "230",
    "end": "5700"
  },
  {
    "text": "in the middle of a kind of exciting yet terrifying time in my opinion not only",
    "start": "5700",
    "end": "10800"
  },
  {
    "text": "are we seeing a huge move towards mobile and connected devices Internet of Things but simultaneously we've got this huge",
    "start": "10800",
    "end": "17940"
  },
  {
    "text": "thirst for just real-time data whether or not you're talking about metrics from you know home automation or online apps",
    "start": "17940",
    "end": "27060"
  },
  {
    "text": "or you know games chat applications or just an overwhelming influx both in",
    "start": "27060",
    "end": "32398"
  },
  {
    "text": "speed and quantity of data and also more and more need for deep analytics and all",
    "start": "32399",
    "end": "39059"
  },
  {
    "text": "of this so all of these factors combined really necessitate having a good",
    "start": "39059",
    "end": "44190"
  },
  {
    "text": "in-memory data strategy and that's we're going to talk about today so just quick level set elastic ashes are managed in",
    "start": "44190",
    "end": "52949"
  },
  {
    "text": "memory service you have the choice of memcache or Redis with the last two cache you define a cluster of nodes you",
    "start": "52949",
    "end": "60390"
  },
  {
    "text": "have the option with Redis to do read replicas as well and then we take care",
    "start": "60390",
    "end": "65400"
  },
  {
    "text": "of all the undifferentiated heavy lists lifting all of the gunk around just managing a cluster of in-memory cache",
    "start": "65400",
    "end": "72270"
  },
  {
    "text": "nodes we do monitoring alerts and so forth and again just refresher the way",
    "start": "72270",
    "end": "78270"
  },
  {
    "start": "76000",
    "end": "76000"
  },
  {
    "text": "that ElastiCache fits into your app is you have your normal load balancer application tier and then you're of",
    "start": "78270",
    "end": "83520"
  },
  {
    "text": "course gonna have some type of database doesn't matter which one you know my sequel DynamoDB cassandra and then",
    "start": "83520",
    "end": "90689"
  },
  {
    "text": "you're gonna undoubtedly be using a ton of external API is whether or not you're talking about login via Facebook or",
    "start": "90689",
    "end": "97049"
  },
  {
    "text": "third-party data streams just overwhelming influx of third-party API is these days and then the way that",
    "start": "97049",
    "end": "104030"
  },
  {
    "text": "ElastiCache fits into your app is they it sits off kind of at the side it's loosely coupled with your data layer so",
    "start": "104030",
    "end": "110850"
  },
  {
    "text": "you get a lot of flexibility in terms of the type of stuff that you want to put into your cache you can cash not only database queries you can cache external",
    "start": "110850",
    "end": "117719"
  },
  {
    "text": "API is etc so one of the big questions that comes up first",
    "start": "117719",
    "end": "122820"
  },
  {
    "text": "when people are evaluating ElastiCache even if they've used it before is you know which one should they choose",
    "start": "122820",
    "end": "129390"
  },
  {
    "text": "memcache your Redis and there's of course a wonderful set if you Google online you can find a ton of",
    "start": "129390",
    "end": "135570"
  },
  {
    "text": "different flame wars people supporting one of the other but I think it boils down to just a couple of salient",
    "start": "135570",
    "end": "141150"
  },
  {
    "start": "139000",
    "end": "139000"
  },
  {
    "text": "features and hopefully I can help disentangle that a little bit so the way",
    "start": "141150",
    "end": "146160"
  },
  {
    "text": "I would think about these two is so memcache there's a few differences here so it's a multi-threaded it has no",
    "start": "146160",
    "end": "152820"
  },
  {
    "text": "persistence so when you bring up a node it's empty if the node goes away or you scale it down you lose that cache memory",
    "start": "152820",
    "end": "160580"
  },
  {
    "text": "and it's really designed to be a flat string cache like when it came out the",
    "start": "160580",
    "end": "166290"
  },
  {
    "text": "whole idea was to cache flat HTML pages also serialized JSON all that is a great",
    "start": "166290",
    "end": "172350"
  },
  {
    "text": "fit for memcache and because of the fact that it's a simpler option it's really",
    "start": "172350",
    "end": "177810"
  },
  {
    "text": "it's very low maintenance very easy to scale up horizontally especially within",
    "start": "177810",
    "end": "182820"
  },
  {
    "text": "the last two cache where you can just go in and just change the number of nodes just scale out scale back in bones",
    "start": "182820",
    "end": "188550"
  },
  {
    "text": "simple to grow and shrink your cache if you're using memcache still hugely popular you know Facebook Twitter these",
    "start": "188550",
    "end": "195270"
  },
  {
    "text": "guys have massive installations of memcache so not going away anytime soon tons of library support so if you're",
    "start": "195270",
    "end": "200760"
  },
  {
    "text": "just looking for a cash like bottom line memcache is still a great choice",
    "start": "200760",
    "end": "206360"
  },
  {
    "text": "so Redis came out a few years ago and you know prior to joining AWS a actually",
    "start": "206360",
    "end": "211560"
  },
  {
    "text": "made games of PlayStation and when it came out it actually really changed a lot of things for gaming and the reason",
    "start": "211560",
    "end": "217200"
  },
  {
    "text": "why is because it has a lot more advanced data types and options so fundamentally a single-threaded not",
    "start": "217200",
    "end": "223380"
  },
  {
    "text": "multi-threaded so when you talk about very large machines it doesn't have quite the same efficiency of making use",
    "start": "223380",
    "end": "229140"
  },
  {
    "text": "of all of those extra CPU cores but it does have persistence which means in some cases you can actually use it as a",
    "start": "229140",
    "end": "235440"
  },
  {
    "text": "primary data store and we'll go through some of those use cases in a bit it's got advanced data types it's got",
    "start": "235440",
    "end": "241320"
  },
  {
    "text": "lists it's got sets it's got hashes a lot a lot more data related stuff you can do with Redis atomic operations",
    "start": "241320",
    "end": "249900"
  },
  {
    "text": "increment decrements so it's great for keeping the distributed counters if you have a large application tier for",
    "start": "249900",
    "end": "255959"
  },
  {
    "text": "example and you're like trying to keep track of work you can keep a counter in Redis it's a great way to coordinate",
    "start": "255959",
    "end": "261500"
  },
  {
    "text": "pub/sub messaging built-in so if you're doing a chat app you just get that for free you can spin up ElastiCache with",
    "start": "261500",
    "end": "266970"
  },
  {
    "text": "red and you can just connect to that and have a back-end chat either between servers or to end-users and then read",
    "start": "266970",
    "end": "274169"
  },
  {
    "text": "replicas and failover are built in so the way I would think about these two in",
    "start": "274169",
    "end": "279660"
  },
  {
    "text": "a nutshell is flat cache go with memcache bones simple you don't have to think about it super easy to scale out",
    "start": "279660",
    "end": "285810"
  },
  {
    "text": "with ElastiCache but if you know that you kind of need Redis you know Redis is",
    "start": "285810",
    "end": "291090"
  },
  {
    "text": "awesome and we'll go through some of those use cases but all of that complexity isn't free there's more",
    "start": "291090",
    "end": "296430"
  },
  {
    "text": "management you have to think about you have to put in more effort into kind of node sizing and we'll talk about that so",
    "start": "296430",
    "end": "302550"
  },
  {
    "text": "it's use Redis if if you want to and if you have a use case for it it'll be awesome but if not then just stick with",
    "start": "302550",
    "end": "308520"
  },
  {
    "text": "memcache still widely use great solution and just to cement this last thing I'll",
    "start": "308520",
    "end": "314460"
  },
  {
    "text": "mention is if you think about how you're gonna store blob a JSON in memcache you",
    "start": "314460",
    "end": "319650"
  },
  {
    "text": "would serialize that you would stuff that into a key and that's basically all you can do but it's great if you want to",
    "start": "319650",
    "end": "325139"
  },
  {
    "text": "just put it in front of your API and have a TTL of you know a couple seconds or a couple minutes the great solution",
    "start": "325139",
    "end": "331229"
  },
  {
    "text": "there but with Redis you have more flexibility you can actually use a hash for example and you can individually store all of your different attributes",
    "start": "331229",
    "end": "337979"
  },
  {
    "text": "and values and so if you had you know a JSON object that maybe had you know my name you know and the date of birth etc",
    "start": "337979",
    "end": "344220"
  },
  {
    "text": "you can store those actually as individual hash elements and then if you wanted a plate just like the date of",
    "start": "344220",
    "end": "350250"
  },
  {
    "text": "birth you could do that and there's some cool you know advantages you have to that in that case because you can do you",
    "start": "350250",
    "end": "356400"
  },
  {
    "text": "know concurrent access you know you can have one part of the application update and the name another part of the application that updating the date of",
    "start": "356400",
    "end": "362820"
  },
  {
    "text": "birth and you don't have to worry about them colliding on this serialized string so we'll start out by spending some time",
    "start": "362820",
    "end": "369210"
  },
  {
    "text": "looking at memcache because it's simplest to understand and then we'll go through Redis second and then finally",
    "start": "369210",
    "end": "376169"
  },
  {
    "text": "end on some performance tuning and then I'm gonna hand it over to tom Kerr from Riot Games who's going to talk through",
    "start": "376169",
    "end": "381300"
  },
  {
    "text": "some of their real-world uses of both memcache and Redis so start with development easy very well documented in",
    "start": "381300",
    "end": "389099"
  },
  {
    "text": "our documentation just go in spin up a one node cluster and great to get you going and that's basically all you have",
    "start": "389099",
    "end": "396419"
  },
  {
    "text": "to do things the highlight here again remember that your Apple patience here is what's actually",
    "start": "396419",
    "end": "402240"
  },
  {
    "text": "communicating with your memcache node not your database so your application tier is going to have a connection to",
    "start": "402240",
    "end": "407639"
  },
  {
    "text": "your memcache nodes and your application tier is going to have a connection to your database or any other API and then",
    "start": "407639",
    "end": "413490"
  },
  {
    "text": "it's gonna be responsible for managing which keys get put in and out of the cache so it comes time for production",
    "start": "413490",
    "end": "419190"
  },
  {
    "text": "we're gonna want to scale it up obviously that's where you just go into ElastiCache very easily either through",
    "start": "419190",
    "end": "425400"
  },
  {
    "start": "421000",
    "end": "421000"
  },
  {
    "text": "the console or the command line API you click add node you say the number I want",
    "start": "425400",
    "end": "431070"
  },
  {
    "text": "to add two more nodes and then there's the option of how you want to distribute those across availability zones and",
    "start": "431070",
    "end": "436979"
  },
  {
    "text": "unless you have a specific need otherwise just always choose the spread across availability zones option",
    "start": "436979",
    "end": "443240"
  },
  {
    "text": "ElastiCache as a service will take care of just making sure you have even distribution across your different AZ's",
    "start": "443240",
    "end": "448979"
  },
  {
    "text": "just let it do the heavy lifting click Add and you're done and again you can easily do this through the command line",
    "start": "448979",
    "end": "454919"
  },
  {
    "text": "as well so once that's all done then we've got you know our cluster this kind",
    "start": "454919",
    "end": "460590"
  },
  {
    "text": "of scaled out has a few extra nodes the advantage again to memcache is the fact",
    "start": "460590",
    "end": "465930"
  },
  {
    "text": "that you can keep adding nodes and as you add nodes then you just evenly distribute your data and we'll look at",
    "start": "465930",
    "end": "472500"
  },
  {
    "text": "how in a second across all of those nodes in the simplest case just build out your cluster your ElastiCache",
    "start": "472500",
    "end": "478740"
  },
  {
    "text": "cluster have all of your different application nodes connect down to all of",
    "start": "478740",
    "end": "484080"
  },
  {
    "text": "the different cluster nodes all of the different cache nodes and just kind of cross connect in the big mesh it's a",
    "start": "484080",
    "end": "491070"
  },
  {
    "text": "great way to just evenly distribute very low maintenance don't have to think about it and then as you scale out and",
    "start": "491070",
    "end": "496979"
  },
  {
    "text": "have more and more need for more and more cache space you just continually add nodes now there's a little gotcha",
    "start": "496979",
    "end": "502919"
  },
  {
    "text": "here that might not be obvious but as you're adding these cache nodes remember memcache is empty right it doesn't have",
    "start": "502919",
    "end": "509789"
  },
  {
    "text": "any kind of persistence so you're adding these nodes and as a side effect you're actually flushing a part of your key space as you're adding these nodes so",
    "start": "509789",
    "end": "516779"
  },
  {
    "text": "just be you know careful when you're adding knows not to suddenly go from 4 nodes to 8 nodes because what's going to",
    "start": "516779",
    "end": "523260"
  },
  {
    "text": "happen in that case is you're gonna flush you know 50% of your cache so if you think about you know your app is",
    "start": "523260",
    "end": "528720"
  },
  {
    "text": "scaling up you know you're like oh wow you know we're gonna need more cash nodes we might as well just be safe let's just add a couple of the same time and all of a sudden you got",
    "start": "528720",
    "end": "534810"
  },
  {
    "text": "the Monsters Inc 23:19 and the big red light starts going off you're like what is happening in your database is just",
    "start": "534810",
    "end": "539850"
  },
  {
    "text": "gonna melt so don't do that just kind of gradually add them so again best",
    "start": "539850",
    "end": "547050"
  },
  {
    "text": "practice just evenly spread your key space but you know there is an anti-pattern here that's definitely worth mentioning and that is there are",
    "start": "547050",
    "end": "553950"
  },
  {
    "start": "552000",
    "end": "552000"
  },
  {
    "text": "you know a handful of cases where you might have a very very latency sensitive app and I'm talking like extremely",
    "start": "553950",
    "end": "560700"
  },
  {
    "text": "latency sensitive where like a millisecond or two makes a difference in that case then what you can do with",
    "start": "560700",
    "end": "567150"
  },
  {
    "text": "ElastiCache is you can set up clusters separate clusters in each of your AZ's and then set up your app application",
    "start": "567150",
    "end": "576000"
  },
  {
    "text": "servers in that AZ just to contact the ElastiCache cluster in that same AZ so",
    "start": "576000",
    "end": "582450"
  },
  {
    "text": "the advantage is you get lower latency disadvantages there's more configuration work you're gonna have to do here you're",
    "start": "582450",
    "end": "588420"
  },
  {
    "text": "gonna have to manage you know when an application server comes up it's gonna have to know what AZ it's in it's gonna",
    "start": "588420",
    "end": "593910"
  },
  {
    "text": "have to look up in some kind of config file which actual clusters should connect to etc and you get less cache",
    "start": "593910",
    "end": "599580"
  },
  {
    "text": "efficiency - if you think about it you're basically are gonna have to have double the amount of cache memory because you're basically duplicating it",
    "start": "599580",
    "end": "604800"
  },
  {
    "text": "so use it if you need to but again don't you know don't just do this because oh you know just to be safe",
    "start": "604800",
    "end": "610710"
  },
  {
    "text": "you know this is something where if you hit it you'll know you can always just add a cluster later rebalance to different AZ's and take care of it later",
    "start": "610710",
    "end": "617310"
  },
  {
    "text": "you don't have to plan for this it's one of the great things about anybody else at least why I became a big fan as a customer is you can just solve these",
    "start": "617310",
    "end": "623880"
  },
  {
    "text": "problems as they come up right if you if you need the latency spawn up another cluster change your app configuration",
    "start": "623880",
    "end": "629220"
  },
  {
    "text": "and deal with it at that point so let's talk about how to actually make use of",
    "start": "629220",
    "end": "634440"
  },
  {
    "text": "our different nodes so we've got all our memcache nodes set up we got them across our AZ's how do we actually distribute",
    "start": "634440",
    "end": "641520"
  },
  {
    "text": "our key space well nowadays it's largely a solved problem and the solution is",
    "start": "641520",
    "end": "647340"
  },
  {
    "start": "643000",
    "end": "643000"
  },
  {
    "text": "consistent hashing and all that means is that you take a hash of the key and then that map's it to a certain node there's",
    "start": "647340",
    "end": "653190"
  },
  {
    "text": "quite a bit of actual underlying math to do this correctly and it's actually",
    "start": "653190",
    "end": "658830"
  },
  {
    "text": "pretty interesting in my opinion but then again I'm giving a deep dive session on caching so you know take that into account but if you want to read up",
    "start": "658830",
    "end": "665430"
  },
  {
    "text": "more on it there's a great thesis out there there's a bunch of links actually the good news is you don't",
    "start": "665430",
    "end": "670670"
  },
  {
    "text": "really have to care about any of that because somebody else has already done it for you this is the list I went out and I just double-checked all of the",
    "start": "670670",
    "end": "676700"
  },
  {
    "text": "different libraries so this is a list of all of the different memcache libraries out there that support consistent",
    "start": "676700",
    "end": "682670"
  },
  {
    "text": "hashing I do want to note that even though it might be supported it doesn't mean it's the default behavior",
    "start": "682670",
    "end": "688790"
  },
  {
    "text": "especially like PHP still it's not dead nor did why but it's still not the",
    "start": "688790",
    "end": "693950"
  },
  {
    "text": "default behavior so you have to make sure to turn it on explicitly so you make sure you download it read the",
    "start": "693950",
    "end": "699890"
  },
  {
    "text": "documentation enable the option and then you're good to go if you are using Java or.net or PHP we",
    "start": "699890",
    "end": "708890"
  },
  {
    "text": "actually have an official ElastiCache library which is open source which you can download and we just build on the",
    "start": "708890",
    "end": "715580"
  },
  {
    "text": "popular libraries like for example Java it's based on spy memcache D which is the the most popular library for Java",
    "start": "715580",
    "end": "722600"
  },
  {
    "text": "the advantage that it gives you is it supports what's called node Auto discovery and so what no tanto discovery",
    "start": "722600",
    "end": "729020"
  },
  {
    "text": "does is it's just a config value we've added to memcache as part of the ElastiCache service and it enables your",
    "start": "729020",
    "end": "736370"
  },
  {
    "text": "application from a client perspective to be able to detect you know all of your nodes and kind of where they are so the",
    "start": "736370",
    "end": "742700"
  },
  {
    "text": "way to use this you just go in you say hey I want to copy the node endpoint this is from the console of course API",
    "start": "742700",
    "end": "748339"
  },
  {
    "start": "743000",
    "end": "743000"
  },
  {
    "text": "as well and then you're going to get this pop up and you're going to get this configuration node endpoint you can't",
    "start": "748339",
    "end": "753680"
  },
  {
    "text": "all you can tell us the configuration notice says dot CFG in the name there so you just take that copy paste it and so",
    "start": "753680",
    "end": "761000"
  },
  {
    "start": "760000",
    "end": "760000"
  },
  {
    "text": "again using PHP as an example you set that config endpoint as your server you",
    "start": "761000",
    "end": "766550"
  },
  {
    "text": "set dynamic client mode which is a flag as part of the ElastiCache library that enables that and then you just set that",
    "start": "766550",
    "end": "773779"
  },
  {
    "text": "config as you own the configure endpoint is the only server and then internally what'll happen is the library will",
    "start": "773779",
    "end": "779360"
  },
  {
    "text": "actually go read that value and say okay here's the list of my nodes you know can",
    "start": "779360",
    "end": "785260"
  },
  {
    "text": "consistent hashing has been enabled so I'm just gonna spread my key space across those nodes and you're done and",
    "start": "785260",
    "end": "794690"
  },
  {
    "text": "then if for some reason you come across a library that doesn't support auto discovery it's actually quite easy to",
    "start": "794690",
    "end": "799970"
  },
  {
    "text": "add in that previous list and these slides will be available there are actually a few different plugins that also add even",
    "start": "799970",
    "end": "806200"
  },
  {
    "text": "though we don't supply it that do also add that out of discovery to those different libraries but if you don't it's actually pretty straightforward and",
    "start": "806200",
    "end": "812500"
  },
  {
    "text": "there's some Stack Overflow answers they're available as well so consistent",
    "start": "812500",
    "end": "819100"
  },
  {
    "text": "hashing is you know great option and the way you should do your cache key",
    "start": "819100",
    "end": "824160"
  },
  {
    "text": "distribution for auto discovery we're having our client then actually discover",
    "start": "824160",
    "end": "829510"
  },
  {
    "text": "the list of nodes there's a different option to having than having the client actually do that discovery process and",
    "start": "829510",
    "end": "836710"
  },
  {
    "text": "that is to put a proxy in between so there's two different very popular products here at whim proxy and the",
    "start": "836710",
    "end": "843280"
  },
  {
    "text": "crowder so two and proxy is by twitter the Crowder's my facebook they do basically the same thing it's an",
    "start": "843280",
    "end": "850000"
  },
  {
    "text": "additional proxy layer so this is something that you would have to host yourself again this is one of those things if you get to the scale you're",
    "start": "850000",
    "end": "856060"
  },
  {
    "text": "gonna know it but here's how it would fit into your application so you would essentially set up a set of ec2",
    "start": "856060",
    "end": "861820"
  },
  {
    "text": "instances you can use an auto scaling group of say two instances a min and max of two so that you're always keep two",
    "start": "861820",
    "end": "868090"
  },
  {
    "text": "nodes and then you would actually put the configuration for the elastic cache servers into the 20 layer and from your",
    "start": "868090",
    "end": "875470"
  },
  {
    "text": "app perspective it's not going to know about anything behind it right the app is just gonna see those two proxy nodes",
    "start": "875470",
    "end": "881920"
  },
  {
    "text": "so as you add and remove servers from ElastiCache you don't have to update your app that's going to be in",
    "start": "881920",
    "end": "887740"
  },
  {
    "text": "configuration settings that's in that proxy layer so looking at what this looks like from a configuration this is",
    "start": "887740",
    "end": "894130"
  },
  {
    "start": "892000",
    "end": "892000"
  },
  {
    "text": "the trend proxy example the distribution is the hash algorithm Gotama for very",
    "start": "894130",
    "end": "901900"
  },
  {
    "text": "bizarre historical reasons because was the first consistent hashing library lib kodama that means consistent hashing and",
    "start": "901900",
    "end": "909400"
  },
  {
    "text": "twin proxy land and then you would just specify your list of ElastiCache servers and there's actually really good",
    "start": "909400",
    "end": "914410"
  },
  {
    "text": "documentation of this on just the readme on github for twin proxy so it's it's a",
    "start": "914410",
    "end": "920440"
  },
  {
    "text": "good solution if you're gonna have like a big pool or if it's gonna change a lot but there are you know some kind of pros",
    "start": "920440",
    "end": "925900"
  },
  {
    "text": "and cons to think about it gives you that extra flexibility you don't have to worry about whether or not the client is",
    "start": "925900",
    "end": "931900"
  },
  {
    "text": "refreshing or auto-discovery etc you have more control the clients again don't have to",
    "start": "931900",
    "end": "937829"
  },
  {
    "text": "care about it but you are adding that additional layer that could be an additional point of latency or",
    "start": "937829",
    "end": "943589"
  },
  {
    "text": "additional point of failure etc so again it's I think there's an inflection point there where if you talk about okay well",
    "start": "943589",
    "end": "949709"
  },
  {
    "text": "I'm gonna have three or four or five nodes of memcache just you know I would not put a proxy in front them personally",
    "start": "949709",
    "end": "956040"
  },
  {
    "text": "just use autodiscover keep it simple but if you're gonna have dozens of different cache nodes in this big giant cluster",
    "start": "956040",
    "end": "962249"
  },
  {
    "text": "and especially if you're gonna scale it dynamically then at that point you're gonna want to say well maybe I should proxy in there to simplify that back-end",
    "start": "962249",
    "end": "968670"
  },
  {
    "text": "configuration all right so we have",
    "start": "968670",
    "end": "973730"
  },
  {
    "text": "memcache set up we've got consistent hashing setup we've got you know our application ready to go",
    "start": "973730",
    "end": "979800"
  },
  {
    "text": "and so now let's talk about some common ways you can actually use this from an application perspective a lot of these",
    "start": "979800",
    "end": "986220"
  },
  {
    "text": "patterns will probably seem familiar to many of you a simplest way is just to be lazy lazy caching lazy fetch is the",
    "start": "986220",
    "end": "993899"
  },
  {
    "text": "predominant method of caching and in this case it's very simple you you check the cache hey can I get it",
    "start": "993899",
    "end": "999839"
  },
  {
    "text": "out of the cache nope cache miss items not there so I go back to the database pull it out stored in",
    "start": "999839",
    "end": "1007399"
  },
  {
    "text": "the cache is a byproduct and then return the record and essentially these little wrapper patterns are in basically all of",
    "start": "1007399",
    "end": "1013639"
  },
  {
    "text": "the major frameworks if you're using any of the web frameworks like rails or Django etc they're all built into this nowadays and you'll just have a little",
    "start": "1013639",
    "end": "1020059"
  },
  {
    "text": "wrapper function that'll get the user and then all of that logic will be built in this is your bread and butter for",
    "start": "1020059",
    "end": "1025819"
  },
  {
    "text": "caching if you only have time to do one thing with caching there's huge bang for buck here you can just go especially if",
    "start": "1025819",
    "end": "1031970"
  },
  {
    "text": "you're using any of those frameworks you just go set some configuration options use their you know framework specific",
    "start": "1031970",
    "end": "1038329"
  },
  {
    "text": "kind of language you know because each of them has a little different way that you call those functions and then you're",
    "start": "1038329",
    "end": "1044270"
  },
  {
    "text": "in really good shape so if you're not doing that already you know go home do it today type thing it's really great paying for Bach easy etc so then the",
    "start": "1044270",
    "end": "1052460"
  },
  {
    "text": "second option is instead to do a write through cache and in this case what you're doing is you're actually studying the key value into the cache when you're",
    "start": "1052460",
    "end": "1059990"
  },
  {
    "start": "1054000",
    "end": "1054000"
  },
  {
    "text": "updating it so it's basically just the reverse flow I'm gonna do some kind of an update statement now I've got my",
    "start": "1059990",
    "end": "1065630"
  },
  {
    "text": "updated right I'm gonna set it in the in the back in the cache and then I'm gonna return it",
    "start": "1065630",
    "end": "1070970"
  },
  {
    "text": "now this is a cool approach and it's a really good idea for data that you know",
    "start": "1070970",
    "end": "1076200"
  },
  {
    "text": "is going to be accessed by a lot of people for example let's say I have a CMS and I'm going and I'm updating you",
    "start": "1076200",
    "end": "1083250"
  },
  {
    "text": "know uploading a new article or I'm pushing out new content or something I'm gonna want to generate some keys like",
    "start": "1083250",
    "end": "1088650"
  },
  {
    "text": "hey you know top comments or the total number of blog articles that said",
    "start": "1088650",
    "end": "1094830"
  },
  {
    "text": "there's stuff that I know as a side effect people are going to hit anyway view this as an optimization there's a",
    "start": "1094830",
    "end": "1101340"
  },
  {
    "text": "whole bunch of cases where even if you do this that you're still gonna have cache misses right there's a whole bunch",
    "start": "1101340",
    "end": "1106830"
  },
  {
    "text": "of stuff that can happen you can have a cache node fail data could go out of date etc so you still need the first",
    "start": "1106830",
    "end": "1113070"
  },
  {
    "text": "method anyway so it's not like an either/or choice it's best viewed is like you're gonna use that first method",
    "start": "1113070",
    "end": "1119310"
  },
  {
    "text": "kind of everywhere and then on a case-by-case basis as you go through you're going to put this as an",
    "start": "1119310",
    "end": "1124680"
  },
  {
    "text": "optimization for those pieces of data where you need it and this is just showing that combination right we've got our get that",
    "start": "1124680",
    "end": "1132210"
  },
  {
    "text": "does our lazy fetch we've got our update that does you know in some cases update the cache directly last thing I want to",
    "start": "1132210",
    "end": "1139800"
  },
  {
    "text": "mention from a pattern standpoint is just TTLs so you should just set a TTL",
    "start": "1139800",
    "end": "1145200"
  },
  {
    "text": "on every one of your cache keys and the reason I mentioned that is because there",
    "start": "1145200",
    "end": "1150630"
  },
  {
    "text": "is always the possibility for you have an application bug or there'd be some kind of background processing that you",
    "start": "1150630",
    "end": "1156660"
  },
  {
    "text": "didn't take into account and you could end up with stale data in your actual cache even if you even if you're",
    "start": "1156660",
    "end": "1162600"
  },
  {
    "text": "following the best practices and you finally tuned your app you just have a bug or something like that so there's",
    "start": "1162600",
    "end": "1168060"
  },
  {
    "text": "basically kind of two different kinds of TTL s there's a TTL like this where we want a short TTL on it and a good",
    "start": "1168060",
    "end": "1174840"
  },
  {
    "text": "example this probably isn't actually the best example for a short TTL like a user update a good example would be like",
    "start": "1174840",
    "end": "1180300"
  },
  {
    "text": "front-page top stories like you just want to buffer the front page top stories or something kind of metadata",
    "start": "1180300",
    "end": "1185760"
  },
  {
    "text": "there for like two seconds just enough to relieve pressure on your database so you just you're updating this cache and",
    "start": "1185760",
    "end": "1191730"
  },
  {
    "text": "just setting a little tiny TTL and then you're always getting updated information out of the databases it",
    "start": "1191730",
    "end": "1197010"
  },
  {
    "text": "comes through in other cases if you're just a user record for example you might want to set a TTL of just like a day or two",
    "start": "1197010",
    "end": "1204390"
  },
  {
    "text": "days and that catches the case where you have the situation where you forgot in this one place to set up your cash",
    "start": "1204390",
    "end": "1211590"
  },
  {
    "text": "expiry for when it was updated and now you've gotten out of date record and that just shows TTL in combination all",
    "start": "1211590",
    "end": "1221250"
  },
  {
    "text": "right so let's talk Redis hopefully that",
    "start": "1221250",
    "end": "1228450"
  },
  {
    "text": "first part showed that there's really not a lot to memcache because there isn't there's some stuff at very large",
    "start": "1228450",
    "end": "1234390"
  },
  {
    "text": "scale like putting in some proxy nodes etc that you do have to think about but there's not a huge amount of low-level",
    "start": "1234390",
    "end": "1240179"
  },
  {
    "text": "tuning Redis is a little bit differently there little different in this regard as we'll see but I first want to start by",
    "start": "1240179",
    "end": "1245820"
  },
  {
    "text": "going through some of the use cases like where would you actually want to use Redis as opposed to memcache what are",
    "start": "1245820",
    "end": "1252720"
  },
  {
    "text": "your advantages so let's talk through that the big one for gaming is real-time leaderboards I would say 99.9% that's an",
    "start": "1252720",
    "end": "1260460"
  },
  {
    "start": "1254000",
    "end": "1254000"
  },
  {
    "text": "official number by the way of game companies nowadays are using Redis for their leaderboards and the reason why is",
    "start": "1260460",
    "end": "1265860"
  },
  {
    "text": "because there's a killer feature in Redis called sorted sets and essentially",
    "start": "1265860",
    "end": "1272039"
  },
  {
    "text": "what you do is you just update a key and you associate a score with it which can just be any number value and then Redis",
    "start": "1272039",
    "end": "1277980"
  },
  {
    "text": "internally in real time is going to keep a ranked list of that so outside of game leaderboards you know most popular",
    "start": "1277980",
    "end": "1283380"
  },
  {
    "text": "people on your website top most popular posts you know top number of likes any of those kinds of things you can use a",
    "start": "1283380",
    "end": "1289470"
  },
  {
    "text": "sorted set for does it all in memory blazing-fast huge bang for buck and actually much more cost efficient to do",
    "start": "1289470",
    "end": "1295679"
  },
  {
    "text": "that in Redis with sorted sets than try to put it in a huge giant my sequel database table and run some kind of",
    "start": "1295679",
    "end": "1300929"
  },
  {
    "text": "background ranking job recommendation engines is another something that you",
    "start": "1300929",
    "end": "1306299"
  },
  {
    "start": "1304000",
    "end": "1304000"
  },
  {
    "text": "can do in Redis but you can't do in memcache so again using some reticence",
    "start": "1306299",
    "end": "1311429"
  },
  {
    "text": "data structures you can increment and decrement like the types of items that are types of products that a person",
    "start": "1311429",
    "end": "1316830"
  },
  {
    "text": "likes you can keep a map of their favorite products and then you can go back and use some of reticent Ertel data",
    "start": "1316830",
    "end": "1322590"
  },
  {
    "text": "structures like unions in order to combine it say okay well based on these other users likes then we think this is",
    "start": "1322590",
    "end": "1328649"
  },
  {
    "text": "going to be a good diction and there's actually open source gems like there's a ruby one called recommendable which is quite popular",
    "start": "1328649",
    "end": "1334410"
  },
  {
    "text": "they use Redis for this and there's been a whole bunch of clones in different languages as well and the last one I",
    "start": "1334410",
    "end": "1339720"
  },
  {
    "text": "want to point out is some kind of chat like pub/sub again great if you're",
    "start": "1339720",
    "end": "1345360"
  },
  {
    "text": "building a chat app for end users but also if you have some way you know if you want to wait for your servers to",
    "start": "1345360",
    "end": "1350490"
  },
  {
    "text": "communicate if you think about like a mobile or gaming app or connected devices you might want some way to pass",
    "start": "1350490",
    "end": "1356010"
  },
  {
    "text": "a message from a server perspective you can have all of your servers join a common channel a topic and then you can",
    "start": "1356010",
    "end": "1362970"
  },
  {
    "text": "just push out messages and your servers can actually dynamically then take actions or reconfigure themselves etc",
    "start": "1362970",
    "end": "1369620"
  },
  {
    "text": "alright so let's see how vastly different Redis is when you deploy it",
    "start": "1369680",
    "end": "1374790"
  },
  {
    "text": "and in development it looks exactly the same so in dev mode you can do the same thing",
    "start": "1374790",
    "end": "1380580"
  },
  {
    "text": "yeah you can just spin up a single node cluster and then it's literally like the same command you just choose reticence Ted but of course when it comes time to",
    "start": "1380580",
    "end": "1387600"
  },
  {
    "text": "prediction it is actually quite different so let's look at that so Redis with ElastiCache supports multi AZ which",
    "start": "1387600",
    "end": "1394860"
  },
  {
    "text": "is a similar concept to our RDS offerings and that is you get one or more replicas in different availability",
    "start": "1394860",
    "end": "1401430"
  },
  {
    "text": "zones and then the last two caches the service takes care of handling failover in the case that there's an issue with",
    "start": "1401430",
    "end": "1407040"
  },
  {
    "text": "the primary endpoint so a couple things I want to highlight here first of all it really is just a checkbox and you can",
    "start": "1407040",
    "end": "1413250"
  },
  {
    "text": "optionally set you know the number of replicas etc that'll give you replica",
    "start": "1413250",
    "end": "1418410"
  },
  {
    "text": "nodes in other AGS and then this is all asynchronous replication so important",
    "start": "1418410",
    "end": "1424470"
  },
  {
    "text": "thing to keep in mind with any asynchronous replication there's always the possibility of data lice data loss",
    "start": "1424470",
    "end": "1430350"
  },
  {
    "text": "even if very slight so in practice the replication lag is very low you know I",
    "start": "1430350",
    "end": "1437220"
  },
  {
    "text": "was talking to the PM earlier and it's on the order of a couple milliseconds or less but be aware there's a couple milliseconds if you're changing a lot of",
    "start": "1437220",
    "end": "1443430"
  },
  {
    "text": "data you could lose some data in the event of a failover so in practice it's not like a massive huge concern for",
    "start": "1443430",
    "end": "1449850"
  },
  {
    "text": "customers but just be aware it is there so do you want to store your financial transactions and multi easy I would",
    "start": "1449850",
    "end": "1455670"
  },
  {
    "text": "probably not recommend that maybe you choose a more durable datastore like dynamo but in the vast majority case",
    "start": "1455670",
    "end": "1462809"
  },
  {
    "text": "this is not a real-world problem because if you think about what you're doing like storing a leaderboard you're still gonna want to save my high",
    "start": "1462809",
    "end": "1469230"
  },
  {
    "text": "score and my profile back in my main user record anyway so you can still have that lazy fetch pattern say hey why",
    "start": "1469230",
    "end": "1475710"
  },
  {
    "text": "don't you check for Nate's entry in the leaderboard oh he doesn't have it that's weird for some reason who knows who cares what happened just pull it out of",
    "start": "1475710",
    "end": "1481860"
  },
  {
    "text": "the main database put it in the sorted set and you're done and so with this set",
    "start": "1481860",
    "end": "1486990"
  },
  {
    "text": "up then when if something happens to the primary endpoint ElastiCache is a",
    "start": "1486990",
    "end": "1492929"
  },
  {
    "text": "service will choose the replica that has the lowest replica lag and then it will move the DNS entry from the primary",
    "start": "1492929",
    "end": "1499350"
  },
  {
    "text": "endpoint over to that replica so your application doesn't have to change there's gonna be a period or a minute or",
    "start": "1499350",
    "end": "1505440"
  },
  {
    "text": "two while the fail failures being detected the DNS endpoint is being updated and then that transition happens",
    "start": "1505440",
    "end": "1512399"
  },
  {
    "text": "when you're gonna lose connectivity to Redis but then your app will just get connected to the new replica node you",
    "start": "1512399",
    "end": "1518279"
  },
  {
    "text": "don't have to do anything and the last thing on this slide which I'm going to go into more detail in a minute is this",
    "start": "1518279",
    "end": "1524879"
  },
  {
    "text": "does give you the option and not requirement this gives you the option to split up your reads and writes so you",
    "start": "1524879",
    "end": "1530429"
  },
  {
    "text": "always have to make sure of course that you're sending all your rights to the primary endpoint but from a read perspective if you have a particularly",
    "start": "1530429",
    "end": "1536700"
  },
  {
    "text": "read heavy application you can use your replicas then and use those for reads as well and a final thing sorry I lied",
    "start": "1536700",
    "end": "1545190"
  },
  {
    "text": "final final thing snapshots this gives you the ability to take snapshots from a replica node as well and that's actually",
    "start": "1545190",
    "end": "1551700"
  },
  {
    "text": "built into ElastiCache where you're setting this up you can either have the snapshot just go off the primary node or",
    "start": "1551700",
    "end": "1557669"
  },
  {
    "text": "you can have it you can assign it to one of the replicas so you can set up a replica even just for taking snapshots",
    "start": "1557669",
    "end": "1564139"
  },
  {
    "text": "visual description of what happens during a failover event probably obvious but notice again with multi a-z here",
    "start": "1565519",
    "end": "1573809"
  },
  {
    "text": "we're gonna have the replicas but unlike memcache work we're just evenly connecting to both nodes and doing reads",
    "start": "1573809",
    "end": "1580200"
  },
  {
    "text": "and writes to both in this case in the simplest case we're just going to do reads and writes to the primary endpoint again I'll come back to the read thing",
    "start": "1580200",
    "end": "1586980"
  },
  {
    "text": "in a second but in the simplest case where many customers do is they just use the replicas for failover and they only",
    "start": "1586980",
    "end": "1593369"
  },
  {
    "text": "configure their application with a primary and and it works very well and then you just have to worry about reading and writing",
    "start": "1593369",
    "end": "1599320"
  },
  {
    "text": "to that primary endpoint we'll move that endpoint on failure and so what happens you know something happens to the",
    "start": "1599320",
    "end": "1605139"
  },
  {
    "text": "primary endpoint and explodes there's gonna be a minute or two of disruption while the DNS endpoint has moved over to",
    "start": "1605139",
    "end": "1610720"
  },
  {
    "text": "the replicas you know it's detected moved etc your application will will then get a connection back it's gonna",
    "start": "1610720",
    "end": "1616090"
  },
  {
    "text": "connect to that replica and then ElastiCache from a service perspective will replace the failed node start",
    "start": "1616090",
    "end": "1621700"
  },
  {
    "text": "replication backup in the opposite way we won't fail you back just because we don't want to double failure if there's",
    "start": "1621700",
    "end": "1626889"
  },
  {
    "text": "no reason to so you'll just be running connected then to the other AC and the",
    "start": "1626889",
    "end": "1632769"
  },
  {
    "text": "way you get that primary endpoint is slightly different for Redis as opposed to memcache in this case you go in",
    "start": "1632769",
    "end": "1639309"
  },
  {
    "text": "through the replication group in the console and you just select the group so within that you know the replica is",
    "start": "1639309",
    "end": "1646000"
  },
  {
    "text": "called a replication group so you select the group you want to look at and about halfway down you'll see the primary",
    "start": "1646000",
    "end": "1652179"
  },
  {
    "text": "endpoint and that is what you want to put in your application is where you're gonna do your rights you can see all of",
    "start": "1652179",
    "end": "1659710"
  },
  {
    "text": "the different nodes that are associated with this replication group and it'll tell you which one's the primary and which ones to read replica and then on",
    "start": "1659710",
    "end": "1667480"
  },
  {
    "text": "the right and I'll come back to this a little bit later you have the ability to change which one's the primary and we'll",
    "start": "1667480",
    "end": "1672639"
  },
  {
    "text": "come back to that at the end all right so again this is the main set up many",
    "start": "1672639",
    "end": "1680409"
  },
  {
    "text": "customers use just looking at from kind of a side view because it's easier to illustrate a couple things this way this",
    "start": "1680409",
    "end": "1685450"
  },
  {
    "text": "is you know one replica and then we're doing reads and writes to the main node and then we decide you know what we have",
    "start": "1685450",
    "end": "1691210"
  },
  {
    "text": "a really read heavy application so let's go ahead and use that read replica and it just looks like this right you're doing your rights to that node you can",
    "start": "1691210",
    "end": "1696879"
  },
  {
    "text": "you can still read from the primary as well but then you're gonna set up your application to read from the replica so",
    "start": "1696879",
    "end": "1703600"
  },
  {
    "text": "important you know point to keep in mind is you're gonna have to be responsible in your app then for managing these two",
    "start": "1703600",
    "end": "1709659"
  },
  {
    "text": "like there's no way we can automatically know like which piece of data needs to go where so you're gonna have two handles then you're gonna have a right",
    "start": "1709659",
    "end": "1716169"
  },
  {
    "text": "handle and a read handle and so within your application then you say okay I'm gonna use the primary endpoint for all",
    "start": "1716169",
    "end": "1722379"
  },
  {
    "text": "of my writes and then you're going to list out the other replicas as the nose that you're gonna read from",
    "start": "1722379",
    "end": "1728510"
  },
  {
    "text": "there's one little gotcha that's just important to keep in mind here is that if a failover event does occur we're",
    "start": "1728510",
    "end": "1735660"
  },
  {
    "text": "gonna update that primary endpoint and it's going to end up pointing to the same place as one of those former",
    "start": "1735660",
    "end": "1741480"
  },
  {
    "text": "replicas so your application will still work but what can happen is that if you're really pushing your cluster to",
    "start": "1741480",
    "end": "1747930"
  },
  {
    "text": "the limit then when this failover event happens all of a sudden you could be overloading one of those other nodes",
    "start": "1747930",
    "end": "1753480"
  },
  {
    "text": "you'll just have to reconfigure your app and say hey here's here's the new replica list you know sent push out a",
    "start": "1753480",
    "end": "1758820"
  },
  {
    "text": "new configuration file etc you can do that during the next deployment so okay",
    "start": "1758820",
    "end": "1765840"
  },
  {
    "start": "1765000",
    "end": "1765000"
  },
  {
    "text": "I'm going to talk a little bit about splitting up and then we're gonna get into some performance tuning of them and hand it over to Tom so you know",
    "start": "1765840",
    "end": "1773520"
  },
  {
    "text": "important thing about Redis that's very different from memcache is you cannot horizontally shard these data structures",
    "start": "1773520",
    "end": "1780210"
  },
  {
    "text": "like it's a key caveat like as awesome as Redis is and like it really is awesome the problem is is that any of",
    "start": "1780210",
    "end": "1787440"
  },
  {
    "text": "these advanced data structures like sorted sets list hash they have to stay in a single memory image so if you think",
    "start": "1787440",
    "end": "1794460"
  },
  {
    "text": "about it what you know the net effect of that is you really can only scale Redis vertically it's just the way Redis is and so what people end up doing at scale",
    "start": "1794460",
    "end": "1803190"
  },
  {
    "text": "is splitting apart their different load based on purpose so to start with you",
    "start": "1803190",
    "end": "1808230"
  },
  {
    "text": "might just have one Redis cluster be fine no problem but it starts growing growing growing you're like okay this is",
    "start": "1808230",
    "end": "1813690"
  },
  {
    "text": "getting too big for mine my nodes I don't want all this just in one giant cluster you can split it out then based",
    "start": "1813690",
    "end": "1819690"
  },
  {
    "text": "on purpose so you can have you could split out in this example just a separate you know set of replicas that",
    "start": "1819690",
    "end": "1826260"
  },
  {
    "text": "would handle all of your counters maybe the number of people in your app and their people online or games being",
    "start": "1826260",
    "end": "1831450"
  },
  {
    "text": "played etc and then in your application then you would have just two sets of handles right you would have something",
    "start": "1831450",
    "end": "1836580"
  },
  {
    "text": "for your leaderboard when you're doing leaderboard out operation is you're gonna access that one when you have counters you know or any of those kind",
    "start": "1836580",
    "end": "1843090"
  },
  {
    "text": "of statute you're accessing the other one and you manage those two separately you know there's an upside to this the",
    "start": "1843090",
    "end": "1849000"
  },
  {
    "text": "fact that then you can scale the to independently maybe you're gonna need a you know big R 3 you know 4 X large or something like that for your leaderboard",
    "start": "1849000",
    "end": "1855510"
  },
  {
    "text": "just because it grows and you've got millions of players but maybe you don't need that for your counters maybe you can do a smaller mode for your counters",
    "start": "1855510",
    "end": "1861550"
  },
  {
    "text": "so you do get some flexibility there and",
    "start": "1861550",
    "end": "1868030"
  },
  {
    "text": "final thing from an overall architecture I want to mention is we just pre announced this morning as part of a",
    "start": "1868030",
    "end": "1873190"
  },
  {
    "text": "Vernors keynote if you saw it support in lambda for vp c so what you're going to",
    "start": "1873190",
    "end": "1879190"
  },
  {
    "text": "be able to do by the end of this year is set up an elastic ash cluster in V PC",
    "start": "1879190",
    "end": "1884370"
  },
  {
    "text": "and then have any kind of arbitrary lambda function just access that elastic ash cluster and this is going to work",
    "start": "1884370",
    "end": "1890320"
  },
  {
    "text": "for both Redis or memcache actually the lambda team has a prototype demo where",
    "start": "1890320",
    "end": "1896050"
  },
  {
    "text": "they actually spin up a lambda function and it accesses an elastic cash Redis cluster via the V PC support so it's",
    "start": "1896050",
    "end": "1903040"
  },
  {
    "text": "kind of cool especially if you geek out on serverless architectures like I do you know why run servers if you don't",
    "start": "1903040",
    "end": "1908200"
  },
  {
    "text": "have to you'll be able to use elastic ash as well in very short order all",
    "start": "1908200",
    "end": "1915040"
  },
  {
    "text": "right so let's talk about performance tuning so within ElastiCache we use",
    "start": "1915040",
    "end": "1924490"
  },
  {
    "text": "cloud watch like many of our other services and this lets you monitor a lot of different there's actually a ton of",
    "start": "1924490",
    "end": "1929740"
  },
  {
    "text": "metrics we push into cloud watch loud nowadays when it comes time for which",
    "start": "1929740",
    "end": "1935020"
  },
  {
    "start": "1933000",
    "end": "1933000"
  },
  {
    "text": "one should you care about there's a couple that are important to highlight first of all CPU utilization that's you",
    "start": "1935020",
    "end": "1942370"
  },
  {
    "text": "know an easy one that you should kind of monitor for most anything you know it's fine actually to have memcache and Redis",
    "start": "1942370",
    "end": "1948760"
  },
  {
    "text": "running at around 90 ish % CPU that's you're actually not gonna have a problem in that they're actually quite good in",
    "start": "1948760",
    "end": "1954550"
  },
  {
    "text": "terms of their CPU usage but there's an important caveat because of the way that cloud watch reports CPU utilization it",
    "start": "1954550",
    "end": "1960970"
  },
  {
    "text": "reports it is an aggregate so from Redis if your alert in this on Redis you've",
    "start": "1960970",
    "end": "1966580"
  },
  {
    "text": "got to divide it by the number of course and that's actually your real CPU utilization so you could be like why is",
    "start": "1966580",
    "end": "1972730"
  },
  {
    "text": "my cache overloaded it says I'm only using 25% like I know it's a 4 core box",
    "start": "1972730",
    "end": "1978190"
  },
  {
    "text": "that's actually a hundred percent of the CPU so keep that in mind it's just because of the fact that it's an aggregate metric so you do have to think",
    "start": "1978190",
    "end": "1984370"
  },
  {
    "text": "about okay for this Redis cluster what size know do I have and then you have to do that division said okay you",
    "start": "1984370",
    "end": "1989520"
  },
  {
    "text": "know if it is 22% this is like a dire situation so it's a little counterintuitive but we've got good",
    "start": "1989520",
    "end": "1994650"
  },
  {
    "text": "documentation on this swap uses you should basically never be swapping right like these are in memory databases like",
    "start": "1994650",
    "end": "2002000"
  },
  {
    "text": "that's how they're designed to work if you're hitting swap it's almost certainly a bad sign I think with",
    "start": "2002000",
    "end": "2007850"
  },
  {
    "text": "memcache you can like I think our guideline is you can have a couple megabytes of cache and it's still probably okay but I would just say it",
    "start": "2007850",
    "end": "2014090"
  },
  {
    "text": "should be zero and especially for Redis if you're going in to swap it's it's a very bad sign you need a bigger instance evictions are",
    "start": "2014090",
    "end": "2022700"
  },
  {
    "text": "a measure of how many keys are getting forced out of cache memory without your",
    "start": "2022700",
    "end": "2029210"
  },
  {
    "text": "intervention so both memcache and Redis use an LRU least recently used and",
    "start": "2029210",
    "end": "2035330"
  },
  {
    "text": "they're their exact implications or their exact implementation is a little different it doesn't really matter but the point is is like evictions happen if",
    "start": "2035330",
    "end": "2042320"
  },
  {
    "text": "you're essentially bumping up against your available memory for that cache node so a few evictions are okay",
    "start": "2042320",
    "end": "2049490"
  },
  {
    "text": "that could just show that you have a perfectly sized cache and everything's great but if you see this number growing",
    "start": "2049490",
    "end": "2054620"
  },
  {
    "text": "especially growing over time and especially if it's a large number begin dozens and dozens of evictions then it's",
    "start": "2054620",
    "end": "2059780"
  },
  {
    "text": "a bad sign or hundreds especially yeah there is an exception and that's the pattern called Russian doll caching",
    "start": "2059780",
    "end": "2066169"
  },
  {
    "text": "which rails for started really popularizing and that approach is you know what just fill your cache up don't",
    "start": "2066169",
    "end": "2072260"
  },
  {
    "text": "care and just let memcache and Redis just evict your keys I'm you know my two cents I'm not personally sold on that",
    "start": "2072260",
    "end": "2078740"
  },
  {
    "text": "because there's side implications for that in the fact that now you're asking memcache and Redis to be running these",
    "start": "2078740",
    "end": "2084919"
  },
  {
    "text": "background processes and selecting nodes and figuring out which ones to push out so you know if you pursue that keep in",
    "start": "2084919",
    "end": "2090710"
  },
  {
    "text": "mind you're gonna have to definitely instrument CPU and make sure your CPU is not spiking up but in general for most",
    "start": "2090710",
    "end": "2096320"
  },
  {
    "text": "apps eviction it should be low similar and realated is cacistis and cache",
    "start": "2096320",
    "end": "2101900"
  },
  {
    "text": "misses you should have a good hits to miss ratio like most of the time you should be doing cache hits and you",
    "start": "2101900",
    "end": "2107360"
  },
  {
    "text": "should have a lower number of cache misses if it's 50/50 then that means you're getting a lot of keys that are",
    "start": "2107360",
    "end": "2112700"
  },
  {
    "text": "getting evicted and pushed out of memory like if you're constantly seeing a punching misses then that means stuff is",
    "start": "2112700",
    "end": "2117830"
  },
  {
    "text": "getting shoved out of memory so again is basically all of these are signs that you're going to need a bigger node or memcache you're going to need to",
    "start": "2117830",
    "end": "2124250"
  },
  {
    "text": "horizontally scale it current connections same kind of thing should be stable if you see in a lot of you know",
    "start": "2124250",
    "end": "2130970"
  },
  {
    "text": "connections growing it could be a sign that your applications not getting a reaction or a response from the actual",
    "start": "2130970",
    "end": "2136910"
  },
  {
    "text": "cash in time so it's opening additional connections and make the pool is kind of growing same kind of thing you're gonna",
    "start": "2136910",
    "end": "2142190"
  },
  {
    "text": "need a bigger node bigger cluster and all of this and more all of this some more is actually in much greater detail",
    "start": "2142190",
    "end": "2149809"
  },
  {
    "text": "in the white paper I wrote earlier this year so you know go through specifics and spend several paragraphs on it so if",
    "start": "2149809",
    "end": "2155930"
  },
  {
    "text": "you're interested in that do check that out so if you do run into this situation say you know what I have to scale up my Redis cluster the process is pretty",
    "start": "2155930",
    "end": "2162740"
  },
  {
    "start": "2158000",
    "end": "2158000"
  },
  {
    "text": "straightforward you take a snapshot to s3 then you create a new cluster with a larger node and profit sit on the beach",
    "start": "2162740",
    "end": "2169809"
  },
  {
    "text": "unfortunately just because the wave Edison is you can't do this without disruption right you're going to a new",
    "start": "2169809",
    "end": "2175220"
  },
  {
    "text": "node it's bigger so there's gonna be some downtime there you just have to plan for that it's also a good approach",
    "start": "2175220",
    "end": "2180710"
  },
  {
    "text": "for a debugging production data the snapshots not disruptive you can take a snapshot from a replica save that s3 and",
    "start": "2180710",
    "end": "2187309"
  },
  {
    "text": "then you can spin that up on a dev node and say okay let's see why this weird data corruption or whatever is happening",
    "start": "2187309",
    "end": "2192589"
  },
  {
    "text": "in production or let's try out a new version of our recommendation engine and see if you know if we use the data in a different way so that that workflow is",
    "start": "2192589",
    "end": "2198619"
  },
  {
    "text": "really nice - alright common issues and then I'm going to hand it over to Tom",
    "start": "2198619",
    "end": "2204440"
  },
  {
    "text": "the big one for caching is called the",
    "start": "2204440",
    "end": "2209569"
  },
  {
    "start": "2208000",
    "end": "2208000"
  },
  {
    "text": "thundering herd and this is just a when you get a huge onslaught of requests and there could be a whole bunch of reasons",
    "start": "2209569",
    "end": "2215720"
  },
  {
    "text": "why you could add a cache note as I mentioned at the outset which means you have part of your cache which is now",
    "start": "2215720",
    "end": "2220760"
  },
  {
    "text": "empty you could just you can make the front page of Reddit you know or the featured in the app store and you have a",
    "start": "2220760",
    "end": "2225920"
  },
  {
    "text": "whole page of a whole bunch of people flood your system all of a sudden and then you have a whole bunch of cache misses which then or database hits so",
    "start": "2225920",
    "end": "2232279"
  },
  {
    "text": "it's kind of like your cache isn't there and then a lot of teeth you know TTL if you're not staggering your TTL that",
    "start": "2232279",
    "end": "2238819"
  },
  {
    "text": "could be a problem as well or just generally running out of cache memory so the mitigations to this are as I mentioned slow you know slowly ramp up",
    "start": "2238819",
    "end": "2245630"
  },
  {
    "text": "your cache nodes randomizing TTL values to is kind of a neat trick so say like",
    "start": "2245630",
    "end": "2250819"
  },
  {
    "text": "you wanted to say you know what I want to set of TTL a day on all user profiles well you can do add a plus R and of",
    "start": "2250819",
    "end": "2258410"
  },
  {
    "text": "something you know so maybe some people are a day in five minutes and other people are you know a day and an hour or",
    "start": "2258410",
    "end": "2264110"
  },
  {
    "text": "something like that again that's for like the bug catching situation not I want it to definitely expire at this",
    "start": "2264110",
    "end": "2269480"
  },
  {
    "text": "time and then that'll get you some randomization so like your whole cache isn't flushing on a nightly basis if",
    "start": "2269480",
    "end": "2275360"
  },
  {
    "text": "that makes sense a couple of gotchas with Redis failover as mentioned it has",
    "start": "2275360",
    "end": "2281570"
  },
  {
    "start": "2277000",
    "end": "2277000"
  },
  {
    "text": "to update a DNS cname can take a couple minutes you got to watch out for your application tier doing naughty things",
    "start": "2281570",
    "end": "2288110"
  },
  {
    "text": "and JVM is terrible in this I have no idea why it's still this way but the JVM specifically has its own internal DNS",
    "start": "2288110",
    "end": "2295760"
  },
  {
    "text": "cache that does not respect this the system DNS cache and this affects a whole bunch of things but it's almost",
    "start": "2295760",
    "end": "2301700"
  },
  {
    "text": "every time when they're like the failover is not working or like you're using a Java they're like yes we're like okay well we know what the problem is so",
    "start": "2301700",
    "end": "2306770"
  },
  {
    "text": "we if it's actually such a problem that we have a link nowadays that says you",
    "start": "2306770",
    "end": "2311960"
  },
  {
    "text": "know specifically go and set this setting to zero so do take a look at that and in terms of testing this we",
    "start": "2311960",
    "end": "2317990"
  },
  {
    "text": "don't really have an API to just force a failover but what you can do as I mentioned on the right side of the",
    "start": "2317990",
    "end": "2323540"
  },
  {
    "text": "replication group there's that promote button where you can change which one's the primary again kids don't do this on",
    "start": "2323540",
    "end": "2330140"
  },
  {
    "text": "your production database please but in terms of dev and testing to make sure your app is behaving correctly you can",
    "start": "2330140",
    "end": "2336740"
  },
  {
    "text": "temporarily turn off multi az and then manually go in and use promote for one",
    "start": "2336740",
    "end": "2341870"
  },
  {
    "text": "of the replicas that'll cause that DNS endpoint to get repointed and then you can make sure that your app is correctly",
    "start": "2341870",
    "end": "2347780"
  },
  {
    "text": "communicating with a new endpoint again don't do it on your live please don't but good for testing make sure your app",
    "start": "2347780",
    "end": "2354890"
  },
  {
    "text": "is behaving properly and the last thing I want to talk about here is some caveats about the way Redis backups work",
    "start": "2354890",
    "end": "2361430"
  },
  {
    "start": "2358000",
    "end": "2358000"
  },
  {
    "text": "and again this is just the way Redis is so within Redis the way backups work is",
    "start": "2361430",
    "end": "2367340"
  },
  {
    "text": "they basically the Redis process Forks a copy of itself which then sits in the",
    "start": "2367340",
    "end": "2372620"
  },
  {
    "text": "background and writes out the actual saved data to disk and then the child process exits because of that it",
    "start": "2372620",
    "end": "2380540"
  },
  {
    "text": "leverages the UNIX copy-on-write semantics which basically means hey you know when you do a fork",
    "start": "2380540",
    "end": "2386370"
  },
  {
    "text": "you're just going to be pointing to the same piece of memory and this is just Unix right anytime you write to that memory though you're gonna make a copy",
    "start": "2386370",
    "end": "2393630"
  },
  {
    "text": "of the page and that main parent process which takes a little bit of time but it also takes up more memory so the side",
    "start": "2393630",
    "end": "2399630"
  },
  {
    "text": "effect is is if you have a very write heavy application and you're doing a ton of updates then as that actually happens",
    "start": "2399630",
    "end": "2406140"
  },
  {
    "text": "and you're doing the backup at the same time your memory is gonna grow possibly quite dramatically so it's a tough",
    "start": "2406140",
    "end": "2413490"
  },
  {
    "text": "problem there's no silver bullet solution but there's a couple things you can do first of all there is an actual",
    "start": "2413490",
    "end": "2418740"
  },
  {
    "start": "2417000",
    "end": "2417000"
  },
  {
    "text": "parameter called reserved memory and basically all that does is just set part",
    "start": "2418740",
    "end": "2424800"
  },
  {
    "text": "of the memory as off-limits it basically just looks like torretta like it's a internally like it has less memory like",
    "start": "2424800",
    "end": "2430200"
  },
  {
    "text": "almost like it's on a smaller box unfortunately you know it increases the cost there because you're marking a whole bunch of memory off-limits but",
    "start": "2430200",
    "end": "2436860"
  },
  {
    "text": "then you know it does solve the problem of during that background of being able to actually fork and be able to write",
    "start": "2436860",
    "end": "2443250"
  },
  {
    "text": "the background process correctly it's the same idea you can just reuse the larger cache know type so again you know",
    "start": "2443250",
    "end": "2449670"
  },
  {
    "text": "if you have 15 gigs of data you might need a node that has you know 20 gigs 24 more gigs of data available just to deal",
    "start": "2449670",
    "end": "2456990"
  },
  {
    "text": "with the backup if you don't do it the problem is then your backups could just fail and just not work and as I",
    "start": "2456990",
    "end": "2465030"
  },
  {
    "text": "mentioned right heavy apps need more memory the good news is and this is kind of my last point that I'll hand it over",
    "start": "2465030",
    "end": "2471570"
  },
  {
    "text": "to Tom here is we've taken steps to try and mitigate a lot of this for you by making enhancements to the Redis engine",
    "start": "2471570",
    "end": "2477930"
  },
  {
    "text": "when it's running within ElastiCache so these are things that are currently only available in ElastiCache but they are",
    "start": "2477930",
    "end": "2484380"
  },
  {
    "start": "2480000",
    "end": "2480000"
  },
  {
    "text": "available in the newest engine for free you don't have to do anything extra to set it up so if you use - 8 - 2 or later",
    "start": "2484380",
    "end": "2490620"
  },
  {
    "text": "you'll get these enhancements automatically the big one is for CLIs backups so if there is enough memory on",
    "start": "2490620",
    "end": "2496650"
  },
  {
    "text": "the box we're still gonna try to do the fork because it's you know it's faster to just write from a forked background",
    "start": "2496650",
    "end": "2502800"
  },
  {
    "text": "process if not we have this cooperative tech it's actually kind of cool in my opinion what it'll do is it'll just it won't",
    "start": "2502800",
    "end": "2508620"
  },
  {
    "text": "fork but it's a process as part of the main process then that it's going to be looking at data and if the data changes",
    "start": "2508620",
    "end": "2515010"
  },
  {
    "text": "it'll then synchronously write out that data change so that it doesn't have to like a background so there's a you know",
    "start": "2515010",
    "end": "2521270"
  },
  {
    "text": "performance penalty minor but the good news is you can use more effective memory on the node the other thing which",
    "start": "2521270",
    "end": "2528920"
  },
  {
    "text": "I didn't really get into too much but under heavy write loads Redis as a whole just replicas can become behind and this",
    "start": "2528920",
    "end": "2537350"
  },
  {
    "text": "is if you set it up yourself it's just a Redis problem the fact is Redis doesn't do any kind of special buffering we've added some of",
    "start": "2537350",
    "end": "2543290"
  },
  {
    "text": "that to it so we actually do some kind of some better buffering so that when you're talking about replication lag it'll be lower especially again as",
    "start": "2543290",
    "end": "2550010"
  },
  {
    "text": "you're doing a lot of Rights that's where a lot of these problems come up is like very heavy right loads a single process so it's doing everything in one",
    "start": "2550010",
    "end": "2555860"
  },
  {
    "text": "process you know you have to use a little bit more intelligence for some of those and similarly related another",
    "start": "2555860",
    "end": "2562910"
  },
  {
    "text": "customer problem has come up is replica resync when you have a failover event by",
    "start": "2562910",
    "end": "2568070"
  },
  {
    "text": "default Redis kind of just flushes the replicas and just as a whole resync from the master well I mean if you have you",
    "start": "2568070",
    "end": "2574400"
  },
  {
    "text": "know 30 gigs of memory that you need to resync it goes back to the Thundering Herd problem and you know flushing",
    "start": "2574400",
    "end": "2580550"
  },
  {
    "text": "basically all of your casts at the worst possible time when you're having a failover event so we've actually made an enhancements to piecing partial",
    "start": "2580550",
    "end": "2587450"
  },
  {
    "text": "synchronization that actually improves that situation and two more things just two new cloud watch metrics that you can",
    "start": "2587450",
    "end": "2595310"
  },
  {
    "text": "monitor or learn on and then as I mentioned 282 or later so that's it for",
    "start": "2595310",
    "end": "2601970"
  },
  {
    "text": "my part thanks a lot and I'm gonna hand it over to Tom Kerr from Riot Games who's going to talk about their usual",
    "start": "2601970",
    "end": "2607600"
  },
  {
    "text": "ElastiCache for their games",
    "start": "2607600",
    "end": "2611080"
  },
  {
    "text": "everyone tom Kerr software engineer at Riot Games it's really cool to work on",
    "start": "2617309",
    "end": "2623109"
  },
  {
    "text": "games games are how I got interested in programming I've been an avid gamer since I convinced my parents that I",
    "start": "2623109",
    "end": "2629049"
  },
  {
    "text": "needed a computer for schoolwork I got I got hooked playing Duke Nukem and red",
    "start": "2629049",
    "end": "2635619"
  },
  {
    "text": "alert over dial-up with friends and I used to string this 30-foot telephone cable through the house and every time",
    "start": "2635619",
    "end": "2641289"
  },
  {
    "text": "somebody walked through the house and stepped on the cable we'd get disconnected and have to start all over somehow our while writing this talk I",
    "start": "2641289",
    "end": "2649690"
  },
  {
    "text": "realize that somehow I'm still mad at my mother about this so after playing for a",
    "start": "2649690",
    "end": "2656109"
  },
  {
    "text": "while I started making maps and this is where I got started down the slippery slope if you get enough of these Maps",
    "start": "2656109",
    "end": "2662289"
  },
  {
    "text": "logistics get weird and you have to make websites to share the maps websites lead",
    "start": "2662289",
    "end": "2667630"
  },
  {
    "text": "to map add-ons map atoms lead to modding communities and so on all of this gaming",
    "start": "2667630",
    "end": "2673359"
  },
  {
    "text": "taught me a very important lesson if you're excited about something if you give a it makes the work feel a lot",
    "start": "2673359",
    "end": "2679930"
  },
  {
    "text": "like play so as I was working on my my",
    "start": "2679930",
    "end": "2685359"
  },
  {
    "text": "maps and my add-ons I always dreamed about working in games and I feel very fortunate to have that opportunity at",
    "start": "2685359",
    "end": "2692049"
  },
  {
    "text": "Riot Games we make a PC game called",
    "start": "2692049",
    "end": "2697839"
  },
  {
    "text": "League of Legends League is a team game 5 B 5 multiplayer battle arena you can",
    "start": "2697839",
    "end": "2704170"
  },
  {
    "text": "think of it as a pickup game of volleyball with players from across the continent there's less sand but we get",
    "start": "2704170",
    "end": "2709749"
  },
  {
    "text": "instead we get swords armor and magical spells and with over 120 champions all",
    "start": "2709749",
    "end": "2716380"
  },
  {
    "text": "with different abilities a big part of the fun is is learning how they'll interact with each other",
    "start": "2716380",
    "end": "2722219"
  },
  {
    "text": "oh right so league isn't just a game though it's",
    "start": "2728510",
    "end": "2734900"
  },
  {
    "text": "also an entire ecosystem of artists writers cosplayers streamers analysts",
    "start": "2734900",
    "end": "2741050"
  },
  {
    "text": "collegiate clubs all doing really cool stuff and really cool stuff that they're",
    "start": "2741050",
    "end": "2746720"
  },
  {
    "text": "excited about and this community is important to riot just as important as",
    "start": "2746720",
    "end": "2752420"
  },
  {
    "text": "all the players that are only playing it for the game in the end it's not about the game it's about the players players",
    "start": "2752420",
    "end": "2759380"
  },
  {
    "text": "are what drives our decisions our motives and our mission we aspire to be",
    "start": "2759380",
    "end": "2765740"
  },
  {
    "text": "the most player focused game company in the world my part in this mission is",
    "start": "2765740",
    "end": "2771560"
  },
  {
    "text": "writing software systems that help support these communities I work on riots commenting infrastructure called",
    "start": "2771560",
    "end": "2778190"
  },
  {
    "text": "Apollo it's a place where you can create discussions and talk about the things that you care about it's the first",
    "start": "2778190",
    "end": "2784040"
  },
  {
    "text": "project we'll talk about today second one is leaderboards which is a progression tracking system that helps",
    "start": "2784040",
    "end": "2792530"
  },
  {
    "text": "us create fun events and activities for players that's done by the by Daniel",
    "start": "2792530",
    "end": "2797720"
  },
  {
    "text": "Kang and the events interactive team",
    "start": "2797720",
    "end": "2801310"
  },
  {
    "text": "part of the challenge of our job is being able to scale these experiences 267 million players and delivering it so",
    "start": "2803410",
    "end": "2813590"
  },
  {
    "text": "delivering that means we need to have software systems at scale as well as teams at scale so as we walk through these two projects well talk about how",
    "start": "2813590",
    "end": "2820700"
  },
  {
    "text": "we've overcome some of these challenges and more specifically we'll talk about how we use elastic cash in our",
    "start": "2820700",
    "end": "2826310"
  },
  {
    "text": "architecture and how that helps us deliver value to players so let's jump",
    "start": "2826310",
    "end": "2834980"
  },
  {
    "text": "in the first project Apollo it's hard to imagine a world where you can't talk about things going on around you",
    "start": "2834980",
    "end": "2841100"
  },
  {
    "text": "discussion is important part of any community but communities don't just",
    "start": "2841100",
    "end": "2846260"
  },
  {
    "text": "have one discussion that one comment might have 20 different responses so when we say comments we mean nested",
    "start": "2846260",
    "end": "2852500"
  },
  {
    "text": "lists of comments any one of these comments can be voted on and that might",
    "start": "2852500",
    "end": "2857630"
  },
  {
    "text": "change how we display them the players this is important because it lets the",
    "start": "2857630",
    "end": "2864290"
  },
  {
    "text": "community control the narrative visibility is earned through players determination of value not from somebody",
    "start": "2864290",
    "end": "2871100"
  },
  {
    "text": "just typing the word bump so Paul is a flexible tool we can use it in a couple",
    "start": "2871100",
    "end": "2877550"
  },
  {
    "text": "ways first is a JavaScript widget that we can plug into any right web property it's",
    "start": "2877550",
    "end": "2882830"
  },
  {
    "text": "kind of like our own flavor of disgust we use that same service layer to power",
    "start": "2882830",
    "end": "2888260"
  },
  {
    "text": "riots boards boards are separated into different topics so if players want to",
    "start": "2888260",
    "end": "2893960"
  },
  {
    "text": "go talk about skin ideas or champion balance they can go they can go wherever",
    "start": "2893960",
    "end": "2899900"
  },
  {
    "text": "that makes sense so here's a look at the architecture behind Apollo 2 entry",
    "start": "2899900",
    "end": "2908330"
  },
  {
    "start": "2902000",
    "end": "2902000"
  },
  {
    "text": "points we just talked about or the commenting widget and the boards there",
    "start": "2908330",
    "end": "2914480"
  },
  {
    "text": "on the left after they you know so we communicate over rest api's into the Paulo core service and eventually that",
    "start": "2914480",
    "end": "2920570"
  },
  {
    "text": "makes it into ElastiCache so we use both Redis and memcache the boards only need",
    "start": "2920570",
    "end": "2927050"
  },
  {
    "text": "like simple kV stuff so we use memcache for that and then we use Redis for our",
    "start": "2927050",
    "end": "2933020"
  },
  {
    "text": "primary data store it's a bit weird use elastic cash for Redis as a primary data",
    "start": "2933020",
    "end": "2940580"
  },
  {
    "text": "store because boxes can go down and when they come back up they could be empty so",
    "start": "2940580",
    "end": "2946700"
  },
  {
    "text": "it's a not a pleasant feeling when that happens I can tell you from experience so we use Redis despite that for some of",
    "start": "2946700",
    "end": "2956090"
  },
  {
    "text": "the nice data types that they give us it's a really convenient model for us we'll talk about that in a little bit so",
    "start": "2956090",
    "end": "2964880"
  },
  {
    "text": "to compensate for possible data loss we design our system to deal with that in a few ways first is set up replication",
    "start": "2964880",
    "end": "2971600"
  },
  {
    "text": "with automatic failover something goes down just failover the replicas this is a good idea even if you're only using",
    "start": "2971600",
    "end": "2977150"
  },
  {
    "text": "Redis as a simple cache like if you're looking at failure conditions and you come up with a cold cash it's not gonna",
    "start": "2977150",
    "end": "2983870"
  },
  {
    "text": "be pleasant so we replicate across availability zone",
    "start": "2983870",
    "end": "2989300"
  },
  {
    "text": "designate said that makes you a little bit slower than staying in the same",
    "start": "2989300",
    "end": "2994940"
  },
  {
    "text": "but you don't have to worry about going dark if the zone goes down say if a",
    "start": "2994940",
    "end": "3001359"
  },
  {
    "text": "lightning hit one and then so",
    "start": "3001359",
    "end": "3010150"
  },
  {
    "text": "ElastiCache only automates snapshots every 24 hours because we're using it as a primary day so we have to be a little",
    "start": "3010150",
    "end": "3015609"
  },
  {
    "text": "bit more careful about that so they only automate every 24 hours but you can manually do more so we do it every 24",
    "start": "3015609",
    "end": "3022930"
  },
  {
    "text": "hours just on a cron job it's pretty simple oh and as you're setting these up make",
    "start": "3022930",
    "end": "3030369"
  },
  {
    "text": "sure to do it off of the setting up the snapshots make sure to do it off of the replica instead of your primary just get",
    "start": "3030369",
    "end": "3035890"
  },
  {
    "text": "less hiccups so why does the last Akash",
    "start": "3035890",
    "end": "3041530"
  },
  {
    "text": "and Redis make sense for boards well the short answer is sorting Rhetta says",
    "start": "3041530",
    "end": "3046900"
  },
  {
    "text": "some neat data types that had helped us manage all the up votes in down votes and all the rage and ecstasy that comes when we make changes the players",
    "start": "3046900",
    "end": "3053050"
  },
  {
    "text": "favorite champions it's a quick snippet which you've just committed to memory",
    "start": "3053050",
    "end": "3060720"
  },
  {
    "text": "here's the important stuff so we take the voting totals from these increments",
    "start": "3061440",
    "end": "3066970"
  },
  {
    "text": "in the code sample and we use them to create a score these scores are put into",
    "start": "3066970",
    "end": "3076780"
  },
  {
    "text": "Redis see sets the sort of sorted sets that Nate was talking about they're used to sort in two indices these indices are",
    "start": "3076780",
    "end": "3084099"
  },
  {
    "text": "ranked based on both recent activities and weird mathematical combinations of all the above to tell you what's hot",
    "start": "3084099",
    "end": "3090280"
  },
  {
    "text": "right now and perhaps surprisingly these",
    "start": "3090280",
    "end": "3097089"
  },
  {
    "text": "indices don't need to be ranked live we only calculate them every few minutes there doesn't really affect the player",
    "start": "3097089",
    "end": "3103180"
  },
  {
    "text": "experience so unless you're spamming refreshing the site you've probably never noticed let's just do it all right",
    "start": "3103180",
    "end": "3111220"
  },
  {
    "text": "so this lets us do some nice things behind the scenes let's let's do our",
    "start": "3111220",
    "end": "3117849"
  },
  {
    "text": "calculations in a single board and then use a union to roll everything up we can",
    "start": "3117849",
    "end": "3123760"
  },
  {
    "text": "mix and match or indices however we like it's very convenient",
    "start": "3123760",
    "end": "3127828"
  },
  {
    "text": "so deployment agility was one of our primary concerns on this project any time we have it spin up a new region or",
    "start": "3130110",
    "end": "3136150"
  },
  {
    "text": "need to support a new web property we might need to roll out a new deployment this doesn't happen every day obviously",
    "start": "3136150",
    "end": "3142810"
  },
  {
    "text": "but I think we're all familiar with the same tragic story everybody needs everything yesterday so ElastiCache",
    "start": "3142810",
    "end": "3151360"
  },
  {
    "text": "helps us stand up one of these silos in a couple hours even if we're being pokey lazy slow",
    "start": "3151360",
    "end": "3156610"
  },
  {
    "text": "really the hard part is making sure that you've edited your configuration after you copy pasted everything so once it's",
    "start": "3156610",
    "end": "3164410"
  },
  {
    "text": "up and running we use a custom tool built on top of Packer to generate our releases and manage our deploys",
    "start": "3164410",
    "end": "3169870"
  },
  {
    "text": "everything's done with simple command line scripts and Jenkins it's the easiest way and so all our deploys are",
    "start": "3169870",
    "end": "3177400"
  },
  {
    "text": "zero downtime unless we're doing something like resizing one of our Redis nodes which doesn't happen that often so",
    "start": "3177400",
    "end": "3187330"
  },
  {
    "text": "that wraps up the palo so a high-level overview of the apollo architecture and",
    "start": "3187330",
    "end": "3192550"
  },
  {
    "text": "how you can use ElastiCache as primary disor if you design for it you do this all without sacrificing your ability to",
    "start": "3192550",
    "end": "3199510"
  },
  {
    "text": "move fast with deployment next step is leader boards leader board supports",
    "start": "3199510",
    "end": "3208810"
  },
  {
    "text": "seasonal events that revolve around game lore holidays could be anything really so the events have a lot of buried content like cinematics are tie-ins with",
    "start": "3208810",
    "end": "3215860"
  },
  {
    "text": "the game they also have activities fund requests are fun quests like seeing all",
    "start": "3215860",
    "end": "3221590"
  },
  {
    "text": "the different shiny things exploring the story and so on so keeping track of",
    "start": "3221590",
    "end": "3228070"
  },
  {
    "text": "players progression through these events seems like something we would dud we would want to do which starts to sound a",
    "start": "3228070",
    "end": "3233410"
  },
  {
    "text": "lot like a progress bar progress bars aren't a new idea or anything but keeping them all up to date and",
    "start": "3233410",
    "end": "3239200"
  },
  {
    "text": "available in real time for millions of players is an interesting scale challenge so the illustrate we'll talk about",
    "start": "3239200",
    "end": "3246070"
  },
  {
    "text": "builds water event and pirates get in rewards by playing through a new Mass",
    "start": "3246070",
    "end": "3252040"
  },
  {
    "text": "spending a lot of prior coin and then finally playing a game as gangplank these numbers aren't quite exactly but I",
    "start": "3252040",
    "end": "3258460"
  },
  {
    "text": "can't give you a sense of the scale the peak we saw for requests was about three million requests per minute in",
    "start": "3258460",
    "end": "3264610"
  },
  {
    "text": "over the course of three weeks Redis saw around forty three billion reads and 52",
    "start": "3264610",
    "end": "3269920"
  },
  {
    "text": "million writes kind of ground that back in terms of non-engineering talked",
    "start": "3269920",
    "end": "3275530"
  },
  {
    "text": "players completed enough quests that that 52 million rewards were given out",
    "start": "3275530",
    "end": "3284220"
  },
  {
    "start": "3284000",
    "end": "3284000"
  },
  {
    "text": "so here's the architecture that handles all those games and events so players",
    "start": "3285720",
    "end": "3294190"
  },
  {
    "text": "play games right when game events are stored into leaderboards via the game processors players come in to the other",
    "start": "3294190",
    "end": "3301540"
  },
  {
    "text": "side and query for their progress leaderboards a much different footprint than Apollo Paulo shards livery and",
    "start": "3301540",
    "end": "3309010"
  },
  {
    "text": "deploys all over Redis has a global deploy in scales vertically in a really big way these events have a bit of a",
    "start": "3309010",
    "end": "3321130"
  },
  {
    "start": "3316000",
    "end": "3316000"
  },
  {
    "text": "black friday feel to them everybody dogpiling an event puts a lot",
    "start": "3321130",
    "end": "3326560"
  },
  {
    "text": "of pressure on elastic cash so we try to offload as much traffic up from the",
    "start": "3326560",
    "end": "3332200"
  },
  {
    "text": "primary as possible each primary has three read replicas all in the same",
    "start": "3332200",
    "end": "3338560"
  },
  {
    "text": "availability zone we leaderboards was much more worried about keeping their",
    "start": "3338560",
    "end": "3343619"
  },
  {
    "text": "replicas in sync so this makes sense for them remember Apollo is split across",
    "start": "3343619",
    "end": "3349690"
  },
  {
    "text": "availability zones this is a trade-off that you'll need to think about when",
    "start": "3349690",
    "end": "3355510"
  },
  {
    "text": "you're designing your system whether you want to be your replication to be faster",
    "start": "3355510",
    "end": "3361119"
  },
  {
    "text": "or whether you want to be more worried about availability like no choice is really wrong it's just outside of the",
    "start": "3361119",
    "end": "3367869"
  },
  {
    "text": "context of your project",
    "start": "3367869",
    "end": "3371130"
  },
  {
    "text": "so I said we hit three million at peak in production per minute but during the",
    "start": "3373830",
    "end": "3380980"
  },
  {
    "text": "pre-production stress test saw where we're hidden safe operation went upwards of 24 million requests per minute and",
    "start": "3380980",
    "end": "3388599"
  },
  {
    "text": "the Redis latency only was 0.5 to 3 milliseconds with no replication lag one",
    "start": "3388599",
    "end": "3398890"
  },
  {
    "text": "thing we learned during these stress tests was that network can significantly bottleneck your replication it sounds",
    "start": "3398890",
    "end": "3407560"
  },
  {
    "text": "kind of pedestrian when I say that but usually you're not thinking about network when you're right sizing your",
    "start": "3407560",
    "end": "3413140"
  },
  {
    "text": "nodes usually thinking about memory footprint but so as we were running",
    "start": "3413140",
    "end": "3418300"
  },
  {
    "text": "these tests we were turning knobs and seeing what happened we noticed that replication lag was creeping up when",
    "start": "3418300",
    "end": "3424270"
  },
  {
    "text": "we're using smaller instances you can just simply go into cloud watch and see that the network bandwidth was capped",
    "start": "3424270",
    "end": "3430410"
  },
  {
    "text": "switch back to a larger instance size and the problem just goes away so when",
    "start": "3430410",
    "end": "3436420"
  },
  {
    "text": "your team is setting up remaining I'd recommend adding replication lag your list of stuff to monitor can help see",
    "start": "3436420",
    "end": "3443800"
  },
  {
    "text": "stuff before it becomes too late in production another cool trick we learned",
    "start": "3443800",
    "end": "3451960"
  },
  {
    "text": "along the way was optimizing the way we store our keys if you have really long repetitive keys your keys might be",
    "start": "3451960",
    "end": "3458650"
  },
  {
    "text": "taking up more space and your values actually are so what you can do is split",
    "start": "3458650",
    "end": "3465609"
  },
  {
    "text": "your keys and put those into your hat into a hash that hash becomes your new key this seems crazy the small",
    "start": "3465609",
    "end": "3472300"
  },
  {
    "text": "optimization alone reduced our memory footprint by 70% there's a few caveats",
    "start": "3472300",
    "end": "3478240"
  },
  {
    "text": "to using this you can read about it in the docs",
    "start": "3478240",
    "end": "3483720"
  },
  {
    "text": "so leaderboard showed us sorry something different than Apollo last the cash and Redis are flexible enough tools that you",
    "start": "3488690",
    "end": "3497160"
  },
  {
    "text": "can use it to scale in a very birth in a vertical way and get some really impressive numbers so this has been a",
    "start": "3497160",
    "end": "3502500"
  },
  {
    "text": "really great project for players it's a great example of something that is very simple in concept but when you start to",
    "start": "3502500",
    "end": "3509880"
  },
  {
    "text": "scale out becomes complex but if you get the right engineers using the right tools you can get back to simplicity",
    "start": "3509880",
    "end": "3515329"
  },
  {
    "text": "that makes software fun and successful",
    "start": "3515329",
    "end": "3520880"
  },
  {
    "text": "so if you were taking notes hopefully these are some of the points you picked",
    "start": "3523460",
    "end": "3528900"
  },
  {
    "text": "up go back over some best practices set up your replicas with automatic failover",
    "start": "3528900",
    "end": "3535380"
  },
  {
    "text": "and snapshot more often if you're not",
    "start": "3535380",
    "end": "3540690"
  },
  {
    "text": "using it as a cache this old help you with safety simple stuff and I know this",
    "start": "3540690",
    "end": "3547470"
  },
  {
    "text": "isn't a very popular opinion but you are",
    "start": "3547470",
    "end": "3552480"
  },
  {
    "text": "going to have to set up monitoring ad replication your list of stuff to",
    "start": "3552480",
    "end": "3557550"
  },
  {
    "text": "monitor you can also use the Redis hash key trick to save a few extra bits so",
    "start": "3557550",
    "end": "3567569"
  },
  {
    "text": "that's all I have I'm gonna be around after this so is Daniel from the events team so if you",
    "start": "3567569",
    "end": "3573720"
  },
  {
    "text": "have any questions come say hello thanks",
    "start": "3573720",
    "end": "3578210"
  }
]