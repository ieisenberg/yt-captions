[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "thanks for coming everyone my name is Tim Griffith I'm a principal engineer working for the Amazon retail business",
    "start": "0",
    "end": "6899"
  },
  {
    "text": "rather than Amazon Web Services based out of New York and I'd like to talk to you today about how we changed the way",
    "start": "6899",
    "end": "15120"
  },
  {
    "text": "that we worked in terms of our orchestration to solve two key problems that we had by using Amazon Kinesis so",
    "start": "15120",
    "end": "22859"
  },
  {
    "text": "what I'd like to go through this session is first of all a little bit of background about how the Amazon retail",
    "start": "22859",
    "end": "29070"
  },
  {
    "start": "23000",
    "end": "23000"
  },
  {
    "text": "business works in terms of how we actually take listing information in from our vendors and sellers I'd like to",
    "start": "29070",
    "end": "36149"
  },
  {
    "text": "go through the challenges that we faced as we've grown as a business and why we needed to do something differently and",
    "start": "36149",
    "end": "41969"
  },
  {
    "text": "then I'd like to dive into the method that we took and why we took it in terms",
    "start": "41969",
    "end": "47579"
  },
  {
    "text": "of using Kinesis as our back-end Orchestrator and so I'd like to look at",
    "start": "47579",
    "end": "52860"
  },
  {
    "text": "first of all the cost mechanics why we did it from a cost perspective why we did it from a distributed coordination",
    "start": "52860",
    "end": "59149"
  },
  {
    "text": "perspective as well as other benefits that we got from working with the distributed log and one of the things",
    "start": "59149",
    "end": "66030"
  },
  {
    "text": "that I hope you take away from this session is that the Kinesis is not just an analytics engine it is very much a",
    "start": "66030",
    "end": "72869"
  },
  {
    "text": "full distributed log and some of the things that we can do with that are fantastic so that's really the",
    "start": "72869",
    "end": "78540"
  },
  {
    "text": "motivation if you like for this talk for me so I want to go through how we got",
    "start": "78540",
    "end": "84560"
  },
  {
    "text": "2/3 of our costs reduced as well as simplifying the processing semantics of the downstream systems that we were",
    "start": "84560",
    "end": "91710"
  },
  {
    "text": "using this Orchestrator to compose over the architecture that we came up with",
    "start": "91710",
    "end": "96960"
  },
  {
    "text": "something that looks like this so has a gateway that gateway has a settings or",
    "start": "96960",
    "end": "103790"
  },
  {
    "text": "SLA configuration so when someone submits work to the system it will go",
    "start": "103790",
    "end": "109530"
  },
  {
    "text": "and work out contractually when it should execute that work and encode that information is where handoff to Kinesis",
    "start": "109530",
    "end": "116460"
  },
  {
    "text": "and then return that contract back to the user this is a very important thing for us and we'll get into why as we go",
    "start": "116460",
    "end": "123509"
  },
  {
    "text": "forward the handoff from Kinesis goes to the backend worker where it uses processing",
    "start": "123509",
    "end": "131540"
  },
  {
    "text": "configuration to work out first of all what is required before that process can",
    "start": "131540",
    "end": "136590"
  },
  {
    "text": "actually run and then when it goes to dispatch it which process incidents should run first so this gives us a",
    "start": "136590",
    "end": "143280"
  },
  {
    "text": "sense of priority as well as ordering we then talk about handing that work off to",
    "start": "143280",
    "end": "149280"
  },
  {
    "text": "action handlers if anyone's work with swf it's very similar in terms of a handler and then those in term will also",
    "start": "149280",
    "end": "157260"
  },
  {
    "text": "use Kinesis to respond back to the orchestrator and when the process is done it will finally communicate back to",
    "start": "157260",
    "end": "163620"
  },
  {
    "text": "the user so a full asynchronous model now in order to understand why we went",
    "start": "163620",
    "end": "170130"
  },
  {
    "text": "and built this we need to really start off with a little bit of information about how listings work on amazon.com",
    "start": "170130",
    "end": "176930"
  },
  {
    "text": "so first of all we always start with a product so this is something that we're",
    "start": "176930",
    "end": "182340"
  },
  {
    "start": "178000",
    "end": "178000"
  },
  {
    "text": "selling and we're really talking about the facts around that lighting so we need to identify it that identifier is",
    "start": "182340",
    "end": "189810"
  },
  {
    "text": "unique to a particular seller or merchant and so they will have some kind",
    "start": "189810",
    "end": "195239"
  },
  {
    "text": "of stock keeping unit or ID and they will provide a series of facts about the",
    "start": "195239",
    "end": "201660"
  },
  {
    "text": "item so it can be very simple things such as what type of product how to",
    "start": "201660",
    "end": "207630"
  },
  {
    "text": "locate that product what color it is product dimensions so size weight etc",
    "start": "207630",
    "end": "213120"
  },
  {
    "text": "who made it brand a whole series of facts that help make the product",
    "start": "213120",
    "end": "218370"
  },
  {
    "text": "interesting and the correct product discoverable for our users moving on from there we obviously not",
    "start": "218370",
    "end": "225720"
  },
  {
    "text": "only want to describe a product we want to be able to sell it so there needs to be some kind of offering condition which",
    "start": "225720",
    "end": "231180"
  },
  {
    "text": "says how much we're going to sell it for and when that price is going to hold so it might be that we have a normal price",
    "start": "231180",
    "end": "237359"
  },
  {
    "text": "we might have a promotional price we may not be able to sell it until a certain period in the future",
    "start": "237359",
    "end": "243079"
  },
  {
    "text": "finally we also have the ability to relate items so if you think about",
    "start": "243079",
    "end": "249139"
  },
  {
    "text": "products some products inherently have sort of groups of the same product the",
    "start": "249139",
    "end": "254850"
  },
  {
    "text": "example we use today is a shoe so if I'm selling a branded shoe there going to be",
    "start": "254850",
    "end": "260609"
  },
  {
    "text": "lots of facts that are the same but yes I still need to have individual products for each size and each color because I",
    "start": "260609",
    "end": "267780"
  },
  {
    "text": "need be able to identify when someone has ordered a size 7 and a half black and green shoe that I send them the",
    "start": "267780",
    "end": "274620"
  },
  {
    "text": "right product so we still need these detailed specific products yet at the same time we probably don't want to",
    "start": "274620",
    "end": "280139"
  },
  {
    "text": "think of them as all specific products because we want to be able to help the user discover these things",
    "start": "280139",
    "end": "286520"
  },
  {
    "text": "so if we were to build this from scratch you might think you know given the AWS",
    "start": "286520",
    "end": "291810"
  },
  {
    "text": "components out there that actually this would be really easy to put together so shared-nothing is easy we could imagine",
    "start": "291810",
    "end": "297479"
  },
  {
    "text": "that if we start from our sellers that would have a big API fleet and then from",
    "start": "297479",
    "end": "302699"
  },
  {
    "text": "our API fleet it would then push down into a no sequel database we need to get",
    "start": "302699",
    "end": "308610"
  },
  {
    "text": "that no sequel database into a form that can be searched obviously people aren't going to search by my particular stock",
    "start": "308610",
    "end": "314400"
  },
  {
    "text": "keeping unit my primary ID so they're going to need to get it into a reverse index obviously cloud searches up here",
    "start": "314400",
    "end": "320220"
  },
  {
    "text": "but as you've heard recently that could easily be elastic search now and we have",
    "start": "320220",
    "end": "325349"
  },
  {
    "text": "DynamoDB streams that would allow us to plug that in seamlessly especially with elastic search so we could get up to a",
    "start": "325349",
    "end": "332310"
  },
  {
    "text": "reverse index quite easily and then working in from the customers visiting the website we'd imagine well there are",
    "start": "332310",
    "end": "338550"
  },
  {
    "text": "lots more of them so what have an even bigger API fleet we don't want to hit that a traverse",
    "start": "338550",
    "end": "344099"
  },
  {
    "text": "index for every single request we imagine there's going to be lots of commonality so imagine a huge caching fleet but we can come up with a fairly",
    "start": "344099",
    "end": "351180"
  },
  {
    "text": "simple architecture albeit one that operates a very large scale so does this equal Amazon well maybe the right hand",
    "start": "351180",
    "end": "359250"
  },
  {
    "text": "side reasonable representation but the left hand side not so much so let's",
    "start": "359250",
    "end": "364710"
  },
  {
    "text": "let's go a little bit through why now we can start off with the fact that we do have the world's largest selection of",
    "start": "364710",
    "end": "370169"
  },
  {
    "text": "products and we have the largest number of sellers on those products enormous",
    "start": "370169",
    "end": "375270"
  },
  {
    "text": "numbers of sellers and so if we imagine that we're building a service that's going to show you each unique sellers",
    "start": "375270",
    "end": "382080"
  },
  {
    "text": "version of a product and imagine doing a product search so if I'm searching for",
    "start": "382080",
    "end": "387120"
  },
  {
    "text": "Knight trainers even if I put the shoe size in maybe I want to see what colors",
    "start": "387120",
    "end": "392940"
  },
  {
    "text": "are available and I'm going to end up having enormous numbers of exactly the",
    "start": "392940",
    "end": "397949"
  },
  {
    "text": "same product listed pages and pages of them potentially tens of it makes actually very difficult to find the",
    "start": "397949",
    "end": "404310"
  },
  {
    "text": "actual product that we want so from a user experience perspective we want to provide a single product we want them",
    "start": "404310",
    "end": "412169"
  },
  {
    "text": "people to be able to work from there to see okay what sort of configuration around this product what what kind of",
    "start": "412169",
    "end": "418259"
  },
  {
    "text": "instance of that product am I particularly looking for and then finally from there to be able to go to",
    "start": "418259",
    "end": "423389"
  },
  {
    "text": "well who's selling this to work out who I should actually buy the products with the specification from so we built the",
    "start": "423389",
    "end": "432210"
  },
  {
    "start": "430000",
    "end": "430000"
  },
  {
    "text": "concept of the the single item detail page and keeping with the shoe analogy here here we can see a Nike shoe you can",
    "start": "432210",
    "end": "439379"
  },
  {
    "text": "see that there are on the right-hand side a large number of different colors and a large number of different sizes now the important thing and the reason",
    "start": "439379",
    "end": "446759"
  },
  {
    "text": "why we've pulled this together as a single page is that you can see that the the core information about the product",
    "start": "446759",
    "end": "452430"
  },
  {
    "text": "so the description that this is a Nike free trainer 5-0 and the sort of bullet",
    "start": "452430",
    "end": "458370"
  },
  {
    "text": "points at the bottom that describe some of the core facts about this they're going to be the same no matter what size",
    "start": "458370",
    "end": "463469"
  },
  {
    "text": "that shoe is no matter what color it is now we then pull up all of those children in order to provide this sort",
    "start": "463469",
    "end": "471029"
  },
  {
    "text": "of matrix sparse matrix of what colors are available and then if you choose the",
    "start": "471029",
    "end": "476819"
  },
  {
    "text": "color first it will show you what sizes if you choose the size first we'll show you what colors are available in that size so we've moved from a sort of a",
    "start": "476819",
    "end": "484979"
  },
  {
    "text": "grouping that has been pulled together of items into the specific item and then",
    "start": "484979",
    "end": "490770"
  },
  {
    "text": "finally once we've got that specific item we'll start off with who we think is the best seller to buy that from and",
    "start": "490770",
    "end": "496800"
  },
  {
    "text": "we're trying to make that on behalf of the users so you know if they have a prime account then maybe it's one which",
    "start": "496800",
    "end": "503520"
  },
  {
    "text": "has free prime to day shipping and obviously there are other sellers and we",
    "start": "503520",
    "end": "508529"
  },
  {
    "text": "will if this was a little bit wider we would show that the other sellers on the right-hand side of the page where you",
    "start": "508529",
    "end": "515578"
  },
  {
    "text": "could choose other sellers so we've tried to bring it to product selection first so that we do discovery first and",
    "start": "515579",
    "end": "521698"
  },
  {
    "text": "then who you're buying from second now this makes things a little bit more",
    "start": "521699",
    "end": "527550"
  },
  {
    "start": "527000",
    "end": "527000"
  },
  {
    "text": "complex from the architectures perspective so whilst indeed we do receive essentially each seller has",
    "start": "527550",
    "end": "534000"
  },
  {
    "text": "their own catalog we need to bring that information together so we need to first",
    "start": "534000",
    "end": "539590"
  },
  {
    "text": "of all standardize the information and validate it the important thing behind standardization is because in order to",
    "start": "539590",
    "end": "546040"
  },
  {
    "text": "bring things together we need to match them and if we don't have standardized values it'll be difficult to match them",
    "start": "546040",
    "end": "552390"
  },
  {
    "text": "we're going to match them and say okay you appear to be talking about this particular Nike product okay now we end",
    "start": "552390",
    "end": "560740"
  },
  {
    "text": "up with lots of information from lots of different sellers describing that product some of it can be contradictory",
    "start": "560740",
    "end": "566670"
  },
  {
    "text": "some of it is overlapping but what we have is an abundance of data and what we",
    "start": "566670",
    "end": "571960"
  },
  {
    "text": "do want to do in this position is we want to do data reconciliation we want to have a look at who's provided the",
    "start": "571960",
    "end": "577870"
  },
  {
    "text": "information the quality of the information they've provided and for each attribute for each product fact we",
    "start": "577870",
    "end": "585040"
  },
  {
    "text": "want to choose what is the best value or the best set of values so that we come up with the best most complete overall",
    "start": "585040",
    "end": "591460"
  },
  {
    "text": "description and then once we've got that we want to store that down by its own proprietary identifier and this is the",
    "start": "591460",
    "end": "598300"
  },
  {
    "text": "Amazon catalog ID now because we've done this when we think about those other",
    "start": "598300",
    "end": "603880"
  },
  {
    "text": "pieces of information so the the offer of piece of information what I'm going to sell it for as well as the",
    "start": "603880",
    "end": "610450"
  },
  {
    "text": "relationship information this has to be relative to the Amazon catalog so we",
    "start": "610450",
    "end": "616090"
  },
  {
    "text": "have to wait until we understand from my SKU what does that match to or what has",
    "start": "616090",
    "end": "621400"
  },
  {
    "text": "that created in the Amazon catalog before I can then put my offer and that offer has to be relative to that ID",
    "start": "621400",
    "end": "627580"
  },
  {
    "text": "because of the structure that we have as you saw on the previous pages we need to",
    "start": "627580",
    "end": "632890"
  },
  {
    "text": "be able to navigate first by product and then once we've got the product selected which is the Amazon ID selected we then",
    "start": "632890",
    "end": "639280"
  },
  {
    "text": "want to navigate the offers relative to that ID so this requires top-level orchestration",
    "start": "639280",
    "end": "645970"
  },
  {
    "text": "fundamentally we need to process product first and then once we've process product we can then process some of the",
    "start": "645970",
    "end": "652000"
  },
  {
    "text": "other data types this is a simplified version obviously but it gets the the",
    "start": "652000",
    "end": "657130"
  },
  {
    "text": "main thrust of our problem spaced across now the easiest and cheapest way when",
    "start": "657130",
    "end": "664030"
  },
  {
    "text": "you're dealing with an Orchestrator especially as the majority of our information comes through like feeds or product templates is to do",
    "start": "664030",
    "end": "671249"
  },
  {
    "text": "the orchestration in aggregate so imagine that I have a feed that has 10,000 things in I'm updating my catalog",
    "start": "671249",
    "end": "678300"
  },
  {
    "text": "to Amazon it can be reasonable to do all of the product first so let's say out of",
    "start": "678300",
    "end": "684120"
  },
  {
    "text": "those 10,000 updates five and a half thousands of them our product and then once I've done all my products updates I",
    "start": "684120",
    "end": "690149"
  },
  {
    "text": "can then do all of my offers and all of my relationships potentially at the same",
    "start": "690149",
    "end": "695730"
  },
  {
    "text": "time now that can obviously be not very responsive and running for a feed",
    "start": "695730",
    "end": "701040"
  },
  {
    "text": "processing platform isn't very responsive if we have a user interaction so if someone comes in to do a correction",
    "start": "701040",
    "end": "706879"
  },
  {
    "text": "they come in through the interactive user experiences then they want to do it",
    "start": "706879",
    "end": "712379"
  },
  {
    "text": "individually and they want a much faster update time so in this situation we want to have you know one by one",
    "start": "712379",
    "end": "718499"
  },
  {
    "text": "orchestration so we have both and the orchestration approach differs a little",
    "start": "718499",
    "end": "724470"
  },
  {
    "text": "bit as it stands in auditor to manage that so was that we've solved this",
    "start": "724470",
    "end": "731370"
  },
  {
    "text": "problem in fact well we solved this problem early in the century so around",
    "start": "731370",
    "end": "736559"
  },
  {
    "start": "733000",
    "end": "733000"
  },
  {
    "text": "about the turn of the century so what changed well we took on a lot more",
    "start": "736559",
    "end": "742800"
  },
  {
    "text": "sellers and a lot more products we are in much much bigger market place than we were 15 years or so ago and with that we",
    "start": "742800",
    "end": "751290"
  },
  {
    "text": "end up having this kind of pattern so we can see that as more sellers come in",
    "start": "751290",
    "end": "757040"
  },
  {
    "text": "lots of those sellers they don't distribute their products evenly we actually end up getting more and more",
    "start": "757040",
    "end": "762149"
  },
  {
    "text": "sellers on a subsection of our products particularly popular products so we",
    "start": "762149",
    "end": "767639"
  },
  {
    "text": "might end up having multiple coming in and the problem with this is that we can",
    "start": "767639",
    "end": "772889"
  },
  {
    "text": "get a very large number of sellers on a given product and this as you can see is not shared nothing this is very much",
    "start": "772889",
    "end": "780839"
  },
  {
    "text": "bringing data together and that gives us a problem now we originally were",
    "start": "780839",
    "end": "785999"
  },
  {
    "text": "optimistic and how we handled this so we would try and update the catalog and we would see if it's changed underneath us",
    "start": "785999",
    "end": "792059"
  },
  {
    "text": "and if it had we just reread it and go and apply our change to it again now this ended up",
    "start": "792059",
    "end": "798040"
  },
  {
    "text": "a reasonable time ago ended up getting rather significant in terms of its overhead and so we ended up moving to",
    "start": "798040",
    "end": "804670"
  },
  {
    "text": "pessimistic sequencing so we would physically say I am going to update the site and lock that item and then proceed",
    "start": "804670",
    "end": "811509"
  },
  {
    "text": "doing our updates now what that does is that sequences or serializers each of",
    "start": "811509",
    "end": "817660"
  },
  {
    "text": "those updates coming in so if I'm the first person in absolutely fine if I'm",
    "start": "817660",
    "end": "822819"
  },
  {
    "text": "the second person in I have to wait for the first person's update to finish if I have 200 people coming in and I'm the",
    "start": "822819",
    "end": "830740"
  },
  {
    "text": "200th I may have to weigh up to 200 other people to complete their processing in order for my processing to",
    "start": "830740",
    "end": "836980"
  },
  {
    "text": "get in so because the number of people that are updating at the same time is not very deterministic we ended up",
    "start": "836980",
    "end": "844089"
  },
  {
    "text": "having very unpredictable performance when are we going to be able to complete this work it depends on how many other",
    "start": "844089",
    "end": "850000"
  },
  {
    "text": "people are updating the same product at the same time it's not something that we can really control likewise the feed processing being that",
    "start": "850000",
    "end": "858069"
  },
  {
    "text": "macro orchestration is doing in layers it ended up compounding the performance",
    "start": "858069",
    "end": "863260"
  },
  {
    "text": "problems if I have problems with multiple products then they end up being",
    "start": "863260",
    "end": "869290"
  },
  {
    "text": "added together often and then if I get to a problem with a relationship afterwards then that time is also",
    "start": "869290",
    "end": "875769"
  },
  {
    "text": "accumulated as a result the volatility in terms of how long it will take to process a feed when it's submitted to us",
    "start": "875769",
    "end": "882040"
  },
  {
    "text": "is even greater than in a one by one situation we also had a problem in that we were",
    "start": "882040",
    "end": "888430"
  },
  {
    "text": "running these two different orchestrators so one by one behaves in a different way to the feeds based system",
    "start": "888430",
    "end": "896019"
  },
  {
    "text": "and as a result we actually had some sequencing issues because someone might send a feed in the unpredictable",
    "start": "896019",
    "end": "902649"
  },
  {
    "text": "performance means that they feel it should have completed by now but hasn't so they go to the one-by-one experience",
    "start": "902649",
    "end": "908470"
  },
  {
    "text": "because it's important and go and send in that update now that update can then overtake the feed and so we now have to",
    "start": "908470",
    "end": "915279"
  },
  {
    "text": "cope with the situation whereby we don't take what was in the feed especially if it was different and overwrite the more",
    "start": "915279",
    "end": "921279"
  },
  {
    "text": "up up-to-date one by one submission so the final one for this is that",
    "start": "921279",
    "end": "927279"
  },
  {
    "text": "besides users taking it into their own hands because you know about date hasn't got",
    "start": "927279",
    "end": "932590"
  },
  {
    "text": "we also need to cope with the fact that many people will want to understand where where their stuff is you know why",
    "start": "932590",
    "end": "938770"
  },
  {
    "text": "has this not reflected on the site why has this not processed so it leads to regular calls regular contacts from the",
    "start": "938770",
    "end": "946240"
  },
  {
    "text": "users which is obviously painful for them and of course it's painful for the development team so from this we ended up with a series of",
    "start": "946240",
    "end": "953620"
  },
  {
    "text": "requirements that we have to work through the first of which from the client we",
    "start": "953620",
    "end": "958900"
  },
  {
    "start": "956000",
    "end": "956000"
  },
  {
    "text": "need to be able to reason when the work will be done and we need to be able to check for a progress of a submission",
    "start": "958900",
    "end": "964330"
  },
  {
    "text": "when we haven't met our expectation but the most important one of these is to be",
    "start": "964330",
    "end": "969580"
  },
  {
    "text": "able to give the user a pretty solid guarantee of when we will actually complete that work now we'll get into",
    "start": "969580",
    "end": "976060"
  },
  {
    "text": "parse of how to achieve that in a bit but one important thing from that is is that we already established the feed",
    "start": "976060",
    "end": "982330"
  },
  {
    "text": "processing platform increase the volatility so and we had a",
    "start": "982330",
    "end": "987520"
  },
  {
    "text": "sequencing problem so this pretty much came down to we needed to be able to do it one by one for everything from the",
    "start": "987520",
    "end": "994780"
  },
  {
    "text": "services that were being orchestrated we wanted to be able to guarantee those services that they were being called",
    "start": "994780",
    "end": "1000630"
  },
  {
    "text": "with the most important work that was available and whilst maintaining strong",
    "start": "1000630",
    "end": "1006120"
  },
  {
    "text": "ordering so that those services don't have to cope with every single one of those services doing an effective",
    "start": "1006120",
    "end": "1011670"
  },
  {
    "text": "version or similar in order to try and work out what the correct representation",
    "start": "1011670",
    "end": "1016680"
  },
  {
    "text": "of the of the work was and to handle out of order updates because it gets expensive again in terms of storage we",
    "start": "1016680",
    "end": "1024240"
  },
  {
    "text": "also have a series of situations as we saw with that funnel in a few slides ago",
    "start": "1024240",
    "end": "1030810"
  },
  {
    "text": "that we end up having lots of work that's on the same key now in many of the cases for us that work could",
    "start": "1030810",
    "end": "1037589"
  },
  {
    "text": "actually be done in a single interaction we could take that work together so we want something from a moral castration",
    "start": "1037589",
    "end": "1043260"
  },
  {
    "text": "platform that can actually give us badging and then finally from the non-functional perspective we want to be",
    "start": "1043260",
    "end": "1049530"
  },
  {
    "text": "able to run as fast as possible a big part of being able to meet contractual guarantees is by being able to make the",
    "start": "1049530",
    "end": "1055410"
  },
  {
    "text": "platform really fast the other part we want it to be cheap we don't want to",
    "start": "1055410",
    "end": "1061050"
  },
  {
    "text": "make going to one by one which gives us the fast as possible to compromise a song cost in fact ideally we'd like to",
    "start": "1061050",
    "end": "1067890"
  },
  {
    "text": "try and reduce the cost of the platform as we go through so this is where we started from so if we move on to our",
    "start": "1067890",
    "end": "1076760"
  },
  {
    "text": "target architecture and then we simplify it a little bit we end up with a",
    "start": "1076760",
    "end": "1081900"
  },
  {
    "text": "interaction whereby we have the seller and the seller needs to be able to submit to the orchestrator and get a",
    "start": "1081900",
    "end": "1088470"
  },
  {
    "text": "guarantee of when this is going to complete the orchestrator then needs to make sure that it executes work whilst",
    "start": "1088470",
    "end": "1095070"
  },
  {
    "text": "preserving ordering and sequencing ordering and priority sorry and then over a series of services so the first",
    "start": "1095070",
    "end": "1103530"
  },
  {
    "text": "and obvious answer to this is well can we just use swf now in our particular",
    "start": "1103530",
    "end": "1109140"
  },
  {
    "start": "1106000",
    "end": "1106000"
  },
  {
    "text": "case and these numbers are illustrative let's say that we're running at 5,000 transactions a second we are a big high",
    "start": "1109140",
    "end": "1116370"
  },
  {
    "text": "cardinality high-volume updates service so if we were running at 5,000",
    "start": "1116370",
    "end": "1121710"
  },
  {
    "text": "transactions a second that would translate to about $1,800 for execution",
    "start": "1121710",
    "end": "1127890"
  },
  {
    "text": "and then the additional tasks would be another $900 per hour so $2,700 per hour",
    "start": "1127890",
    "end": "1134610"
  },
  {
    "text": "now that works out to nearly 24 million dollars a year for the orchestration",
    "start": "1134610",
    "end": "1140130"
  },
  {
    "text": "platform and that's definitely not going to be doing it as cheap as possible that's significantly raising costs so we",
    "start": "1140130",
    "end": "1147660"
  },
  {
    "text": "have to start off saying that this isn't viable for us it would be really nice to have an out-of-the-box service but what",
    "start": "1147660",
    "end": "1153300"
  },
  {
    "text": "we're doing is somewhat unique we'll be operating several orders of magnitude out of what swf is designed to provide",
    "start": "1153300",
    "end": "1160590"
  },
  {
    "text": "so we need to we need to step into this ourselves so the first way that you would do an",
    "start": "1160590",
    "end": "1167220"
  },
  {
    "text": "asynchronous Orchestrator if you were to go and step in and build it is you keep durable state for your asynchronous",
    "start": "1167220",
    "end": "1175080"
  },
  {
    "text": "continuations so whenever you send out work to a client you need to retain the",
    "start": "1175080",
    "end": "1180570"
  },
  {
    "text": "state of what you sent out to them so that when they eventually complete and call back to you asynchronously you can",
    "start": "1180570",
    "end": "1186410"
  },
  {
    "text": "hydrate that process you can join the resultant information and then you can move to the next step we can decouple a",
    "start": "1186410",
    "end": "1194100"
  },
  {
    "text": "handoff both to the services and the seller by using something like SQS and this is this is a fairly simple",
    "start": "1194100",
    "end": "1201310"
  },
  {
    "text": "fella very fairly rudimentary implementation but we can work out that this is going to scale for us we can",
    "start": "1201310",
    "end": "1208760"
  },
  {
    "text": "cost it out and we get down to something which costs around about 188 K a year so",
    "start": "1208760",
    "end": "1214520"
  },
  {
    "text": "this is this is a lot more viable obviously there's a lot more build for us we would have to build that central",
    "start": "1214520",
    "end": "1220070"
  },
  {
    "text": "Orchestrator we would have to manage and own it so there's definitely significant dev cost and ops cost but this is at",
    "start": "1220070",
    "end": "1226610"
  },
  {
    "text": "least viable however we did feel that we could do better and we wanted to investigate further so the first thing",
    "start": "1226610",
    "end": "1233060"
  },
  {
    "text": "that we started looking at was well one of our big costs was handing off to the services and we'd seen the Kinesis",
    "start": "1233060",
    "end": "1239750"
  },
  {
    "text": "pricing and the Kinesis pricing looked good so we went and said well can we go and use a Kinesis worker out to the",
    "start": "1239750",
    "end": "1245930"
  },
  {
    "text": "services the services are going to be working in batch anyway most the time or in small batches so it makes sense we",
    "start": "1245930",
    "end": "1252860"
  },
  {
    "text": "can we can substitute in reasonably easily and this was interesting because this gave us a substantially lower cost",
    "start": "1252860",
    "end": "1259310"
  },
  {
    "text": "for that telemetry we actually dropped from approximately eight and a half dollars an hour to by substituting out",
    "start": "1259310",
    "end": "1266990"
  },
  {
    "text": "that second phase of SQS it dropped to about 40 cents an hour now that's a big deal that's an order of magnitude cost",
    "start": "1266990",
    "end": "1273710"
  },
  {
    "text": "savings so we were pretty excited about this but then we started pushing further so the next one the next most expensive",
    "start": "1273710",
    "end": "1280820"
  },
  {
    "start": "1277000",
    "end": "1277000"
  },
  {
    "text": "thing is storage the biggest cost there is is dynamite so we started having a",
    "start": "1280820",
    "end": "1286730"
  },
  {
    "text": "look at what we actually needed from our process so storage has two functions in an Orchestrator the first is to enable",
    "start": "1286730",
    "end": "1293450"
  },
  {
    "text": "the asynchronous continuation of the process this is critical in order for your Orchestrator to work the second is",
    "start": "1293450",
    "end": "1300770"
  },
  {
    "text": "an audit function being able to go back in at a period of time later whilst retaining that information and being",
    "start": "1300770",
    "end": "1307580"
  },
  {
    "text": "able to see what happened or at least what the final state was now in our",
    "start": "1307580",
    "end": "1312620"
  },
  {
    "text": "situation obviously the continuation is critical we must have that continuation",
    "start": "1312620",
    "end": "1317630"
  },
  {
    "text": "of state we must be able to handle a transient failure now the audit on the",
    "start": "1317630",
    "end": "1323060"
  },
  {
    "text": "other hand is more nuanced there are many other strategies we could use for audit we didn't strictly need",
    "start": "1323060",
    "end": "1331020"
  },
  {
    "text": "the audits that would be naturally guaranteed by writing into the store so we then started to look at well we",
    "start": "1331020",
    "end": "1338100"
  },
  {
    "text": "actually don't really need key addressable storage we need all of the information to route to the same place",
    "start": "1338100",
    "end": "1344820"
  },
  {
    "text": "we need it to be durable enough so that we can continue our processing and that",
    "start": "1344820",
    "end": "1350610"
  },
  {
    "text": "we can remove audit from the situation and of course Kinesis has the concept of",
    "start": "1350610",
    "end": "1356280"
  },
  {
    "text": "affinity and that affinity means that I can write a process and as long as I",
    "start": "1356280",
    "end": "1361530"
  },
  {
    "text": "define that process ID or that natural key that it will get routed to the same shard and even if that shard worker was",
    "start": "1361530",
    "end": "1369690"
  },
  {
    "text": "to fail that I would be able to fail on to a new shard worker and I would get the information through in the same",
    "start": "1369690",
    "end": "1377100"
  },
  {
    "text": "sequences that have occurred and processed in the first shard worker in the second shard worker so this is",
    "start": "1377100",
    "end": "1382650"
  },
  {
    "text": "pretty powerful so we we started saying well can we just get rid of the storage entirely can we move to a pattern like",
    "start": "1382650",
    "end": "1389970"
  },
  {
    "text": "this whereas rather than writing into the storage before sending off we'll do all of our writing through Kinesis so we",
    "start": "1389970",
    "end": "1396810"
  },
  {
    "text": "will write when the process is first given to us into Kinesis that will then be read by the orchestrator back-end",
    "start": "1396810",
    "end": "1403620"
  },
  {
    "text": "working as the Kinesis client it will then in turn dispatch across for the services and when they complete those",
    "start": "1403620",
    "end": "1410640"
  },
  {
    "text": "services again will write their responses through Kinesis so we're gaining our durability through the log",
    "start": "1410640",
    "end": "1417050"
  },
  {
    "text": "now that was really promising but not",
    "start": "1417050",
    "end": "1422850"
  },
  {
    "text": "there quite yet so first of all Kinesis or at least at the time that we were doing this only allowed 24 hours of",
    "start": "1422850",
    "end": "1429810"
  },
  {
    "start": "1427000",
    "end": "1427000"
  },
  {
    "text": "durability now that's not true now it could do seven days and seven days would have been enough for our worst worst",
    "start": "1429810",
    "end": "1436800"
  },
  {
    "text": "possible case scenario so maybe this first bullet point isn't true anymore but the second one the time it takes to",
    "start": "1436800",
    "end": "1444180"
  },
  {
    "text": "reconstruct 24 hours of data bear in mind that we're writing in gigabytes",
    "start": "1444180",
    "end": "1449240"
  },
  {
    "text": "every 5-10 minutes that this is terabytes of data to pull back and",
    "start": "1449240",
    "end": "1454440"
  },
  {
    "text": "rebuild this is very very expensive it would be seven times as worse if we were",
    "start": "1454440",
    "end": "1459600"
  },
  {
    "text": "to rebuild across the whole seven-day slice so it's still not real feasible to",
    "start": "1459600",
    "end": "1464820"
  },
  {
    "text": "just use Kinesis as durability mechanism so we needed something else now we can obviously say if we're",
    "start": "1464820",
    "end": "1472950"
  },
  {
    "text": "pulling up all of the in-flight process state into memory which is the plan that we can we can have many many gigabytes",
    "start": "1472950",
    "end": "1480630"
  },
  {
    "text": "of storage and we can snap shot it but snapshotting has basically two approaches to it we use in musical",
    "start": "1480630",
    "end": "1486720"
  },
  {
    "text": "structures which add a large amount of object overhead and therefore reduces",
    "start": "1486720",
    "end": "1492240"
  },
  {
    "text": "the efficiency of each of our shard worker considerably or we can use a stop the world snapshotting operation which",
    "start": "1492240",
    "end": "1498390"
  },
  {
    "text": "is much better in terms of memory density but then it leads to volatile performance especially if we had 1015",
    "start": "1498390",
    "end": "1504600"
  },
  {
    "text": "gigs of heap that we are looking at snapshotting in order to write it down this this could be a very significant",
    "start": "1504600",
    "end": "1511680"
  },
  {
    "text": "portion Oh 20 30 seconds and that could mean that we're going to breach our quality of service guarantees because",
    "start": "1511680",
    "end": "1518160"
  },
  {
    "text": "the system is itself unpredictable so we really don't want to do a stop the world operation and if possible we don't",
    "start": "1518160",
    "end": "1525360"
  },
  {
    "text": "really want this really dense or rather really sparse data packing with",
    "start": "1525360",
    "end": "1530940"
  },
  {
    "text": "immutable data structures so what can we do well the actual pricing API of all",
    "start": "1530940",
    "end": "1538170"
  },
  {
    "text": "things gives us gives us a clue so the pricing has the caveat that the hourly",
    "start": "1538170",
    "end": "1543270"
  },
  {
    "text": "shard rate is based on one megabyte a second ingest the rate that you can put and two megabytes second that you can",
    "start": "1543270",
    "end": "1550560"
  },
  {
    "text": "read for the same price you get two reads for every single write so this",
    "start": "1550560",
    "end": "1556350"
  },
  {
    "text": "gave us the idea of well why don't we just read it a second time so we have the orchestrator which never does it",
    "start": "1556350",
    "end": "1562890"
  },
  {
    "start": "1557000",
    "end": "1557000"
  },
  {
    "text": "snapshotting and we have a process that looks almost exactly like the orchestrator an archiver which just",
    "start": "1562890",
    "end": "1569010"
  },
  {
    "text": "deals with the snapshotting for us and so this is really powerful we can do some really interesting things",
    "start": "1569010",
    "end": "1577260"
  },
  {
    "text": "with as well so we don't need to run the the archiver at the same size of fleet as our main Orchestrator because we",
    "start": "1577260",
    "end": "1584100"
  },
  {
    "text": "don't need it to keep up that aggressively in fact we can get our archiver to have a subsection of the",
    "start": "1584100",
    "end": "1589440"
  },
  {
    "text": "feast and literally rotate through the shards so it can be a much smaller fleet it's also really interesting to see that",
    "start": "1589440",
    "end": "1596880"
  },
  {
    "text": "actually we can test out our recovery process if we write our Orchestrator and",
    "start": "1596880",
    "end": "1602100"
  },
  {
    "text": "our archiver in such a fashion of the share common code base we're effectively constantly checking our recovery",
    "start": "1602100",
    "end": "1608399"
  },
  {
    "text": "strategy because the orchestrator will recover in exactly the same way as the",
    "start": "1608399",
    "end": "1613860"
  },
  {
    "text": "archiver we'll build up our information so there are a few things that make this",
    "start": "1613860",
    "end": "1619200"
  },
  {
    "text": "a little bit more difficult so the first is the Kinesis client library only",
    "start": "1619200",
    "end": "1624480"
  },
  {
    "text": "supports a single check point we'll call that the high-water mark in this particular case now that check point is",
    "start": "1624480",
    "end": "1631200"
  },
  {
    "text": "not going to reflect what we last snapshotted that's going to be something over here we'll call it the low watermark so we have a previous snapshot",
    "start": "1631200",
    "end": "1637710"
  },
  {
    "text": "sitting there in s3 so we need something that when we actually recover can build",
    "start": "1637710",
    "end": "1643379"
  },
  {
    "text": "up to that new snapshot in other words it can start up at the low watermark it",
    "start": "1643379",
    "end": "1649110"
  },
  {
    "text": "can recover forwards and then at some point it will realize that it's hit the high-water mark which is the last thing",
    "start": "1649110",
    "end": "1655470"
  },
  {
    "text": "was actually processed that we now have that snapshot as a good basis point to start our",
    "start": "1655470",
    "end": "1660840"
  },
  {
    "text": "processing forward and then move our processing forward and in the case of",
    "start": "1660840",
    "end": "1665940"
  },
  {
    "text": "our archiver that processing forward would simply be to write down the s3 State and to write the new low watermark",
    "start": "1665940",
    "end": "1673110"
  },
  {
    "text": "essentially to set a low watermark to the previous high watermark now the other thing is is that we need",
    "start": "1673110",
    "end": "1680789"
  },
  {
    "text": "to make sure that we're making the same decision in both our archiver as well as our Orchestrator in other words the",
    "start": "1680789",
    "end": "1687090"
  },
  {
    "text": "orchestrators decisions which are dependent on the data dependent on things like capacity of what it's going",
    "start": "1687090",
    "end": "1693480"
  },
  {
    "text": "to dispatch need to be reflected actually in the archiver itself and the",
    "start": "1693480",
    "end": "1698519"
  },
  {
    "text": "only way to do that is to use what we call closed input monotonicity so not",
    "start": "1698519",
    "end": "1704490"
  },
  {
    "text": "only must a process in sequence it needs to make sure all of the inputs that it would use to make a decision are also",
    "start": "1704490",
    "end": "1710639"
  },
  {
    "start": "1707000",
    "end": "1707000"
  },
  {
    "text": "flowing through that Kinesis stream which then means any change to those inputs for scaling requirements are",
    "start": "1710639",
    "end": "1715679"
  },
  {
    "text": "going to be received in exactly the same way into the archiver so the archiver can make exactly the same decision based",
    "start": "1715679",
    "end": "1722309"
  },
  {
    "text": "off its reading of the stream so for instance if we're looking at what the replication latency is off our kinesio",
    "start": "1722309",
    "end": "1729450"
  },
  {
    "text": "stream here or how many standing items if we're snapshotting from an sqs hand off we need to make sure that those kind",
    "start": "1729450",
    "end": "1736290"
  },
  {
    "text": "of back pressure statistics are actually fed into the stream itself so another",
    "start": "1736290",
    "end": "1741510"
  },
  {
    "text": "requirement we have to build a slightly more complex service consumption contract but something that we think",
    "start": "1741510",
    "end": "1747960"
  },
  {
    "text": "might be worthwhile so we went and costed it out and the",
    "start": "1747960",
    "end": "1752970"
  },
  {
    "text": "really interesting thing from this and you can see just how much it reduces in the cost that we end up reducing to a",
    "start": "1752970",
    "end": "1760350"
  },
  {
    "text": "roundabout tooth sorry one third of the cost so around about forty percent so here we end up with a little over or a",
    "start": "1760350",
    "end": "1767940"
  },
  {
    "text": "little under eighty one thousand dollars for something that was going to cost us a hundred and eighty eight thousand before and the really big mover here is",
    "start": "1767940",
    "end": "1774990"
  },
  {
    "text": "that Kinesis is now doing all of the heavy lifting and it's actually extremely efficient both cost-efficient",
    "start": "1774990",
    "end": "1781230"
  },
  {
    "text": "as well as processing efficient for us to do so we end up spending a little bit more in terms of our memory instances as",
    "start": "1781230",
    "end": "1787890"
  },
  {
    "text": "you can see here the dominating factor of this costing is actually moving from compute instances to memory optimized",
    "start": "1787890",
    "end": "1794700"
  },
  {
    "text": "instances in order to make sure we have sufficient capacity but at the same time",
    "start": "1794700",
    "end": "1799860"
  },
  {
    "text": "we end up with something which is substantially cheaper and the snapshotting to s3 with a reasonable",
    "start": "1799860",
    "end": "1806520"
  },
  {
    "text": "rotation period ends up being negligible likewise than dynamo usage for writing the high watermarks so this was really",
    "start": "1806520",
    "end": "1813180"
  },
  {
    "text": "powerful for us but in all honesty this was only one of our objectives if who was just this may be the architectural",
    "start": "1813180",
    "end": "1819240"
  },
  {
    "text": "complexity wouldn't be worth it however we got a whole series of additional benefits so first of all let's just say",
    "start": "1819240",
    "end": "1825390"
  },
  {
    "text": "we did manage to show we could run as fast as possible we've got a paradigm that allows us to go down to one by one",
    "start": "1825390",
    "end": "1830940"
  },
  {
    "text": "for all the interactions and a single Orchestrator so one set of infrastructure and we've done our",
    "start": "1830940",
    "end": "1836280"
  },
  {
    "text": "absolute utmost to reduce the cost of being able to run at that one by one but what else well we mentioned that Kinesis",
    "start": "1836280",
    "end": "1844470"
  },
  {
    "text": "works by affinity so how can we exploit that how can we use that to do something",
    "start": "1844470",
    "end": "1849660"
  },
  {
    "text": "useful so let's start with a very simple key and then a scenario and this is a",
    "start": "1849660",
    "end": "1854970"
  },
  {
    "start": "1851000",
    "end": "1851000"
  },
  {
    "text": "very important and common scenario for us we have a low priority update the",
    "start": "1854970",
    "end": "1860340"
  },
  {
    "text": "feed comes in it's a low pro at low priority feed and it makes an update and set of items including are really",
    "start": "1860340",
    "end": "1867760"
  },
  {
    "text": "important item with the impossibly long key to remember their so it makes the version one update a time T now this",
    "start": "1867760",
    "end": "1875170"
  },
  {
    "text": "doesn't get scheduled for a long period of time here we can see that it's due to execute at 2:30 and it has a low quality",
    "start": "1875170",
    "end": "1881440"
  },
  {
    "text": "of service so this isn't going to get executed anytime soon then the user might realize that actually this item is",
    "start": "1881440",
    "end": "1887670"
  },
  {
    "text": "critically needing some kind of change and so they might go into the interactive experience and do a quick",
    "start": "1887670",
    "end": "1894520"
  },
  {
    "text": "change which they want to be done as really as fast as possible as high priority so we end up with a second",
    "start": "1894520",
    "end": "1900250"
  },
  {
    "text": "update coming in it comes in a little bit ater but we need that update to be applied",
    "start": "1900250",
    "end": "1906160"
  },
  {
    "text": "after the first update because we want to preserve ordering but we also want to make sure that updates goes through as",
    "start": "1906160",
    "end": "1911890"
  },
  {
    "text": "quickly as possible and we don't want to the downstream systems to have to cope with out of order updates because there",
    "start": "1911890",
    "end": "1918010"
  },
  {
    "text": "are a lot of systems that we would need to go and put the same kind of logic into all of them we want our orchestrates to be able to handle that",
    "start": "1918010",
    "end": "1923520"
  },
  {
    "text": "now this is actually very easy because if we say that the affinity key is the",
    "start": "1923520",
    "end": "1930730"
  },
  {
    "text": "the key component there so that updates on the same item or the same product go",
    "start": "1930730",
    "end": "1936280"
  },
  {
    "text": "to the same shard we can guarantee that the shard worker will put them in the",
    "start": "1936280",
    "end": "1941380"
  },
  {
    "text": "same memory and so making a dependency in memory is really easy we just say ok",
    "start": "1941380",
    "end": "1946780"
  },
  {
    "text": "we already have one imagine a hashmap style lookup ok we'll put this as dependent behind it then we can see that",
    "start": "1946780",
    "end": "1953410"
  },
  {
    "text": "actually we have a risk you know our high value one is use complete nearly an",
    "start": "1953410",
    "end": "1959320"
  },
  {
    "text": "hour before a low value one so what we need to be able to do is say ok this",
    "start": "1959320",
    "end": "1964390"
  },
  {
    "text": "isn't going to work we're actually a rich risk of breaching that SLA and at the same time we can't just drop the",
    "start": "1964390",
    "end": "1969730"
  },
  {
    "text": "first value because the first value might actually have the second one is an incremental on top of the first so all",
    "start": "1969730",
    "end": "1977380"
  },
  {
    "text": "we can do is we can simply realize that this is a problem and we can just promote forward the SLA as well as the",
    "start": "1977380",
    "end": "1984010"
  },
  {
    "text": "quality of service so we can make sure that that blocking item is going to execute in such a fashion that we can",
    "start": "1984010",
    "end": "1991030"
  },
  {
    "text": "actually proceed forward this is incredibly important and most importantly this is incredibly easy to",
    "start": "1991030",
    "end": "1997420"
  },
  {
    "text": "do in terms of any memory structure off the shard very very simple indeed doing",
    "start": "1997420",
    "end": "2003080"
  },
  {
    "text": "the same on a remote call and and scheduling essentially doing a",
    "start": "2003080",
    "end": "2009049"
  },
  {
    "text": "coordination via a no sequel store is a lot more expensive both the code as well as to operate so we can see that we have",
    "start": "2009049",
    "end": "2017360"
  },
  {
    "text": "now got a solution that gives us both ordering and priority and in fact allows us to honor both without having to give",
    "start": "2017360",
    "end": "2024740"
  },
  {
    "text": "up on either what else well one of the big things we mentioned is the ability to exploit batch ability we want to be",
    "start": "2024740",
    "end": "2031820"
  },
  {
    "start": "2026000",
    "end": "2026000"
  },
  {
    "text": "able to say actually I can do multiple changes on the same key at the same time okay so imagine that we've got the first",
    "start": "2031820",
    "end": "2038690"
  },
  {
    "text": "version ready to dispatch here ie v1 so we can go and move that down to dispatch now logically that satisfies or could",
    "start": "2038690",
    "end": "2046190"
  },
  {
    "text": "satisfy if we're doing a little bit of look ahead the next version if there's something else depend on it we can simply check our dependences at the time",
    "start": "2046190",
    "end": "2052608"
  },
  {
    "text": "that were going to dispatch and say if this was already completed and it's a batch of all tasks is there anything",
    "start": "2052609",
    "end": "2058550"
  },
  {
    "text": "else that could run well in this case v2 can and then we can continue unwinding down that list we can evaluate v3 now in",
    "start": "2058550",
    "end": "2066618"
  },
  {
    "text": "this case v3 has another dependency it's not possible for us to dispatch v3 at this stage because be v1 still hasn't",
    "start": "2066619",
    "end": "2073820"
  },
  {
    "text": "been processed so it's not much we can do that but the whole point is is that we can very easily walk a very simple",
    "start": "2073820",
    "end": "2080450"
  },
  {
    "text": "in-memory tree structure in order to provide this batching logic it's actually extremely easy to implement",
    "start": "2080450",
    "end": "2085790"
  },
  {
    "text": "this on the back end so we've now gained from this the ability to",
    "start": "2085790",
    "end": "2091540"
  },
  {
    "text": "sorry clicking to fast so we've now gained the",
    "start": "2091540",
    "end": "2097190"
  },
  {
    "text": "ability not only to order and prioritize but we've also gained the ability to pouch now this is huge for the system",
    "start": "2097190",
    "end": "2103220"
  },
  {
    "text": "because especially in that reconciliation step it's almost exactly",
    "start": "2103220",
    "end": "2108470"
  },
  {
    "text": "the same cost to run it for one item as it is to run with let's say",
    "start": "2108470",
    "end": "2115280"
  },
  {
    "text": "a couple of hundred inputs it can do that at almost the same cost so by being",
    "start": "2115280",
    "end": "2120349"
  },
  {
    "text": "able to do batching the system holistically gains much more throughput and it gaytan's much lower latency",
    "start": "2120349",
    "end": "2126440"
  },
  {
    "text": "because there's no queuing so this was a massive win for us but we still have one more thing we needed to get through and",
    "start": "2126440",
    "end": "2133160"
  },
  {
    "text": "that was predictable execution so we've tried to remove bottlenecks with batching we've managed to get everyone",
    "start": "2133160",
    "end": "2140270"
  },
  {
    "text": "down to one by one by reducing the cost but we still need to go a little bit further than that we still need to be able to provide a mechanism that can",
    "start": "2140270",
    "end": "2146720"
  },
  {
    "text": "actually guarantee a quality of service to to our users so we need to give them",
    "start": "2146720",
    "end": "2152300"
  },
  {
    "text": "a time that this will complete you know your work will be done at this time together with a contractual guarantee",
    "start": "2152300",
    "end": "2158180"
  },
  {
    "text": "that we will actually be able to make that so working backwards will say let's",
    "start": "2158180",
    "end": "2163850"
  },
  {
    "text": "give them an instance SLA imagine a UTC timestamp and a quality of service against that our confidence internally",
    "start": "2163850",
    "end": "2170840"
  },
  {
    "text": "that we will we will be able to make that so imagine the highest possible quality of service we will guarantee you",
    "start": "2170840",
    "end": "2176720"
  },
  {
    "text": "this even if we're down to one percent capacity whereas the lowest quality of service it's essentially best effort",
    "start": "2176720",
    "end": "2184040"
  },
  {
    "text": "don't you know we will run if nothing else is running however expect there to",
    "start": "2184040",
    "end": "2189050"
  },
  {
    "text": "be disruption a standard smoothing or prioritization so we will then choose to",
    "start": "2189050",
    "end": "2196100"
  },
  {
    "text": "order based on the SLA and then we will make selective decisions as to whether",
    "start": "2196100",
    "end": "2201170"
  },
  {
    "text": "or not we should dispatch in order to meet that SLA or at that point of decision whether or not we should",
    "start": "2201170",
    "end": "2207110"
  },
  {
    "text": "actually just drop the item not drop permanently but droppers in breach the",
    "start": "2207110",
    "end": "2212300"
  },
  {
    "text": "SLA record that it's breached potentially let the user know that we haven't been able to meet their SLA and",
    "start": "2212300",
    "end": "2217760"
  },
  {
    "text": "then schedule into the next interval so if we have a look at that as an example",
    "start": "2217760",
    "end": "2223640"
  },
  {
    "text": "again very simple data structure in order to ensure that we are looking at",
    "start": "2223640",
    "end": "2229070"
  },
  {
    "text": "the most close to dispatch or or close to breaching SLA first we simply do a",
    "start": "2229070",
    "end": "2235100"
  },
  {
    "text": "min priority queue bog-standard data structure so the one on the right has",
    "start": "2235100",
    "end": "2240860"
  },
  {
    "text": "the lowest time I realize it's a little bit difficult to read so it's 13 seconds past the minute the one after it is 14",
    "start": "2240860",
    "end": "2247700"
  },
  {
    "text": "seconds the one after it is 15 seconds so we're going to evaluate the dispatch on this item",
    "start": "2247700",
    "end": "2253810"
  },
  {
    "text": "we're going to look across what other items exist behind it we're going to see",
    "start": "2253810",
    "end": "2258860"
  },
  {
    "text": "whether or not they are more important or less important than us so for",
    "start": "2258860",
    "end": "2264050"
  },
  {
    "text": "instance imagine the quality of service the higher it is the more important we can see that there is something with a",
    "start": "2264050",
    "end": "2270230"
  },
  {
    "text": "high quality of service to items behind it and so by aid dispatching it might be",
    "start": "2270230",
    "end": "2275900"
  },
  {
    "text": "putting that higher quality of service item at risk so we need to factor that in at the same time between the two",
    "start": "2275900",
    "end": "2282410"
  },
  {
    "text": "there is an item of lower quality of service so if it was to decide not to",
    "start": "2282410",
    "end": "2288320"
  },
  {
    "text": "dispatch the quality of service three item and then we were to dispatch the quality of service to item instead",
    "start": "2288320",
    "end": "2294859"
  },
  {
    "text": "because it wasn't putting the the highest priority at risk then we",
    "start": "2294859",
    "end": "2299990"
  },
  {
    "text": "wouldn't have done the right thing in terms of prioritization so what it does is it goes and takes all of those inputs",
    "start": "2299990",
    "end": "2305450"
  },
  {
    "text": "behind it and it doesn't have to calculate it each time you can calculate this as an incremental heuristic but it",
    "start": "2305450",
    "end": "2312080"
  },
  {
    "text": "can go and understand what's behind it in the queue to generate a weighted probability of dispatch and then it'll",
    "start": "2312080",
    "end": "2319550"
  },
  {
    "text": "do a dice roll it will go and work out whether or not it should actually dispatch that item now of course as",
    "start": "2319550",
    "end": "2325490"
  },
  {
    "text": "mentioned we we have to do things in a deterministic fashion so that our archiver will make the same decision as",
    "start": "2325490",
    "end": "2331010"
  },
  {
    "text": "the the main Orchestrator so in this particular case we need to make sure that we seed that random dice roll by a",
    "start": "2331010",
    "end": "2337430"
  },
  {
    "text": "piece of information such as the sequence ID the last red sequence ID before actually runs the dispatch so we",
    "start": "2337430",
    "end": "2345830"
  },
  {
    "text": "need to make sure we're consistent but once we've done that we can choose to dispatch if it passes the dice roll or",
    "start": "2345830",
    "end": "2352369"
  },
  {
    "text": "if it doesn't pass the dice roll we can just say okay it didn't meet the SLA we've deliberately chosen to breach it",
    "start": "2352369",
    "end": "2358400"
  },
  {
    "text": "so that more at-risk things or more important at-risk things behind it can can execute in its place",
    "start": "2358400",
    "end": "2364720"
  },
  {
    "text": "so that's one part of the system but we also understand that a lot of our work",
    "start": "2364720",
    "end": "2369770"
  },
  {
    "start": "2365000",
    "end": "2365000"
  },
  {
    "text": "comes from feeds and that we really want to be tolerant of bursty work we don't want to penalize with aggressive",
    "start": "2369770",
    "end": "2375830"
  },
  {
    "text": "throttling in order to apply fairness so we need we really want a system to be",
    "start": "2375830",
    "end": "2380900"
  },
  {
    "text": "able to smooth out traffic and at the same time prevent people from just abusing their quality of service so just",
    "start": "2380900",
    "end": "2387710"
  },
  {
    "text": "saying everything is the most critical it could possibly be and still ultimately defend against denial",
    "start": "2387710",
    "end": "2394970"
  },
  {
    "text": "of service did defend against the whole system being flooded and the way that we went about that is",
    "start": "2394970",
    "end": "2401130"
  },
  {
    "text": "we go and have an algorithm that looks up configuration contract as to what our",
    "start": "2401130",
    "end": "2406800"
  },
  {
    "text": "commitments are to that user for a given priority we work out what essentially is",
    "start": "2406800",
    "end": "2412350"
  },
  {
    "text": "the pro rata TPF as there is a maximum base time and we'll see this in a worked",
    "start": "2412350",
    "end": "2417930"
  },
  {
    "text": "example in a second to work out what we should use as our as our basis time bear",
    "start": "2417930",
    "end": "2423420"
  },
  {
    "text": "in mind that everything is operating as UTC timestamps and then we should go and",
    "start": "2423420",
    "end": "2428490"
  },
  {
    "text": "see whether or not we can actually issue that SLA or whether we are too far in advance whether we have given too many",
    "start": "2428490",
    "end": "2434990"
  },
  {
    "text": "too many permits if you like or too much capacity at that priority level and that",
    "start": "2434990",
    "end": "2440880"
  },
  {
    "text": "we actually need to downgrade to the next level of quality of service because we're oversaturated and of course if not",
    "start": "2440880",
    "end": "2447060"
  },
  {
    "text": "we'll actually take about that calculation and we will go and issue that out so if we go and do a quick",
    "start": "2447060",
    "end": "2454710"
  },
  {
    "text": "example and this is a little complex but trying to make it as as simple as possible imagine that we have a piece of",
    "start": "2454710",
    "end": "2460740"
  },
  {
    "text": "work that comes in now so the now time is zero we have a high watermark time",
    "start": "2460740",
    "end": "2467100"
  },
  {
    "text": "because we've not actually dispatched anything else at this priority ever apparently and so our base time is going",
    "start": "2467100",
    "end": "2473760"
  },
  {
    "text": "to be whichever is bigger out of the now time and whatever our last high watermark was so that's obviously going",
    "start": "2473760",
    "end": "2480870"
  },
  {
    "text": "to be zero we're then going to add our SLA time which is going to be two",
    "start": "2480870",
    "end": "2486960"
  },
  {
    "text": "seconds because this work is coming in as something in the high priority bound and so we're going to contractually",
    "start": "2486960",
    "end": "2493440"
  },
  {
    "text": "commit to completing this work two seconds out so at two on our time line",
    "start": "2493440",
    "end": "2498690"
  },
  {
    "text": "and we're going to give it a quality of service of five from the top SLA and",
    "start": "2498690",
    "end": "2504600"
  },
  {
    "text": "then finally we'll calculate our new high-water mark and that's based off the",
    "start": "2504600",
    "end": "2510060"
  },
  {
    "text": "previous base that we used plus the pro rata TPS now here we've unrealistically",
    "start": "2510060",
    "end": "2516540"
  },
  {
    "text": "said that we're only going to give one transaction per second to this client about priority so to pro-rata",
    "start": "2516540",
    "end": "2522840"
  },
  {
    "text": "transactions per second into UTC interval time we simply take one over",
    "start": "2522840",
    "end": "2528300"
  },
  {
    "text": "the transactions per second 1 over 1 is 1 so we've calculated our new my watermark has one second in the future",
    "start": "2528300",
    "end": "2535190"
  },
  {
    "text": "now imagine a second item comes in so the calculation is very similar the now",
    "start": "2535190",
    "end": "2540330"
  },
  {
    "text": "time is still zero however or how high watermark time is one because it's",
    "start": "2540330",
    "end": "2545730"
  },
  {
    "text": "taking the current high watermark as to the issued contract so the maximum of",
    "start": "2545730",
    "end": "2551400"
  },
  {
    "text": "those is one however we're saying that actually one is greater than the maximum lead time we're being very stingy here",
    "start": "2551400",
    "end": "2558780"
  },
  {
    "text": "for the sake of a a simple example we wouldn't actually have this little smoothing however we're saying that the",
    "start": "2558780",
    "end": "2565410"
  },
  {
    "text": "maximum time the base can get ahead of of our nail time is one second and now",
    "start": "2565410",
    "end": "2571920"
  },
  {
    "text": "that it has reached that we saying well we can't actually issue this contract so we're going to have to roll down to the",
    "start": "2571920",
    "end": "2577500"
  },
  {
    "text": "lower priority so now we go and reconsider at the lower contract there",
    "start": "2577500",
    "end": "2583320"
  },
  {
    "text": "is no high watermark for the lower contract so it's base ends up as zero",
    "start": "2583320",
    "end": "2589010"
  },
  {
    "text": "I've lost a kicker",
    "start": "2590540",
    "end": "2594050"
  },
  {
    "text": "so we end up taking our base of zero and then we look up our SLA time now is L a",
    "start": "2596570",
    "end": "2603390"
  },
  {
    "text": "time is much longer for the lower priority so we've given a five-second SLA for this",
    "start": "2603390",
    "end": "2609090"
  },
  {
    "text": "one as a result it will end up scheduling all the way out at five",
    "start": "2609090",
    "end": "2614100"
  },
  {
    "text": "seconds in the future and it will have a lower quality of service so we'll have quality of service of three and",
    "start": "2614100",
    "end": "2619760"
  },
  {
    "text": "finally our base calculation needs to be redone so in our particular case here because we get two transactions a second",
    "start": "2619760",
    "end": "2626520"
  },
  {
    "text": "rather than one transaction a second at the higher at the lower priority what we",
    "start": "2626520",
    "end": "2632010"
  },
  {
    "text": "end up with is doing one over transactions per second again so we end up scheduling that for half a second in",
    "start": "2632010",
    "end": "2639000"
  },
  {
    "text": "the future now if we were to have a third piece of work coming in and this shows how it flips back and that third",
    "start": "2639000",
    "end": "2646140"
  },
  {
    "text": "piece of work was to come in half a second in the future so it comes in at half when we go and run our base",
    "start": "2646140",
    "end": "2652770"
  },
  {
    "text": "calculation max of 0.5 and 1 we can see that that then gives us well we'll take",
    "start": "2652770",
    "end": "2659010"
  },
  {
    "text": "the maximum of those which is one one is only half a second of the time that it",
    "start": "2659010",
    "end": "2664260"
  },
  {
    "text": "came in half a second ahead so it's within the max lead time we can issue at our top quality of service and so we can",
    "start": "2664260",
    "end": "2671730"
  },
  {
    "text": "then go and calculate our SLA and we can move our base forward again so the",
    "start": "2671730",
    "end": "2677550"
  },
  {
    "text": "interesting thing here is is that our second piece of work is scheduled much further out than our third and this",
    "start": "2677550",
    "end": "2684540"
  },
  {
    "text": "could be a big problem I imagine that those two pieces of work just so happen to be on the same key would have now inverted the order of them but again",
    "start": "2684540",
    "end": "2691770"
  },
  {
    "text": "this is covered by the fact as we covered previously that it will still go to the same shard it will go to the same",
    "start": "2691770",
    "end": "2698940"
  },
  {
    "text": "in memory and it will realize that okay three actually needs to be executed",
    "start": "2698940",
    "end": "2704070"
  },
  {
    "text": "sooner than two and it will promote up the the ordering and the timestamp so",
    "start": "2704070",
    "end": "2709530"
  },
  {
    "text": "that actually it would end up executing two and three in line to meet three's",
    "start": "2709530",
    "end": "2714750"
  },
  {
    "text": "issue quality of service so again really powerful what we can get out of the back of the Kinesis library",
    "start": "2714750",
    "end": "2722540"
  },
  {
    "text": "so this gave us a much better position to reason about when the work will be",
    "start": "2722540",
    "end": "2728820"
  },
  {
    "text": "done and that's all we really have time to go through in terms of the main session here so I can't really go",
    "start": "2728820",
    "end": "2736319"
  },
  {
    "text": "through how it also allows us to trace in terms of audit we do have that capability as well but unfortunately it",
    "start": "2736319",
    "end": "2741839"
  },
  {
    "text": "makes the slides run long so I'd like to end by saying by putting Kinesis at the",
    "start": "2741839",
    "end": "2747089"
  },
  {
    "text": "core of our architecture we managed to number one reduce our TCO considerably we managed to internalize for all of the",
    "start": "2747089",
    "end": "2756210"
  },
  {
    "text": "services that we were composing over ordering and prioritization we allow",
    "start": "2756210",
    "end": "2761760"
  },
  {
    "text": "both services to be significantly simpler to remove all of the out of ordering logic that they were doing and",
    "start": "2761760",
    "end": "2768540"
  },
  {
    "text": "the storage that they needed to do it which actually reduced cost from the services that we were orchestrating over",
    "start": "2768540",
    "end": "2774329"
  },
  {
    "text": "so it wasn't just a cost reduction for for the orchestrator we actually ended up getting value out of our compost",
    "start": "2774329",
    "end": "2781349"
  },
  {
    "text": "services as well we ended up recovering a significant amount of cost and complexity there which is really",
    "start": "2781349",
    "end": "2786810"
  },
  {
    "text": "important for us and we gained new capabilities so we gain that ability to",
    "start": "2786810",
    "end": "2792510"
  },
  {
    "start": "2788000",
    "end": "2788000"
  },
  {
    "text": "batch and that was incredibly powerful for us there's actually a real game-changer to our cost mechanics so",
    "start": "2792510",
    "end": "2798630"
  },
  {
    "text": "all it was a very significant thing for us and the important thing for this chances are most of you won't need to",
    "start": "2798630",
    "end": "2805300"
  },
  {
    "text": "build an extremely high TPS Orchestrator it's there but what hopefully you've",
    "start": "2805300",
    "end": "2811210"
  },
  {
    "text": "taken away from this is the power of what you can do with the distributed log you know it's a it's a very basic but",
    "start": "2811210",
    "end": "2817420"
  },
  {
    "text": "very powerful building block it is much faster to scale than things like dynamo",
    "start": "2817420",
    "end": "2822730"
  },
  {
    "text": "you know you can split shards and join shards in minutes rather than a couple",
    "start": "2822730",
    "end": "2827740"
  },
  {
    "text": "of times a day so that you can do a lot by having transient storage with Kinesis",
    "start": "2827740",
    "end": "2834630"
  },
  {
    "text": "okay so thank you everyone and I've got time got about five minutes for",
    "start": "2834630",
    "end": "2840910"
  },
  {
    "text": "questions if anyone's got any questions",
    "start": "2840910",
    "end": "2844500"
  }
]