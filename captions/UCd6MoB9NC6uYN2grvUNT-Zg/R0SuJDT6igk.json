[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "welcome everyone thank you for taking time to attend the session um my name is R baa I work as a solution arch with a",
    "start": "1640",
    "end": "8679"
  },
  {
    "text": "and today we'll have amandip kurana from Claud talk about some of the best practices and some of the lessons we learned while working with few customers",
    "start": "8679",
    "end": "14920"
  },
  {
    "text": "on how to run on how to run optimal Hadoop on AWS cloud with that I will have amand stdi and talk through the",
    "start": "14920",
    "end": "20960"
  },
  {
    "text": "rest of the session thanks R um yeah so we we'll talk about uh what all we've",
    "start": "20960",
    "end": "28439"
  },
  {
    "start": "23000",
    "end": "107000"
  },
  {
    "text": "learned at Cloud era uh as we've supported um clusters in AWS and um I'll",
    "start": "28439",
    "end": "36120"
  },
  {
    "text": "talk about different things from Storage compute and like just all all aspects of it uh a little bit about me I'm a",
    "start": "36120",
    "end": "42879"
  },
  {
    "text": "principal Solutions architect at cloud data and uh I help uh our customers build Solutions on top of a dup as well",
    "start": "42879",
    "end": "49760"
  },
  {
    "text": "as work with our partners on um integrating with a dup uh before this I was an engineer at AWS on elastic map",
    "start": "49760",
    "end": "56920"
  },
  {
    "text": "produce and I'm also a co-author of the HB and action book uh so we'll talk about we'll briefly",
    "start": "56920",
    "end": "63480"
  },
  {
    "text": "touch upon the motivation on uh like running hupin AWS and what we hear from our customers are common things uh that",
    "start": "63480",
    "end": "70200"
  },
  {
    "text": "they that they sort of talk about when looking at H deployments in AWS uh from there we'll go into different deployment",
    "start": "70200",
    "end": "76840"
  },
  {
    "text": "paradigms that we that we've seen uh considerations around storage uh networking considerations around",
    "start": "76840",
    "end": "83000"
  },
  {
    "text": "different instance types and how to pick which one's most optimal for your use case uh security we'll talk about high",
    "start": "83000",
    "end": "89240"
  },
  {
    "text": "availability backups and disaster recovery and how that looks a little different in AWS than uh on Prem",
    "start": "89240",
    "end": "94640"
  },
  {
    "text": "deployments and uh from there we'll go into all the things that you want to think through when you're planning a cluster um and uh to to wrap it up I'll",
    "start": "94640",
    "end": "102920"
  },
  {
    "text": "talk about the available resources that you have uh to get you started so you know there's there's",
    "start": "102920",
    "end": "110159"
  },
  {
    "start": "107000",
    "end": "164000"
  },
  {
    "text": "really two Trends and I won't spend too much time on this given that you guys are here you you know what uh sort of is",
    "start": "110159",
    "end": "116799"
  },
  {
    "text": "happening in the cloud uh Market per se and the Big Data space right uh but",
    "start": "116799",
    "end": "122759"
  },
  {
    "text": "there's really two parallel trends that we are seeing of commoditizing infrastructure with with cloud and uh",
    "start": "122759",
    "end": "129920"
  },
  {
    "text": "commoditizing data with all sorts of Big Data Technologies out there and Hado specifically uh and as these two worlds",
    "start": "129920",
    "end": "136000"
  },
  {
    "text": "are sort of converging for our Enterprise customers they talk about different things they talk about cost as",
    "start": "136000",
    "end": "141760"
  },
  {
    "text": "a motivation they talk about just the flexibility of deployments as a motivation uh ease of use somebody else",
    "start": "141760",
    "end": "147440"
  },
  {
    "text": "is managing their operations as something that they care about um where data is being generated and where they want to keep it uh",
    "start": "147440",
    "end": "154319"
  },
  {
    "text": "performance consideration security so these are all the things that come up when people look at uh production",
    "start": "154319",
    "end": "160080"
  },
  {
    "text": "Enterprise grade deployments running in uh running in AWS",
    "start": "160080",
    "end": "165920"
  },
  {
    "start": "164000",
    "end": "242000"
  },
  {
    "text": "so you know as as we look at these two worlds converging there's really traditionally what used to happen and",
    "start": "165920",
    "end": "172640"
  },
  {
    "text": "still happens where customers have uh they they host their infrastructure and they manage it and they manage their",
    "start": "172640",
    "end": "178440"
  },
  {
    "text": "deployments on top of that right right uh then you often times customers H they",
    "start": "178440",
    "end": "183840"
  },
  {
    "text": "Outsource the management portion of it to companies like uh TCS Accenture T Systems and so on so forth uh and that's",
    "start": "183840",
    "end": "190120"
  },
  {
    "text": "a very normal thing that we see in a lot of our Enterprise customers uh so that is where it it is customer hosted but",
    "start": "190120",
    "end": "195560"
  },
  {
    "text": "managed by somebody else then there is this Paradigm of somebody else is hosting it uh but the customers manage",
    "start": "195560",
    "end": "200680"
  },
  {
    "text": "the deployments themselves so they could use infrastructure from software AWS Google Azure what have you uh but deploy",
    "start": "200680",
    "end": "207519"
  },
  {
    "text": "their own software on it and manage it themselves uh and then there's this whole uh other",
    "start": "207519",
    "end": "212640"
  },
  {
    "text": "Paradigm of just somebody else is hosting it somebody else is managing it they're just consuming it as uh as a",
    "start": "212640",
    "end": "219120"
  },
  {
    "text": "software as a service or platform as a service and that's where you have elastic map produce Altis scale CUO and",
    "start": "219120",
    "end": "224640"
  },
  {
    "text": "the likes of those uh fit in that bucket what we'll talk about today is uh a",
    "start": "224640",
    "end": "230319"
  },
  {
    "text": "vendor hosted like so hosted in AWS but customers managing it themselves and that's what we see from a lot of our",
    "start": "230319",
    "end": "236040"
  },
  {
    "text": "Enterprise customers that that's what they want uh so we'll talk about everything that we've learned in in that",
    "start": "236040",
    "end": "242360"
  },
  {
    "start": "242000",
    "end": "257000"
  },
  {
    "text": "bucket and the devil's really in the details right so we talk about what kind of instances whether you want fewer",
    "start": "242360",
    "end": "248599"
  },
  {
    "text": "discs whether you should go to S3 uh you want more CPU less CPU how much memory and so on and so forth and and we'll",
    "start": "248599",
    "end": "255400"
  },
  {
    "text": "sort of talk about all those things you know the primary consideration and from the primary",
    "start": "255400",
    "end": "261239"
  },
  {
    "start": "257000",
    "end": "275000"
  },
  {
    "text": "consideration when deploying in AWS is what is your storage substrate right and",
    "start": "261239",
    "end": "266240"
  },
  {
    "text": "that is that is what you want to look at as the first decision that you have to make what is your storage substrate when",
    "start": "266240",
    "end": "272800"
  },
  {
    "text": "uh for your big data deployment for your deployment and there's really two choices there right so there's there's",
    "start": "272800",
    "end": "277960"
  },
  {
    "start": "275000",
    "end": "416000"
  },
  {
    "text": "either S3 or htfs so your source of Truth would either be S3 or or htfs and",
    "start": "277960",
    "end": "284080"
  },
  {
    "text": "the way you make this decision is by looking at the kind of workloads that you want to run so if you want to run ad",
    "start": "284080",
    "end": "289320"
  },
  {
    "text": "hoc batch workloads so which is I've got uh some data living in in S3 I want to",
    "start": "289320",
    "end": "294639"
  },
  {
    "text": "run some M job to just uh cleanse it and make it available for quering therea but",
    "start": "294639",
    "end": "299880"
  },
  {
    "text": "it's like once I need to do it I don't need to do it again uh so it's sort of ad hoc where somebody wants somebody",
    "start": "299880",
    "end": "305000"
  },
  {
    "text": "comes in and wants to run a particular kind of job you could do that with S3 or hdfs so",
    "start": "305000",
    "end": "310240"
  },
  {
    "text": "data living in either of those systems you can very well run an ad hoc uh batch workload you could do the same thing",
    "start": "310240",
    "end": "316479"
  },
  {
    "text": "with slas say every night um at midnight I want to run this job when uh data",
    "start": "316479",
    "end": "322120"
  },
  {
    "text": "becomes available and for that day data I'm want to cleanse it and make it available for reporting down the road again you could do that with S3 as well",
    "start": "322120",
    "end": "328400"
  },
  {
    "text": "as with hdfs but then there's this other class of workloads which is interactive right so",
    "start": "328400",
    "end": "334199"
  },
  {
    "text": "ad hoc interactive workloads where uh people just come in and they let's a bi right so they have a bi tool that they",
    "start": "334199",
    "end": "340800"
  },
  {
    "text": "want to uh put on top of the data that's living in hdfs uh and do uh exploration",
    "start": "340800",
    "end": "346440"
  },
  {
    "text": "of data so that's more sort of ad hoc interactive uh workloads you can't do that with S3 and uh there's S3 is really",
    "start": "346440",
    "end": "355080"
  },
  {
    "text": "for um you know I won't call it archival storage but it's it's not meant for",
    "start": "355080",
    "end": "360400"
  },
  {
    "text": "interactive uh workloads and with htfs you can do that the Sim similar workload",
    "start": "360400",
    "end": "367080"
  },
  {
    "text": "uh workload Paradigm of interactive workloads can be done with SLA so you could have a nosql store like H running",
    "start": "367080",
    "end": "372639"
  },
  {
    "text": "on top of uh htfs and um interact with have an application that has let's say",
    "start": "372639",
    "end": "378919"
  },
  {
    "text": "10 millisecond uh SLS for gets inputs or search or you have Impala queries",
    "start": "378919",
    "end": "384199"
  },
  {
    "text": "running on on the data in htfs and you have SLS associated with that so you could do that with htfs",
    "start": "384199",
    "end": "391520"
  },
  {
    "text": "so and with these two sort of storage systems the predominant uh predominant",
    "start": "391520",
    "end": "397199"
  },
  {
    "text": "way of running a cluster with S3 is a transient cluster where your cluster doesn't have any State uh it is really",
    "start": "397199",
    "end": "403919"
  },
  {
    "text": "something that you would provision to run a some sort of a workload and then kill it after that uh and with hdfs you",
    "start": "403919",
    "end": "410680"
  },
  {
    "text": "have a long running cluster where the state is stored in htfs right and so",
    "start": "410680",
    "end": "416120"
  },
  {
    "start": "416000",
    "end": "451000"
  },
  {
    "text": "your the the role of your cluster in a transient cluster with 3 is only compute with uh with a long running cluster with",
    "start": "416120",
    "end": "422720"
  },
  {
    "text": "htfs as compute and storage the uh the Frameworks that you would run in a transient cluster is",
    "start": "422720",
    "end": "429120"
  },
  {
    "text": "they're predominantly either map eduse or um or spark right so that's really the batch uh batch Frameworks that you",
    "start": "429120",
    "end": "436120"
  },
  {
    "text": "would use but on the other side with with long running clusters with hdfs you can run those you can run hbas solar and",
    "start": "436120",
    "end": "443080"
  },
  {
    "text": "Impala um so those are the two deployment models right out there that",
    "start": "443080",
    "end": "448240"
  },
  {
    "text": "that that you have available ailable in AWS let's dive a little deeper into uh",
    "start": "448240",
    "end": "454360"
  },
  {
    "text": "different choices in terms of storage and what are the access patterns and sort of performance implications of these different choices",
    "start": "454360",
    "end": "461440"
  },
  {
    "start": "461000",
    "end": "488000"
  },
  {
    "text": "now before we go into the different choices I want to highlight a difference in Paradigm with with Hadoop and how uh",
    "start": "461440",
    "end": "470199"
  },
  {
    "text": "uh sort of how Cloud environments work you know the Hadoop Paradigm is you bring compute to storage so you have uh",
    "start": "470199",
    "end": "476960"
  },
  {
    "text": "data stored in in diss on individual servers and you ship your computer to the to to as close to those discs as",
    "start": "476960",
    "end": "483919"
  },
  {
    "text": "possible and minimize reading over the network and that's the fundamental Paradigm behind uh behind Hado whereas",
    "start": "483919",
    "end": "489800"
  },
  {
    "start": "488000",
    "end": "523000"
  },
  {
    "text": "with when you're running in Cloud environments that's not the Paradigm that you run with uh you consume everything as a service including",
    "start": "489800",
    "end": "496120"
  },
  {
    "text": "storage and which is S3 in this case right so with S3 you're reading everything over the network to your to",
    "start": "496120",
    "end": "503000"
  },
  {
    "text": "your ec2 instances and there are certain performance implications of that um so these are two two different paradigms",
    "start": "503000",
    "end": "509560"
  },
  {
    "text": "with her dup and cloud and that's this is where um this is where it gets",
    "start": "509560",
    "end": "515839"
  },
  {
    "text": "interesting because um you you trade off flexibility uh for performance and and",
    "start": "515839",
    "end": "520919"
  },
  {
    "text": "the kind of workloads that you would run so let's look at the choices of storage in AWS you know you have the",
    "start": "520919",
    "end": "527800"
  },
  {
    "start": "523000",
    "end": "592000"
  },
  {
    "text": "instance stores which is the fmal drives on the individual instances uh they are really local discs attached to the",
    "start": "527800",
    "end": "534600"
  },
  {
    "text": "instances and different instances come with different amounts of local storage uh they are temporary in nature so you",
    "start": "534600",
    "end": "541440"
  },
  {
    "text": "uh if you shut down an instance or uh stop an instance whatever is on your FML drives is lost right they're also",
    "start": "541440",
    "end": "548680"
  },
  {
    "text": "instance dependent uh so you can't really say I want this much storage on this particular instance they come with",
    "start": "548680",
    "end": "554079"
  },
  {
    "text": "whatever they come with and you got to use that that goes into EBS which is you could attach additional uh additional",
    "start": "554079",
    "end": "561279"
  },
  {
    "text": "volumes uh and the life cycle of these is independent of the life cycle of the instance",
    "start": "561279",
    "end": "567560"
  },
  {
    "text": "right but again with e yes you're everything is over the network it's not local discs on the instance themselves",
    "start": "567560",
    "end": "573560"
  },
  {
    "text": "so there's performance implications of that um the third is obviously S3 which is really an external data store right",
    "start": "573560",
    "end": "579640"
  },
  {
    "text": "so you have a very simplistic API you you get uh objects you put objects you delete objects and the the bandwidth to",
    "start": "579640",
    "end": "586000"
  },
  {
    "text": "S3 is dependent on the instance that you uh that you're interacting with S3",
    "start": "586000",
    "end": "591959"
  },
  {
    "text": "from um so let's talk about S3 and then I'll talk about running htfs and uh what",
    "start": "592760",
    "end": "598839"
  },
  {
    "text": "are the consider there you could interact with S3 from App produce jobs by using the s3a URI",
    "start": "598839",
    "end": "605399"
  },
  {
    "start": "600000",
    "end": "630000"
  },
  {
    "text": "which is available in the Upstream Apachi hu project now uh and that uses the native SDK uh so you could do that",
    "start": "605399",
    "end": "613040"
  },
  {
    "text": "and use S3 as a source or destination for your map produce jobs or spark jobs right this CP is really a specialized",
    "start": "613040",
    "end": "620160"
  },
  {
    "text": "map Produce job u under the hood which which basically copies data from",
    "start": "620160",
    "end": "625320"
  },
  {
    "text": "one location to the other and it could be from S3 to htfs or vice versa",
    "start": "625320",
    "end": "630959"
  },
  {
    "start": "630000",
    "end": "658000"
  },
  {
    "text": "uh hbas has snapshots and it has the ability to export snapshot straight to S3 uh and uh you could do that uh from",
    "start": "630959",
    "end": "639200"
  },
  {
    "text": "the hbas command line and say Here's this this particular snapshot name I want to run these many mappers copy it",
    "start": "639200",
    "end": "644399"
  },
  {
    "text": "to this location in S3 and you restore from it later on so those are the three things that you could do with S3 from a",
    "start": "644399",
    "end": "651680"
  },
  {
    "text": "h deployment uh and for backups for example you would use this CP",
    "start": "651680",
    "end": "659200"
  },
  {
    "start": "658000",
    "end": "736000"
  },
  {
    "text": "there are multiple implementations for interacting with S3 in the in Theo project so there's the S3 implementation",
    "start": "659680",
    "end": "665000"
  },
  {
    "text": "which is a block based implementation where uh Hado treats S3 as um as a sto",
    "start": "665000",
    "end": "671839"
  },
  {
    "text": "as sort of a storage system where it would put the blocks of the file so hdfs has files and then splits them into",
    "start": "671839",
    "end": "678440"
  },
  {
    "text": "blocks and it'll store the blocks in S3 now the downside of that implementation is that nobody else can no other",
    "start": "678440",
    "end": "685720"
  },
  {
    "text": "application can actually interact with that data because nobody knows how those blocks uh combined to form a file so that",
    "start": "685720",
    "end": "691480"
  },
  {
    "text": "metadata is still living with with h so that's not the most uh optimal",
    "start": "691480",
    "end": "696880"
  },
  {
    "text": "implementation s3n is a file based implementation which uses jetset under the hood and that treats S3 as uh as a",
    "start": "696880",
    "end": "704320"
  },
  {
    "text": "file store where it'll store files so the metadata is managed by S3 any other application can come in and actually",
    "start": "704320",
    "end": "709839"
  },
  {
    "text": "talk to S3 and get access to those files um it uses jetset which is like I",
    "start": "709839",
    "end": "715760"
  },
  {
    "text": "said a third party Library uh open source Library uh s3a is the latest one",
    "start": "715760",
    "end": "721360"
  },
  {
    "text": "that has been written that uses the AWS SDK and there's a bunch of optimizations there in terms of how it interacts with",
    "start": "721360",
    "end": "726880"
  },
  {
    "text": "S3 that s3n doesn't have so that's the latest one that was just committed a couple of month couple of months ago and",
    "start": "726880",
    "end": "733680"
  },
  {
    "text": "that's the one that we recommend you use now the bandwidth to S3 like I said is dependent on the instance right and",
    "start": "733680",
    "end": "739959"
  },
  {
    "start": "736000",
    "end": "772000"
  },
  {
    "text": "the maximum that we have seen in our testing is about 200 megabytes a second and that is not what Hadoop sees that's",
    "start": "739959",
    "end": "746240"
  },
  {
    "text": "what sort of you that's a raw bandwidth from uh from the instance to uh to S3",
    "start": "746240",
    "end": "751680"
  },
  {
    "text": "and 200 megab a second is with some of the larger instances if you if you go to let's say the M1 family or M1 large",
    "start": "751680",
    "end": "758160"
  },
  {
    "text": "that's not what you'll see you'll see a much lower throughput there so that's a consideration to keep in mind so 200",
    "start": "758160",
    "end": "764160"
  },
  {
    "text": "megab a second you're really looking at it as the equivalent of two local discs right so that's the max that you can get",
    "start": "764160",
    "end": "769880"
  },
  {
    "text": "from an instance to S3 now like I said Hado doesn't see that and there's a reason for that when",
    "start": "769880",
    "end": "776680"
  },
  {
    "start": "772000",
    "end": "854000"
  },
  {
    "text": "you're moving data between hdfs and S3 what happens is from htfs the data gets copied over to each file would get",
    "start": "776680",
    "end": "783279"
  },
  {
    "text": "copied over to the local file system uh where a check sum is calculated and uh",
    "start": "783279",
    "end": "788399"
  },
  {
    "text": "the splits are calculated for multipal uploads that can't be done in hdfs itself because hdfs doesn't have the",
    "start": "788399",
    "end": "794480"
  },
  {
    "text": "ability to give you a check sum for a for an entire file it'll give you check sums for blocks so because of that you",
    "start": "794480",
    "end": "800000"
  },
  {
    "text": "have to copy it to the local file system um now that's that's an overhead and it",
    "start": "800000",
    "end": "806000"
  },
  {
    "text": "takes time to do that because of which you'll see the end to end through drops from 200 megabytes a second because you",
    "start": "806000",
    "end": "811240"
  },
  {
    "text": "have to pay pay that cost now this is the second cost that you pay you upload to S3 in a temporary space you don't",
    "start": "811240",
    "end": "817920"
  },
  {
    "text": "upload to uh S3 in the in the final location you upload to S3 in a temporary space and once the entire file is",
    "start": "817920",
    "end": "823480"
  },
  {
    "text": "complete you rename that file now in S3 a rename is equal to a move operation so let's say you uploaded a 40 gig file",
    "start": "823480",
    "end": "830519"
  },
  {
    "text": "you'll now when to complete that upload you have to wait for that file to get renamed or moved to its final location",
    "start": "830519",
    "end": "837360"
  },
  {
    "text": "right it's not something that's going to happen in a coup seconds uh so you lose some time there both those things put together you",
    "start": "837360",
    "end": "845519"
  },
  {
    "text": "you realistically sort of see a throughput of about 130 to 150 megabytes a second uh from one of the lar from the",
    "start": "845519",
    "end": "852000"
  },
  {
    "text": "larger instances now you can there's there's a",
    "start": "852000",
    "end": "857440"
  },
  {
    "start": "854000",
    "end": "912000"
  },
  {
    "text": "bunch of tuning that you can do uh with with uh S3 interactions from Hadoop and",
    "start": "857440",
    "end": "862600"
  },
  {
    "text": "there's a couple of things that uh from uh when you're moving large files you need to you need to think about uh and",
    "start": "862600",
    "end": "868440"
  },
  {
    "text": "that's predominant around task timeouts and I'll talk about that so tuning the the first thing that you want to do is",
    "start": "868440",
    "end": "873959"
  },
  {
    "text": "paralyze as much as possible now the throughput that a single thread will see to S3 is not 150 megab a second that's",
    "start": "873959",
    "end": "880839"
  },
  {
    "text": "not what you'll see from a single thread but multiple threads combined from a single instance can get you 200 megabytes a second so you want to",
    "start": "880839",
    "end": "887440"
  },
  {
    "text": "paralyze as much as possible now does that mean you run uh four tasks does",
    "start": "887440",
    "end": "893160"
  },
  {
    "text": "that mean you run eight tasks that depends on your environment that's something that you'll have to test and get uh figure out what what the optimal",
    "start": "893160",
    "end": "900120"
  },
  {
    "text": "this thing for you is uh we saw at about 10 tasks we saw most of our uh most of",
    "start": "900120",
    "end": "906320"
  },
  {
    "text": "our experiments sort of max out at about 10 tasks going beyond that doesn't give you any additional",
    "start": "906320",
    "end": "912160"
  },
  {
    "text": "benefit so when you're writing to S3 uh you want to there's multipart uploads",
    "start": "912160",
    "end": "917959"
  },
  {
    "text": "that is enabled that you can enable in uh in the s3a connector uh and that's just a standard uh standard",
    "start": "917959",
    "end": "924720"
  },
  {
    "text": "configuration that is there under the hood but you want to enable it in the H configuration so the connector knows to",
    "start": "924720",
    "end": "929880"
  },
  {
    "text": "run that um for for the local staging directories you want to use multiple",
    "start": "929880",
    "end": "936800"
  },
  {
    "text": "drives because otherwise let's say from a single node you are uploading um five",
    "start": "936800",
    "end": "942560"
  },
  {
    "text": "10 GB files and all of them all the tasks start to write to that single dis",
    "start": "942560",
    "end": "947839"
  },
  {
    "text": "you'll bottleneck there and we saw that happen uh several times so there's this",
    "start": "947839",
    "end": "953759"
  },
  {
    "text": "there's a patch Upstream which allows you to specify multiple drives there and it'll round drop in between those",
    "start": "953759",
    "end": "959880"
  },
  {
    "text": "those and that increases your true put of course up the task timeouts when you're writing large files and what that",
    "start": "959880",
    "end": "966399"
  },
  {
    "text": "means is when so when when a task is writing to to S3 and then it does the",
    "start": "966399",
    "end": "972160"
  },
  {
    "text": "rename operation S3 does not give you back any any heartbeat or sort of status",
    "start": "972160",
    "end": "977720"
  },
  {
    "text": "of that DM operation right it'll block there and when it blocks there the task is not getting any uh it's it doesn't",
    "start": "977720",
    "end": "983480"
  },
  {
    "text": "ping back the the job tracker so what happens is the job tracker after 10 minutes will say I haven't heard heard",
    "start": "983480",
    "end": "989079"
  },
  {
    "text": "back from this task let me kill it it's dead right uh and what would have happened is 40 GB file which which is",
    "start": "989079",
    "end": "996160"
  },
  {
    "text": "pretty common when you're uh exporting HB snapshots 40 GB file at the end of it",
    "start": "996160",
    "end": "1001240"
  },
  {
    "text": "at the end of the move operation gets uh the task gets killed so you want to up the task timeouts the downside of upping",
    "start": "1001240",
    "end": "1007160"
  },
  {
    "text": "the task timeouts is when there's genuinely a stuck task it takes that much longer for the framework to figure",
    "start": "1007160",
    "end": "1012279"
  },
  {
    "text": "it out so what you want to do there is uh play with the default is 10 minutes",
    "start": "1012279",
    "end": "1017680"
  },
  {
    "text": "you want to you want to play with those with that setting and see what the optimal is for you uh we opted to about",
    "start": "1017680",
    "end": "1023959"
  },
  {
    "text": "an hour for uh 40 GB files and uh that's when the job would complete uh",
    "start": "1023959",
    "end": "1033079"
  },
  {
    "start": "1033000",
    "end": "1062000"
  },
  {
    "text": "reliably reading from S3 uh there's range reads that's built inside the s3a connector uh which has multiple threads",
    "start": "1033559",
    "end": "1041079"
  },
  {
    "text": "that read a single file uh it does not do it does not split a single file to multiple map tasks and the reason for",
    "start": "1041079",
    "end": "1047360"
  },
  {
    "text": "that is there's no way for it to then combine those multiple Parts back without a reducer understanding how the",
    "start": "1047360",
    "end": "1053200"
  },
  {
    "text": "file works and uh that is not something that has been built into the connector yet",
    "start": "1053200",
    "end": "1058720"
  },
  {
    "text": "today it's multiple threads within the single map task now when when working with S3",
    "start": "1058720",
    "end": "1065559"
  },
  {
    "start": "1062000",
    "end": "1129000"
  },
  {
    "text": "larger objects are better so think a smaller number of large objects versus a large number of small objects um s when",
    "start": "1065559",
    "end": "1073240"
  },
  {
    "text": "you're doing large reads or sort of saying give me the entire bucket and I'm going to scan through it and run my",
    "start": "1073240",
    "end": "1079200"
  },
  {
    "text": "aggregate uh if you have too many small files there's a load on the metadata",
    "start": "1079200",
    "end": "1085000"
  },
  {
    "text": "server in in S3 that'll uh that'll cut your throughput down so you want to have",
    "start": "1085000",
    "end": "1091720"
  },
  {
    "text": "as many large files as possible and as low number of small files as possible for the metadata lookups uh you",
    "start": "1091720",
    "end": "1099080"
  },
  {
    "text": "also want to randomize your file names so that way your um your load on the",
    "start": "1099080",
    "end": "1104120"
  },
  {
    "text": "metad data service is spread out right so that's the other thing now had doesn't do that for you you will have to do that on your own so which means if if",
    "start": "1104120",
    "end": "1111280"
  },
  {
    "text": "S3 is your Landing zone for uh for data that's coming into uh into your data management framework uh you want to",
    "start": "1111280",
    "end": "1118159"
  },
  {
    "text": "randomize the names as much as possible now often times that's not that's not possible or is just additional work uh",
    "start": "1118159",
    "end": "1123880"
  },
  {
    "text": "but that's the price that you would pay is the metadata service being the",
    "start": "1123880",
    "end": "1129200"
  },
  {
    "start": "1129000",
    "end": "1211000"
  },
  {
    "text": "bottleneck so that's about S3 uh let's talk about how do you run hdfs so you want to use the FM drives in",
    "start": "1129200",
    "end": "1137080"
  },
  {
    "text": "in in ec2 and it's okay to use the FML drives because Loop has replication under the hood uh and if you lose an",
    "start": "1137080",
    "end": "1143760"
  },
  {
    "text": "instance or you lose uh you lose a couple of instances you won't lose data uh soop will take care of that from its",
    "start": "1143760",
    "end": "1150280"
  },
  {
    "text": "replication standpoint so it's okay to use the fmal drives there now it does not give you the same level of",
    "start": "1150280",
    "end": "1155840"
  },
  {
    "text": "durability and reliability as S3 would uh but at the same time it gives you a fairly High fairly high",
    "start": "1155840",
    "end": "1163679"
  },
  {
    "text": "durability and it's obviously HFS will be persistent for as long as the instances are alive you can't pause an",
    "start": "1163679",
    "end": "1170280"
  },
  {
    "text": "htfs instance because the moment you stop the instances you lose the data um and you want to use S3 for",
    "start": "1170280",
    "end": "1177000"
  },
  {
    "text": "backups uh as much as possible now we don't recommend you use",
    "start": "1177000",
    "end": "1182360"
  },
  {
    "text": "EBS to back uh back hdfs and the reason for that is you're going everything is",
    "start": "1182360",
    "end": "1187919"
  },
  {
    "text": "going over the network at that point in time right and EBS is really designed for random iio now as of this morning",
    "start": "1187919",
    "end": "1193640"
  },
  {
    "text": "with the new numbers that um that EBS can support up to 160 megab a second per",
    "start": "1193640",
    "end": "1198960"
  },
  {
    "text": "volume some of this is up for consideration again and we'll be looking into where where EBS can fit in but as",
    "start": "1198960",
    "end": "1206080"
  },
  {
    "text": "it stands today EBS is sort of an anti- pattern let's talk about networking so",
    "start": "1206080",
    "end": "1213880"
  },
  {
    "start": "1211000",
    "end": "1339000"
  },
  {
    "text": "there's really there's really um so we recommend deploying in in a VPC and and the reason",
    "start": "1213880",
    "end": "1220600"
  },
  {
    "text": "for that is it it looks like an uh extension to your data center and um",
    "start": "1220600",
    "end": "1226559"
  },
  {
    "text": "with with VPN or direct connect that's what uh you can configure it to be but if you're deploying ec2 classic",
    "start": "1226559",
    "end": "1233159"
  },
  {
    "text": "everything uh so the only way to access stuff there would be over the public internet and you don't want your hop",
    "start": "1233159",
    "end": "1238480"
  },
  {
    "text": "cluster to be the only way you can access it is by going over the public internet so we recommend you deploying",
    "start": "1238480",
    "end": "1244039"
  },
  {
    "text": "in know VPC in a VPC you can have two kinds of subnets uh one is a subnet",
    "start": "1244039",
    "end": "1249600"
  },
  {
    "text": "where all your instances have uh the ability to interact with things outside",
    "start": "1249600",
    "end": "1255280"
  },
  {
    "text": "the AWS network uh or sort of over the internet uh S3 actually happens to be",
    "start": "1255280",
    "end": "1261080"
  },
  {
    "text": "something that you need to go over the public internet pipe to interact with it uh interact with from ec2 so if your",
    "start": "1261080",
    "end": "1267360"
  },
  {
    "text": "instances or if your hoodo cluster needs to interact with S3 for example for backups or U S3 is your Landing Zone",
    "start": "1267360",
    "end": "1273919"
  },
  {
    "text": "which is what we recommend that you do you need to have your data nodes be deployed in the public subnet so that it",
    "start": "1273919",
    "end": "1280080"
  },
  {
    "text": "can it has full throughput to S3 and on your corporate Network side",
    "start": "1280080",
    "end": "1287039"
  },
  {
    "text": "you can set up a VPN connect so that you have uh the ability to interact with the ec2 instances without having to go",
    "start": "1287039",
    "end": "1293200"
  },
  {
    "text": "through the internet the other deployment model is where a cluster is set up in a private",
    "start": "1293200",
    "end": "1298960"
  },
  {
    "text": "subnet and a private subnet is where these instances don't have the ability to interact with uh anything outside AWS",
    "start": "1298960",
    "end": "1305919"
  },
  {
    "text": "as outside ec2 and the way you would do that is if you needed to access anything",
    "start": "1305919",
    "end": "1311760"
  },
  {
    "text": "over the Internet is by going through uh a net instance that you would set up in a public subnet now",
    "start": "1311760",
    "end": "1319200"
  },
  {
    "text": "that is not the way you would want to interact with S3 though because you're you you'll be bottlenecked by the throughput that that instance can",
    "start": "1319200",
    "end": "1326960"
  },
  {
    "text": "serve so for a large cluster uh that'll you'll choke that pretty soon so those",
    "start": "1326960",
    "end": "1332919"
  },
  {
    "text": "are the two deployment uh options or topologies when deploying inside uh",
    "start": "1332919",
    "end": "1339240"
  },
  {
    "start": "1339000",
    "end": "1484000"
  },
  {
    "text": "VPC then there are performance considerations from a networking standpoint you know the instance to instance link is important because you",
    "start": "1339240",
    "end": "1345679"
  },
  {
    "text": "have replication happening in hdfs U map ruce jobs have shuffles and sort of sort phases so",
    "start": "1345679",
    "end": "1351559"
  },
  {
    "text": "do spark jobs uh and you need a good Network at that point some instances",
    "start": "1351559",
    "end": "1356919"
  },
  {
    "text": "come with 10 gig some instances uh some instances come with 10 gig with enhanced networking and you have to use the hbm",
    "start": "1356919",
    "end": "1363000"
  },
  {
    "text": "Amis for that and there are other instances that don't have a 10 gig link excuse me they have low medium or",
    "start": "1363000",
    "end": "1371520"
  },
  {
    "text": "high uh Network and that does have uh performance implication so we'd recommend using the ones with 10 gig or",
    "start": "1371520",
    "end": "1378799"
  },
  {
    "text": "10 gig plus sov as much as possible the instance to S3 is equal to",
    "start": "1378799",
    "end": "1384960"
  },
  {
    "text": "the instance to public internet link and again is instance specific uh and that is something that you get with the 10",
    "start": "1384960",
    "end": "1391400"
  },
  {
    "text": "gig uh the instances that have a 10 gig link have a higher throughput goes up to 200 megabytes a second like I",
    "start": "1391400",
    "end": "1398559"
  },
  {
    "text": "said placement groups is something that is important to consider here when you",
    "start": "1398559",
    "end": "1404080"
  },
  {
    "text": "deploy instances inside a placement group AWS tries to give you instances as close to each other as possible and they",
    "start": "1404080",
    "end": "1410400"
  },
  {
    "text": "also guarantee that any uh the instance to instance bandwidth inside a placement group would be a full 10 gig",
    "start": "1410400",
    "end": "1418600"
  },
  {
    "text": "link however if you're not uh now the downside of deploying inside the placement group is that you have a",
    "start": "1418600",
    "end": "1424080"
  },
  {
    "text": "limited pool of resources from which you'll provision if you're not deploying inside a placement group uh you could run into",
    "start": "1424080",
    "end": "1432120"
  },
  {
    "text": "situations where instance to instance link would drop to something like 2 gigs and that's what we saw in our testing",
    "start": "1432120",
    "end": "1439200"
  },
  {
    "text": "and you don't want that in a large cluster because that then the your performance is not predictable uh because sometimes your rights would take",
    "start": "1439200",
    "end": "1445919"
  },
  {
    "text": "longer because you're going through that 2 gig pipeline uh and so we'd recommend",
    "start": "1445919",
    "end": "1451240"
  },
  {
    "text": "deploy your cluster inside a placement group and also inside a single",
    "start": "1451240",
    "end": "1458200"
  },
  {
    "text": "availability zone right so there we don't recommend you deploy clusters across availability zones now having",
    "start": "1458200",
    "end": "1463400"
  },
  {
    "text": "said that uh that is sort of an anti- pattern when it comes to uh deploying",
    "start": "1463400",
    "end": "1468480"
  },
  {
    "text": "AWS because going across A's is the uh the recommended way for high",
    "start": "1468480",
    "end": "1473600"
  },
  {
    "text": "availability uh as it stands today Hadoop can't really leverage that very well and you'll see a significant",
    "start": "1473600",
    "end": "1479600"
  },
  {
    "text": "performance drop when you're going cross a let's talk about uh the different ec2",
    "start": "1479600",
    "end": "1489320"
  },
  {
    "start": "1484000",
    "end": "1616000"
  },
  {
    "text": "instances so this transient clusters the the primary consideration for picking",
    "start": "1491960",
    "end": "1498000"
  },
  {
    "text": "instance types are bandwidth CPU and memory right and bandwidth is to S3 that",
    "start": "1498000",
    "end": "1503520"
  },
  {
    "text": "you care about um and it's more the aggregate bandwidth across the entire cluster than the bandwidth from a single",
    "start": "1503520",
    "end": "1508960"
  },
  {
    "text": "instance CPU and memory are obviously important depending on whatever workload you're running so there's no I would say",
    "start": "1508960",
    "end": "1514760"
  },
  {
    "text": "there's a magic number that you need to work with for CPU memory um availability and fall",
    "start": "1514760",
    "end": "1520159"
  },
  {
    "text": "tolerance for those instances is not that important uh because if a cluster dies you're fine you can just provision",
    "start": "1520159",
    "end": "1526159"
  },
  {
    "text": "a new one it's really stateless it doesn't hold hold any state except that particular job uh so from that",
    "start": "1526159",
    "end": "1531919"
  },
  {
    "text": "perspective it is important but otherwise it's not it's not the primary consideration similarly local storage",
    "start": "1531919",
    "end": "1537600"
  },
  {
    "text": "density is not a primary consideration for transient clusters um typical instance choices that we see customers",
    "start": "1537600",
    "end": "1544440"
  },
  {
    "text": "choose for that workload C3 M3 uh M1 families it's really an anti-pattern to",
    "start": "1544440",
    "end": "1550640"
  },
  {
    "text": "use something like an hs1 or a cc2 uh 8xl for a transient",
    "start": "1550640",
    "end": "1555840"
  },
  {
    "text": "cluster long running clusters on the other hand uh local storage is really the key right and you want to cap you want to plan",
    "start": "1555840",
    "end": "1562200"
  },
  {
    "text": "your in plan your cluster and choose your instances based on how much storage you need CPU memory obviously are",
    "start": "1562200",
    "end": "1568640"
  },
  {
    "text": "important from the workload standpoint um availability in Fall tolerance becomes important here because that's",
    "start": "1568640",
    "end": "1574360"
  },
  {
    "text": "where your state is stored and you want the cluster to be as available as possible now some of the larger",
    "start": "1574360",
    "end": "1580600"
  },
  {
    "text": "instances uh have smaller number of virtual machines per physical host than some of the smaller ones and so going",
    "start": "1580600",
    "end": "1587440"
  },
  {
    "text": "using the larger ones for long running clusters makes sense also you'll get a higher bandwidth",
    "start": "1587440",
    "end": "1593279"
  },
  {
    "text": "in those ones so otherwise if you use the smaller instances the instance to instance uh network is",
    "start": "1593279",
    "end": "1599720"
  },
  {
    "text": "smaller typical choices that we see customers use um hs18 extra large cc28",
    "start": "1599720",
    "end": "1605799"
  },
  {
    "text": "extra large i28 extra large mostly for storage heavy use cases hs18 extra large",
    "start": "1605799",
    "end": "1612000"
  },
  {
    "text": "is the instance type that uh that our customers pick from there you obviously uh have to",
    "start": "1612000",
    "end": "1619159"
  },
  {
    "start": "1616000",
    "end": "1696000"
  },
  {
    "text": "pick the right Amis as well you really have two choices right so there's either PB or hbm and um one thing that I would",
    "start": "1619159",
    "end": "1626039"
  },
  {
    "text": "say is pick a Dependable base Ami uh something that a vendor supports because you don't want to be uh you don't want",
    "start": "1626039",
    "end": "1631919"
  },
  {
    "text": "to be worrying about and managing your own Amis uh and so if somebody supports an Ami that would be the place that I",
    "start": "1631919",
    "end": "1638200"
  },
  {
    "text": "would start things to look out for in when you're looking at an Ami are the colel patches up to date um are all sort of",
    "start": "1638200",
    "end": "1645840"
  },
  {
    "text": "the third party software and Library versions up to date based on whatever you need to run uh and uh those those",
    "start": "1645840",
    "end": "1653120"
  },
  {
    "text": "are the things that I would say look out for uh and you know when I say A Dependable base Ami",
    "start": "1653120",
    "end": "1660120"
  },
  {
    "text": "what I what I really mean there is like people like you and I can actually go and create our own Amis put in the marketplace try not to use those try the",
    "start": "1660120",
    "end": "1666760"
  },
  {
    "text": "try to use the ones that um Amazon is providing or red hat is",
    "start": "1666760",
    "end": "1672679"
  },
  {
    "text": "providing one of the things with any Ami that you might choose is increase the root volume size the default root volume",
    "start": "1672679",
    "end": "1679159"
  },
  {
    "text": "is actually pretty small and you'll very quickly find yourself in a place where",
    "start": "1679159",
    "end": "1684320"
  },
  {
    "text": "logs would fill it up or uh packages would fill it up so we recommend going to at least a 500 gig root volume uh for",
    "start": "1684320",
    "end": "1692159"
  },
  {
    "text": "for a production deployment let's talk about",
    "start": "1692159",
    "end": "1699720"
  },
  {
    "start": "1696000",
    "end": "1831000"
  },
  {
    "text": "security now you want to deploy in in bpc so we talked about that right and uh",
    "start": "1699720",
    "end": "1704960"
  },
  {
    "text": "B you have two choices there private subnets or public subnets and it becomes important to talk important to look at",
    "start": "1704960",
    "end": "1711039"
  },
  {
    "text": "how you would secure this environment uh Network ACLS at a subnet level are are",
    "start": "1711039",
    "end": "1716640"
  },
  {
    "text": "available where you can block certain kinds of traffic to a certain IP range right or all inbound traffic from the",
    "start": "1716640",
    "end": "1724120"
  },
  {
    "text": "from anything outside uh ec2 or the internet right so you can you can block those things at the ACL Network ACL",
    "start": "1724120",
    "end": "1731519"
  },
  {
    "text": "level security groups is something that I would say spend you should spend more time on security groups rather than",
    "start": "1731519",
    "end": "1736679"
  },
  {
    "text": "Network ACLS uh because Network ACS are are stateless right the security groups",
    "start": "1736679",
    "end": "1742360"
  },
  {
    "text": "you you um they're stateful and uh give you more flexibility in terms of how you",
    "start": "1742360",
    "end": "1748880"
  },
  {
    "text": "uh how you define your rules uh and allowing for traffic sort of inbound",
    "start": "1748880",
    "end": "1754880"
  },
  {
    "text": "traffic from your subnet to your data center probably not a good idea right unless you really need uh to be able to",
    "start": "1754880",
    "end": "1761679"
  },
  {
    "text": "pushing push something from the cluster down to your data center uh but the other way around is",
    "start": "1761679",
    "end": "1769080"
  },
  {
    "text": "important uh especially if you have if your cluster is in a public subnet right so you're you're accessing",
    "start": "1769080",
    "end": "1774760"
  },
  {
    "text": "S3 uh so work with your I would say the the IT team on figuring out exactly what",
    "start": "1774760",
    "end": "1780480"
  },
  {
    "text": "rules you need to abide by uh to from a from a security standpoint but those are",
    "start": "1780480",
    "end": "1785679"
  },
  {
    "text": "the things that you need to be um careful of beyond that there is nothing AWS specific that you need to do uh",
    "start": "1785679",
    "end": "1793240"
  },
  {
    "text": "there's nothing Hadoop specific that you need to do uh from an AWS deployment standpoint uh it's really thinking about",
    "start": "1793240",
    "end": "1799600"
  },
  {
    "text": "security groups and ACLS and your subnets uh that is just sort of structuring it in AWS for hudo the",
    "start": "1799600",
    "end": "1806320"
  },
  {
    "text": "standard guidelines around uh Cur Bros uh ad integration encryption those those",
    "start": "1806320",
    "end": "1812240"
  },
  {
    "text": "exist ju just as much in AWS as they would in on Prem deployment so there's nothing nothing special",
    "start": "1812240",
    "end": "1819600"
  },
  {
    "text": "there now for backups obviously S3 provides server side encryption so that's the other security consideration",
    "start": "1820320",
    "end": "1826600"
  },
  {
    "text": "uh and our customers use that pretty heavily so look at ha um backups and",
    "start": "1826600",
    "end": "1834679"
  },
  {
    "start": "1831000",
    "end": "1908000"
  },
  {
    "text": "Disaster Recovery now ha uh ha is available in the Hado stack",
    "start": "1834679",
    "end": "1840440"
  },
  {
    "text": "right so you run multiple name nodes you run Journal nodes to to manage your edits uh you run multiple zookeepers if",
    "start": "1840440",
    "end": "1846799"
  },
  {
    "text": "you're running anything with zookeeper you're onun multiple HBS Masters uh that's just available in the",
    "start": "1846799",
    "end": "1852120"
  },
  {
    "text": "H stack and you use it uh just as is again nothing AWS specific there backup",
    "start": "1852120",
    "end": "1857679"
  },
  {
    "text": "and backups and Disaster Recovery are a little little more interesting because uh when you're doing on primp",
    "start": "1857679",
    "end": "1863919"
  },
  {
    "text": "deployments the only way you do this is you have another htfs cluster right so that's that's the only choice from a",
    "start": "1863919",
    "end": "1870720"
  },
  {
    "text": "backup or a Dr standpoint uh or you would use a an expensive shared storage",
    "start": "1870720",
    "end": "1876519"
  },
  {
    "text": "but in AWS you have the ability to use S3 for a warm backup uh so that's I",
    "start": "1876519",
    "end": "1881559"
  },
  {
    "text": "would say that is something that you should absolutely do for an act for a hot backup though running multiple",
    "start": "1881559",
    "end": "1887120"
  },
  {
    "text": "clusters across to availability zones is something that our customers uh our customers consider in that particular",
    "start": "1887120",
    "end": "1893600"
  },
  {
    "text": "use case where they need a hot pup um warm backups S3 and Glacier is obviously",
    "start": "1893600",
    "end": "1899799"
  },
  {
    "text": "available for cold backups we don't really see that being uh that being used a whole",
    "start": "1899799",
    "end": "1906799"
  },
  {
    "start": "1908000",
    "end": "2120000"
  },
  {
    "text": "lot so let's look at sort of planning your cluster right and we we talked about all these different considerations",
    "start": "1908039",
    "end": "1914039"
  },
  {
    "text": "from Storage compute uh networking standpoint but really how you go about using all of this and uh and planning",
    "start": "1914039",
    "end": "1921120"
  },
  {
    "text": "your first deployment the bad news is there's no simple answer unfortunately right you",
    "start": "1921120",
    "end": "1927880"
  },
  {
    "text": "have to think through all these considerations now the good news there is mistakes are really cheap right you don't like what you what you see from a",
    "start": "1927880",
    "end": "1934559"
  },
  {
    "text": "performance standpoint kill the cluster get a new one with uh with beir boxes um",
    "start": "1934559",
    "end": "1940080"
  },
  {
    "text": "and obviously whatever we're sharing here is what we've learned over the course of the last year year and a half supporting production deployments in",
    "start": "1940080",
    "end": "1946840"
  },
  {
    "text": "AWS so what you want to start with is what is the workload type that you care about is it an ad hoc or an SLA uh",
    "start": "1946840",
    "end": "1953360"
  },
  {
    "text": "workload with is it batch if it's purely batch then S3 is obviously a good good",
    "start": "1953360",
    "end": "1958720"
  },
  {
    "text": "choice uh but if you need the interactive uh interactive workload with Impala or or HP or solar S3 is not the",
    "start": "1958720",
    "end": "1966720"
  },
  {
    "text": "storage system that you would want to work with so that's the first place that you want to start from there what",
    "start": "1966720",
    "end": "1971960"
  },
  {
    "text": "percentage of of the day would you use that cluster for so if you're going to use it only for 4 hours and for uh uh",
    "start": "1971960",
    "end": "1978320"
  },
  {
    "text": "for a batch workload uh a transient cluster makes perfect sense right but if you're going to use it for more than 20",
    "start": "1978320",
    "end": "1983840"
  },
  {
    "text": "hours a day you don't want to pay the price of spinning it up spinning it down uh or hydrating it with data you you're",
    "start": "1983840",
    "end": "1990799"
  },
  {
    "text": "probably better off running it with a uh with a long running cluster how much data are you storing",
    "start": "1990799",
    "end": "1997080"
  },
  {
    "text": "and that that would impact the uh the instance types that you would pick right uh because different instance types have",
    "start": "1997080",
    "end": "2003000"
  },
  {
    "text": "different amount of local storage on them and what are your performance requirements so the CPU and sort of memory and bandwidth are things that you",
    "start": "2003000",
    "end": "2010120"
  },
  {
    "text": "would have to think about from that standpoint how you're ingesting your data and what the workflow looks like is",
    "start": "2010120",
    "end": "2016120"
  },
  {
    "text": "also important um so do you have Flume do you have scoop are you bringing it in from a from a database into uh into",
    "start": "2016120",
    "end": "2023120"
  },
  {
    "text": "Hadoop in that way you need a long range cluster that's that's doing that ingest continuously so that's an important",
    "start": "2023120",
    "end": "2029519"
  },
  {
    "text": "consideration and what does your workflow look like after the data lands like does it get cleansed and is it then consumed goes going back to what the",
    "start": "2029519",
    "end": "2036279"
  },
  {
    "text": "workload type is at that point in time so those are the things that you want to think through as you're planning your uh",
    "start": "2036279",
    "end": "2041519"
  },
  {
    "text": "planning your cluster to to make your life easy uh we have a",
    "start": "2041519",
    "end": "2048358"
  },
  {
    "text": "product called cloud data director which uh just Provisions it's it's really a provisioning tool at the moment and",
    "start": "2048359",
    "end": "2054440"
  },
  {
    "text": "it'll it automates the entire provisioning process so you don't have to worry about setting up uh the",
    "start": "2054440",
    "end": "2059720"
  },
  {
    "text": "different teans you don't have to worry about setting the the Mi and things like that uh all that you do is col the",
    "start": "2059720",
    "end": "2065720"
  },
  {
    "text": "instance types that you want the size of the cluster it'll give you a cluster back uh and this was released about a month ago you can consume that using the",
    "start": "2065720",
    "end": "2073440"
  },
  {
    "text": "AWS quick start and uh it'll set up your entire VPC using cloud formation it set up Cloud director and have a running",
    "start": "2073440",
    "end": "2080560"
  },
  {
    "text": "cluster for you at the end of uh that cycle there's a reference architecture",
    "start": "2080560",
    "end": "2086200"
  },
  {
    "text": "that we just refreshed that talks about all these considerations that talks about uh subnets in more detail it talks",
    "start": "2086200",
    "end": "2091800"
  },
  {
    "text": "about the different uh the different instance types that you can pick for different workloads uh and then there's",
    "start": "2091800",
    "end": "2097200"
  },
  {
    "text": "a that's available on our website uh and then there's a best practices blog as well which talks about again a lot of",
    "start": "2097200",
    "end": "2103720"
  },
  {
    "text": "these considerations that we just talked about and goes into some detail so those are the things that are available uh to",
    "start": "2103720",
    "end": "2109640"
  },
  {
    "text": "make your life easy that's what I",
    "start": "2109640",
    "end": "2115240"
  },
  {
    "text": "got thank you",
    "start": "2115240",
    "end": "2119440"
  }
]