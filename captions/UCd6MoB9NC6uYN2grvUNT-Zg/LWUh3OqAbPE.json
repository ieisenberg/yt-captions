[
  {
    "text": "sorry I didn't introduce myself my name is tama Delta I'm a machine learning scientist at Amazon AI I'm involved in",
    "start": "0",
    "end": "7410"
  },
  {
    "text": "the max net team and I mix net committed to the MX net project and today I'm",
    "start": "7410",
    "end": "14160"
  },
  {
    "text": "going to tell you a bit more about how you can speed up deployment training and inferencing and that the the entry ID",
    "start": "14160",
    "end": "20460"
  },
  {
    "text": "for around that is because a lot of time the speed of training and the speed of",
    "start": "20460",
    "end": "27119"
  },
  {
    "text": "inferencing is directly related to costs infrastructure cost and like maintenance",
    "start": "27119",
    "end": "33000"
  },
  {
    "text": "costs so also opportunity cost because one thing that's important is the more",
    "start": "33000",
    "end": "38250"
  },
  {
    "text": "time you spend iterating and training the best time you can experiment and the more you can experiment the more you can",
    "start": "38250",
    "end": "44579"
  },
  {
    "text": "get more complex models and you can get ahead of the competition so the first",
    "start": "44579",
    "end": "51539"
  },
  {
    "text": "part of how I'm gonna show you how you can tackle this is knowing your model and optimizing your code with MX net",
    "start": "51539",
    "end": "57120"
  },
  {
    "text": "blue one so you're going to touch on a few thing that service was saying in the previous talk how you can use all these",
    "start": "57120",
    "end": "62820"
  },
  {
    "text": "tools in order to maximize efficiency and I'm gonna give you some practical tips 5 set of practical tips and each of",
    "start": "62820",
    "end": "70619"
  },
  {
    "text": "them is a one-liner that can move your training speed from one x reference of",
    "start": "70619",
    "end": "75900"
  },
  {
    "text": "one X - 5 X the second one will be we see how you can use sage maker you probably heard a lot of sage maker today",
    "start": "75900",
    "end": "82430"
  },
  {
    "text": "switch Mecca is in an inherently distributed and supports now with an",
    "start": "82430",
    "end": "87689"
  },
  {
    "text": "extent dynamic training so we'll see what that is and we'll see how you can get faster prediction with elastic",
    "start": "87689",
    "end": "93299"
  },
  {
    "text": "inference Sigma Canio and the de planning compiler and idea blaze inferential form which can be released",
    "start": "93299",
    "end": "100020"
  },
  {
    "text": "next year so common problem is a deep learning deep learning model training is",
    "start": "100020",
    "end": "106320"
  },
  {
    "text": "slow well strangely planning model before a lot of you so I hope you can",
    "start": "106320",
    "end": "112619"
  },
  {
    "text": "relate to that you start training sometimes it's like a few hours sometimes it takes a few days on a if",
    "start": "112619",
    "end": "118110"
  },
  {
    "text": "you're not happy with the final result then you start again so common solution throw more hardware it so you start with",
    "start": "118110",
    "end": "125310"
  },
  {
    "text": "a GPU it's already quite happy that's P 3 let's say P 3 2 X Lodge already start",
    "start": "125310",
    "end": "132780"
  },
  {
    "text": "to cost like $4 our so it's an origin investment and then you want to make it faster so what",
    "start": "132780",
    "end": "137910"
  },
  {
    "text": "you do you move to a p38 x-large so",
    "start": "137910",
    "end": "144750"
  },
  {
    "text": "that's almost four times more expensive but you have four GPU so you see your performance increasing and then more you",
    "start": "144750",
    "end": "150930"
  },
  {
    "text": "had four more that's 16x large and then you're like okay it's still not fast enough",
    "start": "150930",
    "end": "155970"
  },
  {
    "text": "what do I do multi horse training so mil chioce multi-gpu training you start",
    "start": "155970",
    "end": "161070"
  },
  {
    "text": "adding more and more costs and they will start getting higher and higher and then you need to start asking your boss can I",
    "start": "161070",
    "end": "168210"
  },
  {
    "text": "actually really do that well you see like we work with very large gustan my AWS and what we see them is it's throw",
    "start": "168210",
    "end": "174360"
  },
  {
    "text": "even more and more hardware but another last one I promise otherwise you know",
    "start": "174360",
    "end": "179760"
  },
  {
    "text": "that's too much work so you can see it start to be very very expensive and you start thinking in a smarter way to",
    "start": "179760",
    "end": "187980"
  },
  {
    "text": "approach the problem so the the smartest solution is to start by optimizing your code making sure that you use fully this",
    "start": "187980",
    "end": "195480"
  },
  {
    "text": "hardware that you're buying from us or renting from us so let's Nick pick to",
    "start": "195480",
    "end": "200820"
  },
  {
    "text": "the last part of the talk which is that's a GP utilization graph and we see",
    "start": "200820",
    "end": "205950"
  },
  {
    "text": "the more and more as we add performance tricks you can see judaization getting",
    "start": "205950",
    "end": "214230"
  },
  {
    "text": "better and better and the time to Train being smaller and smaller so there are a",
    "start": "214230",
    "end": "220860"
  },
  {
    "text": "lot and lot of depending frameworks why we focus on a max net and why AWS peak",
    "start": "220860",
    "end": "226350"
  },
  {
    "text": "time extent is because of how well it scales and how fast you can train",
    "start": "226350",
    "end": "231709"
  },
  {
    "text": "service was presenting gluon how its superior to symbolic because it's as",
    "start": "231709",
    "end": "237930"
  },
  {
    "text": "easy to put at a very very fast we can iterate from your ID from a model that you've seen in a paper to actually",
    "start": "237930",
    "end": "243720"
  },
  {
    "text": "having something training it's debatable when I have more time usually when I do",
    "start": "243720",
    "end": "249989"
  },
  {
    "text": "like this more like practical workshop actually show how you can fire up Python and actually put breakpoints and do the",
    "start": "249989",
    "end": "257130"
  },
  {
    "text": "thing that Cyrus was describing earlier it's habitable so that the idea is you can go from a dynamic graph that you",
    "start": "257130",
    "end": "263850"
  },
  {
    "text": "defined through python to a static symbolic graph in one line and then this graph is going",
    "start": "263850",
    "end": "269260"
  },
  {
    "text": "to be executed and optimized with DMX netback and engine and that's what you're going to train so with one line",
    "start": "269260",
    "end": "275080"
  },
  {
    "text": "you can then you lose it the big ability because this graph is optimized you might have some operator which are fused",
    "start": "275080",
    "end": "281290"
  },
  {
    "text": "but then you get the performance but hopefully you've done your debugging before and also it's fast and scalable",
    "start": "281290",
    "end": "288730"
  },
  {
    "text": "so even if gluon is like super nice to work with you can still do multigp",
    "start": "288730",
    "end": "294370"
  },
  {
    "text": "humility horse training with it I need just a few line so Dibakar bow I just",
    "start": "294370",
    "end": "301030"
  },
  {
    "text": "wanted to show you I'm not actually going to do the live demo but that's if you're recording I spy charm I felt like",
    "start": "301030",
    "end": "306820"
  },
  {
    "text": "a conditional breakpoint in my training loop and here I'm just looking at my convolutional canals from my first layer",
    "start": "306820",
    "end": "314250"
  },
  {
    "text": "so you can really see that my network here has learned to detect patterns",
    "start": "314250",
    "end": "319890"
  },
  {
    "text": "because that each of them each of them is like seven by seven representation of",
    "start": "319890",
    "end": "326020"
  },
  {
    "text": "the weights of my first layer convolution convolutional kernels so how",
    "start": "326020",
    "end": "333010"
  },
  {
    "text": "do you go at visualizing your your training the first thing you need to do is to make sure you visualize your",
    "start": "333010",
    "end": "339130"
  },
  {
    "text": "network and you make sure that you know that the architecture you define is the",
    "start": "339130",
    "end": "344320"
  },
  {
    "text": "one you actually want it so with an extent you can use you have built-in integration with Grevy's and also with",
    "start": "344320",
    "end": "350590"
  },
  {
    "text": "MX bob and also you can get a summary where what's really important we are",
    "start": "350590",
    "end": "356169"
  },
  {
    "text": "talking about debugging shapes you can see the shapes at every step of the way and gluon has the deferred",
    "start": "356169",
    "end": "362020"
  },
  {
    "text": "initialization which means you don't need to say what's your input at each layer you just say what the output at",
    "start": "362020",
    "end": "367419"
  },
  {
    "text": "each layer then you pass data through it and based on your input shape is going to determine we're going to find out",
    "start": "367419",
    "end": "374440"
  },
  {
    "text": "what are the shapes of all your weights and of all your future maps at every layer so it's very very useful you can",
    "start": "374440",
    "end": "379750"
  },
  {
    "text": "use the same Network definition across many many different size of a inputs",
    "start": "379750",
    "end": "385750"
  },
  {
    "text": "because the weights are only going to be initialized after the first time you pass the data in and also what I really",
    "start": "385750",
    "end": "392470"
  },
  {
    "text": "like to do is when I create a new network is look at the actual values of the number of parameters because",
    "start": "392470",
    "end": "398919"
  },
  {
    "text": "sometimes you don't really know it's hard to tell you're going to like to dense layers with like 4,000 units in each and you don't realize that",
    "start": "398919",
    "end": "405729"
  },
  {
    "text": "you just created 16 million parameters okay so that's that the kind of thing that you need to pay attention because",
    "start": "405729",
    "end": "411970"
  },
  {
    "text": "if you create maybe a to complex network for the size of your data set you know your training is not going to convert",
    "start": "411970",
    "end": "417550"
  },
  {
    "text": "well or is going to be prone to overfitting so visiting your network",
    "start": "417550",
    "end": "423729"
  },
  {
    "text": "making sure that you actually build what you meant to build is a first step because that can be have a massive",
    "start": "423729",
    "end": "429610"
  },
  {
    "text": "impact on your performance if you have like too many ways your training is going to be super slow the next one is",
    "start": "429610",
    "end": "436240"
  },
  {
    "text": "sorry actually money saw your hardware and I would say that maybe the most critical one you want to make sure that",
    "start": "436240",
    "end": "444220"
  },
  {
    "text": "you're utilizing your GPU at 100% so what for that I use something called GPU",
    "start": "444220",
    "end": "449800"
  },
  {
    "text": "and the skull monitor you can find it on github you just Google GPRS community github it ties in Nvidia drivers and in",
    "start": "449800",
    "end": "456729"
  },
  {
    "text": "Flex DB and graph owner to give you this kind of graph out of the box so I always kind of front that you can use Nvidia",
    "start": "456729",
    "end": "462880"
  },
  {
    "text": "SMI I guess most of you are using that but this really gives you this time based graph that are really critical to",
    "start": "462880",
    "end": "471030"
  },
  {
    "text": "understand the patterns in your utilization and see where you can do efforts in order to maximize to maximize",
    "start": "471030",
    "end": "477970"
  },
  {
    "text": "it and CP is also very important so you want to make sure that you're using your course for example to do asynchronous",
    "start": "477970",
    "end": "483760"
  },
  {
    "text": "pre-processing or you don't want also to have them all maxed out because then you know that you have an issue and you need",
    "start": "483760",
    "end": "489909"
  },
  {
    "text": "to maybe do some offline processing before your training so that your CPU doesn't become your bottleneck what you",
    "start": "489909",
    "end": "496570"
  },
  {
    "text": "want to avoid is starvation of your GPU these GPUs they are like beasts like the Nvidia 100 120 thousand billion",
    "start": "496570",
    "end": "504550"
  },
  {
    "text": "operation per second that's as much as a fastest supercomputer from 2004 so you",
    "start": "504550",
    "end": "511840"
  },
  {
    "text": "want to make sure it requires actually skill to build a data pipeline to feed this GPU to not have them waiting for",
    "start": "511840",
    "end": "518289"
  },
  {
    "text": "the data and we'll see how is glue on is just one line in order to do that yes",
    "start": "518289",
    "end": "527490"
  },
  {
    "text": "yep",
    "start": "527490",
    "end": "530490"
  },
  {
    "text": "at ya so I'll I'll go back to this and how you",
    "start": "535130",
    "end": "543810"
  },
  {
    "text": "can erase dis patterns and for example here because you know like to do one forward pass on your network you're not",
    "start": "543810",
    "end": "550740"
  },
  {
    "text": "going to be able to sit at that scale like one forward pass might be a few tens of milliseconds here this is",
    "start": "550740",
    "end": "557940"
  },
  {
    "text": "especially going to tell you like if it's slow then is going to be average so what you want to do is this kind of see",
    "start": "557940",
    "end": "566940"
  },
  {
    "text": "so patterns you see here I mean a spoiler for the next up is like IO",
    "start": "566940",
    "end": "572180"
  },
  {
    "text": "issues right and then if you not if you have bottleneck inside your network",
    "start": "572180",
    "end": "578280"
  },
  {
    "text": "you're gonna see that the maximum utilization average so you have different patterns for different problems and for example here if you",
    "start": "578280",
    "end": "586410"
  },
  {
    "text": "want to really know where your competition is happening inside a single",
    "start": "586410",
    "end": "591810"
  },
  {
    "text": "pass you can use a profiler so mixing it come with a profiler or you can just set start and stop and then you get like a",
    "start": "591810",
    "end": "599100"
  },
  {
    "text": "profiling information like you would with any other programs and you can see you know like my convolution took that",
    "start": "599100",
    "end": "605010"
  },
  {
    "text": "many millisecond by density or that many millisecond that much memory has been allocated which thread I've allocated",
    "start": "605010",
    "end": "610740"
  },
  {
    "text": "what and you can visualize all that in from using that from profiling tools so it's really cool I'm not covering that",
    "start": "610740",
    "end": "616680"
  },
  {
    "text": "in this talk where we can chat about it afterwards so the system performance is",
    "start": "616680",
    "end": "622110"
  },
  {
    "text": "one thing and then I'm gonna tell you how you can really tricks to get better throughput but you need to not forget",
    "start": "622110",
    "end": "628170"
  },
  {
    "text": "that throughput is useless if you don't get the accuracy that you want you know",
    "start": "628170",
    "end": "633540"
  },
  {
    "text": "if you if you throw more thousands of images your own network that's a nice metric to look at but the most important",
    "start": "633540",
    "end": "639390"
  },
  {
    "text": "thing is how long it takes to reach a certain accuracy so for that MX bot is",
    "start": "639390",
    "end": "645000"
  },
  {
    "text": "great so MX board is a plug-in from MX net where you can write to the tensor",
    "start": "645000",
    "end": "650640"
  },
  {
    "text": "bot log format and then you can visualize all your training directly intensive bold and obviously you have",
    "start": "650640",
    "end": "657120"
  },
  {
    "text": "like all these images and graph and scaler and text thing you can do so the",
    "start": "657120",
    "end": "662220"
  },
  {
    "text": "actual performance tricks for example someone has an ID of the problem that",
    "start": "662220",
    "end": "668040"
  },
  {
    "text": "you see so that's the same training five times six times",
    "start": "668040",
    "end": "673880"
  },
  {
    "text": "who can tell me what the issue here on the first training who as I guess",
    "start": "673880",
    "end": "680510"
  },
  {
    "text": "exactly yes the data your pipeline what happens here is one batch of data goes",
    "start": "681440",
    "end": "686550"
  },
  {
    "text": "through the network and then the GPU is like okay give me the next batch and then because all the processing is",
    "start": "686550",
    "end": "691920"
  },
  {
    "text": "happening on the same thread then the CPU start ok I'm going to resize the next batch prepare it it's ready then",
    "start": "691920",
    "end": "698430"
  },
  {
    "text": "send it to the GPU so that's extremely inefficient so to solve that what you can do is you can do asynchronous",
    "start": "698430",
    "end": "705450"
  },
  {
    "text": "processing of your data using multiple workers in the background that magic and",
    "start": "705450",
    "end": "711600"
  },
  {
    "text": "I do is it going to prepare your batches of data put them in shared memory and the next time your GPU is ready to get",
    "start": "711600",
    "end": "717510"
  },
  {
    "text": "the data the data is copied across straightaway to the GPU sorry",
    "start": "717510",
    "end": "726079"
  },
  {
    "text": "they should not all do that the thing",
    "start": "726500",
    "end": "731910"
  },
  {
    "text": "hub realization is is what we talked",
    "start": "731910",
    "end": "737339"
  },
  {
    "text": "about which is when you actually convert this dynamic graph to symbolic and it's executed on the mixing back-end so here",
    "start": "737339",
    "end": "742800"
  },
  {
    "text": "you see we go from about 90% deep utilization which is already extremely good for deep learning frameworks to",
    "start": "742800",
    "end": "748740"
  },
  {
    "text": "maybe like 94 then the next thing we can do is you can increase your batch size",
    "start": "748740",
    "end": "755400"
  },
  {
    "text": "because sometimes if use like a too small batch size your GPU is not going to be able to use all these thousands of",
    "start": "755400",
    "end": "761880"
  },
  {
    "text": "parallel calls in order to process the data in parallel so to make sure that",
    "start": "761880",
    "end": "767100"
  },
  {
    "text": "you use a beginning of batch size so that you maximize the parallelization",
    "start": "767100",
    "end": "772220"
  },
  {
    "text": "capabilities of your GPU and see we get another morning improvement and we",
    "start": "772220",
    "end": "778350"
  },
  {
    "text": "almost already at a hundred percent GDP utilization which is extremely good then",
    "start": "778350",
    "end": "783690"
  },
  {
    "text": "the next thing we can do or I won't go into details about that because usually they require a bit of knowledge of gluon",
    "start": "783690",
    "end": "789540"
  },
  {
    "text": "but remember how I said next time the GPU is ready to get the data you just copy across the data to GPU because it's",
    "start": "789540",
    "end": "796620"
  },
  {
    "text": "already prepared what you can do how about you just copy the data even before the GPU needs it so you copy",
    "start": "796620",
    "end": "803970"
  },
  {
    "text": "that the data is already on the GPU and when it's finished one batch forward pass backward pass",
    "start": "803970",
    "end": "811110"
  },
  {
    "text": "update of the way to the next training batch is already on the GPU and it's two lines of code in order to do that in",
    "start": "811110",
    "end": "817110"
  },
  {
    "text": "blue one so yeah like this complex tricks that you you can implement",
    "start": "817110",
    "end": "822329"
  },
  {
    "text": "extremely easily and something that might be extremely hard to do in terms of law require some deep engineering",
    "start": "822329",
    "end": "828510"
  },
  {
    "text": "knowledge in order to use like milchie process pools and do your own asynchronous data loading with glue on",
    "start": "828510",
    "end": "834420"
  },
  {
    "text": "way we build the API so that is very easy for you to do that and the next improvement is mixed precision training",
    "start": "834420",
    "end": "840440"
  },
  {
    "text": "so instead of using float32 use float16 and the latest nvidia gpus the volta 100",
    "start": "840440",
    "end": "847620"
  },
  {
    "text": "they have specific tensile calls to do extremely efficient mixed operation so",
    "start": "847620",
    "end": "854279"
  },
  {
    "text": "you you get like 2x improvement in speed and also because you use less memory you",
    "start": "854279",
    "end": "861209"
  },
  {
    "text": "can increase again your batch size and train even faster so let's go through",
    "start": "861209",
    "end": "867120"
  },
  {
    "text": "this checklist so different ideas are I covered you use the optimized framework",
    "start": "867120",
    "end": "872760"
  },
  {
    "text": "so here also it echoes a little bit to what we said this morning about ADA brace providing optimized tensorflow",
    "start": "872760",
    "end": "879480"
  },
  {
    "text": "binaries well that's true but especially for MX nets so I maxed net we build the MX nets release that we do on pip they",
    "start": "879480",
    "end": "887579"
  },
  {
    "text": "optimized for AWS and AWS ec2 instances for the ability processing is just one",
    "start": "887579",
    "end": "895740"
  },
  {
    "text": "argument in your data loader in gluon you just say I want that many workers so if people are familiar with PI Tosh same",
    "start": "895740",
    "end": "901500"
  },
  {
    "text": "principle except we actually optimize the data loader architecture compared to Python so with the latest release of MX",
    "start": "901500",
    "end": "909600"
  },
  {
    "text": "net you'll see that the data loader are not going to go to sleep between epochs you actually it's like you iterating",
    "start": "909600",
    "end": "915180"
  },
  {
    "text": "forever and little dips that you see sometimes between epochs are going to",
    "start": "915180",
    "end": "920880"
  },
  {
    "text": "disappear the habit ization that we talked about which is like this great",
    "start": "920880",
    "end": "926160"
  },
  {
    "text": "feature that goes from like imperative and dynamics graph to symbolic one is just one line you just say Hubbard ice",
    "start": "926160",
    "end": "933000"
  },
  {
    "text": "to increase the batch size just one argument of your data loader you just say how as much as you want and then is",
    "start": "933000",
    "end": "938790"
  },
  {
    "text": "going to prepare these batches asynchronously on multiple workers to avoid blocking a",
    "start": "938790",
    "end": "944859"
  },
  {
    "text": "oh well I won't go into details but one thing you need to avoid is to have synchronous calls in your training to",
    "start": "944859",
    "end": "951609"
  },
  {
    "text": "make sure that as much as possible is done in DM x90 asynchronous thread a",
    "start": "951609",
    "end": "957819"
  },
  {
    "text": "threaded engine and for mixed precision you might think it's it's a complex",
    "start": "957819",
    "end": "962829"
  },
  {
    "text": "thing to do well you actually you just cast your network to float16 and that's",
    "start": "962829",
    "end": "968470"
  },
  {
    "text": "it and then suddenly you start training on float16 so again to reiterate what I",
    "start": "968470",
    "end": "975729"
  },
  {
    "text": "said earlier which is performance it's not only throughput it's time to accuracy so what's what you can do on",
    "start": "975729",
    "end": "983439"
  },
  {
    "text": "top of actually optimizing your code is to do I per parameter tuning which is a",
    "start": "983439",
    "end": "989199"
  },
  {
    "text": "seismic automatic model tuning because maybe you your choice of optimizer your",
    "start": "989199",
    "end": "994209"
  },
  {
    "text": "choice of learning rate schedule your choice of number of layers that's going",
    "start": "994209",
    "end": "999639"
  },
  {
    "text": "to impact greatly how fast your model is training so once you get I would say 80% of the way where you get a good model",
    "start": "999639",
    "end": "1007649"
  },
  {
    "text": "that does about what you want and that you have maximize your GP utilization next step is to actually go undo this",
    "start": "1007649",
    "end": "1014669"
  },
  {
    "text": "automatic model tuning just to give you an example when we was a few months ago",
    "start": "1014669",
    "end": "1019979"
  },
  {
    "text": "we had an intern who release the fastest training of Seifer ten on a single GPU",
    "start": "1019979",
    "end": "1024990"
  },
  {
    "text": "and this trick was not like a book like groundbreaking research you just instrumented the training code of size",
    "start": "1024990",
    "end": "1031558"
  },
  {
    "text": "of 10 with resonate 50 to have like a bunch of hyper parameters like the different learning rate schedule and so",
    "start": "1031559",
    "end": "1037350"
  },
  {
    "text": "on and then he managed to Train type-a 10 in 3 minutes and the latest the previous best training time was 14",
    "start": "1037350",
    "end": "1044490"
  },
  {
    "text": "minutes that's only through using Sigma's automatic model tuning tuning that",
    "start": "1044490",
    "end": "1050399"
  },
  {
    "text": "managed to find this exact combination of our hyper parameters that gave the best performance so to summarize this",
    "start": "1050399",
    "end": "1058620"
  },
  {
    "text": "first part so the recommended workflow used blue one to debug and iterate quickly we've seen you can use adipate",
    "start": "1058620",
    "end": "1063659"
  },
  {
    "text": "on the burger you can visualize everything you want and anything you want visualize the performance of your",
    "start": "1063659",
    "end": "1069869"
  },
  {
    "text": "hardware make sure that your reach is 90 + GP utilization",
    "start": "1069869",
    "end": "1074880"
  },
  {
    "text": "and then hybridize on optimize for bid for speed finally last step I per parameter tuning with SH Macomb",
    "start": "1074880",
    "end": "1082530"
  },
  {
    "text": "automatic model tuning find this best combination of parameters are we going to give you the best performance for",
    "start": "1082530",
    "end": "1088290"
  },
  {
    "text": "your training because in I mean most of you I've heard before you're like engineers in a or even data scientist",
    "start": "1088290",
    "end": "1095490"
  },
  {
    "text": "but usually your mother you don't train it once you're gonna treat many many many times so you want to make sure that",
    "start": "1095490",
    "end": "1101460"
  },
  {
    "text": "your training script is top-notch before you put it in this like CI CD pipeline where you get every week on your model",
    "start": "1101460",
    "end": "1108030"
  },
  {
    "text": "right so it's really worth taking the time to optimize your model at that point in time so the next part I'm going",
    "start": "1108030",
    "end": "1116940"
  },
  {
    "text": "to tell you how you can scale your training research maker and says make your dynamic training because yeah once",
    "start": "1116940",
    "end": "1123180"
  },
  {
    "text": "you reach this hundred percent utilization and that's when you actually need to start adding more and more GPUs",
    "start": "1123180",
    "end": "1128880"
  },
  {
    "text": "and more and more hosts to your training script so sage maker just to summarize",
    "start": "1128880",
    "end": "1135560"
  },
  {
    "text": "for people who are not familiar with it it's an in twin machine learning platform yes zero setup purely literally",
    "start": "1135560",
    "end": "1142080"
  },
  {
    "text": "just go you can actually create an instant from the UI and you can train any framework you want so okay you might",
    "start": "1142080",
    "end": "1149430"
  },
  {
    "text": "have realized a big advocate of MX net but if you want to use tensorflow I'm not going to be mad at you you pay by",
    "start": "1149430",
    "end": "1157830"
  },
  {
    "text": "the second so when you use him as a shoemaker well one of the thing that really strike me the first time I used",
    "start": "1157830",
    "end": "1163050"
  },
  {
    "text": "it was I run one of the I think there is hundred examples Python notebooks you",
    "start": "1163050",
    "end": "1168330"
  },
  {
    "text": "can just like click and use straight away from sage maker what really struck me was they told me I was built for",
    "start": "1168330",
    "end": "1174810"
  },
  {
    "text": "nineteen second of training so I ran the script and it says ok nineteen second of",
    "start": "1174810",
    "end": "1181320"
  },
  {
    "text": "is billable so you instead of having you know this big instant that you use as your workhorse",
    "start": "1181320",
    "end": "1187520"
  },
  {
    "text": "it's fine if your AWS like me because I don't pay my AWS bill it's just my boss",
    "start": "1187520",
    "end": "1193140"
  },
  {
    "text": "pays it well if you know if you have to account for the resource you use you",
    "start": "1193140",
    "end": "1198300"
  },
  {
    "text": "don't want to be using the big machines and just of them like sitting around when you go for lunch and so on so",
    "start": "1198300",
    "end": "1203490"
  },
  {
    "text": "search maker lets you have like a very tiny machine to drive your execution",
    "start": "1203490",
    "end": "1208530"
  },
  {
    "text": "runs the actual training scripts on big clusters of p3 instances and whatnot and",
    "start": "1208530",
    "end": "1214020"
  },
  {
    "text": "you're just built for what you use so the distributed training of the",
    "start": "1214020",
    "end": "1221280"
  },
  {
    "text": "aspect of stage maker you you basically you pick either CPU or GPU machines you",
    "start": "1221280",
    "end": "1227610"
  },
  {
    "text": "say how many instances you want you have a training script tensorflow MX nets",
    "start": "1227610",
    "end": "1233180"
  },
  {
    "text": "scikit-learn we you can also bring your own docker container if you want if we",
    "start": "1233180",
    "end": "1238320"
  },
  {
    "text": "don't have the estimators that you require and then it's gonna start this training job on a cluster and when the",
    "start": "1238320",
    "end": "1245940"
  },
  {
    "text": "training is finished your model is going to be saved to s3 and then the cluster",
    "start": "1245940",
    "end": "1252690"
  },
  {
    "text": "is going to go down and if you want to deploy this train model it's just one more comment and then you get another",
    "start": "1252690",
    "end": "1258270"
  },
  {
    "text": "cluster based on this yes which is going to serve your inference requests so",
    "start": "1258270",
    "end": "1264360"
  },
  {
    "text": "fairly powerful with dynamic training and it makes that we went one step",
    "start": "1264360",
    "end": "1269640"
  },
  {
    "text": "further we said for example one of the problem is so many people have this",
    "start": "1269640",
    "end": "1274920"
  },
  {
    "text": "reserved pools of p3 instances that sometimes in big cooperation like some",
    "start": "1274920",
    "end": "1280500"
  },
  {
    "text": "people they're gonna use one they're going to do their data center is going to use one and although it needs to but",
    "start": "1280500",
    "end": "1286530"
  },
  {
    "text": "then you might have a bunch of piece for instances that are sitting around it'll with dynamic training you can add",
    "start": "1286530",
    "end": "1292260"
  },
  {
    "text": "instances to your training during training so you don't say at the beginning I need six instance I need ten",
    "start": "1292260",
    "end": "1298860"
  },
  {
    "text": "instance and it's 20 or 256 during training if you have available and",
    "start": "1298860",
    "end": "1304340"
  },
  {
    "text": "preempted piece for instances for example they just get added to your cluster because otherwise they would be",
    "start": "1304340",
    "end": "1312030"
  },
  {
    "text": "useless sitting there IDO and if someone needs one is it get removed so you can between epochs add and remove instances",
    "start": "1312030",
    "end": "1320550"
  },
  {
    "text": "to your training and you can reduce cost to train by 15 to 50 percent and time to",
    "start": "1320550",
    "end": "1326280"
  },
  {
    "text": "Train by 15 to 30 percent so that's just the beginning we literally just really",
    "start": "1326280",
    "end": "1331410"
  },
  {
    "text": "that few weeks ago and it's only going to get better and very soon we're gonna integrate with spot instances as well so",
    "start": "1331410",
    "end": "1337320"
  },
  {
    "text": "you know if it makes sense for you the below a certain threshold and you can adjust both instance is your training and then they get",
    "start": "1337320",
    "end": "1343270"
  },
  {
    "text": "removed if it becomes too expensive so that's what training what about inference service said",
    "start": "1343270",
    "end": "1350020"
  },
  {
    "text": "earlier that most of the money for it nonprofit chick-fil-a example most of",
    "start": "1350020",
    "end": "1356380"
  },
  {
    "text": "the money is spent at inference time when you have like 2000 stores and you want all these like cameras to do like",
    "start": "1356380",
    "end": "1362530"
  },
  {
    "text": "inferencing that's that's where the big the big beads are going to come so we're",
    "start": "1362530",
    "end": "1367630"
  },
  {
    "text": "going to see three different thing we're going to see elastic inference which make your neo for compiling these models",
    "start": "1367630",
    "end": "1373960"
  },
  {
    "text": "and AWS inferential so I just mentioned",
    "start": "1373960",
    "end": "1379870"
  },
  {
    "text": "the deployment so how deployment works is you can actually specify which models you want",
    "start": "1379870",
    "end": "1386800"
  },
  {
    "text": "to deploy the models you have training sage makers they manage for you they're not just file sitting around they actually you can reference them in",
    "start": "1386800",
    "end": "1393700"
  },
  {
    "text": "sage maker and you can say I want to deploy this model and this model and this model and you can spit your traffic",
    "start": "1393700",
    "end": "1399280"
  },
  {
    "text": "across each endpoint and you can do your a B testing like that and you pick which instance you want but most of the times",
    "start": "1399280",
    "end": "1409000"
  },
  {
    "text": "you have this problem with deep learning which is training is fairly slow but you",
    "start": "1409000",
    "end": "1416770"
  },
  {
    "text": "know you had more more and more machine than it gets manageable then inference it's fairly fast on CPU but sometime",
    "start": "1416770",
    "end": "1423490"
  },
  {
    "text": "it's not fast enough for example for a new york-style transfer you do that on CPU is going to be 10 second per image",
    "start": "1423490",
    "end": "1430810"
  },
  {
    "text": "and if you up has like some nice filters that are based on this neural sec transfer that you're doing in the cloud",
    "start": "1430810",
    "end": "1436900"
  },
  {
    "text": "or you cannot wait 10 second so it means you need to deploy on GPU but deploying",
    "start": "1436900",
    "end": "1442840"
  },
  {
    "text": "inference model on GPU that's very expensive because you need if you have many many models you need that many",
    "start": "1442840",
    "end": "1448720"
  },
  {
    "text": "instances this monolithic space in memory very quickly it becomes",
    "start": "1448720",
    "end": "1454240"
  },
  {
    "text": "unmanageable so our customers told us they spend ninety percent of the costs",
    "start": "1454240",
    "end": "1460810"
  },
  {
    "text": "on inference in production and you see usually very low utilization of these",
    "start": "1460810",
    "end": "1467290"
  },
  {
    "text": "history instances we see between ten to thirty percent utilization and it just",
    "start": "1467290",
    "end": "1473860"
  },
  {
    "text": "is just not practical so a lot of people they have to to sacrifice speeds",
    "start": "1473860",
    "end": "1479500"
  },
  {
    "text": "for costs so it's a trade-off that's become a lot more critical in a deepening application and other",
    "start": "1479500",
    "end": "1484960"
  },
  {
    "text": "applications especially that's when you do inference most of the time you do",
    "start": "1484960",
    "end": "1490390"
  },
  {
    "text": "inference at the batch size of one write it because your customer usually sends one request at a time and except if you",
    "start": "1490390",
    "end": "1497590"
  },
  {
    "text": "do like some small dynamic batching most of the time you're going to run a forward pass through your network with a",
    "start": "1497590",
    "end": "1503140"
  },
  {
    "text": "single element which is not very efficient because that's not what GPU",
    "start": "1503140",
    "end": "1508179"
  },
  {
    "text": "has been been made for that's not why GPU are so used in deep learning it's not because they're fast is because it",
    "start": "1508179",
    "end": "1513820"
  },
  {
    "text": "can paralyze across many many matrix operation so what what elastic inference",
    "start": "1513820",
    "end": "1521080"
  },
  {
    "text": "does it basically lets you kick slices of GPU because most of the times you you want a",
    "start": "1521080",
    "end": "1527860"
  },
  {
    "text": "GPU because it's fast because you want the latency you don't really care about the throughput what training time",
    "start": "1527860",
    "end": "1533230"
  },
  {
    "text": "sometimes you see your training at six thousand image per second and you're very happy that's one thing but maybe",
    "start": "1533230",
    "end": "1540280"
  },
  {
    "text": "you don't have six thousand customers sending you images every second but what you want is this very low latency so",
    "start": "1540280",
    "end": "1547570"
  },
  {
    "text": "unless you can translate you do that least you just say I want very very fast but I don't need that much capacity and",
    "start": "1547570",
    "end": "1554040"
  },
  {
    "text": "it's also scalable it goes between 1,000 1 teraflop to certitude teraflops it's integrated",
    "start": "1554040",
    "end": "1562630"
  },
  {
    "text": "with Amazon ec2 even outside of such maker or inside such maker as well you can specify when you create these",
    "start": "1562630",
    "end": "1568720"
  },
  {
    "text": "deployment clusters you can say ok I want let's say C 4 C 5 with a deep",
    "start": "1568720",
    "end": "1574780"
  },
  {
    "text": "learning accelerator a new support single and mix operation so this model that you train with float16 we've seen",
    "start": "1574780",
    "end": "1581290"
  },
  {
    "text": "how you got like twice improvement instead of training speed or you can get the same stuff at inference time so just",
    "start": "1581290",
    "end": "1591370"
  },
  {
    "text": "to give you a point of comparison here we were looking at the costs versus",
    "start": "1591370",
    "end": "1596700"
  },
  {
    "text": "speed so here you see like basically with the p2x large you're gonna pay 90",
    "start": "1596700",
    "end": "1605679"
  },
  {
    "text": "cents per hour for your inference if you do se5 plus a small accelerator you get",
    "start": "1605679",
    "end": "1615160"
  },
  {
    "text": "about the same latency in the same number of image per second for a quarter",
    "start": "1615160",
    "end": "1620860"
  },
  {
    "text": "of the price so for people who deploy a lot of model in production a lot of deep learning models that can be a very very",
    "start": "1620860",
    "end": "1627580"
  },
  {
    "text": "cost effective solution so that's for inference that's how you can get elastic",
    "start": "1627580",
    "end": "1633910"
  },
  {
    "text": "hardware to your models but even more you can actually optimize the speeds of",
    "start": "1633910",
    "end": "1639160"
  },
  {
    "text": "the of your model specifically for your target we mentioned TVM earlier which is",
    "start": "1639160",
    "end": "1645070"
  },
  {
    "text": "open so depending compiler we took that and we optimized it and we improved it and we made search maker neo so training",
    "start": "1645070",
    "end": "1653140"
  },
  {
    "text": "is really done on GPU but the models can be deployed anywhere we're talking about edge deployment earlier you can maybe",
    "start": "1653140",
    "end": "1659230"
  },
  {
    "text": "deploy sometimes in mobile apps you might deploy on in the cloud on GPU I",
    "start": "1659230",
    "end": "1665080"
  },
  {
    "text": "have like a variety of targets and if you remember the slide from this morning",
    "start": "1665080",
    "end": "1670240"
  },
  {
    "text": "at MX net we you know we pride ourselves integrating with all this acceleration library so for example if you want to",
    "start": "1670240",
    "end": "1676450"
  },
  {
    "text": "run ten sortie in MX net you can we're gonna speed the graph so that it's optimized fruit and salty for part of",
    "start": "1676450",
    "end": "1682870"
  },
  {
    "text": "the execution and another part is going to be run maybe on mkl DNN because it's going to be faster but you see this",
    "start": "1682870",
    "end": "1689710"
  },
  {
    "text": "complexity which means every framework needs to optimize for every target and",
    "start": "1689710",
    "end": "1694720"
  },
  {
    "text": "the integration start to become a bit Messier I've seen the code you know we start to be okay should we find a better",
    "start": "1694720",
    "end": "1701320"
  },
  {
    "text": "way of doing that so that's where TVM and neo comes into play so instead of",
    "start": "1701320",
    "end": "1706540"
  },
  {
    "text": "compiling instead of integrating every architecture into your framework what",
    "start": "1706540",
    "end": "1712300"
  },
  {
    "text": "you do is you have one common layer",
    "start": "1712300",
    "end": "1719080"
  },
  {
    "text": "that's what you can represent neo basically what it does it takes your model and converts it to 10 so operation",
    "start": "1719080",
    "end": "1725230"
  },
  {
    "text": "and then each platform as the best way of doing this tensor multiplication so",
    "start": "1725230",
    "end": "1731080"
  },
  {
    "text": "you're going to use CUDA on GPUs you're going to use em like a different type of",
    "start": "1731080",
    "end": "1737860"
  },
  {
    "text": "vectorized operation depending on how many cash you have on your cpu and that's",
    "start": "1737860",
    "end": "1743140"
  },
  {
    "text": "to be different from an Intel platform from a non flat platform and then you have all these accelerator chips that",
    "start": "1743140",
    "end": "1749800"
  },
  {
    "text": "have their own way of doing these vectorized matrix operations so it's a lot saner way to do where you just need",
    "start": "1749800",
    "end": "1756340"
  },
  {
    "text": "each platforms need to integrate with neo and each framework needs to",
    "start": "1756340",
    "end": "1761500"
  },
  {
    "text": "integrate with neo and then it's just like a one-to-one instead of a many to many so under the hood what does it work",
    "start": "1761500",
    "end": "1770590"
  },
  {
    "text": "you have many in deep learning probably a lot of you are familiar with deep learning how it is what it is really is",
    "start": "1770590",
    "end": "1778060"
  },
  {
    "text": "just a computational graph right like you have some input lots of operation you get an output so that's this graph",
    "start": "1778060",
    "end": "1784330"
  },
  {
    "text": "that you can optimize the first tensor flow MX net Onix they all get converted",
    "start": "1784330",
    "end": "1790180"
  },
  {
    "text": "into this unified graph representation and that's when you you start to be able",
    "start": "1790180",
    "end": "1795310"
  },
  {
    "text": "to say okay I'm going to combine these two operation together I'm going to",
    "start": "1795310",
    "end": "1801220"
  },
  {
    "text": "reuse this memory buffer because I don't need it in the next iteration step and so on so that's rough level optimization",
    "start": "1801220",
    "end": "1808360"
  },
  {
    "text": "you don't need to know actually much about what these operations are and then",
    "start": "1808360",
    "end": "1816040"
  },
  {
    "text": "the next step is a tense operation so depending on the size of your tensors",
    "start": "1816040",
    "end": "1822040"
  },
  {
    "text": "and depending on where they are in the graph you can optimize a memory layout so that is extremely sensitive when you",
    "start": "1822040",
    "end": "1827860"
  },
  {
    "text": "do this kind of optimization it's really you we I don't know if all of you picked",
    "start": "1827860",
    "end": "1835120"
  },
  {
    "text": "up on what Cyrus was saying on the channel first and channel last and we say you put two channel first and then",
    "start": "1835120",
    "end": "1841540"
  },
  {
    "text": "you know MX and T's twice faster than terms of law that's a lot to do is how",
    "start": "1841540",
    "end": "1846640"
  },
  {
    "text": "the GPU reads the memory when you when you do channel last basically in the RAM",
    "start": "1846640",
    "end": "1852610"
  },
  {
    "text": "your memory is laid out as all your arch all your pixel then all your G pixel and",
    "start": "1852610",
    "end": "1858910"
  },
  {
    "text": "all your B pixel because the memory is 2d it's like a bitmap but if you have",
    "start": "1858910",
    "end": "1864820"
  },
  {
    "text": "images that are a GB is three dimensions so you need some way to flatten it depending on how you flatten it your",
    "start": "1864820",
    "end": "1871810"
  },
  {
    "text": "cache access is going to be different and that's where you can get massive performance improvements",
    "start": "1871810",
    "end": "1876850"
  },
  {
    "text": "so neo look at that and optimize the tensors for speed and then for each",
    "start": "1876850",
    "end": "1883090"
  },
  {
    "text": "target architecture-specific you convert this tense operation to machine code",
    "start": "1883090",
    "end": "1888610"
  },
  {
    "text": "that is really made for fast execution so architecture-specific optimization",
    "start": "1888610",
    "end": "1896440"
  },
  {
    "text": "can be up to 12 times faster than the like stock frameworks and up to 100",
    "start": "1896440",
    "end": "1903460"
  },
  {
    "text": "times smaller memory footprint when you think about it MMX Nathan's of flow their libraries is",
    "start": "1903460",
    "end": "1909730"
  },
  {
    "text": "like so many operators you can do all sort of convolution transpose convolution you know pulling average",
    "start": "1909730",
    "end": "1916420"
  },
  {
    "text": "pooling batch norm lay your norm and whatnot you know you have so many and",
    "start": "1916420",
    "end": "1921910"
  },
  {
    "text": "every week someone comes up with a new operator that gets implemented into the libraries but your model that you just",
    "start": "1921910",
    "end": "1928180"
  },
  {
    "text": "trained you might just use like five or six different operators in it you don't",
    "start": "1928180",
    "end": "1933460"
  },
  {
    "text": "need to bring the entire MX net of the entire tensor flow with you on your deployment platform so what it does it",
    "start": "1933460",
    "end": "1940060"
  },
  {
    "text": "only compile because only compare to machine code you you just need a runtime that's able to run this 10 so operation",
    "start": "1940060",
    "end": "1947760"
  },
  {
    "text": "and it can be in total like hundred times smaller for example the runtime for new it's about two megabytes and",
    "start": "1947760",
    "end": "1955320"
  },
  {
    "text": "some version of tensorflow are more than 100 megabytes depending on what library you statically",
    "start": "1955320",
    "end": "1961780"
  },
  {
    "text": "linked with it so when I come to to the",
    "start": "1961780",
    "end": "1967210"
  },
  {
    "text": "last part which is been announced at reinvent see it's called AWS inferential",
    "start": "1967210",
    "end": "1972840"
  },
  {
    "text": "inferential is a custom chip is going to be built by Amazon it's up to 10 times",
    "start": "1972840",
    "end": "1980710"
  },
  {
    "text": "faster deep learning inference so we hundreds of terrible ops or thousand even of teraflops and it's going to be",
    "start": "1980710",
    "end": "1988270"
  },
  {
    "text": "available this year so stay tuned so instead of it's going to be available through the elastic inference and you'd",
    "start": "1988270",
    "end": "1995080"
  },
  {
    "text": "be able to attach to any of your ec2 instance this custom AI accelerator and",
    "start": "1995080",
    "end": "2000420"
  },
  {
    "text": "run your inference a lot faster so any questions",
    "start": "2000420",
    "end": "2007010"
  },
  {
    "text": "yes so between neo is a it's a service",
    "start": "2009800",
    "end": "2020790"
  },
  {
    "text": "inside Jamaica where you basically say sorry so neo takes you on your graph whether",
    "start": "2020790",
    "end": "2029820"
  },
  {
    "text": "it's in a tensor flow train model or IMAX net train model take the graph take",
    "start": "2029820",
    "end": "2036300"
  },
  {
    "text": "the weights and it's going to compile this this graph into optimized machine",
    "start": "2036300",
    "end": "2044730"
  },
  {
    "text": "code that's basically instead of having a library like tens of flour mix nets that says ok when you see this node you",
    "start": "2044730",
    "end": "2051060"
  },
  {
    "text": "do this piece of code you see this note you do this piece of code among all the possible operations it compiles all this",
    "start": "2051060",
    "end": "2057570"
  },
  {
    "text": "stuff to machine code for the specific architecture right so for you get CUDA",
    "start": "2057570",
    "end": "2063030"
  },
  {
    "text": "code if you target Nvidia you get you know machine instruction for arm if you",
    "start": "2063030",
    "end": "2071340"
  },
  {
    "text": "target arm you get for CPU you get different set of instructions three you get what you get is like a dot iso file",
    "start": "2071340",
    "end": "2077190"
  },
  {
    "text": "basically that can get executed on specific architecture inferential is hardware so it's not on the software",
    "start": "2077190",
    "end": "2084179"
  },
  {
    "text": "Nimoy's like the actual physical chip that you will be able to say okay link",
    "start": "2084179",
    "end": "2090030"
  },
  {
    "text": "that to my ec2 instance and then use the specific optimized hardware for deep",
    "start": "2090030",
    "end": "2095879"
  },
  {
    "text": "learning and run this because the chip is gonna have like instead of like generic general-purpose transistors like",
    "start": "2095880",
    "end": "2102780"
  },
  {
    "text": "any cpu the chip is made for the specific kind of operation that you do",
    "start": "2102780",
    "end": "2107970"
  },
  {
    "text": "in deep learning convolutions cooling and matrix multiplication and all this stuff yeah it's going to be a specific",
    "start": "2107970",
    "end": "2116490"
  },
  {
    "text": "hardware in-house built if you wanna for competitor something similar if you if",
    "start": "2116490",
    "end": "2123900"
  },
  {
    "text": "you know what I think about yes",
    "start": "2123900",
    "end": "2130910"
  },
  {
    "text": "new",
    "start": "2138610",
    "end": "2141610"
  },
  {
    "text": "yep yep",
    "start": "2144380",
    "end": "2150580"
  },
  {
    "text": "yes so you you if you go through the documentation of new you get a list of",
    "start": "2153069",
    "end": "2158679"
  },
  {
    "text": "compatible architecture so when before if I should user code is really in Sage",
    "start": "2158679",
    "end": "2165400"
  },
  {
    "text": "maker you have like your your model you do model that compile and you say which architecture you want and then then you",
    "start": "2165400",
    "end": "2172749"
  },
  {
    "text": "get a compared model out of fact in s3 and then this we can run it it's going",
    "start": "2172749",
    "end": "2179380"
  },
  {
    "text": "to be run through a specific docker container and you can use that to do inference on your either directly in",
    "start": "2179380",
    "end": "2186880"
  },
  {
    "text": "Sage maker by say you targeted c5 and then you run it on a c5 inside the cloud",
    "start": "2186880",
    "end": "2195099"
  },
  {
    "text": "or you can run it you can target raspberry pi and you can read that on the raspberry pi for example yes",
    "start": "2195099",
    "end": "2203729"
  },
  {
    "text": "I'm not exactly sure for mobile so but what you can do if you want to target mobile you can use the open source",
    "start": "2218290",
    "end": "2226000"
  },
  {
    "text": "version TVM and target on as your compilation platform and then you can",
    "start": "2226000",
    "end": "2231160"
  },
  {
    "text": "use TVM to run that in on your mobile app so neo can take the TF light formats",
    "start": "2231160",
    "end": "2246190"
  },
  {
    "text": "oh you mean size maker in general can it save in chef light yes so if you use tensorflow basically a",
    "start": "2246190",
    "end": "2252400"
  },
  {
    "text": "search maker works if you provide your own training code and your own saving code so if you pour a tensor flow training code and you're saving code you",
    "start": "2252400",
    "end": "2259120"
  },
  {
    "text": "say like export as GF light or whatever that's what was going to be saved in s3 so there's going to be your artifact and",
    "start": "2259120",
    "end": "2265450"
  },
  {
    "text": "then you get that and you deploy it the way you want you can deploy it with search maker or you can take it and",
    "start": "2265450",
    "end": "2271270"
  },
  {
    "text": "deploy its in-house or on a mobile app or whatever so you can use such maker as",
    "start": "2271270",
    "end": "2277000"
  },
  {
    "text": "much as little as you want whether just for training or for inference or compilation you can use it like end to",
    "start": "2277000",
    "end": "2284980"
  },
  {
    "text": "end you can come in at any time of the deep learning kind of pipeline yes it's",
    "start": "2284980",
    "end": "2294700"
  },
  {
    "text": "going to be I think only inside AWS I I",
    "start": "2294700",
    "end": "2303640"
  },
  {
    "text": "don't know is the answer so when we're gonna get closer to the release is going to be more information available yes",
    "start": "2303640",
    "end": "2314010"
  },
  {
    "text": "so Sushma Carell is implemented on top of tensorflow and IMAX net so mixing it",
    "start": "2318339",
    "end": "2325359"
  },
  {
    "text": "supports reinforcement learning and we have example implementation in the repo like dqn and I think we have PPO as well",
    "start": "2325359",
    "end": "2333509"
  },
  {
    "text": "we don't have a toolkit per se same way as we are glue on CV or glue on NLP but",
    "start": "2333509",
    "end": "2340089"
  },
  {
    "text": "we have a lot of reference implementation and you can you can use seismic array that integrates with",
    "start": "2340089",
    "end": "2345670"
  },
  {
    "text": "Scotch in order to you can to use search maker and it makes nets together to do",
    "start": "2345670",
    "end": "2350799"
  },
  {
    "text": "reinforcement learning yes and actually",
    "start": "2350799",
    "end": "2358949"
  },
  {
    "text": "Sigma Corral is based on with MX net is based on glue on so if you want you can",
    "start": "2358949",
    "end": "2372069"
  },
  {
    "text": "do a online learning with them X not the same way as you can do it with tensorflow is simply you need to",
    "start": "2372069",
    "end": "2377549"
  },
  {
    "text": "architect your own you know you you need to architect your application so that you can keep adding examples to your",
    "start": "2377549",
    "end": "2384279"
  },
  {
    "text": "training set and then refine your model so I think it's more like a architectural question I'm accident",
    "start": "2384279",
    "end": "2390640"
  },
  {
    "text": "reports it like any other framework is why say angry on makes it easy because when you train one batch you're not like",
    "start": "2390640",
    "end": "2398799"
  },
  {
    "text": " one batch you just say passing it as a data through compute the",
    "start": "2398799",
    "end": "2403869"
  },
  {
    "text": "gradient updates so you can pass like small batch big batch it's extremely",
    "start": "2403869",
    "end": "2409209"
  },
  {
    "text": "flexible is dynamic yes",
    "start": "2409209",
    "end": "2414479"
  },
  {
    "text": "yes so we just released a beta version of a book that is called dive into deep",
    "start": "2419390",
    "end": "2427490"
  },
  {
    "text": "learning if you come at the end I'll give you the actual URL or maybe if we",
    "start": "2427490",
    "end": "2433010"
  },
  {
    "text": "have time I'll just put it up it's more than 100 different Jupiter known books that goes through the theory of deep",
    "start": "2433010",
    "end": "2440420"
  },
  {
    "text": "learning so if you're kind of you come from the ML background but you want to know more about deep learning it's going",
    "start": "2440420",
    "end": "2446000"
  },
  {
    "text": "to go through the different optimizers Adam at a grad s GD h GD with momentum so on and so forth and every practical",
    "start": "2446000",
    "end": "2453760"
  },
  {
    "text": "implementation or every practical example is done with a mix net and gluon so it's kind of a learning you learn the",
    "start": "2453760",
    "end": "2462590"
  },
  {
    "text": "practical side and you learn this your tickle side at the same time and you start with like these basic building",
    "start": "2462590",
    "end": "2467840"
  },
  {
    "text": "blocks and at the end you do stuff like object detection image segmentation",
    "start": "2467840",
    "end": "2473140"
  },
  {
    "text": "texts machine neural machine translation very advanced NLP and cv techniques or",
    "start": "2473140",
    "end": "2480620"
  },
  {
    "text": "recommend a recommender system is another chapter so really kind of try to",
    "start": "2480620",
    "end": "2485660"
  },
  {
    "text": "and that's a cause as being taught next January at the Berkeley so we have Alex",
    "start": "2485660",
    "end": "2492890"
  },
  {
    "text": "mola we probably know from the SVM papers is a principal scientist or even",
    "start": "2492890",
    "end": "2501440"
  },
  {
    "text": "like director of science at AWS and he is going to teach this course at back",
    "start": "2501440",
    "end": "2506570"
  },
  {
    "text": "layer on January so you can access the same material and you can follow at your own pace",
    "start": "2506570",
    "end": "2513340"
  },
  {
    "text": "it's called dive into deep learning I'll",
    "start": "2516350",
    "end": "2522140"
  },
  {
    "text": "I'll put it up at the end of the talk if you want any other questions okay",
    "start": "2522140",
    "end": "2529180"
  },
  {
    "text": "so I think we almost at the end of the talk actually",
    "start": "2531100",
    "end": "2536980"
  },
  {
    "text": "so to summarize the kind of three steps I was talking about first is optimize",
    "start": "2538210",
    "end": "2544850"
  },
  {
    "text": "your training code to flee as your hardware extremely important and very much underestimated there was this one",
    "start": "2544850",
    "end": "2551450"
  },
  {
    "text": "time I can't name the customer we're working with a customer that was evaluating between different cloud",
    "start": "2551450",
    "end": "2557480"
  },
  {
    "text": "providers and different like type of deep learning a hardware and we went there were like well you know we're not",
    "start": "2557480",
    "end": "2565280"
  },
  {
    "text": "too sure if Amex is actually faster we looked at the code and using the same hardware ever managed to get 64 times PD",
    "start": "2565280",
    "end": "2572030"
  },
  {
    "text": "improvement just optimizing the code so just to tell you how critical it is and",
    "start": "2572030",
    "end": "2577160"
  },
  {
    "text": "even like top-class data scientists might not have the engineering",
    "start": "2577160",
    "end": "2583160"
  },
  {
    "text": "background to do the right data pipelining and with gluon we really aware of that and we patch in all this",
    "start": "2583160",
    "end": "2590480"
  },
  {
    "text": "nice it's synchronous processing and milky multi processing right into the",
    "start": "2590480",
    "end": "2596600"
  },
  {
    "text": "framework and make it very easy easily accessible trained at the lowest call",
    "start": "2596600",
    "end": "2602000"
  },
  {
    "text": "possible using search maker so this record is really nice iodine way you decouple the experiment management from",
    "start": "2602000",
    "end": "2609860"
  },
  {
    "text": "the actual training so you don't need a massive machine to run a jupiter notebook you need a big one to run your",
    "start": "2609860",
    "end": "2616250"
  },
  {
    "text": "training scripts and that's what's america does for you it starts a cluster from 1 to n instances to your training",
    "start": "2616250",
    "end": "2623150"
  },
  {
    "text": "and when it's over it goes down and you only build for what you use you can use dynamic training with MX stats so even",
    "start": "2623150",
    "end": "2629870"
  },
  {
    "text": "during a single training job you can add and remove instances dynamically to make sure none of your piece for instances",
    "start": "2629870",
    "end": "2635810"
  },
  {
    "text": "are sitting IDO and finally point France what we do we can optimize the model we",
    "start": "2635810",
    "end": "2641510"
  },
  {
    "text": "can optimize the deepening models compile them to be super tiny super efficient and optimize for specific",
    "start": "2641510",
    "end": "2648100"
  },
  {
    "text": "deployment target space architecture and you can deploy that research maker on an auto scaling",
    "start": "2648100",
    "end": "2654260"
  },
  {
    "text": "cluster using docker containers and it's all handled for you and finally you can",
    "start": "2654260",
    "end": "2660829"
  },
  {
    "text": "also use next year a custom hardware to really have like the lowest cost and the",
    "start": "2660829",
    "end": "2666799"
  },
  {
    "text": "fastest inference possible with a Debrecen furniture so thank you very much if there is any questions we still",
    "start": "2666799",
    "end": "2680180"
  },
  {
    "text": "have a bit of time so I'm going to take a few if there still yes yes so recently",
    "start": "2680180",
    "end": "2687920"
  },
  {
    "text": "we made a committer so if you create a pull request I I can merge it so that's",
    "start": "2687920",
    "end": "2693770"
  },
  {
    "text": "also something maybe we haven't really touched on which is NX net is its apache",
    "start": "2693770",
    "end": "2699230"
  },
  {
    "text": "MX net so it's part of the Apache Software Foundation which means inside",
    "start": "2699230",
    "end": "2704720"
  },
  {
    "text": "the MX net team people who are actively contributing to it we have people from Intel Nvidia Airbnb whatnot yeah we have",
    "start": "2704720",
    "end": "2713119"
  },
  {
    "text": "plenty of companies who have pets on MX net you've seen the slide with all the customers we have and they all actively",
    "start": "2713119",
    "end": "2719089"
  },
  {
    "text": "contributing to a Mac set one way or another so we have people from Mathematica who are actively using max",
    "start": "2719089",
    "end": "2726319"
  },
  {
    "text": "net and contributing new operators to support their own research and any of you could join the project and if you",
    "start": "2726319",
    "end": "2734510"
  },
  {
    "text": "have like sustain contribution any of you could be made a committer of MX net it's not owned by AWS we bet on it",
    "start": "2734510",
    "end": "2741049"
  },
  {
    "text": "because we think is the fastest most capable framework out there and we want you to use it because then you're going",
    "start": "2741049",
    "end": "2746359"
  },
  {
    "text": "to save money and then you're gonna stay customers of us but we don't have skin in the game we're not selling MX net to",
    "start": "2746359",
    "end": "2752329"
  },
  {
    "text": "you you want to use tensorflow use tensorflow we just think that if used MX net you're gonna spend less money and",
    "start": "2752329",
    "end": "2757490"
  },
  {
    "text": "you're gonna get better models right",
    "start": "2757490",
    "end": "2761260"
  },
  {
    "text": "me yes yeah overall in the usability MX",
    "start": "2779619",
    "end": "2787029"
  },
  {
    "text": "nets with gluon is like a winner by large margin in terms of performance don't trust me but look at Nvidia with",
    "start": "2787029",
    "end": "2794349"
  },
  {
    "text": "like kind of this neutral player in the field just want people to use GPUs they don't care if it's MX net or tensorflow",
    "start": "2794349",
    "end": "2800529"
  },
  {
    "text": "they just published a benchmark where they show that for resonate fifty training it makes it is 50% faster okay",
    "start": "2800529",
    "end": "2807759"
  },
  {
    "text": "and then one thing I'll give to tensorflow right now is they have a larger community because they were there",
    "start": "2807759",
    "end": "2813549"
  },
  {
    "text": "the first and Google has done some incredible research so lots of papers",
    "start": "2813549",
    "end": "2818859"
  },
  {
    "text": "come coming from Google they've already been implemented intensive though and as you all know usually you want to just",
    "start": "2818859",
    "end": "2824859"
  },
  {
    "text": "play your own with something you're gonna take it from whatever paper whatever framework it is and you're",
    "start": "2824859",
    "end": "2830499"
  },
  {
    "text": "gonna run with that so to tackle this problem we created the toolkits computer vision and NLP and we have other",
    "start": "2830499",
    "end": "2837069"
  },
  {
    "text": "verticals coming for example if you've heard of the bert bert-- paper which is like latest NLP paper that shows that",
    "start": "2837069",
    "end": "2843099"
  },
  {
    "text": "you can actually do very efficient transfer learning with NLP was published",
    "start": "2843099",
    "end": "2848170"
  },
  {
    "text": "a months ago we already have the implementation with state of the art result and pre trained model in gluon",
    "start": "2848170",
    "end": "2853329"
  },
  {
    "text": "NLP so we're following closely the bleeding edge research providing reference implementation for it and",
    "start": "2853329",
    "end": "2859719"
  },
  {
    "text": "include integrating it in our tool kits so this becomes like a one-stop shop for",
    "start": "2859719",
    "end": "2865089"
  },
  {
    "text": "you to go there look at the training code so be the actual like pre trained model we give they are actually training",
    "start": "2865089",
    "end": "2872979"
  },
  {
    "text": "or see ICD pipeline stored on s3 and we give you the actual training commands",
    "start": "2872979",
    "end": "2878440"
  },
  {
    "text": "you need in order to get the exact same result and the training script and the model definition so there's nothing",
    "start": "2878440",
    "end": "2883989"
  },
  {
    "text": "hidden it's really what we do is really in the open because we believe that",
    "start": "2883989",
    "end": "2889719"
  },
  {
    "text": "clean in openness and reproducibility in research and that's what's going to make us progress even faster which in",
    "start": "2889719",
    "end": "2897069"
  },
  {
    "text": "affiliate is already like super fast",
    "start": "2897069",
    "end": "2900779"
  },
  {
    "text": "with respect to a mixed attitude so for so I mix that we exploring a bunch of",
    "start": "2908340",
    "end": "2914670"
  },
  {
    "text": "options for example we have like this recently we integrated the fact that the graph",
    "start": "2914670",
    "end": "2920250"
  },
  {
    "text": "you mean like the engine art",
    "start": "2920250",
    "end": "2923900"
  },
  {
    "text": "I mean there's so many things like especially in Amazon research so lots of",
    "start": "2932369",
    "end": "2940019"
  },
  {
    "text": "teams in Amazon are using for example you've heard of light tech to extract to",
    "start": "2940019",
    "end": "2945689"
  },
  {
    "text": "do SEO on extract documents all the teams inside Amazon they're migrating to",
    "start": "2945689",
    "end": "2950729"
  },
  {
    "text": "MX net and I know because we are my team we are providing advice the up type of",
    "start": "2950729",
    "end": "2956819"
  },
  {
    "text": "optimization I just tell you we help these team to implement them so there's",
    "start": "2956819",
    "end": "2963420"
  },
  {
    "text": "a bunch of like new research happening on MX net but well lots of papers they",
    "start": "2963420",
    "end": "2970859"
  },
  {
    "text": "don't publish it's fragmix a mark agnostic right so I think I think it's a",
    "start": "2970859",
    "end": "2976469"
  },
  {
    "text": "good thing that the researchers are not saying that's I taught research oh",
    "start": "2976469",
    "end": "2981660"
  },
  {
    "text": "that's tensorflow research right like what's important is the network the implementation we don't really care that",
    "start": "2981660",
    "end": "2987989"
  },
  {
    "text": "much and that's why you don't see that many papers referencing the framework",
    "start": "2987989",
    "end": "2995009"
  },
  {
    "text": "they used because it's not framework specific what they're doing I would pick by touch yeah no really the",
    "start": "2995009",
    "end": "3008329"
  },
  {
    "text": "usability is everything anybody is actually done yeah yeah",
    "start": "3008329",
    "end": "3020140"
  },
  {
    "text": "yeah I saying I think we they just released last week by tot 1.0 I think",
    "start": "3024200",
    "end": "3029640"
  },
  {
    "text": "they are these capabilities in the latest release",
    "start": "3029640",
    "end": "3034369"
  },
  {
    "text": "so I'm gonna excuse me avy I'm just",
    "start": "3054410",
    "end": "3060320"
  },
  {
    "text": "gonna put the put a website which is",
    "start": "3060320",
    "end": "3065960"
  },
  {
    "text": "this dive oh okay oh no dive into okay",
    "start": "3065960",
    "end": "3090860"
  },
  {
    "text": "so it's a dive into deep learning dot all I know might have some word document",
    "start": "3090860",
    "end": "3106430"
  },
  {
    "text": "I hope is nothing critical and this",
    "start": "3106430",
    "end": "3114260"
  },
  {
    "text": "cannot be zoomed what can you dive into",
    "start": "3114260",
    "end": "3120770"
  },
  {
    "text": "the planning as more chapters now yeah and it's also on github you can yeah if",
    "start": "3120770",
    "end": "3128990"
  },
  {
    "text": "you if you google dive into deep dive into deep learning yeah if you google",
    "start": "3128990",
    "end": "3136010"
  },
  {
    "text": "dive into deep learning that offer if you go on the website there is a link directly to get up to the repo and you have all the notebooks and you can",
    "start": "3136010",
    "end": "3142490"
  },
  {
    "text": "contribute pull requests to it if you find any mistakes and yeah feel free to continue helping spreading deep learning",
    "start": "3142490",
    "end": "3149780"
  },
  {
    "text": "to the world okay thank you [Applause]",
    "start": "3149780",
    "end": "3157760"
  }
]