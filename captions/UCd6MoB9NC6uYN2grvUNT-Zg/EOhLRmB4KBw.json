[
  {
    "text": "good afternoon everybody welcome to reinvent 2018 welcome to reinvent Monday hope you guys",
    "start": "0",
    "end": "6000"
  },
  {
    "text": "have a phenomenal agenda packed week ahead of you it's my pleasure to welcome",
    "start": "6000",
    "end": "11429"
  },
  {
    "text": "you to a our c333 which is from Russia With Love Fox Sports World Cup production on stage",
    "start": "11429",
    "end": "19470"
  },
  {
    "text": "with me today are going to be two presenters our customer speaking today is Fox Sports and we've got Brandon",
    "start": "19470",
    "end": "24630"
  },
  {
    "text": "Potter who's director of post-production at Fox Sports to speak we've also got Mike father's who's CTO of aspera",
    "start": "24630",
    "end": "31830"
  },
  {
    "text": "to also talk and they'll be both joined by myself to talk through well cut",
    "start": "31830",
    "end": "37079"
  },
  {
    "text": "production which i think is a super exciting evolution of the use of the cloud for an extremely high-profile",
    "start": "37079",
    "end": "42570"
  },
  {
    "text": "event that hopefully many of you are chessy a show of hands as to who watch the World Cup all right Lily everyone",
    "start": "42570",
    "end": "49289"
  },
  {
    "text": "it's pretty hard to miss right so this is pretty exciting and I've been a Ted West for about six and a half years it's",
    "start": "49289",
    "end": "54690"
  },
  {
    "text": "wonderful to be able to tell stories like this where our customers are truly innovating around what they can do in",
    "start": "54690",
    "end": "60570"
  },
  {
    "text": "terms of viewer entertainment and differentiating themselves about what they offer from a content and experience",
    "start": "60570",
    "end": "66180"
  },
  {
    "text": "perspective and it's really exciting to see how our technology partners and the a diverse platform are being used to",
    "start": "66180",
    "end": "72810"
  },
  {
    "text": "really help that reinvention and keeping TV very relevant around events like this",
    "start": "72810",
    "end": "78420"
  },
  {
    "text": "specifically things like the World Cup so our presentation is split into three",
    "start": "78420",
    "end": "83549"
  },
  {
    "text": "sections so we were thinking about how to articulate this if you're involved in",
    "start": "83549",
    "end": "89369"
  },
  {
    "text": "live event production or you involved in broadcasting you'll know the complexities involved in servicing all",
    "start": "89369",
    "end": "96030"
  },
  {
    "text": "of your internal stakeholders dealing with all of your external service providers dealing with all the technology components that you need to",
    "start": "96030",
    "end": "102869"
  },
  {
    "text": "piece together to get something to work and what we're going to step through today is essentially how whether the",
    "start": "102869",
    "end": "108869"
  },
  {
    "text": "event going on in Russia which from the Fox Sports was the official broadcaster",
    "start": "108869",
    "end": "113939"
  },
  {
    "text": "for the United States Russia is 6,000 miles away that's 10,000 kilometers it's",
    "start": "113939",
    "end": "119670"
  },
  {
    "text": "about 180 milliseconds over a network so it's really far and what we're going to talk about is how that gaming and that",
    "start": "119670",
    "end": "126060"
  },
  {
    "text": "programming was brought to the US with Fox Sports is based how it was presented to their editors and how it was inserted",
    "start": "126060",
    "end": "132330"
  },
  {
    "text": "into their programming for the duration of the events and so we've got three sections",
    "start": "132330",
    "end": "138099"
  },
  {
    "text": "so fox is going to the folk sports is going to come up on stage and talk about the production aspects which is quite a",
    "start": "138099",
    "end": "144819"
  },
  {
    "text": "unique insight into the complexities that we go through as technology providers in providing the plumbing",
    "start": "144819",
    "end": "150819"
  },
  {
    "text": "providing the software providing the glue to really enable a production and essentially what your viewers see on",
    "start": "150819",
    "end": "157780"
  },
  {
    "text": "their screens or their devices at home on mobile I'm going to talk about the AWS infrastructure that was involved in",
    "start": "157780",
    "end": "164590"
  },
  {
    "text": "facilitating this and so this is both the eight of us networking components the different regions server less",
    "start": "164590",
    "end": "170709"
  },
  {
    "text": "technology is easy to compute things like that and then mike is going to come up on stage from a sparrow to talk about",
    "start": "170709",
    "end": "176829"
  },
  {
    "text": "the video transport and really the data plane so the video plane which drives the consumption of the infrastructure",
    "start": "176829",
    "end": "182680"
  },
  {
    "text": "which delivers the key content to the key people to deliver it ultimately to the consumer and so those are the three",
    "start": "182680",
    "end": "189099"
  },
  {
    "text": "sections of our presentation as we move forward over this next hour so I'm gonna",
    "start": "189099",
    "end": "194680"
  },
  {
    "text": "hand the podium over to Brandon Potter from Foxsports to talk about the production aspects and the business aspects of the beginning of",
    "start": "194680",
    "end": "201370"
  },
  {
    "text": "this presentation so Brandon over to you",
    "start": "201370",
    "end": "205709"
  },
  {
    "text": "hello everybody how you doing my name is Brandon Potter I work in post-production",
    "start": "210310",
    "end": "215749"
  },
  {
    "text": "for Fox ports so let's go to the next",
    "start": "215749",
    "end": "223609"
  },
  {
    "text": "slide here for the the World Cup we had an interesting thing we had 64 matches",
    "start": "223609",
    "end": "228709"
  },
  {
    "text": "and 12 venues spread across all of Russia there was 64 total UHD matches",
    "start": "228709",
    "end": "235669"
  },
  {
    "text": "that we recorded in in reality it was probably closer to a couple thousand 1080 feeds that we had to present to",
    "start": "235669",
    "end": "241340"
  },
  {
    "text": "production to use all their content to edit and cut with that was 2 petabytes of content that was uploaded throughout",
    "start": "241340",
    "end": "249109"
  },
  {
    "text": "4 weeks of the tournament roughly it was about we ended up sending content back",
    "start": "249109",
    "end": "257989"
  },
  {
    "text": "to Los Angeles via fast stream to be edited live in Premiere in a 10 second",
    "start": "257989",
    "end": "264080"
  },
  {
    "text": "delay which might we'll get into in detail our largest upload and one a",
    "start": "264080",
    "end": "270530"
  },
  {
    "text": "single day was 150 terabytes and with one two three percent packet loss so",
    "start": "270530",
    "end": "282139"
  },
  {
    "text": "interesting business challenge that we had like I said our there's about six",
    "start": "282139",
    "end": "287210"
  },
  {
    "text": "thousand miles away from our home broadcast facility in Los Angeles",
    "start": "287210",
    "end": "292030"
  },
  {
    "text": "because our bosses hate us our entire edit facility was back in Los Angeles where all this media was generated in",
    "start": "292449",
    "end": "299120"
  },
  {
    "text": "Russia and something I should point out with large events like this with multiple URLs or media rights licensees",
    "start": "299120",
    "end": "307449"
  },
  {
    "text": "internationally there's usually some sort of host broadcast server I'm sorry host a broadcast service that supplies",
    "start": "307449",
    "end": "314570"
  },
  {
    "text": "you with all the SDI feeds and all the media and all the content but you need to work with you know for live games to",
    "start": "314570",
    "end": "320030"
  },
  {
    "text": "air and and and all the feeds that you need for you with the viewer to see these feeds on Fox so one of the biggest",
    "start": "320030",
    "end": "329960"
  },
  {
    "text": "things that we were dealing with was time I mean we're working in a live environment right so I worked in",
    "start": "329960",
    "end": "335539"
  },
  {
    "text": "post-production in in live what does that mean it's kind of an oxymoron how do you have post in line",
    "start": "335539",
    "end": "342050"
  },
  {
    "text": "so for some context we edit things sometimes you have a couple weeks to",
    "start": "342050",
    "end": "347900"
  },
  {
    "text": "edit some packages some features maybe you have a full-length show that you're working on for a couple months and you're turning around that and you're",
    "start": "347900",
    "end": "354410"
  },
  {
    "text": "getting it to air all getting well sometimes we have a couple seconds a couple minutes to turn around a highlight package and play that out into",
    "start": "354410",
    "end": "361940"
  },
  {
    "text": "studio what do I mean by play out well in a live production there's usually a",
    "start": "361940",
    "end": "367010"
  },
  {
    "text": "director that needs to cue a play out operator to play that piece to air so when you've got Talking Heads and then",
    "start": "367010",
    "end": "372470"
  },
  {
    "text": "it cuts to a you know an edited piece someone's usually physically playing that piece out so that the the time that",
    "start": "372470",
    "end": "378920"
  },
  {
    "text": "we had to work with we didn't have the luxury of waiting or if something failed we had a HAP present all this content now and from so far away like I said",
    "start": "378920",
    "end": "386750"
  },
  {
    "text": "we're a global way 6,000 miles why did we do this it I mean it's a tremendous cost-saving",
    "start": "386750",
    "end": "392870"
  },
  {
    "text": "when you could have an entire post-production facility not uprooted and shipped out to Russia I mean you",
    "start": "392870",
    "end": "400550"
  },
  {
    "text": "could imagine so we were actually when we first walked into this I thought oh god how we're gonna get all this content",
    "start": "400550",
    "end": "406820"
  },
  {
    "text": "back and you know it can't fail we as you can see here we generated",
    "start": "406820",
    "end": "412130"
  },
  {
    "text": "22,000 assets that were managed and accessed by production via web GUI in",
    "start": "412130",
    "end": "417290"
  },
  {
    "text": "levels beyond reach engine another challenge that we had is this is a temp location for the World Cup it was stood",
    "start": "417290",
    "end": "424550"
  },
  {
    "text": "up for us to work in temporarily we didn't have you know the luxury of sitting out and standing up you know",
    "start": "424550",
    "end": "430460"
  },
  {
    "text": "permanent servers and what we had to break it out down in a month or you know we got there a couple months early before the event started security was a",
    "start": "430460",
    "end": "437660"
  },
  {
    "text": "big one we're in Russia we're in a foreign country all that content that we were generated like I said to meta bytes we couldn't afford to ship all that",
    "start": "437660",
    "end": "443930"
  },
  {
    "text": "stuff up to the cloud in AWS and then download it with some sort of virus so that was a big one for us which which",
    "start": "443930",
    "end": "449930"
  },
  {
    "text": "Bhavik we'll get into in a bit so this is basically how we pull this off this",
    "start": "449930",
    "end": "456980"
  },
  {
    "text": "was our whole ecosystem right here on the left you've got it see here yes ah I guess right we've got Moscow with our",
    "start": "456980",
    "end": "465680"
  },
  {
    "text": "EVs playout servers right so that was all where everything in Moscow was really going to air for our shows that",
    "start": "465680",
    "end": "471260"
  },
  {
    "text": "were based in Moscow so that's where our content eventually needed to be delivered so all",
    "start": "471260",
    "end": "476419"
  },
  {
    "text": "this content that was being generated from HBS and all these feeds and all the Eng shoots that we were getting we were sending back to Los Angeles it was",
    "start": "476419",
    "end": "483050"
  },
  {
    "text": "finished pieces features were being cut and then sent back to Moscow to play out to air something I should also mention",
    "start": "483050",
    "end": "489080"
  },
  {
    "text": "is our home based facility all the content in in Moscow was 1080p50 pal",
    "start": "489080",
    "end": "495620"
  },
  {
    "text": "our home base facilities 7 2015 9 4 and mixing those frame rates as a no-no so it was very difficult for us to kind of",
    "start": "495620",
    "end": "501680"
  },
  {
    "text": "incorporate these two worlds and we had a couple shows going we higher shows in in Moscow with our set in Red Square the",
    "start": "501680",
    "end": "509240"
  },
  {
    "text": "IBC the international broadcast Center where we were doing all our work was about 40 miles away where we had our",
    "start": "509240",
    "end": "514279"
  },
  {
    "text": "main control room for the shoulder programming but then we also had a show airing out of our Los Angeles facility again in 59 94 so mixing these frame",
    "start": "514279",
    "end": "521899"
  },
  {
    "text": "rates was difficult and and using that fast stream to send we had three",
    "start": "521899",
    "end": "527329"
  },
  {
    "text": "priority feeds of the 20 or 30 or so per match that we were able to write directly to Moscow storage using fast",
    "start": "527329",
    "end": "534140"
  },
  {
    "text": "stream and then simultaneously replicate that back to Los Angeles on a 10 second",
    "start": "534140",
    "end": "539449"
  },
  {
    "text": "delay and that's a lemon limitation of premiere more than anything else so editors were able to work with that content immediately cut with it do their",
    "start": "539449",
    "end": "545269"
  },
  {
    "text": "packages and then send it all the way back to Moscow to air which you'll see here and this to kind of outlines this",
    "start": "545269",
    "end": "551079"
  },
  {
    "text": "same thing how would how do we present this content all under s3 for people to access well if we've got a nine-hour",
    "start": "551079",
    "end": "557660"
  },
  {
    "text": "record producers the cisnet udders associate producers aren't going to want to download a nine-hour record to use a",
    "start": "557660",
    "end": "564110"
  },
  {
    "text": "10-second piece so one of our business requirements was to present this stuff present the proxy present the growing HLS into the stream and be able to sub",
    "start": "564110",
    "end": "571790"
  },
  {
    "text": "clip pieces of what you need live and then send it to your editor we used a premier panel in inReach",
    "start": "571790",
    "end": "578029"
  },
  {
    "text": "engine in our Pam to be able to look at these sub clips that were presented and pull that in Premiere use your content",
    "start": "578029",
    "end": "583970"
  },
  {
    "text": "cut your pieces and then get it to air whether it was airing out of the Los Angeles production or back in in Moscow",
    "start": "583970",
    "end": "589930"
  },
  {
    "text": "which is pretty incredible so the key to this we called Fox our Fox Port",
    "start": "589930",
    "end": "595699"
  },
  {
    "text": "Charlotte facility little Russia in around March our road to Russia started really then when we stood up and",
    "start": "595699",
    "end": "601640"
  },
  {
    "text": "simulated everything that we were gonna do in Russia we didn't want to get to Russia and realized that something was",
    "start": "601640",
    "end": "607279"
  },
  {
    "text": "broken I think overnight is two weeks from the United States so we stood up everything and we ran all our network connections from",
    "start": "607279",
    "end": "613580"
  },
  {
    "text": "Charlotte the only piece that was missing was really Moscow we ran it through Frank work Frankfurt and then back to AWS West to be able to see you",
    "start": "613580",
    "end": "620960"
  },
  {
    "text": "know what latency are we working with how's this gonna work right here you got Dave Norman in the middle from Telestream kind of I don't know pointing",
    "start": "620960",
    "end": "627320"
  },
  {
    "text": "out some of his servers those were the encoders that we used to take all the SDI feeds that we knew we were gonna get",
    "start": "627320",
    "end": "632540"
  },
  {
    "text": "up to I think 60 for a match turn those into 10 80 50 ABC I to 20 megabit",
    "start": "632540",
    "end": "638630"
  },
  {
    "text": "streams writing to her storage and then with the fast 3 or 4 fast streams that",
    "start": "638630",
    "end": "644510"
  },
  {
    "text": "we were able to send back to Los Angeles and that was the hardware that was really doing it with her type API integration with aspera it was pretty",
    "start": "644510",
    "end": "651080"
  },
  {
    "text": "amazing that was Telestream and then over here it's kind of for some backstage really what was happening in",
    "start": "651080",
    "end": "657470"
  },
  {
    "text": "Russia this was our main ingest room this was basically air traffic control of all our media all the feeds that were",
    "start": "657470",
    "end": "663710"
  },
  {
    "text": "presented on the top row here you basically have monitoring those were all out right now a lot of them are in bars",
    "start": "663710",
    "end": "670240"
  },
  {
    "text": "this is where we could see you know what's coming down those lines what channel is it being recorded on and tell",
    "start": "670240",
    "end": "676160"
  },
  {
    "text": "a stream this middle row this was kind of key right here this is where we recorded and our scheduler was really",
    "start": "676160",
    "end": "683060"
  },
  {
    "text": "set up we needed a way to schedule like I said it was probably between 3000 and",
    "start": "683060",
    "end": "688430"
  },
  {
    "text": "6000 feeds throughout the four week tournament that we recorded we needed an easy way to do that Telestream really",
    "start": "688430",
    "end": "694370"
  },
  {
    "text": "came through in the clutch when we had a vendor drop out the scheduler to create something that we could do this you know a little bit",
    "start": "694370",
    "end": "699740"
  },
  {
    "text": "easier than scheduling one at a time and crash record in every feed we couldn't do that we actually ended up ingesting a",
    "start": "699740",
    "end": "705170"
  },
  {
    "text": "CSV document that's based on times and durations that would auto kind of",
    "start": "705170",
    "end": "710960"
  },
  {
    "text": "schedule these feeds which is pretty great and then down here in the middle or the media operator kind of stations",
    "start": "710960",
    "end": "717200"
  },
  {
    "text": "and this is where it all kind of went down this is where transcoding will potentially happen if we needed to send",
    "start": "717200",
    "end": "723170"
  },
  {
    "text": "files to to EVS for play out and get files back to Los Angeles and the next",
    "start": "723170",
    "end": "729470"
  },
  {
    "text": "slide here this is basically how all the editors and producers wash to the",
    "start": "729470",
    "end": "735320"
  },
  {
    "text": "producers were accessing all their content you could see here in the in the middle you've got the video player you",
    "start": "735320",
    "end": "740720"
  },
  {
    "text": "could sub-clip set in announce you could search one of the interesting things that we had to do and why why we did",
    "start": "740720",
    "end": "748370"
  },
  {
    "text": "this really HBS provided us with all the content that we needed and actually had a great Pam the only the big caveat to",
    "start": "748370",
    "end": "755570"
  },
  {
    "text": "that was it was only available in Moscow so we basically took all the media that was being generated by them and put it",
    "start": "755570",
    "end": "761780"
  },
  {
    "text": "up to s3 and made our own and this is also for our own archive there they had",
    "start": "761780",
    "end": "767300"
  },
  {
    "text": "some great metadata that was linked to their media that we would dump and then ingest into in to reach engine what you see here to present to everybody but",
    "start": "767300",
    "end": "774650"
  },
  {
    "text": "again it was pretty wild I think at some point we were we got to Russia and even in Charlotte there wasn't we didn't get",
    "start": "774650",
    "end": "781340"
  },
  {
    "text": "the facts everything out and to end there was certain technology that when we got to Russia was out of the box we",
    "start": "781340",
    "end": "786380"
  },
  {
    "text": "had to kind of cross our fingers and hope it worked and everything really came together and like I said we didn't have enough time from the end of the",
    "start": "786380",
    "end": "792020"
  },
  {
    "text": "final I think we had seven hours before power was shut down lights and they were ripping things out so if we didn't have",
    "start": "792020",
    "end": "797300"
  },
  {
    "text": "all our content uploaded like I said it was four weeks to petabytes if we were at all behind on that content we didn't",
    "start": "797300",
    "end": "802610"
  },
  {
    "text": "get up to the cloud it was gone forever the storage that we're using on Prem was getting broken down it was actually rental and shipped somewhere else so",
    "start": "802610",
    "end": "809690"
  },
  {
    "text": "there was really little room for failure there and then next I think I'm gonna",
    "start": "809690",
    "end": "815480"
  },
  {
    "text": "throw it back to mr. bhavish to take you through kind of a deep dive into the AWS infrastructure",
    "start": "815480",
    "end": "822760"
  },
  {
    "text": "[Applause] all right I think that's a super",
    "start": "823560",
    "end": "830390"
  },
  {
    "text": "interesting viewpoint from a customer perspective especially as we probe as we provide the infrastructure components to",
    "start": "830390",
    "end": "836960"
  },
  {
    "text": "service all of these users the different stakeholders I think is extremely wide and if again if you're involved in its",
    "start": "836960",
    "end": "842090"
  },
  {
    "text": "production yourselves you'll know the requirements that you have from an editing perspective and their nuances of",
    "start": "842090",
    "end": "847760"
  },
  {
    "text": "your editors and the applications that they use in the on-prem storage and so this next section here is going to talk",
    "start": "847760",
    "end": "853460"
  },
  {
    "text": "I'm going to talk about the infrastructure components that we used for this event and I think what's unique",
    "start": "853460",
    "end": "858890"
  },
  {
    "text": "about the world-cup to say the men's and/or the women's is that it happens every four years it's in a it's in a",
    "start": "858890",
    "end": "865670"
  },
  {
    "text": "different country every time and it's extremely high profile very high very high cost to license the user experience",
    "start": "865670",
    "end": "872570"
  },
  {
    "text": "is increased every four years between SD HD u HD HD are many different aspects",
    "start": "872570",
    "end": "878600"
  },
  {
    "text": "and so the ways in which I think Fox and the partners that supported them and we",
    "start": "878600",
    "end": "884150"
  },
  {
    "text": "in turn supported them have sort of thought differently about approaching an event like this it's hopefully something",
    "start": "884150",
    "end": "889970"
  },
  {
    "text": "that can be reused by yourselves from a learning perspective and again that's why I think it's really exciting to be",
    "start": "889970",
    "end": "895010"
  },
  {
    "text": "able to talk about these type of things as we look forward about ways in which we can which we can think about live",
    "start": "895010",
    "end": "900740"
  },
  {
    "text": "production and or just live linear in general so just to recap the stats of the event",
    "start": "900740",
    "end": "906700"
  },
  {
    "text": "for weeks July 4th so June 14th to July 15th 64 games sourced in UHD HD are the",
    "start": "906700",
    "end": "916490"
  },
  {
    "text": "they wanted their Fox Sports wanted the ability to essentially edit within 10 seconds of something happening in Moscow",
    "start": "916490",
    "end": "922970"
  },
  {
    "text": "so that's in LA to have the feed to have the content to do an edit to reinsert",
    "start": "922970",
    "end": "927980"
  },
  {
    "text": "that back at the direction of the producer for that specific game or match",
    "start": "927980",
    "end": "933370"
  },
  {
    "text": "and then they accumulated two petabytes of content so that's those 64 games",
    "start": "933370",
    "end": "938450"
  },
  {
    "text": "recorded in their entirety plus different camera feeds all recorded and then 22,000 individual assets that were",
    "start": "938450",
    "end": "946400"
  },
  {
    "text": "also accumulated during the event so the trade-off here is do you build this infrastructure or do you use and",
    "start": "946400",
    "end": "952130"
  },
  {
    "text": "leverage something like the iris cloud to give you this flexibility in terms of scale and predictability what do you do",
    "start": "952130",
    "end": "959000"
  },
  {
    "text": "what has we had to be done which is you build for peak and you estimate and then you can you contract your SLA is based on the",
    "start": "959000",
    "end": "965510"
  },
  {
    "text": "capacity that you have so the business requirements as part of supporting this were equally complex so that what I",
    "start": "965510",
    "end": "972830"
  },
  {
    "text": "showed you before is what was achieved during the game going into the game they had these business requirements they",
    "start": "972830",
    "end": "979279"
  },
  {
    "text": "would have to receive all of those 64 match feeds in Moscow but the host broadcasting service and that will be",
    "start": "979279",
    "end": "985400"
  },
  {
    "text": "handed off in SDI and they would need to record all of those matches in real time in Los Angeles so that's that 6000 miles",
    "start": "985400",
    "end": "993440"
  },
  {
    "text": "10,000 kilometres 180 milliseconds away there was a requirement to archive",
    "start": "993440",
    "end": "998480"
  },
  {
    "text": "everything that came in there was a requirement to virus scan every single asset file based asset that came in and",
    "start": "998480",
    "end": "1005740"
  },
  {
    "text": "whether it came from a trusted or untrusted source before it reached a production environments and they had to",
    "start": "1005740",
    "end": "1011500"
  },
  {
    "text": "support the three editor home workflows which is the ability to do live editing and so here it says less than 30 seconds",
    "start": "1011500",
    "end": "1018160"
  },
  {
    "text": "because the original business SLA was to try and do a live effort live edit within 30 seconds of it happening",
    "start": "1018160",
    "end": "1023830"
  },
  {
    "text": "they actually achieved 10 through this through through what was created and engineered and they needed to support",
    "start": "1023830",
    "end": "1029199"
  },
  {
    "text": "live sub clipping and do postgame editing for all the marketing promos replays that might be done post match",
    "start": "1029199",
    "end": "1036339"
  },
  {
    "text": "and as part of the fan experience through mobile or web and things like that so when we look at the matrix of",
    "start": "1036339",
    "end": "1044050"
  },
  {
    "text": "solutions this slide is to is it meant to just give you an insight into some of the components that will be created so",
    "start": "1044050",
    "end": "1049660"
  },
  {
    "text": "we start at the top of the content production layer branded and mentioned that there editor of choice was Adobe",
    "start": "1049660",
    "end": "1055150"
  },
  {
    "text": "Premiere Pro and they had their production asset management system was levels beyond on-premise both in Moscow",
    "start": "1055150",
    "end": "1062679"
  },
  {
    "text": "and in LA they had harmonic media grid and Quan tell storage for different",
    "start": "1062679",
    "end": "1067780"
  },
  {
    "text": "aspects at the AWS component we were using core fundamental building blocks provided by the platform so Amazon ec2",
    "start": "1067780",
    "end": "1075090"
  },
  {
    "text": "Amazon s3 a diverse direct connects Amazon lambda sqs for some of the server",
    "start": "1075090",
    "end": "1081880"
  },
  {
    "text": "list functionality that we'll see shortly that was created to really add a dynamic workflow to the production",
    "start": "1081880",
    "end": "1089200"
  },
  {
    "text": "aspects and then below again driving the consumption all of the video transports of the data",
    "start": "1089200",
    "end": "1094970"
  },
  {
    "text": "plane from the video perspective and so that was transport provided by IBM espera transcoding and packaging by",
    "start": "1094970",
    "end": "1102350"
  },
  {
    "text": "provided by Telestream on-premise and then also transcoding packaging and orchestration provided by Telestream",
    "start": "1102350",
    "end": "1109880"
  },
  {
    "text": "both on-premise and through their cloud-based solution so these are just some of the components that will sort of",
    "start": "1109880",
    "end": "1115190"
  },
  {
    "text": "piece together as we look forward so you can look at this in terms of a matrix you've got a vertically integrated",
    "start": "1115190",
    "end": "1120950"
  },
  {
    "text": "editor to storage to content being written to that storage for production or you can go horizontally and look at",
    "start": "1120950",
    "end": "1127820"
  },
  {
    "text": "the network layer or we could look at the video layer and those are the things that we're gonna look at next if we look",
    "start": "1127820",
    "end": "1133940"
  },
  {
    "text": "at the grant apology on paper it looks super simple we've got Russia and we've got LA and we've got a long distance in between and",
    "start": "1133940",
    "end": "1140720"
  },
  {
    "text": "we've got to carry all of those live feeds all of those raw camera feeds all",
    "start": "1140720",
    "end": "1145789"
  },
  {
    "text": "of those files from one place to the other and meet these business ssl a's",
    "start": "1145789",
    "end": "1151399"
  },
  {
    "text": "what's interesting below this is actually how the video transport layer drove the want apology so this next",
    "start": "1151399",
    "end": "1156950"
  },
  {
    "text": "slide is is a slightly different view on this which is what's driving the consumption of the network and what",
    "start": "1156950",
    "end": "1162409"
  },
  {
    "text": "network decisions were made in order to transport the video that we knew was going to be handed off again there are",
    "start": "1162409",
    "end": "1168230"
  },
  {
    "text": "64 streams in SD with the camera feeds plus the files and in the top plan as",
    "start": "1168230",
    "end": "1173929"
  },
  {
    "text": "you look at it top right are the business SLA is around the production less than 30 seconds due sub clipping",
    "start": "1173929",
    "end": "1179630"
  },
  {
    "text": "allow for postgame editing so if we start off in Moscow we will receive at",
    "start": "1179630",
    "end": "1186350"
  },
  {
    "text": "hosts broadcasting services from 12 different venues across Russia all of the games will be played and",
    "start": "1186350",
    "end": "1191870"
  },
  {
    "text": "those will come in through SDI they're terminated by the Telestream appliance that has a sparrow integrated so at this",
    "start": "1191870",
    "end": "1198740"
  },
  {
    "text": "point we're doing an SDI to IP handoff in real-time output on the over IP now",
    "start": "1198740",
    "end": "1205220"
  },
  {
    "text": "is there live HLS proxies and the ISO camera feeds at the bit rates that brandon mentioned earlier that's going",
    "start": "1205220",
    "end": "1212419"
  },
  {
    "text": "to go across the wide area network and be terminated at the other end in the AWS cloud specifically here we're using",
    "start": "1212419",
    "end": "1219080"
  },
  {
    "text": "us West two which is our Portland or PDX region so in there we've got use me",
    "start": "1219080",
    "end": "1225539"
  },
  {
    "text": "Despero terminating the transport we've got it writing to s3 we've got",
    "start": "1225539",
    "end": "1231690"
  },
  {
    "text": "Telestream cloud reading from s3 and doing whatever processing is required for the for the workflow we've got a",
    "start": "1231690",
    "end": "1239100"
  },
  {
    "text": "next stage which is doing virus scanning on file based content which is also reading and writing from s3 and then",
    "start": "1239100",
    "end": "1245340"
  },
  {
    "text": "we've got the production asset management system which is pulling from s3 content that has been validated for",
    "start": "1245340",
    "end": "1250950"
  },
  {
    "text": "production and moved from staging into a production environment so you'll see here one of the key themes we talk about",
    "start": "1250950",
    "end": "1257489"
  },
  {
    "text": "a lot from the media and entertainment perspective which is a Content leak and what you see here is a great use of s3",
    "start": "1257489",
    "end": "1263009"
  },
  {
    "text": "for that which is it's agnostic to the workflow if you can support a read and write to s3 you've got multiple sources",
    "start": "1263009",
    "end": "1270029"
  },
  {
    "text": "reading and writing and creating a very dynamic scalable and changeable workflow",
    "start": "1270029",
    "end": "1275580"
  },
  {
    "text": "based on the business needs everything's reading and writing from the same storage point but all of those different processes could be provided by",
    "start": "1275580",
    "end": "1281669"
  },
  {
    "text": "different service providers in the case of here it's it's both a sparrow and an Telestream from the cloud perspective",
    "start": "1281669",
    "end": "1287090"
  },
  {
    "text": "we've got reach engine being run as a managed service and on-premise and we've got a fox run antivirus storage and",
    "start": "1287090",
    "end": "1294779"
  },
  {
    "text": "business process function all reading and writing and then the third area we've got now is the Fox Sports",
    "start": "1294779",
    "end": "1300570"
  },
  {
    "text": "headquarters in Los Angeles which is where we have our editors sitting with their workstations with local storage",
    "start": "1300570",
    "end": "1306779"
  },
  {
    "text": "all looking to receive their content during during the match and during the event and so this is the video workflow",
    "start": "1306779",
    "end": "1313859"
  },
  {
    "text": "that then we'll drive the network diagrams that we'll see next so what we",
    "start": "1313859",
    "end": "1319139"
  },
  {
    "text": "have here is a view of the network topology facilitating the transfer of this video and these files so if I look",
    "start": "1319139",
    "end": "1327929"
  },
  {
    "text": "at it from right to left if we start in Moscow we've got all of the content",
    "start": "1327929",
    "end": "1333029"
  },
  {
    "text": "coming in to the Moscow post broadcast services we've got those Telestream of Sparrow appliances transfer code at",
    "start": "1333029",
    "end": "1339830"
  },
  {
    "text": "transforming SDI to IP we've got a Direct Connect that goes from Moscow to",
    "start": "1339830",
    "end": "1345269"
  },
  {
    "text": "the AWS Frankfurt region and Fox chose a1 gear a 110 gigabit per second Direct",
    "start": "1345269",
    "end": "1351929"
  },
  {
    "text": "Connect Frankfurt to Moscow or must go to Frankfurt and in which where the traffic's going",
    "start": "1351929",
    "end": "1358419"
  },
  {
    "text": "and then we've got the a diverse backbone which carries that traffic to the AWS us west to region which is the",
    "start": "1358419",
    "end": "1365230"
  },
  {
    "text": "Portland region then we've got connecting from Portland to LA to 10-gig",
    "start": "1365230",
    "end": "1370570"
  },
  {
    "text": "links via direct connects that actually wrapped through Las Vegas here from from the direct connect pop perspective so",
    "start": "1370570",
    "end": "1377230"
  },
  {
    "text": "this creates the network backbone for which the video and the file based content was transferring and so there",
    "start": "1377230",
    "end": "1383230"
  },
  {
    "text": "were trade offs made here as to redundancy so I think from a cost and business perspective one gig was chosen",
    "start": "1383230",
    "end": "1388720"
  },
  {
    "text": "for the Frankfurt's who have for Moscow to Frankfurt and because of Fox's wider",
    "start": "1388720",
    "end": "1393759"
  },
  {
    "text": "use of AWS they had the two redundant 10 gig links between LA and the AWS us West",
    "start": "1393759",
    "end": "1400210"
  },
  {
    "text": "regions so as a backup you can default to public IP as the default transport",
    "start": "1400210",
    "end": "1407049"
  },
  {
    "text": "should your primary links fail and so there was public IP transit routes that were put in place to do both from Moscow",
    "start": "1407049",
    "end": "1413169"
  },
  {
    "text": "to Frankfurt and also from Moscow straight to LA on the lot and so this",
    "start": "1413169",
    "end": "1418450"
  },
  {
    "text": "way there was a ability to balance cost and short term high bandwidth infrastructure especially there's direct",
    "start": "1418450",
    "end": "1424749"
  },
  {
    "text": "connects only for the duration of the event but allowing you to have redundancy and failover built into the",
    "start": "1424749",
    "end": "1432580"
  },
  {
    "text": "system the other the other decision taken here was could you when you look at file based workloads or live based",
    "start": "1432580",
    "end": "1439450"
  },
  {
    "text": "workflows can you work in a proxy environment so if you could default to if your primary connections fail and you",
    "start": "1439450",
    "end": "1445960"
  },
  {
    "text": "have to switch the public IP perhaps your bandwidth is lower but if you've got a proxy based workflow perhaps",
    "start": "1445960",
    "end": "1451239"
  },
  {
    "text": "that's so that's all that allows you flexibility and a design decision and a cost decision as you're beginning to",
    "start": "1451239",
    "end": "1456639"
  },
  {
    "text": "build out your systems and so this way you have both you have primary over direct connects and over high bandwidth",
    "start": "1456639",
    "end": "1462249"
  },
  {
    "text": "links and then you have secondary failures failover is over a public IP so",
    "start": "1462249",
    "end": "1468100"
  },
  {
    "text": "as we look at this in a little bit more detail we've got multiple components and Demark points that set up the",
    "start": "1468100",
    "end": "1473200"
  },
  {
    "text": "infrastructure so here I'm going to do draw lines right so we've got the host broadcast services from which the Direct",
    "start": "1473200",
    "end": "1479980"
  },
  {
    "text": "Connect is egressing to our AWS Frankfurt region and so that is a that",
    "start": "1479980",
    "end": "1485289"
  },
  {
    "text": "is a dedicated line at 10 gigabits per second which is from which we are rerouting that traffic",
    "start": "1485289",
    "end": "1491580"
  },
  {
    "text": "now to the Portland region over the ADA Brewers backbone and so that is using Direct Connect gateway it's using direct",
    "start": "1491580",
    "end": "1499080"
  },
  {
    "text": "connects as the ingest points and we're using the a we're using BGP routing to ensure that we're getting from source to",
    "start": "1499080",
    "end": "1505500"
  },
  {
    "text": "destination which in this case is the Portland region whereas para is terminating that IP transfer and writing",
    "start": "1505500",
    "end": "1511440"
  },
  {
    "text": "to s3 in in s3 in Portland we've then got a secondary two gig to Tang gig",
    "start": "1511440",
    "end": "1518910"
  },
  {
    "text": "links that come out of Portland to the Fox lot in Los Angeles and those are the",
    "start": "1518910",
    "end": "1524610"
  },
  {
    "text": "redundant 210 gig links and so these are the de mark points that were constructed",
    "start": "1524610",
    "end": "1530430"
  },
  {
    "text": "to facilitate the live and file based transfer of content as you moved from Moscow to LA to support those editors",
    "start": "1530430",
    "end": "1537450"
  },
  {
    "text": "and to support the in-game production requirements that were am meant that were required as part of the experience",
    "start": "1537450",
    "end": "1543720"
  },
  {
    "text": "that Fox was looking to deliver to its audience and so here within these lines you also see failover points so you'll",
    "start": "1543720",
    "end": "1550770"
  },
  {
    "text": "see between and the lower hat part of the screen between Moscow and LA you'll see a VPN connection that can be failed",
    "start": "1550770",
    "end": "1557370"
  },
  {
    "text": "over too so all of the routes all of the way this was constructed allowed for multiple fail overs with different cost",
    "start": "1557370",
    "end": "1564210"
  },
  {
    "text": "profiles right so again we've got one 10 gig link that allows us to failover to IP we've got two 10 gig links in the US",
    "start": "1564210",
    "end": "1570330"
  },
  {
    "text": "we've got multiple paths that the traffic can move between and we've also got flexible workflows that will allow",
    "start": "1570330",
    "end": "1576360"
  },
  {
    "text": "us to to to modify in real time potentially what can be done and what",
    "start": "1576360",
    "end": "1581490"
  },
  {
    "text": "needs to be done should network connectivity go down within now the AWS",
    "start": "1581490",
    "end": "1587390"
  },
  {
    "text": "region in Portland we've got now the AWS architecture piece so this is now gluing",
    "start": "1587390",
    "end": "1593310"
  },
  {
    "text": "together the different V pcs that are used and configured and stood up to",
    "start": "1593310",
    "end": "1598410"
  },
  {
    "text": "support the applications that we've spoken about through this session so within the V PC the main V PC in",
    "start": "1598410",
    "end": "1604890"
  },
  {
    "text": "Portland we've got multiple V pcs that host production and development environments and so for the reach engine",
    "start": "1604890",
    "end": "1612510"
  },
  {
    "text": "deployment they had production and development which allowed them to test the way in which they were going to do both file and live based workflows and",
    "start": "1612510",
    "end": "1620400"
  },
  {
    "text": "how that was going to be presented to their editors had the asset management system going to register assets which I will",
    "start": "1620400",
    "end": "1625510"
  },
  {
    "text": "get into later we had a V PC environment for the espera components a V PC",
    "start": "1625510",
    "end": "1631240"
  },
  {
    "text": "environment for the Telestream components and then we were using standard building blocks like Amazon s3",
    "start": "1631240",
    "end": "1637539"
  },
  {
    "text": "and ADA BIOS lambda and simple queuing service to really build the glue and the",
    "start": "1637539",
    "end": "1643179"
  },
  {
    "text": "orchestration around the workflow so that you could automate a lot of this from end to end and those be pcs were",
    "start": "1643179",
    "end": "1648940"
  },
  {
    "text": "all running in Portland with all of that traffic for the most part being routed through Frankfurt over there diverse",
    "start": "1648940",
    "end": "1654820"
  },
  {
    "text": "backbone to the to the to the u.s. West to region and then in green you'll see",
    "start": "1654820",
    "end": "1660220"
  },
  {
    "text": "the different paths that we have to read and write data so everyone needs different everyone needs access to s3 so",
    "start": "1660220",
    "end": "1666279"
  },
  {
    "text": "you've got that either directly if you're in the same region or over public IP or over direct connects you've got",
    "start": "1666279",
    "end": "1671950"
  },
  {
    "text": "the LA facility for foxsports connected and then also with failover and then you've got direct connects so",
    "start": "1671950",
    "end": "1678490"
  },
  {
    "text": "you've got you've got a network connectivity between Russia and P and LA but that's over public IP so what you",
    "start": "1678490",
    "end": "1685299"
  },
  {
    "text": "see here is a network design that supports redundancy within the file based live workflows and then the V pcs",
    "start": "1685299",
    "end": "1693159"
  },
  {
    "text": "that are constructed to then run the different components and the different service provider technologies to",
    "start": "1693159",
    "end": "1699220"
  },
  {
    "text": "actually glue everything together and so this this now so this explains hopefully",
    "start": "1699220",
    "end": "1704590"
  },
  {
    "text": "a little bit about the network infrastructure what was powering it in terms of the video and live and then the way in which the V pcs were constructed",
    "start": "1704590",
    "end": "1711399"
  },
  {
    "text": "to ensure that each different application provider had a segregated for permission-based",
    "start": "1711399",
    "end": "1716559"
  },
  {
    "text": "control over the concert that they were writing so if we move from the live",
    "start": "1716559",
    "end": "1723039"
  },
  {
    "text": "piece to now the file based workflow what I'd mentioned earlier and Brandon mentioned also that and cumulatively",
    "start": "1723039",
    "end": "1728830"
  },
  {
    "text": "over the course of the event two hundred and twenty twenty two thousand assets",
    "start": "1728830",
    "end": "1733950"
  },
  {
    "text": "were had to be won through antivirus scanning before they entered the production network so that was every",
    "start": "1733950",
    "end": "1740529"
  },
  {
    "text": "piece of file based content and I think a lot of the content here is around metadata clipping source from different",
    "start": "1740529",
    "end": "1746740"
  },
  {
    "text": "people but it's all to support the production and allow the producers and the editors to know what components are",
    "start": "1746740",
    "end": "1753520"
  },
  {
    "text": "necessary for what tasks they're going to do and so to build an infrastructure to do to",
    "start": "1753520",
    "end": "1759530"
  },
  {
    "text": "22,000 assets of antivirus scanning requires quite a little compute and a lot of transitory storage but it's also",
    "start": "1759530",
    "end": "1766170"
  },
  {
    "text": "essential from a design perspective to air-gap that ant the content prior to",
    "start": "1766170",
    "end": "1771809"
  },
  {
    "text": "being antivirus being a virus checks and then moving it into production so why",
    "start": "1771809",
    "end": "1777420"
  },
  {
    "text": "not do this on premise so a number of reasons so the three main ones were firstly this was a peak event for weeks",
    "start": "1777420",
    "end": "1783720"
  },
  {
    "text": "if you and for the duration of the event there was limited on-site capacity so it",
    "start": "1783720",
    "end": "1789540"
  },
  {
    "text": "could have been done on premise but that would have compromised the exist has been existing business conditions and",
    "start": "1789540",
    "end": "1794700"
  },
  {
    "text": "then again it would there is a need necessary need given the source of the content which is unknown even if it's",
    "start": "1794700",
    "end": "1801120"
  },
  {
    "text": "REM trusted contributors to do a full antivirus before you bring it into production so what was built was a",
    "start": "1801120",
    "end": "1807990"
  },
  {
    "text": "mechanism to automate this fully in the u.s. west to region so that was",
    "start": "1807990",
    "end": "1813990"
  },
  {
    "text": "essentially writing everything to s3 it being quarantined by default and then running antivirus against that content",
    "start": "1813990",
    "end": "1821400"
  },
  {
    "text": "and then moving it into production and so that's what I'm going to talk about next so what they--what Fox did with its",
    "start": "1821400",
    "end": "1827490"
  },
  {
    "text": "engineering team for sports did was to automate this antivirus workflow using multiple components of civilus and",
    "start": "1827490",
    "end": "1834020"
  },
  {
    "text": "computing storage and so this is the run-through of what the antivirus",
    "start": "1834020",
    "end": "1840440"
  },
  {
    "text": "workflow looks like so we've got a sparrow reading files in moscow writing",
    "start": "1840440",
    "end": "1846120"
  },
  {
    "text": "them sending them over the wide area network over direct connects and landing them in s3 so step one is a sparrow is",
    "start": "1846120",
    "end": "1853350"
  },
  {
    "text": "going to write the files these 22,000 assets to s3 we're going to use s3 notifications to",
    "start": "1853350",
    "end": "1859440"
  },
  {
    "text": "trigger various things post whites and so wesley notifications reduce drastically reduces the complexity in",
    "start": "1859440",
    "end": "1865620"
  },
  {
    "text": "determining if something's there you don't have to scan the folder you can just listen out for notification and you can do the next steps so in the case of",
    "start": "1865620",
    "end": "1873320"
  },
  {
    "text": "this antivirus scan the next step was to create a queue the queuing system so",
    "start": "1873320",
    "end": "1879630"
  },
  {
    "text": "similar queuing service to basically create a queue that triggered a new asset list and so as each asset was",
    "start": "1879630",
    "end": "1886770"
  },
  {
    "text": "written SQS through the s3 notification will be a dated with the new asset that asset was",
    "start": "1886770",
    "end": "1892560"
  },
  {
    "text": "then transferred to ec2 where the antivirus scan was run that ec2 group as",
    "start": "1892560",
    "end": "1898800"
  },
  {
    "text": "it was in its own V PC so was the right access to s3 so that you can control",
    "start": "1898800",
    "end": "1904950"
  },
  {
    "text": "both within the ec2 there was an auto scaling fleet to basically scale up the ec2 processing nodes to process the",
    "start": "1904950",
    "end": "1912360"
  },
  {
    "text": "queue based on incoming traffic and what's unpredictable areas you don't know how much content is going to come",
    "start": "1912360",
    "end": "1917880"
  },
  {
    "text": "in every hour it may be a very popular game there might be a lot of metadata if it's a very popular team is likely a lot",
    "start": "1917880",
    "end": "1924480"
  },
  {
    "text": "of information that comes in about the players both historical and current and so that alas the the auto scaling group",
    "start": "1924480",
    "end": "1931530"
  },
  {
    "text": "would scale up and down based on the sq sq to do an anti-virus scan once the",
    "start": "1931530",
    "end": "1937380"
  },
  {
    "text": "antivirus scan was done the results of that were sent to ADA breast lambda and iris lambda processed that result to",
    "start": "1937380",
    "end": "1944910"
  },
  {
    "text": "determine what the next step should be so did the antivirus pass were there any flags could we move that asset from",
    "start": "1944910",
    "end": "1951030"
  },
  {
    "text": "quarantined into production and so sqs so once lambda processed that output",
    "start": "1951030",
    "end": "1957270"
  },
  {
    "text": "file it would update a secondary queue to do the antivirus pass so now you've got a list of assets in real time that",
    "start": "1957270",
    "end": "1965340"
  },
  {
    "text": "have passed antivirus that are now picked up by the asset management system and so what's unique here is that we're",
    "start": "1965340",
    "end": "1971640"
  },
  {
    "text": "using one s3 bucket we're not worried about transferring between buckets and",
    "start": "1971640",
    "end": "1976680"
  },
  {
    "text": "we're only going to register assets in the asset management system that have passed an anti-virus scan but everything",
    "start": "1976680",
    "end": "1982200"
  },
  {
    "text": "is in the same s3 bucket to begin with so that's actually it's a simple way to do a very complex task without having to",
    "start": "1982200",
    "end": "1987990"
  },
  {
    "text": "do a lot of complex rule management specifically so the secondary queue",
    "start": "1987990",
    "end": "1993120"
  },
  {
    "text": "would be something that the reach engine production asset management system would listen out for it would listen out for",
    "start": "1993120",
    "end": "1998190"
  },
  {
    "text": "assets that it could register in its system it would register those assets and those assets would then be presented",
    "start": "1998190",
    "end": "2004040"
  },
  {
    "text": "to the production systems as assets that were valid for production and not",
    "start": "2004040",
    "end": "2009440"
  },
  {
    "text": "quarantined or pre virus scan and so this system was built uniquely to do",
    "start": "2009440",
    "end": "2014480"
  },
  {
    "text": "this antivirus scanning offloading all of the infrastructure requirements on premise but doing it in a fully",
    "start": "2014480",
    "end": "2020000"
  },
  {
    "text": "automated way so everything here is is trigger based it's got queuing mechanisms it's using",
    "start": "2020000",
    "end": "2025250"
  },
  {
    "text": "auto-scaling it's integrating with third parties specifically a sparrow on the right and reach engine is the asset",
    "start": "2025250",
    "end": "2031490"
  },
  {
    "text": "management system from the production perspective and it's allowing a very scalable high scale short term",
    "start": "2031490",
    "end": "2038480"
  },
  {
    "text": "environment to be stood up and what this actually represents is potentially a different way of doing many of these things from a live production",
    "start": "2038480",
    "end": "2043940"
  },
  {
    "text": "perspective again because your primary concern here is twofold one is the content secure and there was also a",
    "start": "2043940",
    "end": "2050840"
  },
  {
    "text": "decision made that all live content was secure because the appliances and the feeds were controlled by the customer",
    "start": "2050840",
    "end": "2057169"
  },
  {
    "text": "any any Content that came in from third parties and by default was going to go",
    "start": "2057169",
    "end": "2062270"
  },
  {
    "text": "through this mechanism and so in these last few slides have gone through many things one is the wide area network one",
    "start": "2062270",
    "end": "2068840"
  },
  {
    "text": "was what drove the video transports to consume the networking how the networking was designed and what",
    "start": "2068840",
    "end": "2074030"
  },
  {
    "text": "failover components were there and then how in parallel the file based workflow was built to drive a service workflow to",
    "start": "2074030",
    "end": "2082010"
  },
  {
    "text": "ensure security and timeliness around the processing of that content and the availability of that content in the",
    "start": "2082010",
    "end": "2088070"
  },
  {
    "text": "asset management system so next I'm gonna hand over also so one final piece is that you'll this yellow line sorry",
    "start": "2088070",
    "end": "2094580"
  },
  {
    "text": "it's going back to s3 so you'll see s3 is still the same common source it's the content Lake for this entire workflow",
    "start": "2094580",
    "end": "2101000"
  },
  {
    "text": "still we're not moving necessarily between buckets we're not making more complex we're leveraging the",
    "start": "2101000",
    "end": "2106280"
  },
  {
    "text": "notification systems and the asset management system to really drive the logic that is presented to the",
    "start": "2106280",
    "end": "2111740"
  },
  {
    "text": "production people so next we're going to talk about the video transport and processing so this is what fills the",
    "start": "2111740",
    "end": "2117320"
  },
  {
    "text": "pipes and fills the s3 storage and delivers those assets as part of the final integration and so with that I'm",
    "start": "2117320",
    "end": "2123650"
  },
  {
    "text": "gonna hand over to Mike who will take us through part three of this presentation",
    "start": "2123650",
    "end": "2130190"
  },
  {
    "text": "thanks Babak alright so what you know",
    "start": "2130190",
    "end": "2142860"
  },
  {
    "text": "we're gonna talk in a little bit of detail about you know what we did to actually overcome some of the",
    "start": "2142860",
    "end": "2148290"
  },
  {
    "text": "limitations that that we actually you know had going from from Moscow to the west coast of the US so as Ben mentioned",
    "start": "2148290",
    "end": "2155460"
  },
  {
    "text": "you know we actually brought a bunch of content back to Fox Sports facility you",
    "start": "2155460",
    "end": "2160830"
  },
  {
    "text": "know in LA but then a bunch into into s3 directly as well with in that was Babak",
    "start": "2160830",
    "end": "2167880"
  },
  {
    "text": "mentioned that was us West and Oregon so I just want to recap a few of the",
    "start": "2167880",
    "end": "2174000"
  },
  {
    "text": "challenges here so time and distance Brandon talked about this you know there and I'm going to talk a little bit about",
    "start": "2174000",
    "end": "2180060"
  },
  {
    "text": "how time and distance effects you know some of the technical challenges that that that surround that with regard to",
    "start": "2180060",
    "end": "2186270"
  },
  {
    "text": "actually getting moving the data security of course Bob just went through you know a very good explanation of the",
    "start": "2186270",
    "end": "2192390"
  },
  {
    "text": "detailed workflows for a virus scanning and that kind of thing and then you know",
    "start": "2192390",
    "end": "2198840"
  },
  {
    "text": "that content management as well I will I will say that you know there you know",
    "start": "2198840",
    "end": "2204390"
  },
  {
    "text": "22,000 assets there's a lot of a lot of files to be moved a lot of data to be moved and going into this we we thought",
    "start": "2204390",
    "end": "2213030"
  },
  {
    "text": "that might be transferring about one you know maybe one and a half 1.7 petabytes",
    "start": "2213030",
    "end": "2219350"
  },
  {
    "text": "between Moscow and and Russia are between Russia and and the west coast",
    "start": "2219350",
    "end": "2225030"
  },
  {
    "text": "the US and what happened was was we ended up with about two petabytes we",
    "start": "2225030",
    "end": "2230250"
  },
  {
    "text": "only had one petabyte of storage on-site so as you could see that we absolutely had to get a lot of that content off we",
    "start": "2230250",
    "end": "2236820"
  },
  {
    "text": "didn't have room to store at all yeah as we went throughout the course of the tournament so let's talk about you know",
    "start": "2236820",
    "end": "2244500"
  },
  {
    "text": "the time and distance as it relates to TCP based technologies you know the TCP pays technologies it's just a fraction",
    "start": "2244500",
    "end": "2250950"
  },
  {
    "text": "though you can only get a fraction of the throughput that might be available because of packet loss and latency so",
    "start": "2250950",
    "end": "2257640"
  },
  {
    "text": "anytime you have distance you know packet loss increases latency increases it's just a just a",
    "start": "2257640",
    "end": "2262860"
  },
  {
    "text": "factor with regard to distance and TCP of course wants an acknowledgment for every packet it's in right so and and",
    "start": "2262860",
    "end": "2270720"
  },
  {
    "text": "has this thing called a window size and this TCP window size is basically allows",
    "start": "2270720",
    "end": "2276270"
  },
  {
    "text": "TCP the protocol itself to actually to acknowledge or affirm packet reception",
    "start": "2276270",
    "end": "2282720"
  },
  {
    "text": "with just a single a single roundtrip so you know so the this window size",
    "start": "2282720",
    "end": "2290040"
  },
  {
    "text": "actually gets you by you a little bit but what happens is is as because of one",
    "start": "2290040",
    "end": "2295320"
  },
  {
    "text": "because TCP wants an acknowledgment for B packet it sends the wind as it runs",
    "start": "2295320",
    "end": "2301170"
  },
  {
    "text": "the situation where it's not getting acknowledgment the timeframe that it wants it in starts shrinking this window size effectively artificially",
    "start": "2301170",
    "end": "2307290"
  },
  {
    "text": "constrained your bandwidth so we can actually look at a pretty simple formula to the we use quite often to look at the",
    "start": "2307290",
    "end": "2314760"
  },
  {
    "text": "obtainable throughput with TCP and that's the number of the bit through second throughput is equal to the window",
    "start": "2314760",
    "end": "2320760"
  },
  {
    "text": "size in bits divided by the latency in seconds so if we look at an example here",
    "start": "2320760",
    "end": "2326790"
  },
  {
    "text": "not unlike the conditions that we had in in in Russia between Russia and and the",
    "start": "2326790",
    "end": "2332610"
  },
  {
    "text": "west coast of the US if you consider about 200 milliseconds of latency 2% packet lost and it's 64 KTP window size",
    "start": "2332610",
    "end": "2339420"
  },
  {
    "text": "and if you look at the formula down at the bottom you can see that we're only able to get with over that distance",
    "start": "2339420",
    "end": "2344640"
  },
  {
    "text": "about you know 2.6 2 megabits a second ok now so we have to overcome that",
    "start": "2344640",
    "end": "2350880"
  },
  {
    "text": "somehow and then factor in packet loss and you and and your even worse so again just by looking at this",
    "start": "2350880",
    "end": "2356940"
  },
  {
    "text": "equation here you can actually tell that increasing bandwidth isn't gonna do us any good we can increase bandwidth till",
    "start": "2356940",
    "end": "2363870"
  },
  {
    "text": "the cows come home and we're still not going to to help our actual throughput so what can we do so you know in all",
    "start": "2363870",
    "end": "2371730"
  },
  {
    "text": "three of these things here under what can be done they all kind of work together so one of the first thing we",
    "start": "2371730",
    "end": "2377790"
  },
  {
    "text": "can do is minimize startup time so the amount of time that you know that it takes to actually start processing the",
    "start": "2377790",
    "end": "2383340"
  },
  {
    "text": "data it's been mentioned that that we had a couple of workflows I'll talk about those a little bit of detail one",
    "start": "2383340",
    "end": "2389550"
  },
  {
    "text": "of those being a live at it so camera to edit we the requirement was 30 seconds as you",
    "start": "2389550",
    "end": "2395610"
  },
  {
    "text": "could tell but we ended up being able to do that in less than 10 seconds you know from Moscow to you know to LA",
    "start": "2395610",
    "end": "2402380"
  },
  {
    "text": "the second thing is minimize the bandwidth overhead so you know what we",
    "start": "2402380",
    "end": "2408030"
  },
  {
    "text": "want to do is may is make that bandwidth overhead as close to the actual packet loss as possible you know there are",
    "start": "2408030",
    "end": "2414570"
  },
  {
    "text": "technologies like forward error correction that actually you know send extra data to kind of try and you know",
    "start": "2414570",
    "end": "2419940"
  },
  {
    "text": "you know assure that you have some padding there but you know what that that's not good enough for us we in fact",
    "start": "2419940",
    "end": "2426360"
  },
  {
    "text": "for error correction typically is about 25 times more payload than your our fast",
    "start": "2426360",
    "end": "2432420"
  },
  {
    "text": "protocol is so I'm not going to talk a lot about about our protocol but you know that that's the big thing is we want to minimize that overhead and then",
    "start": "2432420",
    "end": "2439380"
  },
  {
    "text": "reduce the number of retransmissions if we start we start pounding the network and we can't get the the content there",
    "start": "2439380",
    "end": "2445710"
  },
  {
    "text": "as fast no matter what you're using you're putting extra data you're putting extra dated there on the net on the on",
    "start": "2445710",
    "end": "2452340"
  },
  {
    "text": "the network itself and so you're contributing to the problem right you're exasperated the problem so the things",
    "start": "2452340",
    "end": "2458040"
  },
  {
    "text": "you want to do is is kind of you know apply things at a constant and steady",
    "start": "2458040",
    "end": "2463110"
  },
  {
    "text": "rate that that's sustainable ok so just",
    "start": "2463110",
    "end": "2468810"
  },
  {
    "text": "continuing with time and distance here so optimizing the the pipeline you know",
    "start": "2468810",
    "end": "2473850"
  },
  {
    "text": "we we don't want to we didn't want to bring the assets in and stage them you know for example we talked directly into",
    "start": "2473850",
    "end": "2479940"
  },
  {
    "text": "s3 we don't want to just stage them you know on a file system and then move them into s3 you know as these files are",
    "start": "2479940",
    "end": "2485970"
  },
  {
    "text": "growing as we're bringing these em you want to write them to wanted to write them directly to s3 so with that it's",
    "start": "2485970",
    "end": "2492690"
  },
  {
    "text": "something that we've spent a lot of time and effort making sure that that we can eat the best performance out that we can",
    "start": "2492690",
    "end": "2499650"
  },
  {
    "text": "and so we of course use the s3 multi-part API to actually spin off the",
    "start": "2499650",
    "end": "2507000"
  },
  {
    "text": "optimal number of threads and so forth to try and maximize throughput I'm going to talk a little bit about the you know",
    "start": "2507000",
    "end": "2513630"
  },
  {
    "text": "you know per server you know per instances and you know through ports of",
    "start": "2513630",
    "end": "2518790"
  },
  {
    "text": "throughput and that kind of thing and there's just some things you got to keep in mind here you know next is you know",
    "start": "2518790",
    "end": "2525240"
  },
  {
    "text": "direct server to server transmission of of course you don't want to have you know kind of a stopping-off point you",
    "start": "2525240",
    "end": "2530850"
  },
  {
    "text": "know in between you want to make these transfers happen from location a to location B so we had servers in Moscow",
    "start": "2530850",
    "end": "2536940"
  },
  {
    "text": "and reach engine asset management system was actually sending API requests to the",
    "start": "2536940",
    "end": "2544200"
  },
  {
    "text": "the servers between Moscow and and US West in Oregon and was actually you know",
    "start": "2544200",
    "end": "2551340"
  },
  {
    "text": "initiating service server transfer so it doesn't have to go through a central point we're just saying ok server in",
    "start": "2551340",
    "end": "2556380"
  },
  {
    "text": "Moscow transferred to directly into s3 or vice versa and then the next thing",
    "start": "2556380",
    "end": "2562290"
  },
  {
    "text": "and I can't emphasize this enough you know that you know we don't want bespoke workflows we want what we tried to make",
    "start": "2562290",
    "end": "2569400"
  },
  {
    "text": "it so we have the you know direct API integration there's direct API integration goes for not only not only",
    "start": "2569400",
    "end": "2576510"
  },
  {
    "text": "that the the integration with the asset management system but also with the integration it with Telestream",
    "start": "2576510",
    "end": "2583050"
  },
  {
    "text": "Lightspeed live life life captures the Telestream product that actually took the AC SDI feeds in and produced and",
    "start": "2583050",
    "end": "2589650"
  },
  {
    "text": "produced these these growing files I'm going to talk a little bit about you know how we did that because it's kind of that ended up being kind of cool as",
    "start": "2589650",
    "end": "2595710"
  },
  {
    "text": "well so if we look at you know when transfer speed and what we could get out",
    "start": "2595710",
    "end": "2601320"
  },
  {
    "text": "of per each instance so if you take a c-38 extra-extra-large instance they you",
    "start": "2601320",
    "end": "2608370"
  },
  {
    "text": "you have limits on what you can actually push through that instance on a per stream basis so we can only get about",
    "start": "2608370",
    "end": "2613710"
  },
  {
    "text": "2.8 get you know gigabits a second per stream coming in doesn't matter where the origin is from you've got each",
    "start": "2613710",
    "end": "2619470"
  },
  {
    "text": "stream you can only get AM active about 2.1 give us per stream now you know",
    "start": "2619470",
    "end": "2625890"
  },
  {
    "text": "these are actually these actually came from actual performance numbers and we've got to moscow we actually you know",
    "start": "2625890",
    "end": "2631830"
  },
  {
    "text": "that's one of the points that i'll make a little later on is that that you know expect the unexpected so when we got",
    "start": "2631830",
    "end": "2638190"
  },
  {
    "text": "there we we actually you know tested and and a lot of optimization and looked at",
    "start": "2638190",
    "end": "2643470"
  },
  {
    "text": "things and to see what we actually got the staging environment in charlotte that the brandon mentioned was also you",
    "start": "2643470",
    "end": "2650100"
  },
  {
    "text": "know very very helpful so then with a single machine though and multiple sessions we can only get a",
    "start": "2650100",
    "end": "2656790"
  },
  {
    "text": "maximum we can only get them active about three and a half gig if it's a second so you know we more than that who so what could we do",
    "start": "2656790",
    "end": "2662819"
  },
  {
    "text": "right what so what we ended up doing is it's not only you not only have to scale",
    "start": "2662819",
    "end": "2667890"
  },
  {
    "text": "out the back end the number of servers except things you know on the on the server side but also on the local side",
    "start": "2667890",
    "end": "2674729"
  },
  {
    "text": "as well and so we we actually stood up three servers three three servers in in",
    "start": "2674729",
    "end": "2681149"
  },
  {
    "text": "Moscow and we kind of balanced the throughput and it's not just about dawn seeing that we can go into it if you",
    "start": "2681149",
    "end": "2686309"
  },
  {
    "text": "want to you know see me afterwards after the table we can do some QA and I can I can explain some of the things that we",
    "start": "2686309",
    "end": "2692459"
  },
  {
    "text": "did to optimize things there because it's not just about how many transfers does it does a machine have going on or",
    "start": "2692459",
    "end": "2699209"
  },
  {
    "text": "how many sessions isn't that kind of thing there's there's a few tricks that you can do there so now security again",
    "start": "2699209",
    "end": "2707549"
  },
  {
    "text": "you know it's very important not only the security around the the virus",
    "start": "2707549",
    "end": "2715319"
  },
  {
    "text": "scanning but also you know you know session authentication so with we",
    "start": "2715319",
    "end": "2720719"
  },
  {
    "text": "actually have such an authentication which is you know users SSA so Triple DES was used their transfer",
    "start": "2720719",
    "end": "2726569"
  },
  {
    "text": "authorization using tokens and access keys you know and so forth so we actually made sure we had a very secure",
    "start": "2726569",
    "end": "2733319"
  },
  {
    "text": "environment and then of course the data itself was encrypted you know in transit so you know using AAS encryption and",
    "start": "2733319",
    "end": "2741509"
  },
  {
    "text": "then also you know with with the protocol you know there's a md5 hash that's applied to each Datagram and then",
    "start": "2741509",
    "end": "2747899"
  },
  {
    "text": "that's verified on on the other end so we're we actually verify the integrity of that data as it's being moved as well",
    "start": "2747899",
    "end": "2755209"
  },
  {
    "text": "I thought I did mention you know API integration like I said with reach",
    "start": "2755209",
    "end": "2760529"
  },
  {
    "text": "engine it's very you know important that we had that that tight integration as",
    "start": "2760529",
    "end": "2765929"
  },
  {
    "text": "opposed to you know something something bespoke like I said already we had",
    "start": "2765929",
    "end": "2771209"
  },
  {
    "text": "service server transfers we have the three servers in Moscow and then we auto scaled the cluster you know I you know",
    "start": "2771209",
    "end": "2778529"
  },
  {
    "text": "in AWS and then you know in in LA we had",
    "start": "2778529",
    "end": "2783539"
  },
  {
    "text": "you know stance toward shared sand storage that was actually used for that you know full res live editing just a",
    "start": "2783539",
    "end": "2789389"
  },
  {
    "text": "couple of details about the live at its they were 220 megabit video streams you had a do to that you're",
    "start": "2789389",
    "end": "2795030"
  },
  {
    "text": "pushing 250 megabits a second and we had a one gig it was mentioned there was a",
    "start": "2795030",
    "end": "2800070"
  },
  {
    "text": "one gig pipe that that this was running over so and they're doing about you know",
    "start": "2800070",
    "end": "2805740"
  },
  {
    "text": "three streams per match so we were and we're QoS at about 800 megabits so you",
    "start": "2805740",
    "end": "2811560"
  },
  {
    "text": "see we didn't have a lot of overhead there and we were able to keep up live and again I'll talk a little bit more in",
    "start": "2811560",
    "end": "2818430"
  },
  {
    "text": "a second about how we did that the key design principles you know just optimize",
    "start": "2818430",
    "end": "2824520"
  },
  {
    "text": "optimize optimize you know we like I said we got to to Moscow we actually had",
    "start": "2824520",
    "end": "2829950"
  },
  {
    "text": "the first couple of days we actually we kept popping the circuit right you're",
    "start": "2829950",
    "end": "2835440"
  },
  {
    "text": "pushing 10 pushing close to 10 gig give it a second through it but we kept you'll pop in the circuit and it turned out to be a network provider you know",
    "start": "2835440",
    "end": "2842160"
  },
  {
    "text": "issue there and and finally got that worked out but you know you never know you never know what you're gonna run into we talked about the integration and",
    "start": "2842160",
    "end": "2850410"
  },
  {
    "text": "I and I'm a big fan of API integrations focused most a lot of my career at is Berra around helping people integrate",
    "start": "2850410",
    "end": "2857130"
  },
  {
    "text": "our technology into their applications and workflows and the reason is that I",
    "start": "2857130",
    "end": "2862950"
  },
  {
    "text": "I'm so passionate about it is because you know we have you you want that",
    "start": "2862950",
    "end": "2868650"
  },
  {
    "text": "feedback you know you have a bunch of different pieces of hardware and a bunch of different you know workflows and and",
    "start": "2868650",
    "end": "2875400"
  },
  {
    "text": "pieces of software and so forth and you want that feedback all the weight to be able to push back all the way through to the source as well so you can adjust so",
    "start": "2875400",
    "end": "2883050"
  },
  {
    "text": "you know for for example you know as we",
    "start": "2883050",
    "end": "2888270"
  },
  {
    "text": "are with a couple that there's two main workflows that I'll talk about one of the live edit workflow the other one is",
    "start": "2888270",
    "end": "2894300"
  },
  {
    "text": "our live sub sub flipping workflow that was done with both of those it wasn't",
    "start": "2894300",
    "end": "2899340"
  },
  {
    "text": "just enough to just assume we were going to get everything there I'm happy to say that we actually kept up much much",
    "start": "2899340",
    "end": "2905760"
  },
  {
    "text": "better than I ever thought we would by the time the you know World Cup was over we basically had all the content",
    "start": "2905760",
    "end": "2911910"
  },
  {
    "text": "off-site and and and you know up into the cloud so I'm into s3 but you know",
    "start": "2911910",
    "end": "2918690"
  },
  {
    "text": "with that with that excuse me with that in application feedback we're able to to",
    "start": "2918690",
    "end": "2925500"
  },
  {
    "text": "accommodate for situations where there might have been an issue now fortunately we ran into very few issues",
    "start": "2925500",
    "end": "2932250"
  },
  {
    "text": "during the course of the event but we always maintained a local copy as well so at the same time these streams were",
    "start": "2932250",
    "end": "2938760"
  },
  {
    "text": "actually creating files that were being pushed remotely to LA or indirectly into",
    "start": "2938760",
    "end": "2944310"
  },
  {
    "text": "s3 local copies were being stored as well so if air ever came a time let's",
    "start": "2944310",
    "end": "2949500"
  },
  {
    "text": "say you know something happened you know our bandwidth drop to yo substantially",
    "start": "2949500",
    "end": "2955290"
  },
  {
    "text": "lower than we needed to carry out things in a real-time because of that local copy we could actually push that",
    "start": "2955290",
    "end": "2960300"
  },
  {
    "text": "feedback you know back upstream and say and we could actually start pulling from that local copy and as bandwidth",
    "start": "2960300",
    "end": "2966119"
  },
  {
    "text": "increased we could actually catch up and you know to where we alive at any given time like I said we didn't really really",
    "start": "2966119",
    "end": "2972960"
  },
  {
    "text": "have to you know take do that during it during the course of the event but it we",
    "start": "2972960",
    "end": "2980099"
  },
  {
    "text": "had it there just in case likewise we we made sure that we handled you know",
    "start": "2980099",
    "end": "2985859"
  },
  {
    "text": "conditions where there might be a network interruption completed network interruption as well and like I said we have the local backup copies you know as",
    "start": "2985859",
    "end": "2992790"
  },
  {
    "text": "well you know expect the unexpected and I've mentioned several times avoid",
    "start": "2992790",
    "end": "2998099"
  },
  {
    "text": "anything too spoke let's take a quick look at the live sub-clip workflow here sorry I can't really point a pointer at",
    "start": "2998099",
    "end": "3004310"
  },
  {
    "text": "the screen but just bear with me a little bit we're gonna start down at that bottom right hand corner and that bottom right hand corner you can see the",
    "start": "3004310",
    "end": "3011480"
  },
  {
    "text": "very lowest set of servers there is the Telestream so that's Telstra might be live and then the very bottom line that",
    "start": "3011480",
    "end": "3017930"
  },
  {
    "text": "yellow line that's there is a choose me as our direct live edit workflows I've",
    "start": "3017930",
    "end": "3025160"
  },
  {
    "text": "talked a little about those we'll just kind of ignore those for a second but the live edit workflow what's happening",
    "start": "3025160",
    "end": "3030770"
  },
  {
    "text": "what's happening there is we're pushing these these files these HLS streams of these you know the the manifests and",
    "start": "3030770",
    "end": "3036619"
  },
  {
    "text": "segments into into s3 as you know the event was an event was what's going on",
    "start": "3036619",
    "end": "3042290"
  },
  {
    "text": "and that those that H of the HLS stream that was being pushed that was only that",
    "start": "3042290",
    "end": "3047750"
  },
  {
    "text": "was a five megabit stream so it was cut you know kind of a low res proxy that was being done but that what happened is",
    "start": "3047750",
    "end": "3053780"
  },
  {
    "text": "that allowed the reach engine had a player built into it could actually pull",
    "start": "3053780",
    "end": "3059089"
  },
  {
    "text": "directly from s3 so they could look at that low-res proxy and that low-res proxy and they could",
    "start": "3059089",
    "end": "3064790"
  },
  {
    "text": "set in and out points and they could set you know say I want a clip from this point to this point and what happened is",
    "start": "3064790",
    "end": "3070850"
  },
  {
    "text": "and this is kind of on the right side a set of commands get sent to get sent to Telestream Vantage you know on site and",
    "start": "3070850",
    "end": "3077420"
  },
  {
    "text": "then Vantage would actually create that clip at the same in and out points out of the high-res version and then a",
    "start": "3077420",
    "end": "3083180"
  },
  {
    "text": "sparrow would transfer that back up into s3 bucket you know all of a sudden you have in very little time you have a a",
    "start": "3083180",
    "end": "3089450"
  },
  {
    "text": "you know full res you know clip that was generated by somebody that might not even be you know on you know have great",
    "start": "3089450",
    "end": "3095600"
  },
  {
    "text": "connectivity anywhere alright so with that I'm gonna ask Brandon and Babak to",
    "start": "3095600",
    "end": "3101990"
  },
  {
    "text": "come back out with me I'm just gonna hang back all right okay I'm gonna browning with me I guess",
    "start": "3101990",
    "end": "3108080"
  },
  {
    "text": "Bob's gonna hang back all right let's talk about some of the lessons learn you",
    "start": "3108080",
    "end": "3113120"
  },
  {
    "text": "know Brandon we've both talked about you know little Russia Charlotte without",
    "start": "3113120",
    "end": "3118490"
  },
  {
    "text": "that you know we couldn't have done it right I mean we simulation is key right",
    "start": "3118490",
    "end": "3125810"
  },
  {
    "text": "and we I I remember the first time that that we saw all the stuff working and",
    "start": "3125810",
    "end": "3132170"
  },
  {
    "text": "there was a lot of joy in that room yeah a lot of high-fives that simulation was key to stand up everything and make sure the lights came on and all those",
    "start": "3132170",
    "end": "3138530"
  },
  {
    "text": "connections were there yeah ii think collaboration you know there were there",
    "start": "3138530",
    "end": "3143840"
  },
  {
    "text": "were you know there was Telestream involved there was Fox Sports of course was driving things we had aspera we had",
    "start": "3143840",
    "end": "3149900"
  },
  {
    "text": "levels beyond with reach engine and there you know a few other you know vendors as well as harmonic that media",
    "start": "3149900",
    "end": "3157430"
  },
  {
    "text": "grid storage was on site you know and I would say that you know one of the keys",
    "start": "3157430",
    "end": "3163400"
  },
  {
    "text": "to making something like this work because you don't know what you're gonna find you what you're gonna find when you get there you know collaboration",
    "start": "3163400",
    "end": "3170060"
  },
  {
    "text": "avoidant avoiding the walls between vendors I think that was we had several",
    "start": "3170060",
    "end": "3175150"
  },
  {
    "text": "early morning meetings to kind of talk about we were gonna tackle that the next day right right yeah you stepped out of",
    "start": "3175150",
    "end": "3182630"
  },
  {
    "text": "your domain and we're writing code to help us fix some of the bugs that we were seeing completely out of this fair world so yeah that team we really",
    "start": "3182630",
    "end": "3187880"
  },
  {
    "text": "everyone came together to kind of you know accomplish our goals it was it was it was indeed teamwork",
    "start": "3187880",
    "end": "3193720"
  },
  {
    "text": "and this last one on this on this slide you know track progress relentlessly you",
    "start": "3193720",
    "end": "3199119"
  },
  {
    "text": "know at first we started kind of tracking things on on you know post-it notes and that kind of thing and very",
    "start": "3199119",
    "end": "3204849"
  },
  {
    "text": "quickly we realize that okay we're gonna use a tool and we we used you know an",
    "start": "3204849",
    "end": "3210700"
  },
  {
    "text": "actual agile tool even on-site while we're there on site to actually track our progress you lead now we didn't have",
    "start": "3210700",
    "end": "3216280"
  },
  {
    "text": "much time and things were a little behind the bill that was a little bit behind that kind of thing so we happened to do things you know fairly quickly",
    "start": "3216280",
    "end": "3222520"
  },
  {
    "text": "there all right Brandon once you take this slide yeah one thing a simulation",
    "start": "3222520",
    "end": "3228820"
  },
  {
    "text": "that we didn't really get to see was the the Moscow connection and when we got there will be learned was the streaming the HLS was there's a lot of latency",
    "start": "3228820",
    "end": "3235000"
  },
  {
    "text": "from Moscow fortunately for us the people that were using you know reach and trying to access the media war in Moscow or the media was originating so",
    "start": "3235000",
    "end": "3241960"
  },
  {
    "text": "we had other but that was tough and and that we'd probably implement some sort",
    "start": "3241960",
    "end": "3247030"
  },
  {
    "text": "of CDN over over large geographical areas to kind of solve that problem like flatter said building the right team was",
    "start": "3247030",
    "end": "3254349"
  },
  {
    "text": "really paramount nobody said this is my domain I'm and Telestream and I just care about Telestream I'm gonna spare everyone work",
    "start": "3254349",
    "end": "3259750"
  },
  {
    "text": "together to come up with solutions when we hit a lot of speed bumps especially we already got there and you probably flatters godbless slack we had so many",
    "start": "3259750",
    "end": "3273490"
  },
  {
    "text": "conversations and and it was such a good tool that we used with so many people and getting everybody on the same page",
    "start": "3273490",
    "end": "3279130"
  },
  {
    "text": "to solve problems or try to weed out whoever connection issues or what what you're having or whatever what's",
    "start": "3279130",
    "end": "3284830"
  },
  {
    "text": "happening so Brandon if there is one thing that you wish you had in Russia that you didn't have what would it be",
    "start": "3284830",
    "end": "3290710"
  },
  {
    "text": "hot sauce there one of the guys that",
    "start": "3290710",
    "end": "3296170"
  },
  {
    "text": "have been there a couple times actually brought a big bottle of hot sauce with him and came in handy so we leave you",
    "start": "3296170",
    "end": "3302560"
  },
  {
    "text": "with a piece of advice bring hot sauce to rock that's it I think we're we're wrapped up is that it yes obviously",
    "start": "3302560",
    "end": "3311700"
  },
  {
    "text": "looking forward Women's World Cup Men's World Cup was essentially a dress",
    "start": "3312580",
    "end": "3318080"
  },
  {
    "text": "rehearsal almost four wins World Cup so so we're gonna look at that I mean with all this technology I'm curious to see",
    "start": "3318080",
    "end": "3324260"
  },
  {
    "text": "where we could go with virtualized you know workflows as premiered as application start to speak objects store",
    "start": "3324260",
    "end": "3331370"
  },
  {
    "text": "more and more machine learning for metadata so key if you can't search the meta or you know the media that you're",
    "start": "3331370",
    "end": "3336740"
  },
  {
    "text": "looking for when you're working with two petabytes of material it's useless the other thing we want to look at is is",
    "start": "3336740",
    "end": "3342650"
  },
  {
    "text": "maybe even get five to ten times the amount of fast streams for the high-res bitrate that using compressing it and",
    "start": "3342650",
    "end": "3350930"
  },
  {
    "text": "then uncompressing it with hardware back on site or in the cloud so on ascent you know say one gig pipe we were getting",
    "start": "3350930",
    "end": "3356030"
  },
  {
    "text": "three to four feeds now we could probably do in the same workflow we could probably get around twelve I think",
    "start": "3356030",
    "end": "3362150"
  },
  {
    "text": "yeah these were all uncompressed streams so going with sort of my HEV sears you know something like that we can actually",
    "start": "3362150",
    "end": "3367280"
  },
  {
    "text": "you know get we think about five time in fact we've got some we've had some really good luck with some some early",
    "start": "3367280",
    "end": "3372620"
  },
  {
    "text": "prototypes with that so yeah tighter API integrations just like your the API",
    "start": "3372620",
    "end": "3378080"
  },
  {
    "text": "integration with fast and Telestream get us out of the watch folder business yeah there was a in my notes I have you no no",
    "start": "3378080",
    "end": "3385640"
  },
  {
    "text": "watch folders required I I'm not not a huge fan of watch flow sometimes they're necessary but direct API tighten",
    "start": "3385640",
    "end": "3391610"
  },
  {
    "text": "integrations was the key absolutely to making this you know in addition to all the other things were done that was what",
    "start": "3391610",
    "end": "3397130"
  },
  {
    "text": "made all this work we couldn't have done it with a bunch of bespoke workflows heaven forbid we had an issue when you",
    "start": "3397130",
    "end": "3402440"
  },
  {
    "text": "know when something you know during an event with the tight API integration we had that feedback loop all the way",
    "start": "3402440",
    "end": "3408260"
  },
  {
    "text": "through server list service all right thank you everyone a quick request",
    "start": "3408260",
    "end": "3416030"
  },
  {
    "text": "before you all go if you don't mind is please fill out your surveys my thanks to Mike and Brandon for this I hope you",
    "start": "3416030",
    "end": "3423110"
  },
  {
    "text": "found this valuable have a wonderful reinvent and thank you very much thank you",
    "start": "3423110",
    "end": "3428890"
  }
]