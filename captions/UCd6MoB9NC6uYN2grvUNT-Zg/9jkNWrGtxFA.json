[
  {
    "text": "so thanks everyone for coming I know it's like 5:30 p.m. it's almost 6 p.m.",
    "start": "60",
    "end": "5670"
  },
  {
    "text": "dinner time etc so I really appreciate appreciate you being here I'm just curious how many people have",
    "start": "5670",
    "end": "13139"
  },
  {
    "text": "used recognition and built something with it is there anyone that's new to recognition that hasn't used it before",
    "start": "13139",
    "end": "20070"
  },
  {
    "text": "ok awesome so the title here is a little",
    "start": "20070",
    "end": "26160"
  },
  {
    "text": "obtuse if you will you know unlocking media workflows with recognition was a tree that really mean and you know as",
    "start": "26160",
    "end": "33960"
  },
  {
    "text": "many of you that have used recognition though over the past year we've added a whole bunch of new it's a API endpoints",
    "start": "33960",
    "end": "42030"
  },
  {
    "text": "and options we added OCR last week but you know most of the initial use cases",
    "start": "42030",
    "end": "48180"
  },
  {
    "text": "were extracting object labels or doing phase collections etc but really what",
    "start": "48180",
    "end": "55770"
  },
  {
    "text": "we're seeing what we're starting to see now is that quite a few customers are",
    "start": "55770",
    "end": "61320"
  },
  {
    "text": "looking at the labels that come out of recognition and then trying to figure out how do i normalize these labels how",
    "start": "61320",
    "end": "67409"
  },
  {
    "text": "do I use this like with my own taxonomy for example you know maybe I don't want",
    "start": "67409",
    "end": "72479"
  },
  {
    "text": "people person maybe I want human for example and then how can I do other",
    "start": "72479",
    "end": "77880"
  },
  {
    "text": "things that clean up the data the images etc before I actually funnel this into",
    "start": "77880",
    "end": "83280"
  },
  {
    "text": "recognition and then the other thing is if I'm getting labels back you know",
    "start": "83280",
    "end": "88650"
  },
  {
    "text": "potentially I could utilize those labels as a way to figure out how I should say",
    "start": "88650",
    "end": "93750"
  },
  {
    "text": "transcode my content so if I have a digital supply chain and a lot of media",
    "start": "93750",
    "end": "99659"
  },
  {
    "text": "moving through that and it's maybe user-generated content and these are small files but I really want to",
    "start": "99659",
    "end": "105659"
  },
  {
    "text": "optimize the speed at which our transcode create proxies or whatnot maybe I want various encoding profiles",
    "start": "105659",
    "end": "113909"
  },
  {
    "text": "that are keyed off of the labels that are coming back from say the media that's been landed so you know maybe",
    "start": "113909",
    "end": "120299"
  },
  {
    "text": "labels where most of the frames are identified as humans or people I'd want",
    "start": "120299",
    "end": "126360"
  },
  {
    "text": "to use a different encoding profile to say things that are associated with maybe action content",
    "start": "126360",
    "end": "131440"
  },
  {
    "text": "if you well so just about me my name is Constantine forms I work on the M&E team",
    "start": "131440",
    "end": "139270"
  },
  {
    "text": "so a lot of these kind of use cases that we see you know we try and condense down",
    "start": "139270",
    "end": "145150"
  },
  {
    "text": "into things like reference architectures we built a reference architecture for recognition you know you could take that",
    "start": "145150",
    "end": "152530"
  },
  {
    "text": "deploy it and then some of these concepts are interesting things to maybe build on top of that or if you have a",
    "start": "152530",
    "end": "159130"
  },
  {
    "text": "pipeline you know potentially look at ways to enhance that so a lot of this is",
    "start": "159130",
    "end": "166420"
  },
  {
    "text": "fairly explanatory right but I think the interesting thing that drives these use cases for us in terms of reference",
    "start": "166420",
    "end": "174370"
  },
  {
    "text": "architectures and things like that that we build is the scale of content that we see on the media side customers landing",
    "start": "174370",
    "end": "182380"
  },
  {
    "text": "be that UGC or you know contents a feature film content etc and then a need",
    "start": "182380",
    "end": "189640"
  },
  {
    "text": "to extract some kind of automated metadata so if it's an asset management system you know maybe one of the impetus",
    "start": "189640",
    "end": "197739"
  },
  {
    "text": "points there are could we look at automatically curating this content and",
    "start": "197739",
    "end": "203260"
  },
  {
    "text": "tagging it with metadata and then maybe we don't need a first set of QC people",
    "start": "203260",
    "end": "209170"
  },
  {
    "text": "to look at that data before it gets into the asset management system you know maybe we could put it in different bins",
    "start": "209170",
    "end": "215560"
  },
  {
    "text": "and then a different people that are specialized in those areas classified after that you know so we we at least on",
    "start": "215560",
    "end": "224860"
  },
  {
    "text": "the media side there's obviously a lot of image data so if you look at a lot of the media companies many of bringing",
    "start": "224860",
    "end": "232030"
  },
  {
    "text": "full image archives onto AWS and then storing like all the high quality assets",
    "start": "232030",
    "end": "237400"
  },
  {
    "text": "in Glacier and maybe the proxies or the deliverable assets if you will on s3 or",
    "start": "237400",
    "end": "243580"
  },
  {
    "text": "s3 ia and similar for you know media content as well right so we're seen a",
    "start": "243580",
    "end": "249489"
  },
  {
    "text": "big shift from you know what was maybe smaller amounts of media content to a",
    "start": "249489",
    "end": "255730"
  },
  {
    "text": "large amount of VOD you can also look at things like elemental media services so",
    "start": "255730",
    "end": "262150"
  },
  {
    "text": "there's a lot of impetus for live content to come and get recorded and then maybe run through things like recognition so",
    "start": "262150",
    "end": "269900"
  },
  {
    "text": "the question here is you know if we're extracting these labels that's great that enriches the content potentially or",
    "start": "269900",
    "end": "277790"
  },
  {
    "text": "helps enrich it but what about using those same processes you know maybe it's",
    "start": "277790",
    "end": "282920"
  },
  {
    "text": "lambda functions that we have as part of the supply chain or a pipeline using",
    "start": "282920",
    "end": "288140"
  },
  {
    "text": "that to enhance or that data is a feedback mechanism and if we look at",
    "start": "288140",
    "end": "293330"
  },
  {
    "text": "just a simple pipeline right there potentially things we can optimize on the ingest side there are things that we",
    "start": "293330",
    "end": "300800"
  },
  {
    "text": "could optimize on the storage side and an obviously pre and post analysis to",
    "start": "300800",
    "end": "306950"
  },
  {
    "text": "basically look at enriching the data cleaning up images you know denoising all of these other approaches to",
    "start": "306950",
    "end": "313790"
  },
  {
    "text": "processing it and then actually well skip to there using that as a feedback",
    "start": "313790",
    "end": "320750"
  },
  {
    "text": "loop back to our storage system and then potentially we can use that to reprocess",
    "start": "320750",
    "end": "326840"
  },
  {
    "text": "data so like one example that I'll talk about is potentially images that have a",
    "start": "326840",
    "end": "332870"
  },
  {
    "text": "lot of fog in them and that's part of you know maybe it's a camera artifact and it's not a Content artifact",
    "start": "332870",
    "end": "339820"
  },
  {
    "text": "recognition would identify a lot of these types of things as atmosphere as a tag coming back we could look at that",
    "start": "339820",
    "end": "346850"
  },
  {
    "text": "and see okay this is we're not expecting to get this content you know run like a",
    "start": "346850",
    "end": "352190"
  },
  {
    "text": "rednecks algorithm against it pass it back to recognition and then we actually remove that noise of the front of the",
    "start": "352190",
    "end": "359570"
  },
  {
    "text": "image and get back valid labels so these are you know interesting use cases here",
    "start": "359570",
    "end": "364690"
  },
  {
    "text": "and the other thing is obviously why would you do this as a managed service you know if you think about all of the",
    "start": "364690",
    "end": "371900"
  },
  {
    "text": "recognition endpoints if you will you know you should look at each one of",
    "start": "371900",
    "end": "377060"
  },
  {
    "text": "those if you built that out as a you know service from the ground up maybe using tensorflow etc whatever the the",
    "start": "377060",
    "end": "385100"
  },
  {
    "text": "models and frameworks are you know you have to be concerned with versioning",
    "start": "385100",
    "end": "390530"
  },
  {
    "text": "updating blue/green deploying all of these pipelines and then maintaining a",
    "start": "390530",
    "end": "396919"
  },
  {
    "text": "velocity so you know one of the key things with recognition is that if you have facial",
    "start": "396919",
    "end": "403220"
  },
  {
    "text": "collections you can scale up to 10 million faces if you're passing an image",
    "start": "403220",
    "end": "409280"
  },
  {
    "text": "data it could be coming off of s3 and the service knows how to parse it but the other thing is the deterministic",
    "start": "409280",
    "end": "415790"
  },
  {
    "text": "nature of passing in the image data so you know depending on the size and type",
    "start": "415790",
    "end": "421190"
  },
  {
    "text": "of image I know I may get a response back in 500 milliseconds to 2 seconds so",
    "start": "421190",
    "end": "427580"
  },
  {
    "text": "that kind of deterministic response rate for things like supply chain user-generated content you don't want to",
    "start": "427580",
    "end": "435320"
  },
  {
    "text": "have to wait for something like an order scale group to spin up to facilitate that and then all of the you know native",
    "start": "435320",
    "end": "443240"
  },
  {
    "text": "key endpoints that are talked about - so for those that are new to recognition",
    "start": "443240",
    "end": "449180"
  },
  {
    "text": "you know a lot of these api's have different obviously use cases right but",
    "start": "449180",
    "end": "455210"
  },
  {
    "text": "the primary ones that are useful from this particular perspective or object",
    "start": "455210",
    "end": "461150"
  },
  {
    "text": "and scene detection and potentially text detection as well for where we can't get",
    "start": "461150",
    "end": "466340"
  },
  {
    "text": "accurate object and scene confidence returns facial analysis etc to a lesser",
    "start": "466340",
    "end": "473090"
  },
  {
    "text": "extent although those can also be used for as feedback mechanisms the same with",
    "start": "473090",
    "end": "479810"
  },
  {
    "text": "image moderation - and any applicability here also for those that are you know",
    "start": "479810",
    "end": "486260"
  },
  {
    "text": "new to this in this media space is that there's definitely the ability to reuse",
    "start": "486260",
    "end": "493400"
  },
  {
    "text": "a lot of the code so if it's docker containers you're building if it's Python code in lambda if it's no js' you",
    "start": "493400",
    "end": "500690"
  },
  {
    "text": "know whatever it is that you know that code base that you build can start being",
    "start": "500690",
    "end": "506479"
  },
  {
    "text": "reused for different purposes so the Python code that is used for say maybe a",
    "start": "506479",
    "end": "512390"
  },
  {
    "text": "supply chain process to pass an image to recognition and then funnel the output",
    "start": "512390",
    "end": "517610"
  },
  {
    "text": "data into dynamo that code could be repurposed for say local editing where",
    "start": "517610",
    "end": "523550"
  },
  {
    "text": "as images you know land on say a local file system you could run the same Python script but",
    "start": "523550",
    "end": "530780"
  },
  {
    "text": "instead injecting data into dynamo you're maybe re-injecting it into the EXIF data on",
    "start": "530780",
    "end": "536459"
  },
  {
    "text": "the images and then your editors that are using say local Windows or Mac or",
    "start": "536459",
    "end": "543269"
  },
  {
    "text": "whatever their tools are they could have a local file system search or asset",
    "start": "543269",
    "end": "548910"
  },
  {
    "text": "search in the applications that are using so we see a lot of that you know everything from mac OS finder to",
    "start": "548910",
    "end": "555180"
  },
  {
    "text": "injecting tags there to things like premiere you know JSON automation etc",
    "start": "555180",
    "end": "560670"
  },
  {
    "text": "and Photoshop etc so very useful from that perspective and then the other",
    "start": "560670",
    "end": "567750"
  },
  {
    "text": "thing we frequently see is you know recognition is very easy to get up and",
    "start": "567750",
    "end": "573300"
  },
  {
    "text": "running with make API calls get data back with confidence scores and do",
    "start": "573300",
    "end": "579000"
  },
  {
    "text": "whatever we want with it but their considerations across these you know if",
    "start": "579000",
    "end": "585180"
  },
  {
    "text": "you want to look at this as boundaries or pillars within say supply chain or",
    "start": "585180",
    "end": "590760"
  },
  {
    "text": "media processing so on the content side you know dealing with different file",
    "start": "590760",
    "end": "596040"
  },
  {
    "text": "formats we need a way to normalize the data like maybe it's different streaming protocols for example right or mp4 avi",
    "start": "596040",
    "end": "605040"
  },
  {
    "text": "Zoar whatnot we need some way to normalize that before we pass data to",
    "start": "605040",
    "end": "610230"
  },
  {
    "text": "recognition on the processing side you know obviously the storage and",
    "start": "610230",
    "end": "615240"
  },
  {
    "text": "networking so if we wanted to do image cleanup and we're maybe using ECS and",
    "start": "615240",
    "end": "620640"
  },
  {
    "text": "we're hydrating the content say from s3 to EFS to then run workloads against it",
    "start": "620640",
    "end": "626790"
  },
  {
    "text": "to maybe clean up the content transcoded whatever we're doing obviously the storage and network i/o is",
    "start": "626790",
    "end": "633990"
  },
  {
    "text": "important for this and that directly relates to i/o for actually dealing with",
    "start": "633990",
    "end": "639029"
  },
  {
    "text": "the data because if we're getting 500 milliseconds to say 2 seconds and that's",
    "start": "639029",
    "end": "644610"
  },
  {
    "text": "deterministic we don't want to skew that number you know by having a high latency",
    "start": "644610",
    "end": "652040"
  },
  {
    "text": "hydration as part of this process so that's kind of key to that too",
    "start": "652040",
    "end": "657180"
  },
  {
    "text": "and then on the quality side you know sort of relates to the processing velocity so with things like supply",
    "start": "657180",
    "end": "664470"
  },
  {
    "text": "chain you know many of our customers will look at our on-prem SLA that we created was",
    "start": "664470",
    "end": "671580"
  },
  {
    "text": "maybe 24 to 48 hours with a lot of manual processes now that we have it on",
    "start": "671580",
    "end": "677160"
  },
  {
    "text": "AWS we want to reduce that to a couple hours and the goal here is to do that",
    "start": "677160",
    "end": "683540"
  },
  {
    "text": "deterministically but also with the processing velocity so in other words if",
    "start": "683540",
    "end": "689160"
  },
  {
    "text": "we can do more scale we want to learn more content maybe it's you know hundreds of terabytes a day but we",
    "start": "689160",
    "end": "695250"
  },
  {
    "text": "expect as that grows the backend infrastructure should be deterministic",
    "start": "695250",
    "end": "700560"
  },
  {
    "text": "so everything should still work at the same pace so this is kind of key for recognition here in like one of the",
    "start": "700560",
    "end": "707640"
  },
  {
    "text": "benefits that it provides and then conformance is another thing too so you",
    "start": "707640",
    "end": "713580"
  },
  {
    "text": "know not only say being able to ingest the content but then conforming it for local use or maybe we won't want to",
    "start": "713580",
    "end": "720390"
  },
  {
    "text": "perceptually compress the images before we actually store them and that can save",
    "start": "720390",
    "end": "725730"
  },
  {
    "text": "on cost on s3 and then just on the integration side there's some obvious",
    "start": "725730",
    "end": "731820"
  },
  {
    "text": "ones but it really depends you know a lot of the b2b or b2c systems out there",
    "start": "731820",
    "end": "740690"
  },
  {
    "text": "some of them will utilize a CMS others are going to be a traditional mam or damn and then there's also the option of",
    "start": "740690",
    "end": "748290"
  },
  {
    "text": "where do we store the labels do we use dynamo do we inject that back into say",
    "start": "748290",
    "end": "753720"
  },
  {
    "text": "the mp4 headers or do we have sidecar data and maybe Athena's running on top",
    "start": "753720",
    "end": "759270"
  },
  {
    "text": "of that you know we have quick site that's used to interrogate that as well so these are all I'd say things that are",
    "start": "759270",
    "end": "766020"
  },
  {
    "text": "worth looking at before just jumping in and utilizing or integrating recognition",
    "start": "766020",
    "end": "771990"
  },
  {
    "text": "into a pipeline and then just to look at the you know things that are relevant",
    "start": "771990",
    "end": "777150"
  },
  {
    "text": "here right from a object and scene detection really the important thing",
    "start": "777150",
    "end": "782520"
  },
  {
    "text": "here is that you're not only identifying objects but you're also identifying scenes and also concepts so you know it",
    "start": "782520",
    "end": "790650"
  },
  {
    "text": "sort of goes from sky versus mountain to mountain range to you know",
    "start": "790650",
    "end": "796850"
  },
  {
    "text": "door location those types of content or concepts if you all on at least facial",
    "start": "796850",
    "end": "804500"
  },
  {
    "text": "analysis side so some of the feedback loops that we could do is we could look for objects in images instead of passing",
    "start": "804500",
    "end": "812690"
  },
  {
    "text": "everything for facial analysis but if we look for objects in images those objects",
    "start": "812690",
    "end": "818329"
  },
  {
    "text": "that came come back may be detected as human you know with high confidence or person or people the thing that we could",
    "start": "818329",
    "end": "826220"
  },
  {
    "text": "then do is if we have a sale and a step function pipeline is basically take that",
    "start": "826220",
    "end": "831860"
  },
  {
    "text": "and only then pass that for facial analysis so if our object label returns",
    "start": "831860",
    "end": "838370"
  },
  {
    "text": "that there is a person in the scene goal for make a secondary API call as a",
    "start": "838370",
    "end": "843860"
  },
  {
    "text": "second step in a step function framework and then go and look for facial analysis",
    "start": "843860",
    "end": "850250"
  },
  {
    "text": "features and the reason I say to two phase instead of doing that one time is",
    "start": "850250",
    "end": "856579"
  },
  {
    "text": "that if you do do say facial analysis using recognition and you want to return all of these details for example not",
    "start": "856579",
    "end": "864259"
  },
  {
    "text": "just facial landmarks but maybe general attributes like is a person smiling or",
    "start": "864259",
    "end": "869720"
  },
  {
    "text": "not what the demographic data is what the age range is that can also increase the processing time and it's not a lot",
    "start": "869720",
    "end": "878120"
  },
  {
    "text": "of processing time you know your measure this in say hundreds of milliseconds but",
    "start": "878120",
    "end": "883279"
  },
  {
    "text": "if you multiply that out for say a large image archive this could skew it by",
    "start": "883279",
    "end": "888500"
  },
  {
    "text": "potentially days you know depending on if you have not optimized these calls and any other you know a useful thing",
    "start": "888500",
    "end": "896329"
  },
  {
    "text": "here is at least from recognition you know from ability to use feedback loops",
    "start": "896329",
    "end": "902209"
  },
  {
    "text": "as obviously the moderation labels you know the detection of explicit or",
    "start": "902209",
    "end": "907579"
  },
  {
    "text": "suggestive content and then celebrity detection it's worth noting that",
    "start": "907579",
    "end": "913959"
  },
  {
    "text": "celebrity detection works against IMDB so it returns an IMDB URL to those",
    "start": "913959",
    "end": "919610"
  },
  {
    "text": "actors but there are a lot of use cases we see where we get the question what",
    "start": "919610",
    "end": "924829"
  },
  {
    "text": "about my indie production where I know my actors but they're not all of IMDb are not in there yet you",
    "start": "924829",
    "end": "932290"
  },
  {
    "text": "know potentially it's a whole bunch of actors for different shots or programs but they're more in D level maybe",
    "start": "932290",
    "end": "939430"
  },
  {
    "text": "they're targeted for say YouTube or you know other OTT platforms that's actually",
    "start": "939430",
    "end": "945850"
  },
  {
    "text": "a great case for using the celebrity what is not the celebrity detection but",
    "start": "945850",
    "end": "952600"
  },
  {
    "text": "collections right so the the goal there is to basically name each of the actors",
    "start": "952600",
    "end": "959560"
  },
  {
    "text": "there at least their facial images or photos or you know headshots put those",
    "start": "959560",
    "end": "965710"
  },
  {
    "text": "in s3 create a collection index all the actors against that what the back link",
    "start": "965710",
    "end": "971980"
  },
  {
    "text": "being to the location on say dynamo you know with all of their details for",
    "start": "971980",
    "end": "977650"
  },
  {
    "text": "example and then for each one of these you know detected images that come",
    "start": "977650",
    "end": "983440"
  },
  {
    "text": "through showing person for example do a facial comparison against our home-built",
    "start": "983440",
    "end": "988890"
  },
  {
    "text": "collection so it allows you to build a you know your equivalent of a celebrity",
    "start": "988890",
    "end": "994270"
  },
  {
    "text": "detection for say in the use for example just on the topic of you know best",
    "start": "994270",
    "end": "1001620"
  },
  {
    "text": "practices further on that there are limits I'm not going to talk about the limits here those are you know",
    "start": "1001620",
    "end": "1007230"
  },
  {
    "text": "well-documented but I think if you look at the recognition limits and then you try figure out what are these what is",
    "start": "1007230",
    "end": "1013740"
  },
  {
    "text": "the implication of this when I'm saying high-speed processing or having to",
    "start": "1013740",
    "end": "1019410"
  },
  {
    "text": "process a lot of content like I said operations are fairly deterministic so",
    "start": "1019410",
    "end": "1024930"
  },
  {
    "text": "you can take that into account for how long it'll take to process batches of images s3 is optimal versus doing inline",
    "start": "1024930",
    "end": "1033449"
  },
  {
    "text": "calls however if you have the content locally then it's maybe more optimal you",
    "start": "1033450",
    "end": "1039540"
  },
  {
    "text": "know if you have a local say Photoshop or other editing purposes it may be more optimal not to move it to s3 but - maybe",
    "start": "1039540",
    "end": "1047579"
  },
  {
    "text": "compress the image and just make a direct API call there's some other things too that you can do when images",
    "start": "1047580",
    "end": "1054570"
  },
  {
    "text": "arrive and this is more particular for user-generated content you know if the image is less than 80",
    "start": "1054570",
    "end": "1060900"
  },
  {
    "text": "pixels it's fairly certain we get much detail out of it so you could scrub those immediately saving having to",
    "start": "1060900",
    "end": "1068630"
  },
  {
    "text": "make an API call and then there's some other things right so you're at least",
    "start": "1068630",
    "end": "1073970"
  },
  {
    "text": "one 1024 pixels if you're processing say 1080p frames you need a 40 by 40 pixel",
    "start": "1073970",
    "end": "1081950"
  },
  {
    "text": "you know minimum resolution for faces so it's all these things to bear in mind as",
    "start": "1081950",
    "end": "1087470"
  },
  {
    "text": "you pass content into recognition and like I said the reason I listed this",
    "start": "1087470",
    "end": "1092810"
  },
  {
    "text": "here is that often we'll see people or these customers will come back and say it's not a it doesn't appear to work for",
    "start": "1092810",
    "end": "1099830"
  },
  {
    "text": "use case X Y & Z and in many cases at some of these types of things that you",
    "start": "1099830",
    "end": "1105740"
  },
  {
    "text": "know have been a malfunction in that process if you will and then finally you",
    "start": "1105740",
    "end": "1111290"
  },
  {
    "text": "know most of the like the CLI and and API components that we have will do a",
    "start": "1111290",
    "end": "1117380"
  },
  {
    "text": "fairly good job of throttling but if you need to process a lot of content in",
    "start": "1117380",
    "end": "1123020"
  },
  {
    "text": "batches simultaneously in a you know parallel fashion you could look at",
    "start": "1123020",
    "end": "1128360"
  },
  {
    "text": "things like first heading Dynamo and then using dynamo streams and utilizing",
    "start": "1128360",
    "end": "1134660"
  },
  {
    "text": "lambda or for that so then go and make the calls and so in other words instead of making a direct call to recognition",
    "start": "1134660",
    "end": "1142310"
  },
  {
    "text": "you're potentially just putting the asset ID of where the acid is an s3 into",
    "start": "1142310",
    "end": "1147320"
  },
  {
    "text": "dynamo and then using streams and other great ways to use Kinesis streams as",
    "start": "1147320",
    "end": "1152990"
  },
  {
    "text": "well and basically shard that out and the reason for that is to basically if",
    "start": "1152990",
    "end": "1158240"
  },
  {
    "text": "you look at the graph on the right hand side and we have a think we have part of the architecture blog that she has a",
    "start": "1158240",
    "end": "1166010"
  },
  {
    "text": "fairly lengthy article on this on exponential back-off and control one of",
    "start": "1166010",
    "end": "1172760"
  },
  {
    "text": "the approaches here that you could look at doing is exponentially back off as API calls fail but combining that with",
    "start": "1172760",
    "end": "1181730"
  },
  {
    "text": "jittering the calls that are made to try and push more data through the api's and",
    "start": "1181730",
    "end": "1188780"
  },
  {
    "text": "you can basically see there's a there's a fairly linear curve of as you start adding these other ways to alleviate a",
    "start": "1188780",
    "end": "1196910"
  },
  {
    "text": "i blockage if you will and you're able to get more throughput through the system so it's definitely something to",
    "start": "1196910",
    "end": "1202700"
  },
  {
    "text": "look at I talked about most of the you know AWS services that would typically",
    "start": "1202700",
    "end": "1210920"
  },
  {
    "text": "be utilized here you know there are also asset asset management media workflow",
    "start": "1210920",
    "end": "1217090"
  },
  {
    "text": "content processing typical use cases but the other thing that we've seen a lot is",
    "start": "1217090",
    "end": "1222590"
  },
  {
    "text": "you know what's the best practice or how can I go about building some kind of a toolkit to do a lot of this work for me",
    "start": "1222590",
    "end": "1228770"
  },
  {
    "text": "so I'll talk about you know OpenCV image magic ffmpeg and then some other",
    "start": "1228770",
    "end": "1236330"
  },
  {
    "text": "libraries and basically how to componentize those and build access Swiss Army knife for doing cleanup",
    "start": "1236330",
    "end": "1242420"
  },
  {
    "text": "before you make those recognition calls I talked a bit about this you know in",
    "start": "1242420",
    "end": "1248390"
  },
  {
    "text": "terms of complimentary service services so you know that that whole end-to-end",
    "start": "1248390",
    "end": "1253460"
  },
  {
    "text": "supply chain if you will you could look at media processing so say the elemental",
    "start": "1253460",
    "end": "1259160"
  },
  {
    "text": "services to create proxies to then rip frames out of storage and s3 decoupling",
    "start": "1259160",
    "end": "1266810"
  },
  {
    "text": "the infrastructure here's a decoupling when the files arrived from where",
    "start": "1266810",
    "end": "1272600"
  },
  {
    "text": "they're actually processed using say SNS sqs etc fronting things with api gateway",
    "start": "1272600",
    "end": "1279970"
  },
  {
    "text": "and in potentially handing off to batch or ECS or playing ec2 as well for",
    "start": "1279970",
    "end": "1287060"
  },
  {
    "text": "processing and then the last piece to basically maintain context as you move",
    "start": "1287060",
    "end": "1292610"
  },
  {
    "text": "through these workflows is dynamodb so just to get to you know an example here",
    "start": "1292610",
    "end": "1299750"
  },
  {
    "text": "and so the rest of this will be around you know how would we take video clip",
    "start": "1299750",
    "end": "1304760"
  },
  {
    "text": "and this is a open source movie tears of steel so basically looks at this and",
    "start": "1304760",
    "end": "1310460"
  },
  {
    "text": "then how would we take these best practices or requirements that we've seen coming from customers and you know",
    "start": "1310460",
    "end": "1316760"
  },
  {
    "text": "construct something to solve those problems so you know what is the process",
    "start": "1316760",
    "end": "1321890"
  },
  {
    "text": "of the best process for processing this as an example input so the one thing we",
    "start": "1321890",
    "end": "1328010"
  },
  {
    "text": "can do to start off as we could just extract all of the frames right and pass those in",
    "start": "1328010",
    "end": "1333539"
  },
  {
    "text": "maybe we extract and this is using ffmpeg so extract each or each tenth",
    "start": "1333539",
    "end": "1340799"
  },
  {
    "text": "frame one one tenth of a frame per second the other thing we could do is we could extract the eye frames and there",
    "start": "1340799",
    "end": "1348299"
  },
  {
    "text": "are different use cases for this so if you think of potentially a use case like",
    "start": "1348299",
    "end": "1353850"
  },
  {
    "text": "say c-span or you know that type of long-running camera ingest against a",
    "start": "1353850",
    "end": "1361950"
  },
  {
    "text": "political figures or Talking Heads maybe sampling each tenth frame is a",
    "start": "1361950",
    "end": "1367620"
  },
  {
    "text": "good way to make sure that we're tracking the same person over time we",
    "start": "1367620",
    "end": "1372630"
  },
  {
    "text": "could potentially use iframes but it now that is determined by what the length of",
    "start": "1372630",
    "end": "1377789"
  },
  {
    "text": "the gap is and what the duration is between those frames and the third option here is we could do scene change",
    "start": "1377789",
    "end": "1384630"
  },
  {
    "text": "analysis right so only extract frames when we detect that the camera has",
    "start": "1384630",
    "end": "1390000"
  },
  {
    "text": "changed location or there's been a cut from one shot to another based on the",
    "start": "1390000",
    "end": "1395220"
  },
  {
    "text": "difference between those pictures and the reason for this is we see this a lot",
    "start": "1395220",
    "end": "1402090"
  },
  {
    "text": "as well so the one approach is I'm just going to extract everything I'm going to extract a frame every 3 or 5 seconds",
    "start": "1402090",
    "end": "1409279"
  },
  {
    "text": "that alleviates the API calls but it's still a lot of data right so if you look",
    "start": "1409279",
    "end": "1415230"
  },
  {
    "text": "at VOD clips and running those through recognition a 30-second ad is 900 frames",
    "start": "1415230",
    "end": "1421110"
  },
  {
    "text": "that's four hundred and fifty megabytes potentially of data and then all the API",
    "start": "1421110",
    "end": "1426149"
  },
  {
    "text": "calls that you have to make for that and that multiplies out as you get up to say",
    "start": "1426149",
    "end": "1431460"
  },
  {
    "text": "feature-length so there are some things you can do you can reduce the capture",
    "start": "1431460",
    "end": "1436740"
  },
  {
    "text": "rates but maybe that also decimates our accuracy that we're trying to get here because the one thing if you extract a",
    "start": "1436740",
    "end": "1444419"
  },
  {
    "text": "frame every say three or maybe five or six seconds versus doing scene change",
    "start": "1444419",
    "end": "1450330"
  },
  {
    "text": "analysis you may get the wrong the wrong frame or you may get a frame six seconds",
    "start": "1450330",
    "end": "1456149"
  },
  {
    "text": "into that scene so that's potentially problematic for detection too you know",
    "start": "1456149",
    "end": "1461669"
  },
  {
    "text": "so this assumed no post-processing but that's these are typically the payloads that we would see",
    "start": "1461669",
    "end": "1467520"
  },
  {
    "text": "from us now this is one file you know if you had a whole bunch of content",
    "start": "1467520",
    "end": "1473190"
  },
  {
    "text": "arriving and landing on the s3 this impacts your s3 storage it impacts your",
    "start": "1473190",
    "end": "1479160"
  },
  {
    "text": "i/o against your s3 partition's etc you know you may have to look at how you're",
    "start": "1479160",
    "end": "1484950"
  },
  {
    "text": "partitioning the data are you you know putting a hash in front of the file in order to evenly distribute it for good",
    "start": "1484950",
    "end": "1491550"
  },
  {
    "text": "io but regardless so that it's a lot of content and it's just another problem",
    "start": "1491550",
    "end": "1496680"
  },
  {
    "text": "that needs to be solved so on the ingest side you know I'll get",
    "start": "1496680",
    "end": "1502560"
  },
  {
    "text": "to the previous problem on how we can look at solving that but the other component here is ingest right so how",
    "start": "1502560",
    "end": "1509880"
  },
  {
    "text": "can we do images versus on-demand vs. live ingest so if we're looking at",
    "start": "1509880",
    "end": "1515310"
  },
  {
    "text": "recognition for images we essentially want to do correction on it we want to",
    "start": "1515310",
    "end": "1520770"
  },
  {
    "text": "then optimize after we've corrected the data and then process that data and that may enable us to get higher quality",
    "start": "1520770",
    "end": "1527600"
  },
  {
    "text": "labels out of it if it's video you know we'd be creating a proxy from it and",
    "start": "1527600",
    "end": "1534030"
  },
  {
    "text": "then segmenting that and then actually utilizing the image pipeline to process",
    "start": "1534030",
    "end": "1539280"
  },
  {
    "text": "those images that are coming off at on-demand content and third if we're doing live content one of the novel ways",
    "start": "1539280",
    "end": "1547830"
  },
  {
    "text": "you could look at doing that is potentially landing HLS or dash chunks",
    "start": "1547830",
    "end": "1553230"
  },
  {
    "text": "or if the encoder can support it segmenting those files every X amount of seconds if you're doing something like -",
    "start": "1553230",
    "end": "1561030"
  },
  {
    "text": "your HLS and it's starting on an iframe we can potentially take every packet every say five seconds and just pull",
    "start": "1561030",
    "end": "1568860"
  },
  {
    "text": "that out with ffmpeg and do image recognition against the iframe so that's",
    "start": "1568860",
    "end": "1573900"
  },
  {
    "text": "an easy way to go from say a live stream to either is something like you know",
    "start": "1573900",
    "end": "1579630"
  },
  {
    "text": "elemental media life for example or potentially something like nginx with",
    "start": "1579630",
    "end": "1585210"
  },
  {
    "text": "ffmpeg and build a whole live processing pipeline there where you can inspect and",
    "start": "1585210",
    "end": "1591060"
  },
  {
    "text": "then store those images to you know so it's critical here to",
    "start": "1591060",
    "end": "1596350"
  },
  {
    "text": "all these components to be reusable because if you build the image pipeline that same code Maps well to on demand",
    "start": "1596350",
    "end": "1603070"
  },
  {
    "text": "and that same code Maps well to to live it's not much of a stretch to add that",
    "start": "1603070",
    "end": "1609490"
  },
  {
    "text": "kind of functionality on the right hand side I actually you know I think one of",
    "start": "1609490",
    "end": "1615789"
  },
  {
    "text": "the things worth calling out here is that there are some benefits to doing things like extracting only iframes",
    "start": "1615789",
    "end": "1623470"
  },
  {
    "text": "versus extracting a frame every couple seconds because one of the problems you",
    "start": "1623470",
    "end": "1628840"
  },
  {
    "text": "could get is macro blocking if the you know video quality is low quality which",
    "start": "1628840",
    "end": "1634360"
  },
  {
    "text": "is definitely going to skew performance against calls to recognition like",
    "start": "1634360",
    "end": "1639730"
  },
  {
    "text": "objects may get missed the other thing is if you're say live stream that's coming in that you're then",
    "start": "1639730",
    "end": "1645549"
  },
  {
    "text": "processing has a lot of say discontinuity errors or packet loss etc",
    "start": "1645549",
    "end": "1651570"
  },
  {
    "text": "potentially you can still write that all to disk but you may have you know loss",
    "start": "1651570",
    "end": "1656770"
  },
  {
    "text": "in the actual transport as well and that could further skew extracting frames or",
    "start": "1656770",
    "end": "1662919"
  },
  {
    "text": "even being able to extract those so it sort of brings us to this pipeline in",
    "start": "1662919",
    "end": "1668890"
  },
  {
    "text": "terms of the toolkit right so the one thing that can be done here at least",
    "start": "1668890",
    "end": "1673990"
  },
  {
    "text": "with lambda for example is to take ffmpeg OpenCV image magic and spacey",
    "start": "1673990",
    "end": "1682020"
  },
  {
    "text": "spaces NLP processing toolkit for Python that is high performance because it's",
    "start": "1682020",
    "end": "1688240"
  },
  {
    "text": "compiled with scythe on but essentially you know the good thing here is that if",
    "start": "1688240",
    "end": "1695440"
  },
  {
    "text": "you create a custom compiled of ffmpeg you can then binary wrap that for use by",
    "start": "1695440",
    "end": "1702010"
  },
  {
    "text": "lambda the same with OpenCV that can also be compiled image magic is native",
    "start": "1702010",
    "end": "1708370"
  },
  {
    "text": "already with lambda and then Spacey has to be basically compiled and stripped",
    "start": "1708370",
    "end": "1714970"
  },
  {
    "text": "down I'll get to that one late later but you know OpenCV you know if you wanted",
    "start": "1714970",
    "end": "1721840"
  },
  {
    "text": "to reduce the size of that and ffmpeg a lot of the options there or to take out the protocols and codecs you don't need",
    "start": "1721840",
    "end": "1729550"
  },
  {
    "text": "most of these have commandlineoptions for doing that but essentially this allows you or the target here is to",
    "start": "1729550",
    "end": "1736510"
  },
  {
    "text": "build a like multi factor Swiss Army knife that can run with lambda that can",
    "start": "1736510",
    "end": "1741760"
  },
  {
    "text": "do any kind of cleanup we want so we may need to you know extract frames out of",
    "start": "1741760",
    "end": "1747880"
  },
  {
    "text": "video fragments ffmpeg would do that that works great for that you know and",
    "start": "1747880",
    "end": "1753610"
  },
  {
    "text": "it'll cope with errors we may want to do say blob or shape detection or cleanup",
    "start": "1753610",
    "end": "1759250"
  },
  {
    "text": "using open CV which is a good computer vision framework image magic allows us",
    "start": "1759250",
    "end": "1766420"
  },
  {
    "text": "to do things like denoising and so forth and Spacey there are a lot of other",
    "start": "1766420",
    "end": "1772030"
  },
  {
    "text": "options there it's a good one for Python but that allows us to do things like label parsing and normalization if you",
    "start": "1772030",
    "end": "1778929"
  },
  {
    "text": "will and in semantic comparison between labels so there's one other thing we're",
    "start": "1778929",
    "end": "1786640"
  },
  {
    "text": "talking about here and this is you know what about things that we don't have like I talked earlier about you know",
    "start": "1786640",
    "end": "1793900"
  },
  {
    "text": "what about if we did custom celebrity's all we wanted our own set of celebrities",
    "start": "1793900",
    "end": "1800170"
  },
  {
    "text": "there are a lot of these frameworks that you could look at you could deploy this utilizing object stores so s3 on the",
    "start": "1800170",
    "end": "1807490"
  },
  {
    "text": "backend EFS for hydration and then potentially our AI Army's running say tensorflow",
    "start": "1807490",
    "end": "1815260"
  },
  {
    "text": "cafe you know whatever use case you want but a lot of these have already good",
    "start": "1815260",
    "end": "1820870"
  },
  {
    "text": "good starting points so you know YouTube ATM has about 500 thousand hours of",
    "start": "1820870",
    "end": "1826450"
  },
  {
    "text": "small video clips so if you're looking to do video analysis with deep learning",
    "start": "1826450",
    "end": "1832150"
  },
  {
    "text": "that's something worth looking at places as different places in the world they're",
    "start": "1832150",
    "end": "1837250"
  },
  {
    "text": "about 2.5 million images that are labeled and then similar for the others",
    "start": "1837250",
    "end": "1842740"
  },
  {
    "text": "Seifer is about 30 million images of which about ten or a hundred thousand",
    "start": "1842740",
    "end": "1849040"
  },
  {
    "text": "depending on the data set you picked already classified so you can take these",
    "start": "1849040",
    "end": "1855010"
  },
  {
    "text": "you can deploy them you could even use things like Nvidia digits for example which gives you a UI to work against",
    "start": "1855010",
    "end": "1862510"
  },
  {
    "text": "and run it on say p3s or you know g3s for example so getting back to the scene",
    "start": "1862510",
    "end": "1869620"
  },
  {
    "text": "change extraction I ran this movie just through a scene change ffmpeg extraction",
    "start": "1869620",
    "end": "1876970"
  },
  {
    "text": "with a threshold of 0.4 and the threshold here is if there is a change",
    "start": "1876970",
    "end": "1882760"
  },
  {
    "text": "in the scene as we're winding through all of the frames if frame a versus frame B has changed by",
    "start": "1882760",
    "end": "1890440"
  },
  {
    "text": "more than a threshold of X as I believe it's a logarithmic scale but if it's",
    "start": "1890440",
    "end": "1895600"
  },
  {
    "text": "changed but more than a threshold of X then we know it's a scene change extract",
    "start": "1895600",
    "end": "1901299"
  },
  {
    "text": "that frame and store it to local disk so you can see here they're key frames that",
    "start": "1901299",
    "end": "1906490"
  },
  {
    "text": "have been extracted using scene change extraction but the primary thing here is we didn't actually get the title of the",
    "start": "1906490",
    "end": "1914049"
  },
  {
    "text": "movie and that's because it's a fade from black and then the title appears",
    "start": "1914049",
    "end": "1919600"
  },
  {
    "text": "and then as a gradual fade to black so that first scene changed might for some reason have not been captured so",
    "start": "1919600",
    "end": "1927190"
  },
  {
    "text": "our first frame is actually you know not relevant to the content right we also",
    "start": "1927190",
    "end": "1932230"
  },
  {
    "text": "have this frame here that's a side profile view of the actor versus a front profile view so it's valid for the",
    "start": "1932230",
    "end": "1939370"
  },
  {
    "text": "objects but it's not really usable for say facial detection potentially and",
    "start": "1939370",
    "end": "1944890"
  },
  {
    "text": "then we have some things that have been extracted that have from you know from a processing perspective no context right",
    "start": "1944890",
    "end": "1952900"
  },
  {
    "text": "so 40 years later in the middle of the file doesn't really mean anything but there are some things that were valid",
    "start": "1952900",
    "end": "1958809"
  },
  {
    "text": "right so we could do facial detection here or potentially here as well but the",
    "start": "1958809",
    "end": "1964510"
  },
  {
    "text": "gist of this is you know how can we go beyond this and look at cleaning up these frames and maybe normalizing this",
    "start": "1964510",
    "end": "1971710"
  },
  {
    "text": "data set to extract better info from it so some of the things we can do better",
    "start": "1971710",
    "end": "1977230"
  },
  {
    "text": "is the first one would be you know ffmpeg with a scene change filter is",
    "start": "1977230",
    "end": "1983410"
  },
  {
    "text": "very rudimentary content aware detection you know if there is a change in pixels",
    "start": "1983410",
    "end": "1988809"
  },
  {
    "text": "store the frame but there are a lot of other projects that go a little deeper into this so Python scene detect",
    "start": "1988809",
    "end": "1996070"
  },
  {
    "text": "one it's open-source project but essentially if you think of a scene change ffmpeg will do that based on",
    "start": "1996070",
    "end": "2003780"
  },
  {
    "text": "pixel boundaries but there really isn't a scene change per se so if you look at",
    "start": "2003780",
    "end": "2009720"
  },
  {
    "text": "the graph on the right you can see some of the the dips and ebbs right and when it gets to a dip right at the bottom",
    "start": "2009720",
    "end": "2016440"
  },
  {
    "text": "there that's actually a complete fade to black but in many cases you know if it's",
    "start": "2016440",
    "end": "2021930"
  },
  {
    "text": "a fade over time over say one or two seconds the end of that is not the start",
    "start": "2021930",
    "end": "2028050"
  },
  {
    "text": "of the scene we want the middle of that so you know that event between at least",
    "start": "2028050",
    "end": "2034230"
  },
  {
    "text": "the valleys on either side of a piece of content that is the scene that we're looking for and in secondary to that we",
    "start": "2034230",
    "end": "2042270"
  },
  {
    "text": "don't want to just capture the black frame right after we've detected it because it's still fading in video",
    "start": "2042270",
    "end": "2047760"
  },
  {
    "text": "content and we actually want to be able to say maybe look after the threshold",
    "start": "2047760",
    "end": "2052980"
  },
  {
    "text": "and grab the first frame that'll potentially give us better quality and",
    "start": "2052980",
    "end": "2058020"
  },
  {
    "text": "then there's some other things we could do if we expanded that right so the pictures on the top right there is you",
    "start": "2058020",
    "end": "2065460"
  },
  {
    "text": "know film editing technique pretty common one a jump cut but a jump cut is can be useful for things like facial",
    "start": "2065460",
    "end": "2072270"
  },
  {
    "text": "detection so if we if we detect that there is a jump cut and a similarity",
    "start": "2072270",
    "end": "2077970"
  },
  {
    "text": "between you know the images on each side of that scene change potentially this is",
    "start": "2077970",
    "end": "2083730"
  },
  {
    "text": "something that could be used if we detect labels being person for more",
    "start": "2083730",
    "end": "2088889"
  },
  {
    "text": "accurate you know say actor or person identification so going back to the",
    "start": "2088890",
    "end": "2096690"
  },
  {
    "text": "image sequences I'm you know that whole thing of like the jump cut comparison comparing the two of those and then the",
    "start": "2096690",
    "end": "2103830"
  },
  {
    "text": "other problem is if we're capturing these scene changes and maybe there is a",
    "start": "2103830",
    "end": "2109650"
  },
  {
    "text": "camera cut to something but it cuts back to the same scene we don't want the process to store that same image so the",
    "start": "2109650",
    "end": "2117810"
  },
  {
    "text": "image at the top left there is this is all from one scene but you can see in in",
    "start": "2117810",
    "end": "2123750"
  },
  {
    "text": "the middle the subject has the newspaper over his face beneath that he does",
    "start": "2123750",
    "end": "2130560"
  },
  {
    "text": "if we use perceptual hashing against us which is a way of doing image comparison",
    "start": "2130560",
    "end": "2136590"
  },
  {
    "text": "based on frequency domain not based on pixels we can see that between the top",
    "start": "2136590",
    "end": "2143730"
  },
  {
    "text": "one and the middle one we have a 37 you know weighted value of difference",
    "start": "2143730",
    "end": "2149100"
  },
  {
    "text": "between those frames but the bottom one has a 5 so if we use these same",
    "start": "2149100",
    "end": "2154460"
  },
  {
    "text": "thresholds that we get back from recognition and apply it in this fashion",
    "start": "2154460",
    "end": "2160190"
  },
  {
    "text": "we may say you know anything over say a threshold detection of 10 I'm just going",
    "start": "2160190",
    "end": "2167130"
  },
  {
    "text": "to ignore because it's potentially the same people in the same scene and I shouldn't waste time reprocessing that",
    "start": "2167130",
    "end": "2173750"
  },
  {
    "text": "and perceptual image hashing is important because even if we you know",
    "start": "2173750",
    "end": "2180330"
  },
  {
    "text": "even if there's no movement in the scene the pixels if they changed by you know one hue of color value the md5 or the",
    "start": "2180330",
    "end": "2187950"
  },
  {
    "text": "sha-1 the 256 would be completely different so the goal there is you know",
    "start": "2187950",
    "end": "2194790"
  },
  {
    "text": "how can we do this image comparison not based on traditional cryptographic hash",
    "start": "2194790",
    "end": "2201030"
  },
  {
    "text": "if you will and figuring out where the frames actually do change so the",
    "start": "2201030",
    "end": "2206070"
  },
  {
    "text": "performance on this is really good you know if you wrap this in to say you know",
    "start": "2206070",
    "end": "2212940"
  },
  {
    "text": "container or you put it into lambda I think I ran it on like an i5 CPU and",
    "start": "2212940",
    "end": "2219480"
  },
  {
    "text": "about 65 milliseconds to load to 720p frames do a differential hash on them and then",
    "start": "2219480",
    "end": "2226590"
  },
  {
    "text": "compare them and get that value back is there an actual change between the frames so you can process a lot of",
    "start": "2226590",
    "end": "2233070"
  },
  {
    "text": "content without needing a GPU and then you're basically eliminating these false",
    "start": "2233070",
    "end": "2238590"
  },
  {
    "text": "positives that we don't need to store and we don't need to process with recognition so D hash is the most",
    "start": "2238590",
    "end": "2246390"
  },
  {
    "text": "performant there are a couple others a hash is just an average it gives a lot",
    "start": "2246390",
    "end": "2251400"
  },
  {
    "text": "of false positives I would avoid it P hash has been around a bit but D hash is",
    "start": "2251400",
    "end": "2257910"
  },
  {
    "text": "you know it's it's more of a comparison of the pixel gradients so",
    "start": "2257910",
    "end": "2263640"
  },
  {
    "text": "at least the three images in the middle there basically describe the process on how this hashing works and we",
    "start": "2263640",
    "end": "2270480"
  },
  {
    "text": "essentially take the source image reduce the size to a couple pixels extract the",
    "start": "2270480",
    "end": "2276810"
  },
  {
    "text": "brightness potentially and in decimate that and that basically gives us then a",
    "start": "2276810",
    "end": "2282630"
  },
  {
    "text": "matrix of values either of a pixel you know brightness range or potentially",
    "start": "2282630",
    "end": "2288450"
  },
  {
    "text": "frequency domain and that's then used to generate a hash for comparison so the",
    "start": "2288450",
    "end": "2294839"
  },
  {
    "text": "other thing we can do is image correction as a feedback loop and this one is actually you know we get this",
    "start": "2294839",
    "end": "2301950"
  },
  {
    "text": "question pretty often you know my image data is potentially dirty or I don't",
    "start": "2301950",
    "end": "2307950"
  },
  {
    "text": "know the quality of it when it comes in is there anything I can do with recognition how would I do more accurate",
    "start": "2307950",
    "end": "2314910"
  },
  {
    "text": "labels there so at least on the top right I talked earlier about this you",
    "start": "2314910",
    "end": "2321690"
  },
  {
    "text": "know recognition returning labels like atmosphere or fog where you may not",
    "start": "2321690",
    "end": "2327630"
  },
  {
    "text": "really expect that because those are sort of out of the ordinary and if you then took those labels as part of say a",
    "start": "2327630",
    "end": "2334500"
  },
  {
    "text": "step function pipeline and you reposted process the data one way to clean up",
    "start": "2334500",
    "end": "2341010"
  },
  {
    "text": "this type of you know image artifacts if you will is called rednecks color restoration so it basically looks at",
    "start": "2341010",
    "end": "2349140"
  },
  {
    "text": "from a brightness perspective how to correct the image and once you funnel it",
    "start": "2349140",
    "end": "2355200"
  },
  {
    "text": "through that this also does not be the GPU by the way it can be done say within lambda but once you process the image",
    "start": "2355200",
    "end": "2362520"
  },
  {
    "text": "through this it identifies or it does a pretty good job of surfacing the details",
    "start": "2362520",
    "end": "2367920"
  },
  {
    "text": "that are hidden so for things like atmospheric issues like this so",
    "start": "2367920",
    "end": "2374220"
  },
  {
    "text": "potentially low light it also does a really good job of identifying subjects",
    "start": "2374220",
    "end": "2379560"
  },
  {
    "text": "or bringing things back where the camera was maybe used in low light conditions",
    "start": "2379560",
    "end": "2385010"
  },
  {
    "text": "so I've passed images through it which was shot in close to darkness running",
    "start": "2385010",
    "end": "2390510"
  },
  {
    "text": "through rednecks and then actually the people are identified in the scene so",
    "start": "2390510",
    "end": "2395579"
  },
  {
    "text": "where would you start right would you write lambda functions and then use image magic I'd say that",
    "start": "2395579",
    "end": "2402579"
  },
  {
    "text": "the best place to start his recognition will return sharpness and brightness for images so you can almost look at that as",
    "start": "2402579",
    "end": "2409660"
  },
  {
    "text": "like a low cost of zero cost infrastructure to do a first pass and",
    "start": "2409660",
    "end": "2415150"
  },
  {
    "text": "determine if things are out of whack for you know color and brightness and then you could pass it through something like",
    "start": "2415150",
    "end": "2421660"
  },
  {
    "text": "potentially image magic and do a color correction for the histogram select",
    "start": "2421660",
    "end": "2426759"
  },
  {
    "text": "normalize the colors correct the color balance automatically and then maybe",
    "start": "2426759",
    "end": "2431970"
  },
  {
    "text": "advanced you would use something like color correct for an Python image loader",
    "start": "2431970",
    "end": "2438099"
  },
  {
    "text": "to basically run say ret next algorithm over the content there's some other",
    "start": "2438099",
    "end": "2446140"
  },
  {
    "text": "things on the image optimization side so one of the other you know I talked about",
    "start": "2446140",
    "end": "2451150"
  },
  {
    "text": "the image sizes pass to recognition you know it always helps to pass a good",
    "start": "2451150",
    "end": "2457900"
  },
  {
    "text": "quality image into recognition but potentially or at least we see it a lot",
    "start": "2457900",
    "end": "2463390"
  },
  {
    "text": "where you know there is a limit in the API so in terms of either if I make a",
    "start": "2463390",
    "end": "2468460"
  },
  {
    "text": "direct call it's five megabytes if I make a call using s3 saying go process",
    "start": "2468460",
    "end": "2473829"
  },
  {
    "text": "this image it's 15 megabytes how do you overcome that if you have say thirty",
    "start": "2473829",
    "end": "2479499"
  },
  {
    "text": "five-millimeter photos right that are potentially 50 plus megabyte lossless PNG s",
    "start": "2479499",
    "end": "2486660"
  },
  {
    "text": "you can compress it down but that also loses details so the smaller objects",
    "start": "2486660",
    "end": "2492700"
  },
  {
    "text": "then won't get detected or maybe the larger objects would get detected inaccurately or with a lower confidence",
    "start": "2492700",
    "end": "2498930"
  },
  {
    "text": "which is really not something we want coming out of recognition so there are a",
    "start": "2498930",
    "end": "2504249"
  },
  {
    "text": "couple ways around that one is Beamer that makes a product called jpeg money there's also PNG quant which is open",
    "start": "2504249",
    "end": "2511299"
  },
  {
    "text": "source and then mozilla has mods jpeg all of these will do perceptual",
    "start": "2511299",
    "end": "2518009"
  },
  {
    "text": "compression without human visible image",
    "start": "2518009",
    "end": "2523239"
  },
  {
    "text": "loss which is also useful for deep learning systems so on the top right there you can see",
    "start": "2523239",
    "end": "2529740"
  },
  {
    "text": "you know this particular image has been reduced from 189 K to 117 K so it's",
    "start": "2529740",
    "end": "2536820"
  },
  {
    "text": "quite a reduction and it's very hard to make out the difference that's a zoomed in part of it but if you look closer the",
    "start": "2536820",
    "end": "2545280"
  },
  {
    "text": "projector doesn't really reflect it but there is some dithering on the edges that's being introduced but you know you",
    "start": "2545280",
    "end": "2552240"
  },
  {
    "text": "would only see that if you rarely zoom into it so how does this help so this",
    "start": "2552240",
    "end": "2558330"
  },
  {
    "text": "image has been cleaned up with the first pass with Retton X correction and then",
    "start": "2558330",
    "end": "2564030"
  },
  {
    "text": "passed into face detection and I think the important thing here is that you can",
    "start": "2564030",
    "end": "2571230"
  },
  {
    "text": "see the one on the left has an age range detection or estimate of about four to nine years old it's clear this is not a",
    "start": "2571230",
    "end": "2578580"
  },
  {
    "text": "nine-year-old subject but if you then look at the one on the right it's now more accurate because we have better",
    "start": "2578580",
    "end": "2584760"
  },
  {
    "text": "details in the picture at around four to seven the other things that are",
    "start": "2584760",
    "end": "2590910"
  },
  {
    "text": "important here is the confidence factors have gone up now as well so not just it",
    "start": "2590910",
    "end": "2597030"
  },
  {
    "text": "looks like a face but our smiling has gone up from ninety eight point one to ninety-nine point nine so this is very",
    "start": "2597030",
    "end": "2604619"
  },
  {
    "text": "important from that perspective because for a lot of at least recognition",
    "start": "2604619",
    "end": "2610830"
  },
  {
    "text": "operations you'll want to set a threshold and keep that really high so that you don't have false positives",
    "start": "2610830",
    "end": "2617330"
  },
  {
    "text": "returned that are at a lower threshold so that's all the image processing piece",
    "start": "2617330",
    "end": "2624390"
  },
  {
    "text": "right so it's thinking maybe it's interesting to talk about dogs cats and",
    "start": "2624390",
    "end": "2629700"
  },
  {
    "text": "bananas but essentially the concept here is like the terminology or the labels",
    "start": "2629700",
    "end": "2636780"
  },
  {
    "text": "that come back from recognition how do we apply our own custom taxonomy over",
    "start": "2636780",
    "end": "2642300"
  },
  {
    "text": "that so we have a lot of customers that maybe use this for large image archives",
    "start": "2642300",
    "end": "2648359"
  },
  {
    "text": "and they already have a taxonomy and they really want to align those labels with their internal way of tracking",
    "start": "2648359",
    "end": "2655890"
  },
  {
    "text": "assets etc so maybe everything that's I identifies a person people person",
    "start": "2655890",
    "end": "2663410"
  },
  {
    "text": "you know child etc they just want to classify that as human for example so",
    "start": "2663410",
    "end": "2669859"
  },
  {
    "text": "one of the ways around this is to use NLP with lambda as a way to basically",
    "start": "2669859",
    "end": "2676660"
  },
  {
    "text": "calculate the semantic similarity and it's a big advantage to doing it this",
    "start": "2676660",
    "end": "2682789"
  },
  {
    "text": "way versus trying to keep lists and arrays of words and then trying to match",
    "start": "2682789",
    "end": "2688190"
  },
  {
    "text": "across that via something like reg X it's a lot faster than that so one of",
    "start": "2688190",
    "end": "2694280"
  },
  {
    "text": "the examples here is to use something like Spacey which is I talked about",
    "start": "2694280",
    "end": "2700460"
  },
  {
    "text": "earlier so it comes with a bunch of models that you can use for NLP modeling",
    "start": "2700460",
    "end": "2706069"
  },
  {
    "text": "I think like one of the important things here is that these models are done as a",
    "start": "2706069",
    "end": "2712039"
  },
  {
    "text": "you know high performance and then you get vector comparison against them but",
    "start": "2712039",
    "end": "2718549"
  },
  {
    "text": "the corpus of the size of words etc is about six hundred and eighty thousand",
    "start": "2718549",
    "end": "2724329"
  },
  {
    "text": "individual words so from a very basic perspective this can be really useful",
    "start": "2724329",
    "end": "2730190"
  },
  {
    "text": "right so on the right hand side is using Spacey for simple sentiment analysis so",
    "start": "2730190",
    "end": "2737270"
  },
  {
    "text": "essentially all of the labels that come back from recognition we assign as tokens and put into space run NLP",
    "start": "2737270",
    "end": "2744559"
  },
  {
    "text": "against it and this is actually using a higher order level framework called text",
    "start": "2744559",
    "end": "2750140"
  },
  {
    "text": "asset II which is it's the built on top of Spacey as a lower-level framework for",
    "start": "2750140",
    "end": "2755809"
  },
  {
    "text": "NLP if you will but essentially this gives us a bag of words returns by",
    "start": "2755809",
    "end": "2761420"
  },
  {
    "text": "weight so we can see which are the most common things that are coming back from this video clip if we're collecting all",
    "start": "2761420",
    "end": "2768559"
  },
  {
    "text": "of these labels on the bottom side you can see you at least the bottom line",
    "start": "2768559",
    "end": "2774500"
  },
  {
    "text": "piece of code there is basically similarity scoring so we basically want",
    "start": "2774500",
    "end": "2780500"
  },
  {
    "text": "to look at essentially we have you know car boat motorbike but we have say a",
    "start": "2780500",
    "end": "2787520"
  },
  {
    "text": "genre for the media we want to process and we want to identify things such as vehicles we can then iterate through",
    "start": "2787520",
    "end": "2794900"
  },
  {
    "text": "these labels and then get semantic similarity returned on those tokens so the same way that recognition",
    "start": "2794900",
    "end": "2803109"
  },
  {
    "text": "classifies objects by confidence score Spacey does the same thing for words so",
    "start": "2803109",
    "end": "2810499"
  },
  {
    "text": "you can take those returned object identifiers and their confidence scores",
    "start": "2810499",
    "end": "2817309"
  },
  {
    "text": "take the words and then compare that against say something you're trying to",
    "start": "2817309",
    "end": "2822619"
  },
  {
    "text": "figure out are all these things that have come back from this frame similar to this topic I'm trying to model then",
    "start": "2822619",
    "end": "2832069"
  },
  {
    "text": "the other the other last thing here just before I get to the summary is the label",
    "start": "2832069",
    "end": "2838309"
  },
  {
    "text": "data storage right so where do you store the data and how should you store it",
    "start": "2838309",
    "end": "2843910"
  },
  {
    "text": "anything coming back from recognition so at least the one thing to do especially",
    "start": "2843910",
    "end": "2849109"
  },
  {
    "text": "with the video frames were analyzing and the fact that we may be doing scene change analysis is to basically store",
    "start": "2849109",
    "end": "2856249"
  },
  {
    "text": "maybe the wall clock time if it's live or potentially store you know the 70",
    "start": "2856249",
    "end": "2862459"
  },
  {
    "text": "timecode offset so if we're using ffmpeg it will actually return the pts offset",
    "start": "2862459",
    "end": "2868549"
  },
  {
    "text": "so the packets stamp offset so that's very useful and then we can actually do",
    "start": "2868549",
    "end": "2875329"
  },
  {
    "text": "things like inject that data into elasticsearch and/or DynamoDB so the",
    "start": "2875329",
    "end": "2882619"
  },
  {
    "text": "benefit of using it this way in elastic search with more data in addition to these labels that we've normalized is",
    "start": "2882619",
    "end": "2889969"
  },
  {
    "text": "that we can then do you know fairly common searches across that and then",
    "start": "2889969",
    "end": "2895219"
  },
  {
    "text": "find out over time whether these words or whether these terms occur one thing I",
    "start": "2895219",
    "end": "2901189"
  },
  {
    "text": "have seen is that customers often ask you know should should I scale it should I size my cluster for elasticsearch to a",
    "start": "2901189",
    "end": "2908809"
  },
  {
    "text": "certain size although one way you could look at it is if you have a supply chain",
    "start": "2908809",
    "end": "2914089"
  },
  {
    "text": "and your landing content in it and you're using elastic search for maybe your operator dashboard is to",
    "start": "2914089",
    "end": "2920859"
  },
  {
    "text": "potentially use smaller clusters with the data and then prune those collections using something like",
    "start": "2920859",
    "end": "2927589"
  },
  {
    "text": "elasticsearch curator over the top of that so essentially that gives you for those that are familiar",
    "start": "2927589",
    "end": "2933560"
  },
  {
    "text": "with MongoDB is an example pretty close to the notion of a cap collection where",
    "start": "2933560",
    "end": "2940070"
  },
  {
    "text": "we sort of have the sliding window but we don't have to allocate potentially a lot of resources or do a lot of cleanup",
    "start": "2940070",
    "end": "2946630"
  },
  {
    "text": "over time so just getting back to you know taking Spacey and Texas City and",
    "start": "2946630",
    "end": "2954890"
  },
  {
    "text": "then identifying these tokens across this video clip right to this trailer if",
    "start": "2954890",
    "end": "2962480"
  },
  {
    "text": "you look at the one on the left that's the non normalized version so essentially we have a lot of people",
    "start": "2962480",
    "end": "2968300"
  },
  {
    "text": "human person that's great but it's not really useful because I can't really extract any pertinent information out of",
    "start": "2968300",
    "end": "2974810"
  },
  {
    "text": "that but on the right hand side with you know NLP modeling we reduce it to",
    "start": "2974810",
    "end": "2981050"
  },
  {
    "text": "basically statistical analysis and then we can you know move the high-water mark",
    "start": "2981050",
    "end": "2986360"
  },
  {
    "text": "and a low watermark out and you can then see that some of the terminology like",
    "start": "2986360",
    "end": "2991460"
  },
  {
    "text": "military for example is it's better exposed so given that the clip is you",
    "start": "2991460",
    "end": "2997880"
  },
  {
    "text": "know sci-fi movie trailer these are much more relevant labels that",
    "start": "2997880",
    "end": "3003280"
  },
  {
    "text": "that have been come that we got back so just to wrap up you know we can sanitize",
    "start": "3003280",
    "end": "3010540"
  },
  {
    "text": "the media but that actually enables us to get better confidence scores and better positive positives back from",
    "start": "3010540",
    "end": "3017560"
  },
  {
    "text": "recognition versus having to you know maybe do multi stage clean-up on that",
    "start": "3017560",
    "end": "3023110"
  },
  {
    "text": "data but the types of things that we could do since we then have normalized this data essentially we've used the you",
    "start": "3023110",
    "end": "3031450"
  },
  {
    "text": "know scene changes etc to basically visually topic model this data right so",
    "start": "3031450",
    "end": "3037540"
  },
  {
    "text": "it's got a lot of use cases from there we could you know potentially use multi",
    "start": "3037540",
    "end": "3043150"
  },
  {
    "text": "stage transcoding or we could then pass a genre or with an encoding profile -",
    "start": "3043150",
    "end": "3049300"
  },
  {
    "text": "you know Rhian code content a certain fashion we could also do timeline based",
    "start": "3049300",
    "end": "3054640"
  },
  {
    "text": "discovery so if you think of you know what if you wanted to build something like x-ray which",
    "start": "3054640",
    "end": "3062109"
  },
  {
    "text": "you know Amazon video has right one of the options here is to do the scene change analysis and then look at things",
    "start": "3062109",
    "end": "3069160"
  },
  {
    "text": "like those jump cuts where the actual scenes are and then do multi pass",
    "start": "3069160",
    "end": "3074259"
  },
  {
    "text": "against the actors is it a person okay let's go and do you know celebrity",
    "start": "3074259",
    "end": "3080170"
  },
  {
    "text": "detection of detection against our own internal data set in a collection and",
    "start": "3080170",
    "end": "3085839"
  },
  {
    "text": "then utilize that as a way to build something like a x-ray type service so",
    "start": "3085839",
    "end": "3091119"
  },
  {
    "text": "pretty useful from that perspective and then lastly you know if we are getting labels backs of say for example",
    "start": "3091119",
    "end": "3098230"
  },
  {
    "text": "atmosphere as a label we could use that as some layer of intelligence if you",
    "start": "3098230",
    "end": "3103390"
  },
  {
    "text": "will in order to drive cleaning up our content such as rednecks algorithm",
    "start": "3103390",
    "end": "3108700"
  },
  {
    "text": "applications there so with that thank you there are a couple other sessions",
    "start": "3108700",
    "end": "3115420"
  },
  {
    "text": "that are very much related to to this one also on the recognition genre if you",
    "start": "3115420",
    "end": "3122559"
  },
  {
    "text": "will I think there's also a workshop coming up so definitely check these out",
    "start": "3122559",
    "end": "3128650"
  },
  {
    "text": "too so thanks and I'll take any questions",
    "start": "3128650",
    "end": "3133739"
  },
  {
    "text": "yes",
    "start": "3139069",
    "end": "3142069"
  },
  {
    "text": "so yeah so the question was how accurate would this be for something like",
    "start": "3160789",
    "end": "3166289"
  },
  {
    "text": "sentiment analysis where most of the objects coming back are objects so I",
    "start": "3166289",
    "end": "3171659"
  },
  {
    "text": "think that's where you would look at using the ML NLP processing to",
    "start": "3171659",
    "end": "3176989"
  },
  {
    "text": "differentiate between the objects as labels and other things like the concepts and scenes so recognition will",
    "start": "3176989",
    "end": "3185699"
  },
  {
    "text": "give you back all those labels but it's often very hard to tell without looking",
    "start": "3185699",
    "end": "3190799"
  },
  {
    "text": "at it with NLP the difference between an object and a scene or a concept so if",
    "start": "3190799",
    "end": "3196709"
  },
  {
    "text": "you were to use that as a way to separate that you could essentially just drop all of that data on the side and",
    "start": "3196709",
    "end": "3202049"
  },
  {
    "text": "then you've extracted your you know scenes for example or concepts and the",
    "start": "3202049",
    "end": "3208890"
  },
  {
    "text": "thing is you can also then use that as like a second and third dimension for tagging that insane asset management",
    "start": "3208890",
    "end": "3215609"
  },
  {
    "text": "system because that's more beyond you know what you'd have with simple object tagging yeah so the question was could",
    "start": "3215609",
    "end": "3238409"
  },
  {
    "text": "this work for extracting rolling credits so yes but that would be a technique",
    "start": "3238409",
    "end": "3244799"
  },
  {
    "text": "where you may look at the scene changes and then you know your credits are gonna",
    "start": "3244799",
    "end": "3249839"
  },
  {
    "text": "roll last and there's typically not a scene change but since its flipped two",
    "start": "3249839",
    "end": "3255059"
  },
  {
    "text": "credits you know where the boundaries start is of that in terms of the frame offset or the time offset and then",
    "start": "3255059",
    "end": "3261779"
  },
  {
    "text": "basically use that and then extract every single frame off to that and then",
    "start": "3261779",
    "end": "3266789"
  },
  {
    "text": "pass that through OCR recognition so yes",
    "start": "3266789",
    "end": "3273329"
  },
  {
    "text": "so so the OCR functionality on recognition does a really good job of",
    "start": "3273329",
    "end": "3278479"
  },
  {
    "text": "fonts and typefaces even handwritten words for example but it also does a",
    "start": "3278479",
    "end": "3284309"
  },
  {
    "text": "good job of if though if the lettering or the typeface is being rotated tilted",
    "start": "3284309",
    "end": "3289499"
  },
  {
    "text": "or pet so it'll give you confidence scores back on its ability to determine I'm 80%",
    "start": "3289499",
    "end": "3297590"
  },
  {
    "text": "confident that this is wood X so if you have for example a logo and you know",
    "start": "3297590",
    "end": "3304040"
  },
  {
    "text": "maybe the O is replaced with a graphic or something as part of say a branding logo it may infer that you know that's",
    "start": "3304040",
    "end": "3311300"
  },
  {
    "text": "an O for example but it may be a lower confidence because of that but you take that output and then potentially process",
    "start": "3311300",
    "end": "3318860"
  },
  {
    "text": "that using NLP yes",
    "start": "3318860",
    "end": "3324430"
  },
  {
    "text": "internally you mean as part of recognition yeah that's actually a good",
    "start": "3339230",
    "end": "3345170"
  },
  {
    "text": "question so the question was why aren't these image processing algorithms run",
    "start": "3345170",
    "end": "3351380"
  },
  {
    "text": "internally as part of recognition as part of say a pre clean up I think maybe",
    "start": "3351380",
    "end": "3356960"
  },
  {
    "text": "the one one of the reasons there is that what's to prevent the image processing",
    "start": "3356960",
    "end": "3365000"
  },
  {
    "text": "processing it the wrong way and then you basically are creating garbage to feed through the system so I think one of the",
    "start": "3365000",
    "end": "3372080"
  },
  {
    "text": "concepts there is the other thing is it no longer becomes deterministic right",
    "start": "3372080",
    "end": "3378170"
  },
  {
    "text": "because how many how many stages in your pipeline do you then need to process this image data and Attwood going",
    "start": "3378170",
    "end": "3385400"
  },
  {
    "text": "through that pipeline internally would then affect the deterministic nature of returning those labels so doing that at",
    "start": "3385400",
    "end": "3392690"
  },
  {
    "text": "a huge scale may mean that you know that particular feature which is a really",
    "start": "3392690",
    "end": "3399380"
  },
  {
    "text": "good feature for recognition may be skewed or you may not have control over that so lambdas really good at like",
    "start": "3399380",
    "end": "3406160"
  },
  {
    "text": "hundred milliseconds like all of these things I talked about the cleanup even the more advanced ones let's say RL",
    "start": "3406160",
    "end": "3413090"
  },
  {
    "text": "filters for image magic you know that look at doing like say an unsharp mask",
    "start": "3413090",
    "end": "3419869"
  },
  {
    "text": "and the content without embossing the edges even those will execute within a",
    "start": "3419869",
    "end": "3425660"
  },
  {
    "text": "couple hundred milliseconds max and it depends on the image size you know on",
    "start": "3425660",
    "end": "3431119"
  },
  {
    "text": "what you're doing and what that image magic script looks like but you know",
    "start": "3431119",
    "end": "3436760"
  },
  {
    "text": "lambdas are really good fit for that if you're not dealing with huge images that you have to slice up like if it's",
    "start": "3436760",
    "end": "3442730"
  },
  {
    "text": "gigapixel images you want to funnel that through something like you know ECS or containers or AWS batch any other",
    "start": "3442730",
    "end": "3451520"
  },
  {
    "text": "questions nope oh sorry",
    "start": "3451520",
    "end": "3458770"
  },
  {
    "text": "so the question is is it possible to differentiate between someone holding a",
    "start": "3472280",
    "end": "3477470"
  },
  {
    "text": "photo of versus the person themselves using depth analysis so if you do facial",
    "start": "3477470",
    "end": "3483619"
  },
  {
    "text": "analysis with recognition and this is typical for things like authentication",
    "start": "3483619",
    "end": "3489020"
  },
  {
    "text": "where someone walks up and stands in front of a webcam recognition will not",
    "start": "3489020",
    "end": "3494390"
  },
  {
    "text": "grab the people behind and it's not because of depth it's because it grabs the largest face front and center first",
    "start": "3494390",
    "end": "3501220"
  },
  {
    "text": "so that can be a benefit in this use case or could potentially not be a benefit but one of the things I listed",
    "start": "3501220",
    "end": "3508579"
  },
  {
    "text": "on the facial analysis slide in terms of crowd detection that allows you to do a",
    "start": "3508579",
    "end": "3514490"
  },
  {
    "text": "one pass pass an image in and we returned up to a hundred faces if there",
    "start": "3514490",
    "end": "3519650"
  },
  {
    "text": "are a hundred and one people in that shot that hundred and first person will have the smallest pixel density face so",
    "start": "3519650",
    "end": "3527119"
  },
  {
    "text": "X by Y so it sort of goes from the largest working its way through to the smallest",
    "start": "3527119",
    "end": "3534160"
  },
  {
    "text": "yes so if you're passing the question there was and I guess this is you know could this be used as a way to yeah",
    "start": "3544170",
    "end": "3552340"
  },
  {
    "text": "bypass an authentication system yeah it entirely could be used for that because",
    "start": "3552340",
    "end": "3557890"
  },
  {
    "text": "it's not a biometric process right so the facial detection is based on and and",
    "start": "3557890",
    "end": "3566020"
  },
  {
    "text": "it's really hard to think of is is there a way to do that you couldn't do that via color correction because if I held",
    "start": "3566020",
    "end": "3571870"
  },
  {
    "text": "the photo at the right location you know it may be valid from like contrast",
    "start": "3571870",
    "end": "3577230"
  },
  {
    "text": "sharpness perspective and color perspective maybe the one thing you",
    "start": "3577230",
    "end": "3582490"
  },
  {
    "text": "could do is get multiple shots of the same person so in other words in order",
    "start": "3582490",
    "end": "3589720"
  },
  {
    "text": "for you to authenticate successfully we need to grab three photos of your face",
    "start": "3589720",
    "end": "3595210"
  },
  {
    "text": "and we need to make sure that everyone is at a different level of accuracy so the watermark would be like say I think",
    "start": "3595210",
    "end": "3602830"
  },
  {
    "text": "there's a default of like 90% maybe 85% and I expect to see at least a two to",
    "start": "3602830",
    "end": "3608800"
  },
  {
    "text": "three percent differential between the first second and third face and that way",
    "start": "3608800",
    "end": "3614290"
  },
  {
    "text": "anyone holding up a photo even if they tilted or pitched it recognition will",
    "start": "3614290",
    "end": "3619480"
  },
  {
    "text": "look and it'll see you know the pitch your tilt asmath etc and track for that and align",
    "start": "3619480",
    "end": "3625480"
  },
  {
    "text": "it correctly those would all be within the same maybe 1% so you'll be able to",
    "start": "3625480",
    "end": "3631600"
  },
  {
    "text": "say this is someone holding up a photo and trying to move it around to get through the system sure any other",
    "start": "3631600",
    "end": "3640180"
  },
  {
    "text": "questions nope Thanks",
    "start": "3640180",
    "end": "3645890"
  },
  {
    "text": "[Applause]",
    "start": "3645890",
    "end": "3650890"
  }
]