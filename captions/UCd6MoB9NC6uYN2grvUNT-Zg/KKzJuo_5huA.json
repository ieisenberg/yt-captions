[
  {
    "start": "0",
    "end": "312000"
  },
  {
    "text": "okay we have a lot to cover today so i'm going to get started good morning my name is matt yanchin don't worry about",
    "start": "4000",
    "end": "10719"
  },
  {
    "text": "pronouncing my last name you can call me matt y and i'm a solutions architect at amazon web services and i'm really excited to",
    "start": "10719",
    "end": "17199"
  },
  {
    "text": "do this session today it's doing a lot of different events and sessions that reinvent but this is my favorite because",
    "start": "17199",
    "end": "22720"
  },
  {
    "text": "together we're actually going to build something we're going to try the famous live demo we'll see how that",
    "start": "22720",
    "end": "28160"
  },
  {
    "text": "goes we have a video just in case but together we're going to",
    "start": "28160",
    "end": "33760"
  },
  {
    "text": "build a big data application i was thinking of different ways we could do this and i thought that the best way to learn",
    "start": "33760",
    "end": "40719"
  },
  {
    "text": "about your first big data application was to build a big data application that leverages several services so rather than stand up",
    "start": "40719",
    "end": "47280"
  },
  {
    "text": "here and just tell you about the services that you could use together we can actually use them together to",
    "start": "47280",
    "end": "52960"
  },
  {
    "text": "do something real so that's what we're here to do today so unfortunately i had to prepare for",
    "start": "52960",
    "end": "59520"
  },
  {
    "text": "this session so i wasn't able to attend the keynote today but that amazon aurora announcement is amazing it's our new",
    "start": "59520",
    "end": "65680"
  },
  {
    "text": "high performance relational database and didn't quite make it onto my slides here but it's an amazing and exciting new",
    "start": "65680",
    "end": "72320"
  },
  {
    "text": "addition to our big data portfolio so what you see here is the big data set of services at aws",
    "start": "72320",
    "end": "79360"
  },
  {
    "text": "we have a number of services to allow you to collect and store and process and and then automate",
    "start": "79360",
    "end": "85200"
  },
  {
    "text": "everything you need to do with big data pipelines it would be impossible in 40 minutes to cover every",
    "start": "85200",
    "end": "90720"
  },
  {
    "text": "single one of these services but what we'll do is just get your feet wet we'll do a little introduction for the ones that",
    "start": "90720",
    "end": "96240"
  },
  {
    "text": "we'll be covering in our application today and then hopefully it'll generate some interest and you can go deep on some of the services",
    "start": "96240",
    "end": "102560"
  },
  {
    "text": "personally i do a lot of elastic mapreduce work and in the work that i do i'm based out of new york we work with a lot of life sciences",
    "start": "102560",
    "end": "109040"
  },
  {
    "text": "companies and financial services companies people who have big big applications with a lot of data to process",
    "start": "109040",
    "end": "115520"
  },
  {
    "text": "so starting from the top with collect we have our direct connect service and this service allows you to make a",
    "start": "115520",
    "end": "121360"
  },
  {
    "text": "private connection between your co-location facilities and amazon's facilities so you can essentially build a private cloud if you",
    "start": "121360",
    "end": "126880"
  },
  {
    "text": "want with no public facing connectivity and have a private connection a low latency private",
    "start": "126880",
    "end": "132000"
  },
  {
    "text": "connection between amazon and yourself this is really important for a lot of the businesses that i work with because it not only allows them to",
    "start": "132000",
    "end": "137440"
  },
  {
    "text": "push a lot of data at scale into amazon but it also gives them that that",
    "start": "137440",
    "end": "143440"
  },
  {
    "text": "physical isolation that they sometimes require there's a joke that sometimes fedex is",
    "start": "143440",
    "end": "148879"
  },
  {
    "text": "faster than the internet so that's why we have aws import export i don't know about you but how many times have you had",
    "start": "148879",
    "end": "155120"
  },
  {
    "text": "you know i've had many occasions where i've had dozens of terabytes that i've needed to get up in the cloud and",
    "start": "155120",
    "end": "160239"
  },
  {
    "text": "on my little you know files line at home that that takes a while to upload so that's why we have import export you",
    "start": "160239",
    "end": "165360"
  },
  {
    "text": "can ship us the drives and it's a great way to on-ramp onto big data applications on aws",
    "start": "165360",
    "end": "170400"
  },
  {
    "text": "because you can literally send us the hard drives and we'll load them up into s3 and glacier and other storage systems for you we",
    "start": "170400",
    "end": "177519"
  },
  {
    "text": "also have a newer service called amazon kinesis that we're going to use today it allows you to durably store real-time",
    "start": "177519",
    "end": "183040"
  },
  {
    "text": "incoming streaming data at scale it's a really exciting service and hopefully today you'll see it during",
    "start": "183040",
    "end": "188159"
  },
  {
    "text": "the demo um how it works and how you can make it work for you what most people do is they take data",
    "start": "188159",
    "end": "194319"
  },
  {
    "text": "into kinesis and then they need to persist it somewhere durable and that's why s3 is great because it allows you to persist",
    "start": "194319",
    "end": "199519"
  },
  {
    "text": "the data somewhere durable that's high performance that you can also use in your big data applications and we'll see today how you",
    "start": "199519",
    "end": "205280"
  },
  {
    "text": "can actually use s3 with hadoop with emr instead of using hdfs and that combination of elastic",
    "start": "205280",
    "end": "212239"
  },
  {
    "text": "mapreduce of emr with s3 that durable high performance cloud storage is really compelling",
    "start": "212239",
    "end": "218879"
  },
  {
    "text": "dynamodb is our nosql data store we're going to use it to store some metadata that'll allow us to checkpoint that just",
    "start": "218879",
    "end": "224400"
  },
  {
    "text": "is a fancy way of saying if we're going to process a bunch of events in our feed in a big data application we need to keep track of",
    "start": "224400",
    "end": "229599"
  },
  {
    "text": "what we process so dynamodb is a great way to do that it's a low latency high performance nosql datastore",
    "start": "229599",
    "end": "235280"
  },
  {
    "text": "that's not expensive at all we're not going to use glacier today but glacier is s3's cousin it's our cold",
    "start": "235280",
    "end": "241680"
  },
  {
    "text": "storage service it's a great place to store data that you may not you may use one day you're not sure",
    "start": "241680",
    "end": "246959"
  },
  {
    "text": "if you need to use it today i met a customer yesterday who has a pretty cool use case they put all of their data in glacier because",
    "start": "246959",
    "end": "253200"
  },
  {
    "text": "they do analytics pretty rarely and when they need to do analytics they bring it out of glacier and they",
    "start": "253200",
    "end": "258320"
  },
  {
    "text": "persist it in in s3 reduced redundancy storage which doesn't give you quite the same high durability that you get",
    "start": "258320",
    "end": "264720"
  },
  {
    "text": "with regular s3 storage but it's a lot cheaper and for his purposes because he knows the data is safe in glacier",
    "start": "264720",
    "end": "270320"
  },
  {
    "text": "he can keep it in glacier and then put it in rrs and reduce redundancy storage and then process it there for a lot cheaper when he needs it it's",
    "start": "270320",
    "end": "276639"
  },
  {
    "text": "a really cool workflow i really like that one emr is going to be at the center of",
    "start": "276639",
    "end": "281759"
  },
  {
    "text": "today's demo it's uh it's our hadoop as a service it allows you to",
    "start": "281759",
    "end": "286880"
  },
  {
    "text": "run a cluster that can run hadoop and a lot of applications on top of hadoop and we'll talk about some of them today",
    "start": "286880",
    "end": "292880"
  },
  {
    "text": "redshift our data warehousing application we're going to push data from emr into redshift and you'll see how",
    "start": "292880",
    "end": "297919"
  },
  {
    "text": "that works and then of course ec2 you know you can use our managed services but you're free to use any tool that you want to use and",
    "start": "297919",
    "end": "303759"
  },
  {
    "text": "build it up on ec2 almost everything out there in the big data ecosystem can run on ec2",
    "start": "303759",
    "end": "311840"
  },
  {
    "start": "312000",
    "end": "362000"
  },
  {
    "text": "so this is what we're going to build this is a pretty classic design pattern and i think once you",
    "start": "312479",
    "end": "317680"
  },
  {
    "text": "start working with big data you'll see that this type of pattern applies to a lot of different verticals a lot of different",
    "start": "317680",
    "end": "322960"
  },
  {
    "text": "solutions whether you're in marketing digital analytics and life sciences and",
    "start": "322960",
    "end": "328240"
  },
  {
    "text": "doing things like genomics or you know finance going through trades and stocks etc this kind of a",
    "start": "328240",
    "end": "335120"
  },
  {
    "text": "design pattern where you're aggregating information in this case logs pushing it into some kind of a durable stream",
    "start": "335120",
    "end": "341199"
  },
  {
    "text": "in this case kinesis processing it we're using emr for this and then persisting it in s3 and using",
    "start": "341199",
    "end": "347600"
  },
  {
    "text": "amazon redshift to back your visualization layer this is a really common design pattern that we see all the time",
    "start": "347600",
    "end": "352960"
  },
  {
    "text": "this is of course quite high level but it's a great building block a starting point for a pretty general purpose big data",
    "start": "352960",
    "end": "359199"
  },
  {
    "text": "application so what are we going to build together today we're going to collect some log",
    "start": "359199",
    "end": "365440"
  },
  {
    "start": "362000",
    "end": "436000"
  },
  {
    "text": "entries i'm using a tool called log for j which is just a way to grab log entries and and push them",
    "start": "365440",
    "end": "371520"
  },
  {
    "text": "somewhere in this case we're going to push them into a kinesis stream we're going to launch a hadoop cluster and we're also going to launch a",
    "start": "371520",
    "end": "377199"
  },
  {
    "text": "redshift data warehouse cluster we're going to use a hadoop application called hive that i'll talk about in a",
    "start": "377199",
    "end": "382639"
  },
  {
    "text": "second to check out this data what's really cool about running hive on emr specifically is you can actually",
    "start": "382639",
    "end": "390800"
  },
  {
    "text": "access your kinesis stream your incoming streaming data directly from your hadoop cluster and we'll show you how to do that",
    "start": "390800",
    "end": "397600"
  },
  {
    "text": "then we're going to take the data transform a little bit use emr effectively as an etl engine do some transformation and then push",
    "start": "397600",
    "end": "404000"
  },
  {
    "text": "that transform data into redshift and then run some queries against redshift",
    "start": "404000",
    "end": "411039"
  },
  {
    "text": "so going back to that diagram for a second it's going to kind of look like this log for j pushing some logs apache logs in this",
    "start": "411039",
    "end": "416160"
  },
  {
    "text": "case into a stream using the kinesis connector for emr for hadoop and hive in this case",
    "start": "416160",
    "end": "421759"
  },
  {
    "text": "to query the stream directly persist that data in s3 and then load in",
    "start": "421759",
    "end": "427599"
  },
  {
    "text": "parallel into redshift a very common design pattern and hopefully it forms a good building block for you to learn about big data",
    "start": "427599",
    "end": "436400"
  },
  {
    "text": "so now we're starting to get into the heavy text that's a pretty uh actually that's not such a nasty graph i don't know how many of you worked with",
    "start": "436400",
    "end": "442400"
  },
  {
    "text": "uh some of the marketing analytics uh log files can have like 50 60 columns",
    "start": "442400",
    "end": "448479"
  },
  {
    "text": "you know and this is one of the big reasons why we do etl is because we need to clean things up so say for example we had a log file like",
    "start": "448479",
    "end": "454560"
  },
  {
    "text": "this from apache in this case the apache web server httpd and we had another",
    "start": "454560",
    "end": "459919"
  },
  {
    "text": "you know web server iis or something else or some obscure one that had the time stamp in another time zone",
    "start": "459919",
    "end": "465360"
  },
  {
    "text": "not not utc or not gmt and we needed to normalize those time stamps so we could put them into a redshift table and run a query against",
    "start": "465360",
    "end": "471360"
  },
  {
    "text": "it that's a classic you know data transformation problem and that's kind of what we're going to address today and you can use",
    "start": "471360",
    "end": "476720"
  },
  {
    "text": "a tool like emr and hive or pig or other hadoop applications to normalize your data to transform your",
    "start": "476720",
    "end": "482400"
  },
  {
    "text": "data so that it can be ready for queries so that you can take action so you can drive benefit from it",
    "start": "482400",
    "end": "489759"
  },
  {
    "text": "so today we're going to be using the aws cli the command line tool now i'm also going to show you what it looks like in the console but uh",
    "start": "491520",
    "end": "498800"
  },
  {
    "text": "you're going to be able to take this lab home with you and try it in your own account it should only cost you a few cents maybe a couple bucks maximum",
    "start": "498800",
    "end": "505440"
  },
  {
    "text": "to go through this entire lab so feel free to take what i show you today and modify it for your own needs",
    "start": "505440",
    "end": "511440"
  },
  {
    "text": "and purposes and try it and try some creative stuff and let me know you know get in touch i'm always curious to hear what customers",
    "start": "511440",
    "end": "517760"
  },
  {
    "text": "are doing and what you ultimately build out of this building block but today i'm going to use the cli just to show you how easy it is",
    "start": "517760",
    "end": "523760"
  },
  {
    "text": "to run stuff using the command line if you're not comfortable the command line that's fine but it'll also give you a little glimpse on how easy it is to",
    "start": "523760",
    "end": "530080"
  },
  {
    "text": "automate these workflows using things like the command line interface or the api or some of the lower level ways",
    "start": "530080",
    "end": "535120"
  },
  {
    "text": "to launch resources with aws so let's",
    "start": "535120",
    "end": "540480"
  },
  {
    "text": "start so here i am in a console i've actually logged into an ec2 instance",
    "start": "546839",
    "end": "553760"
  },
  {
    "text": "i've installed just the basic aws command line i'm using ssh to log into this instance",
    "start": "553760",
    "end": "563839"
  },
  {
    "text": "i have a mouse that's maybe easier oops sorry they have logged me out let",
    "start": "565360",
    "end": "571440"
  },
  {
    "text": "me get back in there",
    "start": "571440",
    "end": "577839"
  },
  {
    "text": "okay so this is the aws cli the command line way to launch an emr cluster hadoop",
    "start": "582880",
    "end": "587920"
  },
  {
    "text": "cluster as you can see in the aws application the the type of aws service i would like",
    "start": "587920",
    "end": "593440"
  },
  {
    "text": "to launch and i'm going to create a new cluster in this case i'm going to install the hive application",
    "start": "593440",
    "end": "598720"
  },
  {
    "text": "i'm installing a apache hadoop version 2 and it's going to be a three node cluster made up of three m3 extra large",
    "start": "598720",
    "end": "605120"
  },
  {
    "text": "instances there now let's go to the console",
    "start": "605120",
    "end": "618480"
  },
  {
    "text": "i can't actually see the end there so there's our cluster that started if we look inside",
    "start": "618480",
    "end": "625600"
  },
  {
    "text": "you'll see that it's provisioning the three nodes that i specified like dig a little deeper you can see",
    "start": "626320",
    "end": "633360"
  },
  {
    "text": "it's installing the latest version of hive our distribution of hadoop hadoop is like linux in that way you use that word",
    "start": "633360",
    "end": "639839"
  },
  {
    "text": "distribution with linux you have debian and ubuntu and red hat and these other distributions hadoop is kind of the same",
    "start": "639839",
    "end": "645519"
  },
  {
    "text": "on aws emr or amazon emr you can run several distributions currently map r and r amazon",
    "start": "645519",
    "end": "652480"
  },
  {
    "text": "distribution which is pretty close to apache",
    "start": "652480",
    "end": "656480"
  },
  {
    "text": "so if i look under steps we have this concept of steps in hadoop and an emr and steps are really just tasks that you",
    "start": "657680",
    "end": "663360"
  },
  {
    "text": "execute in sequence with your hadoop cluster in this case we're just doing one step and that's install hive",
    "start": "663360",
    "end": "668720"
  },
  {
    "text": "which is the hadoop application that we're going to use to run our queries so this will take a few minutes to spin",
    "start": "668720",
    "end": "674640"
  },
  {
    "text": "out so let's go back",
    "start": "674640",
    "end": "677519"
  },
  {
    "start": "679000",
    "end": "748000"
  },
  {
    "text": "next we need to make an s3 bucket remember i said we're going to take all that data into a kinesis stream and then we need",
    "start": "679839",
    "end": "685519"
  },
  {
    "text": "to persist it somewhere durably kinesis is a durable data store absolutely but the data only persists in",
    "start": "685519",
    "end": "690959"
  },
  {
    "text": "a kinesis stream for 24 hours so you need to take the data out of your stream and put it somewhere",
    "start": "690959",
    "end": "696480"
  },
  {
    "text": "you know where it's going to persist and stay so that you can do analytics on on that data going forward so let's do",
    "start": "696480",
    "end": "702959"
  },
  {
    "text": "that",
    "start": "702959",
    "end": "705120"
  },
  {
    "text": "in this case i'm going to make a bucket called",
    "start": "710800",
    "end": "717839"
  },
  {
    "text": "so that's a bucket i just made in my account again i'm using the command line i could have done this in the in the web interface let's go back to",
    "start": "718880",
    "end": "724800"
  },
  {
    "text": "the web interface",
    "start": "724800",
    "end": "730880"
  },
  {
    "text": "hit refresh",
    "start": "730880",
    "end": "733519"
  },
  {
    "text": "and there i should be able to see there it is there's my empty bucket in case you don't believe me the bucket is empty",
    "start": "738639",
    "end": "743680"
  },
  {
    "text": "it's like a magician up here",
    "start": "743680",
    "end": "746800"
  },
  {
    "start": "748000",
    "end": "852000"
  },
  {
    "text": "next we're going to make a kinesis stream and this is one of our newer services and it's one i use almost every day",
    "start": "748959",
    "end": "754320"
  },
  {
    "text": "and this will be really the front door of our big data application it's where those logs are going to be ingested into so that we can then take",
    "start": "754320",
    "end": "760800"
  },
  {
    "text": "that data out and persist it and operate on it so we're going to create a two shard kinesis stream and i'll explain what",
    "start": "760800",
    "end": "766480"
  },
  {
    "text": "that means in a second",
    "start": "766480",
    "end": "773839"
  },
  {
    "text": "oops oh whatever it already exists let's check it out",
    "start": "780079",
    "end": "787839"
  },
  {
    "text": "so i'm actually going to delete the old stream and create a new one in a second",
    "start": "796079",
    "end": "801440"
  },
  {
    "text": "and while we wait for this to delete uh so kinesis has this concept of shards and that's how you scale your stream",
    "start": "801440",
    "end": "807120"
  },
  {
    "text": "what makes kinesis really powerful is that it's elastic like so many other aws services so say you have a bunch of log files",
    "start": "807120",
    "end": "813440"
  },
  {
    "text": "coming in you can actually size the throughput the capacity of your stream in terms of how fast you can read",
    "start": "813440",
    "end": "819600"
  },
  {
    "text": "and write by adding or removing shards on the fly so by default i'm starting with two shards that'll give me a certain amount",
    "start": "819600",
    "end": "825600"
  },
  {
    "text": "of megabytes per second that i can write to the stream and we'll see how much that equals in a second once we once we build it",
    "start": "825600",
    "end": "832880"
  },
  {
    "text": "there so let's delete it let's try that again",
    "start": "832880",
    "end": "836639"
  },
  {
    "text": "there okay so now if i go back to my stream and hit refresh",
    "start": "839199",
    "end": "845600"
  },
  {
    "text": "i should see and we'll just let that create for a second",
    "start": "847519",
    "end": "851440"
  },
  {
    "start": "852000",
    "end": "1238000"
  },
  {
    "text": "next we're going to make a data warehouse and this part i mean creating a hadoop cluster in a couple minutes is pretty cool but for anyone who's worked",
    "start": "852639",
    "end": "859199"
  },
  {
    "text": "with big enterprise data warehouses you know this kind of blows my mind you know you can make a data warehouse cluster a",
    "start": "859199",
    "end": "865279"
  },
  {
    "text": "columnar olap data store in just a couple minutes and in this case we're going to use the dw2 large there's",
    "start": "865279",
    "end": "871920"
  },
  {
    "text": "a free tier for red for redshift but if you were to do this even without the free tier it would only cost you 25",
    "start": "871920",
    "end": "878000"
  },
  {
    "text": "cents an hour to run and it gives you a fully functional ssd backed cluster to run your olap workloads",
    "start": "878000",
    "end": "884959"
  },
  {
    "text": "against your analytics workloads so let's spin that up",
    "start": "884959",
    "end": "889600"
  },
  {
    "text": "in this case just to walk you through a little bit i specify the node which is kind of like the instance type that",
    "start": "895760",
    "end": "900800"
  },
  {
    "text": "we're going to use for our redshift cluster so the dw2 large it's going to be a single node cluster but i could make this a 20 200 whatever",
    "start": "900800",
    "end": "907600"
  },
  {
    "text": "node cluster the cluster limit for redshift has actually grown recently you can you can grow clusters up to two",
    "start": "907600",
    "end": "913519"
  },
  {
    "text": "petabytes in size and the performance really scales linearly the more nodes you add to your cluster",
    "start": "913519",
    "end": "918880"
  },
  {
    "text": "so it's really powerful you can start small and as your business succeeds or as your data grows you can grow your cluster to meet the",
    "start": "918880",
    "end": "924880"
  },
  {
    "text": "needs of your business so just spat out some json to tell me about the details of my cluster",
    "start": "924880",
    "end": "933040"
  },
  {
    "text": "so we'll just go to redshift here so there it is it's creating that'll",
    "start": "937519",
    "end": "944160"
  },
  {
    "text": "just finish in a couple minutes meanwhile my kinesis stream",
    "start": "944160",
    "end": "951120"
  },
  {
    "text": "is active and it's a two shard stream so you can see that i have write",
    "start": "951120",
    "end": "956720"
  },
  {
    "text": "capacity of two megabytes a second and read capacity of four megabytes a second to increase that i would just add new",
    "start": "956720",
    "end": "962959"
  },
  {
    "text": "shards and it would automatically increase the the aggregate capacity of my stream",
    "start": "962959",
    "end": "968160"
  },
  {
    "text": "now let's do something we've just launched a bunch of stuff but now that the stream is active let's push some data into that stream so",
    "start": "968160",
    "end": "974240"
  },
  {
    "text": "going back to my instance what i've done here is i've pre-downloaded a a big log so this is the access login",
    "start": "974240",
    "end": "981680"
  },
  {
    "text": "if i do the head command which just shows you the first few lines of the log file you'll see that's the apache log that i had up on the screen before",
    "start": "981680",
    "end": "988639"
  },
  {
    "text": "so in reality it probably wouldn't be a static file like this would probably be a rolling log that's growing over time",
    "start": "988639",
    "end": "993680"
  },
  {
    "text": "but log4j can push the log as it changes as entries get added in real time to your kinesis stream for",
    "start": "993680",
    "end": "1000240"
  },
  {
    "text": "the purposes of today's demo i just took a big chunk of log and we're going to push all those lines of logs into the stream",
    "start": "1000240",
    "end": "1005519"
  },
  {
    "text": "so we have a little java application that will do that you can download this java application it's listed in the kinesis documentation",
    "start": "1005519",
    "end": "1012560"
  },
  {
    "text": "and in the emr documentation",
    "start": "1012560",
    "end": "1024959"
  },
  {
    "text": "so this is writing the log entries the log lines into our kinesis stream one line at a time so we'll go pretty fast i",
    "start": "1024959",
    "end": "1031678"
  },
  {
    "text": "think i have about 39 000 lines of logs we'll let that kind of chill out for a second and check out the status of our clusters",
    "start": "1031679",
    "end": "1039839"
  },
  {
    "text": "so here we are back at our redshift console and it is up it's available so our",
    "start": "1043919",
    "end": "1050160"
  },
  {
    "text": "cluster has been created so we have a data warehouse in you know under two minutes",
    "start": "1050160",
    "end": "1055360"
  },
  {
    "text": "let's go look at check the status of our of our emr of our elastic mapreduce",
    "start": "1055600",
    "end": "1061039"
  },
  {
    "text": "cluster and it is also up so you'll see that both nodes are running and if we go down to steps",
    "start": "1061039",
    "end": "1066400"
  },
  {
    "text": "hive has been installed what's important to know about emr is it's a cluster management tool really a",
    "start": "1066400",
    "end": "1073280"
  },
  {
    "text": "high level way to bootstrap and spin up ec2 instances",
    "start": "1073280",
    "end": "1078960"
  },
  {
    "text": "but it doesn't hide those ec2 instances from you you have full root access to the underlying nodes so if you're a",
    "start": "1078960",
    "end": "1084960"
  },
  {
    "text": "hadoop guru and you really want to customize things you can ssh into your masternode of your hadoop cluster and make all the changes",
    "start": "1084960",
    "end": "1090799"
  },
  {
    "text": "you want and that's exactly what we're going to do so if you look in",
    "start": "1090799",
    "end": "1096240"
  },
  {
    "text": "your emr or sorry your ec2 console you'll notice there's some new m3 extra",
    "start": "1096240",
    "end": "1101440"
  },
  {
    "text": "larges here and this is my hadoop cluster that has been spun up if i dig a little deeper oops",
    "start": "1101440",
    "end": "1109360"
  },
  {
    "text": "you'll see that i can tell it's the hadoop cluster because the security group that has been assigned that's that's the master node",
    "start": "1109360",
    "end": "1116400"
  },
  {
    "text": "so how do we connect to it well emr lists the public dns of our master node and we're going to ssh into it",
    "start": "1117679",
    "end": "1125200"
  },
  {
    "text": "let's give me a second to copy paste",
    "start": "1128559",
    "end": "1136000"
  },
  {
    "text": "i'm also going to copy paste the private dns and i'll explain why in a second",
    "start": "1136000",
    "end": "1151840"
  },
  {
    "text": "okay well that's still going let's let that hang out for a second how many of you uh are familiar with ssh and have ever done",
    "start": "1155039",
    "end": "1161600"
  },
  {
    "text": "port forwarding okay good number so what we're going to do is when we connect to the emr master",
    "start": "1161600",
    "end": "1168240"
  },
  {
    "text": "node we'll just use standard ssh but i'm going to use the dash l command to do some port forwarding",
    "start": "1168240",
    "end": "1173520"
  },
  {
    "text": "that's because emr spins up some servers some web servers that allow you to view information about",
    "start": "1173520",
    "end": "1179200"
  },
  {
    "text": "the cluster and those ports and the ability to view the web servers are not available from the outside they're not public",
    "start": "1179200",
    "end": "1185440"
  },
  {
    "text": "so by doing port forwarding we can bring up those web interfaces locally and that'll make more sense once you see",
    "start": "1185440",
    "end": "1190960"
  },
  {
    "text": "in a second but that's what those extra commands are in the ssh command when i run it",
    "start": "1190960",
    "end": "1196320"
  },
  {
    "text": "i think has enough records",
    "start": "1198240",
    "end": "1201360"
  },
  {
    "text": "all right so let's get in there so i'm just sshinging in and you see",
    "start": "1204640",
    "end": "1209679"
  },
  {
    "text": "that right there oops i'll do it again",
    "start": "1209679",
    "end": "1214960"
  },
  {
    "text": "see the dash l so what i'm doing is i'm forwarding two ports and that's the the ports that these web interfaces are",
    "start": "1215679",
    "end": "1221919"
  },
  {
    "text": "running on and you'll see what they look like in a sec we'll also see if port forwarding works on this windows laptop they just gave me",
    "start": "1221919",
    "end": "1229759"
  },
  {
    "text": "okay so let's run hive",
    "start": "1231039",
    "end": "1235840"
  },
  {
    "start": "1238000",
    "end": "1577000"
  },
  {
    "text": "this is what we just did we just downloaded the logs and this is the information you'll get in the lab and i'll give you a link",
    "start": "1238480",
    "end": "1244240"
  },
  {
    "text": "to the lab at the end of this session and we use this java application with log4j to push",
    "start": "1244240",
    "end": "1249440"
  },
  {
    "text": "the log entries up into the kinesis stream so here's where we are we've aggregated the logs in our kinesis",
    "start": "1249440",
    "end": "1256559"
  },
  {
    "text": "stream using log4j and now we get to process them and transform them using our hadoop cluster our emr cluster",
    "start": "1256559",
    "end": "1264720"
  },
  {
    "text": "so we just logged in which i just showed you using the dash l commands to pour forward and we ran hive so what's hive",
    "start": "1265679",
    "end": "1273120"
  },
  {
    "text": "hadoop you can think of in a lot of ways it's like a operating system for big data applications i think that's",
    "start": "1273120",
    "end": "1280640"
  },
  {
    "text": "the easiest way to think about that's how i think about it and within that operating system you can run a number of applications",
    "start": "1280640",
    "end": "1285840"
  },
  {
    "text": "and each of those applications has different purposes there's applications like hbase for nosql data source that runs on your",
    "start": "1285840",
    "end": "1291600"
  },
  {
    "text": "hadoop cluster there's an application called hive and hive is gives you a sequel like",
    "start": "1291600",
    "end": "1297280"
  },
  {
    "text": "because it's not the full ansi sql language but a sql-like interface to run map-reduce jobs to run",
    "start": "1297280",
    "end": "1302640"
  },
  {
    "text": "jobs on your youtube cluster there's another sort of sister application called pig",
    "start": "1302640",
    "end": "1307760"
  },
  {
    "text": "which a lot of people use for data transformation for text transformation it's really good for that and it was developed by yahoo it uses a",
    "start": "1307760",
    "end": "1314159"
  },
  {
    "text": "slightly different language it's sql-ish and then there's a whole ton of other applications but think of hadoop as the",
    "start": "1314159",
    "end": "1319679"
  },
  {
    "text": "base the the operating system for your cluster that can run all these different applications hadoop 2 which we support",
    "start": "1319679",
    "end": "1326240"
  },
  {
    "text": "with emr comes with something called yarn and it has much better scheduling and resource management so it can",
    "start": "1326240",
    "end": "1331440"
  },
  {
    "text": "better run multiple applications and schedule all the resources between them",
    "start": "1331440",
    "end": "1337440"
  },
  {
    "text": "so let's see what hive looks like",
    "start": "1340559",
    "end": "1347600"
  },
  {
    "text": "oops so there i've logged into hive and you see the hive",
    "start": "1347600",
    "end": "1354960"
  },
  {
    "text": "the hive console there going to set some defaults and this will",
    "start": "1355520",
    "end": "1362080"
  },
  {
    "text": "tell hive where my kinesis stream is so here i have the access key and secret key and you can take a picture i'll",
    "start": "1362080",
    "end": "1368960"
  },
  {
    "text": "change them right after this session and the region that we're working in and then just some defaults to make",
    "start": "1368960",
    "end": "1375200"
  },
  {
    "text": "hive and kinesis work well",
    "start": "1375200",
    "end": "1378480"
  },
  {
    "text": "now we're going to create a hive table now anyone who's worked with sql in a database will understand the concept of a table",
    "start": "1381840",
    "end": "1388000"
  },
  {
    "text": "it looks very similar you know i'm sure most of you have done sql before we're creating a table to hold the data",
    "start": "1388000",
    "end": "1393840"
  },
  {
    "text": "what's different with hive is that it's schema on read so with most relational databases when you create a table you load data into",
    "start": "1393840",
    "end": "1399360"
  },
  {
    "text": "that table and the data has to match the schema and it stays fixed in that schema with hive is a little different",
    "start": "1399360",
    "end": "1405120"
  },
  {
    "text": "you effectively define the schema immediately before an operation you say hey there's some data over here",
    "start": "1405120",
    "end": "1410480"
  },
  {
    "text": "and this data i know takes this shape so i'm going to i'm going to define what the shape of",
    "start": "1410480",
    "end": "1416159"
  },
  {
    "text": "is before i run an operation so before you do a select for example you can define the schema and then select against that schema so in this",
    "start": "1416159",
    "end": "1422400"
  },
  {
    "text": "case i know what the log looks like it's a comma delimited file with these columns and that",
    "start": "1422400",
    "end": "1427679"
  },
  {
    "text": "crazy string you see down there is just a regular expression that represents the format of the apache log",
    "start": "1427679",
    "end": "1433919"
  },
  {
    "text": "so we can do is come up with different schemas for different log types or different input types and then on demand map those schemas to",
    "start": "1433919",
    "end": "1441279"
  },
  {
    "text": "whatever data you're processing so it's really powerful in that way now importantly at the",
    "start": "1441279",
    "end": "1446400"
  },
  {
    "text": "bottom there you'll see that we have stored by kinesis storage handler",
    "start": "1446400",
    "end": "1451679"
  },
  {
    "text": "and that means that we're actually going to pull data out of the kinesis stream directly and this is what makes emr so",
    "start": "1451679",
    "end": "1456799"
  },
  {
    "text": "compelling is because it has integration with other amazon services like kinesis you can also pull data out of s3 we're",
    "start": "1456799",
    "end": "1463360"
  },
  {
    "text": "going to see that a second you can write to s3 you can also use dynamodb so you're not limited to just",
    "start": "1463360",
    "end": "1468960"
  },
  {
    "text": "hdfs which is typically what you would use the distributed file system that is the default if you will for hadoop you can use",
    "start": "1468960",
    "end": "1475039"
  },
  {
    "text": "multiple data stores with emr with hadoop and today we're going to use kinesis and s3",
    "start": "1475039",
    "end": "1481600"
  },
  {
    "text": "so the table takes no time to create because we're not actually moving any data we're just defining a schema in the hive meta store that will later",
    "start": "1481679",
    "end": "1488400"
  },
  {
    "text": "manipulate or will later use to manipulate the data so let's make sure it worked okay",
    "start": "1488400",
    "end": "1494400"
  },
  {
    "text": "now we're down to live demo crunch time i'm going to select just a single record from the kinesis stream",
    "start": "1494400",
    "end": "1500559"
  },
  {
    "text": "an emr is going to go out and say hey canes is here i am here are my credentials can you show me one row from your stream",
    "start": "1500559",
    "end": "1507440"
  },
  {
    "text": "one one item tick tock tick tock",
    "start": "1507440",
    "end": "1514480"
  },
  {
    "text": "hive is famous for a lot of things not for being fast",
    "start": "1514480",
    "end": "1518559"
  },
  {
    "text": "let that there we go so you might recognize that as a line from the log file so it's reached into the kinesis stream",
    "start": "1521120",
    "end": "1527520"
  },
  {
    "text": "and it has pulled this out now this is i know kind of white on black and not that exciting to look at but it's",
    "start": "1527520",
    "end": "1532880"
  },
  {
    "text": "exciting i promise you because in order to access a stream or data stored in",
    "start": "1532880",
    "end": "1537919"
  },
  {
    "text": "uh similar technologies like kafka that takes a lot of code and you need to know about iterators and",
    "start": "1537919",
    "end": "1542960"
  },
  {
    "text": "how to reach into a stream and pull things out this makes it super easy and we have big companies using emr",
    "start": "1542960",
    "end": "1549039"
  },
  {
    "text": "just for this purpose to quickly query against their kinesis streams they have real-time data coming in about",
    "start": "1549039",
    "end": "1554080"
  },
  {
    "text": "security or about logs or whatever and they can use emr to quickly query their kinesis stream it's like a",
    "start": "1554080",
    "end": "1559200"
  },
  {
    "text": "sql interface for kinesis and that alone if we stopped right there is pretty cool but we're not going to",
    "start": "1559200",
    "end": "1566799"
  },
  {
    "text": "stop right there don't worry so we need to persist this data summer",
    "start": "1566799",
    "end": "1573279"
  },
  {
    "text": "right got a little bit ahead of myself there",
    "start": "1573279",
    "end": "1579120"
  },
  {
    "start": "1577000",
    "end": "1587000"
  },
  {
    "text": "i'm doing a lot of talking but i think talking is better than slides anyway so this is the table we just created we",
    "start": "1579120",
    "end": "1584960"
  },
  {
    "text": "did that test so here we are we have log for j it's",
    "start": "1584960",
    "end": "1590400"
  },
  {
    "text": "pushed up in the kinesis stream and then we've used emr to reach into the kinesis stream we've connected emr and kinesis via hive",
    "start": "1590400",
    "end": "1596559"
  },
  {
    "text": "but remember the data is only persisted in the kinesis stream for 24 hours so we need to take that data out and put it somewhere",
    "start": "1596559",
    "end": "1602640"
  },
  {
    "text": "so that we can have it there after the 24-hour window and also do more advanced queries against it",
    "start": "1602640",
    "end": "1608000"
  },
  {
    "text": "so we're going to stick it in in amazon s3 which is 119's of durability a great cost effective and high performance",
    "start": "1608000",
    "end": "1614080"
  },
  {
    "text": "place to stick your data",
    "start": "1614080",
    "end": "1617519"
  },
  {
    "start": "1621000",
    "end": "1843000"
  },
  {
    "text": "so to do that i create an external hive table and if you look at the last line",
    "start": "1622320",
    "end": "1628000"
  },
  {
    "text": "the data for this table will be stored in that bucket i created remember when i created that s3 bucket",
    "start": "1628000",
    "end": "1633840"
  },
  {
    "text": "it's going to store the data in the bucket so when i run a query against it and i write to this table it'll",
    "start": "1633840",
    "end": "1640000"
  },
  {
    "text": "write the data in this case from the kinesis stream to the table in s to the bucket in s3",
    "start": "1640000",
    "end": "1647840"
  },
  {
    "text": "so again because we're not moving data right now that happens really quickly",
    "start": "1647840",
    "end": "1655840"
  },
  {
    "text": "we're going to do a couple other things to show you the benefits of hive and some features i'm going to set some variables here",
    "start": "1655919",
    "end": "1661200"
  },
  {
    "text": "some parameters rather the first one says partition mode so we're going to use hive to",
    "start": "1661200",
    "end": "1667440"
  },
  {
    "text": "partition the data so instead of grabbing everything and sticking in a big fat file in s3 it's going to split it up",
    "start": "1667440",
    "end": "1672880"
  },
  {
    "text": "into chunks and this is important because redshift where we're ultimately going to load this data",
    "start": "1672880",
    "end": "1678080"
  },
  {
    "text": "can load data in parallel into the cluster and as we all know it's much much faster to load data in parallel",
    "start": "1678080",
    "end": "1684159"
  },
  {
    "text": "than one file at a time so we're going to use hive to split things up into a bunch of little files that are going to sit in our s3 bucket",
    "start": "1684159",
    "end": "1690320"
  },
  {
    "text": "so they can be loaded all at the same time into redshift really quickly hive is also going to use is going to",
    "start": "1690320",
    "end": "1695840"
  },
  {
    "text": "compress the data so we're going to have little gzip compressed chunks that we can then load into redshift quickly and they'll load",
    "start": "1695840",
    "end": "1701200"
  },
  {
    "text": "quickly because they're compressed and they're smaller so let's do it",
    "start": "1701200",
    "end": "1711840"
  },
  {
    "text": "so i ran an insert command and this is going to now take the data from the kinesis stream",
    "start": "1712480",
    "end": "1717760"
  },
  {
    "text": "and it's going to insert it into s3 and while it's doing that it's going to do a little tiny",
    "start": "1717760",
    "end": "1723600"
  },
  {
    "text": "transformation there where it's going to change the hour field into a normalized timestamp that redshift can recognize from doing a",
    "start": "1723600",
    "end": "1729360"
  },
  {
    "text": "very simple data transformation and then it's going to persist it in s3 and chunk it up and gzip it",
    "start": "1729360",
    "end": "1736159"
  },
  {
    "text": "so we will have a transformed uh pair like ready for parallelization data set",
    "start": "1736159",
    "end": "1742399"
  },
  {
    "text": "we're doing etl wasn't that hard we're now using emr as an etl engine now i'll be totally honest with",
    "start": "1742399",
    "end": "1748880"
  },
  {
    "text": "you using hive for data transformation is not its strong suit you know most people would use pig",
    "start": "1748880",
    "end": "1754320"
  },
  {
    "text": "and other applications for this but it's a great place to start hive is more like a data warehouse if you've ever worked with sql you'll know that",
    "start": "1754320",
    "end": "1760000"
  },
  {
    "text": "sql is not amazing for string transformation necessarily there are other better tools out there for that but it does the trick",
    "start": "1760000",
    "end": "1765600"
  },
  {
    "text": "if you're doing stuff like this if you do want to do more complex string transformation start to look at some of",
    "start": "1765600",
    "end": "1771440"
  },
  {
    "text": "the other hadoop applications start with pig it's a good place to start",
    "start": "1771440",
    "end": "1776480"
  },
  {
    "text": "so what hive does is it's now transforming that sql command that insert sql command into a mapreduce job",
    "start": "1776960",
    "end": "1784240"
  },
  {
    "text": "and it's taking that and it's farming out the work to the cluster to all the nodes in this case three nodes of the cluster",
    "start": "1784240",
    "end": "1790080"
  },
  {
    "text": "so it's looking at the data set it's taking that data and it's figuring out what kind of transformation it has to do and then spreading that work",
    "start": "1790080",
    "end": "1796159"
  },
  {
    "text": "and the data across the nodes of the cluster and that's really what hadoop does it's kind of a cluster management",
    "start": "1796159",
    "end": "1801679"
  },
  {
    "text": "distributed computing tool and hive is a higher level tool so you don't have to write complex java jobs it",
    "start": "1801679",
    "end": "1807120"
  },
  {
    "text": "does that for you based on the sql you you define it goes and runs those java applications on the",
    "start": "1807120",
    "end": "1812720"
  },
  {
    "text": "jvms running in the slices on the on the hadoop nodes so in other words it makes hadoop a lot",
    "start": "1812720",
    "end": "1818559"
  },
  {
    "text": "easier now if you need to do stuff that is not as easily represented in sql or if you",
    "start": "1818559",
    "end": "1824000"
  },
  {
    "text": "want to really optimize your performance that's when you get into writing custom mapreduce jobs but between you and me",
    "start": "1824000",
    "end": "1829039"
  },
  {
    "text": "i use hive and pigs like almost for everything these days i haven't written a custom job at mapreduce job since",
    "start": "1829039",
    "end": "1834080"
  },
  {
    "text": "well maybe yesterday but still it wasn't that complex",
    "start": "1834080",
    "end": "1838640"
  },
  {
    "text": "so this will take a while so let's go back and look at the slides this is where the live demo gets really",
    "start": "1839679",
    "end": "1845679"
  },
  {
    "start": "1843000",
    "end": "1992000"
  },
  {
    "text": "interesting because i actually don't know if this is going to work but let's see if port forwarding will work on windows where i'm going to",
    "start": "1845679",
    "end": "1851279"
  },
  {
    "text": "look at the hadoop web interfaces now remember when we connected with ssh we port forwarded some ports and all",
    "start": "1851279",
    "end": "1857840"
  },
  {
    "text": "that means is that it exposes the ports as you define them on your computer so you can load up localhost",
    "start": "1857840",
    "end": "1862960"
  },
  {
    "text": "and see the web interfaces that are actually hosted remotely where you've sshed into so let's see if this works",
    "start": "1862960",
    "end": "1879840"
  },
  {
    "text": "oops that was the wrong address sorry",
    "start": "1882399",
    "end": "1893440"
  },
  {
    "text": "maybe not okay well that's not working but that's",
    "start": "1893440",
    "end": "1899120"
  },
  {
    "text": "okay i'll show you what i can show you what it looks like in the video after",
    "start": "1899120",
    "end": "1903600"
  },
  {
    "text": "so meanwhile um the hive job is finished and if we scroll up a little bit here",
    "start": "1905600",
    "end": "1910799"
  },
  {
    "text": "what it's done is this is the partitioning that's happening so i define the partition",
    "start": "1910799",
    "end": "1916080"
  },
  {
    "text": "in my insert query i define you'll see this in the lab when you take it home i define the partition to be the hour in the timestamp so it's",
    "start": "1916080",
    "end": "1922080"
  },
  {
    "text": "going to split the files based on the log entries based on the hour so everything that happens at 11 pm",
    "start": "1922080",
    "end": "1927519"
  },
  {
    "text": "goes in in one s3 prefix everything that goes and so on and so forth so let's look at our s3 bucket we",
    "start": "1927519",
    "end": "1933840"
  },
  {
    "text": "created to actually see that",
    "start": "1933840",
    "end": "1948559"
  },
  {
    "text": "so we'll go to the bucket that we created so there's our demo remember it was empty before we go into emr demo and the output",
    "start": "1948559",
    "end": "1956399"
  },
  {
    "text": "and there's our partitions and each side of each one of these partitions is a gzipped chunk of our big log file so remember it was",
    "start": "1956399",
    "end": "1961760"
  },
  {
    "text": "one big access log file we were pushing individual log lines in and now we've done a transformation we've pushed into",
    "start": "1961760",
    "end": "1967039"
  },
  {
    "text": "s3",
    "start": "1967039",
    "end": "1969440"
  },
  {
    "text": "success so we did all this",
    "start": "1974840",
    "end": "1978799"
  },
  {
    "text": "so we're here we have transformed the data and we've persisted the data into amazon s3",
    "start": "1983039",
    "end": "1991840"
  },
  {
    "start": "1992000",
    "end": "2051000"
  },
  {
    "text": "now hue is something that was just released and um i don't have it on this cluster because",
    "start": "1993840",
    "end": "1999760"
  },
  {
    "text": "actually when i built this cluster for this presentation we hadn't come out with a hue feature yet it just came out a few days ago but if sshinging into a masternode and",
    "start": "1999760",
    "end": "2006880"
  },
  {
    "text": "running high queries in the console is not your idea of a good time q is for you it's a web interface that makes emr much much easier to use",
    "start": "2006880",
    "end": "2014159"
  },
  {
    "text": "and what's really cool about hue is it has a file browser that is s3 compatible so you can actually navigate your s3",
    "start": "2014159",
    "end": "2019200"
  },
  {
    "text": "buckets from within hue you can run hive queries you can run big queries you can even visualize the data so i",
    "start": "2019200",
    "end": "2024480"
  },
  {
    "text": "just wanted to give a quick plug to hue because the emr team has put a lot of great work into getting this ready and now when you start your cluster and you can do this",
    "start": "2024480",
    "end": "2031039"
  },
  {
    "text": "with the lab when you when you do this at home you just pick add hue and just like i added hive as an application you can add",
    "start": "2031039",
    "end": "2037120"
  },
  {
    "text": "hue and then you can open up the hue web interface and you'll have a very pretty easy to use and yet powerful web",
    "start": "2037120",
    "end": "2043679"
  },
  {
    "text": "interface for hadoop but today i thought we'd get down and dirty so we're using the ssh",
    "start": "2043679",
    "end": "2050560"
  },
  {
    "start": "2051000",
    "end": "2494000"
  },
  {
    "text": "so we saw our output files there in s3 we've properly partitioned them so now we can connect to our redshift",
    "start": "2052639",
    "end": "2057760"
  },
  {
    "text": "cluster and take these gzip chunks that we've made with hive and load them into redshift and again",
    "start": "2057760",
    "end": "2063919"
  },
  {
    "text": "load loading them into redshift is going to be efficient because when we issue a copy command to load into",
    "start": "2063919",
    "end": "2069440"
  },
  {
    "text": "reshift it does it in parallel so we have a ton of files and we can load them in parallel because they've been split and gzipped",
    "start": "2069440",
    "end": "2075839"
  },
  {
    "text": "and redshift understands how to deal with compressed gzip files so that's not a problem for it so i'm",
    "start": "2075839",
    "end": "2081280"
  },
  {
    "text": "going to use again the command line forgive me to connect to the redshift cluster so i'm going to use the p sql as",
    "start": "2081280",
    "end": "2087200"
  },
  {
    "text": "part of the postgres package to connect but importantly you can use any tool to connect and i put a couple of my favorites down for you there",
    "start": "2087200",
    "end": "2094000"
  },
  {
    "text": "aginity has a great workbench for amazon redshift strongly recommended if you're a windows user",
    "start": "2094000",
    "end": "2099520"
  },
  {
    "text": "and if you're not a windows user you can use sql workbench j and there's other ones out there as well and they provide an easy way to connect",
    "start": "2099520",
    "end": "2105359"
  },
  {
    "text": "to a redshift cluster a redshift is a data warehouse but it's jdbc odbc compatible so long story short",
    "start": "2105359",
    "end": "2111839"
  },
  {
    "text": "pretty much any application that you can use a postgres driver and that can speak over jdbc odbc can",
    "start": "2111839",
    "end": "2118320"
  },
  {
    "text": "connect to your redshift cluster so in this case i'm using the postgres native client to connect to my redshift",
    "start": "2118320",
    "end": "2123680"
  },
  {
    "text": "cluster on the command line",
    "start": "2123680",
    "end": "2130160"
  },
  {
    "text": "so if i go back to my redshift table",
    "start": "2130160",
    "end": "2133838"
  },
  {
    "text": "there's redshift that's the redshift endpoint the address of my redshift cluster i'll copy it",
    "start": "2135760",
    "end": "2145838"
  },
  {
    "text": "quit at a hive i'll put the hostname in my cluster",
    "start": "2146240",
    "end": "2158960"
  },
  {
    "text": "and i have port 18192 username master the database i created",
    "start": "2158960",
    "end": "2165680"
  },
  {
    "text": "was called demo oops oh i didn't install postgres awesome",
    "start": "2165680",
    "end": "2171119"
  },
  {
    "text": "real time uh i'm sorry",
    "start": "2171680",
    "end": "2177119"
  },
  {
    "text": "good thing the internet's good here there we go",
    "start": "2180160",
    "end": "2186160"
  },
  {
    "text": "what's the password secret it's master one two three and here we are connected to the cluster",
    "start": "2186160",
    "end": "2192160"
  },
  {
    "text": "okay so now we can do is create a table to hold the data",
    "start": "2192160",
    "end": "2197760"
  },
  {
    "text": "and again this is just ansi sql so this is a slightly different version of the table from the one we created for",
    "start": "2197760",
    "end": "2203520"
  },
  {
    "text": "hive because we have transformed things now when i did the the hive transformation and you can see in more detail when you have more time when you do the lab",
    "start": "2203520",
    "end": "2209839"
  },
  {
    "text": "is that we didn't take all of the fields i didn't need all the columns in that log file so it's a subset of the columns and also",
    "start": "2209839",
    "end": "2216000"
  },
  {
    "text": "that transform timestamp that we transform with hive",
    "start": "2216000",
    "end": "2220480"
  },
  {
    "text": "i define a distribution key and a sort key which allows redshift to efficiently distribute the data across its cluster",
    "start": "2221359",
    "end": "2227839"
  },
  {
    "text": "nodes in this case it's a single node so it's not such a big deal and then a sort key so we can have more efficient queries because data is sorted",
    "start": "2227839",
    "end": "2233839"
  },
  {
    "text": "in the order that we will usually need it in lastly let's copy the data in",
    "start": "2233839",
    "end": "2240400"
  },
  {
    "text": "so this copy command remember runs in parallel so what i'm doing here is copying apache log so into the apache",
    "start": "2240400",
    "end": "2247200"
  },
  {
    "text": "log table from the location in s3 that i just showed you with all of those gzip files",
    "start": "2247200",
    "end": "2252480"
  },
  {
    "text": "and i'm telling redshift these are gzip files hence the gzip at the end and the delimiter of these files is a",
    "start": "2252480",
    "end": "2258240"
  },
  {
    "text": "slash t a tab so redshift knows this is a tab delimited gzipped file that will correspond to the columns that",
    "start": "2258240",
    "end": "2264480"
  },
  {
    "text": "i defined in that table there",
    "start": "2264480",
    "end": "2271838"
  },
  {
    "text": "let's wait for that to load we created the table we're loading it",
    "start": "2279440",
    "end": "2285280"
  },
  {
    "text": "into redshift",
    "start": "2285280",
    "end": "2289119"
  },
  {
    "text": "then we'll run some queries so there the data is loaded so we can",
    "start": "2290480",
    "end": "2297200"
  },
  {
    "text": "run some queries",
    "start": "2297200",
    "end": "2299838"
  },
  {
    "text": "so in this case i'm just going to do a simple query i'm going to this was an apache log so i'm going to select the hostname and the the request",
    "start": "2304960",
    "end": "2311520"
  },
  {
    "text": "the the web request that was made for this web server where the host was a specific ip address so show me",
    "start": "2311520",
    "end": "2316960"
  },
  {
    "text": "every request that this guy made",
    "start": "2316960",
    "end": "2320560"
  },
  {
    "text": "boom here we go now this is on the console not very",
    "start": "2322240",
    "end": "2327280"
  },
  {
    "text": "pretty but because redshift supports jdbc odbc and because it uses ansi sql and",
    "start": "2327280",
    "end": "2333359"
  },
  {
    "text": "because it uses the postgres drivers you can use pretty much any bi or visualization tool",
    "start": "2333359",
    "end": "2339200"
  },
  {
    "text": "that you're using today so i'm sure a lot of you use tableau microstrategy jaspersoft click excel",
    "start": "2339200",
    "end": "2346640"
  },
  {
    "text": "all of these tools will work natively with redshift now interestingly they'll also work with",
    "start": "2346640",
    "end": "2351760"
  },
  {
    "text": "hive and emr hive and emr also have jdbc and odbc driver than access",
    "start": "2351760",
    "end": "2357359"
  },
  {
    "text": "and in case you saw hive is sometimes a little higher latency in other words a little slower for querying redshift is much much faster and it",
    "start": "2357359",
    "end": "2363920"
  },
  {
    "text": "supports the full ansi sql language so you can do your complex analytical queries",
    "start": "2363920",
    "end": "2369599"
  },
  {
    "text": "that you've been doing for years on other data warehouses and they'll work on redshift your star scheme is your snowflake",
    "start": "2369599",
    "end": "2375440"
  },
  {
    "text": "schemas most of the time none of that has to change you can just take your data set plonk it into redshift and start running",
    "start": "2375440",
    "end": "2381440"
  },
  {
    "text": "queries from tableau or whatever right away and the query time is very fast now that",
    "start": "2381440",
    "end": "2386480"
  },
  {
    "text": "took about a second to run which maybe seems slow but that's because redshift compiles the query",
    "start": "2386480",
    "end": "2392880"
  },
  {
    "text": "or the query plan the first time so it knows where to go get the data and how to properly execute the query subsequent",
    "start": "2392880",
    "end": "2398160"
  },
  {
    "text": "executions of those queries like you just saw are super fast so if you",
    "start": "2398160",
    "end": "2403280"
  },
  {
    "text": "have a predictable query that you're going to run or a predictable set of queries redshift will be extremely fast all of",
    "start": "2403280",
    "end": "2409520"
  },
  {
    "text": "the time so there we go let's go back and see",
    "start": "2409520",
    "end": "2415920"
  },
  {
    "text": "where we are again any we have partners if you go to",
    "start": "2415920",
    "end": "2421280"
  },
  {
    "text": "the aws marketplace we have partners who have a bi visualization software that you can use",
    "start": "2421280",
    "end": "2426880"
  },
  {
    "text": "that are compatible with both redshift and emr i recommend you check them out i'll have a link up in a second",
    "start": "2426880",
    "end": "2432400"
  },
  {
    "text": "but we've done it in just a few minutes live we pushed log data into kinesis we",
    "start": "2432400",
    "end": "2438240"
  },
  {
    "text": "aggregated that data we persisted that data we transformed that data and we loaded it into an enterprise-grade data warehouse and ran some queries",
    "start": "2438240",
    "end": "2444880"
  },
  {
    "text": "against it and from where i'm standing that's pretty cool",
    "start": "2444880",
    "end": "2453838"
  },
  {
    "text": "so if you would like to talk more about this if you have any questions please come find me outside i'll hang",
    "start": "2457119",
    "end": "2463599"
  },
  {
    "text": "out for a good half hour so i'd love to hear from you i love hearing about use cases and oh i forgot to put the link up for the",
    "start": "2463599",
    "end": "2470640"
  },
  {
    "text": "take-home lab most important part so uh i see everyone's taking pictures so",
    "start": "2470640",
    "end": "2476480"
  },
  {
    "text": "someone tweet that so that the rest of you have it and grab the take-home lab give it a spin i'm gonna go change my iam keys",
    "start": "2476480",
    "end": "2482880"
  },
  {
    "text": "right now and uh it's been great talking to you so thanks a lot for your time",
    "start": "2482880",
    "end": "2494800"
  },
  {
    "text": "you",
    "start": "2494800",
    "end": "2496880"
  }
]