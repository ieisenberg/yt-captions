[
  {
    "start": "0",
    "end": "77000"
  },
  {
    "text": "hey everybody happy Thursday thank you for joining our stage maker webinar",
    "start": "3320",
    "end": "8840"
  },
  {
    "text": "today we're gonna cover latent Dirac lay allocation and I think I pasted the",
    "start": "8840",
    "end": "14179"
  },
  {
    "text": "pronunciation of that fund German word into the chat if anybody's really interested in hearing it a few times",
    "start": "14179",
    "end": "20450"
  },
  {
    "text": "my name is su e wig and I'm the AWS global partner marketing manager for AI and machine learning and I want to",
    "start": "20450",
    "end": "26059"
  },
  {
    "text": "welcome all of our AWS machine learning competency consulting partners today our presenter for today is Pratap Rama",
    "start": "26059",
    "end": "33320"
  },
  {
    "text": "Murthy and you guys have heard of heard from Pratap before on multiple topics he's a partner solutions architect who",
    "start": "33320",
    "end": "39740"
  },
  {
    "text": "works with our partners helping them build their solutions on on AWS he works obviously with partners who",
    "start": "39740",
    "end": "45590"
  },
  {
    "text": "specialize in machine learning and AI solutions he's got a particular interest in natural language processing and",
    "start": "45590",
    "end": "50780"
  },
  {
    "text": "always a great presenter on these topics joining Pratap is Chris Burns who you",
    "start": "50780",
    "end": "56060"
  },
  {
    "text": "guys also know he's going to help get your questions answered so as per usual there is the right navigation pane as",
    "start": "56060",
    "end": "63260"
  },
  {
    "text": "that's part of the GoToWebinar application you can find the Q&A section in that pain and just pop your questions",
    "start": "63260",
    "end": "69680"
  },
  {
    "text": "in there and Chris and / top we'll get them addressed during the call so let's jump right in",
    "start": "69680",
    "end": "75080"
  },
  {
    "text": "/ top take it away thanks - good morning folks and happy Thursday um today we're",
    "start": "75080",
    "end": "83420"
  },
  {
    "start": "77000",
    "end": "119000"
  },
  {
    "text": "going to be looking at latent did collect allocation we were just making",
    "start": "83420",
    "end": "89930"
  },
  {
    "text": "jokes about this a lot of people call this dirichlet dule every diet was",
    "start": "89930",
    "end": "95780"
  },
  {
    "text": "French it looks like this is a this is a guy who was German so to avoid any of",
    "start": "95780",
    "end": "103040"
  },
  {
    "text": "these confusion about the pronunciation let's this call it Lda that's the most",
    "start": "103040",
    "end": "109939"
  },
  {
    "text": "common way to pronounce this this is one of the algorithms that are built into",
    "start": "109939",
    "end": "115909"
  },
  {
    "text": "sage maker that you can use today right this is the Fillies just for folks who",
    "start": "115909",
    "end": "123439"
  },
  {
    "text": "are new to this webinar they what you're hearing today is the I think the eighth",
    "start": "123439",
    "end": "130149"
  },
  {
    "text": "ninth webinar in the series where we are discussing about Amazon algorithms",
    "start": "130150",
    "end": "135620"
  },
  {
    "text": "that's available through say it maker today it's Lda and you have",
    "start": "135620",
    "end": "140870"
  },
  {
    "text": "four more lined up and you may be hearing from sue about the more of a mean arts that are to come",
    "start": "140870",
    "end": "147819"
  },
  {
    "start": "147000",
    "end": "201000"
  },
  {
    "text": "so are Amazon algorithms I just met through that in what is an Amazon",
    "start": "148060",
    "end": "153190"
  },
  {
    "text": "algorithm these are popular algorithms that are already being used in the",
    "start": "153190",
    "end": "159520"
  },
  {
    "text": "community these are completely rewritten by Amazon and the main thing is they are",
    "start": "159520",
    "end": "165489"
  },
  {
    "text": "optimized to run on AWS and there and they are mainly rewritten to scale to",
    "start": "165489",
    "end": "172030"
  },
  {
    "text": "web level scales it's consumed as a service you do not download other",
    "start": "172030",
    "end": "178209"
  },
  {
    "text": "algorithm it's a this is the cloud model where you throw in data and then you just subscribe to this algorithm and you",
    "start": "178209",
    "end": "186480"
  },
  {
    "text": "consume the service where the it abuse trains it for you and that our",
    "start": "186480",
    "end": "191590"
  },
  {
    "text": "algorithms are available through sage maker Amazon sage maker is our main",
    "start": "191590",
    "end": "197230"
  },
  {
    "text": "machine learning platform on AWS alright",
    "start": "197230",
    "end": "203950"
  },
  {
    "start": "201000",
    "end": "455000"
  },
  {
    "text": "so today we're talking about Lda this is a very specific algorithm that solves a very specific problem and what is it",
    "start": "203950",
    "end": "211090"
  },
  {
    "text": "used for it specifically usually used in text or natural language processing or",
    "start": "211090",
    "end": "218110"
  },
  {
    "text": "computational linguistics as I like to call it it does a few things it is used",
    "start": "218110",
    "end": "223660"
  },
  {
    "text": "not just for that but the most popular use case for LBA",
    "start": "223660",
    "end": "228790"
  },
  {
    "text": "is what is called topic modeling I'm going to be explaining to you what topic modeling is that is this this actually",
    "start": "228790",
    "end": "237190"
  },
  {
    "text": "the first use case for topic modeling was actually published in the year 2004",
    "start": "237190",
    "end": "242320"
  },
  {
    "text": "there's a paper by the very popular deep learning scientist Andrew English they",
    "start": "242320",
    "end": "248109"
  },
  {
    "text": "probably have heard of him and this was the one of the very seminal paper that",
    "start": "248109",
    "end": "254500"
  },
  {
    "text": "started new news research around Lda and statistical modeling on text it's also",
    "start": "254500",
    "end": "262240"
  },
  {
    "text": "used for text analytics I'll be describing what topic modeling is in the",
    "start": "262240",
    "end": "267310"
  },
  {
    "text": "next few slides but let's look at others as well it's also used to understand text that is a little you have a large",
    "start": "267310",
    "end": "274090"
  },
  {
    "text": "corpus of text data it's a you have you have crawled the web and you have Billy",
    "start": "274090",
    "end": "281200"
  },
  {
    "text": "documents you wanna understand like what kinds of documents there are so that it's used for that as well text",
    "start": "281200",
    "end": "287680"
  },
  {
    "text": "clustering topic modeling is actually this it's actually made of clustered text lets you have a million different",
    "start": "287680",
    "end": "294280"
  },
  {
    "text": "documents in your company and you want to kind of cluster them in a certain way",
    "start": "294280",
    "end": "299310"
  },
  {
    "text": "and understand what kind of clusters exists so you could use Lda you can also",
    "start": "299310",
    "end": "305320"
  },
  {
    "text": "use this for automated tag creation so if you're moving from analytics to more",
    "start": "305320",
    "end": "311430"
  },
  {
    "text": "proactive uses of this for example let's say you have a blog platform and you",
    "start": "311430",
    "end": "316630"
  },
  {
    "text": "have your users creating blogs you can use Lda to automatically attach tags to these",
    "start": "316630",
    "end": "323710"
  },
  {
    "text": "new blogs that they are writing and the this is using the algorithm the 30 the",
    "start": "323710",
    "end": "330300"
  },
  {
    "text": "because they use case here is recommendation engines Lda can also be used because it's helps you cluster",
    "start": "330300",
    "end": "337780"
  },
  {
    "text": "documents into clusters based on topics",
    "start": "337780",
    "end": "343870"
  },
  {
    "text": "you could use this to say that hey i a",
    "start": "343870",
    "end": "349290"
  },
  {
    "text": "document when it belongs to a cluster or a topic you can then start to recommend",
    "start": "349290",
    "end": "354790"
  },
  {
    "text": "other documents that belong to the same topic so that's that's one use case here fixed dimensionality reduction so this",
    "start": "354790",
    "end": "361840"
  },
  {
    "text": "is dimensionality reduction you might have heard of pca principal component analysis for a structured data doing",
    "start": "361840",
    "end": "369310"
  },
  {
    "text": "that for text is not trivial it can't apply PCA in on text so what you could",
    "start": "369310",
    "end": "377590"
  },
  {
    "text": "do is use LD a to reduce the dimensionality and make it into a structured context then you can you have",
    "start": "377590",
    "end": "385750"
  },
  {
    "text": "a way to transform that there the text documents into a structured format and",
    "start": "385750",
    "end": "393370"
  },
  {
    "text": "now it's much more amenable for more analysis and you could actually also use",
    "start": "393370",
    "end": "399100"
  },
  {
    "text": "it for population genetics what it means is let's say you have DNA sequences of a",
    "start": "399100",
    "end": "404910"
  },
  {
    "text": "population you can run lv a to find that these patterns are clusters of DNA sequences",
    "start": "404910",
    "end": "412060"
  },
  {
    "text": "among the large population you can also for example let's say you have DNA",
    "start": "412060",
    "end": "417310"
  },
  {
    "text": "samples of thousand different organisms you can use Lda to actually find out the",
    "start": "417310",
    "end": "424690"
  },
  {
    "text": "species and the clusters of species in in that population of thousands so this",
    "start": "424690",
    "end": "432069"
  },
  {
    "text": "interview to some of the use cases for Lda in the in the last case you can",
    "start": "432069",
    "end": "437470"
  },
  {
    "text": "imagine the DNA sequences to be just like text we have the four molecules in",
    "start": "437470",
    "end": "443259"
  },
  {
    "text": "a DNA if you arrange that in a certain way you have the you have a something like a stick so these are this is a very",
    "start": "443259",
    "end": "448990"
  },
  {
    "text": "large number of use cases for Lda and it's very actively used ok so now let's",
    "start": "448990",
    "end": "456430"
  },
  {
    "start": "455000",
    "end": "761000"
  },
  {
    "text": "look at what Lda is and I'm going to be focusing on my attention on the text",
    "start": "456430",
    "end": "462250"
  },
  {
    "text": "analytics right are the NLP use cases but before we go into NLP I want you",
    "start": "462250",
    "end": "469330"
  },
  {
    "text": "guys to understand the problem like what is the problem is trying to solve and why is it difficult so that we can",
    "start": "469330",
    "end": "475180"
  },
  {
    "text": "appreciate this so here's but before we go to text let's first analyze how it works with this",
    "start": "475180",
    "end": "481870"
  },
  {
    "text": "two-dimensional data right this is two-dimensional data and you have x-axis",
    "start": "481870",
    "end": "487000"
  },
  {
    "text": "going from you have two variables x and y right every data point has X comma Y and X goes from 0 to 1 or 1 Y goes from",
    "start": "487000",
    "end": "495250"
  },
  {
    "text": "0 to 1 and you have this distribution right I'll color coded this for you",
    "start": "495250",
    "end": "500500"
  },
  {
    "text": "because this is synthetically generated data but it looks like a Mickey Mouse this is a very popular way of testing an",
    "start": "500500",
    "end": "507190"
  },
  {
    "text": "algorithm you can have the Mickey Mouse phase in two years but in native the action as you look at this our human",
    "start": "507190",
    "end": "513880"
  },
  {
    "text": "visual cortex is able to identify three clusters right you have the blue cluster green cluster and the red cluster not",
    "start": "513880",
    "end": "522640"
  },
  {
    "text": "just that yeah you also note that the red and the green clusters are is lot",
    "start": "522640",
    "end": "530860"
  },
  {
    "text": "more dense than the blue cluster right so you have the blue cluster that's much",
    "start": "530860",
    "end": "537730"
  },
  {
    "text": "larger and but it spread out over a larger area whereas the green and the",
    "start": "537730",
    "end": "543699"
  },
  {
    "text": "red clusters ar-ar-ar-ar more dense right so VI",
    "start": "543699",
    "end": "549279"
  },
  {
    "text": "able to immediately distinguish these three distinct clusters and you can see",
    "start": "549279",
    "end": "556029"
  },
  {
    "text": "that there are some places where they do overlap right now if you have an algorithm if you have a clustering",
    "start": "556029",
    "end": "561790"
  },
  {
    "text": "algorithm that you run on this you have what you would identify is that you",
    "start": "561790",
    "end": "568240"
  },
  {
    "text": "first need to identify the Centers of these clusters that's the that's it so you know you know that there are three",
    "start": "568240",
    "end": "573370"
  },
  {
    "text": "clusters like say you want to give the algorithm the number right there you're",
    "start": "573370",
    "end": "578800"
  },
  {
    "text": "telling the algorithm there are three clusters and you want them to I want it to identify this and you my expecting",
    "start": "578800",
    "end": "583959"
  },
  {
    "text": "the model or algorithm to find out two things about eld clusters right",
    "start": "583959",
    "end": "591100"
  },
  {
    "text": "you want them to we want the algorithm to identify the centers of the clusters I think in this case is marked by a",
    "start": "591100",
    "end": "598149"
  },
  {
    "text": "cross in the middle of the blue cluster and that you can see a large circle in",
    "start": "598149",
    "end": "603699"
  },
  {
    "text": "the middle of the green cluster and you can also see an indication of a of a",
    "start": "603699",
    "end": "608769"
  },
  {
    "text": "black square cross in the middle of the red cluster so the first thing to enter",
    "start": "608769",
    "end": "614019"
  },
  {
    "text": "Phi is the Centers of these clusters also you all want to identify the",
    "start": "614019",
    "end": "620980"
  },
  {
    "text": "distribution or other properties of this distribution right so you have the",
    "start": "620980",
    "end": "627269"
  },
  {
    "text": "standard deviation or variance which also needs to be identified and so when",
    "start": "627269",
    "end": "633160"
  },
  {
    "text": "you have this that is what is called a statistical model you're building a statistical model from this data set",
    "start": "633160",
    "end": "640800"
  },
  {
    "text": "right now let's see an example of open F of an algorithm that we usually see in",
    "start": "640800",
    "end": "649480"
  },
  {
    "text": "structured data this is this this windows there is run by k-means clustering you notice that how it is",
    "start": "649480",
    "end": "656920"
  },
  {
    "text": "clustered right the central cluster is",
    "start": "656920",
    "end": "662139"
  },
  {
    "text": "now split into three pieces and what what k-means clustering usually does it splits the data set into into three",
    "start": "662139",
    "end": "668470"
  },
  {
    "text": "equal parts and that this does not make sense here because you have now the",
    "start": "668470",
    "end": "675309"
  },
  {
    "text": "phase of Mickey Mouse being split into three there's some blue blue distribution blue cluster now coming",
    "start": "675309",
    "end": "682000"
  },
  {
    "text": "into the phase and green coming to face not just that the Centers of the green cluster and the blue",
    "start": "682000",
    "end": "687970"
  },
  {
    "text": "cluster know move away from the real centers into somewhere in the middle obviously you cannot just run an",
    "start": "687970",
    "end": "693850"
  },
  {
    "text": "algorithm like k-means clustering which only identifies the Centers of clusters",
    "start": "693850",
    "end": "699160"
  },
  {
    "text": "right so you need to have a more sophisticated algorithm that can really properly identify the center's and other",
    "start": "699160",
    "end": "706449"
  },
  {
    "text": "properties of the model right so as you can see they even even with this small",
    "start": "706449",
    "end": "711689"
  },
  {
    "text": "use case of a small dimensional this is just two-dimensional data two dimensional data we need to be careful",
    "start": "711689",
    "end": "718239"
  },
  {
    "text": "about finding out the clusters of data now imagine when you have a space that",
    "start": "718239",
    "end": "726489"
  },
  {
    "text": "has 100,000 dimensions and now you want to understand the clusters in this very",
    "start": "726489",
    "end": "732129"
  },
  {
    "text": "large space and that is the problem space in natural language processing where each word in a language is a",
    "start": "732129",
    "end": "740169"
  },
  {
    "text": "different dimension and so when you take the vocabulary of a language like English which has about 400,000 words on",
    "start": "740169",
    "end": "746769"
  },
  {
    "text": "an average depending on the purpose you have a very high dimensional space and",
    "start": "746769",
    "end": "752439"
  },
  {
    "text": "you want to make sure that you understand like what is going on and have a right scalable algorithm that",
    "start": "752439",
    "end": "758529"
  },
  {
    "text": "predicts the clusters for you right so what is topic modeling right let's go",
    "start": "758529",
    "end": "764230"
  },
  {
    "start": "761000",
    "end": "996000"
  },
  {
    "text": "into topic models what we looked at with structured there and just very low dimensional data now let's look at a really high dimension later and see how",
    "start": "764230",
    "end": "770829"
  },
  {
    "text": "it works right so imagine the topic modeling is a very specific problem in natural language processing what topic",
    "start": "770829",
    "end": "777459"
  },
  {
    "text": "modeling does is let's say you have thousands of documents this actually was",
    "start": "777459",
    "end": "783009"
  },
  {
    "text": "the case with Wikipedia the original and rulings paper was actually trying to solve Wikipedia's problem where they had",
    "start": "783009",
    "end": "789519"
  },
  {
    "text": "we had millions of documents Wikipedia pages I don't know they want to cluster",
    "start": "789519",
    "end": "795249"
  },
  {
    "text": "them into topics right you have a weight you need some kind of a clustering some kind of a way to organize the pages to",
    "start": "795249",
    "end": "802749"
  },
  {
    "text": "be make make Wikipedia useful so they do run SD a to clustered the pages for",
    "start": "802749",
    "end": "810759"
  },
  {
    "text": "example you have let's say you have the documents about computer science you have documents about Phyllis",
    "start": "810759",
    "end": "816760"
  },
  {
    "text": "we even have documents about chemistry these are all clustered based on the LDA",
    "start": "816760",
    "end": "822700"
  },
  {
    "text": "algorithm right so that is the problem of topic modeling you have a large number of documents individual documents",
    "start": "822700",
    "end": "829210"
  },
  {
    "text": "that could have that could be talking about a few topics like a for example",
    "start": "829210",
    "end": "834220"
  },
  {
    "text": "one document could be talking about two different topics not just one topic for example I could be talking I could write",
    "start": "834220",
    "end": "840790"
  },
  {
    "text": "a blog post about cars and philosophy driving cars maybe a very philosophical",
    "start": "840790",
    "end": "845970"
  },
  {
    "text": "experience for me or a spiritual experience for me so I could be talking about both cars and spirituality in the",
    "start": "845970",
    "end": "852940"
  },
  {
    "text": "same document so what this is about what this brings us we are going to be",
    "start": "852940",
    "end": "859720"
  },
  {
    "text": "clustering the documents into topics and and the assumption is that the documents",
    "start": "859720",
    "end": "866170"
  },
  {
    "text": "do not have are not talking about a large and rough topics but only a few number of topics that's an important",
    "start": "866170",
    "end": "871540"
  },
  {
    "text": "assumption here so in the bottom we see examples of three different documents so the first document let's say it's this",
    "start": "871540",
    "end": "878020"
  },
  {
    "text": "leadership are talking about maybe improving leadership skills and it's demonstrating them the second is a",
    "start": "878020",
    "end": "884680"
  },
  {
    "text": "committee in structure the EPA this is more of a legal document or maybe a",
    "start": "884680",
    "end": "889890"
  },
  {
    "text": "documentation on a committee third is heavy metal poisoning maybe a brochure",
    "start": "889890",
    "end": "896710"
  },
  {
    "text": "or warning so these are these are talking about three distinct topics but",
    "start": "896710",
    "end": "903220"
  },
  {
    "text": "now we want to what we want to do is we want to run an algorithm that automatically identifies the topics one",
    "start": "903220",
    "end": "909790"
  },
  {
    "text": "of the important things about Lda or topic modeling is that you do not tell the algorithm what the topics are",
    "start": "909790",
    "end": "915820"
  },
  {
    "text": "this is an unsupervised algorithm because it's for example if we take Wikipedia we have millions of documents",
    "start": "915820",
    "end": "922600"
  },
  {
    "text": "you don't want somebody sitting there I don't want volunteers to tell you what the topics are because it's a very",
    "start": "922600",
    "end": "928330"
  },
  {
    "text": "time-consuming process and it's also error-prone what we are what we want is the process the LDA assumes the topic",
    "start": "928330",
    "end": "935890"
  },
  {
    "text": "modeling problem assumes that the topics are not known also when you don't even",
    "start": "935890",
    "end": "941830"
  },
  {
    "text": "know the topics the second thing is to learn that the model need to tell you is what if you have a topic justify a topic",
    "start": "941830",
    "end": "949120"
  },
  {
    "text": "what are the word that specifically that specify or occur",
    "start": "949120",
    "end": "954280"
  },
  {
    "text": "at a higher probability for that topic pregnant we take the topic of cars you",
    "start": "954280",
    "end": "960460"
  },
  {
    "text": "would expect the name brands of cars may be convertible SUV is a word that appears so for each",
    "start": "960460",
    "end": "966580"
  },
  {
    "text": "topic you also want to identify the words that appear or are in more formal",
    "start": "966580",
    "end": "975130"
  },
  {
    "text": "language the distribution or the probability distribution of words of across the entire language for each",
    "start": "975130",
    "end": "981360"
  },
  {
    "text": "topic so this is called a duty slip prior and the gentlemen work done this",
    "start": "981360",
    "end": "986710"
  },
  {
    "text": "like a hundred years ago I think so in let's say in this example we have three",
    "start": "986710",
    "end": "992190"
  },
  {
    "text": "buckets of documents it's not really bucketed but just I've showed it in that way now you want the topic to be",
    "start": "992190",
    "end": "999250"
  },
  {
    "start": "996000",
    "end": "1323000"
  },
  {
    "text": "identified automatically right so in this case let's say you mentioned or you",
    "start": "999250",
    "end": "1004350"
  },
  {
    "text": "specified that this topic is this this",
    "start": "1004350",
    "end": "1009480"
  },
  {
    "text": "corpus of data had two topics so now it will go through all the words in your in",
    "start": "1009480",
    "end": "1015630"
  },
  {
    "text": "this corpus and understand which are the words that occur so in this case topic 1",
    "start": "1015630",
    "end": "1020880"
  },
  {
    "text": "topic 1 not make sure understand why it's written there so it's beginning you",
    "start": "1020880",
    "end": "1027420"
  },
  {
    "text": "the list of the words that commonly appear in topic 1 and the the font size of the the words",
    "start": "1027420",
    "end": "1036870"
  },
  {
    "text": "or the current topic 1 right we have organized there is an highest one where direct that's the last word topic 1 the",
    "start": "1036870",
    "end": "1043770"
  },
  {
    "text": "font size is here to indicate the probability distribution right organize the topic 1 is expected to appear a lot",
    "start": "1043770",
    "end": "1050550"
  },
  {
    "text": "more than the word directive which is which is shown of the smaller fault similarly in topic 2 you have a metry",
    "start": "1050550",
    "end": "1058050"
  },
  {
    "text": "and pollute and arsenic in slightly higher font size and let in lower font size the assumption is that what is",
    "start": "1058050",
    "end": "1065340"
  },
  {
    "text": "trying to tell you is mercury appears lot is expected to appear in a lot",
    "start": "1065340",
    "end": "1071520"
  },
  {
    "text": "higher frequency than the word arsenic or lead um so why do we do this I notice",
    "start": "1071520",
    "end": "1077760"
  },
  {
    "text": "there are two things you need to understand this one is that the algorithm does not give you a name for a",
    "start": "1077760",
    "end": "1083220"
  },
  {
    "text": "topic it's so topic 1 2 & 3 when I said a topic of philosophy that is how how humans name a",
    "start": "1083220",
    "end": "1090710"
  },
  {
    "text": "topic in this case in topic 2 it did not give you a name for a topic it only",
    "start": "1090710",
    "end": "1095960"
  },
  {
    "text": "gives you a distribution probability distribution of all the words in the language and that appear in that topic",
    "start": "1095960",
    "end": "1103130"
  },
  {
    "text": "ok notice how let is marked in in red color and that is an important thing to",
    "start": "1103130",
    "end": "1111139"
  },
  {
    "text": "note that a word could appear in multiple topics for example in topic 1",
    "start": "1111139",
    "end": "1116809"
  },
  {
    "text": "it's more about leadership and that's the intended meaning of the word of",
    "start": "1116809",
    "end": "1122929"
  },
  {
    "text": "course let is also involved in in poisoning so it could appear in this topic as well however the probability",
    "start": "1122929",
    "end": "1128630"
  },
  {
    "text": "distribution is not just as depends on a single word it depends on the distribution across all the words Nexus",
    "start": "1128630",
    "end": "1137370"
  },
  {
    "text": "right now the second thing we want to do is to identify where what are the topics",
    "start": "1137370",
    "end": "1142500"
  },
  {
    "text": "that's associated with the document what is it so we need a distribution of of",
    "start": "1142500",
    "end": "1147890"
  },
  {
    "text": "each of the probability that a document belongs to each topic right so here now",
    "start": "1147890",
    "end": "1155520"
  },
  {
    "text": "that's the second part of the second thing that the algorithm finds that is if you if you see the arrows the",
    "start": "1155520",
    "end": "1162600"
  },
  {
    "text": "thickness of the arrow that is that its showcase here there's illustrated here",
    "start": "1162600",
    "end": "1168680"
  },
  {
    "text": "tells you the probability that the document belongs to a certain topic so",
    "start": "1168680",
    "end": "1174450"
  },
  {
    "text": "you take the first topic talking about leadership today as you can see there's a it belongs to the first topic more",
    "start": "1174450",
    "end": "1182190"
  },
  {
    "text": "than the second topic and so the arrow from the first topic is ticker right second one it's the the second document",
    "start": "1182190",
    "end": "1190080"
  },
  {
    "text": "is somewhere in the middle third document is talking about poisoning",
    "start": "1190080",
    "end": "1195570"
  },
  {
    "text": "metal poison heavy metal poison so that definitely belongs to topic number two however since it how it might have the",
    "start": "1195570",
    "end": "1201510"
  },
  {
    "text": "word lead if there is a small probability it's making a guess that it's belongs there's a small probability",
    "start": "1201510",
    "end": "1208230"
  },
  {
    "text": "of topic 1 in there as well and you know as you notice one thing this algorithm",
    "start": "1208230",
    "end": "1213480"
  },
  {
    "text": "does not use understand the meanings of words it only takes the the algorithm",
    "start": "1213480",
    "end": "1220350"
  },
  {
    "text": "only takes the back of the words representation of documents if you're familiar with bag of words representation it only takes the words",
    "start": "1220350",
    "end": "1226650"
  },
  {
    "text": "that occur the algorithm does not even try to understand what the documents are talking about it only takes the",
    "start": "1226650",
    "end": "1233520"
  },
  {
    "text": "occurrence of words in these documents right so um so to summarize the process",
    "start": "1233520",
    "end": "1240390"
  },
  {
    "text": "if this is the most important thing to understand the whole webinar what is that is what is topic modeling what is the problem space right so to summarize",
    "start": "1240390",
    "end": "1247650"
  },
  {
    "text": "this slide just given a set of documents we create an unsupervised machine",
    "start": "1247650",
    "end": "1253530"
  },
  {
    "text": "learning a model a setup that is going to tell you what the topics are identify",
    "start": "1253530",
    "end": "1260640"
  },
  {
    "text": "the topics that are latent that's what latent an Lda means lay its de latent dirichlet allocation that is there are",
    "start": "1260640",
    "end": "1267570"
  },
  {
    "text": "hidden topics in this document that we are trying to find out so you have to want to the algorithm to",
    "start": "1267570",
    "end": "1273840"
  },
  {
    "text": "identify the latent topics that is hidden in the documents and also tell",
    "start": "1273840",
    "end": "1280500"
  },
  {
    "text": "you that the what words are expected in each topic also as a third thing you",
    "start": "1280500",
    "end": "1287250"
  },
  {
    "text": "also want to see the distribution or distribution of the topics for each",
    "start": "1287250",
    "end": "1293220"
  },
  {
    "text": "document so this is very similar to the centroid problem in the previous slide where we identified three clusters and",
    "start": "1293220",
    "end": "1300030"
  },
  {
    "text": "we want to know the properties of the cluster and then once you know the property of the cluster now you can classify each dot and c-bear it vary",
    "start": "1300030",
    "end": "1306990"
  },
  {
    "text": "each dot belongs all right which which cluster HR block do is very similar the vCloud the clusters are the topics and",
    "start": "1306990",
    "end": "1313200"
  },
  {
    "text": "the arrows point to the probability that document belongs to that top okay",
    "start": "1313200",
    "end": "1321340"
  },
  {
    "text": "now do you notice the I'm not talking",
    "start": "1321340",
    "end": "1326630"
  },
  {
    "start": "1323000",
    "end": "1508000"
  },
  {
    "text": "about the algorithm itself that's that there is actually a very interesting thing about this so we have so what is",
    "start": "1326630",
    "end": "1332540"
  },
  {
    "text": "the latent dirichlet mixture model it has two variables because this is important to understand just like how",
    "start": "1332540",
    "end": "1338630"
  },
  {
    "text": "you had the forest centroid of k-means are not so for the gaussian mixture models you identify the center of the",
    "start": "1338630",
    "end": "1346309"
  },
  {
    "text": "clusters and you also n defied the standard deviation or the the how closely located are the data points",
    "start": "1346309",
    "end": "1353360"
  },
  {
    "text": "right so this is the variance or the standard deviation so you have some kind of property for each topic so similarly",
    "start": "1353360",
    "end": "1360500"
  },
  {
    "text": "in a blatant Dilek model this is the most important thing to understand the Alpha and Beta also it's called",
    "start": "1360500",
    "end": "1366950"
  },
  {
    "text": "additional prior which which tells you that given a topic what is the frequency",
    "start": "1366950",
    "end": "1375049"
  },
  {
    "text": "that color once again they popped up in",
    "start": "1375049",
    "end": "1380210"
  },
  {
    "text": "my screen all right alpha with a prior estimate on the topic",
    "start": "1380210",
    "end": "1386980"
  },
  {
    "text": "probability that is what is the probability with which document belongs",
    "start": "1386980",
    "end": "1393670"
  },
  {
    "text": "to a certain top right if alpha is high it means there is a lot of mixtures of",
    "start": "1393670",
    "end": "1400780"
  },
  {
    "text": "topics like a single topic is a single document is talking about lots of",
    "start": "1400780",
    "end": "1406270"
  },
  {
    "text": "different topics whereas if alpha is low it means that there is a smaller number of topics for",
    "start": "1406270",
    "end": "1412450"
  },
  {
    "text": "each for each that that each document belong belongs to that is a document",
    "start": "1412450",
    "end": "1419020"
  },
  {
    "text": "starting with only one or two are a few number of topics that is the the the",
    "start": "1419020",
    "end": "1427080"
  },
  {
    "text": "Alpha a property the second a beta that",
    "start": "1427080",
    "end": "1432100"
  },
  {
    "text": "talks about the probability distribution of the frequency distribution of the",
    "start": "1432100",
    "end": "1437320"
  },
  {
    "text": "words the Recker in the topic just like how we identified lead then mercury and poisoning as words that belong to a",
    "start": "1437320",
    "end": "1444730"
  },
  {
    "text": "topic beta talks about this a the Union slide beta should tell you that topic",
    "start": "1444730",
    "end": "1451620"
  },
  {
    "text": "repres is represented by a smaller number of words which is larger number of words a smaller bigger means it was",
    "start": "1451620",
    "end": "1458770"
  },
  {
    "text": "represented by smaller number of so you have that really let's say if it's talking about cars you probably",
    "start": "1458770",
    "end": "1464170"
  },
  {
    "text": "have cars SUV and maybe convertible are the three only three words that represent the the topic but if a bit as",
    "start": "1464170",
    "end": "1472120"
  },
  {
    "text": "very large means you have like lots of words that represent the topic so these are the two properties of a distillate",
    "start": "1472120",
    "end": "1480250"
  },
  {
    "text": "mixture model it's important to understand that this actually alpha and beta are is actually the British late",
    "start": "1480250",
    "end": "1487390"
  },
  {
    "text": "model LD a what this is not the algorithm which is very important",
    "start": "1487390",
    "end": "1493240"
  },
  {
    "text": "understand this LD a is not an algorithm it's more of rules so L when you say L",
    "start": "1493240",
    "end": "1498580"
  },
  {
    "text": "be a it's more of the problem statement or the modeling of the problem into mathematical terms okay now what we want",
    "start": "1498580",
    "end": "1506470"
  },
  {
    "text": "to do is the algorithm itself is it's called spectral decomposition for LD a",
    "start": "1506470",
    "end": "1511660"
  },
  {
    "start": "1508000",
    "end": "1615000"
  },
  {
    "text": "there are multiple ways to do L VA and the Amazon has chosen",
    "start": "1511660",
    "end": "1517600"
  },
  {
    "text": "use what is called a spectral decomposition for Lda here is a very brief four lines of the algorithm it's",
    "start": "1517600",
    "end": "1526750"
  },
  {
    "text": "not possible for immediately go over the algorithm itself during this webinar because that is the two hour or three",
    "start": "1526750",
    "end": "1532690"
  },
  {
    "text": "hour discussion just to describe what this is doing and how it compares to the other algorithms but but very high level",
    "start": "1532690",
    "end": "1538929"
  },
  {
    "text": "this is a high dimensional problem the goal is to calculate the spectral decomposition and more explain to me",
    "start": "1538929",
    "end": "1544330"
  },
  {
    "text": "what spectral they compose in a little bit but you do not need to understand this to use Lda on a sage maker this is",
    "start": "1544330",
    "end": "1551590"
  },
  {
    "text": "all hidden for you this is all done for you in the background so that there are four steps the four steps is the first",
    "start": "1551590",
    "end": "1557260"
  },
  {
    "text": "is to convert the documents into a B baby baby Eve Ector tensor we where V is",
    "start": "1557260",
    "end": "1562570"
  },
  {
    "text": "actually the vocabulary size and then you what you do is you do a something called a whitening matrix of dimension V",
    "start": "1562570",
    "end": "1569169"
  },
  {
    "text": "by K where K is the number of topics so that is the hyper parameter that you that you provide",
    "start": "1569169",
    "end": "1574750"
  },
  {
    "text": "then you procreate a kind of smaller K by K by K attention and last is where",
    "start": "1574750",
    "end": "1581830"
  },
  {
    "text": "you do the ALS or alternate least squares to decompose the smaller K by K tensor so obviously this I don't think I",
    "start": "1581830",
    "end": "1591700"
  },
  {
    "text": "explained this very well I don't have the time to explain in this webinar we can do a separate webinar or there is",
    "start": "1591700",
    "end": "1597070"
  },
  {
    "text": "actually a YouTube recording of of how this works there's also a reference to",
    "start": "1597070",
    "end": "1602740"
  },
  {
    "text": "how Lda works our spectral decomposition works in the documentation link this would be provided to you in the in the",
    "start": "1602740",
    "end": "1609210"
  },
  {
    "text": "flight deck after this webinar go ahead and and if you're interested you can go ahead and check it",
    "start": "1609210",
    "end": "1614870"
  },
  {
    "text": "okay now let's look at the last part so what is called alternate least squares for matrix decomposition so what we did",
    "start": "1614870",
    "end": "1622040"
  },
  {
    "start": "1615000",
    "end": "1837000"
  },
  {
    "text": "was we convert the document carpus into",
    "start": "1622040",
    "end": "1627350"
  },
  {
    "text": "a B by B by B matrix and then we decompose this into in a certain way so that the topics pop out right so let's",
    "start": "1627350",
    "end": "1636320"
  },
  {
    "text": "look at this this is this is not the this is a very very high-level representation of what it does it's not",
    "start": "1636320",
    "end": "1642860"
  },
  {
    "text": "a direct fit into the spectral decomposition used for Lda in sage maker",
    "start": "1642860",
    "end": "1648470"
  },
  {
    "text": "but I think this should give you a good starting point to work to where it goes this is a picture that represents how",
    "start": "1648470",
    "end": "1655160"
  },
  {
    "text": "you separate users and products so so we start with the left side the orange box",
    "start": "1655160",
    "end": "1662060"
  },
  {
    "text": "right this is about a product ratings rate or product purchase patterns you",
    "start": "1662060",
    "end": "1669860"
  },
  {
    "text": "have ie roll height row and jth column which means that it's set to 1 if a",
    "start": "1669860",
    "end": "1674990"
  },
  {
    "text": "customer has bought a product so the the rows represents the column add rows represent the I let say it's amazon.com",
    "start": "1674990",
    "end": "1682940"
  },
  {
    "text": "and your each each row represents a customer and each column represents a",
    "start": "1682940",
    "end": "1689960"
  },
  {
    "text": "product so let's say I bought a book on Amazon I might be the 150th customer",
    "start": "1689960",
    "end": "1698600"
  },
  {
    "text": "150th row and the column could be the book individual book could be an order",
    "start": "1698600",
    "end": "1704390"
  },
  {
    "text": "million million column and that will be set to 1 if I had purchased a book right so we create a large matrix because we",
    "start": "1704390",
    "end": "1711800"
  },
  {
    "text": "have millions of for hundreds of millions of customers and millions of products on Amazon right so this is the",
    "start": "1711800",
    "end": "1717830"
  },
  {
    "text": "left side R that's the mention this R is a very large matrix right and now what",
    "start": "1717830",
    "end": "1724490"
  },
  {
    "text": "we want to do is we want to accept we want convert this or decompose it's",
    "start": "1724490",
    "end": "1730100"
  },
  {
    "text": "about the typical term here is decompose decompose this into two different matrices right so and",
    "start": "1730100",
    "end": "1738340"
  },
  {
    "text": "and that is what we come to the right site right in the right side we split this into a blue matrix and a a green",
    "start": "1738340",
    "end": "1745600"
  },
  {
    "text": "matrix and now the dimensions have now changed the bit of the U matrix is K and",
    "start": "1745600",
    "end": "1752320"
  },
  {
    "text": "the height of U matrix is K again so when you multiply these two blue and green matrix the u u u cross P you would",
    "start": "1752320",
    "end": "1759970"
  },
  {
    "text": "automatically get R now it is a non-trivial process to go from R equals",
    "start": "1759970",
    "end": "1766600"
  },
  {
    "text": "UV cross P because that is that is the algorithm it needs to happen efficiently",
    "start": "1766600",
    "end": "1772539"
  },
  {
    "text": "you need to have certain properties so but what the important thing is K so k",
    "start": "1772539",
    "end": "1778029"
  },
  {
    "text": "here means a user let's say let's take the example of you matrix u u represents",
    "start": "1778029",
    "end": "1784510"
  },
  {
    "text": "users the users can be represented in a k dimensional vector and the in this and",
    "start": "1784510",
    "end": "1790570"
  },
  {
    "text": "similarly you do something very similar in spectral decomposition for LBA where",
    "start": "1790570",
    "end": "1796450"
  },
  {
    "text": "this K becomes the number of topics so the dimension of this u matrix and P",
    "start": "1796450",
    "end": "1802149"
  },
  {
    "text": "matrix the new dimension that we have added becomes the number of topics it's not exactly the same but this is a very",
    "start": "1802149",
    "end": "1809110"
  },
  {
    "text": "high-level view of what is what is happening so what is shown here is not the algorithm also here again it's not",
    "start": "1809110",
    "end": "1815380"
  },
  {
    "text": "alga this is still the setup so what we're doing is we took a be baby-baby matrix in our in our LD a and we're",
    "start": "1815380",
    "end": "1821620"
  },
  {
    "text": "going to decompose this to in a certain way so it has certain properties the algorithm is actually this is the way in",
    "start": "1821620",
    "end": "1829539"
  },
  {
    "text": "which you can work the left matrix and decompose the left matrix into the right",
    "start": "1829539",
    "end": "1834850"
  },
  {
    "text": "matrix that is the real algorithm right there are three popular algorithms a give sampling with one method",
    "start": "1834850",
    "end": "1841320"
  },
  {
    "start": "1837000",
    "end": "1855000"
  },
  {
    "text": "expectation maximization is method is another method that's also popular but the amazon sage maker uses spectral",
    "start": "1841320",
    "end": "1848559"
  },
  {
    "text": "decomposition that is that is what we used and because that is have some very",
    "start": "1848559",
    "end": "1854799"
  },
  {
    "text": "good properties these are the three properties if there are theoretical guarantees on results the en method that",
    "start": "1854799",
    "end": "1861520"
  },
  {
    "start": "1855000",
    "end": "1899000"
  },
  {
    "text": "is the expectation maximization method is guaranteed only to converge to a",
    "start": "1861520",
    "end": "1868059"
  },
  {
    "text": "local optima the spectral decomposition is also very eligible because we want to scale it",
    "start": "1868059",
    "end": "1874330"
  },
  {
    "text": "sort of fact and so when it's fast it's going to sail customers $1 for using",
    "start": "1874330",
    "end": "1882130"
  },
  {
    "text": "this so they have some very nice properties this is also mentioned in the in the paper by an imagery on",
    "start": "1882130",
    "end": "1889450"
  },
  {
    "text": "commodities the scientists at Amazon who was one of the persons who built this",
    "start": "1889450",
    "end": "1895030"
  },
  {
    "text": "algorithm for shake maker so please feel free to refer to this right now let's",
    "start": "1895030",
    "end": "1900310"
  },
  {
    "start": "1899000",
    "end": "1959000"
  },
  {
    "text": "get back to the but we don't need to know all these details to use Lda to use",
    "start": "1900310",
    "end": "1905830"
  },
  {
    "text": "a maker Lda right so what you need to do is understand how it is used right",
    "start": "1905830",
    "end": "1911230"
  },
  {
    "text": "because it's all packaged neatly you just make a few calls and then it would do it for you so I don't want you how",
    "start": "1911230",
    "end": "1916960"
  },
  {
    "text": "you actually use these you start on the left side with its bunch of documents and you do a data pre-processing I pre",
    "start": "1916960",
    "end": "1925780"
  },
  {
    "text": "process data because you cannot send words you need to convert the words into",
    "start": "1925780",
    "end": "1930880"
  },
  {
    "text": "numbers that is you do not use work to make here are blazing texts what you're",
    "start": "1930880",
    "end": "1936100"
  },
  {
    "text": "doing is you reduce the number of cores you remove the words that are not necessary for example stop work like the in all those things do not represent",
    "start": "1936100",
    "end": "1943950"
  },
  {
    "text": "topics so you remove those words and then you bring it reduce the number of words that you need because the number",
    "start": "1943950",
    "end": "1950050"
  },
  {
    "text": "of words the faster the algorithm runs then you run the topic modeling algorithm and then you have the topics",
    "start": "1950050",
    "end": "1955150"
  },
  {
    "text": "and the allocation of topics so that's the general pipeline okay or briefly",
    "start": "1955150",
    "end": "1961660"
  },
  {
    "start": "1959000",
    "end": "1996000"
  },
  {
    "text": "talk with the hyper parameters there is three most important ones number of topics you need to provide the number of",
    "start": "1961660",
    "end": "1967870"
  },
  {
    "text": "topics that is not going to be it's not an open-ended a topic it is a specific",
    "start": "1967870",
    "end": "1973420"
  },
  {
    "text": "number of topic is going to look for future dimension this is the vocabulary size if you have ten thousand words",
    "start": "1973420",
    "end": "1978550"
  },
  {
    "text": "versus twenty thousand words and a mini batch size you can this is this is the way in which it runs the algorithm but",
    "start": "1978550",
    "end": "1985870"
  },
  {
    "text": "other other values are not are optional and are not important so for you to be able to change other algorithms other",
    "start": "1985870",
    "end": "1991200"
  },
  {
    "text": "parameters I want you to really look into the algorithm and understand what it does before you do that all right",
    "start": "1991200",
    "end": "1996970"
  },
  {
    "start": "1996000",
    "end": "2034000"
  },
  {
    "text": "what is a metric I always talk about a metric when talking about an algorithm it's called PWL oh this per word",
    "start": "1996970",
    "end": "2003540"
  },
  {
    "text": "log-likelihood on a data set what this means is let's say the",
    "start": "2003540",
    "end": "2009060"
  },
  {
    "text": "algorithm has come up with model or Lda model that is topics and we're",
    "start": "2009060",
    "end": "2014700"
  },
  {
    "text": "distribution for each topic and the topic distribution for each document what does strength jealous let given",
    "start": "2014700",
    "end": "2021690"
  },
  {
    "text": "that model if you generate the data set how closely does that represent this corpus of data right so you see how well",
    "start": "2021690",
    "end": "2029760"
  },
  {
    "text": "it matches and that is that is what is represented here as per word log likelihood okay here's a sample python",
    "start": "2029760",
    "end": "2037020"
  },
  {
    "text": "pseudocode this is just like five lines of code is all you need to understand I'm going to be showing you a demo very",
    "start": "2037020",
    "end": "2042720"
  },
  {
    "text": "quickly so the first line is starting at section you just copy that the second line is creating an estimator this is to",
    "start": "2042720",
    "end": "2050159"
  },
  {
    "text": "create the object before you start your training job and this is very specify",
    "start": "2050160",
    "end": "2055830"
  },
  {
    "text": "what algorithms here the container not sure if you can see my mouse one of the parameters that you pass is the",
    "start": "2055830",
    "end": "2061320"
  },
  {
    "text": "container name that is where you specify the algorithm where it's gonna be Lda or npm or our xt boost or whatever right",
    "start": "2061320",
    "end": "2069200"
  },
  {
    "text": "the third is you set the hyper parameters this is very high city high price then you call fit and you call fit",
    "start": "2069200",
    "end": "2076260"
  },
  {
    "text": "the training actually starts where this say maker is going to be spinning up resources and finally once it's done",
    "start": "2076260",
    "end": "2082740"
  },
  {
    "text": "it's going to take you certain amount of time depending on the vocabulary size and the size de algorithm chosen and",
    "start": "2082740",
    "end": "2089639"
  },
  {
    "text": "finally you deploy the model to make inference real time in french all right",
    "start": "2089640",
    "end": "2095010"
  },
  {
    "start": "2093000",
    "end": "2285000"
  },
  {
    "text": "so that's the five steps i'm going to show you the my console so",
    "start": "2095010",
    "end": "2102460"
  },
  {
    "text": "this is my stage maker others if free sorry",
    "start": "2102460",
    "end": "2107609"
  },
  {
    "text": "if you're not able to see this this is my fave maker console I clicked on my products demo it gives me this as you",
    "start": "2107609",
    "end": "2115470"
  },
  {
    "text": "can see the third tab in my jupiter notebook has the segment examples when",
    "start": "2115470",
    "end": "2120690"
  },
  {
    "text": "you drop down on the first one you see example notebooks in this and I chose",
    "start": "2120690",
    "end": "2126950"
  },
  {
    "text": "Lda right here ld8 reduction this notebook then is copied and you can run",
    "start": "2126950",
    "end": "2133380"
  },
  {
    "text": "this it takes less than 10 minutes to run the whole notebook it gives you all the details it does some pre-processing",
    "start": "2133380",
    "end": "2139170"
  },
  {
    "text": "because we need a sci-fi that is for preparing the",
    "start": "2139170",
    "end": "2145068"
  },
  {
    "text": "the data set this notebook is actually using synthetic data it's not using real",
    "start": "2145240",
    "end": "2151450"
  },
  {
    "text": "documentation generating a sense distribution of documents and then trying to learn the topics that's what",
    "start": "2151450",
    "end": "2157960"
  },
  {
    "text": "it's trying to do it setting up first it's it's setting up your execution role I think you might be familiar with the",
    "start": "2157960",
    "end": "2163570"
  },
  {
    "text": "previous previous webinars but the first",
    "start": "2163570",
    "end": "2169990"
  },
  {
    "text": "part is about generating the data right then they are visualizing the data and",
    "start": "2169990",
    "end": "2175630"
  },
  {
    "text": "here's where it starts so you from say maker input sake maker here's where you get the container I'm",
    "start": "2175630",
    "end": "2182710"
  },
  {
    "text": "going to I know and I understand I'm going very fast so here you create a section like I mentioned in my",
    "start": "2182710",
    "end": "2188290"
  },
  {
    "text": "pseudocode and then you create an Lda estimator where you give the details",
    "start": "2188290",
    "end": "2193300"
  },
  {
    "text": "about where the output passes what is this instance types to use you set the hyper parameters here the number",
    "start": "2193300",
    "end": "2200650"
  },
  {
    "text": "of topics the vocabulary size the future dimension and the mini back size that's all you need to give and then you call",
    "start": "2200650",
    "end": "2206260"
  },
  {
    "text": "fit so this one took I think five minutes to run very small data size only",
    "start": "2206260",
    "end": "2214150"
  },
  {
    "text": "25 words in my vocabulary weight and the job was complete then I did a inference",
    "start": "2214150",
    "end": "2221590"
  },
  {
    "text": "I deployed that's the last step here I deployed the model on em for extra-large",
    "start": "2221590",
    "end": "2227100"
  },
  {
    "text": "with just one count and here's where we tested right I'm taking the document",
    "start": "2227100",
    "end": "2235200"
  },
  {
    "text": "this in this case the topic the number of topics I provided was five so given",
    "start": "2235200",
    "end": "2241960"
  },
  {
    "text": "the first document it is giving me a mixture of a distribution across the",
    "start": "2241960",
    "end": "2247869"
  },
  {
    "text": "five topics so it gives me three two zero six seven zero zero that means at",
    "start": "2247869",
    "end": "2254260"
  },
  {
    "text": "this document it has a 67 percent is talking about a topic number three and",
    "start": "2254260",
    "end": "2261670"
  },
  {
    "text": "there's 32 percent topic number one and it does not talk about topic two three",
    "start": "2261670",
    "end": "2267100"
  },
  {
    "text": "to four and five that's that's pretty much it so go ahead and try it out this example notebook is",
    "start": "2267100",
    "end": "2274300"
  },
  {
    "text": "available it should take you not less than not more than 10 minutes to run that didi algorithm so with that I want",
    "start": "2274300",
    "end": "2285520"
  },
  {
    "start": "2285000",
    "end": "2310000"
  },
  {
    "text": "to open up for questions and here are my details",
    "start": "2285520",
    "end": "2291180"
  },
  {
    "text": "all right let me know if we have one here and if your notebook is still open",
    "start": "2291500",
    "end": "2298160"
  },
  {
    "text": "the question is about that as the question is its pre processing done automatically by ldaa of them not quite",
    "start": "2298160",
    "end": "2305119"
  },
  {
    "text": "certain exactly which pre-processing they're referring to okay good question",
    "start": "2305119",
    "end": "2312590"
  },
  {
    "start": "2310000",
    "end": "2470000"
  },
  {
    "text": "so Lda does not take words as inputs it does not do any pre-processing um Lda",
    "start": "2312590",
    "end": "2319730"
  },
  {
    "text": "expects a large array right by the LDA is not just used for topic modeling like",
    "start": "2319730",
    "end": "2326359"
  },
  {
    "text": "I mentioned it's used for others as well you can use this for structured data as well what it needs is you need to take",
    "start": "2326359",
    "end": "2332570"
  },
  {
    "text": "your documents convert this into account the number of reduce the words in your",
    "start": "2332570",
    "end": "2339380"
  },
  {
    "text": "document and then create a large matrix like for each document let's say you",
    "start": "2339380",
    "end": "2344510"
  },
  {
    "text": "have a hundred thousand vocabulary of hundred thousand in English what you need to do is for each document create a",
    "start": "2344510",
    "end": "2350810"
  },
  {
    "text": "long array and mark one wherever a word occurs right and mark 0 when the word",
    "start": "2350810",
    "end": "2357560"
  },
  {
    "text": "does not occur for e for that document and then you create that row for each document so now you have a large matrix",
    "start": "2357560",
    "end": "2364310"
  },
  {
    "text": "once you have that matrix as given here I'll show you this here's here's the",
    "start": "2364310",
    "end": "2373250"
  },
  {
    "text": "example document here they're talking about a 25 work average is very small",
    "start": "2373250",
    "end": "2379190"
  },
  {
    "text": "vocabulary of just 25 as you can see this is descending one document so this",
    "start": "2379190",
    "end": "2384200"
  },
  {
    "text": "means the first word is a plank 31 times here the second third and fourth and",
    "start": "2384200",
    "end": "2389359"
  },
  {
    "text": "fifth word do not occur 21 times the sixth word occur and so on so you need to convert your documents",
    "start": "2389359",
    "end": "2395690"
  },
  {
    "text": "into a format like this create an array off of this and feed that into Lda",
    "start": "2395690",
    "end": "2401680"
  },
  {
    "text": "algorithm that is the pre-processing step you probably want to pre-process some more",
    "start": "2401680",
    "end": "2408830"
  },
  {
    "text": "you want to remove the words that do not represent a topic for example stop words",
    "start": "2408830",
    "end": "2414200"
  },
  {
    "text": "like the and these things do not represent a topics so that's where the pre-processing step would really help",
    "start": "2414200",
    "end": "2420440"
  },
  {
    "text": "you in getting the right topics so one thing to note is that the topics that the LDA",
    "start": "2420440",
    "end": "2426950"
  },
  {
    "text": "are the any of these unsupervised algorithms comes up with may not align with what human beings are very newest",
    "start": "2426950",
    "end": "2433520"
  },
  {
    "text": "topics for example it need not come up with the topics that we think are are the right ones because these are is an",
    "start": "2433520",
    "end": "2440750"
  },
  {
    "text": "unsupervised algorithm so you may have to rerun this and fine-tune the pre-processing step add the right words",
    "start": "2440750",
    "end": "2446120"
  },
  {
    "text": "to make sure that it finds the right topics if you cannot influence the",
    "start": "2446120",
    "end": "2453310"
  },
  {
    "text": "condition the topics to be in a certain way this is this is automatically",
    "start": "2453310",
    "end": "2458630"
  },
  {
    "text": "generated all right",
    "start": "2458630",
    "end": "2464600"
  },
  {
    "text": "any other questions",
    "start": "2464600",
    "end": "2467860"
  },
  {
    "start": "2470000",
    "end": "2580000"
  },
  {
    "text": "I see some more what about the clusters that do not have spiritual shape so that",
    "start": "2470450",
    "end": "2477470"
  },
  {
    "text": "is I think that is representing the two-dimensional data you could have elliptical or a higher",
    "start": "2477470",
    "end": "2485660"
  },
  {
    "text": "dimensional data that also would be part of the model soaps so when you create a",
    "start": "2485660",
    "end": "2491359"
  },
  {
    "text": "model that says you're describing the clusters you could have that as well you",
    "start": "2491359",
    "end": "2497270"
  },
  {
    "text": "could have a this is just two-dimensional you can imagine how how",
    "start": "2497270",
    "end": "2502670"
  },
  {
    "text": "to describe this in each of the dimensions right so that is definitely is also one of the aspects of the model",
    "start": "2502670",
    "end": "2510619"
  },
  {
    "text": "search and the Euclidean metric is not applicable that's disagreed that's the",
    "start": "2510619",
    "end": "2517309"
  },
  {
    "text": "algorithm produce topics I try to answer this the algorithm produces topics but",
    "start": "2517309",
    "end": "2525020"
  },
  {
    "text": "that may or may not align with what human beings know by topics what it means identifies this is topic 0 1 2 1 3",
    "start": "2525020",
    "end": "2531890"
  },
  {
    "text": "and each topic would it would give you the list of words that is expected to appear in that topic I think we discuss",
    "start": "2531890",
    "end": "2539450"
  },
  {
    "text": "this another question its pre-processing then automatic by Lda now I think we",
    "start": "2539450",
    "end": "2545180"
  },
  {
    "text": "talked about this can we do pre preparation and processing in parallel no you have to pre-process the data and",
    "start": "2545180",
    "end": "2554119"
  },
  {
    "text": "store it in s3 and then call a sage maker that is separate because this is a job",
    "start": "2554119",
    "end": "2560770"
  },
  {
    "text": "training job submission it takes it takes the data from f3 so you can hear",
    "start": "2560770",
    "end": "2568120"
  },
  {
    "text": "you can if you are running multiple training sessions you could do this but it is generally not recommended",
    "start": "2568120",
    "end": "2575190"
  },
  {
    "text": "any other specific question um yeah we just got a few in I'm not sure if there",
    "start": "2579190",
    "end": "2584690"
  },
  {
    "start": "2580000",
    "end": "2705000"
  },
  {
    "text": "was a buffer issue or what but they just say this came in there was here's a good one how do we see the list of words that",
    "start": "2584690",
    "end": "2590360"
  },
  {
    "text": "describe a topic I'm sorry say that again how do how do you see the list of",
    "start": "2590360",
    "end": "2596420"
  },
  {
    "text": "words that describe a talk so when it gives you a topic it's going to give you",
    "start": "2596420",
    "end": "2601460"
  },
  {
    "text": "a distribution right for their entire vocabulary it's for each topic you're",
    "start": "2601460",
    "end": "2606920"
  },
  {
    "text": "gonna see an array for the entire vocabulary let's say it's ten thousand and this vocabulary is gonna have",
    "start": "2606920",
    "end": "2613340"
  },
  {
    "text": "numbers like you add the entire thing it would add up to some sum of one and for the word that is supposed to appear you",
    "start": "2613340",
    "end": "2620150"
  },
  {
    "text": "three nonzero numbers right for example let's say for example the word mercury",
    "start": "2620150",
    "end": "2625520"
  },
  {
    "text": "lead and the poisoning these are the words which should have nonzero probability the other word would have a",
    "start": "2625520",
    "end": "2631310"
  },
  {
    "text": "zero property of zero or close to zero so that is how the topics would show up",
    "start": "2631310",
    "end": "2637089"
  },
  {
    "text": "okay here's another one can we get the",
    "start": "2637600",
    "end": "2644030"
  },
  {
    "text": "priority or the importance of topics found in a single document yes",
    "start": "2644030",
    "end": "2650510"
  },
  {
    "text": "so for each document you would get a distribution probability distribution",
    "start": "2650510",
    "end": "2655970"
  },
  {
    "text": "across the topics for example let's say if you assigned and beside this notebook example as well let me scroll down when",
    "start": "2655970",
    "end": "2670340"
  },
  {
    "text": "you do an inference I'm selling a whole I'm trying to see very yes here here you",
    "start": "2670340",
    "end": "2675740"
  },
  {
    "text": "go so I'm doing a prediction on the deployed model and I'm setting the first",
    "start": "2675740",
    "end": "2681260"
  },
  {
    "text": "document and you can see that it's giving you a distribution probability switch across the five topics in this",
    "start": "2681260",
    "end": "2686300"
  },
  {
    "text": "case I just fade the top it should be five and there's a 67% words represent",
    "start": "2686300",
    "end": "2693380"
  },
  {
    "text": "our from topic number three and 32% are from topic number one so that is the",
    "start": "2693380",
    "end": "2698540"
  },
  {
    "text": "distribution that you get make sense all right next question",
    "start": "2698540",
    "end": "2707050"
  },
  {
    "start": "2705000",
    "end": "2813000"
  },
  {
    "text": "yeah it's nesting 1 does it make sense during the pre-processing to not only remove the words with low frequency but",
    "start": "2707050",
    "end": "2715240"
  },
  {
    "text": "also the word with extremely high frequency I think he's going for some type of normalization effect here yes",
    "start": "2715240",
    "end": "2721990"
  },
  {
    "text": "yes you should do that that is highly recommended for example words like a and",
    "start": "2721990",
    "end": "2729010"
  },
  {
    "text": "the mean the word appears like 7% in the entire English language or Karpis so you",
    "start": "2729010",
    "end": "2736630"
  },
  {
    "text": "probably want to remove that because you're adding your when you remove the words that there occurred commonly what",
    "start": "2736630",
    "end": "2743349"
  },
  {
    "text": "did what it helps is it reduces the number of dimensions or the vocabulary sides so that's going to make it faster",
    "start": "2743349",
    "end": "2749200"
  },
  {
    "text": "one the second thing is the word the urn and these are called stop words and NLP",
    "start": "2749200",
    "end": "2754530"
  },
  {
    "text": "they really do not help you in identifying the topics right so the topics are represented by more specific",
    "start": "2754530",
    "end": "2761230"
  },
  {
    "text": "terms right for example let than poisoning those are the kinds of topics that that really represent the topics",
    "start": "2761230",
    "end": "2767859"
  },
  {
    "text": "the word these kinds of words do not represent and there's a very specific way to remove them it's called tf-idf",
    "start": "2767859",
    "end": "2773559"
  },
  {
    "text": "pre-processing step what that takes us it takes the entire corpus emphasize the",
    "start": "2773559",
    "end": "2779710"
  },
  {
    "text": "number of times a word occurs and sees if if that word occurs in your near corpus and removes them so you can use a",
    "start": "2779710",
    "end": "2786160"
  },
  {
    "text": "what's called tf-idf a method to remove all those occurring words however I",
    "start": "2786160",
    "end": "2792609"
  },
  {
    "text": "don't think you should remove the words that occur that do not that occur very infrequently those are the words that",
    "start": "2792609",
    "end": "2799780"
  },
  {
    "text": "could represent the topic so you do not want to remove the words that occur that",
    "start": "2799780",
    "end": "2805180"
  },
  {
    "text": "are not common so that is the recommendation for topic modeling",
    "start": "2805180",
    "end": "2810190"
  },
  {
    "text": "but pre-processing for the topic model and then here's a another good one that",
    "start": "2810190",
    "end": "2815450"
  },
  {
    "start": "2813000",
    "end": "2980000"
  },
  {
    "text": "I I missed I do apologize they all streamed in here very quickly how does ld8 handle large data and can I",
    "start": "2815450",
    "end": "2821300"
  },
  {
    "text": "take sparse matrix I'm sorry say it again how does the FDA algorithm handle",
    "start": "2821300",
    "end": "2828920"
  },
  {
    "text": "large amounts of data and can I take sparse matrix um so the very I think I",
    "start": "2828920",
    "end": "2837080"
  },
  {
    "text": "see another example also here there are like thousand pages in my document how does it how does it take in large data",
    "start": "2837080",
    "end": "2847210"
  },
  {
    "text": "the data is when when according to for the LDA it does not look at the document",
    "start": "2847210",
    "end": "2854060"
  },
  {
    "text": "right each document is represented by this very very same by a by an array of",
    "start": "2854060",
    "end": "2860599"
  },
  {
    "text": "the same size whether you could have a document that's one pages and you could have a document with thousand pages both",
    "start": "2860599",
    "end": "2867290"
  },
  {
    "text": "of these documents are represented by one array where where the vocabularies",
    "start": "2867290",
    "end": "2872780"
  },
  {
    "text": "the the there's you increase the count of the words number of times the word occurs in that array right so let me",
    "start": "2872780",
    "end": "2879260"
  },
  {
    "text": "show you in that notebook so it the size of the document this matters only doing",
    "start": "2879260",
    "end": "2884750"
  },
  {
    "text": "the pre-processing step but does not really matter for the algorithm itself let me give you else here is the example",
    "start": "2884750",
    "end": "2891440"
  },
  {
    "text": "right in this case each document is represented by a vocabulary size of finished five this is a very extreme",
    "start": "2891440",
    "end": "2897980"
  },
  {
    "text": "example very small dimensional usually you can expect thousand or ten thousand words this is 25 words is what is",
    "start": "2897980",
    "end": "2905869"
  },
  {
    "text": "represent this could be a very large document but what it's saying is the first word occurs 31 times second 50",
    "start": "2905869",
    "end": "2912140"
  },
  {
    "text": "workers 21 times and 42 times it does not matter if the document is larger wrong what really matters is the",
    "start": "2912140",
    "end": "2918800"
  },
  {
    "text": "vocabulary size you have to be very careful of the vocabulary size if that blows up because it's a B by B by B",
    "start": "2918800",
    "end": "2924109"
  },
  {
    "text": "matrix that starts with AB so that's a very large matrix you have like ten thousand if you can somehow bring it",
    "start": "2924109",
    "end": "2930650"
  },
  {
    "text": "down to thousand your algorithm will run much faster because it's a cube it's a",
    "start": "2930650",
    "end": "2936200"
  },
  {
    "text": "cubic tenser so tend to refractory so that is that is what it matters",
    "start": "2936200",
    "end": "2941769"
  },
  {
    "text": "document states really does not matter you can use popular open-source tools",
    "start": "2941769",
    "end": "2947660"
  },
  {
    "text": "like NL TK or there's a newer tool called a c4 for the pre-processing step",
    "start": "2947660",
    "end": "2954380"
  },
  {
    "text": "that would efficiently work on your documents to create these kinds of vocabulary our bag of words",
    "start": "2954380",
    "end": "2961249"
  },
  {
    "text": "representation so that should not be it that should be fairly easy you don't have to write a no Python code for that",
    "start": "2961249",
    "end": "2968509"
  },
  {
    "text": "there are there are very popular mature open-source projects that are available",
    "start": "2968509",
    "end": "2974029"
  },
  {
    "text": "to help you do that okay any other questions",
    "start": "2974029",
    "end": "2980440"
  },
  {
    "start": "2980000",
    "end": "3075000"
  },
  {
    "text": "yeah there was there's one here I accidently answered it privately so we'll throw it out there so the whole crowd can hear does it work for all",
    "start": "2980970",
    "end": "2987870"
  },
  {
    "text": "languages or just English ah very good question does the word are these words",
    "start": "2987870",
    "end": "2995040"
  },
  {
    "text": "in English or in other languages the example shown here actually the LDA does",
    "start": "2995040",
    "end": "3001310"
  },
  {
    "text": "not know what the words mean or even the spelling of the meanings right you define the vocabulary right",
    "start": "3001310",
    "end": "3007760"
  },
  {
    "text": "the vocabulary could be French German or English it doesn't matter as you can see all it takes all Lda takes is an array",
    "start": "3007760",
    "end": "3014240"
  },
  {
    "text": "of numbers great it only knows that hey this is we're number one were number six word number 10 word number 14 and so on",
    "start": "3014240",
    "end": "3020900"
  },
  {
    "text": "it does not really the LG actually does not even see the words so it is during",
    "start": "3020900",
    "end": "3026960"
  },
  {
    "text": "the pre-processing step that you can work the vocabulary into numbers right",
    "start": "3026960",
    "end": "3032060"
  },
  {
    "text": "or indexes in this array so so one point to be noted here is that when you're",
    "start": "3032060",
    "end": "3038480"
  },
  {
    "text": "dealing with a with Chinese or Japanese which tend to have a very large number",
    "start": "3038480",
    "end": "3044240"
  },
  {
    "text": "of words because everything has its own I think that's a large number of characters I think Oh capillary size",
    "start": "3044240",
    "end": "3050390"
  },
  {
    "text": "might be different so you might want to be careful when you're using the Chinese or Japanese or or languages that belong",
    "start": "3050390",
    "end": "3056360"
  },
  {
    "text": "in that in that family you want to be you want to be careful about how you pre",
    "start": "3056360",
    "end": "3061970"
  },
  {
    "text": "pass but there are open source tools to handle Chinese and Japanese and those kinds of languages you can use that",
    "start": "3061970",
    "end": "3069630"
  },
  {
    "text": "very good question though",
    "start": "3069630",
    "end": "3072588"
  },
  {
    "start": "3075000",
    "end": "3285000"
  },
  {
    "text": "any other questions yeah just got one just looks like we got time for one more",
    "start": "3076310",
    "end": "3083620"
  },
  {
    "text": "is gone go went the same word so it looks like it's a question about you",
    "start": "3083620",
    "end": "3089510"
  },
  {
    "text": "know the the tense past present and future tenses of words so it's asking if",
    "start": "3089510",
    "end": "3095210"
  },
  {
    "text": "Lda understands conjugation yes okay that's also another very good",
    "start": "3095210",
    "end": "3100550"
  },
  {
    "text": "question so I really like these questions because this is talking about the pre-processing step and that is where you need to be focusing on right",
    "start": "3100550",
    "end": "3106370"
  },
  {
    "text": "so the question is about let's say you have word let's say a gone go gone or",
    "start": "3106370",
    "end": "3113750"
  },
  {
    "text": "went these are past past tension and maybe even going future tenses of the same",
    "start": "3113750",
    "end": "3119960"
  },
  {
    "text": "word that is a really important thing to think about saying that does this could",
    "start": "3119960",
    "end": "3126500"
  },
  {
    "text": "be represented in multiple words right each word could be represented separate word or these could represent could be",
    "start": "3126500",
    "end": "3133030"
  },
  {
    "text": "converted into the root word so this is there in the called limit ization in the",
    "start": "3133030",
    "end": "3138440"
  },
  {
    "text": "natural language processing terminology where you take the word and you actually",
    "start": "3138440",
    "end": "3143990"
  },
  {
    "text": "go to the root of the word and you represent that this actually does two things one is that the number of",
    "start": "3143990",
    "end": "3150920"
  },
  {
    "text": "vocabulary the vocabulary size decreases right if you don't represent this way like five words and and go to the root",
    "start": "3150920",
    "end": "3157910"
  },
  {
    "text": "word so it becomes 1 so the you are reducing the vocabulary by a factor of five that is a huge improve that would",
    "start": "3157910",
    "end": "3166160"
  },
  {
    "text": "lead to a huge improvement in the speed of the algorithm and the time it takes to converge number two the accuracy of your",
    "start": "3166160",
    "end": "3174020"
  },
  {
    "text": "algorithm also could improve the reason I say this is this when you use five words to represent the",
    "start": "3174020",
    "end": "3182090"
  },
  {
    "text": "same thing like go going went or will go these things right so when you convert",
    "start": "3182090",
    "end": "3188270"
  },
  {
    "text": "this and make into the same word and you call this the five times of the same word rather than one time of each word",
    "start": "3188270",
    "end": "3195220"
  },
  {
    "text": "the algorithm is now able to understand that this word occurs five times and is",
    "start": "3195220",
    "end": "3201050"
  },
  {
    "text": "able to associate all the crunch the same with root word so Lda will actually be able to give you better",
    "start": "3201050",
    "end": "3207930"
  },
  {
    "text": "topics and better topic distributions because of this pre-processing step",
    "start": "3207930",
    "end": "3213080"
  },
  {
    "text": "however this is there is a catch to this this is the business probably the most important thing to note you have to be",
    "start": "3213080",
    "end": "3219630"
  },
  {
    "text": "careful there about the way the tools that you use to make these this",
    "start": "3219630",
    "end": "3225240"
  },
  {
    "text": "transformation right it's not a broadly applicable you cannot apply other",
    "start": "3225240",
    "end": "3231180"
  },
  {
    "text": "technique the way in which you go to the root word that is used in one one industry and apply to an something else",
    "start": "3231180",
    "end": "3237810"
  },
  {
    "text": "you have to be careful about how the words are transformed how you approach the root word there are multiple",
    "start": "3237810",
    "end": "3243390"
  },
  {
    "text": "different algorithms for example the way stacy this open-source toolkit does this",
    "start": "3243390",
    "end": "3249360"
  },
  {
    "text": "versus the grade NLT cage is a much more mature toolkit for natural language does",
    "start": "3249360",
    "end": "3255420"
  },
  {
    "text": "it are different and there are lots of options in this as well so how to be careful about how you do this you want",
    "start": "3255420",
    "end": "3261120"
  },
  {
    "text": "to test it and see if you are getting the right kind of transformation of the words before you even start the LDA",
    "start": "3261120",
    "end": "3267900"
  },
  {
    "text": "algorithm training very good question that is the right right thing to think about right",
    "start": "3267900",
    "end": "3275170"
  },
  {
    "text": "approach to to actually using LBA effectively",
    "start": "3275170",
    "end": "3281798"
  },
  {
    "text": "yeah so one more sort of a recap question for top could you could you",
    "start": "3285520",
    "end": "3291290"
  },
  {
    "text": "quickly run down the list again of some of the pre-processing tools you have mentioned oh you've mentioned n LT k",
    "start": "3291290",
    "end": "3297290"
  },
  {
    "text": "you'd mentioned Spacey so maybe if you could just mention those one more time for folks to make note of those yes I'm",
    "start": "3297290",
    "end": "3305720"
  },
  {
    "text": "going to create a new slide here so that people can even better yes then we'll",
    "start": "3305720",
    "end": "3313460"
  },
  {
    "text": "distribute these this slide where after the webinar so no need to know need to grab a pen and paper for tops gotcha",
    "start": "3313460",
    "end": "3320480"
  },
  {
    "text": "Covered all right so these are the two words I think it's Spacey",
    "start": "3320480",
    "end": "3326570"
  },
  {
    "text": "I'm not a hundred percent sure about the dispelling but if you search for Spacey",
    "start": "3326570",
    "end": "3333260"
  },
  {
    "text": "in natural language I should be able to get it so you tell a tool popular",
    "start": "3333260",
    "end": "3338320"
  },
  {
    "text": "toolkits ml TK actually is NLP crooked",
    "start": "3338320",
    "end": "3344960"
  },
  {
    "text": "but a natural language toolkit is what it stands for and Spacey there is a very",
    "start": "3344960",
    "end": "3352460"
  },
  {
    "text": "popular YouTube video on spacey that describes how Spacey works and how space",
    "start": "3352460",
    "end": "3357530"
  },
  {
    "text": "is different the parser in space is different from LT J and the transformations and limitation can be",
    "start": "3357530",
    "end": "3363770"
  },
  {
    "text": "done differently so I would recommend that you listen to those and and refer to those before you choose your your",
    "start": "3363770",
    "end": "3370550"
  },
  {
    "text": "toolkit it's not going to take you a lot of time these are toolkit you just like install this and start using it on your",
    "start": "3370550",
    "end": "3376460"
  },
  {
    "text": "documents these toolkit usually come with its own data sets for sample directed so you can even try that on",
    "start": "3376460",
    "end": "3382880"
  },
  {
    "text": "that and new lots of different things very easy PU stools",
    "start": "3382880",
    "end": "3389880"
  },
  {
    "start": "3390000",
    "end": "3443000"
  },
  {
    "text": "okay great thank you very much for top excellent presentation as always I'm gonna I'm gonna hand it back over to",
    "start": "3391559",
    "end": "3396719"
  },
  {
    "text": "Sue's she can close this out hey guys that was awesome I learned a lot and appreciate it and hopefully everybody",
    "start": "3396719",
    "end": "3403289"
  },
  {
    "text": "else did too you're gonna get an email follow-up from this call it will have",
    "start": "3403289",
    "end": "3409229"
  },
  {
    "text": "the recording so if you want to go back and just kind of look through anything that / top talked about get a little bit",
    "start": "3409229",
    "end": "3415109"
  },
  {
    "text": "more information you are welcome to do that also feel free to share that with your colleagues you will also get a link",
    "start": "3415109",
    "end": "3420390"
  },
  {
    "text": "to our next webinar which off the top of my head is early November but I can't",
    "start": "3420390",
    "end": "3425910"
  },
  {
    "text": "think of the date at the moment so hopefully you are gonna sign up for the next couple of webinars that we're",
    "start": "3425910",
    "end": "3431249"
  },
  {
    "text": "running one in November one into some December and then we we go right into January so thanks everybody hope you",
    "start": "3431249",
    "end": "3437789"
  },
  {
    "text": "have a great rest of your day and we'll see you in a couple weeks",
    "start": "3437789",
    "end": "3442699"
  }
]