[
  {
    "start": "0",
    "end": "213000"
  },
  {
    "text": "hello and welcome to today's webinar McGraw Hill education optimizes",
    "start": "30",
    "end": "5819"
  },
  {
    "text": "analytics workloads with data brakes my name is Pratap ramamurti partner",
    "start": "5819",
    "end": "13410"
  },
  {
    "text": "Solutions Architect for AWS and I will be your host and moderator for today's",
    "start": "13410",
    "end": "21180"
  },
  {
    "text": "presentation when you joined today's webinar you selected to join by either",
    "start": "21180",
    "end": "28890"
  },
  {
    "text": "phone call or your computer audio if for any reason you would like to change your",
    "start": "28890",
    "end": "36090"
  },
  {
    "text": "audio selection you can do so by accessing your audio pane in the control",
    "start": "36090",
    "end": "41790"
  },
  {
    "text": "panel from this control panel you will also have the opportunity",
    "start": "41790",
    "end": "49620"
  },
  {
    "text": "to submit questions to today's presenters by typing your questions into the questions panel we will collect the",
    "start": "49620",
    "end": "58410"
  },
  {
    "text": "questions and address as many as we can during the Q&A session at the end of",
    "start": "58410",
    "end": "64350"
  },
  {
    "text": "today's presentation also at the end of today's event is a brief survey",
    "start": "64350",
    "end": "70010"
  },
  {
    "text": "please stay connected until the end of the broadcast and submit your feedback",
    "start": "70010",
    "end": "76050"
  },
  {
    "text": "as your opinions count lastly the PowerPoint presentation will",
    "start": "76050",
    "end": "82500"
  },
  {
    "text": "be available through SlideShare along with the recording of the webinar on YouTube by an email that will be sent",
    "start": "82500",
    "end": "90330"
  },
  {
    "text": "two to three days after the conclusion of this event so keep an eye out for the",
    "start": "90330",
    "end": "96810"
  },
  {
    "text": "follow-up email sent to add the address that you provided",
    "start": "96810",
    "end": "102560"
  },
  {
    "text": "all right okay welcome to today's webinar mcgraw-hill education optimizes analytics workloads with data brakes my",
    "start": "102940",
    "end": "110900"
  },
  {
    "text": "name is Pratap ramamurti I'm a partner Solutions Architect for Amazon Web Services I will be your host and",
    "start": "110900",
    "end": "118790"
  },
  {
    "text": "moderator for today's webinar in addition to learning about AWS we will",
    "start": "118790",
    "end": "125960"
  },
  {
    "text": "also hear from brian Durkin senior director of partner marketing at data bricks and Matthew",
    "start": "125960",
    "end": "134150"
  },
  {
    "text": "Osborne lead software engineer at mcgraw-hill education all right today's agenda is I",
    "start": "134150",
    "end": "141980"
  },
  {
    "text": "will go over AWS and AWS data Lake Services then we will have discuss we",
    "start": "141980",
    "end": "148940"
  },
  {
    "text": "will hear about the dynamic solution that extend the data Lake management capabilities then we'll hear from",
    "start": "148940",
    "end": "156200"
  },
  {
    "text": "mcgraw-hill education recognizing the need to transform and how they did it",
    "start": "156200",
    "end": "163190"
  },
  {
    "text": "with AWS and data breaks finally we will have some time for Q&A",
    "start": "163190",
    "end": "169570"
  },
  {
    "text": "today you will learn how data leaks using a unified analytic platform can",
    "start": "171140",
    "end": "177780"
  },
  {
    "text": "enable advanced analytic use cases such as machine learning how to optimize data",
    "start": "177780",
    "end": "183840"
  },
  {
    "text": "Lakes to work effectively with real-time and fast-moving data how to streamline",
    "start": "183840",
    "end": "190680"
  },
  {
    "text": "the read/write process for data lakes and remember please post your questions",
    "start": "190680",
    "end": "197370"
  },
  {
    "text": "in the chat box throughout this presentation as we will review the questions at the end of today's event",
    "start": "197370",
    "end": "205700"
  },
  {
    "text": "let us begin the daedalic solution our and AWS",
    "start": "205810",
    "end": "214050"
  },
  {
    "start": "213000",
    "end": "213000"
  },
  {
    "text": "all right so in traditionally",
    "start": "214050",
    "end": "219140"
  },
  {
    "text": "enterprises save their data store the data in large legacy data warehouses are",
    "start": "219140",
    "end": "225240"
  },
  {
    "text": "in our DBMS Azure relational databases there are several problems that we have",
    "start": "225240",
    "end": "231960"
  },
  {
    "text": "seen in these that is very easily very complex to set up and even more complex",
    "start": "231960",
    "end": "237690"
  },
  {
    "text": "to manage they do not really scale in any way that is you have to pre",
    "start": "237690",
    "end": "243540"
  },
  {
    "text": "provision the amount of resources ahead of time that means your planning has to",
    "start": "243540",
    "end": "250530"
  },
  {
    "text": "be like hyper accurate it could take months to add new data sources and",
    "start": "250530",
    "end": "260239"
  },
  {
    "text": "queries can take too long of especially if you have multiple users running very",
    "start": "260239",
    "end": "265980"
  },
  {
    "text": "complex queries and you pay upfront for the infrastructure that you're going to",
    "start": "265980",
    "end": "273930"
  },
  {
    "text": "use there is no pay-as-you-go model in these traditional data warehouses",
    "start": "273930",
    "end": "281190"
  },
  {
    "text": "right so I'm sorry so and and these are when we move from that to the new",
    "start": "281190",
    "end": "289200"
  },
  {
    "text": "generation of Daleks there is a not just the infrastructure piece there is also a",
    "start": "289200",
    "end": "295020"
  },
  {
    "text": "different way of thinking of the way of collecting data way of what kind of data",
    "start": "295020",
    "end": "300180"
  },
  {
    "text": "should I really collect previously in in the legacy data warehouses you have to",
    "start": "300180",
    "end": "305550"
  },
  {
    "text": "really plan on like what is the kind of data that you want to collect how are you what is it go schema you're going to",
    "start": "305550",
    "end": "310620"
  },
  {
    "text": "apply on this data and what exactly are you going to store whereas in a data",
    "start": "310620",
    "end": "315690"
  },
  {
    "text": "Lake the concept is to not decide ahead of time what is the schema what kind of",
    "start": "315690",
    "end": "323280"
  },
  {
    "text": "variable store just store anything that you get let's say if you're running a marketing campaign you want to store as",
    "start": "323280",
    "end": "330000"
  },
  {
    "text": "much as data as possible in your data leak and hopefully one day somebody",
    "start": "330000",
    "end": "335190"
  },
  {
    "text": "might find a treasure trove in the data that you collected so throwing everything into into the data leak and",
    "start": "335190",
    "end": "342540"
  },
  {
    "text": "seeing if there's somebody might be able to win the lottery from all the tickets",
    "start": "342540",
    "end": "348240"
  },
  {
    "text": "that you bought so that is that is the general a change in the in debate we",
    "start": "348240",
    "end": "353520"
  },
  {
    "text": "should think about the collecting data right so we're thinking about how to",
    "start": "353520",
    "end": "358980"
  },
  {
    "start": "355000",
    "end": "355000"
  },
  {
    "text": "become a data-driven business like think about business outcomes like when I'm",
    "start": "358980",
    "end": "364560"
  },
  {
    "text": "thinking about a better leg now I first want to start thinking like what is the business outcome or insight you're",
    "start": "364560",
    "end": "369750"
  },
  {
    "text": "trying to derive from this effort right and then work backwards to a streamlined",
    "start": "369750",
    "end": "375810"
  },
  {
    "text": "design another another important thing is to since we do not have an upfront",
    "start": "375810",
    "end": "381120"
  },
  {
    "text": "fee now you can know how I run multiple different experiments on a smaller scale",
    "start": "381120",
    "end": "389010"
  },
  {
    "text": "and see which of these experiments really take off and once you realize",
    "start": "389010",
    "end": "394080"
  },
  {
    "text": "that you've identified the successful experiments versus the ones that did not",
    "start": "394080",
    "end": "399300"
  },
  {
    "text": "panel or you can kill the ones that did not run well and then you can scale the experiments that really worked right and",
    "start": "399300",
    "end": "406910"
  },
  {
    "text": "this flexibility is is very important for being agile right",
    "start": "406910",
    "end": "415110"
  },
  {
    "text": "and and this this runs very well with the general theme of how things run on",
    "start": "415110",
    "end": "421050"
  },
  {
    "text": "AWS that is where you pay for what you use and you can deploy data processing",
    "start": "421050",
    "end": "426120"
  },
  {
    "text": "infrastructure in within minutes and not like months and you can you also get to",
    "start": "426120",
    "end": "432030"
  },
  {
    "text": "choose a wide variety of services that that can cater to your needs right",
    "start": "432030",
    "end": "438210"
  },
  {
    "text": "that's definitely the main three things that help you understand thinking of",
    "start": "438210",
    "end": "443310"
  },
  {
    "start": "442000",
    "end": "442000"
  },
  {
    "text": "like what is how do I start right so here's here's some more a dive a little",
    "start": "443310",
    "end": "449790"
  },
  {
    "text": "deeper into like how you think and about building it data readily right so this",
    "start": "449790",
    "end": "455160"
  },
  {
    "text": "is what you're looking at here is a typical pipeline where there's data then you interested you store it process it",
    "start": "455160",
    "end": "461490"
  },
  {
    "text": "fine-tune it and so on and then finally consume it in the form of a report or a dashboard right so but you do not start",
    "start": "461490",
    "end": "468480"
  },
  {
    "text": "with this you first start with a business case that is like what is the business outcome of this whole thing",
    "start": "468480",
    "end": "474330"
  },
  {
    "text": "right if you are running a marketing campaign you probably want to see hey I want to measure the success criteria of",
    "start": "474330",
    "end": "482700"
  },
  {
    "text": "a marketing campaign say and then from that you're going to start saying hey",
    "start": "482700",
    "end": "488250"
  },
  {
    "text": "what is the kind of data that then you will look at like okay what kind of data",
    "start": "488250",
    "end": "493680"
  },
  {
    "text": "would I need to achieve that business goal this could be excuse me this could",
    "start": "493680",
    "end": "501930"
  },
  {
    "text": "be a metrics and monitoring data could be workflow logs we ERP transactions it",
    "start": "501930",
    "end": "508620"
  },
  {
    "text": "could be point and point of sale transactions it could be a results from",
    "start": "508620",
    "end": "513719"
  },
  {
    "text": "marketing campaigns survey results and whatnot it could be from social media feeds etc now one now that you have that",
    "start": "513720",
    "end": "520830"
  },
  {
    "text": "now you can design your pipeline and make sure that the final outcome",
    "start": "520830",
    "end": "526020"
  },
  {
    "text": "actually may gives you the insights that you are looking for",
    "start": "526020",
    "end": "531290"
  },
  {
    "start": "531000",
    "end": "531000"
  },
  {
    "text": "and what are the kinds of outcomes that you can get from such a pipeline rightly",
    "start": "531930",
    "end": "539880"
  },
  {
    "text": "you can ask me like okay fine we have this data pipeline we can do this but",
    "start": "539880",
    "end": "544990"
  },
  {
    "text": "how exactly is it going to help my business so here are some ideas right the first thing is or time you might",
    "start": "544990",
    "end": "553600"
  },
  {
    "text": "have had several different legacy of data warehouses and other technologies",
    "start": "553600",
    "end": "559510"
  },
  {
    "text": "both over time like or maybe even decades so what you can first think that this helps you do is to consolidate and",
    "start": "559510",
    "end": "566680"
  },
  {
    "text": "modernize all of your data warehousing data stores into one large dynamic",
    "start": "566680",
    "end": "573600"
  },
  {
    "text": "the second thing is now that we have consolidated this now we can think about",
    "start": "574120",
    "end": "580200"
  },
  {
    "text": "new revenues like maybe you can think about personalization of your web",
    "start": "580200",
    "end": "586270"
  },
  {
    "text": "application then we can think about demand forecasting then we can think about new ways of using this data to",
    "start": "586270",
    "end": "592810"
  },
  {
    "text": "increase your revenue right the third thing is you can think about ways to",
    "start": "592810",
    "end": "598660"
  },
  {
    "text": "engage with your customers in real time right this could either be a interactive",
    "start": "598660",
    "end": "605320"
  },
  {
    "text": "customer experience maybe using a chat bot like Lix are you can have a dynamic",
    "start": "605320",
    "end": "611560"
  },
  {
    "text": "real-time fraud detection system that could alert your customers that hey we",
    "start": "611560",
    "end": "618250"
  },
  {
    "text": "identified a front-end translation please change this happened this really happen to me last week right and the",
    "start": "618250",
    "end": "624010"
  },
  {
    "text": "last thing is you have let you we may have existing business you may have a",
    "start": "624010",
    "end": "629790"
  },
  {
    "text": "process you can use automation to improve the the effectiveness or",
    "start": "629790",
    "end": "637210"
  },
  {
    "text": "efficiency of your existing business process so these are like examples of",
    "start": "637210",
    "end": "643060"
  },
  {
    "text": "things that can you can do for your business using once you have this capability that of data leaks okay oops",
    "start": "643060",
    "end": "653240"
  },
  {
    "text": "excuse me and when we talk dental eggs",
    "start": "653240",
    "end": "658700"
  },
  {
    "start": "655000",
    "end": "655000"
  },
  {
    "text": "on enemies let's look at like waters and data lake on AWS AWS the centerpiece in",
    "start": "658700",
    "end": "665660"
  },
  {
    "text": "this if you look at the diagram on the left side it's not an architecture it's more of a representative diagram of a",
    "start": "665660",
    "end": "672080"
  },
  {
    "text": "data Lake you see s3 being the center part of this and this is really",
    "start": "672080",
    "end": "678050"
  },
  {
    "text": "important because we believe that s3 is a is a object storage a simple storage",
    "start": "678050",
    "end": "686420"
  },
  {
    "text": "service on AWS that helps you store data you have a key and you have a value",
    "start": "686420",
    "end": "691640"
  },
  {
    "text": "pairs now why is this essential we believe in AWS that separating the",
    "start": "691640",
    "end": "698000"
  },
  {
    "text": "storage from the compute is the key to scaling your daedalic solution and SC is",
    "start": "698000",
    "end": "705980"
  },
  {
    "text": "going to use an ideal a storage for this and we're going to be discussing a few more and again on the top you can see",
    "start": "705980",
    "end": "712430"
  },
  {
    "text": "that you have all these other tools that our AWS to help you query or analyze the",
    "start": "712430",
    "end": "721460"
  },
  {
    "text": "data that is on s3 for example you could have a relational database like redshift",
    "start": "721460",
    "end": "727460"
  },
  {
    "text": "you can use EMR elastic MapReduce or you can use a tea now you can directly query",
    "start": "727460",
    "end": "733280"
  },
  {
    "text": "a sequel a query on data on s3 you could use Kinesis streams to stream their",
    "start": "733280",
    "end": "741010"
  },
  {
    "text": "elastic search service you for to index and search through the data that you have in your s3 or you can use our AI",
    "start": "741010",
    "end": "748760"
  },
  {
    "text": "other AI services like sage maker to create machine learning models and deploy those models but to be able to to",
    "start": "748760",
    "end": "757430"
  },
  {
    "text": "be able to feed this data into s3 we have several different services like snowball or snowmobile if you if you are",
    "start": "757430",
    "end": "765320"
  },
  {
    "text": "familiar with these are huge devices which can hold 50 terabytes and importance of data that can be shipped",
    "start": "765320",
    "end": "771200"
  },
  {
    "text": "to AWS and this will be directly loaded into s3 or you can use other real-time upload services like Canisius video",
    "start": "771200",
    "end": "777860"
  },
  {
    "text": "streams Kinesis data firehouse or can use data streams and the the most important thing is s3",
    "start": "777860",
    "end": "785720"
  },
  {
    "text": "is is is very cheap you can you can",
    "start": "785720",
    "end": "790730"
  },
  {
    "text": "store data at like two point three cents a month and you can query data at like five cents a month five cents per GB of",
    "start": "790730",
    "end": "797540"
  },
  {
    "text": "data that is scanned all right so why is wife s 3ys 3/4 is",
    "start": "797540",
    "end": "804630"
  },
  {
    "start": "800000",
    "end": "800000"
  },
  {
    "text": "one data faster it's highly durable the first thing that comes to mind is highly durable it has 11 lines of durability",
    "start": "804630",
    "end": "812630"
  },
  {
    "text": "well that is very highly available I have 99.99% of availability high high",
    "start": "812630",
    "end": "819360"
  },
  {
    "text": "performance we can do uploads using multiple streams and you can also do",
    "start": "819360",
    "end": "825029"
  },
  {
    "text": "range gates that is you have a large file that's one terabyte size you can do a get of a part of this file like maybe",
    "start": "825029",
    "end": "833790"
  },
  {
    "text": "just like 100 MB or 100 GB of this file separately so that you don't have to",
    "start": "833790",
    "end": "839310"
  },
  {
    "text": "link download the whole file it's also easy to use you can use our restful api",
    "start": "839310",
    "end": "845130"
  },
  {
    "text": "is to access this it scalable it's almost infinitely scalable and there's",
    "start": "845130",
    "end": "850920"
  },
  {
    "text": "no minimum usage commitments and the and another last but not the least it's really well integrated with our other",
    "start": "850920",
    "end": "857610"
  },
  {
    "text": "services including EMR redshift and all the services that I described earlier",
    "start": "857610",
    "end": "863240"
  },
  {
    "text": "that makes rich h3 an ideal ideal",
    "start": "863240",
    "end": "868330"
  },
  {
    "start": "865000",
    "end": "865000"
  },
  {
    "text": "storage for your data leak but I want to hit upon this point of decoupling",
    "start": "868330",
    "end": "874070"
  },
  {
    "text": "storage and compute what do we mean by that when you have a traditional data",
    "start": "874070",
    "end": "879260"
  },
  {
    "text": "warehouse you're buying this huge piece of hardware right and you're you're gonna have even I mean this is this is",
    "start": "879260",
    "end": "885980"
  },
  {
    "text": "going to be the heart piece of hardware that stores your data as well as the hardware that has this compute now you",
    "start": "885980",
    "end": "893930"
  },
  {
    "text": "cannot let's say you have hit the limit on the storage but this compute is still",
    "start": "893930",
    "end": "899510"
  },
  {
    "text": "not getting utilized properly you have another goal but to buy another piece of",
    "start": "899510",
    "end": "906020"
  },
  {
    "text": "hardware there's like and that is going to come with your storage as well as compute so you're gonna because this is",
    "start": "906020",
    "end": "911330"
  },
  {
    "text": "always tied together at the hip right the storage and compute and we want to Depot pull these two when we decoupled",
    "start": "911330",
    "end": "918230"
  },
  {
    "text": "these two and we separate the storage from the compute what again is you can make these two scale completely",
    "start": "918230",
    "end": "924230"
  },
  {
    "text": "separately right you could have at times you could have a large of campaign and then you can have me getting large",
    "start": "924230",
    "end": "930440"
  },
  {
    "text": "amounts of data that you're collecting right now at this time like let's say for the next two months and maybe you're",
    "start": "930440",
    "end": "936530"
  },
  {
    "text": "not going to be using or running any computational intensive processing on",
    "start": "936530",
    "end": "943250"
  },
  {
    "text": "this data that's fine you can just store this data in s3 and you're just going to be paying for the storage cost and on",
    "start": "943250",
    "end": "950390"
  },
  {
    "text": "the days on the time in which you're I'm going to be actually processing it you can provision a CPU resources instances",
    "start": "950390",
    "end": "956960"
  },
  {
    "text": "to analyze this data and processes data that really helps you scale the storage",
    "start": "956960",
    "end": "962390"
  },
  {
    "text": "separately as well as the compute separately and that flexibility makes it a lot more effective",
    "start": "962390",
    "end": "970400"
  },
  {
    "text": "okay now I'm going to be passing the ball to Brian jerking senior director of",
    "start": "970400",
    "end": "977330"
  },
  {
    "text": "water marketing at data breaks thanks for table great job there you",
    "start": "977330",
    "end": "983240"
  },
  {
    "text": "know another thing make a double track a little can you hear me okay Brian oh yeah perfectly I go ahead okay very",
    "start": "983240",
    "end": "990080"
  },
  {
    "text": "good thank you let me just scroll forward a little bit here can you move",
    "start": "990080",
    "end": "1000550"
  },
  {
    "text": "to the next slide for me perfect thank",
    "start": "1000550",
    "end": "1005860"
  },
  {
    "text": "you you know everybody wants to achieve the promise of AI and machine learning",
    "start": "1005860",
    "end": "1011560"
  },
  {
    "text": "to transform their business and the hardest part is bringing the data and the analytics platform together and we",
    "start": "1011560",
    "end": "1018790"
  },
  {
    "text": "talked talked about separating out storage and compute and and I'm not trying to repudiate that because what I",
    "start": "1018790",
    "end": "1024938"
  },
  {
    "text": "really mean about bringing data and analytics together is more about getting",
    "start": "1024939",
    "end": "1031780"
  },
  {
    "text": "the data through like an ETL process and doing the steps that you need to do in order to feed it into your machine",
    "start": "1031780",
    "end": "1037780"
  },
  {
    "text": "learning or AI system and so that's what we'll talk about a little bit here we talk about the unification and then as",
    "start": "1037780",
    "end": "1044350"
  },
  {
    "text": "we talk about data legs here what we'll really talk about is using s3 as for a",
    "start": "1044350",
    "end": "1049450"
  },
  {
    "text": "tablet out and how do we optimize it in terms of the storage of the data and the",
    "start": "1049450",
    "end": "1054670"
  },
  {
    "text": "access to the data so so we'll cover all that so as I said the hardest part is",
    "start": "1054670",
    "end": "1060040"
  },
  {
    "text": "bringing the data and the analytics platform together and the data rich unified analytics platform unifies data",
    "start": "1060040",
    "end": "1066340"
  },
  {
    "text": "and AI as you'll see in Matthew's talk this unification really enables you to transform your business so a little bit",
    "start": "1066340",
    "end": "1073630"
  },
  {
    "text": "first about data brick we were founded by the original creators of Apache spark the team created data bricks on AWS",
    "start": "1073630",
    "end": "1080410"
  },
  {
    "text": "providing the optimizations that enable data ryx to run data bricks to run faster than vanilla Apache spark",
    "start": "1080410",
    "end": "1086250"
  },
  {
    "text": "benchmarking studies show it running you know up to 8x faster but we have many customers who experience even higher",
    "start": "1086250",
    "end": "1092380"
  },
  {
    "text": "performance improvements when moving from vanilla Apache spark to data bricks so when we talk about a unified",
    "start": "1092380",
    "end": "1099640"
  },
  {
    "text": "analytics platform there's three elements to this collaborative workspaces which remove",
    "start": "1099640",
    "end": "1104749"
  },
  {
    "text": "the silos that teams work in you know typically a data scientist and the data engineers and business analysts they",
    "start": "1104749",
    "end": "1111889"
  },
  {
    "text": "usually have tools that they work in and that ends up creating silos and so data bridge provides a unified analytics",
    "start": "1111889",
    "end": "1118279"
  },
  {
    "text": "platform where they can collaborate when you think about how analytics teams work you know iterating quickly and often",
    "start": "1118279",
    "end": "1124929"
  },
  {
    "text": "asynchronously 24/7 the ability to view the same workspace and see changes and",
    "start": "1124929",
    "end": "1130639"
  },
  {
    "text": "make comments directly directly means teams can iterate faster you know because because one of the things that",
    "start": "1130639",
    "end": "1136909"
  },
  {
    "text": "we always see is we start with a certain question and then you know as we work",
    "start": "1136909",
    "end": "1142340"
  },
  {
    "text": "through the question all of a sudden we have new insights and then the question kind of shifts and we have a different question and it so it's that ability to",
    "start": "1142340",
    "end": "1148909"
  },
  {
    "text": "really iterate quickly that that accelerates innovations and is really a key part of data bricks one other",
    "start": "1148909",
    "end": "1155989"
  },
  {
    "text": "element of this you know we talked a little bit about the scalability up front there but then one other element of this is the way the data bricks",
    "start": "1155989",
    "end": "1161419"
  },
  {
    "text": "removes operational complexity so the process of setting up Apache spark and tuning in",
    "start": "1161419",
    "end": "1166730"
  },
  {
    "text": "spinning clusters up and down is all automated in data bricks and with our auto scaling capabilities",
    "start": "1166730",
    "end": "1172549"
  },
  {
    "text": "data bricks can spin up and down clusters for the specific analytics jobs lowering your TCO directly but also you",
    "start": "1172549",
    "end": "1179989"
  },
  {
    "text": "know it indirectly removes these manual processes and since these processes often fall to data engineers and data",
    "start": "1179989",
    "end": "1186440"
  },
  {
    "text": "scientists this really frees them up to focus on their core value add and that really accelerates innovation so let me",
    "start": "1186440",
    "end": "1193759"
  },
  {
    "text": "move forward one slide there we go ok great that worked that time",
    "start": "1193759",
    "end": "1198820"
  },
  {
    "start": "1194000",
    "end": "1194000"
  },
  {
    "text": "so the elements of the data bricks unified analytics platform include the",
    "start": "1199340",
    "end": "1204450"
  },
  {
    "text": "collaborative workspace that we just discussed where data engineers can create ETL processes to drive the",
    "start": "1204450",
    "end": "1210360"
  },
  {
    "text": "perfect data set and data scientists can create and chain models and business users can see the results and comments",
    "start": "1210360",
    "end": "1216630"
  },
  {
    "text": "and provide feedback these notebooks provide multiple support multiple languages simultaneously so if you if",
    "start": "1216630",
    "end": "1225809"
  },
  {
    "text": "your data engineer is working in sequel or Scala but your data scientist is",
    "start": "1225809",
    "end": "1230880"
  },
  {
    "text": "working in Python or are you know that's just fine so the data bricks runtime is",
    "start": "1230880",
    "end": "1237690"
  },
  {
    "text": "the core engine that includes the optimized version of Apache spark and data review Delta is here to which will",
    "start": "1237690",
    "end": "1242970"
  },
  {
    "text": "focus on further as we explore the issues around streaming data",
    "start": "1242970",
    "end": "1247940"
  },
  {
    "text": "you so looks like I went too far sorry there you go so streaming data presents challenges of",
    "start": "1252310",
    "end": "1261100"
  },
  {
    "start": "1256000",
    "end": "1256000"
  },
  {
    "text": "its own and and this is part of what we really wanted to address today you know a streaming data comes in you need a",
    "start": "1261100",
    "end": "1268600"
  },
  {
    "text": "place to store it and you need a place to to write it in a way that's optimized for retrieval and as you access that",
    "start": "1268600",
    "end": "1276400"
  },
  {
    "text": "data if you're working at large volumes you know new data is being written at the same time and as you are accessing",
    "start": "1276400",
    "end": "1282190"
  },
  {
    "text": "it so how do you ensure you get a clean data set for analysis furthermore you know as you're writing chunks of",
    "start": "1282190",
    "end": "1288760"
  },
  {
    "text": "streaming data to files you can run into issues in terms of the number of files that you're trying to manage and the",
    "start": "1288760",
    "end": "1294940"
  },
  {
    "text": "ability to manage lots of small files and then you know you may need to to",
    "start": "1294940",
    "end": "1301630"
  },
  {
    "text": "blend this data with historical data so if you're looking at a clickstream analysis you may be comparing a website",
    "start": "1301630",
    "end": "1307960"
  },
  {
    "text": "visitors behaviors with past data or you know if you're looking at something like",
    "start": "1307960",
    "end": "1313090"
  },
  {
    "text": "intrusion detection or credit card fraud you might be comparing this to a large history of typical behavior that you",
    "start": "1313090",
    "end": "1319330"
  },
  {
    "text": "have in a you know you know written out database and so the ability to bring",
    "start": "1319330",
    "end": "1325330"
  },
  {
    "text": "together the historical data and the streaming data you know is imperative",
    "start": "1325330",
    "end": "1331029"
  },
  {
    "text": "for being able to do really good analytics and be able to help you really solve the the business case at hand",
    "start": "1331029",
    "end": "1338970"
  },
  {
    "text": "oops one more there we go so based upon",
    "start": "1344350",
    "end": "1349750"
  },
  {
    "text": "customer demand data bricks created delta and delta addresses issues around streaming data how to optimize the right",
    "start": "1349750",
    "end": "1357590"
  },
  {
    "text": "how to access the data set cleanly and the result is a hundred times faster than apache spark accessing park' so we",
    "start": "1357590",
    "end": "1365480"
  },
  {
    "text": "leverage Amazon s3 for massive scale we use open file formats for portability",
    "start": "1365480",
    "end": "1370610"
  },
  {
    "text": "and as you'll see our customers have huge amounts of data and this can really transform their business data bricks",
    "start": "1370610",
    "end": "1377900"
  },
  {
    "text": "ability to provide a unified analytics platform enables your organization to accelerate innovation by bringing your",
    "start": "1377900",
    "end": "1385610"
  },
  {
    "text": "teams together reducing infrastructure overhead and providing the highest",
    "start": "1385610",
    "end": "1391160"
  },
  {
    "text": "performance analytics engine available and so to talk more about the impact that's having I'd like to turn it over",
    "start": "1391160",
    "end": "1396560"
  },
  {
    "text": "to Matthew Ashbourne thanks Brian my",
    "start": "1396560",
    "end": "1401990"
  },
  {
    "text": "name is Matt Ashbourne whoo let's go the slide control could be pass slide",
    "start": "1401990",
    "end": "1411140"
  },
  {
    "text": "control to me perfect yeah as I said I",
    "start": "1411140",
    "end": "1417890"
  },
  {
    "text": "my name is Matt Ashbourne I'm a lead software engineer at McGraw Hill education and I lead up our streaming",
    "start": "1417890",
    "end": "1427280"
  },
  {
    "text": "data and analytics platform and so first a brief history of mcgraw-hill education",
    "start": "1427280",
    "end": "1435380"
  },
  {
    "text": "we're best known as an educational publisher but over the past few years",
    "start": "1435380",
    "end": "1441050"
  },
  {
    "text": "we've been heavily investing in our online learning platforms and from from",
    "start": "1441050",
    "end": "1446960"
  },
  {
    "text": "all these online learning platforms we capture a lot of student interaction data and we use it in both kind of",
    "start": "1446960",
    "end": "1453770"
  },
  {
    "text": "traditional reporting for teachers to know what's going on in their online",
    "start": "1453770",
    "end": "1459080"
  },
  {
    "text": "courses as well as new next generation products leveraging machine learning and",
    "start": "1459080",
    "end": "1465920"
  },
  {
    "text": "the goal here is to optimize the learning experience for students and teachers and ultimately drive student",
    "start": "1465920",
    "end": "1473450"
  },
  {
    "text": "success so first I'd like to start off with success sorts success story using data",
    "start": "1473450",
    "end": "1480409"
  },
  {
    "text": "bricks and it was kind of the first partnership at mcgraw-hill between our",
    "start": "1480409",
    "end": "1485720"
  },
  {
    "start": "1481000",
    "end": "1481000"
  },
  {
    "text": "data science teams and the data engineering teams coming together in this great collaborative data bricks",
    "start": "1485720",
    "end": "1493630"
  },
  {
    "text": "workspace to build something so what we're looking at here is a prototype",
    "start": "1493630",
    "end": "1503350"
  },
  {
    "text": "reporting application this is just hosted in what we call the learning",
    "start": "1503350",
    "end": "1508549"
  },
  {
    "text": "science dashboard it's a portal we put together to kind of expose prototypes",
    "start": "1508549",
    "end": "1515510"
  },
  {
    "text": "and learning science research to pilot customers as we're iterating on on the",
    "start": "1515510",
    "end": "1520669"
  },
  {
    "text": "data on the machine learning models and what what this one specifically was",
    "start": "1520669",
    "end": "1525980"
  },
  {
    "text": "analyzing was a student attrition risk so how likely is it is a student to drop",
    "start": "1525980",
    "end": "1532279"
  },
  {
    "text": "or withdraw from a course the prediction is powered behind the scenes by linear",
    "start": "1532279",
    "end": "1539210"
  },
  {
    "text": "regression model running on data bricks so consuming all the various data points",
    "start": "1539210",
    "end": "1545450"
  },
  {
    "text": "we collect about student behavior we're putting that through a model and what",
    "start": "1545450",
    "end": "1551390"
  },
  {
    "text": "our data scientists were able to do is be able to come up with an accurate prediction of the risk of dropping the",
    "start": "1551390",
    "end": "1558049"
  },
  {
    "text": "course within the first three weeks which is really important when you're",
    "start": "1558049",
    "end": "1563210"
  },
  {
    "text": "looking at wanting to do intervention to",
    "start": "1563210",
    "end": "1568850"
  },
  {
    "text": "keep that student in the course keep them learning keep them on track for graduation and then here we can see just",
    "start": "1568850",
    "end": "1576950"
  },
  {
    "text": "another view of the same data set but this is a more advanced one that our",
    "start": "1576950",
    "end": "1582860"
  },
  {
    "text": "data scientists put together and share with select pilot instructors usually",
    "start": "1582860",
    "end": "1588380"
  },
  {
    "text": "quant professors that really love to see the underlying data this is a very",
    "start": "1588380",
    "end": "1596149"
  },
  {
    "text": "interesting graph there how did you generate this graph that's a",
    "start": "1596149",
    "end": "1601490"
  },
  {
    "text": "good question so basically from within data bricks the data scientists are able",
    "start": "1601490",
    "end": "1607340"
  },
  {
    "text": "to do lots of exploratory analysis inside of the notebook environment in their case as Brian",
    "start": "1607340",
    "end": "1614930"
  },
  {
    "text": "mentioned supports many languages they like using Python and there's some great",
    "start": "1614930",
    "end": "1621470"
  },
  {
    "text": "visualization libraries in Python that output HTML and so from within data",
    "start": "1621470",
    "end": "1627710"
  },
  {
    "text": "bricks they essentially generate this visualization they can see it in their",
    "start": "1627710",
    "end": "1634640"
  },
  {
    "text": "notebook but then we export it just to s3 where this research portal picks it",
    "start": "1634640",
    "end": "1640580"
  },
  {
    "text": "up oh cool thanks great so now I'm going",
    "start": "1640580",
    "end": "1645590"
  },
  {
    "text": "to move on to cover some of the challenges so McGraw Hill had some existing legacy ETL pipelines so we kind",
    "start": "1645590",
    "end": "1653450"
  },
  {
    "start": "1648000",
    "end": "1648000"
  },
  {
    "text": "of already had our business use cases we knew what our data has to look like where it comes from well we had some key",
    "start": "1653450",
    "end": "1660590"
  },
  {
    "text": "challenges in three categories of data access processing and scale and so I'll",
    "start": "1660590",
    "end": "1666650"
  },
  {
    "start": "1666000",
    "end": "1666000"
  },
  {
    "text": "dive into them a bit here so under data access you can see we have various",
    "start": "1666650",
    "end": "1674350"
  },
  {
    "text": "transactional systems some with read replicas so that's there on the left and",
    "start": "1674350",
    "end": "1681560"
  },
  {
    "text": "the big challenge there is the schema binding to transactional systems so your",
    "start": "1681560",
    "end": "1688130"
  },
  {
    "text": "your extract portion of your e-tail pipeline has a direct schema binding to",
    "start": "1688130",
    "end": "1693500"
  },
  {
    "text": "that database which makes it really hard on application teams who want to move a",
    "start": "1693500",
    "end": "1699980"
  },
  {
    "text": "jali perhaps need to change their schema to support new features and that has a",
    "start": "1699980",
    "end": "1707450"
  },
  {
    "text": "cascade effect on analytics which can be problematic another piece of course is",
    "start": "1707450",
    "end": "1713470"
  },
  {
    "text": "just the cost of read replicas mcgraw-hill has a lot of various",
    "start": "1713470",
    "end": "1719480"
  },
  {
    "text": "learning systems that we have accumulated or acquired and having read replicas for all of them can start",
    "start": "1719480",
    "end": "1728050"
  },
  {
    "text": "becoming quite expensive the third area of challenge is under processing and so",
    "start": "1728050",
    "end": "1736460"
  },
  {
    "text": "starting off before data bricks we kind of had this splintered text stack where",
    "start": "1736460",
    "end": "1742340"
  },
  {
    "text": "you see we had some ETL pipelining running in a third party data",
    "start": "1742340",
    "end": "1747700"
  },
  {
    "text": "integration platform we had the data scientists doing analysis in spark but",
    "start": "1747700",
    "end": "1757390"
  },
  {
    "text": "they were actually on a bare metal cluster in the data center so they didn't have any elastic scaling didn't",
    "start": "1757390",
    "end": "1765010"
  },
  {
    "text": "have good op support they were SSH again it was hard to get access to data to be",
    "start": "1765010",
    "end": "1772030"
  },
  {
    "text": "able to process it and they were also using like pandas on their on their",
    "start": "1772030",
    "end": "1778870"
  },
  {
    "text": "local machines on their laptops kind of playing around with stale data sets and",
    "start": "1778870",
    "end": "1784590"
  },
  {
    "text": "then finally we see at the bottom we do have some data pipelines that we're",
    "start": "1784590",
    "end": "1790090"
  },
  {
    "text": "using fully managed Amazon services and",
    "start": "1790090",
    "end": "1795580"
  },
  {
    "text": "I can give those those a thumbs up they're processing wise scalability they're great you could see Kinesis",
    "start": "1795580",
    "end": "1802690"
  },
  {
    "text": "database lambda and was on elastic search in s3 and then finally though",
    "start": "1802690",
    "end": "1808470"
  },
  {
    "text": "when we're talking challenges is one of scale and so in the legacy system we had",
    "start": "1808470",
    "end": "1814780"
  },
  {
    "text": "this third party integration platform that I mentioned it wasn't fully elastic",
    "start": "1814780",
    "end": "1821940"
  },
  {
    "text": "so it meant engaging the DevOps team to",
    "start": "1821940",
    "end": "1827170"
  },
  {
    "text": "help scale it up to do performance tuning JVM tuning issues like that we",
    "start": "1827170",
    "end": "1833440"
  },
  {
    "text": "also were doing quite a lot of the transform and load of our pipelines",
    "start": "1833440",
    "end": "1840070"
  },
  {
    "text": "within postgres RDS instances and started hitting the issues there where",
    "start": "1840070",
    "end": "1848410"
  },
  {
    "text": "your storage and computer coupled and we're hitting some some bottlenecks so",
    "start": "1848410",
    "end": "1856240"
  },
  {
    "text": "just to kind of summarize all of this led to issues being able to production",
    "start": "1856240",
    "end": "1862540"
  },
  {
    "text": "eyes that put of data scientists because they were just kind of off on their own",
    "start": "1862540",
    "end": "1867600"
  },
  {
    "text": "doing some spark stuff in the data center doing some pandas on their",
    "start": "1867600",
    "end": "1872920"
  },
  {
    "text": "laptops we had schema binding - transactional databases which was impacting the bill",
    "start": "1872920",
    "end": "1878800"
  },
  {
    "text": "be for the whole organization to change quickly data engineering was kind of split across these various tech specs",
    "start": "1878800",
    "end": "1885580"
  },
  {
    "text": "and we had some scaling issues oh one more thing I didn't mention from the",
    "start": "1885580",
    "end": "1892030"
  },
  {
    "text": "previous slide this legacy system that was actually capturing data in s3 as a",
    "start": "1892030",
    "end": "1898800"
  },
  {
    "text": "historical store was suffering from the",
    "start": "1898800",
    "end": "1904360"
  },
  {
    "text": "small file issue small file challenges and I'll touch on that a bit more in the",
    "start": "1904360",
    "end": "1909880"
  },
  {
    "text": "next slides and and finally there's just a high overhead of adding new data",
    "start": "1909880",
    "end": "1915220"
  },
  {
    "text": "sources because there's a lot of kind of point-to-point opening up access for whatever JDBC connection open up to some",
    "start": "1915220",
    "end": "1923920"
  },
  {
    "text": "read replicas oh we have to drill a hole and get network routing into some data",
    "start": "1923920",
    "end": "1930160"
  },
  {
    "text": "center somewhere and so that kind of brought us along to thinking of what our",
    "start": "1930160",
    "end": "1936820"
  },
  {
    "text": "alternate solutions and obviously one of",
    "start": "1936820",
    "end": "1942250"
  },
  {
    "text": "the most popular ways to start tackling your data is the data like so I'd like",
    "start": "1942250",
    "end": "1951670"
  },
  {
    "start": "1947000",
    "end": "1947000"
  },
  {
    "text": "to just quickly dive in a bit more by what what is a data Lake so I have this nice diagram which I hope will help",
    "start": "1951670",
    "end": "1960160"
  },
  {
    "text": "anyone who isn't super familiar so of course here on the Left we have our our",
    "start": "1960160",
    "end": "1966220"
  },
  {
    "text": "lake but flowing into it we have these streams of data so to extend the",
    "start": "1966220",
    "end": "1972310"
  },
  {
    "text": "metaphor we have streamed the rivers of data flowing into the lake and that's data that's mostly in a raw form it gets",
    "start": "1972310",
    "end": "1980890"
  },
  {
    "text": "into the data lake where our data scientists and data engineers can do exploratory analysis on it usually using",
    "start": "1980890",
    "end": "1988750"
  },
  {
    "text": "some type of big data tool like Apache spark Hadoop flank etc and the data lake",
    "start": "1988750",
    "end": "1996850"
  },
  {
    "text": "itself is usually implemented as files and some on top of some kind of massive",
    "start": "1996850",
    "end": "2003120"
  },
  {
    "text": "storage layer like HDFS or of course Amazon s3 and then continuing on when",
    "start": "2003120",
    "end": "2011310"
  },
  {
    "text": "you do have okay you know your business case there's value in this data we want to process",
    "start": "2011310",
    "end": "2016559"
  },
  {
    "text": "that you then select and transform data out of the data lake for each of those",
    "start": "2016559",
    "end": "2022589"
  },
  {
    "text": "needs and then now you're getting into kind of bit more of the transformation",
    "start": "2022589",
    "end": "2030359"
  },
  {
    "text": "pipelines and you clean up the data and",
    "start": "2030359",
    "end": "2036269"
  },
  {
    "text": "then maybe load it into what I like to call are these lakeshore data Mart's Oh",
    "start": "2036269",
    "end": "2041599"
  },
  {
    "text": "Matthew can I can I interrupt you there for a second so I love your terminology",
    "start": "2041599",
    "end": "2048029"
  },
  {
    "text": "especially the term of lakeshore data Mart's I have like an intuitive idea of",
    "start": "2048029",
    "end": "2053669"
  },
  {
    "text": "what that is like can you explain what do you mean by lakeshore there mark sure",
    "start": "2053669",
    "end": "2060480"
  },
  {
    "text": "yeah so it's bringing the whole metaphor together if the late if your data lake is kind of the central technology to",
    "start": "2060480",
    "end": "2068519"
  },
  {
    "text": "your data strategy on on the shore of a lake you know you can have cottages or",
    "start": "2068519",
    "end": "2074460"
  },
  {
    "text": "you can have your lake shore data mark where you keep all your goodies like your your jet skis etc perhaps but but",
    "start": "2074460",
    "end": "2084569"
  },
  {
    "text": "mainly this is going to be a much more structured view of your data that's",
    "start": "2084569",
    "end": "2090059"
  },
  {
    "text": "useful for that business use case so think your materialized views of the",
    "start": "2090059",
    "end": "2096000"
  },
  {
    "text": "data that can power some reporting query some API that returns insights that's",
    "start": "2096000",
    "end": "2102900"
  },
  {
    "text": "then integrated right into a transactional application and so one",
    "start": "2102900",
    "end": "2110460"
  },
  {
    "text": "kind of benefit of this whole set up is it makes it really easy to prototype",
    "start": "2110460",
    "end": "2115549"
  },
  {
    "text": "your ETL pipelines because you kind of have you have your data flowing in from",
    "start": "2115549",
    "end": "2121200"
  },
  {
    "text": "other systems you can start looking at it you know before you do your sprint",
    "start": "2121200",
    "end": "2126210"
  },
  {
    "text": "planning etc and have a lot better idea of what's going on and so just to touch",
    "start": "2126210",
    "end": "2135150"
  },
  {
    "text": "on some specific requirements we had one was we wanted to investigate this as a",
    "start": "2135150",
    "end": "2144690"
  },
  {
    "text": "potential solution with a small team so we're looking for something that didn't need a massive",
    "start": "2144690",
    "end": "2150569"
  },
  {
    "text": "engineering effort to get going just due to some of our existing",
    "start": "2150569",
    "end": "2155579"
  },
  {
    "text": "real-time data feeds we want to support for concurrent writers so into the data",
    "start": "2155579",
    "end": "2160980"
  },
  {
    "text": "Lake we want to be able to have multiple different writers loading data and we",
    "start": "2160980",
    "end": "2166530"
  },
  {
    "text": "wanted something that's resilient and auto healing so small team can easily manage it and then find finally we",
    "start": "2166530",
    "end": "2174329"
  },
  {
    "text": "wanted something that can automatically compact small files to improve read",
    "start": "2174329",
    "end": "2179430"
  },
  {
    "text": "performance so streaming data comes in kind of at a high velocity and if you",
    "start": "2179430",
    "end": "2186089"
  },
  {
    "text": "just naively write it to s3 or anywhere you're gonna end up with a lot of small",
    "start": "2186089",
    "end": "2192240"
  },
  {
    "text": "files because your data is amortized over the 24 hours of the day really you",
    "start": "2192240",
    "end": "2200220"
  },
  {
    "text": "want your data in some slightly larger files so when you then go to query it to",
    "start": "2200220",
    "end": "2206460"
  },
  {
    "text": "use it you're not paying a huge overhead listing files finding what to read and",
    "start": "2206460",
    "end": "2216290"
  },
  {
    "start": "2216000",
    "end": "2216000"
  },
  {
    "text": "so that leads us to some of the issues when we just started looking at the open",
    "start": "2216290",
    "end": "2222150"
  },
  {
    "text": "source option options so out of the box you can spin up a spark cluster but you",
    "start": "2222150",
    "end": "2229950"
  },
  {
    "text": "still need to know a lot of a lot of stuff to get what you desire so you can",
    "start": "2229950",
    "end": "2237930"
  },
  {
    "text": "run some UTL pipeline and then just accidentally end up with either smoke too small or too large of output files",
    "start": "2237930",
    "end": "2245210"
  },
  {
    "text": "your jobs can fail you can and then have you have a dirty output directory that",
    "start": "2245210",
    "end": "2251010"
  },
  {
    "text": "then maybe a second phase of your pipeline is supposed to pick up and now",
    "start": "2251010",
    "end": "2257400"
  },
  {
    "text": "it's reading dirty files and complete output also there isn't a whole lot of",
    "start": "2257400",
    "end": "2266040"
  },
  {
    "text": "support for schema management there's no transactional support or safety like you would have in your",
    "start": "2266040",
    "end": "2273690"
  },
  {
    "text": "traditional relational database that a lot of developers are used to",
    "start": "2273690",
    "end": "2279490"
  },
  {
    "text": "and there's no real safe way to have multiple writers because really you're just interacting with files or objects",
    "start": "2279490",
    "end": "2288040"
  },
  {
    "text": "in s3 and you don't have kind of transactional concurrency guarantees and",
    "start": "2288040",
    "end": "2295600"
  },
  {
    "text": "that that's where data breaks Delta comes to the rescue so Brian touched on this I'll run",
    "start": "2295600",
    "end": "2301870"
  },
  {
    "start": "2296000",
    "end": "2296000"
  },
  {
    "text": "through a bit deeper so Delta gives you acid transactions just like your",
    "start": "2301870",
    "end": "2311800"
  },
  {
    "text": "traditional database except over park' files on Amazon s3 or on Azure blob",
    "start": "2311800",
    "end": "2319750"
  },
  {
    "text": "storage it also gives you delete updates up certs so your traditional kind of",
    "start": "2319750",
    "end": "2326080"
  },
  {
    "text": "sequel ETL workflows work cleanly on data breaks Delta and they can modify",
    "start": "2326080",
    "end": "2333280"
  },
  {
    "text": "the data set without interfering with any jobs that are reading that data it",
    "start": "2333280",
    "end": "2340900"
  },
  {
    "text": "gives you some data validation some schema validation and most importantly",
    "start": "2340900",
    "end": "2346930"
  },
  {
    "text": "if you remember I've touched on small file problem it gives you automatic file management",
    "start": "2346930",
    "end": "2352030"
  },
  {
    "text": "so there's a great optimized command that you can just run on your data Lake and it automatically compacts all those",
    "start": "2352030",
    "end": "2358960"
  },
  {
    "text": "small files so you can get performant reads and then finally they also compute",
    "start": "2358960",
    "end": "2364540"
  },
  {
    "text": "some extra statistics that further improve read performance and so now we",
    "start": "2364540",
    "end": "2372070"
  },
  {
    "text": "can have this more simplified architecture where under data access our",
    "start": "2372070",
    "end": "2379510"
  },
  {
    "text": "entire organization that has moved to more of a streaming model where source",
    "start": "2379510",
    "end": "2385330"
  },
  {
    "text": "applications push events into Kafka brokers and then analytics",
    "start": "2385330",
    "end": "2393370"
  },
  {
    "text": "consumes from the Kafka brokers our processing has been standardized on the",
    "start": "2393370",
    "end": "2400960"
  },
  {
    "text": "SPARC API and and Scala running in data bricks it's all cloud instances its spot",
    "start": "2400960",
    "end": "2409270"
  },
  {
    "text": "instances and it's auto scaling so we get a lot of ah savings and not much overhead",
    "start": "2409270",
    "end": "2417640"
  },
  {
    "text": "managing very large clusters that also elastically scale and do exactly what we",
    "start": "2417640",
    "end": "2426160"
  },
  {
    "text": "need them to be doing in multi stage pipelines the cluster can automatically",
    "start": "2426160",
    "end": "2431320"
  },
  {
    "text": "bring on more capacity for the second phase and it'll automatically shrink down if the further phases don't need",
    "start": "2431320",
    "end": "2439570"
  },
  {
    "text": "that many CPU cores for example and then finally when we're talking about scale",
    "start": "2439570",
    "end": "2446170"
  },
  {
    "text": "you see we still have RDS there so that's our data Mart that's where we're loading our super cleaned data ready for",
    "start": "2446170",
    "end": "2454420"
  },
  {
    "text": "reporting but now the majority of our IO",
    "start": "2454420",
    "end": "2460030"
  },
  {
    "text": "our processing we're trying to do with inspark and anywhere we have to persist",
    "start": "2460030",
    "end": "2469350"
  },
  {
    "text": "intermediary data we're trying to move that into data bricks Delta where we",
    "start": "2469800",
    "end": "2475360"
  },
  {
    "text": "have very horizontally scalable i/o with these park' files on on s3 and so just",
    "start": "2475360",
    "end": "2485320"
  },
  {
    "text": "to quickly run down we had quite a few challenges and data bricks unified",
    "start": "2485320",
    "end": "2493930"
  },
  {
    "text": "platform has really helped us on every single one of them and so now I'll dive",
    "start": "2493930",
    "end": "2501940"
  },
  {
    "text": "in a bit on the actual implementation details so number one you still need",
    "start": "2501940",
    "end": "2508960"
  },
  {
    "text": "some information architecture you want to avoid what you call the data swamp by",
    "start": "2508960",
    "end": "2515800"
  },
  {
    "text": "having some consisting names identifiers for all your entities services where the",
    "start": "2515800",
    "end": "2522040"
  },
  {
    "text": "data is coming from that's super important you need some schemas you need to know what data is in your data like",
    "start": "2522040",
    "end": "2528580"
  },
  {
    "text": "to get value out of it you likely want a fairly flexible schema for your data",
    "start": "2528580",
    "end": "2536140"
  },
  {
    "start": "2530000",
    "end": "2530000"
  },
  {
    "text": "Lake because once again you're talking about fairly raw data so I'm mcgraw-hill",
    "start": "2536140",
    "end": "2541240"
  },
  {
    "text": "the approach we took we actually keep the",
    "start": "2541240",
    "end": "2546660"
  },
  {
    "text": "wrong message as a JSON string so this is a park a schema actually so each each",
    "start": "2546730",
    "end": "2552550"
  },
  {
    "text": "one of these columns are kind of partitioned in their own and then we",
    "start": "2552550",
    "end": "2557650"
  },
  {
    "text": "have kind of what type of event is it a header with some traceability routing",
    "start": "2557650",
    "end": "2563190"
  },
  {
    "text": "and metadata a whoops sorry the body and",
    "start": "2563190",
    "end": "2572050"
  },
  {
    "text": "that's the actual event payload some information about the schema of the",
    "start": "2572050",
    "end": "2577960"
  },
  {
    "text": "event versions and then we actually physically partitioned our data Lake on",
    "start": "2577960",
    "end": "2584410"
  },
  {
    "text": "the source of the data event name or the type of the data and then event date",
    "start": "2584410",
    "end": "2590109"
  },
  {
    "text": "event date which is when it happened and this physical partitioning allows you to",
    "start": "2590109",
    "end": "2595660"
  },
  {
    "text": "drill in and have your queries when you're querying this from spark push",
    "start": "2595660",
    "end": "2602440"
  },
  {
    "text": "those predicates down and reduce IO you're doing on s3 so if you only wanted to look at data from one source for",
    "start": "2602440",
    "end": "2609430"
  },
  {
    "text": "today you can launch that query and it's only going to read the files that needs to and you get quite performant queries",
    "start": "2609430",
    "end": "2619650"
  },
  {
    "text": "despite only paying for storage on s3 and and some spot instances and here you",
    "start": "2619650",
    "end": "2627609"
  },
  {
    "start": "2626000",
    "end": "2626000"
  },
  {
    "text": "can see how you read and write batch data frames in spark to Delta and so as",
    "start": "2627609",
    "end": "2635410"
  },
  {
    "text": "Brian said they layered this on top of the open source spark so just instead of park' you say Delta and it works exactly",
    "start": "2635410",
    "end": "2643840"
  },
  {
    "text": "the same similarly with streaming read and writes you can use a Delta table as a streaming",
    "start": "2643840",
    "end": "2651880"
  },
  {
    "text": "source so any changes that are coming into your data Lake you can actually",
    "start": "2651880",
    "end": "2657790"
  },
  {
    "text": "consume that as as a change and implement kind of change data capture",
    "start": "2657790",
    "end": "2662890"
  },
  {
    "text": "patterns etc and similarly you can write a stream of data into a Delta table and",
    "start": "2662890",
    "end": "2673570"
  },
  {
    "text": "this is exactly how I implemented the data Lake so where I have",
    "start": "2673570",
    "end": "2680140"
  },
  {
    "text": "the spark jobs consuming those Kinesis dreams and those Kafka partitions it's",
    "start": "2680140",
    "end": "2686830"
  },
  {
    "text": "writing those streams right into the into the data lake another concept",
    "start": "2686830",
    "end": "2693480"
  },
  {
    "start": "2691000",
    "end": "2691000"
  },
  {
    "text": "that's pretty important when you're talking about data Lakes is kind of",
    "start": "2693480",
    "end": "2698860"
  },
  {
    "text": "deferring work until it produces value so there's there's this concept of",
    "start": "2698860",
    "end": "2705190"
  },
  {
    "text": "schema on read so as you saw when I was",
    "start": "2705190",
    "end": "2710500"
  },
  {
    "text": "sharing the the park' scheme of the data lake we had some columns there that",
    "start": "2710500",
    "end": "2715930"
  },
  {
    "text": "essentially were just JSON strings once we actually know what we want to do with",
    "start": "2715930",
    "end": "2721180"
  },
  {
    "text": "the data we can supply the schema to spark and do that JSON parsing at a",
    "start": "2721180",
    "end": "2728490"
  },
  {
    "text": "massive scale your iOS from s3 are nice",
    "start": "2728490",
    "end": "2733750"
  },
  {
    "text": "horizontally partitioned and your JSON parsing as well and then you end up with",
    "start": "2733750",
    "end": "2739480"
  },
  {
    "text": "a data frame with just the data you're looking for as opposed to try and do all that parsing and breaking out data into",
    "start": "2739480",
    "end": "2745750"
  },
  {
    "text": "columns before you do the write which can be much more computationally",
    "start": "2745750",
    "end": "2751270"
  },
  {
    "text": "expensive and difficult to keep up with high velocity data",
    "start": "2751270",
    "end": "2756900"
  },
  {
    "text": "oops trying to advance slide here could be advance the slide Thanks and you can",
    "start": "2758610",
    "end": "2768670"
  },
  {
    "text": "also do the exact same thing through views I just wanted to share this so",
    "start": "2768670",
    "end": "2773680"
  },
  {
    "text": "this is a view over a Delta table so it could be over your data Lake etc and you",
    "start": "2773680",
    "end": "2779530"
  },
  {
    "text": "see when you query this view it's actually doing some JSON parsing right there similarly as we were doing before",
    "start": "2779530",
    "end": "2787090"
  },
  {
    "text": "with the spark API and so to bring it all together I have a quick case study",
    "start": "2787090",
    "end": "2795580"
  },
  {
    "start": "2793000",
    "end": "2793000"
  },
  {
    "text": "we had a existing detail pipeline that was running and in spark but had this",
    "start": "2795580",
    "end": "2802690"
  },
  {
    "text": "intermediary staging step where data was",
    "start": "2802690",
    "end": "2807790"
  },
  {
    "text": "written to post grosse RDS and then read right back out for the next phase",
    "start": "2807790",
    "end": "2814390"
  },
  {
    "text": "and what we did is we broke it down and kind of use some of these new patterns",
    "start": "2814390",
    "end": "2821709"
  },
  {
    "text": "I've been talking about so my team had the existing spark structured streaming",
    "start": "2821709",
    "end": "2829029"
  },
  {
    "text": "and Delta data Lake already built the source team in this case was rostering",
    "start": "2829029",
    "end": "2837099"
  },
  {
    "text": "so a student being added to a course they started whoops we have some windows",
    "start": "2837099",
    "end": "2843759"
  },
  {
    "text": "start many issues sorry about that they started publishing messages to",
    "start": "2843759",
    "end": "2849759"
  },
  {
    "text": "Kafka and they just automatically got discovered and started being written to",
    "start": "2849759",
    "end": "2855759"
  },
  {
    "text": "the data lake when the data engineers were ready to take on this new",
    "start": "2855759",
    "end": "2861900"
  },
  {
    "text": "refactoring work they already had example date data sitting there in the",
    "start": "2861900",
    "end": "2867759"
  },
  {
    "text": "data lake in both non prod and production and they were kind of able to prototype a new pipeline that that reads",
    "start": "2867759",
    "end": "2875559"
  },
  {
    "text": "out of data breaks Delta does all the transformations in memory and then",
    "start": "2875559",
    "end": "2881019"
  },
  {
    "text": "directly loads the data mark pretty quickly and it worked out really well",
    "start": "2881019",
    "end": "2887259"
  },
  {
    "start": "2887000",
    "end": "2887000"
  },
  {
    "text": "and so the the real deal is there's quite a lot of transactional systems all",
    "start": "2887259",
    "end": "2893079"
  },
  {
    "text": "publishing now into what we call the data backbone behind the scenes it's",
    "start": "2893079",
    "end": "2898359"
  },
  {
    "text": "copy our Kinesis which then we use SPARC structured streaming to stream into a",
    "start": "2898359",
    "end": "2904449"
  },
  {
    "text": "data Lake which is using data bricks Delta which is really just optimized",
    "start": "2904449",
    "end": "2912160"
  },
  {
    "text": "park' files on s3 and then we have further SPARC ETL jobs that read from",
    "start": "2912160",
    "end": "2919660"
  },
  {
    "text": "the data Lake and into a data Mart where then we have kind of API serving layers",
    "start": "2919660",
    "end": "2925229"
  },
  {
    "text": "getting those insights over to students and instructors just want to wrap up",
    "start": "2925229",
    "end": "2932769"
  },
  {
    "start": "2930000",
    "end": "2930000"
  },
  {
    "text": "with some possible pitfalls you definitely need a strategy upfront I've",
    "start": "2932769",
    "end": "2940959"
  },
  {
    "text": "been kind of talking a mile a minute to shrine share",
    "start": "2940959",
    "end": "2946230"
  },
  {
    "text": "as much as I could about this but you can see there's a lot of complexity and",
    "start": "2946230",
    "end": "2951930"
  },
  {
    "text": "likely new ideas if your team hasn't built systems like this before there's",
    "start": "2951930",
    "end": "2958080"
  },
  {
    "text": "some organizational change required when you're talking about moving to eventing",
    "start": "2958080",
    "end": "2963750"
  },
  {
    "text": "and you're gonna need obviously training on this new technology some surprises",
    "start": "2963750",
    "end": "2973230"
  },
  {
    "start": "2970000",
    "end": "2970000"
  },
  {
    "text": "the actual data pipeline development isn't faster but we have a lot more",
    "start": "2973230",
    "end": "2978480"
  },
  {
    "text": "options and flexibility the ETLs themselves the runtimes have gotten",
    "start": "2978480",
    "end": "2988350"
  },
  {
    "text": "shorter and we have a lot more options to scale it and with Delta we're able to",
    "start": "2988350",
    "end": "2995460"
  },
  {
    "text": "easily combine streaming and batch data which was difficult before and really",
    "start": "2995460",
    "end": "3001580"
  },
  {
    "text": "the takeaway is having a unified platform where you can deal with all of this and incorporate data science work",
    "start": "3001580",
    "end": "3009910"
  },
  {
    "text": "is really important oops I get next",
    "start": "3009910",
    "end": "3018110"
  },
  {
    "text": "slide there we go okay I think we have some time for Q&A Oh Thank You Matthew",
    "start": "3018110",
    "end": "3025340"
  },
  {
    "text": "that was wonderful I have a few questions from the audience",
    "start": "3025340",
    "end": "3032000"
  },
  {
    "text": "first question I have for data breaks okay is Delta a new offering and a",
    "start": "3032000",
    "end": "3038780"
  },
  {
    "text": "follow-up question on that is Delta generally available today yeah great",
    "start": "3038780",
    "end": "3045380"
  },
  {
    "text": "question so Delta is a new offering it's in preview at the moment and you can get more information at the data bricks comm",
    "start": "3045380",
    "end": "3051890"
  },
  {
    "text": "website you can also find out some more about some of our various integrations",
    "start": "3051890",
    "end": "3058130"
  },
  {
    "text": "with AWS if you go to data bricks comm slash AWS oh cool thanks um all right so",
    "start": "3058130",
    "end": "3067100"
  },
  {
    "text": "another question is is there a github repo from which you can get the code",
    "start": "3067100",
    "end": "3073460"
  },
  {
    "text": "samples the slides will be shared and the recording will be in YouTube we do not have any code samples very much",
    "start": "3073460",
    "end": "3081950"
  },
  {
    "text": "here so so that that is not applicable here alright question for Matthew how",
    "start": "3081950",
    "end": "3091430"
  },
  {
    "text": "long did it take for you to transform from your old architecture to the new it abuse database architecture a good",
    "start": "3091430",
    "end": "3100130"
  },
  {
    "text": "question so we're the the short answer is we're not done yet it is a lot of",
    "start": "3100130",
    "end": "3106010"
  },
  {
    "text": "work and we're tackling it incrementally so we've had a lot of the foundational",
    "start": "3106010",
    "end": "3111920"
  },
  {
    "text": "pieces like the data backbone Kinesis dreams the data lake up and running in",
    "start": "3111920",
    "end": "3119960"
  },
  {
    "text": "production for quite a few months now and the huge push of teams to instrument",
    "start": "3119960",
    "end": "3127820"
  },
  {
    "text": "events has also been happening in parallel to that and we're nearing kind",
    "start": "3127820",
    "end": "3134750"
  },
  {
    "text": "of a hundred percent of systems sending events but it's definitely something",
    "start": "3134750",
    "end": "3141190"
  },
  {
    "text": "that you tackle incrementally and once",
    "start": "3141190",
    "end": "3147320"
  },
  {
    "text": "you have the foundational pieces to somehow stream data and get it into the",
    "start": "3147320",
    "end": "3153530"
  },
  {
    "text": "data lake bringing new systems online and following these patterns is is",
    "start": "3153530",
    "end": "3159619"
  },
  {
    "text": "fairly simple okay",
    "start": "3159619",
    "end": "3168109"
  },
  {
    "text": "thanks for that all right another question for Brian from data breaks does",
    "start": "3168109",
    "end": "3178220"
  },
  {
    "text": "data breaks use the latest version of spark oh that's a great question because we",
    "start": "3178220",
    "end": "3185300"
  },
  {
    "text": "contribute over 70% of the work on spark we are always up to date on the latest",
    "start": "3185300",
    "end": "3190880"
  },
  {
    "text": "version and actually when you go and and run data bricks on on AWS you can choose",
    "start": "3190880",
    "end": "3197750"
  },
  {
    "text": "different versions of spark so you can actually go backwards to previous versions but we always have the latest",
    "start": "3197750",
    "end": "3203630"
  },
  {
    "start": "3202000",
    "end": "3202000"
  },
  {
    "text": "version up and available since you know we're so involved in in getting that up",
    "start": "3203630",
    "end": "3209660"
  },
  {
    "text": "and running and the whole contribution process a great question you know and",
    "start": "3209660",
    "end": "3214880"
  },
  {
    "text": "not not just the most recent version data breaks makes bit beta releases",
    "start": "3214880",
    "end": "3220880"
  },
  {
    "text": "available so obviously we had a question about Delta mcgraw-hill has actually",
    "start": "3220880",
    "end": "3226670"
  },
  {
    "text": "been using Delta in production since it was out alpha release and private",
    "start": "3226670",
    "end": "3234170"
  },
  {
    "text": "preview and it's it's been very stable and so it's been great kind of",
    "start": "3234170",
    "end": "3239210"
  },
  {
    "text": "partnering with data bricks so they get you access to the goodies quickly but",
    "start": "3239210",
    "end": "3245300"
  },
  {
    "text": "without the headache of being on the bleeding edge yeah and I should also",
    "start": "3245300",
    "end": "3251420"
  },
  {
    "text": "point out as I mentioned that Delta's in preview if you go to data bricks comm / Delta you can get right to the page",
    "start": "3251420",
    "end": "3257510"
  },
  {
    "text": "where you can sign up and take a look and learn more cool awesome a follow-up",
    "start": "3257510",
    "end": "3264109"
  },
  {
    "text": "question for our Matthew from Twitter on the to the previous question like what",
    "start": "3264109",
    "end": "3269480"
  },
  {
    "text": "was the cost difference should be in the previous architecture the new one",
    "start": "3269480",
    "end": "3274809"
  },
  {
    "text": "so cut loss wise yeah so cos wise you",
    "start": "3275990",
    "end": "3283500"
  },
  {
    "text": "can achieve some savings but really a lot of the value is enabling a more",
    "start": "3283500",
    "end": "3293430"
  },
  {
    "text": "agile approach to your data practice so like I touched on how you had that",
    "start": "3293430",
    "end": "3299130"
  },
  {
    "text": "schema binding like sure you're paying for read replicas but having that schema",
    "start": "3299130",
    "end": "3306539"
  },
  {
    "text": "binding just slows down your whole organization not just analytics or you",
    "start": "3306539",
    "end": "3311849"
  },
  {
    "text": "have your transactional teams moving fast and then breaking analytics which",
    "start": "3311849",
    "end": "3317130"
  },
  {
    "text": "isn't great either but for a direct cost comparison I can",
    "start": "3317130",
    "end": "3322829"
  },
  {
    "text": "talk a little bit and just throw out some numbers so basically in our non",
    "start": "3322829",
    "end": "3329369"
  },
  {
    "text": "prod environments the we have two data",
    "start": "3329369",
    "end": "3334589"
  },
  {
    "text": "Mart's and so they're post-post gross RDS and I think they probably both have",
    "start": "3334589",
    "end": "3340650"
  },
  {
    "text": "300 gigs of disk so the the billing for those is equal to my spark clusters and",
    "start": "3340650",
    "end": "3352309"
  },
  {
    "text": "s3 billing but the difference is those data Mart's only have like what 48 cores",
    "start": "3352309",
    "end": "3359519"
  },
  {
    "text": "between the two of them and 600 gigabytes of storage on on s3 I have",
    "start": "3359519",
    "end": "3366210"
  },
  {
    "text": "like 8 terabytes of data and I can spin up a cluster with hundreds of cores",
    "start": "3366210",
    "end": "3372470"
  },
  {
    "text": "within minutes and fully bootstrap with spark run queries and then and then shut",
    "start": "3372470",
    "end": "3377849"
  },
  {
    "text": "it down hope that helps I think so all",
    "start": "3377849",
    "end": "3387690"
  },
  {
    "text": "right another question for Matthew how do you handle duplicate data",
    "start": "3387690",
    "end": "3393590"
  },
  {
    "text": "and how do you maintain their integrity also talked about like compression of the files in storage to reduce cost sure",
    "start": "3393590",
    "end": "3403070"
  },
  {
    "text": "a lot a lot of stuff packed in there so first the my complaint equation you",
    "start": "3403070",
    "end": "3409160"
  },
  {
    "text": "do one yeah so for file compression data bricks kind of has that somewhat tuned",
    "start": "3409160",
    "end": "3415790"
  },
  {
    "text": "out of the box so Delta is on top of a parquet which itself is kind of a highly",
    "start": "3415790",
    "end": "3421850"
  },
  {
    "text": "optimized file format and I believe we're running snappy compression we",
    "start": "3421850",
    "end": "3428090"
  },
  {
    "text": "haven't had to tune that a whole lot basically the the default settings we've",
    "start": "3428090",
    "end": "3435500"
  },
  {
    "text": "been getting from data bricks have got us up and running and seeing very satisfactory performance over the box as",
    "start": "3435500",
    "end": "3442850"
  },
  {
    "text": "far as duplicate data and stuff like that we handle it in two ways so in some",
    "start": "3442850",
    "end": "3450290"
  },
  {
    "text": "workflows we still depend on a relational database that has constraints",
    "start": "3450290",
    "end": "3457090"
  },
  {
    "text": "when you're loading the final kind of materialized view and doing a two-phase",
    "start": "3457090",
    "end": "3465460"
  },
  {
    "text": "what first phase do all your inserts of new data and second phase do do updates",
    "start": "3465460",
    "end": "3471430"
  },
  {
    "text": "the the other approach which is the massively scalable one is just using",
    "start": "3471430",
    "end": "3477590"
  },
  {
    "text": "spark 2d doop records so definitely in in the data Lake using streaming data",
    "start": "3477590",
    "end": "3484100"
  },
  {
    "text": "technologies you don't you don't really have an exactly once and only once",
    "start": "3484100",
    "end": "3489400"
  },
  {
    "text": "guarantee you might have the same event more than once and that's where having",
    "start": "3489400",
    "end": "3496070"
  },
  {
    "text": "proper identifier schemes isn't important so all of our events that are",
    "start": "3496070",
    "end": "3502580"
  },
  {
    "text": "sent are minted with a gooood on them and so in in spark it's really easy they",
    "start": "3502580",
    "end": "3511130"
  },
  {
    "text": "actually even have a little helper on a data frame that you can just say drop duplicates and you pass in that goo it",
    "start": "3511130",
    "end": "3518960"
  },
  {
    "text": "or unique identifier column name and it will just drop all the duplicate data",
    "start": "3518960",
    "end": "3526690"
  },
  {
    "text": "okay cool so thank you so we have a few more questions but we have run out of",
    "start": "3529470",
    "end": "3534480"
  },
  {
    "text": "time so uh Thank You Matthew and Bryan thank",
    "start": "3534480",
    "end": "3540960"
  },
  {
    "text": "you all for attending today's webinar please remember to stay connected and complete the brief survey at the",
    "start": "3540960",
    "end": "3547589"
  },
  {
    "text": "conclusion of this webinar we look forward to supporting you in current and future projects thank you again and have",
    "start": "3547589",
    "end": "3553170"
  },
  {
    "text": "a great day",
    "start": "3553170",
    "end": "3555859"
  }
]