[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "good afternoon everyone uh my name is john einkoff i'm a senior product manager at aws",
    "start": "1360",
    "end": "7279"
  },
  {
    "text": "this session is orchestrating big data integration and analytics with aws data pipeline",
    "start": "7279",
    "end": "13120"
  },
  {
    "text": "i'm going to talk for a few minutes to give you an overview of the data pipeline service",
    "start": "13120",
    "end": "18240"
  },
  {
    "text": "and then i'm joined by two gentlemen from swipely that are going to talk to us about how swipely is using data pipeline for their",
    "start": "18240",
    "end": "24960"
  },
  {
    "text": "big data workflows i'm going to i'm going to try to keep my comments to a minimum",
    "start": "24960",
    "end": "30240"
  },
  {
    "text": "because they have a lot of great material they have some tips and best practices that they're going to share with you",
    "start": "30240",
    "end": "36000"
  },
  {
    "text": "so first i want to talk a little bit about what are the challenges in dealing with data and especially big",
    "start": "36160",
    "end": "43840"
  },
  {
    "text": "so one of the challenges is that data is stored in different formats and different locations",
    "start": "43840",
    "end": "49280"
  },
  {
    "start": "44000",
    "end": "44000"
  },
  {
    "text": "this makes it very difficult to integrate so i'm just i'm curious with a show of hands could you raise your hand",
    "start": "49280",
    "end": "54320"
  },
  {
    "text": "if you currently have data stored in amazon s3 everybody okay how about how many of you",
    "start": "54320",
    "end": "60960"
  },
  {
    "text": "guys are using redshift today okay maybe maybe half a little more than half how many of you are using amazon",
    "start": "60960",
    "end": "67600"
  },
  {
    "text": "emr okay also maybe about half rds",
    "start": "67600",
    "end": "73439"
  },
  {
    "text": "okay about half so so you got and i noticed that many of you guys raised your hand for multiple of these",
    "start": "73439",
    "end": "78640"
  },
  {
    "text": "data stores so you probably agree that it's difficult to integrate data and",
    "start": "78640",
    "end": "83759"
  },
  {
    "text": "process data when it's stored in many different places in different formats you may and you probably also have data",
    "start": "83759",
    "end": "89520"
  },
  {
    "text": "on premise in an oracle data warehouse in flat files and kind of any number of other data stores",
    "start": "89520",
    "end": "95920"
  },
  {
    "text": "and so let's say you have data some of your data is on the cloud and aws and some of your other data is on premise it's especially difficult to",
    "start": "95920",
    "end": "102560"
  },
  {
    "text": "move data back and forth and integrate it into your data workflow",
    "start": "102560",
    "end": "107680"
  },
  {
    "start": "107000",
    "end": "107000"
  },
  {
    "text": "so a second challenge is when you're trying to process data when you're trying to do analytics you need to manage dependencies so you",
    "start": "107680",
    "end": "114159"
  },
  {
    "text": "may have a data workflow that you do not want to run if the input data for that process is",
    "start": "114159",
    "end": "119759"
  },
  {
    "text": "not available you may have a workflow that requires that step one succeed before you proceed",
    "start": "119759",
    "end": "126000"
  },
  {
    "text": "to step two you may have a workflow that you only want to run once a day once a month",
    "start": "126000",
    "end": "131360"
  },
  {
    "text": "a certain time of the day but so you'll have you have all of these conditions and you want to make your pipeline",
    "start": "131360",
    "end": "137440"
  },
  {
    "text": "or your workflow contingent on certain certain of those conditions a third issue a third challenge that you",
    "start": "137440",
    "end": "144560"
  },
  {
    "text": "need to deal with in working with data is that things go wrong there can be quality issues in the data there can be",
    "start": "144560",
    "end": "150480"
  },
  {
    "text": "human errors there can be any number of things that can go wrong and you need to be able to deal with those exceptions",
    "start": "150480",
    "end": "156480"
  },
  {
    "text": "for example do you want to retry in the case of an error or an exception",
    "start": "156480",
    "end": "161840"
  },
  {
    "text": "what do you want to do if one step in your workflow is taking much longer than it was supposed to",
    "start": "161840",
    "end": "167200"
  },
  {
    "text": "that could be a problem it could be okay maybe you're just processing more data than usual that particular execution of the",
    "start": "167200",
    "end": "174239"
  },
  {
    "text": "pipeline do you want to be notified if something goes wrong these are all things that you need to deal with whenever you're",
    "start": "174239",
    "end": "179840"
  },
  {
    "start": "179000",
    "end": "179000"
  },
  {
    "text": "constructing big data workflows so a fourth challenge is that there are",
    "start": "179840",
    "end": "185519"
  },
  {
    "start": "183000",
    "end": "183000"
  },
  {
    "text": "commercial tools on the market uh that attempt to do data integration data analytics",
    "start": "185519",
    "end": "192720"
  },
  {
    "text": "the challenge is that many of them are not really a great fit for the kinds of data and the kinds of",
    "start": "192720",
    "end": "198159"
  },
  {
    "text": "things that we're trying to do today with data many of them come with very expensive upfront licenses",
    "start": "198159",
    "end": "205360"
  },
  {
    "text": "many of them are designed to scale up onto bigger machines rather than scale out some of them do not support",
    "start": "205360",
    "end": "213120"
  },
  {
    "text": "scheduling many of them are not designed for the cloud they have kind of added on cloud services as kind of a bolt-on but",
    "start": "213120",
    "end": "219840"
  },
  {
    "text": "rather than a native natively supported data store and an example of that is some of the tools on",
    "start": "219840",
    "end": "225200"
  },
  {
    "text": "the market today don't support really important data stores like amazon dynamodb",
    "start": "225200",
    "end": "231120"
  },
  {
    "text": "so that is why a year ago at re invent last year we announced aws data pipeline",
    "start": "231360",
    "end": "236640"
  },
  {
    "text": "we are trying to create a service that will address many of the challenges that we just talked about",
    "start": "236640",
    "end": "243280"
  },
  {
    "text": "so you may have seen this slide before this is a very simple abstraction of a pipeline so one",
    "start": "243280",
    "end": "249680"
  },
  {
    "text": "pipeline might look something like this you have an input data node that could be something like an s3 bucket you might attach a precondition",
    "start": "249680",
    "end": "257440"
  },
  {
    "start": "253000",
    "end": "253000"
  },
  {
    "text": "to that data node so the precondition might look something like the s3 bucket must have objects in it",
    "start": "257440",
    "end": "264400"
  },
  {
    "text": "the next step in the pipeline is an activity so an activity is just something that you're going to do to that data it",
    "start": "264400",
    "end": "270160"
  },
  {
    "text": "could be something like a copy it could be something like a hive script and attached to that activity you may",
    "start": "270160",
    "end": "275919"
  },
  {
    "text": "want to have certain failure notifications delay notifications to let you know when",
    "start": "275919",
    "end": "280960"
  },
  {
    "text": "something has gone wrong and then optionally you might have an output data node so if the input data node was an s3 bucket and the activity",
    "start": "280960",
    "end": "287840"
  },
  {
    "text": "is something like a hive script the output data node might be dynamodb or it might be something",
    "start": "287840",
    "end": "292960"
  },
  {
    "text": "like an another s3 bucket so let's let's talk a little bit in the",
    "start": "292960",
    "end": "298639"
  },
  {
    "start": "296000",
    "end": "296000"
  },
  {
    "text": "next couple of slides about what does data pipeline do where does it really shine and then i'll we'll also talk a little bit about",
    "start": "298639",
    "end": "304639"
  },
  {
    "text": "some of the some of the areas where the product is continuing to evolve so one of the things that it really shines at is if you have data stored in",
    "start": "304639",
    "end": "311280"
  },
  {
    "text": "multiple different data stores on aws so we talked earlier about redshift s3 emr amazon rds dynamodb",
    "start": "311280",
    "end": "319840"
  },
  {
    "text": "these are some of the data stores that are natively integrated today as aws adds more data stores you you",
    "start": "319840",
    "end": "325840"
  },
  {
    "text": "will you can expect that the number of data stores that are supported by data pipeline will continue to grow as well",
    "start": "325840",
    "end": "331680"
  },
  {
    "text": "some of the activities that you can do on those data stores include things like copy a mapreduce job a hive",
    "start": "331680",
    "end": "338479"
  },
  {
    "text": "hive job pig job sql activity uh arbitrary shell command that last one is especially important",
    "start": "338479",
    "end": "344880"
  },
  {
    "text": "because it's it's very flexible you can do a lot of things through a shell command and that's one of the most popular things that our customers use",
    "start": "344880",
    "end": "351120"
  },
  {
    "text": "today so we talked earlier about it's difficult when you have data on premise",
    "start": "351120",
    "end": "356479"
  },
  {
    "start": "353000",
    "end": "353000"
  },
  {
    "text": "as well as in the cloud so another area where data pipeline really shines is if you have data on premise and in the cloud",
    "start": "356479",
    "end": "362400"
  },
  {
    "text": "and you need to orchestrate periodic movement from on-premise onto the cloud from the cloud back onto premise",
    "start": "362400",
    "end": "368319"
  },
  {
    "text": "on-premise we see many of our customers using this and it's something this is one of the",
    "start": "368319",
    "end": "373759"
  },
  {
    "text": "areas where data pipeline again really shines",
    "start": "373759",
    "end": "378400"
  },
  {
    "start": "378000",
    "end": "378000"
  },
  {
    "text": "dependencies so preconditions is a very important part of data pipeline uh it's relatively simple to set up a",
    "start": "379039",
    "end": "385840"
  },
  {
    "text": "precondition so some of the preconditions that are supported today are on this slide it includes things like dynamodb table",
    "start": "385840",
    "end": "392080"
  },
  {
    "text": "exist does dynamodb table have data does the s3 key exist is this s3 prefix",
    "start": "392080",
    "end": "400080"
  },
  {
    "text": "empty or not has a custom linux shell command succeeded or failed",
    "start": "400080",
    "end": "405680"
  },
  {
    "text": "so you can attach these preconditions to objects in your pipeline to make sure",
    "start": "405680",
    "end": "410720"
  },
  {
    "text": "that certain preconditions are met we talked earlier about alerting and",
    "start": "410720",
    "end": "416000"
  },
  {
    "start": "414000",
    "end": "414000"
  },
  {
    "text": "exception handling so this is the scenario where you need to deal with things that go wrong so",
    "start": "416000",
    "end": "421120"
  },
  {
    "text": "with within data pipeline you can set up notification on failure on success on delay so on delay might",
    "start": "421120",
    "end": "427199"
  },
  {
    "text": "might look something like this let's say you have a hive script and you expect that it's going to take about 30 minutes",
    "start": "427199",
    "end": "432479"
  },
  {
    "text": "if it starts taking an hour or maybe an hour and a half you want to be notified that something is potentially going",
    "start": "432479",
    "end": "437759"
  },
  {
    "text": "wrong there are automatic retries that are built in i believe that automatically we",
    "start": "437759",
    "end": "443520"
  },
  {
    "text": "configure three retries you can configure that to be less or more you want to just as a quick tip you may",
    "start": "443520",
    "end": "449759"
  },
  {
    "text": "want to be careful about configuring more retries than the default because you are consuming resources",
    "start": "449759",
    "end": "455520"
  },
  {
    "text": "these activities may be running on an emr cluster they may be running on ec2 nodes if you were to set your retry to 10 and",
    "start": "455520",
    "end": "462720"
  },
  {
    "text": "there was legitimately a problem with the data you may not want to actually retry 10 times you might want to be notified",
    "start": "462720",
    "end": "468160"
  },
  {
    "text": "so you can figure out what the issue is and correct it scheduling you can set up your pipeline",
    "start": "468160",
    "end": "476080"
  },
  {
    "start": "472000",
    "end": "472000"
  },
  {
    "text": "to run every hour every week every month the the highest frequency that you can set",
    "start": "476080",
    "end": "481680"
  },
  {
    "text": "up is every 15 minutes um backfill is it",
    "start": "481680",
    "end": "486800"
  },
  {
    "text": "moving down to the second bullet backfill is an interesting feature on data pipeline the idea there is you can start your",
    "start": "486800",
    "end": "492000"
  },
  {
    "text": "pipeline on a time and date in the past so let's say today is november 14th i",
    "start": "492000",
    "end": "497919"
  },
  {
    "text": "can start my pipeline january 1st and data pipeline will spawn multiple instances of that pipeline and",
    "start": "497919",
    "end": "504000"
  },
  {
    "text": "it will execute them in parallel to get you from january 1st all the way to today",
    "start": "504000",
    "end": "510639"
  },
  {
    "text": "that it's very useful if you were just getting started with data pipeline and you need to run it on historical dates",
    "start": "510639",
    "end": "518640"
  },
  {
    "start": "519000",
    "end": "519000"
  },
  {
    "text": "uh we talked earlier about the need to be scalable so rather than scale up onto a larger machine data pipeline",
    "start": "519360",
    "end": "526480"
  },
  {
    "text": "takes a very different approach it is sitting outside of the act of the resources that are actually running the",
    "start": "526480",
    "end": "531839"
  },
  {
    "text": "activities so let's say you're let's say your pipeline looks something like this you have a web server on ec2 it's creating",
    "start": "531839",
    "end": "537839"
  },
  {
    "text": "uh logs you are copying those logs into s3 once a day and then once a week you want",
    "start": "537839",
    "end": "544000"
  },
  {
    "text": "to run an emr mapreduce job to come up with some sort of insight on those logs",
    "start": "544000",
    "end": "550080"
  },
  {
    "text": "instead the data is not actually flowing through data pipeline it is flowing through ec2 s3 and emr and emr",
    "start": "550080",
    "end": "557680"
  },
  {
    "text": "being ec2 underneath the reason that that is very important is that you could add",
    "start": "557680",
    "end": "563360"
  },
  {
    "text": "multiple clusters you could add multiple ec2 nodes you could add an arbitrary number of data nodes",
    "start": "563360",
    "end": "568399"
  },
  {
    "text": "and activities and it will continue to scale this is not true of some of the other alternatives that are on the market",
    "start": "568399",
    "end": "574080"
  },
  {
    "text": "today it's one of the core things that we designed about data pipeline one other point to keep in mind is that",
    "start": "574080",
    "end": "580399"
  },
  {
    "text": "you can have a data pipeline in u.s east that is managing resources in other",
    "start": "580399",
    "end": "585440"
  },
  {
    "text": "regions so let's say you have data in tokyo you could be running a pipeline in u.s east that is orchestrating activities",
    "start": "585440",
    "end": "592640"
  },
  {
    "text": "in tokyo in the tokyo region so an s3 bucket in tokyo ec2 instances in tokyo",
    "start": "592640",
    "end": "599200"
  },
  {
    "text": "it kind of comes back to the the original point which is the data pipeline is designed to scale",
    "start": "599200",
    "end": "605839"
  },
  {
    "start": "606000",
    "end": "606000"
  },
  {
    "text": "we've done a number of things over the last six months or so to make it easier to get started with data pipeline it is still less than a year old so",
    "start": "606640",
    "end": "613680"
  },
  {
    "text": "there's lots of things that we want to continue to do to make the product easier to use some of the things that we provide today",
    "start": "613680",
    "end": "620079"
  },
  {
    "text": "are templates for common use cases so for example some of the templates that we provide is dynamodb to s3 copy",
    "start": "620079",
    "end": "626320"
  },
  {
    "text": "s3 to dynamodb s3 to redshift redshift back to s3 there's",
    "start": "626320",
    "end": "632399"
  },
  {
    "text": "a number of these kind of common use cases that we've created a template for to make it easier to get started",
    "start": "632399",
    "end": "637600"
  },
  {
    "text": "there's a graphical interface so you can certainly uh you don't need to use the graphical",
    "start": "637600",
    "end": "642640"
  },
  {
    "text": "interface but if you're just getting started it may be an easier path data pipeline natively understands csv",
    "start": "642640",
    "end": "648240"
  },
  {
    "text": "and tsv and if you're dealing with emr if you're launching hadoop and map map reduce jobs to analyze the data data",
    "start": "648240",
    "end": "656000"
  },
  {
    "text": "pipeline will automatically configure those clusters for you it's one of the things that is a really",
    "start": "656000",
    "end": "661200"
  },
  {
    "text": "nice feature about data pipeline that you don't need to worry about some of the cluster configuration details of vmr",
    "start": "661200",
    "end": "668640"
  },
  {
    "start": "668000",
    "end": "668000"
  },
  {
    "text": "and lastly it is very inexpensive a simple pipeline very well will be free so there's a",
    "start": "668640",
    "end": "675600"
  },
  {
    "text": "number of preconditions and activities that you get for free because data pipeline is part of the free tier program if you have a more",
    "start": "675600",
    "end": "681519"
  },
  {
    "text": "complex pipeline and you have say 10 preconditions and activities the pricing is all right in front of you",
    "start": "681519",
    "end": "688079"
  },
  {
    "text": "so depending on the frequency that you're running at it could be 60 cents per activity or precondition or it could",
    "start": "688079",
    "end": "693839"
  },
  {
    "text": "be a dollar if you're running at a higher frequency but it's all very it's very simple it's very straightforward",
    "start": "693839",
    "end": "699519"
  },
  {
    "text": "and as you can as you can tell this is not going to cost you an arm and a leg unlike some of the the alternative solutions that are on",
    "start": "699519",
    "end": "706000"
  },
  {
    "text": "the market today so let's just i have two more slides i just want to quickly walk through an etl",
    "start": "706000",
    "end": "712639"
  },
  {
    "start": "709000",
    "end": "709000"
  },
  {
    "text": "example so in this example what we're trying to do is bring together log data in s3 with",
    "start": "712639",
    "end": "718560"
  },
  {
    "text": "customer data in rds we want to process that in hive and load it into redshift so that analysts could",
    "start": "718560",
    "end": "724720"
  },
  {
    "text": "use a tool like microstrategy or tableau to query against it so and i apologize that the icons are a",
    "start": "724720",
    "end": "730399"
  },
  {
    "text": "little bit small so it may be difficult to read this but in the bottom left here we have two data nodes on the left we have s3",
    "start": "730399",
    "end": "737519"
  },
  {
    "text": "which is just unformatted logs and right next to it on the right we have rds and that is containing a customer",
    "start": "737519",
    "end": "743200"
  },
  {
    "text": "dimension data the little gear icon is an emr hive job",
    "start": "743200",
    "end": "748560"
  },
  {
    "text": "that is going to process that data and then the next gear icon is a redshift copy so we're going to be",
    "start": "748560",
    "end": "755440"
  },
  {
    "text": "copying the data into a redshift impressions table and then we're going to run another sql",
    "start": "755440",
    "end": "761680"
  },
  {
    "text": "activity to put it into a final table that and that's the table that we're going to use in our bi tool",
    "start": "761680",
    "end": "768160"
  },
  {
    "text": "so to take it one step further what we can do with this pipeline is we can run it on a schedule so we can run it hourly",
    "start": "768160",
    "end": "774720"
  },
  {
    "text": "weekly whatever frequency you want as long as it's something more than 15 minutes",
    "start": "774720",
    "end": "780720"
  },
  {
    "text": "we could make we could attach a precondition so that hive for example depends on the data being available in",
    "start": "780720",
    "end": "785920"
  },
  {
    "text": "s3 we can set up notifications with sns so that if something goes wrong",
    "start": "785920",
    "end": "791360"
  },
  {
    "text": "say on the hive activity you can get an sns notification letting you know something has gone wrong so you can inspect it and lastly",
    "start": "791360",
    "end": "798079"
  },
  {
    "text": "if we want to we could change some of the default retry logic on say the hive query maybe we only want",
    "start": "798079",
    "end": "803200"
  },
  {
    "text": "to retry twice rather than three times so hopefully this this gives you an idea of for a",
    "start": "803200",
    "end": "808800"
  },
  {
    "text": "very simple etl scenario what a pipeline would look like what i'd what i'm very excited to do now is",
    "start": "808800",
    "end": "814959"
  },
  {
    "text": "invite anthony accardi up from swipely who's going to talk to us about a much more interesting",
    "start": "814959",
    "end": "820240"
  },
  {
    "text": "more detailed scenario of how swipely is using data pipeline thank you",
    "start": "820240",
    "end": "829839"
  },
  {
    "text": "there's a perception in our industry that we can't benefit from big data tools",
    "start": "833839",
    "end": "839279"
  },
  {
    "start": "835000",
    "end": "835000"
  },
  {
    "text": "unless we have so much data that there's no other choice we hear questions like do you have a big",
    "start": "839279",
    "end": "845279"
  },
  {
    "text": "data problem we hear assertions like don't use hadoop you don't have enough data",
    "start": "845279",
    "end": "851279"
  },
  {
    "text": "and perhaps most concerning we see people who are reluctant to take on new data because they want to",
    "start": "851279",
    "end": "857360"
  },
  {
    "text": "hold on to their old way of doing things so what if the problem isn't the data",
    "start": "857360",
    "end": "864079"
  },
  {
    "text": "but rather that the tools are cumbersome and hard to use what if someone would",
    "start": "864079",
    "end": "870560"
  },
  {
    "text": "make the big data tools that are easy to use and painless there are three ways",
    "start": "870560",
    "end": "878000"
  },
  {
    "start": "877000",
    "end": "877000"
  },
  {
    "text": "that we can all benefit from big data tools right now the first is we can build novel features",
    "start": "878000",
    "end": "884560"
  },
  {
    "text": "using batch architecture and i'm not talking about just taking some analytics and loading it into",
    "start": "884560",
    "end": "889600"
  },
  {
    "text": "redshift so that the back office can make use of it i'm talking about real features that customers pay money",
    "start": "889600",
    "end": "896079"
  },
  {
    "text": "for the second thing we can do is decrease development time once you get that new feature out there",
    "start": "896079",
    "end": "903360"
  },
  {
    "text": "it's relatively easy to incorporate new data into it into the future but there's always a backfill challenge",
    "start": "903360",
    "end": "909279"
  },
  {
    "text": "you always have a lot of data that's already out there that your users are already built up and the question",
    "start": "909279",
    "end": "914639"
  },
  {
    "text": "usually comes up how can you build and incorporate that data into your new feature and big data tools can make that easy as",
    "start": "914639",
    "end": "921279"
  },
  {
    "text": "well and finally the third thing we get is that we get a very a simplified operational story with big",
    "start": "921279",
    "end": "928880"
  },
  {
    "text": "data tools we can scale more easily and we can also have a clearer provisioning story and",
    "start": "928880",
    "end": "935199"
  },
  {
    "text": "cost structure so swipely resonates very deeply",
    "start": "935199",
    "end": "940560"
  },
  {
    "text": "with these three needs swipeways the simple way for local merchants to process payments understand customers",
    "start": "940560",
    "end": "946720"
  },
  {
    "text": "and grow revenue so it is core to our business to use big data particularly around",
    "start": "946720",
    "end": "952880"
  },
  {
    "text": "payments data to make that actionable for our merchants our paying customers swipely is also an agile development",
    "start": "952880",
    "end": "959759"
  },
  {
    "text": "company we deploy the production many times a day and we rapidly iterate on our products",
    "start": "959759",
    "end": "965360"
  },
  {
    "text": "so it's very important for us to also keep development time very short and finally swipeways a startup",
    "start": "965360",
    "end": "971680"
  },
  {
    "text": "we don't have many resources we have two operations guys on our team and so it's important to us",
    "start": "971680",
    "end": "977040"
  },
  {
    "text": "to keep the operational side very simple and very easy to maintain",
    "start": "977040",
    "end": "983120"
  },
  {
    "text": "so we use data pipeline to meet these three needs now i'm going to first walk through the",
    "start": "983120",
    "end": "988240"
  },
  {
    "text": "types of product features we build with batch analytics and the feature i'm going to talk about is a fast dynamic",
    "start": "988240",
    "end": "994880"
  },
  {
    "text": "report that involves mashing up data i want you to pretend for a moment that you're a local merchant say you own a",
    "start": "994880",
    "end": "1001040"
  },
  {
    "text": "restaurant and you just you just launched the marketing campaign during mother's day",
    "start": "1001040",
    "end": "1006320"
  },
  {
    "text": "and people came in ordered stuff you want to now know how effective your campaign was",
    "start": "1006320",
    "end": "1011519"
  },
  {
    "text": "you can go to swipely we have this tool called campaign insights you can enter your dates of mother's day",
    "start": "1011519",
    "end": "1017199"
  },
  {
    "start": "1013000",
    "end": "1013000"
  },
  {
    "text": "may 10 through may 12th specify what data you might want to match up with it and click this button",
    "start": "1017199",
    "end": "1022720"
  },
  {
    "text": "and then boom what comes back here's a portion of the report that comes back there's a lot there's a lot of",
    "start": "1022720",
    "end": "1027918"
  },
  {
    "text": "information here right there's sales by day so at the top level you can figure out",
    "start": "1027919",
    "end": "1033600"
  },
  {
    "text": "how much money came in over the weekend mother's day how that compares to your average friday",
    "start": "1033600",
    "end": "1039120"
  },
  {
    "text": "saturday sunday you can see who your best customers are coming in who came in that weekend",
    "start": "1039120",
    "end": "1044558"
  },
  {
    "text": "you can also get a read on what your acquisition and retention is how many new people came in how many",
    "start": "1044559",
    "end": "1050720"
  },
  {
    "text": "people came back that you've seen before now there's no rocket science in",
    "start": "1050720",
    "end": "1056320"
  },
  {
    "text": "building this report the the new thing the novel part of this feature is that you can put any date range in",
    "start": "1056320",
    "end": "1063120"
  },
  {
    "text": "there and click the button and as quickly as it takes me to advance the slide the report like this comes up",
    "start": "1063120",
    "end": "1069840"
  },
  {
    "text": "well how do we do that well we pre-compute as much as we possibly can ahead of time",
    "start": "1069840",
    "end": "1075360"
  },
  {
    "text": "we start with our raw data our foundational data our facts our swipes our transactions on credit",
    "start": "1075360",
    "end": "1081360"
  },
  {
    "text": "cards and we run map-reduce jobs using data pipeline every night to build structured database",
    "start": "1081360",
    "end": "1089039"
  },
  {
    "text": "tables to make this query very very fast okay and so this is a data flow diagram",
    "start": "1089039",
    "end": "1096640"
  },
  {
    "start": "1095000",
    "end": "1095000"
  },
  {
    "text": "you can see we start on the left-hand side with uh facts like these transactions these",
    "start": "1096640",
    "end": "1101679"
  },
  {
    "text": "credit card transactions we run the map reduce job to create intermediate and",
    "start": "1101679",
    "end": "1107039"
  },
  {
    "text": "group basically by day your sales by day report and then we have another step where we insert that into a database so",
    "start": "1107039",
    "end": "1112799"
  },
  {
    "text": "we can be quickly indexed and query now it takes instances to move the state around and process it",
    "start": "1112799",
    "end": "1118799"
  },
  {
    "text": "we need an emr cluster we need instances to do that inserting to do that post-processing",
    "start": "1118799",
    "end": "1124640"
  },
  {
    "text": "and the whole thing's coordinated with aws data pipeline i'm going to run through an example now",
    "start": "1124640",
    "end": "1131760"
  },
  {
    "text": "so if we if you look at the top part you can look at the cells by day in order to compute those numbers",
    "start": "1131760",
    "end": "1137200"
  },
  {
    "start": "1136000",
    "end": "1136000"
  },
  {
    "text": "we take data that looks like transactions so someone went to the cafe on march 30th",
    "start": "1137200",
    "end": "1144240"
  },
  {
    "text": "and using their credit card ending and number 4980 spent 72 dollars that's the sort of raw data and the facts that we",
    "start": "1144240",
    "end": "1150799"
  },
  {
    "text": "start with we run the simple mapreduce job to group all those transactions by the merchant the cafe and by the date",
    "start": "1150799",
    "end": "1158080"
  },
  {
    "text": "to conclude that uh four thousand and thirty dollars were were spent on uh on may 10th and the point is on the",
    "start": "1158080",
    "end": "1165039"
  },
  {
    "text": "right hand side that can be very indexed now we just return with a simple index query",
    "start": "1165039",
    "end": "1170080"
  },
  {
    "text": "three rows to be able to compute the report now this is very simple we don't need amazon",
    "start": "1170080",
    "end": "1176160"
  },
  {
    "text": "data pipeline to manage this one map reduce job the beauty is",
    "start": "1176160",
    "end": "1182000"
  },
  {
    "text": "amazon data pipeline makes much more complicated flows as easy as managing this simple",
    "start": "1182000",
    "end": "1188720"
  },
  {
    "text": "one so if you take a look at the customer visits that's a little bit more interesting right because now we have to",
    "start": "1188720",
    "end": "1194000"
  },
  {
    "text": "know for each visit for each credit card transaction have we seen that person before or not",
    "start": "1194000",
    "end": "1199679"
  },
  {
    "text": "right and we can accomplish that by cascading two map reduced jobs first we go from transactions to visits",
    "start": "1199679",
    "end": "1206880"
  },
  {
    "text": "and determine whether or not we've seen that transaction before you have a you know it's either new or",
    "start": "1206880",
    "end": "1212880"
  },
  {
    "text": "it isn't and then there's more of a standard aggregation map reduced job that groups by merchant and by day so we get to the",
    "start": "1212880",
    "end": "1219440"
  },
  {
    "text": "number of new people so a little bit more complex and if you look at this best customers portion of the report",
    "start": "1219440",
    "end": "1226159"
  },
  {
    "text": "that's even more complex because now we have names we have people's personal information associated with these credit card transactions we",
    "start": "1226159",
    "end": "1232559"
  },
  {
    "text": "have mary instead of a credit card number that comes from a new fundamental data source a new type of fact",
    "start": "1232559",
    "end": "1238400"
  },
  {
    "text": "uh called card opt-in where mary has basically registered her credit card and",
    "start": "1238400",
    "end": "1243520"
  },
  {
    "text": "associate her personal information with that card so we use a hive activity to join",
    "start": "1243520",
    "end": "1249440"
  },
  {
    "text": "the visits that we spoke about before with this new input table to compute what we call on the right",
    "start": "1249440",
    "end": "1255280"
  },
  {
    "text": "hand side analytics documents that can be very efficiently query to build the report that you see",
    "start": "1255280",
    "end": "1260320"
  },
  {
    "text": "now we've wound up with a much more complicated pipeline a more complicated mapreduce flow",
    "start": "1260320",
    "end": "1266240"
  },
  {
    "text": "data pipeline makes it easy to manage this by specifying everything declaratively through dependencies so what i want to",
    "start": "1266240",
    "end": "1272880"
  },
  {
    "text": "do now is uh hand things over to bright fulton he's our lead operations engineer and he's going",
    "start": "1272880",
    "end": "1279520"
  },
  {
    "text": "to walk you guys through how we actually do that with data pipeline thanks",
    "start": "1279520",
    "end": "1291679"
  },
  {
    "text": "all right um so john talked a lot about the different data stores",
    "start": "1291679",
    "end": "1296880"
  },
  {
    "text": "and the different type of nodes that you can put in a data pipeline and anthony talked a lot about the the",
    "start": "1296880",
    "end": "1303440"
  },
  {
    "text": "way you build up complex pipelines to do novel analytics i just want to bring it down to a very",
    "start": "1303440",
    "end": "1309360"
  },
  {
    "text": "simple level we're going to take a piece of a pipeline this is a sales by day",
    "start": "1309360",
    "end": "1314799"
  },
  {
    "text": "aggregation that goes through mapreduce and just want to run through it in order to show you uh what our engineers do on a",
    "start": "1314799",
    "end": "1322559"
  },
  {
    "text": "sort of day-to-day basis as they're developing and looking at pipelines so we um we edit the the json directly",
    "start": "1322559",
    "end": "1329360"
  },
  {
    "text": "that there are other options but this is what we do and so if you think about the",
    "start": "1329360",
    "end": "1336240"
  },
  {
    "text": "slide that anthony left you with where we have our facts store over on the left and it",
    "start": "1336240",
    "end": "1342080"
  },
  {
    "text": "transitions through an emr job into an intermediate",
    "start": "1342080",
    "end": "1347120"
  },
  {
    "text": "uh s3 bucket and then trans uh then gets inserted through a copy activity",
    "start": "1347120",
    "end": "1352240"
  },
  {
    "text": "into an rds instance that flow goes from facts that are um you know coming in",
    "start": "1352240",
    "end": "1360159"
  },
  {
    "text": "without structure and then they get uh mapped and reduced and so this first set of",
    "start": "1360159",
    "end": "1367360"
  },
  {
    "text": "nodes here the s3 transactions that's the stuff on the left this s3 sales by day this is a node",
    "start": "1367360",
    "end": "1374080"
  },
  {
    "text": "that's you know declaratively describing the intermediate bucket that the stuff gets",
    "start": "1374080",
    "end": "1379280"
  },
  {
    "text": "put in and then finally the third data node that we have in that graph is the stuff on the right this is the",
    "start": "1379280",
    "end": "1385200"
  },
  {
    "text": "rds node this is a mysql instance where our reduced",
    "start": "1385200",
    "end": "1390799"
  },
  {
    "text": "values end up getting inserted you can see the insert statement there cool so the next level up in that",
    "start": "1390799",
    "end": "1396960"
  },
  {
    "text": "diagram was the resources that it runs on and there were two if you recall one was the",
    "start": "1396960",
    "end": "1403840"
  },
  {
    "text": "emr cluster and then the other was the ec2 resource which actually did the",
    "start": "1403840",
    "end": "1409520"
  },
  {
    "text": "insertion and then uh sorry that's the emr cluster",
    "start": "1409520",
    "end": "1415520"
  },
  {
    "text": "there and then that's the ec2 resource that it runs on and there are some other nodes that are",
    "start": "1415520",
    "end": "1421120"
  },
  {
    "text": "kind of boilerplate but you can see you get the sense here right we're dealing with a few pages of json",
    "start": "1421120",
    "end": "1426559"
  },
  {
    "text": "and this declaratively describes that whole data flow from left to right",
    "start": "1426559",
    "end": "1432080"
  },
  {
    "text": "cool so another way that we can kind of get confirmation that what we're doing makes sense is by",
    "start": "1432080",
    "end": "1439600"
  },
  {
    "text": "visualizing the pipeline itself so one of our engineers matt gallui wrote a tool called pipeli",
    "start": "1439600",
    "end": "1446159"
  },
  {
    "text": "it's open source swipe please pipely on github and",
    "start": "1446159",
    "end": "1451279"
  },
  {
    "text": "you can use it to visualize the nodes that you author so let me just give you a sense for that",
    "start": "1451279",
    "end": "1457760"
  },
  {
    "text": "so let's just do one exact pipeline",
    "start": "1457760",
    "end": "1464240"
  },
  {
    "text": "so it takes as input the json file and writes as output png that you can just open up",
    "start": "1465760",
    "end": "1473840"
  },
  {
    "text": "okay cool so what we can see here is that we have our",
    "start": "1475760",
    "end": "1481919"
  },
  {
    "text": "transactions input gets processed by the emr activity output of that is the s3 data node",
    "start": "1481919",
    "end": "1490240"
  },
  {
    "text": "then it goes through the copy activity and into the mysql node",
    "start": "1490240",
    "end": "1496640"
  },
  {
    "text": "this is useful if you you know mis-author your your your data pipeline you could end up",
    "start": "1496960",
    "end": "1502400"
  },
  {
    "text": "with a sort of orphaned node or sub-graph that you would want to correct",
    "start": "1502400",
    "end": "1507840"
  },
  {
    "text": "so that's part of the the flow and then i just want to show you a sort of time-compressed video of deploying and watching a",
    "start": "1507840",
    "end": "1514080"
  },
  {
    "text": "pipeline get run through just to give you a sense for how that looks again this is a very small example",
    "start": "1514080",
    "end": "1521840"
  },
  {
    "text": "okay so the first thing that we do is we're just deploying from the command",
    "start": "1522799",
    "end": "1528320"
  },
  {
    "text": "line like any anything else that we do i i don't know if you noticed but we're using our fog credential here",
    "start": "1528320",
    "end": "1534960"
  },
  {
    "text": "one of our engineers keith barrett added data pipeline support to fog so i don't know how many ruby guys are here but",
    "start": "1534960",
    "end": "1540960"
  },
  {
    "text": "fog has good data pipeline support i'm going to definitely use it so it goes through some prep work in s3",
    "start": "1540960",
    "end": "1547520"
  },
  {
    "text": "uploads our mapper and reducer steps and then ultimately puts the pipeline in",
    "start": "1547520",
    "end": "1554720"
  },
  {
    "text": "so here if we visit the data pipeline console in amazon console we can see",
    "start": "1554720",
    "end": "1560799"
  },
  {
    "text": "that our pipeline was created um here are all the steps that we defined in that json file",
    "start": "1560799",
    "end": "1565919"
  },
  {
    "text": "and if you look through in the status column you can see that some of them are in a creating state some of them are",
    "start": "1565919",
    "end": "1573039"
  },
  {
    "text": "already finished those are the data nodes that already exist and some of them are",
    "start": "1573039",
    "end": "1578400"
  },
  {
    "text": "waiting for dependencies if we go into the emr",
    "start": "1578400",
    "end": "1584720"
  },
  {
    "text": "console uh the newly updated emr console very cool we can see that one of those resources",
    "start": "1584720",
    "end": "1592080"
  },
  {
    "text": "that pipeline's orchestrating for us is the as a new cluster so it's spun up a new cluster",
    "start": "1592080",
    "end": "1597279"
  },
  {
    "text": "to process this emr job and we can see that the initial things that it's doing here are installing",
    "start": "1597279",
    "end": "1602880"
  },
  {
    "text": "pig and hive and so forth on the on the cluster so i just want to show you again here",
    "start": "1602880",
    "end": "1609279"
  },
  {
    "text": "using pipeli instead of going at a static json file we're going to connect",
    "start": "1609279",
    "end": "1614799"
  },
  {
    "text": "directly to the data pipeline api and get status of our running",
    "start": "1614799",
    "end": "1622559"
  },
  {
    "text": "pipeline we can open that up and this time we see that you know we have some colored nodes so the ones that are finished are in blue here's one that's",
    "start": "1622559",
    "end": "1629919"
  },
  {
    "text": "waiting on a dependency and then downstream ones",
    "start": "1629919",
    "end": "1634960"
  },
  {
    "text": "so that's a cool way to get feedback as it's running of course you can also look at it in the amazon console",
    "start": "1634960",
    "end": "1642880"
  },
  {
    "text": "here we are back in the emr console and you can see that we have some pending jobs with",
    "start": "1642960",
    "end": "1650799"
  },
  {
    "text": "tasks mapping and reducing tasks and they're all in a pending state so they haven't begun yet right they're they're getting",
    "start": "1650799",
    "end": "1656960"
  },
  {
    "text": "ready to work if a little time goes by in this case the not much time this was a very small job",
    "start": "1656960",
    "end": "1664159"
  },
  {
    "text": "we see that the map ones get turned into running and the reduced ones because they come after",
    "start": "1664159",
    "end": "1669760"
  },
  {
    "text": "are still going to be coming and you know it looks like one map was",
    "start": "1669760",
    "end": "1676559"
  },
  {
    "text": "already completed and then we can kind of keep bouncing back and forth depending on what you're trying to do",
    "start": "1676559",
    "end": "1682240"
  },
  {
    "text": "i just want to show that pipeli can also sort of update as that progress",
    "start": "1682240",
    "end": "1688159"
  },
  {
    "text": "goes through on the pipeline so here we go opening up in pipeline again now we can see that our emr job",
    "start": "1688159",
    "end": "1694799"
  },
  {
    "text": "is in a running state screen and downstream stuff still waiting",
    "start": "1694799",
    "end": "1701759"
  },
  {
    "text": "cool so when that is completed right all of our mapping and reducing steps have",
    "start": "1702320",
    "end": "1708159"
  },
  {
    "text": "been completed then we're going to see that we can you know",
    "start": "1708159",
    "end": "1713600"
  },
  {
    "text": "get one more graph out of it and it'll move on to show the progress past the past the node and this is just you know",
    "start": "1713600",
    "end": "1721120"
  },
  {
    "text": "a kind of a flow a debugging flow or development flow this isn't what we don't like watch our production stuff go through",
    "start": "1721120",
    "end": "1729440"
  },
  {
    "text": "all right and then here we see we're back in the data pipeline console",
    "start": "1729600",
    "end": "1735039"
  },
  {
    "text": "back out of emr which is completed and you can see that now pipeline is orchestrating the shutdown",
    "start": "1735039",
    "end": "1740480"
  },
  {
    "text": "of all of our resources which is very cool because we only paid for exactly the amount of time that we used",
    "start": "1740480",
    "end": "1746080"
  },
  {
    "text": "them for and then just to confirm for ourselves we can see that our our uh",
    "start": "1746080",
    "end": "1753600"
  },
  {
    "text": "graphiz thing goes all green at the end are all blue cool so everything's done um of course",
    "start": "1753600",
    "end": "1760320"
  },
  {
    "text": "we just want to show that the result actually ended up in my sql you can see that we have a very simple",
    "start": "1760320",
    "end": "1767360"
  },
  {
    "text": "table here that just has a primary key which is the store day that's the mapped part right like so we",
    "start": "1767360",
    "end": "1772720"
  },
  {
    "text": "mapped all of our card transactions into a key of store and day and then we reduced them just by summing",
    "start": "1772720",
    "end": "1779360"
  },
  {
    "text": "aggregation into the total sales for the day",
    "start": "1779360",
    "end": "1784960"
  },
  {
    "text": "and then we can use that to generate reports like anthony was mentioning where we just have the sales by day for",
    "start": "1784960",
    "end": "1791360"
  },
  {
    "text": "particular merchants so hopefully that gives you a sense of a simplified flow and what it actually",
    "start": "1791360",
    "end": "1796480"
  },
  {
    "text": "looks like when it runs",
    "start": "1796480",
    "end": "1799360"
  },
  {
    "text": "thanks so that's how we build new features with data pipeline now",
    "start": "1806840",
    "end": "1812799"
  },
  {
    "start": "1807000",
    "end": "1807000"
  },
  {
    "text": "it's not enough to build new features we want to build them quickly and that brings us to the",
    "start": "1812799",
    "end": "1818080"
  },
  {
    "text": "backfilling point to the backfilling issue so remember we pushed the feature out it's easy to get the data and",
    "start": "1818080",
    "end": "1823760"
  },
  {
    "text": "incorporate the ones that come in the future but the challenge is how do you process all the historical data you have to",
    "start": "1823760",
    "end": "1829679"
  },
  {
    "text": "bring it into your feature now what we used to do is there was always a migration we would need to run",
    "start": "1829679",
    "end": "1835440"
  },
  {
    "text": "it was despite our bets efforts uh it was always a one-off sort of thing uh the engineer who's responsible for",
    "start": "1835440",
    "end": "1841760"
  },
  {
    "text": "the feature had to wrangle the resources and figure out well how much computation would he really need we'd have to coordinate with operations",
    "start": "1841760",
    "end": "1848559"
  },
  {
    "text": "uh testing always came up how do we know it's going to work and not going to break on production it was always difficult to get the full",
    "start": "1848559",
    "end": "1854320"
  },
  {
    "text": "data set on production to test so we were concerned about well maybe there's some data in production that we're not",
    "start": "1854320",
    "end": "1859679"
  },
  {
    "text": "seeing in our test set that's going to make it fall over and it was in general just a hassle we don't do any of that anymore",
    "start": "1859679",
    "end": "1866240"
  },
  {
    "text": "with data pipeline instead of that complicated backfilling process we basically rebuild our we rebuild our",
    "start": "1866240",
    "end": "1873279"
  },
  {
    "text": "analytics documents regularly daily okay and that allows us to rapidly",
    "start": "1873279",
    "end": "1878559"
  },
  {
    "text": "iterate our product and we do it with an agile process so this is a block diagram just a",
    "start": "1878559",
    "end": "1884159"
  },
  {
    "text": "description of the architecture and you can see our facts like the transitions in the card opt-in that we talked about in the example",
    "start": "1884159",
    "end": "1890480"
  },
  {
    "text": "go in directly to this fax store that we store on s3 and then data pipeline we schedule to",
    "start": "1890480",
    "end": "1895760"
  },
  {
    "text": "run daily to crunch that and match up all that data to produce our analytics documents",
    "start": "1895760",
    "end": "1901039"
  },
  {
    "text": "and that's what the web tier hits now as a side note i don't want any of you to leave",
    "start": "1901039",
    "end": "1906720"
  },
  {
    "text": "thinking that this architecture only lets you build features that change once a day that is the example that i gave you the",
    "start": "1906720",
    "end": "1912960"
  },
  {
    "text": "the campaign insights but we also use this architecture this batch processing architecture for real more real-time features and we",
    "start": "1912960",
    "end": "1919679"
  },
  {
    "start": "1918000",
    "end": "1918000"
  },
  {
    "text": "do it by adding another another database a recent activity database and the idea is that uh the trans the facts the",
    "start": "1919679",
    "end": "1926399"
  },
  {
    "text": "transactions and the card opt-in data don't just go into the fax store but we also shove them into the recent activity",
    "start": "1926399",
    "end": "1931840"
  },
  {
    "text": "database so that when the query is made the res the analytics documents take us up to",
    "start": "1931840",
    "end": "1937679"
  },
  {
    "text": "this morning and then all the rest of the data from this morning to now is in the recent activity and they",
    "start": "1937679",
    "end": "1944640"
  },
  {
    "text": "get merged together very quickly uh when when the request gets served",
    "start": "1944640",
    "end": "1950480"
  },
  {
    "text": "so let me show you what we can do with this here's the campaign sales by day feature",
    "start": "1950480",
    "end": "1957360"
  },
  {
    "start": "1954000",
    "end": "1954000"
  },
  {
    "text": "and imagine for a moment that we want to compute these numbers differently maybe we want uh we changed our mind we",
    "start": "1957360",
    "end": "1963840"
  },
  {
    "text": "want to compute gross sales instead of net sales or maybe there was a bug maybe we computed it wrong in some cases in all",
    "start": "1963840",
    "end": "1970000"
  },
  {
    "text": "cases the cool thing is that we don't actually care whether it's a bug or a new feature or a change",
    "start": "1970000",
    "end": "1977039"
  },
  {
    "text": "in a feature because when we rebuild the analytics documents every night it's relatively simple in this example",
    "start": "1977039",
    "end": "1983840"
  },
  {
    "text": "to change the logic to fix the bug to change the accounting rules that's a simple code change",
    "start": "1983840",
    "end": "1989120"
  },
  {
    "text": "and now we know by the next morning all the data will be consistent with it now of course if it's an if it's if it's",
    "start": "1989120",
    "end": "1996159"
  },
  {
    "text": "a egregious bug if it's a really bad bug we won't wait till next morning we'll just kick the pipeline",
    "start": "1996159",
    "end": "2001760"
  },
  {
    "text": "off immediately as soon as we fix it so we have that degree of flexibility too this strategy also works with much more",
    "start": "2001760",
    "end": "2007919"
  },
  {
    "text": "complicated features if we iteratively build out a product so for example if you look at our best",
    "start": "2007919",
    "end": "2013200"
  },
  {
    "start": "2012000",
    "end": "2012000"
  },
  {
    "text": "customers report you might reasonably find lots of different ways that you define what your",
    "start": "2013200",
    "end": "2018480"
  },
  {
    "text": "best customers are a simple definition for a campaign spending a weekend is just how much",
    "start": "2018480",
    "end": "2024080"
  },
  {
    "text": "money they spend over that weekend but you can get more sophisticated get at lifetime value you can look at what",
    "start": "2024080",
    "end": "2029200"
  },
  {
    "text": "they're doing online other ways that they're interacting with with the merchant it can get more sophisticated over time",
    "start": "2029200",
    "end": "2035039"
  },
  {
    "text": "so what this uh this batch architecture allows us to do is it really iteratively build out those",
    "start": "2035039",
    "end": "2041039"
  },
  {
    "text": "features make the code changes and then see the updates the next day it's very powerful now let me talk about",
    "start": "2041039",
    "end": "2049440"
  },
  {
    "text": "how we actually implement this in uh in data pipeline because you see we don't actually get an agile development",
    "start": "2049440",
    "end": "2055919"
  },
  {
    "start": "2051000",
    "end": "2051000"
  },
  {
    "text": "process for free amazon gives us a lot of a lot of options there's a lot of functionality features in data pipeline",
    "start": "2055919",
    "end": "2062320"
  },
  {
    "text": "and we've learned the hard way how to best use those features to craft an agile process so i want to share with",
    "start": "2062320",
    "end": "2068320"
  },
  {
    "text": "you guys some of the things we learned and it really falls into four different pieces of advice",
    "start": "2068320",
    "end": "2073760"
  },
  {
    "text": "the first is you saw there were there are different ways you can interact with data pipelines john showed you you can use the gui",
    "start": "2073760",
    "end": "2079440"
  },
  {
    "text": "as bright showed you you can work directly with the json we we operate with the json but a slight",
    "start": "2079440",
    "end": "2085440"
  },
  {
    "text": "layer above we wrap the json in our own code and we'll tell you why in a moment there's also a lot of options a lot of",
    "start": "2085440",
    "end": "2091200"
  },
  {
    "text": "different parameters you can specify in data pipeline nodes there are different ways you might want to set those to reduce the variability",
    "start": "2091200",
    "end": "2097839"
  },
  {
    "text": "that you're dealing with to to make the failure mode simpler amazon built quite a sophisticated",
    "start": "2097839",
    "end": "2104640"
  },
  {
    "text": "logging mechanism for data pipeline but in order to take use of that to make sure you diagnose the failures",
    "start": "2104640",
    "end": "2109760"
  },
  {
    "text": "and tighten your development cycle quickly you need to enable it and configure it properly so let's click about that",
    "start": "2109760",
    "end": "2116960"
  },
  {
    "text": "and finally there are different ways you can automate some common tests with data pipeline to to ease your development burden",
    "start": "2116960",
    "end": "2125119"
  },
  {
    "start": "2125000",
    "end": "2125000"
  },
  {
    "text": "so this is another look at our json this is the emr activity node that breit showed you and you can see there's a description",
    "start": "2125119",
    "end": "2131920"
  },
  {
    "text": "there to specify your input your output and also your mapper and your reducer wherever it be on rail shop so you can",
    "start": "2131920",
    "end": "2137920"
  },
  {
    "text": "see the ruby files there any interesting thing here is that this is a template we interpolate those",
    "start": "2137920",
    "end": "2143359"
  },
  {
    "text": "variables in this example the data path and the code path so we're programmatically generating the json that gives us a lot of power",
    "start": "2143359",
    "end": "2149920"
  },
  {
    "text": "to define these pipelines there's a more subtle benefit too though which is by wrapping this in our own",
    "start": "2149920",
    "end": "2156480"
  },
  {
    "text": "code we now can leverage our existing code infrastructure our existing practices",
    "start": "2156480",
    "end": "2161920"
  },
  {
    "text": "we we undergo code review we check everything into version control we basically treat the pipeline code",
    "start": "2161920",
    "end": "2167680"
  },
  {
    "text": "like any other piece of code in our system and if we were to write this today it would look more like this",
    "start": "2167680",
    "end": "2174960"
  },
  {
    "text": "where we're starting to find our own helper methods and functions to make it easier to just deal with the things that",
    "start": "2174960",
    "end": "2180000"
  },
  {
    "text": "we care about in working with the pipeline and we're actually moving in the direction right now defining our own domain specific",
    "start": "2180000",
    "end": "2185760"
  },
  {
    "text": "language so that we can be more expressive and concise with what we're trying to do with the pipeline",
    "start": "2185760",
    "end": "2191680"
  },
  {
    "text": "let me talk about reducing variability there's a few things we learned in terms of parameter settings we",
    "start": "2191680",
    "end": "2197280"
  },
  {
    "start": "2192000",
    "end": "2192000"
  },
  {
    "text": "haven't had much luck running pipelines on small instances in particular map reduced jobs so we encourage everyone to",
    "start": "2197280",
    "end": "2203440"
  },
  {
    "text": "maybe start with a large you don't need super beefy instances that's not the idea of horizontal scaling but but we haven't",
    "start": "2203440",
    "end": "2209920"
  },
  {
    "text": "had luck with smalls i also encourage folks to lock your versions the amazon data pipeline team makes a",
    "start": "2209920",
    "end": "2216240"
  },
  {
    "text": "lot of updates they're very active and it's painful to learn about their updates when you realize that all your",
    "start": "2216240",
    "end": "2221680"
  },
  {
    "text": "pipelines have broken because of a version dependency because you didn't lock your versions so we learned that one the hard way",
    "start": "2221680",
    "end": "2227119"
  },
  {
    "text": "and finally this this might not be um obvious but we started out we were wondering how the best structure",
    "start": "2227119",
    "end": "2232720"
  },
  {
    "text": "security groups with pipelines and we started out uh adding new security groups for new",
    "start": "2232720",
    "end": "2238160"
  },
  {
    "text": "pipeline resources and new pipelines and that became very cumbersome what we do now is we",
    "start": "2238160",
    "end": "2243599"
  },
  {
    "text": "statically define security groups by the databases by the data that they need to access",
    "start": "2243599",
    "end": "2249200"
  },
  {
    "text": "and then when we define a new pipeline we point to one of those predefined groups that's been working a lot better for us",
    "start": "2249200",
    "end": "2256240"
  },
  {
    "text": "in order to diagnose failures you're going to want to turn on logging and what we do is very convenient is we name",
    "start": "2256240",
    "end": "2261359"
  },
  {
    "start": "2257000",
    "end": "2257000"
  },
  {
    "text": "space the logs so that for a particular environment for a particular time that the pipeline runs it winds up in a particular directory on",
    "start": "2261359",
    "end": "2268079"
  },
  {
    "text": "s3 what that lets us do is when our developers run the pipeline they know exactly where the final logs",
    "start": "2268079",
    "end": "2274400"
  },
  {
    "text": "for that particular run and it makes debugging much easier and also when we're developing",
    "start": "2274400",
    "end": "2280400"
  },
  {
    "text": "it's very useful to be able to log on to the instances directly when things go wrong and so it's helpful there's a key pair",
    "start": "2280400",
    "end": "2286320"
  },
  {
    "text": "an ssh keypair attribute you can set to allow you to do that and lock them directly",
    "start": "2286320",
    "end": "2292000"
  },
  {
    "start": "2291000",
    "end": "2291000"
  },
  {
    "text": "and finally there are a few things to automate this is a very cool parameter terminate after it ensures that no matter what happens",
    "start": "2292000",
    "end": "2298079"
  },
  {
    "text": "to your pipeline it shuts down in that given amount of time you're no longer paying money for it now you might think you want your",
    "start": "2298079",
    "end": "2304320"
  },
  {
    "text": "pipeline all the resources to disappear as soon as the pipeline starts running that's true when your pipeline is",
    "start": "2304320",
    "end": "2310240"
  },
  {
    "text": "successful but when your pipeline fails uh particularly in development you um",
    "start": "2310240",
    "end": "2315280"
  },
  {
    "text": "you probably want some time to have engineer to be able to investigate the dead body and figure out what might went wrong what what might",
    "start": "2315280",
    "end": "2322079"
  },
  {
    "text": "have gone wrong and so something on the order of six hours has been working for us because it gives folks enough time to get around",
    "start": "2322079",
    "end": "2328079"
  },
  {
    "text": "and and and uncover what happened but ensures that we don't wind up paying for pipelines that we ran a long time ago",
    "start": "2328079",
    "end": "2335440"
  },
  {
    "text": "and this is a cool trick uh that you may have noticed in bright's json but we have a need to um to bootstrap our",
    "start": "2335440",
    "end": "2342720"
  },
  {
    "text": "ac2 resources and pipelines we basically set up various ruby gems and whatnot that we need for processing",
    "start": "2342720",
    "end": "2349040"
  },
  {
    "text": "and what we do is we define a shell command activity node and we make it the dependent of those resources and all it does is reference",
    "start": "2349040",
    "end": "2355839"
  },
  {
    "text": "the bootstrap script that we want those resources to load and so brett used that trick before and we use it frequently in our more",
    "start": "2355839",
    "end": "2362480"
  },
  {
    "text": "complicated pipelines so no need to backfill your data using",
    "start": "2362480",
    "end": "2367680"
  },
  {
    "text": "old techniques we can rebuild the analytics documents daily that helps us",
    "start": "2367680",
    "end": "2373280"
  },
  {
    "text": "iterate quickly on our product and we can also craft and use all the options and features that amazon",
    "start": "2373280",
    "end": "2378480"
  },
  {
    "text": "puts into their data pipeline to um to to create an agile development flow",
    "start": "2378480",
    "end": "2385440"
  },
  {
    "text": "now we built something we built it quickly so even better than building is being able to build it quickly",
    "start": "2385440",
    "end": "2390960"
  },
  {
    "text": "but better still is once we ship the feature can we easily scale and maintain it and",
    "start": "2390960",
    "end": "2397440"
  },
  {
    "text": "with amazon data pipeline that lets us have a simple horizontal scaling story",
    "start": "2397440",
    "end": "2403200"
  },
  {
    "text": "and the simple provisioning story we aim to complete all of our pipelines in 50 minutes and",
    "start": "2403200",
    "end": "2410000"
  },
  {
    "text": "the cost structure is very attractive we were able to store all of our data even with that constraint",
    "start": "2410000",
    "end": "2416000"
  },
  {
    "text": "this is the output of pipeline the gem that we wrote it's a more complicated pipeline than bright showed you this is one that",
    "start": "2416000",
    "end": "2422000"
  },
  {
    "text": "we run in production and the interesting feature about this pipeline is that all those unhappy grade nodes",
    "start": "2422000",
    "end": "2427440"
  },
  {
    "text": "are waiting on one emr activity now that's a beautiful thing because",
    "start": "2427440",
    "end": "2433920"
  },
  {
    "text": "mapreduce scales horizontally so if all of your pipeline bottlenecks are emr nodes then you can conceivably",
    "start": "2433920",
    "end": "2442319"
  },
  {
    "text": "the first order throw more resources at the problem and then the entire running time of your",
    "start": "2442319",
    "end": "2447440"
  },
  {
    "text": "pipeline goes down accordingly so we make great use of that",
    "start": "2447440",
    "end": "2453680"
  },
  {
    "start": "2451000",
    "end": "2451000"
  },
  {
    "text": "we start by choosing the smallest capable on demand instance type remember no smalls but we don't want big",
    "start": "2453680",
    "end": "2461119"
  },
  {
    "text": "beefy instances we're not vertically scaling we run a lot of pipelines and then one largest that gives us a fixed hourly",
    "start": "2461119",
    "end": "2467839"
  },
  {
    "text": "cost and we also don't pay for any idle time we just pay for the resources that we",
    "start": "2467839",
    "end": "2473119"
  },
  {
    "text": "use now to first order if things scale horizontally then the cost of running one instance",
    "start": "2473119",
    "end": "2479680"
  },
  {
    "text": "400 hours is the same as running 100 instances in one so if the costs are the same why not do it all in one hour we pretty",
    "start": "2479680",
    "end": "2487359"
  },
  {
    "text": "much do that except there's a little bit of a fudge factor because there's about a 10 minute run time variability",
    "start": "2487359",
    "end": "2492480"
  },
  {
    "text": "just instances starting what up and whatnot we don't want to get accidentally charged for two hours so we shoot for 50 minutes and just for",
    "start": "2492480",
    "end": "2499839"
  },
  {
    "text": "as example this is a relatively it's a very small pipeline there's only 50 gig of facts that we're processing",
    "start": "2499839",
    "end": "2505280"
  },
  {
    "text": "but it's a complex pipeline it's on the order of the one i showed you a slide ago with pipely",
    "start": "2505280",
    "end": "2510960"
  },
  {
    "text": "we can crunch that in 50 minutes using 40 instances under 10 so very cheap and it's",
    "start": "2510960",
    "end": "2517280"
  },
  {
    "text": "interesting to stop for a moment and think about the cost comparison there you know it's 10 versus what well",
    "start": "2517280",
    "end": "2523200"
  },
  {
    "text": "the obvious way to compare it is well what if i'm running 40 instances what if i run that 40 instances throughout the entire day",
    "start": "2523200",
    "end": "2529119"
  },
  {
    "text": "instead of just for that one hour so that's a 24x cost savings but if you just",
    "start": "2529119",
    "end": "2534880"
  },
  {
    "text": "hypothetically put me in a position where amazon data pipeline didn't exist we wouldn't actually do that it would be too expensive at scale right",
    "start": "2534880",
    "end": "2542000"
  },
  {
    "text": "and so what's interesting there is it puts it puts me in a position of um well how how much time can we can",
    "start": "2542000",
    "end": "2549200"
  },
  {
    "text": "we get away with and in production maybe we'd be willing to run a pipeline over or process a mapreduce process over the",
    "start": "2549200",
    "end": "2555839"
  },
  {
    "text": "night and suffer maybe six hours one time or whatnot that gets interesting because then your developers",
    "start": "2555839",
    "end": "2562240"
  },
  {
    "text": "get frustrated because they don't want to necessarily wait six hours to get a pipeline return so then what are the trade-offs there",
    "start": "2562240",
    "end": "2568400"
  },
  {
    "text": "right you might want to throw more give your developers more resources or find a way for them to test and develop with less data and it gets into",
    "start": "2568400",
    "end": "2575280"
  },
  {
    "text": "a trickier trade-off space so i just want to call out the punch line here is that it's not a straight cost comparison for",
    "start": "2575280",
    "end": "2580480"
  },
  {
    "text": "us it's actually a scale cost and latency comparison and we wind",
    "start": "2580480",
    "end": "2585599"
  },
  {
    "text": "up in a pretty attractive place at 50 minutes and those cost numbers now this is instance cost we also should",
    "start": "2585599",
    "end": "2593280"
  },
  {
    "text": "talk about data storage costs because at scale there's a lot of data involved the good news there",
    "start": "2593280",
    "end": "2598880"
  },
  {
    "start": "2597000",
    "end": "2597000"
  },
  {
    "text": "is it turns out to be relatively cheap because we can do a lot on s3 and most of our data are facts in this",
    "start": "2598880",
    "end": "2605440"
  },
  {
    "text": "example we're only talking about 50 gig of facts but you want to store all your facts the facts",
    "start": "2605440",
    "end": "2610800"
  },
  {
    "text": "are our primary data source from which everything else is generated they're the crown jewels if you will the source of truth of everything",
    "start": "2610800",
    "end": "2616800"
  },
  {
    "text": "and the thing is we can we can just use s3 for that because the only thing that",
    "start": "2616800",
    "end": "2621839"
  },
  {
    "text": "consumes those facts are resources for amazon data pipeline we don't need them to be indexed so we can we can have very cheap storage",
    "start": "2621839",
    "end": "2629200"
  },
  {
    "text": "there five dollars a month for just this 50 gig set of fx now your analytics documents are a different",
    "start": "2629200",
    "end": "2635440"
  },
  {
    "text": "story they need to be quickly indexed so they need to go into a real database so but the benefit there is they're",
    "start": "2635440",
    "end": "2641839"
  },
  {
    "text": "typically aggregated so we're talking about less data so in this pipeline the 50 gig of fax",
    "start": "2641839",
    "end": "2647040"
  },
  {
    "text": "translates to 20 gig of analytic stocks which is a bit more expensive 25",
    "start": "2647040",
    "end": "2652079"
  },
  {
    "text": "250 a month but that's mostly instance cost finally there's all the",
    "start": "2652079",
    "end": "2657760"
  },
  {
    "text": "data in between there's a lot of intermediate data that's generated in these pipeline flows",
    "start": "2657760",
    "end": "2663040"
  },
  {
    "text": "and we retain them for diagnosis in this case this pipeline generates a little bit over a terabyte",
    "start": "2663040",
    "end": "2669440"
  },
  {
    "text": "of data that we store all on s3 goes back 60 days that's to be honest much more than we",
    "start": "2669440",
    "end": "2675280"
  },
  {
    "text": "need the point is storage on s3 is relatively cheap and we can afford to store",
    "start": "2675280",
    "end": "2680720"
  },
  {
    "text": "most everything that we want to so that's how we use amazon data pipeline",
    "start": "2680720",
    "end": "2686240"
  },
  {
    "text": "to have a simple horizontal scaling story run all of our pipelines in 50 minutes",
    "start": "2686240",
    "end": "2691920"
  },
  {
    "text": "and store all of our data the impact of aws data pipeline cuts across",
    "start": "2691920",
    "end": "2699359"
  },
  {
    "text": "product development and operations we're able to build build new things",
    "start": "2699359",
    "end": "2704960"
  },
  {
    "text": "build them quickly and we're able to scale and maintain those features easily after we ship them now",
    "start": "2704960",
    "end": "2713440"
  },
  {
    "text": "we adopted amazon data pipeline before we really needed to we got ahead of our big data in that",
    "start": "2714319",
    "end": "2719760"
  },
  {
    "text": "sense and uh before our data grew to be unmanageable and we're in hindsight we're very glad",
    "start": "2719760",
    "end": "2725599"
  },
  {
    "text": "we did that we've talked about a few of the benefits here but i have to tell you guys that the biggest benefit",
    "start": "2725599",
    "end": "2731599"
  },
  {
    "text": "is that now when we look at a new data opportunity instead of the feeling of dread that we used to",
    "start": "2731599",
    "end": "2737119"
  },
  {
    "text": "get about how much effort it would take and pain it would take to handle that new data we now have a",
    "start": "2737119",
    "end": "2743280"
  },
  {
    "text": "feeling of excitement about what we can do with that data",
    "start": "2743280",
    "end": "2747838"
  }
]