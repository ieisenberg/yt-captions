[
  {
    "start": "0",
    "end": "361000"
  },
  {
    "text": "[Music] hello everyone my name is rob hegarida",
    "start": "340",
    "end": "6960"
  },
  {
    "text": "and i'm a principal technical account manager located in austin texas welcome to aws supports you where we share best",
    "start": "6960",
    "end": "13519"
  },
  {
    "text": "practices and troubleshooting tips with aws joining me today is mahath from aws support can you give us a quick",
    "start": "13519",
    "end": "19600"
  },
  {
    "text": "introduction sure thanks rob good day folks my name is mahant jayadeva i'm a solutions",
    "start": "19600",
    "end": "25359"
  },
  {
    "text": "architect on the aws as well architecture team i've been with aws for a little over six years now i spend a",
    "start": "25359",
    "end": "30720"
  },
  {
    "text": "lot of my time working with customers and partners helping them adopt best practices for uh designing and operating",
    "start": "30720",
    "end": "36320"
  },
  {
    "text": "workloads on the cloud great thanks mahatma introduction so today we'll be introducing you to the",
    "start": "36320",
    "end": "41360"
  },
  {
    "text": "audience to some architectural patterns to help improve workload resiliency leveraging sharding and shuffle sharding",
    "start": "41360",
    "end": "47840"
  },
  {
    "text": "which are technologies that enable aws itself to deliver cost-effective multi-tenant services mahaf will go into",
    "start": "47840",
    "end": "53600"
  },
  {
    "text": "depth on these technologies today but before we get into the details a quick note to the attendees online please feel",
    "start": "53600",
    "end": "59520"
  },
  {
    "text": "free to use the chat window on the right hand side of your screen to ask your questions and share your thoughts throughout today's episode and let us",
    "start": "59520",
    "end": "65518"
  },
  {
    "text": "know where you're joining us from today uh we look forward to hearing from you all so muham can you go ahead and give",
    "start": "65519",
    "end": "71360"
  },
  {
    "text": "us a walk through we're going to talk about today yeah uh thanks rob uh so quick discussion of",
    "start": "71360",
    "end": "77759"
  },
  {
    "text": "the agenda so we're going to talk about the impact of failures on workloads and how you know what are some traditional",
    "start": "77759",
    "end": "84000"
  },
  {
    "text": "methods we use to mitigate those impacts then we're going to talk about sharding and how sharding can improve the",
    "start": "84000",
    "end": "90079"
  },
  {
    "text": "resiliency of workloads and then we're going to talk about how shuffle sharding can again further improve that",
    "start": "90079",
    "end": "95439"
  },
  {
    "text": "resiliency and we're going to close off with some implementation conversations about how you can actually build this",
    "start": "95439",
    "end": "100799"
  },
  {
    "text": "out yourself and again throughout the session we'll have we're going to have pauses for questions so if you have any",
    "start": "100799",
    "end": "106000"
  },
  {
    "text": "questions please drop them into the chat window so uh this is a code by our uh cto dr",
    "start": "106000",
    "end": "114079"
  },
  {
    "text": "vernon vogels a lot of you if you're familiar with aws you've been to reinvent or watch some of the keynotes uh you'd have heard dr vogel say uh",
    "start": "114079",
    "end": "121520"
  },
  {
    "text": "everything fails all the time which is true but what he uh follows that up with",
    "start": "121520",
    "end": "126719"
  },
  {
    "text": "is that we need to build systems that embrace this failure as a natural occurrence so we shouldn't really be",
    "start": "126719",
    "end": "132319"
  },
  {
    "text": "afraid of failure failures are common failures happen we need to be in a position to embrace these failures and",
    "start": "132319",
    "end": "139840"
  },
  {
    "text": "work through any failures that might happen as part of natural occurrences within our systems right so",
    "start": "139840",
    "end": "146000"
  },
  {
    "text": "what you see here is a very traditional three-tier architecture that a lot of you might be familiar with this is one",
    "start": "146000",
    "end": "151920"
  },
  {
    "text": "of the most common architectural patterns for a very long time and it's a very good architectural pattern as well",
    "start": "151920",
    "end": "157840"
  },
  {
    "text": "so we have uh resources deployed across three different availability zones so and we have three different tiers so we",
    "start": "157840",
    "end": "164879"
  },
  {
    "text": "have the data tier so that's where we're getting incoming requests so that's our front end if you will so we have an",
    "start": "164879",
    "end": "170400"
  },
  {
    "text": "elastic load balancer that's deployed across three availability zones that's where requests come in uh once requests",
    "start": "170400",
    "end": "175760"
  },
  {
    "text": "come in those are forwarded to our application layer so here we have a group of ec2 instances that are part of",
    "start": "175760",
    "end": "181280"
  },
  {
    "text": "an auto scaling group also deployed across three availability zones and finally we have our uh databases right",
    "start": "181280",
    "end": "187760"
  },
  {
    "text": "so we have a multi-az rds database so we have a primary deployed in one",
    "start": "187760",
    "end": "192959"
  },
  {
    "text": "availability zone a standby and a second availability zone and you could have one or more read replicas depending on you",
    "start": "192959",
    "end": "199280"
  },
  {
    "text": "know what your use case is and this is really nice so if you know one of those azs would go down or you had some sort",
    "start": "199280",
    "end": "204720"
  },
  {
    "text": "of an issue uh you have infrastructure in other availability zones that can absorb the incoming traffic and your",
    "start": "204720",
    "end": "211280"
  },
  {
    "text": "application or your workload continues working as expected right so and this is a very robust way to define and build up",
    "start": "211280",
    "end": "218400"
  },
  {
    "text": "workloads now the issue with that is so i'm going to so let's say the previous",
    "start": "218400",
    "end": "223519"
  },
  {
    "text": "like the traditional architecture uh we can represent that in a more simplistic way by this image here so we have",
    "start": "223519",
    "end": "230879"
  },
  {
    "text": "customers at the top so the chess pieces the animal logo so those are all uh representing our customers and then",
    "start": "230879",
    "end": "237280"
  },
  {
    "text": "below the square box we have a set of nodes and worker nodes right these could be ec2 instances these could be sqs",
    "start": "237280",
    "end": "243439"
  },
  {
    "text": "queues these could be databases really any resource that's under contention so in this situation let's say these are",
    "start": "243439",
    "end": "249280"
  },
  {
    "text": "ec2 instances now again these are deployed across multiple availability zones following all the",
    "start": "249280",
    "end": "254799"
  },
  {
    "text": "best practices awesome now let's say there's a situation where a user intentionally or unintentionally",
    "start": "254799",
    "end": "261440"
  },
  {
    "text": "makes a bad request to your application right so it causes an issue with one of your nodes that",
    "start": "261440",
    "end": "266479"
  },
  {
    "text": "receive the request so it could be an expensive request it could be a request that triggered a bug within your system",
    "start": "266479",
    "end": "271840"
  },
  {
    "text": "essentially something that broke the functionality of your workload now that note goes down now as users just",
    "start": "271840",
    "end": "278320"
  },
  {
    "text": "generally if i'm trying to access an application on a website and if it doesn't load what is the logical next",
    "start": "278320",
    "end": "284000"
  },
  {
    "text": "step i take i refresh the page right so and that's what this use of the cat in this situation does as well right so the",
    "start": "284000",
    "end": "290720"
  },
  {
    "text": "cad retries the request again it's still a bad request so the request goes to a second note that second note also goes",
    "start": "290720",
    "end": "297680"
  },
  {
    "text": "down and the cat continues doing this and before you know it the entire fleet",
    "start": "297680",
    "end": "302800"
  },
  {
    "text": "is down right so that bad request is what we refer to as a poison pill a request that triggers an unexpected",
    "start": "302800",
    "end": "309199"
  },
  {
    "text": "unwanted behavior within your system uh and the process of retrying it again",
    "start": "309199",
    "end": "314320"
  },
  {
    "text": "it's a very common pattern of you know backing off and retrying requests that's what we call a retry storm in the",
    "start": "314320",
    "end": "320320"
  },
  {
    "text": "situation because the cat retried that same bad request so many times the entire fleet went down so now all other",
    "start": "320320",
    "end": "327919"
  },
  {
    "text": "customers are also impacted because there are no resources that can respond to requests",
    "start": "327919",
    "end": "333120"
  },
  {
    "text": "in this case the scope of impact is a hundred percent every single user of that system is now",
    "start": "333120",
    "end": "339440"
  },
  {
    "text": "impacted they can no longer access that's that system right really bad situation you never want to see this",
    "start": "339440",
    "end": "344960"
  },
  {
    "text": "much red on the screen right uh so can we do better how can we improve this how can we uh figure out a way to design our",
    "start": "344960",
    "end": "352800"
  },
  {
    "text": "workload so that uh we're resistant to or you know we're resilient against these site these types of situations",
    "start": "352800",
    "end": "358800"
  },
  {
    "text": "such as poison pills and retry styles so one approach you can take is sharding",
    "start": "358800",
    "end": "364400"
  },
  {
    "start": "361000",
    "end": "954000"
  },
  {
    "text": "now sharding again is a concept that you might have heard a lot uh particularly with databases that's where it's most",
    "start": "364400",
    "end": "370800"
  },
  {
    "text": "commonly done where you have a database and you create multiple shards of it and you have a little bit of data in each of",
    "start": "370800",
    "end": "376639"
  },
  {
    "text": "these shards and depending on what data you're looking for your requests are sent to a specific chart right so this",
    "start": "376639",
    "end": "382000"
  },
  {
    "text": "is a very common architectural pattern when it comes to databases now you can apply the same concept at your",
    "start": "382000",
    "end": "389039"
  },
  {
    "text": "application tier as well and not just for your databases so in this case the same workload uh what if we created",
    "start": "389039",
    "end": "396160"
  },
  {
    "text": "shards instead of just having this sort of flat architecture right it's a distributed horizontally distributed",
    "start": "396160",
    "end": "401440"
  },
  {
    "text": "architecture but what if we were a little more deterministic of how we do it so in this case we can create shards",
    "start": "401440",
    "end": "407680"
  },
  {
    "text": "where we assign each of our users each customer of ours we assign them two different nodes right so in this case",
    "start": "407680",
    "end": "414720"
  },
  {
    "text": "the rook and the knight have the first two nodes the bishop and the queen have the next two nodes so on and so forth",
    "start": "414720",
    "end": "421120"
  },
  {
    "text": "so let's see what happens when we design our architecture when when we design our workload to be this way right so the cat",
    "start": "421120",
    "end": "428400"
  },
  {
    "text": "comes along again or it could be any other users i don't want to pick on the cat but just for the sake of you know",
    "start": "428400",
    "end": "434000"
  },
  {
    "text": "continuity let's say the cat came along again uh made that same expensive request or made that bad request and",
    "start": "434000",
    "end": "439919"
  },
  {
    "text": "that node went down now just like the last time the cat retries the request the request is handled by the other",
    "start": "439919",
    "end": "446160"
  },
  {
    "text": "healthy node and that node also goes down now two of the eight notes that we have available within our fleet are no",
    "start": "446160",
    "end": "452479"
  },
  {
    "text": "longer functional they can no longer serve uh requests uh now who does this impact well we can see that the way we",
    "start": "452479",
    "end": "460000"
  },
  {
    "text": "have created these shards again we said we're going to assign each user two nodes so the cat has two",
    "start": "460000",
    "end": "466000"
  },
  {
    "text": "nodes and that shard is being shared with the cat by the dog so the dog is",
    "start": "466000",
    "end": "471360"
  },
  {
    "text": "also impacted because the dog has no healthy notes that it can send requests to however",
    "start": "471360",
    "end": "477280"
  },
  {
    "text": "because we created shards this means all requests coming in from the cat can only",
    "start": "477280",
    "end": "482560"
  },
  {
    "text": "go to that shard and that shot consists of those two nodes that are no longer available",
    "start": "482560",
    "end": "487599"
  },
  {
    "text": "any additional requests coming in from the cat can never go to any of the other nodes because we have created that",
    "start": "487599",
    "end": "494240"
  },
  {
    "text": "virtual isolation boundary so that the other nodes are not impacted so all the other nodes are good they're healthy",
    "start": "494240",
    "end": "500400"
  },
  {
    "text": "they're awesome so the other customers can continue accessing the application continue working the uh working with the uh",
    "start": "500400",
    "end": "507440"
  },
  {
    "text": "application without any impact whatsoever and this is what sharding does is very similar to how you would do it in uh",
    "start": "507440",
    "end": "513120"
  },
  {
    "text": "with databases uh so the scope of impact in the situation now because we lost two",
    "start": "513120",
    "end": "518320"
  },
  {
    "text": "instances right or two nodes and that was essentially an entire shard all the",
    "start": "518320",
    "end": "523919"
  },
  {
    "text": "users on that shard are now impacted now in this situation we have the cat and the dog as the only two users on that",
    "start": "523919",
    "end": "530560"
  },
  {
    "text": "chart now this could be many more users also mapped that shard but all that impact is localized to that specific",
    "start": "530560",
    "end": "537600"
  },
  {
    "text": "shard so in this case the impact is only scope of impact is only 25 right so with the traditional",
    "start": "537600",
    "end": "543839"
  },
  {
    "text": "architecture we had a 100 impact whereas in this situation we only have 25",
    "start": "543839",
    "end": "549040"
  },
  {
    "text": "but because of the other healthy nodes the other healthy shards uh the other customers can continue accessing the",
    "start": "549040",
    "end": "554959"
  },
  {
    "text": "application as expected uh now how does this typically work on aws now again this might be very common",
    "start": "554959",
    "end": "561760"
  },
  {
    "text": "to many of you uh let's say you have eight worker nodes or ec2 instances in this case and it's usually fronted by",
    "start": "561760",
    "end": "567440"
  },
  {
    "text": "some sort of a load balancer so in this case it's an application load balancer uh where incoming traffic hits the load",
    "start": "567440",
    "end": "572720"
  },
  {
    "text": "balancer endpoint and then the application load balancer then forwards or routes that traffic to one of these",
    "start": "572720",
    "end": "579360"
  },
  {
    "text": "ec2 instances and then a response is sent back so this is how a traditional architecture works now with the same",
    "start": "579360",
    "end": "585360"
  },
  {
    "text": "sort of technology the same services and again the same sort of fleet size",
    "start": "585360",
    "end": "590560"
  },
  {
    "text": "how can we implement sharding well to implement sharding what we can actually use is we can use target groups",
    "start": "590560",
    "end": "597440"
  },
  {
    "text": "as virtual shards and what we can do is we can assign ec to instances to target groups and then",
    "start": "597440",
    "end": "605200"
  },
  {
    "text": "the really nice thing about application load balancers are is you can create listener rules so listener rules",
    "start": "605200",
    "end": "610800"
  },
  {
    "text": "essentially are rules that you define on how traffic needs to be routed right so",
    "start": "610800",
    "end": "616000"
  },
  {
    "text": "when a request comes in so in this situation or in in this architecture diagram uh you i'm using a listener rule",
    "start": "616000",
    "end": "622720"
  },
  {
    "text": "using a query string so query string that's included as part of the url to access the application is what's",
    "start": "622720",
    "end": "629440"
  },
  {
    "text": "being inspected by the application load balancer now the query string has the name is the key of the query string and the",
    "start": "629440",
    "end": "635600"
  },
  {
    "text": "value is the name of the customer so in this case if the customer's name is alpha or if the customer's name is bravo",
    "start": "635600",
    "end": "641360"
  },
  {
    "text": "the application load balancer recognizes that and because of the listener rules that i've defined it's going to route",
    "start": "641360",
    "end": "648160"
  },
  {
    "text": "that specific request to shard one right which is our target group one and target group one consists of worker one and",
    "start": "648160",
    "end": "655200"
  },
  {
    "text": "worker two similarly if a request comes in and that's from customer echo or foxtrot the application load balancer",
    "start": "655200",
    "end": "661760"
  },
  {
    "text": "sends that request to shard 3 or target group 3 which consists of worker 5 and worker 6. and this is how we're able to",
    "start": "661760",
    "end": "668959"
  },
  {
    "text": "actually create this level of sharding and that way any request so in this situation let's say customer alpha made",
    "start": "668959",
    "end": "674720"
  },
  {
    "text": "a bad request and you know that triggered a bug and shard one goes down so that when we say shard one goes",
    "start": "674720",
    "end": "680880"
  },
  {
    "text": "down that basically means that the request was sent to target group one which has worker one and worker two uh",
    "start": "680880",
    "end": "686959"
  },
  {
    "text": "worker one received the request the first time around tried to process it the bug triggered and the host went down",
    "start": "686959",
    "end": "694000"
  },
  {
    "text": "uh the customer so in this case customer alpha re-tried the request it went to worker two this time because again the",
    "start": "694000",
    "end": "699839"
  },
  {
    "text": "load balancer will always forward traffic to that specific target group uh depending on those listener rules right",
    "start": "699839",
    "end": "706000"
  },
  {
    "text": "so in this case alpha retries request worker 2 goes down now every user on",
    "start": "706000",
    "end": "711040"
  },
  {
    "text": "that shard is impacted so bravo is also impacted however any further request",
    "start": "711040",
    "end": "716800"
  },
  {
    "text": "that customer alpha makes will never go to any of our other target groups because we're controlling how traffic is",
    "start": "716800",
    "end": "723360"
  },
  {
    "text": "routed using those lists and rules on the application load balancer so this is one way you could actually",
    "start": "723360",
    "end": "729040"
  },
  {
    "text": "implement sharding within your own applications to limit the blast radius of any failures that might happen",
    "start": "729040",
    "end": "736000"
  },
  {
    "text": "so yeah that sharding let me pause here and see if you have any questions on this so far",
    "start": "736000",
    "end": "743240"
  },
  {
    "text": "uh rob i think you're on mute oh thanks appreciate that uh be helpful",
    "start": "752639",
    "end": "758560"
  },
  {
    "text": "to get all helped uh so no questions from the audience right now so uh audience members please make sure to",
    "start": "758560",
    "end": "764320"
  },
  {
    "text": "post your questions uh i did have a question for you though that we've seen previous reinvent talks about cell-based",
    "start": "764320",
    "end": "770480"
  },
  {
    "text": "architectures how is this sharding approach different than that yeah uh so",
    "start": "770480",
    "end": "776320"
  },
  {
    "text": "cell-based architectures of bulkhead architectures as what they're more commonly called uh they're a similar",
    "start": "776320",
    "end": "782480"
  },
  {
    "text": "concept as as sharding in this case uh the key difference though is with bulkhead architecture complete you're",
    "start": "782480",
    "end": "789120"
  },
  {
    "text": "creating uh complete stacks of your entire workload and you're creating isolation boundaries within those cells",
    "start": "789120",
    "end": "795600"
  },
  {
    "text": "so let me explain it to you this way so in this case we have shards here which is all good but let's say you know with",
    "start": "795600",
    "end": "801760"
  },
  {
    "text": "traditional applications you might have a database or other resources other micro services that",
    "start": "801760",
    "end": "806880"
  },
  {
    "text": "the application is interacting with so let's say there was a database down here right now all of these workers are still",
    "start": "806880",
    "end": "812639"
  },
  {
    "text": "interacting with the same database now in this situation we assume that the poison pill only affects our worker",
    "start": "812639",
    "end": "818720"
  },
  {
    "text": "nodes now what if the request that the one of the customers made uh corrupts",
    "start": "818720",
    "end": "823839"
  },
  {
    "text": "the database right so at that point even if your application theory is resilient uh if your database fails if you know an",
    "start": "823839",
    "end": "830639"
  },
  {
    "text": "entire trade table was lost or something like that uh your application and all the other users are still impacted uh",
    "start": "830639",
    "end": "837680"
  },
  {
    "text": "with bulkhead architectures or cell-based architectures the way we do it is we create complete copies of the",
    "start": "837680",
    "end": "843760"
  },
  {
    "text": "entire workload so this means all the micro services all the resources all the infrastructure",
    "start": "843760",
    "end": "848959"
  },
  {
    "text": "and we create multiple copies of it so in this case so shard one has",
    "start": "848959",
    "end": "854639"
  },
  {
    "text": "is mapped to customers alpha and bravo now let's say it's a database that's specific to uh that shot right specific",
    "start": "854639",
    "end": "861120"
  },
  {
    "text": "to customers alpha and brow you have another database here you have another database here you have another database",
    "start": "861120",
    "end": "866399"
  },
  {
    "text": "here and again all other resources as well so in that case what each chart becomes essentially a cell because it is",
    "start": "866399",
    "end": "872639"
  },
  {
    "text": "a self-contained fully uh working fully functional copy of your",
    "start": "872639",
    "end": "878160"
  },
  {
    "text": "workload and that's what we call as cells so each one of those would be a cell any failures that happen within a cell",
    "start": "878160",
    "end": "885680"
  },
  {
    "text": "is contained within that cell and it i really mean any type of failure uh is",
    "start": "885680",
    "end": "890880"
  },
  {
    "text": "all contained within a cell and that's create that really hard isolation boundary whereas shuffle sharding you",
    "start": "890880",
    "end": "896720"
  },
  {
    "text": "don't really get that level of redundancy but then again with shuffle charting you're using fewer amounts of",
    "start": "896720",
    "end": "902720"
  },
  {
    "text": "resources right you have uh lesser infrastructure you have to manage so that's the difference between cell-based",
    "start": "902720",
    "end": "907760"
  },
  {
    "text": "architectures and sharding great okay thanks for the clarification there appreciate that so i'll turn it",
    "start": "907760",
    "end": "913120"
  },
  {
    "text": "back over to you to continue through the rest of presentation thank you awesome thank you okay so we talked about sharding we saw",
    "start": "913120",
    "end": "919920"
  },
  {
    "text": "that you know with charting we we were able to uh achieve a a uh blast radius",
    "start": "919920",
    "end": "925120"
  },
  {
    "text": "of about 25 right which is good like you know it's a a quarter of your customers",
    "start": "925120",
    "end": "930720"
  },
  {
    "text": "are impacted but the vast majority are fine but can we do even better right and",
    "start": "930720",
    "end": "935759"
  },
  {
    "text": "that's sort of the uh goal that we have here at 80 base we're always improving everything that we have to offer to our",
    "start": "935759",
    "end": "942240"
  },
  {
    "text": "customers so uh this time around we're going to actually uh bring in some math into how",
    "start": "942240",
    "end": "948320"
  },
  {
    "text": "we create and how we design our uh workload so we're going to see how magical math can actually be",
    "start": "948320",
    "end": "954800"
  },
  {
    "start": "954000",
    "end": "2000000"
  },
  {
    "text": "so this is where we left off last right so we implemented sharding we created four different shards and we mapped",
    "start": "954800",
    "end": "960240"
  },
  {
    "text": "customers to these shards so each customer had access to two different ec2 instances in this case",
    "start": "960240",
    "end": "966240"
  },
  {
    "text": "um so with shuffle charting we're essentially still doing the same thing but we're going to be very specific",
    "start": "966240",
    "end": "972480"
  },
  {
    "text": "about how we assign these uh these instances to our customers right so instead of so this what you see on the",
    "start": "972480",
    "end": "979120"
  },
  {
    "text": "screen is essentially a random distribution where every customer is getting two nodes cool uh what if we",
    "start": "979120",
    "end": "984399"
  },
  {
    "text": "were a little more intentional about how we placed place those customers so as an example right so we have eight customers",
    "start": "984399",
    "end": "991199"
  },
  {
    "text": "and we have eight ec2 instances uh so we're going to take the cat and we're going to assign node 1 and node 3 to the",
    "start": "991199",
    "end": "997600"
  },
  {
    "text": "cat okay we're going to take the dog and we're going to give the dog two notes then we're going to take the rook we're",
    "start": "997600",
    "end": "1003199"
  },
  {
    "text": "going to give two notes to the rook we're going to take the bishop give two notes to the bishop so we're still maintaining that principle",
    "start": "1003199",
    "end": "1009759"
  },
  {
    "text": "of assigning two nodes for every customer so that they all have the same capacity",
    "start": "1009759",
    "end": "1014800"
  },
  {
    "text": "uh so we continue doing this uh the key difference between this approach and what we did previously which was just",
    "start": "1014800",
    "end": "1020720"
  },
  {
    "text": "sharding is the way we've created combinations so if you can see on the screen here there is no combination that",
    "start": "1020720",
    "end": "1028319"
  },
  {
    "text": "repeats every combination you see is a unique combination of nodes that's being assigned to",
    "start": "1028319",
    "end": "1034400"
  },
  {
    "text": "our customers right so every customer has a unique combination of two nodes with no overlap or no repetition now",
    "start": "1034400",
    "end": "1041520"
  },
  {
    "text": "let's see how this actually going is uh going to help us if there's a poisson pin situation right",
    "start": "1041520",
    "end": "1047199"
  },
  {
    "text": "so just like the last time uh let's say the cat comes along the cat's been pretty naughty so",
    "start": "1047199",
    "end": "1052640"
  },
  {
    "text": "cat has assigned worker node one and work with note 3. uh the cat makes a bad request again uh expensive request you",
    "start": "1052640",
    "end": "1060000"
  },
  {
    "text": "know creates a lot of latency it triggers a bug what have you uh and that note goes down uh the cat retries the",
    "start": "1060000",
    "end": "1067039"
  },
  {
    "text": "request and the second note that the cat is assigned to also goes down right so uh again this is the same situation we",
    "start": "1067039",
    "end": "1073679"
  },
  {
    "text": "saw as last time when we were talking about sharding with two or four nodes of our fleet size of eight when two nodes",
    "start": "1073679",
    "end": "1079280"
  },
  {
    "text": "went down uh the difference here is let's see uh who",
    "start": "1079280",
    "end": "1084400"
  },
  {
    "text": "or what other customers are sharing these notes with the cat right so in the previous situation there was a dog that",
    "start": "1084400",
    "end": "1089760"
  },
  {
    "text": "was sharing uh that shard with the cat and when that shard went down the dog was also impacted so in this case the",
    "start": "1089760",
    "end": "1096559"
  },
  {
    "text": "first node is being shared by the bishop right so the bishop and the cat are sharing the first note",
    "start": "1096559",
    "end": "1102000"
  },
  {
    "text": "so but the bishop is also assigned two different nodes and that's the whole point of it like every customer gets two",
    "start": "1102000",
    "end": "1108400"
  },
  {
    "text": "different nodes so in this case the bishop has worker node 1 and worker node 2.",
    "start": "1108400",
    "end": "1114240"
  },
  {
    "text": "now the bishop makes a request it cannot go to worker node 1 because the cat has",
    "start": "1114240",
    "end": "1119360"
  },
  {
    "text": "already taken it down however because worker note 2 is still up and running uh",
    "start": "1119360",
    "end": "1124400"
  },
  {
    "text": "it can respond to the request made by the bishop so the bishop is not impacted now again there might be some",
    "start": "1124400",
    "end": "1130559"
  },
  {
    "text": "degradation or some impairment of the service itself depending on what type of service it is but it is still a",
    "start": "1130559",
    "end": "1136880"
  },
  {
    "text": "significant improvement over a complete outage for that customer right so if the bishop could not interact with the",
    "start": "1136880",
    "end": "1142640"
  },
  {
    "text": "service at all that would be the worst case situation um having some degradation some impairment might be",
    "start": "1142640",
    "end": "1148480"
  },
  {
    "text": "good but again it's very use case dependent so in this case you know thinking about a fairly common use case",
    "start": "1148480",
    "end": "1154320"
  },
  {
    "text": "uh the bishop is still able to get a response from the second node so you know that customer can continue uh",
    "start": "1154320",
    "end": "1160799"
  },
  {
    "text": "functioning as expected uh similarly the other uh",
    "start": "1160799",
    "end": "1166240"
  },
  {
    "text": "the uh for worker note 3 that's assigned to the cat uh that's being shared with the",
    "start": "1166240",
    "end": "1171360"
  },
  {
    "text": "queen uh now the queen also has two different worker notes right so in this case what happens queen makes a request",
    "start": "1171360",
    "end": "1178720"
  },
  {
    "text": "worker 3 has already gone down but worker 4 is up and running so worker 4 can actually send a response to the",
    "start": "1178720",
    "end": "1185120"
  },
  {
    "text": "queen and the queen is again not impacted and this is how with shuffle sharding we're able to create these",
    "start": "1185120",
    "end": "1191679"
  },
  {
    "text": "combinations now again we limited the number of nodes that a single user can interact with so",
    "start": "1191679",
    "end": "1197919"
  },
  {
    "text": "the cat was only assigned two specific nodes in this case worker one and worker three uh and because the cat cannot",
    "start": "1197919",
    "end": "1205440"
  },
  {
    "text": "interact with any of the other nodes so we have four other nodes remaining on the screen here the cat can never make a",
    "start": "1205440",
    "end": "1210960"
  },
  {
    "text": "request to any of these nodes so those nodes will always remain healthy and which means",
    "start": "1210960",
    "end": "1216559"
  },
  {
    "text": "all our other customers can continue interacting with the workload as expected and it's only the cat that is",
    "start": "1216559",
    "end": "1222720"
  },
  {
    "text": "going to be impacted right and this is how shuffle sharding actually works like with the same size fleet uh we have you",
    "start": "1222720",
    "end": "1229919"
  },
  {
    "text": "know uh just by being a little uh in creative in terms of how we actually",
    "start": "1229919",
    "end": "1236080"
  },
  {
    "text": "assign uh shards and nodes to our customers we were able to",
    "start": "1236080",
    "end": "1241760"
  },
  {
    "text": "ensure that only one customer was impacted and the previous case we had two customers that were impacted so in",
    "start": "1241760",
    "end": "1247280"
  },
  {
    "text": "this case the scope of impact will be customers over the number of combinations possible so in this case we",
    "start": "1247280",
    "end": "1252480"
  },
  {
    "text": "have eight different combinations and we have one customer so we get one over eight which is twelve and a half percent",
    "start": "1252480",
    "end": "1257919"
  },
  {
    "text": "right so we went from a hundred percent outage to 25 when we implemented sharding and now with shuffle sharding",
    "start": "1257919",
    "end": "1265120"
  },
  {
    "text": "we have again in improved that to 12 and a half percent",
    "start": "1265120",
    "end": "1270158"
  },
  {
    "text": "so how could we implement shuffle sharding right now again that's an interesting topic so with the same sort",
    "start": "1270400",
    "end": "1276080"
  },
  {
    "text": "of process with an application load balancer you can use target groups to create virtual shards right and assign",
    "start": "1276080",
    "end": "1282320"
  },
  {
    "text": "you can assign these shards to your customers now the difference or how this actually works out is because an ec2",
    "start": "1282320",
    "end": "1290240"
  },
  {
    "text": "instance can belong to more than one target group so in this case target group one has worker one and worker two",
    "start": "1290240",
    "end": "1297120"
  },
  {
    "text": "uh target group two which is shar two has worker two and worker three right so",
    "start": "1297120",
    "end": "1302240"
  },
  {
    "text": "we can see that worker two now belongs to both target group one and target group two and that is how we're saying",
    "start": "1302240",
    "end": "1307840"
  },
  {
    "text": "that okay that worker is being shared by customer alpha and customer bravo so in",
    "start": "1307840",
    "end": "1313520"
  },
  {
    "text": "this case again the routing is happening because of the listener rules that we've created again using the query string as",
    "start": "1313520",
    "end": "1318559"
  },
  {
    "text": "an example or you can use any other supported uh way to you know uh actually create those listener rules so if you",
    "start": "1318559",
    "end": "1325200"
  },
  {
    "text": "look at the listener rule documentation on aws documentation you'll see there are",
    "start": "1325200",
    "end": "1330640"
  },
  {
    "text": "different ways and or different attributes you can use to uh create these shards or create these routing",
    "start": "1330640",
    "end": "1335919"
  },
  {
    "text": "patterns so this goes on and on and on so we're essentially assigning our two nodes to",
    "start": "1335919",
    "end": "1342320"
  },
  {
    "text": "uh each target group and each target group here represents the shard so just like the last time let's say customer",
    "start": "1342320",
    "end": "1348720"
  },
  {
    "text": "alpha makes a bad request uh customer alpha is mapped to worker one and worker two so let's say both of these were to",
    "start": "1348720",
    "end": "1355039"
  },
  {
    "text": "go down um customer bravo comes along customer bravo makes a request sure",
    "start": "1355039",
    "end": "1360480"
  },
  {
    "text": "worker 2 is not available for customer bravo however uh worker 3 is still",
    "start": "1360480",
    "end": "1365520"
  },
  {
    "text": "available so that request the load balancer is going to see that the request is coming from customer browser",
    "start": "1365520",
    "end": "1370880"
  },
  {
    "text": "send it to target group 2 because only worker 3 is the uh is the only healthy node on that target group",
    "start": "1370880",
    "end": "1377840"
  },
  {
    "text": "it's going to get forwarded uh the request is going to get forwarded to worker 3 and worker 3 will provide a response right so this is how you can",
    "start": "1377840",
    "end": "1384960"
  },
  {
    "text": "implement shuffle sharding at the application level uh and and get that massive levels of uh resiliency within",
    "start": "1384960",
    "end": "1390799"
  },
  {
    "text": "your workload um one thing i do want to call out though is you know on the previous",
    "start": "1390799",
    "end": "1395840"
  },
  {
    "text": "screen we saw that we dropped it down to 12 and a half percent and we're doing all this with the same you know sized",
    "start": "1395840",
    "end": "1401840"
  },
  {
    "text": "fleet like we only still have eight ec2 instances we're not you know you don't have to add extra instances we're not",
    "start": "1401840",
    "end": "1408320"
  },
  {
    "text": "you know increasing cost uh so from a purely infrastructure perspective you're",
    "start": "1408320",
    "end": "1413360"
  },
  {
    "text": "still maintaining the same infrastructure there's no additional cost associated with creating target",
    "start": "1413360",
    "end": "1418640"
  },
  {
    "text": "groups so with the same size fleet you can have this uh significantly improved",
    "start": "1418640",
    "end": "1424000"
  },
  {
    "text": "resiliency within your workloads so",
    "start": "1424000",
    "end": "1429679"
  },
  {
    "text": "shuffle charting very interesting topic uh something that i've actually like interacted with a lot uh we actually",
    "start": "1429679",
    "end": "1436080"
  },
  {
    "text": "have a hands-on lab that we've created uh that helps you sort of play around with this and and you know get an idea",
    "start": "1436080",
    "end": "1442720"
  },
  {
    "text": "of how this works how things are configured so the the architecture that you're seeing on the screen right now is uh the exact same thing is available to",
    "start": "1442720",
    "end": "1449440"
  },
  {
    "text": "you as a lab uh self-paced lab so feel free to go through it and check it out and and sort of uh help solidify what",
    "start": "1449440",
    "end": "1455919"
  },
  {
    "text": "you've learned here today so i'm going to pause here for a second and see if we have any questions",
    "start": "1455919",
    "end": "1461679"
  },
  {
    "text": "hey well thanks we do have a question that came into the audience here from uh ik",
    "start": "1461679",
    "end": "1468320"
  },
  {
    "text": "uh can this sharding and shuffle sharding only be used with ec2 instances",
    "start": "1468320",
    "end": "1473679"
  },
  {
    "text": "very good um so not necessarily no uh you can apply these same again so before we started",
    "start": "1473679",
    "end": "1480480"
  },
  {
    "text": "into this we talked about sharding with how it applies to databases so it says the same sort of answer still applies uh",
    "start": "1480480",
    "end": "1486960"
  },
  {
    "text": "you can use these techniques for any resource that's under contention right so if there are multiple users multiple",
    "start": "1486960",
    "end": "1493919"
  },
  {
    "text": "uh whatever you want to call it uh multiple entities trying to interact with ace with the same resource uh you",
    "start": "1493919",
    "end": "1501600"
  },
  {
    "text": "can uh implement these uh techniques right so uh in this case it could these could be instead of ec2 instances these",
    "start": "1501600",
    "end": "1507919"
  },
  {
    "text": "could be databases these could be queues uh really any any resource that's under",
    "start": "1507919",
    "end": "1513200"
  },
  {
    "text": "contention that's being accessed by multiple people uh you know the saying like too many cooks spoil the broth so",
    "start": "1513200",
    "end": "1519360"
  },
  {
    "text": "same situation if there are multiple uh users uh interacting with the same resource there's a higher probability",
    "start": "1519360",
    "end": "1525840"
  },
  {
    "text": "that one of those users could potentially break that right could potentially cause an outage there uh so",
    "start": "1525840",
    "end": "1531200"
  },
  {
    "text": "by limiting that is how you're able to uh increase that resiliency so yep not just for ec2 instances any any resource",
    "start": "1531200",
    "end": "1537840"
  },
  {
    "text": "that's under contention really okay great thanks man uh back over to you awesome thank you",
    "start": "1537840",
    "end": "1544000"
  },
  {
    "text": "cool so now that we've talked a little bit about the concept a little bit about you know we've been through a specific",
    "start": "1544000",
    "end": "1550400"
  },
  {
    "text": "use case we've talked about the architecture uh let's get into the actual math behind this right like again",
    "start": "1550400",
    "end": "1555760"
  },
  {
    "text": "i started off this section by saying uh we're going to use math to influence how we design our workloads and we're going",
    "start": "1555760",
    "end": "1562159"
  },
  {
    "text": "to actually see how that math plays out so for shuffle charting the scope of impact",
    "start": "1562159",
    "end": "1569120"
  },
  {
    "text": "the specific formula is 1 over the number of combinations times 100 right this is how you calculate the scope of",
    "start": "1569120",
    "end": "1576000"
  },
  {
    "text": "impact for a workload that's using shuffle sharding now if we use n as the number of worker",
    "start": "1576000",
    "end": "1582720"
  },
  {
    "text": "nodes so in the previous situation we had eight worker nodes that's what fleet size and then s is the shard size how",
    "start": "1582720",
    "end": "1588720"
  },
  {
    "text": "many workers do we assign per shard so throughout this entire talk",
    "start": "1588720",
    "end": "1594080"
  },
  {
    "text": "we've always had a shard size of two now that could be any number depending on what your use cases you could have a",
    "start": "1594080",
    "end": "1599120"
  },
  {
    "text": "short size of three four five uh depending on what your use cases right so in this case we've used a shard size",
    "start": "1599120",
    "end": "1604880"
  },
  {
    "text": "of two now uh a little bit of a high school math refresher uh the way we calculate combinations of two",
    "start": "1604880",
    "end": "1612159"
  },
  {
    "text": "sets of numbers is we do n factorial over s factorial times n minus s",
    "start": "1612159",
    "end": "1618240"
  },
  {
    "text": "factorial now for anybody that doesn't remember what a factorial is don't worry i'll explain it to you a factorial is",
    "start": "1618240",
    "end": "1625360"
  },
  {
    "text": "basically a product of a number and all of these uh previous numbers all the way",
    "start": "1625360",
    "end": "1631440"
  },
  {
    "text": "down to one so as an example if we were talking about seven factorial seven factorial would be seven times six",
    "start": "1631440",
    "end": "1639440"
  },
  {
    "text": "5 times 4 times 3 times 2 times 1 all the way to 1 right so uh every number",
    "start": "1639440",
    "end": "1646559"
  },
  {
    "text": "smaller than that all the way down to 1 we multiply that together right so it's a product of all of that and that's how",
    "start": "1646559",
    "end": "1651679"
  },
  {
    "text": "we calculate factorials so again if we replace the figures that we",
    "start": "1651679",
    "end": "1657679"
  },
  {
    "text": "used for our example uh with into this formula we get so that we can calculate",
    "start": "1657679",
    "end": "1662720"
  },
  {
    "text": "the combinations as 8 factorial over 2 factorial times 8 minus 2 factorial",
    "start": "1662720",
    "end": "1668640"
  },
  {
    "text": "because again n is the number of worker nodes so we had a fleet size of 8 and the shard size is 2.",
    "start": "1668640",
    "end": "1674799"
  },
  {
    "text": "and if we simplify that equation on the right what we get is 1 over 28 so there's 28",
    "start": "1674799",
    "end": "1680399"
  },
  {
    "text": "possible combinations so one over 28 times 100 gives you about 3.6",
    "start": "1680399",
    "end": "1686399"
  },
  {
    "text": "right so 3.6 is the maximum level of uh or the smallest blast radius you can",
    "start": "1686399",
    "end": "1693360"
  },
  {
    "text": "achieve by using shuffle sharding with a fleet size of eight and a shot size of two now a lot of you might be thinking",
    "start": "1693360",
    "end": "1700640"
  },
  {
    "text": "hold on in the previous slide you said that the scope uh the uh scope of impact",
    "start": "1700640",
    "end": "1705919"
  },
  {
    "text": "was only twelve point five percent the blast radius only twelve point five percent because we saw the cat was affected so one out of eight customers",
    "start": "1705919",
    "end": "1712080"
  },
  {
    "text": "are 12.5 percent but now you're saying 3.6 which one's true well",
    "start": "1712080",
    "end": "1717600"
  },
  {
    "text": "in this case we see that there are 28 possible combinations right so",
    "start": "1717600",
    "end": "1723200"
  },
  {
    "text": "again going back to the example we used uh so we only used eight combinations out of the possible 28 right so we only",
    "start": "1723200",
    "end": "1730480"
  },
  {
    "text": "created eight shuffle shards for the eight different customers we had now let's say we had twenty eight possible",
    "start": "1730480",
    "end": "1736720"
  },
  {
    "text": "customers instead of just eight customers we had 28 customers in that case and again when i say a customer it",
    "start": "1736720",
    "end": "1742240"
  },
  {
    "text": "doesn't have to be a single customer right so it's not you're not creating a shuffle chart for every single customer",
    "start": "1742240",
    "end": "1747919"
  },
  {
    "text": "now there might be situations where you have to or where you choose to which is totally fine but that is just a",
    "start": "1747919",
    "end": "1753600"
  },
  {
    "text": "representation so that is essentially a cluster of uh customers right so in this",
    "start": "1753600",
    "end": "1758720"
  },
  {
    "text": "case you have uh we only used eight possible combinations and that's what yielded twelve point five percent now if",
    "start": "1758720",
    "end": "1765120"
  },
  {
    "text": "we had scaled out all the way and if we had created uh 28 target groups and got essentially",
    "start": "1765120",
    "end": "1771679"
  },
  {
    "text": "28 shuffle shards and assigned those to every single customer that's when",
    "start": "1771679",
    "end": "1777120"
  },
  {
    "text": "a cat the cat comes along creates a failure now is when we see that only three point six percent of those",
    "start": "1777120",
    "end": "1783360"
  },
  {
    "text": "customers are impacted right so this is how the math was so the math is accurate right so don't",
    "start": "1783360",
    "end": "1789120"
  },
  {
    "text": "worry that you know the numbers didn't add up the first time because we only used eight combinations is why we're seeing twelve and a half percent but the",
    "start": "1789120",
    "end": "1796159"
  },
  {
    "text": "theoretical or the the upper limit of how much resiliency you can build using shuffle sharding is three point six",
    "start": "1796159",
    "end": "1802880"
  },
  {
    "text": "percent in this situation and that number grows really really fast",
    "start": "1802880",
    "end": "1808320"
  },
  {
    "text": "uh as the numbers get bigger right so let's say you had a fleet of 100 nodes",
    "start": "1808320",
    "end": "1814240"
  },
  {
    "text": "not it's a fairly recently sized fleet like you know it's not uncommon to see workloads that have 100 ec2 instances or",
    "start": "1814240",
    "end": "1821520"
  },
  {
    "text": "you know pardon me a hundred different worker nodes so in this situation if you have a",
    "start": "1821520",
    "end": "1827039"
  },
  {
    "text": "hundred worker nodes and you created shards with five nodes in each shard",
    "start": "1827039",
    "end": "1832799"
  },
  {
    "text": "and if you take the perspective of a single customer so if you know from the perspective of the cat in this situation",
    "start": "1832799",
    "end": "1838320"
  },
  {
    "text": "um 77 of the customers will never share any nodes",
    "start": "1838320",
    "end": "1844000"
  },
  {
    "text": "with that cat right uh 21 so this is again if you're talking about five nodes",
    "start": "1844000",
    "end": "1849360"
  },
  {
    "text": "per chart so out of those five nodes 77 of those customers will never share any",
    "start": "1849360",
    "end": "1856000"
  },
  {
    "text": "of these nodes with the cat right 21 of those customers will share one node with",
    "start": "1856000",
    "end": "1861440"
  },
  {
    "text": "the cat 1.8 of customers will share two nodes with the cap right so this is how we're",
    "start": "1861440",
    "end": "1867679"
  },
  {
    "text": "creating overlaps uh or the degrees of overlap and if you see all the way down there uh for a overlap of five nodes so",
    "start": "1867679",
    "end": "1875200"
  },
  {
    "text": "this is basically all the customers sharing the exact same five notes as the cat is only point zero zero zero zero",
    "start": "1875200",
    "end": "1881840"
  },
  {
    "text": "zero one three percent very very small number again it's a very reasonably",
    "start": "1881840",
    "end": "1887120"
  },
  {
    "text": "sized infrastructure here uh but your blast radius has gone down significantly",
    "start": "1887120",
    "end": "1892159"
  },
  {
    "text": "by using shuffle sharding um this is actually how we achieved that",
    "start": "1892159",
    "end": "1897440"
  },
  {
    "text": "that incredible sls with route 53. um amazon.53 actually uses shuffle sharding",
    "start": "1897440",
    "end": "1902799"
  },
  {
    "text": "and that's how we're able to ensure that our customers are not impacted so route 53",
    "start": "1902799",
    "end": "1908399"
  },
  {
    "text": "has 2048 worker nodes uh and we create shuffle shards with four worker nodes",
    "start": "1908399",
    "end": "1914799"
  },
  {
    "text": "right so we create so out of the 2048 uh we assign four nodes to a particular chart and we",
    "start": "1914799",
    "end": "1921840"
  },
  {
    "text": "you know repeat uh and if you do the same thing if you do the math and that i showed in the previous slide",
    "start": "1921840",
    "end": "1927440"
  },
  {
    "text": "where we you know calculated the number of combinations and did the scope of impact uh that",
    "start": "1927440",
    "end": "1932640"
  },
  {
    "text": "it's it's an astronomically large number you have a 1 in 730 billion chance of",
    "start": "1932640",
    "end": "1939200"
  },
  {
    "text": "one user's actions impacting you so very very small arts that those are numbers large enough where every domain hosted",
    "start": "1939200",
    "end": "1947279"
  },
  {
    "text": "on route 53 can be assigned its own unique shuffle chart with no overlap",
    "start": "1947279",
    "end": "1952640"
  },
  {
    "text": "whatsoever right and these are techniques that we ourselves use and this is how we're able to provide that",
    "start": "1952640",
    "end": "1959519"
  },
  {
    "text": "really high levels of resiliency within our services i'm going to pause again and see if",
    "start": "1959519",
    "end": "1965200"
  },
  {
    "text": "there are any questions i know we were talked a little bit of math uh it's been a while since we all you know graduated",
    "start": "1965200",
    "end": "1970640"
  },
  {
    "text": "from high school so i'm going to pause you and see if there are any questions hey mom thanks uh no question for theons",
    "start": "1970640",
    "end": "1976960"
  },
  {
    "text": "right now working with one user to just get a little clarity on one but i'll let you continue through on to your next",
    "start": "1976960",
    "end": "1982240"
  },
  {
    "text": "slide for right now thanks very good all right so now that we've talked a little bit about the concepts of you know what",
    "start": "1982240",
    "end": "1988640"
  },
  {
    "text": "sharding is and what shuffle charting is and we've we've actually seen like with actual proof of math right numbers don't",
    "start": "1988640",
    "end": "1994559"
  },
  {
    "text": "lie so we've seen the numbers of how this can um impact your workload and what kind of benefits it can provide so",
    "start": "1994559",
    "end": "2000640"
  },
  {
    "start": "2000000",
    "end": "2888000"
  },
  {
    "text": "let's talk a little bit about how you could go about implementing this right so what are some considerations uh things to keep in mind as you start",
    "start": "2000640",
    "end": "2007840"
  },
  {
    "text": "implementing this within your own workloads the first and most important one is placement right",
    "start": "2007840",
    "end": "2013840"
  },
  {
    "text": "to figure out to calculate those chart to calculate those combinations and identify well you know what nodes are assigned to",
    "start": "2013840",
    "end": "2020240"
  },
  {
    "text": "what chart and you know how do we control overlap what degree of overlap do we want right so that's again",
    "start": "2020240",
    "end": "2025279"
  },
  {
    "text": "something you can control you can say that you know i don't want overlap of more than two right so uh so these are",
    "start": "2025279",
    "end": "2031919"
  },
  {
    "text": "strategies that you want to incorporate to uh improve the resilience here even after in implementing shuffle sharding",
    "start": "2031919",
    "end": "2038799"
  },
  {
    "text": "so the placement is really talking about where do we place uh our workers shots",
    "start": "2038799",
    "end": "2044000"
  },
  {
    "text": "and where do we store that data uh now depending on how you're actually routing traffic you want to be able to access",
    "start": "2044000",
    "end": "2050320"
  },
  {
    "text": "this data access these combinations uh extremely fast uh and and it has to be a",
    "start": "2050320",
    "end": "2055919"
  },
  {
    "text": "very high performing data store so that's where services like amazon aurora or amazon dynamodb come in really handy",
    "start": "2055919",
    "end": "2062878"
  },
  {
    "text": "because these are these services provide that extremely high levels of performance uh where you have those",
    "start": "2062879",
    "end": "2068320"
  },
  {
    "text": "those really low latency lookups so you have your you know your information of",
    "start": "2068320",
    "end": "2073919"
  },
  {
    "text": "what the mappings are where the shards are what uh shards the workers are associated with that information can be",
    "start": "2073919",
    "end": "2080480"
  },
  {
    "text": "stored in either of these databases and whenever you have to forward traffic or route traffic you can actually do a",
    "start": "2080480",
    "end": "2086240"
  },
  {
    "text": "lookup against these data stores and see okay i'm getting a request for this specific customer uh what shard is this",
    "start": "2086240",
    "end": "2092800"
  },
  {
    "text": "customer mapped to and i'm going to route that or forward that request to that specific chart because of the",
    "start": "2092800",
    "end": "2099280"
  },
  {
    "text": "information that's in these data stores right so that's where we're talking about placement and how we're going to organize everything",
    "start": "2099280",
    "end": "2105920"
  },
  {
    "text": "uh next is the router right so how do we actually route traffic um so in the uh",
    "start": "2105920",
    "end": "2111839"
  },
  {
    "text": "the architecture diagram we looked at earlier we did it using an application load balancer and using listener rules",
    "start": "2111839",
    "end": "2117599"
  },
  {
    "text": "so because we used a service like that that was able to both manage the actual combinations or the uh the placements of",
    "start": "2117599",
    "end": "2125040"
  },
  {
    "text": "the uh worker nodes by creating those target groups uh we didn't have to worry about using another standalone database",
    "start": "2125040",
    "end": "2131599"
  },
  {
    "text": "to manage that that was already built into the way we configured the listener rules um and for routing traffic as well",
    "start": "2131599",
    "end": "2137680"
  },
  {
    "text": "we used the application load balancer wave the load balancer would look at the uh incoming request it would look at the",
    "start": "2137680",
    "end": "2143920"
  },
  {
    "text": "query string and depending on you know what the customer name was it was uh routing that that request to a specific",
    "start": "2143920",
    "end": "2150160"
  },
  {
    "text": "target group or a specific shard in this case uh and that's how you could do it at the application level now you could",
    "start": "2150160",
    "end": "2155680"
  },
  {
    "text": "also do shuffle charting at the dns level right so instead of having multiple worker nodes or multiple ec2",
    "start": "2155680",
    "end": "2162079"
  },
  {
    "text": "instances uh imagine you had multiple endpoints right so you had and that's where you can use a combination of",
    "start": "2162079",
    "end": "2168000"
  },
  {
    "text": "cell-based architectures with shuffle charting so uh you can have multiple",
    "start": "2168000",
    "end": "2173520"
  },
  {
    "text": "endpoints and each of these endpoints essentially will become your quote unquote worker node and then at the dns",
    "start": "2173520",
    "end": "2179920"
  },
  {
    "text": "level you create shuffle charts where you set where you can essentially say if request comes in from customer alpha uh",
    "start": "2179920",
    "end": "2186480"
  },
  {
    "text": "that resolves to either endpoint a or endpoint b uh if request comes in from customer bravo that resolves to end",
    "start": "2186480",
    "end": "2193680"
  },
  {
    "text": "point b or endpoint c so you could use it using end points and use route 53 to",
    "start": "2193680",
    "end": "2199359"
  },
  {
    "text": "actually manage that and we actually have a nice library that's written in java it's called the route 53 in fema",
    "start": "2199359",
    "end": "2206079"
  },
  {
    "text": "library so if you go look it up on on github or i think we're going to post a link here on the chat",
    "start": "2206079",
    "end": "2211440"
  },
  {
    "text": "you can actually see how you can use uh that library to manage and implement shuffle shards at the dns level right so",
    "start": "2211440",
    "end": "2217760"
  },
  {
    "text": "uh and again these are services that you can use that that are already available within aws for you to",
    "start": "2217760",
    "end": "2224000"
  },
  {
    "text": "implement shuffle sharding but you could always uh build your own custom router right so let's say you have your own",
    "start": "2224000",
    "end": "2229599"
  },
  {
    "text": "sort of a very lightweight front end that's you know getting requests and that's what's actually uh that's when",
    "start": "2229599",
    "end": "2235040"
  },
  {
    "text": "you need an actual database or a data store to manage all those mappings so you could go",
    "start": "2235040",
    "end": "2240160"
  },
  {
    "text": "read through dynamo find out you know what way the traffic or where the request needs to be forwarded to and",
    "start": "2240160",
    "end": "2245280"
  },
  {
    "text": "your own custom router can then forward that traffic if you choose not to use uh use these services so you can build your",
    "start": "2245280",
    "end": "2250880"
  },
  {
    "text": "own as well uh and last one with shuffle charting uh the control plane now this is this is",
    "start": "2250880",
    "end": "2257599"
  },
  {
    "text": "essentially the cockpit for operating your workload right uh how do you create these shuffle shots like it's not",
    "start": "2257599",
    "end": "2263839"
  },
  {
    "text": "possible to create it by hand right you don't want to be clicking around in the console launching instances or",
    "start": "2263839",
    "end": "2269920"
  },
  {
    "text": "changing listener rules and things like that uh that's where you want to use automation specifically things like",
    "start": "2269920",
    "end": "2275359"
  },
  {
    "text": "infrastructure is code uh so you can use the aws cloud development kit or confirmation to provision your actual",
    "start": "2275359",
    "end": "2281520"
  },
  {
    "text": "resources and you can have them all statically defined in terms of what your shuffle shards are uh but you could also",
    "start": "2281520",
    "end": "2287440"
  },
  {
    "text": "use additional automation to vary the fleet size and do some really fancy things with it um",
    "start": "2287440",
    "end": "2293200"
  },
  {
    "text": "you need to have a good way to deploy uh your application to your actual resources so that's where you want to",
    "start": "2293200",
    "end": "2300079"
  },
  {
    "text": "use some sort of something like code pipeline to be able to automate the process of actually deploying and making sure that uh your",
    "start": "2300079",
    "end": "2306800"
  },
  {
    "text": "your worker nodes or your resources are running the right version of the application for the request that they're serving",
    "start": "2306800",
    "end": "2312560"
  },
  {
    "text": "and the last one uh is monitoring monitoring is like really really key in distributed",
    "start": "2312560",
    "end": "2318720"
  },
  {
    "text": "systems even more so in a shuffle sharded system uh because you created so many different shards and you created",
    "start": "2318720",
    "end": "2325280"
  },
  {
    "text": "the shuffle charge and these mappings of how traffic is being routed for specific customers uh you want to have uh very",
    "start": "2325280",
    "end": "2331599"
  },
  {
    "text": "robust monitoring in place and that's when you can use amazon cloud watch uh if you use the you know if you if you're",
    "start": "2331599",
    "end": "2337920"
  },
  {
    "text": "thinking about implementing it using the application load balancer that i walked to just a few minutes ago",
    "start": "2337920",
    "end": "2343680"
  },
  {
    "text": "that actually becomes extremely easy because you can just have alarm so you can look at metrics at the target group",
    "start": "2343680",
    "end": "2350480"
  },
  {
    "text": "level because remember in that example each target group essentially was a shard so you can use the same concept",
    "start": "2350480",
    "end": "2357200"
  },
  {
    "text": "here and use amazon cloud watch to get those metrics at the target group level which is giving you insight at the shard",
    "start": "2357200",
    "end": "2363359"
  },
  {
    "text": "limit right so observability is really really important so these are some things that you have",
    "start": "2363359",
    "end": "2368400"
  },
  {
    "text": "to keep in mind um as you start you know exploring this further and as you start uh implementing this within your own",
    "start": "2368400",
    "end": "2374160"
  },
  {
    "text": "workloads uh like i mentioned earlier uh i am part",
    "start": "2374160",
    "end": "2380160"
  },
  {
    "text": "of the i'm a solutions architect on the aws well architecture team uh anybody who's not familiar with uh aws well",
    "start": "2380160",
    "end": "2386800"
  },
  {
    "text": "architected uh it's a curated set of best practices that you know aws has learned from the tens of thousands of",
    "start": "2386800",
    "end": "2393200"
  },
  {
    "text": "customer interactions we've had on you know we've seen things that work we've seen best practices being used by",
    "start": "2393200",
    "end": "2399359"
  },
  {
    "text": "customers and and how they're successful using those best practices and we've created a framework uh for you to also",
    "start": "2399359",
    "end": "2405680"
  },
  {
    "text": "learn those best practices and implement them in your own workloads now what you see on the screen here is a little",
    "start": "2405680",
    "end": "2412000"
  },
  {
    "text": "screen grab from the aws well architected tool uh it's again available on the aws console at no additional cost",
    "start": "2412000",
    "end": "2418240"
  },
  {
    "text": "it helps you review your workloads to identify what best practices you're following uh what gaps you might have",
    "start": "2418240",
    "end": "2423920"
  },
  {
    "text": "within your workloads and it gives you guidance on how to implement those best practices so that you no longer are at",
    "start": "2423920",
    "end": "2429440"
  },
  {
    "text": "risk right so this particular question talks about fault isolation to protect your workload so this is in the reliability pillar",
    "start": "2429440",
    "end": "2436480"
  },
  {
    "text": "uh the three best practices that we see on screen here is deploy the workload to multiple locations um automate recovery",
    "start": "2436480",
    "end": "2443200"
  },
  {
    "text": "for components constrained to a single location and using bulkhead architectures or cell-based",
    "start": "2443200",
    "end": "2448720"
  },
  {
    "text": "architectures that we talked about earlier now how could you implement these these best practices on aws well to deploy and",
    "start": "2448720",
    "end": "2455839"
  },
  {
    "text": "want your workload across multiple locations uh aws opera offers 25 regions",
    "start": "2455839",
    "end": "2461599"
  },
  {
    "text": "and 81 availability zones worldwide with more being added all the time so you",
    "start": "2461599",
    "end": "2466960"
  },
  {
    "text": "have a plethora of options in terms of where you actually want to deploy your workload or how many different locations",
    "start": "2466960",
    "end": "2473119"
  },
  {
    "text": "you want to deploy your workload to right uh the next one was automating uh or creating self healing",
    "start": "2473119",
    "end": "2480400"
  },
  {
    "text": "capabilities uh that's where services like auto scaling come in really handy if one of your ec2 engines became",
    "start": "2480400",
    "end": "2486720"
  },
  {
    "text": "unhealthy for whatever reason auto scaling would recognize that and it would replace that instance with the new",
    "start": "2486720",
    "end": "2492480"
  },
  {
    "text": "healthy instance so that your your function your application can continue working as expected",
    "start": "2492480",
    "end": "2497680"
  },
  {
    "text": "the same thing goes with rds databases so if you launch a rds database as a",
    "start": "2497680",
    "end": "2504000"
  },
  {
    "text": "multi-az database it actually creates a primary and a standby like we saw in in",
    "start": "2504000",
    "end": "2509839"
  },
  {
    "text": "the earlier slide we talked about traditional architectures you have a primary understand by and if the primary",
    "start": "2509839",
    "end": "2515520"
  },
  {
    "text": "way to fade uh the rds service is going to take care of failing over to the standby so that your application can",
    "start": "2515520",
    "end": "2521440"
  },
  {
    "text": "continue working uninterrupted uh to create bulkier architectures",
    "start": "2521440",
    "end": "2526560"
  },
  {
    "text": "infrastructure as code is your friend again doing this by hand is not possible or it's not really recommended uh so you",
    "start": "2526560",
    "end": "2533040"
  },
  {
    "text": "can use infrastructure as code you can use cloud information to create those different cells and have those bulkhead architectures uh created for your",
    "start": "2533040",
    "end": "2539280"
  },
  {
    "text": "workload um along with all these best practices uh based on today's talk you have a new",
    "start": "2539280",
    "end": "2545520"
  },
  {
    "text": "arrow in your quiver so you can use shuffle charting in addition to all of these uh so that you have the highest",
    "start": "2545520",
    "end": "2551440"
  },
  {
    "text": "level of resiliency possible uh within your workload right so and again uh one thing i do want to emphasize it's not",
    "start": "2551440",
    "end": "2558079"
  },
  {
    "text": "just a one-and-done thing you cannot implement shuffle charting and expect it to solve all of your problems uh you",
    "start": "2558079",
    "end": "2563520"
  },
  {
    "text": "want to have you know just like you uh have the defense in-depth approach where",
    "start": "2563520",
    "end": "2568720"
  },
  {
    "text": "you have uh multiple layers of security so that your application is protected the same concept applies to reliability",
    "start": "2568720",
    "end": "2575040"
  },
  {
    "text": "there are different types of failures that can impact your workload and different best practices are going to",
    "start": "2575040",
    "end": "2580240"
  },
  {
    "text": "work together uh they were cohesively to give you that really high levels of reliability within your workload",
    "start": "2580240",
    "end": "2587760"
  },
  {
    "text": "um cool so let's pause you see if there are any questions uh schwab",
    "start": "2587760",
    "end": "2594160"
  },
  {
    "text": "hey maha thanks i do have a couple cool questions here that came up from the audience and one comes from dw bledsoe",
    "start": "2594160",
    "end": "2600240"
  },
  {
    "text": "and hopefully i get your question right dw bledsoe uh if you use more nodes to make the percentage lower as you showed",
    "start": "2600240",
    "end": "2606079"
  },
  {
    "text": "for the kind of the fault tolerance there at a resiliency uh would this help to take stress off the nodes over using",
    "start": "2606079",
    "end": "2611680"
  },
  {
    "text": "a smaller amount of nodes essentially kind of would increase your performance then as well um",
    "start": "2611680",
    "end": "2618400"
  },
  {
    "text": "kind of so now you're talking really about shard size is what really determines or or the levels of overlap",
    "start": "2618400",
    "end": "2623680"
  },
  {
    "text": "is what really determines how much stress you're putting on nodes uh yes like the default is you know the",
    "start": "2623680",
    "end": "2629520"
  },
  {
    "text": "fewer requests the node has to serve uh the less stress there is on it so the better performance right uh and the same",
    "start": "2629520",
    "end": "2636240"
  },
  {
    "text": "sort of concept applies here if you're sort of reaching a point where your nodes are actually maxing out then yes",
    "start": "2636240",
    "end": "2642160"
  },
  {
    "text": "even if you did this with the traditional architecture pardon me even with the traditional",
    "start": "2642160",
    "end": "2647440"
  },
  {
    "text": "architecture if the number of requests are overwhelming the number of or the infrastructure that you have you have to",
    "start": "2647440",
    "end": "2654160"
  },
  {
    "text": "spin up new infrastructure uh now with shuffle charting it's not really the number of nodes that you're going to",
    "start": "2654160",
    "end": "2659440"
  },
  {
    "text": "want to increase it's the amount of overlap but the more overlap you create uh the higher chances of failure having",
    "start": "2659440",
    "end": "2666319"
  },
  {
    "text": "a height so you're essentially increasing the blast radius as well so it's a trade-off that you have to make",
    "start": "2666319",
    "end": "2671440"
  },
  {
    "text": "there um if you want to you know you obviously want to optimize for performance uh and because it's the same",
    "start": "2671440",
    "end": "2678800"
  },
  {
    "text": "shard size for every customer if there's one customer that's creating a lot of noise it might be worthwhile putting",
    "start": "2678800",
    "end": "2684560"
  },
  {
    "text": "them on a separate chart just by themselves so that they can send as many requests than you want to",
    "start": "2684560",
    "end": "2691040"
  },
  {
    "text": "great thanks mom uh and the next question comes from phil e hw uh what is",
    "start": "2691040",
    "end": "2697119"
  },
  {
    "text": "the recovery strategy we're starting if a client client say brings down their assigned nodes with bad requests is that",
    "start": "2697119",
    "end": "2703520"
  },
  {
    "text": "client doomed essentially to wait until the nose recover or is there an easier recommended kind of you know recovery",
    "start": "2703520",
    "end": "2708560"
  },
  {
    "text": "path uh so typically again depending on how uh when we're talking about recovery",
    "start": "2708560",
    "end": "2714640"
  },
  {
    "text": "little different types of recovery strategies so in this situation um if there is something like auto scaling",
    "start": "2714640",
    "end": "2719920"
  },
  {
    "text": "that's being used then if the request comes in you know those notes went down uh those clients will essentially have",
    "start": "2719920",
    "end": "2726160"
  },
  {
    "text": "to wait before the remediation takes place now if there isn't any you know automated capability that's when",
    "start": "2726160",
    "end": "2732000"
  },
  {
    "text": "monitoring comes in handy and then um as operators as as owners of the workload we would have to then go uh make a",
    "start": "2732000",
    "end": "2738480"
  },
  {
    "text": "change or make the necessary changes to be able to fix that situation so that uh those customers also have functionality",
    "start": "2738480",
    "end": "2744319"
  },
  {
    "text": "returned to them right uh and it's the same process for any type of outage that happens uh any",
    "start": "2744319",
    "end": "2751119"
  },
  {
    "text": "outage that happens there are different types of recoveries either that automated recovery or manual recovery so",
    "start": "2751119",
    "end": "2756160"
  },
  {
    "text": "those processes don't necessarily change what does change here with shuffle sharding is the amount of customers are",
    "start": "2756160",
    "end": "2762640"
  },
  {
    "text": "impacted by your failure right so that's that's what we're really focusing on failures are you know natural they happen all the time",
    "start": "2762640",
    "end": "2769119"
  },
  {
    "text": "but how can we constrain that so that failures only limit a small set of customers is sort of the intent behind",
    "start": "2769119",
    "end": "2775200"
  },
  {
    "text": "shuffle sharding yeah makes sense all right thanks for additional information is there anything else that you wanted to share at all today before we go",
    "start": "2775200",
    "end": "2782160"
  },
  {
    "text": "um talking about again like there are a bunch of resources that we've already shared with you on twitch chat again",
    "start": "2782160",
    "end": "2788480"
  },
  {
    "text": "please check out the original builder's library article that talks about shuffle charting uh that was written by our vp",
    "start": "2788480",
    "end": "2794720"
  },
  {
    "text": "uh mccarthy and he also talks about how they implemented that with route 53 uh",
    "start": "2794720",
    "end": "2800480"
  },
  {
    "text": "we also have a hands-on lab definitely check that out it gives you it helps solidify what you've learned here today",
    "start": "2800480",
    "end": "2806079"
  },
  {
    "text": "um and you know as you start working through this and as you start thinking about how you're going to implement this",
    "start": "2806079",
    "end": "2811119"
  },
  {
    "text": "uh there are algorithms out there that you can use that will make this a lot easier uh consistent hashing is one of",
    "start": "2811119",
    "end": "2816560"
  },
  {
    "text": "those things that you could potentially use uh so check out consistent hashing how that works and that's going to",
    "start": "2816560",
    "end": "2822319"
  },
  {
    "text": "make managing these shards a lot easier great thanks and i did post a lot of those links into the chat for the",
    "start": "2822319",
    "end": "2828000"
  },
  {
    "text": "audience so please take a look through those to uh see the various uh links and such to go out and read so",
    "start": "2828000",
    "end": "2833920"
  },
  {
    "text": "uh overall everyone today we introduce you to some architectural patterns to help improve workload workload resiliency leveraging sharding and",
    "start": "2833920",
    "end": "2840319"
  },
  {
    "text": "shuffle sharding specifically uh if there are any questions that were not answered today you can post your questions on repost.aws",
    "start": "2840319",
    "end": "2847040"
  },
  {
    "text": "uh and email us any feedback to aws supports you at amazon.com we want to hear from you so please let us know what",
    "start": "2847040",
    "end": "2853599"
  },
  {
    "text": "else you would like to see on the show and just a note to our audience we will be off next monday an observance of",
    "start": "2853599",
    "end": "2858880"
  },
  {
    "text": "martin luther king jr day but if you tune in on wednesday the 19th at 11 a.m pacific time for an episode on iem best",
    "start": "2858880",
    "end": "2865520"
  },
  {
    "text": "practices uh we'll be here then so thanks for joining us at aws supports you and happy cloud computing everyone",
    "start": "2865520",
    "end": "2873380"
  },
  {
    "text": "[Music]",
    "start": "2873380",
    "end": "2885328"
  },
  {
    "text": "you",
    "start": "2887599",
    "end": "2889680"
  }
]