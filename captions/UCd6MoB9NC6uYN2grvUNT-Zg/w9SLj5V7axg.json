[
  {
    "start": "0",
    "end": "31000"
  },
  {
    "text": "[Music]",
    "start": "410",
    "end": "11999"
  },
  {
    "text": "hello i'm nemisha a cloud support",
    "start": "12799",
    "end": "15599"
  },
  {
    "text": "engineer here at the aws office in",
    "start": "15599",
    "end": "18400"
  },
  {
    "text": "bangalore today i am going to show you",
    "start": "18400",
    "end": "21119"
  },
  {
    "text": "how you can resolve the error opening",
    "start": "21119",
    "end": "23439"
  },
  {
    "text": "hive split 503 s3 slow down error in",
    "start": "23439",
    "end": "27199"
  },
  {
    "text": "amazon athena",
    "start": "27199",
    "end": "28800"
  },
  {
    "text": "let's get started",
    "start": "28800",
    "end": "31039"
  },
  {
    "start": "31000",
    "end": "70000"
  },
  {
    "text": "we usually face the 503 s3 slow down",
    "start": "31039",
    "end": "34399"
  },
  {
    "text": "error when we exceed the per second per",
    "start": "34399",
    "end": "37680"
  },
  {
    "text": "prefix limits in an amazon simple",
    "start": "37680",
    "end": "40079"
  },
  {
    "text": "storage service bucket by default we can",
    "start": "40079",
    "end": "43440"
  },
  {
    "text": "send up to 3500 put or copy or post or",
    "start": "43440",
    "end": "48239"
  },
  {
    "text": "delete requests or",
    "start": "48239",
    "end": "50600"
  },
  {
    "text": "5500 get or head requests per second per",
    "start": "50600",
    "end": "55120"
  },
  {
    "text": "prefix to an s3 bucket if you receive",
    "start": "55120",
    "end": "58719"
  },
  {
    "text": "503 slow down errors during the first",
    "start": "58719",
    "end": "61600"
  },
  {
    "text": "few runs of the query this could also be",
    "start": "61600",
    "end": "64720"
  },
  {
    "text": "because of the gradual scale up nature",
    "start": "64720",
    "end": "67200"
  },
  {
    "text": "of s3 requests",
    "start": "67200",
    "end": "70080"
  },
  {
    "start": "70000",
    "end": "114000"
  },
  {
    "text": "to resolve this error we can undertake",
    "start": "70080",
    "end": "73040"
  },
  {
    "text": "one of the following three methods",
    "start": "73040",
    "end": "75680"
  },
  {
    "text": "first",
    "start": "75680",
    "end": "76640"
  },
  {
    "text": "let's consider distributing the s3",
    "start": "76640",
    "end": "79200"
  },
  {
    "text": "objects and the requests made among",
    "start": "79200",
    "end": "82240"
  },
  {
    "text": "multiple prefixes",
    "start": "82240",
    "end": "84080"
  },
  {
    "text": "to do so partition your data in s3",
    "start": "84080",
    "end": "88479"
  },
  {
    "text": "the second method is to reduce the",
    "start": "88479",
    "end": "91200"
  },
  {
    "text": "number of files in s3 by merging small",
    "start": "91200",
    "end": "94400"
  },
  {
    "text": "files to reduce the number of requests",
    "start": "94400",
    "end": "97360"
  },
  {
    "text": "made",
    "start": "97360",
    "end": "98320"
  },
  {
    "text": "for example",
    "start": "98320",
    "end": "99680"
  },
  {
    "text": "we can use the s3 dist cp tool on an",
    "start": "99680",
    "end": "103200"
  },
  {
    "text": "amazon emr cluster to merge a large",
    "start": "103200",
    "end": "106399"
  },
  {
    "text": "number of small files",
    "start": "106399",
    "end": "108399"
  },
  {
    "text": "here are the steps to run s3 disk cp on",
    "start": "108399",
    "end": "111439"
  },
  {
    "text": "your emr cluster",
    "start": "111439",
    "end": "114159"
  },
  {
    "start": "114000",
    "end": "143000"
  },
  {
    "text": "first log in to the aws management",
    "start": "114159",
    "end": "116960"
  },
  {
    "text": "console and then navigate to the amazon",
    "start": "116960",
    "end": "119840"
  },
  {
    "text": "emr console",
    "start": "119840",
    "end": "122560"
  },
  {
    "text": "then create a new cluster according to",
    "start": "122560",
    "end": "125520"
  },
  {
    "text": "your requirements or select your already",
    "start": "125520",
    "end": "128720"
  },
  {
    "text": "running cluster",
    "start": "128720",
    "end": "130720"
  },
  {
    "text": "the process to create a cluster will",
    "start": "130720",
    "end": "133200"
  },
  {
    "text": "take around 7 to 10 minutes",
    "start": "133200",
    "end": "136640"
  },
  {
    "text": "when the cluster is in the waiting stage",
    "start": "136640",
    "end": "139360"
  },
  {
    "text": "select connect to the master node using",
    "start": "139360",
    "end": "141920"
  },
  {
    "text": "ssh",
    "start": "141920",
    "end": "143040"
  },
  {
    "start": "143000",
    "end": "188000"
  },
  {
    "text": "and enter this command on the terminal",
    "start": "143040",
    "end": "145280"
  },
  {
    "text": "of your system to establish a connection",
    "start": "145280",
    "end": "148160"
  },
  {
    "text": "with the emr master node",
    "start": "148160",
    "end": "151360"
  },
  {
    "text": "when you have connected into the master",
    "start": "151360",
    "end": "153519"
  },
  {
    "text": "node using ssh successfully run an s3",
    "start": "153519",
    "end": "157200"
  },
  {
    "text": "disk cp command to merge files based on",
    "start": "157200",
    "end": "160319"
  },
  {
    "text": "your requirements for example mys3",
    "start": "160319",
    "end": "163920"
  },
  {
    "text": "bucket has a number of csv files that i",
    "start": "163920",
    "end": "166959"
  },
  {
    "text": "want to merge based on their names",
    "start": "166959",
    "end": "169519"
  },
  {
    "text": "and make sure that if possible the",
    "start": "169519",
    "end": "172319"
  },
  {
    "text": "merged files are greater than 128 mb",
    "start": "172319",
    "end": "176000"
  },
  {
    "text": "each",
    "start": "176000",
    "end": "177040"
  },
  {
    "text": "in this case i'll run the following",
    "start": "177040",
    "end": "179440"
  },
  {
    "text": "command",
    "start": "179440",
    "end": "181200"
  },
  {
    "text": "as you can see my csv files have been",
    "start": "181200",
    "end": "184080"
  },
  {
    "text": "merged",
    "start": "184080",
    "end": "186000"
  },
  {
    "text": "here is the third method",
    "start": "186000",
    "end": "188080"
  },
  {
    "text": "use the cloudwatch 5xx errors metric and",
    "start": "188080",
    "end": "192319"
  },
  {
    "text": "s3 server access logs to find if other",
    "start": "192319",
    "end": "195840"
  },
  {
    "text": "applications such as spark or glue were",
    "start": "195840",
    "end": "199599"
  },
  {
    "text": "using the same s3 prefix when you ran",
    "start": "199599",
    "end": "202319"
  },
  {
    "text": "the athena query",
    "start": "202319",
    "end": "204319"
  },
  {
    "text": "if you see an anomaly in these logs then",
    "start": "204319",
    "end": "207519"
  },
  {
    "text": "avoid scheduling applications to access",
    "start": "207519",
    "end": "210400"
  },
  {
    "text": "the same s3 prefix at the same time",
    "start": "210400",
    "end": "213920"
  },
  {
    "text": "for example if the logs show you that",
    "start": "213920",
    "end": "217040"
  },
  {
    "text": "you received the error for less number",
    "start": "217040",
    "end": "219599"
  },
  {
    "text": "of requests then consider providing",
    "start": "219599",
    "end": "222400"
  },
  {
    "text": "back-offs between subsequent queries to",
    "start": "222400",
    "end": "225200"
  },
  {
    "text": "allow s3 to scale up gradually",
    "start": "225200",
    "end": "228879"
  },
  {
    "text": "so now you know how to resolve the error",
    "start": "228879",
    "end": "231680"
  },
  {
    "text": "opening hive split 503 s3 slow down",
    "start": "231680",
    "end": "235120"
  },
  {
    "text": "error in athena",
    "start": "235120",
    "end": "237439"
  },
  {
    "text": "thanks for watching and happy cloud",
    "start": "237439",
    "end": "239519"
  },
  {
    "text": "computing from all of us here at aws",
    "start": "239519",
    "end": "243350"
  },
  {
    "text": "[Music]",
    "start": "243350",
    "end": "249569"
  }
]