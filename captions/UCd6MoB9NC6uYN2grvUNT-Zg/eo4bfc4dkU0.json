[
  {
    "text": "alright I think we'll get started I realize I'm between you and the",
    "start": "260",
    "end": "6569"
  },
  {
    "text": "reception and the drinks so I think we'll get started and move things along here so my name is Keith Stewart I'm a",
    "start": "6569",
    "end": "14540"
  },
  {
    "text": "specialist solution architect I'm specializing in our EMR elastic",
    "start": "14540",
    "end": "19890"
  },
  {
    "text": "mapreduce platform this presentation is titled interactively querying large-scale data sets on Amazon s3 on an",
    "start": "19890",
    "end": "30869"
  },
  {
    "text": "alternative title here is presto on EMR so what I'm going to do is cover these",
    "start": "30869",
    "end": "37590"
  },
  {
    "text": "points we'll start by looking at the challenges that are typically faced when",
    "start": "37590",
    "end": "44430"
  },
  {
    "text": "using traditional data warehouses to to query large-scale data sets and then",
    "start": "44430",
    "end": "51809"
  },
  {
    "text": "we'll contrast that with a alternative non data warehouse approach and that's",
    "start": "51809",
    "end": "58379"
  },
  {
    "text": "that this non data warehouse approach is is the focus of the rest of the",
    "start": "58379",
    "end": "63600"
  },
  {
    "text": "presentation will next look at the high-level steps that you would go",
    "start": "63600",
    "end": "68729"
  },
  {
    "text": "through to be able to query large-scale data sets that are living on s3 look at",
    "start": "68729",
    "end": "74640"
  },
  {
    "text": "some of the components of this alternative solution specifically amazon s3 will drill down a little bit into",
    "start": "74640",
    "end": "81330"
  },
  {
    "text": "that EMR and how it can work with s3 apache presto which is the secret sauce",
    "start": "81330",
    "end": "88920"
  },
  {
    "text": "for querying large data sets will look at how presto can can be deployed on EMR",
    "start": "88920",
    "end": "96659"
  },
  {
    "text": "very easily and at the end we'll run a demo and show you how we can take this",
    "start": "96659",
    "end": "101899"
  },
  {
    "text": "29-year dataset US air air flight data from from the Department of",
    "start": "101899",
    "end": "107310"
  },
  {
    "text": "Transportation and be able to have that data on s3 and be able to run a query",
    "start": "107310",
    "end": "112560"
  },
  {
    "text": "across this very large data set so you'll be able to see it in action all",
    "start": "112560",
    "end": "118770"
  },
  {
    "text": "right so what when querying data on a",
    "start": "118770",
    "end": "123979"
  },
  {
    "text": "data warehouse there's a number of challenges that you face in particular",
    "start": "123979",
    "end": "130050"
  },
  {
    "text": "the the the whole premise behind data warehouses is it that you're",
    "start": "130050",
    "end": "136400"
  },
  {
    "text": "implementing a schema on right what that means is you're taking the the variety",
    "start": "136400",
    "end": "142950"
  },
  {
    "text": "of input data that you want to later be able to query and you're processing that",
    "start": "142950",
    "end": "148680"
  },
  {
    "text": "in a significant way to structure it in order to write it to the data warehouse",
    "start": "148680",
    "end": "153900"
  },
  {
    "text": "to the schema in a highly highly defined fairly rigid schema in the data",
    "start": "153900",
    "end": "160380"
  },
  {
    "text": "warehouse and there's a lot of work to do that and once the data is in the data warehouse then it's available to",
    "start": "160380",
    "end": "167870"
  },
  {
    "text": "business or bi people data scientists etc but the implications of this model",
    "start": "167870",
    "end": "176220"
  },
  {
    "text": "though are that you're using specialized often proprietary data technology the",
    "start": "176220",
    "end": "184530"
  },
  {
    "text": "data warehouses and because of all the effort involved in taking the date all",
    "start": "184530",
    "end": "190710"
  },
  {
    "text": "the data and trying to track tit transform it and put it in a form that",
    "start": "190710",
    "end": "196740"
  },
  {
    "text": "can fit into your universal schema there's a significant time delay before",
    "start": "196740",
    "end": "201960"
  },
  {
    "text": "people have access to that data to be able to query it also because it's a",
    "start": "201960",
    "end": "207350"
  },
  {
    "text": "well it's a tends to be more expensive because it's a proprietary solutions the",
    "start": "207350",
    "end": "214260"
  },
  {
    "text": "data warehouse also the schema because you're using this Universal schema to",
    "start": "214260",
    "end": "221280"
  },
  {
    "text": "store and manage all the data that puts certain constraints on the types of",
    "start": "221280",
    "end": "226770"
  },
  {
    "text": "queries that you can you can execute against the data okay so so the schema",
    "start": "226770",
    "end": "232350"
  },
  {
    "text": "on right this is model for data warehouses is is the older the older",
    "start": "232350",
    "end": "238350"
  },
  {
    "text": "approach to querying large data an alternative to this is a schema on read",
    "start": "238350",
    "end": "245340"
  },
  {
    "text": "and so in this case what you're doing is taking diverse sets of data lots of",
    "start": "245340",
    "end": "252480"
  },
  {
    "text": "different data sets and immediately storing it okay in its raw original",
    "start": "252480",
    "end": "260280"
  },
  {
    "text": "high-fidelity format so you're taking all this data and immediately storing it",
    "start": "260280",
    "end": "265400"
  },
  {
    "text": "in a distributed fashion in a cluster and typically that's an open-source",
    "start": "265400",
    "end": "272690"
  },
  {
    "text": "Hadoop cluster and then on an as-needed basis depending on the query you know on",
    "start": "272690",
    "end": "280520"
  },
  {
    "text": "a query by query basis I subset of that data will be read during a query in its",
    "start": "280520",
    "end": "290360"
  },
  {
    "text": "raw original form read and structured on the fly to satisfy the query okay so",
    "start": "290360",
    "end": "296900"
  },
  {
    "text": "that's the schema on read so you're deferring the structuring of the data until you actually execute the query",
    "start": "296900",
    "end": "304360"
  },
  {
    "text": "okay and so there's a lot less work because you're typically looking at",
    "start": "304360",
    "end": "309860"
  },
  {
    "text": "subsets of the data and you're doing doing less work on the data because you",
    "start": "309860",
    "end": "315919"
  },
  {
    "text": "don't have have this Universal schema that you're trying to pigeonhole the data okay because it's open source",
    "start": "315919",
    "end": "323810"
  },
  {
    "text": "Hadoop the cost is dramatically lower than a data warehouse solution which is",
    "start": "323810",
    "end": "329720"
  },
  {
    "text": "a specialized proprietary and because the data once it's in the once it's",
    "start": "329720",
    "end": "335630"
  },
  {
    "text": "stored in the distributed cluster it's immediately available to data scientists",
    "start": "335630",
    "end": "340849"
  },
  {
    "text": "to be able to work with and without any further delay so so the benefits are",
    "start": "340849",
    "end": "346159"
  },
  {
    "text": "shorter time to answer higher you know higher agility lower cost and because",
    "start": "346159",
    "end": "353449"
  },
  {
    "text": "you've got the original high fidelity data all of that data in its original",
    "start": "353449",
    "end": "359060"
  },
  {
    "text": "form immediately available there's flexibility in the types of queries that",
    "start": "359060",
    "end": "364580"
  },
  {
    "text": "the data scientists can can ask they can query the same data in a variety of ways",
    "start": "364580",
    "end": "369669"
  },
  {
    "text": "because there isn't this rigid schema that that's organizing it all right so",
    "start": "369669",
    "end": "378110"
  },
  {
    "text": "at a high level what's it take to take large-scale data that's in s 32 and",
    "start": "378110",
    "end": "384590"
  },
  {
    "text": "being able to query it in a high-performance fashion there's basically six high-level steps first of",
    "start": "384590",
    "end": "391190"
  },
  {
    "text": "all you put your data in s3 and we'll look at a variety of ways in which that it can happen you configure and launch an EMR cluster",
    "start": "391190",
    "end": "399300"
  },
  {
    "text": "with presto you log into the cluster and then you tell hive the hive meta service",
    "start": "399300",
    "end": "407430"
  },
  {
    "text": "you tell it about the data that it already lives on as three you expose",
    "start": "407430",
    "end": "412650"
  },
  {
    "text": "those those tables or expose that data as tables once that's done then you can",
    "start": "412650",
    "end": "419430"
  },
  {
    "text": "issue queries sequel queries using presto against that data okay so you're",
    "start": "419430",
    "end": "426240"
  },
  {
    "text": "not you're you're declaring or exposing the existing data in s3 you're not",
    "start": "426240",
    "end": "432030"
  },
  {
    "text": "having to transform it or copy it or anything like that it's simply in a declaration step so you should the",
    "start": "432030",
    "end": "439410"
  },
  {
    "text": "sequel queries and you get your results so fairly straightforward so let's talk",
    "start": "439410",
    "end": "445320"
  },
  {
    "text": "about some of the components to this alternative to data warehouses so that the key component really is amazon s3",
    "start": "445320",
    "end": "454280"
  },
  {
    "text": "amazon s3 is a massively parallel object store it can store anything from a",
    "start": "454280",
    "end": "461190"
  },
  {
    "text": "single bite txt file to a five terabyte video file so can it's very very diverse",
    "start": "461190",
    "end": "468060"
  },
  {
    "text": "and what it can store it's scalable and an elastic so whatever amount of data",
    "start": "468060",
    "end": "474270"
  },
  {
    "text": "you have and you send to s3 it'll accommodate it you don't need to pre provision capacity you don't need to",
    "start": "474270",
    "end": "481020"
  },
  {
    "text": "worry about growth in your data s3 will automatically handle whatever new data",
    "start": "481020",
    "end": "486620"
  },
  {
    "text": "you send to it it's highly durable 11",
    "start": "486620",
    "end": "491880"
  },
  {
    "text": "nines of durability so you don't need to worry about losing your data it has effectively an infinite inbound",
    "start": "491880",
    "end": "498420"
  },
  {
    "text": "bandwidth as I said it's massively parallel and it's extremely low cost",
    "start": "498420",
    "end": "504030"
  },
  {
    "text": "this really changes the economics of large-scale data management and querying",
    "start": "504030",
    "end": "509430"
  },
  {
    "text": "it's a 3 cents per gigabyte month or about 30 bucks per terabyte month and as",
    "start": "509430",
    "end": "517229"
  },
  {
    "text": "you'll see s3 is kind of a foundational service it was the earliest AWS service",
    "start": "517230",
    "end": "524640"
  },
  {
    "text": "but its foundation for a whole host of other AWS services and it's been compared to the",
    "start": "524640",
    "end": "531630"
  },
  {
    "text": "the O'Hara airport in terms of being able to interface with all kinds of",
    "start": "531630",
    "end": "537839"
  },
  {
    "text": "other services it's a hub really for all kinds of data in addition to those",
    "start": "537839",
    "end": "544949"
  },
  {
    "text": "capabilities s3 also has things like lifecycle management so if you're",
    "start": "544949",
    "end": "550769"
  },
  {
    "text": "accumulating data on a continual basis and it's growing rapidly you can",
    "start": "550769",
    "end": "556620"
  },
  {
    "text": "establish threat policies bucket policies or lifecycle policies we're",
    "start": "556620",
    "end": "563480"
  },
  {
    "text": "beyond a certain threshold of age the data will automatically be moved out of",
    "start": "563480",
    "end": "569490"
  },
  {
    "text": "s3 and into cold storage our glacier AWS glacier storage and that gives you even",
    "start": "569490",
    "end": "576089"
  },
  {
    "text": "an even lower cost profile that's about seven tenths of a cent per gigabyte",
    "start": "576089",
    "end": "581880"
  },
  {
    "text": "month so you can set these policies up and they'll take care of any any old",
    "start": "581880",
    "end": "587370"
  },
  {
    "text": "data that needs to be held on to but but not necessarily kept an active active",
    "start": "587370",
    "end": "593600"
  },
  {
    "text": "queryable form and there are the things like triggers and and so on AES 256-bit",
    "start": "593600",
    "end": "601470"
  },
  {
    "text": "encryption both at rest and in transit whole host of capabilities for s3 so as",
    "start": "601470",
    "end": "609000"
  },
  {
    "text": "I said s3 is a foundational service for a lot of other AWS services you can",
    "start": "609000",
    "end": "615690"
  },
  {
    "text": "exchange data both reading and writing from all these different services we'll take a look at EMR specifically but",
    "start": "615690",
    "end": "622860"
  },
  {
    "text": "Kinesis firehose as a way to get data from somewhere in the internet into the",
    "start": "622860",
    "end": "628620"
  },
  {
    "text": "cloud and into s3 in a hike highly parallel sharded fashion data pipeline",
    "start": "628620",
    "end": "637079"
  },
  {
    "text": "can you can orchestrate workflows that move data among different AWS services",
    "start": "637079",
    "end": "642829"
  },
  {
    "text": "including s3 redshift the data warehouse",
    "start": "642829",
    "end": "648230"
  },
  {
    "text": "offering no sequel databases like dynamo DB relational services RDS and as we",
    "start": "648230",
    "end": "656730"
  },
  {
    "text": "announced last last snowball import/export snowball get are",
    "start": "656730",
    "end": "662380"
  },
  {
    "text": "these appliances 80 terabyte appliances that we can ship to customers and they",
    "start": "662380",
    "end": "668620"
  },
  {
    "text": "plug into their network copy their data locally empty onto the appliance and then have that FedEx to AWS and put into",
    "start": "668620",
    "end": "676900"
  },
  {
    "text": "the data put into s3 for them and of course spark streaming and other things",
    "start": "676900",
    "end": "682720"
  },
  {
    "text": "like storm can interface with so s 3 is a very versatile you you can think of it as future proofing your data once it's",
    "start": "682720",
    "end": "689440"
  },
  {
    "text": "an s3 there's a variety of things that you can do with your data and we refer",
    "start": "689440",
    "end": "696940"
  },
  {
    "text": "to that as the data lake the data lake is this schema on read you take your",
    "start": "696940",
    "end": "702460"
  },
  {
    "text": "data from a wide variety of sources and diverse formats and store that in high",
    "start": "702460",
    "end": "709630"
  },
  {
    "text": "fidelity form in s3 so what does it take to expose that data once it's an s3 so",
    "start": "709630",
    "end": "717820"
  },
  {
    "text": "that you can do the these high-performance queries well this is a little snippet of a screenshot from the",
    "start": "717820",
    "end": "724870"
  },
  {
    "text": "AWS web console yeah in s3 in particular and I've got this data set flight delays",
    "start": "724870",
    "end": "733440"
  },
  {
    "text": "and I've got a path that shows that this this is where I've stored CSV formatted",
    "start": "733440",
    "end": "739540"
  },
  {
    "text": "data and I've included some additional path information this year equals some",
    "start": "739540",
    "end": "746800"
  },
  {
    "text": "value 1987 in this case doing that in",
    "start": "746800",
    "end": "752380"
  },
  {
    "text": "other words spreading your data across s three partitions leveraging distinct",
    "start": "752380",
    "end": "760500"
  },
  {
    "text": "information in the path helps you maximize the parallel lism of s3 and",
    "start": "760500",
    "end": "766450"
  },
  {
    "text": "avoid any kind of hot spots the more entropy the more uniqueness you can have",
    "start": "766450",
    "end": "771760"
  },
  {
    "text": "in the path to the object the better chance that it runs in a less separate",
    "start": "771760",
    "end": "779440"
  },
  {
    "text": "i/o process so it maximizes the parallel ilysm at any rate this is a sample file",
    "start": "779440",
    "end": "786070"
  },
  {
    "text": "here the CSV it's been compressed with visa compressing data on s3 is a best",
    "start": "786070",
    "end": "792540"
  },
  {
    "text": "practice in addition to partitioning that allows it minimizes the aisle time",
    "start": "792540",
    "end": "800940"
  },
  {
    "text": "because data can can move back and forth much more quickly and you know if you if",
    "start": "800940",
    "end": "809160"
  },
  {
    "text": "you think about s3 is a massively parallel object store with lots of parallel lism and you've got cluster",
    "start": "809160",
    "end": "817020"
  },
  {
    "text": "technology EMR for example the nodes in the EMR cluster can query that object",
    "start": "817020",
    "end": "824220"
  },
  {
    "text": "store in parallel so you so you maximize the parallel ISM and so these best",
    "start": "824220",
    "end": "830820"
  },
  {
    "text": "practices of a partitioning and compressing the data can really help and",
    "start": "830820",
    "end": "837350"
  },
  {
    "text": "you can also transform the data into more efficient formats our demo will",
    "start": "837350",
    "end": "844020"
  },
  {
    "text": "actually instead of using CSV will take advantage of parquet and parquet is a",
    "start": "844020",
    "end": "850560"
  },
  {
    "text": "very popular column-oriented datos data format what that means is that when you",
    "start": "850560",
    "end": "856440"
  },
  {
    "text": "perform queries if you have sequel queries that have selects on certain",
    "start": "856440",
    "end": "861510"
  },
  {
    "text": "columns only though the data in those columns are touched that's in contrast",
    "start": "861510",
    "end": "867420"
  },
  {
    "text": "to a traditional row oriented data data organization where the entire row has to",
    "start": "867420",
    "end": "873750"
  },
  {
    "text": "be handled in order to get the data that you're asking for so column-oriented is particularly good for wide data tables",
    "start": "873750",
    "end": "882360"
  },
  {
    "text": "wide wide sets of fields and that's",
    "start": "882360",
    "end": "887730"
  },
  {
    "text": "another best practice alright so once the data is an s3 how do you make it",
    "start": "887730",
    "end": "893130"
  },
  {
    "text": "known and available to other systems well you can do that by running hive and",
    "start": "893130",
    "end": "901460"
  },
  {
    "text": "you can run a create external table statement so this is a ddl and what",
    "start": "901460",
    "end": "908850"
  },
  {
    "text": "you're doing is essentially declaring that the that this Pat this location this",
    "start": "908850",
    "end": "917570"
  },
  {
    "text": "bucket and path that the flight delay CSV in this case is going to be",
    "start": "917570",
    "end": "926170"
  },
  {
    "text": "interpreted as a table even though they're flat files and they're compressed it'll treat it as a table and",
    "start": "926170",
    "end": "934130"
  },
  {
    "text": "and so the way you do that is you do the create external table declare the",
    "start": "934130",
    "end": "940459"
  },
  {
    "text": "attributes that correspond to to the fields you indicate something about the",
    "start": "940459",
    "end": "946370"
  },
  {
    "text": "format of the flat flat file in this case it's a comma separated so we're indicating it's separated by commas",
    "start": "946370",
    "end": "953000"
  },
  {
    "text": "there's some escapes and the records are terminated by new lines and most",
    "start": "953000",
    "end": "958130"
  },
  {
    "text": "importantly we're specifying that this table lives at an s3 URI in this case",
    "start": "958130",
    "end": "964100"
  },
  {
    "text": "flight delays CSV so that when executed within hive declares that that data as",
    "start": "964100",
    "end": "971899"
  },
  {
    "text": "being accessible as a table in this case",
    "start": "971899",
    "end": "977270"
  },
  {
    "text": "we call it air delays and you can prove that it exists by asking hi I've to",
    "start": "977270",
    "end": "983420"
  },
  {
    "text": "describe that table and so you'll get results coming back like this all right",
    "start": "983420",
    "end": "993770"
  },
  {
    "text": "let's talk about EMR so EMR is AWS is Hadoop as a service it gives you",
    "start": "993770",
    "end": "1001089"
  },
  {
    "text": "scalable Hadoop clusters that you can scale in a variety of ways you can pick",
    "start": "1001089",
    "end": "1008140"
  },
  {
    "text": "different ec2 family types you can have memory optimized for things like like",
    "start": "1008140",
    "end": "1014140"
  },
  {
    "text": "spark and and and presto as well if you have a lot of number crunching",
    "start": "1014140",
    "end": "1021160"
  },
  {
    "text": "statistical calculations machine learning those kinds of things you can use a cpu optimized ec2 if you have a",
    "start": "1021160",
    "end": "1029290"
  },
  {
    "text": "lot of data that that's disk intensive you can use i/o optimized so there's",
    "start": "1029290",
    "end": "1035438"
  },
  {
    "text": "different family ec2 family types that you can leverage you can use different ec2 sizes different numbers of cores",
    "start": "1035439",
    "end": "1043058"
  },
  {
    "text": "different amounts of ram etc and you can you can scale the cluster to",
    "start": "1043059",
    "end": "1049600"
  },
  {
    "text": "whatever size is appropriate to your workload we have a variety of applications that are available through",
    "start": "1049600",
    "end": "1056350"
  },
  {
    "text": "a mere checking of checkboxes and you'll see that shortly with 15 different",
    "start": "1056350",
    "end": "1063130"
  },
  {
    "text": "applications currently and we continue to add new Hadoop applications to that and as you can see presto is one of the",
    "start": "1063130",
    "end": "1070630"
  },
  {
    "text": "applications that you can readily deploy it's a managed service so you have the",
    "start": "1070630",
    "end": "1078130"
  },
  {
    "text": "you know the the EMR automation takes care of spinning up the ec2 instances",
    "start": "1078130",
    "end": "1085720"
  },
  {
    "text": "that are part of your cluster it takes care of all the wiring the configuration",
    "start": "1085720",
    "end": "1090790"
  },
  {
    "text": "the communication among those and also will monitor the health of the nodes in",
    "start": "1090790",
    "end": "1098020"
  },
  {
    "text": "the a.m and the Duke cluster and any node that dies EMR will automatically",
    "start": "1098020",
    "end": "1103720"
  },
  {
    "text": "replace that with another node and even if a node is is still live but it's very slow the EMR will detect that as well",
    "start": "1103720",
    "end": "1111880"
  },
  {
    "text": "and replace it with a much more active one and in in the process it will",
    "start": "1111880",
    "end": "1117030"
  },
  {
    "text": "redistribute jobs to handle and restart jobs to be able to recover from any any",
    "start": "1117030",
    "end": "1123100"
  },
  {
    "text": "failures in the node you can pay for it in a variety of ways you can pay with a",
    "start": "1123100",
    "end": "1130120"
  },
  {
    "text": "traditional on-demand ec tues with reserved instances and then spot pricing",
    "start": "1130120",
    "end": "1135790"
  },
  {
    "text": "spot pricing is a real bargain you can often achieve an eighty to ninety",
    "start": "1135790",
    "end": "1140890"
  },
  {
    "text": "percent discount compared to on-demand ec2 instances and leveraging spot within",
    "start": "1140890",
    "end": "1146800"
  },
  {
    "text": "EMR is as simple as in typing in the bid price that you're prepared to pay and",
    "start": "1146800",
    "end": "1152200"
  },
  {
    "text": "that's at EMR will take care of spinning up though grabbing those instances and",
    "start": "1152200",
    "end": "1157810"
  },
  {
    "text": "making them available to your cluster there's a variety of persistence or data",
    "start": "1157810",
    "end": "1163420"
  },
  {
    "text": "sources that EMR will work with H of course HDFS the traditional Hadoop",
    "start": "1163420",
    "end": "1169420"
  },
  {
    "text": "datastore most importantly is s3 but we",
    "start": "1169420",
    "end": "1174490"
  },
  {
    "text": "also now have the EBS volumes that you can Don on EMR nodes and for those workloads",
    "start": "1174490",
    "end": "1184780"
  },
  {
    "text": "that need to be PCI or HIPAA compliant we have end-to-end security available",
    "start": "1184780",
    "end": "1192010"
  },
  {
    "text": "everything from encryption of the host discs of course s3 where the data lives",
    "start": "1192010",
    "end": "1198610"
  },
  {
    "text": "is aes-256 encrypted both in transit and in at rest one of the most unique and",
    "start": "1198610",
    "end": "1209620"
  },
  {
    "text": "powerful features of EMR is a component called EMR FS so EMR FS is the file",
    "start": "1209620",
    "end": "1218050"
  },
  {
    "text": "system it's the abstraction layer that allows Hadoop applications to talk to a",
    "start": "1218050",
    "end": "1224350"
  },
  {
    "text": "variety of persistence stores but most especially s3 so EMR FS is optimized for",
    "start": "1224350",
    "end": "1232090"
  },
  {
    "text": "s3 it gives a high-performance reads and writes direct reads and writes to to s3",
    "start": "1232090",
    "end": "1239880"
  },
  {
    "text": "it's not it's not copying the data to HDFS what it's doing is actually reading",
    "start": "1239880",
    "end": "1247870"
  },
  {
    "text": "and writing directly to s3 so from s3 to the CPU memory and back of course",
    "start": "1247870",
    "end": "1255300"
  },
  {
    "text": "intermediate temporary results can be written to HDFS particularly for",
    "start": "1255300",
    "end": "1261370"
  },
  {
    "text": "MapReduce applications using s3 within EMR within the Hadoop on the mr is as",
    "start": "1261370",
    "end": "1268870"
  },
  {
    "text": "simple as changing the protocol in the URI so you're you our eyes use s 3 colon",
    "start": "1268870",
    "end": "1275920"
  },
  {
    "text": "slash slash instead of HDFS that's it once the data is on this 3 you can you",
    "start": "1275920",
    "end": "1281290"
  },
  {
    "text": "can access it with that minor change EMR",
    "start": "1281290",
    "end": "1286390"
  },
  {
    "text": "will work with the encryption that s 3 provides both server side or client side",
    "start": "1286390",
    "end": "1293190"
  },
  {
    "text": "there's a variety of ways in which you can manage the keys it's optimized for",
    "start": "1293190",
    "end": "1298690"
  },
  {
    "text": "listing of this listing of the the metadata for the objects on s3 so we've",
    "start": "1298690",
    "end": "1306070"
  },
  {
    "text": "done a lot of work under the hood to make that and high performance and you can still",
    "start": "1306070",
    "end": "1312679"
  },
  {
    "text": "use HDFS if you want to in fact you can mix you can have an application that simultaneously reads from from s3 reads",
    "start": "1312679",
    "end": "1320960"
  },
  {
    "text": "from HDFS and rights to either one okay",
    "start": "1320960",
    "end": "1327830"
  },
  {
    "text": "so why is s3 such an important an EMR FS",
    "start": "1327830",
    "end": "1332990"
  },
  {
    "text": "which enables communication with as3 why is that such an empowering the biggest",
    "start": "1332990",
    "end": "1340910"
  },
  {
    "text": "impact is that it allows for decoupling of storage from from compute okay this",
    "start": "1340910",
    "end": "1348559"
  },
  {
    "text": "is very different than an on-premise tur or even a Hadoop cluster that you deploy",
    "start": "1348559",
    "end": "1354830"
  },
  {
    "text": "yourself on a fleet of ec2 instances when you deploy on pram or you deploy a",
    "start": "1354830",
    "end": "1361370"
  },
  {
    "text": "fleet of ec2 instances and you're leveraging the HDFS storage on those",
    "start": "1361370",
    "end": "1366830"
  },
  {
    "text": "nodes you are requiring a certain binding a tight coupling between the",
    "start": "1366830",
    "end": "1373040"
  },
  {
    "text": "amount of CPU the amount of storage and RAM you have to determine how many nodes",
    "start": "1373040",
    "end": "1378770"
  },
  {
    "text": "you want to give you the aggregate memory the aggregate disk space and the",
    "start": "1378770",
    "end": "1384350"
  },
  {
    "text": "aggregate CPU power so that tight coupling in a traditional Hadoop cluster",
    "start": "1384350",
    "end": "1389950"
  },
  {
    "text": "means that when you scale the cluster when you store more data you add more",
    "start": "1389950",
    "end": "1395510"
  },
  {
    "text": "nodes but that also means you're adding more CPU and RAM which you're paying for okay so so that tight coupling in a",
    "start": "1395510",
    "end": "1403340"
  },
  {
    "text": "traditional Hadoop deployment it is very limiting with this decoupling of storage",
    "start": "1403340",
    "end": "1410660"
  },
  {
    "text": "and compute you have a whole host of flexibility so with the data stored in",
    "start": "1410660",
    "end": "1416059"
  },
  {
    "text": "s3 because it's elastic and it's highly parallel whatever that storage is is",
    "start": "1416059",
    "end": "1423919"
  },
  {
    "text": "able to grow to handle whatever amount of growth and your data you see okay so",
    "start": "1423919",
    "end": "1430790"
  },
  {
    "text": "on the storage side you've got a lot of flexibility there on the compute side if",
    "start": "1430790",
    "end": "1436460"
  },
  {
    "text": "the data is stored in s3 and we have 11 nines of durability",
    "start": "1436460",
    "end": "1442429"
  },
  {
    "text": "that means that you can actually you have a lot of flexibility in the sizing",
    "start": "1442429",
    "end": "1447559"
  },
  {
    "text": "of the cluster so you can in fact have a cluster that starts out a certain size",
    "start": "1447559",
    "end": "1453679"
  },
  {
    "text": "does its job and then you can shut the cluster down so you can actually terminate your cluster because your data",
    "start": "1453679",
    "end": "1460549"
  },
  {
    "text": "is safe and sound on s3 the input records are there the computation",
    "start": "1460549",
    "end": "1466789"
  },
  {
    "text": "generates records that go back to s3 and then you can terminate the the EMR",
    "start": "1466789",
    "end": "1472129"
  },
  {
    "text": "cluster so that ability to shut it down gives you gives you tremendous cost",
    "start": "1472129",
    "end": "1478249"
  },
  {
    "text": "savings particularly in a batch batch model of workloads okay you also have",
    "start": "1478249",
    "end": "1485570"
  },
  {
    "text": "the ability to resize you can have different sized clusters you don't have",
    "start": "1485570",
    "end": "1490669"
  },
  {
    "text": "to worry about the size of the cluster for data storage that's handled by s3 you worry purely in terms of what kind",
    "start": "1490669",
    "end": "1498110"
  },
  {
    "text": "of SLA you know what kind of window time window you need the computation and",
    "start": "1498110",
    "end": "1503570"
  },
  {
    "text": "processing to complete it and very interestingly you can run multiple EMR",
    "start": "1503570",
    "end": "1511039"
  },
  {
    "text": "clusters so you could have for example separation of development test and",
    "start": "1511039",
    "end": "1517549"
  },
  {
    "text": "production okay you get three separate clusters you could have a small development cluster that that your data",
    "start": "1517549",
    "end": "1525590"
  },
  {
    "text": "scientists can work on you can have a separate cluster that's representative",
    "start": "1525590",
    "end": "1530779"
  },
  {
    "text": "of production in terms of size that your software QA team chem test and then of",
    "start": "1530779",
    "end": "1537289"
  },
  {
    "text": "course in production you can have as large a cluster as you need but they can",
    "start": "1537289",
    "end": "1543169"
  },
  {
    "text": "all point to and work with the same data set that's in s3 so so there's a",
    "start": "1543169",
    "end": "1548470"
  },
  {
    "text": "considerable flexibility there the other thing you can do is if you have a variety of workloads instead of",
    "start": "1548470",
    "end": "1555950"
  },
  {
    "text": "provisioning one cluster with an amount of capacity for your biggest job you",
    "start": "1555950",
    "end": "1563179"
  },
  {
    "text": "provision a cluster that's appropriate for the type of workload that that you're doing the one workload you're",
    "start": "1563179",
    "end": "1570019"
  },
  {
    "text": "doing in the second workload you might require a bigger cluster so you have again you can bigger cluster but they both work of the",
    "start": "1570019",
    "end": "1576560"
  },
  {
    "text": "same the same data set so there's a lot of flexibility you can right size the compute in a variety of ways okay so",
    "start": "1576560",
    "end": "1586310"
  },
  {
    "text": "let's actually go ahead and spin up an EMR cluster to show people how",
    "start": "1586310",
    "end": "1593510"
  },
  {
    "text": "straightforward this is that readable I guess it is all right so let's we're",
    "start": "1593510",
    "end": "1604760"
  },
  {
    "text": "going to create cluster here and what I'm going to do is actually go to the Advanced Options all right and I",
    "start": "1604760",
    "end": "1613100"
  },
  {
    "text": "mentioned we have 15 applications currently that are available through a mere check box and so we're going to",
    "start": "1613100",
    "end": "1620840"
  },
  {
    "text": "make sure we select presto and that's all we need on this screen will go with",
    "start": "1620840",
    "end": "1627320"
  },
  {
    "text": "defaults for the rest of it we're not going to submit a step next so this is",
    "start": "1627320",
    "end": "1633470"
  },
  {
    "text": "where you you specify the amount of hardware the number of nodes and the",
    "start": "1633470",
    "end": "1639110"
  },
  {
    "text": "size of the nodes we have a whole host of VC two types as you can see broken",
    "start": "1639110",
    "end": "1644180"
  },
  {
    "text": "out by different family types so you can pick those will go with a single master a single a couple of core nodes they",
    "start": "1644180",
    "end": "1651260"
  },
  {
    "text": "manage the HDFS there is some intermediate data that will be stored on",
    "start": "1651260",
    "end": "1656960"
  },
  {
    "text": "on HDFS and then we're going to boost the task nodes up to 10 we could attach",
    "start": "1656960",
    "end": "1664250"
  },
  {
    "text": "EBS volumes if we wanted we could go to good specify spot if we wanted to and in",
    "start": "1664250",
    "end": "1672170"
  },
  {
    "text": "that case we would be prompted for yeah I guess it's hard to see on my screen",
    "start": "1672170",
    "end": "1678320"
  },
  {
    "text": "we'd be prompted for our bid bid price and that's all we would need to be able",
    "start": "1678320",
    "end": "1684230"
  },
  {
    "text": "to take advantage of a spot we'll just go ahead with 10 knows they're all right",
    "start": "1684230",
    "end": "1690250"
  },
  {
    "text": "we just need the most give this a name here call it summit demo we'll call it presto",
    "start": "1690250",
    "end": "1698500"
  },
  {
    "text": "demo all right and that's all we need",
    "start": "1698500",
    "end": "1704620"
  },
  {
    "text": "there and we're going to SSH into it so we have to specify a key pair and the",
    "start": "1704620",
    "end": "1712000"
  },
  {
    "text": "rest of the are fine is defaults that's it now we're spinning up an EMR cluster",
    "start": "1712000",
    "end": "1719320"
  },
  {
    "text": "and it reminds you summarizes what you know what we've asked for and if you",
    "start": "1719320",
    "end": "1726370"
  },
  {
    "text": "wanted to sit and watch the hardware come up you can do that we're going to continue with the slides and let this",
    "start": "1726370",
    "end": "1732220"
  },
  {
    "text": "cluster come up all right okay so the",
    "start": "1732220",
    "end": "1741310"
  },
  {
    "text": "other piece of the puzzle the other key component to this non data warehouse",
    "start": "1741310",
    "end": "1746380"
  },
  {
    "text": "alternative is the the Apache presto software this is open source software",
    "start": "1746380",
    "end": "1753240"
  },
  {
    "text": "Hadoop is open source so again you're avoiding that the expensive proprietary",
    "start": "1753240",
    "end": "1759360"
  },
  {
    "text": "specialized data warehouse software you're leveraging something that's very",
    "start": "1759360",
    "end": "1766110"
  },
  {
    "text": "inexpensive what is presto and where where did it come from so presto was",
    "start": "1766110",
    "end": "1771580"
  },
  {
    "text": "developed by Facebook in 2012 they designed it to work at petabytes scale",
    "start": "1771580",
    "end": "1777960"
  },
  {
    "text": "data and and they designed it to be able to execute sequel queries okay",
    "start": "1777960",
    "end": "1785220"
  },
  {
    "text": "originally they look they designed it to avoid the map the slow MapReduce step of",
    "start": "1785220",
    "end": "1791830"
  },
  {
    "text": "high they wanted to be able to examine hive tables without having to wait for",
    "start": "1791830",
    "end": "1797080"
  },
  {
    "text": "MapReduce jobs that was the original motivation but ultimately they evolved",
    "start": "1797080",
    "end": "1802960"
  },
  {
    "text": "it into a full-scale data query engine on its own and they open source this as",
    "start": "1802960",
    "end": "1811870"
  },
  {
    "text": "part of Apache in 2013 what are the",
    "start": "1811870",
    "end": "1817120"
  },
  {
    "text": "benefits so the key distinction is it's an in-memory distributed query engine so",
    "start": "1817120",
    "end": "1824130"
  },
  {
    "text": "as I said it avoids the map you step the next slide will will make",
    "start": "1824130",
    "end": "1829740"
  },
  {
    "text": "the point why why that's so important but it's an in-memory engine that's very powerful it it was designed explicitly",
    "start": "1829740",
    "end": "1836640"
  },
  {
    "text": "to support an c compliant sequel as you may know hive and pig and other query",
    "start": "1836640",
    "end": "1845370"
  },
  {
    "text": "engines that are part of the Hadoop ecosystem are not necessarily an see compatible so this was a big design",
    "start": "1845370",
    "end": "1853710"
  },
  {
    "text": "focus it was also designed to work with a variety of data sources in different",
    "start": "1853710",
    "end": "1859980"
  },
  {
    "text": "using different data persistence technologies and also to be able to",
    "start": "1859980",
    "end": "1865490"
  },
  {
    "text": "execute queries that can pull data simultaneously from multiple data",
    "start": "1865490",
    "end": "1871110"
  },
  {
    "text": "sources at and most importantly was designed to provide rapid responses on",
    "start": "1871110",
    "end": "1878670"
  },
  {
    "text": "the order of seconds to two maybe less than a minute good so what it what's",
    "start": "1878670",
    "end": "1888000"
  },
  {
    "text": "been achieved so far with presto so they've they've accomplished a",
    "start": "1888000",
    "end": "1895580"
  },
  {
    "text": "high-performance about ten times faster than hive as a proof point there are",
    "start": "1895580",
    "end": "1901830"
  },
  {
    "text": "many companies using it but as a one proof point Netflix is a huge user of presto they have 35 they run about 3,500",
    "start": "1901830",
    "end": "1909900"
  },
  {
    "text": "presto queries per day on a data set that's 25 petabytes in size that lives",
    "start": "1909900",
    "end": "1916170"
  },
  {
    "text": "on s3 so Netflix has in fact standardized on storing their data in s3",
    "start": "1916170",
    "end": "1922140"
  },
  {
    "text": "that's what they use as their data warehouse because of the many benefits that we talked about and they have on",
    "start": "1922140",
    "end": "1929910"
  },
  {
    "text": "the order of 350 users actively using it at the same time it's been it's evolved",
    "start": "1929910",
    "end": "1939810"
  },
  {
    "text": "to support a variety of plugins different persistence engines listed",
    "start": "1939810",
    "end": "1945000"
  },
  {
    "text": "their data bi tools can interface with it using jdbc and",
    "start": "1945000",
    "end": "1950870"
  },
  {
    "text": "odbc and different clients can talk to",
    "start": "1950870",
    "end": "1956840"
  },
  {
    "text": "presto in in a variety of protocols but particularly HTTP and JSON and there's a",
    "start": "1956840",
    "end": "1963299"
  },
  {
    "text": "variety of languages that that can interface with it very powerful is the",
    "start": "1963299",
    "end": "1969450"
  },
  {
    "text": "ability to run complex sequel queries things that couldn't be done with hive",
    "start": "1969450",
    "end": "1976049"
  },
  {
    "text": "and Pagan and other query engines this includes things like joins aggregations",
    "start": "1976049",
    "end": "1981510"
  },
  {
    "text": "roll-ups a variety of functions windowing functions etc this is a high",
    "start": "1981510",
    "end": "1989760"
  },
  {
    "text": "level architecture for for presto it's it runs on a cluster it the cluster is",
    "start": "1989760",
    "end": "1996240"
  },
  {
    "text": "comprised of a coordinator and then a bunch of workers the the client will",
    "start": "1996240",
    "end": "2003950"
  },
  {
    "text": "talk to the coordinator and submit sequel queries using using different",
    "start": "2003950",
    "end": "2009679"
  },
  {
    "text": "protocols will actually in the demo use presto command-line client running on",
    "start": "2009679",
    "end": "2017840"
  },
  {
    "text": "the cluster itself and then the coordinator receives the sequel parses",
    "start": "2017840",
    "end": "2023330"
  },
  {
    "text": "it analyzes it and then plans the execution distributing the query across",
    "start": "2023330",
    "end": "2028850"
  },
  {
    "text": "the set of of workers and in this diagram it's showing data persistence on",
    "start": "2028850",
    "end": "2035620"
  },
  {
    "text": "HDFS but very importantly is showing that it works with the hive meta store",
    "start": "2035620",
    "end": "2040970"
  },
  {
    "text": "which is a central directory of data sources within Hadoop clusters a little",
    "start": "2040970",
    "end": "2049190"
  },
  {
    "text": "more detail I mean there's a discovery service so all the components the coordinator and the worker nodes",
    "start": "2049190",
    "end": "2054919"
  },
  {
    "text": "register with the discovery service at startup and then the sequence of events",
    "start": "2054919",
    "end": "2060618"
  },
  {
    "text": "has shown here in the client sends the query the coordinator builds the query plan distributes it to the workers the",
    "start": "2060619",
    "end": "2068179"
  },
  {
    "text": "workers execute the execute the protocol",
    "start": "2068179",
    "end": "2074000"
  },
  {
    "text": "I sorry execute the task underlying the query in a pipelined",
    "start": "2074000",
    "end": "2080138"
  },
  {
    "text": "fashion okay and running in a multi-threaded manner and then using a",
    "start": "2080139",
    "end": "2085148"
  },
  {
    "text": "variety of plugins is able to work with the data in a variety of storage or",
    "start": "2085149",
    "end": "2090669"
  },
  {
    "text": "persistence forms okay so as I've said",
    "start": "2090669",
    "end": "2096219"
  },
  {
    "text": "presto is very high-performance the question then is why is it so fast so we",
    "start": "2096219",
    "end": "2101769"
  },
  {
    "text": "talked earlier one of the key things is it's an in-memory parallel queries and",
    "start": "2101769",
    "end": "2108579"
  },
  {
    "text": "that in memory is very very different much much faster than the mandap MapReduce used by traditional Hadoop",
    "start": "2108579",
    "end": "2117219"
  },
  {
    "text": "applications pipelining is a key aspect as well working with the data locally on",
    "start": "2117219",
    "end": "2123249"
  },
  {
    "text": "the worker nodes with multithreading caching and also the just-in-time",
    "start": "2123249",
    "end": "2129539"
  },
  {
    "text": "compiler and the sequel optimization that that it does before it distributes",
    "start": "2129539",
    "end": "2137289"
  },
  {
    "text": "the queries and then other optimizations exist one powerful one is predicate push",
    "start": "2137289",
    "end": "2142779"
  },
  {
    "text": "down and that allows the query predicates to be pushed down to the data",
    "start": "2142779",
    "end": "2149259"
  },
  {
    "text": "data persistence layers to be able to execute more more quickly with with the",
    "start": "2149259",
    "end": "2154809"
  },
  {
    "text": "local data all right so just to contrast presto with the traditional MapReduce as",
    "start": "2154809",
    "end": "2162940"
  },
  {
    "text": "most of you probably know MapReduce has a chaining of map steps that transform",
    "start": "2162940",
    "end": "2168759"
  },
  {
    "text": "data and then reducing steps that kegger egat or pull together but that chaining",
    "start": "2168759",
    "end": "2174670"
  },
  {
    "text": "that handoff between the map step and the reduced step is with disk i/o and",
    "start": "2174670",
    "end": "2183459"
  },
  {
    "text": "this causes not only slower latency but it also means that the map and the",
    "start": "2183459",
    "end": "2188949"
  },
  {
    "text": "reduce steps are the reduced step is waiting for the map step to complete so",
    "start": "2188949",
    "end": "2195910"
  },
  {
    "text": "there's extra time wasted while the reduced step sits there and waits for the map map steps to finish so to get",
    "start": "2195910",
    "end": "2205299"
  },
  {
    "text": "away from that presto avoids disk i/o the worker nodes run the the task and",
    "start": "2205299",
    "end": "2213150"
  },
  {
    "text": "the the querying totally in memory and that avoids the wait time there is no",
    "start": "2213150",
    "end": "2220320"
  },
  {
    "text": "separate reduce or sorry map and reduced step no no step waiting for the the",
    "start": "2220320",
    "end": "2226619"
  },
  {
    "text": "previous step and everything is running in a pipeline fashion so there's no no",
    "start": "2226619",
    "end": "2231660"
  },
  {
    "text": "delays at all alright so how do you use",
    "start": "2231660",
    "end": "2238110"
  },
  {
    "text": "data in s3 with presto well any table that you've declared within within the",
    "start": "2238110",
    "end": "2245310"
  },
  {
    "text": "hive meta store is immediately accessible and queryable by presto and",
    "start": "2245310",
    "end": "2251460"
  },
  {
    "text": "that of course includes data that's in s3 exposed via create external table so",
    "start": "2251460",
    "end": "2257880"
  },
  {
    "text": "I showed you in this earlier slide an example of that but we'll actually do that in the demo all right so once a",
    "start": "2257880",
    "end": "2269130"
  },
  {
    "text": "data source is declared within the meta store it's available to presto you have",
    "start": "2269130",
    "end": "2275430"
  },
  {
    "text": "a variety of ways in which you can deploy the meta store for testing development and and unit testing you can",
    "start": "2275430",
    "end": "2282900"
  },
  {
    "text": "use an embedded mode where everything is contained within the same jvm a better",
    "start": "2282900",
    "end": "2289140"
  },
  {
    "text": "mode is a local mode where you you decouple the persistence from the from",
    "start": "2289140",
    "end": "2294720"
  },
  {
    "text": "the JVM so you have a separate process for the persistence and then a fully",
    "start": "2294720",
    "end": "2300570"
  },
  {
    "text": "decoupled mode called remote mode allows the meta store service and the",
    "start": "2300570",
    "end": "2306690"
  },
  {
    "text": "persistence to run in their own processes and then clients running in their process separate processes talk to",
    "start": "2306690",
    "end": "2314070"
  },
  {
    "text": "the meta store service so that gives you more flexibility in a variety of clients",
    "start": "2314070",
    "end": "2319590"
  },
  {
    "text": "etc list of data sources that presto",
    "start": "2319590",
    "end": "2326369"
  },
  {
    "text": "works with and there are others as well but you've got a hive of course no",
    "start": "2326369",
    "end": "2331619"
  },
  {
    "text": "sequel stores like Cassandra my sequel and postgres some couple of relational",
    "start": "2331619",
    "end": "2336960"
  },
  {
    "text": "ones and some some pub sub communication our messaging hubs ok so",
    "start": "2336960",
    "end": "2345170"
  },
  {
    "text": "you you heard what presto can do the question is always when when do you use",
    "start": "2345170",
    "end": "2352010"
  },
  {
    "text": "it and when might you not use it so when you might use presto where it would be",
    "start": "2352010",
    "end": "2357200"
  },
  {
    "text": "recommended is if you have a need for fast and interactive query capabilities",
    "start": "2357200",
    "end": "2363500"
  },
  {
    "text": "with a large number of concurrent queries running and also if you have a high demand for antsy as sequel where",
    "start": "2363500",
    "end": "2373490"
  },
  {
    "text": "you might not want to use presto and maybe use other technologies is if",
    "start": "2373490",
    "end": "2378590"
  },
  {
    "text": "you're doing primarily batch processing especially in a mission-critical fashion",
    "start": "2378590",
    "end": "2385250"
  },
  {
    "text": "where you're doing some ETL some prawns formation of the data enriching",
    "start": "2385250",
    "end": "2390560"
  },
  {
    "text": "aggregation etc in that case you can use hive which is a it's an older but more",
    "start": "2390560",
    "end": "2397670"
  },
  {
    "text": "established tool it's it's less capable on the query in front but it's much much",
    "start": "2397670",
    "end": "2403460"
  },
  {
    "text": "more powerful in terms of being able to transform the data and of course the rising star these days is spark spark",
    "start": "2403460",
    "end": "2410810"
  },
  {
    "text": "gives you a lot of capability to to do etl and process the data also if you're",
    "start": "2410810",
    "end": "2418490"
  },
  {
    "text": "going to do more than just query the data with sequel if you're going to actually take the results of that query",
    "start": "2418490",
    "end": "2425060"
  },
  {
    "text": "and then run some computation over it presto will not do that presto does have",
    "start": "2425060",
    "end": "2431210"
  },
  {
    "text": "functions it has even has some user-defined functions but it's real strength is in the sequel querying",
    "start": "2431210",
    "end": "2437870"
  },
  {
    "text": "capability it gets you the data if you actually want to compute over the data things like machine learning and doing",
    "start": "2437870",
    "end": "2444620"
  },
  {
    "text": "other algorithms like graph graph algorithms then spark is probably the",
    "start": "2444620",
    "end": "2449900"
  },
  {
    "text": "best bet their spark with sparks equal can interface with data stores declared",
    "start": "2449900",
    "end": "2456530"
  },
  {
    "text": "in hive for example but it has a whole whole bunch of libraries and modules for",
    "start": "2456530",
    "end": "2461960"
  },
  {
    "text": "doing machine learning and other other computation and then finally if your",
    "start": "2461960",
    "end": "2467000"
  },
  {
    "text": "data is primarily if it's well-defined and it's primarily of a star schema",
    "start": "2467000",
    "end": "2473109"
  },
  {
    "text": "format where you've got you know a fact table and then dimension tables if",
    "start": "2473109",
    "end": "2478130"
  },
  {
    "text": "that's the primary form of your data and you've got a limited diversity in the data types then probably using a",
    "start": "2478130",
    "end": "2485119"
  },
  {
    "text": "traditional data warehouse might might be a better way and redshift would be",
    "start": "2485119",
    "end": "2490369"
  },
  {
    "text": "the recommendation there if you want to expose a web UI for your users to talk",
    "start": "2490369",
    "end": "2497570"
  },
  {
    "text": "to presto the popular one is air-pal developed by Airbnb whole host of",
    "start": "2497570",
    "end": "2504380"
  },
  {
    "text": "features you know including access control for users you can find and",
    "start": "2504380",
    "end": "2510380"
  },
  {
    "text": "discover the tables that have been declared you can look at the metadata",
    "start": "2510380",
    "end": "2515420"
  },
  {
    "text": "and even look at sample sample data from from the tables there's a query editor",
    "start": "2515420",
    "end": "2521990"
  },
  {
    "text": "that makes it convenient and you can submit the queries through the web interface and track the status of those",
    "start": "2521990",
    "end": "2529790"
  },
  {
    "text": "queries and and how they're progressing once you've got results back you can export them as CSV you can create hive",
    "start": "2529790",
    "end": "2538369"
  },
  {
    "text": "tables directly from from the web UI based on the results that you get back",
    "start": "2538369",
    "end": "2544130"
  },
  {
    "text": "from some query so you can create a new table from that you can also save the queries and go back to them later and",
    "start": "2544130",
    "end": "2550609"
  },
  {
    "text": "browse through them and we execute them we have them we have a CloudFormation",
    "start": "2550609",
    "end": "2558050"
  },
  {
    "text": "template that allows you to spin up an ec2 instance with air-pal and configures",
    "start": "2558050",
    "end": "2566240"
  },
  {
    "text": "it to talk to your EMR cluster so if you're looking to deploy air-pal there's",
    "start": "2566240",
    "end": "2571819"
  },
  {
    "text": "a convenient way to do that with a conformation template all right so let's",
    "start": "2571819",
    "end": "2577190"
  },
  {
    "text": "let's actually execute some queries I think our cluster should be up and",
    "start": "2577190",
    "end": "2582380"
  },
  {
    "text": "running now so just refresher I guess it's still now",
    "start": "2582380",
    "end": "2592099"
  },
  {
    "text": "we've got six nodes that's probably enough to get going six nodes of the task task nose task nodes I didn't",
    "start": "2592099",
    "end": "2600050"
  },
  {
    "text": "explain those before task nodes are primarily cpu and ram so you don't you",
    "start": "2600050",
    "end": "2606290"
  },
  {
    "text": "don't add task nodes to give you disk space there is some you know local disk space but it's primarily there to boost",
    "start": "2606290",
    "end": "2613490"
  },
  {
    "text": "the aggregate CPU and memory for your cluster so you can add as many task",
    "start": "2613490",
    "end": "2619250"
  },
  {
    "text": "nodes as you need to be able to complete the processing in a shorter period of time for example now looks like we've",
    "start": "2619250",
    "end": "2626300"
  },
  {
    "text": "got 10 nodes now running okay so let's sum so let's actually before I do that",
    "start": "2626300",
    "end": "2631640"
  },
  {
    "text": "let me go into s3 and show you the data we're going to work with all right so",
    "start": "2631640",
    "end": "2640339"
  },
  {
    "text": "good flight delays all right so as I showed you in the slide we have we have",
    "start": "2640339",
    "end": "2647450"
  },
  {
    "text": "a bucket called flight delays this is a from the Department of Transportation",
    "start": "2647450",
    "end": "2653619"
  },
  {
    "text": "their air flight statistics from 1987 2 2015 29 years worth of data it was",
    "start": "2653619",
    "end": "2662060"
  },
  {
    "text": "originally in in CSV format and as I said I partitioned it in in two years",
    "start": "2662060",
    "end": "2669160"
  },
  {
    "text": "and within each partition there's a CSV file that's been bzip2 compressed but",
    "start": "2669160",
    "end": "2675980"
  },
  {
    "text": "what I've also done is converted that data into park a similarly you know",
    "start": "2675980",
    "end": "2681560"
  },
  {
    "text": "using partitioning but here we have the the data in parque which is column-oriented which gives us an extra",
    "start": "2681560",
    "end": "2688040"
  },
  {
    "text": "boostin in performance so that's that's where the data lives we haven't we",
    "start": "2688040",
    "end": "2693710"
  },
  {
    "text": "haven't transformed the data it's and it's an original original form so let's go to EMR here's our cluster and we're",
    "start": "2693710",
    "end": "2704720"
  },
  {
    "text": "going to SSH into the cluster so we click on that link get this handy",
    "start": "2704720",
    "end": "2710170"
  },
  {
    "text": "command it's telling us how we can do that my terminal let me see if I can make",
    "start": "2710170",
    "end": "2716470"
  },
  {
    "text": "this larger all right let's try that is",
    "start": "2716470",
    "end": "2721990"
  },
  {
    "text": "it readable all right so I'm SS a Qing",
    "start": "2721990",
    "end": "2727839"
  },
  {
    "text": "into the cluster and it gives us a splash screen letting us know we're on",
    "start": "2727839",
    "end": "2733570"
  },
  {
    "text": "the cluster it's using the ssh key that's stored on my Mac so that's how we we deal with the security so what I'm",
    "start": "2733570",
    "end": "2740740"
  },
  {
    "text": "going to do is I'm going to run hive and",
    "start": "2740740",
    "end": "2748540"
  },
  {
    "text": "this is where we're going to declare the existence of this large data set on s3 so for that I'm going to grab",
    "start": "2748540",
    "end": "2758580"
  },
  {
    "text": "so I have this as an exercise and I'll share the the shortened shortened URL",
    "start": "2762740",
    "end": "2769440"
  },
  {
    "text": "for this late at the end if anyone wants to go back in their own account later and and try this but we're going to take",
    "start": "2769440",
    "end": "2777830"
  },
  {
    "text": "we're going to take the create external table commands here actually this is",
    "start": "2777830",
    "end": "2786270"
  },
  {
    "text": "probably ok that's the CSV version i want i'm going to grab the park a version so we're just going to copy and",
    "start": "2786270",
    "end": "2795030"
  },
  {
    "text": "paste this into into hive so you see the",
    "start": "2795030",
    "end": "2800160"
  },
  {
    "text": "in bold at the bottom we've got it we've got partitioned by year you know those",
    "start": "2800160",
    "end": "2805470"
  },
  {
    "text": "ear equal partitions stored as parquet the column oriented and we're indicating",
    "start": "2805470",
    "end": "2811170"
  },
  {
    "text": "where it lives so I copy that it's here",
    "start": "2811170",
    "end": "2816890"
  },
  {
    "text": "alright paste that and again I'm not",
    "start": "2816890",
    "end": "2824510"
  },
  {
    "text": "creating a table in I'm not populating a table all I'm doing is saying data",
    "start": "2824510",
    "end": "2831000"
  },
  {
    "text": "exists elsewhere interpret it in this way so this is the schema on read when",
    "start": "2831000",
    "end": "2837090"
  },
  {
    "text": "we actually run our queries the queries will leverage this this metadata if you",
    "start": "2837090",
    "end": "2842700"
  },
  {
    "text": "will for that query to be able to work with the data ok so we've I've now knows",
    "start": "2842700",
    "end": "2849150"
  },
  {
    "text": "about that table we can say show tables and so you can see the tables air delays",
    "start": "2849150",
    "end": "2857390"
  },
  {
    "text": "underscore par the park a version we can even do describe table actually don't",
    "start": "2857390",
    "end": "2866850"
  },
  {
    "text": "like a neat table air delays",
    "start": "2866850",
    "end": "2871310"
  },
  {
    "text": "so there's the metadata associated with this table so hive knows about our table",
    "start": "2873430",
    "end": "2878530"
  },
  {
    "text": "the one last thing we need to do is to tell the meta store MSC k mehta mehta",
    "start": "2878530",
    "end": "2884589"
  },
  {
    "text": "store check to repair it's not really repairing but what it's doing is air",
    "start": "2884589",
    "end": "2892329"
  },
  {
    "text": "delays are what is seen tables not would",
    "start": "2892329",
    "end": "2900790"
  },
  {
    "text": "I yep my dyslexic fingers your delays",
    "start": "2900790",
    "end": "2909099"
  },
  {
    "text": "part okay there we go so what that's doing is essentially synchronizing",
    "start": "2909099",
    "end": "2915700"
  },
  {
    "text": "things so that it sits aware of the partitioning in particular okay so so",
    "start": "2915700",
    "end": "2923530"
  },
  {
    "text": "I've now knows about the the hive store meta store continues on even though I've",
    "start": "2923530",
    "end": "2928780"
  },
  {
    "text": "exited from the client now what I'm going to do is run presto seal the",
    "start": "2928780",
    "end": "2934390"
  },
  {
    "text": "command line for presto and we're going",
    "start": "2934390",
    "end": "2939670"
  },
  {
    "text": "to tell it to use hive default that's",
    "start": "2939670",
    "end": "2944740"
  },
  {
    "text": "the the table space that we we have okay and now what we're going to do is run some queries so I'm going to take take",
    "start": "2944740",
    "end": "2955630"
  },
  {
    "text": "the let seems do this here",
    "start": "2955630",
    "end": "2960539"
  },
  {
    "text": "so we're going to try to answer the question you know given this large data",
    "start": "2962309",
    "end": "2968339"
  },
  {
    "text": "set 29 years it includes all kits got like a hundred and fifteen columns so",
    "start": "2968339",
    "end": "2973680"
  },
  {
    "text": "it's a wide table it's got all kinds of information about delays you know flight",
    "start": "2973680",
    "end": "2979079"
  },
  {
    "text": "delays which error airline carriers which origin airports destination",
    "start": "2979079",
    "end": "2986999"
  },
  {
    "text": "airports etc whole whole bunch of information but we're going to end timing information we're going to ask",
    "start": "2986999",
    "end": "2992369"
  },
  {
    "text": "the question which months have the greatest number of delays so we have a",
    "start": "2992369",
    "end": "2997439"
  },
  {
    "text": "query for that so we're going to do a group by soil tonight aggregation let's",
    "start": "2997439",
    "end": "3007309"
  },
  {
    "text": "go to our terminal and I'm going to paste that there",
    "start": "3007309",
    "end": "3012400"
  },
  {
    "text": "boom so you can see it's a pretty quick 29 years this was just 10 nodes of",
    "start": "3022840",
    "end": "3030220"
  },
  {
    "text": "course you could get higher parallel ISM with more nodes in the cluster but it",
    "start": "3030220",
    "end": "3035500"
  },
  {
    "text": "gives you you know typical sequel type capability here it also gives you some",
    "start": "3035500",
    "end": "3041800"
  },
  {
    "text": "statistics at the bottom I'm not sure if you can read those so 167 million rows",
    "start": "3041800",
    "end": "3048600"
  },
  {
    "text": "and the speed was 15 million rows per",
    "start": "3048600",
    "end": "3054010"
  },
  {
    "text": "second just not too bad all right let's try one last query this time we're going",
    "start": "3054010",
    "end": "3061180"
  },
  {
    "text": "to ask the question which carriers airline carriers have the most numerous departures so paste that in",
    "start": "3061180",
    "end": "3072750"
  },
  {
    "text": "now you can see so here's a different these are the short form carrier",
    "start": "3077290",
    "end": "3083230"
  },
  {
    "text": "designations the number of delays but one of the things you'll see is there's actually a boost in speed now 35 million",
    "start": "3083230",
    "end": "3092350"
  },
  {
    "text": "rows per second due to the caching and and other internal optimizations all",
    "start": "3092350",
    "end": "3100810"
  },
  {
    "text": "right so and and you could play around with the data in a variety of ways doing ad-hoc data exploration etc all right",
    "start": "3100810",
    "end": "3113730"
  },
  {
    "text": "all right so in summary about you know presto on EMR querying data and s3 data",
    "start": "3113730",
    "end": "3122260"
  },
  {
    "text": "and s3 is queryable in press using presto on EMR you don't need to do any",
    "start": "3122260",
    "end": "3127930"
  },
  {
    "text": "additional structuring or creation of schemas populating tables any of that",
    "start": "3127930",
    "end": "3134320"
  },
  {
    "text": "it's immediately available for querying presto is trivial to deploy on EMR that",
    "start": "3134320",
    "end": "3140770"
  },
  {
    "text": "check box will do it you may know that you can run you can launch an EMR",
    "start": "3140770",
    "end": "3146770"
  },
  {
    "text": "cluster from the command line so you can automate that process using presto is is",
    "start": "3146770",
    "end": "3152110"
  },
  {
    "text": "simply a matter of a parameter so as to use presto as a as one of the many applications presto as you saw we could",
    "start": "3152110",
    "end": "3159640"
  },
  {
    "text": "run some ad-hoc queries and do some data exploration very quickly weezer it uses",
    "start": "3159640",
    "end": "3167770"
  },
  {
    "text": "a variety of data sources fast performance due to the in-memory processing and the pipelining it's",
    "start": "3167770",
    "end": "3174520"
  },
  {
    "text": "feature-rich and it's increasingly being adopted in production we have it marked",
    "start": "3174520",
    "end": "3180580"
  },
  {
    "text": "as a sandbox application currently within AWS within EMR but in fact a",
    "start": "3180580",
    "end": "3187090"
  },
  {
    "text": "number of companies are using it in production you saw the netflix example but other companies like like Facebook",
    "start": "3187090",
    "end": "3194680"
  },
  {
    "text": "and Airbnb and a whole host of other large companies are in fact using it in",
    "start": "3194680",
    "end": "3200320"
  },
  {
    "text": "production so it's it's it's still early days for presto but it's already proven its its capabilities",
    "start": "3200320",
    "end": "3208710"
  },
  {
    "text": "that's it some references there if you want to explore some some other things",
    "start": "3209839",
    "end": "3215609"
  },
  {
    "text": "that you can copy the shortened you you are URL down if you want to go back",
    "start": "3215609",
    "end": "3222119"
  },
  {
    "text": "later and with your own AWS account and try out that exercise the exercise that",
    "start": "3222119",
    "end": "3229619"
  },
  {
    "text": "I have there does two things first it declares the CSV table and then shows",
    "start": "3229619",
    "end": "3240209"
  },
  {
    "text": "you how to create the park a formatted and snappy formatted version of that for",
    "start": "3240209",
    "end": "3247199"
  },
  {
    "text": "the for the higher performance shows you how to do that in a shorter way so it's",
    "start": "3247199",
    "end": "3254640"
  },
  {
    "text": "got some additional things but it's essentially what i showed you today and",
    "start": "3254640",
    "end": "3260130"
  },
  {
    "text": "last but not least please complete evaluations and i'll take any any",
    "start": "3260130",
    "end": "3265829"
  },
  {
    "text": "questions",
    "start": "3265829",
    "end": "3268309"
  },
  {
    "text": "Thanks is regarding spark mm-hmm prepare",
    "start": "3275609",
    "end": "3281169"
  },
  {
    "text": "depressed oh right um could you go into a little bit more in terms of the streaming issues that you were",
    "start": "3281169",
    "end": "3287890"
  },
  {
    "text": "discussing because the way you described it is not how I've experienced where the",
    "start": "3287890",
    "end": "3293769"
  },
  {
    "text": "spark strengths are versus presto so maybe just take a minute or two just to expand on that well the fundamental",
    "start": "3293769",
    "end": "3301449"
  },
  {
    "text": "difference is that presto is a query engine only so it executes sequel spark",
    "start": "3301449",
    "end": "3309099"
  },
  {
    "text": "does sequel queries and a whole host of other things as I mentioned if you're",
    "start": "3309099",
    "end": "3314499"
  },
  {
    "text": "going to do computation on the data machine learning etc spark is the way to go it's going to give you a much more",
    "start": "3314499",
    "end": "3321669"
  },
  {
    "text": "complete now in terms of performance presto actually works faster than sparks",
    "start": "3321669",
    "end": "3327939"
  },
  {
    "text": "equal sparks equal works at about the same speed as high so it's leveraging",
    "start": "3327939",
    "end": "3335109"
  },
  {
    "text": "some of the same components as hive but it probably will get faster with time as",
    "start": "3335109",
    "end": "3341559"
  },
  {
    "text": "well one of the things they've worked on with spark is to reduce the errors you",
    "start": "3341559",
    "end": "3349329"
  },
  {
    "text": "what what is this situation with ours with presto um so presto is is not fault",
    "start": "3349329",
    "end": "3359949"
  },
  {
    "text": "tolerant that's where I mentioned if you're going to run mission-critical",
    "start": "3359949",
    "end": "3366219"
  },
  {
    "text": "batch type processing that's we're working with something more mature like hive is the way to go presto as I said",
    "start": "3366219",
    "end": "3375939"
  },
  {
    "text": "is not fault tolerant it's getting better and we'll get better with time so it'll be able to handle things better",
    "start": "3375939",
    "end": "3382150"
  },
  {
    "text": "and errors in particular but that spark is probably more more robust in terms of",
    "start": "3382150",
    "end": "3388150"
  },
  {
    "text": "error handling thank you",
    "start": "3388150",
    "end": "3392038"
  },
  {
    "text": "hey thanks for the nice presentation so one question for EMR clusters there's an",
    "start": "3396930",
    "end": "3405030"
  },
  {
    "text": "option to create different instance type for different types of nodes is it a good idea for have like em 14 master",
    "start": "3405030",
    "end": "3412349"
  },
  {
    "text": "node than r34 corner yeah the master node isn't doing a lot it's certainly",
    "start": "3412349",
    "end": "3417569"
  },
  {
    "text": "not doing much in terms of computation and the core nodes are primarily for for managing HDFS data so those you can use",
    "start": "3417569",
    "end": "3426990"
  },
  {
    "text": "mainstream type type mm1 type type maybe",
    "start": "3426990",
    "end": "3432420"
  },
  {
    "text": "m3s for the cores because of the additional RAM if you're working with",
    "start": "3432420",
    "end": "3438540"
  },
  {
    "text": "very large data sets particularly in HDFS then then the name server needs to",
    "start": "3438540",
    "end": "3446220"
  },
  {
    "text": "have enough RAM 22 because it has to hold the directory to all the all the",
    "start": "3446220",
    "end": "3453390"
  },
  {
    "text": "objects in memory so you'll need enough ram for that for the task nodes that's",
    "start": "3453390",
    "end": "3458970"
  },
  {
    "text": "where you're doing computation so cpu and ram those you're probably best to",
    "start": "3458970",
    "end": "3464490"
  },
  {
    "text": "use things like m3 maybe m34 x-large m3 beta x large depending on what what",
    "start": "3464490",
    "end": "3471210"
  },
  {
    "text": "you're doing and as i mentioned earlier if you're doing spark workloads your",
    "start": "3471210",
    "end": "3477359"
  },
  {
    "text": "memory memory intensive workloads including presto going with memory optimized like our threes and m4s those",
    "start": "3477359",
    "end": "3486720"
  },
  {
    "text": "r memory optimized those would be recommended for for those kind of work loans yeah so right now we are using",
    "start": "3486720",
    "end": "3493140"
  },
  {
    "text": "this spark and forest park we're using our 38 are three to exploit but we are",
    "start": "3493140",
    "end": "3499890"
  },
  {
    "text": "creating for master Corrin talks all our Archie to alert so according to you",
    "start": "3499890",
    "end": "3505829"
  },
  {
    "text": "yours we can use em for master and core and there are three for transfer yep",
    "start": "3505829",
    "end": "3511260"
  },
  {
    "text": "okay yep thank you",
    "start": "3511260",
    "end": "3514700"
  },
  {
    "text": "any other questions comments if not",
    "start": "3517680",
    "end": "3525430"
  },
  {
    "text": "thank you very much and be sure to fill in your ear evaluations",
    "start": "3525430",
    "end": "3532319"
  }
]