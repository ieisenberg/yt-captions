[
  {
    "text": "In this video, you’ll see how to send change \ndata capture (CDC) information from relational",
    "start": "0",
    "end": "5325"
  },
  {
    "text": "databases to Amazon Kinesis Data Streams by \nusing AWS Database Migration Service (AWS DMS).",
    "start": "5325",
    "end": "12520"
  },
  {
    "text": "With this capability, you can configure\nCDC replication and implement upstream",
    "start": "12845",
    "end": "16983"
  },
  {
    "text": "data changes to a data lake or data \nwarehouse with near real-time capability.",
    "start": "16983",
    "end": "21338"
  },
  {
    "text": "This will allow you to get additional \ndata insights by unlocking use cases",
    "start": "21555",
    "end": "25090"
  },
  {
    "text": "for analytics as well as artificial \nintelligence and machine learning.",
    "start": "25090",
    "end": "28370"
  },
  {
    "text": "In this demo we'll be using an Amazon\nAurora MySQL-compatible database",
    "start": "29293",
    "end": "33353"
  },
  {
    "text": "as our upstream datastore.",
    "start": "33353",
    "end": "34747"
  },
  {
    "text": "We’ll use AWS DMS to send CDC \ninformation to Amazon Kinesis Data Streams.",
    "start": "35036",
    "end": "40079"
  },
  {
    "text": "Kinesis Data Streams is a massively \nscalable and durable data streaming",
    "start": "40459",
    "end": "43997"
  },
  {
    "text": "service that can collect and process large \nstreams of data records in near real-time.",
    "start": "43997",
    "end": "48393"
  },
  {
    "text": "This allows upstream data changes to be \nmade available downstream with low latency.",
    "start": "48682",
    "end": "52893"
  },
  {
    "text": "The sample data used in this demo \nincludes personally identifiable information (PII).",
    "start": "53147",
    "end": "58082"
  },
  {
    "text": "We’ll use AWS Glue, a fully managed \nETL service, to stream the data from",
    "start": "58498",
    "end": "62859"
  },
  {
    "text": "Kinesis Data Streams, mask the PII \nfound in our change data capture,",
    "start": "62859",
    "end": "67160"
  },
  {
    "text": "and store the transformed data \nin an Amazon S3 object store.",
    "start": "67160",
    "end": "70919"
  },
  {
    "text": "Let's begin by navigating to Amazon \nRelational Database Service (Amazon RDS),",
    "start": "73017",
    "end": "77866"
  },
  {
    "text": "where we’ll create our Amazon \nAurora MySQL-compatible database.",
    "start": "77866",
    "end": "81301"
  },
  {
    "text": "First, we'll modify a few parameters \nspecific to our database engine (DB engine)",
    "start": "82387",
    "end": "86844"
  },
  {
    "text": "for AWS DMS to get the CDC information.",
    "start": "86844",
    "end": "89864"
  },
  {
    "text": "Each DB engine has a specific \nset of parameter changes needed.",
    "start": "90443",
    "end": "93645"
  },
  {
    "text": "To learn what these are, please refer \nto the AWS DMS documentation for the",
    "start": "93917",
    "end": "97992"
  },
  {
    "text": "appropriate source DB engine.",
    "start": "97992",
    "end": "99737"
  },
  {
    "text": "Let’s save our changes.",
    "start": "100840",
    "end": "102088"
  },
  {
    "text": "Now we’ll create our database.",
    "start": "103607",
    "end": "105274"
  },
  {
    "text": "We’ll use the standard create method and specify\nthe engine type as Aurora (MySQL Compatible).",
    "start": "108800",
    "end": "113743"
  },
  {
    "text": "Let’s use the latest available \nversion of the engine.",
    "start": "115786",
    "end": "118251"
  },
  {
    "text": "We’ll use the Production template.",
    "start": "120963",
    "end": "122446"
  },
  {
    "text": "Let’s name the DB cluster \n“aurora-mysql-database-1.”",
    "start": "124616",
    "end": "127864"
  },
  {
    "text": "We’ll retain the username of \n“admin” and enter a password.",
    "start": "129094",
    "end": "132077"
  },
  {
    "text": "Let’s retain the default settings for the instance \nconfiguration and for availability and durability.",
    "start": "137857",
    "end": "142541"
  },
  {
    "text": "We’ll set up a connection to an Amazon\n Elastic Compute Cloud (Amazon EC2)",
    "start": "144349",
    "end": "148025"
  },
  {
    "text": "instance for this database.",
    "start": "148025",
    "end": "149508"
  },
  {
    "text": "For the purposes of this example, we \nhave an EC2 instance already prepared.",
    "start": "150485",
    "end": "154410"
  },
  {
    "text": "We’ll retain the default subnet group settings.",
    "start": "157394",
    "end": "159722"
  },
  {
    "text": "The cluster is not publicly accessible.",
    "start": "160174",
    "end": "162163"
  },
  {
    "text": "We’ll use the default security group.",
    "start": "164424",
    "end": "166232"
  },
  {
    "text": "In this case, let’s make sure we’re \nusing 3306 as our database port.",
    "start": "169052",
    "end": "172980"
  },
  {
    "text": "Let’s make some additional configurations.",
    "start": "176560",
    "end": "178511"
  },
  {
    "text": "We’ll specify an initial database name.",
    "start": "181260",
    "end": "183499"
  },
  {
    "text": "Next, we’ll select the DB cluster \nparameter group we created earlier.",
    "start": "185416",
    "end": "188876"
  },
  {
    "text": "Finally, let’s create the database.",
    "start": "192673",
    "end": "194533"
  },
  {
    "text": "Now that the database is available, let’s go to the \nEC2 instance that we’ll be using to interact with it.",
    "start": "196612",
    "end": "201580"
  },
  {
    "text": "For the purposes of this example, we’ve already \ncreated some files for interacting with this cluster.",
    "start": "203334",
    "end": "207934"
  },
  {
    "text": "The first one we’ll use \ncreates a table called users.",
    "start": "208349",
    "end": "211213"
  },
  {
    "text": "The users table will include product-\nplatform information such as the user",
    "start": "212208",
    "end": "215590"
  },
  {
    "text": "ID, username, first and last names, \naddress, email, phone number, and the",
    "start": "215591",
    "end": "219982"
  },
  {
    "text": "number of documents modified, which\nis something we’re going to track.",
    "start": "219982",
    "end": "223064"
  },
  {
    "text": "We will then load a sample data set \ninto the newly created users table.",
    "start": "223752",
    "end": "227409"
  },
  {
    "text": "Let’s go back to RDS and copy the \nwriter instance endpoint for this database.",
    "start": "228549",
    "end": "232489"
  },
  {
    "text": "We’ll paste the endpoint into the MySQL \ncommand line and start loading the data.",
    "start": "238908",
    "end": "242618"
  },
  {
    "text": "We have 49,990 rows in this table.",
    "start": "248407",
    "end": "251670"
  },
  {
    "text": "Next, let’s navigate to Amazon \nKinesis and create a data stream.",
    "start": "252000",
    "end": "255505"
  },
  {
    "text": "Let’s specify a name for our data stream.",
    "start": "260857",
    "end": "262884"
  },
  {
    "text": "Kinesis Data Streams provides On-demand \nand Provisioned capacity modes.",
    "start": "266465",
    "end": "270408"
  },
  {
    "text": "With on-demand capacity mode, there \nis no need for capacity planning--",
    "start": "270553",
    "end": "274241"
  },
  {
    "text": "Kinesis Data Streams instantly accommodates \nyour workloads as they ramp up or down.",
    "start": "274241",
    "end": "278328"
  },
  {
    "text": "In this example, we are using \nthe Provisioned capacity mode.",
    "start": "278707",
    "end": "281601"
  },
  {
    "text": "Let’s retain the default configuration \nand create the data stream.",
    "start": "282704",
    "end": "285755"
  },
  {
    "text": "Next, we’ll navigate to AWS Identity and \nAccess Management (IAM) to create a",
    "start": "287907",
    "end": "292609"
  },
  {
    "text": "role that will be used by AWS DMS to \ninteract with the data stream we just created.",
    "start": "292609",
    "end": "297487"
  },
  {
    "text": "The trusted entity will be an \nAWS service, namely DMS.",
    "start": "298500",
    "end": "302313"
  },
  {
    "text": "Let’s create a new policy to be used by this role.",
    "start": "309148",
    "end": "311488"
  },
  {
    "text": "We’ll specify Kinesis as the service.",
    "start": "312175",
    "end": "314453"
  },
  {
    "text": "In the actions, we’ll need three permissions.",
    "start": "319173",
    "end": "321346"
  },
  {
    "text": "The first one is DescribeStream, \nwhich is in the Read access level.",
    "start": "321599",
    "end": "325069"
  },
  {
    "text": "The other two are PutRecord and PutRecords, \nboth of which are in the Write access level.",
    "start": "326136",
    "end": "330660"
  },
  {
    "text": "Next, we’ll specify a stream resource ARN.",
    "start": "334459",
    "end": "337050"
  },
  {
    "text": "Let’s specify the region we’re currently working \nin and the name of the stream we just created.",
    "start": "339000",
    "end": "343148"
  },
  {
    "text": "Let’s proceed.",
    "start": "345770",
    "end": "346655"
  },
  {
    "text": "We’ll specify a name and create the policy.",
    "start": "350454",
    "end": "352854"
  },
  {
    "text": "Now that we’ve created the policy, let’s go \nback and add it to the IAM role we’re creating.",
    "start": "355837",
    "end": "360480"
  },
  {
    "text": "Let’s refresh the view, select the policy,\nand proceed with the creation process.",
    "start": "361619",
    "end": "365724"
  },
  {
    "text": "Now that the role has been \ncreated, let’s take a closer look at it.",
    "start": "379687",
    "end": "382557"
  },
  {
    "text": "We’ll need the role’s ARN \nlater on, so let’s copy it.",
    "start": "384944",
    "end": "387860"
  },
  {
    "text": "Next, we’ll navigate to AWS Database \nMigration Service to create a replication",
    "start": "388837",
    "end": "393194"
  },
  {
    "text": "instance, which will be used \nto run our migration tasks.",
    "start": "393194",
    "end": "396000"
  },
  {
    "text": "Let’s name the replication instance.",
    "start": "397591",
    "end": "399428"
  },
  {
    "text": "We’ll retain the default settings for \nthe instance class and engine version.",
    "start": "404364",
    "end": "407687"
  },
  {
    "text": "We’ll select the failover option \nwith multiple Availability Zones.",
    "start": "408808",
    "end": "412214"
  },
  {
    "text": "For the virtual private cloud, we’ll select the \nVPC our replication instances will run on.",
    "start": "415831",
    "end": "420373"
  },
  {
    "text": "We won’t make this instance publicly accessible.",
    "start": "424098",
    "end": "426373"
  },
  {
    "text": "Under Advanced settings, we can select \nan appropriate security group to connect",
    "start": "428688",
    "end": "432503"
  },
  {
    "text": "to the Aurora MySQL compatible database.",
    "start": "432503",
    "end": "435000"
  },
  {
    "text": "We’ll use the Default security group.",
    "start": "435398",
    "end": "437261"
  },
  {
    "text": "Let’s create the replication instance.",
    "start": "439738",
    "end": "441688"
  },
  {
    "text": "Next, we’ll create the endpoints that \nwill allow AWS Database Migration",
    "start": "442629",
    "end": "446333"
  },
  {
    "text": "Service to read from and write to our database.",
    "start": "446333",
    "end": "449014"
  },
  {
    "text": "We’ll create the source endpoint first.",
    "start": "450000",
    "end": "451871"
  },
  {
    "text": "Let’s select an RDS DB instance.",
    "start": "452323",
    "end": "454661"
  },
  {
    "text": "We’ll select aurora-mysql-database-1-\ninstance-1, which is our reader instance.",
    "start": "455637",
    "end": "460518"
  },
  {
    "text": "This will prepopulate our endpoint configuration.",
    "start": "460898",
    "end": "463390"
  },
  {
    "text": "Amazon Aurora MySQL is selected \nas the source engine, as expected.",
    "start": "464457",
    "end": "468271"
  },
  {
    "text": "Since we won’t be storing credentials \nwith AWS Secrets Manager, we’ll",
    "start": "469483",
    "end": "472812"
  },
  {
    "text": "provide the access information manually.",
    "start": "472812",
    "end": "474921"
  },
  {
    "text": "Let’s enter the password.",
    "start": "476042",
    "end": "477380"
  },
  {
    "text": "Optionally, we can test the endpoint connection.",
    "start": "479152",
    "end": "481539"
  },
  {
    "text": "Let’s do so.",
    "start": "481756",
    "end": "482714"
  },
  {
    "text": "Let’s select the VPC designated \nfor replication instances.",
    "start": "485915",
    "end": "489268"
  },
  {
    "text": "Let’s run the test.",
    "start": "493119",
    "end": "494328"
  },
  {
    "text": "The test was successful, which indicates \nthat our source endpoint is able to connect",
    "start": "496606",
    "end": "500263"
  },
  {
    "text": "to the database using the \nspecified replication instance.",
    "start": "500264",
    "end": "503247"
  },
  {
    "text": "Let’s create the endpoint.",
    "start": "503555",
    "end": "504875"
  },
  {
    "text": "Next, we’ll create the target endpoint.",
    "start": "506900",
    "end": "508867"
  },
  {
    "text": "We’ll specify Amazon \nKinesis as the target engine.",
    "start": "518488",
    "end": "521298"
  },
  {
    "text": "For the service access role ARN, we’ll paste \nin the ARN of the IAM role we copied earlier.",
    "start": "523704",
    "end": "528719"
  },
  {
    "text": "Next, we’ll need the ARN for the \nKinesis data stream we created.",
    "start": "529804",
    "end": "533001"
  },
  {
    "text": "Let’s navigate to Amazon Kinesis to copy it.",
    "start": "533507",
    "end": "535875"
  },
  {
    "text": "Now we’ll paste the ARN into \nour endpoint configuration.",
    "start": "546246",
    "end": "549363"
  },
  {
    "text": "We’ll leave the message format set to JSON.",
    "start": "553215",
    "end": "555667"
  },
  {
    "text": "Let’s run a test like we did \nwith the source endpoint.",
    "start": "556734",
    "end": "559284"
  },
  {
    "text": "The test was successful, so \nlet’s create the target endpoint.",
    "start": "569030",
    "end": "572114"
  },
  {
    "text": "Now that we have our two endpoints \nand our replication instance, we’ll",
    "start": "573868",
    "end": "576829"
  },
  {
    "text": "create the database migration task.",
    "start": "576830",
    "end": "578848"
  },
  {
    "text": "Let’s give the task an identifier and specify \nthe replication instance we created earlier.",
    "start": "581650",
    "end": "586250"
  },
  {
    "text": "We’ll specify the two endpoints we created.",
    "start": "591259",
    "end": "593552"
  },
  {
    "text": "We’re going to track the number of documents \nthat different users back up into the database.",
    "start": "599447",
    "end": "603381"
  },
  {
    "text": "For the migration type, we’ll select \nthe option to migrate existing data",
    "start": "603688",
    "end": "607274"
  },
  {
    "text": "and replicate ongoing changes.",
    "start": "607275",
    "end": "609138"
  },
  {
    "text": "Let’s retain the default task settings.",
    "start": "610060",
    "end": "612072"
  },
  {
    "text": "In the table mappings, let’s add a new \nselection rule and enter a schema.",
    "start": "613519",
    "end": "617243"
  },
  {
    "text": "Next, we’ll specify the names of the \nsource and source table we want to include.",
    "start": "620390",
    "end": "624311"
  },
  {
    "text": "We’ll leave the rest of the settings \nas they are and create the task.",
    "start": "626445",
    "end": "629479"
  },
  {
    "text": "The task has been created and is now running.",
    "start": "631595",
    "end": "633688"
  },
  {
    "text": "Let’s wait for the load to complete.",
    "start": "634268",
    "end": "635864"
  },
  {
    "text": "The load is now complete, and the \ntask is performing ongoing replication.",
    "start": "638504",
    "end": "642416"
  },
  {
    "text": "Let’s drill down to the table statistics \nto see how long the load process took.",
    "start": "642868",
    "end": "646557"
  },
  {
    "text": "The elapsed load time is \n4 minutes and 37 seconds.",
    "start": "647624",
    "end": "650739"
  },
  {
    "text": "As expected, the number of rows in the table \nmatches what we saw earlier in the EC2 instance.",
    "start": "654000",
    "end": "658944"
  },
  {
    "text": "Let’s return there now.",
    "start": "659107",
    "end": "660354"
  },
  {
    "text": "We’ll use the Python SDK for AWS \n(Boto3) to read from the Kinesis data streams.",
    "start": "662380",
    "end": "667863"
  },
  {
    "text": "Let’s read and print the first 10 records.",
    "start": "668912",
    "end": "671062"
  },
  {
    "text": "This nested JSON structure includes a sequence \nnumber, a data element, and a metadata element.",
    "start": "672000",
    "end": "676999"
  },
  {
    "text": "Let's take a closer look at one of these records.",
    "start": "678083",
    "end": "680193"
  },
  {
    "text": "Notice the sequence number.",
    "start": "680392",
    "end": "682019"
  },
  {
    "text": "The data includes the raw, full \nrow from the MySQL database.",
    "start": "682923",
    "end": "686496"
  },
  {
    "text": "The metadata information includes\n the record type, which is “data.”",
    "start": "687546",
    "end": "690755"
  },
  {
    "text": "The operation is “load.”",
    "start": "691840",
    "end": "693486"
  },
  {
    "text": "Next, we’ll perform some updates to \nthe Aurora MySQL database and see",
    "start": "694571",
    "end": "698266"
  },
  {
    "text": "if they’re replicated to Kinesis.",
    "start": "698266",
    "end": "699955"
  },
  {
    "text": "Let’s create a duplicate \nwindow for the EC2 instance.",
    "start": "700859",
    "end": "703611"
  },
  {
    "text": "We’ll use one window to perform the \nupdates and the other tab to perform the reads.",
    "start": "703756",
    "end": "707556"
  },
  {
    "text": "We’ll use this window for the reads.",
    "start": "708207",
    "end": "710082"
  },
  {
    "text": "We’ll connect to the database we \ncreated and perform a single row update.",
    "start": "712976",
    "end": "716392"
  },
  {
    "text": "At the same time, in this other window, \nwe’ll perform a continuous read so that",
    "start": "717459",
    "end": "720884"
  },
  {
    "text": "Kinesis will look for any \nchanges made from upstream.",
    "start": "720884",
    "end": "723621"
  },
  {
    "text": "Right now, this window does not show \nany changes because it’s reading from our",
    "start": "724688",
    "end": "727641"
  },
  {
    "text": "latest position in the Kinesis data stream.",
    "start": "727642",
    "end": "729913"
  },
  {
    "text": "Let’s go ahead and run an update.",
    "start": "731016",
    "end": "732633"
  },
  {
    "text": "User ID 25750 has increased the \ndocument count from 30417 to 30418.",
    "start": "737335",
    "end": "744111"
  },
  {
    "text": "Let’s take a look at how this change \nappears in the Kinesis data stream,",
    "start": "744364",
    "end": "747289"
  },
  {
    "text": "which is replicated via DMS.",
    "start": "747289",
    "end": "749187"
  },
  {
    "text": "The read found the update \nand provided the entire row.",
    "start": "750290",
    "end": "753000"
  },
  {
    "text": "Next, let’s take this a little further and \nperform continuous updates in the",
    "start": "754157",
    "end": "757376"
  },
  {
    "text": "upstream Aurora database.",
    "start": "757376",
    "end": "759092"
  },
  {
    "text": "This script will perform 20 updates, one \nafter the other, in two-second intervals.",
    "start": "760104",
    "end": "764379"
  },
  {
    "text": "These changes will instantly be visible \ndownstream in our Kinesis data stream.",
    "start": "764812",
    "end": "768572"
  },
  {
    "text": "As these updates are happening, \nwe should see the changes being",
    "start": "769729",
    "end": "772266"
  },
  {
    "text": "replicated to Kinesis data stream instantly.",
    "start": "772266",
    "end": "774843"
  },
  {
    "text": "As expected, the changes are \nbeing replicated to the data stream.",
    "start": "775784",
    "end": "779162"
  },
  {
    "text": "This capability unlocks a lot of potential \nfor downstream analytics use cases.",
    "start": "779524",
    "end": "783554"
  },
  {
    "text": "However, the data being passed on may contain \na lot of Personally Identifiable Information (PII).",
    "start": "784187",
    "end": "789816"
  },
  {
    "text": "In a moment, we’ll see how to mask PII.",
    "start": "790232",
    "end": "792351"
  },
  {
    "text": "The updates have finished.",
    "start": "793473",
    "end": "794582"
  },
  {
    "text": "Next, we’ll use AWS Glue, our serverless \nextract, transform, and load (ETL) service,",
    "start": "795631",
    "end": "801133"
  },
  {
    "text": "to read the data from the Kinesis data \nstream, perform some updates that will",
    "start": "801133",
    "end": "804382"
  },
  {
    "text": "flatten the entire JSON payload, and mask some of \nPII such as email addresses and phone numbers.",
    "start": "804382",
    "end": "810187"
  },
  {
    "text": "In the left navigation panel, \nwe’ll select Visual ETL.",
    "start": "811290",
    "end": "814800"
  },
  {
    "text": "Let’s create a job starting with a blank canvas.",
    "start": "815831",
    "end": "818424"
  },
  {
    "text": "Let’s give the job a name.",
    "start": "820341",
    "end": "821792"
  },
  {
    "text": "We’ll specify Amazon Kinesis as the source.",
    "start": "824125",
    "end": "826808"
  },
  {
    "text": "Next, we’ll select the stream name.",
    "start": "829195",
    "end": "831000"
  },
  {
    "text": "We’ll fetch everything from \nthe earliest known position.",
    "start": "834345",
    "end": "837000"
  },
  {
    "text": "Let’s set the window size to 60 seconds.",
    "start": "837289",
    "end": "839668"
  },
  {
    "text": "Next, we’ll add some transformation actions.",
    "start": "841928",
    "end": "844230"
  },
  {
    "text": "Let’s flatten the entire JSON structure.",
    "start": "845224",
    "end": "847627"
  },
  {
    "text": "Since we want the data to \nbe completely flattened out,",
    "start": "850213",
    "end": "852838"
  },
  {
    "text": "we’ll retain the default value of zero.",
    "start": "852838",
    "end": "854874"
  },
  {
    "text": "Next, we’ll detect sensitive \ndata so we can mask it.",
    "start": "855127",
    "end": "857857"
  },
  {
    "text": "Let’s find columns that contain sensitive data.",
    "start": "862877",
    "end": "865328"
  },
  {
    "text": "Let’s retain the default settings and \nsample 100 percent of the data set,",
    "start": "866377",
    "end": "870000"
  },
  {
    "text": "with a detection threshold of 10 percent.",
    "start": "870000",
    "end": "872187"
  },
  {
    "text": "We already know that our data contains\nemail addresses and US phone numbers,",
    "start": "872783",
    "end": "876326"
  },
  {
    "text": "so let’s browse for those patterns and \ndesignate them as sensitive information.",
    "start": "876326",
    "end": "879942"
  },
  {
    "text": "Let’s redact the selected PII text \nand replace it with a hash value.",
    "start": "885422",
    "end": "889250"
  },
  {
    "text": "Finally, we’ll store the results \nin an Amazon S3 bucket.",
    "start": "892288",
    "end": "895285"
  },
  {
    "text": "Let’s store the results in the Parquet \nformat with a Snappy compression type.",
    "start": "897400",
    "end": "900879"
  },
  {
    "text": "We’ll specify a bucket that \nwas created for this example.",
    "start": "905671",
    "end": "908515"
  },
  {
    "text": "We’ll use the data-output folder.",
    "start": "910775",
    "end": "912697"
  },
  {
    "text": "To store what we detect, let’s create \na table in the AWS Data Catalog and",
    "start": "915373",
    "end": "919432"
  },
  {
    "text": "update the schema on subsequent runs.",
    "start": "919432",
    "end": "921578"
  },
  {
    "text": "We’ll use the default database.",
    "start": "922084",
    "end": "923730"
  },
  {
    "text": "Let’s enter a table name and save the job.",
    "start": "928693",
    "end": "930969"
  },
  {
    "text": "Let’s take a look at the script that \nhas been generated based on the",
    "start": "934223",
    "end": "936640"
  },
  {
    "text": "transformations we’ve selected.",
    "start": "936640",
    "end": "938190"
  },
  {
    "text": "Let’s see what job details we still need to provide.",
    "start": "938747",
    "end": "941118"
  },
  {
    "text": "We need to specify an IAM \nrole, so let’s do that now.",
    "start": "943505",
    "end": "946709"
  },
  {
    "text": "For our purposes, we’ve already created \nan IAM role with the required permissions",
    "start": "946890",
    "end": "950612"
  },
  {
    "text": "to read from Kinesis Data Streams and write to S3, \nalong with the required Glue service permissions.",
    "start": "950612",
    "end": "955887"
  },
  {
    "text": "We’ll keep the other values \nas default and save this job.",
    "start": "956877",
    "end": "959774"
  },
  {
    "text": "Next, let’s run the job to start \nexecuting the ETL process.",
    "start": "961763",
    "end": "965366"
  },
  {
    "text": "Let’s take a look at the run status.",
    "start": "966288",
    "end": "968151"
  },
  {
    "text": "Right now, the job is running.",
    "start": "969127",
    "end": "970640"
  },
  {
    "text": "In a moment, this console will \nshow real-time logs being ingested.",
    "start": "971019",
    "end": "974321"
  },
  {
    "text": "Logs are beginning to populate.",
    "start": "977089",
    "end": "978729"
  },
  {
    "text": "While the job is running, let’s navigate to \nthe Glue Catalog and check our table list.",
    "start": "979814",
    "end": "984000"
  },
  {
    "text": "There’s a new table here.",
    "start": "988123",
    "end": "989377"
  },
  {
    "text": "Let’s take a quick look at the schema.",
    "start": "989594",
    "end": "991257"
  },
  {
    "text": "Next, let’s view the data in Amazon Athena.",
    "start": "996411",
    "end": "999000"
  },
  {
    "text": "We’ll query the Kinesis users \ntable in the AWS Data Catalog.",
    "start": "1002906",
    "end": "1006626"
  },
  {
    "text": "Notice that the email addresses \nand phone numbers are masked.",
    "start": "1009067",
    "end": "1011967"
  },
  {
    "text": "Finally, let’s navigate to Amazon S3 and view \nthe raw Parquet files in the data-output folder.",
    "start": "1012961",
    "end": "1018166"
  },
  {
    "text": "The data is partitioned by time, with \nfolders based on the year, month, day, and hour.",
    "start": "1025091",
    "end": "1032000"
  },
  {
    "text": "Here are the raw Parquet files.",
    "start": "1034098",
    "end": "1035922"
  },
  {
    "text": "Let’s go back to the Glue console \nand stop the job run so we don’t get",
    "start": "1036392",
    "end": "1039427"
  },
  {
    "text": "charged for anything we don’t need.",
    "start": "1039427",
    "end": "1041347"
  },
  {
    "text": "To finish up, we’ll also want to delete the \nKinesis data stream to avoid incurring costs.",
    "start": "1046446",
    "end": "1051078"
  },
  {
    "text": "You’ve just seen how to send CDC \ninformation from relational databases",
    "start": "1053592",
    "end": "1057066"
  },
  {
    "text": "to Amazon Kinesis Data Streams.",
    "start": "1057066",
    "end": "1059102"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "1060187",
    "end": "1063486"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "1063739",
    "end": "1065898"
  }
]