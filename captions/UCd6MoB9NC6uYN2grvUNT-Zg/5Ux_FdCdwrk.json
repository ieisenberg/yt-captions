[
  {
    "start": "0",
    "end": "36000"
  },
  {
    "text": "good afternoon everyone thank you so much for being here I know this talk is about microservices but this room is",
    "start": "599",
    "end": "7200"
  },
  {
    "text": "anything but Micro I think it can see it like almost 2,000 people uh we are truly humbled by the interest in this talk you",
    "start": "7200",
    "end": "14759"
  },
  {
    "text": "are here for Arc 43 and today I'm joined by Kyle who is a tech lead at Warner",
    "start": "14759",
    "end": "20640"
  },
  {
    "text": "Brothers turbine and will who's a senior software engineer at turbine I am duw",
    "start": "20640",
    "end": "25920"
  },
  {
    "text": "tal I'm a gaming Solutions architect but more importantly I'm a friend of Will and Kyle we have been working for such a long",
    "start": "25920",
    "end": "32000"
  },
  {
    "text": "time we are pretty much on each other's Christmas card list so let me give you a quick summary",
    "start": "32000",
    "end": "39399"
  },
  {
    "start": "36000",
    "end": "36000"
  },
  {
    "text": "of what you can expect for the next hour I'll start I'm the opening act I'm the warm-up act um I'll go over an overview",
    "start": "39399",
    "end": "46559"
  },
  {
    "text": "of microservices for folks who are not familiar with the Paradigm uh we will then uh follow up by it's technically",
    "start": "46559",
    "end": "54120"
  },
  {
    "text": "part two of a two-part Series so we'll go over a quick overview of last year's talk and that's followed up by a deep",
    "start": "54120",
    "end": "61559"
  },
  {
    "text": "dive of turbines microservices platform we'll go into details of their",
    "start": "61559",
    "end": "66560"
  },
  {
    "text": "infrastructure and then we have some real life stories to share from launch we also have a little bit of wisdom to",
    "start": "66560",
    "end": "72520"
  },
  {
    "text": "share from our experiences and then finally since this is the night of the replay party uh we're going to have some",
    "start": "72520",
    "end": "77880"
  },
  {
    "text": "beer after this hopefully you guys can find us if you have any questions that you want to ask us you know feel free to",
    "start": "77880",
    "end": "83560"
  },
  {
    "text": "find us there so let's get started on a quick overview of microservices",
    "start": "83560",
    "end": "91920"
  },
  {
    "text": "by definition microservices are small autonomous services that work well",
    "start": "92479",
    "end": "97520"
  },
  {
    "text": "together a question that gets asked quite a bit is how small is small and it's really an art as well as a science",
    "start": "97520",
    "end": "104479"
  },
  {
    "text": "but if you want to start out start out by dividing your services across business boundaries autonomy means that",
    "start": "104479",
    "end": "111680"
  },
  {
    "text": "any change or uh upload to a service can be deployed independently and that",
    "start": "111680",
    "end": "117719"
  },
  {
    "text": "change should not result in a change in another service or any of its",
    "start": "117719",
    "end": "124280"
  },
  {
    "text": "dependencies so we always like to compare a new architectual paradigm with our good old friend a monolithic",
    "start": "124799",
    "end": "130479"
  },
  {
    "text": "architecture Everyone likes to really talk about the big box that has everything and the Box goes offline and",
    "start": "130479",
    "end": "136120"
  },
  {
    "text": "your entire system goes offline um but we've evolved from that you know we're you leveraging the power of the cloud",
    "start": "136120",
    "end": "142680"
  },
  {
    "text": "you have a kind of a three- tier architecture that you see and um you",
    "start": "142680",
    "end": "147840"
  },
  {
    "text": "know we basically have your clients connect to an elb and they distribute traffic over an app or a web tier you",
    "start": "147840",
    "end": "154040"
  },
  {
    "text": "have your database and you have a database made highly available by adding a replica your assets and static files",
    "start": "154040",
    "end": "160640"
  },
  {
    "text": "are put up on S3 and so this is a good starting point you know you have your three tier architecture and then how",
    "start": "160640",
    "end": "167480"
  },
  {
    "text": "would you scale this well you know you would start with an autoscaling group you would respond you know in capacity",
    "start": "167480",
    "end": "173680"
  },
  {
    "text": "to you respond by change in capacity to the number of incoming users um you can add a caching layer to",
    "start": "173680",
    "end": "180720"
  },
  {
    "text": "offload some of the U reads and writes um on your database depending on how you",
    "start": "180720",
    "end": "186000"
  },
  {
    "text": "configure your cache and then you know you can start you know using a cloud for CDN on top of",
    "start": "186000",
    "end": "192159"
  },
  {
    "text": "a S3 bucket to provide a global point of presence for all your assets so you know",
    "start": "192159",
    "end": "198040"
  },
  {
    "text": "this is it looks pretty good and it is not a monolitic architecture but it can",
    "start": "198040",
    "end": "204840"
  },
  {
    "text": "actually become a monolithic platform and what does that mean",
    "start": "204840",
    "end": "211280"
  },
  {
    "text": "that means there's still some things that we can improve upon so in this case you know you have a platform that",
    "start": "211280",
    "end": "217120"
  },
  {
    "text": "consists of say multiple calls let's say you have an authenticate API a login API a friends API a leaderboard maybe a",
    "start": "217120",
    "end": "223400"
  },
  {
    "text": "shopping cart well when you have to scale a tier you have to scale the entire tier um let's say you're",
    "start": "223400",
    "end": "229640"
  },
  {
    "text": "launching your app you're going to get tons and tons of traffic on say authenticate and login but you have to scale the rest of the services with it",
    "start": "229640",
    "end": "236200"
  },
  {
    "text": "as well what happens if you're lucky enough to be the it admin of the school I'll",
    "start": "236200",
    "end": "242280"
  },
  {
    "text": "let you guys read this for a second so XK CD is a really good place",
    "start": "242280",
    "end": "249159"
  },
  {
    "text": "to go to decompress especially when your codee's compiling or if you have answered a bunch of emails but what I",
    "start": "249159",
    "end": "255760"
  },
  {
    "text": "mean to say is you don't want to be in a position because in this case your entire database tier goes offline and",
    "start": "255760",
    "end": "262479"
  },
  {
    "text": "most of your services will probably go offline they might be live because you have a caching tier but you don't want",
    "start": "262479",
    "end": "268440"
  },
  {
    "text": "to be in this situation and then how are your teams structured",
    "start": "268440",
    "end": "274280"
  },
  {
    "start": "272000",
    "end": "272000"
  },
  {
    "text": "your teams are structured according to a tier so you have your front end developer you know this guy looks pretty happy because the CSS finally rendered",
    "start": "274280",
    "end": "281720"
  },
  {
    "text": "um you have your backend developer you know she looks pretty happy you know she's on AWS uh can't wait to use the",
    "start": "281720",
    "end": "287800"
  },
  {
    "text": "agility U you have a database admin um who probably just migrated to RDS these",
    "start": "287800",
    "end": "293720"
  },
  {
    "text": "are stock photos by the way no wonder they look so happy um it's just how Google works speaking of stock photos",
    "start": "293720",
    "end": "300479"
  },
  {
    "text": "this is your Solutions architect my job is the ability to paint really pretty icons and write text backwards on glass",
    "start": "300479",
    "end": "306919"
  },
  {
    "text": "that's the value I bring to the table don't believe stock pictures so what are the benefits of",
    "start": "306919",
    "end": "314840"
  },
  {
    "start": "312000",
    "end": "312000"
  },
  {
    "text": "microservices this is a quick list and I'll go into a little bit of a detail and then Kyle and will will actually go",
    "start": "314840",
    "end": "320080"
  },
  {
    "text": "into the implementation of all these benefits but the first benefit is technology heterogenity you can choose",
    "start": "320080",
    "end": "326600"
  },
  {
    "start": "323000",
    "end": "323000"
  },
  {
    "text": "the right technology solution for your service so i t aled about a friend service well you can leverage a graph",
    "start": "326600",
    "end": "332720"
  },
  {
    "text": "database and you can use that graph database for just your friend Service uh if you have a real-time feed that",
    "start": "332720",
    "end": "338000"
  },
  {
    "text": "benefits from speed you can write that service in cc++ even when it comes to AWS you can make fine grain choices such",
    "start": "338000",
    "end": "345600"
  },
  {
    "text": "as an instance type uh if you have a service that is very heavy on memory utiliz utilization uh you can use an R3",
    "start": "345600",
    "end": "352039"
  },
  {
    "text": "or an R4 instance U if you have a service that is heavy on iO you can choose the appropriate EBS volume type",
    "start": "352039",
    "end": "359039"
  },
  {
    "text": "you can use a g gp2 or an i1 volume type the next benefit Is Res resilience",
    "start": "359039",
    "end": "366039"
  },
  {
    "start": "363000",
    "end": "363000"
  },
  {
    "text": "uh if one component of a system fails that failure does not Cascade to other services you can isolate the problem",
    "start": "366039",
    "end": "372160"
  },
  {
    "text": "quickly and the rest of the system can carry working on so same example if your authenticate API falls apart your",
    "start": "372160",
    "end": "379160"
  },
  {
    "text": "existing users can still use a login API so you're not losing all your",
    "start": "379160",
    "end": "384919"
  },
  {
    "text": "traffic same thing with scaling a large monolithic system basically in involves",
    "start": "384919",
    "end": "390000"
  },
  {
    "text": "a full deployment of the entire system um that is high impact if something goes wrong and hence a higher risk um if you",
    "start": "390000",
    "end": "397199"
  },
  {
    "text": "saw the keynote today a few customers talked about how they went from like a few deployments a week to hundreds of",
    "start": "397199",
    "end": "403199"
  },
  {
    "text": "deployments a week and so that benefits uh from the M microservices",
    "start": "403199",
    "end": "409599"
  },
  {
    "start": "410000",
    "end": "410000"
  },
  {
    "text": "Paradigm same thing with ease of deployment you know a oneline change in a monolithic code base requires a",
    "start": "410720",
    "end": "417560"
  },
  {
    "text": "full-on redeploy um and this particular case um you know you can make a change to a single service and deploy it",
    "start": "417560",
    "end": "422960"
  },
  {
    "text": "independently to the rest of the system and one thing that is you know",
    "start": "422960",
    "end": "428599"
  },
  {
    "start": "425000",
    "end": "425000"
  },
  {
    "text": "kind of like a soft benefit and you know you need to really think about this pretty hard organizational alignment",
    "start": "428599",
    "end": "434520"
  },
  {
    "text": "Works differently in a microservices paradigm so I talked about that slide about organizational alignment when it comes to microservices you basically",
    "start": "434520",
    "end": "441520"
  },
  {
    "text": "align teams based on the service itself so the service ownership relies with the",
    "start": "441520",
    "end": "446560"
  },
  {
    "text": "entire team because they have been given the autonomy to make the technology decisions for their particular",
    "start": "446560",
    "end": "453919"
  },
  {
    "text": "service so how does this translate into a real world implementation I'm going to hand over uh deck to Kyle and he can",
    "start": "453919",
    "end": "461479"
  },
  {
    "text": "talk about the turbine Mobile gaming platform that is powered by microservices hi I'm Kyle bori and I'm",
    "start": "461479",
    "end": "468520"
  },
  {
    "text": "one of the tech leads on turbines mobile game platform team and now that we've learned a little bit about microservices",
    "start": "468520",
    "end": "475520"
  },
  {
    "text": "let's talk about what we do specifically so so at a super high level",
    "start": "475520",
    "end": "481479"
  },
  {
    "text": "my team has two platforms on the right up here we've got our analytics event",
    "start": "481479",
    "end": "487800"
  },
  {
    "text": "ingestion platform which Services multiple games both console and mobile",
    "start": "487800",
    "end": "493800"
  },
  {
    "text": "and on the left we have two examples of our game backend platform so right now um we manage two",
    "start": "493800",
    "end": "502919"
  },
  {
    "text": "games that have our full backend platform and nine that use our analytics event ingestion platform and we do this",
    "start": "502919",
    "end": "509800"
  },
  {
    "text": "with an 8 person team so how did we get here well we started with this old code",
    "start": "509800",
    "end": "518240"
  },
  {
    "text": "base that kind of was passed from Team to team and built up Cru over time and",
    "start": "518240",
    "end": "524200"
  },
  {
    "text": "by the time it got passed to us it was this uh distributed monolith that was",
    "start": "524200",
    "end": "530240"
  },
  {
    "text": "really tightly coupled um it was really expensive and hard to scale and it was really really",
    "start": "530240",
    "end": "536680"
  },
  {
    "text": "challenging for us to deploy so luckily we had the opportunity to start",
    "start": "536680",
    "end": "541800"
  },
  {
    "text": "fresh with our mobile platform and we knew mobile was going to",
    "start": "541800",
    "end": "547399"
  },
  {
    "text": "be big so we really wanted to focus on scale and on price and because we're a group of",
    "start": "547399",
    "end": "553120"
  },
  {
    "text": "Engineers we really wanted to take a Cutting Edge approach learn some new stuff have some",
    "start": "553120",
    "end": "559440"
  },
  {
    "text": "fun so we had this Greenfield project and we had this new team and we had a",
    "start": "559839",
    "end": "566279"
  },
  {
    "start": "561000",
    "end": "561000"
  },
  {
    "text": "goal to build a robust platform to support all of turbines future mobile",
    "start": "566279",
    "end": "571839"
  },
  {
    "text": "games but we built a team from Engineers across the studio so we all had different backgrounds we had some people",
    "start": "571839",
    "end": "578279"
  },
  {
    "text": "that loved Java we had other people that despised it um and even though microservices",
    "start": "578279",
    "end": "585680"
  },
  {
    "text": "allow for a heterogeneous stack we decided that we really wanted to focus on one language for the most part and",
    "start": "585680",
    "end": "592839"
  },
  {
    "text": "that's basically so that everybody on the team had the opportunity to work on any part of the platform so we kind of",
    "start": "592839",
    "end": "600320"
  },
  {
    "text": "coalesced around goang and we're liking it so far so after we made that Tech choice we",
    "start": "600320",
    "end": "608360"
  },
  {
    "text": "decided to focus on our culture and figure out how we wanted our team to work so we knew we were building",
    "start": "608360",
    "end": "615120"
  },
  {
    "text": "microservices and that kind of led us down the path towards devops now devops",
    "start": "615120",
    "end": "620920"
  },
  {
    "text": "means different things to everybody you talk to um so for us it basically means",
    "start": "620920",
    "end": "627440"
  },
  {
    "text": "that everybody on the team does everything thing we all write service code we all write infrastructure code we",
    "start": "627440",
    "end": "634320"
  },
  {
    "text": "all write stored procs we all write automated tests we manually test and we",
    "start": "634320",
    "end": "640800"
  },
  {
    "text": "run our infrastructure and then of course after",
    "start": "640800",
    "end": "647959"
  },
  {
    "text": "that we did something stupid and we volunteered to be on",
    "start": "647959",
    "end": "653639"
  },
  {
    "text": "call so if anybody here has ever done that you know that immediately after you",
    "start": "653639",
    "end": "660360"
  },
  {
    "text": "do you think to yourself oh no what have I done how am I going to stay happy in",
    "start": "660360",
    "end": "666160"
  },
  {
    "start": "665000",
    "end": "665000"
  },
  {
    "text": "this environment well first of all you don't want to get woken up at 3: in the",
    "start": "666160",
    "end": "672320"
  },
  {
    "text": "morning that one's pretty obvious but because we're in a microservices environment and we have lots of services",
    "start": "672320",
    "end": "678839"
  },
  {
    "text": "that means lots of deployments so we want to make deployments really easy and we want to be able to do them early and",
    "start": "678839",
    "end": "684959"
  },
  {
    "text": "often so we have to focus on our deployment tools early on",
    "start": "684959",
    "end": "690160"
  },
  {
    "text": "and also because we're a devops team we are already handling all of these different issues and we don't want to be",
    "start": "690160",
    "end": "697480"
  },
  {
    "text": "distracted so we need to enable some self-service so that other people can",
    "start": "697480",
    "end": "702720"
  },
  {
    "text": "get what they need without bothering us",
    "start": "702720",
    "end": "708040"
  },
  {
    "text": "additionally in microservice land you want to make sure that your bugs when you have them hopefully you don't but",
    "start": "708040",
    "end": "714560"
  },
  {
    "text": "when you do have them you want to M you want to contain them and make sure they don't Ripple out across your platform",
    "start": "714560",
    "end": "721079"
  },
  {
    "text": "and then you need to understand how to scale and you want to be able to do it cheaply because that's one of our main goals",
    "start": "721079",
    "end": "726880"
  },
  {
    "text": "right and then when something does go wrong you want to make sure that you can fix it without sshing into a box you",
    "start": "726880",
    "end": "733800"
  },
  {
    "text": "want to make your troubleshooting steps really easy and because we're building a platform that needs to support multiple",
    "start": "733800",
    "end": "739839"
  },
  {
    "text": "games we really need a modular solution so let's talk about our",
    "start": "739839",
    "end": "745480"
  },
  {
    "text": "deployment process and how that helps us check off some of those boxes",
    "start": "745480",
    "end": "751160"
  },
  {
    "text": "this is what a standard ec2 instance looks like in our platform um we run coreos with a separate cluster for ETD",
    "start": "751600",
    "end": "759519"
  },
  {
    "text": "and console which are both key value Stores um ETD is the data store that",
    "start": "759519",
    "end": "764839"
  },
  {
    "text": "backs Fleet which I'll talk about in a minute and console stores our deployment configuration and service",
    "start": "764839",
    "end": "770839"
  },
  {
    "text": "configuration uh as well as handles our internal DNS for service",
    "start": "770839",
    "end": "776120"
  },
  {
    "text": "Discovery so on the actual instance itself we we have our console agent which basically just maintains",
    "start": "776120",
    "end": "781519"
  },
  {
    "text": "membership information for console and then we have a custom tool called deploy",
    "start": "781519",
    "end": "786639"
  },
  {
    "text": "agent and deploy agent job is basically to monitor console when it sees new",
    "start": "786639",
    "end": "792600"
  },
  {
    "text": "deployment configuration it it will generate unit files from that configuration and pass",
    "start": "792600",
    "end": "800120"
  },
  {
    "text": "those off to Fleet for scheduling so Fleet is a scheduler and",
    "start": "800120",
    "end": "805720"
  },
  {
    "text": "it's a distributed init system built on top of ETD and systemd and it basically uses systemd unit files with some",
    "start": "805720",
    "end": "812240"
  },
  {
    "text": "extended syntax to give you a little bit more control over where in your platform uh your units are starting up and deploy",
    "start": "812240",
    "end": "819720"
  },
  {
    "text": "agent basically gives Fleet a set of unit files and those unit files specify",
    "start": "819720",
    "end": "824760"
  },
  {
    "text": "their start order as well as the fact that they have to start on the same machine and we call that collection of",
    "start": "824760",
    "end": "830880"
  },
  {
    "text": "unit files a pod and all of our services are deployed as part of a pod and for people familiar",
    "start": "830880",
    "end": "837839"
  },
  {
    "text": "with kubernetes it's pretty similar to their pod concept each pod consists of a",
    "start": "837839",
    "end": "842880"
  },
  {
    "text": "service container and then any number of reusable sidekick containers that might",
    "start": "842880",
    "end": "848279"
  },
  {
    "text": "be necessary for the particular environment you're deploying into so if I'm deploying a service into AWS I might",
    "start": "848279",
    "end": "855079"
  },
  {
    "text": "need an ALB sidekick so I can register my service with an application load balancer but if I'm deploying that same",
    "start": "855079",
    "end": "861199"
  },
  {
    "text": "service on my laptop I probably don't need that because I don't have alvs",
    "start": "861199",
    "end": "866240"
  },
  {
    "text": "there so in this example Fleet is going to start up an EBS",
    "start": "866240",
    "end": "872160"
  },
  {
    "text": "sidekick which will reach out through the Amazon API find a particular EBS",
    "start": "872160",
    "end": "877639"
  },
  {
    "text": "volume and mount it to the machine that it's running on once that's done the service",
    "start": "877639",
    "end": "883079"
  },
  {
    "text": "container will start up and it'll have access to that EBS volume and then an ALB sidekick will start register that",
    "start": "883079",
    "end": "889920"
  },
  {
    "text": "service so that it starts getting external traffic through the ALB and then a DNS sidekick will start up and",
    "start": "889920",
    "end": "896160"
  },
  {
    "text": "register that service in console for internal service discovery so it'll start getting internal Services",
    "start": "896160",
    "end": "902199"
  },
  {
    "text": "uh traffic as well so you may have noticed that we had",
    "start": "902199",
    "end": "907360"
  },
  {
    "text": "Docker logos all over that last diagram we use Docker everywhere um and the main",
    "start": "907360",
    "end": "913160"
  },
  {
    "text": "reason for that is it lets us deploy everything in the same way so we have lots of go Services um we",
    "start": "913160",
    "end": "921440"
  },
  {
    "text": "have some C services that other internal teams have written we have a lot of third party uh systems like Kafka and",
    "start": "921440",
    "end": "928160"
  },
  {
    "text": "zookeeper and deploy them all the same way in Docker containers but that's not the only thing",
    "start": "928160",
    "end": "934800"
  },
  {
    "text": "we get from containerization uh you also have your code and your built-in dependencies all together in one binary",
    "start": "934800",
    "end": "941600"
  },
  {
    "text": "so it's really easy to deploy you just take that binary you drop it on an instance and you start",
    "start": "941600",
    "end": "946959"
  },
  {
    "text": "it we also use from scratch containers where we can um which means they're pretty",
    "start": "946959",
    "end": "953079"
  },
  {
    "text": "lightweight uh most of our go service containers are around a megabyte so we",
    "start": "953079",
    "end": "958240"
  },
  {
    "text": "have really quick deployments because there's not a whole lot of data that needs to get moved",
    "start": "958240",
    "end": "963959"
  },
  {
    "text": "around there's some security benefits as well we drop all of our Linux kernel",
    "start": "963959",
    "end": "969480"
  },
  {
    "text": "capabilities so that if somebody does get into a container they can't really do too much and then there's also the added",
    "start": "969480",
    "end": "977079"
  },
  {
    "text": "benefit of it being relatively easy to run multiple services on a single",
    "start": "977079",
    "end": "982319"
  },
  {
    "text": "instance because they're all wrapped up in a container and this gives you some resource utilization benefits",
    "start": "982319",
    "end": "990920"
  },
  {
    "text": "so how does that work exactly well we have a pool of R3 instances and a pool",
    "start": "990920",
    "end": "997920"
  },
  {
    "start": "991000",
    "end": "991000"
  },
  {
    "text": "of C4 instances and then each service is configured to have a preference towards",
    "start": "997920",
    "end": "1003120"
  },
  {
    "text": "Ram or CPU depending on their needs and Fleet will basically decide that it",
    "start": "1003120",
    "end": "1008519"
  },
  {
    "text": "wants to put that service in a particular pool based on that configuration but we don't actually care",
    "start": "1008519",
    "end": "1014639"
  },
  {
    "text": "which specific instance it runs on it can run anywhere",
    "start": "1014639",
    "end": "1019800"
  },
  {
    "text": "so we use albs for our Edge services and things that need websockets and then for",
    "start": "1019800",
    "end": "1025438"
  },
  {
    "text": "services that are behind authentication we have an elb that routes traffic through our front door uh which we use",
    "start": "1025439",
    "end": "1031160"
  },
  {
    "text": "Vulcan D for we break off there and then that forwards to the appropriate service um so in this example we're",
    "start": "1031160",
    "end": "1040918"
  },
  {
    "text": "running a feed service and a leaderboard service on the same instance and if we want to scale up feed",
    "start": "1040919",
    "end": "1049799"
  },
  {
    "text": "uh potentially Fleet will decide that this service is this instance is pretty empty so it's going to start a feed",
    "start": "1049799",
    "end": "1057880"
  },
  {
    "text": "service container here after that happens it'll start the DNS sidekick which you'll register it in",
    "start": "1057880",
    "end": "1064400"
  },
  {
    "text": "console and then it'll start the AL sidekick which will register its specific port in the ALB now we use D we",
    "start": "1064400",
    "end": "1072400"
  },
  {
    "text": "use uh Docker to dynamically generate our Port assignments so there's no",
    "start": "1072400",
    "end": "1077480"
  },
  {
    "text": "conflict here between the the first speed service and the new one so once it's registered they both start getting",
    "start": "1077480",
    "end": "1083400"
  },
  {
    "text": "traffic and everyone's happy so in this way we end up running a",
    "start": "1083400",
    "end": "1089919"
  },
  {
    "text": "mix of 30 different service pods on a single instance in ec2 um and because we're using Dynamic",
    "start": "1089919",
    "end": "1097280"
  },
  {
    "text": "Port mapping it there could be duplicate services on the same instance which is",
    "start": "1097280",
    "end": "1103880"
  },
  {
    "text": "awesome so I mentioned earlier that we run everything in containers and I was",
    "start": "1103880",
    "end": "1109120"
  },
  {
    "start": "1105000",
    "end": "1105000"
  },
  {
    "text": "not kidding uh we run post gressing containers and most people call us crazy but uh it works for us we treat our",
    "start": "1109120",
    "end": "1117840"
  },
  {
    "text": "databases as just another service we deploy them the same way with",
    "start": "1117840",
    "end": "1123039"
  },
  {
    "text": "the same pod Paradigm and of course our post postgress containers are EBS backed so",
    "start": "1123039",
    "end": "1131440"
  },
  {
    "text": "um they have a persistent storage to right 2 we also use Wall-E to stream our wall",
    "start": "1131440",
    "end": "1137000"
  },
  {
    "text": "logs to S3 which is really powerful because it allows us to set up asynchronous replicas with no production",
    "start": "1137000",
    "end": "1146280"
  },
  {
    "text": "impact um and that's also really nice because if somebody from analytics comes",
    "start": "1146280",
    "end": "1151679"
  },
  {
    "text": "over to us and says hey we really need to look at your production data we can just give them readon access",
    "start": "1151679",
    "end": "1158440"
  },
  {
    "text": "to our S3 bucket and a script to set up an asynchronous replica and then they can run it on whatever Hardware they",
    "start": "1158440",
    "end": "1164280"
  },
  {
    "text": "want and we don't have to care about it we don't have to manage it it's not our problem",
    "start": "1164280",
    "end": "1170720"
  },
  {
    "text": "so we talked about how deployments work but how do they actually get triggered it all starts with our chat",
    "start": "1171080",
    "end": "1177840"
  },
  {
    "text": "bot so we have a hip chatbot that we call harbinger of deploys and it can deploy specific G",
    "start": "1177840",
    "end": "1185840"
  },
  {
    "text": "Shaws or tags uh to Any Given environment um it can undeploy Services",
    "start": "1185840",
    "end": "1191360"
  },
  {
    "text": "it can roll them it can scale them up and down it can get environment Health do a bunch of really helpful stuff for",
    "start": "1191360",
    "end": "1197440"
  },
  {
    "text": "us but when it gets a command it basically passes it off to our deploy Service uh which is where all the magic",
    "start": "1197440",
    "end": "1204600"
  },
  {
    "text": "happens so our deploy service when it gets a",
    "start": "1204600",
    "end": "1210919"
  },
  {
    "text": "deployment command it reaches out to our Enterprise GitHub pulls in deployment",
    "start": "1210919",
    "end": "1216520"
  },
  {
    "text": "and service configuration and that uses github's ooth so if you don't have access to the",
    "start": "1216520",
    "end": "1223159"
  },
  {
    "text": "configuration repo then you can't deploy things and change production on us after",
    "start": "1223159",
    "end": "1228240"
  },
  {
    "text": "it does that it figures out what containers are going to be deployed as part of this process and it pulls them",
    "start": "1228240",
    "end": "1234280"
  },
  {
    "text": "down from our container repository then it generates a deployment specific encryption key and",
    "start": "1234280",
    "end": "1241960"
  },
  {
    "text": "injects that into the containers and pushes them back up to our container repository after that it uses the",
    "start": "1241960",
    "end": "1247840"
  },
  {
    "text": "encryption key to encrypt our service configuration and send it out to our game vpc's console and if you remember",
    "start": "1247840",
    "end": "1255280"
  },
  {
    "text": "that's where deploy agent is Sitting Waiting for new configuration to show up when that config shows up it launches",
    "start": "1255280",
    "end": "1262000"
  },
  {
    "text": "the new service and it's important to note that only the new images that have that",
    "start": "1262000",
    "end": "1267280"
  },
  {
    "text": "encryption key can start up and decrypt their configuration this is really important because it means that we know",
    "start": "1267280",
    "end": "1273919"
  },
  {
    "text": "for a fact that our configuration hasn't been tampered with so none of us uh can",
    "start": "1273919",
    "end": "1280520"
  },
  {
    "text": "go in there and handle Matic config to fix a production issue we're just straight up disallowing it",
    "start": "1280520",
    "end": "1290520"
  },
  {
    "start": "1290000",
    "end": "1290000"
  },
  {
    "text": "so to recap we have our deploy service which lives in a centralized VPC pulls its config from G generates encryption",
    "start": "1290840",
    "end": "1297960"
  },
  {
    "text": "Keys injects them into the Container encrypts the config and writes it out to the console that's sitting in the game",
    "start": "1297960",
    "end": "1304960"
  },
  {
    "text": "specific VPC over there we have deploy agent which takes that config deploys the",
    "start": "1304960",
    "end": "1312360"
  },
  {
    "text": "service pod via Fleet and then it sits there and it maintains that Services scale and its health so if one of those",
    "start": "1312360",
    "end": "1319279"
  },
  {
    "text": "service containers dies deploy agent will start it back up so how are we doing well we don't get",
    "start": "1319279",
    "end": "1327600"
  },
  {
    "text": "woken up at 3: in the morning anymore because we have a self-healing",
    "start": "1327600",
    "end": "1332720"
  },
  {
    "text": "ecosystem we've made our deployments pretty easy so we can deploy early and often with our hip chat bot and we've",
    "start": "1332720",
    "end": "1339440"
  },
  {
    "text": "enabled some self-service so we don't get bugged by our analytics guys some of them are out here right now um so let's",
    "start": "1339440",
    "end": "1347520"
  },
  {
    "text": "talk about our platform and how that helps us check off the rest of the",
    "start": "1347520",
    "end": "1351960"
  },
  {
    "start": "1352000",
    "end": "1352000"
  },
  {
    "text": "boxes we really take an API first design approach and mainly this is to help us",
    "start": "1353360",
    "end": "1361039"
  },
  {
    "text": "avoid the classic microservices Pitfall of creating a really painful tangled web of",
    "start": "1361039",
    "end": "1367080"
  },
  {
    "text": "calls um if you get into a situation like that it can be really challenging to debug problems and it can be really",
    "start": "1367080",
    "end": "1373720"
  },
  {
    "text": "challenging to deploy because you've got so many different dependencies",
    "start": "1373720",
    "end": "1379880"
  },
  {
    "text": "but we really start by defining our sour source of Truth for a particular set of data once we've done that we decide how",
    "start": "1379880",
    "end": "1387039"
  },
  {
    "text": "micro we need to go this is really all about segregating your data and encapsulating the functionality that",
    "start": "1387039",
    "end": "1392679"
  },
  {
    "text": "acts on that data together in one service most of our apis are restful uh",
    "start": "1392679",
    "end": "1400279"
  },
  {
    "text": "but even the ones that aren't we have URLs that make sense and that are",
    "start": "1400279",
    "end": "1405600"
  },
  {
    "text": "consistent and we want to be consistent within our service but so with across our whole",
    "start": "1405600",
    "end": "1411039"
  },
  {
    "text": "platform and then this one's pretty obvious but I think people miss it a lot",
    "start": "1411039",
    "end": "1416279"
  },
  {
    "text": "um and you really want to treat your service as just one implementation of your interface because at some point in",
    "start": "1416279",
    "end": "1422840"
  },
  {
    "text": "the future you're going to want to change things and you're going to want to replace a service with something that",
    "start": "1422840",
    "end": "1429720"
  },
  {
    "text": "implements the same API but has completely different behavior and then lastly you really want",
    "start": "1429720",
    "end": "1436320"
  },
  {
    "text": "to think about backwards compatibility when you're first designing an API um in microservice land you may have",
    "start": "1436320",
    "end": "1443320"
  },
  {
    "text": "dozens of services that are dependent on this interface that you're designing if you break backwards",
    "start": "1443320",
    "end": "1450480"
  },
  {
    "text": "compatibility it can be really painful so spend some time thinking about it upfront if you need to add data to a",
    "start": "1450480",
    "end": "1458679"
  },
  {
    "text": "response or a request at some point in the future make sure you can do that",
    "start": "1458679",
    "end": "1463799"
  },
  {
    "text": "without breaking breaking your compatibility so I mentioned segregated",
    "start": "1463799",
    "end": "1470080"
  },
  {
    "start": "1468000",
    "end": "1468000"
  },
  {
    "text": "data why do we do that well it gives us the ability to have per service tuning",
    "start": "1470080",
    "end": "1476559"
  },
  {
    "text": "so we can use postgress or we can use Cassandra or we can use a graph database",
    "start": "1476559",
    "end": "1481960"
  },
  {
    "text": "or we can use whatever we want but we also can set specific volume types we can figure out how many iops we need we",
    "start": "1481960",
    "end": "1488960"
  },
  {
    "text": "can say oh this service needs to autov vacuum in this way and this other one",
    "start": "1488960",
    "end": "1494120"
  },
  {
    "text": "needs to autov vacuum differently but you also when you have smaller data databases you just have smaller problems",
    "start": "1494120",
    "end": "1501520"
  },
  {
    "text": "so if you have 50 databases instead of one each one of those is handling a lot",
    "start": "1501520",
    "end": "1506679"
  },
  {
    "text": "less traffic and it's just that much easier to handle and then of course going back to",
    "start": "1506679",
    "end": "1512200"
  },
  {
    "text": "what dreo was saying earlier if one of your databases goes offline it's not as big of a deal so if our o database goes",
    "start": "1512200",
    "end": "1519760"
  },
  {
    "text": "offline then no new players can log into our game but everybody that's currently playing won't even",
    "start": "1519760",
    "end": "1526559"
  },
  {
    "text": "notice I think the biggest benefit here though is in maintenance so if you're writing a",
    "start": "1526559",
    "end": "1532919"
  },
  {
    "text": "database migration script that modifies a schema that can be really complicated",
    "start": "1532919",
    "end": "1538279"
  },
  {
    "text": "when you're in a A system that has shared data um if you've segregated your",
    "start": "1538279",
    "end": "1544080"
  },
  {
    "text": "data it's much easier to think about because you know that this change can only affect one service and it really",
    "start": "1544080",
    "end": "1551320"
  },
  {
    "text": "limits your testing scope as well so you also want to plan ahead when",
    "start": "1551320",
    "end": "1558320"
  },
  {
    "start": "1556000",
    "end": "1556000"
  },
  {
    "text": "you're building a platform like this you really want to have configurable service Discovery we use console for that um but",
    "start": "1558320",
    "end": "1565360"
  },
  {
    "text": "there's lots of options out there when you combine that with pluggable Services",
    "start": "1565360",
    "end": "1570880"
  },
  {
    "text": "where you're basically coupling your service to dependent interfaces instead of dependent Services it makes it really",
    "start": "1570880",
    "end": "1577080"
  },
  {
    "text": "easy to replace parts of your platform and that's really helpful for us because we can isolate our game logic from our",
    "start": "1577080",
    "end": "1585039"
  },
  {
    "text": "from our platform logic and basically for game a we can run our game a logic",
    "start": "1585039",
    "end": "1591000"
  },
  {
    "text": "service and our platform can talk to that to get stuff done and for game B we can run our game B logic service and our",
    "start": "1591000",
    "end": "1598039"
  },
  {
    "text": "platform can talk to that and that can basically change how our whole platform",
    "start": "1598039",
    "end": "1604080"
  },
  {
    "text": "works you also want to plan ahead plan for failure um in a microservices world",
    "start": "1604080",
    "end": "1610799"
  },
  {
    "text": "you're making a lot of internal calls and some of them are going to 503 some of them will 500 and in that case you",
    "start": "1610799",
    "end": "1617880"
  },
  {
    "text": "probably want to retry but you don't want to Dos yourself it's not fun um so make sure you're doing",
    "start": "1617880",
    "end": "1625919"
  },
  {
    "text": "jittered retries with exponential back off um if you're not familiar with those",
    "start": "1625919",
    "end": "1631399"
  },
  {
    "text": "check out Amazon's blog it's actually really awesome um if you just Google jittered retries it'll be your first",
    "start": "1631399",
    "end": "1639440"
  },
  {
    "text": "hit so we also write a lot of automated tests and really the key thing there is",
    "start": "1640000",
    "end": "1645919"
  },
  {
    "start": "1641000",
    "end": "1641000"
  },
  {
    "text": "we want to ensure that our service is not breaking its API contact contract we",
    "start": "1645919",
    "end": "1651880"
  },
  {
    "text": "really don't want to break backwards compatibility and we found that writing",
    "start": "1651880",
    "end": "1657159"
  },
  {
    "text": "code that uses a lot of dependency injection can be really helpful especially when you're writing unit",
    "start": "1657159",
    "end": "1662480"
  },
  {
    "text": "tests but you really you don't want just unit tests you want to test at all the different levels so you want to have",
    "start": "1662480",
    "end": "1667880"
  },
  {
    "text": "unit tests for your logic integration tests for any data interactions that you have and then functional tests to",
    "start": "1667880",
    "end": "1673960"
  },
  {
    "text": "confirm that your API is working and the most important part that there is you want to run those tests on",
    "start": "1673960",
    "end": "1681000"
  },
  {
    "text": "build so integrate it with your CI system if you have tests that aren't",
    "start": "1681000",
    "end": "1686519"
  },
  {
    "text": "being run regularly then they're useless and they're probably broken additionally to automated tests",
    "start": "1686519",
    "end": "1694159"
  },
  {
    "text": "you really need to load test things so the key takeaway there is you want to estimate your load and your call Profile",
    "start": "1694159",
    "end": "1700279"
  },
  {
    "start": "1695000",
    "end": "1695000"
  },
  {
    "text": "run your tests and then collect metrics and soft launch and then go back reestimate your",
    "start": "1700279",
    "end": "1707880"
  },
  {
    "text": "load would refigure out what your call profile is because you were probably",
    "start": "1707880",
    "end": "1712919"
  },
  {
    "text": "wrong and then when you are low testing don't just stop at a magic number that",
    "start": "1712919",
    "end": "1718279"
  },
  {
    "text": "you got from marketing find your bottleneck push yourself push your",
    "start": "1718279",
    "end": "1724159"
  },
  {
    "text": "service make sure you have a plan for how to fix that bottleneck because it's much easier to think about that now than",
    "start": "1724159",
    "end": "1730320"
  },
  {
    "text": "it is when everything's on fire but low tests aren't really helpful",
    "start": "1730320",
    "end": "1736559"
  },
  {
    "text": "unless you have metrics so make sure you're collecting at a minimum call",
    "start": "1736559",
    "end": "1741720"
  },
  {
    "start": "1737000",
    "end": "1737000"
  },
  {
    "text": "metrics you know you need endpoint response times and throughput you need database response times and throughput",
    "start": "1741720",
    "end": "1747640"
  },
  {
    "text": "at a bare minimum and you want to be able to set up alerts on those as well and then you want to expose them as",
    "start": "1747640",
    "end": "1754919"
  },
  {
    "text": "well as your logs in an easily searchable way so that when you do get",
    "start": "1754919",
    "end": "1760000"
  },
  {
    "text": "an alert when you're out with your friends on your phone you pick up your phone you look at your alert you click a",
    "start": "1760000",
    "end": "1765279"
  },
  {
    "text": "link it brings you right to your graphs brings you right to your logs and you can easily figure out okay now I know",
    "start": "1765279",
    "end": "1772039"
  },
  {
    "text": "what's wrong I can switch over to my chat application type a few commands in and then the problem solved and then",
    "start": "1772039",
    "end": "1778640"
  },
  {
    "text": "when I go into work the next day I can sit down and make sure that I never get that alert ever again because who likes",
    "start": "1778640",
    "end": "1786000"
  },
  {
    "text": "alerts they suck so how are we doing now well we've minimized our bugs by segregating our",
    "start": "1786000",
    "end": "1792440"
  },
  {
    "text": "data and writing lots of tests um we understand how to SC scale because",
    "start": "1792440",
    "end": "1798519"
  },
  {
    "text": "we've low tested everything we don't need to SSH anywhere because we've exposed our metrics and our",
    "start": "1798519",
    "end": "1806480"
  },
  {
    "text": "logs and we've built a pretty modular solution so we can support multiple different games by having pluggable",
    "start": "1806480",
    "end": "1812919"
  },
  {
    "text": "services with configurable service Discovery so now I'm going to pass it over to my teammate will and he's going",
    "start": "1812919",
    "end": "1818120"
  },
  {
    "text": "to talk a little bit about infrastructure hi everybody my name is",
    "start": "1818120",
    "end": "1823200"
  },
  {
    "text": "will uh I'm a senior software engineer on the mobile game platform at turbine so we've talked about the platform service architecture uh let's talk about",
    "start": "1823200",
    "end": "1830399"
  },
  {
    "text": "what we run everything on so I'm going to talk about the infrastructure that we've built and we built this infrastructure with three big goals in",
    "start": "1830399",
    "end": "1836880"
  },
  {
    "start": "1833000",
    "end": "1833000"
  },
  {
    "text": "mind we wanted to limit the blast radius of any problems that we have so we started with this because we know we're",
    "start": "1836880",
    "end": "1842279"
  },
  {
    "text": "going to run multiple multiple games and we don't want one game that has a problem to affect other games we also want the infrastructure to",
    "start": "1842279",
    "end": "1848760"
  },
  {
    "text": "be immutable because we want to make it hard to have pets we want the infrastructure as code",
    "start": "1848760",
    "end": "1854760"
  },
  {
    "text": "so that the state of the infrastructure is well defined and we also have a very clear disaster recover Disaster Recovery",
    "start": "1854760",
    "end": "1861200"
  },
  {
    "text": "plan as Kyle showed you earlier we've isolated things into",
    "start": "1861200",
    "end": "1867519"
  },
  {
    "text": "individual game backends where they're isolated in vpcs with load bouncers on the edge to get traffic in from clients",
    "start": "1867519",
    "end": "1873440"
  },
  {
    "text": "we're running those separately and then we have a shared analytics of vestion Stack if you want to know more about the analytics of vestion Stack you can check",
    "start": "1873440",
    "end": "1879559"
  },
  {
    "text": "out our presentation from last year where we went into detail about it when we start thinking about how we",
    "start": "1879559",
    "end": "1885559"
  },
  {
    "text": "want to limit the blast radius of any problems we we at the highest level separate everything out into accounts so we have a shared services account with a",
    "start": "1885559",
    "end": "1892039"
  },
  {
    "text": "shared services VPC this VPC is where the deploy service lives and it's the only VPC that has access to our",
    "start": "1892039",
    "end": "1897519"
  },
  {
    "text": "corporate resources like our Enterprise GitHub and has a VPN to our corporate Network this then in a spoken Hub model",
    "start": "1897519",
    "end": "1904760"
  },
  {
    "text": "can reach out into the game accounts that we run where within we have an account per game and within that account",
    "start": "1904760",
    "end": "1909840"
  },
  {
    "text": "we have vpcs for different environments of that game these VPC environments are linked back to the shared shared",
    "start": "1909840",
    "end": "1915840"
  },
  {
    "text": "services VPC over VPC peing then for another game we set up another account same structure vpcs for",
    "start": "1915840",
    "end": "1922279"
  },
  {
    "text": "environments linked back to the shared services VPC but these accounts and vpcs for the",
    "start": "1922279",
    "end": "1927480"
  },
  {
    "text": "games have no way to communicate they can only talk to the shared services uh account and within the account the vpcs",
    "start": "1927480",
    "end": "1934080"
  },
  {
    "text": "can't talk to each other they can only talk back to the deploy service and the shared services",
    "start": "1934080",
    "end": "1940039"
  },
  {
    "text": "VPC when we start zooming into what we have in the environment at vpcs we build these with cloud formation and then we",
    "start": "1940240",
    "end": "1946279"
  },
  {
    "text": "start layering other stuff on top of that we start with the autoscaling group of the server nodes that we talked about earlier we call them server nodes",
    "start": "1946279",
    "end": "1952360"
  },
  {
    "text": "because they're the quum bearing nodes for our distributed key value stores at CD and console once we have those in",
    "start": "1952360",
    "end": "1958639"
  },
  {
    "text": "place we set up different autoscaling groups of worker nodes that talk to at CDN console so we set up autoscaling",
    "start": "1958639",
    "end": "1965039"
  },
  {
    "text": "groups for in this case we'll have a C4 node for compute intensive processes we'll set up a auto scaling group of R",
    "start": "1965039",
    "end": "1971399"
  },
  {
    "text": "R3 nodes for memory intensive processes and those all then talk to the server nodes for their configuration and",
    "start": "1971399",
    "end": "1978440"
  },
  {
    "text": "scheduling our original design had the worker nodes talking to the server nodes",
    "start": "1978440",
    "end": "1983559"
  },
  {
    "text": "through an internal elastic load balancer but as we've used cloud formation over the last three years we've really tried to simplify how we do",
    "start": "1983559",
    "end": "1989039"
  },
  {
    "text": "everything and collapse the number of templates that we have so we're able to really simplify this by switching out the load balancer for Route 53 now we",
    "start": "1989039",
    "end": "1996039"
  },
  {
    "text": "have server nodes and we have worker nodes using the same template but we start up the server nodes and they pull",
    "start": "1996039",
    "end": "2001360"
  },
  {
    "text": "in server cloud config and the worker nodes pulling worker Cloud config for people who aren't familiar with Cloud config it's just an on configuration",
    "start": "2001360",
    "end": "2008600"
  },
  {
    "text": "process used to do a one-time configuration of a node and it's all or nothing if it doesn't work the node is dead and we have to go in and figure out",
    "start": "2008600",
    "end": "2014760"
  },
  {
    "text": "why it didn't work and then we don't change it after boot zooming out this optimization also",
    "start": "2014760",
    "end": "2021200"
  },
  {
    "text": "made it easy to unify how everything finds everything the server nodes are using the Route 53 record to find the other server nodes and set up the quum",
    "start": "2021200",
    "end": "2027960"
  },
  {
    "text": "systems the worker nodes are using the Route 53 record to find the server nodes to get configuration and the deploy",
    "start": "2027960",
    "end": "2033639"
  },
  {
    "text": "service is using Route 53 to find the cluster to send the configuration to for deploying",
    "start": "2033639",
    "end": "2039720"
  },
  {
    "text": "services our cloud formation approach has really evolved over time where we initially started with one large stack",
    "start": "2040360",
    "end": "2046120"
  },
  {
    "text": "for an environment but this was tricky to manage because you could make a small change like changing a security group and that would Ripple up and delete",
    "start": "2046120",
    "end": "2051878"
  },
  {
    "text": "resources you never expected to delete and of course get into the worst thing ever update roll back failed which if",
    "start": "2051879",
    "end": "2058079"
  },
  {
    "text": "anyone's seen that they know how painful it is although this is not this is probably",
    "start": "2058079",
    "end": "2063440"
  },
  {
    "text": "not so much a problem now because there are change sets which you can use to see the full extent of a change but this is",
    "start": "2063440",
    "end": "2068679"
  },
  {
    "text": "far before change sets existed so we came up with a layered approach to where we build the dependencies for VPC first",
    "start": "2068679",
    "end": "2075679"
  },
  {
    "text": "as a one stack so that has the connection to the Ops of bpc the pier that we use it has the subnets that we",
    "start": "2075679",
    "end": "2081638"
  },
  {
    "text": "use for communication it has the natat nodes that we use to get out to the internet once that's in place and working we take the outputs of that",
    "start": "2081639",
    "end": "2087599"
  },
  {
    "text": "template and feed it up into the cluster based template now we used to have a bunch of Python scripts to manage this",
    "start": "2087599",
    "end": "2092800"
  },
  {
    "text": "wiring if outputs to inputs for the templates but just recently cloud formation added a feature to actually read output directly into the templates",
    "start": "2092800",
    "end": "2099119"
  },
  {
    "text": "so we've switched over to that and it's a very nice feature once the cluster base is in",
    "start": "2099119",
    "end": "2104800"
  },
  {
    "text": "place that has all of the IM policies Q policies security groups that we need to run clusters we start building all of",
    "start": "2104800",
    "end": "2111280"
  },
  {
    "text": "the other components workers load balancers elasticache clusters as separate stacks on top of that reading",
    "start": "2111280",
    "end": "2117720"
  },
  {
    "text": "the outputs that have propagated up through the VPC base through the cluster base and then into the Clusters this is",
    "start": "2117720",
    "end": "2122880"
  },
  {
    "text": "really nice because everything's isolated and if we need to work on a single cluster we have no chance of affecting other clusters",
    "start": "2122880",
    "end": "2129640"
  },
  {
    "text": "so we have everything in place to recap we've limited the scope we don't we've limited blast radius by using Amazon",
    "start": "2132640",
    "end": "2138720"
  },
  {
    "text": "accounts and vpcs per environment we've made things immutable by using core OS and Cloud",
    "start": "2138720",
    "end": "2144079"
  },
  {
    "text": "config we have our infrastructure well defined as code by using cloud formation and Cloud config but it's immutable how",
    "start": "2144079",
    "end": "2150800"
  },
  {
    "text": "do we maintain it how do we upgrade it over time we use a feature of Autos scaling",
    "start": "2150800",
    "end": "2156119"
  },
  {
    "start": "2155000",
    "end": "2155000"
  },
  {
    "text": "groups called life cycle hooks and what this is is you you can subscribe to notifications from the auto scaling group to do work on state changes in the",
    "start": "2156119",
    "end": "2162800"
  },
  {
    "text": "auto scaling group in this case when nodes come into pending we get a notification that hey we want to add a node to the autoscaling group should we",
    "start": "2162800",
    "end": "2169400"
  },
  {
    "text": "continue and go ahead and do this we we look at that and then add the node to the cluster and when it's safe",
    "start": "2169400",
    "end": "2175520"
  },
  {
    "text": "we move on similarly we can get a notification that things are",
    "start": "2175520",
    "end": "2182560"
  },
  {
    "text": "terminating do the work to remove it from the cluster and then tell the auto scaling group to continue on and let it",
    "start": "2182560",
    "end": "2188640"
  },
  {
    "text": "terminate this is obviously really tedious and we're lazy I'm not going to do it by hand so we built an agent this",
    "start": "2188640",
    "end": "2195599"
  },
  {
    "start": "2190000",
    "end": "2190000"
  },
  {
    "text": "agent subscribes to the auto scaling notifications and Cloud information notifications it's parsing these",
    "start": "2195599",
    "end": "2201839"
  },
  {
    "text": "notifications in Hooks and when it sees that nodes are out of date it does the work to safely replace them in the",
    "start": "2201839",
    "end": "2207359"
  },
  {
    "text": "infrastructure and what I mean by out of date is that it sees there's a node in an autoscaling group that has an old version of the launch configuration when",
    "start": "2207359",
    "end": "2214400"
  },
  {
    "text": "it sees that node with the old version of the launch configuration it goes ahead and kills",
    "start": "2214400",
    "end": "2219839"
  },
  {
    "text": "it then it uses the autoscaling life cycle States and hooks to go through and safely manage that transition this is",
    "start": "2219839",
    "end": "2226720"
  },
  {
    "text": "much easier to understand with a quick visualization so what you're looking at here is a Nal 3 quum bearing set of",
    "start": "2226720",
    "end": "2232680"
  },
  {
    "text": "nodes in a cluster they're all communicating together in a distributed system they're gray because they have an",
    "start": "2232680",
    "end": "2238040"
  },
  {
    "text": "out ofate launch configuration the cluster agent has noticed this and it goes in and terminates a node now the",
    "start": "2238040",
    "end": "2245240"
  },
  {
    "text": "Autos scaling group sends the notification to the agent it sees there's a node in terminate weight it goes in and does the work to",
    "start": "2245240",
    "end": "2252480"
  },
  {
    "text": "safely remove it from the cluster now the distributed system is in an N equals 2 but healthy",
    "start": "2252480",
    "end": "2257560"
  },
  {
    "text": "State and it tells the auto scaling group to continue removing the node at the same time the autoscaling group has",
    "start": "2257560",
    "end": "2264119"
  },
  {
    "text": "started bringing a node up into the pending State we get the notification that there's a node in pending weight State",
    "start": "2264119",
    "end": "2271599"
  },
  {
    "text": "at the same time the old node is done it fully terminates and goes away and we start doing the work to add",
    "start": "2271599",
    "end": "2277720"
  },
  {
    "text": "this node back to the cluster to bring it up to an N equals 3 State we tell the Autos SC group to",
    "start": "2277720",
    "end": "2283599"
  },
  {
    "text": "continue and move on the agent will then go through and terminate all of the out-of-date nodes and then iteratively",
    "start": "2283599",
    "end": "2289480"
  },
  {
    "text": "will'll end up with a fully upgraded cluster just by changing the launch configuration on the autoscaling group so we have everything in place but",
    "start": "2289480",
    "end": "2297480"
  },
  {
    "text": "does any of this actually work let's talk about what we learned from launching a Batman Arkham underworld a mobile game we launched on iOS back in",
    "start": "2297480",
    "end": "2305319"
  },
  {
    "text": "July so I'm going to show you some abs ra graphs without absolute numbers but the interesting thing here",
    "start": "2305319",
    "end": "2311960"
  },
  {
    "text": "coming from previously releasing console games where people have discs ready to go they've downloaded the game they have",
    "start": "2311960",
    "end": "2317400"
  },
  {
    "text": "the DRM unlock right at midnight we were used to a launch where everything comes in right at the same time you see the",
    "start": "2317400",
    "end": "2322880"
  },
  {
    "text": "most traffic ever see immediately and have to handle it whereas with mobile games we had this more gradual launch",
    "start": "2322880",
    "end": "2328119"
  },
  {
    "text": "this is 3 days of traffic this is the calls through each endpoint stacked up over time and over a period of three",
    "start": "2328119",
    "end": "2333720"
  },
  {
    "text": "days we started to climb up the charts and then got to Peak traffic over the weekend",
    "start": "2333720",
    "end": "2339440"
  },
  {
    "text": "but since we're using microservices this traffic is distributed out over different services and what is happening",
    "start": "2339920",
    "end": "2346560"
  },
  {
    "text": "here is no one service is getting more than 20% of the traffic and what that means is no one database is getting more",
    "start": "2346560",
    "end": "2351680"
  },
  {
    "text": "than 20% of the traffic when we look at the databases we're seeing that the DB",
    "start": "2351680",
    "end": "2358480"
  },
  {
    "text": "throughput climb on the right it's climbing up over time but since we're well within our load test predictions",
    "start": "2358480",
    "end": "2363760"
  },
  {
    "text": "the response time is flat as traffic goes up",
    "start": "2363760",
    "end": "2369000"
  },
  {
    "start": "2369000",
    "end": "2369000"
  },
  {
    "text": "at the same time we haven't stopped doing work the deployment system is still doing its regular maintenance maintenance the sort of background churn",
    "start": "2369000",
    "end": "2375839"
  },
  {
    "text": "you're seeing here is old instances being ttld to clean up make sure there's no memory leaks that we didn't find out",
    "start": "2375839",
    "end": "2381240"
  },
  {
    "text": "at the same time we're also releasing new versions of code those are the spikes you see where deployments Spike up for a while releasing new versions",
    "start": "2381240",
    "end": "2388119"
  },
  {
    "text": "kind of cool for us we thought it was interesting that over this weekend we were doing more than 150,000 deployments per day with no player impact throughout",
    "start": "2388119",
    "end": "2394560"
  },
  {
    "text": "the launch so let's talk about what worked some of the things that didn't work so",
    "start": "2394560",
    "end": "2400400"
  },
  {
    "start": "2397000",
    "end": "2397000"
  },
  {
    "text": "well and the lessons that we learned and what we're going to move forward with we're a small platform team that",
    "start": "2400400",
    "end": "2405560"
  },
  {
    "text": "scaled a really large platform across our live environments we have well last week we had 88 databases 300 plus",
    "start": "2405560",
    "end": "2412400"
  },
  {
    "text": "services and 100 plus instances it's probably more than 88 now that we've been at reent for the",
    "start": "2412400",
    "end": "2417720"
  },
  {
    "text": "week we're also running the analytics event ingestion platform for more than nine game for nine",
    "start": "2417720",
    "end": "2424720"
  },
  {
    "text": "games and that come with was great as a platform team launch was mostly boring",
    "start": "2425760",
    "end": "2431079"
  },
  {
    "text": "we' validated our expected distribution and soft launch everything worked within our load test",
    "start": "2431079",
    "end": "2436520"
  },
  {
    "text": "predictions we hit High scale overall but we've distributed this out over various microservices and no individual",
    "start": "2436520",
    "end": "2442560"
  },
  {
    "text": "service handled more than 20% of the traffic we had the deployment tools and",
    "start": "2442560",
    "end": "2448560"
  },
  {
    "text": "the metrics in place to be able to easily scale with traffic and it was also interesting that we saw this mobile",
    "start": "2448560",
    "end": "2453960"
  },
  {
    "text": "scale up was much more gradual than a digital console Game launch so it took a few days but we just scaled up gradually",
    "start": "2453960",
    "end": "2459560"
  },
  {
    "text": "as the game climbed up the charts and got more popular one of the things we learned",
    "start": "2459560",
    "end": "2464640"
  },
  {
    "text": "from our last game is we really didn't want to have a war room we didn't want people sitting in a room staring at all of the metrics all day and all night for",
    "start": "2464640",
    "end": "2470319"
  },
  {
    "text": "days we have alerts we trusted our alerts and we had an on call rotation but people were at home and not required",
    "start": "2470319",
    "end": "2476760"
  },
  {
    "text": "to sit in a room and watch metrics all night this was really",
    "start": "2476760",
    "end": "2482319"
  },
  {
    "text": "successful the other thing is we didn't show all the metrics we went through the metrics and we were very careful with",
    "start": "2482480",
    "end": "2488119"
  },
  {
    "text": "what we showed we curated the visible metrics we had like really useful highle",
    "start": "2488119",
    "end": "2493280"
  },
  {
    "text": "metrics for everyone to look at that were all over the place the number of new players we had the amount of money we were",
    "start": "2493280",
    "end": "2498520"
  },
  {
    "text": "making and we also had some fun metrics to keep people engaged one of the cool things we had is we took our analytics",
    "start": "2498520",
    "end": "2503760"
  },
  {
    "text": "data and made a pew pew map of PVP matches across the world so we could see a real-time map of players attacking",
    "start": "2503760",
    "end": "2509000"
  },
  {
    "text": "other players across the globe that was pretty cool we also just had some fun stuff like players named Joker versus",
    "start": "2509000",
    "end": "2514160"
  },
  {
    "text": "players named Batman to see whether the light or the dark side was winning",
    "start": "2514160",
    "end": "2519240"
  },
  {
    "text": "these are definite hits but at the same time we still had all of the detailed metrics that we",
    "start": "2520160",
    "end": "2525480"
  },
  {
    "text": "needed to make scaling decisions and to trigger our alerts but they just weren't displayed",
    "start": "2525480",
    "end": "2531640"
  },
  {
    "start": "2531000",
    "end": "2531000"
  },
  {
    "text": "everywhere what didn't work one of the big things we learned is that oops that's two clicks uh one of the big",
    "start": "2531640",
    "end": "2538000"
  },
  {
    "text": "things we learned is that under load you have that local console agent and the etcd agent maintaining the nodes the",
    "start": "2538000",
    "end": "2543839"
  },
  {
    "text": "worker nodes in the cluster but under load they can stop responding to act and the members will flap out of the cluster",
    "start": "2543839",
    "end": "2550240"
  },
  {
    "text": "the problem here is when they flap out of the cluster the schedulers say oh this service isn't healthy anymore I better send it somewhere else of course",
    "start": "2550240",
    "end": "2557359"
  },
  {
    "text": "for our stateless Services where we're running many copies for high availability it's not a problem no errors but since we're running databases",
    "start": "2557359",
    "end": "2564240"
  },
  {
    "text": "just as another service if a database was on a node that flaps it would get sent to another node and the service would be unavailable while the database",
    "start": "2564240",
    "end": "2570720"
  },
  {
    "text": "moved this is unfortunate so we came up with a quick",
    "start": "2570720",
    "end": "2575880"
  },
  {
    "text": "workaround we just put a bunch of retries on and said we're not going to move a we're not going to move a service",
    "start": "2575880",
    "end": "2581800"
  },
  {
    "text": "until we've seen the node flap at least three or four times this is pretty successful but this is an active area of",
    "start": "2581800",
    "end": "2587599"
  },
  {
    "text": "research and one of the interesting things we've seen is that surf which Powers console has this new thing called lifeguard which is a more Dynamic",
    "start": "2587599",
    "end": "2593760"
  },
  {
    "text": "approach to handling AXS under load it looks like it'll be a really interesting",
    "start": "2593760",
    "end": "2599079"
  },
  {
    "text": "approach what we took away from this is that we didn't learn everything we could have from soft launch we'd seen these",
    "start": "2599079",
    "end": "2605040"
  },
  {
    "text": "weird reschedules in soft launch infrequently but because it was very infrequent it was very easy to write off as just oh",
    "start": "2605040",
    "end": "2610800"
  },
  {
    "text": "it's the network being the network once we were underload live it",
    "start": "2610800",
    "end": "2616480"
  },
  {
    "text": "became much more common but we had the logs and metrics to go in and rapidly root cause it and come up with that workaround that I mentioned the lesson",
    "start": "2616480",
    "end": "2623680"
  },
  {
    "text": "we're really taking away from this is don't ignore transient problems and pre-prod they're only going to get worse at launch and if you can't root cause it",
    "start": "2623680",
    "end": "2629599"
  },
  {
    "text": "you've got a real problem the other thing we saw is in Dev",
    "start": "2629599",
    "end": "2634880"
  },
  {
    "text": "partial partial failure handling is not something that you see all the time so it's easy to overlook",
    "start": "2634880",
    "end": "2640440"
  },
  {
    "text": "it but at load someone somewhere is having an error so you really need to think about",
    "start": "2640440",
    "end": "2645559"
  },
  {
    "text": "it the thing we're doing now is when something is broken in Dev and people are having problems with errors we don't",
    "start": "2645559",
    "end": "2652000"
  },
  {
    "text": "fix it so fast we figure out how to handle the errors before we fix the problem and as we move forward we're",
    "start": "2652000",
    "end": "2657319"
  },
  {
    "text": "going to start implementing chaos monkeys and just generating errors so that we have to deal with problems in Dev before we get",
    "start": "2657319",
    "end": "2664920"
  },
  {
    "text": "live of course the bane of anybody running a database at load is latency spikes and we had load tested heavily",
    "start": "2665599",
    "end": "2671920"
  },
  {
    "text": "but we saw some anomalous behavior in prod this was a pretty unique database as Kyle mentioned we were migrating uh",
    "start": "2671920",
    "end": "2678160"
  },
  {
    "text": "from our Legacy systems to this new platform and we had migrated one database that we couldn't split up in a migrated data into microservices as just",
    "start": "2678160",
    "end": "2685200"
  },
  {
    "text": "a blob store that we moved to a new service um on our new platform so it had really unique",
    "start": "2685200",
    "end": "2691920"
  },
  {
    "text": "performance characteristics and it turned out the problem was we just hadn't disabled transparent huge Pages for people aren't familiar with this",
    "start": "2691920",
    "end": "2697880"
  },
  {
    "text": "this is a Linux kernel feature which will go through and defragment memory for you to satisfy allocations but if",
    "start": "2697880",
    "end": "2703520"
  },
  {
    "text": "you have 64 gigs of RAM that can stall your process for quite a while so we saw",
    "start": "2703520",
    "end": "2708880"
  },
  {
    "text": "this memory compaction happening fre frequently correlated with latency spikes went through and disabled thp and",
    "start": "2708880",
    "end": "2715319"
  },
  {
    "text": "this reduced the compaction rate and reduced the latency spikes we also had a more periodic",
    "start": "2715319",
    "end": "2720680"
  },
  {
    "text": "latency Spike where it turned out every time we took a backup we were saturating the network interface so we went through",
    "start": "2720680",
    "end": "2726440"
  },
  {
    "text": "and throttled the backup alert that was one of the deploys you could see uh turned that throttling on and that got",
    "start": "2726440",
    "end": "2731640"
  },
  {
    "text": "rid of that periodic latency Spike so we're lazy we have this cluster",
    "start": "2731640",
    "end": "2738640"
  },
  {
    "text": "agent it can go through and terminate nodes for us to get upgrades well it got a little overzealous and deleted",
    "start": "2738640",
    "end": "2746040"
  },
  {
    "text": "prod uh So what had happened is we had ignored the classic if it hurts do it",
    "start": "2748599",
    "end": "2753720"
  },
  {
    "text": "often and we hadn't been running the agent we had this delayed prod upgrade so we finally got to the point where we're like oh yeah we should turn on the",
    "start": "2753720",
    "end": "2759599"
  },
  {
    "text": "agent and do this prod upgrade but we'd accumulated some bugs and Tech debt and it ended up being two",
    "start": "2759599",
    "end": "2765359"
  },
  {
    "text": "and a half hours of downtime which really for deleting prod not that bad uh really simple bug where running",
    "start": "2765359",
    "end": "2773040"
  },
  {
    "text": "is not equal to running the mistake we made is that ec2 running is not the same as autoscaling group in Service",
    "start": "2773040",
    "end": "2778280"
  },
  {
    "text": "autoscaling Group in service means you've done the work to add it and it's safe to move forward so the agent saw",
    "start": "2778280",
    "end": "2783680"
  },
  {
    "text": "enough ec2 running nodes and thought it's cool to terminate stuff and went through and terminated everything well",
    "start": "2783680",
    "end": "2789599"
  },
  {
    "text": "now we have a quorum system but it's fresh everything's been deleted we've lost all of our configuration we've lost",
    "start": "2789599",
    "end": "2795319"
  },
  {
    "text": "all of our service Discovery we've lost all of our scheduling information so good job deleted prod",
    "start": "2795319",
    "end": "2803000"
  },
  {
    "text": "thank you but we've planned for this we have a mut a mutable config we can redeploy everything from chat this will",
    "start": "2803000",
    "end": "2809720"
  },
  {
    "text": "be really easy we'll just type a bunch of compans will be back up in 15 20 30 minutes but because we hadn't been doing",
    "start": "2809720",
    "end": "2815599"
  },
  {
    "text": "it often it took about 2 hours found bugs had some external issues that",
    "start": "2815599",
    "end": "2821359"
  },
  {
    "text": "were out of our control but we learned a lot so we have",
    "start": "2821359",
    "end": "2826720"
  },
  {
    "text": "some new procedures out of this first we're going to make errors deploy the",
    "start": "2826720",
    "end": "2832000"
  },
  {
    "text": "semi Army to shake out these kind of bugs early and also one other thing we ran",
    "start": "2832000",
    "end": "2837359"
  },
  {
    "text": "into we were running n equals 3 Quorum nodes in prod but of course when you're",
    "start": "2837359",
    "end": "2842680"
  },
  {
    "text": "doing the work maintenance work on one node and n equals 3 it's inevitable that ec2 node will fail a health check and",
    "start": "2842680",
    "end": "2848400"
  },
  {
    "text": "get replaced automatically at the same time and you're now down to one out of three nodes and you've broken Quorum and you're in trouble so I'm proud We Run",
    "start": "2848400",
    "end": "2854599"
  },
  {
    "text": "four five Corum nodes for safety we had a lot of really interesting challenges along the way",
    "start": "2854599",
    "end": "2861040"
  },
  {
    "text": "getting to this place one of the things was we were migrating from this Legacy system running out a nosql",
    "start": "2861040",
    "end": "2868519"
  },
  {
    "text": "database and we had split up a lot of the features of the old platform into microservices for the platform but there",
    "start": "2869640",
    "end": "2875480"
  },
  {
    "text": "was one service which was just a blob store that the game logic needed to be able to put stuff in there to make the game",
    "start": "2875480",
    "end": "2882160"
  },
  {
    "text": "work um but this was incompatible with our current API design so we we wrote a new service supporting equivalent",
    "start": "2882599",
    "end": "2888720"
  },
  {
    "text": "features to the old service but based on postgress 9.5 with our current API design goals just using Json b Blobs and",
    "start": "2888720",
    "end": "2895240"
  },
  {
    "text": "some metadata rows about those documents but at this point we're live in soft launch we can't take the game",
    "start": "2895240",
    "end": "2901480"
  },
  {
    "text": "down for weeks to do this full migration so we built a shim to migrate from the old service to the new service where",
    "start": "2901480",
    "end": "2908520"
  },
  {
    "text": "what it was able to do is translate from the old API to the new API and also lazily migrate data from the old service",
    "start": "2908520",
    "end": "2914119"
  },
  {
    "text": "to the new service this was pretty straightforward we just did it in three phases where we set up a pass through phase we put the",
    "start": "2914119",
    "end": "2920400"
  },
  {
    "text": "shim in place and it would just send requests through the shim we then started validating the new service where",
    "start": "2920400",
    "end": "2925680"
  },
  {
    "text": "we would send requests to both Services track any differences and go through and fix whatever was causing those",
    "start": "2925680",
    "end": "2932040"
  },
  {
    "text": "differences once there were no differences we started doing a lazy migration from the old service to the new service we would send requests to",
    "start": "2932040",
    "end": "2938359"
  },
  {
    "text": "the new service if it's working if it's fine if we see a player who was not in the new service database the shim would",
    "start": "2938359",
    "end": "2944119"
  },
  {
    "text": "copy their data over and then let it come from the new service once that had been running for a couple weeks we",
    "start": "2944119",
    "end": "2949400"
  },
  {
    "text": "copied over all of the active players and just did a bulk load to get the less active players over the new system and",
    "start": "2949400",
    "end": "2954440"
  },
  {
    "text": "shut down the old service this worked really well and it was pretty cool to do this live without any downtime over the",
    "start": "2954440",
    "end": "2959920"
  },
  {
    "text": "period of a few weeks this is a really interesting",
    "start": "2959920",
    "end": "2965240"
  },
  {
    "text": "lesson in running postgress at scale one of the things we learned is that large Json b Blobs plus mvcc leads to a",
    "start": "2965240",
    "end": "2972119"
  },
  {
    "text": "lot of IO and from people so mvcc is multiversion concurrency control and what this means is you get row level",
    "start": "2972119",
    "end": "2978280"
  },
  {
    "text": "locking but when you update a row a new copy of the row is made and when the old version is no longer needed it gets",
    "start": "2978280",
    "end": "2983480"
  },
  {
    "text": "vacuumed up but because we had these large documents which contain things like locks and time stamps they were being updated on almost every request",
    "start": "2983480",
    "end": "2990680"
  },
  {
    "text": "it's important to note that probably 90% of our requests are rights because they're touching these locks and time stamps so every time hit a row we would",
    "start": "2990680",
    "end": "2997400"
  },
  {
    "text": "make a new copy of a 50k row and getting a ton of IO but since we're in a relational",
    "start": "2997400",
    "end": "3003839"
  },
  {
    "text": "database we can pull out the frequently updated Fields put them in a regular row and pull them out of The Blob and leave",
    "start": "3003839",
    "end": "3009720"
  },
  {
    "text": "the less frequently touched stuff in the blob this worked really well and reduced our",
    "start": "3009720",
    "end": "3015000"
  },
  {
    "text": "IO we also had a problem where we have these large documents large tables they're growing on dis really",
    "start": "3015000",
    "end": "3021440"
  },
  {
    "text": "fast and previous to post gross 9.6 it it grows tables on discs by a fixed amount out and every time it grows a",
    "start": "3021440",
    "end": "3028640"
  },
  {
    "text": "table it takes an exclusive lock on the table since we're growing tables all the time this is a real",
    "start": "3028640",
    "end": "3034040"
  },
  {
    "text": "problem so what we had to do is partition the tables so that the table locks were on much smaller sections of",
    "start": "3034040",
    "end": "3039319"
  },
  {
    "text": "the overall table that we're working with this worked well but in postgress 9.6 they added a new feature where the",
    "start": "3039319",
    "end": "3045599"
  },
  {
    "text": "growth on dis is dynamic based on the rate the table is growing at so this is a cool feature that we're really excited",
    "start": "3045599",
    "end": "3052318"
  },
  {
    "text": "about now at this point the platform is stable it's reason stable so we're really focusing on how can we squeeze",
    "start": "3052400",
    "end": "3059119"
  },
  {
    "start": "3053000",
    "end": "3053000"
  },
  {
    "text": "out all of the efficiency we can get out of the system and we have a lot of data we've been collecting metrics for",
    "start": "3059119",
    "end": "3064599"
  },
  {
    "text": "ages so we started with some questions you know how periodic is the resource",
    "start": "3064599",
    "end": "3070319"
  },
  {
    "text": "usage is it just a simple daily pattern does it vary by day by week by",
    "start": "3070319",
    "end": "3075480"
  },
  {
    "text": "month and moreover should we scale or is this something anomalous and should we alert on it instead of",
    "start": "3075480",
    "end": "3082119"
  },
  {
    "text": "scaling basically how can we extract all the information we can out of the data that we have accumulated over",
    "start": "3082119",
    "end": "3089319"
  },
  {
    "start": "3089000",
    "end": "3089000"
  },
  {
    "text": "time so what I'm going to show you is a bunch of data that we've collected and kind of organized to start answering these questions what you're seeing here",
    "start": "3089319",
    "end": "3095520"
  },
  {
    "text": "is a month of CPU utilization for one game one individual game it starts on Sunday and goes through Saturday and you",
    "start": "3095520",
    "end": "3102280"
  },
  {
    "text": "can see the sort of Monday pattern is pretty similar Tuesday pattern is pretty similar Sunday through Tuesday are",
    "start": "3102280",
    "end": "3108440"
  },
  {
    "text": "pretty similar looking but Friday is unique so Friday has this like very sharp increase and then rapid decrease",
    "start": "3108440",
    "end": "3114559"
  },
  {
    "text": "in utilization so if we just took a 24 a 24hour average and use that as our model to scale we'd underestimate how much to",
    "start": "3114559",
    "end": "3121640"
  },
  {
    "text": "scale on Fridays and then scale down too slowly on the back end of the peak when we look at CPU utilization for",
    "start": "3121640",
    "end": "3128599"
  },
  {
    "text": "a different game but just that game it's very different it has this bodal Behavior where us players and China",
    "start": "3128599",
    "end": "3134440"
  },
  {
    "text": "players have Peaks that are individual and the ratio of those Peaks change on different",
    "start": "3134440",
    "end": "3140240"
  },
  {
    "text": "days so the lesson that we're trying to embed into the model that we're building and validating at this point is that the",
    "start": "3140920",
    "end": "3146680"
  },
  {
    "text": "day of week pattern appears to be consistent but it's a uni a unique pattern for each day of the week so we",
    "start": "3146680",
    "end": "3152000"
  },
  {
    "text": "have historical data for each day of the week so we can look at the current rate of change and say oh this is Friday how",
    "start": "3152000",
    "end": "3157760"
  },
  {
    "text": "much would we scale on a Friday based on the historical data we have for scaling on",
    "start": "3157760",
    "end": "3163400"
  },
  {
    "text": "Fridays so I'm going to hand this back over to Kyle to talk about some of the conclusions we've drawn from all",
    "start": "3163480",
    "end": "3170079"
  },
  {
    "text": "this so what have we learned well we really like our microservices",
    "start": "3174079",
    "end": "3180680"
  },
  {
    "text": "platform um it's really allowed us to quickly add and change features and get them deployed rapidly into production",
    "start": "3180680",
    "end": "3188000"
  },
  {
    "start": "3181000",
    "end": "3181000"
  },
  {
    "text": "for uh both games so we can also really easily swap out",
    "start": "3188000",
    "end": "3194720"
  },
  {
    "text": "game logic from one to the other when we need to which is really helpful as well and it's been easy to scale so",
    "start": "3194720",
    "end": "3202280"
  },
  {
    "text": "far but this doesn't come without cost um we spent a lot of upfront time",
    "start": "3202280",
    "end": "3207839"
  },
  {
    "text": "investing in our deployment tools and these days there are other",
    "start": "3207839",
    "end": "3214280"
  },
  {
    "text": "options out there with Amazon ECS and API Gateway and things like kubernetes",
    "start": "3214280",
    "end": "3219720"
  },
  {
    "text": "you can probably save yourself a lot of time and money um but those weren't available back when we did this",
    "start": "3219720",
    "end": "3226119"
  },
  {
    "text": "so um but even with that said you still need to spend a good amount of time",
    "start": "3226119",
    "end": "3231359"
  },
  {
    "text": "planning out your apis to avoid getting into into trouble with a tangled web of",
    "start": "3231359",
    "end": "3238040"
  },
  {
    "text": "calls um so remember if it hurts do it",
    "start": "3238040",
    "end": "3244920"
  },
  {
    "start": "3240000",
    "end": "3240000"
  },
  {
    "text": "often break things in Dev all the time force your client teams and yourself to",
    "start": "3244920",
    "end": "3251240"
  },
  {
    "text": "handle errors load test everything together so",
    "start": "3251240",
    "end": "3259000"
  },
  {
    "text": "we spend a lot of time load testing but we only low tested our services one at a time if we had low tested our platform",
    "start": "3259000",
    "end": "3266200"
  },
  {
    "text": "as as a whole we would have found that flapping issue and we would have fixed it before we went to",
    "start": "3266200",
    "end": "3271720"
  },
  {
    "text": "production so remember that you want to do both right you want",
    "start": "3271720",
    "end": "3278000"
  },
  {
    "text": "to test your services see how far they can go and you want to test your platform as a",
    "start": "3278000",
    "end": "3283240"
  },
  {
    "text": "whole so what's next for us well we're going to be writing more",
    "start": "3283240",
    "end": "3288280"
  },
  {
    "start": "3284000",
    "end": "3284000"
  },
  {
    "text": "microservices we're going to be deploying the predictive scaling algorithm that Will was talking",
    "start": "3288280",
    "end": "3294799"
  },
  {
    "text": "about we're going to add more chaos to our Dev environments we're going to be dropping packets we're going to be",
    "start": "3294799",
    "end": "3301079"
  },
  {
    "text": "returning invalid results if you're expecting Json you might get a random stream of btes",
    "start": "3301079",
    "end": "3308200"
  },
  {
    "text": "back we're going to force ourselves to be better about errors and we're investigating Nomad as",
    "start": "3308200",
    "end": "3315680"
  },
  {
    "text": "a replacement for our Fleet scheduler um and the reason there is",
    "start": "3315680",
    "end": "3321000"
  },
  {
    "text": "that it'll allow us to remove ETD from our platform which will simplify things",
    "start": "3321000",
    "end": "3326359"
  },
  {
    "text": "even for even further so hopefully we'll be back for",
    "start": "3326359",
    "end": "3332599"
  },
  {
    "text": "another talk next year thank you for your",
    "start": "3332599",
    "end": "3336680"
  },
  {
    "text": "time",
    "start": "3338599",
    "end": "3341599"
  }
]