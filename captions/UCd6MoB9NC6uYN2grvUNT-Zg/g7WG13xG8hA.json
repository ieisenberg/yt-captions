[
  {
    "start": "0",
    "end": "16000"
  },
  {
    "text": "greetings I'm Matt word and I'm here at",
    "start": "4520",
    "end": "6399"
  },
  {
    "text": "the AWS headquarters in Seattle to tell",
    "start": "6399",
    "end": "8480"
  },
  {
    "text": "you about a new way to analyze huge",
    "start": "8480",
    "end": "10080"
  },
  {
    "text": "amounts of data more efficiently with",
    "start": "10080",
    "end": "12080"
  },
  {
    "text": "elastic M produse by using S3 as a data",
    "start": "12080",
    "end": "14759"
  },
  {
    "text": "store for hbas hbas is an open- Source",
    "start": "14759",
    "end": "17680"
  },
  {
    "start": "16000",
    "end": "32000"
  },
  {
    "text": "tool which is perfect for Random Access",
    "start": "17680",
    "end": "19720"
  },
  {
    "text": "reads and writes over very large",
    "start": "19720",
    "end": "21600"
  },
  {
    "text": "distributed data sets in her dup",
    "start": "21600",
    "end": "23680"
  },
  {
    "text": "previously you needed to run relatively",
    "start": "23680",
    "end": "25560"
  },
  {
    "text": "large dupe clusters sized to fit the",
    "start": "25560",
    "end": "27640"
  },
  {
    "text": "volume of your data instead of the",
    "start": "27640",
    "end": "29400"
  },
  {
    "text": "compute res sources required to analyze",
    "start": "29400",
    "end": "31240"
  },
  {
    "text": "it with this new feature you can access",
    "start": "31240",
    "end": "33719"
  },
  {
    "text": "data through hbas directly from S3",
    "start": "33719",
    "end": "36600"
  },
  {
    "text": "decoupling your big data storage and",
    "start": "36600",
    "end": "38440"
  },
  {
    "text": "compute requirements this means you can",
    "start": "38440",
    "end": "40520"
  },
  {
    "start": "40000",
    "end": "59000"
  },
  {
    "text": "size your cluster based entirely on your",
    "start": "40520",
    "end": "42559"
  },
  {
    "text": "compute requirements dramatically",
    "start": "42559",
    "end": "44160"
  },
  {
    "text": "reducing the size of your Hadoop cluster",
    "start": "44160",
    "end": "46039"
  },
  {
    "text": "and lowering cost you can get started",
    "start": "46039",
    "end": "48000"
  },
  {
    "text": "with S3 hbas and EMR today using the",
    "start": "48000",
    "end": "50600"
  },
  {
    "text": "console the sdks or the apis thanks a",
    "start": "50600",
    "end": "53320"
  },
  {
    "text": "lot",
    "start": "53320",
    "end": "56320"
  }
]