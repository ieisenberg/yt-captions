[
  {
    "start": "0",
    "end": "180000"
  },
  {
    "text": "hi everyone I hope you're feeling good after lunch",
    "start": "680",
    "end": "7880"
  },
  {
    "text": "thank you for joining us in this session securing enterprise big data workloads",
    "start": "7880",
    "end": "13440"
  },
  {
    "text": "on AWS my name is mo tez and any I'm a Solutions Architect and I'm happy to be",
    "start": "13440",
    "end": "20130"
  },
  {
    "text": "with you today I am also joined by Nate Salmons who is a principal architect at",
    "start": "20130",
    "end": "25590"
  },
  {
    "text": "NASDAQ now let me start by saying that",
    "start": "25590",
    "end": "32880"
  },
  {
    "text": "enterprise data warehousing is still very relevant and alive today and there",
    "start": "32880",
    "end": "38160"
  },
  {
    "text": "is no better evidence to that than what we see in the field companies keep pouring terabytes upon",
    "start": "38160",
    "end": "44520"
  },
  {
    "text": "terabytes of data into the traditional on-premises data warehouses and they",
    "start": "44520",
    "end": "50190"
  },
  {
    "text": "often keep doing so until they reach the breaking point of traditional technology and cross into the realm of big data",
    "start": "50190",
    "end": "59059"
  },
  {
    "text": "and when that happens companies turn to the cloud for a scalable solution that",
    "start": "59059",
    "end": "64408"
  },
  {
    "text": "can handle their data growth for years to come and that solution is the hybrid",
    "start": "64409",
    "end": "69780"
  },
  {
    "text": "enterprise data warehouse were part of the data warehouses on-premises and the",
    "start": "69780",
    "end": "75299"
  },
  {
    "text": "other part is in the AWS cloud but just as companies start making this",
    "start": "75299",
    "end": "81780"
  },
  {
    "text": "transition and after a lot of enthusiasm have been built security often comes in as a",
    "start": "81780",
    "end": "88829"
  },
  {
    "text": "reality check questions come up such as what are the security controls that are available for",
    "start": "88829",
    "end": "94890"
  },
  {
    "text": "us in the hybrid enterprise data warehouse will we be able to maintain or improve",
    "start": "94890",
    "end": "100079"
  },
  {
    "text": "our security posture as we make this transition this session aims to help you answer",
    "start": "100079",
    "end": "106290"
  },
  {
    "text": "these questions and more and to that end I'm going to show you what the typical",
    "start": "106290",
    "end": "112110"
  },
  {
    "text": "hybrid enterprise data warehouse looks like I'm going to take you in a walkthrough",
    "start": "112110",
    "end": "117119"
  },
  {
    "text": "of its architecture and its data flows and then we are going to apply security controls across this hybrid enterprise",
    "start": "117119",
    "end": "124350"
  },
  {
    "text": "data warehouse and finally we are saving the best for last Nate is going to share with you how",
    "start": "124350",
    "end": "130890"
  },
  {
    "text": "hybrid enterprise data warehouse security is actually at Nasdaq",
    "start": "130890",
    "end": "136099"
  },
  {
    "text": "in order for this session to live up to its promise it has been designed to be high level enough and broad in its",
    "start": "138080",
    "end": "144150"
  },
  {
    "text": "coverage of relevant AWS services such that it helps you put together an effective information security plan for",
    "start": "144150",
    "end": "150810"
  },
  {
    "text": "your hybrid enterprise data warehouse that said this session must be supplemented",
    "start": "150810",
    "end": "157380"
  },
  {
    "text": "with diligent information security planning planning that starts with active thinking and brainstorming of the",
    "start": "157380",
    "end": "163860"
  },
  {
    "text": "relevant risks and threats and end with a justified and concise list of security",
    "start": "163860",
    "end": "170670"
  },
  {
    "text": "controls that you must implement now let us take a look at the hybrid",
    "start": "170670",
    "end": "177450"
  },
  {
    "text": "enterprise data warehouse this is a high level conceptual architecture on the right hand side",
    "start": "177450",
    "end": "184170"
  },
  {
    "start": "180000",
    "end": "180000"
  },
  {
    "text": "there is your AWS environment in which you would leverage services such as Amazon s3 Amazon EMR and Amazon redshift",
    "start": "184170",
    "end": "191630"
  },
  {
    "text": "to store process and analyze your data Amazon s3 offers you a highly durable",
    "start": "191630",
    "end": "198630"
  },
  {
    "text": "object store with virtually unlimited scalability you can store as much late as you want Amazon EMR is a cluster",
    "start": "198630",
    "end": "204720"
  },
  {
    "text": "management platform that simplifies running Big Data frameworks such as Apache Hadoop and Apache spark and",
    "start": "204720",
    "end": "211200"
  },
  {
    "text": "Amazon redshift is a petabyte scale relational database management and querying system that has been designed",
    "start": "211200",
    "end": "216959"
  },
  {
    "text": "for business intelligence workloads now on the left hand side you have your",
    "start": "216959",
    "end": "223139"
  },
  {
    "text": "on-premises environment in which your various enterprise data sources resides such as your database servers file",
    "start": "223139",
    "end": "229859"
  },
  {
    "text": "shares and FTP servers and from which your different end-user groups would",
    "start": "229859",
    "end": "235739"
  },
  {
    "text": "access the data in AWS and because we are talking about a",
    "start": "235739",
    "end": "240959"
  },
  {
    "text": "hybrid data warehouse that means potentially up to few hundreds of gigabytes of data exchanged daily",
    "start": "240959",
    "end": "246810"
  },
  {
    "text": "between both environments in order to support that both environments should be",
    "start": "246810",
    "end": "252000"
  },
  {
    "text": "connected via an AWS direct connect which provides you with up to 10 gigabits per second of direct physical",
    "start": "252000",
    "end": "258840"
  },
  {
    "text": "connectivity from your on-premises networking equipment and into an AWS environment",
    "start": "258840",
    "end": "265410"
  },
  {
    "text": "now let's take a look at the data flows the first data flow is the extract",
    "start": "265410",
    "end": "271840"
  },
  {
    "text": "upload and transform data flow in this data flow your data engineers would set",
    "start": "271840",
    "end": "277630"
  },
  {
    "text": "up automated tasks and agents that would extract data from your enterprise data sources and then upload this data into",
    "start": "277630",
    "end": "283930"
  },
  {
    "text": "Amazon s3 once you delete is an Amazon s3 your data engineers would usually want to",
    "start": "283930",
    "end": "289720"
  },
  {
    "text": "apply transformations to make the data suitable for two more purposes",
    "start": "289720",
    "end": "294750"
  },
  {
    "text": "the first purpose is further exploration analysis and manipulation by your data",
    "start": "294750",
    "end": "300820"
  },
  {
    "text": "scientists using the Hadoop ecosystem of tools on Amazon EMR and the second",
    "start": "300820",
    "end": "306610"
  },
  {
    "text": "purpose is business intelligence creating and analysis which includes visualization",
    "start": "306610",
    "end": "312340"
  },
  {
    "text": "and dashboarding by your business analysts and end-users using Amazon",
    "start": "312340",
    "end": "317830"
  },
  {
    "text": "redshift along with Amazon quick site or other partner ecosystem tools",
    "start": "317830",
    "end": "323280"
  },
  {
    "text": "now the question is how can you make such an architecture secure and the",
    "start": "323280",
    "end": "328450"
  },
  {
    "text": "answer is to start addy foundation and start with AWS Identity and Access Management",
    "start": "328450",
    "end": "333960"
  },
  {
    "text": "AWS I am is the service that you'd use to authenticate and authorize access to",
    "start": "333960",
    "end": "339580"
  },
  {
    "start": "335000",
    "end": "335000"
  },
  {
    "text": "your AWS account and resources within it using AWS I am you can define users",
    "start": "339580",
    "end": "344980"
  },
  {
    "text": "groups and roles and you can author Identity and Access Management policies that allow or deny access to specific",
    "start": "344980",
    "end": "351460"
  },
  {
    "text": "AWS API actions and because AWS I am operates at the API action level there",
    "start": "351460",
    "end": "358450"
  },
  {
    "text": "are two important implications first is that AWS ions reach extends",
    "start": "358450",
    "end": "365830"
  },
  {
    "text": "into Amazon s3 and can be used to provide fine-grained permissions over Amazon s3 buckets and up even objects",
    "start": "365830",
    "end": "373090"
  },
  {
    "text": "within those buckets that is possible because Amazon s3 is a fully managed service",
    "start": "373090",
    "end": "378810"
  },
  {
    "text": "that is only exposed through AWS API actions we can contrast that to Amazon",
    "start": "378810",
    "end": "384850"
  },
  {
    "text": "EMR and Amazon redshift with both EMR and redshift AWS I am controls only what happens outside your",
    "start": "384850",
    "end": "391810"
  },
  {
    "text": "cluster that includes things done to your clusters such as cluster creation",
    "start": "391810",
    "end": "396820"
  },
  {
    "text": "modification or deletion and includes things done by your clusters such as reading or writing data",
    "start": "396820",
    "end": "402580"
  },
  {
    "text": "to Amazon s3 buckets what happens inside your cluster is a",
    "start": "402580",
    "end": "408820"
  },
  {
    "text": "totally different story and such activity is controlled using EMR and ratchets own internal authentication",
    "start": "408820",
    "end": "415390"
  },
  {
    "text": "authorization mechanisms which we are going to discuss later on in this session",
    "start": "415390",
    "end": "421229"
  },
  {
    "text": "now the second important implication is that it is absolutely crucial to understand each and every API action for",
    "start": "422310",
    "end": "430030"
  },
  {
    "text": "Amazon s3 Amazon EMR Amazon redshift and that understanding is necessary in order to be able to offer concise and least",
    "start": "430030",
    "end": "437140"
  },
  {
    "text": "privileged Identity and Access Management policies that closely match common activities that are performed in",
    "start": "437140",
    "end": "443500"
  },
  {
    "text": "your enterprise data warehouse such activities include cluster management for example for Amazon EMR",
    "start": "443500",
    "end": "449680"
  },
  {
    "text": "and Amazon redshift or just general administration of Amazon s3",
    "start": "449680",
    "end": "454500"
  },
  {
    "text": "once we are done with understanding those API actions and putting together Identity and Access Management policies",
    "start": "456450",
    "end": "462910"
  },
  {
    "text": "a next step would be to attach those policies to the users groups and roles of our enterprise data warehouse in",
    "start": "462910",
    "end": "469950"
  },
  {
    "text": "order to make such an effort manageable you can design an access control matrix",
    "start": "469950",
    "end": "475330"
  },
  {
    "text": "such as this one on such an axis control matrix the different rows represent individual AWS",
    "start": "475330",
    "end": "482590"
  },
  {
    "text": "identities and the and the columns represent individual AWS services or resources",
    "start": "482590",
    "end": "488100"
  },
  {
    "text": "the intersection between a row and the column would be the list of Identity and Access Management policies that must",
    "start": "488100",
    "end": "494320"
  },
  {
    "text": "apply to that AWS identity and AWS resource and because we will be applying",
    "start": "494320",
    "end": "500080"
  },
  {
    "text": "combinations of iron policies it is a recommended practice that you leverage the AWS I am policy simulator in order",
    "start": "500080",
    "end": "507490"
  },
  {
    "text": "to verify that those combinations of policies yield the intended permissions",
    "start": "507490",
    "end": "513510"
  },
  {
    "text": "now for sensitive API actions you would want to layer extra security controls in",
    "start": "514200",
    "end": "519460"
  },
  {
    "text": "order to prevent malicious or accidental execution and for that you can use AWS I",
    "start": "519460",
    "end": "525670"
  },
  {
    "text": "am policy conditions in order to require multi-factor authentication to execute such sensitive api's",
    "start": "525670",
    "end": "532150"
  },
  {
    "text": "when you enable MFA on a set of epi actions you'd require that your AWS user",
    "start": "532150",
    "end": "538120"
  },
  {
    "text": "has virtual or a physical token at hand in order to be able to execute such actions",
    "start": "538120",
    "end": "545010"
  },
  {
    "text": "you can also use I M policy conditions on order to specify further restrictions",
    "start": "545010",
    "end": "550810"
  },
  {
    "text": "on such api's including for example a specific source IP address range or a",
    "start": "550810",
    "end": "556900"
  },
  {
    "text": "specific time of day and just as you'd use AWS I am to",
    "start": "556900",
    "end": "563860"
  },
  {
    "text": "control access for users and groups within your account you would also use AWS I am to control the interactions",
    "start": "563860",
    "end": "570220"
  },
  {
    "text": "between AWS services within your account for example for Amazon EMR if you decide",
    "start": "570220",
    "end": "576670"
  },
  {
    "text": "to enable debugging and logging for your cluster when you launch a cluster that would require Amazon EMR to archive log",
    "start": "576670",
    "end": "584320"
  },
  {
    "text": "files to Amazon s3 publish messages to Amazon sqs queues and publish",
    "start": "584320",
    "end": "589390"
  },
  {
    "text": "notifications to Amazon SNS topics now for your convenience these default",
    "start": "589390",
    "end": "594850"
  },
  {
    "text": "permissions have been wrapped together a bundled under AWS managed Identity and",
    "start": "594850",
    "end": "600070"
  },
  {
    "text": "Access Management policies those policies are then attached to a pair of",
    "start": "600070",
    "end": "605220"
  },
  {
    "text": "identity and access management roles and those roles are then assumed by your",
    "start": "605220",
    "end": "610390"
  },
  {
    "text": "Amazon EMR cluster and the Amazon EMR service the recommendation here is that you make",
    "start": "610390",
    "end": "616570"
  },
  {
    "text": "sure you understand those default permissions and tailor and customize a new set of permissions that matches the",
    "start": "616570",
    "end": "623770"
  },
  {
    "text": "intended use of your cluster now let us turn our attention to network",
    "start": "623770",
    "end": "630880"
  },
  {
    "text": "security in the hybrid enterprise data warehouse this architecture is similar to the one",
    "start": "630880",
    "end": "636370"
  },
  {
    "start": "635000",
    "end": "635000"
  },
  {
    "text": "that I have shown you earlier only with more networking details visible on the right hand side there is your AWS",
    "start": "636370",
    "end": "642220"
  },
  {
    "text": "environment which in this case is a single virtual private cloud of EPC a",
    "start": "642220",
    "end": "647820"
  },
  {
    "text": "VPC is just your private logically isolated network within the AWS cloud a",
    "start": "647820",
    "end": "654029"
  },
  {
    "text": "VPC can be segmented into public samisen private subnet a public subnet is a",
    "start": "654029",
    "end": "659740"
  },
  {
    "text": "subnet that has a direct network route to an Internet gateway an Internet",
    "start": "659740",
    "end": "665230"
  },
  {
    "text": "gateway is the access point through which traffic can cross from within your VPC out to the public internet and vice",
    "start": "665230",
    "end": "671920"
  },
  {
    "text": "versa a private subnet on the other hand is a subnet that does not have a direct route",
    "start": "671920",
    "end": "677380"
  },
  {
    "text": "to an Internet gateway instead it has a direct network route to a virtual private gateway or a vgw the vgw is the",
    "start": "677380",
    "end": "685240"
  },
  {
    "text": "access point through which traffic can cross from within your VPC to your on-premises networks and vice versa",
    "start": "685240",
    "end": "691980"
  },
  {
    "text": "now the question here is how can we make our sensitive data flows secure in such",
    "start": "691980",
    "end": "697720"
  },
  {
    "text": "an architecture ideally we would want our network traffic to be encrypted",
    "start": "697720",
    "end": "703140"
  },
  {
    "text": "we would also want our network traffic to be private never to traverse the public Internet and finally we would",
    "start": "703140",
    "end": "710770"
  },
  {
    "text": "want to be able to prevent unauthorized network traffic to unknown IP addresses and ports",
    "start": "710770",
    "end": "717390"
  },
  {
    "text": "now I'm told I'm going to talk about encryption in transit later on this session so for now let us focus on",
    "start": "717390",
    "end": "723970"
  },
  {
    "text": "network traffic privacy and authorization an important step towards",
    "start": "723970",
    "end": "729460"
  },
  {
    "text": "making your network traffic private is to launch both your Amazon EMR and",
    "start": "729460",
    "end": "734650"
  },
  {
    "text": "Amazon redshift in private V PC subnets with private IP addresses both services",
    "start": "734650",
    "end": "740410"
  },
  {
    "text": "support doing so today and doing so would render your clusters effectively inaccessible from the public Internet",
    "start": "740410",
    "end": "748080"
  },
  {
    "text": "another important step that you should take to make your data flows private is",
    "start": "748080",
    "end": "753310"
  },
  {
    "text": "to set up and configure an s3 V PC endpoint s3 V PC endpoint is simply a point where",
    "start": "753310",
    "end": "761230"
  },
  {
    "text": "traffic crossing from within your V PC to Amazon s3 and vice versa is private",
    "start": "761230",
    "end": "766660"
  },
  {
    "text": "it only traverses your V PC and the internal Amazon network",
    "start": "766660",
    "end": "772140"
  },
  {
    "text": "now to force all of the upload traffic from your on-premises network to Amazon s3 through that s3 V PC endpoint that",
    "start": "773040",
    "end": "781300"
  },
  {
    "text": "traffic would need to be proxied through a set of auto scaling and load balanced proxy instances within your V PC",
    "start": "781300",
    "end": "788820"
  },
  {
    "text": "that is necessary because V pcs today do not allow transitive routing",
    "start": "788820",
    "end": "795980"
  },
  {
    "text": "and once your data is in Amazon s3 we would want to ensure that data traffic",
    "start": "795980",
    "end": "801139"
  },
  {
    "text": "between your clusters and Amazon s3 would go through that s3 V PC endpoint and for Amazon EMR that simply means",
    "start": "801139",
    "end": "810019"
  },
  {
    "text": "properly configuring the cluster during a cluster launch for Amazon redshift",
    "start": "810019",
    "end": "815540"
  },
  {
    "text": "that means enhancing enabling the enhanced V PC routing feature for your",
    "start": "815540",
    "end": "820820"
  },
  {
    "text": "cluster interactions between your different user",
    "start": "820820",
    "end": "825920"
  },
  {
    "text": "groups and your clusters would flow over the AWS direct connect so those are private as well",
    "start": "825920",
    "end": "832089"
  },
  {
    "text": "but any traffic that is destined to a public AWS service endpoint must cross",
    "start": "832089",
    "end": "838279"
  },
  {
    "text": "through the internet gateway and for that to happen we would need to setup a VP C net gateway in our public subnet",
    "start": "838279",
    "end": "846519"
  },
  {
    "text": "now that takes care of network traffic privacy so now let's talk about network",
    "start": "846519",
    "end": "851779"
  },
  {
    "text": "traffic authorization a network traffic authorization is about applying rules on inbound and outbound traffic",
    "start": "851779",
    "end": "858310"
  },
  {
    "text": "those rules might be based on a specific IP address range or a specific network",
    "start": "858310",
    "end": "863779"
  },
  {
    "text": "protocol or specific ports and there are three mechanisms that you can use to",
    "start": "863779",
    "end": "869660"
  },
  {
    "text": "define and enforce network traffic rules first you can use V PC security groups",
    "start": "869660",
    "end": "876320"
  },
  {
    "text": "with your to protect your red shift and EMR clusters in fact Amazon EMR is protected your",
    "start": "876320",
    "end": "883940"
  },
  {
    "text": "amazonia our cluster is protected by default using a pair of security groups that you can customize for Amazon",
    "start": "883940",
    "end": "890269"
  },
  {
    "text": "redshift you can choose a security group that you already configured",
    "start": "890269",
    "end": "895720"
  },
  {
    "text": "the second mechanism that you can use is network access control lists network ACLs apply at the subnet level and thus",
    "start": "895720",
    "end": "903380"
  },
  {
    "text": "you can use those to protect your public and private subnets the final mechanism that you can use is your",
    "start": "903380",
    "end": "911540"
  },
  {
    "text": "own customer router or firewall at your end of the AWS direct connect",
    "start": "911540",
    "end": "917500"
  },
  {
    "text": "so just to sum up the benefits of this architecture all of the data flows are",
    "start": "917500",
    "end": "922880"
  },
  {
    "text": "private you do not traverse the public internet and there are a number of choke points where you can define and enforce",
    "start": "922880",
    "end": "929690"
  },
  {
    "text": "traffic rules also traffic logging is possible with VPC flow logs so you can log all traffic traversing your VPC and",
    "start": "929690",
    "end": "936700"
  },
  {
    "text": "store those logs in s3 for further analysis and finally with such an",
    "start": "936700",
    "end": "942440"
  },
  {
    "text": "architecture dedicated tenancy is possible and with dedicated tenancy you'd run your cluster nodes on hardware",
    "start": "942440",
    "end": "948260"
  },
  {
    "text": "that is dedicated to you",
    "start": "948260",
    "end": "951250"
  },
  {
    "text": "now let us resume our discussion about access control and in the architecture",
    "start": "953410",
    "end": "959030"
  },
  {
    "text": "that I have shown you so far there were just three teams a team of data engineers producing data entered into",
    "start": "959030",
    "end": "965330"
  },
  {
    "text": "the data warehouse and two teams consuming data team of data scientists and a team of business analysts and",
    "start": "965330",
    "end": "971450"
  },
  {
    "text": "end-users now we all know that reality is more complicated than that what we",
    "start": "971450",
    "end": "976610"
  },
  {
    "text": "often see instead is that there are multiple teams across different departments and different lines of business who would wish to consume data",
    "start": "976610",
    "end": "982970"
  },
  {
    "text": "from the data warehouse but not only consume data they might also need to produce their own datasets and be able",
    "start": "982970",
    "end": "988910"
  },
  {
    "text": "to share those with other teams so in order to support this multiple",
    "start": "988910",
    "end": "994280"
  },
  {
    "text": "team scenario we need to achieve to access control objectives the first objective is that every team",
    "start": "994280",
    "end": "1001000"
  },
  {
    "start": "997000",
    "end": "997000"
  },
  {
    "text": "must be able to get secure and segregated access to Amazon s3 Amazon EMR and Amazon redshift and the second",
    "start": "1001000",
    "end": "1007630"
  },
  {
    "text": "objective is that every team must be able to securely share their data with other teams",
    "start": "1007630",
    "end": "1012660"
  },
  {
    "text": "now there are two strategies that you can follow to that end the first strategy is fine-grained data",
    "start": "1012660",
    "end": "1019870"
  },
  {
    "text": "and resource ownership under such a strategy as central data warehousing team owns and managers Amazon s3 Amazon",
    "start": "1019870",
    "end": "1027760"
  },
  {
    "text": "EMR and Amazon redshift resources within a single account every team is then granted ownership of",
    "start": "1027760",
    "end": "1035230"
  },
  {
    "text": "fractions of those resources in Amazon redshift a team would own",
    "start": "1035230",
    "end": "1040630"
  },
  {
    "text": "specific databases schemas or even tables and in addition they get their",
    "start": "1040630",
    "end": "1046030"
  },
  {
    "text": "own share of their Amazon redshift cluster using work redshift workload management",
    "start": "1046030",
    "end": "1051990"
  },
  {
    "text": "in Amazon EMR a team would get access permission to specific Hadoop distributed file system paths in",
    "start": "1051990",
    "end": "1058990"
  },
  {
    "text": "addition to permissions for using specific Hadoop tools in addition to their own share of the Amazon EMR",
    "start": "1058990",
    "end": "1065550"
  },
  {
    "text": "cluster using yarn job scheduling on apache hadoop",
    "start": "1065550",
    "end": "1071780"
  },
  {
    "text": "now because access is segregated in such such a strategy access segregated within",
    "start": "1071780",
    "end": "1077490"
  },
  {
    "text": "Amazon EMR and within Amazon redshift access control is generally complicated",
    "start": "1077490",
    "end": "1082680"
  },
  {
    "text": "to set up maintain and scale as your teams grow in size and number now we can contrast that strategy to a",
    "start": "1082680",
    "end": "1089460"
  },
  {
    "text": "strategy of coarse-grained data and resource ownership under such a strategy",
    "start": "1089460",
    "end": "1095910"
  },
  {
    "text": "every team is entitled to their own Amazon s3 buckets in addition to their own Amazon EMR and Amazon redshift",
    "start": "1095910",
    "end": "1102780"
  },
  {
    "text": "clusters a team's Amazon s3 buckets would contain datasets prepared for and by that team",
    "start": "1102780",
    "end": "1110040"
  },
  {
    "text": "and a team can securely share their data with teams in other accounts using",
    "start": "1110040",
    "end": "1117170"
  },
  {
    "text": "AWS Identity and Access Management because team resource ownership is",
    "start": "1117170",
    "end": "1123510"
  },
  {
    "text": "established by the AWS account access control is generally simpler to set up",
    "start": "1123510",
    "end": "1130110"
  },
  {
    "text": "maintain and scale as your team grows in size and number",
    "start": "1130110",
    "end": "1135590"
  },
  {
    "text": "now we talked a little bit about strategies so let us talk about some access control specifics for Amazon s3",
    "start": "1135830",
    "end": "1141990"
  },
  {
    "text": "and here I would like to call out three important points the first point pertains to team data",
    "start": "1141990",
    "end": "1148590"
  },
  {
    "text": "sharing so in a multiple team environment where teens share their data through Amazon s3 a team should be able",
    "start": "1148590",
    "end": "1155580"
  },
  {
    "text": "to quickly and reliably answer the question of who has what access to my data",
    "start": "1155580",
    "end": "1161180"
  },
  {
    "text": "in order to do that a team should consider centralizing the access control",
    "start": "1161180",
    "end": "1167190"
  },
  {
    "text": "permissions to their Amazon s3 buckets in s3 bucket policies an s3 bucket policy is just a usual",
    "start": "1167190",
    "end": "1174720"
  },
  {
    "text": "identity access management policy only one that is attached to an Amazon s3 bucket rather than attached to a user or",
    "start": "1174720",
    "end": "1181350"
  },
  {
    "text": "a group the second point that I want to call out pertains to access control for Amazon",
    "start": "1181350",
    "end": "1187530"
  },
  {
    "text": "EMR cluster users to Amazon s3 your Amazon EMR cluster users have",
    "start": "1187530",
    "end": "1193620"
  },
  {
    "text": "access to Amazon s3 through the cluster and that is possible because of the default permissions attached to the ec2",
    "start": "1193620",
    "end": "1201230"
  },
  {
    "text": "default role that is assumed by your cluster nodes so make sure to review those permissions",
    "start": "1201230",
    "end": "1206990"
  },
  {
    "text": "to Amazon s3 and tighten them as necessary to grant release privileged access a",
    "start": "1206990",
    "end": "1212950"
  },
  {
    "text": "final point that I want to call out pertains to access control of Amazon redshift users to Amazon s3 as well",
    "start": "1212950",
    "end": "1220340"
  },
  {
    "text": "redshift users would often want to copy data from Amazon s3 to Amazon redshift and vice versa in order to do that they",
    "start": "1220340",
    "end": "1227659"
  },
  {
    "text": "would use redshift copy and unload commands and for those comments to succeed they need to run with proper",
    "start": "1227659",
    "end": "1233419"
  },
  {
    "text": "permissions to Amazon s3 resources today Amazon redshift makes it possible",
    "start": "1233419",
    "end": "1238580"
  },
  {
    "text": "to pass those required s3 permissions to your copy and unload commands using",
    "start": "1238580",
    "end": "1243860"
  },
  {
    "text": "Identity and Access Management roles the other than hard-coded AWS credentials",
    "start": "1243860",
    "end": "1250600"
  },
  {
    "text": "now moving on to Amazon EMR Amazon EMR is the cluster management",
    "start": "1252700",
    "end": "1258619"
  },
  {
    "text": "platform that on top of which runs Apache Hadoop and Apache spark so in order to enable strong authentication",
    "start": "1258619",
    "end": "1265249"
  },
  {
    "text": "and fine-grained authorization within Apache Hadoop you need to configure Apache Hadoop in secure mode",
    "start": "1265249",
    "end": "1271419"
  },
  {
    "text": "when Apache Hadoop is configured in secure mode Hadoop users and demons are",
    "start": "1271419",
    "end": "1277399"
  },
  {
    "text": "then required to authenticate themselves to one another using through the Kerberos protocol and another Hadoop",
    "start": "1277399",
    "end": "1284749"
  },
  {
    "text": "mechanism called dedication tokens when you configure Hadoop in secure mode",
    "start": "1284749",
    "end": "1290769"
  },
  {
    "text": "users Hadoop users and demons are also granted permissions based on access",
    "start": "1290769",
    "end": "1295999"
  },
  {
    "text": "control lists that you can configure in Hadoop cluster configuration files",
    "start": "1295999",
    "end": "1301330"
  },
  {
    "text": "now there are two important consequences for that first is that you need to setup a",
    "start": "1301330",
    "end": "1307340"
  },
  {
    "text": "dedicated Kerberos key distribution center for your clusters and",
    "start": "1307340",
    "end": "1312490"
  },
  {
    "text": "properly configure Kerberos across your cluster nodes and the second consequence",
    "start": "1312490",
    "end": "1318200"
  },
  {
    "text": "is that as you're a Duke user base grows in size and number manually managing",
    "start": "1318200",
    "end": "1325039"
  },
  {
    "text": "Hadoop ACLs in cluster configuration files can quickly become unwieldy and",
    "start": "1325039",
    "end": "1330090"
  },
  {
    "text": "that is when you can leverage the help of open source frameworks that have been",
    "start": "1330090",
    "end": "1335250"
  },
  {
    "text": "created to solve this problem such as Apache Ranger and Apache sentry",
    "start": "1335250",
    "end": "1341480"
  },
  {
    "text": "now moving on to access control within Amazon redshift it is much easier to set",
    "start": "1342860",
    "end": "1348060"
  },
  {
    "text": "up in comparison with Apache Hadoop on Amazon EMR and that is because redshift is based on policy code so you would",
    "start": "1348060",
    "end": "1355710"
  },
  {
    "text": "found you would find the familiar security controls that you would find in a in a relational database management",
    "start": "1355710",
    "end": "1361080"
  },
  {
    "text": "system this includes the ability to grant or revoke privileges on",
    "start": "1361080",
    "end": "1366380"
  },
  {
    "text": "SQL activities such as insert create insert or delete and",
    "start": "1366380",
    "end": "1372530"
  },
  {
    "text": "those privileges apply also to specific redshift objects such as databases schemas tables and user-defined",
    "start": "1372530",
    "end": "1378990"
  },
  {
    "text": "functions now that concludes our discussion of",
    "start": "1378990",
    "end": "1385590"
  },
  {
    "text": "access control let us move on to see how you can protect the confidentiality and",
    "start": "1385590",
    "end": "1390840"
  },
  {
    "text": "integrity of your data both at rest and in transit using encryption and here let",
    "start": "1390840",
    "end": "1396750"
  },
  {
    "text": "us remember that we are talking about the data warehouse that supports multiple teams and if every team is",
    "start": "1396750",
    "end": "1401910"
  },
  {
    "text": "required to protect their data at rest using symmetric key encryption it is not hard to imagine how such",
    "start": "1401910",
    "end": "1408480"
  },
  {
    "text": "encryption how those encryption keys can proliferate and get out of control and that is why our first step to",
    "start": "1408480",
    "end": "1414540"
  },
  {
    "text": "implementing encryption and the hybrid enterprise data warehouse is deciding on an encryption key management strategy",
    "start": "1414540",
    "end": "1421790"
  },
  {
    "start": "1418000",
    "end": "1418000"
  },
  {
    "text": "after that we can proceed to pick an encryption mode for Amazon s3 we can then configure encryption Amazon EMR and",
    "start": "1421790",
    "end": "1428790"
  },
  {
    "text": "then launch an encrypted redshift cluster",
    "start": "1428790",
    "end": "1433910"
  },
  {
    "text": "and when it comes to your encryption key management strategy there are a number of options to choose from on the one",
    "start": "1434240",
    "end": "1441330"
  },
  {
    "text": "hand if you just want to ensure that your data is encrypted and you do not want to worry about managing the",
    "start": "1441330",
    "end": "1446700"
  },
  {
    "text": "encryption keys yourself you can just let Amazon s3 and Amazon redshift do encryption key management on your behalf",
    "start": "1446700",
    "end": "1452520"
  },
  {
    "text": "with minimal configuration on the other hand if you want total control on your keys in addition to the",
    "start": "1452520",
    "end": "1459150"
  },
  {
    "text": "ability to store those keys and let it that hardware security modules you can",
    "start": "1459150",
    "end": "1465120"
  },
  {
    "text": "then choose to either use AWS cloud HSM or use your own custom key management",
    "start": "1465120",
    "end": "1470670"
  },
  {
    "text": "solution either on-premises or an AWS now if your encryption key management",
    "start": "1470670",
    "end": "1476430"
  },
  {
    "text": "needs lie in between those two ends of the spectrum you should consider AWS key management service",
    "start": "1476430",
    "end": "1484070"
  },
  {
    "text": "AWS key management service kms is a managed service that simplifies creating",
    "start": "1484070",
    "end": "1489600"
  },
  {
    "text": "and importing your own encryption keys in addition to managing those keys throughout their lifecycle this includes",
    "start": "1489600",
    "end": "1496740"
  },
  {
    "text": "rotating your keys disabling or enabling your keys deleting them and other things",
    "start": "1496740",
    "end": "1502020"
  },
  {
    "text": "that are used kms uses highly available hardware security modules behind the scene to",
    "start": "1502020",
    "end": "1508410"
  },
  {
    "text": "protect your keys and all three services s3 EMR and redshift can use kms for key",
    "start": "1508410",
    "end": "1514830"
  },
  {
    "text": "management now with all these options how can you make a decision here are three questions",
    "start": "1514830",
    "end": "1521760"
  },
  {
    "text": "that you can answer as a starting point the first question is do I have to",
    "start": "1521760",
    "end": "1527940"
  },
  {
    "text": "manage my Christian Keys myself at all the second question is if I do manage if",
    "start": "1527940",
    "end": "1533820"
  },
  {
    "text": "I do have to manage my Christian Keys do I have to do that in dedicated hardware security modules and the final question",
    "start": "1533820",
    "end": "1540960"
  },
  {
    "text": "is do I have to manage my Christian Keys on premises of course your situation",
    "start": "1540960",
    "end": "1546150"
  },
  {
    "text": "might be more nuanced but this is a good starting point and based on your answers you can decide on an encryption key",
    "start": "1546150",
    "end": "1552210"
  },
  {
    "text": "management strategy moving on the next step would be to",
    "start": "1552210",
    "end": "1557520"
  },
  {
    "text": "configure encryption in Amazon s3 and this involves deciding on whether you",
    "start": "1557520",
    "end": "1563550"
  },
  {
    "text": "want to do client-side encryption or server-side encryption client-side encryption is where you put",
    "start": "1563550",
    "end": "1568920"
  },
  {
    "text": "your applications in charge of the encryption process your data is encrypted and then it is",
    "start": "1568920",
    "end": "1574770"
  },
  {
    "text": "uploaded into Amazon s3 server side encryption is where you put Amazon s3 in charge of the encryption",
    "start": "1574770",
    "end": "1581460"
  },
  {
    "text": "process so you upload your encrypted data into Amazon s3 and then it is encrypted right after it is uploaded",
    "start": "1581460",
    "end": "1589190"
  },
  {
    "text": "now the combination of client side or server side encryption plus your encryption key management strategy will",
    "start": "1589190",
    "end": "1595810"
  },
  {
    "text": "the encryption mode that you should choose for Amazon s3 CS e stands for Clyde sienten SS e",
    "start": "1595810",
    "end": "1601900"
  },
  {
    "text": "stands for server-side encryption - KMS means that the keys are the master encryption keys are managed in KMS and -",
    "start": "1601900",
    "end": "1609250"
  },
  {
    "text": "c means that you manage your own master keys",
    "start": "1609250",
    "end": "1614520"
  },
  {
    "text": "once we configure encryption Amazon s3 we can move on to configuring encryption amazonia mark and here it is helpful to",
    "start": "1617250",
    "end": "1623710"
  },
  {
    "text": "understand how an EMR cluster stores and accesses data an amazonian or cluster is",
    "start": "1623710",
    "end": "1629500"
  },
  {
    "text": "composed of a master node that orchestrates a number of core nodes the store and process data",
    "start": "1629500",
    "end": "1636990"
  },
  {
    "text": "every node in the cluster has storage that is split into two kinds of volumes",
    "start": "1636990",
    "end": "1642160"
  },
  {
    "text": "a root volume and one or more data volumes the root volume is the volume from which",
    "start": "1642160",
    "end": "1648130"
  },
  {
    "text": "you cluster node boots and it also stores the Hadoop binaries a date and a",
    "start": "1648130",
    "end": "1653560"
  },
  {
    "text": "data volume would contain things such as they adopt distributed file system blocks in addition to the lock files and",
    "start": "1653560",
    "end": "1659740"
  },
  {
    "text": "the hive meta store database your Amazon EMR cluster is also capable",
    "start": "1659740",
    "end": "1665110"
  },
  {
    "text": "of reading and writing data to Amazon s3 through the EMR filesystem client",
    "start": "1665110",
    "end": "1670950"
  },
  {
    "text": "the mr filesystem client exposes Amazon s3 as a distributed file system to your",
    "start": "1670950",
    "end": "1676720"
  },
  {
    "text": "entire cluster the MRFs client supports all encryption modes for Amazon s3 except server-side",
    "start": "1676720",
    "end": "1684310"
  },
  {
    "text": "encryption with customer provider keys and all traffic in transit between the",
    "start": "1684310",
    "end": "1689770"
  },
  {
    "text": "MRFs client and Amazon s3 is encrypted by default in transit using TLS",
    "start": "1689770",
    "end": "1696510"
  },
  {
    "text": "Amazon EMR also supports local volume encryption using the Linux unified key",
    "start": "1696510",
    "end": "1702370"
  },
  {
    "text": "setup mechanism note that only data volumes can be encrypted and",
    "start": "1702370",
    "end": "1707550"
  },
  {
    "text": "for local volume encryption key management amazonia mark and integrate will either EWS kms or use a custom key",
    "start": "1707550",
    "end": "1715960"
  },
  {
    "text": "provider when you enable local volume encryption it also enables to hadoop in transit",
    "start": "1715960",
    "end": "1724270"
  },
  {
    "text": "encryption in transit features those two features are encryption of RPC traffic between diamonds and encryption of HDFS",
    "start": "1724270",
    "end": "1732580"
  },
  {
    "text": "block transfers now you can specify all of these",
    "start": "1732580",
    "end": "1738670"
  },
  {
    "text": "configuration settings today in a managed security configuration object the benefit of using a managed security",
    "start": "1738670",
    "end": "1745960"
  },
  {
    "text": "configuration object is that you can reuse your encryption settings across many Amazon EMR clusters and that would",
    "start": "1745960",
    "end": "1752440"
  },
  {
    "text": "help you to standardize your encryption settings across your hybrid enterprise data warehouse",
    "start": "1752440",
    "end": "1758760"
  },
  {
    "text": "finally moving on to encryption in Amazon redshift encryption at rest is an",
    "start": "1759480",
    "end": "1765130"
  },
  {
    "text": "immutable setting of the cluster and that simply means that if you launch an encrypted redshift cluster you cannot",
    "start": "1765130",
    "end": "1771940"
  },
  {
    "text": "switch to unencrypted or vice-versa once encryption is enabled for your",
    "start": "1771940",
    "end": "1778090"
  },
  {
    "text": "Amazon redshift cluster all redshift data blocks in addition to system",
    "start": "1778090",
    "end": "1783100"
  },
  {
    "text": "metadata and any backups or snapshots taken are all encrypted for encryption key management Amazon",
    "start": "1783100",
    "end": "1790600"
  },
  {
    "text": "redshift supports integrating with AWS kms or with an with a hardware security",
    "start": "1790600",
    "end": "1795820"
  },
  {
    "text": "module finally to wrap up our discussion about",
    "start": "1795820",
    "end": "1802090"
  },
  {
    "start": "1799000",
    "end": "1799000"
  },
  {
    "text": "encryption this is an overview of how you should implement encryption in transit the new hybrid enterprise data",
    "start": "1802090",
    "end": "1807610"
  },
  {
    "text": "warehouse Amazon s3 supports encryption in transit using SSL or TLS so you should take",
    "start": "1807610",
    "end": "1814000"
  },
  {
    "text": "advantage of that when uploading your data to Amazon s3 or downloading data from it Amazon EMR can also",
    "start": "1814000",
    "end": "1822059"
  },
  {
    "text": "write reuse encryption in transit with Amazon s3 using or through the EMR FS",
    "start": "1822059",
    "end": "1828250"
  },
  {
    "text": "client an Amazon redshift by default uses SSL for the redshift copy and",
    "start": "1828250",
    "end": "1834070"
  },
  {
    "text": "unload commands for Amazon redshift clients you can enforce that clients connect over SSL",
    "start": "1834070",
    "end": "1841390"
  },
  {
    "text": "but to do that you need to enable the require SSL setting in your cluster",
    "start": "1841390",
    "end": "1846790"
  },
  {
    "text": "configuration for Amazon name our clients this is",
    "start": "1846790",
    "end": "1852130"
  },
  {
    "text": "effectively Apache Hadoop clients so for that Hadoop web UI scan support that you",
    "start": "1852130",
    "end": "1857350"
  },
  {
    "text": "configure SSL but for that you need to consult the documentation for the individual Hadoop you that you would",
    "start": "1857350",
    "end": "1863599"
  },
  {
    "text": "like to use this wraps up the first part of the",
    "start": "1863599",
    "end": "1869149"
  },
  {
    "text": "session so thank you for your attention and I'd be happy to answer any questions after that and Nate is going to share",
    "start": "1869149",
    "end": "1875690"
  },
  {
    "text": "with you how hybrid enterprise data warehouse security is actually done",
    "start": "1875690",
    "end": "1881349"
  },
  {
    "text": "hey everybody I'm Nate Salmons I'm a principal architect at NASDAQ and it's",
    "start": "1882849",
    "end": "1889099"
  },
  {
    "text": "just going to give a quick intro to who we are and what we're doing and then I'm going to talk about the choices that",
    "start": "1889099",
    "end": "1895339"
  },
  {
    "text": "we've made along our path to using Amazon redshift and Amazon EMR for our data warehousing use and then I'm going",
    "start": "1895339",
    "end": "1901729"
  },
  {
    "text": "to talk a little bit about future directions for Big Data at Nasdaq so first if you haven't heard of us most",
    "start": "1901729",
    "end": "1908359"
  },
  {
    "text": "people know us as a stock exchange in the United States we actually own and operate 33 different exchanges around",
    "start": "1908359",
    "end": "1914329"
  },
  {
    "text": "the world and we sell exchange software to over a hundred different customers in 50 countries around the world as well",
    "start": "1914329",
    "end": "1921819"
  },
  {
    "text": "so first I'm going to talk about red shift red shift has been in use at NASDAQ since it was in beta and it's now",
    "start": "1921819",
    "end": "1928819"
  },
  {
    "text": "our main data warehousing workhorse for all of our activities and at least in the United States",
    "start": "1928819",
    "end": "1934029"
  },
  {
    "text": "we daily ingest data from hundreds of different internal sources and that's in",
    "start": "1934029",
    "end": "1939589"
  },
  {
    "text": "the 6 to 20 billion rows a day arranged the current footprint for that warehouse is an 18 node D s to 8 X large cluster",
    "start": "1939589",
    "end": "1947569"
  },
  {
    "text": "and it stores just under three trillion rows that's all highly sensitive data so",
    "start": "1947569",
    "end": "1952819"
  },
  {
    "text": "it's all the orders trades quotes basically any message that crosses any",
    "start": "1952819",
    "end": "1958579"
  },
  {
    "text": "of our exchanges gets into that warehouse for analytics and billing and that kind of thing we also store",
    "start": "1958579",
    "end": "1964759"
  },
  {
    "text": "membership information which is which is used for billing basically",
    "start": "1964759",
    "end": "1969879"
  },
  {
    "text": "so here's a graph of the last 18 months of data ingest every day in billions of",
    "start": "1969879",
    "end": "1976279"
  },
  {
    "text": "rows you can see that it's it's pretty sporadic and it kind of tracks daily market volumes in general but you can",
    "start": "1976279",
    "end": "1982249"
  },
  {
    "text": "see there's like a gap at Thanksgiving Christmas it usually drops down and then the beginning a year kind of goes crazy",
    "start": "1982249",
    "end": "1988790"
  },
  {
    "text": "again so what are we doing with redshift first and foremost billing and reporting",
    "start": "1988790",
    "end": "1995000"
  },
  {
    "start": "1991000",
    "end": "1991000"
  },
  {
    "text": "is where it really all started for us a few years ago we rewrote the billing system on top of our equities exchange",
    "start": "1995000",
    "end": "2001570"
  },
  {
    "text": "and that involved porting it to redshift after we started to ingest all this data",
    "start": "2001570",
    "end": "2006910"
  },
  {
    "text": "out of the equities exchange it became really useful for a lot of other activities around the company so as a as",
    "start": "2006910",
    "end": "2014230"
  },
  {
    "text": "what's called an self-regulating organization with the SEC we're required to police activity on our own exchange",
    "start": "2014230",
    "end": "2019960"
  },
  {
    "text": "so market surveillance is a real big part of what we do on redshift and that's looking for you know fraudulent",
    "start": "2019960",
    "end": "2027610"
  },
  {
    "text": "or illegal trading activity and that sort of thing we have a big economic research department which does a lot of work on",
    "start": "2027610",
    "end": "2034030"
  },
  {
    "text": "market structure and figuring out how we can use those 33 different markets to you know basically make more money and",
    "start": "2034030",
    "end": "2041410"
  },
  {
    "text": "then we have a trade history system called trade info that lets you log into",
    "start": "2041410",
    "end": "2047200"
  },
  {
    "text": "a web UI and look at the status of orders from you know anything from ongoing orders today back you know",
    "start": "2047200",
    "end": "2054129"
  },
  {
    "text": "several years so those are kind of the main uses that we have right now",
    "start": "2054130",
    "end": "2059310"
  },
  {
    "start": "2059000",
    "end": "2059000"
  },
  {
    "text": "let's talk about red ship network security so the main idea here is that we want to put red shift into a very",
    "start": "2059310",
    "end": "2065408"
  },
  {
    "text": "tightly sealed box and we want to control all access to that database in or out right so to that end we use we",
    "start": "2065409",
    "end": "2073929"
  },
  {
    "text": "run our clusters inside VPC subnets with lockdown security groups that limit the number of you know the set of IPs that",
    "start": "2073929",
    "end": "2080710"
  },
  {
    "text": "are allowed to connect into those clusters when red shift talks to s3 to do copy or",
    "start": "2080710",
    "end": "2086350"
  },
  {
    "text": "unload operations it does so across a VPC endpoint to s3 we require SSL",
    "start": "2086350",
    "end": "2092970"
  },
  {
    "text": "connectivity into those clusters and we also assign individual ssl certificates to each cluster and then distribute",
    "start": "2092970",
    "end": "2099670"
  },
  {
    "text": "those certificates to the various groups internally that need to actually connect to them and then all of that data",
    "start": "2099670",
    "end": "2107410"
  },
  {
    "text": "transfer is occurring over a AWS Direct Connect connection so that nothing",
    "start": "2107410",
    "end": "2113500"
  },
  {
    "text": "traverses the public Internet and then furthermore on premise we have a set of firewalls behind that Direct Connect",
    "start": "2113500",
    "end": "2120359"
  },
  {
    "text": "connection and those are also used to further limit the limit activity and now",
    "start": "2120359",
    "end": "2126869"
  },
  {
    "start": "2126000",
    "end": "2126000"
  },
  {
    "text": "let's talk about the actual data security on those red shift clusters so we use on-premise HSMs to store the the",
    "start": "2126869",
    "end": "2134760"
  },
  {
    "text": "master encryption keys for those red shift clusters we also operate red shift with a minimal iam policy that basically",
    "start": "2134760",
    "end": "2141510"
  },
  {
    "text": "says that it can you know read and write from particular s3 buckets and and really do very little else",
    "start": "2141510",
    "end": "2147799"
  },
  {
    "text": "we encrypt all the data that we write into s3 using client-side encryption and",
    "start": "2147799",
    "end": "2153240"
  },
  {
    "text": "that's you know opposed to server side encryption so that we make sure that all the encryption operations are happening",
    "start": "2153240",
    "end": "2158730"
  },
  {
    "text": "either on-premise or they're operating in you know ec2 instances that we're in",
    "start": "2158730",
    "end": "2163980"
  },
  {
    "text": "control of we use a custom key management system that stores all of our encryption keys on-premise in a database",
    "start": "2163980",
    "end": "2170670"
  },
  {
    "text": "and then they're also rooted in that on-premise HSM as well",
    "start": "2170670",
    "end": "2176450"
  },
  {
    "start": "2177000",
    "end": "2177000"
  },
  {
    "text": "you know I talked about key management right so we've chosen to use on-premise HSMs for our activity and that does give",
    "start": "2177140",
    "end": "2184799"
  },
  {
    "text": "us a physical separation of the keys right I can sever that Direct Connect connection and terminate the red shift",
    "start": "2184799",
    "end": "2191220"
  },
  {
    "text": "cluster and then all that data is completely useless to anyone who doesn't have you know physical access to our",
    "start": "2191220",
    "end": "2196920"
  },
  {
    "text": "HSMs it does require you to use an EIP on your red shift cluster and the reason",
    "start": "2196920",
    "end": "2202950"
  },
  {
    "text": "there is that when you pair something with an HSM there's a certificate exchange and it involves the the actual",
    "start": "2202950",
    "end": "2208650"
  },
  {
    "text": "IP address of the client for the hsm and then if you were to reconstitute that red shift",
    "start": "2208650",
    "end": "2214559"
  },
  {
    "text": "cluster on other hardware it's obviously going to have a different IP address but using an EIP it's a it's able to talk",
    "start": "2214559",
    "end": "2221160"
  },
  {
    "text": "back to that hsm again but it's really important to note that HSMs are really",
    "start": "2221160",
    "end": "2226319"
  },
  {
    "text": "delicate hardware devices and they require a lot of special handling so unless you really",
    "start": "2226319",
    "end": "2232319"
  },
  {
    "text": "need to use them I would recommend against it but you know that's something that you're gonna have to figure out for",
    "start": "2232319",
    "end": "2237539"
  },
  {
    "text": "for each of your own organizations in contrast the Amazon kms gives you",
    "start": "2237539",
    "end": "2243089"
  },
  {
    "text": "policy based key rotation gives you I am policy governed all the usage of those keys it gives you you know detailed",
    "start": "2243089",
    "end": "2250290"
  },
  {
    "text": "cloud trail access logs for every kind of encrypt and decrypt operation and you",
    "start": "2250290",
    "end": "2255390"
  },
  {
    "text": "get high durability storage so in our case we have a number of different HSMs in different geographic locations but",
    "start": "2255390",
    "end": "2261930"
  },
  {
    "text": "it's important to note that you know if you have all your keys in one HSM that's where all your keys are right so you",
    "start": "2261930",
    "end": "2268830"
  },
  {
    "text": "literally have your eggs in that basket right and the other nice thing about the Amazon kms is it supports other Amazon",
    "start": "2268830",
    "end": "2275760"
  },
  {
    "text": "services like EBS and other RDS databases and that sort of thing but you",
    "start": "2275760",
    "end": "2280920"
  },
  {
    "text": "do need to trust Amazon right so you need to figure out for your organization where you lie on that spectrum of you",
    "start": "2280920",
    "end": "2287310"
  },
  {
    "text": "know trust versus not trust basically um and that's a that's a big decision that you're gonna have to make for yourselves",
    "start": "2287310",
    "end": "2294980"
  },
  {
    "text": "for access control and monitoring we maintain very tight control over who's",
    "start": "2294980",
    "end": "2300510"
  },
  {
    "start": "2295000",
    "end": "2295000"
  },
  {
    "text": "allowed to write into that redshift database and that's mostly because we have a lot of regulatory reasons for",
    "start": "2300510",
    "end": "2306150"
  },
  {
    "text": "keeping that data and we also just don't want to you know accidentally delete anything users are granted access to specific",
    "start": "2306150",
    "end": "2312720"
  },
  {
    "text": "schemas and specific tables for whatever it is that they need to do and we also grant them a specific set of workload",
    "start": "2312720",
    "end": "2318990"
  },
  {
    "text": "management you know resource constraints right so",
    "start": "2318990",
    "end": "2324210"
  },
  {
    "text": "that so they only get a certain slice of the clusters CPU and memory because you",
    "start": "2324210",
    "end": "2329610"
  },
  {
    "text": "know you don't want any one group to be able to completely paralyze the the cluster by running crazy queries and",
    "start": "2329610",
    "end": "2335490"
  },
  {
    "text": "that kind of thing after that we also pull all the logs out of s3 down on to",
    "start": "2335490",
    "end": "2341700"
  },
  {
    "text": "on-premise for analysis and that includes redshift activity logs which is",
    "start": "2341700",
    "end": "2347210"
  },
  {
    "text": "you know connect and disconnect all the DDL operations all that kind of thing we pull down cloud trail API logs and VPC",
    "start": "2347210",
    "end": "2355200"
  },
  {
    "text": "flow logs and then we also monitor a connection or a table inside redshift called STL connection log which is just",
    "start": "2355200",
    "end": "2362550"
  },
  {
    "text": "a live log of all the connections to the database so that we can figure out you know if we know who's connecting to to",
    "start": "2362550",
    "end": "2369570"
  },
  {
    "text": "what database for managing redshift clusters",
    "start": "2369570",
    "end": "2375619"
  },
  {
    "start": "2372000",
    "end": "2372000"
  },
  {
    "text": "we used to not delete anything right because nobody wants to delete data that kind of led us to growing those clusters",
    "start": "2375619",
    "end": "2381930"
  },
  {
    "text": "once per quarter and that is actually not all that sustainable so one of the reasons we've we've switched",
    "start": "2381930",
    "end": "2389069"
  },
  {
    "text": "to keeping a 1 year rolling window of data and redshift is that old data is really accessed very infrequently right",
    "start": "2389069",
    "end": "2395489"
  },
  {
    "text": "so it doesn't make a lot of sense to spend money on live CPUs and spinning disk attached to your cluster for data",
    "start": "2395489",
    "end": "2402959"
  },
  {
    "text": "that you're not going to look at right so now we resize those clusters you know",
    "start": "2402959",
    "end": "2408949"
  },
  {
    "text": "basically as as market volumes increase and as we have acquisitions so we",
    "start": "2408949",
    "end": "2414390"
  },
  {
    "text": "recently bought a company called the ISE and all of their data is going to end up in redshift so we're gonna have to",
    "start": "2414390",
    "end": "2419729"
  },
  {
    "text": "expand this cluster actually pretty substantially but the other reason is that",
    "start": "2419729",
    "end": "2425119"
  },
  {
    "text": "resizing a redshift cluster is really not an instantaneous operation especially if you have a you know a",
    "start": "2425119",
    "end": "2430890"
  },
  {
    "text": "couple hundred terabytes of data that it has to rebalance it's become much much much faster over the last few years but",
    "start": "2430890",
    "end": "2436589"
  },
  {
    "text": "it's still something that you need to plan for your operations group and that sort of thing so for all those reasons",
    "start": "2436589",
    "end": "2442099"
  },
  {
    "text": "we've extended our warehousing to EMR and s3 and that's what I'm gonna talk about",
    "start": "2442099",
    "end": "2448140"
  },
  {
    "text": "next so EMR and ESDA is gaining a lot of traction internally we're building",
    "start": "2448140",
    "end": "2454199"
  },
  {
    "text": "basically an open data platform on top of s3 where we parallel load data in",
    "start": "2454199",
    "end": "2459209"
  },
  {
    "text": "every day to redshift and to EMR and when I say we're loading data into EMR",
    "start": "2459209",
    "end": "2464339"
  },
  {
    "text": "what I really mean is that we're producing encrypted park' files and we're storing those in s3",
    "start": "2464339",
    "end": "2469699"
  },
  {
    "text": "and in that case we're keeping the data forever or as long as we can you know",
    "start": "2469699",
    "end": "2474900"
  },
  {
    "text": "humanly possibly keep it the current footprint there is about 5.1 million",
    "start": "2474900",
    "end": "2480119"
  },
  {
    "text": "objects and about 500 terabytes and that's approximately six and a half trillion rows since the beginning of",
    "start": "2480119",
    "end": "2486719"
  },
  {
    "text": "2014 internally we actually have a data archive of all the transactions on our",
    "start": "2486719",
    "end": "2492209"
  },
  {
    "text": "exchanges dating back to the mid 1990s and we're in the process of moving all",
    "start": "2492209",
    "end": "2497519"
  },
  {
    "text": "that data into into s3 as well and that's around a petabyte and a half of data again because nobody wants to",
    "start": "2497519",
    "end": "2503829"
  },
  {
    "text": "delete anything so I talked about Parque we actually",
    "start": "2503829",
    "end": "2508990"
  },
  {
    "start": "2506000",
    "end": "2506000"
  },
  {
    "text": "evaluated both Parque and/or RC files and we arrived at using Parque it's a",
    "start": "2508990",
    "end": "2514599"
  },
  {
    "text": "modern columnar format with good data compression you know it's a it's a columnar format right so you get better",
    "start": "2514599",
    "end": "2519730"
  },
  {
    "text": "data compression with adjacent columns and or adjacent values for a column it's",
    "start": "2519730",
    "end": "2524890"
  },
  {
    "text": "a self describing format and what I mean by that is that I can take any random Park a file crack it open and I can",
    "start": "2524890",
    "end": "2532510"
  },
  {
    "text": "examine the data structure that's inside right I can figure out the name of the table the the name of all the columns",
    "start": "2532510",
    "end": "2538359"
  },
  {
    "text": "that types all that kind of thing and you know if you use CSV files or whatever you can't you know you have to",
    "start": "2538359",
    "end": "2544299"
  },
  {
    "text": "interpret what kind of data is in there the other nice thing is it has a really growing support in a lot of different",
    "start": "2544299",
    "end": "2551470"
  },
  {
    "text": "open-source communities and for our two main use cases SPARC and presto it works very well and then finally it has good",
    "start": "2551470",
    "end": "2558640"
  },
  {
    "text": "good encryption or a good performance while encrypted which for us is really a deal breaker because we have to encrypt",
    "start": "2558640",
    "end": "2564970"
  },
  {
    "text": "everything that we store in s3 so what are the workloads that we're",
    "start": "2564970",
    "end": "2570640"
  },
  {
    "start": "2568000",
    "end": "2568000"
  },
  {
    "text": "running on EMR like I said we have basically SPARC and presto on SPARC we're running Zeppelin",
    "start": "2570640",
    "end": "2577480"
  },
  {
    "text": "which is a nice notebook interface on top of it economic research has been using spark like crazy market",
    "start": "2577480",
    "end": "2584349"
  },
  {
    "text": "surveillance is starting to evaluate SPARC with some of the machine learning technologies there and we also have a",
    "start": "2584349",
    "end": "2590140"
  },
  {
    "text": "dedicated machine learning group that is doing a lot of experimentation with derivative datasets and you know",
    "start": "2590140",
    "end": "2596380"
  },
  {
    "text": "sentiment analysis and all that kind of thing they're all using spark and Zeppelin presto is a sequel interface on",
    "start": "2596380",
    "end": "2603970"
  },
  {
    "text": "top of data which was originally written by Facebook and they they open sourced it a few years ago that system is used by our trade history",
    "start": "2603970",
    "end": "2612119"
  },
  {
    "text": "system called trade info so if you if you query for an order that's six months old it's going to hit redshift and if",
    "start": "2612119",
    "end": "2618549"
  },
  {
    "text": "you query for an order that's over a year old it's gonna hit presto and then finally we're also experimenting with",
    "start": "2618549",
    "end": "2624309"
  },
  {
    "text": "business intelligence and reporting activities on top of presto well and it's it's actually going really well",
    "start": "2624309",
    "end": "2629980"
  },
  {
    "text": "it's a it's a great system so it's important to talk about our data",
    "start": "2629980",
    "end": "2635110"
  },
  {
    "start": "2632000",
    "end": "2632000"
  },
  {
    "text": "strategy for EMR so with redshift you have CPU memory and disk coupled",
    "start": "2635110",
    "end": "2640480"
  },
  {
    "text": "together on each node right and if you expand your cluster you're expanding your storage and your your compute and",
    "start": "2640480",
    "end": "2646600"
  },
  {
    "text": "your memory all at the same time with EMR and our strategy of using s3 we're",
    "start": "2646600",
    "end": "2651730"
  },
  {
    "text": "able to scale those two independently right I don't even need any CPU for instance on the weekends I don't have to",
    "start": "2651730",
    "end": "2656830"
  },
  {
    "text": "run anything but I can keep expanding that data footprint in s3 basically infinitely we",
    "start": "2656830",
    "end": "2664840"
  },
  {
    "text": "use a hive directory structure on top of our data in s3 basically all of our data is time-series",
    "start": "2664840",
    "end": "2670720"
  },
  {
    "text": "right we get orders every day they're all timestamp to a nanosecond precision and this directory structure lends",
    "start": "2670720",
    "end": "2677170"
  },
  {
    "text": "itself really well to to partitioning data by date so the the",
    "start": "2677170",
    "end": "2682180"
  },
  {
    "text": "directory structure in s3 is basically schema is the first directory and then the table is the next directory and then",
    "start": "2682180",
    "end": "2687880"
  },
  {
    "text": "there's a partition and then there's a bunch of park' files right that also lends itself well to fine grained access",
    "start": "2687880",
    "end": "2693190"
  },
  {
    "text": "control using bucket policies because I can grant access to a specific subset of",
    "start": "2693190",
    "end": "2698350"
  },
  {
    "text": "directories for a particular user or a particular group of users that kind of thing it all works very well and then",
    "start": "2698350",
    "end": "2704820"
  },
  {
    "text": "the other nice thing is that we can use s3 is really our our source of truth",
    "start": "2704820",
    "end": "2710530"
  },
  {
    "text": "right so if our hive meta store were to be corrupted or destroyed somehow we can",
    "start": "2710530",
    "end": "2715600"
  },
  {
    "text": "always recreate all of that information by just looking at the files in s3 and you can use the hive ms CK repair table",
    "start": "2715600",
    "end": "2722410"
  },
  {
    "text": "command and it'll figure out all the partitions for a given table again just by crawling s3 basically",
    "start": "2722410",
    "end": "2728790"
  },
  {
    "text": "so we use a multi account strategy with s3 and EMR as well and this lets us do a",
    "start": "2728790",
    "end": "2735070"
  },
  {
    "start": "2729000",
    "end": "2729000"
  },
  {
    "text": "lot of nice things so we have a centralized account where we store all of the actual data and then we have a",
    "start": "2735070",
    "end": "2741160"
  },
  {
    "text": "meta store there that we update every day as we're adding new partitions to that data right this lets and then and",
    "start": "2741160",
    "end": "2747130"
  },
  {
    "text": "then individual applications or internal departments have their own Amazon account and they get their own bill so",
    "start": "2747130",
    "end": "2754030"
  },
  {
    "text": "if they economic research for instance wants to run a ton of CPU on something they can do that and they're not to",
    "start": "2754030",
    "end": "2760359"
  },
  {
    "text": "compete with CPUs for the Trading Post system for instance so it's it's uh it's",
    "start": "2760359",
    "end": "2765700"
  },
  {
    "text": "really clean it lets them run whatever they want we we've never really had a contention issue with s3 as well because",
    "start": "2765700",
    "end": "2772150"
  },
  {
    "text": "it scales extremely well like I said we use cross account bucket policies to grant access to these",
    "start": "2772150",
    "end": "2778390"
  },
  {
    "text": "different groups and then we use a V PC peering for a hive meadow store connection right because there's a",
    "start": "2778390",
    "end": "2784089"
  },
  {
    "text": "there's a thrift pipeline basically that goes from the the customer EMR cluster",
    "start": "2784089",
    "end": "2789220"
  },
  {
    "text": "over to the hive meadow store cluster so for network security this looks very",
    "start": "2789220",
    "end": "2796240"
  },
  {
    "text": "similar to the redshift setup we run our clusters in private V PC subnets we use",
    "start": "2796240",
    "end": "2801369"
  },
  {
    "text": "a V PC endpoint for for talking to s3 and we lock down security groups if the",
    "start": "2801369",
    "end": "2808780"
  },
  {
    "text": "EMR cluster needs to talk to other Amazon API s that aren't s3 that has to go through an at Gateway that's in a",
    "start": "2808780",
    "end": "2814900"
  },
  {
    "text": "public V PC subnet we can monitor all the access and security groups across all of this to keep you know very tight",
    "start": "2814900",
    "end": "2821349"
  },
  {
    "text": "control over everything and again accessing these clusters from on-premise is done across a B PC or Amazon Direct",
    "start": "2821349",
    "end": "2829030"
  },
  {
    "text": "Connect so that nothing really transverse arch reverses the public internet and again on-premise firewalls",
    "start": "2829030",
    "end": "2835089"
  },
  {
    "text": "to further limit you know connectivity so for data security",
    "start": "2835089",
    "end": "2841080"
  },
  {
    "start": "2839000",
    "end": "2839000"
  },
  {
    "text": "these clusters are not long-running other than the the hive meta store which runs all the time but it doesn't",
    "start": "2841080",
    "end": "2847270"
  },
  {
    "text": "actually store any data so we use HDFS only for scratch space all the permanent",
    "start": "2847270",
    "end": "2852580"
  },
  {
    "text": "data is stored in s3 it's a lot cheaper anyway on the actual instances we've started to",
    "start": "2852580",
    "end": "2859599"
  },
  {
    "text": "use the new EMR security configurations which allow us to encrypt the local disks or the the data volumes at least",
    "start": "2859599",
    "end": "2864849"
  },
  {
    "text": "on those local disks for scratch space and that kind of thing and we also have an EMR bootstrap action that configures",
    "start": "2864849",
    "end": "2871720"
  },
  {
    "text": "SELinux on each of the nodes in the cluster and that script in particular is kind of special in that it it has to",
    "start": "2871720",
    "end": "2878320"
  },
  {
    "text": "reboot the entire cluster you know when you start it up so you have to basically wait for the cluster dough to end up in",
    "start": "2878320",
    "end": "2884650"
  },
  {
    "text": "the waiting state and then you reboot everything and everybody comes back just fine",
    "start": "2884650",
    "end": "2889720"
  },
  {
    "text": "for data security on EMR we use EMR FS which is an HDFS interface on top of s3",
    "start": "2889720",
    "end": "2896740"
  },
  {
    "start": "2890000",
    "end": "2890000"
  },
  {
    "text": "that has client-side encryption built in as part of EMR FS so it's it's actually very easy to use we use a custom key",
    "start": "2896740",
    "end": "2904850"
  },
  {
    "text": "management system so we have a custom encryption materials provider jar that we place in s3 and then it gets copied",
    "start": "2904850",
    "end": "2910940"
  },
  {
    "text": "down on each cluster node and then it's it's also really important that request to seek within an object in s3 work even",
    "start": "2910940",
    "end": "2919010"
  },
  {
    "text": "though it's encrypted and that's really important for all these modern file formats like park' because they have an",
    "start": "2919010",
    "end": "2924140"
  },
  {
    "text": "internal block structure and you know that you have to seek around within the file and not just read it sequentially",
    "start": "2924140",
    "end": "2929480"
  },
  {
    "text": "from the beginning and then again you know we're using bucket policies for for multi account access",
    "start": "2929480",
    "end": "2936460"
  },
  {
    "text": "so for SPARC data security EMR FS just works you you basically do a couple",
    "start": "2936460",
    "end": "2942110"
  },
  {
    "start": "2937000",
    "end": "2937000"
  },
  {
    "text": "configurations on EMR and it's off to the races it was very easy to get going for apache Zeppelin notebook storage in",
    "start": "2942110",
    "end": "2950210"
  },
  {
    "text": "s3 we actually contributed to that project to allow custom KMS and amazon KMS for",
    "start": "2950210",
    "end": "2958190"
  },
  {
    "text": "client-side encryption of the notebooks that are stored in s3 so as of version 0.6 not 0 of Zeppelin you can just set",
    "start": "2958190",
    "end": "2965540"
  },
  {
    "text": "up configurations to make I go if you want to look at the the github pull request you can see it wasn't all that",
    "start": "2965540",
    "end": "2971600"
  },
  {
    "text": "much work and then for presto presto doesn't use the MRFs it has its own s3",
    "start": "2971600",
    "end": "2978380"
  },
  {
    "start": "2973000",
    "end": "2973000"
  },
  {
    "text": "file system called presto s3 filesystem and that's part of what presto calls the hive connector which allows you to talk",
    "start": "2978380",
    "end": "2985160"
  },
  {
    "text": "to a hive meta store for metadata and then talk to s3 for your actual data so",
    "start": "2985160",
    "end": "2991330"
  },
  {
    "text": "Nasdaq contributed client-side encryption support for presto as a version 0.1 29 which was a while ago I",
    "start": "2991330",
    "end": "2999200"
  },
  {
    "text": "mean presto terms anyway we contributed client-side support with",
    "start": "2999200",
    "end": "3004480"
  },
  {
    "text": "our client-side encryption support with custom KMS and then for version 153 we",
    "start": "3004480",
    "end": "3010359"
  },
  {
    "text": "contributed client-side encryption support with the Amazon kms as your key management",
    "start": "3010359",
    "end": "3015920"
  },
  {
    "text": "and you can again look at those pull requests if you want to see what happened there and then finally",
    "start": "3015920",
    "end": "3022810"
  },
  {
    "text": "the next thing that we're working on is we you know we have all these groups that are producing datasets for their",
    "start": "3022810",
    "end": "3028310"
  },
  {
    "text": "own use in their own accounts but in some cases those datasets are broadly useful to the rest of the organization",
    "start": "3028310",
    "end": "3034460"
  },
  {
    "text": "so we wanted to figure out a way to let those groups share that data with with the overall organization but still",
    "start": "3034460",
    "end": "3041030"
  },
  {
    "start": "3039000",
    "end": "3039000"
  },
  {
    "text": "maintain you know very tight control over who's writing to the to this archive right I don't want to just give out s3 write access to anybody in that",
    "start": "3041030",
    "end": "3049580"
  },
  {
    "text": "in that bucket because you know bad things could happen so what happens now is clients can perform analytics in their",
    "start": "3049580",
    "end": "3057110"
  },
  {
    "text": "own account using whatever tools they want you know spark presto drill like anything they feel like new data sets",
    "start": "3057110",
    "end": "3063740"
  },
  {
    "text": "are created in that account and placed into a staging bucket where we the centralized warehouse account we receive",
    "start": "3063740",
    "end": "3072080"
  },
  {
    "text": "sqs messages notifying us that there's a new file in that staging bucket and then we can examine that file figure out",
    "start": "3072080",
    "end": "3078860"
  },
  {
    "text": "where it's supposed to go and then place it at the correct location in the in the main archive so this is this is nice",
    "start": "3078860",
    "end": "3084920"
  },
  {
    "text": "because it allows all these customer accounts to do whatever they want and produce these data sets in any way they",
    "start": "3084920",
    "end": "3090110"
  },
  {
    "text": "feel like and then we can still maintain tight control over how the entire archive is is managed so that's all I",
    "start": "3090110",
    "end": "3097010"
  },
  {
    "text": "have thank you very much for your time and remember to do your evaluations and",
    "start": "3097010",
    "end": "3102260"
  },
  {
    "text": "I think we're gonna take questions if you like",
    "start": "3102260",
    "end": "3106810"
  },
  {
    "text": "[Applause]",
    "start": "3107770",
    "end": "3115510"
  }
]