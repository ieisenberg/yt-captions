[
  {
    "text": "- Hi folks, Emily here. My name is Emily Webber. I'm a Machine Learning\nSpecialist Solutions Architect",
    "start": "1200",
    "end": "6540"
  },
  {
    "text": "at Amazon Web Services, and\ntoday we are gonna learn about reinforcement learning\nwith human feedback.",
    "start": "6540",
    "end": "12599"
  },
  {
    "text": "So this is a super cool topic\nin a ML and in generative AI.",
    "start": "12600",
    "end": "17600"
  },
  {
    "text": "And I believe that reinforcement learning with human feedback is\nprobably the single best way",
    "start": "17730",
    "end": "23043"
  },
  {
    "text": "to get a really good large language model and a really good generative\nAI model writ large.",
    "start": "23043",
    "end": "29160"
  },
  {
    "text": "So let's get started. All right, so first we'll come to find out",
    "start": "29160",
    "end": "34327"
  },
  {
    "text": "that not all human feedback is the same. So this is one of the core reasons",
    "start": "34327",
    "end": "40230"
  },
  {
    "text": "why generative AI has been a\nsuper hard task for many years. Quite a few of us, including\nmyself, have spent years trying",
    "start": "40230",
    "end": "47760"
  },
  {
    "text": "to generate stories, generate\nmusic, generate all sorts of things, and at its\nheart, the hardest part",
    "start": "47760",
    "end": "54523"
  },
  {
    "text": "about generative AI is\nevaluating the context and evaluating the quality of the result.",
    "start": "54523",
    "end": "61109"
  },
  {
    "text": "And so reinforcement\nlearning with human feedback, or RLHF is one really\npromising way to do this.",
    "start": "61110",
    "end": "67920"
  },
  {
    "text": "However, it's not the only one. There are many kinds of reward modeling. And so we'll look at different ways",
    "start": "67920",
    "end": "73350"
  },
  {
    "text": "of building reward models and we'll talk through implementation options on AWS.",
    "start": "73350",
    "end": "79770"
  },
  {
    "text": "So I have a couple options for you of ways that you can build and design your own reward modeling systems on AWS",
    "start": "79770",
    "end": "87930"
  },
  {
    "text": "so that you can build\nyour own foundation models and create the breakthrough\napps of the future.",
    "start": "87930",
    "end": "93930"
  },
  {
    "text": "So, and then we'll close out\nwith the hands-on walkthrough of RLHF on AWS.",
    "start": "93930",
    "end": "100229"
  },
  {
    "text": "So let's do it. Alright, so the first concept",
    "start": "100230",
    "end": "107370"
  },
  {
    "text": "that we wanna really understand here is that not all human feedback is the same. Specifically, there are really\ntwo very different kinds",
    "start": "107370",
    "end": "115680"
  },
  {
    "text": "of human feedback. One type of human feedback is objective. This is when humans say\nlike, yes, obviously one",
    "start": "115680",
    "end": "123743"
  },
  {
    "text": "plus one is two. Or if you say like,\nthe literal translation or the literal classification",
    "start": "123743",
    "end": "131362"
  },
  {
    "text": "of something is very objective. If you look at external\noutcomes of some experiment",
    "start": "131362",
    "end": "138630"
  },
  {
    "text": "or external outcomes of\nsome action like physics, it's very objective, it's very\nobvious, it's very empirical.",
    "start": "138630",
    "end": "146670"
  },
  {
    "text": "So for example, this\nsquare is obviously black, and this square is obviously white.",
    "start": "146670",
    "end": "152670"
  },
  {
    "text": "Right there, there are two\nends of the spectrum here. They're not the same, they are different.",
    "start": "152670",
    "end": "158220"
  },
  {
    "text": "That is sort of objective human feedback. Not all human feedback is like this. Some type of human feedback\nis more subjective,",
    "start": "158220",
    "end": "166769"
  },
  {
    "text": "which is to say not every human\nfeels the same way about it. That's sort of the difference.",
    "start": "166770",
    "end": "172049"
  },
  {
    "text": "So like with objective human feedback, if you have maybe 10, 20, 30,",
    "start": "172050",
    "end": "178069"
  },
  {
    "text": "or all humans who look at\nsomething, everyone can agree that yes, this is true.",
    "start": "178070",
    "end": "184140"
  },
  {
    "text": "That's subjective human feedback. Subjective human feedback is\nwhere it's much more according",
    "start": "184140",
    "end": "189690"
  },
  {
    "text": "to individual's interpretations, where people have these\nnuanced preferences. They just like something not\nfor any particular reason,",
    "start": "189690",
    "end": "198900"
  },
  {
    "text": "but that's just because that's how they feel about something. They have a gut reaction, they respond",
    "start": "198900",
    "end": "204659"
  },
  {
    "text": "to content differently. We all interpret artwork differently. And so it's sort of like\na gray scale, right?",
    "start": "204660",
    "end": "210780"
  },
  {
    "text": "There's a large spectrum of colors, there's a large spectrum\nof human responses.",
    "start": "210780",
    "end": "216558"
  },
  {
    "text": "And so our challenge in\ngenerative AI is incorporating not",
    "start": "216558",
    "end": "221558"
  },
  {
    "text": "just objective human feedback, but this subjective human feedback. And here's why.",
    "start": "222240",
    "end": "228150"
  },
  {
    "text": "Objective human feedback\nlike classification, like empirical sort of\nstudies and empirical results",
    "start": "228150",
    "end": "235200"
  },
  {
    "text": "from your dataset, that's\ngreat for traditional ML tasks. That's perfect for, again,\nclassification, regression,",
    "start": "235200",
    "end": "243163"
  },
  {
    "text": "for recommendations, for\nforecasting is perfect for things like this\nwhere you take your data",
    "start": "244140",
    "end": "251580"
  },
  {
    "text": "and then you use that to train\nyour model and you're done. In generative AI, it's different",
    "start": "251580",
    "end": "256829"
  },
  {
    "text": "because the models, again,\nare generating not just tasks",
    "start": "256830",
    "end": "261829"
  },
  {
    "text": "or not just entities, but they're\ngenerating artwork really. They're generating images,\nall these different types",
    "start": "263990",
    "end": "271560"
  },
  {
    "text": "of images and they're generating\nlanguage and not just sort of obvious or clear or\nempirical answers to questions,",
    "start": "271560",
    "end": "280410"
  },
  {
    "text": "but many times it's subjective. And so what we want is a way to aggregate this subjective\nhuman feedback at scale.",
    "start": "280410",
    "end": "289169"
  },
  {
    "text": "That's what reward modeling\nis really, really good at. So for example, in a traditional ML\ntask, we'll take our text",
    "start": "289170",
    "end": "298020"
  },
  {
    "text": "which is, \"I am not into this\nhouse, it's too expensive, too far from the train line.\"",
    "start": "298020",
    "end": "303750"
  },
  {
    "text": "We send that to a model,\nand then the model looks at the labels in the\ndataset where the labels",
    "start": "303750",
    "end": "310289"
  },
  {
    "text": "that humans could provide were one of five or in this case, one of two. It's either positive or it's negative.",
    "start": "310290",
    "end": "318030"
  },
  {
    "text": "It's not both. It's not somewhere between. It's either positive or negative. So that type of objective\nhuman feedback is useful",
    "start": "318030",
    "end": "327090"
  },
  {
    "text": "to train a classification model. However, a generative model is gonna give",
    "start": "327090",
    "end": "333060"
  },
  {
    "text": "you something more like this where we say, is the\nperson gonna buy the house? And then the agent sort of\nhypothesizes like, it's unlikely.",
    "start": "333060",
    "end": "343040"
  },
  {
    "text": "It's very unlikely. But what we want is a way\nto say, for the people",
    "start": "343320",
    "end": "350776"
  },
  {
    "text": "to say is this a good response or is this not a good response? So in any case, generative ML tasks need",
    "start": "350880",
    "end": "359280"
  },
  {
    "text": "subjective human responses\naggregated at scale. So we'll take an LLM, we'll\nhave the LLM generate tons",
    "start": "359280",
    "end": "368970"
  },
  {
    "text": "of responses to different questions. And then your human labelers\nactually rank those.",
    "start": "368970",
    "end": "375330"
  },
  {
    "text": "They're gonna put those into categories. They'll label them\nfavorite to least favorite,",
    "start": "375330",
    "end": "380460"
  },
  {
    "text": "and then we're gonna use\nthat to train a new model. We'll use that to train a reward model,",
    "start": "380460",
    "end": "385650"
  },
  {
    "text": "which is called reward modeling. All right, so reward modeling aggregates",
    "start": "385650",
    "end": "393060"
  },
  {
    "text": "human feedback at scale. Again, specifically\nsubjective human feedback is where it's really perfect for this.",
    "start": "393060",
    "end": "400290"
  },
  {
    "text": "So we'll pick a base foundation model, any foundation model, pick a\nbase LLM, base diffusion model,",
    "start": "400290",
    "end": "407700"
  },
  {
    "text": "base clip, base llama, base\nfalcon, whatever you want. Pick your base foundation model.",
    "start": "408210",
    "end": "414780"
  },
  {
    "text": "And then we're gonna bring\nprompts to that foundation model. So we'll bring your prompt data set",
    "start": "414780",
    "end": "420120"
  },
  {
    "text": "with all of the questions,\nall of the content that you're really interested\nin for your use cases.",
    "start": "420120",
    "end": "426120"
  },
  {
    "text": "For each prompt, we'll send\nthat to your base model and get multiple responses.",
    "start": "426120",
    "end": "431610"
  },
  {
    "text": "So for each prompt, you\nwant four or five answers. So then you have this one\nto many data set of prompts",
    "start": "431610",
    "end": "439358"
  },
  {
    "text": "and responses from this model. Then we're gonna hire human\nlabelers to rank the responses.",
    "start": "439358",
    "end": "446640"
  },
  {
    "text": "So you'll use like SageMaker\nGround Truth for example, to basically ask the humans\nto label these responses.",
    "start": "447420",
    "end": "457100"
  },
  {
    "text": "So from the one prompt, they\ncan then rank their responses. So they can say like, oh\nyes, this one's my favorite,",
    "start": "457170",
    "end": "464729"
  },
  {
    "text": "this one's my least favorite and these two are in the middle. Once you have this data set,",
    "start": "464730",
    "end": "470250"
  },
  {
    "text": "again with one prompt\nand multiple responses, and the preferences for those\nwill train a reward model.",
    "start": "470250",
    "end": "478169"
  },
  {
    "text": "So we'll actually train\na new generative model that uses those responses.",
    "start": "478170",
    "end": "486557"
  },
  {
    "text": "But instead of just outputting a text, it's gonna output a number. So the reward model\noutputs literally a scaler,",
    "start": "486557",
    "end": "495000"
  },
  {
    "text": "just a number instead of generating text or generating images.",
    "start": "495000",
    "end": "500940"
  },
  {
    "text": "And the reason, so it's\nlike a regressive model. So we train this regressive\nreward model, we're gonna use",
    "start": "500940",
    "end": "508050"
  },
  {
    "text": "that to train a new generative model. So now we're actually\ngonna fine tune a LLM",
    "start": "508050",
    "end": "515250"
  },
  {
    "text": "or a vision-based generative\nmodel using this reward model that we just built.",
    "start": "515250",
    "end": "521580"
  },
  {
    "text": "So we're gonna use the reward model to fine tune our new LLM, fine tune",
    "start": "521580",
    "end": "527130"
  },
  {
    "text": "our new generative model, and\nthen we'll evaluate this thing to see how it performs and start\nthe whole thing over again.",
    "start": "527130",
    "end": "533100"
  },
  {
    "text": "So let's learn how to do this. All right, so again,\nreinforcement learning",
    "start": "533100",
    "end": "538199"
  },
  {
    "text": "with human feedback is one\nof the most common ways to perform reward modeling. So what is this?",
    "start": "538200",
    "end": "544050"
  },
  {
    "text": "So literally just as we described. You're gonna start with\na data set of prompts",
    "start": "544050",
    "end": "549663"
  },
  {
    "text": "and then you have multiple responses, so multiple responses per prompt. You're gonna send this whole\ncorpus to humans for ranking.",
    "start": "549663",
    "end": "558450"
  },
  {
    "text": "The humans review this. And you're probably asking like, oh, well, why will they pick certain things?",
    "start": "558450",
    "end": "566073"
  },
  {
    "text": "Some of them will like things differently. Some of them will prefer\nreally short ones.",
    "start": "567150",
    "end": "573210"
  },
  {
    "text": "Some of them will prefer really long ones. Some of them will prefer the silly ones.",
    "start": "573210",
    "end": "578430"
  },
  {
    "text": "So you can determine some\nof that in the instructions for the humans saying,\n\"Please pick something",
    "start": "578430",
    "end": "585000"
  },
  {
    "text": "that's more clear or something\nthat's less harmful.\" Or you can actually say\nlike, \"Please search online",
    "start": "585000",
    "end": "591630"
  },
  {
    "text": "and do fact checking for this\nto ensure that it's accurate.\" So in your instructions, you\nhave ways of kind of indicating",
    "start": "591630",
    "end": "598950"
  },
  {
    "text": "to the humans what you prefer. So in any case, your humans rank these.",
    "start": "598950",
    "end": "605279"
  },
  {
    "text": "Then you're gonna train a reward\nmodel using those rankings. So you'll train this reward\nmodel on the human rankings.",
    "start": "605280",
    "end": "614760"
  },
  {
    "text": "And then the reinforcement\nlearning comes in at the second piece actually where we're gonna use\nreinforcement learning",
    "start": "614760",
    "end": "620670"
  },
  {
    "text": "to connect the reward model\nto the new generative model. And then the reason for this is",
    "start": "620670",
    "end": "626730"
  },
  {
    "text": "that your final model should be two to three times better than the original. So the quality of this\nmodel is so much higher",
    "start": "626730",
    "end": "636560"
  },
  {
    "text": "than it would've been previously. It's so much more preferred by humans",
    "start": "637054",
    "end": "642300"
  },
  {
    "text": "than it would've been without. So let's do a quick recap of\nreinforcement learning just",
    "start": "642300",
    "end": "648690"
  },
  {
    "text": "to refresh this. And for those of you who are new, this is to help you understand what reinforcement learning is all about.",
    "start": "648690",
    "end": "654878"
  },
  {
    "text": "So reinforcement learning is\na type of machine learning that's commonly used to sort\nof train robotic agents.",
    "start": "654878",
    "end": "662190"
  },
  {
    "text": "So you'll use reinforcement\nlearning to take this agent. And an agent is an autonomous entity",
    "start": "662190",
    "end": "669660"
  },
  {
    "text": "that we're trying to train. A policy is sort of like\nthe brain of the agent.",
    "start": "669660",
    "end": "675480"
  },
  {
    "text": "The policy is how the agent learns. So like if we were\ntraining a literal robot,",
    "start": "675480",
    "end": "680819"
  },
  {
    "text": "the robot would be the agent and then the robot uses the policy, which is like their neural network,",
    "start": "680820",
    "end": "687330"
  },
  {
    "text": "just like Arnold Schwarzenegger when he promised us he'd be back. And so yeah, so we use the policy.",
    "start": "687330",
    "end": "695263"
  },
  {
    "text": "Yes, we have a art reinforcement\nlearning, agent, policy, and then the policy is going\nto learn how to pick actions.",
    "start": "695263",
    "end": "704610"
  },
  {
    "text": "So it searches in what\nwe call the action space. So it searches in the space\nof all possible agent actions.",
    "start": "704610",
    "end": "713190"
  },
  {
    "text": "It's gonna take an action. And so the policy picks the action",
    "start": "713190",
    "end": "718529"
  },
  {
    "text": "and then the agent takes the action and then the action produces some results.",
    "start": "718530",
    "end": "725250"
  },
  {
    "text": "And so the result in RL\nthe result is produced",
    "start": "725250",
    "end": "730250"
  },
  {
    "text": "by the environment. So you define sort of this environment that the agent is operating inside of.",
    "start": "730260",
    "end": "737514"
  },
  {
    "text": "And so the action produces this result. Then your job as the designer\nof the system is to figure out",
    "start": "737515",
    "end": "746300"
  },
  {
    "text": "what type of rewards signal\nto provide to this agent. And so your action produces a result.",
    "start": "746430",
    "end": "754050"
  },
  {
    "text": "Then you build this reward\nsignal or this reward function",
    "start": "754050",
    "end": "759050"
  },
  {
    "text": "that tells the agent, does\nthe agent get a lot of reward from this function or a little reward?",
    "start": "759120",
    "end": "768139"
  },
  {
    "text": "So it's common in video games for example. You can train an agent to solve the game,",
    "start": "768139",
    "end": "775350"
  },
  {
    "text": "to just play the game, in\nwhich case the reward signal is literally the number of\npoints that the agent can get.",
    "start": "775350",
    "end": "782070"
  },
  {
    "text": "Or if you're training a physical robot, you might use a reward signal",
    "start": "782070",
    "end": "787470"
  },
  {
    "text": "that is did the robot pick up the computer or did the robot grasp the widget?",
    "start": "787470",
    "end": "794798"
  },
  {
    "text": "In which case that's the reward signal. So you as the data scientist here get to build the reward signal and sort",
    "start": "794798",
    "end": "801900"
  },
  {
    "text": "of set up this whole stack. So that's a quick recap\nof reinforcement learning.",
    "start": "801900",
    "end": "808290"
  },
  {
    "text": "Now let's learn how we're going to apply reinforcement\nlearning to updating our LLM.",
    "start": "808290",
    "end": "813990"
  },
  {
    "text": "So remember in this case\nwe're going to use RL to not to pre-train, although\nI am a little bit skeptical",
    "start": "813990",
    "end": "821718"
  },
  {
    "text": "about this 'cause I think\nthat's probably the way to go. But in any case, right now we're going to use reinforcement\nlearning to fine tune an LLM.",
    "start": "822120",
    "end": "830070"
  },
  {
    "text": "And so the policy of the agent is the LLM.",
    "start": "830070",
    "end": "835070"
  },
  {
    "text": "So that this new large language\nmodel that we're trying to fine tune is the policy\nand it's orchestrated",
    "start": "835380",
    "end": "842880"
  },
  {
    "text": "by, for example, one\nreinforcement learning algorithm that's called proxy policy\noptimization, so PPO.",
    "start": "842880",
    "end": "849813"
  },
  {
    "text": "And then the action space\nhere is all possible tokens in the vocabulary.",
    "start": "850800",
    "end": "856590"
  },
  {
    "text": "So for the LLM, it's\nlike all possible words that this LLM could be generating.",
    "start": "856590",
    "end": "862260"
  },
  {
    "text": "So that's the action space. And then the reward model is that model",
    "start": "862260",
    "end": "868170"
  },
  {
    "text": "that we just trained. So that model that we\ntrained ahead of time on the human ranking data\nsets that takes in new text",
    "start": "868170",
    "end": "877517"
  },
  {
    "text": "and uses the response from\nhumans to say how good",
    "start": "880218",
    "end": "885218"
  },
  {
    "text": "that is empirically, so translating a subjective\nproblem to an empirical problem,",
    "start": "885300",
    "end": "891180"
  },
  {
    "text": "which is super cool. so we'll use the reward model to do this.",
    "start": "891180",
    "end": "895503"
  },
  {
    "text": "And then it's been found\nthat it's actually helpful to have a distance function\nto keep the original LLM",
    "start": "896610",
    "end": "904350"
  },
  {
    "text": "and the one that we're\ntraining much closer. If we didn't use this distance function than the reward model and\nthe PPO would go crazy.",
    "start": "904350",
    "end": "912714"
  },
  {
    "text": "Basically the PPO would be able to just hack the reward model",
    "start": "912714",
    "end": "917823"
  },
  {
    "text": "and it wouldn't abstract,\nit wouldn't learn anything, it would just be optimizing for reward.",
    "start": "917823",
    "end": "923850"
  },
  {
    "text": "And so this way when you add this divergence penalty function, that keeps the model generating content",
    "start": "923850",
    "end": "932370"
  },
  {
    "text": "that's much more in line\nwith the original LLM. Great, so a reward model,\nthis divergence function",
    "start": "932370",
    "end": "940200"
  },
  {
    "text": "and then a reward function. And so the reward function\ncombines basically",
    "start": "940200",
    "end": "946379"
  },
  {
    "text": "that divergence term with the\noutput of this reward model.",
    "start": "946380",
    "end": "951380"
  },
  {
    "text": "And it's gonna combine both of\nthese two to update the agent and specifically to update the\npolicy or the neural network",
    "start": "952320",
    "end": "960389"
  },
  {
    "text": "or the PPO of the agent. So let's take a closer look. So mathematically speaking, let's see",
    "start": "960390",
    "end": "967410"
  },
  {
    "text": "if we can break this down here. All right, so let's\ncall X just the prompts",
    "start": "967410",
    "end": "972690"
  },
  {
    "text": "from your training dataset, simple enough. X here is just the\nprompts that are coming in",
    "start": "972690",
    "end": "977920"
  },
  {
    "text": "from your training dataset. Maybe you're just using\none from open source, maybe you're bringing your own.",
    "start": "977920",
    "end": "984870"
  },
  {
    "text": "I'm gonna throw two Y's at you. So this Y*, so Y* is the text\nthat's generated by the LLM",
    "start": "984870",
    "end": "991970"
  },
  {
    "text": "or the PPO that we are\ntraining using the prompt. So I take X, which is my training prompts.",
    "start": "995310",
    "end": "1004310"
  },
  {
    "text": "I send X into the LLM that I am training and I get this Y*.",
    "start": "1004310",
    "end": "1010985"
  },
  {
    "text": "So Y* is gonna change\nevery time I update my LLM.",
    "start": "1010985",
    "end": "1015985"
  },
  {
    "text": "Because I take my prompts, my prompts are like my mini batch. I throw my minibatch\nthrough my neural network.",
    "start": "1016163",
    "end": "1024110"
  },
  {
    "text": "I run the forward pass. I get Y* at the end of every batch.",
    "start": "1024110",
    "end": "1029793"
  },
  {
    "text": "So that's Y*. I'm gonna call Y0 text that was generated",
    "start": "1029794",
    "end": "1036380"
  },
  {
    "text": "from that original LLM\nalso using the prompts. So let's say, so I take\nthe same X data set",
    "start": "1036380",
    "end": "1044610"
  },
  {
    "text": "and then I take X and I throw\nthat into my original LLM, so this first GPT that I'm using.",
    "start": "1047840",
    "end": "1054528"
  },
  {
    "text": "And I run this X through my first GPT and I'm gonna get this thing I call Y0.",
    "start": "1054528",
    "end": "1061310"
  },
  {
    "text": "So again, both Y* and Y0 are\nlike responses to the prompts.",
    "start": "1061310",
    "end": "1066310"
  },
  {
    "text": "The difference is that\nY* changes every time, 'cause it's from the LLM that\nI'm fine tuning per batch",
    "start": "1067850",
    "end": "1076490"
  },
  {
    "text": "and per step, while Y0\nis constant because Y0 is",
    "start": "1076490",
    "end": "1081490"
  },
  {
    "text": "from my original training data set. Actually no, Y0 is from my original LLM.",
    "start": "1083438",
    "end": "1091700"
  },
  {
    "text": "So Y0 doesn't change. And Y* changes constantly.",
    "start": "1091700",
    "end": "1096653"
  },
  {
    "text": "All right, so now we got those. Let's break this down. So we're gonna compute this R\ntheta using this reward model.",
    "start": "1097820",
    "end": "1106182"
  },
  {
    "text": "So remember the reward model is the result of this training process that we just did",
    "start": "1108380",
    "end": "1115159"
  },
  {
    "text": "where I take my data set, I\nhave my humans rank my data set,",
    "start": "1115160",
    "end": "1120160"
  },
  {
    "text": "and then I train a model that takes text and gives me a scaler.",
    "start": "1120500",
    "end": "1125540"
  },
  {
    "text": "So that's this R0, it's\njust what comes out of the reward model. Think regressive LLM.",
    "start": "1125540",
    "end": "1132020"
  },
  {
    "text": "And into my regressive\nLLM, I'm gonna send in both the prompt and this Y*.",
    "start": "1132020",
    "end": "1139640"
  },
  {
    "text": "So, and this response from my LLM and that is basically telling\nme what humans prefer.",
    "start": "1139640",
    "end": "1146659"
  },
  {
    "text": "That's this R theta is like, how much did humans like this thing? That's what this means.",
    "start": "1146660",
    "end": "1152840"
  },
  {
    "text": "Then I'm gonna compute another term here. And so the second term\nis the KLDivergence.",
    "start": "1152840",
    "end": "1159830"
  },
  {
    "text": "I'm gonna say Kulberlyblock\nor something like this. My apologies for forgetting\nthe names poorly,",
    "start": "1159830",
    "end": "1166520"
  },
  {
    "text": "but it's the KLDivergence\nthat basically looks at two distributions and computes the difference between them.",
    "start": "1166520",
    "end": "1172907"
  },
  {
    "text": "And so this divergence\nterm is looking at Y*.",
    "start": "1172907",
    "end": "1177677"
  },
  {
    "text": "So it's looking at, again,\nboth of the responses, the response generated\nby the LLM I'm training",
    "start": "1178610",
    "end": "1184591"
  },
  {
    "text": "and the response generated\nfrom my original LLM. And so it's looking at how far\napart those two things are.",
    "start": "1184591",
    "end": "1192049"
  },
  {
    "text": "And so I get this R sub\nKD, this divergence,",
    "start": "1192050",
    "end": "1197050"
  },
  {
    "text": "and again, this prevents\nout of character RL hacking. So this is stopping that\nPPO from just going crazy",
    "start": "1197128",
    "end": "1205254"
  },
  {
    "text": "and it's ensuring that the quality of the generated text is much\nmore in line with my LLM.",
    "start": "1205254",
    "end": "1211758"
  },
  {
    "text": "So I'm gonna take these two things and I'm just gonna put\nthem together basically.",
    "start": "1211759",
    "end": "1218390"
  },
  {
    "text": "So the reward signal for the PPO, which is basically the loss that I use",
    "start": "1218390",
    "end": "1225530"
  },
  {
    "text": "to update the LLM is just equal\nto the combination of these.",
    "start": "1225530",
    "end": "1230530"
  },
  {
    "text": "So it's the overall reward\nthat I get from humans by telling me how good\nthis content is minus",
    "start": "1231290",
    "end": "1240434"
  },
  {
    "text": "the penalty term for this\ndivergence times some epsilon",
    "start": "1240435",
    "end": "1245435"
  },
  {
    "text": "for however much we\nwanna weight this thing. And then the original authors\nfound that it was useful",
    "start": "1247670",
    "end": "1253220"
  },
  {
    "text": "to add some pre-training gradients here. And this is why I still believe\nthat once you've built this,",
    "start": "1253220",
    "end": "1261353"
  },
  {
    "text": "there's no reason to not just throw this into the pre-training. I don't know why you would\never wanna just do this in fine tuning, so, TBD on that.",
    "start": "1261410",
    "end": "1268694"
  },
  {
    "text": "But in any case, RLHF appears\nto be quite promising.",
    "start": "1268694",
    "end": "1273694"
  },
  {
    "text": "So this is from the\nOuyang et al work in 2022 in the instruct GPT\npaper where they showed",
    "start": "1274460",
    "end": "1281990"
  },
  {
    "text": "that PPO is really good,\nRLHF is really powerful.",
    "start": "1281990",
    "end": "1286990"
  },
  {
    "text": "So this is your base GPT-3,\nand then they're comparing it",
    "start": "1288650",
    "end": "1293650"
  },
  {
    "text": "with prompted GPT-3. So remember that few shot\nprompting we learned about",
    "start": "1293750",
    "end": "1299330"
  },
  {
    "text": "with the instructions, so\ncompared with prompting GPT, and then they're looking\nat this human win rate.",
    "start": "1299330",
    "end": "1305540"
  },
  {
    "text": "So basically, how much do\npeople like the response of this model relative to the\nresponses of other models?",
    "start": "1305540",
    "end": "1313520"
  },
  {
    "text": "And so yeah, so prompted\nGPT is clearly better than base GPT.",
    "start": "1313520",
    "end": "1319970"
  },
  {
    "text": "And then this green line here is supervised fine tuning, SFT.",
    "start": "1319970",
    "end": "1325490"
  },
  {
    "text": "So supervised fine tuning mostly refers to instruction fine tuning in this work",
    "start": "1325490",
    "end": "1331970"
  },
  {
    "text": "where they again, take\nthe prompts like, tell me about a time you had fun at the zoo,",
    "start": "1331970",
    "end": "1338990"
  },
  {
    "text": "or what's the number in my bank account? Or when is father's day in\n2023 or something like this.",
    "start": "1338990",
    "end": "1348820"
  },
  {
    "text": "So you tag those instructions\nand then you run fine tuning. So that's supervised fine tuning.",
    "start": "1350750",
    "end": "1356570"
  },
  {
    "text": "So GPT based, GPT prompted,\nsupervised fine tuning and then this one up here\nis PPO and PPO stands",
    "start": "1357830",
    "end": "1365240"
  },
  {
    "text": "for the reinforcement\nlearning with human feedback. And then this PPO, this is\nlike a special juiced version",
    "start": "1365240",
    "end": "1371570"
  },
  {
    "text": "of this, so, that was even better. And so your net net\nhere is that RLHF gives",
    "start": "1371570",
    "end": "1378190"
  },
  {
    "text": "you a lot of boost. It's two to three times\nbetter than base GPT,",
    "start": "1378770",
    "end": "1383930"
  },
  {
    "text": "and it's much better than\njust supervised fine tuning and it's certainly much\nbetter than prompting.",
    "start": "1383930",
    "end": "1389360"
  },
  {
    "text": "So reinforcement learning\nwith human feedback is shown to be a critical and extremely useful way",
    "start": "1389360",
    "end": "1396099"
  },
  {
    "text": "to improve the performance\nof your generative AI models. Another concept, this one\nis coming from Anthropic",
    "start": "1396099",
    "end": "1405260"
  },
  {
    "text": "on 2022, is this concept\nof constitutional AI where they suggest using\nAI's to evaluate the AI.",
    "start": "1405260",
    "end": "1414143"
  },
  {
    "text": "So certainly this makes\nit easier for the humans, but it's also because, I mean, models are",
    "start": "1415880",
    "end": "1422090"
  },
  {
    "text": "really good these days. LLM's are great, they're really\nstrong, they're powerful, they're knowledgeable, they're excellent.",
    "start": "1422090",
    "end": "1428809"
  },
  {
    "text": "And so there's this concept, why don't I just use AI to evaluate my AI?",
    "start": "1428810",
    "end": "1434477"
  },
  {
    "text": "And so that's what\nconstitutional AI is exploring. They train a variety of models",
    "start": "1434477",
    "end": "1440659"
  },
  {
    "text": "and then essentially use\nthe AI models to evaluate and critique the\nresponses of these models.",
    "start": "1440660",
    "end": "1448545"
  },
  {
    "text": "They also propose a what's\ncalled a red teamed dataset. The dataset is available on Hugging Face.",
    "start": "1448545",
    "end": "1455144"
  },
  {
    "text": "The red team dataset is\nfull of every stereotype",
    "start": "1455145",
    "end": "1460145"
  },
  {
    "text": "and every aggressive, harmful, toxic thing you have ever seen or heard or dreamed of.",
    "start": "1462770",
    "end": "1469340"
  },
  {
    "text": "So approach with caution,\nbut it's a useful data set because it helps mitigate the behavior",
    "start": "1469340",
    "end": "1477470"
  },
  {
    "text": "of that in your model. So I like this approach of\nusing AI to evaluate AI's,",
    "start": "1477470",
    "end": "1483860"
  },
  {
    "text": "particularly because I myself finds a red teaming data set\nextremely triggering.",
    "start": "1483860",
    "end": "1489740"
  },
  {
    "text": "It's so triggering and it's so toxic that I can barely even load\nit into a pandas data frame",
    "start": "1489740",
    "end": "1495020"
  },
  {
    "text": "before I need to go leave my computer and clear my head because\nit's so triggering.",
    "start": "1495020",
    "end": "1501346"
  },
  {
    "text": "And so I like this approach of using AI to evaluate your AI's\nbecause not only is it easier",
    "start": "1501347",
    "end": "1508310"
  },
  {
    "text": "and more cost effective,\nit also saves people from this extremely triggering content",
    "start": "1508310",
    "end": "1514610"
  },
  {
    "text": "which they can then just avoid. So that's great. So this is an interesting approach.",
    "start": "1514610",
    "end": "1520290"
  },
  {
    "text": "So again, what do you need\nto train a reward model? So to train your own\nreward model, net net here,",
    "start": "1521720",
    "end": "1528409"
  },
  {
    "text": "you want a one-to-many data\nset with prompts and responses. Remember, you've got your prompt library.",
    "start": "1528410",
    "end": "1534230"
  },
  {
    "text": "You've got this text file, this CSV file, maybe you've got a dynamo\ntable if you're going crazy",
    "start": "1534230",
    "end": "1540440"
  },
  {
    "text": "with all of your prompts. So you've got a prompt dataset and then each prompt\nhas multiple responses.",
    "start": "1540440",
    "end": "1548180"
  },
  {
    "text": "So for each prompt, you\nmight have 5, 10, I mean, just for that, maybe 5,\nmaybe just 5 prompts.",
    "start": "1548180",
    "end": "1555770"
  },
  {
    "text": "But you have multiple\nresponses for each prompt. So one to many. And then you need a GPT\nbased model that's going",
    "start": "1555770",
    "end": "1564950"
  },
  {
    "text": "to return a scaler, so\nit returns a number. So it's still a generative\npre-trained transformer,",
    "start": "1564950",
    "end": "1571670"
  },
  {
    "text": "but it's not generating text,\nit's returning in numbers. So I call that a regressive\nlarge language model.",
    "start": "1571670",
    "end": "1578870"
  },
  {
    "text": "Because it's a regressive\nmodel in the sense that it's producing this\ncontinuous scaler output,",
    "start": "1578870",
    "end": "1584172"
  },
  {
    "text": "but it is still a\nregressive large, it's still a large language model at the base.",
    "start": "1584172",
    "end": "1591830"
  },
  {
    "text": "Fortunately, you don't need a\nmassively large reward model. You can get away with a 6\nbillion parameter model,",
    "start": "1591830",
    "end": "1599480"
  },
  {
    "text": "which appears to be good enough. And so you need this model, and I'm happy",
    "start": "1599480",
    "end": "1606110"
  },
  {
    "text": "to say I have some PyTorch\ncode to share with you that you can copy and paste to get this",
    "start": "1606110",
    "end": "1611750"
  },
  {
    "text": "and to run this in your own project. So you need a GPT based PyTorch model and that returns a scaler.",
    "start": "1611750",
    "end": "1619220"
  },
  {
    "text": "And then lo and behold, you need some distributed training systems",
    "start": "1619220",
    "end": "1624451"
  },
  {
    "text": "to train this entire process for you. So let's break it down.",
    "start": "1624451",
    "end": "1628320"
  },
  {
    "text": "All right, so again, let's try and really understand these\ndata sets for reward modeling.",
    "start": "1629960",
    "end": "1634793"
  },
  {
    "text": "So let's say I give you a\nprompt, and the prompt is like, what's the weather like in\nWashington DC for something kind",
    "start": "1635660",
    "end": "1642195"
  },
  {
    "text": "of casual and fun? And I live in Washington, DC so I grapple with these weather questions constantly.",
    "start": "1642195",
    "end": "1648410"
  },
  {
    "text": "So what's the weather\nlike in Washington DC? So you got maybe three responses that your LLM comes up with.",
    "start": "1648410",
    "end": "1654140"
  },
  {
    "text": "Maybe one is \"The local weather\nin DC is sunny and humid at a temperature of 82\ndegrees Fahrenheit.\"",
    "start": "1654140",
    "end": "1661610"
  },
  {
    "text": "So that's one response. Another response is \"It's freaking hot.\" And then another response\nis, well, \"Relative",
    "start": "1661610",
    "end": "1669410"
  },
  {
    "text": "to Phoenix, Arizona, Washington\nDC is a cool 82 degrees.\"",
    "start": "1669410",
    "end": "1674410"
  },
  {
    "text": "So can you see how preferences\nto these change per person?",
    "start": "1674546",
    "end": "1679546"
  },
  {
    "text": "Like some people in some circumstances\nprefer different ones? I love the second one.",
    "start": "1680330",
    "end": "1686866"
  },
  {
    "text": "I find it hilarious. I find it entertaining. I find it honest. However, if I were building\na software application",
    "start": "1686866",
    "end": "1696404"
  },
  {
    "text": "for someone who's not me,\nI might pick number two as my preference.",
    "start": "1696404",
    "end": "1701870"
  },
  {
    "text": "Or if I were building an application for Phoenix, Arizona\nconsumers and who just wanted",
    "start": "1701870",
    "end": "1709309"
  },
  {
    "text": "to ask questions about\nlife relative to Phoenix, then I might go with this third ranking.",
    "start": "1709310",
    "end": "1715940"
  },
  {
    "text": "So in any case, rankings vary, but in your data labeling solution,",
    "start": "1715940",
    "end": "1723650"
  },
  {
    "text": "you want to give your human\nlabelers clear instructions for what their labeling\ncriteria should be.",
    "start": "1723650",
    "end": "1731030"
  },
  {
    "text": "And then you wanna give them a way to easily say, okay, I\nprefer this, and then that, and then that. And so you provide these rankings",
    "start": "1731030",
    "end": "1738860"
  },
  {
    "text": "for the different responses. And then again, you can use\nhumans, you can use AI's,",
    "start": "1738860",
    "end": "1745760"
  },
  {
    "text": "you can create really any\nkind of digital signal to create these rankings. You want some preference\nnumber to rank all",
    "start": "1745760",
    "end": "1753890"
  },
  {
    "text": "of these possible responses. So your dataset has multiple prompts, and then you take some\ntype of preference ranking,",
    "start": "1753890",
    "end": "1760970"
  },
  {
    "text": "and you rank those things and then these rankings become the label.",
    "start": "1760970",
    "end": "1767030"
  },
  {
    "text": "So when I train my reward model, this is my input and this is my label.",
    "start": "1767030",
    "end": "1773450"
  },
  {
    "text": "So I'm actually training\na regressive model, which will produce this numerical output.",
    "start": "1773450",
    "end": "1779240"
  },
  {
    "text": "So it's still a GPT, a\nlarge language model, it's just producing an output.",
    "start": "1779240",
    "end": "1783743"
  },
  {
    "text": "All right, so then how do\nwe train a reward model on AWS, build and train one?",
    "start": "1785870",
    "end": "1791389"
  },
  {
    "text": "So first is always, I'm\ngonna pop my data sets in S3, assuming I already\nhave the labeled data,",
    "start": "1791389",
    "end": "1797240"
  },
  {
    "text": "which I no doubt created\nusing SageMaker Ground Truth. So I store my data sets in S3, easy, done.",
    "start": "1797240",
    "end": "1805730"
  },
  {
    "text": "I'm gonna run a notebook such\nas on SageMaker Training and such as SageMaker Notebook\nInstances or Studio",
    "start": "1805730",
    "end": "1812480"
  },
  {
    "text": "to analyze my data, I'm\ngonna develop my scripts and then I'm gonna run\njobs for those things.",
    "start": "1812480",
    "end": "1818840"
  },
  {
    "text": "And so again, I'll develop\nmy training scripts from my notebooks,",
    "start": "1818840",
    "end": "1824120"
  },
  {
    "text": "and then I'll run some training instances. And this is just to\ntrain the reward model. This is just to produce the reward model",
    "start": "1824120",
    "end": "1831800"
  },
  {
    "text": "to give me a GPT based model\nthat again takes my text and returns the scaler output.",
    "start": "1831800",
    "end": "1838970"
  },
  {
    "text": "So that's how to build and\ntrain a reward model on AWS.",
    "start": "1838970",
    "end": "1842693"
  },
  {
    "text": "Now I'm going to use my reward\nmodel to train a new LLM.",
    "start": "1844160",
    "end": "1849160"
  },
  {
    "text": "So now I have this reward model, and I wanna use this reward model to fine tune this other LLM.",
    "start": "1851150",
    "end": "1858590"
  },
  {
    "text": "So I'm gonna do this in two stages. And the first is like my ahead\nof time pre-computing job.",
    "start": "1858590",
    "end": "1865280"
  },
  {
    "text": "So ahead of time, I'm gonna pre-compute my original model responses.",
    "start": "1865280",
    "end": "1871490"
  },
  {
    "text": "So I'm gonna run a CPU or a\nserverless job ahead of time.",
    "start": "1871490",
    "end": "1876490"
  },
  {
    "text": "And this serverless job is gonna go into my prompts data set. It's gonna run those against\nmy original pre-trained LLM.",
    "start": "1876800",
    "end": "1885980"
  },
  {
    "text": "And then I'll just store\nmy responses on Lustre. So the key here is that Lustre,\nmy distributed training,",
    "start": "1885980",
    "end": "1893540"
  },
  {
    "text": "or my distributed file\nsystem, has both my prompts",
    "start": "1893540",
    "end": "1898540"
  },
  {
    "text": "and the responses from my original LLM. From my top LLM, my top\nfoundation model, I'm storing",
    "start": "1899000",
    "end": "1905780"
  },
  {
    "text": "both of those things. So I have both of those\nstored and it might already be in my ranking dataset.",
    "start": "1905780",
    "end": "1911900"
  },
  {
    "text": "My ranking dataset might just have this, so maybe it's easy. And so yeah, so I'm gonna run\nthis whole thing and prep it",
    "start": "1911900",
    "end": "1919684"
  },
  {
    "text": "and store it on my Lustre\nvolume ahead of time. Then the fun part begins.",
    "start": "1919685",
    "end": "1926690"
  },
  {
    "text": "Then I'm gonna use what's\ncalled heterogeneous clusters. So heterogeneous clusters\nare a way in SageMaker",
    "start": "1926690",
    "end": "1935486"
  },
  {
    "text": "to run different types of instance groups. So I can run instance\ngroups with CPU and GPU.",
    "start": "1935486",
    "end": "1944470"
  },
  {
    "text": "I can run GPU and ANAPERNA\naccelerators with Cranium,",
    "start": "1944750",
    "end": "1949750"
  },
  {
    "text": "or I can run small GPU and large GPU. So in this case, I might run\nthe G5 series with four GPUs",
    "start": "1949910",
    "end": "1957760"
  },
  {
    "text": "to host my reward model. So again, because the\nreward model is smaller,",
    "start": "1959623",
    "end": "1966320"
  },
  {
    "text": "I might have one small G5 instance type that is holding my reward model.",
    "start": "1966320",
    "end": "1973343"
  },
  {
    "text": "And then, and remember, this\nthing is tiny, it's gonna sit on just four GPUs.",
    "start": "1974270",
    "end": "1980059"
  },
  {
    "text": "And then over here I've got big Bertha with all of my instances.",
    "start": "1980060",
    "end": "1986480"
  },
  {
    "text": "And this is holding my massive LLM. So this is the 20 million parameter model",
    "start": "1986480",
    "end": "1993113"
  },
  {
    "text": "that I'm trying to find tune using RLHF. So I've got my quite a few, I've got",
    "start": "1993113",
    "end": "2000730"
  },
  {
    "text": "my 32 accelerators right here. And then here's how this process works.",
    "start": "2000730",
    "end": "2005740"
  },
  {
    "text": "So for each step in this\nprocess, I'm gonna go read my prompts and my responses\nfrom the original LLM",
    "start": "2006670",
    "end": "2016440"
  },
  {
    "text": "and I'm just gonna read\nthose from Lustre right here",
    "start": "2016600",
    "end": "2019340"
  },
  {
    "text": "and then the fun begins. So, for each step for the LLM that I'm fine tuning,\nwe're gonna run the prompts",
    "start": "2023202",
    "end": "2032340"
  },
  {
    "text": "through this LLM, and we\ncompute the responses.",
    "start": "2032440",
    "end": "2037440"
  },
  {
    "text": "So in our original syntax, that was Y*. So we compute this Y*.",
    "start": "2037570",
    "end": "2042127"
  },
  {
    "text": "Yeah, so we take X and we compute Y* and then we're gonna send\nthat to the reward model.",
    "start": "2043810",
    "end": "2049869"
  },
  {
    "text": "So Y* goes to the reward model, which is sitting on these\nfour accelerators over here.",
    "start": "2049870",
    "end": "2056770"
  },
  {
    "text": "And I get this scaler reward signal. So I produce this reward signal,\nso that was that R theta.",
    "start": "2056770",
    "end": "2063940"
  },
  {
    "text": "So I get this R theta, then I'm gonna run that KLDivergence penalty\nfunction to compare those.",
    "start": "2063940",
    "end": "2072070"
  },
  {
    "text": "So then here in step three, I\ncompare Y* that I got in 0.1",
    "start": "2072070",
    "end": "2077070"
  },
  {
    "text": "with Y0 that I picked up over here. So I just compare these two\nand then I add them together.",
    "start": "2078790",
    "end": "2086919"
  },
  {
    "text": "So then in step four, I\nadd, I say I take R theta,",
    "start": "2086920",
    "end": "2091919"
  },
  {
    "text": "remember which came outta\nthe reward model, minus my epsilon times this penalty function.",
    "start": "2092014",
    "end": "2100599"
  },
  {
    "text": "And then I use that to update my LLM. And I might add some gradients\nfrom the pre-training",
    "start": "2100600",
    "end": "2108970"
  },
  {
    "text": "as they suggested and instruct GPT or just pop this whole thing\nin my pre-training process,",
    "start": "2108970",
    "end": "2115930"
  },
  {
    "text": "which to me seems like the\nmore logical thing to do. So in any case, a couple pro tips here.",
    "start": "2115930",
    "end": "2121270"
  },
  {
    "text": "I'm going to use smaller\naccelerators for the reward model 'cause this thing is tiny. I'm gonna use larger\naccelerators for the LLM",
    "start": "2121270",
    "end": "2128650"
  },
  {
    "text": "that I'm fine tuning. So larger accelerators and more of them because it's a bigger model.",
    "start": "2128650",
    "end": "2134332"
  },
  {
    "text": "And then definitely when\nyou keep all of them in the same cluster using\nheterogeneous clusters,",
    "start": "2134332",
    "end": "2142134"
  },
  {
    "text": "your runtime is so much faster. If they're sitting on all\ndifferent parts of the cloud",
    "start": "2142134",
    "end": "2148295"
  },
  {
    "text": "or if they're sitting in different regions or different accounts, or if\nthey're not even in the cloud",
    "start": "2148295",
    "end": "2154210"
  },
  {
    "text": "and you have to hop on another\nbandwidth, it's just gonna be so expensive to train this\nmodel because you want the steps",
    "start": "2154210",
    "end": "2161500"
  },
  {
    "text": "to be really fast, otherwise\nit'll be totally inefficient. So yeah, strongly recommend\nusing heterogeneous clusters",
    "start": "2161500",
    "end": "2169116"
  },
  {
    "text": "to use reinforcement\nlearning to update your LLM.",
    "start": "2169116",
    "end": "2173773"
  },
  {
    "text": "All right, let's check out this demo. So you've heard about RLHF. You think it's cool.",
    "start": "2175330",
    "end": "2180910"
  },
  {
    "text": "You've learned about heterogeneous\nclusters and all sorts of related topics. In this demo, we are going to do our best",
    "start": "2180910",
    "end": "2189549"
  },
  {
    "text": "to implement this thing locally. RLHF, as you learned,\nis extremely complex.",
    "start": "2189550",
    "end": "2195190"
  },
  {
    "text": "Lots of moving pieces here. And let's try and break it down.",
    "start": "2195190",
    "end": "2200530"
  },
  {
    "text": "So same as last time,\nactually, we're in a new repo. This is the SageMaker Distributed\nTraining Workshop Repository.",
    "start": "2200530",
    "end": "2208720"
  },
  {
    "text": "And you'll notice this is number nine\nhere, RLHF, RLHF locally.",
    "start": "2208720",
    "end": "2213790"
  },
  {
    "text": "Feel free to grab that\nQR code for the notebook. Alternatively, you're welcome\nto just use the short URL.",
    "start": "2213790",
    "end": "2221773"
  },
  {
    "text": "All right, so I do indeed have this up and let's see what we can make of this.",
    "start": "2225940",
    "end": "2231220"
  },
  {
    "text": "So same as last time, I am in a notebook. This is using SageMaker studio.",
    "start": "2231220",
    "end": "2237765"
  },
  {
    "text": "Major kudos to my friend EK\nhere, Senior Applied Scientist at Amazon for building this out.",
    "start": "2237766",
    "end": "2245380"
  },
  {
    "text": "This thing is awesome. And so it's nice because this notebook is\ncompletely self-contained,",
    "start": "2245380",
    "end": "2252970"
  },
  {
    "text": "so there aren't really many other files that you need in this notebook.",
    "start": "2252970",
    "end": "2258640"
  },
  {
    "text": "Really everything is actually\nimplemented in one notebook. So it's very, very tightly self-contained.",
    "start": "2258640",
    "end": "2265900"
  },
  {
    "text": "And you'll find that it\nhas a lot of PyTorch.",
    "start": "2265900",
    "end": "2270900"
  },
  {
    "text": "There is a absolutely massive amount of custom PyTorch objects\nand custom PyTorch code",
    "start": "2270910",
    "end": "2276282"
  },
  {
    "text": "that we're gonna use to get this done. Now, I really want to see this type",
    "start": "2276282",
    "end": "2283840"
  },
  {
    "text": "of technology become easier and become more simplified\nand more abstracted.",
    "start": "2283840",
    "end": "2289240"
  },
  {
    "text": "So as time goes by, you builders\nout there, help me think",
    "start": "2289240",
    "end": "2294240"
  },
  {
    "text": "through how to make this easier. It's really insanely\ncomplicated right now,",
    "start": "2295960",
    "end": "2301210"
  },
  {
    "text": "and it's a bottleneck for a lot of people because many obviously\nbelieve that RLHF can give you",
    "start": "2301210",
    "end": "2308049"
  },
  {
    "text": "a good model, but very few people have really state-of-the-art RLHF stacks.",
    "start": "2308050",
    "end": "2314980"
  },
  {
    "text": "I honestly, I think that's\nprobably the most important piece to the puzzle. But in any case, let's check this out.",
    "start": "2314980",
    "end": "2320800"
  },
  {
    "text": "So we're gonna install our requirements. Got a long list here for\nyou of all the things that you want, including xformers.",
    "start": "2320800",
    "end": "2327580"
  },
  {
    "text": "We'll pip install that thing. Then we're gonna go to our imports. These are all normal tqdm,\ntorchtyping, transformers,",
    "start": "2327580",
    "end": "2336880"
  },
  {
    "text": "data collator with padding apparently, and then the load data sets. And then lots and lots of PyTorch.",
    "start": "2336880",
    "end": "2344412"
  },
  {
    "text": "And then we're gonna define\nthe model parameters. So these are the training configurations,",
    "start": "2347170",
    "end": "2352960"
  },
  {
    "text": "the seed, 1024 sequence\nlength, total number of epochs, steps, your batch\nsize, things like this.",
    "start": "2352960",
    "end": "2360849"
  },
  {
    "text": "I ran this on a G4.X. Yeah, so this has one GPU,\nand it was still slow.",
    "start": "2360850",
    "end": "2370270"
  },
  {
    "text": "It was a good 10, 20 minutes\njust watching this baby run. So if you want to upgrade this thing,",
    "start": "2370270",
    "end": "2377225"
  },
  {
    "text": "it'll run faster for you. Cuda, CPU, et cetera.",
    "start": "2377226",
    "end": "2384402"
  },
  {
    "text": "Hmm, maybe that's why\nit's running so slowly because it wasn't even running on the GPU. That'll do it.",
    "start": "2387070",
    "end": "2393670"
  },
  {
    "text": "Alright, and then we're gonna\nuse Louis's GPT-2, co-author",
    "start": "2393670",
    "end": "2398670"
  },
  {
    "text": "of the book, \"Natural Language Processing\nwith Transformers\". And then we've got the Adam\noptimizer, weight decay,",
    "start": "2399790",
    "end": "2407830"
  },
  {
    "text": "scheduler, this mysterious\nwhitening method that I'm not sure what this is.",
    "start": "2407830",
    "end": "2413620"
  },
  {
    "text": "PPO and all sorts of other\nhyper parameters here. So anyway, we'll load those",
    "start": "2413620",
    "end": "2419560"
  },
  {
    "text": "into this DictConfig, make\nthem nicely accessible and set the seeds.",
    "start": "2419560",
    "end": "2424843"
  },
  {
    "text": "And again, remember we're\nsetting the seeds in the notebook because right now all of\nthis is literally training",
    "start": "2425950",
    "end": "2432010"
  },
  {
    "text": "on this G4. So now we have many PyTorch\nobjects we're gonna define.",
    "start": "2432010",
    "end": "2441490"
  },
  {
    "text": "We have this prompt pipeline,\nand then that PPO RL Element, this PPO RL Batch, some rollout storage",
    "start": "2441490",
    "end": "2449920"
  },
  {
    "text": "that includes a data loader. So your prompt pipeline as a tokenizer.",
    "start": "2449920",
    "end": "2454422"
  },
  {
    "text": "And then we set the prompts\nand then we set this input ID.",
    "start": "2455854",
    "end": "2460313"
  },
  {
    "text": "Then we're gonna use these decorators here and call this a PPO RL Element.",
    "start": "2463960",
    "end": "2469152"
  },
  {
    "text": "And then RL batch, this\nis taking the, hmm, this is setting the query tensor.",
    "start": "2470740",
    "end": "2476083"
  },
  {
    "text": "And then this is setting the log prompts, values and rewards.",
    "start": "2479680",
    "end": "2483942"
  },
  {
    "text": "And then we're setting,\nyeah, similar terms,",
    "start": "2485860",
    "end": "2490860"
  },
  {
    "text": "but for a slightly different object. Then we have this PPO rollout storage.",
    "start": "2491748",
    "end": "2498672"
  },
  {
    "text": "I wasn't exaggerating,\nit's a lot of PyTorch. We're gonna create this loader inside of",
    "start": "2501820",
    "end": "2507673"
  },
  {
    "text": "which we are defining\nanother function, oh joy. We'll return it.",
    "start": "2508570",
    "end": "2513850"
  },
  {
    "text": "And then we have yet more\nPyTorch functions to define. So there's this whiten process, gae takes",
    "start": "2513850",
    "end": "2521170"
  },
  {
    "text": "the values and rewards. Then we have this PPO loss.",
    "start": "2522640",
    "end": "2527863"
  },
  {
    "text": "It takes the values. Loss one, loss two, then the delta.",
    "start": "2529090",
    "end": "2537733"
  },
  {
    "text": "Ratio, and that is your net loss. And then the loss function,\nwhich invokes that.",
    "start": "2542530",
    "end": "2549609"
  },
  {
    "text": "Model that device, query tensors, response tensors, all right.",
    "start": "2549610",
    "end": "2558580"
  },
  {
    "text": "And then the PPO loss that\ntakes all these things and then returns it out.",
    "start": "2558580",
    "end": "2563890"
  },
  {
    "text": "And then your actor, which operates at a slightly higher level, takes the prompts, eats them in chunks.",
    "start": "2563890",
    "end": "2571183"
  },
  {
    "text": "And then we make this experience.",
    "start": "2572078",
    "end": "2574543"
  },
  {
    "text": "I'm so glad EK wrote this thing. (laughs) Glad because it would take\nme a lot longer to do that.",
    "start": "2578890",
    "end": "2585520"
  },
  {
    "text": "All right, generate, get\nmodel inputs, log probs.",
    "start": "2586420",
    "end": "2591420"
  },
  {
    "text": "Goodness gracious, agent,\ngenerate, transformer outputs",
    "start": "2593470",
    "end": "2598470"
  },
  {
    "text": "and so on. All right, so now we have all of those very verbose things defined.",
    "start": "2599440",
    "end": "2607510"
  },
  {
    "text": "Now we're going to define the pipeline that invokes those things and uses them.",
    "start": "2607510",
    "end": "2613030"
  },
  {
    "text": "So this is gonna use a\ndistilbert imdb still from Lewis.",
    "start": "2613030",
    "end": "2617923"
  },
  {
    "text": "We're gonna load it through\nthe load data set function. So we got the imdb with\nour train test splits.",
    "start": "2622120",
    "end": "2629319"
  },
  {
    "text": "Pick out the prompts from those, get your tokenizer, download\nthe prompt pipeline.",
    "start": "2629320",
    "end": "2636128"
  },
  {
    "text": "Then we're gonna load the model. So let's just see what this model is here.",
    "start": "2636128",
    "end": "2641590"
  },
  {
    "text": "So the model path, which points to that model we downloaded above.",
    "start": "2641590",
    "end": "2648670"
  },
  {
    "text": "And then we have this config\nnumber of layers unfrozen,",
    "start": "2648670",
    "end": "2653670"
  },
  {
    "text": "and then that we set to the device. So those are the trainable layers.",
    "start": "2654730",
    "end": "2659880"
  },
  {
    "text": "Sounds about right. Tokenizer, we generate the output IDs.",
    "start": "2660730",
    "end": "2668026"
  },
  {
    "text": "Okay, so here's the before. Essentially, we're taking\nsome of the prompts",
    "start": "2672520",
    "end": "2681210"
  },
  {
    "text": "and then we're gonna send\nthose prompts to the model. And the model's generating\nmovie reviews, actually,",
    "start": "2682630",
    "end": "2690990"
  },
  {
    "text": "just to use some basic data. So it's generating movie reviews. And then we're gonna put\nthis into a reward function.",
    "start": "2691310",
    "end": "2699369"
  },
  {
    "text": "So the reward function\ntakes the generated text, which here is these three sentences.",
    "start": "2699370",
    "end": "2706573"
  },
  {
    "text": "So this is the first review, and then this is your second review.",
    "start": "2707470",
    "end": "2712900"
  },
  {
    "text": "And then this is the third one. And the goal here, so imagine",
    "start": "2712900",
    "end": "2718900"
  },
  {
    "text": "that this is the response coming out of the GPT model that you\nwant to fine tune or you want",
    "start": "2718900",
    "end": "2725680"
  },
  {
    "text": "to update and optimize. The responses are okay,\nbut they're not that good. And so you want to use\nyour reward modeling stack",
    "start": "2725680",
    "end": "2734380"
  },
  {
    "text": "to make them better. This is how we're gonna do that. So first we need a way of saying how good",
    "start": "2734380",
    "end": "2742260"
  },
  {
    "text": "and how bad these reviews are. And that's what the reward function does. Actually in this notebook\nright now it's not",
    "start": "2743350",
    "end": "2750700"
  },
  {
    "text": "even training a reward model, it's just returning these scalers. So we're still working on\nthe reward model training.",
    "start": "2750700",
    "end": "2758290"
  },
  {
    "text": "But in any case, it takes this generated text, one, two,\nthree and then it generates",
    "start": "2758290",
    "end": "2764950"
  },
  {
    "text": "or it outputs this number. Higher is better. And so this is basically saying\nhow confident the model is",
    "start": "2764950",
    "end": "2773970"
  },
  {
    "text": "that this really is a\nlegitimate movie review. And so it feels very\nconfident in the first one,",
    "start": "2776110",
    "end": "2784330"
  },
  {
    "text": "and notably not just\nbeing a real movie review, but being a positive review actually. So in this first one, clearly positive.",
    "start": "2784330",
    "end": "2792520"
  },
  {
    "text": "Second one, clearly not positive.",
    "start": "2792520",
    "end": "2796603"
  },
  {
    "text": "And then the third one\nis positive as well. Alright, so now that we have\nthat, we're gonna put this",
    "start": "2798100",
    "end": "2806859"
  },
  {
    "text": "into an overall training loop. And then we're gonna run this loop. So it takes the prompt\npipeline, which is this.",
    "start": "2806860",
    "end": "2814090"
  },
  {
    "text": "It takes an actor, which we built earlier. So the actor takes a\nprompt pipeline, actually.",
    "start": "2814090",
    "end": "2820839"
  },
  {
    "text": "The actor takes a tokenizer\nand then your chunk size.",
    "start": "2820840",
    "end": "2825223"
  },
  {
    "text": "And then the storage here\nand the PPO rollout storage. You have this atom\noptimizer, then a scheduler,",
    "start": "2826390",
    "end": "2835183"
  },
  {
    "text": "and then the updates per batch. And so we're gonna take 400 steps.",
    "start": "2835184",
    "end": "2839533"
  },
  {
    "text": "And then here's our training loop. So we're walking through\nthe number of epochs,",
    "start": "2841000",
    "end": "2849039"
  },
  {
    "text": "and then essentially for\neach point in the number of epochs, the actor makes an experience.",
    "start": "2849040",
    "end": "2856840"
  },
  {
    "text": "And so in RL that means the\nactor takes an action basically,",
    "start": "2856840",
    "end": "2861840"
  },
  {
    "text": "and then should gets a\nreward from that action. We're gonna try and replicate that.",
    "start": "2862271",
    "end": "2869110"
  },
  {
    "text": "So the actor makes some\nsort of experience, which creates this rollouts concept",
    "start": "2869110",
    "end": "2876190"
  },
  {
    "text": "and then stores that in the history. So this is how the PPO process\ntracks where the actor is",
    "start": "2876190",
    "end": "2883830"
  },
  {
    "text": "in the environment is with the store. So store.clearhistory,\nand then store.push,",
    "start": "2884591",
    "end": "2890727"
  },
  {
    "text": "which takes the rollouts. And then actually here we\ncreate the data loader.",
    "start": "2890727",
    "end": "2897400"
  },
  {
    "text": "So then based on the, yeah, so we create",
    "start": "2897400",
    "end": "2902137"
  },
  {
    "text": "this data loader evidently, and then we're gonna walk\nthrough the data loader.",
    "start": "2903430",
    "end": "2909162"
  },
  {
    "text": "And for each of those, we'll take a step.",
    "start": "2910300",
    "end": "2915300"
  },
  {
    "text": "And so if we're taking those steps, we get this batch, we pass the batch\nthrough this loss function,",
    "start": "2917080",
    "end": "2924690"
  },
  {
    "text": "which is gonna invoke the model, which returns this loss and the reward.",
    "start": "2924880",
    "end": "2931210"
  },
  {
    "text": "And then we say loss.backward. We take a step on the optimizer, zero out",
    "start": "2931210",
    "end": "2937000"
  },
  {
    "text": "the gradients, take a\nstep on the scheduler, and then update this description.",
    "start": "2937000",
    "end": "2941203"
  },
  {
    "text": "And so this runs all the way through, it\ntakes 400 steps, hits 100%.",
    "start": "2942490",
    "end": "2949650"
  },
  {
    "text": "And then in the last part,\nwe're gonna use this updated LLM to generate new text.",
    "start": "2950007",
    "end": "2956740"
  },
  {
    "text": "And then we're gonna evaluate\nthat with the reward model. And so our tokenizer takes\nthe, actually these were",
    "start": "2956740",
    "end": "2964920"
  },
  {
    "text": "the same three prompts. So my feeling about the movie, prompt one. This is prompt two,",
    "start": "2965440",
    "end": "2972610"
  },
  {
    "text": "and I can tell with\ncertainty, prompt three. So we take the same\nthree prompts, tokenize",
    "start": "2972610",
    "end": "2978040"
  },
  {
    "text": "those things, pop them\ninto the model, generate the text, run that back\nthrough the tokenizer.",
    "start": "2978040",
    "end": "2987090"
  },
  {
    "text": "And then once it's back in\nnatural language, we put that into the reward function.",
    "start": "2987730",
    "end": "2992980"
  },
  {
    "text": "And then we get this part. So this is the full review with a 98%.",
    "start": "2992980",
    "end": "3000590"
  },
  {
    "text": "And then the second one,\n99%, the third one, 98%.",
    "start": "3001767",
    "end": "3006767"
  },
  {
    "text": "So much higher than we saw previously. And that is because of this\nvery strong reward signal.",
    "start": "3007350",
    "end": "3015053"
  },
  {
    "text": "So remember the model is\njust really optimizing for this reward signal.",
    "start": "3015053",
    "end": "3021429"
  },
  {
    "text": "And then on top of it,\nwe have the RLHF process,",
    "start": "3021429",
    "end": "3026429"
  },
  {
    "text": "sort of tracking the\ndifference between the text that's generated from the\nmodel we're fine tuning",
    "start": "3029250",
    "end": "3037289"
  },
  {
    "text": "with the original model to mitigate the reward\nhacking too aggressively.",
    "start": "3037290",
    "end": "3042840"
  },
  {
    "text": "So you still have some reward\nhacking, which is good, but mostly you want sort\nof quality learning.",
    "start": "3042840",
    "end": "3049683"
  },
  {
    "text": "All right, so that's RLHF in a notebook.",
    "start": "3051480",
    "end": "3056480"
  },
  {
    "text": "And actually I mentioned\non the GitHub that again, this is sort of insanely complicated.",
    "start": "3057000",
    "end": "3064230"
  },
  {
    "text": "And so we're working on basically\nother examples just here in the GitHub to make this easier for you.",
    "start": "3064230",
    "end": "3072297"
  },
  {
    "text": "And so that's gonna be\nin the work in progress, a little directory, containerizing\nthis, simplifying it,",
    "start": "3072297",
    "end": "3079115"
  },
  {
    "text": "making it easier for you to poke at things and generally train your models.",
    "start": "3079115",
    "end": "3085290"
  },
  {
    "text": "And so with that, I hope\nyou enjoyed this video. In the next one, we're gonna learn how",
    "start": "3085290",
    "end": "3090960"
  },
  {
    "text": "to deploy large models\non AWS and on SageMaker.",
    "start": "3090960",
    "end": "3095960"
  },
  {
    "text": "So hope you enjoyed this. And again, EK, massive shout out. Thank you for this awesome notebook.",
    "start": "3097110",
    "end": "3103647"
  },
  {
    "text": "It's truly, truly a labor of love. So thanks folks. See you next time.",
    "start": "3103647",
    "end": "3108573"
  }
]