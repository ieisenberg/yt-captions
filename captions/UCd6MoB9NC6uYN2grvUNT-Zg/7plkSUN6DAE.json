[
  {
    "text": "good morning welcome welcome to a first session of the day very exciting thank",
    "start": "30",
    "end": "6480"
  },
  {
    "text": "you all for coming if you're here to find out what AWS lambda and danwoah DB can do in your",
    "start": "6480",
    "end": "14070"
  },
  {
    "text": "service applications you come to the right place and if you're here to find out how",
    "start": "14070",
    "end": "19609"
  },
  {
    "text": "they're using serverless and AWS lambda and dynamo DB at Capital One you've come",
    "start": "19609",
    "end": "26189"
  },
  {
    "text": "to the right place again my name is Ed mooch and I'm a Solutions Architect AWS I'm really excited to have strain Apollo",
    "start": "26189",
    "end": "33360"
  },
  {
    "text": "party with me who is the vice president of consumer bank engineering at Capital One",
    "start": "33360",
    "end": "39920"
  },
  {
    "text": "my biggest challenge for this session was shrinking the content down to make it fit in the time that we have so",
    "start": "41899",
    "end": "48809"
  },
  {
    "text": "here's what I have so first you'll want to hear what dynamo DB is really good at",
    "start": "48809",
    "end": "54329"
  },
  {
    "text": "then we'll get into how to do things with AWS lambda and dynamo DB we'll go",
    "start": "54329",
    "end": "60840"
  },
  {
    "text": "into how to implement transactional behavior as well as enable concurrent",
    "start": "60840",
    "end": "66750"
  },
  {
    "text": "querying in service micro services and then we'll talk about ingesting high",
    "start": "66750",
    "end": "72960"
  },
  {
    "text": "volume time series data with Kinesis AWS lambda and dynamo DB after that Trini",
    "start": "72960",
    "end": "79110"
  },
  {
    "text": "will take over to talk about serve elastic Capital One and what he and his team have been doing with AWS lambda and",
    "start": "79110",
    "end": "86640"
  },
  {
    "text": "dynamo DB how they're creating new services for their customers and random reliably at scale AWS service platform",
    "start": "86640",
    "end": "95820"
  },
  {
    "text": "is much more than just function as a service it includes services and capabilities that enable you to build",
    "start": "95820",
    "end": "103860"
  },
  {
    "text": "sophisticated applications and random at scale dynamo DB is one of these services",
    "start": "103860",
    "end": "110100"
  },
  {
    "text": "and in fact it's the only database servicing the AWS service platform so",
    "start": "110100",
    "end": "115229"
  },
  {
    "text": "these are the characteristics that your applications can benefit in your service",
    "start": "115229",
    "end": "120770"
  },
  {
    "text": "your service applications can benefit from so let's see how dynamo DB delivers",
    "start": "120770",
    "end": "126210"
  },
  {
    "text": "in these no server management dynamo DB is a fully managed service there is no servers or clusters to manage flexible",
    "start": "126210",
    "end": "133349"
  },
  {
    "text": "scaling DynamoDB especially now with auto-scaling a new feature that was",
    "start": "133349",
    "end": "138420"
  },
  {
    "text": "released earlier this year provides an easy way to scale your capacity for for",
    "start": "138420",
    "end": "145830"
  },
  {
    "text": "the load that it needs and high availability and durability are built in",
    "start": "145830",
    "end": "150840"
  },
  {
    "text": "and in fact dynamodb is a zero downtime service it's designed to run 24/7/365",
    "start": "150840",
    "end": "158310"
  },
  {
    "text": "and finally when it comes to idle capacity that's where DynamoDB still has some work to do however with some of the",
    "start": "158310",
    "end": "165150"
  },
  {
    "text": "new features such as Auto scaling that gap is really being closed as well so",
    "start": "165150",
    "end": "171660"
  },
  {
    "text": "are the scaling in addition to increasing the capacity to meet your load also decreases the capacity when",
    "start": "171660",
    "end": "178500"
  },
  {
    "text": "your load goes down which helps reduce that idle capacity so I'm sure you",
    "start": "178500",
    "end": "184980"
  },
  {
    "text": "already know what dynamodb is but I want to share one example of what it does as",
    "start": "184980",
    "end": "190320"
  },
  {
    "text": "that's a great way to describe what it really is Prime Day this year dynamodb",
    "start": "190320",
    "end": "196950"
  },
  {
    "text": "served over three trillion requests from all amazon.com sites alexa and amazon",
    "start": "196950",
    "end": "204870"
  },
  {
    "text": "fulfillment centers and all this were the same consistent performance and high availability so talk about a",
    "start": "204870",
    "end": "210989"
  },
  {
    "text": "mission-critical service so dynamodb is designed to be used as an operational",
    "start": "210989",
    "end": "218190"
  },
  {
    "text": "database in OLTP use cases where you know access patterns and you can design",
    "start": "218190",
    "end": "223890"
  },
  {
    "text": "your data model to support those access patterns and that's really important to remember about dynamodb also dynamodb is",
    "start": "223890",
    "end": "231660"
  },
  {
    "text": "designed for applications that need scale performance high availability and",
    "start": "231660",
    "end": "237510"
  },
  {
    "text": "reliability in other words DynamoDB is designed for mission-critical applications so these are some of the",
    "start": "237510",
    "end": "244050"
  },
  {
    "text": "things that your applications can benefit from just by using dynamodb some",
    "start": "244050",
    "end": "249420"
  },
  {
    "text": "of the common use cases here include shopping cart and sessions two of the",
    "start": "249420",
    "end": "255239"
  },
  {
    "text": "original use cases that required at Amazon that required the kind of scale and performance that the traditional",
    "start": "255239",
    "end": "262530"
  },
  {
    "text": "database at the time just could not deliver so let's take a look at building blocks that we use for these",
    "start": "262530",
    "end": "270070"
  },
  {
    "text": "use cases this is what I call a basic building block for micro service micro",
    "start": "270070",
    "end": "276010"
  },
  {
    "text": "services and and it's a pattern that's used in many use cases including mobile",
    "start": "276010",
    "end": "282370"
  },
  {
    "text": "backend web applications or e-commerce applications so DynamoDB is low latency",
    "start": "282370",
    "end": "290500"
  },
  {
    "text": "both for reads and writes so in single-digit milliseconds as well as the fact that it's API driven so",
    "start": "290500",
    "end": "296949"
  },
  {
    "text": "there is no need for object relational mapping make it well suited for use in",
    "start": "296949",
    "end": "302260"
  },
  {
    "text": "AWS lambda functions so in addition to the ability to store your data",
    "start": "302260",
    "end": "308310"
  },
  {
    "text": "operational data in DynamoDB and access it quickly you can stream those data to",
    "start": "308310",
    "end": "318160"
  },
  {
    "text": "consumers that might then do some post-processing downstream processing or",
    "start": "318160",
    "end": "323560"
  },
  {
    "text": "enable different materialized views for various kinds of queries and this is",
    "start": "323560",
    "end": "330669"
  },
  {
    "text": "enabled by DynamoDB streams which is basically a feature that's a transaction",
    "start": "330669",
    "end": "337419"
  },
  {
    "text": "log tailing mechanism that reliably delivers all updates to your data",
    "start": "337419",
    "end": "343060"
  },
  {
    "text": "downstream now those updates when you use dynamodb streams with",
    "start": "343060",
    "end": "348520"
  },
  {
    "text": "lambda and they're really well integrated then just show up in your",
    "start": "348520",
    "end": "353580"
  },
  {
    "text": "lambda function invocation as event that you don't do whatever you need to do with and so this enables us to segregate",
    "start": "353580",
    "end": "362349"
  },
  {
    "text": "our operational database and still be able to stream data and enable querying",
    "start": "362349",
    "end": "369270"
  },
  {
    "text": "basically for whatever requirements that we might have on the querying side and this enables us to enable this query",
    "start": "369270",
    "end": "376840"
  },
  {
    "text": "without ever having to change the data model in our operational database so one",
    "start": "376840",
    "end": "382680"
  },
  {
    "text": "core building block API gateway London DynamoDB and then the second building",
    "start": "382680",
    "end": "389500"
  },
  {
    "text": "block that I want point out is the building block for reliable event delivery which which is made up of",
    "start": "389500",
    "end": "396130"
  },
  {
    "text": "dynamodb streams and so let's take a quick look under the",
    "start": "396130",
    "end": "402160"
  },
  {
    "text": "hood of dynamodb streams and see how it works with dynamo on one side and then",
    "start": "402160",
    "end": "409060"
  },
  {
    "text": "your lambda functions on the other strings consists of shards and shards",
    "start": "409060",
    "end": "414070"
  },
  {
    "text": "are there to enable scaling however streams are fully managed and so unlike",
    "start": "414070",
    "end": "419920"
  },
  {
    "text": "with Kinesis streams there is nothing for you to do with those shards so the",
    "start": "419920",
    "end": "425770"
  },
  {
    "text": "way it works is DynamoDB table as you probably know stores data in partitions",
    "start": "425770",
    "end": "431410"
  },
  {
    "text": "for each active partition there is an active shard in the DynamoDB stream",
    "start": "431410",
    "end": "436570"
  },
  {
    "text": "that's attached for that table and then on the other side of that for each shard there's an instance of your AWS lambda",
    "start": "436570",
    "end": "445390"
  },
  {
    "text": "function so DynamoDB streams takes care",
    "start": "445390",
    "end": "450550"
  },
  {
    "text": "of managing your data in the stream and providing the data exactly once in a",
    "start": "450550",
    "end": "455740"
  },
  {
    "text": "strictly ordered fashion based on on the item on the item key the stream is",
    "start": "455740",
    "end": "462190"
  },
  {
    "text": "durable and it's scalable so as you add partitions to your or as dynamodb rather",
    "start": "462190",
    "end": "467770"
  },
  {
    "text": "adds partitions to your DynamoDB table the string that's attached to the table",
    "start": "467770",
    "end": "474040"
  },
  {
    "text": "also adds the shards to be able to also scale accordingly the data is retained",
    "start": "474040",
    "end": "480940"
  },
  {
    "text": "for 24 hours and then it's deleted without you having to do anything about it and since dynamodb streams is an event",
    "start": "480940",
    "end": "488530"
  },
  {
    "text": "source for AWS lambda you simply consume events from the stream by basically",
    "start": "488530",
    "end": "494170"
  },
  {
    "text": "taking the event passed into your lambda handler function now the way this works",
    "start": "494170",
    "end": "499470"
  },
  {
    "text": "lambda pulls the stream up to four times a second if it needs to and then",
    "start": "499470",
    "end": "505330"
  },
  {
    "text": "synchronously invokes your function now this is important to note because it will keep retrying until it succeeds and",
    "start": "505330",
    "end": "513159"
  },
  {
    "text": "this is related to the guarantee of ordered strictly by key order delivery so you're guaranteed that your data will",
    "start": "513160",
    "end": "520810"
  },
  {
    "text": "be ordered will be delivered in order and that lambda will execute on it in",
    "start": "520810",
    "end": "526600"
  },
  {
    "text": "the same order so dynamodb streams in lambda provides a reliable at",
    "start": "526600",
    "end": "532270"
  },
  {
    "text": "least once event delivery so you say this is great but DynamoDB is no SQL",
    "start": "532270",
    "end": "541000"
  },
  {
    "text": "database I need SQL when we looked at how we were using our existing databases at Amazon",
    "start": "541000",
    "end": "548620"
  },
  {
    "text": "we found out that in many cases we weren't using relational databases for",
    "start": "548620",
    "end": "553840"
  },
  {
    "text": "the relational capabilities the access pattern a lot of times was key-value based now that said there are plenty of",
    "start": "553840",
    "end": "560740"
  },
  {
    "text": "use cases where SQL is called for however in many LTP applicate",
    "start": "560740",
    "end": "565900"
  },
  {
    "text": "applications a non relational database is more than adequate in fact it can",
    "start": "565900",
    "end": "571930"
  },
  {
    "text": "deliver better performance and scale so let's take a look at an example where we",
    "start": "571930",
    "end": "579400"
  },
  {
    "text": "use micro services for an e-commerce application we'll first compare",
    "start": "579400",
    "end": "585150"
  },
  {
    "text": "relational versus non relational data model and then we'll look into",
    "start": "585150",
    "end": "590520"
  },
  {
    "text": "implementing transactional behavior as well as enabling querying in distributed",
    "start": "590520",
    "end": "595930"
  },
  {
    "text": "micro-services architecture so let's consider three micro services and since",
    "start": "595930",
    "end": "602590"
  },
  {
    "text": "these are micro services each one gets its own database and of course each database is bound by the same business",
    "start": "602590",
    "end": "610270"
  },
  {
    "text": "context that the micro service is bound by and so for is for example in the case",
    "start": "610270",
    "end": "615460"
  },
  {
    "text": "of shopping cart service our data model might simply consist of the cart and cart item and if you you're if we're",
    "start": "615460",
    "end": "623290"
  },
  {
    "text": "using our service platform at AWS to do all this we'll be using API gateway AWS",
    "start": "623290",
    "end": "630880"
  },
  {
    "text": "lambda and Amazon DynamoDB let's take a",
    "start": "630880",
    "end": "636190"
  },
  {
    "text": "look at how we can model our shopping cart and compare what that looks like in",
    "start": "636190",
    "end": "641650"
  },
  {
    "text": "a relational versus non relational database so in a relational database we're likely to have a couple of tables",
    "start": "641650",
    "end": "648040"
  },
  {
    "text": "one for the cart one for the cart items and then update would be a transaction",
    "start": "648040",
    "end": "654100"
  },
  {
    "text": "that includes both tables by contrast in a non relational database we can put all",
    "start": "654100",
    "end": "660700"
  },
  {
    "text": "the data for the shopping cart in a single item in a single table non-relational database",
    "start": "660700",
    "end": "666430"
  },
  {
    "text": "in this case DynamoDB have a flexible schema so the schema is still there the",
    "start": "666430",
    "end": "672160"
  },
  {
    "text": "relations are still there too but we use a different approach we use aggregate",
    "start": "672160",
    "end": "678100"
  },
  {
    "text": "orientation to store data in a non relational database what that means is that we're storing our data in",
    "start": "678100",
    "end": "684700"
  },
  {
    "text": "aggregates and in this particular case the aggregate is the shopping cart aggregate is a collection of data or",
    "start": "684700",
    "end": "692170"
  },
  {
    "text": "objects that are used and therefore stored together and that really allows",
    "start": "692170",
    "end": "697600"
  },
  {
    "text": "us to simply have really good performance both for reads and writes",
    "start": "697600",
    "end": "702910"
  },
  {
    "text": "and also implement atomic updates to our shopping cart so on one side normalized",
    "start": "702910",
    "end": "710080"
  },
  {
    "text": "schema where we use what we use in a relational database and then on the other the normalized schema we use",
    "start": "710080",
    "end": "716529"
  },
  {
    "text": "aggregates to store data let's see how we go about updating a cart in DynamoDB",
    "start": "716529",
    "end": "723580"
  },
  {
    "text": "we use an approach that's based on optimistic concurrency control which",
    "start": "723580",
    "end": "729610"
  },
  {
    "text": "means we first read data we modified the data and then update the data in the",
    "start": "729610",
    "end": "735550"
  },
  {
    "text": "database but only if it has not been updated since we read it and the way we",
    "start": "735550",
    "end": "740709"
  },
  {
    "text": "ensure that in dynamodb is by using conditional expression in either our put",
    "start": "740709",
    "end": "746529"
  },
  {
    "text": "item or update item operation so in this case the conditional expression we're",
    "start": "746529",
    "end": "753010"
  },
  {
    "text": "using for this example takes the last update timestamp and so we'll compare",
    "start": "753010",
    "end": "758380"
  },
  {
    "text": "the timestamp that we that we read with the timestamp at the time of update and",
    "start": "758380",
    "end": "764230"
  },
  {
    "text": "if they match it means that nothing else updated the data in the meantime this",
    "start": "764230",
    "end": "769360"
  },
  {
    "text": "allows us to ensure data consistency using using basically conditional put or",
    "start": "769360",
    "end": "777970"
  },
  {
    "text": "conditional update in dynamodb so that's great you say but then how do we do",
    "start": "777970",
    "end": "784089"
  },
  {
    "text": "transaction that might spend micro-services so in this case a traditional approach might be to use",
    "start": "784089",
    "end": "791620"
  },
  {
    "text": "something like a two-phase commit and the first part we do check out and then",
    "start": "791620",
    "end": "798339"
  },
  {
    "text": "inventory update in the catalog service well here we'd like to",
    "start": "798339",
    "end": "803510"
  },
  {
    "text": "see how we can do this without blocking so without any locking involved and we",
    "start": "803510",
    "end": "809750"
  },
  {
    "text": "can do that as an eventually consistent sequence of steps in which each step is",
    "start": "809750",
    "end": "816170"
  },
  {
    "text": "transactional with respect to its own shopping cart service so in our case the first step would be the checkout and",
    "start": "816170",
    "end": "822950"
  },
  {
    "text": "then the second step would be inventory update the key to doing this correctly is making sure that any updates to the",
    "start": "822950",
    "end": "831800"
  },
  {
    "text": "database in our case to the cart table are also reliably or atomically",
    "start": "831800",
    "end": "837640"
  },
  {
    "text": "published for downstream consumption by the product catalog service and we do",
    "start": "837640",
    "end": "844250"
  },
  {
    "text": "that so we can do that a couple of different ways one approach is to use event sourcing another approach is to",
    "start": "844250",
    "end": "852440"
  },
  {
    "text": "use database transaction log tailing well guess what dynamodb streams is",
    "start": "852440",
    "end": "858620"
  },
  {
    "text": "exactly that it's a transaction log tailing mechanism and that's exactly what we're going to use so here we",
    "start": "858620",
    "end": "866540"
  },
  {
    "text": "initiate the check out by using may be dedicated attribute and setting the",
    "start": "866540",
    "end": "872150"
  },
  {
    "text": "checkout attribute to true that's published downstream by DynamoDB streams",
    "start": "872150",
    "end": "877880"
  },
  {
    "text": "and then consumed by the lambda which belongs to the product catalog service now that lambda is triggered by this",
    "start": "877880",
    "end": "884960"
  },
  {
    "text": "attribute checkout set to true and then it invokes the API on the product",
    "start": "884960",
    "end": "891290"
  },
  {
    "text": "catalog service to update the product catalog and reserve the the items in the",
    "start": "891290",
    "end": "897410"
  },
  {
    "text": "shopping cart and if it all goes well this lambda will invoke the commit API",
    "start": "897410",
    "end": "903530"
  },
  {
    "text": "on the shopping cart service which will then signal the end of the of the",
    "start": "903530",
    "end": "910730"
  },
  {
    "text": "transaction now both these operations have to be idempotent and that's one one",
    "start": "910730",
    "end": "916460"
  },
  {
    "text": "thing we should remember so then this pattern is known as a saga pattern and",
    "start": "916460",
    "end": "923210"
  },
  {
    "text": "in this case we're implementing saga pattern using dynamodb streams and lambda so in case there is a failure",
    "start": "923210",
    "end": "930370"
  },
  {
    "text": "let's say that one of the products is out of stock and we can't reserve it for our shopping",
    "start": "930370",
    "end": "936200"
  },
  {
    "text": "cart our shopping cart service has to have a compensating action available so",
    "start": "936200",
    "end": "942560"
  },
  {
    "text": "that we can do a rollback so in this case we would invoke the rollback API on",
    "start": "942560",
    "end": "948020"
  },
  {
    "text": "the cart service and and in our example that API will simply set our checkout",
    "start": "948020",
    "end": "954170"
  },
  {
    "text": "flag in our shopping cart item to false to mark that the transaction has been",
    "start": "954170",
    "end": "960650"
  },
  {
    "text": "aborted basically and the shopping cart data is still in the table the transaction has ended in a rollback but",
    "start": "960650",
    "end": "968600"
  },
  {
    "text": "everything is consistent in the end this is the list of update items for this",
    "start": "968600",
    "end": "976460"
  },
  {
    "text": "particular use case so add remove item checkout begin checkout commit checkout",
    "start": "976460",
    "end": "981529"
  },
  {
    "text": "rollback you'll notice that each one is the same optimistic concurrency control approach that consists of reading the",
    "start": "981529",
    "end": "988370"
  },
  {
    "text": "item and then updating the item based on the last update timestamp and then the",
    "start": "988370",
    "end": "993920"
  },
  {
    "text": "additional checkout condition I have to mention another building block for",
    "start": "993920",
    "end": "1000010"
  },
  {
    "text": "transactions and that's the step function service step functions make it easier to implement more complex",
    "start": "1000010",
    "end": "1006670"
  },
  {
    "text": "workflows and it can actually be combined with the approach that we just",
    "start": "1006670",
    "end": "1011910"
  },
  {
    "text": "went over so the approach that uses dynamodb streams and and lambda for",
    "start": "1011910",
    "end": "1017680"
  },
  {
    "text": "example you could start with the with the streams based approach and then if",
    "start": "1017680",
    "end": "1023670"
  },
  {
    "text": "the next step in the transaction is more involved simply implement that step",
    "start": "1023670",
    "end": "1030250"
  },
  {
    "text": "using step functions and in this case lambda functions have to be idempotent",
    "start": "1030250",
    "end": "1035709"
  },
  {
    "text": "as well and but step functions is really good way of making sure that all your",
    "start": "1035709",
    "end": "1043780"
  },
  {
    "text": "steps work together as a state machine and it provides observability into your",
    "start": "1043780",
    "end": "1050230"
  },
  {
    "text": "workflow so to recap real quick we just went over",
    "start": "1050230",
    "end": "1056140"
  },
  {
    "text": "how to implement transactions using dynamodb and lambda and the building box",
    "start": "1056140",
    "end": "1062860"
  },
  {
    "text": "for that we're DynamoDB streams lambda and also step functions one thing about this approach is that it",
    "start": "1062860",
    "end": "1070890"
  },
  {
    "text": "can increase the total latency of the transaction because it involves",
    "start": "1070890",
    "end": "1076370"
  },
  {
    "text": "constructs that are asynchronous and eventually consistent such as dynamodb",
    "start": "1076370",
    "end": "1082530"
  },
  {
    "text": "streams next let's talk a little bit about querying micro-services",
    "start": "1082530",
    "end": "1087540"
  },
  {
    "text": "architectures there are some challenges when it comes to querying in microservices right we have all these",
    "start": "1087540",
    "end": "1094080"
  },
  {
    "text": "different micro services and they all own their own data that's one -",
    "start": "1094080",
    "end": "1099440"
  },
  {
    "text": "operational view of data does not necessarily satisfy the needs that we might have for our queries and also if",
    "start": "1099440",
    "end": "1107550"
  },
  {
    "text": "we would like to implement support for new queries in the system we'd like to",
    "start": "1107550",
    "end": "1114450"
  },
  {
    "text": "be able to do that without having to mess with the model of our operational database the solution is to separate the",
    "start": "1114450",
    "end": "1120960"
  },
  {
    "text": "operational and querying views and this is where polyglot persistence really comes into play it enables you this",
    "start": "1120960",
    "end": "1128520"
  },
  {
    "text": "solution enables you to use the right database for the right job and that's command query responsibility segregation",
    "start": "1128520",
    "end": "1136080"
  },
  {
    "text": "pattern now you already remember this slide and we talked about this earlier",
    "start": "1136080",
    "end": "1141240"
  },
  {
    "text": "and how dynamodb streams and AW as lambda enable reliable streaming of your",
    "start": "1141240",
    "end": "1147570"
  },
  {
    "text": "data from your operational database and that's exactly what we can do to implement CQRS",
    "start": "1147570",
    "end": "1152640"
  },
  {
    "text": "in this context so we have reliable downstream delivery of events from your",
    "start": "1152640",
    "end": "1159630"
  },
  {
    "text": "operational database this really allows you to optimize your operational",
    "start": "1159630",
    "end": "1165360"
  },
  {
    "text": "database for the use cases that it needs to support and also optimize your querying use cases for their own needs",
    "start": "1165360",
    "end": "1171780"
  },
  {
    "text": "and guess what they also get to be scalable independently from from each",
    "start": "1171780",
    "end": "1177330"
  },
  {
    "text": "other that's the command query responsibility segregation now each one",
    "start": "1177330",
    "end": "1183810"
  },
  {
    "text": "of those querying services could be exposed as a micro service if you have",
    "start": "1183810",
    "end": "1189240"
  },
  {
    "text": "several of those you won't be able to just use DynamoDB streams as a pubsub",
    "start": "1189240",
    "end": "1194520"
  },
  {
    "text": "mechanism it's not designed for that then with a bit strange designed to just make those",
    "start": "1194520",
    "end": "1200550"
  },
  {
    "text": "updates available but if you need a pub/sub mechanism you have to use some kind of a Fanning approach one way to do",
    "start": "1200550",
    "end": "1208350"
  },
  {
    "text": "that is by using a lambda fan-out function and then fan the data out to different consumers another approach",
    "start": "1208350",
    "end": "1215430"
  },
  {
    "text": "that we see being used more and more relies on Amazon Kinesis streams to do the fan-out",
    "start": "1215430",
    "end": "1220850"
  },
  {
    "text": "what's nice about using Kinesis for this is that it preserves the same ordering",
    "start": "1220850",
    "end": "1226500"
  },
  {
    "text": "if you need that the second thing is it provides a little bit more decoupling because the stream is accessible via its",
    "start": "1226500",
    "end": "1234360"
  },
  {
    "text": "endpoint AR n and so some takeaways for",
    "start": "1234360",
    "end": "1240240"
  },
  {
    "text": "acquiring and enabling querying with micro-services CQRS provides separation",
    "start": "1240240",
    "end": "1245340"
  },
  {
    "text": "of concerns I think of it as the single responsibility principle applied at the",
    "start": "1245340",
    "end": "1252570"
  },
  {
    "text": "architectural level right we're segregating responsibilities and",
    "start": "1252570",
    "end": "1258080"
  },
  {
    "text": "optimizing subsystems basically to do what they what what they're designed to",
    "start": "1258080",
    "end": "1264690"
  },
  {
    "text": "do best each side is independently scalable and we can expose each query as",
    "start": "1264690",
    "end": "1269970"
  },
  {
    "text": "a micro service now we all know that with micro services we already have to deal with that added complexity because",
    "start": "1269970",
    "end": "1276600"
  },
  {
    "text": "they're now distributed so on so nothing new there and and the latency we have to",
    "start": "1276600",
    "end": "1282420"
  },
  {
    "text": "be tolerant to to possible just larger or longer latency next I'd like to talk",
    "start": "1282420",
    "end": "1289860"
  },
  {
    "text": "about ingesting high volume time series data so what we need is high volume ingest let's say we're talking about",
    "start": "1289860",
    "end": "1296700"
  },
  {
    "text": "hundred thousand data points per second but we would also like to access data",
    "start": "1296700",
    "end": "1301950"
  },
  {
    "text": "for a number of days maybe in a month by sensor ID and time range so in this case",
    "start": "1301950",
    "end": "1307980"
  },
  {
    "text": "we're talking about sensor data and and",
    "start": "1307980",
    "end": "1313140"
  },
  {
    "text": "then being able to access those data in in in a very short access time under 10",
    "start": "1313140",
    "end": "1319740"
  },
  {
    "text": "milliseconds so given the scale and and access requirements we need to use the",
    "start": "1319740",
    "end": "1326760"
  },
  {
    "text": "services that are designed for this Amazon Kinesis streams is designed to",
    "start": "1326760",
    "end": "1332460"
  },
  {
    "text": "enable large scale large volume in just of data we lose a WS lambda to",
    "start": "1332460",
    "end": "1338679"
  },
  {
    "text": "get the events from Kinesis and then to store them in dynamo DB now why are we",
    "start": "1338679",
    "end": "1343779"
  },
  {
    "text": "using dynamo DB here we're accessing data on a per sensor basis and DynamoDB",
    "start": "1343779",
    "end": "1350409"
  },
  {
    "text": "is really good at that and in fact we're accessing by sensor ID and time range",
    "start": "1350409",
    "end": "1355659"
  },
  {
    "text": "and we can organize our schema in dynamo DB to really match that use case very",
    "start": "1355659",
    "end": "1361960"
  },
  {
    "text": "well by selecting the partition key to be the sensor ID and by making the",
    "start": "1361960",
    "end": "1367510"
  },
  {
    "text": "timestamp the short key this allows us to do quick queries on gimme the range",
    "start": "1367510",
    "end": "1373570"
  },
  {
    "text": "of data for this sensor between time a and time B and this is very efficient",
    "start": "1373570",
    "end": "1380100"
  },
  {
    "text": "millisecond latency access because DynamoDB based on our keys will store",
    "start": "1380100",
    "end": "1387179"
  },
  {
    "text": "related data for the same sensor together and solutions architecture for",
    "start": "1387179",
    "end": "1394840"
  },
  {
    "text": "this might look like this we're using cloud watch to monitor our services and",
    "start": "1394840",
    "end": "1400350"
  },
  {
    "text": "possibly also to automatically to implement automatic scaling for our Kinesis streams lambda and DynamoDB can",
    "start": "1400350",
    "end": "1408490"
  },
  {
    "text": "can scale automatically now for dynamo DB i should say that as long as your",
    "start": "1408490",
    "end": "1413770"
  },
  {
    "text": "changes in volume are gradual dynamodb is designed to accommodate the scale out",
    "start": "1413770",
    "end": "1420130"
  },
  {
    "text": "of gradual gradually changing workload but for example if you add a large",
    "start": "1420130",
    "end": "1426340"
  },
  {
    "text": "number of sensors at the same time you might have to use explicit update table",
    "start": "1426340",
    "end": "1433960"
  },
  {
    "text": "api to increase the capacity of your dynamodb table so streams are here",
    "start": "1433960",
    "end": "1440350"
  },
  {
    "text": "because we're using the time-to-live feature in dynamo DB to expire data",
    "start": "1440350",
    "end": "1446980"
  },
  {
    "text": "after the time that we don't need it anymore so TTL you can think of it as an",
    "start": "1446980",
    "end": "1453399"
  },
  {
    "text": "expiration date on items in a grocery store except with DynamoDB when that item",
    "start": "1453399",
    "end": "1459610"
  },
  {
    "text": "expires it's automatically removed from the Shelf by dynamodb and with in point",
    "start": "1459610",
    "end": "1465940"
  },
  {
    "text": "within 48 hours in that's the that's the timeframe on that now the the data that's expired we can",
    "start": "1465940",
    "end": "1474000"
  },
  {
    "text": "actually stream an archive in s3 using Kinesis firehose and again we were using",
    "start": "1474000",
    "end": "1481890"
  },
  {
    "text": "streams and AWS lambda to do that now to access data and DynamoDB you'll notice",
    "start": "1481890",
    "end": "1486990"
  },
  {
    "text": "that I'm not using that building block of API gateway here I just simply wanted",
    "start": "1486990",
    "end": "1492360"
  },
  {
    "text": "to show different approach you can access data directly from dynamo DB using many different client api's in",
    "start": "1492360",
    "end": "1498270"
  },
  {
    "text": "this case i'm using javascript and I'm using Cognito to authenticate my client",
    "start": "1498270",
    "end": "1504120"
  },
  {
    "text": "before accessing data in dynamo DB so every good solution should be cost",
    "start": "1504120",
    "end": "1510240"
  },
  {
    "text": "optimized to avoid unnecessary cost main input data into our cost calculation",
    "start": "1510240",
    "end": "1515970"
  },
  {
    "text": "here is the data rate 100,000 data points per second and data storage now let's talk about one month's worth of",
    "start": "1515970",
    "end": "1522690"
  },
  {
    "text": "data storage which I calculated to be about two two point five terabytes if each data point is about 50 bytes",
    "start": "1522690",
    "end": "1530690"
  },
  {
    "text": "let's estimate some cost here lambda will come out to about two to five",
    "start": "1530690",
    "end": "1536370"
  },
  {
    "text": "thousand dollars per month there's more than one way to calculate and then you can you can change the maximum batch",
    "start": "1536370",
    "end": "1542040"
  },
  {
    "text": "size and so depending on the batch size as well as the memory size of your lambda function that all kind of factors",
    "start": "1542040",
    "end": "1549960"
  },
  {
    "text": "into the cost calculation for Canisius one hundred thousand records pretty straightforward you need at least",
    "start": "1549960",
    "end": "1555720"
  },
  {
    "text": "hundred shards because the single Kinesis shard can handle one thousand requests per second that comes out to",
    "start": "1555720",
    "end": "1562830"
  },
  {
    "text": "five thousand dollars a month now dynamodb if I use a single write for",
    "start": "1562830",
    "end": "1568230"
  },
  {
    "text": "each data point that I have I'm going to need a hundred thousand write capacity units of dynamodb that will come out to",
    "start": "1568230",
    "end": "1575700"
  },
  {
    "text": "fifty thousand dollars a month that's a significant cost for dynamodb so where",
    "start": "1575700",
    "end": "1581820"
  },
  {
    "text": "is the problem here if we look at the pricing model for dynamodb one right",
    "start": "1581820",
    "end": "1587130"
  },
  {
    "text": "capacity unit is one kilobyte but in our scenario we're storing data points that are about let's say 50 bytes in size",
    "start": "1587130",
    "end": "1594330"
  },
  {
    "text": "so our write capacity utilization for write is less than five percent now that",
    "start": "1594330",
    "end": "1600960"
  },
  {
    "text": "reminds me my gym membership I pay for the whole month and I use about five percent of it",
    "start": "1600960",
    "end": "1607760"
  },
  {
    "text": "but let's see what we can do about this so in our architecture batches of data",
    "start": "1607760",
    "end": "1616610"
  },
  {
    "text": "are delivered from khaleesies to our lambda function so when we invoke that function we have a number of events",
    "start": "1616610",
    "end": "1623980"
  },
  {
    "text": "multiple events for multiple sensors how about if we just group events for each",
    "start": "1623980",
    "end": "1631460"
  },
  {
    "text": "sensor and store them in a single item so if I'm dealing with temperature",
    "start": "1631460",
    "end": "1637549"
  },
  {
    "text": "readings it could be anything and and these metrics could be even more complex I could simply group multiple metrics",
    "start": "1637549",
    "end": "1643940"
  },
  {
    "text": "into single DynamoDB item and use one dynamodb right capacity unit instead of",
    "start": "1643940",
    "end": "1651950"
  },
  {
    "text": "16 in this case and so we're grouping multiple data points into a single item",
    "start": "1651950",
    "end": "1657290"
  },
  {
    "text": "that can say the Sun right now there is no reason why we could not compress data",
    "start": "1657290",
    "end": "1663230"
  },
  {
    "text": "that we store in DynamoDB and we have many customers who do that too so in this case since we're accessing data",
    "start": "1663230",
    "end": "1669830"
  },
  {
    "text": "based on ID and this attribute called DT which is timestamp the data attribute",
    "start": "1669830",
    "end": "1676910"
  },
  {
    "text": "that contains our metric metrics could simply be compressed and so we can increase our storage efficiency and and",
    "start": "1676910",
    "end": "1686030"
  },
  {
    "text": "save even more on Rights and we're also managing a lifetime of data and DynamoDB",
    "start": "1686030",
    "end": "1691790"
  },
  {
    "text": "using TTL speaking of which TTL is used",
    "start": "1691790",
    "end": "1698179"
  },
  {
    "text": "by designating an attribute to be an expiration timestamp and it's on a per",
    "start": "1698179",
    "end": "1703669"
  },
  {
    "text": "item basis so in our case we could configure this TTL and set it on each",
    "start": "1703669",
    "end": "1710919"
  },
  {
    "text": "record for example in our lambda function in fact we could even use",
    "start": "1710919",
    "end": "1716059"
  },
  {
    "text": "environment variables to pass our TTL value that the lifetime duration of our",
    "start": "1716059",
    "end": "1721910"
  },
  {
    "text": "data into our lambda function as an environment variable and then learn the",
    "start": "1721910",
    "end": "1728240"
  },
  {
    "text": "function the other lambda function that's used in TTL mechanism so if we if you'd like to",
    "start": "1728240",
    "end": "1736860"
  },
  {
    "text": "archive our data into s3 we can create a lambda function that can detect items",
    "start": "1736860",
    "end": "1742590"
  },
  {
    "text": "that are deleted by TTL in the stream and then maybe use Kinesis fire hose to store them in s3 so if we look at the",
    "start": "1742590",
    "end": "1751800"
  },
  {
    "text": "cost here we see that we can store stored items efficiently by by grouping",
    "start": "1751800",
    "end": "1757860"
  },
  {
    "text": "them in DynamoDB and we can save 10x on cost and we can even use compression to",
    "start": "1757860",
    "end": "1765660"
  },
  {
    "text": "save even more a little bit about scaling considerations if we if we're adding more sensors for Canisius you",
    "start": "1765660",
    "end": "1772680"
  },
  {
    "text": "have to add more shards one that down or DB scale automatically and DynamoDB auto",
    "start": "1772680",
    "end": "1777840"
  },
  {
    "text": "scaling enables that now if you're adding more events per sensor then the function will simply create",
    "start": "1777840",
    "end": "1784910"
  },
  {
    "text": "denser DynamoDB items it'll end up storing more data points in each item",
    "start": "1784910",
    "end": "1790400"
  },
  {
    "text": "monitoring very important I listed what I think are fairly important metrics to",
    "start": "1790400",
    "end": "1797550"
  },
  {
    "text": "consider for your for your production use cases and then some some takeaways",
    "start": "1797550",
    "end": "1802640"
  },
  {
    "text": "finally for dynamo DB it's very important to know your data know the",
    "start": "1802640",
    "end": "1808380"
  },
  {
    "text": "structure how big are my data items access patterns and know the cost model",
    "start": "1808380",
    "end": "1813630"
  },
  {
    "text": "and based on that you can optimize your architecture your solution now in our case the optimizations that we used for",
    "start": "1813630",
    "end": "1820890"
  },
  {
    "text": "billing storing multiple data points in a single item compression TTL to delete",
    "start": "1820890",
    "end": "1826110"
  },
  {
    "text": "cold data and then cubase load leveling for an even workload patterns now that's",
    "start": "1826110",
    "end": "1832050"
  },
  {
    "text": "what we can use Kinesis for for lambda it's important to reuse database",
    "start": "1832050",
    "end": "1837570"
  },
  {
    "text": "connections by creating your clients to DynamoDB outside that lambda handler function and then test for optimal batch",
    "start": "1837570",
    "end": "1844260"
  },
  {
    "text": "size and memory CPU and execution time and finally monitoring is critical for",
    "start": "1844260",
    "end": "1850380"
  },
  {
    "text": "any production production application with that my part of the talk is is done",
    "start": "1850380",
    "end": "1857340"
  },
  {
    "text": "and I'd like to turn it over the streaming much hidden",
    "start": "1857340",
    "end": "1862780"
  },
  {
    "text": "for sharing such important details about serverless architectures on DynamoDB",
    "start": "1862780",
    "end": "1869580"
  },
  {
    "text": "hello everyone my name is shrimp a la partie I'm a vice president with the consumer bank engineering team within",
    "start": "1869580",
    "end": "1876490"
  },
  {
    "text": "Capital One and I'm here today to share with you the journey me and my teams",
    "start": "1876490",
    "end": "1881860"
  },
  {
    "text": "have taken in basically moving millions of customers financial transactions from",
    "start": "1881860",
    "end": "1888190"
  },
  {
    "text": "a large monolith mainframe system into the cloud with a lean towards serverless",
    "start": "1888190",
    "end": "1893260"
  },
  {
    "text": "architectures Capital One is one of the 10 largest banks within the United",
    "start": "1893260",
    "end": "1899770"
  },
  {
    "text": "States we serve approximately 45 million customer accounts and we offer a range",
    "start": "1899770",
    "end": "1906190"
  },
  {
    "text": "of financial products from credit cards checking and savings accounts small",
    "start": "1906190",
    "end": "1911320"
  },
  {
    "text": "business accounts consumer bank accounts and auto finance accounts and we also have investing platform and the team is",
    "start": "1911320",
    "end": "1917680"
  },
  {
    "text": "based in Seattle the consumer bank engineering team is spread across",
    "start": "1917680",
    "end": "1922690"
  },
  {
    "text": "multiple sites I have engineering teams spread between New York office Wilmington office McLean",
    "start": "1922690",
    "end": "1928720"
  },
  {
    "text": "enrichment and as I mentioned we also have an investing ring in Seattle",
    "start": "1928720",
    "end": "1934830"
  },
  {
    "text": "traditionally the retail bank has always been powered by a large monolith",
    "start": "1936510",
    "end": "1941650"
  },
  {
    "text": "mainframe system when I talk about a monolith mainframe system think of it as a bank in a box the mainframe system has",
    "start": "1941650",
    "end": "1949600"
  },
  {
    "text": "customer information account information the ledger taxes statements the whole",
    "start": "1949600",
    "end": "1956140"
  },
  {
    "text": "nine yards around providing banking services for our customers when you",
    "start": "1956140",
    "end": "1961450"
  },
  {
    "text": "think about the various service channels through which our customers service their accounts mobile web and the",
    "start": "1961450",
    "end": "1966850"
  },
  {
    "text": "branches they end up integrating with our modern restful api platform on which",
    "start": "1966850",
    "end": "1972910"
  },
  {
    "text": "Capital One has invested heavily in the recent years we have a full API lifecycle management our own internal",
    "start": "1972910",
    "end": "1979960"
  },
  {
    "text": "API gateway and an external gateway to expose our api's to merchants as well as",
    "start": "1979960",
    "end": "1985420"
  },
  {
    "text": "developers these set of modern restful api s-- end up integrating with our",
    "start": "1985420",
    "end": "1992010"
  },
  {
    "text": "legacy soap layer and the soap layer into integrates with the mainframe system for",
    "start": "1992010",
    "end": "1998429"
  },
  {
    "text": "reading underwriting information in a nutshell this high-level architecture is",
    "start": "1998429",
    "end": "2004400"
  },
  {
    "text": "pretty much consistent for many large financial institutions across the whole world about a decade ago this",
    "start": "2004400",
    "end": "2011809"
  },
  {
    "text": "architecture worked fine but now with the advent of cloud and all the modern DevOps infrastructure available to us",
    "start": "2011809",
    "end": "2019130"
  },
  {
    "text": "there are some significant challenges that we starting to face with this architecture in the last few years",
    "start": "2019130",
    "end": "2028580"
  },
  {
    "text": "Capital One has invested heavily on our mobile platform both iOS and Android and",
    "start": "2028580",
    "end": "2033860"
  },
  {
    "text": "we have launched an industry best rated banking app and our customers love it we",
    "start": "2033860",
    "end": "2040730"
  },
  {
    "text": "see a studdy adoption of mobile traffic on both the platforms and the two new customers servicing their accounts on",
    "start": "2040730",
    "end": "2047840"
  },
  {
    "text": "their mobile channel basically in the next few slides I'm going to set up the",
    "start": "2047840",
    "end": "2052970"
  },
  {
    "text": "context as to why we embarked on the journey of building this transactions hub in the cloud at Capital One I also",
    "start": "2052970",
    "end": "2063349"
  },
  {
    "text": "have the good fortune of collaboratively working with a brilliant products team both tech and product are collaborating",
    "start": "2063349",
    "end": "2069858"
  },
  {
    "text": "to come up with innovative financial digital experiences for our customers and majority of them are dependent on",
    "start": "2069859",
    "end": "2076940"
  },
  {
    "text": "relying critical financial data like transactions and accounts which are boxed in the monolith mainframe system",
    "start": "2076940",
    "end": "2084429"
  },
  {
    "text": "with Capital One taking advantage of the AWS services many of our restful api s--",
    "start": "2084429",
    "end": "2089990"
  },
  {
    "text": "and some of our front-end applications are continuously deployed into production around the clock in some",
    "start": "2089990",
    "end": "2096888"
  },
  {
    "text": "cases we deploy hundreds of applications each month leveraging the robust DevOps",
    "start": "2096889",
    "end": "2102680"
  },
  {
    "text": "infrastructure and our CI CD pipelines when you look at the traditional mainframe system they still aligned to",
    "start": "2102680",
    "end": "2109670"
  },
  {
    "text": "the waterfall methodology of moving software from one environment to the environment depending on large amounts",
    "start": "2109670",
    "end": "2116000"
  },
  {
    "text": "of manual regression testing and if you think about it it's largely because of how much kludgy dependency and features",
    "start": "2116000",
    "end": "2123320"
  },
  {
    "text": "we have within the monolith system and then suddenly we have a clash of the newer infrastructure versus the old",
    "start": "2123320",
    "end": "2129830"
  },
  {
    "text": "impacting our ability to ship product faster into the hands of our customers",
    "start": "2129830",
    "end": "2137080"
  },
  {
    "text": "during holiday season like Thanksgiving and Christmas we also see a study",
    "start": "2137380",
    "end": "2142850"
  },
  {
    "text": "increase in traffic and with the legacy architecture we are starting to see slow",
    "start": "2142850",
    "end": "2148130"
  },
  {
    "text": "response times for our critical services there have been instances of reliability because of lack of resiliency and the",
    "start": "2148130",
    "end": "2156410"
  },
  {
    "text": "only way out was to throw more money at the mainframe system to basically remediate these challenges so a",
    "start": "2156410",
    "end": "2163940"
  },
  {
    "text": "combination of new traffic large amounts of traffic and our inability to scale",
    "start": "2163940",
    "end": "2169970"
  },
  {
    "text": "and be resilient and on top of that making the data available in the modern infrastructure to build new digital",
    "start": "2169970",
    "end": "2176660"
  },
  {
    "text": "experiences collectively basically enticed us to solve this problem by taking advantage of the surveillance",
    "start": "2176660",
    "end": "2183080"
  },
  {
    "text": "architecture there are a few things that",
    "start": "2183080",
    "end": "2189080"
  },
  {
    "text": "we we said so when you think about a mainframe system where do we start given the intricate dependencies and so",
    "start": "2189080",
    "end": "2195200"
  },
  {
    "text": "many data so where exactly do we start on closer analysis we figured out that 80% of the traffic is all garnered",
    "start": "2195200",
    "end": "2202490"
  },
  {
    "text": "towards reading customers financial transactions the same transactional data",
    "start": "2202490",
    "end": "2207830"
  },
  {
    "text": "is extremely critical for the new digital products that we are building and hence that gave us the focus to",
    "start": "2207830",
    "end": "2214700"
  },
  {
    "text": "prioritize to tackle the read traffic problem for customers transactional data",
    "start": "2214700",
    "end": "2221050"
  },
  {
    "text": "so if you look at the challenges that the team was faced with first and",
    "start": "2221050",
    "end": "2226340"
  },
  {
    "text": "foremost we had to bootstrap 18 months of transactional data for audit and compliance reasons for each account",
    "start": "2226340",
    "end": "2232610"
  },
  {
    "text": "which translates to billions of customers transactions and make it available in the cloud and we choose",
    "start": "2232610",
    "end": "2238430"
  },
  {
    "text": "DynamoDB as our data store we also had to put in place a near real-time",
    "start": "2238430",
    "end": "2244340"
  },
  {
    "text": "messaging infrastructure to keep the transactions in sync between the mainframe and the data in DynamoDB it",
    "start": "2244340",
    "end": "2253730"
  },
  {
    "text": "was also very critical that we actually handled the data from an encryption and security perspective because we are",
    "start": "2253730",
    "end": "2259940"
  },
  {
    "text": "talking about customers sensitive financial transactional data and then finally we had to build the new",
    "start": "2259940",
    "end": "2266810"
  },
  {
    "text": "version of the gate transactions API in the cloud that integrates with DynamoDB api and presents the transactions in a",
    "start": "2266810",
    "end": "2273710"
  },
  {
    "text": "consistent format across all the channels and then needless to say the",
    "start": "2273710",
    "end": "2278960"
  },
  {
    "text": "consistency of data was extremely important as we live to cache and sync data between mainframe and DynamoDB so",
    "start": "2278960",
    "end": "2288050"
  },
  {
    "text": "how did we go about trying to solve these challenges we put together a top-notch cross-functional engineering",
    "start": "2288050",
    "end": "2294859"
  },
  {
    "text": "team and empowered them to solve for certain data-driven facts around how we",
    "start": "2294859",
    "end": "2300890"
  },
  {
    "text": "declared success criteria for transaction hub first and foremost the speed the new service must perform much",
    "start": "2300890",
    "end": "2308869"
  },
  {
    "text": "better than the legacy system and then the second thing is data integrity we could not afford to lose any of our",
    "start": "2308869",
    "end": "2315530"
  },
  {
    "text": "transactions in the new system and then it was very very important to ensure that the customers financial",
    "start": "2315530",
    "end": "2320900"
  },
  {
    "text": "transactions were secure so we ended up adopting the CQRS pattern the command",
    "start": "2320900",
    "end": "2328430"
  },
  {
    "text": "query responsibility segregation implementation and I know Aiden touched about it the same pattern and few slides",
    "start": "2328430",
    "end": "2334520"
  },
  {
    "text": "ago basically we are talking about two data models one for rights and one for reads as the customers continue to",
    "start": "2334520",
    "end": "2341750"
  },
  {
    "text": "deposit their cheques tap their phones at point of sale or swipe their debit cards all those transactions end up",
    "start": "2341750",
    "end": "2348650"
  },
  {
    "text": "streaming into the mainframe system and then we will have a near real-time infrastructure to keep those",
    "start": "2348650",
    "end": "2354680"
  },
  {
    "text": "transactions in sync in dynamodb and the goal of this entire initiative was to",
    "start": "2354680",
    "end": "2359869"
  },
  {
    "text": "migrate all of the reed traffic that was hitting the mainframe system consuming a",
    "start": "2359869",
    "end": "2364940"
  },
  {
    "text": "lot of MIPS and CPU cycles which were quite cost costly proposition for",
    "start": "2364940",
    "end": "2370640"
  },
  {
    "text": "Capital One and then move it to dynamo DB I'm going to walk through the",
    "start": "2370640",
    "end": "2377839"
  },
  {
    "text": "high-level logical architecture diagram for this transaction hub ecosystem starting with the batch and then",
    "start": "2377839",
    "end": "2384650"
  },
  {
    "text": "followed by the near real-time messaging infrastructure on the mainframe system",
    "start": "2384650",
    "end": "2389839"
  },
  {
    "text": "we have a COBOL program which actually reads millions of transactions and puts them in files and",
    "start": "2389839",
    "end": "2397110"
  },
  {
    "text": "transmitted scan through our internal gateway and seamlessly securely FTP stem",
    "start": "2397110",
    "end": "2402930"
  },
  {
    "text": "into the s3 bucket where all the files are fully encrypted the moment the file",
    "start": "2402930",
    "end": "2408240"
  },
  {
    "text": "arrives on the s3 bucket an SNS notification gets relayed and there is a",
    "start": "2408240",
    "end": "2413970"
  },
  {
    "text": "lambda function which is subscribing to the test honest notification and that lambda function connects to an RDS",
    "start": "2413970",
    "end": "2420150"
  },
  {
    "text": "database and basically inserts the state of the file indicating that the file has arrived we have a cloud watch rule which",
    "start": "2420150",
    "end": "2428160"
  },
  {
    "text": "triggers a second lambda function which looks at the state of the RDS function and basically spins up a new ec2",
    "start": "2428160",
    "end": "2434940"
  },
  {
    "text": "instance on which we have a spring - application running this spring batch application connects to the s3 bucket",
    "start": "2434940",
    "end": "2442340"
  },
  {
    "text": "passes that transactions encrypts each transaction and inserts them into dynamo",
    "start": "2442340",
    "end": "2447840"
  },
  {
    "text": "dB if you realize dynamo DB as of today does not support encryption at rest so",
    "start": "2447840",
    "end": "2454260"
  },
  {
    "text": "we end up taking advantage of the custom dynamo DB encryption libraries and in conjunction with the kms keys which are",
    "start": "2454260",
    "end": "2460980"
  },
  {
    "text": "region based we encrypt each transactional record and then ingest that transactional recording to dynamo",
    "start": "2460980",
    "end": "2467790"
  },
  {
    "text": "DB we then have the new version of the clea transactions API running on an ec2",
    "start": "2467790",
    "end": "2474090"
  },
  {
    "text": "instance in front of our own capital one API gateway and this new version of the",
    "start": "2474090",
    "end": "2479760"
  },
  {
    "text": "API is made available to all our channels and we end up serving the transactional data consistently across",
    "start": "2479760",
    "end": "2486570"
  },
  {
    "text": "all our channels we also have a near real-time messaging infrastructure which",
    "start": "2486570",
    "end": "2492000"
  },
  {
    "text": "basically keeps the mainframe transactions from the transactions in dynamodb in sync in this case we are",
    "start": "2492000",
    "end": "2498300"
  },
  {
    "text": "taking advantage of an existing IBM MQ messaging infrastructure and for resiliency purposes we have for queue",
    "start": "2498300",
    "end": "2504930"
  },
  {
    "text": "managers and then on the ec2 instance we have a Spring Buddha plication which is listening to the transactional events",
    "start": "2504930",
    "end": "2511560"
  },
  {
    "text": "which are being relayed via the messaging infrastructure and then this Springwood application reads the",
    "start": "2511560",
    "end": "2517440"
  },
  {
    "text": "transactions encrypts the transactions and again inserts them into dynamo dB so",
    "start": "2517440",
    "end": "2523020"
  },
  {
    "text": "you can see that through a combination of lambda functions and messaging infrastructure we have support for both",
    "start": "2523020",
    "end": "2530250"
  },
  {
    "text": "and real-time to keep data in sync in our CQRS pattern this slide demonstrates",
    "start": "2530250",
    "end": "2538080"
  },
  {
    "text": "a list of all AWS services we have used in the transaction hub ecosystem and I",
    "start": "2538080",
    "end": "2544349"
  },
  {
    "text": "would also like to add one specific lambda function we have in place which is to dynamically provision read and",
    "start": "2544349",
    "end": "2551550"
  },
  {
    "text": "write throughput settings on DynamoDB based on our usage patterns and we are",
    "start": "2551550",
    "end": "2557400"
  },
  {
    "text": "able to affect cost savings for Capital One again through a lambda function",
    "start": "2557400",
    "end": "2563630"
  },
  {
    "text": "within Capital One we have lot of rigor when it comes to operational and business monitoring and those aspects",
    "start": "2564650",
    "end": "2571980"
  },
  {
    "text": "are firmly baked into the doneness criteria of every product we build and ship into the hands of the customers and",
    "start": "2571980",
    "end": "2578460"
  },
  {
    "text": "pleased to announce that the entire transaction hub ecosystem is live in production for the last several months",
    "start": "2578460",
    "end": "2583800"
  },
  {
    "text": "and serving millions of transactions for our customers and the new API and the transaction of ecosystem has an average",
    "start": "2583800",
    "end": "2590190"
  },
  {
    "text": "response time of 50 milliseconds and what you are looking at is our production new relic dashboard and we",
    "start": "2590190",
    "end": "2596250"
  },
  {
    "text": "also use in some cases Maggio's for operational monitoring for the transaction hub ecosystem in Capital One",
    "start": "2596250",
    "end": "2605520"
  },
  {
    "text": "for this initiative we leverage elasticsearch in conjunction with logstash and Cabana dashboards for",
    "start": "2605520",
    "end": "2611430"
  },
  {
    "text": "business monitoring what you are looking at on the left hand side is the Cabana dashboards for all batch ingestion you",
    "start": "2611430",
    "end": "2618030"
  },
  {
    "text": "can see that every day we are able to successfully read and ingest millions of transactions in the batch mode in",
    "start": "2618030",
    "end": "2624530"
  },
  {
    "text": "approximately 40 minutes and having the ability to dial up the provision capacity units and bring down the time",
    "start": "2624530",
    "end": "2630599"
  },
  {
    "text": "by which we can catch up on the right hand side you're seeing the number of transactional events on a given day by",
    "start": "2630599",
    "end": "2637349"
  },
  {
    "text": "hour we are able to read via the MQ infrastructure and ingest seamlessly into dynamodb it is also fantastic",
    "start": "2637349",
    "end": "2644490"
  },
  {
    "text": "insight for our business because we can now see the peak volume hours on a given 24 hour period during which we will need",
    "start": "2644490",
    "end": "2651540"
  },
  {
    "text": "to accordingly provision the appropriate throughput settings to meet our needs of",
    "start": "2651540",
    "end": "2656640"
  },
  {
    "text": "catching up dynamodb as we are live with transactions hub we",
    "start": "2656640",
    "end": "2663369"
  },
  {
    "text": "are now looking to expand this architecture to also cash account related information and this time we are",
    "start": "2663369",
    "end": "2669249"
  },
  {
    "text": "iteratively and progressively leaning towards more server less components in the first logical architecture diagram",
    "start": "2669249",
    "end": "2675609"
  },
  {
    "text": "few slides ago I demonstrated that we had a spring batch application running on ec2 instance right now my team is",
    "start": "2675609",
    "end": "2682239"
  },
  {
    "text": "actually cutting down that entire infrastructure and instead replacing it with three stat functions one at the",
    "start": "2682239",
    "end": "2688539"
  },
  {
    "text": "batch level one at the file level and one at a smaller chunk level and then using these step functions to",
    "start": "2688539",
    "end": "2694359"
  },
  {
    "text": "orchestrate multiple lambda functions to seamlessly read and ingest millions of",
    "start": "2694359",
    "end": "2700269"
  },
  {
    "text": "account related information into DynamoDB again bringing in efficiencies",
    "start": "2700269",
    "end": "2705519"
  },
  {
    "text": "around speed to market and also bringing in cost savings relative to the ec2 based infrastructure in the next few",
    "start": "2705519",
    "end": "2714189"
  },
  {
    "text": "slides I'm going to touch upon three specific business use cases within Capital One where we have built upon the",
    "start": "2714189",
    "end": "2721569"
  },
  {
    "text": "transaction hub to meet our consumer needs the first use case is around",
    "start": "2721569",
    "end": "2727049"
  },
  {
    "text": "sending out statement email notifications statement ready email notifications for our customers the",
    "start": "2727049",
    "end": "2734109"
  },
  {
    "text": "second use case is where we are now incrementally starting to move some of our api's which are running on ec2",
    "start": "2734109",
    "end": "2741400"
  },
  {
    "text": "instances into lambda functions and making it available for our channels and the third use case is a new product in a",
    "start": "2741400",
    "end": "2748660"
  },
  {
    "text": "feature called second look which our card customers already enjoy where we are again leveraging dynamodb streams in",
    "start": "2748660",
    "end": "2755019"
  },
  {
    "text": "conjunction with data in dynamodb and lambda functions to actually implement that feature so this is the high-level",
    "start": "2755019",
    "end": "2764140"
  },
  {
    "text": "logical architecture diagram for how we are managing to send statement ready",
    "start": "2764140",
    "end": "2769719"
  },
  {
    "text": "email notifications to millions of our customers as it stands today we have a",
    "start": "2769719",
    "end": "2775779"
  },
  {
    "text": "legacy data warehouse and a control-m batch program that is manually kicked off to send off these email",
    "start": "2775779",
    "end": "2782319"
  },
  {
    "text": "notifications to our customers and there is a manual review process to verify these statements what we are now doing",
    "start": "2782319",
    "end": "2789819"
  },
  {
    "text": "in production is to replace that infrastructure with a seamless lambda function that actually reads the percentage",
    "start": "2789819",
    "end": "2796360"
  },
  {
    "text": "allocation of customers to which we have to send the email alerts at a particular cycle date and at the same time this",
    "start": "2796360",
    "end": "2803590"
  },
  {
    "text": "lambda function reads data of all the customer information from s3 bucket and",
    "start": "2803590",
    "end": "2808660"
  },
  {
    "text": "it juxtaposes the appropriate email templates and basically creates an s3",
    "start": "2808660",
    "end": "2814930"
  },
  {
    "text": "file and we have an email communication application that processes this s3 file",
    "start": "2814930",
    "end": "2820180"
  },
  {
    "text": "and sends emails to millions of our customers indicating to them that the",
    "start": "2820180",
    "end": "2825760"
  },
  {
    "text": "statements are ready for them to view on the digital channels as you can see this",
    "start": "2825760",
    "end": "2831100"
  },
  {
    "text": "entire infrastructure has been stood up both on East and West regions for resiliency purposes again one other",
    "start": "2831100",
    "end": "2838690"
  },
  {
    "text": "great use case of where we are tearing down a legacy data warehouse infrastructure and instead replacing",
    "start": "2838690",
    "end": "2843910"
  },
  {
    "text": "that with serverless and lambda to meet the needs of our business this is a",
    "start": "2843910",
    "end": "2851110"
  },
  {
    "text": "specific use case where we are actually exposing or get statements API which",
    "start": "2851110",
    "end": "2856540"
  },
  {
    "text": "traditionally used to be running on an ec2 instance instead to be replaced by a lambda function this particular API",
    "start": "2856540",
    "end": "2863860"
  },
  {
    "text": "actually provides the PDF statements back to the various digital channels and",
    "start": "2863860",
    "end": "2869710"
  },
  {
    "text": "now we are replacing that with a lambda function so you can see that any requests pertaining to get statements",
    "start": "2869710",
    "end": "2874930"
  },
  {
    "text": "coming from our mobile and web comes through a wolf layer for security purposes and once it crosses the Wolf's",
    "start": "2874930",
    "end": "2881650"
  },
  {
    "text": "Lair we have our own orchestration layer within the Capital One V PC then we have",
    "start": "2881650",
    "end": "2888400"
  },
  {
    "text": "our own API gateway within Capital One V PC which authenticates the requests coming in from valid client applications",
    "start": "2888400",
    "end": "2895210"
  },
  {
    "text": "in this case the orchestration layer once the Gateway actually authenticates the request it is then able to",
    "start": "2895210",
    "end": "2901600"
  },
  {
    "text": "seamlessly throughout the request to or a to the lambda function and this is",
    "start": "2901600",
    "end": "2907270"
  },
  {
    "text": "again giving more opportunities for our engineers and products to build appropriate business API so as lambda",
    "start": "2907270",
    "end": "2913540"
  },
  {
    "text": "functions and securely expose them by onboarding them onto our internal gateway to our various channels",
    "start": "2913540",
    "end": "2921660"
  },
  {
    "text": "second-look today we have had this feature in action for our card customers for a long",
    "start": "2923610",
    "end": "2930130"
  },
  {
    "text": "time we're round-the-clock we do account level monitoring and then we are looking for duplicate transactions and automated",
    "start": "2930130",
    "end": "2937750"
  },
  {
    "text": "renewal subscription fees from certain vendors and merchants and proactively alerting our customers through our",
    "start": "2937750",
    "end": "2943900"
  },
  {
    "text": "mobile app through email we are now working with our product to make the same feature available to our debit card",
    "start": "2943900",
    "end": "2950470"
  },
  {
    "text": "customers but this time taking advantage of DynamoDB streams and lambda functions",
    "start": "2950470",
    "end": "2955630"
  },
  {
    "text": "in a true server less architecture so we have transactional data already",
    "start": "2955630",
    "end": "2961510"
  },
  {
    "text": "available in DynamoDB and we are now enabling DynamoDB streams and we are streaming tens and millions of",
    "start": "2961510",
    "end": "2967599"
  },
  {
    "text": "transactional events through the DynamoDB stream we have a lambda function which inspects the transactions",
    "start": "2967599",
    "end": "2974470"
  },
  {
    "text": "in the DynamoDB stream and identifies new transactions that are streaming in and then it hands it over to a second",
    "start": "2974470",
    "end": "2981430"
  },
  {
    "text": "lambda function which inspects the transaction for duplicate transactions and unwanted you know subscription fees",
    "start": "2981430",
    "end": "2988720"
  },
  {
    "text": "for our customers and sends email notifications again one other fantastic use case of you know enabling fabulous",
    "start": "2988720",
    "end": "2996580"
  },
  {
    "text": "architectures to make you know meet the needs of our customers",
    "start": "2996580",
    "end": "3001579"
  },
  {
    "text": "there are various law lessons that we have learnt in our journey of you know migrating customers transactional data",
    "start": "3002180",
    "end": "3009210"
  },
  {
    "text": "from a mainframe system to dynamodb in the cloud I'll touch upon few of them it",
    "start": "3009210",
    "end": "3014369"
  },
  {
    "text": "is very very important to know your data the initial files that we got from the source system we're all coming through",
    "start": "3014369",
    "end": "3021480"
  },
  {
    "text": "primary keys from the mainframe system and this was creating a lot of you know hot partition issues and throttling",
    "start": "3021480",
    "end": "3027750"
  },
  {
    "text": "errors in production and we had to basically ensure that the data was distributed so that the data was",
    "start": "3027750",
    "end": "3034230"
  },
  {
    "text": "actually being ingested in parallel partitions at the same time to avoid avoid the hard partition problem it is",
    "start": "3034230",
    "end": "3041910"
  },
  {
    "text": "very very important to test at scale we did not notice the hard partition issue in production in in the lower",
    "start": "3041910",
    "end": "3048810"
  },
  {
    "text": "environments and we only identified it in production at large volumes and it",
    "start": "3048810",
    "end": "3053820"
  },
  {
    "text": "becomes very very critical that you actually test your use cases in the lower environments you know so that you",
    "start": "3053820",
    "end": "3060540"
  },
  {
    "text": "are not faced with a similar situation know your access patterns upfront what I",
    "start": "3060540",
    "end": "3066000"
  },
  {
    "text": "mean by that is once you actually create your indexes and tables and ingest the data and you then try to identify your",
    "start": "3066000",
    "end": "3071820"
  },
  {
    "text": "patterns you cannot have the opportunity to change the keys and you'll basically be forced to readjust the data so you",
    "start": "3071820",
    "end": "3078900"
  },
  {
    "text": "will be served good by knowing what your access patterns are so that you can design your indexes and your tables",
    "start": "3078900",
    "end": "3084900"
  },
  {
    "text": "appropriately V PC based endpoints so few months ago the endpoint of dynamodb",
    "start": "3084900",
    "end": "3093930"
  },
  {
    "text": "was actually a public face endpoint now what that really meant was for security purposes the request from Capital One",
    "start": "3093930",
    "end": "3100950"
  },
  {
    "text": "had to go through a proxy layer now thanks to the collaborative effort between Capital One and the DynamoDB",
    "start": "3100950",
    "end": "3107370"
  },
  {
    "text": "team now it is available as a VP C based endpoint we were able to avoid the",
    "start": "3107370",
    "end": "3112620"
  },
  {
    "text": "additional hop through a proxy layer and we are now seeing significant improvement in performance time approximately around 20% both for reads",
    "start": "3112620",
    "end": "3119790"
  },
  {
    "text": "and writes by skipping the additional proxy layer I already touched upon the",
    "start": "3119790",
    "end": "3125250"
  },
  {
    "text": "fact that today dynamodb does not support encryption at rest and our way of working around that situation is by",
    "start": "3125250",
    "end": "3131670"
  },
  {
    "text": "leveraging region based KMS keys and again working with the DynamoDB team to take advantage of the client encryption",
    "start": "3131670",
    "end": "3137940"
  },
  {
    "text": "libraries to custom encrypt each transactional record and then securely",
    "start": "3137940",
    "end": "3142950"
  },
  {
    "text": "save it in dynamo DB that brings an end",
    "start": "3142950",
    "end": "3149580"
  },
  {
    "text": "to the entire presentation I have two announcements to make first one Capital One has a booth at the",
    "start": "3149580",
    "end": "3156840"
  },
  {
    "text": "Expo Center I would encourage you guys to please stop by so that we can share more fantastic use cases of how we are",
    "start": "3156840",
    "end": "3163230"
  },
  {
    "text": "leveraging AWS in our technological strategy and digital transformation journey within Capital One the second",
    "start": "3163230",
    "end": "3170160"
  },
  {
    "text": "announcement is Capital One is contributing and creating an open-source project called cloud custodian which",
    "start": "3170160",
    "end": "3175710"
  },
  {
    "text": "basically helps organizations to better manage their AWS environments when it comes to security policies tagging",
    "start": "3175710",
    "end": "3182460"
  },
  {
    "text": "policies and cost management all day Tuesday the dev exhange team in Capital",
    "start": "3182460",
    "end": "3187680"
  },
  {
    "text": "One is available to interact with the entire user community team you will get an opportunity to talk face to face with",
    "start": "3187680",
    "end": "3193740"
  },
  {
    "text": "the developers behind the cloud custodian product I would encourage you guys to please stop at the end core and take",
    "start": "3193740",
    "end": "3199980"
  },
  {
    "text": "advantage of that all day Tuesday now finally it's humbling experience to",
    "start": "3199980",
    "end": "3206580"
  },
  {
    "text": "stand in front of you and share the fantastic word that me and my teams have done and I want to thank each one of you",
    "start": "3206580",
    "end": "3212430"
  },
  {
    "text": "for the time and the opportunity and I will turn this over to and then to say the closing words",
    "start": "3212430",
    "end": "3219230"
  },
  {
    "text": "[Applause]",
    "start": "3220610",
    "end": "3229190"
  },
  {
    "text": "feedback check out what's new in DynamoDB this Wednesday 3:15 and very",
    "start": "3229190",
    "end": "3234450"
  },
  {
    "text": "neezy 24:05 thanks again thank you I will I will stick around for questions",
    "start": "3234450",
    "end": "3240570"
  },
  {
    "text": "after this I will stick around and and take any questions off at the stage thank you",
    "start": "3240570",
    "end": "3248210"
  }
]