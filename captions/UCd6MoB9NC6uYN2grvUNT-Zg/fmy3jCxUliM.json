[
  {
    "start": "0",
    "end": "232000"
  },
  {
    "text": "good afternoon make sure in the right room this is the Amazon redshift deep",
    "start": "60",
    "end": "6330"
  },
  {
    "text": "dive my name is Eric fajita I'm a database engineer with the red chief development team and sharing the stage",
    "start": "6330",
    "end": "12719"
  },
  {
    "text": "with me I have Ari Miller one of our customers we're going to talk about specifically today CUNY and best",
    "start": "12719",
    "end": "18600"
  },
  {
    "text": "practices for performance on redshift used some of the topics we're going to",
    "start": "18600",
    "end": "24720"
  },
  {
    "text": "talk about today ingestion some recent features we added for you some migration",
    "start": "24720",
    "end": "31080"
  },
  {
    "text": "tips workload unis and then I'll have the privilege to have Ari from",
    "start": "31080",
    "end": "36180"
  },
  {
    "text": "TripAdvisor talk to you about how they implemented their redshift environment",
    "start": "36180",
    "end": "41489"
  },
  {
    "text": "for performance and I highly recommend you stay to the end at the beginning I took a picture I will know who left or",
    "start": "41489",
    "end": "48270"
  },
  {
    "text": "who didn't okay so be aware so this is an advanced ed sesh I'm not gonna go to",
    "start": "48270",
    "end": "54629"
  },
  {
    "text": "the details redshift but it's always good to remember redshift is our fast simple petabyte scale data warehouse for",
    "start": "54629",
    "end": "61710"
  },
  {
    "text": "less than $1,000 per terabytes per year and today I want to focus on the fast",
    "start": "61710",
    "end": "67409"
  },
  {
    "text": "part and there are many customers that are really happy about the performance",
    "start": "67409",
    "end": "72450"
  },
  {
    "text": "they get with redshift and the idea of this session is to make sure you also",
    "start": "72450",
    "end": "77790"
  },
  {
    "text": "that in using redshift can achieve the same performance as those customers are enjoying here's a review of the",
    "start": "77790",
    "end": "86400"
  },
  {
    "text": "architecture of redshift we are a common columnar Moochie node share nothing",
    "start": "86400",
    "end": "92970"
  },
  {
    "text": "cluster database service and you have a leader node where you connect for a",
    "start": "92970",
    "end": "98040"
  },
  {
    "text": "sequel and then the compute nodes where the data resides and they all the work happens and a lot of the operations",
    "start": "98040",
    "end": "105060"
  },
  {
    "text": "happens in parallel directly through the compute nodes and that some of the things we're going to talk about today",
    "start": "105060",
    "end": "111200"
  },
  {
    "text": "if you focused on the compute nodes details you're gonna see that the node",
    "start": "111200",
    "end": "117930"
  },
  {
    "text": "has different slices and the unit of work on redshift is actually the slice and so it cares more about how many",
    "start": "117930",
    "end": "126090"
  },
  {
    "text": "slices total we have in your cluster than the number of nodes you have in your cluster we have two platforms one",
    "start": "126090",
    "end": "132690"
  },
  {
    "text": "with SSD that has different types that have the large and 8xl and normally we have ratio",
    "start": "132690",
    "end": "140190"
  },
  {
    "text": "one-to-one slices per CPU and we have our dense storage nodes which we have",
    "start": "140190",
    "end": "146100"
  },
  {
    "text": "hard drives and on this particular platform we have two CPUs for slice and",
    "start": "146100",
    "end": "151260"
  },
  {
    "text": "by the way these RDS choose if you're still using a just ones or D s ones I",
    "start": "151260",
    "end": "157050"
  },
  {
    "text": "highly recommend you move because it's double the CPU double the memory same",
    "start": "157050",
    "end": "162540"
  },
  {
    "text": "price no-brainer so focus on how the redshift does the work on each slice",
    "start": "162540",
    "end": "168480"
  },
  {
    "text": "we're going to talk about a different things to achieve the parallelism the performance you need for your workload",
    "start": "168480",
    "end": "174650"
  },
  {
    "text": "so first let's talk about ingestion if",
    "start": "174650",
    "end": "180000"
  },
  {
    "text": "you copy a single file into redshift only one slice is doing work because",
    "start": "180000",
    "end": "185610"
  },
  {
    "text": "they slice is the one that cop is a different file each slice copy a different file so instead of getting a",
    "start": "185610",
    "end": "191970"
  },
  {
    "text": "hundred megabytes per second you're gonna get six megabytes per second and that's not fun if you have enough files",
    "start": "191970",
    "end": "199019"
  },
  {
    "text": "then on the phase one of your copy command you're gonna have every slice doing work and then you can achieve",
    "start": "199019",
    "end": "205410"
  },
  {
    "text": "their 100 megabytes per second per node and expect to scale near linearly as you",
    "start": "205410",
    "end": "212040"
  },
  {
    "text": "add more nodes assume you have enough files to feed those slices right you want those files to be similar size",
    "start": "212040",
    "end": "219090"
  },
  {
    "text": "amongst each other of course and just one extra tip files between one megabyte",
    "start": "219090",
    "end": "225390"
  },
  {
    "text": "and one gigabyte gzipped works best in terms of performance another thing",
    "start": "225390",
    "end": "233760"
  },
  {
    "start": "232000",
    "end": "232000"
  },
  {
    "text": "related to ingestion is the primary keys redshift allows you to add primary key",
    "start": "233760",
    "end": "240000"
  },
  {
    "text": "to the table and to define them and it's good documentation for you and the",
    "start": "240000",
    "end": "245519"
  },
  {
    "text": "optimizer can use that information to make optimizations during the query",
    "start": "245519",
    "end": "251360"
  },
  {
    "text": "however if you try to load the same data twice red chip is not going to complain we don't enforce primary keys but we",
    "start": "251360",
    "end": "259109"
  },
  {
    "text": "still recommend you to use them so make sure if you declare a column as primary key you maintain that unique otherwise",
    "start": "259109",
    "end": "265860"
  },
  {
    "text": "you might get wrong results when the optimizer assumes the data is unique and it's not also when",
    "start": "265860",
    "end": "273910"
  },
  {
    "text": "you load in data into redshift you might create a number of files on s3 and",
    "start": "273910",
    "end": "279460"
  },
  {
    "text": "immediately try to copy with the prefix giving the eventual consistency on s3",
    "start": "279460",
    "end": "284800"
  },
  {
    "text": "you may not see all the files immediately on the other copy command so you might leave some files behind and of",
    "start": "284800",
    "end": "291940"
  },
  {
    "text": "course you can go on STL load commits and see which files you actually load it and make sure you got all the files you",
    "start": "291940",
    "end": "299050"
  },
  {
    "text": "wanted but if you use a manifest file then you declare what exact files you",
    "start": "299050",
    "end": "304540"
  },
  {
    "text": "want to load and what you do with the file is not there error out or ignore",
    "start": "304540",
    "end": "309580"
  },
  {
    "text": "and at the end you guarantee you load exactly the files you want to load the",
    "start": "309580",
    "end": "314890"
  },
  {
    "text": "beauty of the manifest file is that if a Phi is supposed to be there and it's not",
    "start": "314890",
    "end": "319900"
  },
  {
    "text": "it retries a few times so if I might show up and if it doesn't show up after",
    "start": "319900",
    "end": "325600"
  },
  {
    "text": "a few tries that is when it errors out",
    "start": "325600",
    "end": "330330"
  },
  {
    "start": "330000",
    "end": "330000"
  },
  {
    "text": "let's talk about data hygiene lots of customers create their clusters start",
    "start": "331320",
    "end": "337900"
  },
  {
    "text": "loading your data are amazed by the performance and month down the road they say oh my god my performance tanked",
    "start": "337900",
    "end": "343479"
  },
  {
    "text": "something is wrong and they're not doing data hygiene so gathering stats on table",
    "start": "343479",
    "end": "350440"
  },
  {
    "text": "is very important for the optimizer to know which table is bigger which table is gonna return more rows so you can do",
    "start": "350440",
    "end": "356860"
  },
  {
    "text": "the joint on the right order so every time you add rows to a table you should",
    "start": "356860",
    "end": "362110"
  },
  {
    "text": "analyze at least the sort key column it's very fast because you do on a single column and it's enough for the",
    "start": "362110",
    "end": "368710"
  },
  {
    "text": "optimizer to know the size of the table weekly you should do a full analyse of",
    "start": "368710",
    "end": "374770"
  },
  {
    "text": "all the columns so the optimizer has better statistics about all the data in your table there is a view we added",
    "start": "374770",
    "end": "382210"
  },
  {
    "text": "recently that's called svv table info and the column called stats off can tell",
    "start": "382210",
    "end": "388120"
  },
  {
    "text": "you how much percent of the table has stea stats and then you know when to",
    "start": "388120",
    "end": "394510"
  },
  {
    "text": "gather steps also you can look on the stl alert event log for tables that are",
    "start": "394510",
    "end": "401320"
  },
  {
    "text": "nice nice tats altogether if you never gather stats on a table which is possible and can cause you a lot of",
    "start": "401320",
    "end": "406480"
  },
  {
    "text": "problems vacuum vacuum I don't want you",
    "start": "406480",
    "end": "411940"
  },
  {
    "text": "to run with every load not part of it ETL vacuum is a very expensive process",
    "start": "411940",
    "end": "417970"
  },
  {
    "text": "you want to run kind of once a week is a good start and see how that works for",
    "start": "417970",
    "end": "423670"
  },
  {
    "text": "you used for your environment the number of unsorted blocks as a trigger to know",
    "start": "423670",
    "end": "430600"
  },
  {
    "text": "when to run vacuum on your table again if you look on svv table info on the",
    "start": "430600",
    "end": "435670"
  },
  {
    "text": "column unsorted or empty you know when it's time to run vacuum because you",
    "start": "435670",
    "end": "440830"
  },
  {
    "text": "deleted too many rows or because we have too many unsorted rows be aware however",
    "start": "440830",
    "end": "445840"
  },
  {
    "text": "that if you have too many on sorted blocks or too many deleted rows it might",
    "start": "445840",
    "end": "451570"
  },
  {
    "text": "be faster to do a DP copy on your table create a brand new table just copy the",
    "start": "451570",
    "end": "456580"
  },
  {
    "text": "data over then run vacuum usually that thresholds about 20% but your mileage",
    "start": "456580",
    "end": "462700"
  },
  {
    "text": "will vary automatic compression it's a great thing because automatically",
    "start": "462700",
    "end": "469630"
  },
  {
    "text": "compress the data for you you get better performance on your scans and lower costs on your storage because you can",
    "start": "469630",
    "end": "475630"
  },
  {
    "text": "fit more dating class space and what it does is actually samples the data about",
    "start": "475630",
    "end": "480970"
  },
  {
    "text": "a hundred thousand million rows worth of data it tries to compress every possible",
    "start": "480970",
    "end": "486160"
  },
  {
    "text": "way figure is out which one is gonna be smaller drops the table creates the",
    "start": "486160",
    "end": "492640"
  },
  {
    "text": "table again and starts loading all over again that's great to do once not every time",
    "start": "492640",
    "end": "498790"
  },
  {
    "text": "you load the same type of data so if you load using copy command into up empty",
    "start": "498790",
    "end": "505030"
  },
  {
    "text": "table like a staging table or a temporary table load the data analyze",
    "start": "505030",
    "end": "510430"
  },
  {
    "text": "compression define what's the best compression for that column for each column on that table and then you bake",
    "start": "510430",
    "end": "516640"
  },
  {
    "text": "those on the create table then you don't do all to compression every single load its victim cost you a lot of time",
    "start": "516640",
    "end": "523900"
  },
  {
    "text": "loading if you try to figure out the compression of every single time okay",
    "start": "523900",
    "end": "530070"
  },
  {
    "start": "530000",
    "end": "530000"
  },
  {
    "text": "talking about compression be careful when you compress your sort keys",
    "start": "530070",
    "end": "536519"
  },
  {
    "text": "on redshift we use zone maps to figure out which blocks we're gonna actually",
    "start": "536639",
    "end": "543160"
  },
  {
    "text": "scan now we start on the short key because your records",
    "start": "543160",
    "end": "548319"
  },
  {
    "text": "likely to be on the short key and if for example you have a column one there to",
    "start": "548319",
    "end": "555579"
  },
  {
    "text": "be your short key it's highly compressed I have a hundred thousand rows on a single block when I find the one row I'm",
    "start": "555579",
    "end": "563350"
  },
  {
    "text": "looking for it marked that block okay I need to scan the row offset of this",
    "start": "563350",
    "end": "569230"
  },
  {
    "text": "block so I'm gonna scan between rows 1 0 and 100,000 when I go to the other",
    "start": "569230",
    "end": "574389"
  },
  {
    "text": "columns like in our example column number 2 which is 5 times bigger I gonna",
    "start": "574389",
    "end": "580389"
  },
  {
    "text": "scan five blocks for each one block of the third key I need to scan and that",
    "start": "580389",
    "end": "585399"
  },
  {
    "text": "can be wasteful if my date is only one block of every single column the goal is",
    "start": "585399",
    "end": "590980"
  },
  {
    "text": "to have all columns similar size between the sort key and the non search key",
    "start": "590980",
    "end": "596019"
  },
  {
    "text": "column so I can most efficient map row offsets from one column to the next a",
    "start": "596019",
    "end": "602069"
  },
  {
    "text": "good rule of thumb is don't compress those are keys compress everything else",
    "start": "602069",
    "end": "607899"
  },
  {
    "text": "don't compare the search Keys because usually the start key will be a date or an integer or something like that and",
    "start": "607899",
    "end": "614110"
  },
  {
    "text": "you compress really well because of you ordering by that so be careful rule of thumb don't compress short keys",
    "start": "614110",
    "end": "620730"
  },
  {
    "text": "now if you want to check your data for possible problems again svv table info",
    "start": "620730",
    "end": "627309"
  },
  {
    "text": "there's a column called excuse alt key one which the ratio between the largest column after compression and the first",
    "start": "627309",
    "end": "634420"
  },
  {
    "text": "column the short key that number is too big you might have this problem and if",
    "start": "634420",
    "end": "639850"
  },
  {
    "text": "you have this problem even having the proper start key and even having the queries using the third key you're gonna",
    "start": "639850",
    "end": "645970"
  },
  {
    "text": "show the alert that says very selective filter meaning it scans too many rows or",
    "start": "645970",
    "end": "652029"
  },
  {
    "text": "too many blocks in our case to return to few rows and that's their sign that",
    "start": "652029",
    "end": "657279"
  },
  {
    "text": "either a search key is wrong or you would need to uncompress your sort keys as you create your design your tables",
    "start": "657279",
    "end": "665829"
  },
  {
    "text": "you want to keep your tables as narrow as for so in terms of the size of your varchars",
    "start": "665829",
    "end": "670930"
  },
  {
    "text": "we only store the amount of data that the varchar' has however as we pull the",
    "start": "670930",
    "end": "677950"
  },
  {
    "text": "data back we create a buffer off the declared size because we don't know how much the size of the date we're gonna",
    "start": "677950",
    "end": "683920"
  },
  {
    "text": "get back and why there means less rows in the same memory means spilling more",
    "start": "683920",
    "end": "689260"
  },
  {
    "text": "to disk and low and poor performance on queries so make sure you don't have a VAR char 2004 our state column of two",
    "start": "689260",
    "end": "696670"
  },
  {
    "text": "okay I've seen that trust me is no I'm not kidding look at your tables on svv",
    "start": "696670",
    "end": "702130"
  },
  {
    "text": "table info for the max varchar' and that might give a hint that you have a VAR",
    "start": "702130",
    "end": "708460"
  },
  {
    "text": "char 64,000 that maybe should you need to exist if you have the data that feels",
    "start": "708460",
    "end": "714190"
  },
  {
    "text": "out large for a chart that's fine right but if you don't don't create a bunch of our char 1000 var char 256 when you need",
    "start": "714190",
    "end": "721630"
  },
  {
    "text": "to 10 12 right be careful about how you size your table now I want to talk to",
    "start": "721630",
    "end": "730450"
  },
  {
    "text": "you about some recent features we launched and some some people I have a customer talking today where he said you",
    "start": "730450",
    "end": "736150"
  },
  {
    "text": "know I got surprised you guys launched such-and-such feature I'm gonna talk about here and we didn't know for a",
    "start": "736150",
    "end": "742390"
  },
  {
    "text": "month so there are lists on the AWS blog that you can see what's new and getting",
    "start": "742390",
    "end": "747880"
  },
  {
    "text": "information but who has time to be looking for what's going on all the time so I want to catch you up on the new",
    "start": "747880",
    "end": "753340"
  },
  {
    "text": "features that can help with performance as well on redshift we added a number of",
    "start": "753340",
    "end": "759490"
  },
  {
    "text": "new SQL functions my favorite a list egg and there's two version of it there's the Wyndham function version and the",
    "start": "759490",
    "end": "766360"
  },
  {
    "text": "regular aggregate function version we add an approximate count drop if he",
    "start": "766360",
    "end": "772000"
  },
  {
    "text": "exists in a number of others and we keep intereting and we're going to keep adding more functions for your workload",
    "start": "772000",
    "end": "779200"
  },
  {
    "text": "but we also want to allow you to create your own functions and we launch user-defined functions you can write",
    "start": "779200",
    "end": "786370"
  },
  {
    "start": "784000",
    "end": "784000"
  },
  {
    "text": "your code in Python 2.7 and and use whatever you want in terms of how you're",
    "start": "786370",
    "end": "793210"
  },
  {
    "text": "gonna use your functions we still want to hear about specific functions you use",
    "start": "793210",
    "end": "798340"
  },
  {
    "text": "a lot they might be useful for other customers we might bake that into the system directly so don't stop",
    "start": "798340",
    "end": "804430"
  },
  {
    "text": "talking to us about what features and not what functions you want added to redshift we still wanna listen because",
    "start": "804430",
    "end": "810670"
  },
  {
    "text": "we might add them for you the library we added for Python already comes with",
    "start": "810670",
    "end": "817120"
  },
  {
    "text": "pandas numpy and Syfy and you can create and install your own libraries and it's",
    "start": "817120",
    "end": "822160"
  },
  {
    "text": "a great great use case for for example you create outside of the base a huge",
    "start": "822160",
    "end": "827290"
  },
  {
    "text": "scoring model very complicated save on a small little python library load the",
    "start": "827290",
    "end": "832329"
  },
  {
    "text": "library on redshift and use to score very fast your data so there's limitless",
    "start": "832329",
    "end": "837850"
  },
  {
    "text": "applications and we love to hear what you're gonna do with UDF's a couple of examples is could be like if",
    "start": "837850",
    "end": "847060"
  },
  {
    "text": "you have a hostname to extract from a URL you could use our rag x but it's",
    "start": "847060",
    "end": "854889"
  },
  {
    "text": "kind of complex you might miss the comma here and get something wrong or it can create a UDF that make it simple for",
    "start": "854889",
    "end": "860769"
  },
  {
    "text": "anybody to pull a hostname out of you RL let's a scene for example that's a kind of child's play we had",
    "start": "860769",
    "end": "868120"
  },
  {
    "text": "partners that as soon as we announce the feature they created a list of examples",
    "start": "868120",
    "end": "874360"
  },
  {
    "text": "of UDF that are quite useful both lucre and periscope created a very good set of",
    "start": "874360",
    "end": "880990"
  },
  {
    "text": "functions some related to date manipulation json manipulation I highly recommend you take a look go they have",
    "start": "880990",
    "end": "888040"
  },
  {
    "text": "tons of them and we love because it's immediately can be used for to you on your application talking about partners",
    "start": "888040",
    "end": "897329"
  },
  {
    "text": "don't forget there there's a number of partners that have their services run",
    "start": "897329",
    "end": "902500"
  },
  {
    "text": "directly on top of red shift that you can deploy in come together when a single click things like eternity or",
    "start": "902500",
    "end": "910480"
  },
  {
    "text": "MicroStrategy or looker so make sure you don't forget that the mark marketplace can have things for you now I want to",
    "start": "910480",
    "end": "919269"
  },
  {
    "text": "talk to you about interleaved start keys we added some time ago and we making it better as as we process and get more",
    "start": "919269",
    "end": "927040"
  },
  {
    "text": "feedback from customers and we want to make sure that you understand what it does and how can help you with your",
    "start": "927040",
    "end": "932110"
  },
  {
    "text": "queries so before I go there let me explain how compound Serkis work when I have a table with a compound salt",
    "start": "932110",
    "end": "939579"
  },
  {
    "start": "938000",
    "end": "938000"
  },
  {
    "text": "key I sort everything for the first column in our example here customer ID",
    "start": "939579",
    "end": "944950"
  },
  {
    "text": "and then for the same value of customer ID I start everything by product ID the",
    "start": "944950",
    "end": "951279"
  },
  {
    "text": "table is physically ordered by that every column even though it's a separate file it's ordered by the same order",
    "start": "951279",
    "end": "958110"
  },
  {
    "text": "sorting by customer ID and Product ID so my first Seco you see there when I do a",
    "start": "958110",
    "end": "964180"
  },
  {
    "text": "query and let's say customer ID equals 1 for example I will hit only 1 block",
    "start": "964180",
    "end": "969700"
  },
  {
    "text": "because of zone maps in Emacs varies I know I don't have to scan all the blocks",
    "start": "969700",
    "end": "975070"
  },
  {
    "text": "beautiful I get the sum I only open that that file that has the amount and Macri",
    "start": "975070",
    "end": "980529"
  },
  {
    "text": "is really really fast when I do I want a some by-product now I'm using the second",
    "start": "980529",
    "end": "987820"
  },
  {
    "text": "column of the third key and it doesn't help me I have to scan the whole table and because of parallelism and brute",
    "start": "987820",
    "end": "994540"
  },
  {
    "text": "force share nothing class it's not that slow but I have to scan the whole tape is linear cost I have to scan the whole",
    "start": "994540",
    "end": "1001230"
  },
  {
    "text": "table the interleaved start key instead of sorting by one column then by another",
    "start": "1001230",
    "end": "1006990"
  },
  {
    "text": "then by another what we do is we get all the columns you declare on your sort key",
    "start": "1006990",
    "end": "1012360"
  },
  {
    "text": "and we get bits information from all of them in a complex math equation and we",
    "start": "1012360",
    "end": "1019230"
  },
  {
    "text": "ordered a table by that result of that math equation in our example now my",
    "start": "1019230",
    "end": "1025520"
  },
  {
    "text": "customer ID is spread across two blocks and my product ID is a scare about two",
    "start": "1025520",
    "end": "1031079"
  },
  {
    "text": "blocks they have a like a two dimension way of entry on the table so I mixed up proud of that information and customer",
    "start": "1031079",
    "end": "1037350"
  },
  {
    "text": "the information on a new value looking at my distribution of data to find the",
    "start": "1037350",
    "end": "1043319"
  },
  {
    "text": "best mathematical function that will represent the new number in order by that when a query by product IG I hit",
    "start": "1043319",
    "end": "1050850"
  },
  {
    "text": "two blocks not for when a query by customer ID I hit two blocks not one anymore",
    "start": "1050850",
    "end": "1055980"
  },
  {
    "text": "however if I hit by customer ID and Product ID I go straight to a single",
    "start": "1055980",
    "end": "1062160"
  },
  {
    "text": "block so it's a great feature if you have tables where sometimes you create",
    "start": "1062160",
    "end": "1067590"
  },
  {
    "start": "1065000",
    "end": "1065000"
  },
  {
    "text": "by sometimes you worry by customers sometimes you worry by region and so forth it can be very helpful many other",
    "start": "1067590",
    "end": "1074610"
  },
  {
    "text": "database what they do is makes you create another index or a projection or a joint index however they call it but",
    "start": "1074610",
    "end": "1081420"
  },
  {
    "text": "it's redundant data on this case there's no redundant data the data is ordered in",
    "start": "1081420",
    "end": "1086730"
  },
  {
    "text": "a different way and mathematics makes us the scan and a log in fashion instead of",
    "start": "1086730",
    "end": "1092160"
  },
  {
    "text": "linear on the table the way to use the feature you just add the interleaved",
    "start": "1092160",
    "end": "1098490"
  },
  {
    "text": "keyword on when you create the table you can have up to eight columns on the",
    "start": "1098490",
    "end": "1104160"
  },
  {
    "text": "interleaved search key and of course the more columns you add the less each one gets relevance on the on the on the",
    "start": "1104160",
    "end": "1111450"
  },
  {
    "text": "scheme of things and the data as you first copy into the table we analyze the",
    "start": "1111450",
    "end": "1117360"
  },
  {
    "text": "data to create that special function over time if the distribution of your",
    "start": "1117360",
    "end": "1122550"
  },
  {
    "text": "customer IDs and product is changed too much you get skew on the function and we have",
    "start": "1122550",
    "end": "1128610"
  },
  {
    "text": "a way to monitor that so if you look on the SVD interleaved columns view on the",
    "start": "1128610",
    "end": "1133680"
  },
  {
    "text": "interleaved skill when that number grows like above five it's time to run a",
    "start": "1133680",
    "end": "1138750"
  },
  {
    "text": "command called vacuum reindex which is going to happen is we're going to reread all the data reanalyze the distributions",
    "start": "1138750",
    "end": "1145560"
  },
  {
    "text": "you get the best mathematical function and then restart the table by that particular value and then you get",
    "start": "1145560",
    "end": "1152310"
  },
  {
    "text": "performance back again very useful feature we are we are getting started",
    "start": "1152310",
    "end": "1158520"
  },
  {
    "text": "with the feature make it better make it more useful making faster to do different things there is a load penalty",
    "start": "1158520",
    "end": "1164640"
  },
  {
    "text": "because there's some mathematical calculations that need to happen but it's I highly recommend you try to",
    "start": "1164640",
    "end": "1170640"
  },
  {
    "text": "see not normally this feature is not very conducive for range scans for date so if your main filter is like a date",
    "start": "1170640",
    "end": "1178440"
  },
  {
    "text": "range that might not be for you but you should try just to make sure that all your generic queries work better or",
    "start": "1178440",
    "end": "1185280"
  },
  {
    "text": "worse with that feature now let's talk about migrating your existing workloads",
    "start": "1185280",
    "end": "1192530"
  },
  {
    "text": "if you don't get anything else out of here don't for cliff your system because",
    "start": "1192530",
    "end": "1198990"
  },
  {
    "text": "it going to be fast to get create new tables alright she's loaded at the point your system to",
    "start": "1198990",
    "end": "1204090"
  },
  {
    "text": "read chieftain okay I'm done right you're gonna spend a lot of time tuning and troubleshooting when things go wrong don't do that the problem is a lot of",
    "start": "1204090",
    "end": "1212130"
  },
  {
    "start": "1212000",
    "end": "1212000"
  },
  {
    "text": "legacy data warehouses have some patterns of usage that people have used for 10z tens of years and they are",
    "start": "1212130",
    "end": "1218790"
  },
  {
    "text": "actually anti patterns for red shift for example I've used to load large day with",
    "start": "1218790",
    "end": "1224880"
  },
  {
    "text": "a single file extract a single file and load to the final system and we already talked about single file not good for a",
    "start": "1224880",
    "end": "1231120"
  },
  {
    "text": "load or I run many updates on my data I load my date and then a massage massage",
    "start": "1231120",
    "end": "1236550"
  },
  {
    "text": "massage and then okay now it's good instead of doing a one step processing that's more complex hard to write but",
    "start": "1236550",
    "end": "1243420"
  },
  {
    "text": "you do a single step you massage the date and that's anti parent for my chief because updates we're going to talk",
    "start": "1243420",
    "end": "1249300"
  },
  {
    "text": "about that or every job clears all the data for the day before loading in I'm not sure if I load it yet or not so I'm",
    "start": "1249300",
    "end": "1256050"
  },
  {
    "text": "just gonna clear the data you should know if you load your data or not you should not just clear it's like I'm not",
    "start": "1256050",
    "end": "1261840"
  },
  {
    "text": "sure if I have breakfast or not so I'm just gonna anyway you get it here or you",
    "start": "1261840",
    "end": "1267810"
  },
  {
    "text": "count on primary keys to avoid you from duplicating your data again ratch if",
    "start": "1267810",
    "end": "1273300"
  },
  {
    "text": "does not enforce primary keys yet and you should make sure you know what you load in your data the other thing it's a",
    "start": "1273300",
    "end": "1280380"
  },
  {
    "text": "normal pattern on other legacy data house is that you have a high concurrency of load jobs and the reason",
    "start": "1280380",
    "end": "1287460"
  },
  {
    "text": "is a single job on a regular legacy data house cannot use all your hardware so if",
    "start": "1287460",
    "end": "1293040"
  },
  {
    "text": "you load one table at a time you're never going to be done so you have to load 10 20 200 2000 tables at a time",
    "start": "1293040",
    "end": "1299490"
  },
  {
    "text": "because then you the different jobs are kind of using more the harder that's the way for you to utilize the hardware you",
    "start": "1299490",
    "end": "1305550"
  },
  {
    "text": "bought and you're paying for on red chip it's not quite that Andra chief a single job can use the whole cluster Ohio all",
    "start": "1305550",
    "end": "1312930"
  },
  {
    "text": "CPU or memory and it might be bad that you let happen this way we're gonna talk a little bit about that in a few seconds",
    "start": "1312930",
    "end": "1319830"
  },
  {
    "text": "and the other thing which is a big pet peeve of mine is you create a small",
    "start": "1319830",
    "end": "1324930"
  },
  {
    "text": "control table to control your load process inside the target data house database where you do a single row",
    "start": "1324930",
    "end": "1331440"
  },
  {
    "text": "insert a single row update and then a single update again and similar update and then I insert and",
    "start": "1331440",
    "end": "1337830"
  },
  {
    "text": "then another delete and over time those transactions are expensive in redshift",
    "start": "1337830",
    "end": "1343170"
  },
  {
    "text": "and you get in a way of actually loading your data we're going to talk a little bit about that as well when you start",
    "start": "1343170",
    "end": "1351390"
  },
  {
    "start": "1351000",
    "end": "1351000"
  },
  {
    "text": "your process of migrating to redshift I want you to ask two questions first why",
    "start": "1351390",
    "end": "1357120"
  },
  {
    "text": "you do what you do people may not know or it might be a limitation that it",
    "start": "1357120",
    "end": "1362610"
  },
  {
    "text": "doesn't exist anymore or might be a limitation that you know the new system does not have right you should know why",
    "start": "1362610",
    "end": "1368490"
  },
  {
    "text": "you do what you do we've always done this way why I don't know you have to",
    "start": "1368490",
    "end": "1373650"
  },
  {
    "text": "know the other thing what your customer needs the system that was developed ten",
    "start": "1373650",
    "end": "1379470"
  },
  {
    "text": "years ago may not meet their requirements anymore since you're gonna get a new system why not get the fresh",
    "start": "1379470",
    "end": "1384660"
  },
  {
    "text": "requirements understand what they need now so it implemented and on the process you might even benefit from using other",
    "start": "1384660",
    "end": "1390810"
  },
  {
    "text": "AWS services that can make the whole package much more attractive for your customer so on redshift why those things",
    "start": "1390810",
    "end": "1401730"
  },
  {
    "start": "1399000",
    "end": "1399000"
  },
  {
    "text": "are ant patterns an update on redshift is a delete and insert there's no real updates blocks",
    "start": "1401730",
    "end": "1407520"
  },
  {
    "text": "are immutable and of course the deletes themselves are not real deletes we mark the row for deletion and later on vacuum",
    "start": "1407520",
    "end": "1415260"
  },
  {
    "text": "we clear the data so it can be expensive to update over and over again many rows",
    "start": "1415260",
    "end": "1421290"
  },
  {
    "text": "a single column of a table the blocks are immutable the minimum space used by",
    "start": "1421290",
    "end": "1428280"
  },
  {
    "text": "a table is one block per slice per column blocks are one megabyte you do",
    "start": "1428280",
    "end": "1434580"
  },
  {
    "text": "the math so you want to load thousands of rows at a time not single row operations because then you add a block",
    "start": "1434580",
    "end": "1440250"
  },
  {
    "text": "every time first slice per column and it becomes expensive over at the end because your table grows a lot and",
    "start": "1440250",
    "end": "1446550"
  },
  {
    "text": "there's not a lot of data there commit some red chief are expensive because they are designed for batch processing",
    "start": "1446550",
    "end": "1452270"
  },
  {
    "text": "every commit is about four gigabytes right on each node on an eight Excel",
    "start": "1452270",
    "end": "1457620"
  },
  {
    "text": "node it mirrors the whole dictionary from the leader node to the compute nodes for recovery just in case you",
    "start": "1457620",
    "end": "1463770"
  },
  {
    "text": "break right after the commit and it cluster wide sterilized so there's no suggesting",
    "start": "1463770",
    "end": "1469260"
  },
  {
    "text": "small commits on redshift you want to make the most of it bet your work and do",
    "start": "1469260",
    "end": "1474390"
  },
  {
    "text": "many steps and then a single commit on",
    "start": "1474390",
    "end": "1480030"
  },
  {
    "text": "red chip aggregates can be really really really fast especially if the first column of the group by matches the",
    "start": "1480030",
    "end": "1487200"
  },
  {
    "text": "distribution key because then we can do a single step aggregation all the data is already on the same node if your that",
    "start": "1487200",
    "end": "1494880"
  },
  {
    "text": "does not true you might want to do a pre aggregation to reduce the amount of data that goes across nodes or the order of",
    "start": "1494880",
    "end": "1502860"
  },
  {
    "text": "your group I can change your performance because if the first call on the blue bar is not the distribution key but the",
    "start": "1502860",
    "end": "1508410"
  },
  {
    "text": "second is you just flip them in order to the difference try me on that concurrent",
    "start": "1508410",
    "end": "1514530"
  },
  {
    "text": "red shift should be low especially for load jobs because every job will have",
    "start": "1514530",
    "end": "1520080"
  },
  {
    "text": "the opportunity to use the whole cluster use the whole hardware paid for for reads then you have heavy reads and",
    "start": "1520080",
    "end": "1527490"
  },
  {
    "text": "small reads and then small reads can play together well with other reads go at the same time but very heavy reads",
    "start": "1527490",
    "end": "1533700"
  },
  {
    "text": "the last concurrence you do the faster the whole thing runs so instead of thinking about how long it takes to run",
    "start": "1533700",
    "end": "1539820"
  },
  {
    "text": "one job or thinking about how long it takes how many jobs that can submit at the same time think about how long it",
    "start": "1539820",
    "end": "1546870"
  },
  {
    "text": "takes to run the 2,000 jobs I have to run today and you can tune it up the concurrency one or time two at a time",
    "start": "1546870",
    "end": "1552690"
  },
  {
    "text": "until you find the ideal usually the writes would be good at local currency and small reads can be good at high",
    "start": "1552690",
    "end": "1559890"
  },
  {
    "text": "concurrency and large queries is somewhere in between and you can test this if you have a dashboard that you",
    "start": "1559890",
    "end": "1567540"
  },
  {
    "text": "want to connect against red shift make sure you have a caching layer in between to protect the cluster from repeated",
    "start": "1567540",
    "end": "1573660"
  },
  {
    "text": "queries of the exact same data that will take a long way and you may not you can",
    "start": "1573660",
    "end": "1578850"
  },
  {
    "text": "save money by having smaller cluster with a cache on top they have to have a huge cluster so you can get the",
    "start": "1578850",
    "end": "1584160"
  },
  {
    "text": "performance and of course think about dashboards I think of a quick site who",
    "start": "1584160",
    "end": "1590130"
  },
  {
    "text": "is looking a quick site already it's a great tool and the the spice thing in the middle can be a great thing to put",
    "start": "1590130",
    "end": "1595920"
  },
  {
    "text": "in front of redshift I wryly recommend you look into that it was made for redshift now workload management on",
    "start": "1595920",
    "end": "1602440"
  },
  {
    "text": "cheese we call mm it parses only Ram two different queries so you should configure different queues for different",
    "start": "1602440",
    "end": "1609340"
  },
  {
    "text": "usages and there's a new feature that's in right now then I'm going to talk to you about when you create data from",
    "start": "1609340",
    "end": "1618010"
  },
  {
    "text": "redshift sometimes you need to get not only the data but the metadata about the date what the name of the columns so you",
    "start": "1618010",
    "end": "1624160"
  },
  {
    "text": "run select or Java you can't get that easily from a query no problem on redshift when you load extract a lot of",
    "start": "1624160",
    "end": "1631450"
  },
  {
    "text": "data out we recommend using unload which in parallel write files directly from the compute nodes into s3 however we",
    "start": "1631450",
    "end": "1640930"
  },
  {
    "text": "don't give column names on the files yet and that's as far as I'm gonna go on that but what people do is they run the",
    "start": "1640930",
    "end": "1649510"
  },
  {
    "text": "unload and then they run the same query again with one equals zero at the end so",
    "start": "1649510",
    "end": "1654580"
  },
  {
    "text": "he can people can get the metadata of the of the columns the names of the columns and things the problem is very",
    "start": "1654580",
    "end": "1661270"
  },
  {
    "text": "complex queries by the time to optimize a guest to the equals one equals zero in",
    "start": "1661270",
    "end": "1667360"
  },
  {
    "text": "already create temporary tables already loaded a launch of data and it kind of takes that the same time as the original query took instead until we fix this you",
    "start": "1667360",
    "end": "1676570"
  },
  {
    "text": "can do a select top 0 because the optimizer we know right off the bat is a fake select and it runs much faster we",
    "start": "1676570",
    "end": "1686080"
  },
  {
    "start": "1684000",
    "end": "1684000"
  },
  {
    "text": "made available recently a number of open source tools for you to manage your",
    "start": "1686080",
    "end": "1691390"
  },
  {
    "text": "database I'm a 25-year DBA and I'd like to have my scripts to do things on the database and I got those scripts along",
    "start": "1691390",
    "end": "1698620"
  },
  {
    "text": "with other people that helped a lot and put this together and made them available to everybody so we have",
    "start": "1698620",
    "end": "1704830"
  },
  {
    "text": "multiple sections that are highly recommend you keep your eyes on this it keeps growing keeps adding new things and make them better",
    "start": "1704830",
    "end": "1710950"
  },
  {
    "text": "we have atom in scripts with a bunch of tuning and monitoring for the database views to get DTLS out of table",
    "start": "1710950",
    "end": "1718870"
  },
  {
    "text": "permissions things like that column encodings to for analyzing vacuum a to",
    "start": "1718870",
    "end": "1725380"
  },
  {
    "text": "for unload and copy moved it from one class to another cluster I highly recommend to take a look on that the",
    "start": "1725380",
    "end": "1730900"
  },
  {
    "text": "bottom three are Python written tools you can get the source code change make it yours make it",
    "start": "1730900",
    "end": "1736269"
  },
  {
    "text": "better give us suggestions give us a this is broken we're gonna fix it make contributions we love to have your code",
    "start": "1736269",
    "end": "1743289"
  },
  {
    "text": "being there too now for dead means scrapes I'm gonna show you a few of them so you can have an idea of how they work",
    "start": "1743289",
    "end": "1749669"
  },
  {
    "text": "so imagine you want to tune your workload you notice it's not quite",
    "start": "1749669",
    "end": "1754719"
  },
  {
    "start": "1750000",
    "end": "1750000"
  },
  {
    "text": "running as fast as you want so the first thing I look top queries what are the",
    "start": "1754719",
    "end": "1760359"
  },
  {
    "text": "queries that are using most of my time on my cluster either because a single query like the first one takes a long",
    "start": "1760359",
    "end": "1766209"
  },
  {
    "text": "time or the second one there where it rains too many times it's fast on itself",
    "start": "1766209",
    "end": "1771729"
  },
  {
    "text": "but runs too many times and then you can go and see how can I fix those queries of course on the right inside on the",
    "start": "1771729",
    "end": "1778749"
  },
  {
    "text": "event column there on the right you see there is some of them that shares future or stats those are performance alerts",
    "start": "1778749",
    "end": "1785799"
  },
  {
    "text": "where the system's telling you what is wrong with the query it missing stats or the future is not appropriate for the",
    "start": "1785799",
    "end": "1791919"
  },
  {
    "text": "sort key of the table another way of doing look at your performance is looking at the summary of your alerts",
    "start": "1791919",
    "end": "1799119"
  },
  {
    "text": "related to a particular table so you summarize all your alerts for a table",
    "start": "1799119",
    "end": "1804219"
  },
  {
    "text": "and also add the number of minutes you spend scanning that table so you have a",
    "start": "1804219",
    "end": "1809289"
  },
  {
    "text": "prioritized order which tables you can fix and if you fix them everybody that",
    "start": "1809289",
    "end": "1814539"
  },
  {
    "text": "use the table benefits from the performance improvement for example this particular flights table needs a new",
    "start": "1814539",
    "end": "1820029"
  },
  {
    "text": "sort key or maybe need to uncompress the sort key I don't know we have to look specific is there but then it gets a",
    "start": "1820029",
    "end": "1825459"
  },
  {
    "text": "point where to start your work another way of performance tuning is to making",
    "start": "1825459",
    "end": "1831759"
  },
  {
    "start": "1829000",
    "end": "1829000"
  },
  {
    "text": "sure that not my careers are running fast but actually I'm not waiting queue if I'm submitting more queries that can",
    "start": "1831759",
    "end": "1837849"
  },
  {
    "text": "run at the same time if you look on the WM apex hourly it's gonna give you perk",
    "start": "1837849",
    "end": "1842979"
  },
  {
    "text": "you and per hour what's the pressure on the queue my queue is 5 I have 10 people",
    "start": "1842979",
    "end": "1848169"
  },
  {
    "text": "waiting line maybe the queue needs to be adjusted right and now that we allow you",
    "start": "1848169",
    "end": "1853389"
  },
  {
    "text": "to change the memory and the concurrency of the workload management on the fly without bouncing the cluster you can",
    "start": "1853389",
    "end": "1860079"
  },
  {
    "text": "change those parameters throughout the day for example you're going to start your load process you give more memory",
    "start": "1860079",
    "end": "1865419"
  },
  {
    "text": "to the queue number one there and then when your ETL ends you back to 5050 for example so it's an",
    "start": "1865419",
    "end": "1872200"
  },
  {
    "text": "example right now this is all to point you to a particular location and where",
    "start": "1872200",
    "end": "1877960"
  },
  {
    "text": "to look now you're gonna ask me how do I tune a specific query how do I know",
    "start": "1877960",
    "end": "1882999"
  },
  {
    "text": "explain plane is kind of fuzzy I kind of know how to read the explain plane but those queries are big right how do I",
    "start": "1882999",
    "end": "1890080"
  },
  {
    "text": "know exactly what's going on so I'm gonna show you something we launched some time ago that I know many of you",
    "start": "1890080",
    "end": "1895299"
  },
  {
    "text": "haven't used yet it's the console and query detail for the console so imagine you have a query",
    "start": "1895299",
    "end": "1901869"
  },
  {
    "start": "1901000",
    "end": "1901000"
  },
  {
    "text": "that runs on the console that has the query and they explain play on the bottom but you don't know each of the",
    "start": "1901869",
    "end": "1906879"
  },
  {
    "text": "steps really took longer but then you click on the actual tab and voila you have a list of all the explained plane",
    "start": "1906879",
    "end": "1913659"
  },
  {
    "text": "steps and how long each one took if they are unbalanced or not balance is skewed",
    "start": "1913659",
    "end": "1919059"
  },
  {
    "text": "and you can't even get details about a step like how many megabytes Oh 29",
    "start": "1919059",
    "end": "1924309"
  },
  {
    "text": "gigabytes of data transmitted that's why that step was slow and you can figure out what's going on it's a very good",
    "start": "1924309",
    "end": "1930609"
  },
  {
    "text": "tool when you've narrowed down to one query what's wrong with that query and then you can go and fix it",
    "start": "1930609",
    "end": "1936309"
  },
  {
    "text": "and understand exactly where who is contributing where you need to fix sometimes multiple steps look bad",
    "start": "1936309",
    "end": "1943480"
  },
  {
    "text": "Oh distribute data across the network that must be bad well if it's 10 bytes is ok if it's 10 megabytes I'm ok 10",
    "start": "1943480",
    "end": "1950080"
  },
  {
    "text": "gigabytes that is really bad so this can tell you exactly what's going on which step of decree took longest and help you",
    "start": "1950080",
    "end": "1956769"
  },
  {
    "text": "figure out and even if there's alerts they're gonna show here a these tables missing stats that's what your problem",
    "start": "1956769",
    "end": "1962440"
  },
  {
    "text": "right so I highly recommend you take a look on this and I hope you can you can enjoy tuning on red shift all the",
    "start": "1962440",
    "end": "1969580"
  },
  {
    "text": "performs those customers show in the beginning now it's my pleasure to bring",
    "start": "1969580",
    "end": "1974700"
  },
  {
    "text": "Arie Miller from Cupid device we're gonna tell you how they created their environment to get the performance they",
    "start": "1974700",
    "end": "1981850"
  },
  {
    "text": "needed from redshift",
    "start": "1981850",
    "end": "1984840"
  },
  {
    "text": "so I'm gonna talk for about 15 minutes and then we'll have ten minutes at the end for questions because I know how I",
    "start": "1990980",
    "end": "1996870"
  },
  {
    "text": "have some follow-up questions so where this presentation is about sharing",
    "start": "1996870",
    "end": "2002690"
  },
  {
    "text": "Amazon redshift so it's how do you maximize your particular cluster especially with a whole bunch of",
    "start": "2002690",
    "end": "2007789"
  },
  {
    "text": "different users using it it's a variety of patterns we're using some tools that were in the process of open sourcing",
    "start": "2007789",
    "end": "2012980"
  },
  {
    "text": "hopefully you guys will recognize some of these problems and be able to benefit from some of the patterns that we're",
    "start": "2012980",
    "end": "2018080"
  },
  {
    "text": "using or some of the tools we've built so a brief introduction tripAdvisor's",
    "start": "2018080",
    "end": "2023260"
  },
  {
    "start": "2021000",
    "end": "2021000"
  },
  {
    "text": "375 million unique monthly visitors we're using a fairly small cluster it's an eight node ds2 8x large our largest",
    "start": "2023260",
    "end": "2031100"
  },
  {
    "text": "table is 12 terabytes it's got about a month of data and everything for us is time series so we're really not about",
    "start": "2031100",
    "end": "2037159"
  },
  {
    "text": "real-time analytics most of our use of redshift is about understanding user behavior what people are looking for and",
    "start": "2037159",
    "end": "2042980"
  },
  {
    "text": "how we can better serve them we have a big shared resource problem which is we",
    "start": "2042980",
    "end": "2049730"
  },
  {
    "text": "have about 460 engineers data scientists analysts and product managers all of whom have personal logins to redshift",
    "start": "2049730",
    "end": "2055850"
  },
  {
    "text": "they've created more than 2,500 tables and our experience has been that if you",
    "start": "2055850",
    "end": "2061250"
  },
  {
    "text": "don't build it be an infrastructure or whatever they're still going to come the company motto is speed wins there's no",
    "start": "2061250",
    "end": "2067790"
  },
  {
    "text": "tolerance for delay in analysis if we don't make the correct path the easiest",
    "start": "2067790",
    "end": "2073760"
  },
  {
    "text": "path they're still gonna find a way to accomplish it it will just be accomplished in a way that interferes with the other 459 users my favorite",
    "start": "2073760",
    "end": "2082190"
  },
  {
    "text": "recent example was one of our data scientists needed to bring over a 293 million row table and we hadn't",
    "start": "2082190",
    "end": "2087980"
  },
  {
    "text": "communicated as well as we should have how you would do that and so they noticed that you could just use JDBC and",
    "start": "2087980",
    "end": "2094190"
  },
  {
    "text": "do a bunch of insert statements and it worked but it kind of crippled some of",
    "start": "2094190",
    "end": "2100250"
  },
  {
    "text": "the underlying redshift used for query monitoring because you now have 293 million rows and those underlying tables",
    "start": "2100250",
    "end": "2106369"
  },
  {
    "text": "for each individual insert so those are the kind of things that happen unless you establish the patterns and the tools",
    "start": "2106369",
    "end": "2113150"
  },
  {
    "text": "that users need they will find a way to do it anyway so our solution",
    "start": "2113150",
    "end": "2119460"
  },
  {
    "start": "2117000",
    "end": "2117000"
  },
  {
    "text": "brutally break down into four points reduce contention so where you're doing something expensive that you control",
    "start": "2119460",
    "end": "2125540"
  },
  {
    "text": "find a way to do it on a custom cluster that's really effective for what you're trying to accomplish and I'll talk",
    "start": "2125540",
    "end": "2131069"
  },
  {
    "text": "specifically about the types of custom clusters we use for different types of tasks ad infrastructure and automation",
    "start": "2131069",
    "end": "2136680"
  },
  {
    "text": "add a lot of monitoring tables performance and usage and where you can",
    "start": "2136680",
    "end": "2141809"
  },
  {
    "text": "if you know everyone's trying to accomplish some similar goals do as much of that as you can yourself so that they",
    "start": "2141809",
    "end": "2147869"
  },
  {
    "text": "can then benefit in just query instead of doing the ETL you know and how five people doing the same ETL so for custom",
    "start": "2147869",
    "end": "2155640"
  },
  {
    "start": "2155000",
    "end": "2155000"
  },
  {
    "text": "computation clusters your goal there is to stay out of the way and our favorite type of custom computation cluster from",
    "start": "2155640",
    "end": "2161609"
  },
  {
    "text": "a performance and cost perspective it's what we call a Mighty Mouse cluster and that's a specialized ephemeral cluster",
    "start": "2161609",
    "end": "2169380"
  },
  {
    "text": "the Mighty Mouse one in particular is 32 DC one large nodes so that's great bang",
    "start": "2169380",
    "end": "2174420"
  },
  {
    "text": "for the buck in terms of a lot of computation cases it's $8 an hour for the whole cluster for all 32 nodes for a",
    "start": "2174420",
    "end": "2183089"
  },
  {
    "text": "four hour job on 8d one asses you could end up doing it in 40 minutes on a Mighty Mouse cluster so you can get some",
    "start": "2183089",
    "end": "2189599"
  },
  {
    "text": "real performance wins the d2 is for us we've seen 50 to 200 percent performance",
    "start": "2189599",
    "end": "2194910"
  },
  {
    "text": "improvements compared to the D ones thus Eric's advice to if you're still in the D ones get off so much smaller wins",
    "start": "2194910",
    "end": "2201450"
  },
  {
    "text": "versus the d 2s but if you look at performance per cost you can still see 3x better performance per cost for a lot",
    "start": "2201450",
    "end": "2208680"
  },
  {
    "text": "of use cases compared to sort of the bigger nodes so we had seven of these 32 node clusters running last week doing",
    "start": "2208680",
    "end": "2215130"
  },
  {
    "text": "some custom computation and we ended up saving it's about three times cheaper than it would have been how we done the",
    "start": "2215130",
    "end": "2220559"
  },
  {
    "text": "same computation on the D twos some of the wins are getting or that you can customize a configuration of these",
    "start": "2220559",
    "end": "2227160"
  },
  {
    "text": "ephemeral clusters so as Eric was saying redshift is really good at paralyzing",
    "start": "2227160",
    "end": "2232650"
  },
  {
    "text": "and operating a single query at a time in spreading the workload over nodes so we'll a lot of times run with two slots",
    "start": "2232650",
    "end": "2239010"
  },
  {
    "text": "only to maximize the amount of memory available to each query and that's been very effective on us you're just relying",
    "start": "2239010",
    "end": "2245040"
  },
  {
    "text": "on the underlying technology to parallelize across the nodes you do have",
    "start": "2245040",
    "end": "2250109"
  },
  {
    "text": "to be careful right if you're making any mistake in your query that are causing nodes to need to talk to one another more than",
    "start": "2250109",
    "end": "2255540"
  },
  {
    "text": "they have to that blows up when you're dealing with 32 nodes at a time so you have to experiment make sure you're",
    "start": "2255540",
    "end": "2261000"
  },
  {
    "text": "doing things in an optimized way but it actually is kind of a good way canary in a coalmine style to really know that",
    "start": "2261000",
    "end": "2266220"
  },
  {
    "text": "you're not doing something effective with a query because while you might get away with it with 8 nodes if there's you",
    "start": "2266220",
    "end": "2271349"
  },
  {
    "text": "know bad distribution keys or too much node communication it really blows up once you start talking about 32 nodes at",
    "start": "2271349",
    "end": "2277859"
  },
  {
    "text": "a time the primary limitation we found with these custom computation clusters is the size of the copy",
    "start": "2277859",
    "end": "2284970"
  },
  {
    "text": "so with 32 of these nodes you have 5 terabytes of storage but if you're trying to bring in for us anything more",
    "start": "2284970",
    "end": "2291270"
  },
  {
    "text": "than 300 gigabytes compressed you end up in the process of bringing it in using all the available space on the cluster",
    "start": "2291270",
    "end": "2297540"
  },
  {
    "text": "and you're quite your load will fail so that's it's not a an awful limitation but you got to keep that in mind like",
    "start": "2297540",
    "end": "2303570"
  },
  {
    "text": "just because you have 5 terabytes usable on a cluster like this doesn't mean you can bring in 5 terabytes of data",
    "start": "2303570",
    "end": "2309089"
  },
  {
    "text": "compressed all at once the additional",
    "start": "2309089",
    "end": "2314670"
  },
  {
    "text": "pieces of infrastructure and as I said we're in the process of open sourcing some of these if you're going to use custom computation clusters you don't",
    "start": "2314670",
    "end": "2321150"
  },
  {
    "text": "want to rebuild your cluster every time you want tools that let you say ok I need this schema from my main cluster on",
    "start": "2321150",
    "end": "2327119"
  },
  {
    "text": "this ephemeral cluster so we have some of those tools sync redshift objects is basically a way to bring over entire",
    "start": "2327119",
    "end": "2333930"
  },
  {
    "text": "schemas to an ephemeral cluster from your main cluster we have specific unload and copy for transferring the",
    "start": "2333930",
    "end": "2340470"
  },
  {
    "text": "data back and forth so it sounds like some of the tools that Eric's already open source might be more effective than that one key thing to keep in mind is",
    "start": "2340470",
    "end": "2347790"
  },
  {
    "text": "use a manifest file wherever possible with analytics it's much better to fail",
    "start": "2347790",
    "end": "2352859"
  },
  {
    "text": "than it is to lie and a really easy way to lie is to get into trouble with",
    "start": "2352859",
    "end": "2358020"
  },
  {
    "text": "eventual consistency in the standard region and only be you know only have some of your files that got unloaded",
    "start": "2358020",
    "end": "2363960"
  },
  {
    "text": "available at the time you do the copy so I heavily recommend whenever you're doing it unload or ever whenever you're",
    "start": "2363960",
    "end": "2369810"
  },
  {
    "text": "doing a copy you take advantage of the manifest file option don't find out later that you've come to the wrong",
    "start": "2369810",
    "end": "2375420"
  },
  {
    "text": "business decision because you only had you know 80% of your data at the time you copied back in the next bit is",
    "start": "2375420",
    "end": "2384390"
  },
  {
    "start": "2382000",
    "end": "2382000"
  },
  {
    "text": "infrastructure in automation in the easiest path there is to start with your engineers so what can you add that",
    "start": "2384390",
    "end": "2391599"
  },
  {
    "text": "lets engineers take advantage of things you've built for yourself already and one of the things that we're using is",
    "start": "2391599",
    "end": "2398469"
  },
  {
    "text": "something that LinkedIn open source called azkaban and that's a batch workflow and UI and scheduler and it's",
    "start": "2398469",
    "end": "2403809"
  },
  {
    "text": "really very easy for people to adopt it's nice you can basically divide your",
    "start": "2403809",
    "end": "2409029"
  },
  {
    "text": "logic for bringing data over or running sequel into a series of jobs and it'll create visual dependencies for you you",
    "start": "2409029",
    "end": "2416319"
  },
  {
    "text": "can parameterize running your jobs you can parallelize running them you can visually see when things are happening",
    "start": "2416319",
    "end": "2422109"
  },
  {
    "text": "it gives you a list of how long things takes you get a whole bunch of things baked in and it's a very easy adoption",
    "start": "2422109",
    "end": "2428499"
  },
  {
    "text": "for engineers so these jobs get run and Azkaban is calling the data transfer infrastructure",
    "start": "2428499",
    "end": "2435009"
  },
  {
    "text": "that we've already built so that's been very effective for us in terms of sket as a scheduler and understanding",
    "start": "2435009",
    "end": "2440469"
  },
  {
    "text": "dependencies and making sure things happen in the right order and to Eric's point we actually have a normal Postgres",
    "start": "2440469",
    "end": "2446919"
  },
  {
    "text": "admin database that's tracking what ran what runs when we're not trying to have",
    "start": "2446919",
    "end": "2452469"
  },
  {
    "text": "a small control table that we're doing lots of individual rights to in sort of a transactional way that's living on red",
    "start": "2452469",
    "end": "2459339"
  },
  {
    "text": "ship so that's another pattern that you might want to make sure to copy once",
    "start": "2459339",
    "end": "2465489"
  },
  {
    "text": "you've handled the engineer problem and you've given them a pathway for writing custom flows and we're at about 70",
    "start": "2465489",
    "end": "2470709"
  },
  {
    "text": "individual flows at different engineering teams have created at this point with their own independent schedules you want to go for solutions",
    "start": "2470709",
    "end": "2478390"
  },
  {
    "text": "for your data your data analysts your data scientists give them a way to have",
    "start": "2478390",
    "end": "2483849"
  },
  {
    "text": "a point-and-click UI for scheduling so custom UI for non engineers something",
    "start": "2483849",
    "end": "2489309"
  },
  {
    "text": "that allows you to import from file or hive and as I said we're in the process of trying to open-source these solutions",
    "start": "2489309",
    "end": "2495029"
  },
  {
    "text": "what we do in these cases is we ought to analyze the source data either from file or high if we suggest a table DDL sort",
    "start": "2495029",
    "end": "2502029"
  },
  {
    "text": "distribution and compression and we can schedule automated imports and automated runs of particular sequel we it was",
    "start": "2502029",
    "end": "2509829"
  },
  {
    "text": "probably a mistake to have automatically suggested compression we probably would have been better off bringing in a",
    "start": "2509829",
    "end": "2515529"
  },
  {
    "text": "subset of the data and relying on reg to suggest the compression or just not trying for that at all and relying on",
    "start": "2515529",
    "end": "2520970"
  },
  {
    "text": "automated redshift it depends whether as Eric said if you're doing something over and over again you're better off",
    "start": "2520970",
    "end": "2526130"
  },
  {
    "text": "eventually coming up with the compression through and analyze if you're just doing a one-off much better",
    "start": "2526130",
    "end": "2531710"
  },
  {
    "text": "off just not to specify compression and let redshift do it the next thing we",
    "start": "2531710",
    "end": "2537320"
  },
  {
    "start": "2536000",
    "end": "2536000"
  },
  {
    "text": "added was monitoring and the key thing to keep in mind here is you've got compatibility with Postgres so you can",
    "start": "2537320",
    "end": "2543740"
  },
  {
    "text": "throw up simple monitoring that's emailing alerts out with a simple query statement and then emailing the results",
    "start": "2543740",
    "end": "2550820"
  },
  {
    "text": "of that query which is what we're doing at the bottom so we have long-running query alerts every account that gets",
    "start": "2550820",
    "end": "2557150"
  },
  {
    "text": "created is mapped to an email address so we know who to tell if a query has gone off the rails are these different users",
    "start": "2557150",
    "end": "2563560"
  },
  {
    "text": "we have daily and peak usage reports by user and type of activity daily table size space taken up I delete a rows by",
    "start": "2563560",
    "end": "2570710"
  },
  {
    "text": "table we tend to use deep copy instead of vacuumed because by the time we see that a table is a problem we're well",
    "start": "2570710",
    "end": "2576950"
  },
  {
    "text": "over the 20% where you're gonna be feasible to vacuum and as I said the",
    "start": "2576950",
    "end": "2581990"
  },
  {
    "text": "initial monitoring cost for this type of solution it can be really really cheap all you have to do is write a query like",
    "start": "2581990",
    "end": "2588260"
  },
  {
    "text": "that piece equal quiet HTML output you go from a file do an output and then",
    "start": "2588260",
    "end": "2594080"
  },
  {
    "text": "email it out to whoever needs to be notified so it's just a very simple way of setting up alerts seeing if they have",
    "start": "2594080",
    "end": "2599900"
  },
  {
    "text": "value we've actually found a slightly better pattern is to insert the results",
    "start": "2599900",
    "end": "2605960"
  },
  {
    "text": "of the query into a date partition table so you have a record of what things were going on at what time you can build",
    "start": "2605960",
    "end": "2611690"
  },
  {
    "text": "dashboards on top of it then you select and then you email so slightly more complicated but it gives you a historical record for things that prove",
    "start": "2611690",
    "end": "2619070"
  },
  {
    "text": "themselves out on the monitoring side you can then do things like build tableau dashboards on top of your monitoring so this is you know different",
    "start": "2619070",
    "end": "2626030"
  },
  {
    "text": "size tip is is for different users is how much of the cluster resources a given users taking off so we can start",
    "start": "2626030",
    "end": "2632240"
  },
  {
    "text": "knowing who to communicate with if someone's sort of overly aggressively using different cluster resources the",
    "start": "2632240",
    "end": "2640190"
  },
  {
    "start": "2639000",
    "end": "2639000"
  },
  {
    "text": "next thing I wanted to talk about from an engineering side is a shared development cluster so you obviously",
    "start": "2640190",
    "end": "2646420"
  },
  {
    "text": "don't want to be doing most of your development on your main clusters if you're testing",
    "start": "2646420",
    "end": "2651520"
  },
  {
    "text": "we a good example for us we had people testing out and new uses of the list AG query and we found you can take up 10%",
    "start": "2651520",
    "end": "2658730"
  },
  {
    "text": "of the usable space on a fairly large cluster in about 30 minutes if you use the list add query wrong so you know you",
    "start": "2658730",
    "end": "2665660"
  },
  {
    "text": "want as much of that to happen not on your main cluster so we found can be very effective as a shared development",
    "start": "2665660",
    "end": "2671660"
  },
  {
    "text": "cluster and we use something we call Khaleesi syntax after the engineer who invented it which is a very simple way",
    "start": "2671660",
    "end": "2677150"
  },
  {
    "text": "of marking up the schemas and your query so what this lets you do is if let's say",
    "start": "2677150",
    "end": "2682940"
  },
  {
    "text": "a given flow a given sort of unit of work might operate on a particular schema to say we're looking at something",
    "start": "2682940",
    "end": "2690050"
  },
  {
    "text": "like hotel bookings what you do is you mark it up so everywhere you reference",
    "start": "2690050",
    "end": "2695420"
  },
  {
    "text": "that schema is noted like this so it's completely legitimate sequel can run as is but it's marked up in a way that lets",
    "start": "2695420",
    "end": "2701690"
  },
  {
    "text": "a tool do substitutions so when people are operating on that flow trying to engineer it and make it better on the",
    "start": "2701690",
    "end": "2707690"
  },
  {
    "text": "shared development cluster you can automatically create copies of the schema that this runs against for their",
    "start": "2707690",
    "end": "2713930"
  },
  {
    "text": "own for their own personal users so if both Jeff Bezos and Werner Vogel's wants to develop the shirt on the Sherpa flow",
    "start": "2713930",
    "end": "2720110"
  },
  {
    "text": "you basically can just have some tools that go and create them their own version of the schema and run against",
    "start": "2720110",
    "end": "2726080"
  },
  {
    "text": "that schema and use the same precursors so rather than having the overhead of spinning up an individual cluster per",
    "start": "2726080",
    "end": "2731960"
  },
  {
    "text": "user you can instead all operate on the same shared development environment and as I said we're working on open sourcing",
    "start": "2731960",
    "end": "2738260"
  },
  {
    "text": "this so it'll be sort of available if you guys want to see exactly what we did but your feel free to reach out as well there's a little bit of example code for",
    "start": "2738260",
    "end": "2745340"
  },
  {
    "text": "what we do to do that you can see it's not particularly complicated we're creating a schema we have a target",
    "start": "2745340",
    "end": "2750380"
  },
  {
    "text": "schema and then the schema for the individual user we just go through the tables we load the information and we",
    "start": "2750380",
    "end": "2756560"
  },
  {
    "text": "substitute basically for the schema so that we can recreate the entire schema and clone it over to another to another",
    "start": "2756560",
    "end": "2763100"
  },
  {
    "text": "schema that's personalized for the developer to operate on the next trick I",
    "start": "2763100",
    "end": "2769190"
  },
  {
    "start": "2767000",
    "end": "2767000"
  },
  {
    "text": "wanted to talk about is view performance this is actually less relevant because of the recent improvements that they've",
    "start": "2769190",
    "end": "2774200"
  },
  {
    "text": "made in view performance but you can also mark up views such that if the users using your tools and you note that",
    "start": "2774200",
    "end": "2782210"
  },
  {
    "text": "they're using a subsection of the that only comes from one table as I said we're using the time series pattern so",
    "start": "2782210",
    "end": "2788010"
  },
  {
    "text": "we tend to divide our tables up by months so we can drop old months without having to deal with vacuum costs and you",
    "start": "2788010",
    "end": "2794579"
  },
  {
    "text": "can find that pattern on the redshift documentation so the syntax is completely legitimate sequel against a view you can have it autocomplete if",
    "start": "2794579",
    "end": "2801480"
  },
  {
    "text": "you're developing your sequel in an IDE but your underlying framework can substitute in and run against an",
    "start": "2801480",
    "end": "2808290"
  },
  {
    "text": "individual table instead of using the view to get performance optimizations that way as I said a lot less relevant",
    "start": "2808290",
    "end": "2814650"
  },
  {
    "text": "now that the view performance is around is good for union all type views but it used to be a very large wins to avoid",
    "start": "2814650",
    "end": "2821309"
  },
  {
    "text": "doing union all running queries against Union all views final thing I wanted to",
    "start": "2821309",
    "end": "2826680"
  },
  {
    "text": "talk about is queue management so the idea there is keep it simple because the memories reserved though you no longer",
    "start": "2826680",
    "end": "2833069"
  },
  {
    "text": "need to keep it as simple because you can now dynamically move queues around without actually needing to reboot your",
    "start": "2833069",
    "end": "2838859"
  },
  {
    "text": "cluster so we have four queues for example mission-critical queries that might run a while a queue for shared",
    "start": "2838859",
    "end": "2845130"
  },
  {
    "text": "basically shared computation the DBA queue a quick queue which we use for tableau so if you need to run quick",
    "start": "2845130",
    "end": "2851160"
  },
  {
    "text": "queries to find very quick answers or for tableau dashboards where you need immediate responses but don't use a lot",
    "start": "2851160",
    "end": "2857670"
  },
  {
    "text": "of computational resources and then a default queue that everyone fell in we have a default timeout so when some user",
    "start": "2857670",
    "end": "2864390"
  },
  {
    "text": "does a Cartesian product on it you know a 12 billion row table you don't end up",
    "start": "2864390",
    "end": "2869520"
  },
  {
    "text": "bringing the whole cluster down for three hours it's only sort of slow for 30 minutes since that's been pretty",
    "start": "2869520",
    "end": "2875130"
  },
  {
    "text": "helpful to have default timeouts on the particular keys and then groups allow you to dynamically switch queues for",
    "start": "2875130",
    "end": "2881490"
  },
  {
    "text": "particular users you can associate a group with a queue and you can dynamically move users from group to",
    "start": "2881490",
    "end": "2887549"
  },
  {
    "text": "group so for environments like tableau where you can't set your query group you can",
    "start": "2887549",
    "end": "2893549"
  },
  {
    "text": "actually just have you eyes that let a tableau user move between groups depending on whether it's loading a",
    "start": "2893549",
    "end": "2898890"
  },
  {
    "text": "large extract or just servicing the needs of a dashboard so we found that to be pretty effective where users can't",
    "start": "2898890",
    "end": "2905849"
  },
  {
    "text": "set their own query groups as a way to dynamically adjust which queues users fall in so in wrap-up Amazon redshift is",
    "start": "2905849",
    "end": "2915480"
  },
  {
    "text": "transformed analytics at TripAdvisor we get thousands of queries hundreds of users and all unknot that",
    "start": "2915480",
    "end": "2921600"
  },
  {
    "text": "larger clusters you can really optimize your use of a cluster if you really spend the time to optimize it in these",
    "start": "2921600",
    "end": "2927210"
  },
  {
    "text": "ways we generally use tableau dashboards built directly on Amazon redshift and it",
    "start": "2927210",
    "end": "2932430"
  },
  {
    "text": "allows for iterative real-time exploration which is hundreds of times faster than poking around in hive or a",
    "start": "2932430",
    "end": "2938220"
  },
  {
    "text": "variety of other options and we have open LDAP access for any employee we had to do a little bit of custom work to",
    "start": "2938220",
    "end": "2943980"
  },
  {
    "text": "just sort of man automatically crate instead of manually create accounts but on that the LDAP access and ability to",
    "start": "2943980",
    "end": "2951270"
  },
  {
    "text": "plug into LDAP repositories is I think coming at some point and we're gonna be",
    "start": "2951270",
    "end": "2956490"
  },
  {
    "text": "open sourcing these solutions we're currently targeting February of 2016 but if there's anything you particularly",
    "start": "2956490",
    "end": "2961620"
  },
  {
    "text": "interested in early feel free to reach out and contact me we're just trying to sort of divorce some of our custom stuff",
    "start": "2961620",
    "end": "2966810"
  },
  {
    "text": "from some of the more generically usable aspects of the tools we've built and some of the patterns are usable now and",
    "start": "2966810",
    "end": "2973080"
  },
  {
    "text": "when we do open source it you'll be able to find the link it HTTP engineering tripadvisor.com",
    "start": "2973080",
    "end": "2979580"
  },
  {
    "text": "thank you very much",
    "start": "2979580",
    "end": "2983000"
  },
  {
    "text": "so another example a customer they started with redshift and create their own environment around it there is those",
    "start": "2988359",
    "end": "2994579"
  },
  {
    "text": "other sessions with other customers are discussing their use of redshift some of them already happened and you can go on",
    "start": "2994579",
    "end": "3000490"
  },
  {
    "text": "the archives on the video or audio or the share of the slides but the ones",
    "start": "3000490",
    "end": "3005890"
  },
  {
    "text": "that are still gonna happen I highly recommend you take a look and and see what you can learn from redshift now",
    "start": "3005890",
    "end": "3012010"
  },
  {
    "text": "we're gonna take a few questions",
    "start": "3012010",
    "end": "3014730"
  }
]