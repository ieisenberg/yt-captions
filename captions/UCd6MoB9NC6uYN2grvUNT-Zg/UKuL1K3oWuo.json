[
  {
    "start": "0",
    "end": "167000"
  },
  {
    "text": "hello everyone hello and welcome to best practices for Amazon s3 my name is Rob",
    "start": "0",
    "end": "5790"
  },
  {
    "text": "Wilson I'm a product manager on the Amazon s3 team and I'll be joined later on stage by Katie Lampkin who is a",
    "start": "5790",
    "end": "11400"
  },
  {
    "text": "software engineer at human longevity this is a company that's using data to deliver health intelligence you'll hear",
    "start": "11400",
    "end": "17070"
  },
  {
    "text": "much more about their business and how they're using s3 later on so let's jump",
    "start": "17070",
    "end": "22410"
  },
  {
    "text": "right in today we'll talk about we'll start with an overview of Amazon s3 then we'll talk about how you can manage and",
    "start": "22410",
    "end": "28349"
  },
  {
    "text": "secure your storage in s3 talk about how you can add additional levels of data protection to retain certain subsets of",
    "start": "28349",
    "end": "35370"
  },
  {
    "text": "data that are more important to you we'll talk about achieving high levels of performance on s3 and then we'll hear",
    "start": "35370",
    "end": "41040"
  },
  {
    "text": "from HLA today's talk will focus on Amazon s3 and Amazon glacier these are",
    "start": "41040",
    "end": "47430"
  },
  {
    "text": "our object storage services in AWS we also have block and file offerings which",
    "start": "47430",
    "end": "52739"
  },
  {
    "text": "have other discussions this week they have their own booth if you want to ask questions about them and then we also have data transfer tools both to enable",
    "start": "52739",
    "end": "59670"
  },
  {
    "text": "you to do large-scale migrations and then also manage the ongoing uploads and downloads the cloud we'll talk a bit",
    "start": "59670",
    "end": "65610"
  },
  {
    "text": "about transfer acceleration in particular later on so when customers",
    "start": "65610",
    "end": "71610"
  },
  {
    "text": "look at s3 they're choosing it for these reasons it's designed to be a durable object store designed for 11 lines of",
    "start": "71610",
    "end": "77369"
  },
  {
    "text": "durability you can achieve four nines of availability in s3 standard you can also scale to petabytes or exabytes using",
    "start": "77369",
    "end": "84840"
  },
  {
    "text": "Amazon s3 we look at security and compliance you have three different ways to encrypt your objects in s3 so you",
    "start": "84840",
    "end": "91350"
  },
  {
    "text": "could use server-side server-side advantage by a customer where you provide the key or you can use key",
    "start": "91350",
    "end": "96360"
  },
  {
    "text": "management service Amazon AWS key a kms to encrypt your objects we also have",
    "start": "96360",
    "end": "101729"
  },
  {
    "text": "compliance storage offerings you can achieve FedRAMP HIPAA or PCI compliance using Amazon s3 storage i'm query in",
    "start": "101729",
    "end": "109259"
  },
  {
    "text": "place we have Amazonas athena and Amazon redshift spectrum both of which allow",
    "start": "109259",
    "end": "114479"
  },
  {
    "text": "you to analyze data using sequel across s3 and also taking in data from your",
    "start": "114479",
    "end": "120420"
  },
  {
    "text": "data warehouse as well with redshift spectrum without requiring you to extract or load that data into any other",
    "start": "120420",
    "end": "126329"
  },
  {
    "text": "service we have flexible management tools we'll talk today about object tags storage class analysis and s",
    "start": "126329",
    "end": "132959"
  },
  {
    "text": "inventory so these are management tools in addition to cross region replication that you can use to get better",
    "start": "132959",
    "end": "138510"
  },
  {
    "text": "visibility on your storage to automate monitor set alarms for cross region replication you can replicate your data",
    "start": "138510",
    "end": "144810"
  },
  {
    "text": "across AWS regions so you have a lot of flexibility there on the management front and then our eco system consists",
    "start": "144810",
    "end": "151170"
  },
  {
    "text": "of tens of thousands of consulting system integrator and ISV partners you also are highly integrated with the rest",
    "start": "151170",
    "end": "157680"
  },
  {
    "text": "of AWS services so you can use your storage in it Amazon s3 and use it with many other services some of which we'll",
    "start": "157680",
    "end": "164310"
  },
  {
    "text": "be launching this week you have a choice of storage classes when you choose",
    "start": "164310",
    "end": "169439"
  },
  {
    "start": "167000",
    "end": "167000"
  },
  {
    "text": "Amazon s3 this ranges from s3 standard to s3 standard infrequent access to",
    "start": "169439",
    "end": "174569"
  },
  {
    "text": "Amazon glacier these all have different cost profiles so you see that Amazon standard is from 2.1 cents per gigabyte",
    "start": "174569",
    "end": "181109"
  },
  {
    "text": "per month all the way to glacier at 0.4 cents per gigabyte per month what you're",
    "start": "181109",
    "end": "186150"
  },
  {
    "text": "trading off when you choose between these storage classes and the different characteristics you're going to see is about data retrieval times and also data",
    "start": "186150",
    "end": "194340"
  },
  {
    "text": "retrieval fees so with s3 standard there's no retrieval fee and you're able to retrieve your data within",
    "start": "194340",
    "end": "199919"
  },
  {
    "text": "milliseconds so high-performance storage class with Amazon s3 standard infrequent",
    "start": "199919",
    "end": "205590"
  },
  {
    "text": "access you have basically the same performance trades as you see an s3 standard but now you're paying a data",
    "start": "205590",
    "end": "211409"
  },
  {
    "text": "retrieval fee of 1 cent per gig per month so a simple thumb rule and we'll go into this in more detail later is",
    "start": "211409",
    "end": "217079"
  },
  {
    "text": "let's say you retrieve all your data one time per month so you have about a hundred percent retrieval rate on",
    "start": "217079",
    "end": "222509"
  },
  {
    "text": "whatever you're storing in Amazon s3 you're about indifferent between s3 standard and s3 standard infrequent",
    "start": "222509",
    "end": "228269"
  },
  {
    "text": "access just based on the pricing so that's really the decision point at which you're neutral between the two if",
    "start": "228269",
    "end": "233430"
  },
  {
    "text": "you're retrieving any less than a hundred percent of your data you should really consider s3 infrequent access for",
    "start": "233430",
    "end": "239970"
  },
  {
    "text": "a subset or a majority of your data so that's something we'll talk about some of the tools that would help you with that later on as your data gets even",
    "start": "239970",
    "end": "246900"
  },
  {
    "text": "colder and moves more toward archived here where yes I might need the data back but I can store it in Amazon",
    "start": "246900",
    "end": "252900"
  },
  {
    "text": "glacier and I have the option to retrieve it within minutes if I need to or I have the option to retrieve it in",
    "start": "252900",
    "end": "258449"
  },
  {
    "text": "bulk so if I can retrieve it maybe five to twelve hours later I could use bulk retrieval from Amazon glacier get a",
    "start": "258449",
    "end": "265680"
  },
  {
    "text": "whole lot of day back in that period of time at a very low cost so there's retrieval tears give you an",
    "start": "265680",
    "end": "271400"
  },
  {
    "text": "option of getting your data back very quick within minutes or within hours and there's a different fee associated with",
    "start": "271400",
    "end": "277550"
  },
  {
    "text": "the retrieval there so you save a lot when you move your data to Amazon glacier especially if you're not retrieving it very frequently the",
    "start": "277550",
    "end": "285740"
  },
  {
    "start": "284000",
    "end": "284000"
  },
  {
    "text": "durability and availability I want to go into a little bit more detail here when you store your data in Amazon s3 it's",
    "start": "285740",
    "end": "290960"
  },
  {
    "text": "stored across three different availability zones so these are geographically separate locations they",
    "start": "290960",
    "end": "297110"
  },
  {
    "text": "have different networking connecting them so they have independent networking trip traffic between the two between the",
    "start": "297110",
    "end": "302990"
  },
  {
    "text": "three of them and you also have independent power sources see every dunnan's he built in between those different availability zones ensuring",
    "start": "302990",
    "end": "310160"
  },
  {
    "text": "that we can lose an availability zone and still be able to deliver your data to you so your data is still durably",
    "start": "310160",
    "end": "316070"
  },
  {
    "text": "stored and it's available and we can deliver it back to you if one of those AZ good daisies goes down and as I said",
    "start": "316070",
    "end": "321770"
  },
  {
    "text": "designed for eleven nines of durability and then you see the availability numbers there for s3 standard and s3",
    "start": "321770",
    "end": "327320"
  },
  {
    "text": "standard infrequent access first we'll kick off and we'll talk about Storage",
    "start": "327320",
    "end": "333050"
  },
  {
    "text": "Management as a section talked about object tags lifecycle management storage class analysis and then talk about some",
    "start": "333050",
    "end": "339020"
  },
  {
    "text": "of the new enhancements to s3 inventory that some of you might not already be familiar with but really adds a lot of",
    "start": "339020",
    "end": "345440"
  },
  {
    "text": "new capabilities to s3 inventory so we'll go into those in detail at the end this first slide shows you the storage",
    "start": "345440",
    "end": "352010"
  },
  {
    "start": "350000",
    "end": "350000"
  },
  {
    "text": "management management portfolio and this is really a reflection of how we continue to improve s3 and how we listen",
    "start": "352010",
    "end": "357950"
  },
  {
    "text": "to customer feedback and make it easier to automate monitor and alarm on your storage so we have a few features that",
    "start": "357950",
    "end": "364190"
  },
  {
    "text": "we'll go into in more detail now but you have a lot of flexibility here where you no longer have to worry about those",
    "start": "364190",
    "end": "370220"
  },
  {
    "text": "management tasks because you configure alarms and take action directly off of those using cloud watch events so you",
    "start": "370220",
    "end": "375980"
  },
  {
    "text": "can revert changes that are made to your storage using some of these tools and then things like storage class analysis",
    "start": "375980",
    "end": "381200"
  },
  {
    "text": "give you the insights you need to know what data should I move to infrequent access take advantage of the savings and",
    "start": "381200",
    "end": "387320"
  },
  {
    "text": "not worry about the retrieval fees so a lot of powerful features here that we'll go into more detail now starting with",
    "start": "387320",
    "end": "394430"
  },
  {
    "text": "object tags these are key value pairs so this is custom metadata that you add to",
    "start": "394430",
    "end": "399680"
  },
  {
    "text": "any of your three objects you can add up to ten tags per object and you can change and edit",
    "start": "399680",
    "end": "405049"
  },
  {
    "text": "these at any point in time and you'll see how important that is with some of the examples we go through I'll talk a",
    "start": "405049",
    "end": "410059"
  },
  {
    "text": "bit about alert logic and part of this slide but then when Katie talks about human longevity you'll see how it's",
    "start": "410059",
    "end": "415609"
  },
  {
    "text": "great that you can change these tags over time and control access to your storage based on just these tags you",
    "start": "415609",
    "end": "421459"
  },
  {
    "text": "don't have to do anything with the data you don't have to copy the data you don't have to rewrite the data it's all just a manipulation of a tag on the data",
    "start": "421459",
    "end": "427369"
  },
  {
    "text": "that's going to control access and allow you to do a lot with it so number one thing is access control as I just",
    "start": "427369",
    "end": "433519"
  },
  {
    "text": "mentioned where you can set bucket policies or I am user policies based on what tagged objects should this user",
    "start": "433519",
    "end": "440419"
  },
  {
    "text": "have access to so I only want this user to read particular objects with a tag attached to them and we'll walk through",
    "start": "440419",
    "end": "445879"
  },
  {
    "text": "some code examples on that the next one is lifecycle policies so many of you I'm sure are familiar with life cycle you",
    "start": "445879",
    "end": "451999"
  },
  {
    "text": "can write those policies at a bucket or prefix level depending on how granular you want to control your lifecycle policies but now you can do that with",
    "start": "451999",
    "end": "458899"
  },
  {
    "text": "object tags as well so you can apply tags particularly do a subset of objects knowing that that lifecycle action",
    "start": "458899",
    "end": "465589"
  },
  {
    "text": "whether the transition or expire storage is only going to take effect on those tagged objects once again you can change",
    "start": "465589",
    "end": "471469"
  },
  {
    "text": "those at any point in time to apply to a different set of objects or you can delete those tags and then that lifecycle policy won't take action on",
    "start": "471469",
    "end": "477679"
  },
  {
    "text": "those objects what alert logic is doing and some of you might have attended their session",
    "start": "477679",
    "end": "483169"
  },
  {
    "text": "earlier is they have a number of end customers stored in a single s3 bucket so they'll have 950 customers worth of",
    "start": "483169",
    "end": "489949"
  },
  {
    "text": "data in one bucket they use tags to keep track of which customers data is a",
    "start": "489949",
    "end": "495169"
  },
  {
    "text": "particular object and when was that data originally created so for them the",
    "start": "495169",
    "end": "500569"
  },
  {
    "text": "creation data in s3 when it's originally uploaded does not match necessarily with when that object was originally created",
    "start": "500569",
    "end": "505909"
  },
  {
    "text": "and when that data Maps back to so they also tracked the date the month in particular that that data was created as",
    "start": "505909",
    "end": "511879"
  },
  {
    "text": "part of an object tag then they write custom lifecycle policies each month saying okay this customers data should",
    "start": "511879",
    "end": "518180"
  },
  {
    "text": "now transition to Sandra and frequent access because three months has elapsed so that's the way that they use tags to",
    "start": "518180",
    "end": "523879"
  },
  {
    "text": "control lifecycle policies now we'll talk a bit more about lifecycle policies",
    "start": "523879",
    "end": "529970"
  },
  {
    "start": "528000",
    "end": "528000"
  },
  {
    "text": "so these are rules that allow you to automatically transition or fire your storage transitions refers to",
    "start": "529970",
    "end": "536529"
  },
  {
    "text": "moving s three standard storage to s 3 and frequent access storage or moving that data to Amazon glacier so once",
    "start": "536529",
    "end": "543339"
  },
  {
    "text": "again you're taking advantage of the savings because of a storage class or lower cost and you are doing that over",
    "start": "543339",
    "end": "548829"
  },
  {
    "text": "time as your data cools off and you retrieve it less frequently and we're taking that action based on object age",
    "start": "548829",
    "end": "554079"
  },
  {
    "text": "so potentially you'd read a policy saying and we'll talk about an example right now an example would be after 30",
    "start": "554079",
    "end": "560259"
  },
  {
    "text": "days move the data to standard infrequent access maybe I have a lot of processes that kick off after a day",
    "start": "560259",
    "end": "566829"
  },
  {
    "text": "maybe in the first week so I see a lot of retrievals within that short period of time and then over time you see that",
    "start": "566829",
    "end": "572199"
  },
  {
    "text": "retrieval really cools off and after a month I'm almost never touching that data I still want to get it within",
    "start": "572199",
    "end": "577720"
  },
  {
    "text": "milliseconds so maybe a month later we have applications to kick off and need this data and it's gonna need a small",
    "start": "577720",
    "end": "583060"
  },
  {
    "text": "subset of the data might be a great fit to move it to standard in a frequent axis I can still get it in milliseconds",
    "start": "583060",
    "end": "588130"
  },
  {
    "text": "so it's available if I need it after 90 days though so after about three months is elapsed now I'm not seeing nearly any",
    "start": "588130",
    "end": "596589"
  },
  {
    "text": "users needing this data maybe this is billing data maybe this is compliance storage this is something I might need",
    "start": "596589",
    "end": "601930"
  },
  {
    "text": "to retain for a long period of time but it's not important that I get it within milliseconds if I need it",
    "start": "601930",
    "end": "606940"
  },
  {
    "text": "I know I can get it in a couple minutes and that's good for me and if I need a large subset of it you could use bulk or standard retrieval from Amazon glacier",
    "start": "606940",
    "end": "613449"
  },
  {
    "text": "as well so this is a way that I put in place on this bucket move it to infrequent access after 30 move it to",
    "start": "613449",
    "end": "619060"
  },
  {
    "text": "Amazon glacier after 90 and now it's hands-off those rules are now taking action as new objects are uploaded",
    "start": "619060",
    "end": "624610"
  },
  {
    "text": "they'll follow these rules and I'll just take advantage of the savings over time so as time goes on storage will move to",
    "start": "624610",
    "end": "630040"
  },
  {
    "text": "these lower tiers and you'll see your storage bill adjust accordingly but",
    "start": "630040",
    "end": "635470"
  },
  {
    "start": "634000",
    "end": "634000"
  },
  {
    "text": "we've got feedback from customers so it was the feedback was really around how do I know what the right number is how",
    "start": "635470",
    "end": "641500"
  },
  {
    "text": "do I know if 30 days is the right number to move to infrequent access and how do I know 90 days is the right number for Amazon glacier what storage class",
    "start": "641500",
    "end": "648100"
  },
  {
    "text": "analysis does when you configure it at the bucket prefix or tag level saying monitor this subset of my storage it's",
    "start": "648100",
    "end": "654759"
  },
  {
    "text": "monitoring the data retrieval in that storage so how much that storage was retrieved over a period of time and when",
    "start": "654759",
    "end": "660939"
  },
  {
    "text": "do we recommend moving that storage to infrequent access specifically so this is taking into account the cost",
    "start": "660939",
    "end": "666069"
  },
  {
    "text": "difference between standard and stead infrequent access and looking at the data retrieval fee so that one cent per",
    "start": "666069",
    "end": "671529"
  },
  {
    "text": "gig per month as long as you're retrieving about a hundred percent of the data or less you'll see savings and infrequent access this tool is showing",
    "start": "671529",
    "end": "678790"
  },
  {
    "text": "you how that data retrieval how that frequency of data retrieval changes over",
    "start": "678790",
    "end": "684639"
  },
  {
    "text": "time and then giving you a number to go back and writing your lifecycle policies so I want to show you a visual of what",
    "start": "684639",
    "end": "689920"
  },
  {
    "text": "that looks like this is what you're gonna see in the s3 console when you set up this feature after 30 days of",
    "start": "689920",
    "end": "695139"
  },
  {
    "text": "monitoring your storage you're gonna get a view like this where it's showing you the different object tiers so there's a",
    "start": "695139",
    "end": "701050"
  },
  {
    "text": "less than 30 days and a 30 to 45 days and one of the things that draws my eye initially is these labels so at first",
    "start": "701050",
    "end": "708339"
  },
  {
    "text": "you see frequently accessed okay that's interesting now I want to drill in if you look at that less than 30 days so",
    "start": "708339",
    "end": "714189"
  },
  {
    "text": "this is in the top left it's classified is frequently accessed because you see in purple there I'm retrieving about 300",
    "start": "714189",
    "end": "720730"
  },
  {
    "text": "terabytes of storage but I only have 200 terabytes stored so it's about a hundred and fifty percent retrieval versus the",
    "start": "720730",
    "end": "726910"
  },
  {
    "text": "storage if a hundred percent is where I'm you know in different between the two storage classes standardized inter",
    "start": "726910",
    "end": "732309"
  },
  {
    "text": "frequent access we're seeing a higher amount of retrievals here so as I walk my eyes over at the first tier where I",
    "start": "732309",
    "end": "738370"
  },
  {
    "text": "see that it's infrequently access is after 90 days for this particular bucket so now we're labeling it as infrequently",
    "start": "738370",
    "end": "744309"
  },
  {
    "text": "access storage and recommending the move to infrequent access so you're seeing now retrievals in the range of you know",
    "start": "744309",
    "end": "750490"
  },
  {
    "text": "10% or less for these tiers so after my date is about three months old I'm retrieving a very little of it this",
    "start": "750490",
    "end": "757180"
  },
  {
    "text": "would be a great fit for infrequent access I'm going to save money if I move to that storage class so I'd go back to",
    "start": "757180",
    "end": "762610"
  },
  {
    "text": "the previous slide I'd look at the lifecycle policy I'd written or maybe I haven't created one yet and say write a",
    "start": "762610",
    "end": "768459"
  },
  {
    "text": "lifecycle policy transition all storage to infrequent access after 90 days that's a way I take this information",
    "start": "768459",
    "end": "773500"
  },
  {
    "text": "plug it right into the lifecycle policy and now I have a data-driven way to write my lifecycle policy mention s3",
    "start": "773500",
    "end": "781660"
  },
  {
    "start": "780000",
    "end": "780000"
  },
  {
    "text": "inventory earlier a lot of changes to this feature a lot of new updates at a basic level it's a way to list out all",
    "start": "781660",
    "end": "788230"
  },
  {
    "text": "the objects in an s3 bucket or an s3 prefix so we have customers have grown to an incredible scale in s3 millions",
    "start": "788230",
    "end": "794589"
  },
  {
    "text": "and billions of objects and it's hard to list those using a list API call where you're listing those",
    "start": "794589",
    "end": "799750"
  },
  {
    "text": "a thousand at a time this requires a separate process to list all those objects and then create a list you can",
    "start": "799750",
    "end": "804940"
  },
  {
    "text": "really work with to run queries or something else that you want to kick off with your storage s3 inventory is a",
    "start": "804940",
    "end": "809950"
  },
  {
    "text": "feature you configure say on a daily or weekly basis I want an inventory list delivered to my bucket and this is a",
    "start": "809950",
    "end": "815950"
  },
  {
    "text": "list of all your s3 objects and associated metadata things like what storage class that object is stored in",
    "start": "815950",
    "end": "821410"
  },
  {
    "text": "how many bytes that individual object is or replication status so if you have something like cross region replication",
    "start": "821410",
    "end": "827200"
  },
  {
    "text": "configured it's going to show you whether a replication has succeeded whether something is a replica copy for a destination bucket it's an easy way to",
    "start": "827200",
    "end": "833710"
  },
  {
    "text": "view how your storage is and do advanced analytics based on that to see you know how your storage is by the different",
    "start": "833710",
    "end": "839830"
  },
  {
    "text": "tiers or to do another process based on maybe what the object key name is if that's meaningful to you this is a great",
    "start": "839830",
    "end": "844990"
  },
  {
    "text": "way to look at the object key names and see what you can learn from that recently early in November we launched a",
    "start": "844990",
    "end": "852100"
  },
  {
    "text": "number of enhancements to this feature so I'll touch on them now one of the big ones and customers like Capital One",
    "start": "852100",
    "end": "857860"
  },
  {
    "text": "we're asking for this is I want to see the encryption status of all the objects in my bucket so I have compliance",
    "start": "857860",
    "end": "863380"
  },
  {
    "text": "requirements I've requirements saying I need to encrypt all my objects and maybe it's tough to enforce that on all your",
    "start": "863380",
    "end": "869440"
  },
  {
    "text": "users so you're not sure if all your objects have been encrypted and you want an easy way to view all your objects and see that now you have encryption status",
    "start": "869440",
    "end": "876220"
  },
  {
    "text": "as part of the s3 inventory report so you see not only whether it's encrypted but whether it's server side or kms",
    "start": "876220",
    "end": "881920"
  },
  {
    "text": "encrypted using the key management service you can also now encrypt your s3 inventory file so once again if you have",
    "start": "881920",
    "end": "888940"
  },
  {
    "text": "compliance requirements around your storage this is an extra file that's being written now I can specify whether",
    "start": "888940",
    "end": "893980"
  },
  {
    "text": "it should be server side or kms encrypted when it's delivered to my bucket we also made the inventory report",
    "start": "893980",
    "end": "900010"
  },
  {
    "text": "no longer just available in CSV format but now available in orz which is a columnar data format that makes it much",
    "start": "900010",
    "end": "906370"
  },
  {
    "text": "higher performance when you run queries on your s3 inventory file and as I mentioned when you get to millions and",
    "start": "906370",
    "end": "911500"
  },
  {
    "text": "billions of objects this can be a rather large file using a columnar format and using high base tools will allow you to",
    "start": "911500",
    "end": "917230"
  },
  {
    "text": "get faster query results and get to the information you want faster we also",
    "start": "917230",
    "end": "922420"
  },
  {
    "text": "launched the ability to now now it's more compatible and easier to integrate with Amazon Athena and redshift spectrum",
    "start": "922420",
    "end": "928390"
  },
  {
    "text": "so it allows you to get quicker up and running you'll see these examples in the Docs but now you can point your inventory",
    "start": "928390",
    "end": "934579"
  },
  {
    "text": "file you can point Athena to it and say I want to run a query based on what objects are encrypted and what types",
    "start": "934579",
    "end": "939709"
  },
  {
    "text": "show me all my object keys which ones are encrypted using server-side kms or which ones are unencrypted you could get",
    "start": "939709",
    "end": "945649"
  },
  {
    "text": "started with that within seconds get its results back in a matter of seconds or minutes depending on how big your file",
    "start": "945649",
    "end": "951110"
  },
  {
    "text": "is so this is a great way to take the new enhancements to this and get to easy insights and easy visibility on what's",
    "start": "951110",
    "end": "958250"
  },
  {
    "text": "encrypted what's not what you have to go back and encrypt which segues to the",
    "start": "958250",
    "end": "963709"
  },
  {
    "text": "discussion of security where we'll talk more about encryption access controls cloud trail data events and then Amazon",
    "start": "963709",
    "end": "969860"
  },
  {
    "text": "Macy which is a new service I launched in August a lot of powerful capabilities there I want to make sure you're all aware of so when you want to encrypt",
    "start": "969860",
    "end": "977600"
  },
  {
    "start": "976000",
    "end": "976000"
  },
  {
    "text": "your data in Amazon s3 you have options you can use server-side encryption where we have three different",
    "start": "977600",
    "end": "982670"
  },
  {
    "text": "options or client-side encryption client-side encryption would be encrypting the object before you even",
    "start": "982670",
    "end": "987980"
  },
  {
    "text": "upload it to s3 so you upload encrypted data to us we'll sort as an object we'll deliver it back to you still in its",
    "start": "987980",
    "end": "994070"
  },
  {
    "text": "encrypted form you can manage that all on the client side if you want to take advantage of some of the services we",
    "start": "994070",
    "end": "999110"
  },
  {
    "text": "have on the server side though takes the work off of you and lets Amazon s3 manage it for you the first one in the",
    "start": "999110",
    "end": "1005740"
  },
  {
    "text": "most basic is server side encryption s3 so we manage the data and master keys you just specify that you on your",
    "start": "1005740",
    "end": "1011560"
  },
  {
    "text": "objects encrypted when they're uploaded this is encryption at rest so keep in mind that if an authorized request comes",
    "start": "1011560",
    "end": "1017800"
  },
  {
    "text": "in to read your object we're going to decrypt that object and then deliver that object out to whatever customer",
    "start": "1017800",
    "end": "1022930"
  },
  {
    "text": "whatever Account requested so that's something to keep in mind here it is encryption at rest surface ID encryption",
    "start": "1022930",
    "end": "1028630"
  },
  {
    "text": "customer refers to you providing customer key so with your request both",
    "start": "1028630",
    "end": "1033760"
  },
  {
    "text": "to put the object in s3 and then also to get it back you specify the encryption key and we use your key that you provide",
    "start": "1033760",
    "end": "1040150"
  },
  {
    "text": "to encrypt or decrypt the object depending on whether it's a footer again this is something where you manage to",
    "start": "1040150",
    "end": "1046329"
  },
  {
    "text": "the encryption keys at that point and we just use them for the encryption decryption and throw it away at that point so it's not stored in s3 the other",
    "start": "1046329",
    "end": "1053260"
  },
  {
    "text": "one you have is server side encryption with the key management service so this is AWS kms and using kms there's a data",
    "start": "1053260",
    "end": "1060970"
  },
  {
    "text": "key used in s3 to encrypt your data and then there's a master key retain the kms service different from sses 3",
    "start": "1060970",
    "end": "1070010"
  },
  {
    "text": "which is the first one I laid out where it's encryption and rest you have an additional level of protection when you choose kms to encrypt your data because",
    "start": "1070010",
    "end": "1077000"
  },
  {
    "text": "customers not only need the approval and authorization to read that object from s3 but they also need the permissions",
    "start": "1077000",
    "end": "1083419"
  },
  {
    "text": "and the proper permissions from the kms service to actually get access to the master key to decrypt that object so if",
    "start": "1083419",
    "end": "1089900"
  },
  {
    "text": "you have customers reading data who aren't authorized by the kms service they're just going to be getting absolute gibberish from s3 so that's a",
    "start": "1089900",
    "end": "1096799"
  },
  {
    "text": "way to keep your data at an extra level of security and make sure it's encrypted only to the authorized users who have",
    "start": "1096799",
    "end": "1103640"
  },
  {
    "text": "authorization both in s3 and kms to read your data so this is an additional level of data protection and we have a few",
    "start": "1103640",
    "end": "1109429"
  },
  {
    "text": "other features that interact with kms later on so that's a way to add an additional level of protection when you're choosing to encrypt your objects",
    "start": "1109429",
    "end": "1115700"
  },
  {
    "text": "and then use KMS one of the other new features launched in november is",
    "start": "1115700",
    "end": "1120860"
  },
  {
    "text": "encryption by default which is a feature at the bucket level allowing you to specify for a given s3 bucket whether",
    "start": "1120860",
    "end": "1127400"
  },
  {
    "text": "you want all objects encrypted with server-side encryption or encryption using the KMS service so for customers",
    "start": "1127400",
    "end": "1134809"
  },
  {
    "text": "that want to enforce encryption on all objects uploaded to a bucket this is a great way to do that it makes it easy to",
    "start": "1134809",
    "end": "1140960"
  },
  {
    "text": "satisfy compliance requirements and ensures that all those objects will be encrypted if you upload an object and you're going",
    "start": "1140960",
    "end": "1147650"
  },
  {
    "text": "to put an object to this bucket that already has a specified encryption type so let's say I said default encryption",
    "start": "1147650",
    "end": "1152809"
  },
  {
    "text": "as you see up on the screen where it's server-side encryption by default if I do a put with kms encryption will",
    "start": "1152809",
    "end": "1158570"
  },
  {
    "text": "continue to store that object with kms encryption so will respect to whatever encryptions in the request and if",
    "start": "1158570",
    "end": "1164330"
  },
  {
    "text": "there's no encryption specified then we pick up the default encryption status here so you're still able to specify for",
    "start": "1164330",
    "end": "1169669"
  },
  {
    "text": "potentially higher requirements what encryption you want and then we'll just ensure the default encryption applies to",
    "start": "1169669",
    "end": "1174980"
  },
  {
    "text": "any request where there isn't an encryption specified and this is a way to on an ongoing basis ensure everything",
    "start": "1174980",
    "end": "1180380"
  },
  {
    "text": "is encrypting your bucket and in conjunction with this you could then use s3 inventory to see what objects are not",
    "start": "1180380",
    "end": "1186020"
  },
  {
    "text": "encrypted and still need to be encrypted great way to put those features together and ensure everything you need to be encrypted is encrypted next I'll walk",
    "start": "1186020",
    "end": "1194540"
  },
  {
    "text": "through a bit of a progression just talking about security and how it works in s3 so this is a great way",
    "start": "1194540",
    "end": "1199730"
  },
  {
    "text": "think about their access control list both at the object and bucket level bucket policies and then I am user",
    "start": "1199730",
    "end": "1206390"
  },
  {
    "text": "permissions that you can manage so these are a few different options and we want to give you those options but I want to talk through briefly now some of the",
    "start": "1206390",
    "end": "1213410"
  },
  {
    "text": "choices you might make on and how a request is actually authorized to give you some information to then do more",
    "start": "1213410",
    "end": "1219050"
  },
  {
    "text": "research and find out exactly what's right for your data and what what's right for what part of your data one",
    "start": "1219050",
    "end": "1224900"
  },
  {
    "text": "disclaimer I'll add to this one is for the bucket access control list so for the bucket ackles the only recommended",
    "start": "1224900",
    "end": "1231250"
  },
  {
    "text": "reason we have to use that would be for an s3 bucket logging group so for log delivery from Amazon s3 for example",
    "start": "1231250",
    "end": "1238370"
  },
  {
    "text": "where you're delivering logs to a bucket that's a great way to give permission to the logging group to actually write",
    "start": "1238370",
    "end": "1243380"
  },
  {
    "text": "those logs but outside of that we really recommend bucket policies at the bucket level and then object ACLs is necessary",
    "start": "1243380",
    "end": "1249920"
  },
  {
    "text": "at the object level so bucket axles really won't be used that frequently unless you're giving permission to the logging group so we'll start with just a",
    "start": "1249920",
    "end": "1258080"
  },
  {
    "start": "1257000",
    "end": "1257000"
  },
  {
    "text": "simple s3 bucket and then I'll start uploading some objects just so we have a reference point for later on and then",
    "start": "1258080",
    "end": "1264380"
  },
  {
    "text": "here you see the folder icon appear for those familiar with the s3 console when you add a prefix to your object key",
    "start": "1264380",
    "end": "1270650"
  },
  {
    "text": "names so this is just a slash and then adding additional information to an object key name when you want to organize your data it looks like a",
    "start": "1270650",
    "end": "1277220"
  },
  {
    "text": "folder in the s3 console so an easy way to visualize it but it really is just an additional part of your object key name",
    "start": "1277220",
    "end": "1282680"
  },
  {
    "text": "that's just the way we're rendering it visually so now we have a bucket we have some objects in it we also have a prefix",
    "start": "1282680",
    "end": "1288230"
  },
  {
    "text": "where we have additional objects so music files documents and we'll talk about how we authorize your request so",
    "start": "1288230",
    "end": "1294350"
  },
  {
    "text": "at the bucket level it's fairly simple there's really just two layers we're looking at so I am roles which you can",
    "start": "1294350",
    "end": "1299750"
  },
  {
    "text": "set up on your account or other users might have set up on their accounts have permission for what services they get access if the I am user who's trying to",
    "start": "1299750",
    "end": "1306950"
  },
  {
    "text": "make a request against your bucket doesn't have permission to access s3 doesn't have permission to access your account as three depending on how it's",
    "start": "1306950",
    "end": "1313640"
  },
  {
    "text": "set up then that request is declined right away if the user if an actual",
    "start": "1313640",
    "end": "1319130"
  },
  {
    "text": "account is making the request though a not IIM user we're going to look ahead then to the bucket context so this is",
    "start": "1319130",
    "end": "1324740"
  },
  {
    "text": "evaluating bucket policies and bucket access control lists to make the decision so there's a particular type of",
    "start": "1324740",
    "end": "1329900"
  },
  {
    "text": "request it might be changing a configuration on the bucket is that allowed in both the bucket policy we'll look at or the bucket",
    "start": "1329900",
    "end": "1335960"
  },
  {
    "text": "access control list at the object level you add an additional layer to that so",
    "start": "1335960",
    "end": "1341390"
  },
  {
    "text": "once again does this I am user have permission to access s3 and this type of requests the bucket context which is",
    "start": "1341390",
    "end": "1347990"
  },
  {
    "text": "looking at in this case whether there's an explicit denied so if there's a statement in a bucket policy saying no",
    "start": "1347990",
    "end": "1353779"
  },
  {
    "text": "one can get data from this bucket then that requests to get an object from this bucket would be declined at that level",
    "start": "1353779",
    "end": "1359450"
  },
  {
    "text": "but if there's not an explicit deny we're going to then look at an object access control list to see whether",
    "start": "1359450",
    "end": "1364850"
  },
  {
    "text": "permission is granted so you'll see another feature in just a second but I want to make sure you understand that an object access control",
    "start": "1364850",
    "end": "1371179"
  },
  {
    "text": "list will be evaluated in both cases if there's not an iam user restriction or a bucket restriction that that's an",
    "start": "1371179",
    "end": "1378169"
  },
  {
    "text": "explicit deny so you need to ensure your permissions are aligned both at the bucket and the object level to ensure",
    "start": "1378169",
    "end": "1383870"
  },
  {
    "text": "your permissions are exactly what you want them to be to limit access as needed for your data for those that use",
    "start": "1383870",
    "end": "1391610"
  },
  {
    "text": "the s3 console I'm sure you've noticed this change so under the access column there you're seeing an easy way to",
    "start": "1391610",
    "end": "1398179"
  },
  {
    "text": "easily identify whether your buckets are publicly readable so this is evaluating both bucket policies and bucket access",
    "start": "1398179",
    "end": "1404360"
  },
  {
    "text": "control lists to say okay is this content world readable and it'll label it as public so you can easily see that",
    "start": "1404360",
    "end": "1410659"
  },
  {
    "text": "in the console so if that's not the right configuration if this bucket was improperly changed to that permission",
    "start": "1410659",
    "end": "1416330"
  },
  {
    "text": "level or improperly set up in the first place it's an easy visual way for you to identify what buckets you should go back",
    "start": "1416330",
    "end": "1421700"
  },
  {
    "text": "and change the permissions on for the not public bucket you see the asterisks there and that's talking about just we",
    "start": "1421700",
    "end": "1427309"
  },
  {
    "text": "covered on the previous slide where you'll still want to look at your object access control list to understand the permissions that are being granted for a",
    "start": "1427309",
    "end": "1433610"
  },
  {
    "text": "particular object request so how do we",
    "start": "1433610",
    "end": "1439340"
  },
  {
    "text": "authorize a request this is now looking at the prefix level so when I append that prefix to an object name I did that",
    "start": "1439340",
    "end": "1445700"
  },
  {
    "text": "for all three of these documents that you see stored here so they're actually stored within a prefix and can all really be grouped together that way so I",
    "start": "1445700",
    "end": "1452450"
  },
  {
    "text": "could write policies based on a wild-card after that specific prefix so this bucket this prefix and then all the",
    "start": "1452450",
    "end": "1459320"
  },
  {
    "text": "objects under that I want to control and we'll skip ahead of the examples so really it allows you to write a",
    "start": "1459320",
    "end": "1465169"
  },
  {
    "start": "1465000",
    "end": "1465000"
  },
  {
    "text": "permission policy for a lot of objects at once rather than using just object Ackles one of the time so in this",
    "start": "1465169",
    "end": "1470510"
  },
  {
    "text": "user policy example somebody wanted to grant permission to put get and delete objects to a particular user in this",
    "start": "1470510",
    "end": "1476990"
  },
  {
    "text": "case Alice so when you see the object key names that start with example bucket",
    "start": "1476990",
    "end": "1482059"
  },
  {
    "text": "under prefix Alice and any object key after that we've now authorized permission for this particular user to",
    "start": "1482059",
    "end": "1488330"
  },
  {
    "text": "interact with that subset of storage so it's an easy way for you to say that amount of storage could really scale up",
    "start": "1488330",
    "end": "1494390"
  },
  {
    "text": "or scale down depends on the user and it depends on their needs but this is an easy way to manage that at the prefixed level so you're not having to give this",
    "start": "1494390",
    "end": "1501080"
  },
  {
    "text": "user bucket level permission which might be too broad for their needs but a prefix might be perfect for them next",
    "start": "1501080",
    "end": "1507919"
  },
  {
    "text": "example we'll talk about is going back to object tags so in this case I want a user to be able to get all the objects",
    "start": "1507919",
    "end": "1513620"
  },
  {
    "text": "that are associated with project Delta project Delta might start very small it might grow to be a very large project",
    "start": "1513620",
    "end": "1519140"
  },
  {
    "text": "over time and we'll just continue to tag the data associated with the project and then if a project ends let's say this is",
    "start": "1519140",
    "end": "1524720"
  },
  {
    "text": "a consult consultant or somebody else who's working with our data for a short period of time when that project ends you go ahead and delete all those tags",
    "start": "1524720",
    "end": "1530960"
  },
  {
    "text": "or delete the user permission and now that permissions going so it allows you to change that permission over time you",
    "start": "1530960",
    "end": "1536809"
  },
  {
    "text": "could tag all the data upon ingest that's associated with this project and this is an easy way to assure that this particular user has access to the data",
    "start": "1536809",
    "end": "1543650"
  },
  {
    "text": "they need next we'll talk about data events in cloud trail and when you turn",
    "start": "1543650",
    "end": "1550190"
  },
  {
    "start": "1547000",
    "end": "1547000"
  },
  {
    "text": "the data events in cloud trail it's looking at object level activity and recording things like gets puts in",
    "start": "1550190",
    "end": "1555230"
  },
  {
    "text": "deletes against individual objects in s3 this gives you the visibility at the object level and who's making changes to",
    "start": "1555230",
    "end": "1561470"
  },
  {
    "text": "your storage and who's taking action allows you to monitor changes to bucket configurations and then you can automate",
    "start": "1561470",
    "end": "1568040"
  },
  {
    "text": "alerts from this functionality so one of the better examples of this is you set",
    "start": "1568040",
    "end": "1574280"
  },
  {
    "text": "up cloud trail data events and you say ok my object access control lists I know exactly the way I want them to be today",
    "start": "1574280",
    "end": "1580370"
  },
  {
    "text": "if there's ever a change to those access control lists its unintended I don't want them to happen and I want to revert",
    "start": "1580370",
    "end": "1585470"
  },
  {
    "text": "that change so you can set up an alarm and in a actual automatic action to take place based on these data events where",
    "start": "1585470",
    "end": "1592520"
  },
  {
    "text": "will trigger a lambda function if it sees a change to an object access control list that lambda will then",
    "start": "1592520",
    "end": "1597950"
  },
  {
    "text": "revert the change go back and right at the original object access control list ensuring that there was no change",
    "start": "1597950",
    "end": "1604159"
  },
  {
    "text": "to the permissions that was unintended so using data events cloud watch events and lambda to make that change is a",
    "start": "1604159",
    "end": "1610520"
  },
  {
    "text": "simple loop that you can set up to make sure that changes you don't want to be made to your bucket you're setting up an",
    "start": "1610520",
    "end": "1615980"
  },
  {
    "text": "alarm and an automatic function to revert those and change them back as needed and when you set up data events",
    "start": "1615980",
    "end": "1622580"
  },
  {
    "text": "in cloud trail you can also use Amazon Macy which is a new service we launched in August and this is using machine",
    "start": "1622580",
    "end": "1629029"
  },
  {
    "text": "learning to look at what data you're storing in s3 so it's looking at objects to look for things like personally",
    "start": "1629029",
    "end": "1634940"
  },
  {
    "text": "identifiable information things like add justice addresses credit card numbers social security numbers this is an",
    "start": "1634940",
    "end": "1641059"
  },
  {
    "text": "indication that there's more sensitive data associated with that particular object and Amazon Macy's then looking at",
    "start": "1641059",
    "end": "1646190"
  },
  {
    "text": "the permissions associated with that object and giving you alarms and dashboards that are an easy way for you",
    "start": "1646190",
    "end": "1651799"
  },
  {
    "text": "to see okay I have sensitive data in s3 and I know I have sensitive data but are the permissions I'm allowing to those",
    "start": "1651799",
    "end": "1658190"
  },
  {
    "text": "objects aligning with how sensitive that data is Amazon Macy's a great way to visualize that it's recognized as a",
    "start": "1658190",
    "end": "1664190"
  },
  {
    "text": "sensitive data it's monitoring and alarming based on changes that are happening and also looking for",
    "start": "1664190",
    "end": "1669679"
  },
  {
    "text": "suspicious behavior so if you see a lot of changes to permissions for example maybe a malicious user is accessing your",
    "start": "1669679",
    "end": "1675590"
  },
  {
    "text": "data and is changing the permissions on who's able to access to that data Amazon",
    "start": "1675590",
    "end": "1680779"
  },
  {
    "text": "Macy would see that behavior alarm on it and give you the ability to make a change lock down access to that remove",
    "start": "1680779",
    "end": "1687200"
  },
  {
    "text": "that particular user it's going to give you all the visibility to do that the other one we talked about earlier was the object access control us and Amazon",
    "start": "1687200",
    "end": "1694279"
  },
  {
    "text": "Macy is a powerful dashboard feature that will show you based on all the objects you've stored in s3 how many of",
    "start": "1694279",
    "end": "1700340"
  },
  {
    "text": "them are publicly readable at an object ACL level so an object Akal it's going to give you an easy way to view that",
    "start": "1700340",
    "end": "1705890"
  },
  {
    "text": "with one of their dashboards that's going to show you okay what are all the object permissions I have configured easy dashboards seeing what users have",
    "start": "1705890",
    "end": "1712760"
  },
  {
    "text": "permissions and then also how much of the data is publicly readable when that's the appropriate configuration things like websites or content that",
    "start": "1712760",
    "end": "1719510"
  },
  {
    "text": "you're serving worldwide that's great if not Amazon Macy would be a great way to view that and make changes at the object",
    "start": "1719510",
    "end": "1725149"
  },
  {
    "text": "level the next session we'll talk about this section we'll talk about data protection these are additional controls",
    "start": "1725149",
    "end": "1732049"
  },
  {
    "text": "you can put in place to ensure your data is and knowing unintended deletes are occurring across region replication is",
    "start": "1732049",
    "end": "1738870"
  },
  {
    "text": "the first one we'll touch upon we'll talk about versioning and we'll talk about multi-factor authentication",
    "start": "1738870",
    "end": "1744020"
  },
  {
    "start": "1744000",
    "end": "1744000"
  },
  {
    "text": "so cross region replication allows you to replicate data from any AWS region to",
    "start": "1744020",
    "end": "1749250"
  },
  {
    "text": "any other AWS region and you can figure what the source and destination regions are so I could take data that's uploaded",
    "start": "1749250",
    "end": "1755640"
  },
  {
    "text": "in Mumbai and move it to Northern Virginia I could point that the other way as well saying that all the data",
    "start": "1755640",
    "end": "1761190"
  },
  {
    "text": "uploaded to Northern Virginia should also be copied over to Mumbai I have all the flexibility here as we launch new",
    "start": "1761190",
    "end": "1767640"
  },
  {
    "text": "AWS regions to change my replication getting either closer to my users or just achieving the geographic separation",
    "start": "1767640",
    "end": "1774899"
  },
  {
    "text": "I want to achieve for compliance purposes so you can replicate you can configure that at the bucket or prefix level and then you could also trigger",
    "start": "1774899",
    "end": "1781289"
  },
  {
    "text": "replication on individual objects as needed two of the recent enhancements for this also launched in the early in",
    "start": "1781289",
    "end": "1788070"
  },
  {
    "text": "the month of November is now we support kms encrypted objects so if you're using data and and encrypting it with a key",
    "start": "1788070",
    "end": "1795480"
  },
  {
    "text": "management service you can now replicate that data which is still encrypted across to your destination region and be",
    "start": "1795480",
    "end": "1801630"
  },
  {
    "text": "able to access it there as well so you can use kms encrypted data and use replication now the other one is",
    "start": "1801630",
    "end": "1807779"
  },
  {
    "text": "ownership over right and this allows you to do cross account replication which",
    "start": "1807779",
    "end": "1813690"
  },
  {
    "text": "you are already able to do but now change the object access control list so the object Akal is the replication is",
    "start": "1813690",
    "end": "1819330"
  },
  {
    "text": "happening so now I ensure that the proper permissions are there in the destination bucket so if I want to",
    "start": "1819330",
    "end": "1825000"
  },
  {
    "text": "maintain different ownership stacks of this object in my source and destination region allowing ownership overwrite",
    "start": "1825000",
    "end": "1830730"
  },
  {
    "text": "ensures that the bucket owner in the destination can read the object and access the object as needed so it allows",
    "start": "1830730",
    "end": "1836640"
  },
  {
    "text": "you to set up different ownership sacs which can be an additional level of data protection versioning protects your data",
    "start": "1836640",
    "end": "1845220"
  },
  {
    "text": "from accidental deletion and it does this by creating new versions of your object every time you overwrite a",
    "start": "1845220",
    "end": "1851460"
  },
  {
    "text": "particular object key so every time you update an object with the same heat we're gonna maintain a new version of it",
    "start": "1851460",
    "end": "1857370"
  },
  {
    "text": "and you see that visually on the right where I've uploaded the same image multiple times and now I'm maintaining five versions three versions one version",
    "start": "1857370",
    "end": "1864270"
  },
  {
    "text": "as necessary so as those additional rights come in or maintaining all of them so you can",
    "start": "1864270",
    "end": "1869370"
  },
  {
    "text": "actually go back and using the get on the object key name and the particular version you can go back and get any of",
    "start": "1869370",
    "end": "1875880"
  },
  {
    "text": "those particular objects you need so it tracks that changes over time it also protects against unintended delete",
    "start": "1875880",
    "end": "1881750"
  },
  {
    "text": "because if I issue a delete command against a object in a version bucket we're placing a delete marker on top of",
    "start": "1881750",
    "end": "1888960"
  },
  {
    "text": "that object which means that if you go back and get it you'll error as if the object doesn't exist but an administrator or anyone else with",
    "start": "1888960",
    "end": "1895440"
  },
  {
    "text": "the proper permission could go back remove the delete marker and actually get access to the data so when a deletes",
    "start": "1895440",
    "end": "1901440"
  },
  {
    "text": "issued against a version bucket and a particular object we're still retaining the data we're just removing access for",
    "start": "1901440",
    "end": "1907380"
  },
  {
    "text": "people to go ahead and get that data so that allows you to configure roll back those deletes and get the data that you",
    "start": "1907380",
    "end": "1914130"
  },
  {
    "text": "need you're also allowed to configure lifecycle policies which we mentioned earlier based on whether something is",
    "start": "1914130",
    "end": "1919799"
  },
  {
    "text": "the current or previous version so if you're worried about building up multiple versions of a particular object",
    "start": "1919799",
    "end": "1925590"
  },
  {
    "text": "and building up the storage over time you can configure a lifecycle policy saying after a certain period of time go",
    "start": "1925590",
    "end": "1931320"
  },
  {
    "text": "ahead and delete the old versions of the object so it's easy to set up a policy that will help you control the amount of",
    "start": "1931320",
    "end": "1936390"
  },
  {
    "text": "data that's being retained when you set up version multi-factor authentication",
    "start": "1936390",
    "end": "1941820"
  },
  {
    "start": "1941000",
    "end": "1941000"
  },
  {
    "text": "is another way to control deletes on your object so it adds another layer of protection by requiring not only an",
    "start": "1941820",
    "end": "1948150"
  },
  {
    "text": "authorized requests against s3 to delete an object but also requiring a unique code from a token or a an authentication",
    "start": "1948150",
    "end": "1956190"
  },
  {
    "text": "device so this can be a virtual device or it can be a hardware device as you see a token up on the screen which is",
    "start": "1956190",
    "end": "1961650"
  },
  {
    "text": "going to have a unique code that you'll have to submit to s3 to actually go ahead and delete an object so if you",
    "start": "1961650",
    "end": "1968220"
  },
  {
    "text": "really need to retain your data in s3 we want to ensure it's not deleted this is a setting you can set requires that",
    "start": "1968220",
    "end": "1973650"
  },
  {
    "text": "multi-factor authentication no user without that code is going to be able to delete objects within your bucket so you",
    "start": "1973650",
    "end": "1979559"
  },
  {
    "text": "can lock down access to that code to whatever users lines with your needs",
    "start": "1979559",
    "end": "1984650"
  },
  {
    "text": "next we'll talk about achieving high levels of performance on s3 so the first one we'll touch upon is Beck's best",
    "start": "1984650",
    "end": "1992640"
  },
  {
    "text": "practices around object key naming then we'll talk about s3 transfer acceleration and then a few other best",
    "start": "1992640",
    "end": "1997770"
  },
  {
    "text": "practices you can use including including using content every network like Amazon CloudFront",
    "start": "1997770",
    "end": "2004220"
  },
  {
    "text": "first thing I want to highlight here is to achieve high levels of performance on s3 happens by default if you're going to",
    "start": "2004220",
    "end": "2011850"
  },
  {
    "text": "achieve higher than about a thousand requests per second these this particular best practice where you pad",
    "start": "2011850",
    "end": "2017790"
  },
  {
    "text": "hashes to the any of your key name is something you should consider so if you anticipate growing to above that level",
    "start": "2017790",
    "end": "2022920"
  },
  {
    "text": "this is a combination of puts and gets in your object over about a thousand requests you want to look at this best",
    "start": "2022920",
    "end": "2028560"
  },
  {
    "text": "practice so if you think you might grow to that over time it's easy to implement these changes today or implement these",
    "start": "2028560",
    "end": "2034230"
  },
  {
    "text": "today on your bucket and ensure you're going to scale with no issues on s3 if",
    "start": "2034230",
    "end": "2039660"
  },
  {
    "text": "you're gonna be below that level and you don't see a lot of traffic in your storage don't worry about this at all we",
    "start": "2039660",
    "end": "2044820"
  },
  {
    "text": "automatically partition your data over time as your request pattern grows and that's something that you really never",
    "start": "2044820",
    "end": "2050790"
  },
  {
    "text": "have to worry about if you have a lower amount of requests want to go into detail though on how you can add that",
    "start": "2050790",
    "end": "2056310"
  },
  {
    "start": "2054000",
    "end": "2054000"
  },
  {
    "text": "hash and this is important because object keys are stored in an index in all AWS regions and if you're constantly",
    "start": "2056310",
    "end": "2063929"
  },
  {
    "text": "writing the same object key over and over again like let's say it starts with a date and it starts with a year so you just keep writing the same bucket and",
    "start": "2063929",
    "end": "2070620"
  },
  {
    "text": "2017 over and over again for each new put that's going to place all your objects pretty much next to each other",
    "start": "2070620",
    "end": "2076679"
  },
  {
    "text": "very close to each other within the same partition in the index that means if your traffic really skills up readily",
    "start": "2076679",
    "end": "2082020"
  },
  {
    "text": "it's going to be trying to do all those reads from the same section of the index and that's where you may experience some",
    "start": "2082020",
    "end": "2088710"
  },
  {
    "text": "performance slowdowns as we try to spread out your data and allow you to achieve those higher levels of throughput that you're trying to achieve",
    "start": "2088710",
    "end": "2095389"
  },
  {
    "text": "by putting the hash at the beginning of your key name you're then adding randomness so you could hash the key",
    "start": "2095390",
    "end": "2101040"
  },
  {
    "text": "name place it at the beginning of your object right after the bucket name and now you have randomness inserted at the",
    "start": "2101040",
    "end": "2106530"
  },
  {
    "text": "beginning of your object key name very early on ensuring that your data is going to be spread across different",
    "start": "2106530",
    "end": "2112530"
  },
  {
    "text": "partitions and allowing you to grow to a higher level of throughput which without a cheat experiencing any slowdown along",
    "start": "2112530",
    "end": "2118260"
  },
  {
    "text": "the way the second example you see down below is let's say I'm storing a lot of animations videos and photos in s3 if a",
    "start": "2118260",
    "end": "2125190"
  },
  {
    "text": "lot of my traffic is going to fall under those headers and I know I'm going to store my storage that way I can add the",
    "start": "2125190",
    "end": "2130590"
  },
  {
    "text": "prefix ahead of the - knowing that I'm gonna have a lot of traffic to those individual prefixes and",
    "start": "2130590",
    "end": "2136839"
  },
  {
    "text": "that allows me to do things like write prefixes into my lifecycle policies or also do list API calls against a",
    "start": "2136839",
    "end": "2143200"
  },
  {
    "text": "particular prefix using animations videos or photos I'm still getting the performance benefit by adding the hash",
    "start": "2143200",
    "end": "2149140"
  },
  {
    "text": "to my key name but now I can also use the prefix as necessary so it's a way you can balance the needs",
    "start": "2149140",
    "end": "2154270"
  },
  {
    "text": "to list your objects and organize them against the need to spread out your data across different partitions one of the",
    "start": "2154270",
    "end": "2163000"
  },
  {
    "text": "features I mentioned I touched on earlier is s3 transfer acceleration the real benefit you're going to see here is",
    "start": "2163000",
    "end": "2168310"
  },
  {
    "text": "for larger objects across larger Geographic distances so instead of using the public Internet in this case from",
    "start": "2168310",
    "end": "2174520"
  },
  {
    "text": "somewhere in Southeast Asia uploading data all the way to Northern Virginia you're now taking the advantage of s3 is",
    "start": "2174520",
    "end": "2179829"
  },
  {
    "text": "content delivery network where we have edge locations around the world you're uploading your data to the",
    "start": "2179829",
    "end": "2185230"
  },
  {
    "text": "closest edge location and then you're traveling across the AWS backbone to your destination region so you're no",
    "start": "2185230",
    "end": "2190900"
  },
  {
    "text": "longer experiencing any issues or slowdowns we might associate with just traveling across the public internet",
    "start": "2190900",
    "end": "2196240"
  },
  {
    "text": "you're now on the AWS backbone which can lead to a significant performance improvement and importantly when you",
    "start": "2196240",
    "end": "2202510"
  },
  {
    "text": "look at this you're specifying this in the request there's really no application changes it's just something you're specifying in the request I want",
    "start": "2202510",
    "end": "2208359"
  },
  {
    "text": "to use transfer acceleration or I do not you can continue to admit that but if you want to achieve higher levels of performance this might be a great fit",
    "start": "2208359",
    "end": "2214780"
  },
  {
    "text": "and if it doesn't achieve faster performance and your upload isn't going",
    "start": "2214780",
    "end": "2219790"
  },
  {
    "text": "to be faster than it would have been normally over the Internet there's no additional charge for using transfer acceleration so there's a small fee for",
    "start": "2219790",
    "end": "2225640"
  },
  {
    "text": "using it if it improves your performance if it doesn't improve your performance there's no charge for using transfer acceleration and this is an example",
    "start": "2225640",
    "end": "2234099"
  },
  {
    "start": "2232000",
    "end": "2232000"
  },
  {
    "text": "showing so this is uploading to a bucket in Singapore from different edge locations around the world",
    "start": "2234099",
    "end": "2239680"
  },
  {
    "text": "and you see that typically transfer acceleration is being uploaded in this particular example in about half the",
    "start": "2239680",
    "end": "2245230"
  },
  {
    "text": "time this is a 500 gigabyte object so it's a fairly large object and you're seeing that you get significant savings",
    "start": "2245230",
    "end": "2251950"
  },
  {
    "text": "in time for this upload by using transfer acceleration so where that's important to you and where that's important to your end users to get data",
    "start": "2251950",
    "end": "2258220"
  },
  {
    "text": "up to the cloud quickly for analysis or to kick off another process transfer acceleration can be a great fit",
    "start": "2258220",
    "end": "2265230"
  },
  {
    "text": "three of the other ones I want to touch upon briefly is using a CDN like cloud front where you'll get lower latency",
    "start": "2265390",
    "end": "2271360"
  },
  {
    "text": "potentially higher throughput performance and you won't experience as many requests to s3 so if you have",
    "start": "2271360",
    "end": "2277000"
  },
  {
    "text": "objects that are being read very frequently it'll cache those at the edge and then users will experience the performance",
    "start": "2277000",
    "end": "2283000"
  },
  {
    "text": "improvement of having cache storage through account CDN versus going back to Amazon s3 for each new get on that",
    "start": "2283000",
    "end": "2290050"
  },
  {
    "text": "object so if you're seeing a lot of traffic cloud front could improve your performance there for multi-part uploads",
    "start": "2290050",
    "end": "2295420"
  },
  {
    "text": "instead of uploading an object all at once whether it's gigabytes scale terabytes scale up to five terabytes you",
    "start": "2295420",
    "end": "2301330"
  },
  {
    "text": "can upload an object to s3 uploading that all at once is a single-threaded process can take a long time I'm sure",
    "start": "2301330",
    "end": "2307330"
  },
  {
    "text": "for some of you have experienced using multi-part uploads you break that object into smaller parts you get the you can",
    "start": "2307330",
    "end": "2315040"
  },
  {
    "text": "paralyze the uploads then you submit a manifest file telling us okay all of that multi-part upload now assemble all",
    "start": "2315040",
    "end": "2321790"
  },
  {
    "text": "of those individual pieces to a single s3 objects so I still want to store it as one object but allows me to parallel",
    "start": "2321790",
    "end": "2327910"
  },
  {
    "text": "eyes and then also improve the performance get back successful requests more frequently because I'm doing a",
    "start": "2327910",
    "end": "2333520"
  },
  {
    "text": "smaller upload for the multi-part upload as you upload those smaller chunks the other one is range against so it's a",
    "start": "2333520",
    "end": "2339490"
  },
  {
    "text": "similar idea but in Reverse so I'm downloading potentially a large object if I'm tracking the offsets I could use",
    "start": "2339490",
    "end": "2345370"
  },
  {
    "text": "range gets to say okay I want to download this object not as one full object but I want to download ten parts",
    "start": "2345370",
    "end": "2350980"
  },
  {
    "text": "of it so you could use range gets to specify the individual parts of an object you want to download now I'm",
    "start": "2350980",
    "end": "2356170"
  },
  {
    "text": "downloading them all in parallel and potentially seeing your performance improvement there as well",
    "start": "2356170",
    "end": "2361500"
  },
  {
    "text": "and now I will bring up Katie Lampkin she's a software engineer for human",
    "start": "2361500",
    "end": "2366610"
  },
  {
    "text": "longevity she's leading the effort to control access to their Amazon s3 storage she has extensive experience in",
    "start": "2366610",
    "end": "2373780"
  },
  {
    "text": "service technologies integrating with Amazon s3 and also experience with",
    "start": "2373780",
    "end": "2378820"
  },
  {
    "text": "managing billions of objects at a multi petabytes scale so bring up",
    "start": "2378820",
    "end": "2385080"
  },
  {
    "text": "hi as Rob stated I'm Katie Lampkin and I'm a software engineer at human longevity so just to give you guys a",
    "start": "2389919",
    "end": "2396979"
  },
  {
    "text": "little bit of background about human longevity it's a human genomics company founded by craig Venter and it was",
    "start": "2396979",
    "end": "2403640"
  },
  {
    "text": "founded in 2013 its headquarters is in San Diego and we're making an impact by",
    "start": "2403640",
    "end": "2409609"
  },
  {
    "text": "gathering quality genomic data as well as phenotypic data we perform human",
    "start": "2409609",
    "end": "2415579"
  },
  {
    "text": "genome sequencing in order to make discoveries based off the entirety of the genome rather than just a portion of",
    "start": "2415579",
    "end": "2421189"
  },
  {
    "text": "it in conjunction with phenotypic data so the goal of human longevity is not",
    "start": "2421189",
    "end": "2427729"
  },
  {
    "text": "only to make life longer but make life worth living we reach this goal by attempting to build a large database of",
    "start": "2427729",
    "end": "2435079"
  },
  {
    "text": "the quality genotypic data and phenotypic data and performing analysis on this data using machine learning",
    "start": "2435079",
    "end": "2440869"
  },
  {
    "text": "technologies and deep learning techniques we are bringing preventive healthcare to the market instead of",
    "start": "2440869",
    "end": "2447169"
  },
  {
    "text": "using reactive healthcare through providing a new level of health intelligence so here's a little bit",
    "start": "2447169",
    "end": "2453349"
  },
  {
    "start": "2452000",
    "end": "2452000"
  },
  {
    "text": "about our flow of data through hli the most streamlined way that we receive our data is through our clinical research",
    "start": "2453349",
    "end": "2459919"
  },
  {
    "text": "facility called health nucleus within the health nucleus people can come in and we will sequence their genome as",
    "start": "2459919",
    "end": "2466579"
  },
  {
    "text": "well as gather different types of phenotype data so some examples of that phenotype data can be a full body MRI",
    "start": "2466579",
    "end": "2473269"
  },
  {
    "text": "and brain MRI CT scans blood labs other types of tests and gathering information",
    "start": "2473269",
    "end": "2479989"
  },
  {
    "text": "like height weight other information about their medical history etc then",
    "start": "2479989",
    "end": "2486919"
  },
  {
    "text": "from there we sequence their genome and analyze their data and then the result",
    "start": "2486919",
    "end": "2492499"
  },
  {
    "text": "is we give them back a report that is their health intelligence so we can give them information like you know hey you",
    "start": "2492499",
    "end": "2498589"
  },
  {
    "text": "might have these risks for cancers based off your genome or these neurological risks but we can also give them some fun",
    "start": "2498589",
    "end": "2504919"
  },
  {
    "text": "information like their ancestry or whether they turn red when they drink alcohol so a little bit about our",
    "start": "2504919",
    "end": "2512119"
  },
  {
    "text": "sequencing lab we have about 32 sequencers and we sequence both human genomes and microbiome genome",
    "start": "2512119",
    "end": "2518930"
  },
  {
    "text": "so for those that don't know the microbiome were first to the bacteria in the gut so we will sequence the bacteria",
    "start": "2518930",
    "end": "2525109"
  },
  {
    "text": "in your gut to get a little bit more information about how that works out too right now our database consists of a",
    "start": "2525109",
    "end": "2531170"
  },
  {
    "text": "little over 40,000 whole genome human genomes as well as 3,000 microbiome",
    "start": "2531170",
    "end": "2536779"
  },
  {
    "text": "genomes and in conjunction with the 40,000 whole genomes we also have phenotype data to go along with it now",
    "start": "2536779",
    "end": "2543890"
  },
  {
    "text": "that adds up to about 6 petabytes of data in s3 and 10 petabytes of data in glacier so where does this data come",
    "start": "2543890",
    "end": "2552559"
  },
  {
    "start": "2550000",
    "end": "2550000"
  },
  {
    "text": "from we put the samples into our sequencer and after the sequencer is done sequencing it dumps the data into",
    "start": "2552559",
    "end": "2559609"
  },
  {
    "text": "our local Isilon storage and then aspera uploads that data into s3 now those",
    "start": "2559609",
    "end": "2565400"
  },
  {
    "text": "files can be about 400,000 what we call BCL files and those get tarred into a",
    "start": "2565400",
    "end": "2571160"
  },
  {
    "text": "file that's about 600 gigabytes now when that file gets dropped into s3 it triggers our primary analysis",
    "start": "2571160",
    "end": "2578150"
  },
  {
    "text": "platform that takes those files and turns it into a raw genome file which is",
    "start": "2578150",
    "end": "2583400"
  },
  {
    "text": "about 200 gigabytes from there after primary analysis done a lambda gets",
    "start": "2583400",
    "end": "2589279"
  },
  {
    "text": "triggered and checks ok once this workflow has finished what is the next workflow I need to run after primary",
    "start": "2589279",
    "end": "2595880"
  },
  {
    "text": "analysis comes secondary analysis and so on and so forth so after every workflow",
    "start": "2595880",
    "end": "2601190"
  },
  {
    "text": "completes we want to make sure that we run all of our tools against our set and we do that through it through the lambda",
    "start": "2601190",
    "end": "2608329"
  },
  {
    "text": "triggers and after every workflow completes all of that data gets dumped",
    "start": "2608329",
    "end": "2613819"
  },
  {
    "text": "into s3 so while this analysis is happening in the cloud we have",
    "start": "2613819",
    "end": "2619630"
  },
  {
    "text": "professionals analyze the phenotype data and map it and standardize it to specific ontologies and then that data",
    "start": "2619630",
    "end": "2626510"
  },
  {
    "text": "once standardized gets uploaded into s3 as well so s3 is kind of our merging",
    "start": "2626510",
    "end": "2631940"
  },
  {
    "text": "ground for a genotypic data and our phenotypic data so we have four main",
    "start": "2631940",
    "end": "2637579"
  },
  {
    "start": "2636000",
    "end": "2636000"
  },
  {
    "text": "business needs when it comes to securing our data the default cases the patient or partner allows us access to the data",
    "start": "2637579",
    "end": "2644839"
  },
  {
    "text": "so there are no restrictions on that data now the three edge cases are a patient or partner will deny complete",
    "start": "2644839",
    "end": "2651710"
  },
  {
    "text": "access to that day so no one ever should have access to that data the second one is a patient or",
    "start": "2651710",
    "end": "2658010"
  },
  {
    "text": "partner only allows our specific research team to have access to that data which means no one else can have",
    "start": "2658010",
    "end": "2663320"
  },
  {
    "text": "access but that select group and the third and final one is a partner allows hli ownership of that data after in a",
    "start": "2663320",
    "end": "2670100"
  },
  {
    "text": "lot of period of time so let's say it after a year of sequencing so a year",
    "start": "2670100",
    "end": "2675440"
  },
  {
    "text": "after that sequencing date that data belongs to H Li and we can have access to it but during that year period we are",
    "start": "2675440",
    "end": "2682370"
  },
  {
    "text": "not allowed to have access to it so it is restricted now how do we control",
    "start": "2682370",
    "end": "2687440"
  },
  {
    "text": "those business needs in a technological way we use Amazon s3 Amazon s3 tags as",
    "start": "2687440",
    "end": "2693650"
  },
  {
    "text": "well as I am managed policies so the three tags that we decided to use wore rights restrictions and project ID so",
    "start": "2693650",
    "end": "2702170"
  },
  {
    "text": "let's go through those rights do we have rights to that data and now or will we",
    "start": "2702170",
    "end": "2707330"
  },
  {
    "text": "ever have rights to that data in the future if we do we will mark it as true the value if we do not then the value",
    "start": "2707330",
    "end": "2715040"
  },
  {
    "text": "will be false restriction is there currently a restriction on that on that",
    "start": "2715040",
    "end": "2720170"
  },
  {
    "text": "piece of data if there is the value is true if there's no restriction the value",
    "start": "2720170",
    "end": "2725720"
  },
  {
    "text": "is false now project ID if this data relates back to a specific project then",
    "start": "2725720",
    "end": "2731930"
  },
  {
    "text": "we mark it with the project ID this relates back to the edge business case where if a partner or patient only wants",
    "start": "2731930",
    "end": "2738650"
  },
  {
    "text": "our research team to have access to that data we can keep track of it using this project ID and allow only specific end",
    "start": "2738650",
    "end": "2745610"
  },
  {
    "text": "users to have access to it based off that project ID so as I said we have",
    "start": "2745610",
    "end": "2752990"
  },
  {
    "text": "petabytes of data in s3 and that adds up to hundreds of millions of files so if",
    "start": "2752990",
    "end": "2758360"
  },
  {
    "text": "we wanted to tag all these files we had to come up with a solution that could tag them and tag them efficiently and",
    "start": "2758360",
    "end": "2763520"
  },
  {
    "text": "quickly so we did that using a service solution so once a workflow completes",
    "start": "2763520",
    "end": "2769340"
  },
  {
    "text": "and has uploaded files into s3 an SMS message gets sent off and that triggers a step function that has a",
    "start": "2769340",
    "end": "2776150"
  },
  {
    "text": "series of controlled lambdas controlled parallelized lambdas that go out and tag",
    "start": "2776150",
    "end": "2781340"
  },
  {
    "text": "all the files now each of these individual lambdas that go out and tag the file",
    "start": "2781340",
    "end": "2786510"
  },
  {
    "text": "the hit our own API gather the list of files they need to tag as well as the tags that they need to tag them with and",
    "start": "2786510",
    "end": "2793260"
  },
  {
    "text": "then go out and tag all the files in s3 if we were to only do this with one",
    "start": "2793260",
    "end": "2798750"
  },
  {
    "text": "lambda it would be extremely slow and it wouldn't have the efficiency that we wanted to if we were to you know trigger",
    "start": "2798750",
    "end": "2805500"
  },
  {
    "text": "lambda thousands of times and have the peril Asian just a parallelization expand out as lambda does it would cause",
    "start": "2805500",
    "end": "2812490"
  },
  {
    "text": "throttling on our api's as well as the s3 API so step functions was a really",
    "start": "2812490",
    "end": "2818099"
  },
  {
    "text": "good way to control that parallelization to have the optimized speed but not throttle any of the services so let's",
    "start": "2818099",
    "end": "2826230"
  },
  {
    "text": "talk a little bit about that i am manish policies so we had two different types of policies we have implicit allow and",
    "start": "2826230",
    "end": "2832589"
  },
  {
    "text": "implicit deny now implicit allow when we map that to a bucket says the default",
    "start": "2832589",
    "end": "2838589"
  },
  {
    "text": "case is allow so any file in that bucket that does not have the respective tags",
    "start": "2838589",
    "end": "2843720"
  },
  {
    "text": "is open access to everyone but if we have a specific tag that matches to a",
    "start": "2843720",
    "end": "2849330"
  },
  {
    "text": "specific value then that file will be restricted now implicit deny is the opposite everything if the win + a deny",
    "start": "2849330",
    "end": "2858500"
  },
  {
    "text": "policy will mean map it to that bucket everything in that bucket will be restricted now if it has a specific key",
    "start": "2858500",
    "end": "2866010"
  },
  {
    "text": "value pair for the tags then that is when it is allowed to have access so",
    "start": "2866010",
    "end": "2872160"
  },
  {
    "text": "here's an example of our implicit allowed policy we're controlling get",
    "start": "2872160",
    "end": "2877380"
  },
  {
    "text": "object and get object version because we not only want to restrict access to just getting access to the object but all of",
    "start": "2877380",
    "end": "2884550"
  },
  {
    "text": "its versions as well now here are the two conditions that really matter so when rights is false and restriction",
    "start": "2884550",
    "end": "2892530"
  },
  {
    "text": "is true that means that we should not have access to that data so we say ok",
    "start": "2892530",
    "end": "2897839"
  },
  {
    "text": "other if those values are there then we're restricting access otherwise it's open the second condition leaves it",
    "start": "2897839",
    "end": "2905550"
  },
  {
    "text": "available for having a project ID in there and for in this case the project ID is PJ - 1 2 3 4 so later on we can",
    "start": "2905550",
    "end": "2914940"
  },
  {
    "text": "take rolls and map it to that particular tag and say hey even if this is restrict if there's this project ID tag allow",
    "start": "2914940",
    "end": "2922680"
  },
  {
    "text": "this role to have access to this object now here's an example of implicit deny",
    "start": "2922680",
    "end": "2928859"
  },
  {
    "text": "policy so the implicit deny policy we actually broke it out into two separate statements to show not only can you do",
    "start": "2928859",
    "end": "2934829"
  },
  {
    "text": "it in one statement but you can do it in two statements and separate the conditions so again we're controlling",
    "start": "2934829",
    "end": "2941819"
  },
  {
    "text": "access to the object and all of its versions and then we're saying hey if",
    "start": "2941819",
    "end": "2947369"
  },
  {
    "text": "Rights is true and restrictions is false then we have access to this data so we",
    "start": "2947369",
    "end": "2952799"
  },
  {
    "text": "allow access on that and then we have our second condition in our second statement that says hey if we have a",
    "start": "2952799",
    "end": "2958260"
  },
  {
    "text": "project ID and we need to allow our specific groups to have access to it we can use that project ID tag to have",
    "start": "2958260",
    "end": "2965069"
  },
  {
    "text": "access so a cholo has done a lot with our data and I think one of the most",
    "start": "2965069",
    "end": "2970920"
  },
  {
    "text": "impressive things in my opinion being a software engineer is what we call our open search tool so we take all of this",
    "start": "2970920",
    "end": "2977700"
  },
  {
    "text": "data that we're allowed to have access to and we upload it into this tool that we can do queries on both the phenotypic",
    "start": "2977700",
    "end": "2984660"
  },
  {
    "text": "data and the genotypic data so we can do queries based off of demographics or age",
    "start": "2984660",
    "end": "2990720"
  },
  {
    "text": "or a specific variant in your genome and gather information build these graphs so",
    "start": "2990720",
    "end": "2996750"
  },
  {
    "text": "that our scientists and researchers can analyze that data and make conclusions",
    "start": "2996750",
    "end": "3002539"
  },
  {
    "text": "from it another one that's kind of interesting is that we produce a paper that was that said that we can have",
    "start": "3002539",
    "end": "3009470"
  },
  {
    "text": "machine learning technologies to kind of analyze physical traits of people so when we sequence the human genome we can",
    "start": "3009470",
    "end": "3017539"
  },
  {
    "text": "go through and say hey you know based off of these parts of your genome this is kind of what your face is going to",
    "start": "3017539",
    "end": "3023150"
  },
  {
    "text": "look like and I thought that was really interesting to me we've done a lot with AWS and Amazon s3 and I really look",
    "start": "3023150",
    "end": "3030559"
  },
  {
    "text": "forward to everything that we're gonna do within the future",
    "start": "3030559",
    "end": "3034508"
  },
  {
    "text": "thank you very much Katie this is only the beginning of a great week of storage presentations so you have options on",
    "start": "3044530",
    "end": "3050600"
  },
  {
    "text": "Tuesday Wednesday and Thursday go ahead and mark any of those down and I'll flip",
    "start": "3050600",
    "end": "3056240"
  },
  {
    "text": "back to this slide in a second I would love to thank all of you for joining us today keep an eye out for those with the",
    "start": "3056240",
    "end": "3062990"
  },
  {
    "text": "pins on with the s3 logo and that will help you do additional get additional",
    "start": "3062990",
    "end": "3068600"
  },
  {
    "text": "questions answered I'll go back to this slide and we do have a few minutes for Q&A so for anyone who wants to come up",
    "start": "3068600",
    "end": "3074780"
  },
  {
    "text": "to the microphone and ask any questions now we can take those for myself and Katie",
    "start": "3074780",
    "end": "3080680"
  }
]