[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "hello everyone and thanks for joining this webinar on deep dive and best",
    "start": "4560",
    "end": "10059"
  },
  {
    "text": "practices for Amazon redshift over the next one our we will take this",
    "start": "10059",
    "end": "15669"
  },
  {
    "text": "opportunity to understand Amazon redshift in a little bit more detail and look at some of the best practices that",
    "start": "15669",
    "end": "22570"
  },
  {
    "text": "you can apply to make sure you're optimally utilizing Amazon redshift my",
    "start": "22570",
    "end": "28270"
  },
  {
    "text": "name is a degrom on Balachandran I work as a Big Data Solutions Architect for Amazon internet services in India here's",
    "start": "28270",
    "end": "37000"
  },
  {
    "text": "a quick look at the agenda that we'll go for today we will start with a brief overview of what Amazon redshift is for",
    "start": "37000",
    "end": "44230"
  },
  {
    "start": "38000",
    "end": "38000"
  },
  {
    "text": "those of you who are new to Amazon redshift and then we will jump and dive deep deep into some of the concepts on",
    "start": "44230",
    "end": "51460"
  },
  {
    "text": "the table design we will then move on to things like data storage how do you ingest data how do you do alt and so on",
    "start": "51460",
    "end": "59559"
  },
  {
    "text": "so forth later we will talk a little bit around workload management which is one",
    "start": "59559",
    "end": "65710"
  },
  {
    "text": "of the key elements of Amazon redshift and some of the newer ways through which you can automate workload management",
    "start": "65710",
    "end": "72549"
  },
  {
    "text": "using things like query monitoring rules towards the end we will talk about node",
    "start": "72549",
    "end": "78580"
  },
  {
    "text": "types and cluster sizing and so on so forth and I will also leave you with some additional resources that you can",
    "start": "78580",
    "end": "84549"
  },
  {
    "text": "refer for your further reading at any point in during the webinar if you have",
    "start": "84549",
    "end": "90220"
  },
  {
    "text": "any questions please do drop the questions through the questions window on the webinar panel we have solution",
    "start": "90220",
    "end": "98200"
  },
  {
    "text": "architects who are standing by on the webinar and we will do live Q&A where",
    "start": "98200",
    "end": "103630"
  },
  {
    "text": "they would respond to your questions through the questions window so feel free and go and drop your questions on",
    "start": "103630",
    "end": "110290"
  },
  {
    "text": "the questions panel yeah okay so with that let's get started let's quickly",
    "start": "110290",
    "end": "116830"
  },
  {
    "text": "understand at a high level what Amazon redshift is and then we'll start diving deep into those topics right so for",
    "start": "116830",
    "end": "123130"
  },
  {
    "start": "123000",
    "end": "123000"
  },
  {
    "text": "those of you who are already using Amazon redshift you would be well aware",
    "start": "123130",
    "end": "128500"
  },
  {
    "text": "that it's a relational data warehouse built on MPP architecture massively",
    "start": "128500",
    "end": "134290"
  },
  {
    "text": "parallel processing architecture it's a shared nothing architecture where you have multiple nodes in the cluster",
    "start": "134290",
    "end": "141320"
  },
  {
    "text": "and you can utilize all the nodes for your processing requirements right so",
    "start": "141320",
    "end": "147950"
  },
  {
    "text": "when you go to Amazon redshift console and say that you want data warehousing cluster you go and specify a few",
    "start": "147950",
    "end": "155480"
  },
  {
    "text": "properties like how many nodes you want and so on so forth and go ahead and spin up the cluster so bring these things",
    "start": "155480",
    "end": "161390"
  },
  {
    "text": "what happens is when you say I want so many nodes in the cluster you're",
    "start": "161390",
    "end": "166459"
  },
  {
    "text": "essentially referring to what is called as compute nodes these compute nodes are the ones which are holding your data and",
    "start": "166459",
    "end": "173390"
  },
  {
    "text": "they are the ones which are running your queries when you submit a query to the cluster you as a customer you can decide",
    "start": "173390",
    "end": "180440"
  },
  {
    "text": "how many number of compute nodes you want and there are different types of compute nodes we will talk about it towards the end of the webinar but our",
    "start": "180440",
    "end": "187820"
  },
  {
    "text": "ones the provision the cluster we automatically throw a leader node and",
    "start": "187820",
    "end": "193730"
  },
  {
    "text": "the responsibility of the leader node is to expose Dana's endpoint through which",
    "start": "193730",
    "end": "199070"
  },
  {
    "text": "I it can receive sequel queries so you can use any popular sequel client or be",
    "start": "199070",
    "end": "206480"
  },
  {
    "text": "a tool which supports JDBC ODBC that can directly connect to the DNS endpoint",
    "start": "206480",
    "end": "211730"
  },
  {
    "text": "that are the leader node exposes and submit queries to the cluster now once the leader node receives the",
    "start": "211730",
    "end": "218420"
  },
  {
    "text": "query it prepares the query execution plan compiles the runtime compiled code",
    "start": "218420",
    "end": "224769"
  },
  {
    "text": "since the code down to the compute nodes the compute node work on their own data",
    "start": "224769",
    "end": "231290"
  },
  {
    "text": "set based on the code ever given by the leader node and then they submit partial",
    "start": "231290",
    "end": "237079"
  },
  {
    "text": "results back to the leader node the leader node does the final aggregation",
    "start": "237079",
    "end": "242269"
  },
  {
    "text": "and sends back the result of cycle client right in addition to the compute",
    "start": "242269",
    "end": "248570"
  },
  {
    "text": "nodes every redshift cluster also has access to what is called as Amazon redshift spectrum the spectrum is is is",
    "start": "248570",
    "end": "257239"
  },
  {
    "text": "a feature of Amazon redshift through which your redshift cluster can directly talk to Amazon s3 right you can hold",
    "start": "257239",
    "end": "264979"
  },
  {
    "text": "your data on Amazon s3 and you can submit a query which can directly run against the data set available on Amazon s3 so",
    "start": "264979",
    "end": "273110"
  },
  {
    "text": "that's the high-level architecture of our friendship right now let's dive",
    "start": "273110",
    "end": "279080"
  },
  {
    "start": "279000",
    "end": "279000"
  },
  {
    "text": "little deep on to some of the concepts starting with the first one which is one of the fundamental differences of fret",
    "start": "279080",
    "end": "284509"
  },
  {
    "text": "shift right so redshift like I said earlier it is a relational database but it's a columnar database right so it",
    "start": "284509",
    "end": "292940"
  },
  {
    "text": "uses a columnar architecture when it stores the data in the underlying disk that the key goal by following a",
    "start": "292940",
    "end": "300139"
  },
  {
    "text": "columnar architecture is essentially to reduce the i/o operations for your queries right so when you're storing the",
    "start": "300139",
    "end": "307970"
  },
  {
    "text": "data physically on a columnar format when you're submitting analytical queries which typically scan only a",
    "start": "307970",
    "end": "315650"
  },
  {
    "text": "subset of the columns of your table through a columnar architecture you can effectively read only the columns that",
    "start": "315650",
    "end": "323300"
  },
  {
    "text": "are interested and skip everything else of your table right this drastically reduces the amount of i/o that the",
    "start": "323300",
    "end": "329300"
  },
  {
    "text": "system has to do from the underlying disk so let's look at example so let's",
    "start": "329300",
    "end": "335240"
  },
  {
    "text": "say you have that table called as deep dive right it has three columns and you're ingesting let's say those four",
    "start": "335240",
    "end": "341960"
  },
  {
    "text": "records into this particular table right what you see on the bottom left is",
    "start": "341960",
    "end": "348380"
  },
  {
    "text": "basically a representation of the underlying blocks for for those columns",
    "start": "348380",
    "end": "354740"
  },
  {
    "text": "in the ania table right so when you when",
    "start": "354740",
    "end": "361639"
  },
  {
    "start": "358000",
    "end": "358000"
  },
  {
    "text": "you let's say have that kind of a table and you submit a query like select min off date from a deep dive table if",
    "start": "361639",
    "end": "368210"
  },
  {
    "text": "you're storing on a row based storage what needs to happen is it needs to read",
    "start": "368210",
    "end": "374000"
  },
  {
    "text": "all the rows of all the columns but the query is interested in only on the date",
    "start": "374000",
    "end": "379220"
  },
  {
    "text": "column right so while it has to read all the columns bring that all the data into the memory and then filter out what is",
    "start": "379220",
    "end": "386000"
  },
  {
    "text": "not required and only compute the minimum of the date column right so this basically leads to a lot of unnecessary",
    "start": "386000",
    "end": "393039"
  },
  {
    "text": "read from the underlying disk unnecessary i/o operations on the underlying storage system",
    "start": "393039",
    "end": "398950"
  },
  {
    "text": "now if you're able to flip that model and if you're able to store in a columnar fashion right instead of a",
    "start": "398950",
    "end": "406030"
  },
  {
    "text": "robust fashion what really happens is when you submit the same query you're able to read only the date column and",
    "start": "406030",
    "end": "413220"
  },
  {
    "text": "compute the mean of the data right so this basically scans only the relevant blocks from the underlying disk for the",
    "start": "413220",
    "end": "419650"
  },
  {
    "text": "particular column drastically reducing the amount of i/o that needs to happen on the system right awesome so that's",
    "start": "419650",
    "end": "429790"
  },
  {
    "text": "columnar right the first concepts the redshift employ is to reduce the amount of i/o right the second thing that",
    "start": "429790",
    "end": "436110"
  },
  {
    "text": "redshift can do to reduce the i/o is compression right so as the as is fairly",
    "start": "436110",
    "end": "442930"
  },
  {
    "text": "evident so compression allows us to store more data on a given block right",
    "start": "442930",
    "end": "448090"
  },
  {
    "text": "which means that on a single operation we could basically read more data from",
    "start": "448090",
    "end": "453460"
  },
  {
    "text": "the underlying blocks than compared to let's say a not so compressed table",
    "start": "453460",
    "end": "459280"
  },
  {
    "text": "right now typically on a conservative number you would be easily able to store",
    "start": "459280",
    "end": "468370"
  },
  {
    "text": "two to four times of raw data by leveraging compression within the cluster right we will talk about the",
    "start": "468370",
    "end": "476230"
  },
  {
    "text": "incision mechanism but we recommend using the copy command to load data into",
    "start": "476230",
    "end": "481600"
  },
  {
    "text": "redshift from s3 when you leverage the copy command it automatically compresses your data by",
    "start": "481600",
    "end": "488470"
  },
  {
    "text": "picking the right compression n coatings for your table right we will talk that a we'll talk about it",
    "start": "488470",
    "end": "494140"
  },
  {
    "text": "in a little bit more detail when we talk about the ingestion mechanisms so going back to the same table the deep dive",
    "start": "494140",
    "end": "500260"
  },
  {
    "text": "table what you could do as a customer is that we support about 11 different",
    "start": "500260",
    "end": "507690"
  },
  {
    "start": "501000",
    "end": "501000"
  },
  {
    "text": "compression n codings while the copy command allows you to automatically pick",
    "start": "507690",
    "end": "513580"
  },
  {
    "text": "up the right compression encodings you as a customer if you know a particular column benefits from a",
    "start": "513580",
    "end": "519539"
  },
  {
    "text": "particular compression encoding so you have the option to define a compression",
    "start": "519540",
    "end": "524950"
  },
  {
    "text": "encoding for a particular column right so in this example you can go and say the ID column needs to use Z standard",
    "start": "524950",
    "end": "532870"
  },
  {
    "text": "the layered column needs to use run-length and so on so forth now if you're able to couple this with the",
    "start": "532870",
    "end": "538899"
  },
  {
    "text": "columnar architecture what this allows you to do is that each column can potentially occupy different storage",
    "start": "538899",
    "end": "547300"
  },
  {
    "text": "space because its columnar and each can benefit from its own compression encodings now think about this on a robe",
    "start": "547300",
    "end": "554019"
  },
  {
    "text": "a storage if you apply compression because a block contains multiple",
    "start": "554019",
    "end": "559240"
  },
  {
    "text": "columns of a single robe the compression encoding will not work equally for all",
    "start": "559240",
    "end": "564370"
  },
  {
    "text": "of the columns right it is not effective on a robe a system but in a columnar system the compression encodings are",
    "start": "564370",
    "end": "570370"
  },
  {
    "text": "very effective because it's oregan columnar format a particular block only contains values of a particular column",
    "start": "570370",
    "end": "576579"
  },
  {
    "text": "and you're able to get the maximum compression benefits for the particular column right so this drastically reduces",
    "start": "576579",
    "end": "583329"
  },
  {
    "text": "again the Ru operations from the underlying storage system so the best practice recommendation there is to",
    "start": "583329",
    "end": "590800"
  },
  {
    "start": "589000",
    "end": "589000"
  },
  {
    "text": "apply compression to all the tables if you are starting out fresh when you have an empty table when you load using the",
    "start": "590800",
    "end": "597699"
  },
  {
    "text": "copy command the copy command automatically analyzes samples the first",
    "start": "597699",
    "end": "603029"
  },
  {
    "text": "few hundred thousand records and then picks up the right compression encoding",
    "start": "603029",
    "end": "608290"
  },
  {
    "text": "z' and applies compression for your table so that happens automatically when you load data into a let's say an empty",
    "start": "608290",
    "end": "615100"
  },
  {
    "text": "table right if you want to figure out whether your tables are compressed or",
    "start": "615100",
    "end": "621639"
  },
  {
    "text": "not if you're unsure there is a query at the bottom right here if you run that",
    "start": "621639",
    "end": "627100"
  },
  {
    "text": "query it can tell you basically whether a particular column of your table is is",
    "start": "627100",
    "end": "633250"
  },
  {
    "text": "compressed or not right if it is not we would highly recommend you to go and compress your data so that it helps us",
    "start": "633250",
    "end": "640750"
  },
  {
    "text": "reduce the amount of i/o so we spoke about columnar we spoke about",
    "start": "640750",
    "end": "646689"
  },
  {
    "text": "compression let's talk about the next concept which is zone maps right so zone",
    "start": "646689",
    "end": "653829"
  },
  {
    "text": "maps are nothing but in-memory metadata about the underlying blocks in your",
    "start": "653829",
    "end": "659910"
  },
  {
    "text": "cluster right so remember each block is holding a particular columns value",
    "start": "659910",
    "end": "666030"
  },
  {
    "text": "so let's say you have a ID column and a particular block has values of let's say",
    "start": "666030",
    "end": "671790"
  },
  {
    "text": "10,000 IDs right now what this one mag does is that it maintains the min and",
    "start": "671790",
    "end": "677730"
  },
  {
    "text": "Max value of that ID in that particular block all right now where this helps is",
    "start": "677730",
    "end": "684120"
  },
  {
    "text": "that when the leader node is is preparing that execution plan it can",
    "start": "684120",
    "end": "689880"
  },
  {
    "text": "refer to zone map and figure out which blocks needs to be read and which block",
    "start": "689880",
    "end": "695190"
  },
  {
    "text": "can be skipped so let's say you have a filter clause on that ID column you're only interested in let's say I put it or",
    "start": "695190",
    "end": "701490"
  },
  {
    "text": "ID or a particular date if we know the min and Max values we can figure out",
    "start": "701490",
    "end": "707730"
  },
  {
    "text": "which block can be skipped because they are filter clause for your query right",
    "start": "707730",
    "end": "713670"
  },
  {
    "text": "that's where a zone map helps so that's one concept so zone maps is a result is",
    "start": "713670",
    "end": "722880"
  },
  {
    "text": "a behavior on the system that happens automatically right now but what you can do as a customer to influence a better",
    "start": "722880",
    "end": "731520"
  },
  {
    "text": "zone map is to do something called as data sorting right now what data sorting",
    "start": "731520",
    "end": "737130"
  },
  {
    "text": "allows us to do is if you define sorting based on a particular column we are",
    "start": "737130",
    "end": "743520"
  },
  {
    "text": "going to physically sort the data and store the data in a sorted fashion in",
    "start": "743520",
    "end": "749250"
  },
  {
    "text": "the underlying blocks and use the sadiq mechanism to prune the blocks by",
    "start": "749250",
    "end": "756030"
  },
  {
    "text": "leveraging the zone Maps right so if you have a sorted block it's it's it's",
    "start": "756030",
    "end": "761520"
  },
  {
    "text": "perfectly sorted you exactly know what is the min and the max value and when you fire a query with a filter we can",
    "start": "761520",
    "end": "768120"
  },
  {
    "text": "clearly figure out okay go to this particular block read only these values in a particular block right so sorting",
    "start": "768120",
    "end": "775170"
  },
  {
    "text": "helps us to run all those range restricted scans and prune blocks",
    "start": "775170",
    "end": "780260"
  },
  {
    "text": "effectively right how do you pick up a shot key it's going to depend on your",
    "start": "780260",
    "end": "786510"
  },
  {
    "text": "query patterns your business use cases how you defend your schema and so on so forth right so most data warehousing use",
    "start": "786510",
    "end": "793170"
  },
  {
    "text": "cases have some kind of a a date/time column as part of the filter so it's",
    "start": "793170",
    "end": "798990"
  },
  {
    "text": "generally a temporal column which is used as a sort key and you can pick up that as you sake",
    "start": "798990",
    "end": "804570"
  },
  {
    "text": "for your table right so here's an example let's go back to the same deep dive table we have those three columns",
    "start": "804570",
    "end": "810600"
  },
  {
    "start": "810000",
    "end": "810000"
  },
  {
    "text": "right so this was the original table right we had insisted for records right",
    "start": "810600",
    "end": "817910"
  },
  {
    "text": "to SFO locations to JFK locations and a few values for the date column",
    "start": "817910",
    "end": "823800"
  },
  {
    "text": "right now if we go and add a sort key right you can add a composite short key",
    "start": "823800",
    "end": "829950"
  },
  {
    "text": "on one or more columns so let's say we go and add sort key on date and location",
    "start": "829950",
    "end": "835730"
  },
  {
    "text": "essentially we go and sort by date and then by location and then store that in",
    "start": "835730",
    "end": "840870"
  },
  {
    "text": "that fashion in the underlying disk physically right we are effectively sorting it and",
    "start": "840870",
    "end": "846420"
  },
  {
    "text": "storing it as it as a Saturday data set in the underlying disk now let's see an",
    "start": "846420",
    "end": "852149"
  },
  {
    "text": "impact of this on how zone maps help in optimal partition pruning right so let's",
    "start": "852149",
    "end": "859950"
  },
  {
    "text": "say we had an unsorted table right and these ello boxes represent the physical",
    "start": "859950",
    "end": "865740"
  },
  {
    "text": "blocks of this unsorted table right the zone map for this particular runs or a",
    "start": "865740",
    "end": "871560"
  },
  {
    "text": "table will have values like min of first June and Max of 28th June for the first",
    "start": "871560",
    "end": "877740"
  },
  {
    "text": "block and so on so forth right so while the first block says min is 1 and max is",
    "start": "877740",
    "end": "883110"
  },
  {
    "text": "20 right now let's say I go on fire a query where it says date is equal to",
    "start": "883110",
    "end": "891089"
  },
  {
    "text": "let's say sixth right now we may have to go and read all those three blocks",
    "start": "891089",
    "end": "899579"
  },
  {
    "text": "because the first block has min and Max as one and 20 the third block also has",
    "start": "899579",
    "end": "905579"
  },
  {
    "text": "and fourth block also has and so on and so forth now it's quite likely that",
    "start": "905579",
    "end": "911370"
  },
  {
    "text": "while the first block has one and 20 and you had a condition for 6 it may only",
    "start": "911370",
    "end": "916620"
  },
  {
    "text": "have values of 1 and 26 may not be present because it is not sorted all",
    "start": "916620",
    "end": "923040"
  },
  {
    "text": "right now if I sort it and the table is perfectly sorted based on the particular",
    "start": "923040",
    "end": "928770"
  },
  {
    "text": "let's say date column and I file the same query we can go to a particular block and say read only this block and skip",
    "start": "928770",
    "end": "935910"
  },
  {
    "text": "all other blocks from the underlying disk right this has a direct impact on how much data we have to physically read",
    "start": "935910",
    "end": "942750"
  },
  {
    "text": "from the underlying disks right so the",
    "start": "942750",
    "end": "948330"
  },
  {
    "text": "best practice around schottky is to place a sort key on columns that are",
    "start": "948330",
    "end": "954120"
  },
  {
    "text": "frequently filtered right on most fact tables we find a temporal column like a",
    "start": "954120",
    "end": "961320"
  },
  {
    "text": "date date time column as a natural choice right so pick up those and",
    "start": "961320",
    "end": "967220"
  },
  {
    "start": "963000",
    "end": "963000"
  },
  {
    "text": "leverage sort keys for all your partition pruning right if you have a",
    "start": "967220",
    "end": "973230"
  },
  {
    "text": "well-established workload so let's say you picked up a sort key it's been under",
    "start": "973230",
    "end": "979020"
  },
  {
    "text": "used for let's say last six to eight months a lot of careers have gone through you could use these utilities",
    "start": "979020",
    "end": "985770"
  },
  {
    "text": "I'll give you a reference to at the end of the dig but these utilities I can figure out what is the right sort keys",
    "start": "985770",
    "end": "993240"
  },
  {
    "text": "based on what queries are being fired by the end user right so you might have",
    "start": "993240",
    "end": "998730"
  },
  {
    "text": "designed the schema based on some initial inputs picking up a particular sort key but users could be using a",
    "start": "998730",
    "end": "1005600"
  },
  {
    "text": "different filter class altogether so this particular queries will help you figure out what are the optimal sort key",
    "start": "1005600",
    "end": "1012170"
  },
  {
    "text": "recommendations right okay so that's data sorting let's move on to the next",
    "start": "1012170",
    "end": "1019460"
  },
  {
    "text": "concept which is around slices right so if you if you recollect and the red",
    "start": "1019460",
    "end": "1025100"
  },
  {
    "start": "1020000",
    "end": "1020000"
  },
  {
    "text": "shift cluster has one leader node and we have a bunch of compute nodes you can",
    "start": "1025100",
    "end": "1030110"
  },
  {
    "text": "decide how many compute nodes you want right from one all the way up to 128",
    "start": "1030110",
    "end": "1035959"
  },
  {
    "text": "right now a slice can be thought of like a virtual compute node or let's say at",
    "start": "1035959",
    "end": "1044688"
  },
  {
    "text": "the smallest unit of compute resources available within a compute node right",
    "start": "1044689",
    "end": "1051980"
  },
  {
    "text": "so each compute node is further bifurcated into what is called as slices",
    "start": "1051980",
    "end": "1057410"
  },
  {
    "text": "right the number of slices for a particular compute node is fixed it's",
    "start": "1057410",
    "end": "1062780"
  },
  {
    "text": "documented it doesn't change so it's either 2 or 16 or 32 depending a the node type right so let's say if it's",
    "start": "1062780",
    "end": "1070390"
  },
  {
    "text": "a DC 2 dot large instance type it has 2 slices and it's documented right what",
    "start": "1070390",
    "end": "1076630"
  },
  {
    "text": "that essentially means is that if a node has 2 slices the total resources",
    "start": "1076630",
    "end": "1081790"
  },
  {
    "text": "available in that node is divided by 2 and is given to each slice right so on",
    "start": "1081790",
    "end": "1087730"
  },
  {
    "text": "that DC 2 dot large it has 2 CPUs 15 GB of memory and 160 GB of storage if you",
    "start": "1087730",
    "end": "1095320"
  },
  {
    "text": "have 2 slices each slice get an equal portion of that all right now well this",
    "start": "1095320",
    "end": "1101320"
  },
  {
    "text": "has an implication is when you submit a query each slice is basically going to",
    "start": "1101320",
    "end": "1106480"
  },
  {
    "text": "process only its own data what this means for us is that if you have a 10",
    "start": "1106480",
    "end": "1113380"
  },
  {
    "text": "node cluster on each node as two slices you have 20 slices altogether you want to make sure that all 20 slices are able",
    "start": "1113380",
    "end": "1120610"
  },
  {
    "text": "to utilize and and work the same way you",
    "start": "1120610",
    "end": "1125740"
  },
  {
    "text": "don't have one slice doing more work than the other slice in the cluster right you want some kind of a uniform",
    "start": "1125740",
    "end": "1131260"
  },
  {
    "text": "usage across all the slices in your cluster all right the new load data into",
    "start": "1131260",
    "end": "1139870"
  },
  {
    "text": "redshift we're going to distribute the data and store that data across all the",
    "start": "1139870",
    "end": "1145650"
  },
  {
    "text": "compute nodes or actually all the slices right slice is the one which is actually holding the data as well and so we're",
    "start": "1145650",
    "end": "1152650"
  },
  {
    "text": "going to distribute the data across all the slices in your cluster now there are",
    "start": "1152650",
    "end": "1158230"
  },
  {
    "text": "a few choices in terms of how data gets distributed depending upon how you want",
    "start": "1158230",
    "end": "1163690"
  },
  {
    "text": "the system to distribute it you as a customer have some some control over how",
    "start": "1163690",
    "end": "1168970"
  },
  {
    "text": "the distribution happens it's called as distribution style and there are three options really right you star you can",
    "start": "1168970",
    "end": "1175780"
  },
  {
    "text": "have a key based distribution or all based distribution or even based distribution the primary goal with",
    "start": "1175780",
    "end": "1183160"
  },
  {
    "text": "distribution is to make sure that we are able to evenly distribute data across all the slices and evenly process the",
    "start": "1183160",
    "end": "1191440"
  },
  {
    "text": "data when you submit a query so when you submit the query you want every slice to do equal amount of work on its own data",
    "start": "1191440",
    "end": "1198460"
  },
  {
    "text": "without doing a lot of chatter between the this so that you get like maximum power",
    "start": "1198460",
    "end": "1203600"
  },
  {
    "text": "parallel processing each slice does its own work sends back its own results to the leader node the leader node",
    "start": "1203600",
    "end": "1209090"
  },
  {
    "text": "basically aggregates the result and since back to the client so there's nothing in between no internal",
    "start": "1209090",
    "end": "1214670"
  },
  {
    "text": "communications no interest less communications no moment of data and so on so forth that's our eventual goal right now let's",
    "start": "1214670",
    "end": "1221570"
  },
  {
    "text": "see how you can use different distribution styles and distribute the data right so let's go back to the same",
    "start": "1221570",
    "end": "1228770"
  },
  {
    "text": "table the deep dive table we have three columns right and we have three choices right in terms of the distribution style",
    "start": "1228770",
    "end": "1235700"
  },
  {
    "start": "1234000",
    "end": "1234000"
  },
  {
    "text": "so when you create the table you're going to say disc style and then nominate one of those styles even key",
    "start": "1235700",
    "end": "1242840"
  },
  {
    "text": "are all the default distribution style if you don't mention anything is even",
    "start": "1242840",
    "end": "1248450"
  },
  {
    "text": "right now let's go back to an example let's say we have the table and we define a distribution style as even and",
    "start": "1248450",
    "end": "1255520"
  },
  {
    "text": "we are inserting those four records into the table now what even does is that it",
    "start": "1255520",
    "end": "1261230"
  },
  {
    "text": "does a round-robin distribution of the data right so we we simply look at the",
    "start": "1261230",
    "end": "1270200"
  },
  {
    "text": "incoming data we simply distribute that in a round robin fashion from one slice",
    "start": "1270200",
    "end": "1276320"
  },
  {
    "text": "to the next slice and so on so forth so those four records if you see the first record is going to go to the slice one",
    "start": "1276320",
    "end": "1281630"
  },
  {
    "text": "the second record their third record their fourth record they're right so",
    "start": "1281630",
    "end": "1286730"
  },
  {
    "text": "each slice basically gets in a round robin fashion if you lower subsequent data it keeps going on a round robin",
    "start": "1286730",
    "end": "1292850"
  },
  {
    "text": "fashion so you really you don't have any strategy here it's simple round robin pure brainless round robin kind of fun",
    "start": "1292850",
    "end": "1299570"
  },
  {
    "text": "model right now let's look at the other distribution style which is a key based",
    "start": "1299570",
    "end": "1306800"
  },
  {
    "text": "distribution right and this is where you say when you create the table you want a key based distribution style and you",
    "start": "1306800",
    "end": "1314090"
  },
  {
    "text": "actually nominate a particular column on which the distribution needs to happen",
    "start": "1314090",
    "end": "1319630"
  },
  {
    "text": "right so in this case you are saying that it's based on a location column now when you define something like this we",
    "start": "1319630",
    "end": "1326510"
  },
  {
    "text": "take the same data set or it happens is the first record goes to the first slice because it's SFO and a second record",
    "start": "1326510",
    "end": "1334410"
  },
  {
    "text": "to the second slice because it's JFK so what you see here is we are basically hashing the value of that particular",
    "start": "1334410",
    "end": "1342030"
  },
  {
    "text": "column and for every unique hash we're basically sending it to a particular slice right now first went there second",
    "start": "1342030",
    "end": "1350430"
  },
  {
    "text": "went to the second slice the third record because it is again SFO it goes to the first slice the fourth record",
    "start": "1350430",
    "end": "1357150"
  },
  {
    "text": "which is JFK it goes to the second slice right so if you have nominated a",
    "start": "1357150",
    "end": "1363030"
  },
  {
    "text": "particular column we take the value of the particular column we hash it and all the values of a particular hash goes to",
    "start": "1363030",
    "end": "1371010"
  },
  {
    "text": "the same slice in the cluster right now where this is helpful is that when you",
    "start": "1371010",
    "end": "1378720"
  },
  {
    "text": "are let's say performing joins on group buys each slice can independently work",
    "start": "1378720",
    "end": "1384540"
  },
  {
    "text": "on its own data right so if you fire a guru by operation if it's going to group by by SFO that operation can work within",
    "start": "1384540",
    "end": "1393150"
  },
  {
    "text": "slice 0 and for JFK it can happen within slice 1 and there is no need for let's",
    "start": "1393150",
    "end": "1399990"
  },
  {
    "text": "say cross slice or cross nord communication right what you'll also",
    "start": "1399990",
    "end": "1406290"
  },
  {
    "text": "notice in in this model is a downside where the other node which has two slices they don't have any data at all",
    "start": "1406290",
    "end": "1413370"
  },
  {
    "text": "in this example right now that's basically an anti-pattern where if I",
    "start": "1413370",
    "end": "1420000"
  },
  {
    "text": "pick up a distribution key and it results on a certain number of slices",
    "start": "1420000",
    "end": "1428970"
  },
  {
    "text": "having more data than other slices and essentially when your query is running",
    "start": "1428970",
    "end": "1434340"
  },
  {
    "text": "it is not going to utilize equally all the slices in your cluster G slice is",
    "start": "1434340",
    "end": "1440010"
  },
  {
    "text": "going to do different work right so some slaves could be doing more work some slaves could be just sleeping because",
    "start": "1440010",
    "end": "1445710"
  },
  {
    "text": "they don't have any data for the particular query right so the second Arianna why'd you want to pick up a key",
    "start": "1445710",
    "end": "1450780"
  },
  {
    "text": "which gives you good cardinality and gives you good distribution across all the slices well let's say we change the",
    "start": "1450780",
    "end": "1459720"
  },
  {
    "text": "distribution key instead of location we pick up let's say ID now what that happens is it again you",
    "start": "1459720",
    "end": "1467440"
  },
  {
    "text": "the better distribution where different IDs goes to different slices and if you again get ID is equal to one and two if",
    "start": "1467440",
    "end": "1474250"
  },
  {
    "text": "you go to the respective slices right so obviously I kind of faked this example but hopefully gives you an idea in terms",
    "start": "1474250",
    "end": "1480370"
  },
  {
    "text": "of how the disky based distribution works for you all right",
    "start": "1480370",
    "end": "1486179"
  },
  {
    "text": "the last option for you from a distribution style perspective is an all based distribution where if you define a",
    "start": "1486179",
    "end": "1493690"
  },
  {
    "text": "table with all as a distribution style we take a copy of the data and we",
    "start": "1493690",
    "end": "1499990"
  },
  {
    "text": "maintain the entire tables data on the first slice of every node in the cluster",
    "start": "1499990",
    "end": "1506879"
  },
  {
    "text": "right so every first slice in every node contains the full copy of the entire",
    "start": "1506879",
    "end": "1513879"
  },
  {
    "text": "tables data set right that's why it's called as all based distribution right",
    "start": "1513879",
    "end": "1520200"
  },
  {
    "text": "so those are the three different distribution Styles you really get so when do you choose a key based",
    "start": "1520200",
    "end": "1526120"
  },
  {
    "text": "distribution when do you choose an even and all and so on and so forth right so a key based distribution works really",
    "start": "1526120",
    "end": "1532750"
  },
  {
    "text": "well for for large fact tables right so",
    "start": "1532750",
    "end": "1537850"
  },
  {
    "text": "if you have large fact tables and you frequently join those fat tables with dimension tables right the join",
    "start": "1537850",
    "end": "1545110"
  },
  {
    "text": "performance can be better because if you define a common if you define the join",
    "start": "1545110",
    "end": "1550149"
  },
  {
    "text": "clause as the distribution key then data colocation happens right between those",
    "start": "1550149",
    "end": "1555820"
  },
  {
    "text": "tables so because of that there is no inter node communication but join happens locally within the slice and the",
    "start": "1555820",
    "end": "1562059"
  },
  {
    "text": "performance is much much better right so so similarly grew by operations join",
    "start": "1562059",
    "end": "1567700"
  },
  {
    "text": "operations all of them benefit by a key based distribution right and like I said",
    "start": "1567700",
    "end": "1572950"
  },
  {
    "text": "earlier if you are picking a column you need to ensure that there is high cardinality so that it doesn't result in",
    "start": "1572950",
    "end": "1579519"
  },
  {
    "text": "a skewness where one slice has more data than other slices in your cluster so",
    "start": "1579519",
    "end": "1585159"
  },
  {
    "text": "there is a system table called as SVD table info where if you file the particular query there is a column",
    "start": "1585159",
    "end": "1591279"
  },
  {
    "text": "called ask you rows which can tell you if there is data skewness what that skew",
    "start": "1591279",
    "end": "1596590"
  },
  {
    "text": "rows really does is that it gives you a ratio between the slice that has most data with the",
    "start": "1596590",
    "end": "1604540"
  },
  {
    "text": "slice that has least data ideally if that number is close to 1 it means that",
    "start": "1604540",
    "end": "1610630"
  },
  {
    "text": "more or less all the slices have equal amount of data if that number starts",
    "start": "1610630",
    "end": "1616150"
  },
  {
    "text": "going to let's say 2 and 3 and so far you mean it means that you have one",
    "start": "1616150",
    "end": "1621190"
  },
  {
    "text": "slice which is having twice or Trice the data of other slice in the cluster so",
    "start": "1621190",
    "end": "1626530"
  },
  {
    "text": "there is significant data skewness right so that's helpful an all based",
    "start": "1626530",
    "end": "1632070"
  },
  {
    "text": "distribution we really see that coming along for smaller tables where if you",
    "start": "1632070",
    "end": "1638470"
  },
  {
    "text": "are it's a smaller dimension tables you can keep a copy of that across each",
    "start": "1638470",
    "end": "1643960"
  },
  {
    "text": "slice in the in your cluster right now what we call as a small table in in red",
    "start": "1643960",
    "end": "1650170"
  },
  {
    "text": "ship is typically tables which have less than 3 million rows or small-to-medium",
    "start": "1650170",
    "end": "1655240"
  },
  {
    "text": "tables if you have those dimension tables you could simply use an all based",
    "start": "1655240",
    "end": "1660250"
  },
  {
    "text": "distribution and it it keeps basically a copy of the entire table on the first slice in the cluster now an always",
    "start": "1660250",
    "end": "1667570"
  },
  {
    "text": "distribution and may not be efficient for very large tables because you'd",
    "start": "1667570",
    "end": "1673450"
  },
  {
    "text": "basically pay you you basically end up spending a lot of money on just storage",
    "start": "1673450",
    "end": "1679120"
  },
  {
    "text": "right because you're replicating the entire table data across all the slices right and human based distribution style",
    "start": "1679120",
    "end": "1687100"
  },
  {
    "text": "is really helpful let's see if you are completely unsure you know you do not know where to start start with even base",
    "start": "1687100",
    "end": "1692620"
  },
  {
    "text": "distribution it's not so bad it could be a good start and once you start running",
    "start": "1692620",
    "end": "1698350"
  },
  {
    "text": "some use cases if you figure out a pattern then you can look at changing a distribution style for your table so for",
    "start": "1698350",
    "end": "1707890"
  },
  {
    "text": "the summary from a table design perspective is picking up the right distribution style for your use cases",
    "start": "1707890",
    "end": "1713260"
  },
  {
    "text": "that's going to be a key driver for getting the best performance from your system adding sort keys on on frequently",
    "start": "1713260",
    "end": "1722110"
  },
  {
    "start": "1715000",
    "end": "1715000"
  },
  {
    "text": "filtered columns is going to help you maintain a better zone map so that we are able to prune the right partitions",
    "start": "1722110",
    "end": "1728230"
  },
  {
    "text": "and read only the blocks that are necessary right do apply compression to",
    "start": "1728230",
    "end": "1733330"
  },
  {
    "text": "all the Able's the copy command on an empty table picks up compression automatically",
    "start": "1733330",
    "end": "1739549"
  },
  {
    "text": "you can also optimally specify a compression encoding for your table",
    "start": "1739549",
    "end": "1745340"
  },
  {
    "text": "right you can also run the command call us analyze compression which will automatically recommend which is a right",
    "start": "1745340",
    "end": "1751549"
  },
  {
    "text": "or optimal compression encoding for a particular column the other thing is",
    "start": "1751549",
    "end": "1756770"
  },
  {
    "text": "there is a general behavior we're developers when you define your table you try to define column lens with with",
    "start": "1756770",
    "end": "1766490"
  },
  {
    "text": "it the maximum possible length right so let's say if you have a ver care or a care you just say six five five three",
    "start": "1766490",
    "end": "1772580"
  },
  {
    "text": "six right because you're unsure what's going to come in we generally don't recommend that because what's going to",
    "start": "1772580",
    "end": "1778580"
  },
  {
    "text": "happen is if you define a column which which is much bigger than what it what",
    "start": "1778580",
    "end": "1784549"
  },
  {
    "text": "it's actually going to hold the the memory allocation for the particular column is going to depend on how wide",
    "start": "1784549",
    "end": "1791720"
  },
  {
    "text": "that column is right let's say you are defined of a column as Val care of six five five three six the system is going",
    "start": "1791720",
    "end": "1799220"
  },
  {
    "text": "to allocate when the query is running 65536 of memory for the particular column right for every column in your for every",
    "start": "1799220",
    "end": "1806780"
  },
  {
    "text": "column value of your of your table but in reality it might be just it's just where care of 256 or whatever it is so",
    "start": "1806780",
    "end": "1814010"
  },
  {
    "text": "you're gonna waste a lot of memory just because you're declaring very very wide columns right so keep it as wide as",
    "start": "1814010",
    "end": "1819980"
  },
  {
    "text": "necessary so that you are able to benefit from better memory utilization",
    "start": "1819980",
    "end": "1826090"
  },
  {
    "text": "okay so that's table design and let's move to the next topic which is around",
    "start": "1826630",
    "end": "1832400"
  },
  {
    "text": "storage ingestion and elt okay so Before",
    "start": "1832400",
    "end": "1840860"
  },
  {
    "text": "we jump into ingestion and elt based practices sub quick note on the",
    "start": "1840860",
    "end": "1847400"
  },
  {
    "start": "1846000",
    "end": "1846000"
  },
  {
    "text": "underlying storage and how does it really work in the system right so when",
    "start": "1847400",
    "end": "1852770"
  },
  {
    "text": "when you spin up a cluster you choose the node type or the instance type we",
    "start": "1852770",
    "end": "1859010"
  },
  {
    "text": "advertise the storage capacity for the particular instance type right so for for example DC 2 dot large has 160 GB of",
    "start": "1859010",
    "end": "1867440"
  },
  {
    "text": "storage right so that's the storage that as a customer you get to store data that's",
    "start": "1867440",
    "end": "1874239"
  },
  {
    "text": "your actual effective storage space that you can use to store your data and with",
    "start": "1874239",
    "end": "1879940"
  },
  {
    "text": "compression obviously you can store more data as well but behind the scenes for that particular compute node we are",
    "start": "1879940",
    "end": "1886989"
  },
  {
    "text": "throwing up typically two and half to three times of that advertised storage",
    "start": "1886989",
    "end": "1892269"
  },
  {
    "text": "capacity right why because each compute node actually holds its own local data whatever it's",
    "start": "1892269",
    "end": "1899769"
  },
  {
    "text": "supposed to hold but it also holds the mirror copy of another compute node so",
    "start": "1899769",
    "end": "1907690"
  },
  {
    "text": "if you have let's say a three node cluster each node is going to hold its",
    "start": "1907690",
    "end": "1912940"
  },
  {
    "text": "own data and also is going to maintain a copy of the other node in the cluster",
    "start": "1912940",
    "end": "1919539"
  },
  {
    "text": "right now this basically allows us for handling things like failures like for",
    "start": "1919539",
    "end": "1926889"
  },
  {
    "text": "disk failures node failures and so on so forth where we'll be able to quickly",
    "start": "1926889",
    "end": "1931959"
  },
  {
    "text": "replicate data from other nodes in the cluster right so it works",
    "start": "1931959",
    "end": "1937119"
  },
  {
    "text": "no big in the scenes so when you have a table when you load data into the table",
    "start": "1937119",
    "end": "1942779"
  },
  {
    "start": "1942000",
    "end": "1942000"
  },
  {
    "text": "a global commit will ensure that all the permanent tables right when you are",
    "start": "1942779",
    "end": "1948549"
  },
  {
    "text": "ingesting data we have returned the data to the local node and also to another",
    "start": "1948549",
    "end": "1956440"
  },
  {
    "text": "node in the cluster only then the commit is successful all right so only if we are successfully",
    "start": "1956440",
    "end": "1962499"
  },
  {
    "text": "returned to the local node and also a copy of the data into the other node",
    "start": "1962499",
    "end": "1967869"
  },
  {
    "text": "only then the commit goes successful right so this ensures that you have redundancy for your data when it come it",
    "start": "1967869",
    "end": "1974679"
  },
  {
    "text": "happens right so you basically get a tendency out of the box what also happens within the scenes is that while",
    "start": "1974679",
    "end": "1981369"
  },
  {
    "text": "we are redundantly copying the data to the other compute node we were also",
    "start": "1981369",
    "end": "1986429"
  },
  {
    "text": "asynchronously backing up the data to Amazon s3 as automated snapshots right",
    "start": "1986429",
    "end": "1993519"
  },
  {
    "text": "now how these snapshots are triggered are either on 5 GB increments of data or",
    "start": "1993519",
    "end": "2000329"
  },
  {
    "text": "8 hours which happens early we automatically trigger automated snapshot and keep a copy of",
    "start": "2000329",
    "end": "2007980"
  },
  {
    "text": "the data in history now this is useful for again handling failures if you have",
    "start": "2007980",
    "end": "2014220"
  },
  {
    "text": "a node failure or an entire cluster level failures to restore data to your cluster we would use a combination of",
    "start": "2014220",
    "end": "2021210"
  },
  {
    "text": "what's available on other compute nodes and also what's available in s3 to quickly replicate the data right but",
    "start": "2021210",
    "end": "2029429"
  },
  {
    "text": "that happens automatically for permanent tables but there are also a lot of use cases where you would create temporary",
    "start": "2029429",
    "end": "2035130"
  },
  {
    "text": "tables right so when you create temporary tables these tables are actually not mirrored to the remote",
    "start": "2035130",
    "end": "2040950"
  },
  {
    "text": "partition right so it spins out these temporary tables the data ingestion is",
    "start": "2040950",
    "end": "2046139"
  },
  {
    "text": "much more faster than permanent tables right so we will talk when you talk a",
    "start": "2046139",
    "end": "2052919"
  },
  {
    "text": "little bit about the ELT aspect we will talk about the staging mechanism where you can create temporary staging tables",
    "start": "2052919",
    "end": "2058470"
  },
  {
    "text": "bring data into that quickly ingest into those tables and then move to your production tables if you have a lot of",
    "start": "2058470",
    "end": "2065730"
  },
  {
    "text": "those temporary tables on if you also have users creating lot of scratchpad",
    "start": "2065730",
    "end": "2070829"
  },
  {
    "text": "tables right I create I do a CTS and create a table user tool a scratch pad and then I don't need this later what",
    "start": "2070829",
    "end": "2078148"
  },
  {
    "text": "you could also do is define the table definition with backup no which means a",
    "start": "2078149",
    "end": "2084030"
  },
  {
    "text": "be willing exclude these tables from the backups so that will help you save on the storage cost right ok so now that we",
    "start": "2084030",
    "end": "2093540"
  },
  {
    "text": "know a little bit about the the underlying storage let's talk about how to ingest data right so so redshift is a",
    "start": "2093540",
    "end": "2100500"
  },
  {
    "start": "2096000",
    "end": "2096000"
  },
  {
    "text": "relational database which means that you could use the little node to start firing insert queries into it and start",
    "start": "2100500",
    "end": "2107369"
  },
  {
    "text": "loading data into the table right but it's not recommended because it's not an oil TP system it's a de Rivero stand",
    "start": "2107369",
    "end": "2113760"
  },
  {
    "text": "overlap kind of a system so it's designed for really bulk writes right and right you want to load data in bulk",
    "start": "2113760",
    "end": "2120990"
  },
  {
    "text": "now what we recommend for data loading into redshift is to leverage something",
    "start": "2120990",
    "end": "2126359"
  },
  {
    "text": "called as a copy statement where you can bring your data into s3 and issue the copy statement to load the data into",
    "start": "2126359",
    "end": "2133020"
  },
  {
    "text": "your cluster but when you issued a copy statement the compute node directly picks up the file",
    "start": "2133020",
    "end": "2138840"
  },
  {
    "text": "from s3 and starts loading the data into into the cluster right now one thing to",
    "start": "2138840",
    "end": "2145020"
  },
  {
    "text": "note here is that if you have a large cluster with many number of compute nodes you have many number of slices",
    "start": "2145020",
    "end": "2151470"
  },
  {
    "text": "right so here's an example where a dc28 Excel has 16 slices on a single computer",
    "start": "2151470",
    "end": "2157020"
  },
  {
    "text": "note if I have one single file a slice is going to pick up that file and only",
    "start": "2157020",
    "end": "2163080"
  },
  {
    "text": "one slice will will load that file and distribute the data across the slices",
    "start": "2163080",
    "end": "2170070"
  },
  {
    "text": "right so the other 515 slices are going to remain idle because only one slice is",
    "start": "2170070",
    "end": "2175440"
  },
  {
    "text": "basically picking up the file now instead of let's say one large file if",
    "start": "2175440",
    "end": "2181350"
  },
  {
    "text": "somehow you're able to split the input file into multiple fights right and what",
    "start": "2181350",
    "end": "2186750"
  },
  {
    "text": "happens is each slice can independently pick up one one file and start working",
    "start": "2186750",
    "end": "2191850"
  },
  {
    "text": "independently this will help you insist much more quickly right because you get",
    "start": "2191850",
    "end": "2197940"
  },
  {
    "text": "paddle is some by utilizing all the slices so our recommendation there really is to have the number of files as",
    "start": "2197940",
    "end": "2205740"
  },
  {
    "text": "a multiplier of the number of slices in your cluster if you have 16 slices have",
    "start": "2205740",
    "end": "2213590"
  },
  {
    "text": "number of files is a multiple of 16 all right 16 32 and so on so forth",
    "start": "2213590",
    "end": "2218670"
  },
  {
    "text": "so that each slice can pick up a file and work on it independently you can",
    "start": "2218670",
    "end": "2225780"
  },
  {
    "text": "also look at an optimal file size the recommendation there is a file size of",
    "start": "2225780",
    "end": "2231540"
  },
  {
    "text": "between 1 MB and 1 GB you don't want to go too small that'll make like more s3",
    "start": "2231540",
    "end": "2237360"
  },
  {
    "text": "calls you also don't want to have too big files right so our recommendation there is around 1 and B 2 1 GB after",
    "start": "2237360",
    "end": "2244830"
  },
  {
    "text": "compression we the copy command supports loading compressed file directly using",
    "start": "2244830",
    "end": "2250680"
  },
  {
    "text": "the copy command to the cluster so it's really designed for large writes it's a",
    "start": "2250680",
    "end": "2256290"
  },
  {
    "text": "batch processing system so try to load data in bulk by bringing the data into",
    "start": "2256290",
    "end": "2263490"
  },
  {
    "start": "2258000",
    "end": "2258000"
  },
  {
    "text": "s3 and leveraging the copy command right",
    "start": "2263490",
    "end": "2268800"
  },
  {
    "text": "updates and deletes so I underneath we basically have immutable blocks which",
    "start": "2268800",
    "end": "2274500"
  },
  {
    "text": "means that when you have let's say updates and on deletes we really go we really don't go and",
    "start": "2274500",
    "end": "2281250"
  },
  {
    "text": "delete the data physically from the underlying blocks we basically only go and logically mark data so when you go",
    "start": "2281250",
    "end": "2288390"
  },
  {
    "text": "and delete the data we say simply logically this particular data is deleted an update is also a delete",
    "start": "2288390",
    "end": "2294570"
  },
  {
    "text": "insert for us all right so the dailies are essentially logical which means that you have to do some",
    "start": "2294570",
    "end": "2300300"
  },
  {
    "text": "kind of maintenance to recover the space and reorganize data right and that's",
    "start": "2300300",
    "end": "2306900"
  },
  {
    "text": "where something called as a vacuum operation is required to remove those ghost rows which are like logically",
    "start": "2306900",
    "end": "2312060"
  },
  {
    "text": "deleted from your table so let's talk about a practical use case where you",
    "start": "2312060",
    "end": "2319200"
  },
  {
    "start": "2317000",
    "end": "2317000"
  },
  {
    "text": "have some kind of an absurd happening you have an existing data so this is again going back to the same deep dive table I have this for records and I'm",
    "start": "2319200",
    "end": "2326190"
  },
  {
    "text": "getting a new data set which has two records which are like an update for an",
    "start": "2326190",
    "end": "2332070"
  },
  {
    "text": "existing record the same a ID one and two there's an update for that record and I'm also getting two new records",
    "start": "2332070",
    "end": "2338910"
  },
  {
    "text": "which are going to be new records like inserts basically a typical absurd operation so how do you kind of go about",
    "start": "2338910",
    "end": "2345210"
  },
  {
    "text": "doing this on redshift now the recommendation there is to think about",
    "start": "2345210",
    "end": "2350369"
  },
  {
    "text": "having a staging table right so define a staging table load all that data into",
    "start": "2350369",
    "end": "2356400"
  },
  {
    "text": "the staging table delete the duplicates from production table and simply insert an append from staging to the production",
    "start": "2356400",
    "end": "2363200"
  },
  {
    "text": "so the way you would do it is let's say you begin a transaction you create a temporary table called as a staging",
    "start": "2363200",
    "end": "2369210"
  },
  {
    "text": "table you can say like the production table so when you say like production table we",
    "start": "2369210",
    "end": "2375200"
  },
  {
    "text": "copy the entire table definition from the correction table including the compression end codings which means that",
    "start": "2375200",
    "end": "2381210"
  },
  {
    "text": "when you run the copy command you can turn off the compression exercise you",
    "start": "2381210",
    "end": "2386790"
  },
  {
    "text": "can say comp update off so come back upon does not do automatic compression right it doesn't have to sample and",
    "start": "2386790",
    "end": "2392550"
  },
  {
    "text": "figure out what compression encodings and so on and so forth because the table already has predefined compression in",
    "start": "2392550",
    "end": "2398490"
  },
  {
    "text": "codings right so you trigger the copy command never gets copied into the temporary table and then you say delete from",
    "start": "2398490",
    "end": "2406270"
  },
  {
    "text": "production using staging where ID is equal to ID from production is equal to ID from staging right and then you",
    "start": "2406270",
    "end": "2413320"
  },
  {
    "text": "insert from staging into production and drop and commit drop with sitting people",
    "start": "2413320",
    "end": "2419830"
  },
  {
    "text": "in commit right so this is the classic absurd recommendation from our side what",
    "start": "2419830",
    "end": "2426430"
  },
  {
    "text": "this will definitely require is to run those vacuum operations later because you are deleting records here at some",
    "start": "2426430",
    "end": "2432100"
  },
  {
    "text": "careful maintenance interval you have to run those vacuums to recover those ghost use-cases",
    "start": "2432100",
    "end": "2437110"
  },
  {
    "text": "right so the the best practice from an LP perspective is leverage a trading",
    "start": "2437110",
    "end": "2443380"
  },
  {
    "start": "2441000",
    "end": "2441000"
  },
  {
    "text": "table with cob with like a command like",
    "start": "2443380",
    "end": "2448390"
  },
  {
    "text": "like product production table so that compression in codings are automatically copied you can turn off comp update off",
    "start": "2448390",
    "end": "2454440"
  },
  {
    "text": "so that copy does not perform additional compression activities and for very",
    "start": "2454440",
    "end": "2463000"
  },
  {
    "text": "large number of rows let's say if you have a table value of load hundreds of",
    "start": "2463000",
    "end": "2469300"
  },
  {
    "text": "millions of records right very very large ingestion what you could do is you",
    "start": "2469300",
    "end": "2474490"
  },
  {
    "text": "instead of doing insert into select you could also use a command called as alter table append where you can have a",
    "start": "2474490",
    "end": "2481630"
  },
  {
    "text": "staging table which has let's say hundreds of millions of records you could say alter table production append",
    "start": "2481630",
    "end": "2487750"
  },
  {
    "text": "staging table what we really do is we don't actually do an insert into the",
    "start": "2487750",
    "end": "2492880"
  },
  {
    "text": "production table but we simply move the blocks around saying that these are the",
    "start": "2492880",
    "end": "2498400"
  },
  {
    "text": "new blocks the production table points to right it's much more efficient it's not it's not known a lot of people but",
    "start": "2498400",
    "end": "2504970"
  },
  {
    "text": "if you have like very very large ingestion all the table append is is also useful I touched upon vacuum",
    "start": "2504970",
    "end": "2513640"
  },
  {
    "text": "analyze right so vacuum is the command that you have to run to globally sort",
    "start": "2513640",
    "end": "2519220"
  },
  {
    "start": "2517000",
    "end": "2517000"
  },
  {
    "text": "the table again and remove the the rows that were marked as deleted so when you",
    "start": "2519220",
    "end": "2524770"
  },
  {
    "text": "delete the record we actually do a logical deletion which means that we have to physically again recover those",
    "start": "2524770",
    "end": "2529990"
  },
  {
    "text": "data so vacuum will do that now because it's a logical deli and these zone maps also goes for a toss",
    "start": "2529990",
    "end": "2536430"
  },
  {
    "text": "right so for the if you delete and let's say insert a new value for the same data",
    "start": "2536430",
    "end": "2541769"
  },
  {
    "text": "the new one it gets appended at the end of the block so the zone maps are actually out of a data right so that Kim",
    "start": "2541769",
    "end": "2548970"
  },
  {
    "text": "will remove all those rows and also globally sort the table again so that the zone maps are also updated right",
    "start": "2548970",
    "end": "2556400"
  },
  {
    "text": "analyze is another command which also helps in updating the state table",
    "start": "2556400",
    "end": "2561809"
  },
  {
    "text": "statistics if you have done let's say lots of observes and inserts and data loads the table statistics could be out",
    "start": "2561809",
    "end": "2568049"
  },
  {
    "text": "of date right so analyze will also help you to update those table statistics so",
    "start": "2568049",
    "end": "2573809"
  },
  {
    "text": "our recommendation around vacuum and analyze is run it as necessary like",
    "start": "2573809",
    "end": "2579089"
  },
  {
    "text": "depending upon how frequently you load data and so on so forth so we see most",
    "start": "2579089",
    "end": "2584430"
  },
  {
    "text": "customers doing like a daily vacuum or a weekly vacuum duty may see a maintenance",
    "start": "2584430",
    "end": "2589859"
  },
  {
    "text": "window or off-peak time right we also have a utility called as analyze vacuum",
    "start": "2589859",
    "end": "2596069"
  },
  {
    "text": "utility on our github repository which can be used to analyze and vacuum all",
    "start": "2596069",
    "end": "2601619"
  },
  {
    "text": "the tables in a cluster in an automatic fashion without you doing this manually for every table ok so hopefully that",
    "start": "2601619",
    "end": "2609630"
  },
  {
    "text": "gave you some kind of a idea around data ingestion some best practices there let's move the other topic which is",
    "start": "2609630",
    "end": "2616229"
  },
  {
    "text": "around workload management and query monitoring rules right now this is another important concept of redshift",
    "start": "2616229",
    "end": "2622589"
  },
  {
    "text": "where so you're going to use the cluster people are going to submit different types of queries",
    "start": "2622589",
    "end": "2627900"
  },
  {
    "start": "2623000",
    "end": "2623000"
  },
  {
    "text": "maybe people are using bi dashboards like your tableau or click and so on so",
    "start": "2627900",
    "end": "2632910"
  },
  {
    "text": "forth or they simply use a sequel client to fire queries to a cluster right now",
    "start": "2632910",
    "end": "2639420"
  },
  {
    "text": "when you expose a cluster to many users naturally you're going to have different types of workloads running on the",
    "start": "2639420",
    "end": "2647369"
  },
  {
    "text": "cluster so what I really mean by different types of workloads are so some could be firing a very very simple query",
    "start": "2647369",
    "end": "2653489"
  },
  {
    "text": "that runs for about five six seconds some other user could be firing a very",
    "start": "2653489",
    "end": "2658619"
  },
  {
    "text": "long query bit scans let's say billions of rows when these things are happening you could also have some kind of ETL or",
    "start": "2658619",
    "end": "2666359"
  },
  {
    "text": "new lt pipeline which is loading some data into your cluster right so you have all these",
    "start": "2666359",
    "end": "2671900"
  },
  {
    "text": "different things happening in the cluster and all of them are kind of fighting for resources right they are",
    "start": "2671900",
    "end": "2678080"
  },
  {
    "text": "kind of fighting with each other to get maximum resource from the cluster so how do you manage these different types of",
    "start": "2678080",
    "end": "2684800"
  },
  {
    "text": "workloads is all about workload management so through our workload",
    "start": "2684800",
    "end": "2692359"
  },
  {
    "text": "management what you could do is that you could define what's called a SKUs right now by default in the cluster has",
    "start": "2692359",
    "end": "2701000"
  },
  {
    "start": "2697000",
    "end": "2697000"
  },
  {
    "text": "one default queue when you submit a query the query goes third default queue in addition you as a customer it can",
    "start": "2701000",
    "end": "2709130"
  },
  {
    "text": "define additional queues and assign a percentage of memory to each queue if I",
    "start": "2709130",
    "end": "2716210"
  },
  {
    "text": "let's say total 100 personal media available on the cluster you could say 50 percent goes here 30 goes here and 20",
    "start": "2716210",
    "end": "2723020"
  },
  {
    "text": "goes here for different types of cues and then when your user submits the query you can route the queries to a",
    "start": "2723020",
    "end": "2730640"
  },
  {
    "text": "particular queue based on user groups our query groups that you can define",
    "start": "2730640",
    "end": "2735740"
  },
  {
    "text": "right now let's where is is useful so let's go with an example right so let's",
    "start": "2735740",
    "end": "2742040"
  },
  {
    "start": "2741000",
    "end": "2741000"
  },
  {
    "text": "say you have three different types of use cases I have a ingestion light ingestion that happens let's say every",
    "start": "2742040",
    "end": "2749270"
  },
  {
    "text": "30 minutes or every 15 minutes and so on so forth you also have the business are",
    "start": "2749270",
    "end": "2755810"
  },
  {
    "text": "during which your users are utilizing it for peak queries right 7:00 a.m. to 7:00",
    "start": "2755810",
    "end": "2762290"
  },
  {
    "text": "p.m. and during the night you have your heavy ingestion you know daily loads",
    "start": "2762290",
    "end": "2767480"
  },
  {
    "text": "where you're loading data in bulk doing multiple LPS and ETL and so on so forth alright so different types of use cases",
    "start": "2767480",
    "end": "2775099"
  },
  {
    "text": "used by different types of people in your organization right and what you",
    "start": "2775099",
    "end": "2780140"
  },
  {
    "text": "could do to make sure that each one gets their own resources right I can go and",
    "start": "2780140",
    "end": "2787160"
  },
  {
    "text": "define multiple queues in the system so that is a default queue which a system",
    "start": "2787160",
    "end": "2792260"
  },
  {
    "text": "comes with right it comes with five slots right so each slot can basically",
    "start": "2792260",
    "end": "2797330"
  },
  {
    "text": "run one query if it has five slots it can run fie query right by in addition you can create additional",
    "start": "2797330",
    "end": "2803180"
  },
  {
    "text": "cues like insertion cues and dashboard cues and you could say for dashboard cue",
    "start": "2803180",
    "end": "2809360"
  },
  {
    "text": "which is where all your users are using a dashboard with submit queries I want to give 50% of the memory that's where",
    "start": "2809360",
    "end": "2816050"
  },
  {
    "text": "my preferences give more memory they're right for me ingestion give 20% of",
    "start": "2816050",
    "end": "2822020"
  },
  {
    "text": "memory for my default which is where everything else goes give 25% of the memory right now this way you are able",
    "start": "2822020",
    "end": "2828650"
  },
  {
    "text": "to kind of give different resources to different cues and what you're also",
    "start": "2828650",
    "end": "2834680"
  },
  {
    "text": "doing is that you are defining how many slots you want to have in each cube the",
    "start": "2834680",
    "end": "2841820"
  },
  {
    "text": "total number of slots that you can have is is fifty five zero right that's for the system supports right now we don't",
    "start": "2841820",
    "end": "2848680"
  },
  {
    "text": "recommend you to go all the way up to 50 right we recommend you to keep it around",
    "start": "2848680",
    "end": "2854480"
  },
  {
    "text": "12 to 15 I'll tell you what these slots really mean these laughs are nothing but slots in which your queries count on if",
    "start": "2854480",
    "end": "2860870"
  },
  {
    "text": "I have 10 slots I can run 10 queries in parallel right so you never it kind of",
    "start": "2860870",
    "end": "2866320"
  },
  {
    "text": "defines what's the concurrency is right but what's also not understood is that",
    "start": "2866320",
    "end": "2871640"
  },
  {
    "text": "the slot divides the memory available in the queue equally so let's say you go",
    "start": "2871640",
    "end": "2879620"
  },
  {
    "text": "back to the dashboard queue it has 50 percent memory right let's say the cluster has 3 nodes and each node has 15",
    "start": "2879620",
    "end": "2889130"
  },
  {
    "text": "GB memory so you have 45 GB memory across three nodes so this particular Q",
    "start": "2889130",
    "end": "2894170"
  },
  {
    "text": "takes 50% which is about 2223 GB memory right I have 10 slots so 10 slots that",
    "start": "2894170",
    "end": "2902210"
  },
  {
    "text": "22 GB is divided across 10 slots each slot gets 2 GB of memory that's what",
    "start": "2902210",
    "end": "2908570"
  },
  {
    "text": "this really means right so when a query runs in the particular slot it can take up to 2 GB of memory now what if you go",
    "start": "2908570",
    "end": "2916220"
  },
  {
    "text": "overboard and you say no I want more concurrency I let me go and increase that slot from 10 to 25 right what it",
    "start": "2916220",
    "end": "2924170"
  },
  {
    "text": "really means is that that 22 GB instead of 10 to 2 GB by 10 it's not only 2 GB",
    "start": "2924170",
    "end": "2931370"
  },
  {
    "text": "by 25 so each slot is now getting less than 1 GB of memory what that means is that if your query",
    "start": "2931370",
    "end": "2938270"
  },
  {
    "text": "needs more memory it's now not going to have that much mama support of memory it's not starting to go into spill to",
    "start": "2938270",
    "end": "2945500"
  },
  {
    "text": "disk so clearly that glove completely run in the memory will start spilling to",
    "start": "2945500",
    "end": "2950510"
  },
  {
    "text": "disk by causing more damage if you have more number of slots right so our",
    "start": "2950510",
    "end": "2956119"
  },
  {
    "text": "recommendation that's why is to keep the total number of slots are on 15 but",
    "start": "2956119",
    "end": "2961250"
  },
  {
    "text": "effectively define multiple queues and start segregating workloads and send",
    "start": "2961250",
    "end": "2967310"
  },
  {
    "text": "them to different queues now okay this is this is okay you can have different",
    "start": "2967310",
    "end": "2973160"
  },
  {
    "text": "queues I can define I can send queries to different queues and so on so forth",
    "start": "2973160",
    "end": "2979130"
  },
  {
    "text": "but even within the default queue I have no control over what the user does right",
    "start": "2979130",
    "end": "2984230"
  },
  {
    "text": "I don't know what they will submit the sum will submit long-running queries some will submit short queries some will",
    "start": "2984230",
    "end": "2990950"
  },
  {
    "text": "write a query and they'll forget to put a limit clause this is a classic problem right how do I handle that right it's",
    "start": "2990950",
    "end": "2997849"
  },
  {
    "text": "where query monitoring rules come into picture where you can define rules for",
    "start": "2997849",
    "end": "3003280"
  },
  {
    "text": "your queues to automatically handle these kind of conflicts and runaway",
    "start": "3003280",
    "end": "3008890"
  },
  {
    "text": "queries in your cluster now what you could do is you can start having rules",
    "start": "3008890",
    "end": "3015520"
  },
  {
    "text": "defined which look at specific metrics and start taking action based on those",
    "start": "3015520",
    "end": "3021670"
  },
  {
    "text": "rules right so we expose a number of metrics for the query things like you",
    "start": "3021670",
    "end": "3027220"
  },
  {
    "text": "know query execution time how much CPU was used how many rows were scanned and",
    "start": "3027220",
    "end": "3032230"
  },
  {
    "text": "so on so very exhaustive right and you could say if query in this particular",
    "start": "3032230",
    "end": "3039310"
  },
  {
    "text": "queue scans more than 1 million records do something take some action that's",
    "start": "3039310",
    "end": "3045550"
  },
  {
    "text": "basically a rule right so you could simply say pick up the metric as number",
    "start": "3045550",
    "end": "3051160"
  },
  {
    "text": "of rows scan if it's more than 1 million take some action right what the action",
    "start": "3051160",
    "end": "3056170"
  },
  {
    "text": "can be or one among the three simply log it or abort the query or hop the query",
    "start": "3056170",
    "end": "3062859"
  },
  {
    "text": "so logging simply logs the query into a system table where you can come back next day morning and find out what",
    "start": "3062859",
    "end": "3068650"
  },
  {
    "text": "happened a bot is simply punishing the user you fired a bad query I'm gonna go and kill",
    "start": "3068650",
    "end": "3073900"
  },
  {
    "text": "you right hopping is okay so I have a default queue that's where everyone is",
    "start": "3073900",
    "end": "3079839"
  },
  {
    "text": "submitting somebody sent a bad query I want to send I want to knock you off from this queue and send you to let's",
    "start": "3079839",
    "end": "3086770"
  },
  {
    "text": "say different queue I have a lesser priority queue send that person over there right so that will help you again",
    "start": "3086770",
    "end": "3094000"
  },
  {
    "text": "manage the workloads in your cluster right so the recommendation from wlm and Kumar is limit the total number of slots",
    "start": "3094000",
    "end": "3103299"
  },
  {
    "start": "3096000",
    "end": "3096000"
  },
  {
    "text": "to 15 or less define multiple queues depending upon different workloads",
    "start": "3103299",
    "end": "3108970"
  },
  {
    "text": "different users and so on so for in your cluster leverage Kumar to start taking",
    "start": "3108970",
    "end": "3115270"
  },
  {
    "text": "actions the first step could be simply log it or if you want to you and go ahead and punish people you can start",
    "start": "3115270",
    "end": "3121450"
  },
  {
    "text": "aborting queries and you could also start setting time modes right a particular query can only run for this",
    "start": "3121450",
    "end": "3128829"
  },
  {
    "text": "much amount of seconds or this one's a mode of minutes right in addition to all of this there is also a default super",
    "start": "3128829",
    "end": "3135700"
  },
  {
    "text": "user queue that we create for every cluster retain that so that you can use",
    "start": "3135700",
    "end": "3141730"
  },
  {
    "text": "that for any admin activity so in case things are have us our and administer",
    "start": "3141730",
    "end": "3146859"
  },
  {
    "text": "take some action you do need that super user queue right so do use that for any of your administration activities okay",
    "start": "3146859",
    "end": "3155559"
  },
  {
    "text": "so that's around W element Kumar from for workload management what we also did",
    "start": "3155559",
    "end": "3160589"
  },
  {
    "start": "3160000",
    "end": "3160000"
  },
  {
    "text": "was came up with a feature recently called as result caching now what this",
    "start": "3160589",
    "end": "3166329"
  },
  {
    "text": "really helps is that lot of times we find this BA dashboards where there are",
    "start": "3166329",
    "end": "3172569"
  },
  {
    "text": "lot of typical repeated queries right people keep sending repeated queries to",
    "start": "3172569",
    "end": "3178029"
  },
  {
    "text": "your cluster it could be that they're just refreshing the screen or they went",
    "start": "3178029",
    "end": "3183549"
  },
  {
    "text": "out and came back and they want to see the data again and do something else so a lot of these repeated queries does get",
    "start": "3183549",
    "end": "3189579"
  },
  {
    "text": "submitted to the cluster right and again they kind of go and hog the resources right now with results at caching what",
    "start": "3189579",
    "end": "3196720"
  },
  {
    "text": "we really do is we start caching the result of your query so",
    "start": "3196720",
    "end": "3202220"
  },
  {
    "text": "the same query comes back again we skip everything we skip wlm we skip",
    "start": "3202220",
    "end": "3207980"
  },
  {
    "text": "processing nothing really happens we simply return the data from the cache",
    "start": "3207980",
    "end": "3214130"
  },
  {
    "text": "right so this is maintained in the leader nodes memory as a cache so that",
    "start": "3214130",
    "end": "3220790"
  },
  {
    "text": "when you're getting a let's say if the query rains for the first time it doesn't find anything in the cache it",
    "start": "3220790",
    "end": "3226070"
  },
  {
    "text": "actually goes and physically runs on the cluster the result set gets cached in the leader node and the result gets sent",
    "start": "3226070",
    "end": "3231470"
  },
  {
    "text": "back to the be a client next time the same query comes no processing no wlm nothing simply return",
    "start": "3231470",
    "end": "3238790"
  },
  {
    "text": "the result from leader node right so this helps in again optimizing the",
    "start": "3238790",
    "end": "3243980"
  },
  {
    "text": "workloads because there's lesser number of work being done for repeated queries right so resulted caching is is turned",
    "start": "3243980",
    "end": "3253130"
  },
  {
    "start": "3253000",
    "end": "3253000"
  },
  {
    "text": "on by default no so you should start seeing benefits in your cluster right the next thing that we did is something",
    "start": "3253130",
    "end": "3260119"
  },
  {
    "text": "called as short query acceleration this is again in the in the wlm space to help",
    "start": "3260119",
    "end": "3266390"
  },
  {
    "text": "you optimize throughput from your system right now the other common concern that",
    "start": "3266390",
    "end": "3271820"
  },
  {
    "text": "we heard from your customers is that when different types of workloads start",
    "start": "3271820",
    "end": "3277010"
  },
  {
    "text": "running on the cluster what we are not able to avoid is there could be a small simple query a query that runs for 3-4",
    "start": "3277010",
    "end": "3284750"
  },
  {
    "text": "seconds right but because of the wlm settings what's happening is my long",
    "start": "3284750",
    "end": "3291050"
  },
  {
    "text": "running queries are blocking my shot queries they are currently running I have no free slots from a row of form a",
    "start": "3291050",
    "end": "3296599"
  },
  {
    "text": "short query to run so it has to go and wait in the queue right so typically the long-running queries start kind of",
    "start": "3296599",
    "end": "3303200"
  },
  {
    "text": "blocking the shot queries and they don't find enough space to run right but these",
    "start": "3303200",
    "end": "3308630"
  },
  {
    "text": "short queries given a chance they can actually quickly run and get out of the way all right so what sqa really does",
    "start": "3308630",
    "end": "3314270"
  },
  {
    "text": "sq8 short for short query acceleration we deployed a machine learning model in",
    "start": "3314270",
    "end": "3320720"
  },
  {
    "text": "your cluster it's a simple classifier model very uses machine learning to",
    "start": "3320720",
    "end": "3326330"
  },
  {
    "text": "start classifying your queries into short and long write it basically",
    "start": "3326330",
    "end": "3332150"
  },
  {
    "text": "becomes better over a period of time it starts learning as you submit more ammo different types of queries into the",
    "start": "3332150",
    "end": "3338660"
  },
  {
    "text": "cluster and it starts classifying okay here's a new query I find it to be a short query now if it finds a short",
    "start": "3338660",
    "end": "3345349"
  },
  {
    "text": "query it momentarily pauses one of the existing long-running queries gives the",
    "start": "3345349",
    "end": "3352489"
  },
  {
    "text": "short query to to run gives a chance for a shot query to run and then the short",
    "start": "3352489",
    "end": "3358759"
  },
  {
    "text": "query finishes and then the long query starts running again right so this way you're short query kind of gets an",
    "start": "3358759",
    "end": "3365239"
  },
  {
    "text": "express lane where they're kind of by pausing all the the queues and the W Elam's and quickly run and get out of",
    "start": "3365239",
    "end": "3372140"
  },
  {
    "text": "the way right now the impact of this is that the long running queries could take",
    "start": "3372140",
    "end": "3377630"
  },
  {
    "text": "slightly longer to run right so let's say the query was running for 28 minutes it could run for 28 minutes and 30",
    "start": "3377630",
    "end": "3385160"
  },
  {
    "text": "seconds because it got passed in between and so on and so forth right because a short query has to run which is probably okay because it's a",
    "start": "3385160",
    "end": "3392089"
  },
  {
    "text": "longer inning query anyway somebody fired it went for a coffee they're gonna probably get the result in another 30",
    "start": "3392089",
    "end": "3397640"
  },
  {
    "text": "seconds or so it's probably okay but if you see from the overall system perspective the total number of queries",
    "start": "3397640",
    "end": "3403640"
  },
  {
    "text": "that are able to run in an hour increases significantly because these short queries are now running much",
    "start": "3403640",
    "end": "3409279"
  },
  {
    "text": "quickly from an end user perspective so you get more throughput from the system right now this is also now enabled by",
    "start": "3409279",
    "end": "3416299"
  },
  {
    "text": "default for new clusters are you can run a command is then in documentation to find out whether asqa is enabled in your",
    "start": "3416299",
    "end": "3422539"
  },
  {
    "text": "cluster or not right if it's not enabled we would highly recommend you to go and enable it for your cluster okay so we I",
    "start": "3422539",
    "end": "3431119"
  },
  {
    "text": "think we spoke quite a bit about WLAN kurma Charcot reacceleration resulted and so on and so forth all of them kind",
    "start": "3431119",
    "end": "3437390"
  },
  {
    "text": "of help you manage the workloads in a cluster so that we are able to give optimal resources for the right kind of",
    "start": "3437390",
    "end": "3443449"
  },
  {
    "text": "workloads within your cluster so let's talk a little bit about the node type from the cluster sizing so as I said",
    "start": "3443449",
    "end": "3450859"
  },
  {
    "text": "earlier we offer different types of nodes primarily classified as dense",
    "start": "3450859",
    "end": "3456739"
  },
  {
    "start": "3454000",
    "end": "3454000"
  },
  {
    "text": "compute and dense storage dense computes or nothing but nodes with solid state disk so they have SSDs on them they come",
    "start": "3456739",
    "end": "3464829"
  },
  {
    "text": "with better performance than the den storage dense storage are magnetic discs so they are built they",
    "start": "3464829",
    "end": "3473000"
  },
  {
    "text": "are really meant for very very large deployments where if you have let's say",
    "start": "3473000",
    "end": "3478100"
  },
  {
    "text": "hundreds of terabytes or petabytes of deployments you need like much more largest storage footprint that's where",
    "start": "3478100",
    "end": "3484130"
  },
  {
    "text": "dense storage comes into picture you do get two different types of instance types within them dc2 has large in a",
    "start": "3484130",
    "end": "3491120"
  },
  {
    "text": "textual ds2 has X large latex large write each with varying configurations",
    "start": "3491120",
    "end": "3496940"
  },
  {
    "text": "on CPU memory and storage from a cluster sizing perspective our recommendation",
    "start": "3496940",
    "end": "3504230"
  },
  {
    "text": "there is if you are running it in production we recommend minimum to",
    "start": "3504230",
    "end": "3509450"
  },
  {
    "text": "compute nodes so that you get high availability and redundancy for your data the leader node is automatically",
    "start": "3509450",
    "end": "3516620"
  },
  {
    "text": "given to you at no additional cost we will also recommend you to maintain 20 percent free space on your cluster so",
    "start": "3516620",
    "end": "3526490"
  },
  {
    "text": "why do we recommend this is primarily because we need free space in the system for things like scratchpad",
    "start": "3526490",
    "end": "3532130"
  },
  {
    "text": "logs if you're running vacuum we need storage space to basically reset the",
    "start": "3532130",
    "end": "3537950"
  },
  {
    "text": "table temporary tables will be used we will use space for intermediate results",
    "start": "3537950",
    "end": "3544430"
  },
  {
    "text": "and so on and so forth right so do leave out 20 percent free space in your cluster so which means that 60 to 70",
    "start": "3544430",
    "end": "3552860"
  },
  {
    "text": "percent is what you should look at in terms of overall cluster space utilization the moment you are hitting",
    "start": "3552860",
    "end": "3558590"
  },
  {
    "text": "at 50 60 you should start thinking about adding more storage to your cluster right if you are currently on bc one",
    "start": "3558590",
    "end": "3566720"
  },
  {
    "text": "which was a previous generation dense compute we highly recommend you to move to DC to DC to is significantly faster",
    "start": "3566720",
    "end": "3574640"
  },
  {
    "text": "than bc one and it came it comes as the same price as DC one so there is no",
    "start": "3574640",
    "end": "3580430"
  },
  {
    "text": "additional cost for you and you get better performance from DC - right so",
    "start": "3580430",
    "end": "3588200"
  },
  {
    "text": "that's about redshift and I also mentioned the redshift can help you extend your derelict dead a velocity at",
    "start": "3588200",
    "end": "3596000"
  },
  {
    "text": "a lake right you can use spectrum to start querying data available in s3 right so what redshift spectrum",
    "start": "3596000",
    "end": "3602800"
  },
  {
    "text": "you really do is from your redshift cluster itself I can now start firing",
    "start": "3602800",
    "end": "3608500"
  },
  {
    "text": "queries which can directly run against s3's data I can store some data on my s3",
    "start": "3608500",
    "end": "3614530"
  },
  {
    "text": "bucket and the query can directly run against the data you could also go one step ahead and say I want to join data",
    "start": "3614530",
    "end": "3621280"
  },
  {
    "text": "between what is available for in redshift and what's available on s3 right well best part of a spectrum is",
    "start": "3621280",
    "end": "3628360"
  },
  {
    "text": "that it's an infrastructure that you do not manage so we basically have one pool",
    "start": "3628360",
    "end": "3633400"
  },
  {
    "text": "of infrastructure that we are managing on your behalf and you pay only when you",
    "start": "3633400",
    "end": "3639940"
  },
  {
    "text": "utilize the infrastructure it's a paper query kind of model unlike in redshift where you are having a clustered you're",
    "start": "3639940",
    "end": "3646990"
  },
  {
    "text": "going to pay by the AR for your notes in spectrum it's a paper query model if you fire a query if the query scans fire a",
    "start": "3646990",
    "end": "3653920"
  },
  {
    "text": "terabyte of data you pay five dollars right and we support open file formats like parquet or C Avro CSV and so on so",
    "start": "3653920",
    "end": "3663850"
  },
  {
    "text": "forth right now here's how it works right so you have your sequel client you have your redshift cluster which is a",
    "start": "3663850",
    "end": "3670090"
  },
  {
    "text": "blue portion you also have the spectrum layer the green portion which is really not visible to you but what you could do",
    "start": "3670090",
    "end": "3676690"
  },
  {
    "text": "is you could create an external table that points to the s3 data set and when you fire a query against that data set",
    "start": "3676690",
    "end": "3683740"
  },
  {
    "text": "it actually leverages the spectrum layer and runs that query against your s3 data",
    "start": "3683740",
    "end": "3689230"
  },
  {
    "text": "set right now what this model allows you to do is even have multiple redshift",
    "start": "3689230",
    "end": "3695140"
  },
  {
    "text": "clusters leveraging the spectrum layer and accessing the same data set that is",
    "start": "3695140",
    "end": "3701410"
  },
  {
    "text": "available on s3 so this allows you to scale much more than what you could",
    "start": "3701410",
    "end": "3707020"
  },
  {
    "text": "achieve through just one single cluster right so all those different clusters can go to a common spectrum layer and go",
    "start": "3707020",
    "end": "3713890"
  },
  {
    "text": "to the same s3 dataset without you ingesting data into multiple clusters and the best part is a lab Louis glue",
    "start": "3713890",
    "end": "3721390"
  },
  {
    "text": "gives you a centralized catalog which can be leveraged across all those different clusters right so few things",
    "start": "3721390",
    "end": "3728770"
  },
  {
    "text": "from spectrum perspective right so spectrum like I said is a pair per query model so you pay based on how much data",
    "start": "3728770",
    "end": "3736450"
  },
  {
    "text": "your query scans if I scan a terabyte I pay $5 if I scan hundred GB I pay 1/10",
    "start": "3736450",
    "end": "3744609"
  },
  {
    "text": "of that which is 50 cents right so the more you scan the more you pay the less you scandalous or you pay no",
    "start": "3744609",
    "end": "3751060"
  },
  {
    "text": "brainer right now again if you are leveraging columnar formats it will help",
    "start": "3751060",
    "end": "3757359"
  },
  {
    "text": "you to scan lesser right because again you are going to run analytical queries against this data set right and columnar",
    "start": "3757359",
    "end": "3764260"
  },
  {
    "text": "format unlike text formats allow you to selectively read data if I am able to",
    "start": "3764260",
    "end": "3770829"
  },
  {
    "text": "let's say store data in Park where were see on those corner formats when I fire a columnar query it reads only the",
    "start": "3770829",
    "end": "3778000"
  },
  {
    "text": "particular data from s3 which means that we are able to scan lesser and we will",
    "start": "3778000",
    "end": "3783070"
  },
  {
    "text": "charge you much lesser right so here's an example where I had a 1.15 tera by data set an s3 a text data set a query",
    "start": "3783070",
    "end": "3791320"
  },
  {
    "text": "was run against this it took 200 seconds to run it scan the entire data set it cost you $5 right the same data set if",
    "start": "3791320",
    "end": "3799720"
  },
  {
    "text": "you're able to convert into park' the data set becomes much smaller because Parker has a form that comes with",
    "start": "3799720",
    "end": "3805810"
  },
  {
    "text": "compression right it becomes much lesser 1 30 GB the same query first thing is it",
    "start": "3805810",
    "end": "3812349"
  },
  {
    "text": "runs much faster because it's able to scan and read only the specific columns and more importantly the data scanned is",
    "start": "3812349",
    "end": "3820660"
  },
  {
    "text": "much lesser it's only 2.5 GB which means that you end up paying much lesser than what you paid for the tech space format",
    "start": "3820660",
    "end": "3827589"
  },
  {
    "text": "right so when you are leveraging spectrum always think about keeping your data in columnar formats in s3 the other",
    "start": "3827589",
    "end": "3837490"
  },
  {
    "text": "thing from spectrum perspective is data partitioning so for those of you who are coming from Hadoop kind of a background",
    "start": "3837490",
    "end": "3843760"
  },
  {
    "text": "you know what partitioning means right so you are able to now separate the data",
    "start": "3843760",
    "end": "3849310"
  },
  {
    "text": "files based on columns and if you are having queries which only look at",
    "start": "3849310",
    "end": "3855280"
  },
  {
    "text": "certain portion of the data you have a filter class through partitioning we can",
    "start": "3855280",
    "end": "3861250"
  },
  {
    "text": "only read a subset of the files that again reducing the amount of data that",
    "start": "3861250",
    "end": "3866859"
  },
  {
    "text": "we scan right so here is an example I go and create an external table called as sales event I have few columns on the",
    "start": "3866859",
    "end": "3875440"
  },
  {
    "text": "table what I can say is partitioned by a particular column in the table call us",
    "start": "3875440",
    "end": "3881080"
  },
  {
    "text": "that same month and event right and I say location is the patellar s3 bucket",
    "start": "3881080",
    "end": "3886660"
  },
  {
    "text": "now in that s3 bucket what I can do is I can create subfolders like sales month",
    "start": "3886660",
    "end": "3892420"
  },
  {
    "text": "is equal to 18 - 1 / event is equal to 1 0 1 I can keep creating these kind of",
    "start": "3892420",
    "end": "3900190"
  },
  {
    "text": "folders and putting data within the folders and when I have a partitioned table that particular sales month points",
    "start": "3900190",
    "end": "3908230"
  },
  {
    "text": "the sales month folder and even partition for points to the events folder now what is the effect of this so",
    "start": "3908230",
    "end": "3916060"
  },
  {
    "text": "when I fire a query on an on partition table again it's going to scan the",
    "start": "3916060",
    "end": "3921850"
  },
  {
    "text": "entire data set by the first example there right where it scans soundy 4 GB",
    "start": "3921850",
    "end": "3929770"
  },
  {
    "text": "of data and it cost you 36 cents they're the same query on the right-hand side on",
    "start": "3929770",
    "end": "3937090"
  },
  {
    "text": "a partition table because there is a ver class where ship date is equal to 96 right so on a partition table because of",
    "start": "3937090",
    "end": "3944530"
  },
  {
    "text": "the where clause I can go to the specific s3 folder with the folders name could be ship date is equal to 1996 - so",
    "start": "3944530",
    "end": "3952000"
  },
  {
    "text": "on so forth and read-only that that will reduce the amount or data being scanned",
    "start": "3952000",
    "end": "3957390"
  },
  {
    "text": "significant performance difference significant cost savings right so think",
    "start": "3957390",
    "end": "3963190"
  },
  {
    "text": "about that so when you are leveraging spectrum think about columnar formats compression think about having partition",
    "start": "3963190",
    "end": "3970360"
  },
  {
    "text": "data set on s3 all of them will help you reduce the amount of data that you scan and also improve the performance right",
    "start": "3970360",
    "end": "3976930"
  },
  {
    "text": "now beyond all of that these are the recommendation that you could think about within redshift on leveraging",
    "start": "3976930",
    "end": "3984100"
  },
  {
    "text": "spectrum right now what we also recommend customers is to start thinking about various analytic services that can",
    "start": "3984100",
    "end": "3992890"
  },
  {
    "text": "be used to run different types of workloads so while you might have started by the redshift and you are",
    "start": "3992890",
    "end": "3998680"
  },
  {
    "text": "trying to optimize what's possible within redshift do not forget that there are other options available with",
    "start": "3998680",
    "end": "4003990"
  },
  {
    "text": "newest things like EMR for Hadoop based workloads you could start for example",
    "start": "4003990",
    "end": "4009119"
  },
  {
    "text": "running presto based queries on EMR or athina which is again a paper query",
    "start": "4009119",
    "end": "4014190"
  },
  {
    "text": "model completely serverless right and the beauty of all these services is that all of them can directly interact with",
    "start": "4014190",
    "end": "4021660"
  },
  {
    "text": "the s3 as the underlying storage spectrum EMR and athena all of them have direct capability of reading data from",
    "start": "4021660",
    "end": "4029250"
  },
  {
    "text": "history and all of them can leverage a centralized data catalog so your glue",
    "start": "4029250",
    "end": "4036540"
  },
  {
    "text": "data catalog can maintain the table information table definitions column",
    "start": "4036540",
    "end": "4041550"
  },
  {
    "text": "information partition information so on so forth in one place and you could even use multiple query engines to start",
    "start": "4041550",
    "end": "4048780"
  },
  {
    "text": "processing the same data set right so this is taking one step further so what",
    "start": "4048780",
    "end": "4053850"
  },
  {
    "text": "we discuss so far is one of the best practices what's possible to optimize your current redshift workload using",
    "start": "4053850",
    "end": "4060510"
  },
  {
    "text": "redshift and spectrum but beyond that all this there are other options available within the area bicycle system",
    "start": "4060510",
    "end": "4065970"
  },
  {
    "text": "and you could use a combination of these different services to do different types of use cases right if you have a",
    "start": "4065970",
    "end": "4073170"
  },
  {
    "text": "developer who would like to leverage presto a spark to process the data you",
    "start": "4073170",
    "end": "4080010"
  },
  {
    "text": "could send them to EMR right if you have lot of work current RDoc workloads being",
    "start": "4080010",
    "end": "4086040"
  },
  {
    "text": "run on redshift maybe you can start thinking about moving those workloads to Athena right so that helps you again",
    "start": "4086040",
    "end": "4092100"
  },
  {
    "text": "distribute the workload to different services and make sure everyone gets better experience from different",
    "start": "4092100",
    "end": "4098220"
  },
  {
    "text": "services right so that's all I want to",
    "start": "4098220",
    "end": "4103740"
  },
  {
    "text": "cover but I would leave with few more resources that you can probably read later they're a bunch of utilities",
    "start": "4103740",
    "end": "4111210"
  },
  {
    "text": "available in our github repository so our AWS labs get up repository has utilities for redshift there are better",
    "start": "4111210",
    "end": "4119850"
  },
  {
    "text": "monitoring utilities and UDF functions and so on so forth so these are things like admin scripts that you can run for",
    "start": "4119850",
    "end": "4126508"
  },
  {
    "text": "getting more diagnostic information from your cluster things like you analyze vacuum utility",
    "start": "4126509",
    "end": "4131940"
  },
  {
    "text": "that I spoke about all of them are available in that github repository do take a look at the gate",
    "start": "4131940",
    "end": "4137940"
  },
  {
    "text": "positi it's very very useful if you're not following the AWS between a blog I",
    "start": "4137940",
    "end": "4144028"
  },
  {
    "text": "highly recommend that you follow the big data blog we specifically write about",
    "start": "4144029",
    "end": "4150060"
  },
  {
    "text": "lot of architecture patterns and also some of these best practices there right",
    "start": "4150060",
    "end": "4156568"
  },
  {
    "text": "so from a redshift perspective you could look up the redshift Engineering's advance table design playbook it was",
    "start": "4156569",
    "end": "4164278"
  },
  {
    "text": "written by one of the redshift engineers he talks about lot of these best practice recommendations and also a",
    "start": "4164279",
    "end": "4171508"
  },
  {
    "text": "couple of blogs around redshift and spectrum for top ten best practices and",
    "start": "4171509",
    "end": "4177390"
  },
  {
    "text": "performance tuning right so do look at those blogs will give you some more insights into what you could do in your",
    "start": "4177390",
    "end": "4182699"
  },
  {
    "text": "redshift cluster that's all I had hopefully this was useful if you do have",
    "start": "4182699",
    "end": "4190350"
  },
  {
    "text": "any further questions there will be a survey right after this webinar I do you",
    "start": "4190350",
    "end": "4197340"
  },
  {
    "text": "can use a survey to send in your questions we will get back to you with our answers if you do need let's say",
    "start": "4197340",
    "end": "4205110"
  },
  {
    "text": "help from us where you need to speak to someone in some solution architect for your queries do mention that in the",
    "start": "4205110",
    "end": "4211080"
  },
  {
    "text": "survey we will be happy to have a conversation with you so that's all I had hopefully it is full and you guys",
    "start": "4211080",
    "end": "4217770"
  },
  {
    "text": "have a great evening thank you",
    "start": "4217770",
    "end": "4221659"
  }
]