[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "In this video, you'll see how you can accelerate machine learning (ML)\npipelines using a modern data architecture.",
    "start": "80",
    "end": "5880"
  },
  {
    "text": "With this approach, you can easily integrate\nML services into data analytics pipelines,",
    "start": "6325",
    "end": "10976"
  },
  {
    "text": "leverage visual tooling to lower the entry barrier\nfor creating and orchestrating pipelines,",
    "start": "10976",
    "end": "15559"
  },
  {
    "text": "and quickly generate and visualize forecasts\nagainst custom datasets.",
    "start": "15559",
    "end": "19492"
  },
  {
    "start": "20000",
    "end": "95000"
  },
  {
    "text": "Let's start at the AWS Lake Formation console, \nwhere we have data lake registered in Amazon S3.",
    "start": "20725",
    "end": "26005"
  },
  {
    "text": "The data lake contains information \nabout crude oil production in the US.",
    "start": "26788",
    "end": "30126"
  },
  {
    "text": "The S3 bucket contains three objects.",
    "start": "32479",
    "end": "34579"
  },
  {
    "text": "We'll fill the processed and published folders\nwith data as we move through the workflow.",
    "start": "34865",
    "end": "38559"
  },
  {
    "text": "The raw folder contains a public dataset file \nfrom the U.S. Energy Information Administration",
    "start": "40874",
    "end": "45354"
  },
  {
    "text": "representing 10 years' worth of monthly crude \noil and natural gas production variables.",
    "start": "45354",
    "end": "49374"
  },
  {
    "text": "We also have two databases that serve \nas repositories for our metadata.",
    "start": "53727",
    "end": "57193"
  },
  {
    "text": "As you can see, we've granted permissions to the various\nAWS services and Identity and Access Management (AWS IAM) roles",
    "start": "59765",
    "end": "66546"
  },
  {
    "text": "that will access the data pipeline.",
    "start": "66546",
    "end": "68147"
  },
  {
    "text": "Next, let's review a project we \nhave set up in AWS Glue DataBrew.",
    "start": "69280",
    "end": "73045"
  },
  {
    "text": "This project transforms the connected dataset to coordinate\nwith the datatypes used in Amazon Forecast.",
    "start": "75674",
    "end": "80830"
  },
  {
    "text": "We have already defined and run the recipe.",
    "start": "81370",
    "end": "83268"
  },
  {
    "text": "Let’s view the job output.",
    "start": "83828",
    "end": "85183"
  },
  {
    "text": "The transformed dataset is saved as a CSV \nfile in the S3 data lake’s processed folder.",
    "start": "88346",
    "end": "93226"
  },
  {
    "start": "95000",
    "end": "217000"
  },
  {
    "text": "Next, let's go to the AWS Step Functions\nconsole to view our workflow.",
    "start": "95920",
    "end": "99606"
  },
  {
    "text": "We can use Workflow Studio to see \nour data pipeline in a visual format.",
    "start": "102228",
    "end": "105908"
  },
  {
    "text": "The first step runs an AWS Glue \nDataBrew job to clean the raw data.",
    "start": "108527",
    "end": "112447"
  },
  {
    "text": "Next, the workflow splits \ninto two parallel streams.",
    "start": "113680",
    "end": "116500"
  },
  {
    "text": "The stream we see on the right invokes a data crawler\nin AWS Glue to run against the processed dataset.",
    "start": "119599",
    "end": "124798"
  },
  {
    "text": "A feedback loop responds according \nto the crawler's performance.",
    "start": "126000",
    "end": "129000"
  },
  {
    "text": "If the crawler fails, the workflow ends.",
    "start": "130868",
    "end": "132963"
  },
  {
    "text": "If the crawler runs, other steps monitor \nits progress in 30-second intervals.",
    "start": "134554",
    "end": "138862"
  },
  {
    "text": "The parallel stream imports \ndata into Amazon Forecast.",
    "start": "143167",
    "end": "146320"
  },
  {
    "text": "A feedback loop monitors the status of the import.",
    "start": "146490",
    "end": "148922"
  },
  {
    "text": "With a successful import, the workflow assigns steps\nto train the Forecast predictor and monitor its status.",
    "start": "150058",
    "end": "155310"
  },
  {
    "text": "Once the predictor is fully trained, we have \na step to create a new forecast dataset.",
    "start": "159818",
    "end": "163978"
  },
  {
    "text": "Again, we’ll check the status of \nthat new Forecast job continually.",
    "start": "164560",
    "end": "167765"
  },
  {
    "text": "After the forecasts have \nbeen generated successfully,",
    "start": "169557",
    "end": "172117"
  },
  {
    "text": "they are exported to the data lake on S3 \nand stored in the published data folder.",
    "start": "172117",
    "end": "176159"
  },
  {
    "text": "Finally, another Glue crawler is assigned \nto collect the published metadata,",
    "start": "179146",
    "end": "182986"
  },
  {
    "text": "populating our data catalog \nwith the forecasted material.",
    "start": "182986",
    "end": "185786"
  },
  {
    "text": "Let's save the workflow and start execution.",
    "start": "187135",
    "end": "189215"
  },
  {
    "text": "Let's name the run and remove this sample input.",
    "start": "193397",
    "end": "195806"
  },
  {
    "text": "While the workflow is running, \nwe can watch its progress.",
    "start": "203797",
    "end": "206244"
  },
  {
    "text": "As you can see, the first step is underway.",
    "start": "208788",
    "end": "210972"
  },
  {
    "text": "Let's check the AWS Glue console to see \nwhether the DataBrew job is running there.",
    "start": "212095",
    "end": "216121"
  },
  {
    "text": "The job is running.",
    "start": "217348",
    "end": "218320"
  },
  {
    "text": "Let's continue monitoring \nthe workflow's execution.",
    "start": "218500",
    "end": "220820"
  },
  {
    "text": "The Glue DataBrew job succeeded,",
    "start": "222485",
    "end": "224327"
  },
  {
    "text": "and the workflow has invoked the parallel functions to crawl\nthe processed data and train the Forecast predictor.",
    "start": "224327",
    "end": "229444"
  },
  {
    "text": "In the AWS Glue console, we can see that the \ncrawler for the processed data has started.",
    "start": "232026",
    "end": "236586"
  },
  {
    "text": "And from our Amazon Forecast dashboard,",
    "start": "238698",
    "end": "241097"
  },
  {
    "text": "we see that the dataset is currently \nimporting target time series data.",
    "start": "241098",
    "end": "244644"
  },
  {
    "text": "Let's continue monitoring our visual workflow.",
    "start": "246058",
    "end": "248218"
  },
  {
    "text": "Now the Lambda function is \ntraining the Forecast predictor.",
    "start": "249578",
    "end": "252242"
  },
  {
    "text": "Let's return to the Forecast\ndashboard to take a look.",
    "start": "252719",
    "end": "255118"
  },
  {
    "text": "Here, you can see our new predictor\nbeing created and trained.",
    "start": "257578",
    "end": "260298"
  },
  {
    "text": "This can take several minutes, so for the purposes \nof this demonstration we've sped up the process.",
    "start": "261071",
    "end": "265403"
  },
  {
    "text": "Let's refresh the page to see if \nany more steps have been completed.",
    "start": "268031",
    "end": "270868"
  },
  {
    "text": "The right side of our visual \nworkflow has successfully completed.",
    "start": "273428",
    "end": "276388"
  },
  {
    "text": "In the Glue console, we can see that \nthe crawler's status is now \"Ready.\"",
    "start": "278938",
    "end": "282457"
  },
  {
    "text": "The workflow has updated again.",
    "start": "285087",
    "end": "286607"
  },
  {
    "text": "The Forecast predictor has completed its training.",
    "start": "287919",
    "end": "290239"
  },
  {
    "text": "Let's make sure that this progress \nis reflected in the Forecast console.",
    "start": "291000",
    "end": "294080"
  },
  {
    "text": "The new predictor is now active.",
    "start": "296831",
    "end": "298350"
  },
  {
    "text": "This predictor was configured to use an Amazon Forecast machine learning algorithm\ncalled CNN-QR (Convolutional Neural Network - Quantile Regression).",
    "start": "299588",
    "end": "307668"
  },
  {
    "text": "CNN-QR works best with large datasets \ncontaining hundreds of time series.",
    "start": "308234",
    "end": "312511"
  },
  {
    "text": "To produce accuracy metrics for predictors, \nForecast uses backtesting,",
    "start": "313690",
    "end": "317645"
  },
  {
    "text": "which is the process of testing a predictor against historical data\nand comparing forecasted values to actual values.",
    "start": "317645",
    "end": "323436"
  },
  {
    "text": "Let’s review the forecasts currently \nbeing created against the predictor.",
    "start": "324677",
    "end": "327957"
  },
  {
    "text": "Another forecast is in the \nprocess of being created.",
    "start": "330645",
    "end": "333151"
  },
  {
    "text": "Our workflow is continuing its progress.",
    "start": "335717",
    "end": "337834"
  },
  {
    "text": "When the forecasts complete,\nthey’ll be exported to our S3 data lake.",
    "start": "338004",
    "end": "341722"
  },
  {
    "text": "According to our workflow, the \ndata is now exporting to S3.",
    "start": "343013",
    "end": "346471"
  },
  {
    "text": "Let’s quickly confirm that the \nforecasts finished generating.",
    "start": "346980",
    "end": "349750"
  },
  {
    "text": "The forecasts have successfully \ngenerated and are active.",
    "start": "352378",
    "end": "355098"
  },
  {
    "text": "Let's refresh the workflow.",
    "start": "357780",
    "end": "358980"
  },
  {
    "text": "The final step, now in progress,  ",
    "start": "359839",
    "end": "361759"
  },
  {
    "text": "runs a Glue crawler against the \npublished metadata in the data lake.",
    "start": "361759",
    "end": "364993"
  },
  {
    "text": "Let's check in S3 to confirm the \nforecasts were successfully exported.",
    "start": "366159",
    "end": "369679"
  },
  {
    "text": "We'll select our bucket and \nlook in the published folder.",
    "start": "373167",
    "end": "375567"
  },
  {
    "text": "The folder has been populated.",
    "start": "377578",
    "end": "379014"
  },
  {
    "text": "We can also review the crawler \nthat coordinated the export.",
    "start": "381508",
    "end": "384148"
  },
  {
    "text": "This page indicates that the crawler exported\nthe forecasted data to the data lake's published data folder.",
    "start": "385946",
    "end": "390906"
  },
  {
    "text": "Back in Workflow Studio, we see that \nthe final steps are being processed.",
    "start": "392320",
    "end": "396372"
  },
  {
    "text": "Let's refresh the workflow.",
    "start": "397610",
    "end": "398889"
  },
  {
    "text": "The entire data pipeline \nhas successfully completed.",
    "start": "399908",
    "end": "402468"
  },
  {
    "text": "Now that our data lake is \npopulated with the forecasted data,",
    "start": "403946",
    "end": "406826"
  },
  {
    "text": "we can produce visualizations \nand perform data analytics.",
    "start": "406826",
    "end": "409854"
  },
  {
    "start": "412000",
    "end": "514000"
  },
  {
    "text": "First, we'll prepare the datasets for \nvisualization using Amazon Athena.",
    "start": "412485",
    "end": "416085"
  },
  {
    "text": "In the Athena query editor, we can run SQL queries against the tables\nin our AWS Glue Data Catalog as if it was a traditional database.",
    "start": "416868",
    "end": "424206"
  },
  {
    "text": "Let's preview the data in our processed \ntable, which is a historical aggregate.",
    "start": "426740",
    "end": "430340"
  },
  {
    "text": "We can see the timestamp, state ID, and demand variables\nthat were collected and formatted from the raw data.",
    "start": "431519",
    "end": "436799"
  },
  {
    "text": "No let’s preview our published table.",
    "start": "439418",
    "end": "441098"
  },
  {
    "text": "Here, we see the results\nof the forecast predictor.",
    "start": "444239",
    "end": "446438"
  },
  {
    "text": "Forecasts were generated at the P10, P50, and P90 intervals.",
    "start": "446957",
    "end": "451044"
  },
  {
    "text": "Next, we'll go to Amazon QuickSight\nto visualize this data.",
    "start": "452255",
    "end": "455310"
  },
  {
    "text": "We've already produced two visualizations,\nshowing the historical data and the forecasted data, side by side.",
    "start": "457881",
    "end": "463426"
  },
  {
    "text": "We can filter this data.",
    "start": "465045",
    "end": "466364"
  },
  {
    "text": "For instance, let's view data for only the state of Texas.",
    "start": "466650",
    "end": "469544"
  },
  {
    "text": "Finally, let’s review the \ncomposition of our dataset.",
    "start": "477514",
    "end": "480000"
  },
  {
    "text": "As you can see, our visualizations dataset joins \nthe two datasets we previewed in Amazon Athena.",
    "start": "483647",
    "end": "488767"
  },
  {
    "text": "When viewing the joined dataset, we can see that the historical\nand forecasted data sets are still distinct, as they are in our visualizations.",
    "start": "491721",
    "end": "498521"
  },
  {
    "text": "You've just seen how you can accelerate ML \npipelines using a modern data architecture.",
    "start": "502127",
    "end": "506179"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "507188",
    "end": "510307"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "510815",
    "end": "512875"
  }
]