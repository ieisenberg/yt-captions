[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "hey uh just an introduction to myself Andrew Spiker uh pretty easy to find online if you're looking to followup",
    "start": "359",
    "end": "5920"
  },
  {
    "text": "questions that you don't get answered afterwards um I have been an individual contributor engineer on this project for",
    "start": "5920",
    "end": "12719"
  },
  {
    "text": "uh the last year and uh became a product manager for it internally inside of Netflix and more recently became the",
    "start": "12719",
    "end": "19279"
  },
  {
    "text": "manager for the team so deep in the project from the beginning um what are what are we going",
    "start": "19279",
    "end": "25279"
  },
  {
    "start": "22000",
    "end": "22000"
  },
  {
    "text": "to learn from this session first we're going to start about why Netflix is using containers um some of the use",
    "start": "25279",
    "end": "30960"
  },
  {
    "text": "cases in scale um how did we get there what technologies did we end up building to use containers and then finally talk",
    "start": "30960",
    "end": "37600"
  },
  {
    "text": "a little bit about uh the work we've been doing with ECS so uh everybody knows a little",
    "start": "37600",
    "end": "43360"
  },
  {
    "text": "company called Netflix okay um some interesting numbers uh we're 86 million",
    "start": "43360",
    "end": "49640"
  },
  {
    "text": "members strong uh we went worldwide um except for a few countries uh we are",
    "start": "49640",
    "end": "56399"
  },
  {
    "text": "running out of three different regions of the world um in terms of I'm Amazon um regions we run on our containers are",
    "start": "56399",
    "end": "63920"
  },
  {
    "text": "smaller but in our VM World we're in 100,000 VM category at this point um",
    "start": "63920",
    "end": "69640"
  },
  {
    "text": "lots and lots of microservices probably the most interesting number up here is in North America we do a third of the",
    "start": "69640",
    "end": "75040"
  },
  {
    "text": "internet download traffic every night that's kind of fun um containers so why",
    "start": "75040",
    "end": "80840"
  },
  {
    "start": "77000",
    "end": "77000"
  },
  {
    "text": "containers uh hopefully you all know that we're quite big in Virtual machines",
    "start": "80840",
    "end": "86200"
  },
  {
    "text": "we built uh which this is showing up here is our uh actual live screenshot of the uh microservices network with all",
    "start": "86200",
    "end": "92880"
  },
  {
    "text": "the traffic coming in on the left and going all the way down to the data tiers on the right um we very Cloud native",
    "start": "92880",
    "end": "98520"
  },
  {
    "text": "we've been on the cloud for years uh we have no data centers uh we're everything",
    "start": "98520",
    "end": "104320"
  },
  {
    "text": "I'm going to show you here um I'm still on call for our team owns and operates we're completely devops um and it's",
    "start": "104320",
    "end": "110640"
  },
  {
    "text": "totally elastically scalable um and very highly resilient you know for folks that don't know we do chaos testing against",
    "start": "110640",
    "end": "116159"
  },
  {
    "text": "our own system so everything you show up here I show up here we've got a random process running around shooting",
    "start": "116159",
    "end": "121399"
  },
  {
    "text": "instances uh you know uh daily so we did this all with VMS you know why do we need",
    "start": "121399",
    "end": "127360"
  },
  {
    "text": "containers um really where it came from was developer velocity so being able to",
    "start": "127360",
    "end": "132520"
  },
  {
    "text": "innovate if you look at Netflix and some of our key values for what we do in infrastructure at the very top is being",
    "start": "132520",
    "end": "139360"
  },
  {
    "text": "able to get features that make our experience as pleasing to you as possible as fast as possible then comes",
    "start": "139360",
    "end": "145360"
  },
  {
    "text": "resiliency it should always be on it should be no different than electricity you should always be able to watch a movie it finally comes down to",
    "start": "145360",
    "end": "151959"
  },
  {
    "text": "efficiency a lot of people come at containers from efficiency play that's not why we're coming about it um we had",
    "start": "151959",
    "end": "158480"
  },
  {
    "text": "Amis we do baking we do immutable infrastructure but we didn't have the to",
    "start": "158480",
    "end": "164080"
  },
  {
    "text": "I think there we go we didn't have the ability to uh do local iterative development in the same deployment",
    "start": "164080",
    "end": "170360"
  },
  {
    "text": "artifact that we were doing in production and this was important both for being able to roundtrip up but also round trip down so if something happens",
    "start": "170360",
    "end": "176959"
  },
  {
    "text": "in production being able to pull down the exact image running it locally and debug it um we also had this great",
    "start": "176959",
    "end": "183799"
  },
  {
    "text": "system around baking of U Amis that was built around operating system concept uh",
    "start": "183799",
    "end": "189400"
  },
  {
    "text": "Concepts so you would bake to a Debian package and you know how to do upstart scripts and that kind of thing but when",
    "start": "189400",
    "end": "195159"
  },
  {
    "text": "people were trying to just run an application they just wanted to write their application and its dependencies",
    "start": "195159",
    "end": "200440"
  },
  {
    "text": "and simplify that uh build artifact um the finally was people yes they",
    "start": "200440",
    "end": "206840"
  },
  {
    "text": "understand instances but do they really want to understand instance types they'd rather just Express their resources and let someone else I.E myself manage those",
    "start": "206840",
    "end": "214439"
  },
  {
    "text": "resources for them talking about real value that we've realized over the last year of using uh containers um there's",
    "start": "214439",
    "end": "222560"
  },
  {
    "start": "216000",
    "end": "216000"
  },
  {
    "text": "actually I think it just came out on the tech blog um talking about some of the work that we're doing in our algorithms",
    "start": "222560",
    "end": "228120"
  },
  {
    "text": "around encoding um and how we were doing some stuff in load bandwidth uh markets",
    "start": "228120",
    "end": "233599"
  },
  {
    "text": "that we were able to do faster than any other competitor um an example that came out of this is with using our container",
    "start": "233599",
    "end": "239280"
  },
  {
    "text": "platform um setting up a really complex infrastructure for doing a widescale algorithm test was taking about a month",
    "start": "239280",
    "end": "245879"
  },
  {
    "text": "with containers that's down to a week uh we also have a what we like to call um a",
    "start": "245879",
    "end": "252519"
  },
  {
    "text": "distributed eventually consistent uh monor repo I we do not have a monor repo",
    "start": "252519",
    "end": "257560"
  },
  {
    "text": "Forest C code we have all these microservices and they have Upstream consumers um and when we change things",
    "start": "257560",
    "end": "263800"
  },
  {
    "text": "like our build infrastructure we change things like our uh Cloud platform libraries or we change a microservice",
    "start": "263800",
    "end": "269479"
  },
  {
    "text": "that 10 other people depend on what happens typically we run the test on it",
    "start": "269479",
    "end": "274680"
  },
  {
    "text": "it passes a test yay the next day the people that depend on it build their stuff it doesn't pass their test boo um",
    "start": "274680",
    "end": "281560"
  },
  {
    "text": "and they have to go through this cycle of debugging it all the consumers what we did with this system once we could",
    "start": "281560",
    "end": "286880"
  },
  {
    "text": "run some massively parallel um build systems is now we can automatically build not only the test that someone is",
    "start": "286880",
    "end": "293680"
  },
  {
    "text": "running but all the test of everyone depends on the component that we're changing uh we call that system a",
    "start": "293680",
    "end": "299039"
  },
  {
    "text": "Niagara and it's just literally saved hundreds of hours of debugging of Upstream systems and then the final one",
    "start": "299039",
    "end": "305560"
  },
  {
    "text": "here is in the services space we've been working in a container space with these smaller run times like node.js and",
    "start": "305560",
    "end": "312520"
  },
  {
    "text": "people that were used to writing just nodejs scripts for our Edge didn't",
    "start": "312520",
    "end": "317600"
  },
  {
    "text": "really want to understand what infrastructure was they wanted to say hey I just want to code the application I don't want to have to understand how",
    "start": "317600",
    "end": "323280"
  },
  {
    "text": "to package it um what instances resources all that kind of stuff we could let them Focus back on their",
    "start": "323280",
    "end": "328759"
  },
  {
    "text": "application so a lot a question we get a lot is why didn't we pick another container",
    "start": "328759",
    "end": "334759"
  },
  {
    "start": "329000",
    "end": "329000"
  },
  {
    "text": "management solution why' we build our own um the fact is most of the other container Management Solutions out there",
    "start": "334759",
    "end": "340960"
  },
  {
    "text": "are focused on the data center or crossing Data Center and cloud and abstracting the cloud that was working",
    "start": "340960",
    "end": "346360"
  },
  {
    "text": "under them or even multicloud um we're on for folks don't we're all in on AWS",
    "start": "346360",
    "end": "352520"
  },
  {
    "text": "so we wanted an accelerated um highly tuned to Amazon container solution uh we",
    "start": "352520",
    "end": "358479"
  },
  {
    "text": "also had this amazing cloud platform that I talked about with microservices and we just needed that puzzle piece that was a cluster manager for",
    "start": "358479",
    "end": "365199"
  },
  {
    "text": "containers that would fit into that existing Cloud platform uh we didn't want to have a a lot of the other",
    "start": "365199",
    "end": "370680"
  },
  {
    "text": "Solutions out there have continuous delivery aspects built into them they have service Discovery they have Telemetry systems we didn't need all",
    "start": "370680",
    "end": "377440"
  },
  {
    "text": "that we just needed a container cluster manager just like we had a VM cluster manager with asgs so and and finally",
    "start": "377440",
    "end": "385599"
  },
  {
    "text": "pretty important also really not ready for the scale that we were at so where",
    "start": "385599",
    "end": "390800"
  },
  {
    "text": "did we come into containers this is a little history as we build through we started with batch um batch users as I",
    "start": "390800",
    "end": "398199"
  },
  {
    "start": "396000",
    "end": "396000"
  },
  {
    "text": "was saying kind of want to focus on their application they've got some workload that needs to run they've got a",
    "start": "398199",
    "end": "403360"
  },
  {
    "text": "couple files that Define their job a couple dependencies and libraries that Define how that job works and they just",
    "start": "403360",
    "end": "409599"
  },
  {
    "text": "say can please someone just run it to completion reliably they don't want to understand what an instance size is they",
    "start": "409599",
    "end": "415960"
  },
  {
    "text": "don't want to understand what autoscaling groups are they don't want to understand an operating system for that matter they just have something",
    "start": "415960",
    "end": "422479"
  },
  {
    "text": "that says please run this for me we had some definite experience with",
    "start": "422479",
    "end": "427800"
  },
  {
    "start": "425000",
    "end": "425000"
  },
  {
    "text": "containers not Docker containers which we'll get into in a second um but in we've Tech blogged about both of these",
    "start": "427800",
    "end": "433599"
  },
  {
    "text": "our workflow engine mson our stream processing system mantis we using",
    "start": "433599",
    "end": "438720"
  },
  {
    "text": "straightup cgroups and msos uh for a very long time at Netflix so we knew this technology could work for us it was",
    "start": "438720",
    "end": "445720"
  },
  {
    "text": "pretty simplistic isolation but the most important part was they were very specific packaging formats so if you",
    "start": "445720",
    "end": "452360"
  },
  {
    "text": "wanted to run a spark job you code the spark job you deploy as the spark job or you want to do a mantis or mesus job you",
    "start": "452360",
    "end": "458479"
  },
  {
    "text": "would do exactly those packaging formats if you want to run something that doesn't fit into those Frameworks you're",
    "start": "458479",
    "end": "463560"
  },
  {
    "text": "out of luck so enter Titus so Titus is my project um that's our logo on the",
    "start": "463560",
    "end": "469720"
  },
  {
    "start": "465000",
    "end": "465000"
  },
  {
    "text": "left hand side there um and what it is is it's container execution engine using Docker at the bottom uh with a lot of",
    "start": "469720",
    "end": "476879"
  },
  {
    "text": "AWS integration which we'll go through we do the resource management and optimization on behalf of all the jobs",
    "start": "476879",
    "end": "482680"
  },
  {
    "text": "that are coming into it um and at at this point in time it was just batch job",
    "start": "482680",
    "end": "488919"
  },
  {
    "text": "management giving you an idea what batch is to us we do algorithm uh model training so this is It's kind of fuzzy",
    "start": "488919",
    "end": "495960"
  },
  {
    "text": "because there's so many things on it but this is an a model training that's spanning out to all the languages I",
    "start": "495960",
    "end": "501639"
  },
  {
    "text": "think that we support and then filtering merging all these kind of things each one of these would turn into a Docker",
    "start": "501639",
    "end": "507080"
  },
  {
    "text": "container that would get run in massively par scale um in our infrastructure we also in this space use",
    "start": "507080",
    "end": "514240"
  },
  {
    "start": "512000",
    "end": "512000"
  },
  {
    "text": "gpus uh so we do deep learning do neural Nets um and we started with g2s we used",
    "start": "514240",
    "end": "520440"
  },
  {
    "text": "the Nvidia Docker driver to map up the Nvidia uh drivers so the programs could",
    "start": "520440",
    "end": "526360"
  },
  {
    "text": "load it as well as the devices up into the docker containers um and now this is",
    "start": "526360",
    "end": "531600"
  },
  {
    "text": "like really cool people get gpus On Demand so they're like okay I just package up a Docker container that's",
    "start": "531600",
    "end": "537800"
  },
  {
    "text": "going to Cuda I'll go ahead and code that up and I'll let somebody else run the infrastructure that most makes those",
    "start": "537800",
    "end": "543600"
  },
  {
    "text": "gpus available to me we started on g2s we actually just recently moved to p2s",
    "start": "543600",
    "end": "549320"
  },
  {
    "text": "uh 8 exls and we recognized uh about a 2X performance Improvement of moving to",
    "start": "549320",
    "end": "554440"
  },
  {
    "text": "the p2s we haven't done the work yet to sort of optimize to the newer features that are available in the gpus and p2s",
    "start": "554440",
    "end": "561279"
  },
  {
    "text": "but just moving from g2s to p2s we got a 2X performance Improvement um talking about some of the",
    "start": "561279",
    "end": "567800"
  },
  {
    "start": "564000",
    "end": "564000"
  },
  {
    "text": "other use cases uh median coding I sort of already covered a little bit earlier but you think about a a video coming in",
    "start": "567800",
    "end": "576320"
  },
  {
    "text": "um in raw format it usually gets chunked up it then each of those chunks have get encoded in all kinds of different",
    "start": "576320",
    "end": "582800"
  },
  {
    "text": "encoding algorithms um in all different formats for your TV for your phone for",
    "start": "582800",
    "end": "588600"
  },
  {
    "text": "your uh laptop and that's just a lot of work that has to happen it happens in batch um so we run that in this system",
    "start": "588600",
    "end": "595720"
  },
  {
    "text": "uh we also do water marking which is pretty cool um this is where some of our time sensitive batch came from initially",
    "start": "595720",
    "end": "602480"
  },
  {
    "text": "um so this is if we're for folks that don't know I hope everyone knows we have a Originals uh so we have a lot of",
    "start": "602480",
    "end": "609399"
  },
  {
    "text": "content that's original to us that means we have a lot of content that has to go out to people before you guys see it um",
    "start": "609399",
    "end": "616399"
  },
  {
    "text": "the most interesting one in the watermarking space is when the scripts go out to the actors um the PDFs get",
    "start": "616399",
    "end": "623040"
  },
  {
    "text": "digitally watermarked to who we're sending it to so you can imagine a big file comes in it gets transcoded out",
    "start": "623040",
    "end": "629040"
  },
  {
    "text": "many many different times with different watermarks in the case of a video it's going to have the actors and you're going to see green screen like their",
    "start": "629040",
    "end": "635920"
  },
  {
    "text": "name behind them so if it gets out we know uh who gave it out um and all this is happening as batch in our container",
    "start": "635920",
    "end": "643720"
  },
  {
    "text": "um another thing you'll see in batches everybody wants to write reports um hourly um daily reports probably the",
    "start": "643720",
    "end": "650320"
  },
  {
    "text": "most interesting use of the report is we have this big CDN that serves all the movie bits all the reporting for our CDN",
    "start": "650320",
    "end": "656519"
  },
  {
    "text": "network is built um on top of the Titus container cloud and containers so we learned some lessons",
    "start": "656519",
    "end": "662839"
  },
  {
    "start": "661000",
    "end": "661000"
  },
  {
    "text": "from batch um first Docker and having a more generic um here's my files here's",
    "start": "662839",
    "end": "669000"
  },
  {
    "text": "my entry point gave us a more General uh batch processing system than we ever had before um we also found that uh every uh",
    "start": "669000",
    "end": "678440"
  },
  {
    "text": "scheduling and containers kind of go hand inand um you can't just start doing containers you need to schedule the",
    "start": "678440",
    "end": "684000"
  },
  {
    "text": "resources um and given that we're on Amazon we can do something that those other cluster managers don't do yet and",
    "start": "684000",
    "end": "689880"
  },
  {
    "text": "elastically scale so as workload comes into our system we'll actually scale the underlying resource pool get the jobs",
    "start": "689880",
    "end": "696880"
  },
  {
    "text": "done and scale it back down um and then finally we'll talk more about this as we go towards the end um it was easy",
    "start": "696880",
    "end": "704000"
  },
  {
    "text": "initially when people were like well I have a batch job it has to be done sometime before tomorrow morning um but",
    "start": "704000",
    "end": "709600"
  },
  {
    "text": "then people started to show up like the um like the original engineering team and say hey I have these scripts if they",
    "start": "709600",
    "end": "715560"
  },
  {
    "text": "don't make it out by tomorrow morning shooting doesn't happen so time sensitive batch starts to show up as",
    "start": "715560",
    "end": "721560"
  },
  {
    "start": "721000",
    "end": "721000"
  },
  {
    "text": "well um ignore these numbers because we have to get our slides in really early for reinvent um this is this was the",
    "start": "721560",
    "end": "728160"
  },
  {
    "text": "numbers on 117 I just pulled it up uh for last week um and it says test and",
    "start": "728160",
    "end": "733800"
  },
  {
    "text": "prod don't think of test and PR actually the whole um uh the build system I",
    "start": "733800",
    "end": "738880"
  },
  {
    "text": "talked about Niagara runs and test so to test and prod for us here actually prod",
    "start": "738880",
    "end": "744440"
  },
  {
    "text": "um our customers are running in both uh we did 600,000 containers last week um",
    "start": "744440",
    "end": "750600"
  },
  {
    "text": "in Titus um we Peak at thousands of containers per minute um and we Peak up",
    "start": "750600",
    "end": "756360"
  },
  {
    "text": "at 3,000 virtual machines that are sliced up um to handle these containers",
    "start": "756360",
    "end": "761440"
  },
  {
    "text": "mostly across r3s and m4s uh which are both pretty beefy",
    "start": "761440",
    "end": "767240"
  },
  {
    "text": "boxes so batch is interesting but what about services so easy right we'll take Titus",
    "start": "767240",
    "end": "773160"
  },
  {
    "start": "771000",
    "end": "771000"
  },
  {
    "text": "We'll add now instead of batch in the job management tier We'll add service we'll reuse all that great AWS",
    "start": "773160",
    "end": "778399"
  },
  {
    "text": "integration we use Docker services are just a long running batch right no much more complex so this is",
    "start": "778399",
    "end": "786199"
  },
  {
    "start": "784000",
    "end": "784000"
  },
  {
    "text": "where we spend a lot of our work over the last three quarters um they Services",
    "start": "786199",
    "end": "791360"
  },
  {
    "text": "constantly resize there's autoscaling requirements um they have more State um",
    "start": "791360",
    "end": "797079"
  },
  {
    "text": "you know a batch job is either running or it's not um a u service job is well",
    "start": "797079",
    "end": "803360"
  },
  {
    "text": "it's started now it's registered in Discovery now it's ready for uh handling",
    "start": "803360",
    "end": "808480"
  },
  {
    "text": "traffic um and there's a lot more State Management and underlying both of those as people that under that maintain the",
    "start": "808480",
    "end": "814639"
  },
  {
    "text": "resources below them it makes it really hard for us to up upgrade our infrastructure which we'll touch on more",
    "start": "814639",
    "end": "820600"
  },
  {
    "text": "as we go the other thing is that amazingly resilient um devops uh",
    "start": "820600",
    "end": "827560"
  },
  {
    "text": "microservices what other acronyms can I throw in there or buzzword um there's a lot of tools at Netflix that made that",
    "start": "827560",
    "end": "834079"
  },
  {
    "text": "work for services for virtual machines we had to make those same tools work for",
    "start": "834079",
    "end": "839120"
  },
  {
    "text": "four containers so there was quite a bit of expectation for reusing the existing",
    "start": "839120",
    "end": "844480"
  },
  {
    "text": "well- defined tool sets but probably the hardest problem was networking um so you",
    "start": "844480",
    "end": "850279"
  },
  {
    "start": "846000",
    "end": "846000"
  },
  {
    "text": "know if if you're going to run a bunch of containers on a single virtual machine and um you know you're thinking",
    "start": "850279",
    "end": "855800"
  },
  {
    "text": "about virtual machines and each of them have an IP stack maybe it's like the blue uh plugs up here it's really easy",
    "start": "855800",
    "end": "862360"
  },
  {
    "text": "once you go to containers and you start making a multi-tenant and everyone's sharing the red wire gets a lot harder so we kind of let we we sat back",
    "start": "862360",
    "end": "870880"
  },
  {
    "start": "867000",
    "end": "867000"
  },
  {
    "text": "and said what are our requirements as we do networking we wanted full IP stacks for every container that wanted an IP",
    "start": "870880",
    "end": "877480"
  },
  {
    "text": "stack we did not believe in Port mapping we did not believe in proxies other things that could possibly make multi-",
    "start": "877480",
    "end": "884079"
  },
  {
    "text": "tendency work but wouldn't work for applications like AA and other things that hand out endpoints that have no",
    "start": "884079",
    "end": "889480"
  },
  {
    "text": "idea if you done some Network virtualization around them we wanted a full IP stack um also I think Verner",
    "start": "889480",
    "end": "896800"
  },
  {
    "text": "talked about this this morning um we didn't want to leave security behind so we wanted Security Group support we",
    "start": "896800",
    "end": "902079"
  },
  {
    "text": "wanted IM roles and then finally you don't want one of your containers taking 90% of that red plug and everyone else",
    "start": "902079",
    "end": "909360"
  },
  {
    "text": "fighting for the last 10% we need a bandwidth isolation in our networking so how did we get that done we did two",
    "start": "909360",
    "end": "916240"
  },
  {
    "text": "things we wrote uh what we call our networking driver um that's integrated with VPC um and we wrote a metadata",
    "start": "916240",
    "end": "923120"
  },
  {
    "text": "proxy and I'm going to take you through how we did each one of these so our networking integration with",
    "start": "923120",
    "end": "929720"
  },
  {
    "start": "926000",
    "end": "926000"
  },
  {
    "text": "doctor so how do we get VPC to work with Doctor um we have this thing called the executor that runs on each one of the",
    "start": "929720",
    "end": "935839"
  },
  {
    "text": "host nodes that is told to start containers when it's told to start a container it creates a networking",
    "start": "935839",
    "end": "941639"
  },
  {
    "text": "namespace it does this by um look it can reuse if it's already been this has",
    "start": "941639",
    "end": "947519"
  },
  {
    "text": "already occurred but taking you through a new case um it will create and attach an eni um to the virtual machine it'll",
    "start": "947519",
    "end": "955360"
  },
  {
    "text": "adjust its Security Group um and it'll give it an IP address we will then launch what we're calling a",
    "start": "955360",
    "end": "962480"
  },
  {
    "text": "pod root container U for folks that are familiar with kubernetes it's really kind of a similar concept um so we start",
    "start": "962480",
    "end": "969880"
  },
  {
    "text": "a pause container which is nothing more than a long running sleep that takes no resources um and we configure that from",
    "start": "969880",
    "end": "977120"
  },
  {
    "text": "a Docker perspective with net equals none it means don't set up any of the networking stuff we're going to do",
    "start": "977120",
    "end": "983040"
  },
  {
    "text": "that we then will configure uh the virtual interfaces we'll create some routing rules that Associated uh with",
    "start": "983040",
    "end": "990399"
  },
  {
    "text": "the right eni to get it back out to VPC we'll configure a metadata proxy Nat",
    "start": "990399",
    "end": "995839"
  },
  {
    "text": "which I'll talk more in length about um and then we configure via traffic control the bandwidth all of this what",
    "start": "995839",
    "end": "1001920"
  },
  {
    "text": "we're calling the Pod rout container and basically it's just a container to hold all the information about networking um",
    "start": "1001920",
    "end": "1007920"
  },
  {
    "text": "that we're going to use so then at that point um oh sorry let me go back at that",
    "start": "1007920",
    "end": "1014079"
  },
  {
    "text": "point we return back to our executor what that uh container ID is for the Pod rout",
    "start": "1014079",
    "end": "1019600"
  },
  {
    "text": "and we then will launch the real application container so the one that the uh the user wanted to run will say",
    "start": "1019600",
    "end": "1025558"
  },
  {
    "text": "create the application container but create it with the networking name space of the Pod rout so it just Associates",
    "start": "1025559",
    "end": "1031918"
  },
  {
    "text": "itself with all that networking goodness that we just gave it I talked about a metadata proxy how",
    "start": "1031919",
    "end": "1037918"
  },
  {
    "start": "1036000",
    "end": "1036000"
  },
  {
    "text": "many people familiar with a metadata proxy okay it's a it's not everyone so",
    "start": "1037919",
    "end": "1043160"
  },
  {
    "text": "let me cover that real quick so there's this magical URL that if you're inside a virtual machine 169 254 169 254 that",
    "start": "1043160",
    "end": "1052080"
  },
  {
    "text": "things in the Amazon SDK will call to um to get I credentials um get what's your",
    "start": "1052080",
    "end": "1058520"
  },
  {
    "text": "instance ID what's your Ami ID there's all this basically who am I information",
    "start": "1058520",
    "end": "1064160"
  },
  {
    "text": "that's in the Amazon metadata uh service and we didn't want all the containers",
    "start": "1064160",
    "end": "1069679"
  },
  {
    "text": "seeing the hosts because we don't want the containers to get the IM credentials of the host it may have very different",
    "start": "1069679",
    "end": "1075679"
  },
  {
    "text": "uh needs from a security perspective and we also didn't want to confuse the containers to say hey what instance am I",
    "start": "1075679",
    "end": "1081480"
  },
  {
    "text": "well you're instance two3 the other container says hey what instance am I instance two3 you're not the same",
    "start": "1081480",
    "end": "1086880"
  },
  {
    "text": "instance so um what we did was we wrote um as we configur the networking we NAD in this server that we run on each of",
    "start": "1086880",
    "end": "1093280"
  },
  {
    "text": "the hosts called the metadata proxy um if the container asks um for 169 254 169",
    "start": "1093280",
    "end": "1100840"
  },
  {
    "text": "254 it basically gets sent to Port local Port 9999 um and if it asks for what's",
    "start": "1100840",
    "end": "1107400"
  },
  {
    "text": "my IP address what's my instance ID what's my host name everything that the VPC networking driver figured out in the",
    "start": "1107400",
    "end": "1113320"
  },
  {
    "text": "past it's going to return that information instead of the host's information if it asked about things",
    "start": "1113320",
    "end": "1118640"
  },
  {
    "text": "that are very specific to Virtual machines um it's going to go I don't I don't know that that doesn't really",
    "start": "1118640",
    "end": "1124200"
  },
  {
    "text": "matter to you so you're not going to get back what your Ami is of the host um and",
    "start": "1124200",
    "end": "1129320"
  },
  {
    "text": "then most importantly we wanted um Amazon SDK based applications just to",
    "start": "1129320",
    "end": "1134679"
  },
  {
    "text": "continue to work on change so when we schedule a container we'll actually pass down what the IM role is of that",
    "start": "1134679",
    "end": "1141120"
  },
  {
    "text": "container and when the container says hey what's my IM role back through the metadata service which is how it works",
    "start": "1141120",
    "end": "1146400"
  },
  {
    "text": "give me my credentials will assume roll into that Ro get the credentials and",
    "start": "1146400",
    "end": "1151559"
  },
  {
    "text": "pass them back to the actual container and do that on their behalf anything else that's not blacklisted or wh listed",
    "start": "1151559",
    "end": "1158039"
  },
  {
    "text": "we'll just proxy back to the original metadata proxy putting it all together this is",
    "start": "1158039",
    "end": "1163720"
  },
  {
    "start": "1162000",
    "end": "1162000"
  },
  {
    "text": "you can take a picture but these will be in the slides um this is basically the mess of netw working that you have to",
    "start": "1163720",
    "end": "1169520"
  },
  {
    "text": "get working um so starting on the left side um you can see uh that would say",
    "start": "1169520",
    "end": "1176320"
  },
  {
    "text": "maybe a batch container so it said I don't need a routable IP address but I do want access from a security group",
    "start": "1176320",
    "end": "1183039"
  },
  {
    "text": "perspective I want to be able to reach out um what you can see is because of that happening we created an eni1 we",
    "start": "1183039",
    "end": "1190360"
  },
  {
    "text": "Associated Security Group a with it and we give it an IP address but we won't let it um accept traffic we'll just let",
    "start": "1190360",
    "end": "1197480"
  },
  {
    "text": "it get out um containers two and three uh you can see are both asking for",
    "start": "1197480",
    "end": "1202880"
  },
  {
    "text": "Security Group X so what we were able to do is collocate them on en2 and you can see ipa1 and ip2 on eii 2 we give back",
    "start": "1202880",
    "end": "1211720"
  },
  {
    "text": "to Containers two and three um container four which is a very different set of",
    "start": "1211720",
    "end": "1216799"
  },
  {
    "text": "security groups so it's it's using Y and Z we'll have to allocate yet another eni",
    "start": "1216799",
    "end": "1222039"
  },
  {
    "text": "uh eni three for that and give it IP address 3 um all of this is done and configured via uh Linux policy BAS",
    "start": "1222039",
    "end": "1228799"
  },
  {
    "text": "routing um traffic control gets applied to do bandwidth isolation um so we can guarantee the same megabits uh whatever",
    "start": "1228799",
    "end": "1236840"
  },
  {
    "text": "container one two and three asked for from a uh megabits per second perspective we uh control via traffic",
    "start": "1236840",
    "end": "1242720"
  },
  {
    "text": "control and don't let anyone get more than what they ask for and then finally we'll natat into all these containers",
    "start": "1242720",
    "end": "1248679"
  },
  {
    "text": "the metadata proxy additional AWS integration points",
    "start": "1248679",
    "end": "1255000"
  },
  {
    "start": "1251000",
    "end": "1251000"
  },
  {
    "text": "um we on behalf of the containers we we realize if they write log files that are",
    "start": "1255000",
    "end": "1261000"
  },
  {
    "text": "in a rotated format so it has like the timestamp or the the 1.2 um when we see",
    "start": "1261000",
    "end": "1266640"
  },
  {
    "text": "those occur our executor is uh uploading those log files to S3 um as they are",
    "start": "1266640",
    "end": "1273159"
  },
  {
    "text": "being rotated we also give people access to directly view any log file on inside",
    "start": "1273159",
    "end": "1278279"
  },
  {
    "text": "of the container um and when the container shuts down we'll make sure all the log files get up um I don't think I",
    "start": "1278279",
    "end": "1285240"
  },
  {
    "text": "touch on it here but we we also through um Docker exec and some trickery we",
    "start": "1285240",
    "end": "1291200"
  },
  {
    "text": "actually have a onor Bastion and SSH command that can let you get into the Container it looks like you have a shell",
    "start": "1291200",
    "end": "1297919"
  },
  {
    "text": "uh running inside of the container um we isolate dis from a quoting perspective we use EFS for that so if you say I want",
    "start": "1297919",
    "end": "1305159"
  },
  {
    "text": "this many gigabytes of dis everybody can make sure they get the amount of dis that they were uh requesting we also do",
    "start": "1305159",
    "end": "1312039"
  },
  {
    "text": "sort of the user data um so every container gets started with a set of environment variables also we B mount in",
    "start": "1312039",
    "end": "1318880"
  },
  {
    "text": "a script file for people that don't want to use environment variables of you know what cluster are you in what your",
    "start": "1318880",
    "end": "1324000"
  },
  {
    "text": "application name is um all the things that you would expect to come down through user data um we also do instance",
    "start": "1324000",
    "end": "1330760"
  },
  {
    "text": "type selection so I said before users don't care about instances but we care",
    "start": "1330760",
    "end": "1336039"
  },
  {
    "text": "because we want to run uh their applications most efficiently so if we see an application is memory hungry like",
    "start": "1336039",
    "end": "1342240"
  },
  {
    "text": "an algorithm training job uh we'll say well your CPU to memory ratio is X that",
    "start": "1342240",
    "end": "1347279"
  },
  {
    "text": "means we're going to land you an R R3 we're going to pin your entire job to an R3 um if you're more of a service",
    "start": "1347279",
    "end": "1352600"
  },
  {
    "text": "oriented job and your ratio is like a 1:2 or 1: four ratio there we'll put you",
    "start": "1352600",
    "end": "1357760"
  },
  {
    "text": "on the C4s or m4s and we'll pin you to that we all we do that as part of the",
    "start": "1357760",
    "end": "1362960"
  },
  {
    "text": "scheduling system as opposed to letting the user control that um we also I'll cover how we do the",
    "start": "1362960",
    "end": "1370400"
  },
  {
    "text": "elastic scaling later um I also said we wanted this system to integrate with the",
    "start": "1370400",
    "end": "1376240"
  },
  {
    "start": "1372000",
    "end": "1372000"
  },
  {
    "text": "Netflix infrastructure uh what I mean by this most of these things up here are open source um so Spiner Atlas next",
    "start": "1376240",
    "end": "1383799"
  },
  {
    "text": "one's Eureka Eda um and then Sim Army at the bottom um we have all these",
    "start": "1383799",
    "end": "1390600"
  },
  {
    "text": "technologies that people expect to use when they're doing cicd deployments to",
    "start": "1390600",
    "end": "1396279"
  },
  {
    "text": "our Cloud infrastructure they use Spiner um when they're looking up information about um what's giving insight into the",
    "start": "1396279",
    "end": "1403320"
  },
  {
    "text": "application how it's working they go to atlas um when they register and",
    "start": "1403320",
    "end": "1408520"
  },
  {
    "text": "interconnect from an IP perspective they go to our Discovery systems we had to make each one of these systems work with",
    "start": "1408520",
    "end": "1414440"
  },
  {
    "text": "containers uh when we started most of these systems um basically dup based off",
    "start": "1414440",
    "end": "1419720"
  },
  {
    "text": "the idh instance because they thought there would only ever be one thing that was reporting in from an instance so you",
    "start": "1419720",
    "end": "1424919"
  },
  {
    "text": "can imagine what happens with something like health check when I have three containers come in all registering with",
    "start": "1424919",
    "end": "1430760"
  },
  {
    "text": "the same uh idh instance and now like two of them are healthy one is not um",
    "start": "1430760",
    "end": "1436559"
  },
  {
    "text": "last one in was kind of winning which was a big problem so we did extensive amount of work in each one of these",
    "start": "1436559",
    "end": "1441720"
  },
  {
    "text": "systems to bring multi-tenancy back into them where they could realize that um a",
    "start": "1441720",
    "end": "1447679"
  },
  {
    "text": "container ID was different than a virtual machine instance",
    "start": "1447679",
    "end": "1452880"
  },
  {
    "text": "ID we did that for two reasons one um",
    "start": "1452880",
    "end": "1457960"
  },
  {
    "start": "1453000",
    "end": "1453000"
  },
  {
    "text": "this is basically on the left hand side in the blue is where we were before with virtual machines so we're running",
    "start": "1457960",
    "end": "1464320"
  },
  {
    "text": "service applications they have certain Cloud platform libraries um that are in them that do metrics IPC other things",
    "start": "1464320",
    "end": "1471240"
  },
  {
    "text": "and they basically were running in clusters they were running in clusters based on the AWS Auto scaler which we",
    "start": "1471240",
    "end": "1477480"
  },
  {
    "text": "all love know and love um we wrote a job control system for services that looked",
    "start": "1477480",
    "end": "1483799"
  },
  {
    "text": "like the AWS autoscaler so it has clusters that have stack detail sequence",
    "start": "1483799",
    "end": "1490200"
  },
  {
    "text": "number um and application name U which is how we code into an ASG uh what an",
    "start": "1490200",
    "end": "1495880"
  },
  {
    "text": "application uh versioned ASG is um it has min max desired as you would expect",
    "start": "1495880",
    "end": "1501320"
  },
  {
    "text": "and if you adjust Min it starts things if you adjust desired it starts things if you drop uh Max it stops things so uh",
    "start": "1501320",
    "end": "1509039"
  },
  {
    "text": "basically we have service control that do does all the same things um now we",
    "start": "1509039",
    "end": "1514360"
  },
  {
    "text": "can do both service and batch behind that same job control system um the two reasons why I did this is when people",
    "start": "1514360",
    "end": "1520600"
  },
  {
    "text": "interact with this system developers interact with the system they deploy with Spiner um they go to Telemetry",
    "start": "1520600",
    "end": "1527120"
  },
  {
    "text": "system Atlas they go to our um uh cache in ETA and there's lots of systems that",
    "start": "1527120",
    "end": "1532440"
  },
  {
    "text": "build on top of that cache they go to our service Discovery we didn't want them to go to different systems when",
    "start": "1532440",
    "end": "1537880"
  },
  {
    "text": "they were in containers than when they were in VMS so we kept all those external interfaces exactly the same the",
    "start": "1537880",
    "end": "1544039"
  },
  {
    "text": "second thing is for a very long time we're not we're at Netflix we're not saying hey everyone that's in a virtual",
    "start": "1544039",
    "end": "1549279"
  },
  {
    "text": "machine you got to move the containers that's not that's not our thing we're as I talked through some of these use cases",
    "start": "1549279",
    "end": "1554799"
  },
  {
    "text": "the cases where it made sense is where we uh new cases is where it makes sense um we wanted to be able to operationally",
    "start": "1554799",
    "end": "1562000"
  },
  {
    "text": "debug the systems the same so I didn't want to uh have people that are working in an application that's in a virtual",
    "start": "1562000",
    "end": "1568320"
  },
  {
    "text": "machine that's connecting to a microservice in a container have to know that well in the virtual machines I'm",
    "start": "1568320",
    "end": "1573720"
  },
  {
    "text": "going to use VPC for the networking but in the container Cloud I'm going to use some overlay Network or whatnot and I",
    "start": "1573720",
    "end": "1579760"
  },
  {
    "text": "have to debug them differently we wanted to make sure we leverage all of those existing systems from an operational",
    "start": "1579760",
    "end": "1584880"
  },
  {
    "text": "perspective as well how many people have seen sper oh awesome awesome so so for people",
    "start": "1584880",
    "end": "1593200"
  },
  {
    "start": "1587000",
    "end": "1587000"
  },
  {
    "text": "that haven't seen Spiner this is our deployment pipelines our CD system as a",
    "start": "1593200",
    "end": "1598279"
  },
  {
    "text": "service this is as a developer at Netflix this is how you work with a microservices network um what you're",
    "start": "1598279",
    "end": "1603919"
  },
  {
    "text": "seeing here uh typically is you'd see these little green chicklets the green chicklets actually represent they'll be",
    "start": "1603919",
    "end": "1610960"
  },
  {
    "text": "green if they're up and in Discovery and healthy there'll be various colors if they're not one of those things and they",
    "start": "1610960",
    "end": "1616480"
  },
  {
    "text": "usually represent a virtual machine so you could go out and see some tiers that are pretty big lots and lots of these",
    "start": "1616480",
    "end": "1621960"
  },
  {
    "text": "chicklets um and then you can do various deployment pipelines against them um for",
    "start": "1621960",
    "end": "1627720"
  },
  {
    "text": "people that are using containers they go to the exact same thing this is actually uh a personal project of mine at open",
    "start": "1627720",
    "end": "1633480"
  },
  {
    "text": "source called OSS tracker um and it's running in containers the difference is",
    "start": "1633480",
    "end": "1638799"
  },
  {
    "text": "you can see there's a little alien well not alien head I guess robot head with Titus um as the little icon there um as",
    "start": "1638799",
    "end": "1645960"
  },
  {
    "text": "opposed to it would be the blocks if you were running an Amazon when they create the server group they say which cloud",
    "start": "1645960",
    "end": "1651600"
  },
  {
    "text": "provider do you want to use and this is a little of a misnomer right it's on the left if you pick Amazon web services you",
    "start": "1651600",
    "end": "1657679"
  },
  {
    "text": "get virtual machines on the right if you pick Titus you'll get containers running on top of Amazon virtual machines so",
    "start": "1657679",
    "end": "1663640"
  },
  {
    "text": "it's not like we're moving clouds we basically have a new I layer on top of the existing I layer from a Spiner",
    "start": "1663640",
    "end": "1670600"
  },
  {
    "text": "perspective um but now the deployment artifact starts to change so for folks that have seen Spiner for um virtual",
    "start": "1670600",
    "end": "1677760"
  },
  {
    "text": "machines and Amis typically there's a trigger that kicks off your pipeline that says hey there's a new build out in",
    "start": "1677760",
    "end": "1684240"
  },
  {
    "text": "that's been built in Jenkins of an OS package that now is ready to be baked into an image um with containers it",
    "start": "1684240",
    "end": "1691120"
  },
  {
    "text": "differs right so now we have this deployment artifact that is the image so I can kick off a deploy pipeline now",
    "start": "1691120",
    "end": "1697000"
  },
  {
    "text": "based on a Docker registry so in this case what this does is it'll kick this pipeline off every time a new label I.E",
    "start": "1697000",
    "end": "1704480"
  },
  {
    "text": "a new version of my Docker image shows up out in the docker Registries that we",
    "start": "1704480",
    "end": "1710480"
  },
  {
    "text": "run creating new server group is mostly the same um the biggest difference is",
    "start": "1710480",
    "end": "1716679"
  },
  {
    "text": "down here in the left hand side uh bottom left instead of saying Hey I want an R3 adxl instance I say I want one CPU",
    "start": "1716679",
    "end": "1724399"
  },
  {
    "text": "I want this much memory I want this much dis I want this much Network you just express your resources you don't think",
    "start": "1724399",
    "end": "1730559"
  },
  {
    "text": "about instance types anymore um but then we wanted to make sure things continue to work so you can see because of a work",
    "start": "1730559",
    "end": "1737480"
  },
  {
    "text": "that we did in the VPC driver and the work in the metadata proxy you can still assign IM roles and security groups uh",
    "start": "1737480",
    "end": "1744360"
  },
  {
    "text": "to this cluster of containers that going to get started and most importantly the deployment strategy is exactly the same",
    "start": "1744360",
    "end": "1750799"
  },
  {
    "text": "so we have many different deployment strategies many different ways people approach deployments across Netflix they",
    "start": "1750799",
    "end": "1756200"
  },
  {
    "text": "can now use this because at the end of the day Spiner treats the things it's deploying as instances and clusters and",
    "start": "1756200",
    "end": "1762679"
  },
  {
    "text": "now we have instances and clusters in containers just like we had them with virtual machines and auto scaling groups",
    "start": "1762679",
    "end": "1770039"
  },
  {
    "text": "and from an operational perspective all the same stuff occur um is there so at a",
    "start": "1771399",
    "end": "1776480"
  },
  {
    "text": "glance you can see what your instance is um I think um in this case I clicked on",
    "start": "1776480",
    "end": "1782200"
  },
  {
    "text": "the first chicklet here it tells me that the task ID which is the container um is",
    "start": "1782200",
    "end": "1787480"
  },
  {
    "text": "Titus 4036 and it's the uh first worker third or sorry first partition third",
    "start": "1787480",
    "end": "1793120"
  },
  {
    "text": "worker um it tells me what job it's running of think of that as ASG um it's",
    "start": "1793120",
    "end": "1798159"
  },
  {
    "text": "going to give me both the host's IP address and the container's IP address um so very similar to what we had before",
    "start": "1798159",
    "end": "1804559"
  },
  {
    "text": "but now I just list resources I don't have to list what uh instance size it's running on but all of the same inside",
    "start": "1804559",
    "end": "1811799"
  },
  {
    "text": "actions and um operational action still work um whether now now that it's in",
    "start": "1811799",
    "end": "1818039"
  },
  {
    "text": "containers we've also done Advanced integration like chaos monkey um again I",
    "start": "1818039",
    "end": "1823519"
  },
  {
    "text": "I think I covered this a little bit at the beginning but now uh chaos monkey is inte with Spiner chaos monkey is a",
    "start": "1823519",
    "end": "1829679"
  },
  {
    "text": "little process that runs around in every part of my infrastructure it shoots one of the instances at least a day um we",
    "start": "1829679",
    "end": "1835799"
  },
  {
    "text": "enable that for containers especially now that we have this extraction lay of of Spiner that chaos monkey can just say",
    "start": "1835799",
    "end": "1842480"
  },
  {
    "text": "now if it's on if it's on Titus I'm going to kill containers if it's on",
    "start": "1842480",
    "end": "1847679"
  },
  {
    "text": "Amazon I'm going to kill virtual machines um so people even think about resiliency in the exact same way they",
    "start": "1847679",
    "end": "1853519"
  },
  {
    "text": "think about in Virtual machines uh Telemetry was a little bit work um so we actually leveraged uh",
    "start": "1853519",
    "end": "1861159"
  },
  {
    "text": "Intel snap uh open source technology um what you're seeing up here is this is",
    "start": "1861159",
    "end": "1866279"
  },
  {
    "text": "the dashboard that anyone would go to whether they're in Virtual machines or whether they're in containers um and the",
    "start": "1866279",
    "end": "1873120"
  },
  {
    "text": "difference here is now instead of NF node is like the instance ID you it would be in a virtual machine it would",
    "start": "1873120",
    "end": "1879440"
  },
  {
    "text": "be ID Dash something something something in the container world it's this nice beautiful long uh uu ID um definitely uh",
    "start": "1879440",
    "end": "1887000"
  },
  {
    "text": "um cattle not pets um so those are very three different instances and I think um",
    "start": "1887000",
    "end": "1894360"
  },
  {
    "text": "what did I put up there uh their CPU time the other thing as you can see for for um uh system level Telemetry we're",
    "start": "1894360",
    "end": "1902159"
  },
  {
    "text": "pulling that out of the cgroup metrics um as opposed to pulling it back from the host operating system um all of our",
    "start": "1902159",
    "end": "1908639"
  },
  {
    "text": "same tagging works um everything works as before this then allows people to do alerts and dashboards on top of this in",
    "start": "1908639",
    "end": "1915799"
  },
  {
    "text": "a very transparent way whether it's containers um or virtual",
    "start": "1915799",
    "end": "1921240"
  },
  {
    "start": "1921000",
    "end": "1921000"
  },
  {
    "text": "machines let's talk a little bit about scheduling I said they go hand in hand um",
    "start": "1921279",
    "end": "1927440"
  },
  {
    "text": "so this is so this is n this is open source uh something called fenzo um",
    "start": "1927440",
    "end": "1933159"
  },
  {
    "text": "basically the heart of our scheduling system is this open source Library called fenzo it's not a scheduler it's a",
    "start": "1933159",
    "end": "1939320"
  },
  {
    "text": "library to help you build schedulers um and what we found was if you write a",
    "start": "1939320",
    "end": "1944519"
  },
  {
    "text": "batch uh job management you write service job management write stream maybe interactive you end up writing a",
    "start": "1944519",
    "end": "1950519"
  },
  {
    "text": "lot of the common boilerplate um scheduling facilities that we decided to",
    "start": "1950519",
    "end": "1955720"
  },
  {
    "text": "put in the library so we abstract the what we call task placement which fenzo does from the job management which is",
    "start": "1955720",
    "end": "1962159"
  },
  {
    "text": "the life cycle of any one task so essentially what fenzo does is it has a",
    "start": "1962159",
    "end": "1968080"
  },
  {
    "text": "you give it a list of things that need to run you also give it a list of resources that are available and it",
    "start": "1968080",
    "end": "1973639"
  },
  {
    "text": "makes a decision across those two and gives you a decision back of how you should place those resources um it does",
    "start": "1973639",
    "end": "1980720"
  },
  {
    "text": "all kinds of scheduling objectives like bin packing which is pretty important we'll cover it for elastic scaling um it",
    "start": "1980720",
    "end": "1986960"
  },
  {
    "text": "deals with totally heterogeneous sized uh resources I.E many different instance types as well as",
    "start": "1986960",
    "end": "1993720"
  },
  {
    "text": "containers um it also has call outs for autoscaling which is another cool thing that I think is unique to us so when you",
    "start": "1993720",
    "end": "2002480"
  },
  {
    "text": "call fenzo and you say hey here's all the work I need to do here's all the resources I have to do fenzo can say you know what you don't have enough",
    "start": "2002480",
    "end": "2008320"
  },
  {
    "text": "resources anymore why don't you scale up and it'll make it'll tell us how much to scale up by or how much to scale down by",
    "start": "2008320",
    "end": "2014600"
  },
  {
    "text": "and what instances to tear down um so pretty cool uh technology here uh also",
    "start": "2014600",
    "end": "2019880"
  },
  {
    "text": "does all kinds of different constraints which we'll talk about and then really important as we get into the final part of the presentation um originally this",
    "start": "2019880",
    "end": "2027279"
  },
  {
    "text": "was a library that did it was a library to help you write msos baced schedulers",
    "start": "2027279",
    "end": "2032919"
  },
  {
    "text": "um and Mos works by giving you a whole bunch of offers um maybe some that are duplicate out of the same machine um we",
    "start": "2032919",
    "end": "2040279"
  },
  {
    "text": "rewrote it to also support ECS where with ECS I know how many uh resources",
    "start": "2040279",
    "end": "2045519"
  },
  {
    "text": "are available on a machine right up front so we call that single offer mode and we've added that to fenzo as",
    "start": "2045519",
    "end": "2051040"
  },
  {
    "start": "2051000",
    "end": "2051000"
  },
  {
    "text": "well so fenzo basically the highlevel algorithm for fenzo that you get to put",
    "start": "2051040",
    "end": "2056679"
  },
  {
    "text": "plugins to the things that are circled in Orange is it will for every bit of work that you tell it it needs to run",
    "start": "2056679",
    "end": "2063158"
  },
  {
    "text": "it'll go through every host um set of resources it knows about it will say are",
    "start": "2063159",
    "end": "2068240"
  },
  {
    "text": "there any hard constraints I like if you need a GPU you probably shouldn't uh Lear on an instance that doesn't have a",
    "start": "2068240",
    "end": "2074320"
  },
  {
    "text": "GPU so there a hard constraint um you're going to evaluate the fitness um so",
    "start": "2074320",
    "end": "2079440"
  },
  {
    "text": "maybe you want uh to fit based on memory and CPU and certain ratios um you can",
    "start": "2079440",
    "end": "2085240"
  },
  {
    "text": "you can change uh that that Fitness calculation through plugins uh soft constraints is a great example would be",
    "start": "2085240",
    "end": "2092158"
  },
  {
    "text": "AZ balancing so you say I'd really like a third of these to land in az1 a third",
    "start": "2092159",
    "end": "2097280"
  },
  {
    "text": "in a Z2 a third in az3 now maybe sometimes it can't be satisfied but as",
    "start": "2097280",
    "end": "2103320"
  },
  {
    "text": "long as the fitness is good enough that's a soft constraint um and we basically evaluate that until the fit is",
    "start": "2103320",
    "end": "2108800"
  },
  {
    "text": "good enough and a certain number of hosts are evaluated and then we'll return that result back to our scheduler",
    "start": "2108800",
    "end": "2115079"
  },
  {
    "text": "um with the task placement decisions talking about some of the cool",
    "start": "2115079",
    "end": "2120119"
  },
  {
    "start": "2118000",
    "end": "2118000"
  },
  {
    "text": "things we're doing in scheduling um these are some pretty neat stuff uh the",
    "start": "2120119",
    "end": "2125839"
  },
  {
    "text": "amount of work that's in scheduling is just obscene once you actually start to getting into containers and some of the",
    "start": "2125839",
    "end": "2131119"
  },
  {
    "text": "scale and and and complexity that we deal with um so first when we started we basically had a big resource pool that",
    "start": "2131119",
    "end": "2138079"
  },
  {
    "text": "would Auto scale up and down um but it was everybody was equal so basically if",
    "start": "2138079",
    "end": "2144200"
  },
  {
    "text": "you could imagine a scenario where you had a long running service running and then someone comes in with batch and we do collocate the work between service",
    "start": "2144200",
    "end": "2150880"
  },
  {
    "text": "and batch uh a big batch job comes in and takes the rest of all of the resources up to the ASG Max and it's",
    "start": "2150880",
    "end": "2158240"
  },
  {
    "text": "going to run for the next 5 hours and all of a sudden something happens in a production and we have to redeploy that",
    "start": "2158240",
    "end": "2163560"
  },
  {
    "text": "service typically when we redeploy Services we'll create a new version of the ASG before we tear down the old no",
    "start": "2163560",
    "end": "2170800"
  },
  {
    "text": "resources left well let's just wait until that batch job finishes obviously not a good solution so we've just",
    "start": "2170800",
    "end": "2176680"
  },
  {
    "text": "recently added into fenzo and then our schuer on top of it multiple tiers of",
    "start": "2176680",
    "end": "2181800"
  },
  {
    "text": "capacity guarantees um so we have the flex tier which kind of operates the same way I just talked about where you'd",
    "start": "2181800",
    "end": "2188040"
  },
  {
    "text": "run uh things that really can deal with Dynamic capacity and don't care about what their start latency is and then we",
    "start": "2188040",
    "end": "2194560"
  },
  {
    "text": "have a critical tier the critical tier people call Titus and say U much like reserved instances in the Virtual",
    "start": "2194560",
    "end": "2201319"
  },
  {
    "text": "Machine World um they can reserve resources they can say I'm going to now be running on Titus this application and",
    "start": "2201319",
    "end": "2207839"
  },
  {
    "text": "it's going to use this many resources we'll make sure Titus will actually run two resource pools one for the flex tier",
    "start": "2207839",
    "end": "2214000"
  },
  {
    "text": "and one for the critical tier and in the critical tier we'll make sure those resources are available with some buffering to allow people to do",
    "start": "2214000",
    "end": "2220400"
  },
  {
    "text": "redeployments that guaranteed you're going to have guaranteed capacity and your start latency is going to be no no",
    "start": "2220400",
    "end": "2226240"
  },
  {
    "text": "slower than starting a container not starting up a virtual",
    "start": "2226240",
    "end": "2231560"
  },
  {
    "text": "machine um bin packing is really important to us um and and really cool that we can do this because we're on",
    "start": "2231880",
    "end": "2237720"
  },
  {
    "start": "2232000",
    "end": "2232000"
  },
  {
    "text": "elastic Cloud um so it's very easy to do um scaling up very hard to do scaling",
    "start": "2237720",
    "end": "2244079"
  },
  {
    "text": "down if you don't do bin packing um so what this shows is is in that Flex tier",
    "start": "2244079",
    "end": "2249160"
  },
  {
    "text": "we have min max and desired from an ASG perspective um and we'll continue adjust",
    "start": "2249160",
    "end": "2255520"
  },
  {
    "text": "desired as there's more or less work in the system but when we're scheduling tasks in fenzo we'll make sure that we",
    "start": "2255520",
    "end": "2262000"
  },
  {
    "text": "land them we'll do some level of a spreading for reliability but then after that we'll pack them into nodes um as",
    "start": "2262000",
    "end": "2269200"
  },
  {
    "text": "opposed to randomly spraying them across the nodes so when that work finishes we don't have a random task on each of the",
    "start": "2269200",
    "end": "2275440"
  },
  {
    "text": "machines and we can't shut down any of the machines we'll pack them in so once that work passes we can uh decrease the",
    "start": "2275440",
    "end": "2282160"
  },
  {
    "text": "desired back below um and and make forward progress in being able to reclaim those",
    "start": "2282160",
    "end": "2288319"
  },
  {
    "text": "hosts um the next one would be constraints I talked about this a little bit um the obvious constraint would if",
    "start": "2288800",
    "end": "2295440"
  },
  {
    "text": "I'm starting these two tasks it would be nice to have a soft constraint or even a hard constraint if you choose to um that",
    "start": "2295440",
    "end": "2301839"
  },
  {
    "text": "these when jobs start that they get sprayed across availability zones but there's also some other really",
    "start": "2301839",
    "end": "2307720"
  },
  {
    "text": "interesting sort of um business impactful um um constraints that are",
    "start": "2307720",
    "end": "2314680"
  },
  {
    "text": "interesting right so um application locality so if I put the edge server",
    "start": "2314680",
    "end": "2320880"
  },
  {
    "text": "right beside the um remote uh basically the edge uh API right beside supporting",
    "start": "2320880",
    "end": "2327240"
  },
  {
    "text": "services I can get some interesting uh benefits in terms of latency um if I'm doing data streaming uh I can consider",
    "start": "2327240",
    "end": "2334119"
  },
  {
    "text": "data locality that it would be nice if this part of the pipeline laid beside this of the P Pipeline and again doing",
    "start": "2334119",
    "end": "2339400"
  },
  {
    "text": "that in both a hard and soft constraint way for optimization uh but then if if that's not possible being able to still",
    "start": "2339400",
    "end": "2345800"
  },
  {
    "text": "run the workload another one that we've done recently um and this gets back to I had a slide that said it's even harder to",
    "start": "2345800",
    "end": "2352880"
  },
  {
    "start": "2347000",
    "end": "2347000"
  },
  {
    "text": "upgrade underlying hosts we treat everything in Netflix as immutable including our container infrastructure",
    "start": "2352880",
    "end": "2358880"
  },
  {
    "text": "which is quite challenging um so imagine uh I uh want to make a change to the",
    "start": "2358880",
    "end": "2364960"
  },
  {
    "text": "Titus infrastructure um what that would mean is I start up a new ASG to run my",
    "start": "2364960",
    "end": "2370280"
  },
  {
    "text": "containers in and it's pretty easy for batch because this new ASG any new batch",
    "start": "2370280",
    "end": "2376079"
  },
  {
    "text": "jobs will land in the new ASG as the batch completes and drains out of the old ASG they just disappear but what",
    "start": "2376079",
    "end": "2382280"
  },
  {
    "text": "happens for services um because they run forever so they're never going to move from the old version to the new version um we started",
    "start": "2382280",
    "end": "2389880"
  },
  {
    "text": "with a really sort of naive approach to that which was well we'll just randomly shoot them and of course they restart so",
    "start": "2389880",
    "end": "2396160"
  },
  {
    "text": "they move um if you do that very noncoordinated you can cause capacity",
    "start": "2396160",
    "end": "2401240"
  },
  {
    "text": "concerns um that maybe you shoot them all and they don't come back up um oops um so what we did was again leveraging",
    "start": "2401240",
    "end": "2408800"
  },
  {
    "text": "the infrastructure we had around us is for service we will actually as part of our job Management in the service job",
    "start": "2408800",
    "end": "2415160"
  },
  {
    "text": "manager we'll make calls out to Spiner and say hey um this task that's running",
    "start": "2415160",
    "end": "2421000"
  },
  {
    "text": "as a container is about to be rebooted what's your deployment strategy um and",
    "start": "2421000",
    "end": "2426240"
  },
  {
    "text": "that deployment strategy maybe spinco will reach out create a new instance wait for it to come up wait for it to be",
    "start": "2426240",
    "end": "2432000"
  },
  {
    "text": "ready for accepting traffic once it's all green we'll kill the task and it will let Spiner do the continuous",
    "start": "2432000",
    "end": "2439079"
  },
  {
    "text": "deployment aspects of doing that service task migration from old to new and then we can completely drain the old cluster",
    "start": "2439079",
    "end": "2445560"
  },
  {
    "text": "and shut it down when the tasks are all moved and healthy so service usage is a little",
    "start": "2445560",
    "end": "2451920"
  },
  {
    "start": "2450000",
    "end": "2450000"
  },
  {
    "text": "smaller than our batch at this point um we were doing batch predominantly in first quarter um in second quarter of",
    "start": "2451920",
    "end": "2458680"
  },
  {
    "text": "this year we started working with some early adopters um we looked looked at internal Services um in third quarter we",
    "start": "2458680",
    "end": "2467000"
  },
  {
    "text": "actually started there's various parts of the interface if you know the right parts um that are actually running a",
    "start": "2467000",
    "end": "2472880"
  },
  {
    "text": "dual path mode right now where you'll make a certain request that has to be item potent uh by the way but it's it's",
    "start": "2472880",
    "end": "2479160"
  },
  {
    "text": "a certain request from one of the devices doing one of the things in Netflix uh it will make a call down the",
    "start": "2479160",
    "end": "2485480"
  },
  {
    "text": "microservices network in the virtual machines it'll make a parallel call down the container Network um in all",
    "start": "2485480",
    "end": "2492040"
  },
  {
    "text": "containers um that lets us operate the container Network like it's live um it",
    "start": "2492040",
    "end": "2497319"
  },
  {
    "text": "allows us to compare and contrast the results that come back from the containers and the virtual machines and we just throw away the",
    "start": "2497319",
    "end": "2503839"
  },
  {
    "text": "result that comes back from the containers at this point in Shadow we basically call that shadowing um in",
    "start": "2503839",
    "end": "2509640"
  },
  {
    "text": "fourth fourth quarter which is getting short and uh short for me right now uh so I have to go home um we are moving",
    "start": "2509640",
    "end": "2516680"
  },
  {
    "text": "services to production um the biggest and this is probably one of our biggest scale UPS as well the production usage",
    "start": "2516680",
    "end": "2523400"
  },
  {
    "text": "of services that we're attacking this quarter is in our stream processing as a service uh offering inside of the",
    "start": "2523400",
    "end": "2529480"
  },
  {
    "text": "company uh so we're using Flink for that um we're running that completely in containers and our whole that aspect of",
    "start": "2529480",
    "end": "2536359"
  },
  {
    "text": "our data pipeline is going to be running in containers and production uh by hopefully by",
    "start": "2536359",
    "end": "2542160"
  },
  {
    "text": "Christmas um right now we have about 2,000 long running containers that were manag in so a little bit smaller scale",
    "start": "2542160",
    "end": "2549000"
  },
  {
    "text": "than our batch um still small but still growing every",
    "start": "2549000",
    "end": "2554280"
  },
  {
    "text": "day so everything we've talked about this far",
    "start": "2554280",
    "end": "2559440"
  },
  {
    "text": "um we we haven't talked about ECS so somebody would ask me what about ECS uh we started collaboration with ECS",
    "start": "2559440",
    "end": "2566599"
  },
  {
    "text": "because um as Neil talked about at the keynote this morning we realized why this is really cool technology we don't",
    "start": "2566599",
    "end": "2573200"
  },
  {
    "text": "want to Forever own and operate this aspect of our technology um there's definitely some operational",
    "start": "2573200",
    "end": "2579280"
  },
  {
    "text": "overhead in maintaining the underlying cluster State um that's that's built",
    "start": "2579280",
    "end": "2584319"
  },
  {
    "text": "into this uh we also have all of this ec2 integration that we've done um in",
    "start": "2584319",
    "end": "2590359"
  },
  {
    "text": "terms of eni based networking um in terms of IMR support all these kind of things that we know we could contribute",
    "start": "2590359",
    "end": "2597119"
  },
  {
    "text": "back into the open source ECS agent and would get us to the point where we could collaborate with all of you as opposed",
    "start": "2597119",
    "end": "2602520"
  },
  {
    "text": "to doing this just by ourselves so how does Titus work today",
    "start": "2602520",
    "end": "2608040"
  },
  {
    "start": "2605000",
    "end": "2605000"
  },
  {
    "text": "to give you an idea of what uh work we did as we evaluating ECS um there's a",
    "start": "2608040",
    "end": "2613559"
  },
  {
    "text": "much more complicated version of this online if you want to find it um under my slides share but basically at a high",
    "start": "2613559",
    "end": "2619280"
  },
  {
    "text": "level we make both outbound and inbound calls through msos uh for scheduling containers so outbound calls think about",
    "start": "2619280",
    "end": "2627000"
  },
  {
    "text": "a user comes in and says I want to launch these containers or I want to terminate the specific container or",
    "start": "2627000",
    "end": "2632040"
  },
  {
    "text": "reconciliation where we're trying to figure out the state of what we know versus what of the state of the the cluster know",
    "start": "2632040",
    "end": "2637920"
  },
  {
    "text": "um but we get inbound events um so if over on the right hand side one of those",
    "start": "2637920",
    "end": "2643000"
  },
  {
    "text": "containers dies outside of our control um the executor notices it tells the MOs",
    "start": "2643000",
    "end": "2649040"
  },
  {
    "text": "agent the MOs agent sends a m a message back through master that Master message gets back to our scheduler and we say",
    "start": "2649040",
    "end": "2655000"
  },
  {
    "text": "hey it died we're going to restart it so we get an inbound call for that um same thing with containers uh container",
    "start": "2655000",
    "end": "2660880"
  },
  {
    "text": "instances in the ec2 world or hosts um if a container instance comes up and now",
    "start": "2660880",
    "end": "2666440"
  },
  {
    "text": "is ready to participate in the cluster or an instance get shut down um we need to know about that that comes again up",
    "start": "2666440",
    "end": "2672599"
  },
  {
    "text": "as an inbound event to our Schuler and we can take action on it we did the",
    "start": "2672599",
    "end": "2678040"
  },
  {
    "start": "2676000",
    "end": "2676000"
  },
  {
    "text": "first uh Titus ECS implementation so you can see if you flip between these basically we changed the control plane",
    "start": "2678040",
    "end": "2684079"
  },
  {
    "text": "out for ECS as opposed to msos um I will say right up front as part of this",
    "start": "2684079",
    "end": "2689720"
  },
  {
    "text": "evaluation we haven't done the full uh ec2 integration we're basically just trying to show the control plane aspects",
    "start": "2689720",
    "end": "2696079"
  },
  {
    "text": "of running containers so the whole like IP per container and security groups and all that kind of stuff we're not talking",
    "start": "2696079",
    "end": "2701920"
  },
  {
    "text": "about that in this implementation but still at the control plane level what we did is we integrated uh ECS and it was",
    "start": "2701920",
    "end": "2709240"
  },
  {
    "text": "all outbound so we would launch and terminate containers um if we wanted to find out what changed outside of our",
    "start": "2709240",
    "end": "2716040"
  },
  {
    "text": "control we had to pull the ECS control plane so we'd say hey we think this",
    "start": "2716040",
    "end": "2721359"
  },
  {
    "text": "container is still running can you tell us the state of this container um at our level of scale that immediately hit us",
    "start": "2721359",
    "end": "2728040"
  },
  {
    "text": "with rate limiting so we have a real-time scheduler that's making decisions every couple 100 milliseconds",
    "start": "2728040",
    "end": "2735280"
  },
  {
    "text": "we are then calling at our level of scale for give me the state of all this existing infrastructure that's out there",
    "start": "2735280",
    "end": "2741160"
  },
  {
    "text": "it's not a really good thing to do against uh rate limited API so we started talking to the ECS team uh and",
    "start": "2741160",
    "end": "2748359"
  },
  {
    "start": "2745000",
    "end": "2745000"
  },
  {
    "text": "had done some really great collaboration with them I think you heard some of that in the the keynote this morning um",
    "start": "2748359",
    "end": "2754040"
  },
  {
    "text": "basically we said is hey we want a quote unquote real time not not real time in terms of like something that we run a",
    "start": "2754040",
    "end": "2760599"
  },
  {
    "text": "pacemaker or something like that real time in terms of um we want to know as",
    "start": "2760599",
    "end": "2766160"
  },
  {
    "text": "absolutely soon as possible when things outside of our schedulers control changes um we want it to be event based",
    "start": "2766160",
    "end": "2772880"
  },
  {
    "text": "we didn't want to be polling based um and we started this collaboration we we kicked off a face tof face meeting we've",
    "start": "2772880",
    "end": "2778440"
  },
  {
    "text": "done monthly Intero on this and we've just done engineering to engineering level uh discussions and where we landed up I",
    "start": "2778440",
    "end": "2785480"
  },
  {
    "start": "2784000",
    "end": "2784000"
  },
  {
    "text": "think this got announced a week ago but it was also in the keynote this morning was cloudwatch enablement for ECS so now",
    "start": "2785480",
    "end": "2793040"
  },
  {
    "text": "I get a back channel so now again I can do launches and I can do terminations and I can do reconciliation all against",
    "start": "2793040",
    "end": "2799839"
  },
  {
    "text": "ECS but I get inbound events so if one of the ECS agents connects I'll get an",
    "start": "2799839",
    "end": "2805559"
  },
  {
    "text": "event from cloudwatch that I can p pipe through sqs and now my scheduler is listening to sqs and it say hey I just",
    "start": "2805559",
    "end": "2812200"
  },
  {
    "text": "saw a container host pop up start scheduling on it if a container dies the event comes up through cloudwatch I see",
    "start": "2812200",
    "end": "2818720"
  },
  {
    "text": "it through sqs and now my job schedulers can take um uh action on that and",
    "start": "2818720",
    "end": "2824880"
  },
  {
    "text": "restart the task so this is this is this work that you saw with cl cloudwatch",
    "start": "2824880",
    "end": "2830240"
  },
  {
    "text": "events has been uh quite a bit of work between our teams uh from a Netflix and ECS perspective really happy to see this",
    "start": "2830240",
    "end": "2836720"
  },
  {
    "text": "uh showing up and I'll show you some of the analysis we did on top of that so starting with that",
    "start": "2836720",
    "end": "2842359"
  },
  {
    "start": "2840000",
    "end": "2840000"
  },
  {
    "text": "reconciliation um anyone that's writing a scheduler has to deal with reconciliation um so there's two systems",
    "start": "2842359",
    "end": "2848280"
  },
  {
    "text": "at play there's the control plane that knows of everything it knows is running and there's their scheduler that says",
    "start": "2848280",
    "end": "2854400"
  },
  {
    "text": "these are all the things that are running that we know about um we in Titus do a a flip-flopping every five",
    "start": "2854400",
    "end": "2861640"
  },
  {
    "text": "minute uh analysis that says hey ECS can you tell us everything you know is",
    "start": "2861640",
    "end": "2867079"
  },
  {
    "text": "running and then we compare it to what we're running and if we see things that are incs that we we don't think are",
    "start": "2867079",
    "end": "2872319"
  },
  {
    "text": "running we can tell them to die um and then the other 5 minute flip-flop will do hey we're running all this can you",
    "start": "2872319",
    "end": "2879680"
  },
  {
    "text": "tell us the status of that in ECS and if we find uh ECS doesn't know about something we think is running we can",
    "start": "2879680",
    "end": "2886040"
  },
  {
    "text": "restart it um this is pretty good from an API perspective uh so you're able to",
    "start": "2886040",
    "end": "2892200"
  },
  {
    "text": "essentially that turns into one call to get the Arns for all the tasks in the ECS system um and then you can batch",
    "start": "2892200",
    "end": "2899520"
  },
  {
    "text": "call to get status updates about tasks um at a batch size of 100 so essentially",
    "start": "2899520",
    "end": "2904839"
  },
  {
    "text": "whatever the scale of the number of containers you're running divide it by 100 add one that's how many apis calls",
    "start": "2904839",
    "end": "2910880"
  },
  {
    "text": "you have to make um and every 5 minutes that's not a problem at all where we still do have a problem",
    "start": "2910880",
    "end": "2917240"
  },
  {
    "start": "2915000",
    "end": "2915000"
  },
  {
    "text": "that we're working through is in the scheduling side so um I talked about earlier that we Peak at sometimes and",
    "start": "2917240",
    "end": "2923880"
  },
  {
    "text": "this is abnormal but it still it happens uh a thousand containers being restarted within a minute's time frame in fact",
    "start": "2923880",
    "end": "2930520"
  },
  {
    "text": "it's a thousand containers instantaneously because someone has pulled you know I'm redeploying this whole tier and it's anywhere between 600",
    "start": "2930520",
    "end": "2937839"
  },
  {
    "text": "and a th000 containers at once um we were able to working with the ECS team",
    "start": "2937839",
    "end": "2943359"
  },
  {
    "text": "um get that to the point where um with increased rate limits we were able to get through that um but the the problem",
    "start": "2943359",
    "end": "2950559"
  },
  {
    "text": "here is you end up with however many tasks you're starting you end up having to make 2x the number of calls so if I",
    "start": "2950559",
    "end": "2956880"
  },
  {
    "text": "start um actually I should have given you the numbers I start 1280 so I'm",
    "start": "2956880",
    "end": "2963119"
  },
  {
    "text": "doing R38 excels 32 cores times 40 nodes that's why 1280 um the graph here shows",
    "start": "2963119",
    "end": "2969200"
  },
  {
    "text": "with the rais rate limits I was able to start 1280 within a minute's time frame so it went from 0 to 1280 um back to",
    "start": "2969200",
    "end": "2975440"
  },
  {
    "text": "zero in terms of started task per minute um so but you still you end up making",
    "start": "2975440",
    "end": "2980520"
  },
  {
    "text": "for that it was 1280 time 2 API calls within a really short amount of time so continue area of scheduling",
    "start": "2980520",
    "end": "2988240"
  },
  {
    "text": "collaboration here um is the first one is to address that uh Second problem so we'd like to see that register task and",
    "start": "2988240",
    "end": "2995480"
  },
  {
    "text": "start Task became one an API call and it'd be nice to have them as batchable so I could start a 100 at a time um then",
    "start": "2995480",
    "end": "3002760"
  },
  {
    "text": "if I'm going to do uh you know a couple thousand divide by 100 that's how many",
    "start": "3002760",
    "end": "3007839"
  },
  {
    "start": "3005000",
    "end": "3005000"
  },
  {
    "text": "API calls You' need to make um so we're working through that the other thing is we currently are managing more resource",
    "start": "3007839",
    "end": "3014079"
  },
  {
    "text": "types um actually even more resource types than msos um offered to us but more than uh ECS as well um in terms of",
    "start": "3014079",
    "end": "3021559"
  },
  {
    "text": "we we manage the size of the disc available yet we manage the networking bandwidth and the eni and security",
    "start": "3021559",
    "end": "3027880"
  },
  {
    "text": "groups which is actually part of the reason why we're doing that Beyond msos is it's actually a two-level um resource",
    "start": "3027880",
    "end": "3033920"
  },
  {
    "text": "the number of ips remaining within an eni that's associated with the security group um we've been able to do that with",
    "start": "3033920",
    "end": "3040240"
  },
  {
    "text": "the agent today by passing them back as attributes um but then ECS actually",
    "start": "3040240",
    "end": "3045520"
  },
  {
    "text": "doesn't even know about these resources um it would be nice as we certainly as we restart schedulers and need a resync",
    "start": "3045520",
    "end": "3051839"
  },
  {
    "text": "state if this was known uh within ECS control plane a couple other smaller ones um that we're working through is we",
    "start": "3051839",
    "end": "3058839"
  },
  {
    "text": "have a few more states that we're used to um so with ECS um it has pending and started um with Titus today we have",
    "start": "3058839",
    "end": "3066280"
  },
  {
    "text": "starting versus started uh starting is when the agent actually says I saw your",
    "start": "3066280",
    "end": "3072200"
  },
  {
    "text": "request to start a container um started is after it did a Docker pull it did a run it created the the Pod route all",
    "start": "3072200",
    "end": "3078839"
  },
  {
    "text": "those different things that occur and we can we can take action earlier with a starting versus started state if we know",
    "start": "3078839",
    "end": "3085799"
  },
  {
    "text": "the message didn't actually make it to the host um some other ones I think that are important is working on um name",
    "start": "3085799",
    "end": "3092960"
  },
  {
    "text": "tasks so right now uh essentially a task has an Arn um I container has an Arn if",
    "start": "3092960",
    "end": "3099920"
  },
  {
    "text": "we especially consider Network partitions and we go ahead and we start something but we never get back to Arn",
    "start": "3099920",
    "end": "3107119"
  },
  {
    "text": "we're never going to know how to kill it because we associate a task number with that um independent of what Arn gets",
    "start": "3107119",
    "end": "3113200"
  },
  {
    "text": "selected from the ECS control plane and these are the things that were continuing to work on uh with the ECS",
    "start": "3113200",
    "end": "3119240"
  },
  {
    "start": "3118000",
    "end": "3118000"
  },
  {
    "text": "team uh so it's a work in progress uh We've um basically those things we",
    "start": "3119240",
    "end": "3125119"
  },
  {
    "text": "talked about on the previous slide get those out of the way um then we would look at taking the msos uh resource",
    "start": "3125119",
    "end": "3131280"
  },
  {
    "text": "management and replacing it with ECS with those that feature set and at that point we could contribute the uh the",
    "start": "3131280",
    "end": "3138640"
  },
  {
    "text": "open- source aspects now blocks that was announced this morning changes a lot of that um still working through that so so",
    "start": "3138640",
    "end": "3145440"
  },
  {
    "text": "keep tuned on that um but you know a lot of that is really exciting to us because",
    "start": "3145440",
    "end": "3150760"
  },
  {
    "text": "a lot of this work that we've done we'd like to be able to contribute to um Amazon as well as all of you and have us",
    "start": "3150760",
    "end": "3157440"
  },
  {
    "text": "all working on that together um in the future um you saw some of this announced today with ECS",
    "start": "3157440",
    "end": "3164960"
  },
  {
    "text": "with the um I forget what they called it the constraints and the the task placement advances in ECS uh fenzo is a",
    "start": "3164960",
    "end": "3172440"
  },
  {
    "text": "nice plugable Library we'd like to see that same pluggability in the task placement engine kind of move up the",
    "start": "3172440",
    "end": "3177640"
  },
  {
    "text": "stack another level and then finally moving up the stack to the highest level would be our job managers for service",
    "start": "3177640",
    "end": "3183359"
  },
  {
    "text": "and batch being able to reuse the ones that are in ECS with plug ability that lets us bring our Netflix business",
    "start": "3183359",
    "end": "3190559"
  },
  {
    "text": "Concepts into that job management so future Focus uh for our",
    "start": "3190559",
    "end": "3196359"
  },
  {
    "text": "teams um we want to uh right now we do autoscaling of the underlying cluster",
    "start": "3196359",
    "end": "3202960"
  },
  {
    "start": "3197000",
    "end": "3197000"
  },
  {
    "text": "but we don't yet autoscale the um the service jobs themselves we're ECS does",
    "start": "3202960",
    "end": "3209640"
  },
  {
    "text": "that through app scaling we're working with app scaling team to do that as well uh we also ask the people that are doing",
    "start": "3209640",
    "end": "3215920"
  },
  {
    "text": "services today to provision for PE a PE of failure um if you go out and look we",
    "start": "3215920",
    "end": "3221200"
  },
  {
    "text": "actually shift traffic around the world when there's issues um and we call that traffic integration we haven't yet done",
    "start": "3221200",
    "end": "3227000"
  },
  {
    "text": "that for the services that are running in the container Cloud um we also want to get to even more SLA management for",
    "start": "3227000",
    "end": "3233440"
  },
  {
    "text": "our service and batch uh so right now we have the these two different tiers it would be nice if we could take the flex",
    "start": "3233440",
    "end": "3239559"
  },
  {
    "text": "tier and run it inside of the critical tier and do preemption to kick stuff out um when we actually need that Reserve",
    "start": "3239559",
    "end": "3245640"
  },
  {
    "text": "capacity back uh we also have a system internally that's based on Virtual",
    "start": "3245640",
    "end": "3250760"
  },
  {
    "text": "machines that is basically a spot Market that people can use our trough capacity as you guys go to sleep to do all kinds",
    "start": "3250760",
    "end": "3257200"
  },
  {
    "text": "of things like media and coating uh we want to make the container platform a platform that offers that to anyone that",
    "start": "3257200",
    "end": "3262680"
  },
  {
    "text": "runs a batch job within the uh uh container ecosystem I talked about pods",
    "start": "3262680",
    "end": "3267760"
  },
  {
    "text": "for how we do our networking uh for folks that are aware of PODS we believe in them pretty strongly we want to take",
    "start": "3267760",
    "end": "3272960"
  },
  {
    "text": "that concept out to the developers it's kind of a hidden feature uh we want to get that back exposed for how people",
    "start": "3272960",
    "end": "3278520"
  },
  {
    "text": "break up their monolithic containers and of course um as you saw on the slides our scale just keeps going up uh so",
    "start": "3278520",
    "end": "3285960"
  },
  {
    "text": "that's going to keep us busy as well as we move into the future so with that",
    "start": "3285960",
    "end": "3291359"
  },
  {
    "text": "there's Titus again and we'll ask for questions",
    "start": "3291359",
    "end": "3298319"
  },
  {
    "text": "[Applause]",
    "start": "3298970",
    "end": "3303160"
  }
]