[
  {
    "text": "hey everyone I'm dul I am the principal solution architect uh J Specialist Team",
    "start": "240",
    "end": "7359"
  },
  {
    "text": "uh so in the previous session you learned about how to get started using Amazon stagemaker and deploy and run",
    "start": "7359",
    "end": "13280"
  },
  {
    "text": "inference for Lama 2 models in this session I'm going to walk you through the best practices on selecting the",
    "start": "13280",
    "end": "19000"
  },
  {
    "text": "right instance type the right inference option and the right container for hosting your llama 2 models on Sage",
    "start": "19000",
    "end": "26679"
  },
  {
    "text": "maker so let's dive in so Amazon Sage maker offers you the fully managed",
    "start": "26679",
    "end": "32320"
  },
  {
    "text": "service for endt end machine learning and as part of that Amazon Serv Amazon sagemaker offers you the broadest range",
    "start": "32320",
    "end": "39120"
  },
  {
    "text": "for inference uh options to serve any of your business use cases with sage maker",
    "start": "39120",
    "end": "44360"
  },
  {
    "text": "you can deploy your model endpoint um behind an inference endpoint using a",
    "start": "44360",
    "end": "50079"
  },
  {
    "text": "restful API and invoke the endpoint with a payload via restful API call and get",
    "start": "50079",
    "end": "55359"
  },
  {
    "text": "the inflence response uh in the streaming fashion too now you can use uh the real time inference endpoint for",
    "start": "55359",
    "end": "62600"
  },
  {
    "text": "ultra low latency applications or ultra low latency Productions which we are",
    "start": "62600",
    "end": "68520"
  },
  {
    "text": "going to dive into the details today in this session or you can use batch transforms for offline inferencing when",
    "start": "68520",
    "end": "75240"
  },
  {
    "text": "you have very large data sets for running the inference now if your inference requires large payload",
    "start": "75240",
    "end": "81400"
  },
  {
    "text": "processing like image file or video predictions um and if the inference is",
    "start": "81400",
    "end": "86560"
  },
  {
    "text": "likely to run long time uh then you can use Sage Maker's asynchronous inference",
    "start": "86560",
    "end": "93119"
  },
  {
    "text": "option now you either can deploy single model behind an inference endpoint or",
    "start": "93119",
    "end": "98280"
  },
  {
    "text": "you can deploy thousands of models behind an endpoint you can also jointly pre uh pre uh deploy the pre-processing",
    "start": "98280",
    "end": "106040"
  },
  {
    "text": "and postprocessing steps as a serial inference Pipeline on Sage maker at the infrastructure layer you have choice of",
    "start": "106040",
    "end": "113560"
  },
  {
    "text": "deploying uh the models on CPUs or gpus or AWS silicon like kws INF frenia 2 or",
    "start": "113560",
    "end": "120920"
  },
  {
    "text": "trainum or go entirely serverless so with sage maker you not only get the right model hosting option",
    "start": "120920",
    "end": "127880"
  },
  {
    "text": "for a business use case but also the best performance with the lowest cost so in this session I'm going to walk you",
    "start": "127880",
    "end": "134720"
  },
  {
    "text": "through how to select uh the inference option for the host for hosting the Lama",
    "start": "134720",
    "end": "140360"
  },
  {
    "text": "2 models for your ultra low latency applications which in this case is going to be the realtime inference uh endpoint",
    "start": "140360",
    "end": "147879"
  },
  {
    "text": "for sag maker and then we are going to I'm going to walk you through how to select the instance type and then the",
    "start": "147879",
    "end": "153760"
  },
  {
    "text": "right container for running the Lama 2 models on Sage maker so let's dive in so let's the first is the instance type",
    "start": "153760",
    "end": "160200"
  },
  {
    "text": "selection now selecting an instance type for a few uh is based on the few um few",
    "start": "160200",
    "end": "166840"
  },
  {
    "text": "factors uh the first and very important factor is the runtime uh memory uh and",
    "start": "166840",
    "end": "172720"
  },
  {
    "text": "that is dependent on the model size and the that's the sequence length which is basically the length of your uh",
    "start": "172720",
    "end": "178200"
  },
  {
    "text": "inference request and then the and then it is also dependent on the the amount of the the concurrent request that you",
    "start": "178200",
    "end": "185319"
  },
  {
    "text": "get or you want to process at any point of time uh while you're running the inference that means the the desired",
    "start": "185319",
    "end": "191720"
  },
  {
    "text": "concurrency or we say it as batch size which uh means batching multiple",
    "start": "191720",
    "end": "197959"
  },
  {
    "text": "inference requests and crossing at the same time to get the best price performance um and and the third point",
    "start": "197959",
    "end": "205040"
  },
  {
    "text": "of is the the response time of your applications um the response time is not",
    "start": "205040",
    "end": "210280"
  },
  {
    "text": "only just dependent on the size of your payload which in this case for the large",
    "start": "210280",
    "end": "215599"
  },
  {
    "text": "language model is input size and the output size which is input tokens and output tokens but also dependent on the",
    "start": "215599",
    "end": "222640"
  },
  {
    "text": "hardware um which basically drives the the response time",
    "start": "222640",
    "end": "227840"
  },
  {
    "text": "slas um so the response time uh slas for your business use case is a primary",
    "start": "227840",
    "end": "233760"
  },
  {
    "text": "driver in selecting the right instance type and the right Hardware now we",
    "start": "233760",
    "end": "239120"
  },
  {
    "text": "recommend using instances with a1g or a100 or 8100 which basically is G5 or P4",
    "start": "239120",
    "end": "249599"
  },
  {
    "text": "or P5 instance family uh and and the performance is much greater if compared",
    "start": "249599",
    "end": "255159"
  },
  {
    "text": "to the older TCS Nvidia gpus now you should select uh an instance type from",
    "start": "255159",
    "end": "263440"
  },
  {
    "text": "these recommended instance types that I just uh mentioned which has sufficient",
    "start": "263440",
    "end": "269000"
  },
  {
    "text": "aggregate memory across all of the gpus or across all of the uh ml accelerator",
    "start": "269000",
    "end": "275840"
  },
  {
    "text": "devices for both loading the model and making requests at the run time to",
    "start": "275840",
    "end": "282759"
  },
  {
    "text": "achieve the concurrency level that you're looking to um and so I'm going to walk you through quickly we have got uh",
    "start": "282759",
    "end": "290440"
  },
  {
    "text": "a good example on uh using the Lama 23 billion model and I'm going to walk you",
    "start": "290440",
    "end": "295720"
  },
  {
    "text": "through quickly how to calculate the runtime memory required to load the 13 billion model and then um actually show",
    "start": "295720",
    "end": "302680"
  },
  {
    "text": "you the step by step all these recommendations uh that we just walked through all right so you have fine-tuned",
    "start": "302680",
    "end": "308520"
  },
  {
    "text": "or either you want to deploy an open source Lama to 13 billion model Let's uh let's put it put it that in the",
    "start": "308520",
    "end": "314800"
  },
  {
    "text": "production for for the inference all right so the first thing that you do is",
    "start": "314800",
    "end": "320440"
  },
  {
    "text": "to calculate the amount of memory needed to load the 13 billion model um and and",
    "start": "320440",
    "end": "327440"
  },
  {
    "text": "which is dependent on primarily the model size and the model size is primar dependent on the number of parameters U",
    "start": "327440",
    "end": "335080"
  },
  {
    "text": "and the size for each parameter in this case we're talking about 13 billion uh",
    "start": "335080",
    "end": "341039"
  },
  {
    "text": "parameters and each parameter takes um two bytes in this case now this uh I've",
    "start": "341039",
    "end": "348520"
  },
  {
    "text": "shown fp16 here and that means that it it's a quantized model size to use two",
    "start": "348520",
    "end": "355919"
  },
  {
    "text": "bytes per model parameter now quantization is a technique to compress",
    "start": "355919",
    "end": "361600"
  },
  {
    "text": "the model size by converting High Precision floating Point representation",
    "start": "361600",
    "end": "366759"
  },
  {
    "text": "to a lower Precision like fp16 or bf16 or even int 4 or int 8 uh and there are",
    "start": "366759",
    "end": "375199"
  },
  {
    "text": "other optimization techniques will reduce the model size without much compromising the accuracy part of it now",
    "start": "375199",
    "end": "381880"
  },
  {
    "text": "if you use fp32 data type it will double the size so for now we'll just use fp16",
    "start": "381880",
    "end": "388840"
  },
  {
    "text": "data type uh version of the model that gives us around 26 gigabyte of the of",
    "start": "388840",
    "end": "395720"
  },
  {
    "text": "the of the size for loading one copy of the marel into the memory now in the",
    "start": "395720",
    "end": "402520"
  },
  {
    "text": "auto regressive uh Auto regressive decoding process all of the input tokens to the",
    "start": "402520",
    "end": "410080"
  },
  {
    "text": "llm produce their tension key value tensors and these tenses are kept in",
    "start": "410080",
    "end": "417039"
  },
  {
    "text": "accelerators uh memory to generate next tokens these cached key and value",
    "start": "417039",
    "end": "423800"
  },
  {
    "text": "tensors are often called uh are often called as KV cache that also occupies",
    "start": "423800",
    "end": "429879"
  },
  {
    "text": "about 4 to 13 gigabyt of additional memory uh which depends on the input and",
    "start": "429879",
    "end": "437000"
  },
  {
    "text": "output tokens and the the amount of concurrency that you wish to handle uh",
    "start": "437000",
    "end": "442520"
  },
  {
    "text": "for your application so the total memory required um to load a single instance",
    "start": "442520",
    "end": "449560"
  },
  {
    "text": "for Lama 23 billion for fp16 comes out to be approximately 46",
    "start": "449560",
    "end": "457879"
  },
  {
    "text": "gigabyte now how did we calculate the KV cache size let's dive",
    "start": "457879",
    "end": "463599"
  },
  {
    "text": "in so the KV cache here's the formula for calculating the KV cach in order to",
    "start": "463599",
    "end": "469159"
  },
  {
    "text": "calculate the total run time to run the insurance uh the KB cache is first",
    "start": "469159",
    "end": "475879"
  },
  {
    "text": "counted based on the token per token basis and and it's it's basically two",
    "start": "475879",
    "end": "481199"
  },
  {
    "text": "times the data type which here in this case we are using two bytes or an fp16",
    "start": "481199",
    "end": "487800"
  },
  {
    "text": "data type and the number of hidden layers for that model and the hidden",
    "start": "487800",
    "end": "493280"
  },
  {
    "text": "size um the number of layer represents the number of Transformer blocks in the in the model and the hidden size",
    "start": "493280",
    "end": "500840"
  },
  {
    "text": "represents the dimension of the attention block so let me show you how",
    "start": "500840",
    "end": "506599"
  },
  {
    "text": "to get the values for the the number of hidden uh layers and the hidden size",
    "start": "506599",
    "end": "512159"
  },
  {
    "text": "that is required to calculate the K Cav size as per this formula uh so I'm in",
    "start": "512159",
    "end": "517518"
  },
  {
    "text": "the hugging phas website and I've pulled in the metadata for Lama to13 billion",
    "start": "517519",
    "end": "523839"
  },
  {
    "text": "with fp16 quantized uh model and uh the basic file the the metadata file which",
    "start": "523839",
    "end": "530560"
  },
  {
    "text": "is called as config.js in this case uh will give you a lot of information about the model architecture including the",
    "start": "530560",
    "end": "536560"
  },
  {
    "text": "number of hidden layers and the uh and the hidden size which in this case I'm going to highlight hidden size in this",
    "start": "536560",
    "end": "542920"
  },
  {
    "text": "case is five and20 and the number of uh hidden layers",
    "start": "542920",
    "end": "548519"
  },
  {
    "text": "and the hidden heads are uh sorry attention heads and the hidden layers is",
    "start": "548519",
    "end": "554399"
  },
  {
    "text": "a 40 um so we are going to use this values and um in in calculating the KV",
    "start": "554399",
    "end": "563160"
  },
  {
    "text": "cach per token for Lama 233 billion as per the the formula that I showed um and",
    "start": "563160",
    "end": "569480"
  },
  {
    "text": "then we'll calculate how much aggregate memory you require in order to process",
    "start": "569480",
    "end": "575680"
  },
  {
    "text": "uh four concurrent requests um uh at any point of time per instance so let's",
    "start": "575680",
    "end": "582800"
  },
  {
    "text": "let's use these values that we show that we saw in the config.js for the the",
    "start": "582800",
    "end": "588079"
  },
  {
    "text": "number of hidden layers and the the hidden size which is 40 and 5 and2",
    "start": "588079",
    "end": "593399"
  },
  {
    "text": "respectively so you get the KV cache size per token to be uh to be this value",
    "start": "593399",
    "end": "599600"
  },
  {
    "text": "that I'm showing on your screen um and then you kind of go back to the inference request that you're going to",
    "start": "599600",
    "end": "604760"
  },
  {
    "text": "send and you're going to get an idea of how many input tokens and output tokens is probably going to generate which in",
    "start": "604760",
    "end": "610320"
  },
  {
    "text": "this case we are going to assume 4096 tokens per inference request which comes",
    "start": "610320",
    "end": "615839"
  },
  {
    "text": "down to be 3.36 gab required in terms of the KB cach you just have to reserve that much",
    "start": "615839",
    "end": "622399"
  },
  {
    "text": "uh in order to serve one request with 496 tokens in it now now if you want to",
    "start": "622399",
    "end": "630800"
  },
  {
    "text": "enhance the performance in terms of the trut and also the concurrency uh you might want to uh handle more than one",
    "start": "630800",
    "end": "639839"
  },
  {
    "text": "request concurrently in this case let's just assume we have since we have um",
    "start": "639839",
    "end": "645160"
  },
  {
    "text": "since we have uh more memory in the GPU uh probably we should start with the",
    "start": "645160",
    "end": "652519"
  },
  {
    "text": "with handling four concurrent request at the at at the same time which comes to be almost 13 gig",
    "start": "652519",
    "end": "659760"
  },
  {
    "text": "required just for KV cache and the autor regressive process in addition to the",
    "start": "659760",
    "end": "666760"
  },
  {
    "text": "model size itself all right so now now that you",
    "start": "666760",
    "end": "672639"
  },
  {
    "text": "know the memory required to load the model and have an estimate of the runtime memory required per token we can",
    "start": "672639",
    "end": "680920"
  },
  {
    "text": "figure out the instance type to use so we recommend you have an understanding",
    "start": "680920",
    "end": "687160"
  },
  {
    "text": "of the max sequence length you will be operating with which is prom tokens and",
    "start": "687160",
    "end": "692880"
  },
  {
    "text": "generated tokens alternatively you can select an instance type and calculate",
    "start": "692880",
    "end": "698360"
  },
  {
    "text": "and Max bat size estimate based on the available memory so for our llama 2 13",
    "start": "698360",
    "end": "704800"
  },
  {
    "text": "billion model we need a minimum of 26",
    "start": "704800",
    "end": "710800"
  },
  {
    "text": "gab um of the memory or sorry break for",
    "start": "710800",
    "end": "717720"
  },
  {
    "text": "for our llama to 13 billion model we need a minimum of 46 gigabytes of memory",
    "start": "717720",
    "end": "724839"
  },
  {
    "text": "um and so let's consider two instance types G5 12x large which has four Nvidia",
    "start": "724839",
    "end": "732200"
  },
  {
    "text": "G 810 gpus with an aggregated memory of 96 GB which can very well fit the 46 gab",
    "start": "732200",
    "end": "739320"
  },
  {
    "text": "overall memory requirement for our model or you can go higher which is g540 XL um",
    "start": "739320",
    "end": "746160"
  },
  {
    "text": "and load uh multiple copies of the model and can also increase the concurrency to",
    "start": "746160",
    "end": "752360"
  },
  {
    "text": "accommodate more number of concurrent request um so this was provided enough",
    "start": "752360",
    "end": "757480"
  },
  {
    "text": "memory for uh for running four inference requests at at currently four request uh",
    "start": "757480",
    "end": "763399"
  },
  {
    "text": "currently at uh currently with our current requirement um with and let's go",
    "start": "763399",
    "end": "768480"
  },
  {
    "text": "with G5 12 XEL to start with all right so now you selected so",
    "start": "768480",
    "end": "776279"
  },
  {
    "text": "you have the inference of inference running for Lama 23 billion and you need approximately 46 GB um in uh any V",
    "start": "776279",
    "end": "786120"
  },
  {
    "text": "select g52 XL which has four 81g Nvidia gpus with an aggregated memory of 96 uh",
    "start": "786120",
    "end": "792639"
  },
  {
    "text": "gab you can easily fit uh the 46 gig model into 96 uh gig of the host memory",
    "start": "792639",
    "end": "800480"
  },
  {
    "text": "or the GPU uh memory um but there's but let's let's take a look at how it looks",
    "start": "800480",
    "end": "806560"
  },
  {
    "text": "like so in a G5 12 Excel you have four Nvidia 810g devices with with each with",
    "start": "806560",
    "end": "814320"
  },
  {
    "text": "24 gig now you can't possibly fit a 46 gab um into any one of these devices",
    "start": "814320",
    "end": "822040"
  },
  {
    "text": "entirely so there's no way you'll be able to fit this model on a single device even if you select a larger",
    "start": "822040",
    "end": "828399"
  },
  {
    "text": "instance with a100 or even h100s possibly h100s you you may be able",
    "start": "828399",
    "end": "834000"
  },
  {
    "text": "to to to fit in uh but not necessarily with a100 or A10 GS um so what do we do",
    "start": "834000",
    "end": "841720"
  },
  {
    "text": "here cuz you're going to get C out of memory here so what do we do here you",
    "start": "841720",
    "end": "846800"
  },
  {
    "text": "will need to Shard the model uh into at least two shards so that you can fit the",
    "start": "846800",
    "end": "853639"
  },
  {
    "text": "model into two GPU devices however there are still uh two",
    "start": "853639",
    "end": "861079"
  },
  {
    "text": "GPU devices left idle which we can utilize now you can increase the shards",
    "start": "861079",
    "end": "867600"
  },
  {
    "text": "to now to four um and can fit more uh inference request",
    "start": "867600",
    "end": "873160"
  },
  {
    "text": "uh in terms of the concurrency um and so uh remember the sequence length and the",
    "start": "873160",
    "end": "878560"
  },
  {
    "text": "number of concurrent requests plays a very critical role in the KV cache size",
    "start": "878560",
    "end": "884759"
  },
  {
    "text": "and overall runtime memory requirements for loading the model and selecting the right instance type uh to achieve your",
    "start": "884759",
    "end": "892440"
  },
  {
    "text": "concurrency levels and the performance in terms of response time slas all right so we are going going to",
    "start": "892440",
    "end": "899360"
  },
  {
    "text": "use g512 Xcel and I'm going to walk you through uh real quick um on how do we",
    "start": "899360",
    "end": "905600"
  },
  {
    "text": "select uh the insurance option and the INF uh instance type um and the",
    "start": "905600",
    "end": "911160"
  },
  {
    "text": "container now before that um just quick note about what is the right container",
    "start": "911160",
    "end": "916800"
  },
  {
    "text": "to use to deploy your um your Lama 2 model or any other Foundation model on",
    "start": "916800",
    "end": "923600"
  },
  {
    "text": "stagemaker um we recommend you uh we recommend you have you use large model",
    "start": "923600",
    "end": "929279"
  },
  {
    "text": "inference container which is pre-built uh container for fully compatible with",
    "start": "929279",
    "end": "935319"
  },
  {
    "text": "uh Sage maker which is with high performance outof thebox features for you to use it to increase the trut and",
    "start": "935319",
    "end": "942399"
  },
  {
    "text": "reduce response times and overall get the best price performance and the large",
    "start": "942399",
    "end": "947759"
  },
  {
    "text": "model inference container um supports um different types of distributed inference",
    "start": "947759",
    "end": "952839"
  },
  {
    "text": "options including tensor parallel and pipeline parallel and various different optimizing techniques to uh increase the",
    "start": "952839",
    "end": "959920"
  },
  {
    "text": "performance and low the cost it also supports different uh open source Frameworks including hugging face um the",
    "start": "959920",
    "end": "967199"
  },
  {
    "text": "AWS um uh the AWS uh neuron Library which basically is required for running",
    "start": "967199",
    "end": "973279"
  },
  {
    "text": "the inflence on AWS silicon chips including infantry and training and open",
    "start": "973279",
    "end": "979240"
  },
  {
    "text": "source deep speed uh open source uh Frameworks um like PM or uh tensor R",
    "start": "979240",
    "end": "987279"
  },
  {
    "text": "tlm um we also have uh different optimization techniques to load to speed",
    "start": "987279",
    "end": "993240"
  },
  {
    "text": "up the loading of the models uh compilation of these models to improve the response times uh low code no code",
    "start": "993240",
    "end": "1001279"
  },
  {
    "text": "um way to configure all these different types of performance options including",
    "start": "1001279",
    "end": "1006880"
  },
  {
    "text": "the quantization are all built in and we offer them as outbox features uh with",
    "start": "1006880",
    "end": "1012160"
  },
  {
    "text": "almost zero cord setup and which we're going to see in this uh in the Corde that I'm going to show you so let's go",
    "start": "1012160",
    "end": "1018199"
  },
  {
    "text": "into the cord and see how we do it all right so um this example uh is basically to",
    "start": "1018199",
    "end": "1026720"
  },
  {
    "text": "serve Lama 213 billion on Sage maker using large marel inference container so",
    "start": "1026720",
    "end": "1033400"
  },
  {
    "text": "let's walk through so here we create the sagemaker session um and use the board of three",
    "start": "1033400",
    "end": "1041160"
  },
  {
    "text": "client python Board of three client in this case and create the sagemaker um",
    "start": "1041160",
    "end": "1046438"
  },
  {
    "text": "sagemaker client and sagemaker runtime client and then basically what we are uh taking",
    "start": "1046439",
    "end": "1052760"
  },
  {
    "text": "up the use case where uh you you're deploying this model for the open-ended",
    "start": "1052760",
    "end": "1058559"
  },
  {
    "text": "uh generation like chatbots um and the chatbots uh types of use case generally",
    "start": "1058559",
    "end": "1063760"
  },
  {
    "text": "have a small or small input uh size with a small uh with a small generated token",
    "start": "1063760",
    "end": "1071640"
  },
  {
    "text": "uh text um or it could also generate um large number of tokens as well so you",
    "start": "1071640",
    "end": "1078640"
  },
  {
    "text": "never never know that how much uh output it's going to generate um so we call it as open-ended um Generation Um so if",
    "start": "1078640",
    "end": "1086520"
  },
  {
    "text": "you're looking to deploy llama 2 model for chatbot type of use cases here is the best practice of of selecting the",
    "start": "1086520",
    "end": "1093679"
  },
  {
    "text": "right instance type and the right container for running the realtime ultra",
    "start": "1093679",
    "end": "1101480"
  },
  {
    "text": "low latency applications all right so I'm going to show you a demo of deploying the Llama to 13 billion model",
    "start": "1101480",
    "end": "1107640"
  },
  {
    "text": "on sagemaker with a Ultra for ultra low latency application using real time",
    "start": "1107640",
    "end": "1113360"
  },
  {
    "text": "inference endpoint on sagemaker um and then I'll go I'll show you how to",
    "start": "1113360",
    "end": "1119039"
  },
  {
    "text": "configure the the large model inflence container um for your use case and then",
    "start": "1119039",
    "end": "1125720"
  },
  {
    "text": "um how we are able to fit in the uh model um and basically the KV cach cache",
    "start": "1125720",
    "end": "1132520"
  },
  {
    "text": "and all of those calculations that I've showed you earlier how does that uh how",
    "start": "1132520",
    "end": "1137960"
  },
  {
    "text": "does that work on in the the large model INF container and how does it help reduce a lot of heavy lifting on your",
    "start": "1137960",
    "end": "1144640"
  },
  {
    "text": "end as well so I'm going to walk you through all those options so let's get started so um uh this uh this example",
    "start": "1144640",
    "end": "1152480"
  },
  {
    "text": "shows you the deployment of llama to 13 billion on sagemaker using large model",
    "start": "1152480",
    "end": "1157720"
  },
  {
    "text": "inflence container for uh realtime use cases we're going to spin up a realtime inflence endpoint with the model loaded",
    "start": "1157720",
    "end": "1166000"
  },
  {
    "text": "um so we first um create a sagemaker session with uh an bordo three client",
    "start": "1166000",
    "end": "1173120"
  },
  {
    "text": "for sagemaker and sagemaker runtime uh to invoke the inference uh endpoint and",
    "start": "1173120",
    "end": "1179280"
  },
  {
    "text": "basically the use case here is an open-ended generation a text generation use case like uh chatbots um and um",
    "start": "1179280",
    "end": "1187720"
  },
  {
    "text": "generally chatbots have to deal with a smaller user uh input size and uh and",
    "start": "1187720",
    "end": "1194600"
  },
  {
    "text": "possibly a variable sized output tokens um and so uh for the chatboard type of",
    "start": "1194600",
    "end": "1201799"
  },
  {
    "text": "open-ended generation we recommend uh using large model inference container",
    "start": "1201799",
    "end": "1207360"
  },
  {
    "text": "and specific backend which is called as VM now VM is a fast and easy um easy",
    "start": "1207360",
    "end": "1216159"
  },
  {
    "text": "open-source framework for serving large language models at scale with high",
    "start": "1216159",
    "end": "1221919"
  },
  {
    "text": "performance which is fully integrated and offered as an outof the Box backend",
    "start": "1221919",
    "end": "1227520"
  },
  {
    "text": "um as a feature is part of the large model influence container on Sage maker so you",
    "start": "1227520",
    "end": "1233840"
  },
  {
    "text": "can uh so the first thing in order to configure the um in order to configure",
    "start": "1233840",
    "end": "1241039"
  },
  {
    "text": "the model uh for deploying uh into the inference Endo is to retrieve the the",
    "start": "1241039",
    "end": "1249559"
  },
  {
    "text": "container image uh URI that we are going to use in in this case it's going to be",
    "start": "1249559",
    "end": "1255120"
  },
  {
    "text": "large model inference container you can see here the version is the the latest version that we just released which is",
    "start": "1255120",
    "end": "1261280"
  },
  {
    "text": "uh 0.27 and um you can configure a lot of",
    "start": "1261280",
    "end": "1267640"
  },
  {
    "text": "performance T enables including the back end which in this case we're going to use as VM you can configure all of these",
    "start": "1267640",
    "end": "1274919"
  },
  {
    "text": "parameters as part of the environment variable in this case the uh one of the",
    "start": "1274919",
    "end": "1282360"
  },
  {
    "text": "important environment variable is the model ID which is the model that you",
    "start": "1282360",
    "end": "1287480"
  },
  {
    "text": "want to deploy which is the model ID in the hugging phase and so we just simply copy paste the hugging phase uh model ID",
    "start": "1287480",
    "end": "1295559"
  },
  {
    "text": "from here to this uh you can see the same value here in earlier section we",
    "start": "1295559",
    "end": "1303400"
  },
  {
    "text": "saw that you will not be able to fit the entire model um into one of the gpus um",
    "start": "1303400",
    "end": "1310000"
  },
  {
    "text": "um available in the g51 12x large um and that you have to Shard your model at",
    "start": "1310000",
    "end": "1315679"
  },
  {
    "text": "least four shards or at least two shards and then you can extend it to four shards um in this case we are going to",
    "start": "1315679",
    "end": "1322039"
  },
  {
    "text": "Shard our model into four shards and that's that's what it represents here uh",
    "start": "1322039",
    "end": "1329200"
  },
  {
    "text": "uh by setting the value for tensor parallel degree to four all right now",
    "start": "1329200",
    "end": "1335279"
  },
  {
    "text": "here is another uh parameter that you should use which is called as continuous batching which will increase your",
    "start": "1335279",
    "end": "1341880"
  },
  {
    "text": "throughput um and we are going to dive into these optimization techniques in the next session as well um but just and",
    "start": "1341880",
    "end": "1348760"
  },
  {
    "text": "overview rolling batch is also called as continuous batch which is a great uh performance throughput optimization",
    "start": "1348760",
    "end": "1356159"
  },
  {
    "text": "technique uh to increase the concurrency and gets get best out of in terms of the",
    "start": "1356159",
    "end": "1361880"
  },
  {
    "text": "price performance so I would highly recommend you use uh rolling batch um using the VM open source uh for",
    "start": "1361880",
    "end": "1370200"
  },
  {
    "text": "open-ended uh text generation now there are couple of other",
    "start": "1370200",
    "end": "1375679"
  },
  {
    "text": "performance parameters that you can set which is uh the the bat size for the continuous batching or the rolling",
    "start": "1375679",
    "end": "1381880"
  },
  {
    "text": "batching which in this case we have set it to 32 because we have a little bit of more memory left uh in uh in in the in",
    "start": "1381880",
    "end": "1390640"
  },
  {
    "text": "the in the host in terms of the GPU memory and so we can leverage that by increasing the batch size to 32 um so",
    "start": "1390640",
    "end": "1397679"
  },
  {
    "text": "that we can um we can we can we can uh squeeze a maximum out of the computer",
    "start": "1397679",
    "end": "1403240"
  },
  {
    "text": "and the memory that we have in that host uh and you can get the best price performance and then the D type is the",
    "start": "1403240",
    "end": "1409159"
  },
  {
    "text": "quantization level which in this case is fp16 which basically takes two bytes per",
    "start": "1409159",
    "end": "1414200"
  },
  {
    "text": "parameter so all right so after you set all these parameters um the next step is",
    "start": "1414200",
    "end": "1422360"
  },
  {
    "text": "to create a model object which is stemer model object with the with the model",
    "start": "1422360",
    "end": "1427400"
  },
  {
    "text": "name and the container name with the image that we set which is LMI container with version",
    "start": "1427400",
    "end": "1433200"
  },
  {
    "text": "0.27 with the environment variable we just went through and then you create the model",
    "start": "1433200",
    "end": "1439520"
  },
  {
    "text": "and then the next step is to create the the inference in uh inference endpoint",
    "start": "1439520",
    "end": "1445159"
  },
  {
    "text": "config which basically allows you to specify the instance type in in this",
    "start": "1445159",
    "end": "1450840"
  },
  {
    "text": "case is g512 XL um um and um you can",
    "start": "1450840",
    "end": "1456279"
  },
  {
    "text": "also mention the initial instance count which is one you can also autoscale the inference um inference end point to",
    "start": "1456279",
    "end": "1463880"
  },
  {
    "text": "multiple uh instances you can configure the scaling policy on the basis of which",
    "start": "1463880",
    "end": "1469000"
  },
  {
    "text": "will scale out and you can also scale in um and so and so forth now very important part of the uh of the of this",
    "start": "1469000",
    "end": "1479320"
  },
  {
    "text": "configuration is the routing strategy the way how the insurance requests gets",
    "start": "1479320",
    "end": "1484720"
  },
  {
    "text": "routed to the individual instances behind the behind the autoscaling fleet",
    "start": "1484720",
    "end": "1491720"
  },
  {
    "text": "is extremely important for in terms of the performance uh wherein uh using the",
    "start": "1491720",
    "end": "1497799"
  },
  {
    "text": "least outstanding request policy in this case uh will allow um almost even",
    "start": "1497799",
    "end": "1505520"
  },
  {
    "text": "distribution of your inance request across the instance types um so I would",
    "start": "1505520",
    "end": "1511000"
  },
  {
    "text": "highly recommend you use least outstanding request policy for the for",
    "start": "1511000",
    "end": "1517320"
  },
  {
    "text": "routing strategy for running large language model insrance all right and",
    "start": "1517320",
    "end": "1522559"
  },
  {
    "text": "then let's go to the next step which is creating an end point",
    "start": "1522559",
    "end": "1528919"
  },
  {
    "text": "using the model object and the endpoint config object be created and so the um",
    "start": "1528919",
    "end": "1537120"
  },
  {
    "text": "here is the create end point step uh which basically you have to wait a few",
    "start": "1537120",
    "end": "1543320"
  },
  {
    "text": "minutes before the end point comes up and then once the end point comes up you",
    "start": "1543320",
    "end": "1548840"
  },
  {
    "text": "can inv walk the end point with uh the sample prom um and here is the format of",
    "start": "1548840",
    "end": "1558399"
  },
  {
    "text": "of the prompt which is a Json format um basically has inputs as the key and you",
    "start": "1558399",
    "end": "1566559"
  },
  {
    "text": "can um you can send the prompt as part of the input attribute and then along",
    "start": "1566559",
    "end": "1572360"
  },
  {
    "text": "with the along with the input you also can send additional hyperparameters that",
    "start": "1572360",
    "end": "1578679"
  },
  {
    "text": "you want the container to use in order to generate the output text in this case",
    "start": "1578679",
    "end": "1585760"
  },
  {
    "text": "it's in this case um we using there there's an additional parameter call as parameters where you can send",
    "start": "1585760",
    "end": "1592440"
  },
  {
    "text": "additional um hyper parameters like top P temperature max new tokens do sample",
    "start": "1592440",
    "end": "1600559"
  },
  {
    "text": "and you can also use different types of sampling techniques and you can um um",
    "start": "1600559",
    "end": "1606039"
  },
  {
    "text": "you can get different types of um outputs uh depending on your business use case um so we are going to use um we",
    "start": "1606039",
    "end": "1614760"
  },
  {
    "text": "are going to use a sample uh prompt in this case is a very small prompt uh with",
    "start": "1614760",
    "end": "1620559"
  },
  {
    "text": "the parameter of Max new tokens with um 100 and the sampling as false um and",
    "start": "1620559",
    "end": "1627480"
  },
  {
    "text": "then you just simply invoke the endpoint using invoke _ endpoint using B of three",
    "start": "1627480",
    "end": "1632880"
  },
  {
    "text": "and you get that all right so this was uh I just walk you through uh the",
    "start": "1632880",
    "end": "1639360"
  },
  {
    "text": "typical deployment uh steps uh for deploying the Llama 2 13 billion model",
    "start": "1639360",
    "end": "1645559"
  },
  {
    "text": "for realtime use case uh using real time inference uh endpoint inference endpoint",
    "start": "1645559",
    "end": "1651880"
  },
  {
    "text": "um and I will walk you through the the best practices on selecting the right instance type and the the right uh",
    "start": "1651880",
    "end": "1660120"
  },
  {
    "text": "inference container to use now we also have a low code no code option for you",
    "start": "1660120",
    "end": "1666760"
  },
  {
    "text": "to Leverage is part of the large model inflence container where you really do not need to worry about conf configuring",
    "start": "1666760",
    "end": "1674360"
  },
  {
    "text": "the rolling batch size or continuous batching size um configuring the the the",
    "start": "1674360",
    "end": "1681519"
  },
  {
    "text": "tensor parallel degree um or configuring the number of shards and so and so forth",
    "start": "1681519",
    "end": "1687440"
  },
  {
    "text": "what we have what we are offering from the version of from uh from",
    "start": "1687440",
    "end": "1694720"
  },
  {
    "text": "027.0 version of LMI container is a no code low code option where we will do",
    "start": "1694720",
    "end": "1701760"
  },
  {
    "text": "the selection of the best performance tunables um based on the instance type",
    "start": "1701760",
    "end": "1708360"
  },
  {
    "text": "that you specify and the hugging face model ID that you specify in the environment variables and we do the",
    "start": "1708360",
    "end": "1715159"
  },
  {
    "text": "heavy lifting of figuring out the the right parameters so let's walk you through uh how can you uh use that",
    "start": "1715159",
    "end": "1723000"
  },
  {
    "text": "feature um so the only thing we require in order to use this feature is to uh",
    "start": "1723000",
    "end": "1729600"
  },
  {
    "text": "give us the uh HF uncore modore ID which is the hugging face model identifier uh",
    "start": "1729600",
    "end": "1738120"
  },
  {
    "text": "part of the environment variable when you create a sag maker model object and",
    "start": "1738120",
    "end": "1743559"
  },
  {
    "text": "once you give that that's it you do not need to configure any kind of other",
    "start": "1743559",
    "end": "1749200"
  },
  {
    "text": "additional environment variables or you do not need to set any other parameters we will automatically figure figure out",
    "start": "1749200",
    "end": "1756519"
  },
  {
    "text": "how many shards we can fit in into the given instance type and uh and for that",
    "start": "1756519",
    "end": "1762559"
  },
  {
    "text": "specific model ID that you have specified and then we will do our best to give you the highly performance uh uh",
    "start": "1762559",
    "end": "1769840"
  },
  {
    "text": "performance uh tunable configuration so that's about it you just give us the hugging face model ID and and then you",
    "start": "1769840",
    "end": "1777799"
  },
  {
    "text": "then you create a model object and then um you give us the type of the instance",
    "start": "1777799",
    "end": "1784080"
  },
  {
    "text": "which in this case is G5 12x large and you spin up the uh the endpoint uh here",
    "start": "1784080",
    "end": "1790360"
  },
  {
    "text": "in this case I'm using sagemaker python SDK Just For Change um you can also use B of 3 as I was using in the earlier uh",
    "start": "1790360",
    "end": "1798519"
  },
  {
    "text": "section um and so here in this case predictor represents the uh the",
    "start": "1798519",
    "end": "1804399"
  },
  {
    "text": "predictor object for running the inference for the uh real time inference uh with all the configuration details um",
    "start": "1804399",
    "end": "1812399"
  },
  {
    "text": "including the uh the instance type and the the model uh and the container um",
    "start": "1812399",
    "end": "1819320"
  },
  {
    "text": "and and additional details so basically uh model. deploy is going to deploy your",
    "start": "1819320",
    "end": "1824399"
  },
  {
    "text": "model and then you run the prediction in by invoking the endpoint um and get the output that's about it",
    "start": "1824399",
    "end": "1831240"
  },
  {
    "text": "predictor do predict will invoke the endpoint and you get the output text right no um no no you don't need to",
    "start": "1831240",
    "end": "1840240"
  },
  {
    "text": "you don't need to configure anything else except hugging face model ID and we do the the heavy lifting um and then you",
    "start": "1840240",
    "end": "1847600"
  },
  {
    "text": "ultimately clean up the environment by deleting the endpoints and endpoint config and the model object don't forget that it's very important all right so",
    "start": "1847600",
    "end": "1854880"
  },
  {
    "text": "overall in summary we saw how to select the right inference option the right",
    "start": "1854880",
    "end": "1860039"
  },
  {
    "text": "instance type and the right container for running large language models",
    "start": "1860039",
    "end": "1866399"
  },
  {
    "text": "specifically in this case for Lama 2 models on Sage maker here are some of the resources uh for you here's the",
    "start": "1866399",
    "end": "1872320"
  },
  {
    "text": "sample notebook link and the large model INF container documentation where you can find plethora information including",
    "start": "1872320",
    "end": "1878799"
  },
  {
    "text": "additional performance tunables um and additional best practices on selecting the right instance type I hope this",
    "start": "1878799",
    "end": "1885440"
  },
  {
    "text": "session was useful I'll see you in my next video thank you very much",
    "start": "1885440",
    "end": "1891720"
  }
]