[
  {
    "text": "welcome to analytics learning series at aws this is dipankar kushari i am a senior",
    "start": "1360",
    "end": "7440"
  },
  {
    "text": "analytics solutions architect with aws and today i am going to talk about and show you",
    "start": "7440",
    "end": "13360"
  },
  {
    "text": "how you can use delta lake on amazon emr to easily maintain ever-changing data in your amazon s3",
    "start": "13360",
    "end": "20320"
  },
  {
    "text": "data lake i'll also show you how you can analyze delta lake data with amazon athena",
    "start": "20320",
    "end": "27439"
  },
  {
    "text": "and spark as you might have already experienced moving your data initially",
    "start": "27439",
    "end": "33600"
  },
  {
    "text": "to your s3 data lake using bulk load operations is quite easy but as your data keeps growing and",
    "start": "33600",
    "end": "40960"
  },
  {
    "text": "changing over time keeping that data fresh up to date and accurate in a scalable",
    "start": "40960",
    "end": "46000"
  },
  {
    "text": "manner is not you do not only have to take on the burden of managing and updating files in",
    "start": "46000",
    "end": "52320"
  },
  {
    "text": "your data lake yourself but also need to devise a mechanism to snapshot restore or roll back",
    "start": "52320",
    "end": "59359"
  },
  {
    "text": "specif specific changes as necessary when trying to troubleshoot and solve problems resulting from poor",
    "start": "59359",
    "end": "66960"
  },
  {
    "text": "data quality in upstream systems or logical data corruption",
    "start": "66960",
    "end": "72560"
  },
  {
    "text": "as data privacy regulations began to go into effect it require companies that store and",
    "start": "72560",
    "end": "78720"
  },
  {
    "text": "process user data to provide users with the ability to remove their users data",
    "start": "78720",
    "end": "84159"
  },
  {
    "text": "on demand this is more commonly known as the right to be forgotten",
    "start": "84159",
    "end": "89759"
  },
  {
    "text": "this represents a difficult challenge for data lake to read modify and then rewrite those files in",
    "start": "89759",
    "end": "96240"
  },
  {
    "text": "order to comply to user request which can be very i o intensive",
    "start": "96240",
    "end": "102000"
  },
  {
    "text": "process as large data sets often need to be rewritten just to remove",
    "start": "102000",
    "end": "107759"
  },
  {
    "text": "a few records to address these challenges delta lake was created that provides",
    "start": "107759",
    "end": "114479"
  },
  {
    "text": "transactional guarantees and enables schema enforcement and evolution",
    "start": "114479",
    "end": "119759"
  },
  {
    "text": "delta lake is an open source storage layer that brings reliability to data lake",
    "start": "119759",
    "end": "125439"
  },
  {
    "text": "it runs on top of your existing data lake and is fully compatible with apache spark apis delta lake offers acid",
    "start": "125439",
    "end": "134319"
  },
  {
    "text": "transactions on spark upsorts and deletes scalable metadata handling schema enforcement",
    "start": "134319",
    "end": "140400"
  },
  {
    "text": "time travel and unified streaming and batch data processing here is how",
    "start": "140400",
    "end": "147840"
  },
  {
    "text": "a high level diagram would look like as your raw data is ingested into your data",
    "start": "147840",
    "end": "153519"
  },
  {
    "text": "lake you will process those files and save the files into delta lake storage format",
    "start": "153519",
    "end": "159760"
  },
  {
    "text": "emr with pi spark is used to read and process this data a glue data catalog table is created on",
    "start": "159760",
    "end": "167280"
  },
  {
    "text": "the delta lake storage on s3 and then we can query it from athena we",
    "start": "167280",
    "end": "172480"
  },
  {
    "text": "can also query this data using redshift spectrum after we create an external schema for redshift spectrum",
    "start": "172480",
    "end": "179519"
  },
  {
    "text": "pointing towards the glue data catalog database aws lake formation will be used to",
    "start": "179519",
    "end": "185440"
  },
  {
    "text": "control accesses to the database and the tables created during the process",
    "start": "185440",
    "end": "191360"
  },
  {
    "text": "we will use a simple json data generator to perform this demo it simulates a trip information",
    "start": "191360",
    "end": "198720"
  },
  {
    "text": "where each record has a trip id that acts as the primary key route id can have 10 different values",
    "start": "198720",
    "end": "206239"
  },
  {
    "text": "and used as partition key destination timestamp and origin",
    "start": "206239",
    "end": "213840"
  },
  {
    "text": "we will use a emr notebook to demonstrate the following using delta lag",
    "start": "214319",
    "end": "220239"
  },
  {
    "text": "we will create a spark data frame from synthesized trip data save it in delta storage",
    "start": "220239",
    "end": "227120"
  },
  {
    "text": "format and create spark table create a table in athena query the data",
    "start": "227120",
    "end": "232720"
  },
  {
    "text": "in athena and spark sql delete update and insert data with schema evolution",
    "start": "232720",
    "end": "239360"
  },
  {
    "text": "query updated data and finally we will perform time table queries so without further",
    "start": "239360",
    "end": "246000"
  },
  {
    "text": "delay let's go to the aws console and show you how this can be done",
    "start": "246000",
    "end": "251439"
  },
  {
    "text": "i have already provisioned a emr cluster which is having spark",
    "start": "251439",
    "end": "260079"
  },
  {
    "text": "levy jupiter hub jupiter enterprise gateway and high",
    "start": "260079",
    "end": "265280"
  },
  {
    "text": "and there is also a emr notebook that has been created and is associated with this cluster",
    "start": "265280",
    "end": "272880"
  },
  {
    "text": "that you just saw",
    "start": "272880",
    "end": "275840"
  },
  {
    "text": "now this cluster with this notebook",
    "start": "278639",
    "end": "284960"
  },
  {
    "text": "is already up and running this this notebook i have already",
    "start": "284960",
    "end": "293040"
  },
  {
    "text": "started one thing to note here is the package this package is required",
    "start": "293040",
    "end": "299440"
  },
  {
    "text": "is part of the configuration for delta lake to work you'd also need",
    "start": "299440",
    "end": "305120"
  },
  {
    "text": "to set these variables and configuration",
    "start": "305120",
    "end": "310320"
  },
  {
    "text": "so here i have imported the delta tables",
    "start": "310320",
    "end": "316479"
  },
  {
    "text": "and also set up two path on s3 location so let me show you the s3 location this",
    "start": "316479",
    "end": "324160"
  },
  {
    "text": "is where currently you don't you see only input jar and output but you will have more",
    "start": "324160",
    "end": "330880"
  },
  {
    "text": "folders getting created this is where our delta lake or delta storage format files will be",
    "start": "330880",
    "end": "337600"
  },
  {
    "text": "stored so let me go back to the notebook first thing we will do",
    "start": "337600",
    "end": "344000"
  },
  {
    "text": "we will define the methods which will actually create or synthesize the data so let's",
    "start": "344000",
    "end": "352479"
  },
  {
    "text": "do that now we will create a spark data frame from the trip data",
    "start": "352479",
    "end": "358800"
  },
  {
    "text": "so here i have created a list python list which is having these values",
    "start": "358800",
    "end": "365440"
  },
  {
    "text": "these are destination values and then i will call this method here",
    "start": "365440",
    "end": "371440"
  },
  {
    "text": "the get json data and create json df and that will create two million records",
    "start": "371440",
    "end": "379039"
  },
  {
    "text": "so it will start from zero and it will create two million records",
    "start": "379039",
    "end": "386080"
  },
  {
    "text": "let's do that",
    "start": "386080",
    "end": "395840"
  },
  {
    "text": "so the records are getting generated and data frame will be populated",
    "start": "396560",
    "end": "409840"
  },
  {
    "text": "okay so it is done this is some sample record so this is the column that you have",
    "start": "412240",
    "end": "420960"
  },
  {
    "text": "for this data frame we can also look at this if we do the print schema so this",
    "start": "420960",
    "end": "426240"
  },
  {
    "text": "is how the schema looks like and as it shows it has df1.com so it has",
    "start": "426240",
    "end": "433840"
  },
  {
    "text": "2 million records",
    "start": "433840",
    "end": "437840"
  },
  {
    "text": "now we have the data frame df1 as you saw here df1 let's save it",
    "start": "442560",
    "end": "450000"
  },
  {
    "text": "in delta lake format in s3 so let me again show you the s3 location you will",
    "start": "450000",
    "end": "455280"
  },
  {
    "text": "see that the files will get created here so let me go back to the notebook again",
    "start": "455280",
    "end": "461120"
  },
  {
    "text": "i would like to highlight that we are going to save it in the delta hype path",
    "start": "461120",
    "end": "468080"
  },
  {
    "text": "which is basically inside the delta will have a demo video and then we'll have a",
    "start": "468080",
    "end": "474000"
  },
  {
    "text": "prefix delta hype table the delta spark path will be",
    "start": "474000",
    "end": "480240"
  },
  {
    "text": "inside demo video inside delta and then it will have a separate path delta spark table i just wanted to show",
    "start": "480240",
    "end": "486800"
  },
  {
    "text": "you or i will be showing you that you can create both hype table and spark table and you",
    "start": "486800",
    "end": "492400"
  },
  {
    "text": "can actually access the hype table from athena so let's",
    "start": "492400",
    "end": "498160"
  },
  {
    "text": "create the files in delta format and we will partition it by route id",
    "start": "498160",
    "end": "504400"
  },
  {
    "text": "route id as you can see this is the route id as i was saying earlier there will be 10 different",
    "start": "504400",
    "end": "510560"
  },
  {
    "text": "values so a through j and it will keep repeating that's how",
    "start": "510560",
    "end": "516240"
  },
  {
    "text": "the process works basically it is this process that creates 10",
    "start": "516240",
    "end": "521440"
  },
  {
    "text": "different route id as you can see here so let's go and",
    "start": "521440",
    "end": "529360"
  },
  {
    "text": "save the data frame in s3 in delta format again highlighting the",
    "start": "529360",
    "end": "537040"
  },
  {
    "text": "delta format so it's getting created let's go to s3",
    "start": "537040",
    "end": "542399"
  },
  {
    "text": "and let's refresh this as you can see it has created the prefix demo video inside",
    "start": "542399",
    "end": "547680"
  },
  {
    "text": "this delta hype table and it's going to create the folders or",
    "start": "547680",
    "end": "555279"
  },
  {
    "text": "the prefixes with the hive like partition so if i go back here i can see it",
    "start": "555279",
    "end": "562720"
  },
  {
    "text": "is saving in the delta high path okay so this is done so if i go there i",
    "start": "562720",
    "end": "569760"
  },
  {
    "text": "will see 10 different folders or prefixes a through j",
    "start": "569760",
    "end": "577040"
  },
  {
    "text": "is created and we have the files created it's actually in parquet format and",
    "start": "577040",
    "end": "582880"
  },
  {
    "text": "compressed in snappy so now let's actually we can start",
    "start": "582880",
    "end": "588320"
  },
  {
    "text": "reviewing this data using spark sql temporary view so we are reading the file that were",
    "start": "588320",
    "end": "595040"
  },
  {
    "text": "created in delta format from that path and then we are creating using the spark",
    "start": "595040",
    "end": "602160"
  },
  {
    "text": "create or replace temp view and calling the table as a temp trip table so we are going to run some",
    "start": "602160",
    "end": "609760"
  },
  {
    "text": "queries like we are going to do a count we are going to perform",
    "start": "609760",
    "end": "615120"
  },
  {
    "text": "some sample data and we will look at the maximum trip id again the trip id is",
    "start": "615120",
    "end": "622079"
  },
  {
    "text": "basically uh auto incremented number that goes from starts from 0 and goes up",
    "start": "622079",
    "end": "628720"
  },
  {
    "text": "to 1 less than two million because it is starting from zero so let's run this",
    "start": "628720",
    "end": "636240"
  },
  {
    "text": "okay so first query was count start so i can see that there are two million records and remember again",
    "start": "640320",
    "end": "646720"
  },
  {
    "text": "this is not being read from the data frame that we created earlier this is actually rate from the s3 location where",
    "start": "646720",
    "end": "654320"
  },
  {
    "text": "the data was written in delta format we are reading it and then in a temporary table in spark we are",
    "start": "654320",
    "end": "661440"
  },
  {
    "text": "our view in spark and we are reading that and writing it here this is",
    "start": "661440",
    "end": "667600"
  },
  {
    "text": "some sample data san francisco is obviously one of the destination values here",
    "start": "667600",
    "end": "673200"
  },
  {
    "text": "which got created and i am seeing it here",
    "start": "673200",
    "end": "679760"
  },
  {
    "text": "and then the maximum trip as we were saying earlier one less than the 2 million",
    "start": "679839",
    "end": "685440"
  },
  {
    "text": "but if i go in athena so let me show you in athena so if i go in athena i wouldn't see any",
    "start": "685440",
    "end": "692720"
  },
  {
    "text": "table yet for that to happen i have to create a manifest file and we'll soon see",
    "start": "692720",
    "end": "699519"
  },
  {
    "text": "that how a manifest file look like but before that i can actually create the table now see",
    "start": "699519",
    "end": "706320"
  },
  {
    "text": "the table is looking at a location which is called sim link format manifest and you will soon see why i am",
    "start": "706320",
    "end": "712720"
  },
  {
    "text": "doing that also one thing to highlight is the sardi",
    "start": "712720",
    "end": "718000"
  },
  {
    "text": "serializer d serializer which is a parquet hypes rd but the input format",
    "start": "718000",
    "end": "725120"
  },
  {
    "text": "is a symbolic text input format so this is a specific format that you have to give",
    "start": "725120",
    "end": "730480"
  },
  {
    "text": "when you want the files which are written in delta lake format to be read",
    "start": "730480",
    "end": "735839"
  },
  {
    "text": "from athena and the output format is again high output text format",
    "start": "735839",
    "end": "742880"
  },
  {
    "text": "so this has created this is created with the same val same columns and then it's",
    "start": "742880",
    "end": "750240"
  },
  {
    "text": "partitioned by route id how the partitioning scheme is followed in while",
    "start": "750240",
    "end": "756240"
  },
  {
    "text": "saving the files in s3 as i was showing you let me",
    "start": "756240",
    "end": "761360"
  },
  {
    "text": "go back to s3 just to make sure uh that you you remember that so",
    "start": "761360",
    "end": "768399"
  },
  {
    "text": "see high black partition based on route id so in athena also we are following",
    "start": "768399",
    "end": "773440"
  },
  {
    "text": "the same hive like partition with route id so let me create the table",
    "start": "773440",
    "end": "787839"
  },
  {
    "text": "yeah it just said that only one sql is allowed at a time since i have to i just selected i didn't select one i",
    "start": "790800",
    "end": "798480"
  },
  {
    "text": "just selected now and the query is successful so now the location is still not there",
    "start": "798480",
    "end": "806959"
  },
  {
    "text": "remember i have delta hive table i still don't have this so going back to the notebook you can",
    "start": "806959",
    "end": "813519"
  },
  {
    "text": "see the way you will do it is to you will read the you will create object called delta",
    "start": "813519",
    "end": "819600"
  },
  {
    "text": "table for from that path which is delta hype path and then you will generate this manifest",
    "start": "819600",
    "end": "827040"
  },
  {
    "text": "file so let's run this and see what happens",
    "start": "827040",
    "end": "831360"
  },
  {
    "text": "so it is now successfully run so if i go to s3 see let me refresh it",
    "start": "836880",
    "end": "843920"
  },
  {
    "text": "and see what happens so i see that another prefix symbolic format manifest is created and if i go to",
    "start": "843920",
    "end": "851040"
  },
  {
    "text": "athena this is exactly what i have selected here symlink format manifest so going back here see how",
    "start": "851040",
    "end": "859519"
  },
  {
    "text": "the high black partition is created inside that as well basically",
    "start": "859519",
    "end": "866240"
  },
  {
    "text": "each of this partition creates a manifest file and let me show you what",
    "start": "866240",
    "end": "871279"
  },
  {
    "text": "that content of that manifest file is so if i run this query you will see",
    "start": "871279",
    "end": "877760"
  },
  {
    "text": "this is nothing but a text file which actually has the list of files or parker",
    "start": "877760",
    "end": "884800"
  },
  {
    "text": "files in snappy compressed parker files which are created and stored in the actual uh",
    "start": "884800",
    "end": "892240"
  },
  {
    "text": "delta storage format under delta hype table so if i open another",
    "start": "892240",
    "end": "900079"
  },
  {
    "text": "tab here for s3 and if i go back and go inside",
    "start": "900079",
    "end": "908560"
  },
  {
    "text": "route id a so you will see that this has this file which is",
    "start": "908560",
    "end": "915920"
  },
  {
    "text": "seven zero two eight and four nine se so if i go here inside the manifest i",
    "start": "915920",
    "end": "921920"
  },
  {
    "text": "can remember i'm inside the manifest folder or prefix with the route id equal to a",
    "start": "921920",
    "end": "927920"
  },
  {
    "text": "inside that partition and see here it has a7028 and",
    "start": "927920",
    "end": "933120"
  },
  {
    "text": "a49sc so basically it is giving you all the list of files that athena should",
    "start": "933120",
    "end": "939040"
  },
  {
    "text": "read when it has to read that table so now",
    "start": "939040",
    "end": "944720"
  },
  {
    "text": "that this information is created in the manifest what i need to do next for the files to",
    "start": "944720",
    "end": "950639"
  },
  {
    "text": "be available or the data to be available i need to do a msck repair table so that the",
    "start": "950639",
    "end": "957120"
  },
  {
    "text": "partition information is updated so if i go in the glue console you see i am",
    "start": "957120",
    "end": "962480"
  },
  {
    "text": "inside the demo delta db and already delta trip table is showing up",
    "start": "962480",
    "end": "968480"
  },
  {
    "text": "and this is what the delta table looks like",
    "start": "968480",
    "end": "974399"
  },
  {
    "text": "and if i look at the partitions i just now created this partition so these",
    "start": "974399",
    "end": "979440"
  },
  {
    "text": "files are pointing to those manifest manifest files",
    "start": "979440",
    "end": "985120"
  },
  {
    "text": "so as you can see as i was showing you in this case i clicked on the partition h so that is what is showing",
    "start": "985120",
    "end": "992000"
  },
  {
    "text": "up so going back to the notebook emr notebook let's also",
    "start": "992000",
    "end": "997680"
  },
  {
    "text": "create a spark sql table or the spark table right so we already",
    "start": "997680",
    "end": "1004320"
  },
  {
    "text": "have the database now remember this was a temporary view or table in spark",
    "start": "1004320",
    "end": "1010959"
  },
  {
    "text": "but i am now talking about creating a table in the glue data catalog which is more like permanent in nature",
    "start": "1010959",
    "end": "1016959"
  },
  {
    "text": "it's still an external table so this demo delta db is the database in the glue data catalog",
    "start": "1016959",
    "end": "1023920"
  },
  {
    "text": "as you saw demo delta db so i'm still creating a spark table over there",
    "start": "1023920",
    "end": "1030000"
  },
  {
    "text": "and in this case what i'm doing i am actually using the save as table option and i am saving",
    "start": "1030000",
    "end": "1037120"
  },
  {
    "text": "it in the in the delta spark path so if you remember i mentioned this at the very",
    "start": "1037120",
    "end": "1043678"
  },
  {
    "text": "beginning that i am also creating a another variable called delta spark path",
    "start": "1043679",
    "end": "1049919"
  },
  {
    "text": "now it goes under the demo video but it goes to a different prefix so delta spark table is what we will see",
    "start": "1049919",
    "end": "1056880"
  },
  {
    "text": "getting created inside the s3 location so if i go here and under demo video today i only have",
    "start": "1056880",
    "end": "1063760"
  },
  {
    "text": "delta hype table so now once i run this you will see that i also have that location created i will",
    "start": "1063760",
    "end": "1070720"
  },
  {
    "text": "also have so in the glue catalog today under the demo delta db i only have one",
    "start": "1070720",
    "end": "1077520"
  },
  {
    "text": "table but once i create that you will see i also will have a delta spark table but i",
    "start": "1077520",
    "end": "1084320"
  },
  {
    "text": "cannot access it from athena because you just saw to access from ethereum you need to create that manifest file",
    "start": "1084320",
    "end": "1090640"
  },
  {
    "text": "but i should be able to access this table from the spark using the spark command",
    "start": "1090640",
    "end": "1098720"
  },
  {
    "text": "or the using the spark api right so let's save it again this is in delta",
    "start": "1098720",
    "end": "1103840"
  },
  {
    "text": "format it's again partitioned by route write route id it's going to be saved in the delta",
    "start": "1103840",
    "end": "1110160"
  },
  {
    "text": "spark path and it is going to be saved as a table which will be called delta spark table",
    "start": "1110160",
    "end": "1115840"
  },
  {
    "text": "so let's run this and then let's run that so once it is",
    "start": "1115840",
    "end": "1121520"
  },
  {
    "text": "created if i go back to the s3 location remember i only have delta hype table now you see i have",
    "start": "1121520",
    "end": "1128559"
  },
  {
    "text": "delta spark table and again it is partitioned by the route id and it's",
    "start": "1128559",
    "end": "1135039"
  },
  {
    "text": "written in delta format so it's done so now if i look at",
    "start": "1135039",
    "end": "1141679"
  },
  {
    "text": "the tables that i have in the demo delta db i see three of them right two of them",
    "start": "1141679",
    "end": "1147919"
  },
  {
    "text": "are permanent and one of them is temporary for this session in the spark one is the",
    "start": "1147919",
    "end": "1153120"
  },
  {
    "text": "delta spark table another is the delta trip table okay",
    "start": "1153120",
    "end": "1160720"
  },
  {
    "text": "now let's look at the ddl for the delta spark table i just now created in the",
    "start": "1160720",
    "end": "1166559"
  },
  {
    "text": "spark so this is how it looks like and let's query some data from the delta",
    "start": "1166559",
    "end": "1174080"
  },
  {
    "text": "spark table we are using spark magic here so the output will be written into the",
    "start": "1174080",
    "end": "1179120"
  },
  {
    "text": "delta trip op and first query we are running is the same queries",
    "start": "1179120",
    "end": "1184400"
  },
  {
    "text": "select count star and then we are doing a max trip and then we are getting 10 records and",
    "start": "1184400",
    "end": "1190559"
  },
  {
    "text": "finally we are getting the distinct route id so remember again this is from the delta",
    "start": "1190559",
    "end": "1196960"
  },
  {
    "text": "spark table right so let's do that",
    "start": "1196960",
    "end": "1207200"
  },
  {
    "text": "so again you can see there are 2 million records",
    "start": "1207200",
    "end": "1211679"
  },
  {
    "text": "and then the max is obviously one less than the 2 million these are some sample records",
    "start": "1212320",
    "end": "1219120"
  },
  {
    "text": "and again i have 10 different drought ids right so let's run this same query but this",
    "start": "1219120",
    "end": "1224559"
  },
  {
    "text": "time we will run it against the delta trip table let me refresh this so you will see two tables shows up but",
    "start": "1224559",
    "end": "1231600"
  },
  {
    "text": "we can only query delta trip table from here so let's run similar queries from here right so",
    "start": "1231600",
    "end": "1237360"
  },
  {
    "text": "let's do the select count star first remember we are running this on the",
    "start": "1237360",
    "end": "1244159"
  },
  {
    "text": "delta storage format inside s3 that is stored as delta",
    "start": "1244159",
    "end": "1251279"
  },
  {
    "text": "hive table inside that it is again",
    "start": "1251520",
    "end": "1256559"
  },
  {
    "text": "partitioned using the the hype type of partition and then i or",
    "start": "1257280",
    "end": "1263440"
  },
  {
    "text": "i am getting two million rows there right in the delta trip table let's do some similar queries that we",
    "start": "1263440",
    "end": "1270159"
  },
  {
    "text": "did for the delta spark table so here you will see that i will get",
    "start": "1270159",
    "end": "1277840"
  },
  {
    "text": "actually this should be trip id not route id obviously up between a and j",
    "start": "1277840",
    "end": "1285039"
  },
  {
    "text": "a is the highest so j is the highest so we are getting j over there but in this",
    "start": "1285039",
    "end": "1290480"
  },
  {
    "text": "history part is one less than two million then i'm getting the some sample",
    "start": "1290480",
    "end": "1298080"
  },
  {
    "text": "data in this case i'm getting new jersey if i run it again by the nature of how database works",
    "start": "1298080",
    "end": "1306320"
  },
  {
    "text": "now we are getting seattle last time we got new jersey and then let's look at some of the",
    "start": "1306320",
    "end": "1311679"
  },
  {
    "text": "distinct route id so here again i should be getting so it's exactly giving you the same result so just wanted to show",
    "start": "1311679",
    "end": "1318880"
  },
  {
    "text": "you that um that's why the purpose of creating two different tables in two different",
    "start": "1318880",
    "end": "1324559"
  },
  {
    "text": "location but this point forward we will only be using the athena table so the first thing we'll do remember our",
    "start": "1324559",
    "end": "1331280"
  },
  {
    "text": "whole purpose was to perform some insert update delete basically cdc operation delete is very important especially",
    "start": "1331280",
    "end": "1337600"
  },
  {
    "text": "someone request you to delete their data on certain ids you should be able to do it without any",
    "start": "1337600",
    "end": "1344240"
  },
  {
    "text": "problem so that's what delta lake is your savior here so first thing we'll do we",
    "start": "1344240",
    "end": "1350159"
  },
  {
    "text": "will insert some new records and we will use a schema evolution so let me explain you that",
    "start": "1350159",
    "end": "1356640"
  },
  {
    "text": "so far if you have seen the schema for the data frame or the table",
    "start": "1356640",
    "end": "1363039"
  },
  {
    "text": "that we are using it has route id trip id timestamp and destination now we are going to add a new id",
    "start": "1363039",
    "end": "1371360"
  },
  {
    "text": "a new column called origin so in the origin we will add some values here while we",
    "start": "1371360",
    "end": "1378400"
  },
  {
    "text": "create the data frame and what i am trying to show you is that as your time passes by your schema could evolve",
    "start": "1378400",
    "end": "1385840"
  },
  {
    "text": "and even when your schema is evolving if delta lake has the ability to handle",
    "start": "1385840",
    "end": "1392799"
  },
  {
    "text": "that and it can actually have not only append the data but it can actually append the",
    "start": "1392799",
    "end": "1398240"
  },
  {
    "text": "schema that means it can update the schema so basically what i am doing here starting",
    "start": "1398240",
    "end": "1405039"
  },
  {
    "text": "at 2 million again using the very similar concepts here creating the",
    "start": "1405039",
    "end": "1410640"
  },
  {
    "text": "json but in this case i am just using a new",
    "start": "1410640",
    "end": "1416080"
  },
  {
    "text": "column called origin so i will add origin as well which was not there in the previous case",
    "start": "1416080",
    "end": "1422400"
  },
  {
    "text": "um so along with the destination you will have already know and i am just adding 20 records so",
    "start": "1422400",
    "end": "1428880"
  },
  {
    "text": "starting at 2 million remember my maximum was 1 less than 2 million so i'm starting at 2 million and",
    "start": "1428880",
    "end": "1435600"
  },
  {
    "text": "i am actually appending or inserting 20 new records with",
    "start": "1435600",
    "end": "1440799"
  },
  {
    "text": "origin being a new column right so let's run this one first",
    "start": "1440799",
    "end": "1447840"
  },
  {
    "text": "to define the define the additional methods and then let's",
    "start": "1447840",
    "end": "1454000"
  },
  {
    "text": "create the data frame as you can see i am actually let me do a print",
    "start": "1454000",
    "end": "1461840"
  },
  {
    "text": "so it will show up here yeah so you can see 20 records and then",
    "start": "1462480",
    "end": "1469520"
  },
  {
    "text": "i have these records these are created this is the data frame right tip trip",
    "start": "1469520",
    "end": "1474799"
  },
  {
    "text": "updates is the name of the data frame so if i do a trip update dot print schema you see that now i have something",
    "start": "1474799",
    "end": "1481840"
  },
  {
    "text": "called origin which was not there earlier so again i am going to save it in the",
    "start": "1481840",
    "end": "1488240"
  },
  {
    "text": "delta hype path and i am going to save it in delta format",
    "start": "1488240",
    "end": "1493840"
  },
  {
    "text": "i am going to use the mode append earlier so far i was using overwrite so this is a new mode happen",
    "start": "1493840",
    "end": "1501520"
  },
  {
    "text": "and the thing that is of interest here is the merge schema so this smart schema option actually takes care of evolving",
    "start": "1501520",
    "end": "1508720"
  },
  {
    "text": "schema or revolution of schema and again it is partitioned by route id i am saving it in the delta",
    "start": "1508720",
    "end": "1514799"
  },
  {
    "text": "high path so let us do that",
    "start": "1514799",
    "end": "1519760"
  },
  {
    "text": "okay so this is done that means i have some new records that has been",
    "start": "1524799",
    "end": "1531360"
  },
  {
    "text": "written right so this data has gone into a so if i go to s3",
    "start": "1531360",
    "end": "1541120"
  },
  {
    "text": "and if i look at delta hype table in inside route a i see that new data",
    "start": "1541120",
    "end": "1548080"
  },
  {
    "text": "has arrived so this 1838 data",
    "start": "1548080",
    "end": "1553120"
  },
  {
    "text": "is what has been added newly so it's 1839 right now which is just few seconds back",
    "start": "1553120",
    "end": "1560720"
  },
  {
    "text": "it was written so these are the two records which has been written in two files",
    "start": "1560720",
    "end": "1566880"
  },
  {
    "text": "interestingly if i go to the manifest so let me go to the manifest right because",
    "start": "1566880",
    "end": "1573120"
  },
  {
    "text": "for athena to show you this new data it still needs to know where to",
    "start": "1573120",
    "end": "1580400"
  },
  {
    "text": "read it from and if i look at the manifest currently it only still shows only",
    "start": "1580400",
    "end": "1586640"
  },
  {
    "text": "two entries so manifest doesn't know that there is a update but here there",
    "start": "1586640",
    "end": "1593120"
  },
  {
    "text": "are actually i'm still looking at the partition route ida there are actually four",
    "start": "1593120",
    "end": "1598880"
  },
  {
    "text": "files per k files but here i'm still looking at route id a i have only",
    "start": "1598880",
    "end": "1606480"
  },
  {
    "text": "two files in the manifest so if i go to athena actually before",
    "start": "1606480",
    "end": "1613520"
  },
  {
    "text": "that let me query it here so this this has been created this",
    "start": "1613520",
    "end": "1622080"
  },
  {
    "text": "data is created and then let's query the updated data here",
    "start": "1622080",
    "end": "1627360"
  },
  {
    "text": "again the same concept we are using the temp table here and we are doing a count start so",
    "start": "1627360",
    "end": "1633840"
  },
  {
    "text": "ideally 2 million plus 20 that's what you should see in the count",
    "start": "1633840",
    "end": "1639840"
  },
  {
    "text": "that's exactly what is happening here and just to show you some sample data",
    "start": "1639840",
    "end": "1645679"
  },
  {
    "text": "beyond this if i run this i should see this new data",
    "start": "1645679",
    "end": "1654559"
  },
  {
    "text": "so this is up to this we had data and as you can see the origin was not there so it was it is",
    "start": "1654559",
    "end": "1660640"
  },
  {
    "text": "coming as null and then all the new data where the destination is side occurs",
    "start": "1660640",
    "end": "1666159"
  },
  {
    "text": "and this data has been created and you can see the difference in the timestamp right so it was created at",
    "start": "1666159",
    "end": "1673039"
  },
  {
    "text": "these are all utc times so 23 on 11th march 23 19 now this data as we were talking time",
    "start": "1673039",
    "end": "1680000"
  },
  {
    "text": "passed by and it is now 23 37 so that's when it got created",
    "start": "1680000",
    "end": "1686240"
  },
  {
    "text": "but uh if i go to athena so let's do a count star right first",
    "start": "1686240",
    "end": "1692399"
  },
  {
    "text": "thing so it still shows 2 million so that's 20",
    "start": "1692399",
    "end": "1698159"
  },
  {
    "text": "because the manifest file as i was showing you is not updated that's why it doesn't know about those files and it still thinks",
    "start": "1698159",
    "end": "1704880"
  },
  {
    "text": "that is only 20. sorry 2 million so no new records added right so let's update the",
    "start": "1704880",
    "end": "1711360"
  },
  {
    "text": "or generate the manifest file basically updating the benefits file",
    "start": "1711360",
    "end": "1716720"
  },
  {
    "text": "in the same location so once this is updated now if we go",
    "start": "1716720",
    "end": "1722880"
  },
  {
    "text": "here and if we go to manifest and if we query the same date",
    "start": "1722880",
    "end": "1730799"
  },
  {
    "text": "same file you see now it has been updated with four so basically it has now",
    "start": "1730799",
    "end": "1738000"
  },
  {
    "text": "updated all the files that is there under this partition same thing happened",
    "start": "1738000",
    "end": "1743520"
  },
  {
    "text": "for the other partitions as well so this job is done right",
    "start": "1743520",
    "end": "1749600"
  },
  {
    "text": "now if i go to athena and run the same query of count star",
    "start": "1749600",
    "end": "1755840"
  },
  {
    "text": "now it shows the updated one so it has the information now updated now let's do some update and see",
    "start": "1755840",
    "end": "1763360"
  },
  {
    "text": "how the data can be easily queried and once it is updated it can be easily",
    "start": "1763360",
    "end": "1768640"
  },
  {
    "text": "acquired using the delta first of all just by using one command you can update it",
    "start": "1768640",
    "end": "1774720"
  },
  {
    "text": "just like you do in rdbms and then you can immediately query that updated data",
    "start": "1774720",
    "end": "1780399"
  },
  {
    "text": "so again you are creating a delta table object for that path and before i update i just",
    "start": "1780399",
    "end": "1787919"
  },
  {
    "text": "want to show you again using that sql tem table or temp",
    "start": "1787919",
    "end": "1793200"
  },
  {
    "text": "view i just want to show you that what is the count for side accus",
    "start": "1793200",
    "end": "1799760"
  },
  {
    "text": "the number of records that we have where the destination is side occurs and",
    "start": "1799760",
    "end": "1805600"
  },
  {
    "text": "then the number of counts that we have where the distinction is philadelphia",
    "start": "1805600",
    "end": "1811120"
  },
  {
    "text": "so if i run this query you should be able to see the count",
    "start": "1811120",
    "end": "1817120"
  },
  {
    "text": "as you remember i just added 20 records for sidereca so that's exactly what it is showing and then there are",
    "start": "1817440",
    "end": "1824480"
  },
  {
    "text": "200k records for philadelphia so our objective for this update is",
    "start": "1824480",
    "end": "1830480"
  },
  {
    "text": "for all the destination or all the records where the destination is side us i will update that using",
    "start": "1830480",
    "end": "1838480"
  },
  {
    "text": "philadelphia so that means any destination that has side across it will be updated with philadelphia and i just",
    "start": "1838480",
    "end": "1844799"
  },
  {
    "text": "need to fire this command and at the same time since i want to get",
    "start": "1844799",
    "end": "1850720"
  },
  {
    "text": "that same information from athena this time i am actually taking care of updating the",
    "start": "1850720",
    "end": "1857600"
  },
  {
    "text": "manifest file for athena in the same at the same time so let's do that",
    "start": "1857600",
    "end": "1865840"
  },
  {
    "text": "so it's done now let's run the same queries exactly same queries",
    "start": "1874799",
    "end": "1880080"
  },
  {
    "text": "how many records are there for the destination inside occurs and where the destination is philadelphia now see",
    "start": "1880080",
    "end": "1885519"
  },
  {
    "text": "immediately change to zero but says 220 200",
    "start": "1885519",
    "end": "1890880"
  },
  {
    "text": "plus 20 right 200 k plus 20 so so immediately the update is now done",
    "start": "1890880",
    "end": "1896080"
  },
  {
    "text": "and if i run the same queries in athena i should be able to see the same results because",
    "start": "1896080",
    "end": "1903760"
  },
  {
    "text": "now i have already taken care of updating the manifest files so i just need to change",
    "start": "1903760",
    "end": "1910320"
  },
  {
    "text": "this to the athena table",
    "start": "1910320",
    "end": "1916000"
  },
  {
    "text": "let's run this so it should show zero right away and as",
    "start": "1918880",
    "end": "1925440"
  },
  {
    "text": "we are saying it is showing below going back here um i just need to change",
    "start": "1925440",
    "end": "1931200"
  },
  {
    "text": "this to philadelphia for athena query",
    "start": "1931200",
    "end": "1936799"
  },
  {
    "text": "and here let's run the query again and it should",
    "start": "1936799",
    "end": "1942399"
  },
  {
    "text": "say 200k plus 20. that's exactly what it does",
    "start": "1942399",
    "end": "1947679"
  },
  {
    "text": "so it's now consistent when i'm quiting from athena or from sql so let's now show you some",
    "start": "1947679",
    "end": "1955440"
  },
  {
    "text": "delete queries and this delete is very important as we have been telling because of the data privacy act and all these",
    "start": "1955440",
    "end": "1964240"
  },
  {
    "text": "things going around so right to be forgotten right a customer comes to you they say that i don't want you to keep",
    "start": "1964240",
    "end": "1970799"
  },
  {
    "text": "my data so make sure all my data so you get get data is deleted so you get those ids",
    "start": "1970799",
    "end": "1976559"
  },
  {
    "text": "or somehow you identify that customer using some mechanism and then you have",
    "start": "1976559",
    "end": "1981679"
  },
  {
    "text": "to go in your data lake and update sorry delete that so again in this case",
    "start": "1981679",
    "end": "1988720"
  },
  {
    "text": "let's say whatever data i have for new jersey as the destination i want to delete those so let's first look at the",
    "start": "1988720",
    "end": "1995200"
  },
  {
    "text": "total count and then we will look at the new jersey council new jersey county is 200k",
    "start": "1995200",
    "end": "2002000"
  },
  {
    "text": "and total count as we know is 2 million 20. again it's a simple one line that you",
    "start": "2002000",
    "end": "2009279"
  },
  {
    "text": "want to do just give you the give the predicate and then it will delete those records which",
    "start": "2009279",
    "end": "2014640"
  },
  {
    "text": "matches that predicate let's run that",
    "start": "2014640",
    "end": "2020640"
  },
  {
    "text": "and let's query the same data",
    "start": "2024720",
    "end": "2033120"
  },
  {
    "text": "if i run the same query now i see that there is no data for destination new jersey and also",
    "start": "2033120",
    "end": "2039279"
  },
  {
    "text": "there is 200k records which are less compared to this",
    "start": "2039279",
    "end": "2046159"
  },
  {
    "text": "now let's do some absurd so upset is when you already have some data and then you get",
    "start": "2046159",
    "end": "2051679"
  },
  {
    "text": "the data based on the same primary keys certain times you get an update and then you also get some",
    "start": "2051679",
    "end": "2058480"
  },
  {
    "text": "new data which you have to insert so the data that has come now is actually then updated",
    "start": "2058480",
    "end": "2065919"
  },
  {
    "text": "so certain columns might have got updated but the primary key is matching",
    "start": "2065919",
    "end": "2071040"
  },
  {
    "text": "so whenever you have a matching primary key you update that those records when the",
    "start": "2071040",
    "end": "2077839"
  },
  {
    "text": "primary key is not matching then you insert so that's basically how a lot of these cdc",
    "start": "2077839",
    "end": "2084800"
  },
  {
    "text": "operations work in etl world so this is what this is how we will do it it deep id is",
    "start": "2084800",
    "end": "2092158"
  },
  {
    "text": "our primary key so using trip id as the primary key will merge it",
    "start": "2092159",
    "end": "2097359"
  },
  {
    "text": "and then when you matched you update all of them when not matched you insert so that's basically it first",
    "start": "2097359",
    "end": "2105119"
  },
  {
    "text": "we are creating some records now to do that i have to create a data",
    "start": "2105119",
    "end": "2112160"
  },
  {
    "text": "set or a data frame where i have some matching trip id and some non-matching trip id so",
    "start": "2112160",
    "end": "2117760"
  },
  {
    "text": "first i had to look at the maximum trip id so maximum trip id is 2 million 19 and",
    "start": "2117760",
    "end": "2124960"
  },
  {
    "text": "then i went back little bit so i took all the trip is where it is greater than 2 million 14",
    "start": "2124960",
    "end": "2132640"
  },
  {
    "text": "so that means i have 5 overlapping and just to show you this is how the",
    "start": "2132640",
    "end": "2139040"
  },
  {
    "text": "data looks like for anything greater than two million up to two million nineteen so basically two",
    "start": "2139040",
    "end": "2145920"
  },
  {
    "text": "million fifteen through two million nineteen it's all destination is philly or philadelphia and these are the origin",
    "start": "2145920",
    "end": "2152960"
  },
  {
    "text": "right so i am creating a data set starting at two million 15 remember 2",
    "start": "2152960",
    "end": "2158320"
  },
  {
    "text": "million for beyond 2 million 14. so what will happen if i create 10 records then i will have these overlapping",
    "start": "2158320",
    "end": "2167119"
  },
  {
    "text": "5 records which will have the same trip id in this data frame trip updates one",
    "start": "2167119",
    "end": "2172640"
  },
  {
    "text": "and then i will have five more records which are not present today in the actual delta",
    "start": "2172640",
    "end": "2180400"
  },
  {
    "text": "table so i will have starting from 2020 through 2 million",
    "start": "2180400",
    "end": "2186079"
  },
  {
    "text": "24. so let's run this and as i said so i will have up to 2",
    "start": "2186079",
    "end": "2192720"
  },
  {
    "text": "million 24 2015 but the interesting thing is look at this data frame instead of",
    "start": "2192720",
    "end": "2198560"
  },
  {
    "text": "all free being the destination now i have this different",
    "start": "2198560",
    "end": "2203599"
  },
  {
    "text": "destination right and then i have origin also changed all to seattle just to show",
    "start": "2203599",
    "end": "2210560"
  },
  {
    "text": "you the difference but the tripod remains same because that's my primary key it has to be same",
    "start": "2210560",
    "end": "2217119"
  },
  {
    "text": "and obviously this is 23 37 on 3 11 but now this is 3 12",
    "start": "2217119",
    "end": "2225440"
  },
  {
    "text": "at midnight uh 33 minutes past midnight right",
    "start": "2225440",
    "end": "2230640"
  },
  {
    "text": "so again we will merge this as i was explaining so let's",
    "start": "2230640",
    "end": "2237599"
  },
  {
    "text": "look at this so first of all we create the delta table object and then we do a delta table",
    "start": "2237599",
    "end": "2242640"
  },
  {
    "text": "we alias this with t so that means this is my existing table so this",
    "start": "2242640",
    "end": "2247920"
  },
  {
    "text": "has data from 2 million 15 to 2 million 19",
    "start": "2247920",
    "end": "2253200"
  },
  {
    "text": "and i am joining on trip id with the trip updates one data frame",
    "start": "2253200",
    "end": "2259760"
  },
  {
    "text": "which is the new data frame that i created with which i am going to merge and when",
    "start": "2259760",
    "end": "2266400"
  },
  {
    "text": "it is matching it will update so that means everything that matches will be updated",
    "start": "2266400",
    "end": "2271599"
  },
  {
    "text": "with these data and when it is not matched then it will be inserted so i'll",
    "start": "2271599",
    "end": "2276960"
  },
  {
    "text": "have this data inserted so let's run this",
    "start": "2276960",
    "end": "2285838"
  },
  {
    "text": "okay so again i'm reading from that delta hype path but in this case i am",
    "start": "2293599",
    "end": "2299359"
  },
  {
    "text": "just going to print everything beyond 2 million 10 so let's look at this",
    "start": "2299359",
    "end": "2307359"
  },
  {
    "text": "so notice the change so up to 2 million 14 everything is still philly",
    "start": "2310800",
    "end": "2316240"
  },
  {
    "text": "the destination but starting from 2 million 15 where it actually matched",
    "start": "2316240",
    "end": "2321520"
  },
  {
    "text": "from 2 million 15 through 2 million 19 it has now changed completely the origin is crtel",
    "start": "2321520",
    "end": "2327760"
  },
  {
    "text": "so if you see here 2015 to 2019 this was",
    "start": "2327760",
    "end": "2333440"
  },
  {
    "text": "the data earlier right now if you see the destination has completely changed",
    "start": "2333440",
    "end": "2340160"
  },
  {
    "text": "and it got update also the timestamp which is very important just to show you that it is actually",
    "start": "2340160",
    "end": "2346320"
  },
  {
    "text": "getting updated i use the timestamp field so it is getting updated and",
    "start": "2346320",
    "end": "2352720"
  },
  {
    "text": "then we have these five new records that got inserted up to the 200 2 million 24",
    "start": "2352720",
    "end": "2360960"
  },
  {
    "text": "right so this is how you can do upsert also i wanted to highlight that you can",
    "start": "2360960",
    "end": "2367520"
  },
  {
    "text": "do some time travel queries so what does that mean is it keeps all the variations of this table as time",
    "start": "2367520",
    "end": "2374160"
  },
  {
    "text": "passes by you are making changes to the table it keeps these so the way you will read that is",
    "start": "2374160",
    "end": "2381520"
  },
  {
    "text": "by using this history and you then you can select certain metadata from the",
    "start": "2381520",
    "end": "2387760"
  },
  {
    "text": "table and let's look at it so it will show you all the versions that the table has gone through it starts",
    "start": "2387760",
    "end": "2394160"
  },
  {
    "text": "from the zeroth version and then it increments the version number it also shows you as",
    "start": "2394160",
    "end": "2399839"
  },
  {
    "text": "of timestamp what that version was relevant as of which timestamp and what operation was made",
    "start": "2399839",
    "end": "2406560"
  },
  {
    "text": "to create that version and if there is a predicate that is important or that is relevant it will",
    "start": "2406560",
    "end": "2412240"
  },
  {
    "text": "show you that so for example first we created the table at the very beginning and the",
    "start": "2412240",
    "end": "2418480"
  },
  {
    "text": "mode was override and we partitioned it by route id then if you remember we",
    "start": "2418480",
    "end": "2423520"
  },
  {
    "text": "appended 20 records and we still partitioned it by route id then we actually updated the table right",
    "start": "2423520",
    "end": "2432160"
  },
  {
    "text": "and finally we deleted the table with um with new jersey data getting",
    "start": "2432160",
    "end": "2440079"
  },
  {
    "text": "deleted when we updated we updated side accounts data with philadelphia",
    "start": "2440079",
    "end": "2445599"
  },
  {
    "text": "and then we did a merge so it it shows you all these operations",
    "start": "2445599",
    "end": "2451920"
  },
  {
    "text": "that has happened and i can actually access this different version",
    "start": "2451920",
    "end": "2457359"
  },
  {
    "text": "or checkpoint or image whatever way you want to call it of this table by using a timestamp as of",
    "start": "2457359",
    "end": "2466480"
  },
  {
    "text": "or even a version as of so if i do a timestamp as of now",
    "start": "2466480",
    "end": "2471839"
  },
  {
    "text": "when i previously ran it this is the timestamp which is before actually the table got created so",
    "start": "2471839",
    "end": "2478480"
  },
  {
    "text": "let me first show you what happens if i try to run it it's saying the provided timestamp",
    "start": "2478480",
    "end": "2484319"
  },
  {
    "text": "whatever i have provided right 1704 46 at 3 11 20 21 is before the earliest",
    "start": "2484319",
    "end": "2490960"
  },
  {
    "text": "version so that means it is even before the table got created right or the delta files got saved so let's",
    "start": "2490960",
    "end": "2498079"
  },
  {
    "text": "copy this timestamp and let paste it here so that it can",
    "start": "2498079",
    "end": "2505520"
  },
  {
    "text": "actually be queried properly so if i query it now",
    "start": "2505520",
    "end": "2510720"
  },
  {
    "text": "so now what i have done is i have created a data frame by reading that specific version of the",
    "start": "2510720",
    "end": "2517920"
  },
  {
    "text": "table and i called it df underscore v0 the purpose of this is to show you that",
    "start": "2517920",
    "end": "2527200"
  },
  {
    "text": "at that specific version you can only query or you can only see whatever that",
    "start": "2527200",
    "end": "2532640"
  },
  {
    "text": "version looked like right so for example it had only data up to 1 less than 2 million remember",
    "start": "2532640",
    "end": "2540800"
  },
  {
    "text": "first when we created we created 10 million or rather 2 million records so if i run",
    "start": "2540800",
    "end": "2546960"
  },
  {
    "text": "this now and try to see anything beyond 2 million there was nothing at in that version",
    "start": "2546960",
    "end": "2552480"
  },
  {
    "text": "p0 version there was nothing right so it shows that there is no data",
    "start": "2552480",
    "end": "2559280"
  },
  {
    "text": "now let's query the version 2. so version 2 is when i will",
    "start": "2559280",
    "end": "2565839"
  },
  {
    "text": "have 2 20 more records than 2 million but i will have updated the side across",
    "start": "2565839",
    "end": "2571359"
  },
  {
    "text": "data with philadelphia right so again the same concept i am reading but",
    "start": "2571359",
    "end": "2577359"
  },
  {
    "text": "in this case instead of using the timestamp i am using version other so if i do that and create the data",
    "start": "2577359",
    "end": "2584319"
  },
  {
    "text": "frame df underscore p2 so this is a specific version version 2 of the",
    "start": "2584319",
    "end": "2589359"
  },
  {
    "text": "table which has been loaded into the data frame and if i run the same query",
    "start": "2589359",
    "end": "2597359"
  },
  {
    "text": "beyond 2 million now i see because it was updated with sarah cartworth updated with",
    "start": "2598240",
    "end": "2604000"
  },
  {
    "text": "philadelphia i have all this destination as philadelphia but i do have records beyond two million right",
    "start": "2604000",
    "end": "2613520"
  },
  {
    "text": "and then again if i create two different data frames spark data frame one is for",
    "start": "2613520",
    "end": "2620079"
  },
  {
    "text": "version one and then the other one is for version three i can actually",
    "start": "2620079",
    "end": "2626640"
  },
  {
    "text": "see sample data there and we can we can see some interesting",
    "start": "2626640",
    "end": "2632400"
  },
  {
    "text": "facts here right so if i for version one if i do a cycles versus",
    "start": "2632400",
    "end": "2638319"
  },
  {
    "text": "philly count i can still see 20 and 200k right um because it was still not updated",
    "start": "2638319",
    "end": "2644800"
  },
  {
    "text": "cycles was not updated with philly so i can still see 20 and 2 million",
    "start": "2644800",
    "end": "2650240"
  },
  {
    "text": "or rather 200k um but if i now query the version 2 table for the",
    "start": "2650240",
    "end": "2656880"
  },
  {
    "text": "same exact predicate which is destination science and philadelphia",
    "start": "2656880",
    "end": "2663040"
  },
  {
    "text": "i would see that side cars got updated in version two all the records of syracuse got updated",
    "start": "2663040",
    "end": "2669760"
  },
  {
    "text": "with philly so these 20 got switched here",
    "start": "2669760",
    "end": "2675520"
  },
  {
    "text": "and then for version three um actually let me",
    "start": "2675520",
    "end": "2682480"
  },
  {
    "text": "call it version three",
    "start": "2682480",
    "end": "2686079"
  },
  {
    "text": "right now this is version 2 it's fine because i still want to see how the data looked like before the new",
    "start": "2688839",
    "end": "2696000"
  },
  {
    "text": "jersey data was deleted so this is fine um so yeah so you can see there were 2",
    "start": "2696000",
    "end": "2704160"
  },
  {
    "text": "million 20 records and there were 200k records for new jersey now in version 3",
    "start": "2704160",
    "end": "2709599"
  },
  {
    "text": "let me also go back and show you just to make sure you understand here",
    "start": "2709599",
    "end": "2714640"
  },
  {
    "text": "so version 2 is only the update and in version 3 only that particular version i deleted",
    "start": "2714640",
    "end": "2722079"
  },
  {
    "text": "the new jersey record and that particular version got created so you can see the operation was still it",
    "start": "2722079",
    "end": "2727520"
  },
  {
    "text": "so when i queried version 2 right then i still get new jersey record",
    "start": "2727520",
    "end": "2733599"
  },
  {
    "text": "200k records and 200 million 20 records total now if i query this",
    "start": "2733599",
    "end": "2738960"
  },
  {
    "text": "i will see that i have no records for new york c",
    "start": "2738960",
    "end": "2745599"
  },
  {
    "text": "and i have 1.8 million and 20",
    "start": "2745599",
    "end": "2751359"
  },
  {
    "text": "records overall so that's how i can utilize the or kind of take advantage of",
    "start": "2751359",
    "end": "2757200"
  },
  {
    "text": "the time travel get back those data by acquiring the data using time as",
    "start": "2757200",
    "end": "2762319"
  },
  {
    "text": "of or version as of and that's a very handy way to kind of roll back if i",
    "start": "2762319",
    "end": "2767839"
  },
  {
    "text": "need to or kind of get a view at certain point in time whenever",
    "start": "2767839",
    "end": "2773440"
  },
  {
    "text": "certain operations have happened also i would like to",
    "start": "2773440",
    "end": "2778720"
  },
  {
    "text": "point out that what you need to do to get this setup right so you saw how i have set up the emr and the emr",
    "start": "2778720",
    "end": "2787440"
  },
  {
    "text": "notebook just to make sure you can do it yourself let me show you what you need to do so",
    "start": "2787440",
    "end": "2794319"
  },
  {
    "text": "just uh cloning of these and then",
    "start": "2794319",
    "end": "2799839"
  },
  {
    "text": "i can show you so when you create the emr cluster make sure that you have",
    "start": "2799839",
    "end": "2806160"
  },
  {
    "text": "hadoop you have jupiter hub you use hype you use jupiter enterprise gateway you",
    "start": "2806160",
    "end": "2812800"
  },
  {
    "text": "have spark and you have living i have used the latest dmr cluster so",
    "start": "2812800",
    "end": "2820880"
  },
  {
    "text": "you can do the same and then you can use hive table metadata and use spark",
    "start": "2820880",
    "end": "2827920"
  },
  {
    "text": "table metadata for both of them you can use blue data catalog",
    "start": "2827920",
    "end": "2833839"
  },
  {
    "text": "okay and then make sure the other thing i must mention you can",
    "start": "2833839",
    "end": "2840640"
  },
  {
    "text": "use any cluster if you are just you going to use the same type of data 2 million is not a big",
    "start": "2840640",
    "end": "2848319"
  },
  {
    "text": "deal you can give any name you can put some location where you want to keep the",
    "start": "2848319",
    "end": "2854160"
  },
  {
    "text": "log files in s3 you if you are not using a custom army then you can just",
    "start": "2854160",
    "end": "2860079"
  },
  {
    "text": "make sure you have a easy to keep here because if you really want to",
    "start": "2860079",
    "end": "2865839"
  },
  {
    "text": "log into the or ssh into the master node then you can use easy to keep here to do that and you",
    "start": "2865839",
    "end": "2873359"
  },
  {
    "text": "can use the default uh security group for master i am now and code and tax notes i'm not going to",
    "start": "2873359",
    "end": "2880480"
  },
  {
    "text": "create this so make sure your clusters are",
    "start": "2880480",
    "end": "2886079"
  },
  {
    "text": "available and accessible before you try to log into them so",
    "start": "2886079",
    "end": "2892319"
  },
  {
    "text": "once you do that then you can create a notebook so you can go here this is you can give a name you can give",
    "start": "2892319",
    "end": "2898880"
  },
  {
    "text": "a description and then once your cluster is up or running you can actually",
    "start": "2898880",
    "end": "2904079"
  },
  {
    "text": "choose that cluster in while you are creating the notebook and you can use a default security group",
    "start": "2904079",
    "end": "2913359"
  },
  {
    "text": "and default role also is fine and then you can create the notebook",
    "start": "2913359",
    "end": "2918480"
  },
  {
    "text": "once your notebook is created let me cancel it here once your notebook is created if you click on that and then you can just",
    "start": "2918480",
    "end": "2924880"
  },
  {
    "text": "do a open in jupyter so that's what i did and then um in the delta lag let me go to the",
    "start": "2924880",
    "end": "2933760"
  },
  {
    "text": "top dental lake notebook i created a spark session then i",
    "start": "2933760",
    "end": "2940240"
  },
  {
    "text": "configured this right as i said this is very important so make sure you configure this jar",
    "start": "2940240",
    "end": "2947760"
  },
  {
    "text": "and then when you do this right it actually creates or downloads those",
    "start": "2947760",
    "end": "2954880"
  },
  {
    "text": "chart if it is not if you are running it for the first time then it downloads it into that so let me actually",
    "start": "2954880",
    "end": "2963359"
  },
  {
    "text": "show you by sshing into that location so",
    "start": "2963359",
    "end": "2970800"
  },
  {
    "text": "this is ip let me make sure i'm accessing the right",
    "start": "2970800",
    "end": "2976000"
  },
  {
    "text": "cluster here now it has connected to the emr",
    "start": "2976000",
    "end": "2983599"
  },
  {
    "text": "cluster so i have ssh into that and let me clear it",
    "start": "2983599",
    "end": "2991359"
  },
  {
    "text": "so if i go into not this one not this one",
    "start": "2991760",
    "end": "2999839"
  },
  {
    "text": "if i go here let me just quickly open up give me one second",
    "start": "2999839",
    "end": "3008400"
  },
  {
    "text": "yeah info.txt so i just want to show you where these charts are",
    "start": "3008960",
    "end": "3014480"
  },
  {
    "text": "located so if you are doing it for the first time",
    "start": "3014480",
    "end": "3019680"
  },
  {
    "text": "this is where the jar files will be downloaded when you run that configure so you see this is where the jar is",
    "start": "3019680",
    "end": "3027119"
  },
  {
    "text": "and you can actually use pie spark command also to get into that",
    "start": "3027119",
    "end": "3033359"
  },
  {
    "text": "so where the way you do that is like this so if you do this",
    "start": "3033359",
    "end": "3039200"
  },
  {
    "text": "you can get into the spice park",
    "start": "3039200",
    "end": "3043920"
  },
  {
    "text": "and you can actually access it using that so it is also looking for",
    "start": "3044319",
    "end": "3051119"
  },
  {
    "text": "those eight jars it downloaded it just because it is now getting downloaded in a different location for pi spark to",
    "start": "3051119",
    "end": "3058079"
  },
  {
    "text": "access it it actually needs a different location so this actually got downloaded in here",
    "start": "3058079",
    "end": "3064240"
  },
  {
    "text": "so if i go in the top you will see that it says home hadoop dot iv to cache",
    "start": "3064240",
    "end": "3072400"
  },
  {
    "text": "so this is and then sorry home hadoop.iv2charts so this is where it got",
    "start": "3072400",
    "end": "3077440"
  },
  {
    "text": "downloaded right so if i do a quick if i do ls minus ltr",
    "start": "3077440",
    "end": "3086720"
  },
  {
    "text": "so this is where your jars are getting downloaded you can also save it in a",
    "start": "3087040",
    "end": "3095359"
  },
  {
    "text": "pi spark file like a python file",
    "start": "3095359",
    "end": "3101519"
  },
  {
    "text": "yeah here so you can actually save it in a python file and you can even",
    "start": "3102800",
    "end": "3108880"
  },
  {
    "text": "run it as a python file so you can pass this as an argument",
    "start": "3108880",
    "end": "3115920"
  },
  {
    "text": "and then you can run it right so in this case you are going to use spark session",
    "start": "3115920",
    "end": "3122319"
  },
  {
    "text": "configure like this you will config this configure this like this and then",
    "start": "3122319",
    "end": "3129040"
  },
  {
    "text": "you can actually call this pi spark so you can use spark",
    "start": "3129040",
    "end": "3135040"
  },
  {
    "text": "submit and then you can pass the packages and then you can",
    "start": "3135040",
    "end": "3140480"
  },
  {
    "text": "uh give the file name which is the python file and then you can as you as you have seen probably here",
    "start": "3140480",
    "end": "3148240"
  },
  {
    "text": "let me again open it to make sure you understand um so here there are two arguments i am",
    "start": "3148240",
    "end": "3155119"
  },
  {
    "text": "passing right source path where the files are if you are reading from an existing file instead of a data",
    "start": "3155119",
    "end": "3162240"
  },
  {
    "text": "frame like we did by formulating some json data right",
    "start": "3162240",
    "end": "3167760"
  },
  {
    "text": "or synthesizing some json data if you are reading actual file let's say in csv format or",
    "start": "3167760",
    "end": "3173359"
  },
  {
    "text": "some other format and then saving it in the delta format so you can basically",
    "start": "3173359",
    "end": "3178960"
  },
  {
    "text": "say a source path like in this case here a source path as an argument and then",
    "start": "3178960",
    "end": "3184319"
  },
  {
    "text": "the delta path where you want to save it right and then pass the delta word so there are several ways you can do it i just",
    "start": "3184319",
    "end": "3191119"
  },
  {
    "text": "showed you using the notebook because that's one of the easier way to run it but you as i showed you",
    "start": "3191119",
    "end": "3197119"
  },
  {
    "text": "you can do it from the pi spark you can do it from",
    "start": "3197119",
    "end": "3202160"
  },
  {
    "text": "within a file and then calling it so if i go here",
    "start": "3202160",
    "end": "3209839"
  },
  {
    "text": "yeah let's do a control l so first thing i'll do let me",
    "start": "3220559",
    "end": "3228720"
  },
  {
    "text": "go back here and open the delta lake so let me see if it is going to",
    "start": "3229119",
    "end": "3236160"
  },
  {
    "text": "be able to import that so it's imported properly right so let's actually",
    "start": "3236160",
    "end": "3244400"
  },
  {
    "text": "set these delta high path and let's just show you",
    "start": "3245839",
    "end": "3252559"
  },
  {
    "text": "how you can run the same query over there right um so the table is already there so",
    "start": "3252559",
    "end": "3259200"
  },
  {
    "text": "if i just do this i should be able to run the same query",
    "start": "3259200",
    "end": "3264880"
  },
  {
    "text": "from here",
    "start": "3264880",
    "end": "3279838"
  },
  {
    "text": "yeah it's just reading that so let's do a count start so at the end we got like 1.8 million and 20 right so i",
    "start": "3281520",
    "end": "3289760"
  },
  {
    "text": "should see the same thing here actually i think we got 1.8 million 25",
    "start": "3289760",
    "end": "3299040"
  },
  {
    "text": "let's see yeah i think we lastly we added",
    "start": "3299040",
    "end": "3306000"
  },
  {
    "text": "another",
    "start": "3306000",
    "end": "3308400"
  },
  {
    "text": "10 records right beyond that so the latest version is version 4",
    "start": "3312079",
    "end": "3319599"
  },
  {
    "text": "so that's what we should be doing so let's do a version 4 and see here",
    "start": "3319599",
    "end": "3327359"
  },
  {
    "text": "because last we queried version 3 where so if you remember at the very end we",
    "start": "3328880",
    "end": "3335440"
  },
  {
    "text": "did a march and in the march we updated five records",
    "start": "3335440",
    "end": "3340880"
  },
  {
    "text": "and we added five records so that's why we we are seeing that one point eight million with 25",
    "start": "3340880",
    "end": "3349599"
  },
  {
    "text": "so let's do that so version 4 is the latest one",
    "start": "3349599",
    "end": "3357119"
  },
  {
    "text": "version as of 4 and then let's run it",
    "start": "3357119",
    "end": "3362640"
  },
  {
    "text": "okay and then let's do this uh we'll just need the constant overall",
    "start": "3362640",
    "end": "3369359"
  },
  {
    "text": "count so instead of that we'll do a version four and then here version 4",
    "start": "3369359",
    "end": "3377599"
  },
  {
    "text": "and yeah making it version 4 definitely",
    "start": "3388839",
    "end": "3394880"
  },
  {
    "text": "would help so it's matching right so that's the latest",
    "start": "3394880",
    "end": "3400799"
  },
  {
    "text": "so if i do the same thing here just to make sure that we are on",
    "start": "3400799",
    "end": "3408000"
  },
  {
    "text": "the same page again but remember version 4 was march so just to make sure we are doing the same thing",
    "start": "3408000",
    "end": "3414880"
  },
  {
    "text": "here let's just do that",
    "start": "3414880",
    "end": "3420400"
  },
  {
    "text": "so we are doing that and then we will do this so",
    "start": "3422799",
    "end": "3430400"
  },
  {
    "text": "again trip table and then temp tip treble because we were doing a version 4 here",
    "start": "3430400",
    "end": "3437040"
  },
  {
    "text": "on the data frame version 4 that's why we got that now we should be able to get",
    "start": "3437040",
    "end": "3442640"
  },
  {
    "text": "the same result here right same so version 4 is the latest we are",
    "start": "3442640",
    "end": "3448000"
  },
  {
    "text": "getting these many records here also we are getting this many records going back to the console here so",
    "start": "3448000",
    "end": "3455440"
  },
  {
    "text": "the shell shows the same thing so that's about it i think we will end",
    "start": "3455440",
    "end": "3461599"
  },
  {
    "text": "this session again my name is dipankakushari and um thanks a lot for joining me today",
    "start": "3461599",
    "end": "3475760"
  }
]