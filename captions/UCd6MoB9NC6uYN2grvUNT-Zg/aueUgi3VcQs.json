[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "hello hello and welcome to DAT 330 my",
    "start": "30",
    "end": "5100"
  },
  {
    "text": "name is Andrews I am solution architect with AWS based out of Luxembourg and today we're going to talk about how",
    "start": "5100",
    "end": "12570"
  },
  {
    "text": "sprinker uses Amazon eBay as to maximize its no SQL deployment I would like to ask you to welcome sprinker team Yamaha",
    "start": "12570",
    "end": "20550"
  },
  {
    "text": "man's car who is a head of infrastructure and DevOps in sprinker and capella krishna who is a platform",
    "start": "20550",
    "end": "26430"
  },
  {
    "text": "architect ins blinker today we're going to talk about how and why this is no",
    "start": "26430",
    "end": "32008"
  },
  {
    "text": "sequel database for your solutions and of course we're going to talk about some",
    "start": "32009",
    "end": "37010"
  },
  {
    "text": "introduction and internal stuff how MongoDB works and we're going to cover",
    "start": "37010",
    "end": "42239"
  },
  {
    "text": "some various aspects of designing your geo distributed clusters for global",
    "start": "42239",
    "end": "48780"
  },
  {
    "text": "deployments and for sure we're going to go through Splinter implementation and challenges including database they use",
    "start": "48780",
    "end": "54809"
  },
  {
    "text": "and the scale of these databases topology from MongoDB deployment some operational challenges they faced with",
    "start": "54809",
    "end": "60989"
  },
  {
    "text": "and of course how they are address these challenges I would like to start with",
    "start": "60989",
    "end": "66710"
  },
  {
    "start": "63000",
    "end": "63000"
  },
  {
    "text": "answering one question what is the reason for nossek arise because it's kind of important to understand why",
    "start": "66710",
    "end": "72780"
  },
  {
    "text": "they're using these types of databases first thing which is important is",
    "start": "72780",
    "end": "77820"
  },
  {
    "text": "complexity of data schemas because like during the life cycle of the project",
    "start": "77820",
    "end": "83159"
  },
  {
    "text": "data schemas in relational databases can become more and more complex and can bring a lot of issues from terms of",
    "start": "83159",
    "end": "91220"
  },
  {
    "text": "approachable operational terms right from different performance issues and so on the second thing is the data volume",
    "start": "91220",
    "end": "98009"
  },
  {
    "text": "and as like during the last years and during the even these minutes the data",
    "start": "98009",
    "end": "104970"
  },
  {
    "text": "volumes were processing growth they're growing significantly and normal way of",
    "start": "104970",
    "end": "110369"
  },
  {
    "text": "handling like a vertical scaling doesn't work anymore especially for new distributed deployments and that's",
    "start": "110369",
    "end": "115950"
  },
  {
    "text": "something where no SQL shines really and the next thing last but not",
    "start": "115950",
    "end": "121710"
  },
  {
    "text": "least its uptime and latency so for non single database is normally you can you",
    "start": "121710",
    "end": "128160"
  },
  {
    "text": "can have much higher uptime and you can satisfy like a kind of SLA especially which is important for",
    "start": "128160",
    "end": "135760"
  },
  {
    "text": "b2c projects and so on and with geo distributed features you can use to buta",
    "start": "135760",
    "end": "141520"
  },
  {
    "text": "databases across the world and you can bring your database as close to your customers as it's possible with",
    "start": "141520",
    "end": "147730"
  },
  {
    "text": "dependent on infrastructure use of course noticed queue like a system nowadays it's quite the big one there",
    "start": "147730",
    "end": "154090"
  },
  {
    "text": "are actually hundreds of different databases out there in in a wild right",
    "start": "154090",
    "end": "159760"
  },
  {
    "text": "some of them are really well-known like MongoDB Cassandra or a spike but there",
    "start": "159760",
    "end": "165730"
  },
  {
    "text": "are some some others which are not as well-known so it's kind of a problem of",
    "start": "165730",
    "end": "171700"
  },
  {
    "text": "how to pick up the proper database for your deployment of course we have own",
    "start": "171700",
    "end": "178030"
  },
  {
    "text": "AWS we have our own understanding of non sequel and we have different options for",
    "start": "178030",
    "end": "183550"
  },
  {
    "text": "deployment of non-sequel solutions like Amazon DynamoDB which is super fast and",
    "start": "183550",
    "end": "188760"
  },
  {
    "text": "key value storage it's really nice it plays well with most deployments",
    "start": "188760",
    "end": "194560"
  },
  {
    "text": "of course you can do Amazon data like on Amazon s3 and use Athena to query your",
    "start": "194560",
    "end": "200739"
  },
  {
    "text": "data so it can be quite an approach like a database and data a kind of approach",
    "start": "200739",
    "end": "206050"
  },
  {
    "text": "and you can have your clusters based on ec2 and EBS so it's kind of DIY",
    "start": "206050",
    "end": "213400"
  },
  {
    "text": "solutions before you start picking up the right database for your solutions",
    "start": "213400",
    "end": "218770"
  },
  {
    "text": "you have to answer one simple questions do you really need distributed no sequel clusters for your deployments for most",
    "start": "218770",
    "end": "226510"
  },
  {
    "text": "cases still in our days relational databases are like a Swiss Army knife for most of the use cases but if you",
    "start": "226510",
    "end": "233709"
  },
  {
    "text": "have like really and clear load if you have like bursty workloads if you need",
    "start": "233709",
    "end": "238930"
  },
  {
    "text": "really high uptime if your users are globally distributed or you have a huge data volume then probably you will need",
    "start": "238930",
    "end": "246640"
  },
  {
    "start": "246000",
    "end": "246000"
  },
  {
    "text": "no sequel database there are three kind of approaches to picking up the proper",
    "start": "246640",
    "end": "251769"
  },
  {
    "text": "non sequel database for your deployment one is kind of know almost no real choice right if you have legislation",
    "start": "251769",
    "end": "258030"
  },
  {
    "text": "constraints if you have your homegrown expertise and you don't want to spend",
    "start": "258030",
    "end": "263500"
  },
  {
    "text": "some time of accommodating new technologies or for example if have some hybrid or legacy solutions you",
    "start": "263500",
    "end": "270009"
  },
  {
    "text": "don't need to really to make a choice right the next thing is a wheelchair experiment it's kind of a really good",
    "start": "270009",
    "end": "275680"
  },
  {
    "text": "thing we're all experimenting all the time so there are plenty of databases you saw that and you can pick one which",
    "start": "275680",
    "end": "282009"
  },
  {
    "text": "better suits your your use case and the things you are trying to achieve but you",
    "start": "282009",
    "end": "289389"
  },
  {
    "text": "have to admit and you have to mitigate the specific challenges for not very",
    "start": "289389",
    "end": "294639"
  },
  {
    "text": "mature technologies and the last but not least it's like one of the most",
    "start": "294639",
    "end": "299800"
  },
  {
    "text": "important things it's an informal choice so for example if you really need something which is highly available",
    "start": "299800",
    "end": "307419"
  },
  {
    "text": "which has a high throughput but you don't want to have a lot of issues with operations and so on",
    "start": "307419",
    "end": "312940"
  },
  {
    "text": "you can go for dynamically be same for reliability from the hardware perspective from networking and so on if",
    "start": "312940",
    "end": "319599"
  },
  {
    "text": "you need some pure performance like let's say data streaming platform or caching or in-memory key value data",
    "start": "319599",
    "end": "327550"
  },
  {
    "text": "stores you can use radius memcache Kafka or spike there are plenty of them on the",
    "start": "327550",
    "end": "332620"
  },
  {
    "text": "market if you if we're gonna talk about like really big clusters geo distributed",
    "start": "332620",
    "end": "338770"
  },
  {
    "text": "clusters handling a high throughput and a lot of load on that we're going to talk about MongoDB Cassandra",
    "start": "338770",
    "end": "345490"
  },
  {
    "text": "react coach base and some other databases like that and of course you can pick up the database based on your",
    "start": "345490",
    "end": "352020"
  },
  {
    "text": "requirements for data schema like let's say for graph databases you can go for",
    "start": "352020",
    "end": "357069"
  },
  {
    "text": "neo4j for j√°nos or something something else or like solar and elasticsearch for",
    "start": "357069",
    "end": "363639"
  },
  {
    "text": "arbitrary queries and so on so today we're going to talk mostly about MongoDB",
    "start": "363639",
    "end": "370419"
  },
  {
    "start": "366000",
    "end": "366000"
  },
  {
    "text": "so it's kind of the crash course of how MongoDB works on the right hand side you can see how MongoDB is organized on the",
    "start": "370419",
    "end": "378310"
  },
  {
    "text": "cluster level on the left hand side how it's organized on the node level so in the cluster level MongoDB uses Charlotte",
    "start": "378310",
    "end": "385810"
  },
  {
    "text": "clusters which consists of four shards and short effectively is a replica set and within the replica set you can have",
    "start": "385810",
    "end": "391930"
  },
  {
    "text": "master nodes and one or more secondary notes or even no secondary notes right",
    "start": "391930",
    "end": "397680"
  },
  {
    "text": "in addition you can have special types of notes are bitter notes note for",
    "start": "397680",
    "end": "403470"
  },
  {
    "text": "election procedure and shadow note like to make backups and so on in",
    "start": "403470",
    "end": "409789"
  },
  {
    "text": "configuration sorry in the cluster itself in Sharlee cluster you can have and you must have a configuration",
    "start": "409789",
    "end": "416400"
  },
  {
    "text": "replica set which holds the information about where the data resides how it's",
    "start": "416400",
    "end": "422759"
  },
  {
    "text": "organized between the shards between the notes and so on on the note level we can",
    "start": "422759",
    "end": "427860"
  },
  {
    "text": "see it's a read and write process for data dotted line is the write process",
    "start": "427860",
    "end": "432900"
  },
  {
    "text": "and the solid line is a read process so a MongoDB reads and writes through memory and it writes firstly to the",
    "start": "432900",
    "end": "439979"
  },
  {
    "text": "journal and after journal becomes a certain size or it's sometime after a",
    "start": "439979",
    "end": "447990"
  },
  {
    "text": "certain amount of time like let's say 100 milliseconds it's flashed on disk and then becomes B song it's a binary",
    "start": "447990",
    "end": "454229"
  },
  {
    "text": "format which is actually stored on disk in these form documents are stored in X",
    "start": "454229",
    "end": "460050"
  },
  {
    "text": "dance actually it is this is specific data structure which holds some metadata and B so on itself and of course it has",
    "start": "460050",
    "end": "467340"
  },
  {
    "text": "some indexes built on that to show where actually the data is on disk as you can",
    "start": "467340",
    "end": "474599"
  },
  {
    "text": "imagine this on is a binary document right so which means that when the data is deleted or like something goes wrong",
    "start": "474599",
    "end": "482039"
  },
  {
    "text": "like something is updated or you insert it in you nested documents the fragmentation can occur and the thing is",
    "start": "482039",
    "end": "488849"
  },
  {
    "text": "like as MongoDB Rize rised through memory the fragmentation can occur both in memory and on disk so Jamal can you",
    "start": "488849",
    "end": "497039"
  },
  {
    "text": "please tell us a bit more about your use case and how what was your approach in Springer to pick up the right database",
    "start": "497039",
    "end": "502919"
  },
  {
    "text": "for your nautical deployment Thank You Andre before I talk about we talk about",
    "start": "502919",
    "end": "508319"
  },
  {
    "text": "the sprinkler use of databases let me give a little bit of overview of a sprinkler so you have some more context",
    "start": "508319",
    "end": "513539"
  },
  {
    "text": "of what we do sprinkler is the first unified customer experience management platform for the enterprise we help",
    "start": "513539",
    "end": "520050"
  },
  {
    "text": "world's largest brands including Amazon to do marketing advertising commerce",
    "start": "520050",
    "end": "526320"
  },
  {
    "text": "care research on Facebook Twitter Linkedin and 22 other social channels globally all on",
    "start": "526320",
    "end": "532589"
  },
  {
    "text": "one integrated platform our customers can integrate sprinkler with their",
    "start": "532589",
    "end": "537899"
  },
  {
    "text": "legacy systems like CRM to get a unified system of engagement for their employees",
    "start": "537899",
    "end": "543269"
  },
  {
    "text": "to work together across silos in real time to manage customer experience at",
    "start": "543269",
    "end": "548610"
  },
  {
    "text": "scale this is a very high-level view of",
    "start": "548610",
    "end": "555389"
  },
  {
    "start": "552000",
    "end": "552000"
  },
  {
    "text": "our architecture the most interesting aspect for our architecture is on the top right where you see a list of all",
    "start": "555389",
    "end": "561509"
  },
  {
    "text": "the social channels that we are integrated with we are integrated with all social media channels we get data",
    "start": "561509",
    "end": "567180"
  },
  {
    "text": "from these channels we analyze them we process it we do sentiment analysis and we may provide meaningful insights to",
    "start": "567180",
    "end": "574110"
  },
  {
    "text": "the user for our platform the users of a platform can use our application to",
    "start": "574110",
    "end": "579990"
  },
  {
    "text": "engage and listen to their customers on any social Channel since we are integrated with all these social media",
    "start": "579990",
    "end": "586199"
  },
  {
    "text": "channel we get a lot of data every day we process billions of transactions we",
    "start": "586199",
    "end": "591329"
  },
  {
    "text": "have para bytes of data thousands of servers across different geo locations just to put things in perspective from",
    "start": "591329",
    "end": "596939"
  },
  {
    "text": "Twitter alone daily we get few hundred million tweets every day this is a very",
    "start": "596939",
    "end": "607170"
  },
  {
    "text": "high level network overview how we have done the deployment so if you can see we have different layers of security we run",
    "start": "607170",
    "end": "615120"
  },
  {
    "text": "everything within VPC our front end servers are running in the web security",
    "start": "615120",
    "end": "620189"
  },
  {
    "text": "group then we have a middle layer security group for our mid middleware data services and then the last and the",
    "start": "620189",
    "end": "627930"
  },
  {
    "text": "most important one is all our data store so the data store as security group is the one which has the most restrictive",
    "start": "627930",
    "end": "633689"
  },
  {
    "text": "ingress and egress rules and this is where we make sure that it's difficult",
    "start": "633689",
    "end": "639720"
  },
  {
    "text": "to penetrate the all our services on the front end in web tier we put load",
    "start": "639720",
    "end": "645870"
  },
  {
    "text": "balancers and this the reason we do that is because lo AWS load balancer provides an extra layer of security against any",
    "start": "645870",
    "end": "652439"
  },
  {
    "text": "potential DDoS attack this is a list of",
    "start": "652439",
    "end": "658559"
  },
  {
    "text": "some of the technologies that we use we use several AWS we use several open source products and",
    "start": "658559",
    "end": "664950"
  },
  {
    "text": "then we also use ansible and Jenkins for",
    "start": "664950",
    "end": "670500"
  },
  {
    "text": "our build and automation mostly we are running Java on Linux in terms of",
    "start": "670500",
    "end": "675890"
  },
  {
    "text": "automation we have also written a lot of custom code we have written probably more than",
    "start": "675890",
    "end": "681450"
  },
  {
    "text": "a million lines of code you just manage infrastructure at a scale so now I'll",
    "start": "681450",
    "end": "688800"
  },
  {
    "start": "687000",
    "end": "687000"
  },
  {
    "text": "hand it over to Krishna to talk about our database journey thanks Yama's",
    "start": "688800",
    "end": "694640"
  },
  {
    "text": "initially we were storing all types of data in my sequel once data increased",
    "start": "694640",
    "end": "699930"
  },
  {
    "text": "more than 100 million records we started seeing these slowness in aggregation",
    "start": "699930",
    "end": "706260"
  },
  {
    "text": "queries so we started writing the data in solar as well and redirecting all our",
    "start": "706260",
    "end": "711690"
  },
  {
    "text": "aggregation queries to the solar this worked for us for a couple of years once",
    "start": "711690",
    "end": "718260"
  },
  {
    "text": "data increased more than 200 million records we started seeing the slowness in my sequel write operations so we",
    "start": "718260",
    "end": "725820"
  },
  {
    "text": "decided to migrate all our big tables data from my sequel to Charlotte",
    "start": "725820",
    "end": "731850"
  },
  {
    "text": "clusters once we migrated all the big",
    "start": "731850",
    "end": "738150"
  },
  {
    "text": "tables data to shaddid cluster and we were writing data in solar as well",
    "start": "738150",
    "end": "745260"
  },
  {
    "text": "and all our aggregation queries are redirected to solar this has worked",
    "start": "745260",
    "end": "750960"
  },
  {
    "text": "great for couple more years once that I increased more than 800 million records",
    "start": "750960",
    "end": "757230"
  },
  {
    "text": "we started seeing the issues in solar we",
    "start": "757230",
    "end": "764040"
  },
  {
    "text": "started seeing the issues related to load and GC issues in solar we want to horizontally scale this solar deployment",
    "start": "764040",
    "end": "770660"
  },
  {
    "text": "so we tried with the solar cloud in solar cloud we faced issues related to",
    "start": "770660",
    "end": "778080"
  },
  {
    "text": "the data balancing and incremental backups so we slowly moving from solar",
    "start": "778080",
    "end": "787020"
  },
  {
    "text": "to elasticsearch elasticsearch is scaling great for us with the billions",
    "start": "787020",
    "end": "792720"
  },
  {
    "text": "of records",
    "start": "792720",
    "end": "795290"
  },
  {
    "text": "how big is our database deployment we",
    "start": "799009",
    "end": "804029"
  },
  {
    "text": "have more than 1300 servers in elasticsearch and the second largest",
    "start": "804029",
    "end": "810120"
  },
  {
    "text": "deployment is MongoDB which has 900 plus servers and salary is being replaced",
    "start": "810120",
    "end": "816990"
  },
  {
    "text": "with the elasticsearch we have right now 380 servers still we use RDS servers for",
    "start": "816990",
    "end": "824550"
  },
  {
    "text": "transactional related operations we have 70 multi AG RDS servers hanging out to",
    "start": "824550",
    "end": "832440"
  },
  {
    "text": "the hundred to talk about choosing the right deal so if you can imagine it's a",
    "start": "832440",
    "end": "837839"
  },
  {
    "start": "834000",
    "end": "834000"
  },
  {
    "text": "massive deployment right more than nine hundreds of servers for MongoDB it's quite big and I often see this situation",
    "start": "837839",
    "end": "844740"
  },
  {
    "text": "when the customers start their deployment like a small startup we're gonna choose some random geography like",
    "start": "844740",
    "end": "850529"
  },
  {
    "text": "for sake of the cost for example or for latency for our deploy for our developers and after a certain amount of",
    "start": "850529",
    "end": "858000"
  },
  {
    "text": "time when like the user base grows and the number of requests grows the diet SF grows often such a customers experience",
    "start": "858000",
    "end": "865949"
  },
  {
    "text": "really bad problems with latency because one customer had for example their",
    "start": "865949",
    "end": "871230"
  },
  {
    "text": "deployment in you in Europe but most of their customers were located in US and",
    "start": "871230",
    "end": "876420"
  },
  {
    "text": "they had to provide like a really big projects for migration of data of their",
    "start": "876420",
    "end": "882389"
  },
  {
    "text": "non-sequel databases from Europe to us which wasn't funny actually so you have",
    "start": "882389",
    "end": "888509"
  },
  {
    "text": "to think carefully about where your data is where are you users and how you will",
    "start": "888509",
    "end": "894449"
  },
  {
    "text": "deploy it and of course with the MongoDB for example with the Charlotte clusters what you can do you can do hybrid",
    "start": "894449",
    "end": "900389"
  },
  {
    "text": "deployments for some like data locality laws and stuff like that",
    "start": "900389",
    "end": "905459"
  },
  {
    "text": "okay after choosing your geo you have to design the deployment architecture for",
    "start": "905459",
    "end": "910709"
  },
  {
    "text": "your no the no sequel databases right first thing is the network of course you have to take care about that and you",
    "start": "910709",
    "end": "917130"
  },
  {
    "text": "have to be sure that the network's work network works and different parts of the",
    "start": "917130",
    "end": "922290"
  },
  {
    "text": "cluster can work together after that you have to design and pick up the proper",
    "start": "922290",
    "end": "928140"
  },
  {
    "text": "of the instances of the storage and have you you have to design how many nodes general nodes will be there like master",
    "start": "928140",
    "end": "935160"
  },
  {
    "text": "and secondaries and you have to design how many special nodes will be there in your deployment like hidden sand arbiter",
    "start": "935160",
    "end": "941820"
  },
  {
    "text": "nodes and where they will be located and we're going to cover that in more details later on and of course you have",
    "start": "941820",
    "end": "948270"
  },
  {
    "text": "to think about service discovery like where's the configuration replica SEPA",
    "start": "948270",
    "end": "953310"
  },
  {
    "text": "MongoDB say it's you have to think about access patterns and read and write consistency so what will be your sip",
    "start": "953310",
    "end": "960840"
  },
  {
    "text": "setup what will be your configuration for read and write and you have to replay that in four different edge cases",
    "start": "960840",
    "end": "967170"
  },
  {
    "start": "967000",
    "end": "967000"
  },
  {
    "text": "for your database so for example what you can do here is like to deploy a MongoDB Charlotte cluster in three",
    "start": "967170",
    "end": "974220"
  },
  {
    "text": "different locations it's a kind of hybrid example right the only thing to think about and the big trade-off is the",
    "start": "974220",
    "end": "981780"
  },
  {
    "text": "deployment of replicas set of the configuration replicas set so one approach is you can deploy it in one",
    "start": "981780",
    "end": "988470"
  },
  {
    "text": "region close to the biggest let's say deployment of MongoDB or you can",
    "start": "988470",
    "end": "993600"
  },
  {
    "text": "distribute that across the region's in this case right the trade-off of distribution is that the shuffling of",
    "start": "993600",
    "end": "1000470"
  },
  {
    "text": "data will be slow and you can experience some but situations with latency for",
    "start": "1000470",
    "end": "1006500"
  },
  {
    "text": "writes so you have to think about it and that's where access patterns really",
    "start": "1006500",
    "end": "1011690"
  },
  {
    "start": "1011000",
    "end": "1011000"
  },
  {
    "text": "matter after that you have to design the application interactions with your",
    "start": "1011690",
    "end": "1017360"
  },
  {
    "text": "database the first thing which is really super important is error handling inside the application because it gives you a",
    "start": "1017360",
    "end": "1023330"
  },
  {
    "text": "lot of interesting information around how actually your application works how do your cluster works how they",
    "start": "1023330",
    "end": "1029510"
  },
  {
    "text": "accommodate load and so on and so forth so error handling is super important the next thing is DB request so normally we",
    "start": "1029510",
    "end": "1037400"
  },
  {
    "text": "are saying that MongoDB is hemolyzed database that's true however there are some certain recommendations around how",
    "start": "1037400",
    "end": "1043970"
  },
  {
    "text": "the document should look like not to use a lot of nested documents not to scan inside the documents and so on so there",
    "start": "1043970",
    "end": "1050240"
  },
  {
    "text": "are certain recommendations and you can of course you can plan and design your indexes MongoDB interacts with the",
    "start": "1050240",
    "end": "1057560"
  },
  {
    "text": "application with the use of as process so you have to fine-tune the SPRO is dependent on the use case as well and",
    "start": "1057560",
    "end": "1064220"
  },
  {
    "text": "of course don't forget about network connectivity on the database cluster side and it linked with it links with",
    "start": "1064220",
    "end": "1072920"
  },
  {
    "text": "consistency levels and consistency guarantees if the network is there it's not working",
    "start": "1072920",
    "end": "1078890"
  },
  {
    "text": "they cannot nonce cannot communicate with each other most probably you will not be able to read or write data on the",
    "start": "1078890",
    "end": "1085820"
  },
  {
    "text": "levels of and with the guarantees you want so if we're going to talk about",
    "start": "1085820",
    "end": "1092180"
  },
  {
    "text": "like really geo distributed real jewel or distributed deployments what we can",
    "start": "1092180",
    "end": "1097460"
  },
  {
    "text": "use you can benefit from load L for load balancing on top of s processes",
    "start": "1097460",
    "end": "1103010"
  },
  {
    "text": "which will help you to implement some interesting logic around like routing",
    "start": "1103010",
    "end": "1108430"
  },
  {
    "text": "requests r√∂tteln in case one of the big your one of the big deployments just",
    "start": "1108430",
    "end": "1114080"
  },
  {
    "text": "fails completely so and we're going to cover that in more details and of course",
    "start": "1114080",
    "end": "1119150"
  },
  {
    "text": "you have to plan the configuration replica set and request throttling",
    "start": "1119150",
    "end": "1124640"
  },
  {
    "text": "Driver errors and look for small or slow queries and some schema issues so",
    "start": "1124640",
    "end": "1130490"
  },
  {
    "text": "Krishna can you give us some rough numbers around how your MongoDB works",
    "start": "1130490",
    "end": "1135530"
  },
  {
    "start": "1132000",
    "end": "1132000"
  },
  {
    "text": "yep let's look at some numbers to understand at what Callaway operating",
    "start": "1135530",
    "end": "1143120"
  },
  {
    "text": "MongoDB we have nine hundred plus service and each service we attach one",
    "start": "1143120",
    "end": "1148580"
  },
  {
    "text": "ABS disk which is ranging from 500 gb to 4 terabytes and we have 200 terabytes of",
    "start": "1148580",
    "end": "1155950"
  },
  {
    "text": "compressed primary data and we keep two secondary nodes in each replicas set",
    "start": "1155950",
    "end": "1161840"
  },
  {
    "text": "that means we have 600 terabytes of compressed primary compressed data",
    "start": "1161840",
    "end": "1166910"
  },
  {
    "text": "across our MongoDB deployment in terms of replica sets we have 250 replicas",
    "start": "1166910",
    "end": "1172670"
  },
  {
    "text": "sets and 20 sharded clusters and some of the shaded clusters holding that",
    "start": "1172670",
    "end": "1179330"
  },
  {
    "text": "t-shirts and which which can hold up to 30 terabytes of data and coming to",
    "start": "1179330",
    "end": "1185360"
  },
  {
    "text": "number of operations we do on our MongoDB deployment is 2 million",
    "start": "1185360",
    "end": "1190790"
  },
  {
    "text": "operations per second we have daily 30 terabytes of",
    "start": "1190790",
    "end": "1196360"
  },
  {
    "text": "data changes are there and these 30 terabytes of data we think from east",
    "start": "1196360",
    "end": "1203690"
  },
  {
    "text": "region to west region for disaster recovery purpose so for such deployments",
    "start": "1203690",
    "end": "1210350"
  },
  {
    "text": "and it's quite big right to millions of operations and like terabytes and terabytes of data you don't want to lose",
    "start": "1210350",
    "end": "1217790"
  },
  {
    "text": "these data the whole data set right and you don't want your users to experience",
    "start": "1217790",
    "end": "1223070"
  },
  {
    "text": "some issues with connectivity some issues that with LaTourette request and so on so you have to plan a a high",
    "start": "1223070",
    "end": "1230360"
  },
  {
    "text": "availability and disaster recovery so from high availability standpoint there are certain aspects right so the most",
    "start": "1230360",
    "end": "1238100"
  },
  {
    "text": "important part is on your application side let's say so you your application",
    "start": "1238100",
    "end": "1244730"
  },
  {
    "text": "should know that if this part of the cluster fails it can go to another part of the cluster right and to process the",
    "start": "1244730",
    "end": "1251240"
  },
  {
    "text": "request here you have to have authentication configurations routing",
    "start": "1251240",
    "end": "1257210"
  },
  {
    "text": "and route load balancing or from your application side and one really",
    "start": "1257210",
    "end": "1262640"
  },
  {
    "text": "important thing is that you need to be able to throttle these requests because",
    "start": "1262640",
    "end": "1268429"
  },
  {
    "text": "as you can see on this diagram is if let's say one of the regions just fails",
    "start": "1268429",
    "end": "1274580"
  },
  {
    "text": "completely and you're not able to request to do process requests there the",
    "start": "1274580",
    "end": "1280490"
  },
  {
    "text": "whole amount of traffic for example can be routed to another another region and if it's like 42 percent mo load which",
    "start": "1280490",
    "end": "1287990"
  },
  {
    "text": "comes out of nowhere it can be really challenging right so you have to be prepared to scale out these parts of the",
    "start": "1287990",
    "end": "1294830"
  },
  {
    "text": "clusters for MongoDB there is a classic",
    "start": "1294830",
    "end": "1301130"
  },
  {
    "text": "set up with master and two secondary nodes and with hidden nodes for the deployments but if we're going to talk",
    "start": "1301130",
    "end": "1307880"
  },
  {
    "text": "about two ACS in one region and to easy deployment for the classic set up it can",
    "start": "1307880",
    "end": "1313460"
  },
  {
    "text": "have some edge cases so for example if one AC fails and your muster and",
    "start": "1313460",
    "end": "1318800"
  },
  {
    "text": "secondary node wasn't that AZ so you will be only able to have a level of",
    "start": "1318800",
    "end": "1324350"
  },
  {
    "text": "concerns of local writes a local read sorry so you only will be able to",
    "start": "1324350",
    "end": "1329539"
  },
  {
    "text": "read some data from this secondary node so there are some some techniques so you",
    "start": "1329539",
    "end": "1335929"
  },
  {
    "text": "can for example use the predictive predictable elections so set up the",
    "start": "1335929",
    "end": "1341570"
  },
  {
    "text": "whites for elections you can use master he didn't know if setup if your",
    "start": "1341570",
    "end": "1347929"
  },
  {
    "text": "application can tolerate with some downtime or you can use remote node in",
    "start": "1347929",
    "end": "1354440"
  },
  {
    "text": "replica set krishnan what's your approach for the are how you do that yeah in charted cluster each shard is a",
    "start": "1354440",
    "end": "1362899"
  },
  {
    "start": "1358000",
    "end": "1358000"
  },
  {
    "text": "replica set let's look at the biggest sect apology in any replica set all",
    "start": "1362899",
    "end": "1368809"
  },
  {
    "text": "rights happens through the primary data node and all these rights track inside  collection called up log and we",
    "start": "1368809",
    "end": "1378139"
  },
  {
    "text": "add one more data note to the the biggest set for the high availability purpose from primary to secondary",
    "start": "1378139",
    "end": "1384820"
  },
  {
    "text": "replication happens through the up lock collection when primary primary data",
    "start": "1384820",
    "end": "1392450"
  },
  {
    "text": "node goes down secondary data nodes should reelected as a primary for this reason for real",
    "start": "1392450",
    "end": "1400399"
  },
  {
    "text": "action to happen at least more than half of the voting member should be available in a repeater set so we had one arbiter",
    "start": "1400399",
    "end": "1408979"
  },
  {
    "text": "to the replica set in third yo we add",
    "start": "1408979",
    "end": "1414379"
  },
  {
    "text": "one more that are not to the typical set as a hidden secondary from this we take",
    "start": "1414379",
    "end": "1421249"
  },
  {
    "text": "daily backups daily backups",
    "start": "1421249",
    "end": "1425859"
  },
  {
    "text": "okay let's see what under challenges we faced with the our MongoDB deployment",
    "start": "1429510",
    "end": "1435330"
  },
  {
    "text": "the first challenge we faced with three backups why because while we migrating",
    "start": "1435330",
    "end": "1442420"
  },
  {
    "text": "from my sequel to our primary concern was scaling up their rights so",
    "start": "1442420",
    "end": "1447700"
  },
  {
    "text": "we launched all our data nodes with the instance store volumes H a 1.4 X la",
    "start": "1447700",
    "end": "1453190"
  },
  {
    "text": "service which has two terabytes of instance tour volume so our data is",
    "start": "1453190",
    "end": "1460090"
  },
  {
    "text": "there on instance store we cannot take the snapshots so we had couple of",
    "start": "1460090",
    "end": "1465400"
  },
  {
    "text": "options for this backups the first option was among atoms but longer dump",
    "start": "1465400",
    "end": "1471130"
  },
  {
    "text": "does not support incremental backups and in our case we used to see this boom or",
    "start": "1471130",
    "end": "1479260"
  },
  {
    "text": "dump process speed is around it used to take one hour for dumping the 50 GB of",
    "start": "1479260",
    "end": "1486070"
  },
  {
    "text": "data with large deployments like sprinkler this is not the feasible",
    "start": "1486070",
    "end": "1491770"
  },
  {
    "text": "solution then we tried with the second",
    "start": "1491770",
    "end": "1497470"
  },
  {
    "text": "option that is file system copy using a sync with the assing method we are able",
    "start": "1497470",
    "end": "1505120"
  },
  {
    "text": "to do the Delta sync but this process used to take more than two hours while",
    "start": "1505120",
    "end": "1512430"
  },
  {
    "text": "backup process is going on we have to stop their rights on hidden secondary and and if you want to take multiple",
    "start": "1512430",
    "end": "1524110"
  },
  {
    "text": "backups we have to do the awesome from the start on new volume this used to",
    "start": "1524110",
    "end": "1531010"
  },
  {
    "text": "take more than 15 hours so with all these pain points we decided to migrate",
    "start": "1531010",
    "end": "1537610"
  },
  {
    "text": "all our HS hidden secondary data to the EBS volumes once we migrated all our",
    "start": "1537610",
    "end": "1546190"
  },
  {
    "text": "data to the EBS volumes we are able to take multiple backups using abs",
    "start": "1546190",
    "end": "1553240"
  },
  {
    "text": "snapshots and our backup process time has reduced from couple of hours to 10",
    "start": "1553240",
    "end": "1560380"
  },
  {
    "text": "minutes less than 10 minutes and the amount of time you have to lock the hidden secondary was reduced to less",
    "start": "1560380",
    "end": "1567830"
  },
  {
    "text": "than one minute and as this EBS snapshots are",
    "start": "1567830",
    "end": "1572840"
  },
  {
    "text": "incremental and even though we maintain multiple EBS snapshots the backup cost",
    "start": "1572840",
    "end": "1581780"
  },
  {
    "text": "has reduced significantly the next challenge we face with me that",
    "start": "1581780",
    "end": "1588830"
  },
  {
    "text": "are not recovery is still our primary and secondary data on instance stores if",
    "start": "1588830",
    "end": "1595910"
  },
  {
    "text": "any of the data not goes down we have to rebuild the data node s data size is",
    "start": "1595910",
    "end": "1602840"
  },
  {
    "text": "Hugh's on our replica sets it used to take lots lots of time we had couple of",
    "start": "1602840",
    "end": "1610910"
  },
  {
    "text": "options first building the data node the first option was as we have already be a",
    "start": "1610910",
    "end": "1617270"
  },
  {
    "text": "snapshots so copying all the data from EBS snapshots snapshots to instant store",
    "start": "1617270",
    "end": "1623710"
  },
  {
    "text": "this used to take more than 15 hours in",
    "start": "1623710",
    "end": "1628790"
  },
  {
    "text": "our case the second option what we had is sync just add back the server",
    "start": "1628790",
    "end": "1635600"
  },
  {
    "text": "to the typical set and syncs all this data to the new node this happens",
    "start": "1635600",
    "end": "1642470"
  },
  {
    "text": "in three steps in the first step the new node copies all data from the primary",
    "start": "1642470",
    "end": "1649130"
  },
  {
    "text": "data node once all the data is copied to the local it build the indexes based on",
    "start": "1649130",
    "end": "1655850"
  },
  {
    "text": "the metadata once index creation is completed it it's sync they all the",
    "start": "1655850",
    "end": "1663200"
  },
  {
    "text": "write operations from this step onto current type in our case this whole",
    "start": "1663200",
    "end": "1669080"
  },
  {
    "text": "process used to take more than two to four days because of data size and",
    "start": "1669080",
    "end": "1674870"
  },
  {
    "text": "because of the number of indexes we have and the complexity of our indexes soap",
    "start": "1674870",
    "end": "1683410"
  },
  {
    "text": "as it used to take four days at least we have to maintain four days of operations",
    "start": "1683410",
    "end": "1690470"
  },
  {
    "text": "in our up log across all our data nodes and as this is a full sink it used to",
    "start": "1690470",
    "end": "1698000"
  },
  {
    "text": "put extra load on primary servers and",
    "start": "1698000",
    "end": "1703420"
  },
  {
    "text": "all this for days we have a great list at high availability so with with all",
    "start": "1703420",
    "end": "1711860"
  },
  {
    "text": "these problems we decided to migrate instance store volumes data to EBS we",
    "start": "1711860",
    "end": "1719420"
  },
  {
    "start": "1717000",
    "end": "1717000"
  },
  {
    "text": "migrated all our primary and secondary data to the EBS in two steps in the",
    "start": "1719420",
    "end": "1727520"
  },
  {
    "text": "first step we migrated all data to Iowan volumes how did we do this",
    "start": "1727520",
    "end": "1736220"
  },
  {
    "text": "without impacting application we added two hidden secondaries from the EBS",
    "start": "1736220",
    "end": "1742610"
  },
  {
    "text": "snapshots to each replica set we waited couple of days to make sure",
    "start": "1742610",
    "end": "1748370"
  },
  {
    "text": "the read and write latencies has come down to less than one millisecond then",
    "start": "1748370",
    "end": "1756920"
  },
  {
    "text": "we promoted one of the hidden secondary as a primary and we monitored read and",
    "start": "1756920",
    "end": "1763220"
  },
  {
    "text": "write latencies for couple of weeks once we are happy with the read and write latencies we removed all instance store",
    "start": "1763220",
    "end": "1771500"
  },
  {
    "text": "service in the second step we migrated",
    "start": "1771500",
    "end": "1776630"
  },
  {
    "text": "all our data from i1 to GP 2 we followed this same procedure with this migration",
    "start": "1776630",
    "end": "1784300"
  },
  {
    "text": "we are able to recover our data nodes",
    "start": "1784300",
    "end": "1790270"
  },
  {
    "text": "much quickly previously it used to take four days that process has come down to",
    "start": "1790270",
    "end": "1796310"
  },
  {
    "text": "one or two hours and also with this process we are able to separate",
    "start": "1796310",
    "end": "1803440"
  },
  {
    "text": "computing power and storage this helped us scaling scaling up or scaling down",
    "start": "1803440",
    "end": "1811190"
  },
  {
    "text": "computing power when we required and with the EBS the volume size increase",
    "start": "1811190",
    "end": "1818810"
  },
  {
    "text": "increases it's become so easy it's just couple of clicks as",
    "start": "1818810",
    "end": "1825630"
  },
  {
    "text": "that annoyed recovery time has reduced to couple of hours they reduced our up",
    "start": "1825630",
    "end": "1830820"
  },
  {
    "text": "locks size to hold at at least of 24 hours of operations the next challenge",
    "start": "1830820",
    "end": "1840060"
  },
  {
    "text": "we faced with Lee disaster recovery process implementation in disaster",
    "start": "1840060",
    "end": "1845940"
  },
  {
    "text": "recovery implementation process mainly we have three major tasks one is",
    "start": "1845940",
    "end": "1852740"
  },
  {
    "text": "regularly taking backups the most most",
    "start": "1852740",
    "end": "1858210"
  },
  {
    "text": "of the our backup issues are resolved by using ABS snapshots the second one is",
    "start": "1858210",
    "end": "1865130"
  },
  {
    "text": "copying all this data to the dr side yes",
    "start": "1865130",
    "end": "1871020"
  },
  {
    "text": "we have around 30 terabytes of data changes daily initially it used to take",
    "start": "1871020",
    "end": "1876540"
  },
  {
    "text": "more than 24 hours to think this data from East Side to west side we work with",
    "start": "1876540",
    "end": "1883500"
  },
  {
    "text": "the ADA base team to increase our bandwidth limits and to increase number",
    "start": "1883500",
    "end": "1889410"
  },
  {
    "text": "of snapshots we can copy from East Side to West Side after increasing these",
    "start": "1889410",
    "end": "1895320"
  },
  {
    "text": "limits we are able to transfer all 30 terabytes of data in less than four",
    "start": "1895320",
    "end": "1901260"
  },
  {
    "text": "hours the next challenge we faced in",
    "start": "1901260",
    "end": "1907760"
  },
  {
    "text": "restauration process in the which duration the first thing is launching",
    "start": "1907760",
    "end": "1913890"
  },
  {
    "text": "thousands of servers and creating volumes from the snapshots and attaching those volumes to the servers once",
    "start": "1913890",
    "end": "1922020"
  },
  {
    "text": "attachment is done we have to start the manga process while starting the manga",
    "start": "1922020",
    "end": "1928470"
  },
  {
    "text": "process we were observing we were seeing manga startup was taking more than an",
    "start": "1928470",
    "end": "1935010"
  },
  {
    "text": "hour so we worked with the DB team MongoDB MongoDB team to understand why",
    "start": "1935010",
    "end": "1942500"
  },
  {
    "text": "startup as taking more than an hour just cut up is taking more than an hour",
    "start": "1942500",
    "end": "1948530"
  },
  {
    "text": "so finally we come to know that it's all related to our plug on a plug milestones",
    "start": "1948530",
    "end": "1955670"
  },
  {
    "text": "what are these block milestones  internally maintains milestones",
    "start": "1955670",
    "end": "1962590"
  },
  {
    "text": "Ignace the applica lection to efficiently remove the oldest up log",
    "start": "1962590",
    "end": "1968169"
  },
  {
    "text": "entries from the collection when it reaches the maximum size and this up",
    "start": "1968169",
    "end": "1974110"
  },
  {
    "text": "lock milestones are not persistent that means whenever Mangal restart happens it will chew just the new",
    "start": "1974110",
    "end": "1980740"
  },
  {
    "text": "milestones because we have a block size more than 100 JB in most of the our",
    "start": "1980740",
    "end": "1986830"
  },
  {
    "text": "typical sets and this data has restored from the EBS snapshots the milestones",
    "start": "1986830",
    "end": "1994120"
  },
  {
    "text": "choosing was taking the most of the time so we work with the MongoDB development",
    "start": "1994120",
    "end": "1999880"
  },
  {
    "text": "team and implemented optimisation in backup and restoration process in the",
    "start": "1999880",
    "end": "2008520"
  },
  {
    "text": "backup process we added couple more steps before taking the snapshot we",
    "start": "2008520",
    "end": "2014429"
  },
  {
    "text": "restart as a standalone mode and take the latest up log entry to the",
    "start": "2014429",
    "end": "2021240"
  },
  {
    "text": "temporary collection and trigger a snapshot and once this snapshot is",
    "start": "2021240",
    "end": "2026750"
  },
  {
    "text": "available in be aside after restoring it just remove the app lock files and",
    "start": "2026750",
    "end": "2035059"
  },
  {
    "text": "recreate the app lock file with we just headers and start process in",
    "start": "2035059",
    "end": "2041880"
  },
  {
    "text": "standalone mode and copy back the app lock entry from temporary collection to",
    "start": "2041880",
    "end": "2047460"
  },
  {
    "text": "apply collection once we restart process in a repeat a second mode in",
    "start": "2047460",
    "end": "2053790"
  },
  {
    "text": "this case it has to scan only one record for choosing the new milestones this",
    "start": "2053790",
    "end": "2061368"
  },
  {
    "text": "reduced startup time with this process after implementing this process",
    "start": "2061369",
    "end": "2069260"
  },
  {
    "start": "2062000",
    "end": "2062000"
  },
  {
    "text": " start time was taking around 30 seconds before that it was taking around",
    "start": "2069260",
    "end": "2076950"
  },
  {
    "text": "80 plus 80 plus minutes ok next comes to",
    "start": "2076950",
    "end": "2083280"
  },
  {
    "text": "workload we do over two million",
    "start": "2083280",
    "end": "2088648"
  },
  {
    "text": "operations per second on our MongoDB deployment how did we get this two million number",
    "start": "2088649",
    "end": "2095720"
  },
  {
    "text": "if you check this in one of the our primary data note we do around 2000",
    "start": "2095720",
    "end": "2103380"
  },
  {
    "text": "insert operations and six to seven K of raid operations and deletes updates and",
    "start": "2103380",
    "end": "2110220"
  },
  {
    "text": "get most all this will be around one thousand operations on you check every",
    "start": "2110220",
    "end": "2116400"
  },
  {
    "text": "primary data node we do on an average more than ten thousand operations per",
    "start": "2116400",
    "end": "2122280"
  },
  {
    "text": "second comes to write workload some of",
    "start": "2122280",
    "end": "2128460"
  },
  {
    "text": "the our modules produce heavy data and all this data will store it in so",
    "start": "2128460",
    "end": "2136970"
  },
  {
    "text": "we have seen some of our replicas set were able to handle 80 GB of 80 GB",
    "start": "2136970",
    "end": "2144960"
  },
  {
    "text": "of data per hour okay with this heavy",
    "start": "2144960",
    "end": "2152580"
  },
  {
    "start": "2148000",
    "end": "2148000"
  },
  {
    "text": "write and read workloads what under performance metrics we are getting it",
    "start": "2152580",
    "end": "2157800"
  },
  {
    "text": "from the EBS once we restored from the snapshot the first couple of hours the",
    "start": "2157800",
    "end": "2165840"
  },
  {
    "text": "average write latency is around 50 milliseconds this is very huge but once",
    "start": "2165840",
    "end": "2173070"
  },
  {
    "text": "most of the blocks are local the average write latency has come down to less than",
    "start": "2173070",
    "end": "2181230"
  },
  {
    "text": "5 milliseconds the same case with the read latencies as well initially first",
    "start": "2181230",
    "end": "2188610"
  },
  {
    "start": "2184000",
    "end": "2184000"
  },
  {
    "text": "couple of hours the read late insists work around 70 to 80 milliseconds on an",
    "start": "2188610",
    "end": "2194460"
  },
  {
    "text": "average once most of the black most of the data blocks are there on the local",
    "start": "2194460",
    "end": "2199740"
  },
  {
    "text": "the read latencies has come down to less than 10 milliseconds in most of the our",
    "start": "2199740",
    "end": "2206280"
  },
  {
    "text": "Deepika's sets the read latencies are around sub milliseconds okay",
    "start": "2206280",
    "end": "2215430"
  },
  {
    "start": "2213000",
    "end": "2213000"
  },
  {
    "text": "next comes to cost savings the first cost savings we got it from the",
    "start": "2215430",
    "end": "2222290"
  },
  {
    "text": "replacing expensive ha1 dot 4x last service with the r3 service now we are replacing",
    "start": "2222290",
    "end": "2229300"
  },
  {
    "text": "with the r4 sir our four series servers the next cost savings we got it from the",
    "start": "2229300",
    "end": "2235750"
  },
  {
    "text": "backups as EBS snapshots are incremental",
    "start": "2235750",
    "end": "2240820"
  },
  {
    "text": "we are able to save lots of money in",
    "start": "2240820",
    "end": "2246420"
  },
  {
    "text": "backups the next one is white tiger compression Bongo introduced white",
    "start": "2246420",
    "end": "2254770"
  },
  {
    "text": "higher compression from 3.4 version onwards because of this white tiger",
    "start": "2254770",
    "end": "2260770"
  },
  {
    "text": "compression we were able to reduce our data size on the disc about 60",
    "start": "2260770",
    "end": "2266290"
  },
  {
    "text": "percentage and we changed all our data",
    "start": "2266290",
    "end": "2271530"
  },
  {
    "text": "from I 1 discs to GP 2 discs and dynamic",
    "start": "2271530",
    "end": "2279880"
  },
  {
    "text": "volume changes eb is from EBS without",
    "start": "2279880",
    "end": "2286210"
  },
  {
    "text": "detaching the server without detaching the volume from the server we are able to increase volume size this helped us",
    "start": "2286210",
    "end": "2294780"
  },
  {
    "text": "while launching the new replica sets launched with the minimum minimum size",
    "start": "2294780",
    "end": "2300220"
  },
  {
    "text": "of discs and as data grows increase the volume size and with all these cost",
    "start": "2300220",
    "end": "2307240"
  },
  {
    "text": "initiatives we are able to save our  cost storage cost from $155 $155",
    "start": "2307240",
    "end": "2316750"
  },
  {
    "text": "scope just 53k dollars per month this is almost 66 percentage of savings this is",
    "start": "2316750",
    "end": "2324070"
  },
  {
    "text": "the huge win for us handing out to jamal",
    "start": "2324070",
    "end": "2330190"
  },
  {
    "start": "2326000",
    "end": "2326000"
  },
  {
    "text": "to talk about security Thank You Krishna so this is a different view of the",
    "start": "2330190",
    "end": "2336550"
  },
  {
    "text": "security that I talked about earlier it just gives a little bit more details we use various AWS services we run",
    "start": "2336550",
    "end": "2344020"
  },
  {
    "text": "everything in V PC and there are different security groups for a different layer of architecture the two",
    "start": "2344020",
    "end": "2349030"
  },
  {
    "text": "important things I want to highlight here is one is that we use server roles I am roles for all our internal services",
    "start": "2349030",
    "end": "2358270"
  },
  {
    "text": "or API calls and we do this because if you don't want to deal with the API Keys sitting around",
    "start": "2358270",
    "end": "2363400"
  },
  {
    "text": "here and there so that makes the life very easy second thing that we do is we use EBS encryption and this is something",
    "start": "2363400",
    "end": "2370480"
  },
  {
    "text": "which is works very well initially when you were trying the encryption you are not sure how it will perform but after",
    "start": "2370480",
    "end": "2375880"
  },
  {
    "text": "we start enable it we didn't see any noticeable performance issues with using the EBS encryption so and we don't have",
    "start": "2375880",
    "end": "2383680"
  },
  {
    "text": "to worry about any key management either this is another thing which is a very",
    "start": "2383680",
    "end": "2390370"
  },
  {
    "start": "2387000",
    "end": "2387000"
  },
  {
    "text": "interesting feature which I like it's which works like magic because we are running our primary infrastructure in",
    "start": "2390370",
    "end": "2395910"
  },
  {
    "text": "Virginia and we have the disaster recovery in Oregon so if you are using encryption you are you need to have the",
    "start": "2395910",
    "end": "2403240"
  },
  {
    "text": "keys available in both regions in our case some of our customers are very",
    "start": "2403240",
    "end": "2408660"
  },
  {
    "text": "security sensitive they don't want the key to leave the primary region they want the key to stay on the physical",
    "start": "2408660",
    "end": "2414730"
  },
  {
    "text": "Hardware in the primary region so in this kind of scenario if you have to manage encryption across two different",
    "start": "2414730",
    "end": "2421240"
  },
  {
    "text": "regions then you have to encrypt the date decrypt the data in Virginia send",
    "start": "2421240",
    "end": "2426580"
  },
  {
    "text": "it over securely to Oregon then re encrypt it again in Oregon again using the key in Oregon so this is a and you",
    "start": "2426580",
    "end": "2433090"
  },
  {
    "text": "have to manage the key in both regions so this whole thing operation we don't",
    "start": "2433090",
    "end": "2438760"
  },
  {
    "text": "have to do anything using EBS it happens behind the scene for us like a magic so I was headed over",
    "start": "2438760",
    "end": "2445060"
  },
  {
    "text": "to Andre now to talk about how to pick the optimal storage and instance type thanks Jamal and you guys have a great",
    "start": "2445060",
    "end": "2452560"
  },
  {
    "start": "2446000",
    "end": "2446000"
  },
  {
    "text": "deployment a huge system right but we completely understand that like every",
    "start": "2452560",
    "end": "2459070"
  },
  {
    "text": "use case it's different its unique so that's why there are a couple of couple",
    "start": "2459070",
    "end": "2465430"
  },
  {
    "text": "of hints around how to pick up the proper instance and storage types for your deployments of course everything",
    "start": "2465430",
    "end": "2472210"
  },
  {
    "text": "depends on database implementation on your data schema on access patterns and",
    "start": "2472210",
    "end": "2477550"
  },
  {
    "text": "it's something always to consider but what you can do and is to use these",
    "start": "2477550",
    "end": "2483340"
  },
  {
    "text": "benchmarks the benchmark has to access so one is requests per second another",
    "start": "2483340",
    "end": "2488770"
  },
  {
    "text": "one is the volume size so for MongoDB and general war clothes",
    "start": "2488770",
    "end": "2493920"
  },
  {
    "text": "without much pressure on the database and with different sizes of data set you",
    "start": "2493920",
    "end": "2500970"
  },
  {
    "text": "can use em force with gp2 volumes it's kind of a standard and pretty good",
    "start": "2500970",
    "end": "2507210"
  },
  {
    "text": "working on most of the use cases but if you have some something like if you need really high throughput if you need a lot",
    "start": "2507210",
    "end": "2515130"
  },
  {
    "text": "of read and write operations per node for for your MongoDB deployment and your",
    "start": "2515130",
    "end": "2521009"
  },
  {
    "text": "data set is relatively small you can go with instance stores and you can use I freeze or iTunes but while your data",
    "start": "2521009",
    "end": "2528329"
  },
  {
    "text": "size data set grows and you have more and more data on the notes as guys",
    "start": "2528329",
    "end": "2534029"
  },
  {
    "text": "showed us it were using our force with gp2 volumes because they are cost",
    "start": "2534029",
    "end": "2541650"
  },
  {
    "text": "effective they have a lot of nice features like encryption elastic volumes",
    "start": "2541650",
    "end": "2547170"
  },
  {
    "text": "you can just increase the amount of i/o per volume and increase the size of volume on the fly it snapshotting and",
    "start": "2547170",
    "end": "2555150"
  },
  {
    "text": "encryption for snapshots and a lot of interesting features and for hidden",
    "start": "2555150",
    "end": "2561599"
  },
  {
    "text": "nodes you can use unforce with GPUs even though if you're like deployment is on",
    "start": "2561599",
    "end": "2567210"
  },
  {
    "text": "top of I freeze it's better to use em force with GPUs for hidden nodes because",
    "start": "2567210",
    "end": "2573749"
  },
  {
    "text": "you will be able to make to do a backups quite quite fast and efficient way we",
    "start": "2573749",
    "end": "2580230"
  },
  {
    "text": "are not covering Cassandra in these talks but just to show you to give you an idea how database deploy how database",
    "start": "2580230",
    "end": "2588180"
  },
  {
    "text": "implementation affects the process for picking up the write instances storage",
    "start": "2588180",
    "end": "2594150"
  },
  {
    "text": "type I put the Cassandra here as well so for Cassandra the general recommendations are the same but if you",
    "start": "2594150",
    "end": "2600029"
  },
  {
    "text": "have really high throughput and on writes and what you would do you will",
    "start": "2600029",
    "end": "2605759"
  },
  {
    "text": "use C force most probably with gp2 in gp2 volumes and ht1 volumes for commit",
    "start": "2605759",
    "end": "2611460"
  },
  {
    "text": "lock why because Cassandra is bounded CPU bounded on writes so you're going to",
    "start": "2611460",
    "end": "2616920"
  },
  {
    "text": "use most probably C force so for different data bases different use cases different access patterns they can be",
    "start": "2616920",
    "end": "2623339"
  },
  {
    "text": "different type of volumes on different types of fine-tuning on on your database side so",
    "start": "2623339",
    "end": "2630770"
  },
  {
    "text": "we considered like in in our practice what we can see is that picking up the proper instance and storage type is kind",
    "start": "2630770",
    "end": "2638090"
  },
  {
    "text": "of a journey it's not like said I'm set a set up in stone and it not gonna change any time right so we all start",
    "start": "2638090",
    "end": "2646310"
  },
  {
    "text": "small and normally all the products all the startup start with a small amount of data small clusters and there you can",
    "start": "2646310",
    "end": "2653600"
  },
  {
    "text": "use em force until your your customer base grows significantly and you will",
    "start": "2653600",
    "end": "2660680"
  },
  {
    "text": "need like for example iTunes or I freeze forum with instance stores to accommodate as log that's law but as",
    "start": "2660680",
    "end": "2667790"
  },
  {
    "text": "your data volume grows and up to the certain moment you can reconsider and",
    "start": "2667790",
    "end": "2673310"
  },
  {
    "text": "you can go back for example to EBS volume because it will be help you with",
    "start": "2673310",
    "end": "2678530"
  },
  {
    "text": "operations and it will be much more cost-effective so Krishna can you please",
    "start": "2678530",
    "end": "2684410"
  },
  {
    "text": "sum up our today's talk we are able to run our large-scale Bangor deployment on",
    "start": "2684410",
    "end": "2691610"
  },
  {
    "text": "EBS and it helped us solving these some of the critical operational challenges",
    "start": "2691610",
    "end": "2697870"
  },
  {
    "text": "the first one is backups we are able to take multiple snapshots multiple backups",
    "start": "2697870",
    "end": "2705050"
  },
  {
    "text": "using EBS snapshots the second one is high availability SLA has improved we",
    "start": "2705050",
    "end": "2712790"
  },
  {
    "text": "are able to reduce our data node recovery time from days to less than an",
    "start": "2712790",
    "end": "2719690"
  },
  {
    "text": "hour and also our read and write latencies has come down to sub",
    "start": "2719690",
    "end": "2725660"
  },
  {
    "text": "millisecond the next one is cost savings we are able to save around 66 percentage",
    "start": "2725660",
    "end": "2733670"
  },
  {
    "text": "of cost on our stories the last",
    "start": "2733670",
    "end": "2739370"
  },
  {
    "text": "one with the help of cross region snapshot transfer we are able to",
    "start": "2739370",
    "end": "2746770"
  },
  {
    "text": "implement our dia process with the acceptable RTO and RPO s-- thank you",
    "start": "2746770",
    "end": "2756700"
  },
  {
    "text": "thank you and I think it's time for your questions if you have any questions there are microphones set up so you can",
    "start": "2759100",
    "end": "2767420"
  },
  {
    "text": "use one thank you",
    "start": "2767420",
    "end": "2770680"
  },
  {
    "text": "I tell me please here you implemented some technique for",
    "start": "2776380",
    "end": "2783910"
  },
  {
    "text": "warming up volumes after restoring from snapshots you will have cold data and",
    "start": "2783910",
    "end": "2793109"
  },
  {
    "text": "much more latency after restoring yes we",
    "start": "2793109",
    "end": "2801069"
  },
  {
    "text": "can run some simple shell script to warm up the data but in our case we did not",
    "start": "2801069",
    "end": "2808599"
  },
  {
    "text": "do we let it to warm up the data on its one thank you and second question",
    "start": "2808599",
    "end": "2815729"
  },
  {
    "text": "mmm where is your Mungo's is placed it",
    "start": "2815729",
    "end": "2822640"
  },
  {
    "text": "is dedicated easy to instances with balancer or maybe it is right near",
    "start": "2822640",
    "end": "2832859"
  },
  {
    "text": "application no it's on dedicated balancers album is dedicated on",
    "start": "2832859",
    "end": "2840009"
  },
  {
    "text": "dedicated balancer servers it's not on application service ok thank you",
    "start": "2840009",
    "end": "2847619"
  },
  {
    "text": "No so you're using no relational you're hosting it on AWS have you",
    "start": "2851450",
    "end": "2857910"
  },
  {
    "text": "thought about just moving to dynamodb I think I can answer that one of the",
    "start": "2857910",
    "end": "2863940"
  },
  {
    "text": "things is that we do run on multiple clouds a little bit in our case we have",
    "start": "2863940",
    "end": "2869040"
  },
  {
    "text": "to do be a cloud agnostic so some of the infrastructure that we have built and optimized and it historically I mean",
    "start": "2869040",
    "end": "2875190"
  },
  {
    "text": "it's a huge deployment so a logistically it's not easy B we also want to keep it",
    "start": "2875190",
    "end": "2881070"
  },
  {
    "text": "a little bit more less tightly coupled to the cloud infrastructure that we are running on so those are the two main",
    "start": "2881070",
    "end": "2887370"
  },
  {
    "text": "reason otherwise yeah I mean we have looked into that and lot of times it I",
    "start": "2887370",
    "end": "2892440"
  },
  {
    "text": "would scale what happens is that cost efficiency wise is another thing to think about because in some services we",
    "start": "2892440",
    "end": "2898980"
  },
  {
    "text": "have noticed that the scale that we are running it weak because we get all the discounts and volumes and other things",
    "start": "2898980",
    "end": "2905330"
  },
  {
    "text": "using a dedicated service it's if there is always a little bit markup and that",
    "start": "2905330",
    "end": "2910740"
  },
  {
    "text": "for us makes a big difference on cost so",
    "start": "2910740",
    "end": "2918930"
  },
  {
    "text": "I might have just missed something but are you how do you account for the the",
    "start": "2918930",
    "end": "2924480"
  },
  {
    "text": "performance like difference between I ops and GP to like have you seen any",
    "start": "2924480",
    "end": "2931490"
  },
  {
    "text": "like IO drop-off moving from I like an",
    "start": "2931490",
    "end": "2936900"
  },
  {
    "text": "AI ops EBS storage to like a lower tier am I just missing that no I think it",
    "start": "2936900",
    "end": "2943530"
  },
  {
    "text": "depends on the size of the volume that using and also the how much operations you are doing so EBS volumes",
    "start": "2943530",
    "end": "2949860"
  },
  {
    "text": "can handle anywhere like three thousand ten thousand they can burst up so as long as your i/o operations are within",
    "start": "2949860",
    "end": "2956520"
  },
  {
    "text": "that range EBS volumes were great we do use for elasticsearch SSD but in",
    "start": "2956520",
    "end": "2962880"
  },
  {
    "text": "that case some of our eye operations are you're talking about 80,000 I ops so that's a different use case but I think",
    "start": "2962880",
    "end": "2969120"
  },
  {
    "text": "for our use cases specifically we didn't see that issue of using EBS",
    "start": "2969120",
    "end": "2974490"
  },
  {
    "text": "volumes okay and in fact like we started out with provision I ops and then when we notice the load we realize that we",
    "start": "2974490",
    "end": "2980280"
  },
  {
    "text": "can actually do fine with gp2 volumes do you know like what low-level you like",
    "start": "2980280",
    "end": "2987510"
  },
  {
    "text": "gp2 doesn't work for you anymore and you needed to do a higher like I think that",
    "start": "2987510",
    "end": "2992670"
  },
  {
    "text": "there is like aspects that you can look at for the EBS I don't know the exact number right now but it can burst up",
    "start": "2992670",
    "end": "2999360"
  },
  {
    "text": "maybe I don't know under if you know the in our tastes like the elasticsearch I'd of eighty thousand maybe I think like",
    "start": "2999360",
    "end": "3005480"
  },
  {
    "text": "ten to twenty thousand when you start going beyond that and you may want to look into SSDs so okay thank you all and",
    "start": "3005480",
    "end": "3018110"
  },
  {
    "text": "have a nice day if you have some other follow-up questions we're staying here for some",
    "start": "3018110",
    "end": "3023480"
  },
  {
    "text": "time so thank you guys for attending",
    "start": "3023480",
    "end": "3027520"
  }
]