[
  {
    "text": "good afternoon I'm Lisa Perez Olie and I'm a senior product manager with Amazon",
    "start": "1520",
    "end": "7500"
  },
  {
    "text": "redshift I'm delighted to be here this afternoon with equinox fitness and in",
    "start": "7500",
    "end": "12780"
  },
  {
    "text": "just a few minutes Ryan and Elliot are going to talk about their data journey and how they use redshift as their",
    "start": "12780",
    "end": "18990"
  },
  {
    "text": "modern data warehouse and s3 as their data Lake so by a show of hands how many",
    "start": "18990",
    "end": "28050"
  },
  {
    "text": "of you are already using or have played around with redshift great it's good to",
    "start": "28050",
    "end": "34320"
  },
  {
    "text": "see that most of you are using it and I'm excited to share how the service is evolving to meet your needs",
    "start": "34320",
    "end": "42260"
  },
  {
    "text": "so Equinox Fitness is a good example of a forward-looking company that's",
    "start": "45050",
    "end": "51030"
  },
  {
    "text": "evolving their analytic strategy before Ryan and Elliot's talk about their use case I want to take",
    "start": "51030",
    "end": "58170"
  },
  {
    "text": "a step back and talk about data warehouse modernization we hear firsthand from customers that you want",
    "start": "58170",
    "end": "65189"
  },
  {
    "text": "to store unstructured data at exabyte scale this includes machine generated",
    "start": "65189",
    "end": "70590"
  },
  {
    "text": "data like log file and click stream data you want a single view of all of your",
    "start": "70590",
    "end": "76200"
  },
  {
    "text": "data in a unified space in an easy way to catalog and search all this data and",
    "start": "76200",
    "end": "82110"
  },
  {
    "text": "then do analytics on top of that you want to do new types of analysis on your",
    "start": "82110",
    "end": "87840"
  },
  {
    "text": "data you want to move from answering questions about what happened in the past and move towards answering",
    "start": "87840",
    "end": "94049"
  },
  {
    "text": "questions about what could happen in the future so traditionally if you started",
    "start": "94049",
    "end": "101850"
  },
  {
    "text": "doing data warehousing back in the day you had your s AP database in your",
    "start": "101850",
    "end": "107070"
  },
  {
    "text": "Oracle data warehouse and a pipe that moved data between the two and you",
    "start": "107070",
    "end": "112439"
  },
  {
    "text": "updated it maybe once a week or if you were really cool once a night and for",
    "start": "112439",
    "end": "118320"
  },
  {
    "text": "the most part it worked pretty well so what's causing people to look at a",
    "start": "118320",
    "end": "123570"
  },
  {
    "text": "change well across a large number of organizations data typically grows 10",
    "start": "123570",
    "end": "131780"
  },
  {
    "text": "every five years so you need a system that scales up to a thousand X and that",
    "start": "131780",
    "end": "139190"
  },
  {
    "text": "leads to a whole new architecture we also hear about new requirements from",
    "start": "139190",
    "end": "144590"
  },
  {
    "text": "you because of the increase in all of this data you want the ability to store",
    "start": "144590",
    "end": "150380"
  },
  {
    "text": "and run analytics on unstructured data at exabyte scale and you've probably",
    "start": "150380",
    "end": "156260"
  },
  {
    "text": "realized by now that the traditional data warehouse was not optimized for",
    "start": "156260",
    "end": "161540"
  },
  {
    "text": "storing all of this unstructured data also there are more data types that",
    "start": "161540",
    "end": "169010"
  },
  {
    "text": "exist today than ever before you want the ability to incorporate machine",
    "start": "169010",
    "end": "174080"
  },
  {
    "text": "learning and this includes data generated from IOT devices log files",
    "start": "174080",
    "end": "179120"
  },
  {
    "text": "clickstream data data processing and real-time analytics the traditional",
    "start": "179120",
    "end": "185450"
  },
  {
    "text": "model could only accommodate reporting and ad hoc and analysis sorry an ad hoc",
    "start": "185450",
    "end": "190820"
  },
  {
    "text": "analytics on relational data and then because of this we've also seen a lot of",
    "start": "190820",
    "end": "198680"
  },
  {
    "text": "new frameworks that have all taken a stab at solving part of this problem and you really want to be able to use Hadoop",
    "start": "198680",
    "end": "206480"
  },
  {
    "text": "frameworks to compliment your data warehouse so what do we even mean when",
    "start": "206480",
    "end": "214340"
  },
  {
    "text": "we say data warehouse modernization data warehouses and data lakes are coming",
    "start": "214340",
    "end": "220010"
  },
  {
    "text": "together however you still want to analyze your data in a highly optimized",
    "start": "220010",
    "end": "225650"
  },
  {
    "text": "analytical database that has tight integration with the data link there are",
    "start": "225650",
    "end": "231410"
  },
  {
    "text": "four key pillars that we see for a modern data warehouse first it should be designed to support",
    "start": "231410",
    "end": "237769"
  },
  {
    "text": "rapid data growth and analytics over a variety of data types it should scale up",
    "start": "237769",
    "end": "244100"
  },
  {
    "text": "for increasing analytical demand and scale down to help you save cost it",
    "start": "244100",
    "end": "249440"
  },
  {
    "text": "should do this both automatically and on demand it should be consistently fast",
    "start": "249440",
    "end": "255739"
  },
  {
    "text": "even with thousands of users and concurrent queries and a modern data",
    "start": "255739",
    "end": "261109"
  },
  {
    "text": "warehouse should be easy to use you shouldn't have to waste time administrative tasks and you also want",
    "start": "261109",
    "end": "269150"
  },
  {
    "text": "to date a warehouse that extends directly to your data Lake you should be able to query across the data warehouse",
    "start": "269150",
    "end": "275780"
  },
  {
    "text": "and data link Amazon redshift is a fast",
    "start": "275780",
    "end": "281900"
  },
  {
    "text": "scalable data warehouse that makes it simple and cost effective to analyze all",
    "start": "281900",
    "end": "287780"
  },
  {
    "text": "your data across your data warehouse and your data Lake redshift delivers fast",
    "start": "287780",
    "end": "293870"
  },
  {
    "text": "performance by using machine learning massively parallel query execution and",
    "start": "293870",
    "end": "299450"
  },
  {
    "text": "columnar storage on high-performance disk you can start small with a few",
    "start": "299450",
    "end": "305000"
  },
  {
    "text": "hundred gigabytes and scale out to exabytes and unlike a traditional data",
    "start": "305000",
    "end": "310880"
  },
  {
    "text": "warehouse you don't have to commit to buying capacity and can easily try out",
    "start": "310880",
    "end": "316430"
  },
  {
    "text": "redshift for new use cases there are over 10,000 organizations that trust",
    "start": "316430",
    "end": "324110"
  },
  {
    "text": "redshift is the foundation for their analytics platform from startups to enterprises across nearly every industry",
    "start": "324110",
    "end": "331030"
  },
  {
    "text": "and you can use redshift with the third party tools you already used today or",
    "start": "331030",
    "end": "337700"
  },
  {
    "text": "the tools you plan to use in the future one differentiator for redshift is the",
    "start": "337700",
    "end": "345440"
  },
  {
    "text": "pace of innovation these results are due to our focus on performance we released",
    "start": "345440",
    "end": "351530"
  },
  {
    "text": "several under the hood enhancements such as improvement and resource management",
    "start": "351530",
    "end": "356800"
  },
  {
    "text": "query planning committee formance and short query acceleration that",
    "start": "356800",
    "end": "362210"
  },
  {
    "text": "transparently boosted performance over the past year based on these",
    "start": "362210",
    "end": "367880"
  },
  {
    "text": "improvements that we've made to redshift we've experienced a 3x improvement in speed compared to where redshift was out",
    "start": "367880",
    "end": "374870"
  },
  {
    "text": "just six months ago we're always looking",
    "start": "374870",
    "end": "380330"
  },
  {
    "text": "for ways to make redshift even easier to use we recently launched query editor",
    "start": "380330",
    "end": "386060"
  },
  {
    "text": "which allows you to query data in your redshift cluster directly from the AWS management console",
    "start": "386060",
    "end": "392830"
  },
  {
    "text": "this provides an easy way for users to run sequel queries without",
    "start": "392830",
    "end": "398129"
  },
  {
    "text": "having to install and setup a JDBC ODBC client and the query results are",
    "start": "398129",
    "end": "404550"
  },
  {
    "text": "instantly visible within the console advisor is another relatively new",
    "start": "404550",
    "end": "411779"
  },
  {
    "text": "feature that provides automated recommendations to help you optimize database performance and decrease",
    "start": "411779",
    "end": "418139"
  },
  {
    "text": "operating costs advisor is available via the redshift console at no additional",
    "start": "418139",
    "end": "423809"
  },
  {
    "text": "charge and you can think of advisor as a personal database assistant that",
    "start": "423809",
    "end": "429389"
  },
  {
    "text": "generates tailored recommendations to your operations and cluster configurations based on analyzing your",
    "start": "429389",
    "end": "436259"
  },
  {
    "text": "clusters performance in usage metrics",
    "start": "436259",
    "end": "440240"
  },
  {
    "text": "we're also working to move towards a zero maintenance world so we want to",
    "start": "441439",
    "end": "447330"
  },
  {
    "text": "completely remove the need for you to do any tuning or administration with the",
    "start": "447330",
    "end": "453029"
  },
  {
    "text": "new auto analyze feature redshift will automatically update the table statistics in on your cluster and then",
    "start": "453029",
    "end": "463219"
  },
  {
    "text": "the vacuum command reclaimed space in the specified table or database and",
    "start": "463219",
    "end": "469469"
  },
  {
    "text": "previously this was a manual operation that you had to go in and think about and what we're doing now is we're",
    "start": "469469",
    "end": "476459"
  },
  {
    "text": "planning to automatically run the vacuum operation in the background so you don't have to spend time thinking about it",
    "start": "476459",
    "end": "483769"
  },
  {
    "text": "we're also automating the workload manager concurrency setting that",
    "start": "483769",
    "end": "488999"
  },
  {
    "text": "determines the number of parallel queries so it will automatically deliver",
    "start": "488999",
    "end": "494009"
  },
  {
    "text": "optimal throughput for your changing workloads",
    "start": "494009",
    "end": "498709"
  },
  {
    "text": "and then many many redshift customers",
    "start": "502420",
    "end": "507760"
  },
  {
    "text": "have workloads which need additional compute and storage resources to meet their changing and growing workloads so",
    "start": "507760",
    "end": "514930"
  },
  {
    "text": "for example you might have an ETL workload that runs for certain hours in a day or you're gonna have month and",
    "start": "514930",
    "end": "521919"
  },
  {
    "text": "reporting and you know you're gonna have a demanding workload and need more resources with redshift you can scale",
    "start": "521920",
    "end": "528550"
  },
  {
    "text": "quickly in a couple of ways first you can query your data directly",
    "start": "528550",
    "end": "533650"
  },
  {
    "text": "in your s3 data Lake with the spectrum feature without needing to load it in the cluster so this flexibility lets you",
    "start": "533650",
    "end": "541780"
  },
  {
    "text": "analyze growing data volumes without waiting for ETL jobs or adding more storage capacity we also recently",
    "start": "541780",
    "end": "550330"
  },
  {
    "text": "announced elastic resize which is a new feature which allows scaling your",
    "start": "550330",
    "end": "555340"
  },
  {
    "text": "redshift cluster up or down in minutes to get better performance and more storage for demanding workloads and it",
    "start": "555340",
    "end": "563410"
  },
  {
    "text": "removes nodes when they are no longer needed to save cost so when you enable",
    "start": "563410",
    "end": "569050"
  },
  {
    "text": "elastic resize redshift adds additional nodes to the cluster and distributes the",
    "start": "569050",
    "end": "575410"
  },
  {
    "text": "data across the new configuration within minutes this feature is the significant",
    "start": "575410",
    "end": "581380"
  },
  {
    "text": "improvement over the traditional classic resize in which you had to provision a new cluster and this could potentially",
    "start": "581380",
    "end": "587920"
  },
  {
    "text": "take several hours with the last degree size you can easily increase the size of",
    "start": "587920",
    "end": "594160"
  },
  {
    "text": "your clusters compute and storage and quickly process complex workloads and",
    "start": "594160",
    "end": "599200"
  },
  {
    "text": "decrease your query response time concurrency scaling is another new",
    "start": "599200",
    "end": "606370"
  },
  {
    "text": "feature we recently announced and it adds transient capacity when needed to handle heavy demand from concurrent",
    "start": "606370",
    "end": "613390"
  },
  {
    "text": "users and queries so a typical data warehouse has changes in concurrent",
    "start": "613390",
    "end": "619090"
  },
  {
    "text": "query usage over the course of a day and with concurrency scaling you simply",
    "start": "619090",
    "end": "624700"
  },
  {
    "text": "configure how many clusters you would like to use and the AWS management console and redshift automatically",
    "start": "624700",
    "end": "631810"
  },
  {
    "text": "ensures they're ready to handle additional users as they arrived and then due to the performance",
    "start": "631810",
    "end": "638790"
  },
  {
    "text": "improvements that we previously discussed what we saw was that 87 percent of the current customers don't",
    "start": "638790",
    "end": "645780"
  },
  {
    "text": "have any significant query wait times so they don't really need concurrency beyond what the main cluster already",
    "start": "645780",
    "end": "652050"
  },
  {
    "text": "provides but for every 24 hours that your main cluster is in use",
    "start": "652050",
    "end": "657510"
  },
  {
    "text": "you'll accrue a one hour credit for concurrency scaling what this means is",
    "start": "657510",
    "end": "663600"
  },
  {
    "text": "that concurrency scaling will be free for more than 97% of customers and then",
    "start": "663600",
    "end": "672390"
  },
  {
    "text": "keep in mind that it's not just about the data warehouse but also about the",
    "start": "672390",
    "end": "677730"
  },
  {
    "text": "transition to data Lakes moving to a data Lake can have the greatest single",
    "start": "677730",
    "end": "683100"
  },
  {
    "text": "impact on your long-term storage management and analysis capabilities",
    "start": "683100",
    "end": "690230"
  },
  {
    "text": "data Lake have caught on really quickly and for good reason they allow you to store all of your data",
    "start": "690230",
    "end": "696780"
  },
  {
    "text": "in a unified single place where you can collect and store it any amount of data",
    "start": "696780",
    "end": "702090"
  },
  {
    "text": "at any scale in all at a low cost one of the advantages of a data Lake is that",
    "start": "702090",
    "end": "708330"
  },
  {
    "text": "you can run a variety of analytic tools against it so I like to think of a data",
    "start": "708330",
    "end": "715680"
  },
  {
    "text": "Lake in terms of concentric circles let's assume that you put your data into",
    "start": "715680",
    "end": "721050"
  },
  {
    "text": "s3 or an object store of your choice you want a system that can ingest quickly",
    "start": "721050",
    "end": "727020"
  },
  {
    "text": "and there are a lot of ways to get data into s3 once your data is catalogue once",
    "start": "727020",
    "end": "733530"
  },
  {
    "text": "your data is in s3 you want a way to easily understand what data you have in",
    "start": "733530",
    "end": "738750"
  },
  {
    "text": "the lake and we offer a service called AWS glue that automatically crawls in",
    "start": "738750",
    "end": "744900"
  },
  {
    "text": "catalogs your data so you're not stuck doing all of this out of this manual data entry and then once catalogued with",
    "start": "744900",
    "end": "753240"
  },
  {
    "text": "glue your data is immediately searchable and queryable so once you know what data",
    "start": "753240",
    "end": "760620"
  },
  {
    "text": "is stored in the lake you can start to run your query processing engines on top",
    "start": "760620",
    "end": "765900"
  },
  {
    "text": "of that and it might be a service like Athena for ad-hoc queries",
    "start": "765900",
    "end": "772199"
  },
  {
    "text": "that are done once in a while you might use Amazon redshift for your bread-and-butter data warehousing it",
    "start": "772199",
    "end": "778679"
  },
  {
    "text": "might be EMR to process complex SPARC and Hadoop jobs the differentiator for a",
    "start": "778679",
    "end": "786779"
  },
  {
    "text": "data Lake is that it really allows you to run the best analytic tool for your the job and you want that flexibility",
    "start": "786779",
    "end": "793949"
  },
  {
    "text": "and then most importantly you want to ensure that your data Lake is secure and",
    "start": "793949",
    "end": "800009"
  },
  {
    "text": "with AWS security really is our top priority with the s3 data Lake model you can encrypt",
    "start": "800009",
    "end": "806759"
  },
  {
    "text": "all of your data with AWS kms you've",
    "start": "806759",
    "end": "812999"
  },
  {
    "text": "probably all heard the phrase data has gravity it's hard to change once you",
    "start": "812999",
    "end": "818970"
  },
  {
    "text": "have a lot of data as you move to a data Lake model you want a system that lets",
    "start": "818970",
    "end": "824850"
  },
  {
    "text": "you store your data in open formats JSON park' or RC etc and not in a vendors",
    "start": "824850",
    "end": "832290"
  },
  {
    "text": "proprietary format you want to use open data formats in open api's this is",
    "start": "832290",
    "end": "839669"
  },
  {
    "text": "because the decision you're making is a long-term decision open data formats",
    "start": "839669",
    "end": "844799"
  },
  {
    "text": "allow you to query the data with the best tool for the job they also eliminate data silos and",
    "start": "844799",
    "end": "851100"
  },
  {
    "text": "lock-ins and they allow you to bring in new analytic engines or change your engines",
    "start": "851100",
    "end": "856470"
  },
  {
    "text": "ensuring that your solution is truly built for the future customers choose",
    "start": "856470",
    "end": "864600"
  },
  {
    "text": "Amazon redshift over traditional data warehouses because it really does provide fast performance at any scale",
    "start": "864600",
    "end": "872249"
  },
  {
    "text": "for any workload complexity and it provides deep integration with the data",
    "start": "872249",
    "end": "877619"
  },
  {
    "text": "Lake so with that I like to turn it over to Ryan and Elliott from Equinox Fitness",
    "start": "877619",
    "end": "883679"
  },
  {
    "text": "to talk about why they chose Amazon redshift as their data warehouse and s3",
    "start": "883679",
    "end": "889379"
  },
  {
    "text": "as their data Lake thank you thank you",
    "start": "889379",
    "end": "894499"
  },
  {
    "text": "hi everyone I'm Elliot and this is Ryan and we're with Equinox so a little bit",
    "start": "897430",
    "end": "904339"
  },
  {
    "text": "about equinox for those of you who are not members yet okay",
    "start": "904339",
    "end": "911540"
  },
  {
    "text": "the equinox is a company with integrated luxury and lifestyles offering centered",
    "start": "911540",
    "end": "917450"
  },
  {
    "text": "around movement nutrition and regeneration so that encompasses the brick-and-mortar Equinox brand which is",
    "start": "917450",
    "end": "923029"
  },
  {
    "text": "our flagship blink which is another fitness offering Pure Yoga soul cycle",
    "start": "923029",
    "end": "929630"
  },
  {
    "text": "which is a boutique high end studio cycling experience furthermore in Equinox hotels and we'll be opening our",
    "start": "929630",
    "end": "936529"
  },
  {
    "text": "first hotel in Hudson Yards in New York City in this coming spring and we have several other fantastic properties and",
    "start": "936529",
    "end": "942649"
  },
  {
    "text": "development as well so total we operate approximately 250 locations across every",
    "start": "942649",
    "end": "949370"
  },
  {
    "text": "major city in the US as well as Canada and the UK so you may be thinking your",
    "start": "949370",
    "end": "958760"
  },
  {
    "text": "gym how hard could it be people who come check in to the club lift weight put it down sometimes very",
    "start": "958760",
    "end": "964640"
  },
  {
    "text": "loudly we apologize to Men's Wearhouse for shaking the building sometimes Bottomly Elliott how hard could it be",
    "start": "964640",
    "end": "970750"
  },
  {
    "text": "yeah so we're a prime example of you can make anything complicated if you try hard enough",
    "start": "970750",
    "end": "976279"
  },
  {
    "text": "so aside from our scale again like 250 brick-and-mortar locations 98 just for",
    "start": "976279",
    "end": "982730"
  },
  {
    "text": "our flagship Equinox brand which has 14,000 employees each one of our gyms",
    "start": "982730",
    "end": "987920"
  },
  {
    "text": "has a series of lines of business within it you know over a dozen lines of business ranging from personal training",
    "start": "987920",
    "end": "994070"
  },
  {
    "text": "to group fitness to membership to spa where we operate the nation's largest",
    "start": "994070",
    "end": "1000190"
  },
  {
    "text": "corporate operated spa chain we have heavy investments in digital and we have all the supporting functions that you'd",
    "start": "1000190",
    "end": "1006880"
  },
  {
    "text": "expect from a large company from CRM to finance to member services not only that",
    "start": "1006880",
    "end": "1013300"
  },
  {
    "text": "more and more of it is becoming connected we have our digital products which are servicing end users so these",
    "start": "1013300",
    "end": "1019660"
  },
  {
    "text": "end user applications are used by people in corporate positions we have people in the gyms who are using them maybe it's",
    "start": "1019660",
    "end": "1025870"
  },
  {
    "text": "the front desk maybe it's our personal trainers maybe it's somebody who is a membership advisor or works in the spa but not only",
    "start": "1025870",
    "end": "1032589"
  },
  {
    "text": "that we also have our members who are using our applications as well in addition to that we have connected to",
    "start": "1032589",
    "end": "1038199"
  },
  {
    "text": "equipment in the gym itself so persued which is our gamified cycling experience is a spin class that actually collects",
    "start": "1038199",
    "end": "1045399"
  },
  {
    "text": "data from the bikes in real time and shows the performance on screens for friendly competition in addition to that",
    "start": "1045399",
    "end": "1052179"
  },
  {
    "text": "we actually have a freshly lawns digital assessment program where you can get a fitness assessment in a room with a",
    "start": "1052179",
    "end": "1058059"
  },
  {
    "text": "digital experience to accompany it so a little bit about our data journey and",
    "start": "1058059",
    "end": "1064239"
  },
  {
    "text": "then we'll hop into the new stuff so you know we our first data warehouse was",
    "start": "1064239",
    "end": "1069700"
  },
  {
    "text": "named life you know our company logo is it's not fitness its life so it was",
    "start": "1069700",
    "end": "1076389"
  },
  {
    "text": "created and launched somewhere around 2008 like most traditional data warehouses it was cylindrical in shape",
    "start": "1076389",
    "end": "1083190"
  },
  {
    "text": "it was a very traditional data warehouse we used informatica it ran on sequel",
    "start": "1083190",
    "end": "1090340"
  },
  {
    "text": "server it was rigorously kimball we followed the book and made sure we did everything you know as perfect as",
    "start": "1090340",
    "end": "1098049"
  },
  {
    "text": "possible from a Kimball methodology perspective and the results were pretty good you know it was a solid methodology",
    "start": "1098049",
    "end": "1105429"
  },
  {
    "text": "decent technology at its time reporting was reliable we had a growing analyst",
    "start": "1105429",
    "end": "1110739"
  },
  {
    "text": "community and letter signed data science community we were able to do really cool things in terms of advanced crm email",
    "start": "1110739",
    "end": "1117879"
  },
  {
    "text": "marketing personalization but all good things come to an end sometimes life is",
    "start": "1117879",
    "end": "1126009"
  },
  {
    "text": "bad we ended up with a lot of direct integration into our data warehouse the",
    "start": "1126009",
    "end": "1132100"
  },
  {
    "text": "reason this was is because like many data warehouses we end up with a lot of tight coupling with other applications it's the first place where facts about",
    "start": "1132100",
    "end": "1138639"
  },
  {
    "text": "your customers are born it's the first place where data is brought together from various systems so people want to",
    "start": "1138639",
    "end": "1144580"
  },
  {
    "text": "reach in from different applications and use that data and maybe use it in a way that you didn't intend that along with a",
    "start": "1144580",
    "end": "1153009"
  },
  {
    "text": "difficult sdlc you know so we weren't able to really adopt like modern like C",
    "start": "1153009",
    "end": "1158559"
  },
  {
    "text": "ICD principles because we had fixed and structure made it very difficult to you",
    "start": "1158559",
    "end": "1163700"
  },
  {
    "text": "do automated testing most of our testing was manual and exploratory this plus the direct the tight coupling where our",
    "start": "1163700",
    "end": "1170660"
  },
  {
    "text": "partners and our didn't want us to change our system we ended up with a lot of functional debt and then we had the",
    "start": "1170660",
    "end": "1176510"
  },
  {
    "text": "new problems of not having a place to put new data data from our clickstream or are cycling logs or you know",
    "start": "1176510",
    "end": "1184100"
  },
  {
    "text": "different marketing data that we had access to and this created an flexibility for analysts and our data",
    "start": "1184100",
    "end": "1190190"
  },
  {
    "text": "scientists and also we're just running like expensive commercial software a decent amount of expensive commercial",
    "start": "1190190",
    "end": "1196250"
  },
  {
    "text": "software so we decided to spend more money on a commercial solution so about",
    "start": "1196250",
    "end": "1203240"
  },
  {
    "text": "five years ago now we purchased teradata to really help with some of our workload problems as well as to help us mitigate",
    "start": "1203240",
    "end": "1211610"
  },
  {
    "text": "the fact that we had an overgrown like very large multi terabyte sequel server database so we launched several apps and",
    "start": "1211610",
    "end": "1218630"
  },
  {
    "text": "data and we just ran into like a lot of challenges first it was pretty difficult to build on tera data we found that he",
    "start": "1218630",
    "end": "1224540"
  },
  {
    "text": "needed a lot of platform specific knowledge a lot of our engineers had to really figure out how to interact with the platform we were at the point that",
    "start": "1224540",
    "end": "1231080"
  },
  {
    "text": "we were gonna hire like at teradata DBA and we found that we had like a lot of problems with integrations as well we",
    "start": "1231080",
    "end": "1237350"
  },
  {
    "text": "wanted to integrate with the cloud you know kind of be able to integrate what other systems very easily and that",
    "start": "1237350",
    "end": "1242540"
  },
  {
    "text": "wasn't possible without buying more expensive integrations and overall a system was very expensive aside from the",
    "start": "1242540",
    "end": "1249650"
  },
  {
    "text": "initial product purchase which was very expensive the maintenance and licensing",
    "start": "1249650",
    "end": "1255350"
  },
  {
    "text": "of Tara data is quite oppressive as well so we decided to backup first I can and",
    "start": "1255350",
    "end": "1261320"
  },
  {
    "text": "think about what we're trying to accomplish figure out if this is the right approach after we got so far into the project we decided we wanted to",
    "start": "1261320",
    "end": "1268640"
  },
  {
    "text": "build technology that differentiates and provide business value we just didn't want to get stuck doing an infrastructure project for several years",
    "start": "1268640",
    "end": "1275210"
  },
  {
    "text": "that nobody in the business really cared about we wanted to reduce costs and go all in in the cloud and adopt modern",
    "start": "1275210",
    "end": "1281660"
  },
  {
    "text": "engineering principles the rest of our tech organization had already started to do this with heavy investments and we",
    "start": "1281660",
    "end": "1289070"
  },
  {
    "text": "were not so we wanted to make everything scalable we wanted to use ephemeral resources",
    "start": "1289070",
    "end": "1294780"
  },
  {
    "text": "get into distributed databases and worry less about individual servers so at this",
    "start": "1294780",
    "end": "1302280"
  },
  {
    "text": "point Elliott what if we just threw everything in a daily and made everything late find do we even need a data warehouse would that work",
    "start": "1302280",
    "end": "1308910"
  },
  {
    "text": "a very good question so so you know we",
    "start": "1308910",
    "end": "1314280"
  },
  {
    "text": "tend the time to think about data warehouses versus data lakes like this so data lakes are really good for large",
    "start": "1314280",
    "end": "1319950"
  },
  {
    "text": "immutable data sets semi structured data and you know unstructured data sets log",
    "start": "1319950",
    "end": "1325650"
  },
  {
    "text": "data data warehouses on the other hand or you know I find to be useful for high",
    "start": "1325650",
    "end": "1331830"
  },
  {
    "text": "SLA report reporting they provide generally better performance they are very perform developer and",
    "start": "1331830",
    "end": "1338790"
  },
  {
    "text": "analyst friendly they are very familiar very easy to work with in the case of redshift it's Postgres like and it's",
    "start": "1338790",
    "end": "1347250"
  },
  {
    "text": "efficient for specific types of pipelines so if you have a data pipeline which is mostly immutable log data click",
    "start": "1347250",
    "end": "1353610"
  },
  {
    "text": "stream data data lakes are very easy if you have customer data and you have systems that allow update statements on",
    "start": "1353610",
    "end": "1360750"
  },
  {
    "text": "the data the immutable data that stuff is pretty challenging to accomplish in blob storage within a data Lake so",
    "start": "1360750",
    "end": "1367020"
  },
  {
    "text": "immutable data is very easy to work with within a data warehouse application so",
    "start": "1367020",
    "end": "1374570"
  },
  {
    "text": "we're at a crossroads we decided to do a two-week proof-of-concept using Amazon",
    "start": "1374570",
    "end": "1379650"
  },
  {
    "text": "redshift and s3 I had a decent amount of experience with Amazon redshift than s3",
    "start": "1379650",
    "end": "1384690"
  },
  {
    "text": "so it was an obvious choice for us so we re platformed one Terra dated app to an",
    "start": "1384690",
    "end": "1392670"
  },
  {
    "text": "AWS solution in our spare time buy on our own and it worked it was successful",
    "start": "1392670",
    "end": "1399240"
  },
  {
    "text": "and we were really happy with the results so we decided to make a decision",
    "start": "1399240",
    "end": "1405450"
  },
  {
    "text": "and unfortunately we couldn't find any buyers for our turret a cluster anywhere and it ended up going to technology",
    "start": "1405450",
    "end": "1411900"
  },
  {
    "text": "Salvage so a little bit on our move to",
    "start": "1411900",
    "end": "1418140"
  },
  {
    "text": "Amazon redshift so our new system is called Jarvis it is",
    "start": "1418140",
    "end": "1426759"
  },
  {
    "text": "a data warehouse and data lake system as well as a series of data services and",
    "start": "1426759",
    "end": "1432429"
  },
  {
    "text": "data Mart's so our main data warehouse is platformed on Amazon redshift that's why we're here our data Lake on Amazon",
    "start": "1432429",
    "end": "1438669"
  },
  {
    "text": "s3 and using a number of services to process and interact with data and so",
    "start": "1438669",
    "end": "1445840"
  },
  {
    "text": "the way that we've laid this out is you know from left to right here all of the data that we're bringing in we have a",
    "start": "1445840",
    "end": "1451509"
  },
  {
    "text": "number of different methods on how we actually want to deal with that data so in instances where we want to essentially lift that data and move it",
    "start": "1451509",
    "end": "1458080"
  },
  {
    "text": "over to Amazon redshift we use our own Maximillian platform or will use informatica for that and the main goal",
    "start": "1458080",
    "end": "1464259"
  },
  {
    "text": "there is to ultimately use ELT so that once it gets into redshift we can then transform it and then put it in another",
    "start": "1464259",
    "end": "1470769"
  },
  {
    "text": "table there for things that are larger crunches well we want to use here is we kind of take we want to take that query",
    "start": "1470769",
    "end": "1477309"
  },
  {
    "text": "pick capacity away from redshift and say let's actually do that on EMR and run it there and so we'll transform at any",
    "start": "1477309",
    "end": "1483519"
  },
  {
    "text": "Ammar and then place that in red shoot for other data sources where we leverage the data leak we'll go ahead and move",
    "start": "1483519",
    "end": "1489309"
  },
  {
    "text": "that data into the data leak and then we can also provide some transformations using Redford redshift spectrum and then",
    "start": "1489309",
    "end": "1495369"
  },
  {
    "text": "at the interior we have the presentation layer so these are you know services or applications now you're going to want to",
    "start": "1495369",
    "end": "1501070"
  },
  {
    "text": "reach in and get that data and so we've essentially done here has created agreements or contracts with these",
    "start": "1501070",
    "end": "1507609"
  },
  {
    "text": "applications to say here is the Mart the data Mart the API the Postgres database",
    "start": "1507609",
    "end": "1512700"
  },
  {
    "text": "that you'll want to use based on your you know specific use case and so those",
    "start": "1512700",
    "end": "1519369"
  },
  {
    "text": "are the things that we'll set up and we'll place data there so they can reach into those services instead there are some instances where our analyst wall",
    "start": "1519369",
    "end": "1525999"
  },
  {
    "text": "goes straight into redshift or we'll have VI tools read from there but ultimately we really try and focus on",
    "start": "1525999",
    "end": "1531549"
  },
  {
    "text": "creating those contracts and agreements and underlying all this we have our data monitoring and quality services that we",
    "start": "1531549",
    "end": "1537159"
  },
  {
    "text": "built so in terms of redshift benefit benefits obviously cost-effective so",
    "start": "1537159",
    "end": "1544379"
  },
  {
    "text": "just not a not considering the initial purchase price of Tara data we are",
    "start": "1544379",
    "end": "1550659"
  },
  {
    "text": "essentially paying one tenth of the cost of Tara data and sequel server just in licensing and me",
    "start": "1550659",
    "end": "1555830"
  },
  {
    "text": "alone so huge cost savings we've had very low barrier of entry for developers",
    "start": "1555830",
    "end": "1561350"
  },
  {
    "text": "and found that redshift is very easy to maintain we do not have a redshift DBA",
    "start": "1561350",
    "end": "1566799"
  },
  {
    "text": "and our developers are very productive it's very familiar to them they find it",
    "start": "1566799",
    "end": "1572390"
  },
  {
    "text": "very easy to work with fast and performance so we had workloads that",
    "start": "1572390",
    "end": "1578000"
  },
  {
    "text": "were running in sequel server or informatica that we're taking you know four or five hours to complete these now",
    "start": "1578000",
    "end": "1584059"
  },
  {
    "text": "are done in spark in EMR as well as redshift spectrum the pipeline's are",
    "start": "1584059",
    "end": "1589159"
  },
  {
    "text": "taking at max like 15 minutes for our like biggest crunch something that we",
    "start": "1589159",
    "end": "1594830"
  },
  {
    "text": "really love about AWS and about redshift is they create an API to backup every service that allows you to interact with",
    "start": "1594830",
    "end": "1600200"
  },
  {
    "text": "it so we found it very DevOps friendly and we're gonna talk a little bit about the automation and stuff that we've been",
    "start": "1600200",
    "end": "1606019"
  },
  {
    "text": "able to do with this but we're able to do really interesting things in terms of multi cluster deployment automated",
    "start": "1606019",
    "end": "1613340"
  },
  {
    "text": "regression testing of our data platform and just automating things to make",
    "start": "1613340",
    "end": "1618919"
  },
  {
    "text": "things really efficient and reduce sources admin overhead and then of course like other things in AWS",
    "start": "1618919",
    "end": "1625220"
  },
  {
    "text": "integration what other AWS services so there's a lot of really easy integrations that you can do with",
    "start": "1625220",
    "end": "1631580"
  },
  {
    "text": "various AWS services with redshift and",
    "start": "1631580",
    "end": "1636980"
  },
  {
    "text": "so with s3 and with our data link we we just wonder how I'd seen benefits that we've seen there as well so besides",
    "start": "1636980",
    "end": "1643940"
  },
  {
    "text": "being highly performant and a low cost blob storage it actually is a great functioning analytics store when you",
    "start": "1643940",
    "end": "1650000"
  },
  {
    "text": "have good strategies in place it's very tempting to once you throw data in there want to throw everything in there but",
    "start": "1650000",
    "end": "1655250"
  },
  {
    "text": "you just need to make sure that you have good strategies and then you can really make it a functioning analytics store the other thing is that it's extremely",
    "start": "1655250",
    "end": "1661159"
  },
  {
    "text": "flexible so when you're bringing this data in you can describe it at any point that you want as long as you're bring it",
    "start": "1661159",
    "end": "1667370"
  },
  {
    "text": "all in you can use a late bind strategy and describe it at a later point for setting up",
    "start": "1667370",
    "end": "1673460"
  },
  {
    "text": "external tables and making it queryable data it's actually a very fast setup for it we've actually had instances where",
    "start": "1673460",
    "end": "1679070"
  },
  {
    "text": "our data scientists have asked for new data and we've been able to set up an external table on top of that data",
    "start": "1679070",
    "end": "1684350"
  },
  {
    "text": "within a matter of minutes and then of course for any disaster recovery it's actually very easy to set",
    "start": "1684350",
    "end": "1689750"
  },
  {
    "text": "that out it's just a configuration option that you can set in the bucket itself and so I just want to describe",
    "start": "1689750",
    "end": "1697580"
  },
  {
    "text": "some of the data that we actually have in the data Lake the very first one here is what really kicked it off for us and",
    "start": "1697580",
    "end": "1702950"
  },
  {
    "text": "that's the clickstream data originally what we had done is we had set that in red shipped itself and then we said",
    "start": "1702950",
    "end": "1708710"
  },
  {
    "text": "here's about 20 to 30 columns worth of data that the analyst can use to enrich their other data sets what we found is",
    "start": "1708710",
    "end": "1714800"
  },
  {
    "text": "that they actually liked it so much they wanted more and this causes us to have to essentially add more columns in and",
    "start": "1714800",
    "end": "1720800"
  },
  {
    "text": "then recast that data historically but the older the data God and the further back in time we had to recast it just",
    "start": "1720800",
    "end": "1727070"
  },
  {
    "text": "add it on now on to it so what we did is we ended up putting that into s3 and",
    "start": "1727070",
    "end": "1732680"
  },
  {
    "text": "then using that late buying strategy to say okay we already have all the data there let's go ahead and describe a new",
    "start": "1732680",
    "end": "1737750"
  },
  {
    "text": "column the other benefit was anytime there was an evolution in the schema we",
    "start": "1737750",
    "end": "1742880"
  },
  {
    "text": "would have to find a way to you know figure out how to deal with that we actually had an instance where adobe",
    "start": "1742880",
    "end": "1748220"
  },
  {
    "text": "analytics our quick stream provider deprecated the column and then it ended up changing the shape of the data itself",
    "start": "1748220",
    "end": "1753260"
  },
  {
    "text": "and so the way that we had the data described it was now offset and so that was something we had to accommodate and",
    "start": "1753260",
    "end": "1758810"
  },
  {
    "text": "so because of you know these problems that we kind of had and we went to the data lake we actually found a lot of",
    "start": "1758810",
    "end": "1765260"
  },
  {
    "text": "success with it and then we started putting more and more data in there so this includes our persued cycling logs that i described earlier",
    "start": "1765260",
    "end": "1771350"
  },
  {
    "text": "which this one's actually really cool because each of these bikes is sending about six data points per second so",
    "start": "1771350",
    "end": "1777050"
  },
  {
    "text": "across 40 50 bikes in a class for about 45 50 minutes happening multiple times a",
    "start": "1777050",
    "end": "1783260"
  },
  {
    "text": "day across ninety-eight clubs you end up with hundreds of millions of data points there we're also bringing in logs from",
    "start": "1783260",
    "end": "1789770"
  },
  {
    "text": "our club management software and really any other data services that kind of enhance what we do at equinox and so I",
    "start": "1789770",
    "end": "1798200"
  },
  {
    "text": "actually want to take you through a data Lake setup example this is what we did for our clickstream data and I just want",
    "start": "1798200",
    "end": "1803870"
  },
  {
    "text": "to show you how easy it really is so just to take a quick step back the",
    "start": "1803870",
    "end": "1809090"
  },
  {
    "text": "toolkit that you'll be using is just a couple of things here so at the very bottom of it the foundation of",
    "start": "1809090",
    "end": "1814460"
  },
  {
    "text": "everything is going to be s3 so once I date is in there you can and say I'm gonna describe this data and",
    "start": "1814460",
    "end": "1820450"
  },
  {
    "text": "eight of us glue and once it's described you actually have a number of tools that you can use in order to query the data",
    "start": "1820450",
    "end": "1825940"
  },
  {
    "text": "that can be Amazon thena which is a server list querying tool it can be redshift spectrum which allows you to",
    "start": "1825940",
    "end": "1831700"
  },
  {
    "text": "query data in s3 in redshift where we can be Amazon EMR which is elastic MapReduce and so getting that data in",
    "start": "1831700",
    "end": "1841289"
  },
  {
    "text": "this was actually a very easy setup for us so Adobe Analytics had an option to send data to s3 so it was just a matter",
    "start": "1841289",
    "end": "1848110"
  },
  {
    "text": "of specifying our credentials I'm telling him which bucket to send the data to and then we got the files daily",
    "start": "1848110",
    "end": "1853470"
  },
  {
    "text": "we weren't really sure how the files were gonna get sent to us so what we ended up finding out was that each of",
    "start": "1853470",
    "end": "1859240"
  },
  {
    "text": "these files that came in there would be multiple sets of them so we had multiple data files we had multiple lookup files",
    "start": "1859240",
    "end": "1864940"
  },
  {
    "text": "and then we had a manifest file that told us everything that they sent which is actually pretty nice for us so what",
    "start": "1864940",
    "end": "1873309"
  },
  {
    "text": "we decided to do is we said well let's go and land everything into our data Lake we'll send it in raw and then we'll",
    "start": "1873309",
    "end": "1880960"
  },
  {
    "text": "launch up EMR to read that manifest file because that's going to know each and every file that it needs to access from",
    "start": "1880960",
    "end": "1887169"
  },
  {
    "text": "there it's going to look to collapse everything into one pristine data file and save that in our data Lake and so we",
    "start": "1887169",
    "end": "1893950"
  },
  {
    "text": "decided to say that this part K because it's a self-describing columnar storage file format very similar to how redshift",
    "start": "1893950",
    "end": "1900279"
  },
  {
    "text": "stores its data something that we also decided to do here was leverage or partitioning strategy and so you can see",
    "start": "1900279",
    "end": "1906580"
  },
  {
    "text": "that it's a key value pair and that's actually all the folders named so because we're getting these files daily",
    "start": "1906580",
    "end": "1911700"
  },
  {
    "text": "that's how we partition the data so we did DT for date and then a equals the",
    "start": "1911700",
    "end": "1916749"
  },
  {
    "text": "Year month and day and the whole benefit here is that when you partition your data you're actually reducing the amount",
    "start": "1916749",
    "end": "1922240"
  },
  {
    "text": "of data that needs to be scanned so when you query it you're not Korean terabytes at a time but maybe mere megabytes or",
    "start": "1922240",
    "end": "1927879"
  },
  {
    "text": "gigabytes so once that data's in s3 you need to describe it this was a two-part",
    "start": "1927879",
    "end": "1934389"
  },
  {
    "text": "process that we had to do and so what you have to do first is just set up your database you can just click add database",
    "start": "1934389",
    "end": "1940600"
  },
  {
    "text": "type in the name that you want and then hit save the next step is to actually",
    "start": "1940600",
    "end": "1946990"
  },
  {
    "text": "define the data so through the interface itself it's mere four steps",
    "start": "1946990",
    "end": "1952120"
  },
  {
    "text": "in just an add table and then point that data source to a bucket and s3 and then",
    "start": "1952120",
    "end": "1958120"
  },
  {
    "text": "you can define your schema so you're gonna find the columns that you want and the data types for those columns and as",
    "start": "1958120",
    "end": "1963820"
  },
  {
    "text": "mentioned if you ever want to add one it's very easy to do so you just go in edit the schema add one and then hit",
    "start": "1963820",
    "end": "1969040"
  },
  {
    "text": "save and that it becomes queryable automatically you'll notice at the very bottom here the very last item actually",
    "start": "1969040",
    "end": "1975430"
  },
  {
    "text": "has a value and that last column on the right and then just says partition there",
    "start": "1975430",
    "end": "1980590"
  },
  {
    "text": "and so what it actually is is its dt and so even though dt wasn't actually in the data itself it becomes a column which",
    "start": "1980590",
    "end": "1987910"
  },
  {
    "text": "you can query on because it's in that folder structure you can also create external tables through amazon athena or",
    "start": "1987910",
    "end": "1994390"
  },
  {
    "text": "through redshift itself but just running a create external table command and so",
    "start": "1994390",
    "end": "2001560"
  },
  {
    "text": "this was our full description of our table for a quick stream so we had the name of the database name of the table",
    "start": "2001560",
    "end": "2007530"
  },
  {
    "text": "the location in s3 that we pointed it to and then each and every one of our definitions at this point we have about",
    "start": "2007530",
    "end": "2014250"
  },
  {
    "text": "66 columns to find but ever since we started moving over to this you know",
    "start": "2014250",
    "end": "2020070"
  },
  {
    "text": "life's been a lot happier for us so if that schema ever changes we just need to add a new column and then point it to",
    "start": "2020070",
    "end": "2025590"
  },
  {
    "text": "that and then it'll take care of that or if a column gets deprecated you can still have it named in there and it",
    "start": "2025590",
    "end": "2030600"
  },
  {
    "text": "won't break when you go to query it and so here's the assemble pipeline so we",
    "start": "2030600",
    "end": "2036540"
  },
  {
    "text": "bring that data in from Adobe Analytics it lands in CSV with the manifest file as well we transform that to parque",
    "start": "2036540",
    "end": "2043410"
  },
  {
    "text": "using a mr wheel and those files into our data light and its pristine file",
    "start": "2043410",
    "end": "2048810"
  },
  {
    "text": "format and barking we then run an alter table command on that which i haven't touched on yet but i won the next slide",
    "start": "2048810",
    "end": "2054840"
  },
  {
    "text": "and then we allow glue to sit there and define the data which then allows us to query it and redshift and so you may be",
    "start": "2054840",
    "end": "2063720"
  },
  {
    "text": "thinking well why did you run that alter table command so anytime that you partitioned table partition tables the",
    "start": "2063720",
    "end": "2071220"
  },
  {
    "text": "data basically needs to be described in blue it needs to know that you've added another folder of data so what you can",
    "start": "2071220",
    "end": "2078720"
  },
  {
    "text": "do is you can actually run the crawler at whatever cadence you want there's number of different options and that'll",
    "start": "2078720",
    "end": "2084240"
  },
  {
    "text": "take care of crawling the data for you and describing in the way that we did is we actually had an Athena interaction class and",
    "start": "2084240",
    "end": "2090628"
  },
  {
    "text": "Python that we built to interact with Athena so we just added a method on top of that to our pair table so once the",
    "start": "2090629",
    "end": "2097079"
  },
  {
    "text": "job finishes in EMR will just say repair the table and close the job and so ever",
    "start": "2097079",
    "end": "2102990"
  },
  {
    "text": "since we did this this really showed us the benefit of having a data leak and we got rid of a lot of problems that we saw",
    "start": "2102990",
    "end": "2108690"
  },
  {
    "text": "when we cast that data into redshift and it really made everything a lot easier and so you I can actually see we have an",
    "start": "2108690",
    "end": "2114059"
  },
  {
    "text": "example here of our cycling long data because it was hundreds of millions data points that were coming in we actually",
    "start": "2114059",
    "end": "2119220"
  },
  {
    "text": "used two different partitions here so not only was it day but it was also the",
    "start": "2119220",
    "end": "2124230"
  },
  {
    "text": "facility that those bikes belonged to so we'll talk about a few other",
    "start": "2124230",
    "end": "2130109"
  },
  {
    "text": "optimizations so we realized as building our system so first off a little bit about processing so we do do processing",
    "start": "2130109",
    "end": "2138029"
  },
  {
    "text": "on redshift as we described we do mainly our light transformation the e-elt",
    "start": "2138029",
    "end": "2143460"
  },
  {
    "text": "scripts that we have built some orchestration around using our proprietary Maximilian framework this",
    "start": "2143460",
    "end": "2150119"
  },
  {
    "text": "happens inside a redshift but again it's rather light crunches you know transformation of data into final models all of our big",
    "start": "2150119",
    "end": "2157349"
  },
  {
    "text": "crunches and structure processing of semi structured data happens outside redshift and is either loaded into",
    "start": "2157349",
    "end": "2163829"
  },
  {
    "text": "partitioned buckets much like some of the data sets that Ryan described or brought in to redshift is new tables but",
    "start": "2163829",
    "end": "2170250"
  },
  {
    "text": "we do that to really keep a redshift cluster as small as possible and help preserve query capacity so additionally",
    "start": "2170250",
    "end": "2180089"
  },
  {
    "text": "a lot of people want to know what our data models look like or they you know star schemas snowflakes flat tables",
    "start": "2180089",
    "end": "2186829"
  },
  {
    "text": "somewhere in between a star schema and flatten tables so our tables are",
    "start": "2186829",
    "end": "2194490"
  },
  {
    "text": "flattened you know we in redshift since it is columnar it's okay to have really wide tables you don't have to think",
    "start": "2194490",
    "end": "2200460"
  },
  {
    "text": "about the same optimizations that you would for a row oriented relational database so we do that because columnar",
    "start": "2200460",
    "end": "2207150"
  },
  {
    "text": "it's okay to add wide tables as well as distributed joins can be expensive and we also use rational and conservative",
    "start": "2207150",
    "end": "2214680"
  },
  {
    "text": "usage of dimensions as well like we have dimensions for our major master data elements mainly for like a",
    "start": "2214680",
    "end": "2221310"
  },
  {
    "text": "data management purpose but in general we try to reduce a number of dimensions basically we just want to get the answer",
    "start": "2221310",
    "end": "2228119"
  },
  {
    "text": "and put it in a table and that's quite a relief if you've you know been building rigorous Kimball data warehouses for",
    "start": "2228119",
    "end": "2235020"
  },
  {
    "text": "most of your life so here's a quick example of what our data models one that you guys can probably figure out it is our F check-in table it is an event",
    "start": "2235020",
    "end": "2241950"
  },
  {
    "text": "table which holds every time that a member of visits one of our clubs so it has about 300 million records in it so",
    "start": "2241950",
    "end": "2250050"
  },
  {
    "text": "as you'll see in one we avoid creating dimensions we don't have to you know so if you were like a Kimball guy you'd be",
    "start": "2250050",
    "end": "2255869"
  },
  {
    "text": "really tempted to create a terminal dimension with names and attributes and stuff that you'd probably never ever use",
    "start": "2255869",
    "end": "2261000"
  },
  {
    "text": "here we just flatten it in and this is just pragmatism as well as the fact that",
    "start": "2261000",
    "end": "2266069"
  },
  {
    "text": "redshift doesn't mind being wider and from a table perspective we resist other",
    "start": "2266069",
    "end": "2271650"
  },
  {
    "text": "dimensional modeling temptations as well such as creating junk dimensions like let's have a dimension which has",
    "start": "2271650",
    "end": "2277319"
  },
  {
    "text": "attributes of the check in we flatten that into the fact as well and we also you know avoid other miserable",
    "start": "2277319",
    "end": "2284310"
  },
  {
    "text": "dimensional modeling concepts such as mystery or flag dimensions and bridge tables and stuff like that as well we",
    "start": "2284310",
    "end": "2290010"
  },
  {
    "text": "you guessed it flatten things into the fact and again it makes it really easy for analysts they don't have to worry",
    "start": "2290010",
    "end": "2296790"
  },
  {
    "text": "about joins and strange tables they don't understand the structure of as well as our developers they can",
    "start": "2296790",
    "end": "2302250"
  },
  {
    "text": "basically create the data set that they want to create put it in a table and figure out how to update that daily so",
    "start": "2302250",
    "end": "2308640"
  },
  {
    "text": "they don't have all that weight of dimensional transformation and key lookups and stuff like that",
    "start": "2308640",
    "end": "2314838"
  },
  {
    "text": "and so as Lisa mentioned you know Amazon's moving more and more towards zero maintenance type of operation so",
    "start": "2318940",
    "end": "2326660"
  },
  {
    "text": "what this is one of the things that'll be fully automated company next year in the meantime I did want to share some of",
    "start": "2326660",
    "end": "2333080"
  },
  {
    "text": "the optimizations that can be done through distribution keys and distribution styles so just some",
    "start": "2333080",
    "end": "2338330"
  },
  {
    "text": "information on the three different types you know you have to style all where essentially each node is receiving a",
    "start": "2338330",
    "end": "2344000"
  },
  {
    "text": "complete table and what the seems to do is reduce the disk usage for small and medium sized tables and this is really",
    "start": "2344000",
    "end": "2351140"
  },
  {
    "text": "preferred for table sizes up to about three million rows I'm slow changing data the other two are just out key",
    "start": "2351140",
    "end": "2357470"
  },
  {
    "text": "where each note is receiving a portion of the data based on a chosen key and what you're trying to do here is really",
    "start": "2357470",
    "end": "2363290"
  },
  {
    "text": "optimize the performance of joins insert in two's group eyes and commands like that and then lastly there's just out",
    "start": "2363290",
    "end": "2370100"
  },
  {
    "text": "even which essentially you're distributing the data in a round-robin style and this you would use if neither option of above applies and so when it",
    "start": "2370100",
    "end": "2379910"
  },
  {
    "text": "comes to decision time I just want to highlight that there it doesn't have to be between three what its most likely going to be a session",
    "start": "2379910",
    "end": "2386120"
  },
  {
    "text": "between two and that's really that's really mostly just because of is there a",
    "start": "2386120",
    "end": "2391760"
  },
  {
    "text": "disk key available if there is then it's gonna between it's gonna be between key or all so if you're within that three",
    "start": "2391760",
    "end": "2399290"
  },
  {
    "text": "million row mark you're probably going to use Paul and you're gonna find the greatest optimizations there once you",
    "start": "2399290",
    "end": "2404450"
  },
  {
    "text": "start getting past that that's when you'll want to consider using key instead and then if there is not a valid",
    "start": "2404450",
    "end": "2410030"
  },
  {
    "text": "dis tile key then you'll probably want to use all up until about three million row mark and then use even thereafter",
    "start": "2410030",
    "end": "2416590"
  },
  {
    "text": "and you know if you're looking for a flow chart like this to help you with the decision time there's plenty on the",
    "start": "2416590",
    "end": "2422030"
  },
  {
    "text": "blog on the a to its blog that you can find and so just some optimizations that",
    "start": "2422030",
    "end": "2429080"
  },
  {
    "text": "we found with des Lakes just want to highlight five of them here so number one first and foremost for us was",
    "start": "2429080",
    "end": "2435230"
  },
  {
    "text": "leveraging self-described high compression parte files you know bringing in all of that Adobe Analytics",
    "start": "2435230",
    "end": "2440750"
  },
  {
    "text": "data that clickstream data these are huge data sets that we're coming in and having them in CSV kind of worried is a",
    "start": "2440750",
    "end": "2445970"
  },
  {
    "text": "little bit when it came to data storage so if we could really leverage a high compression file format like part that would be huge and so we were number",
    "start": "2445970",
    "end": "2453670"
  },
  {
    "text": "to definitely use partitions where you can so this is definitely going to reduce the amount of data that's queried",
    "start": "2453670",
    "end": "2460329"
  },
  {
    "text": "and will reduce the cost whenever you go to query that data as well so you have faster cheaper queries but I just want",
    "start": "2460329",
    "end": "2466989"
  },
  {
    "text": "to warn that you shouldn't over partition even though you'll be querying less and less data the more and more folders that you add you're gonna add",
    "start": "2466989",
    "end": "2473170"
  },
  {
    "text": "some overhead in there so if you're over partitioning your data queries may actually take longer remember three is",
    "start": "2473170",
    "end": "2478809"
  },
  {
    "text": "lighting the compute load on redshift when you can because that data is described and glue it can be accessible",
    "start": "2478809",
    "end": "2485019"
  },
  {
    "text": "by many different query engines so you can really leverage the power of Athena or EMR where appropriate and foreign",
    "start": "2485019",
    "end": "2491920"
  },
  {
    "text": "would be healty so as we described earlier we use this whenever we land our data into redshift in order to make",
    "start": "2491920",
    "end": "2498069"
  },
  {
    "text": "other tables in redshift that have been joined together what you can do here is you can actually say data that's an s3",
    "start": "2498069",
    "end": "2504420"
  },
  {
    "text": "we can basically query that in redshift join it with some other redshift data and then save it back to s3 and so just",
    "start": "2504420",
    "end": "2512170"
  },
  {
    "text": "by wrapping it in that unload statement you've essentially done alt from s3 to s3 without ever bringing it into",
    "start": "2512170",
    "end": "2517660"
  },
  {
    "text": "redshift itself and just make sure whenever you unload that you make make sure to compress and coming soon you'll",
    "start": "2517660",
    "end": "2524650"
  },
  {
    "text": "be able to actually bring a park a file in and save it back to Parque and then easily perform a delta queries so for",
    "start": "2524650",
    "end": "2531789"
  },
  {
    "text": "those mutable tables that we have in redshift sometimes we may want to perform a diagnostic analysis such as",
    "start": "2531789",
    "end": "2537759"
  },
  {
    "text": "what changed so if we bring those tables in to s3 and we unload this every day we",
    "start": "2537759",
    "end": "2544749"
  },
  {
    "text": "can actually query the tables against themselves maybe two days ago versus yesterday and then say okay here's",
    "start": "2544749",
    "end": "2550869"
  },
  {
    "text": "actually what change between these two that's great for diagnostic purposes the other use case that we actually have for",
    "start": "2550869",
    "end": "2556539"
  },
  {
    "text": "this is third-party services that want our data they don't want us sending all of the data each and every day so what",
    "start": "2556539",
    "end": "2562869"
  },
  {
    "text": "we can do is say here are the specific items that changed and send them those",
    "start": "2562869",
    "end": "2568769"
  },
  {
    "text": "so a little bit of a few optimizations that we've been able to do and",
    "start": "2568950",
    "end": "2574019"
  },
  {
    "text": "automation and DevOps so you know we're using some you know well-known tools in",
    "start": "2574019",
    "end": "2580749"
  },
  {
    "text": "our pipelines as well as we built to several platforms as well so we've invested and",
    "start": "2580749",
    "end": "2586030"
  },
  {
    "text": "built our own batch and dag execution framework which will hopefully be open sourcing soon called Bochy we have built",
    "start": "2586030",
    "end": "2594190"
  },
  {
    "text": "our own data quality and monitoring system called ham bot which is used to monitor the quality of our warehouse",
    "start": "2594190",
    "end": "2600460"
  },
  {
    "text": "throughout the day as well as used for automated regression we built other ops",
    "start": "2600460",
    "end": "2608289"
  },
  {
    "text": "tools which help us monitor the data and just make sure that it's in good shape we do all of our scheduling through run",
    "start": "2608289",
    "end": "2613990"
  },
  {
    "text": "deck along with our batch e dag execution engine and we use Jenkins for deployment operations so a little bit",
    "start": "2613990",
    "end": "2622119"
  },
  {
    "text": "about how we do deployments on a warehouse so a few years ago just when I'm in a completely novel concept but a lot more people are starting to try to",
    "start": "2622119",
    "end": "2629710"
  },
  {
    "text": "automate the deployment and testing of their warehouse so essentially what we do is we have jenkin workflows which are",
    "start": "2629710",
    "end": "2635589"
  },
  {
    "text": "kicked off when everybody does certain actions against our git repo so essentially what we'll do is we'll go",
    "start": "2635589",
    "end": "2642069"
  },
  {
    "text": "and spin up ephemeral redshift clusters EMR clusters and and assets from our",
    "start": "2642069",
    "end": "2647980"
  },
  {
    "text": "repo in kind of like a cleanroom State so we might restore a redshift cluster which has specific fixture data that we",
    "start": "2647980",
    "end": "2654940"
  },
  {
    "text": "have known outputs for or it might be that we're spinning up a cluster based on yesterday's backup this is all",
    "start": "2654940",
    "end": "2660880"
  },
  {
    "text": "automated and it's depends on the use case well then go and run all of our",
    "start": "2660880",
    "end": "2666010"
  },
  {
    "text": "major transformations from that branch and then we run a series of ham bot checks to assure that we are within",
    "start": "2666010",
    "end": "2671890"
  },
  {
    "text": "domain norms that we haven't created any data quality issues and that nothing has changed that we didn't expect and if all",
    "start": "2671890",
    "end": "2678460"
  },
  {
    "text": "that works we merge and deploy additionally because AWS has like these",
    "start": "2678460",
    "end": "2685809"
  },
  {
    "text": "great api's and we really wanted to empower our developers now we can have as many redshift clusters EMR clusters",
    "start": "2685809",
    "end": "2691809"
  },
  {
    "text": "as we want we've actually created BOTS which allow our users to kind of interact with these with these services",
    "start": "2691809",
    "end": "2698740"
  },
  {
    "text": "through slack you know keep our admins and arthas office people from going crazy spinning up infrastructure for",
    "start": "2698740",
    "end": "2705640"
  },
  {
    "text": "these people to do development so essentially they can go on start a cluster kill a cluster and really",
    "start": "2705640",
    "end": "2712869"
  },
  {
    "text": "reduces the need for our tech ops as well as our users needing to have console access but",
    "start": "2712869",
    "end": "2721450"
  },
  {
    "text": "every good guy bought needs a bad guy button so we also built further ops",
    "start": "2721450",
    "end": "2726819"
  },
  {
    "text": "interaction and slack and so this is a bot that's really looking for those clusters that people have brought up and",
    "start": "2726819",
    "end": "2732280"
  },
  {
    "text": "maybe have left idle after you know work ended and so what it does is a couple",
    "start": "2732280",
    "end": "2738220"
  },
  {
    "text": "times a day it comes to seek and destroy those clusters you can intervene and say that you want to save the clustering if",
    "start": "2738220",
    "end": "2743829"
  },
  {
    "text": "you do he'll ignore it but if you don't he'll actually tear it down and so really what we're aiming to do here is",
    "start": "2743829",
    "end": "2749319"
  },
  {
    "text": "save money on unused infrastructure and really you know take out the responsibility to have people go in and",
    "start": "2749319",
    "end": "2756190"
  },
  {
    "text": "just check afterwards and bother everyone to say you know is this cluster does it still need to be running can I",
    "start": "2756190",
    "end": "2761559"
  },
  {
    "text": "take it down and have that conversation he'll just come in and just blow it all the way so a little on our results so",
    "start": "2761559",
    "end": "2770049"
  },
  {
    "text": "this has obviously been a really good experience for us we're very happy with redshift and s3 in our data Lake",
    "start": "2770049",
    "end": "2775510"
  },
  {
    "text": "strategy so we're fully reap lat formed and production alized the first two apps",
    "start": "2775510",
    "end": "2781089"
  },
  {
    "text": "which we bought live took about four months which was that original app that we did in a POC as well as one one",
    "start": "2781089",
    "end": "2786940"
  },
  {
    "text": "additional we finished our reap lat form in about a year and you know in general",
    "start": "2786940",
    "end": "2793450"
  },
  {
    "text": "we've just been very happy with the dependability you know we have very few operational issues we don't have a",
    "start": "2793450",
    "end": "2798670"
  },
  {
    "text": "redshift DBA and things are coming along pretty well something that we've really",
    "start": "2798670",
    "end": "2805450"
  },
  {
    "text": "enjoyed it is a faster time to benefit via automated regression so one of the",
    "start": "2805450",
    "end": "2810940"
  },
  {
    "text": "things we've done is obviously through the regression automation so now we can release much more often and bring value",
    "start": "2810940",
    "end": "2817540"
  },
  {
    "text": "to our business users much quicker without engaging a lot of QA resources and obviously the huge cost savings over",
    "start": "2817540",
    "end": "2824440"
  },
  {
    "text": "Teradata so we're being approximately one tenth of what we were in licensing",
    "start": "2824440",
    "end": "2829720"
  },
  {
    "text": "and maintenance alone and it worked out so well we decided to spread the love and build a new platform for a blank as",
    "start": "2829720",
    "end": "2836290"
  },
  {
    "text": "well so we built them a brand new data platform very similar to our own using",
    "start": "2836290",
    "end": "2842619"
  },
  {
    "text": "the same technologies and it only took four months to complete leary platform that",
    "start": "2842619",
    "end": "2848068"
  },
  {
    "text": "so in terms of the lessons learned you know so taking advantage of that s3",
    "start": "2848450",
    "end": "2854130"
  },
  {
    "text": "redshift integration whenever possible use an s3 proach rather possible so",
    "start": "2854130",
    "end": "2860730"
  },
  {
    "text": "obviously if you have like the semi structured data this in immutable data definitely and then just really you know",
    "start": "2860730",
    "end": "2867650"
  },
  {
    "text": "think about how you can use that strategy because it has a number of benefits including disaster recovery",
    "start": "2867650",
    "end": "2873480"
  },
  {
    "text": "access from many different tools and so on and so forth",
    "start": "2873480",
    "end": "2878910"
  },
  {
    "text": "develop an architecture which accommodates change so this isn't just change in the data onboarding new data",
    "start": "2878910",
    "end": "2884339"
  },
  {
    "text": "sets it's also embracing new technology so spectrum hasn't been around all that long we saw that it could really help",
    "start": "2884339",
    "end": "2889380"
  },
  {
    "text": "our architecture and you know solve problems for us so you know we're quick to bring it on one size doesn't fit all",
    "start": "2889380",
    "end": "2897420"
  },
  {
    "text": "really think about the use case sometimes redshift makes a lot of sense it makes sense to actually physically copy that data and process that data in",
    "start": "2897420",
    "end": "2904410"
  },
  {
    "text": "redshift other times spectrum is dual for the job and other times the thena is a good tool for a job and then obviously",
    "start": "2904410",
    "end": "2910619"
  },
  {
    "text": "my last one is automate everything so leverage automating automated testing",
    "start": "2910619",
    "end": "2915680"
  },
  {
    "text": "deployments and take advantage of the kind of automation capabilities and epi",
    "start": "2915680",
    "end": "2920910"
  },
  {
    "text": "is at AWS offers you to really modernize your your data warehouse deployment and",
    "start": "2920910",
    "end": "2927900"
  },
  {
    "text": "that's all we have and I'll just leave this slide up for a minute hey thank you",
    "start": "2927900",
    "end": "2934530"
  },
  {
    "text": "very much thank you [Applause]",
    "start": "2934530",
    "end": "2942369"
  },
  {
    "text": "any questions mm-hmm we actually have a mic up here",
    "start": "2942369",
    "end": "2947839"
  },
  {
    "text": "yeah go ahead sure fire away right so",
    "start": "2947839",
    "end": "2955250"
  },
  {
    "text": "when you decided to go with your your data Lake approach and you moved to AWS",
    "start": "2955250",
    "end": "2960740"
  },
  {
    "text": "I would imagine you were introducing a number of new skills to your team how",
    "start": "2960740",
    "end": "2966710"
  },
  {
    "text": "did you go about that did you enlist the help of partners what was that learning curve like yeah so we actually we",
    "start": "2966710",
    "end": "2974299"
  },
  {
    "text": "actually a really cool experience with that someone from AWS actually came in and gave a presentation to everybody so",
    "start": "2974299",
    "end": "2979609"
  },
  {
    "text": "it definitely helped us a lot but once we actually saw some success with it ended up becoming a I guess like",
    "start": "2979609",
    "end": "2986240"
  },
  {
    "text": "an evangelizing campaign from there and so we try and have you know meetings with our team members as much as",
    "start": "2986240",
    "end": "2992329"
  },
  {
    "text": "possible when new technologies are available we kind of got a little bit lucky just because once we started you",
    "start": "2992329",
    "end": "2999049"
  },
  {
    "text": "know putting some data and the data leak there were other data sources that became available on so other team members we were able to go in and get",
    "start": "2999049",
    "end": "3005319"
  },
  {
    "text": "their hands dirty with it yeah yeah we found that um you know the Postgres compatibility of red ship was a big",
    "start": "3005319",
    "end": "3011500"
  },
  {
    "text": "thing the sequel was you know for the most part even when we're processing in spark were wrapping sequel people found",
    "start": "3011500",
    "end": "3018849"
  },
  {
    "text": "the technologies pretty easy and pretty familiar so we found a lot of our folks",
    "start": "3018849",
    "end": "3024789"
  },
  {
    "text": "who were maybe traditional sequel developers and not that big of a barrier of entry and again what Ryan was saying",
    "start": "3024789",
    "end": "3029950"
  },
  {
    "text": "as people were really excited about the technology they saw the winds that we had and you know some of the",
    "start": "3029950",
    "end": "3035529"
  },
  {
    "text": "capabilities that these unlocked and they were really excited to work for it so if probably a third of our team is",
    "start": "3035529",
    "end": "3042700"
  },
  {
    "text": "now like AWS certified so yeah it's been great",
    "start": "3042700",
    "end": "3048359"
  },
  {
    "text": "how couple of questions actually but I'll just start with one stand we are",
    "start": "3056219",
    "end": "3063789"
  },
  {
    "text": "having a similar situation where we one of my great Netezza to redshift and we are still yet",
    "start": "3063789",
    "end": "3072069"
  },
  {
    "text": "to start POC so my question would be is",
    "start": "3072069",
    "end": "3077289"
  },
  {
    "text": "like I can understand the whole migration of this date on to a shift but",
    "start": "3077289",
    "end": "3082329"
  },
  {
    "text": "what is the other processes like informatica ETL tool and then stored",
    "start": "3082329",
    "end": "3087459"
  },
  {
    "text": "procedures how what are your insights like how did you guys convert that and then where did you place the scripts when yesterday or like",
    "start": "3087459",
    "end": "3095140"
  },
  {
    "text": "how you're calling lecture insights into that yeah yeah so so one of the things",
    "start": "3095140",
    "end": "3100509"
  },
  {
    "text": "you have is you have informatica jobs and you haven't you know it is a sequel which is doing your processing right so",
    "start": "3100509",
    "end": "3107349"
  },
  {
    "text": "we use informatica only for lift of shift then we're actually slowly getting away from that with our own internal",
    "start": "3107349",
    "end": "3112959"
  },
  {
    "text": "platform but in terms of the sequel what we do is we store all of our sequel in a",
    "start": "3112959",
    "end": "3118239"
  },
  {
    "text": "git repository and essentially our script Runner our Maximillian platform",
    "start": "3118239",
    "end": "3123429"
  },
  {
    "text": "takes those sequel scripts and essentially macros them to expand parameters and you know insert dates and",
    "start": "3123429",
    "end": "3131229"
  },
  {
    "text": "batches and make them more dynamic so we essentially have parameter I sequel scripts that we store and get and then",
    "start": "3131229",
    "end": "3137079"
  },
  {
    "text": "we run in workflows so that's essentially how we process data did you",
    "start": "3137079",
    "end": "3142539"
  },
  {
    "text": "come to microphone so we're not recalling that github scripts from in AWS how are you calling that scripts in",
    "start": "3142539",
    "end": "3150849"
  },
  {
    "text": "AWS I'm sorry sorry so you how you guys told all the scripts like scripts in the",
    "start": "3150849",
    "end": "3156609"
  },
  {
    "text": "github repository right yeah and that's your version control thing and then you're gonna call that from AWS one of",
    "start": "3156609",
    "end": "3162039"
  },
  {
    "text": "the services right oh so how do we actually run the stuff yes for me - oh okay so all of us that all of our stuff is",
    "start": "3162039",
    "end": "3168009"
  },
  {
    "text": "containerized so we use ECS so we have a several containers and we actually",
    "start": "3168009",
    "end": "3174009"
  },
  {
    "text": "create containers in some cases dynamically but our main container is our run deck container so run deck is",
    "start": "3174009",
    "end": "3179709"
  },
  {
    "text": "just a scheduler you could choose your scheduler and we essentially pull all those repos local",
    "start": "3179709",
    "end": "3186220"
  },
  {
    "text": "in that run duct container and then run back essentially execute within the container or outside the container by",
    "start": "3186220",
    "end": "3192730"
  },
  {
    "text": "spinning up other containers kind of complicated but but essentially we have a Linux container where we issue you",
    "start": "3192730",
    "end": "3200650"
  },
  {
    "text": "know we use Python to orchestrate sequel scripts ship could be an ec2 instance whatever whatever you like you could use",
    "start": "3200650",
    "end": "3206830"
  },
  {
    "text": "cron and data quality how did you",
    "start": "3206830",
    "end": "3215140"
  },
  {
    "text": "maintain the data sanity in redshift as I understand I'm pretty new to AWS sorry so I understand that the redshift does",
    "start": "3215140",
    "end": "3223270"
  },
  {
    "text": "not really do a primary key constraint check sure so how did you handle the",
    "start": "3223270",
    "end": "3229510"
  },
  {
    "text": "data sanity as part of the data called yes that's a good question so first off",
    "start": "3229510",
    "end": "3235840"
  },
  {
    "text": "I'd say after you know building a lot of like really big data warehouses first thing you do for performance is throw",
    "start": "3235840",
    "end": "3241210"
  },
  {
    "text": "away all the constraints and stuff in generally but the way we handle it is to our data quality framework so the things",
    "start": "3241210",
    "end": "3247480"
  },
  {
    "text": "like relationship assertions and lack of duplicates are all handled in our data",
    "start": "3247480",
    "end": "3252700"
  },
  {
    "text": "quality system so after a load we essentially you know we'll do an assertion do we have duplicates do we",
    "start": "3252700",
    "end": "3258040"
  },
  {
    "text": "have missing data do we have relationships that don't work if we are do have tables that will join together",
    "start": "3258040",
    "end": "3263650"
  },
  {
    "text": "so we essentially would load then run the assertions and you know either block downstream processes ring page or duty",
    "start": "3263650",
    "end": "3271810"
  },
  {
    "text": "for some Engineer to look for it look at so essentially we handle at post load and we also do inspections up front as",
    "start": "3271810",
    "end": "3278770"
  },
  {
    "text": "well so if we have data in s3 that we're not so sure about or data that comes in from our various systems that we pull",
    "start": "3278770",
    "end": "3284320"
  },
  {
    "text": "down into redshift or s3 will run assertions on that through spectrum or through redshift to essentially see the",
    "start": "3284320",
    "end": "3290350"
  },
  {
    "text": "quality of the data before we go unload those downstream mana models so make",
    "start": "3290350",
    "end": "3295870"
  },
  {
    "text": "sense I think I catch up with you after",
    "start": "3295870",
    "end": "3300880"
  },
  {
    "text": "they mean short for absolutely",
    "start": "3300880",
    "end": "3304619"
  },
  {
    "text": "so why did you choose to do the maximilian proprietary Data Platform",
    "start": "3309619",
    "end": "3315690"
  },
  {
    "text": "thing instead of going with something like glue yeah that's a good question",
    "start": "3315690",
    "end": "3321089"
  },
  {
    "text": "so we started this I think before glue was created glues also primary execution",
    "start": "3321089",
    "end": "3327690"
  },
  {
    "text": "engine is spark right so some of our workloads are very small we're",
    "start": "3327690",
    "end": "3333000"
  },
  {
    "text": "interacting with various like relational data sets so it made sense to us to just be able to massively parallel eyes you",
    "start": "3333000",
    "end": "3340109"
  },
  {
    "text": "know pulling data from relational databases through our own framework plus we have a lot of features in there not",
    "start": "3340109",
    "end": "3345750"
  },
  {
    "text": "just moving data but also orchestration and so on and so forth so it's more than",
    "start": "3345750",
    "end": "3350910"
  },
  {
    "text": "glue I'd say it's like glue plus Luigi plus you know a lot of other you know kind of execution frameworks and",
    "start": "3350910",
    "end": "3357210"
  },
  {
    "text": "automation frameworks thank you no problem",
    "start": "3357210",
    "end": "3362300"
  },
  {
    "text": "how did you deal with their data stored procedures did you have any BT or any",
    "start": "3369480",
    "end": "3374770"
  },
  {
    "text": "any any of these sort of the terror data specific yeah so when when we moved",
    "start": "3374770",
    "end": "3380290"
  },
  {
    "text": "those apps from Terra data it was a rewrite you know we weren't able to use",
    "start": "3380290",
    "end": "3386290"
  },
  {
    "text": "the modeling technique so we'd use in tera data that you model the data in a specific way nor the teradata sequel we",
    "start": "3386290",
    "end": "3392200"
  },
  {
    "text": "just essentially use the business logic and a lot of the kind of modeling and",
    "start": "3392200",
    "end": "3398080"
  },
  {
    "text": "you know kind of requirements that we put into that tera day to work to inspire the work that we did moving to redshift you know so it wasn't directly",
    "start": "3398080",
    "end": "3405550"
  },
  {
    "text": "usable in our case I see so you didn't actually have to convert any data sequel",
    "start": "3405550",
    "end": "3410680"
  },
  {
    "text": "sort of you know no we just we just crane filled it so we just started from scratch it was easier but we use the",
    "start": "3410680",
    "end": "3416590"
  },
  {
    "text": "Terra data for sequel for guidance you know in terms of the business logic like in a lot of cases its sequel the calyx",
    "start": "3416590",
    "end": "3423550"
  },
  {
    "text": "or the calyx it was really like what's the model that we're loading you know what are the specific techniques for",
    "start": "3423550",
    "end": "3428650"
  },
  {
    "text": "processing the data",
    "start": "3428650",
    "end": "3431730"
  },
  {
    "text": "okay oh yeah time I just want to add one",
    "start": "3437970",
    "end": "3444180"
  },
  {
    "text": "point as up in the morning what I heard is like that ship is gonna support it's",
    "start": "3444180",
    "end": "3457200"
  },
  {
    "text": "gonna be coming soon and the months first quarter yeah that's least happy news with me",
    "start": "3457200",
    "end": "3464119"
  },
  {
    "text": "happy to stored procedures for so that's a big relief where we don't how to",
    "start": "3464119",
    "end": "3470460"
  },
  {
    "text": "really spit those things up and then the code for services on AWS to really speed",
    "start": "3470460",
    "end": "3475470"
  },
  {
    "text": "that stored procedures so that's a savior so come back I heard something",
    "start": "3475470",
    "end": "3481319"
  },
  {
    "text": "about ELT approach where you're taking stuff on yesterday and in doing something back under shift and putting",
    "start": "3481319",
    "end": "3486630"
  },
  {
    "text": "back into yesterday right yes the question was how are we doing that yeah",
    "start": "3486630",
    "end": "3492359"
  },
  {
    "text": "you said something right something about",
    "start": "3492359",
    "end": "3498660"
  },
  {
    "text": "test three inserting back to history and in the operations is happening in fact shift right it could be yeah the way the",
    "start": "3498660",
    "end": "3506579"
  },
  {
    "text": "way that we're actually doing it so for instance there the specific use case was we had our collects cream data that was",
    "start": "3506579",
    "end": "3512609"
  },
  {
    "text": "coming in but you really had to be a pro if you wanted to query that effectively and so you know there were conversations",
    "start": "3512609",
    "end": "3518099"
  },
  {
    "text": "that we had to have with our analyst where it's like oh no you need to make sure it's this specific ID or it's that",
    "start": "3518099",
    "end": "3523109"
  },
  {
    "text": "specific ID and so what we ended up doing there as we said let's bring in that data and then summarize it so we",
    "start": "3523109",
    "end": "3530160"
  },
  {
    "text": "actually summarized it by each visit that everybody has by every day and by every month and so we ran those scripts",
    "start": "3530160",
    "end": "3537180"
  },
  {
    "text": "in red shift ended an unload so we ran those Gribbs didn't unload basically summarized the data and then put it in",
    "start": "3537180",
    "end": "3543480"
  },
  {
    "text": "another section in our data like so now what we do is we say oh if you want to",
    "start": "3543480",
    "end": "3548940"
  },
  {
    "text": "analyze the data just go to the summary data because that takes care of a lot of the logic that's kind of nuanced about",
    "start": "3548940",
    "end": "3555480"
  },
  {
    "text": "clickstream data and this was in this case like could have been accomplished through a spork job or a hive job or",
    "start": "3555480",
    "end": "3560700"
  },
  {
    "text": "something in externally but the ease of use and the fact that when you do run like s3 only queries it's mostly taken",
    "start": "3560700",
    "end": "3567450"
  },
  {
    "text": "care of in spectrum it created a very light load so we decided to do it for convenience",
    "start": "3567450",
    "end": "3573450"
  },
  {
    "text": "and you know started out as a quick win for for our analysts like oh it's just",
    "start": "3573450",
    "end": "3582870"
  },
  {
    "text": "yeah it's just a summarization it all gets processed on spectrum for the most part hey and I would come back to the",
    "start": "3582870",
    "end": "3589200"
  },
  {
    "text": "stored procedures just real quick we may or may not accept that because we've really started to like you know having",
    "start": "3589200",
    "end": "3595740"
  },
  {
    "text": "that integration with GUID and being able to manage branches you know the first thing that happens when you start putting stuff in the database nobody",
    "start": "3595740",
    "end": "3601800"
  },
  {
    "text": "tracks changes anymore yeah etcetera etcetera like to start to prove that",
    "start": "3601800",
    "end": "3611210"
  },
  {
    "text": "works and one question you're talking",
    "start": "3611210",
    "end": "3619050"
  },
  {
    "text": "about resizing on redshift right yeah is that gonna repartition the new data the",
    "start": "3619050",
    "end": "3625350"
  },
  {
    "text": "date of the new notes so they're like four notes on let's say that there's a",
    "start": "3625350",
    "end": "3630690"
  },
  {
    "text": "four node cluster on reaction I'm pretty much neutral ship so I might ask them questions but so there's four notes on",
    "start": "3630690",
    "end": "3637170"
  },
  {
    "text": "photos clustered redshift okay and because of the scale I'm gonna increase that to like two or three more notes",
    "start": "3637170",
    "end": "3643200"
  },
  {
    "text": "right hmm so certain tables most of the",
    "start": "3643200",
    "end": "3648450"
  },
  {
    "text": "tables because redshift is exactly like Netezza so I can exactly borrow all the experience what I have in it easier so",
    "start": "3648450",
    "end": "3655170"
  },
  {
    "text": "data is like pretty much partition for a good joints or for easy business of the",
    "start": "3655170",
    "end": "3661830"
  },
  {
    "text": "complicated joints now that the data is partitioned on those four nodes based on hash algorithms I don't need two more",
    "start": "3661830",
    "end": "3668910"
  },
  {
    "text": "nodes right so this will this I don't know it has to be partitioned the data",
    "start": "3668910",
    "end": "3675210"
  },
  {
    "text": "right guess what rebalance those petitions across will",
    "start": "3675210",
    "end": "3680640"
  },
  {
    "text": "rebalance those partitions reconfigure all of the data and it will be rebalanced and that shouldn't be anybody",
    "start": "3680640",
    "end": "3687450"
  },
  {
    "text": "lack right while doing sir sorry that wouldn't be about it like while doing sir because no it should not be a",
    "start": "3687450",
    "end": "3692460"
  },
  {
    "text": "bottleneck okay an elastic recess is",
    "start": "3692460",
    "end": "3697530"
  },
  {
    "text": "available today so you can start trying it out now Joe thanks yeah of course",
    "start": "3697530",
    "end": "3702840"
  },
  {
    "text": "we're at a time thanks guys thanks everyone so much",
    "start": "3702840",
    "end": "3708320"
  }
]