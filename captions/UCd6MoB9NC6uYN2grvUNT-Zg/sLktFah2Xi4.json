[
  {
    "text": "okay time to start so if you're just",
    "start": "1429",
    "end": "12120"
  },
  {
    "text": "joining my name is Julian I'm the machine learning guy and I'm gonna give",
    "start": "12120",
    "end": "17130"
  },
  {
    "text": "you headache this afternoon so this session focuses on sage makers so it's",
    "start": "17130",
    "end": "23730"
  },
  {
    "text": "gonna take you through the different features in Sage maker if you've never heard about it no worries I'm gonna show",
    "start": "23730",
    "end": "29310"
  },
  {
    "text": "you how to use it we're gonna run some notebooks and I'm going to cram all those new reinvent",
    "start": "29310",
    "end": "37110"
  },
  {
    "text": "features that I mentioned this morning into this talk as well so it's gonna be quite dense so let's get started so as",
    "start": "37110",
    "end": "46110"
  },
  {
    "text": "explained this morning our mission is to let all of you do machine learning it doesn't matter if you're just starting",
    "start": "46110",
    "end": "51360"
  },
  {
    "text": "with it or if you're an expert we try to bring you the api's the building blocks that you can use to either start",
    "start": "51360",
    "end": "58649"
  },
  {
    "text": "experimenting or keep scaling keep building bigger things and again as",
    "start": "58649",
    "end": "64768"
  },
  {
    "text": "explained this morning this is the stack and in this talk we're really talking about that middle layer where we need to",
    "start": "64769",
    "end": "72540"
  },
  {
    "text": "have full control over the machine learning process we want to use our own datasets we want to use potentially our",
    "start": "72540",
    "end": "79229"
  },
  {
    "text": "own algos or use existing algos but anyhow we want to control exactly what's",
    "start": "79229",
    "end": "85229"
  },
  {
    "text": "going on but we don't want to spend a minute managing infrastructure okay we",
    "start": "85229",
    "end": "91500"
  },
  {
    "text": "don't want to deal with servers we don't want to deal with clusters we don't want to deal with SSH keys and all those good",
    "start": "91500",
    "end": "98460"
  },
  {
    "text": "things except here you know we want to do machine learning and that's what we want to focus on 100% so I'm gonna show you",
    "start": "98460",
    "end": "105149"
  },
  {
    "text": "how to do this but before we actually get started so who's already doing",
    "start": "105149",
    "end": "110189"
  },
  {
    "text": "machine learning today even small-scale and you know like that's okay all right",
    "start": "110189",
    "end": "115880"
  },
  {
    "text": "so let's take a few minutes to to go",
    "start": "115880",
    "end": "121020"
  },
  {
    "text": "through the typical machine learning process okay just to set the stage and see what the pain points are and how",
    "start": "121020",
    "end": "127079"
  },
  {
    "text": "sage maker helps so it starts with a business problem okay so let me repeat",
    "start": "127079",
    "end": "133680"
  },
  {
    "text": "that it starts with a business problem let me repeat that if you don't have a business",
    "start": "133680",
    "end": "139440"
  },
  {
    "text": "problem stop right now okay I'm still meeting people who want to go into",
    "start": "139440",
    "end": "145379"
  },
  {
    "text": "machine learning because oh it's like that Dilbert comic right it's the boss shows up to your desk",
    "start": "145379",
    "end": "151019"
  },
  {
    "text": "we need a machine learning strategy right okay those of you who are old enough to remember Big Data 10 years ago",
    "start": "151019",
    "end": "156540"
  },
  {
    "text": "how did that work out yeah yeah you're laughing so I'm guessing it turned out just the way it turned out for me which",
    "start": "156540",
    "end": "163110"
  },
  {
    "text": "was pretty bad okay we need a big data strategy because probably they played golf with a friend on the weekend and he",
    "start": "163110",
    "end": "168930"
  },
  {
    "text": "had a big data strategy and and well you need to have one for the next game right",
    "start": "168930",
    "end": "174780"
  },
  {
    "text": "next Sunday okay so I'm just kidding but it is seriously a major major problem so",
    "start": "174780",
    "end": "181590"
  },
  {
    "text": "some things need to be defined before you actually go into the whole thing",
    "start": "181590",
    "end": "188129"
  },
  {
    "text": "into the whole machine learning process okay what's the business question you're trying to answer okay so you need to",
    "start": "188129",
    "end": "195810"
  },
  {
    "text": "have a list and make probably you know I'm guessing there are plenty of problems in your companies and organizations stuff that you've been",
    "start": "195810",
    "end": "202319"
  },
  {
    "text": "trying to solve for years and haven't solved okay so let's shine a machine-learning light on those and see",
    "start": "202319",
    "end": "207900"
  },
  {
    "text": "if that works okay so this is called framing trying to put try to come up with a high-level",
    "start": "207900",
    "end": "216709"
  },
  {
    "text": "approach based on data that could actually help answering the question okay and a lot well sometimes you will",
    "start": "216709",
    "end": "224849"
  },
  {
    "text": "stop right there too because not all problems in the world could be solved with machine learning unfortunately",
    "start": "224849",
    "end": "230690"
  },
  {
    "text": "right which means friends will keep writing for a few more years I suppose",
    "start": "230690",
    "end": "236629"
  },
  {
    "text": "not everything can be solved with machine learning maybe we need AI governments maybe that would stop us from writing who knows but",
    "start": "236629",
    "end": "243720"
  },
  {
    "text": "some problems simply cannot be soaked with data think about you know regulatory problems compliance problems",
    "start": "243720",
    "end": "250040"
  },
  {
    "text": "you know I don't know competition problems not everything can be solved",
    "start": "250040",
    "end": "255239"
  },
  {
    "text": "with data so assuming it is possible to solve it well data then you need to start collecting data integrating data",
    "start": "255239",
    "end": "262070"
  },
  {
    "text": "into a central repository and please don't get me start",
    "start": "262070",
    "end": "267180"
  },
  {
    "text": "you don't data legs I don't want to get there but this is probably what you need to look up and you need to clean the",
    "start": "267180",
    "end": "272490"
  },
  {
    "text": "data and transform it and make it nice right because your real life data is",
    "start": "272490",
    "end": "278220"
  },
  {
    "text": "usually terrible so those of you you do machine learning today you know how much",
    "start": "278220",
    "end": "284400"
  },
  {
    "text": "of your time you spend cleaning data yeah over 50% 80% okay 80% is the number",
    "start": "284400",
    "end": "292410"
  },
  {
    "text": "I was looking for thank you because I think that's the real life metric so in a day in a week's work you're gonna be",
    "start": "292410",
    "end": "297990"
  },
  {
    "text": "cleaning accessing cleaning you know",
    "start": "297990",
    "end": "303410"
  },
  {
    "text": "gardening weeding out bad data and then",
    "start": "303410",
    "end": "308610"
  },
  {
    "text": "maybe you're lucky you get one day of actual work yeah that's like look like your life yeah sorry about that but it's",
    "start": "308610",
    "end": "315509"
  },
  {
    "text": "it's it's true it's true so hopefully you know sage maker ground truth and so on they can help you a little bit with",
    "start": "315509",
    "end": "320820"
  },
  {
    "text": "that at some point you have data that you kind of like you want to start",
    "start": "320820",
    "end": "326070"
  },
  {
    "text": "visualizing it you want to start building features so features are high level variables built from the raw data",
    "start": "326070",
    "end": "332729"
  },
  {
    "text": "and those high-level variables will help you build a model ok the example I take",
    "start": "332729",
    "end": "338039"
  },
  {
    "text": "all the time if you're never heard about feature engineering before imagine you have a customer data set when where one",
    "start": "338039",
    "end": "343500"
  },
  {
    "text": "column is the street number one column is the street name and one column is the zip code okay so strict number all by",
    "start": "343500",
    "end": "351449"
  },
  {
    "text": "itself is it doesn't mean anything right street name by itself doesn't mean anything okay I guess there's a there's",
    "start": "351449",
    "end": "359639"
  },
  {
    "text": "a waterloo road everywhere right that doesn't tell you a lot zip code okay that's interesting but",
    "start": "359639",
    "end": "365639"
  },
  {
    "text": "it's pretty too general so if you need precise location data in your model if",
    "start": "365639",
    "end": "370770"
  },
  {
    "text": "that matters then maybe you need to engineer a new feature by concatenating",
    "start": "370770",
    "end": "376169"
  },
  {
    "text": "those three things like street name street number and address that gives you a precise location may be an even better",
    "start": "376169",
    "end": "382349"
  },
  {
    "text": "feature would be to compute latitude and longitude for that address okay and then",
    "start": "382349",
    "end": "387419"
  },
  {
    "text": "you can do geo geo location predictions etc okay so very basic very silly",
    "start": "387419",
    "end": "393240"
  },
  {
    "text": "example but that's what feature engineering is starting from raw data and building you know higher-level",
    "start": "393240",
    "end": "400020"
  },
  {
    "text": "features probably human would look at to decide what to do and then once you have that",
    "start": "400020",
    "end": "406020"
  },
  {
    "text": "you train and you train and you train and you train and you try to figure out what the right parameters are and you do",
    "start": "406020",
    "end": "412619"
  },
  {
    "text": "these hundreds of times and now you start after cleaning stuff for 80% of your time now you start doing DevOps for",
    "start": "412619",
    "end": "418710"
  },
  {
    "text": "80% of your time building clusters fixing clusters scaling clusters life is",
    "start": "418710",
    "end": "424259"
  },
  {
    "text": "not good it's even worse if your model doesn't work because you have to do it all over again but okay at some point",
    "start": "424259",
    "end": "431699"
  },
  {
    "text": "you get lucky and you have a model that you like and you need to deploy it in production and now you're in real",
    "start": "431699",
    "end": "437279"
  },
  {
    "text": "trouble because so far you were in the machine learning sandbox you could not really break anything now if you deploy",
    "start": "437279",
    "end": "442649"
  },
  {
    "text": "by accident the wrong model or show Efram model doesn't scale all kinds of things can go interesting me wrong then",
    "start": "442649",
    "end": "450059"
  },
  {
    "text": "you start breaking productions potentially you know disrupting business apps and users losing money life's not",
    "start": "450059",
    "end": "456689"
  },
  {
    "text": "good okay and in the process you're gonna do debugging and so on and so on so again DevOps dev up step ups okay and",
    "start": "456689",
    "end": "463020"
  },
  {
    "text": "you thought you would do data science right and again 80% of your time",
    "start": "463020",
    "end": "468029"
  },
  {
    "text": "end-to-end is spent doing something else so that's the problem a stage makers trying to solve where we're trying to",
    "start": "468029",
    "end": "475199"
  },
  {
    "text": "help you with data cleaning data preparation by providing built-in I'll girls that do some of that providing",
    "start": "475199",
    "end": "482819"
  },
  {
    "text": "with your tools to do that providing you with manage infrastructure to train and deploy and really trying to cut that 80%",
    "start": "482819",
    "end": "489889"
  },
  {
    "text": "frustration to something that's significantly smaller okay so that's the purpose so sage maker comes with a",
    "start": "489889",
    "end": "498870"
  },
  {
    "text": "number of modules it's not like I was saying this morning it's not like a big monolithic service that you have to use",
    "start": "498870",
    "end": "505319"
  },
  {
    "text": "okay so it's not Amazon's way of doing machine learning and you know you enter here and you exit here and just follow",
    "start": "505319",
    "end": "511860"
  },
  {
    "text": "the path and you have no flexibility you use all of these through an SDK that I will cover in a minute and you get to",
    "start": "511860",
    "end": "518610"
  },
  {
    "text": "pick what you like and you get to drop what you don't like okay so whatever fits your purpose fine don't use the rest okay so notebook",
    "start": "518610",
    "end": "525750"
  },
  {
    "text": "instances pre-installed ec2 instances - to get to work quickly so dev",
    "start": "525750",
    "end": "531120"
  },
  {
    "text": "environments experimentation environments that's what's what I'm using a Truong no nothing to install just click",
    "start": "531120",
    "end": "537900"
  },
  {
    "text": "in and get to work built-in algos we'll talk about those so off-the-shelf algo that you can use to",
    "start": "537900",
    "end": "543300"
  },
  {
    "text": "solve typical machine learning and deep learning problems just throw your data at them and start training no coding",
    "start": "543300",
    "end": "549900"
  },
  {
    "text": "know machine learning coding required and the built in environments for deep learning as well tensorflow MX net ty",
    "start": "549900",
    "end": "556620"
  },
  {
    "text": "Torchic cetera so you'll never install tensorflow again you've never installed GPU drivers again I can promise you that",
    "start": "556620",
    "end": "562590"
  },
  {
    "text": "okay and you'll save a lot of time and we keep optimizing those for performance as well so one of some new features that",
    "start": "562590",
    "end": "570060"
  },
  {
    "text": "we added at reinvent our guest productivity features like git integration so now you can add your",
    "start": "570060",
    "end": "576120"
  },
  {
    "text": "favorite repositories to Sage Maker and elastic inference which I explained this",
    "start": "576120",
    "end": "581280"
  },
  {
    "text": "morning the ability to attach acceleration to a notebook instance and",
    "start": "581280",
    "end": "588900"
  },
  {
    "text": "to any ec2 instance okay so give integration is what you would expect you can either use code commit or internal",
    "start": "588900",
    "end": "596160"
  },
  {
    "text": "get service or you can connect to you know get github get lab etc and",
    "start": "596160",
    "end": "602300"
  },
  {
    "text": "reference your repositories into stage maker so that your your developers your",
    "start": "602300",
    "end": "607920"
  },
  {
    "text": "data scientists have easy access to them okay just a simple thing to do elastic inference for notebook instances",
    "start": "607920",
    "end": "614400"
  },
  {
    "text": "is super easy so you you pick the instance type that you need for the actual notebook instance where you will",
    "start": "614400",
    "end": "620280"
  },
  {
    "text": "be running your notebooks and it doesn't need to be really large ok I'm going to be using t3 today and that's the you",
    "start": "620280",
    "end": "628770"
  },
  {
    "text": "know the inexpensive choice and it's a reasonable choice because chances are you will not run heavy-duty computation",
    "start": "628770",
    "end": "635460"
  },
  {
    "text": "on the notebook instance you are running the notebook itself and maybe you're doing basic pre-processing but you're",
    "start": "635460",
    "end": "641220"
  },
  {
    "text": "not doing the heavy lifting on the notebook instance you could write you can go and run crazy large notebook",
    "start": "641220",
    "end": "647100"
  },
  {
    "text": "instances if you really need to but chances are some t2 t3 is the right",
    "start": "647100",
    "end": "652170"
  },
  {
    "text": "choice still maybe you'd like to have some acceleration attached to that maybe you want to experiment with a you know",
    "start": "652170",
    "end": "660480"
  },
  {
    "text": "it's a subset of your data and you're doing image classification or something so it would be cool to be able to do",
    "start": "660480",
    "end": "666810"
  },
  {
    "text": "everything on the node instance and having some kind of acceleration is nice so you can just do",
    "start": "666810",
    "end": "672230"
  },
  {
    "text": "this select medium large extra-large acceleration and and you'll and you'll",
    "start": "672230",
    "end": "677660"
  },
  {
    "text": "get that and actually I've got one of them in notebooks one of my notebooks used that local acceleration and then",
    "start": "677660",
    "end": "686509"
  },
  {
    "text": "when it comes to two built in Argos so we have new ones I'll talk about those we added we have seventeen out goes now",
    "start": "686509",
    "end": "692720"
  },
  {
    "text": "we added an extra built in environment for scikit-learn so that's a that's a",
    "start": "692720",
    "end": "697790"
  },
  {
    "text": "really popular machine learning library so again you'll never have to install that again we have a model marketplace",
    "start": "697790",
    "end": "703759"
  },
  {
    "text": "and that's sounds like a simple feature but it's super useful we have the ability to search all the past training",
    "start": "703759",
    "end": "711410"
  },
  {
    "text": "jobs because over time you will be training hundreds and hundreds and hundreds of times and and it wasn't so",
    "start": "711410",
    "end": "718610"
  },
  {
    "text": "easy early on to find that training job all right you know I remember training",
    "start": "718610",
    "end": "724040"
  },
  {
    "text": "with X J blue slacks last week with this data set so okay now I've got to scroll",
    "start": "724040",
    "end": "729230"
  },
  {
    "text": "through or go through you know pages of JSON information it's not nice now you can just quickly start to say and and",
    "start": "729230",
    "end": "735949"
  },
  {
    "text": "figure out what that training job was the marketplace we mentioned this morning is similar to the ec2",
    "start": "735949",
    "end": "742879"
  },
  {
    "text": "marketplace it's a collection of three trend models that are made available so",
    "start": "742879",
    "end": "749149"
  },
  {
    "text": "you could just go and grab off-the-shelf model solving solving all kinds of problems deploy them on stage maker in a",
    "start": "749149",
    "end": "755300"
  },
  {
    "text": "couple of clicks and start predicting so totally ignoring that full machine",
    "start": "755300",
    "end": "761779"
  },
  {
    "text": "learning cycle that I showed you just grab something off the shelf predict job done this might work and if you're if",
    "start": "761779",
    "end": "768680"
  },
  {
    "text": "you're a machine learning developer or your machine learning company you could obviously publish your own models in",
    "start": "768680",
    "end": "774050"
  },
  {
    "text": "there right and and make money out of that why not interesting interesting",
    "start": "774050",
    "end": "780259"
  },
  {
    "text": "topic when it comes to training you can train in in just one click or one API",
    "start": "780259",
    "end": "786769"
  },
  {
    "text": "call at any scale without ever managing a server you can tune your hyper parameters so automatically figure out",
    "start": "786769",
    "end": "793970"
  },
  {
    "text": "the right parameters for that training job I'll show you an example later on and we added a few things so I mentioned",
    "start": "793970",
    "end": "800870"
  },
  {
    "text": "this morning the new 3dn instance family so even more powerful still based on the Nvidia 100",
    "start": "800870",
    "end": "810360"
  },
  {
    "text": "supporting 100 gigabit networking for faster training c5n again more CPU based",
    "start": "810360",
    "end": "820410"
  },
  {
    "text": "100 gigabit networking for the bigger ones and and the fastest single clock a",
    "start": "820410",
    "end": "826480"
  },
  {
    "text": "single core clock speed available at 3.5 gigahertz we optimize tensorflow to",
    "start": "826480",
    "end": "832510"
  },
  {
    "text": "scale linearly up to 256 GPUs and now if you do hyper parameter optimization you",
    "start": "832510",
    "end": "838300"
  },
  {
    "text": "can actually resume for a training a previous job you don't have to start all over again and when you have a model",
    "start": "838300",
    "end": "845440"
  },
  {
    "text": "that you like you can deploy it again super simple you can either deploy a fleet of fully managed web servers and",
    "start": "845440",
    "end": "851800"
  },
  {
    "text": "serve HTTP predictions or you can use batch transform if your use case fits",
    "start": "851800",
    "end": "859780"
  },
  {
    "text": "batch prediction better ok and we added a few things so here you",
    "start": "859780",
    "end": "865930"
  },
  {
    "text": "could use elastic inference as well so you could deploy to instances that have",
    "start": "865930",
    "end": "872950"
  },
  {
    "text": "the right type and an attached acceleration to that you can use neo to",
    "start": "872950",
    "end": "878890"
  },
  {
    "text": "compile the model for a given hardware architecture I'll show you the Raspberry Pi version of that and we have another",
    "start": "878890",
    "end": "886450"
  },
  {
    "text": "feature called inference pipelines that I'll cover at the end where you can actually build you can actually chain",
    "start": "886450",
    "end": "893400"
  },
  {
    "text": "multiple models to do to the prediction so that's useful if you want to do",
    "start": "893400",
    "end": "899530"
  },
  {
    "text": "pre-processing prediction post-processing things like that so you can actually have a chain of models that",
    "start": "899530",
    "end": "906550"
  },
  {
    "text": "are called in sequence to build predictions so how do we work with this",
    "start": "906550",
    "end": "912840"
  },
  {
    "text": "like I said it's all driven by by api's and a high-level SDK so that's what we",
    "start": "912840",
    "end": "918820"
  },
  {
    "text": "refer to as the stage maker SDK so high-level Python SDK for algo selection",
    "start": "918820",
    "end": "924970"
  },
  {
    "text": "training deploying tuning etc and you will see when I say high-level I really mean it you don't get into the",
    "start": "924970",
    "end": "930910"
  },
  {
    "text": "nitty-gritty details you you end up writing very little code there's also a spark SDK that you can",
    "start": "930910",
    "end": "937700"
  },
  {
    "text": "use on yours in your spark applications so it could be on-premise spark or it",
    "start": "937700",
    "end": "943460"
  },
  {
    "text": "could be EMR spark and we have a Python version and a scalar version I won't",
    "start": "943460",
    "end": "949580"
  },
  {
    "text": "cover that today because it's already very full if your spark user and you're curious on how to integrate spark and",
    "start": "949580",
    "end": "955190"
  },
  {
    "text": "sage maker and why would you do that etc etc you know ask me questions at the",
    "start": "955190",
    "end": "960590"
  },
  {
    "text": "end but it is very very of course you could always rely on the on the SS SDKs",
    "start": "960590",
    "end": "969680"
  },
  {
    "text": "so these are the API level SDKs available in the command-line interface",
    "start": "969680",
    "end": "975740"
  },
  {
    "text": "or in all the languages decays so both of three for Python etc these are the",
    "start": "975740",
    "end": "982780"
  },
  {
    "text": "low-level api's okay so for experimentation and notebooks etc I'll",
    "start": "982780",
    "end": "988160"
  },
  {
    "text": "definitely use the high-level SDK because it's so convenient for production automation you know",
    "start": "988160",
    "end": "994970"
  },
  {
    "text": "infrastructure level work etc where you want to have fine control etc etc probably the lower-level SDK is is a",
    "start": "994970",
    "end": "1003460"
  },
  {
    "text": "better choice so under the hood it all works with docker containers okay so",
    "start": "1003460",
    "end": "1009310"
  },
  {
    "text": "that's not scary because in 99% 99% of the time you don't need to know the",
    "start": "1009310",
    "end": "1014740"
  },
  {
    "text": "first thing about docker so let me explain how this works we have a collection of docker containers storing",
    "start": "1014740",
    "end": "1022210"
  },
  {
    "text": "the built-in algos and the pre-built environment for tensorflow and and",
    "start": "1022210",
    "end": "1029439"
  },
  {
    "text": "they're available in Amazon ECR which is our our internal service or internal",
    "start": "1029440",
    "end": "1034660"
  },
  {
    "text": "repository for for docker images ok so we build them we push them there so once",
    "start": "1034660",
    "end": "1041319"
  },
  {
    "text": "you want to use them what you really do is you write yeah it is ok you write",
    "start": "1041320",
    "end": "1049480"
  },
  {
    "text": "this what I call this helper code so that's the notebook part or your Python",
    "start": "1049480",
    "end": "1054490"
  },
  {
    "text": "script and you will just use the SDK to select the the algo select the container",
    "start": "1054490",
    "end": "1061000"
  },
  {
    "text": "that you want to use define where your data lives in a three set some parameters",
    "start": "1061000",
    "end": "1066050"
  },
  {
    "text": "train and that's it okay so it's very very little code as you will see and so",
    "start": "1066050",
    "end": "1071690"
  },
  {
    "text": "once you do this stage make your fires up the training infrastructure based on what you asked for and it will train the",
    "start": "1071690",
    "end": "1080300"
  },
  {
    "text": "model load your data from s3 get everything get everything going once the",
    "start": "1080300",
    "end": "1085760"
  },
  {
    "text": "model has been trained it saves the model back to a3 and shuts down the training instances okay so you",
    "start": "1085760",
    "end": "1092270"
  },
  {
    "text": "will never overpay for training you will never leave training infrastructure running from nothing okay because it's",
    "start": "1092270",
    "end": "1097790"
  },
  {
    "text": "terminated automatically as we discussed earlier you could stop there you could grab the model in s3 and you could",
    "start": "1097790",
    "end": "1104150"
  },
  {
    "text": "deploy it on your laptop if you like or you could bring into s3 a model that you",
    "start": "1104150",
    "end": "1109910"
  },
  {
    "text": "already trained that works and then you could go and deploy and it works exactly in the same way write a few lines of",
    "start": "1109910",
    "end": "1116300"
  },
  {
    "text": "Python saying hey please deploy this model to these okay this number of servers with this instance type and",
    "start": "1116300",
    "end": "1123290"
  },
  {
    "text": "again sage maker is going to create all of that I pull the right container inject your model create the prediction",
    "start": "1123290",
    "end": "1131630"
  },
  {
    "text": "endpoint all fully managed and you can use that endpoint to perform predictions",
    "start": "1131630",
    "end": "1138080"
  },
  {
    "text": "okay and it is really HTTP posting to the endpoint posting your data are getting back results so that's how it",
    "start": "1138080",
    "end": "1145130"
  },
  {
    "text": "works so like I said today we're not gonna go into custom containers so we really",
    "start": "1145130",
    "end": "1150530"
  },
  {
    "text": "don't need to know anything about docker you could really run anything in there okay so these are the options built in",
    "start": "1150530",
    "end": "1156559"
  },
  {
    "text": "algos built in environments so I call this bring your own script because you",
    "start": "1156559",
    "end": "1163370"
  },
  {
    "text": "just bring your tensorflow code or your MX net code into an existing environment or bring your own container so if you",
    "start": "1163370",
    "end": "1170270"
  },
  {
    "text": "have your in-house C++ library for prediction and that's",
    "start": "1170270",
    "end": "1175490"
  },
  {
    "text": "what you want to run on stage maker fine just build a dr container following some simple guidelines and and push that to",
    "start": "1175490",
    "end": "1183559"
  },
  {
    "text": "ECR and then you can use it on stage maker to train and deploy okay so if you",
    "start": "1183559",
    "end": "1189170"
  },
  {
    "text": "if you use something that's not in that list it's okay it's very very easy to do if you use R if you want to use Java if",
    "start": "1189170",
    "end": "1196220"
  },
  {
    "text": "you want to use any other language fine build a container put the training scrape the prediction script and sage",
    "start": "1196220",
    "end": "1202420"
  },
  {
    "text": "Riker will figure it out ok and you can you can scale that so let's focus on the built-in algos which",
    "start": "1202420",
    "end": "1208060"
  },
  {
    "text": "i think are one of the key interesting things in stage maker especially if",
    "start": "1208060",
    "end": "1213430"
  },
  {
    "text": "you're just starting with machine learning so there's a bit of a color code here so the it's it's looking",
    "start": "1213430",
    "end": "1219220"
  },
  {
    "text": "better on my screen that's okay so the lights are orange which is supposed to be yellow is unsupervised learning and",
    "start": "1219220",
    "end": "1226380"
  },
  {
    "text": "the the proper orange is supervised learning okay so we have seventeen algos",
    "start": "1226380",
    "end": "1232320"
  },
  {
    "text": "some some typical ones like you know linear aggression clustering key and",
    "start": "1232320",
    "end": "1237520"
  },
  {
    "text": "then pca factorization machines you know stuff that is it's been around for",
    "start": "1237520",
    "end": "1243220"
  },
  {
    "text": "decades unless but these are really amazon versions right these are amazon",
    "start": "1243220",
    "end": "1248260"
  },
  {
    "text": "implementations they'll scale to the to the moon hopefully and learn more and we",
    "start": "1248260",
    "end": "1254800"
  },
  {
    "text": "also have some slightly more advanced things like you know anomaly detection",
    "start": "1254800",
    "end": "1261270"
  },
  {
    "text": "blazing text which i'll show you in a minute which actually amazon invented",
    "start": "1261270",
    "end": "1266880"
  },
  {
    "text": "dpr for forecasting that also was also invented by amazon and a few deep",
    "start": "1266880",
    "end": "1273580"
  },
  {
    "text": "learning ones yeah for a classification detection and segmentation so it does",
    "start": "1273580",
    "end": "1279430"
  },
  {
    "text": "cover quite a lot so before you go and decide to build your own convolutional",
    "start": "1279430",
    "end": "1284530"
  },
  {
    "text": "neural network to classify images or detect images well why don't you give",
    "start": "1284530",
    "end": "1289660"
  },
  {
    "text": "those a try right and you'll save months right and lots of trouble these might",
    "start": "1289660",
    "end": "1294940"
  },
  {
    "text": "just work out of the box and you can train on your old data okay so actually",
    "start": "1294940",
    "end": "1300640"
  },
  {
    "text": "let me show you a first example so I'm gonna use blazing text because it can't just be about images right so a blazing",
    "start": "1300640",
    "end": "1307780"
  },
  {
    "text": "text can be used in two modes you can use it to compute word embeddings when",
    "start": "1307780",
    "end": "1313240"
  },
  {
    "text": "when you work with text and a natural language that's usually one of the first",
    "start": "1313240",
    "end": "1319030"
  },
  {
    "text": "steps that you that you run you get rid of the actual text and transform every word into a vector",
    "start": "1319030",
    "end": "1326320"
  },
  {
    "text": "that can be used by the machine learning model so that's not what we're going to use we're going to use it in a slightly",
    "start": "1326320",
    "end": "1332950"
  },
  {
    "text": "higher level fashion doing supervised learning and actually text",
    "start": "1332950",
    "end": "1338110"
  },
  {
    "text": "classification okay so let me switch to Maya let me switch to this so here's a",
    "start": "1338110",
    "end": "1347009"
  },
  {
    "text": "here's my notebook instance again just for the record this is how you create",
    "start": "1347009",
    "end": "1353110"
  },
  {
    "text": "one you go to the sage maker console and of course you can use api's you give it a name select the type decide if you",
    "start": "1353110",
    "end": "1362860"
  },
  {
    "text": "want acceleration on that if that needs to run in a V PC or not a few more",
    "start": "1362860",
    "end": "1370029"
  },
  {
    "text": "things grab a repo that could be in your list and create it wait for two three minutes",
    "start": "1370029",
    "end": "1376210"
  },
  {
    "text": "and you can get to work okay and then you'll see it in your console click open",
    "start": "1376210",
    "end": "1383740"
  },
  {
    "text": "and you jump into your Jupiter notebooks directly okay",
    "start": "1383740",
    "end": "1389559"
  },
  {
    "text": "so super easy okay so let's look at blazing text so I'll try to go",
    "start": "1389559",
    "end": "1396690"
  },
  {
    "text": "reasonably slow on that one because it's the first one but the important thing to",
    "start": "1396690",
    "end": "1402129"
  },
  {
    "text": "focus on is the workflow okay don't don't worry about the tiny details don't",
    "start": "1402129",
    "end": "1407679"
  },
  {
    "text": "worry about that focus on the workflow the steps that are required to get the job done and the SDK API is that I will",
    "start": "1407679",
    "end": "1414759"
  },
  {
    "text": "be using because this is really what you will see in every single notebook okay we have a growing collection of",
    "start": "1414759",
    "end": "1421029"
  },
  {
    "text": "notebooks on github the URL is at the end of the presentation and I don't know",
    "start": "1421029",
    "end": "1427179"
  },
  {
    "text": "there must be maybe close to 100 notebooks in there so it's a lot they show you how to use sage maker and they",
    "start": "1427179",
    "end": "1433299"
  },
  {
    "text": "they they all have that same structure so that's what you should focus on so",
    "start": "1433299",
    "end": "1438340"
  },
  {
    "text": "import some libraries grab an s3 bucket because that's that's really one of the",
    "start": "1438340",
    "end": "1443500"
  },
  {
    "text": "key things your data must be in s3 okay so if your data set lives in redshift or",
    "start": "1443500",
    "end": "1449769"
  },
  {
    "text": "a DynamoDB or somewhere else you need to put it in a three okay and there are",
    "start": "1449769",
    "end": "1455080"
  },
  {
    "text": "plenty of options to do that we're going to download the data set",
    "start": "1455080",
    "end": "1461000"
  },
  {
    "text": "gbpd our data said it has 560 K training",
    "start": "1461000",
    "end": "1466040"
  },
  {
    "text": "samples so let's download everything to the notebook instance extract it and",
    "start": "1466040",
    "end": "1472720"
  },
  {
    "text": "look at a few samples so the samples are sentences right labeled with a class",
    "start": "1472720",
    "end": "1481809"
  },
  {
    "text": "okay that's an integer value and those classes are the following so we have 14",
    "start": "1481809",
    "end": "1489230"
  },
  {
    "text": "classes describing what that sentence is about okay so the name of the game is to",
    "start": "1489230",
    "end": "1494420"
  },
  {
    "text": "build a classification model that we'll be able to figure out in which of those first 14 categories your new samples are",
    "start": "1494420",
    "end": "1503870"
  },
  {
    "text": "okay alright so load that stuff in a dictionary and here it's a reasonably",
    "start": "1503870",
    "end": "1511070"
  },
  {
    "text": "clean data sets to reference data set so we don't need to do much pre-processing",
    "start": "1511070",
    "end": "1516160"
  },
  {
    "text": "the only thing we'll do is tokenizing so tokenizing means we want to guarantee",
    "start": "1516160",
    "end": "1523130"
  },
  {
    "text": "that every word all words are spaced separated by a single space and that punctuation is also space separated okay",
    "start": "1523130",
    "end": "1531290"
  },
  {
    "text": "so if you have world comma space we're gonna turn this into word space comma space okay we need one single space",
    "start": "1531290",
    "end": "1537980"
  },
  {
    "text": "between each word and each punctuation symbol so we use a NLT kay a natural",
    "start": "1537980",
    "end": "1545570"
  },
  {
    "text": "language toolkit it's an open source library to do this it's don't don't",
    "start": "1545570",
    "end": "1550730"
  },
  {
    "text": "worry about that code it's it's vanilla Python code so we just tokenized the data set we have a training set we have",
    "start": "1550730",
    "end": "1557570"
  },
  {
    "text": "a validation set okay pre process everything and then upload to s3 okay and sage maker needs",
    "start": "1557570",
    "end": "1566090"
  },
  {
    "text": "that training data and that validation data to be split right that's what it's what sage maker channels so we need one",
    "start": "1566090",
    "end": "1572960"
  },
  {
    "text": "location one prefix when s3 prefix for the training data and we need one s3",
    "start": "1572960",
    "end": "1578540"
  },
  {
    "text": "prefix for the validation data okay and these are the two locations that we use all right we need to define an output",
    "start": "1578540",
    "end": "1585530"
  },
  {
    "text": "location which is where the train model will be written and that's about it okay so of course",
    "start": "1585530",
    "end": "1592509"
  },
  {
    "text": "for real-life data you would spend weeks cleaning it okay but at some point it's clean it's split into training and",
    "start": "1592509",
    "end": "1599139"
  },
  {
    "text": "validation and it's in s3 so now we get really to the sage maker part so the",
    "start": "1599139",
    "end": "1605859"
  },
  {
    "text": "first thing we want to do is what I'll go do we want to use okay so blazing text is the one here and",
    "start": "1605859",
    "end": "1613029"
  },
  {
    "text": "remember what I said about containers all that code is in containers well sounds complicated not really because",
    "start": "1613029",
    "end": "1621159"
  },
  {
    "text": "you could grab this you could use this very friendly function to to grab the name of the blazing text container in",
    "start": "1621159",
    "end": "1629409"
  },
  {
    "text": "the region I'm running in okay ECR is an sage maker our regional services so we",
    "start": "1629409",
    "end": "1636129"
  },
  {
    "text": "have copies of that blazing text container in each region where sage maker is available so we just grab the",
    "start": "1636129",
    "end": "1642879"
  },
  {
    "text": "region name and I'm running in US East one here and just asking sage maker okay",
    "start": "1642879",
    "end": "1649479"
  },
  {
    "text": "give us the image name the docker image name for that blazing text container in US East one okay and it looks like this",
    "start": "1649479",
    "end": "1656409"
  },
  {
    "text": "alright and this is as much darker as you need to know when you're working with built-in algos and yes oh sure of",
    "start": "1656409",
    "end": "1666460"
  },
  {
    "text": "course is that better alright okay so this is what you get the",
    "start": "1666460",
    "end": "1675309"
  },
  {
    "text": "doctor name and that's what we're gonna pass to the API yeah sorry",
    "start": "1675309",
    "end": "1683099"
  },
  {
    "text": "so I install so you can sew on the notebook so this is all this stuff is running on a notebook instance and so I",
    "start": "1687240",
    "end": "1696760"
  },
  {
    "text": "guess then ltk was was already installed on that it's probably part of the Python environment that we provide on the on",
    "start": "1696760",
    "end": "1704500"
  },
  {
    "text": "the notebook instance if you need to install anything you know you just you just pip install whatever alright let me",
    "start": "1704500",
    "end": "1711640"
  },
  {
    "text": "start breaking things okay so yeah it's",
    "start": "1711640",
    "end": "1720790"
  },
  {
    "text": "already it's already installed but that's what you would do of course it's there but I didn't take any risks okay",
    "start": "1720790",
    "end": "1727390"
  },
  {
    "text": "so you can you can do this so in your notebook you can pip install and do whatever you want you can run shell",
    "start": "1727390",
    "end": "1732850"
  },
  {
    "text": "commands with the exclamation mark there's another way to do this automatically we have like what we call",
    "start": "1732850",
    "end": "1739870"
  },
  {
    "text": "lifecycle configurations where it's like user data for ec2 instances so you can",
    "start": "1739870",
    "end": "1746740"
  },
  {
    "text": "you can actually automatically install automatically run an installation script",
    "start": "1746740",
    "end": "1752170"
  },
  {
    "text": "whenever you start a notebook instance or create a notebook instance okay so",
    "start": "1752170",
    "end": "1757480"
  },
  {
    "text": "that's the problem and that's of course the preferred way to do this and this is where you would maybe clone you'll get",
    "start": "1757480",
    "end": "1762700"
  },
  {
    "text": "your github repos and so on okay but when you're experimenting quick and dirty you can do this and and if you",
    "start": "1762700",
    "end": "1769660"
  },
  {
    "text": "really really you know if you're really anxious about the CLI you can always",
    "start": "1769660",
    "end": "1779850"
  },
  {
    "text": "open a terminal in and you know fire",
    "start": "1779850",
    "end": "1784990"
  },
  {
    "text": "away your CLI commands right and by the way I want to mention that this is all",
    "start": "1784990",
    "end": "1791320"
  },
  {
    "text": "the different environments are managed",
    "start": "1791320",
    "end": "1797020"
  },
  {
    "text": "with Condor okay so to manage different environments for Python to Python 3mx",
    "start": "1797020",
    "end": "1804340"
  },
  {
    "text": "NetFlow blah blah blah we have pre-existing Condor environments and you can add your own so you avoid messing up",
    "start": "1804340",
    "end": "1811350"
  },
  {
    "text": "your Python environment just like I did on this Mac and it's hopeless right so that's why I'm",
    "start": "1811350",
    "end": "1818279"
  },
  {
    "text": "using notebook instances okay thanks for the question all right so let's get back to let's get",
    "start": "1818279",
    "end": "1828240"
  },
  {
    "text": "back to training okay all right so we",
    "start": "1828240",
    "end": "1833309"
  },
  {
    "text": "grab the container and okay well ignore this text what we really want to do now",
    "start": "1833309",
    "end": "1839220"
  },
  {
    "text": "is configure the training job so that's probably the most important object in",
    "start": "1839220",
    "end": "1845970"
  },
  {
    "text": "the SDK that estimator object it it's the generic object to configure training jobs okay so let's look at their",
    "start": "1845970",
    "end": "1852809"
  },
  {
    "text": "parameters so we pass the container specifying the algo but we want to use",
    "start": "1852809",
    "end": "1859679"
  },
  {
    "text": "the role the I am role for stage maker which we grabbed in the first cell I",
    "start": "1859679",
    "end": "1865169"
  },
  {
    "text": "think because stage maker needs permission to access s3 blah blah blah blah blah",
    "start": "1865169",
    "end": "1871080"
  },
  {
    "text": "and then how much infrastructure do we want to train on okay so here it will train on one c-44 excel instance okay",
    "start": "1871080",
    "end": "1879960"
  },
  {
    "text": "and this is as much infrastructure as you will have to deal with that's it okay if I was working on a crazy large",
    "start": "1879960",
    "end": "1887639"
  },
  {
    "text": "data set I would say all right give me maybe 100 instances and this is the only thing I'd do right no work needed just",
    "start": "1887639",
    "end": "1896309"
  },
  {
    "text": "say how many instances you want and what type they should be so for built-in algos actually the",
    "start": "1896309",
    "end": "1901649"
  },
  {
    "text": "documentation tells you the recommended instance type okay but feel free to to experiment and then okay those",
    "start": "1901649",
    "end": "1909480"
  },
  {
    "text": "parameters are really not nothing to write home about the output location",
    "start": "1909480",
    "end": "1914720"
  },
  {
    "text": "this one is important input mode so input mode is how the data will be actually sent to the training instances",
    "start": "1914720",
    "end": "1922320"
  },
  {
    "text": "so file means we're going to copy it okay so we copy the full data set to the training instance which is fine when you",
    "start": "1922320",
    "end": "1928980"
  },
  {
    "text": "have small data sets like this if you have a really large data set you can you can shard the data set across the",
    "start": "1928980",
    "end": "1936330"
  },
  {
    "text": "training instances and if you have crazy large data sets you can use pipe mode okay and fight mode streams the data",
    "start": "1936330",
    "end": "1944639"
  },
  {
    "text": "from s3 to the training instances the benefit being one you save training time by not",
    "start": "1944639",
    "end": "1951960"
  },
  {
    "text": "copying stuff so you start running faster and second now you can train on",
    "start": "1951960",
    "end": "1958200"
  },
  {
    "text": "infinitely large data sets so if you could stream a petabyte of data if you want it okay it's it doesn't matter if",
    "start": "1958200",
    "end": "1964920"
  },
  {
    "text": "it doesn't fit in memory because say Jamaica will just shrink chunks to the training instance so you could train on",
    "start": "1964920",
    "end": "1972270"
  },
  {
    "text": "infinitely large data sets this is supported by almost all the built-in",
    "start": "1972270",
    "end": "1977430"
  },
  {
    "text": "algos and tensorflow okay so we still have to do a bit of machine",
    "start": "1977430",
    "end": "1983850"
  },
  {
    "text": "running right we need to read the algorithm documentation and we need to set some parameters so yes why not take",
    "start": "1983850",
    "end": "1990840"
  },
  {
    "text": "a look okay so that's the stage maker dog",
    "start": "1990840",
    "end": "1997130"
  },
  {
    "text": "and let's look at the classification hyper parameters okay so we'll stick to",
    "start": "1998150",
    "end": "2004490"
  },
  {
    "text": "the required ones okay so okay training mode that's an easy one right there's only one value supervised okay all the",
    "start": "2004490",
    "end": "2013490"
  },
  {
    "text": "other one are optional right so why did",
    "start": "2013490",
    "end": "2020900"
  },
  {
    "text": "we bother using them where's that thing",
    "start": "2020900",
    "end": "2026270"
  },
  {
    "text": "here it is so actually I could yeah okay",
    "start": "2026270",
    "end": "2032710"
  },
  {
    "text": "whoever wrote that cool notebook wanted to go a little more a little fancier but my actual recommendation would be if",
    "start": "2032710",
    "end": "2039680"
  },
  {
    "text": "you're not super familiar with other algos stick to the default parameters okay see what kind of accuracy you get",
    "start": "2039680",
    "end": "2046100"
  },
  {
    "text": "and then read a little more read the research paper for blazing text and you",
    "start": "2046100",
    "end": "2051110"
  },
  {
    "text": "can you know you can go a little deeper and figure it out but or you could do tuning as we will see later but you",
    "start": "2051110",
    "end": "2058639"
  },
  {
    "text": "could stick to this right okay the other ones are just slightly more advanced but",
    "start": "2058640",
    "end": "2065210"
  },
  {
    "text": "again do some reading okay I need to define where my training set is and my",
    "start": "2065210",
    "end": "2072830"
  },
  {
    "text": "validation my validation set is using the prefix that I define earlier and",
    "start": "2072830",
    "end": "2079669"
  },
  {
    "text": "then I just do this fit okay so when I said one API call to",
    "start": "2079670",
    "end": "2086138"
  },
  {
    "text": "get things going here it is you call fit on the estimator that you configured passing the location of the",
    "start": "2086139",
    "end": "2093760"
  },
  {
    "text": "training channel and the validation channel and off you go okay so sage maker creates that instance pulls the",
    "start": "2093760",
    "end": "2101560"
  },
  {
    "text": "docker container - it injects your parameters inject the location of the",
    "start": "2101560",
    "end": "2108849"
  },
  {
    "text": "data those two things are actually written as files inside the container",
    "start": "2108849",
    "end": "2114339"
  },
  {
    "text": "that's how sage maker does it it writes that information into files inside the container and then the algo picks up",
    "start": "2114339",
    "end": "2119650"
  },
  {
    "text": "that data and then we train okay so lots",
    "start": "2119650",
    "end": "2125950"
  },
  {
    "text": "of logging very nice validation accuracy is 97% yeah I can",
    "start": "2125950",
    "end": "2134740"
  },
  {
    "text": "live with that and we train for 46 seconds okay so this morning if somebody",
    "start": "2134740",
    "end": "2144849"
  },
  {
    "text": "asks you we need to build a classification model for text articles right and how long is it gonna take",
    "start": "2144849",
    "end": "2154589"
  },
  {
    "text": "well you would have thought about it and say I need to think about it okay so now",
    "start": "2154589",
    "end": "2162970"
  },
  {
    "text": "it's it's literally taking 46 seconds right because you don't need to write",
    "start": "2162970",
    "end": "2168609"
  },
  {
    "text": "machine learning cone at all you look at this notebook you adapt it your own problem and you train and without",
    "start": "2168609",
    "end": "2175000"
  },
  {
    "text": "knowing much you get to 97% accuracy which is kind of good okay the cost of this is 46 seconds of c4 for",
    "start": "2175000",
    "end": "2184329"
  },
  {
    "text": "Excel so pennies right okay so we're",
    "start": "2184329",
    "end": "2190720"
  },
  {
    "text": "happy with that and we want to deploy it how difficult is that well it's one line of code deploy it to one and for excel",
    "start": "2190720",
    "end": "2198640"
  },
  {
    "text": "instance again yes so good question so the question is",
    "start": "2198640",
    "end": "2205050"
  },
  {
    "text": "which framework is behind that so all the built-in algos use MX net except XJ",
    "start": "2205050",
    "end": "2211050"
  },
  {
    "text": "boost which is an open source I'll go so it uses that implementation okay but",
    "start": "2211050",
    "end": "2216240"
  },
  {
    "text": "it's an MX net model so you could go and grab the model from s3 and load it in MX net okay yeah so you just decide to",
    "start": "2216240",
    "end": "2229290"
  },
  {
    "text": "deploy on that m4 XL instance that Sage maker will completely manage for you",
    "start": "2229290",
    "end": "2235620"
  },
  {
    "text": "wait for a few minutes for the instance to show up if you look at the stage maker console you should see that",
    "start": "2235620",
    "end": "2242240"
  },
  {
    "text": "endpoint somewhere blazing text here it is and here's the URL it was here this",
    "start": "2242240",
    "end": "2253640"
  },
  {
    "text": "okay so this is the actual URL of your model like I said you could HTG post",
    "start": "2253640",
    "end": "2259830"
  },
  {
    "text": "using your favorite language or you could in this case you could keep it",
    "start": "2259830",
    "end": "2266040"
  },
  {
    "text": "simple we have a predict API in the SDK that performs that invocation so here we take",
    "start": "2266040",
    "end": "2273510"
  },
  {
    "text": "a couple of sentences the one is clearly about a company the second one is",
    "start": "2273510",
    "end": "2279390"
  },
  {
    "text": "clearly about a college we apply tokenizing okay that's important you need to apply to your real life data the",
    "start": "2279390",
    "end": "2287220"
  },
  {
    "text": "same processing that you applied on the training data so tokenizing everything making sure we have space separate",
    "start": "2287220",
    "end": "2293250"
  },
  {
    "text": "everything and I don't see that",
    "start": "2293250",
    "end": "2299790"
  },
  {
    "text": "prediction okay god got stuck somewhere okay let me so this notebook decided to",
    "start": "2299790",
    "end": "2307380"
  },
  {
    "text": "have a life of its own can I run this now okay here we go",
    "start": "2307380",
    "end": "2312740"
  },
  {
    "text": "okay so here I use the predict API to to predict those two sentences and I get",
    "start": "2312740",
    "end": "2319980"
  },
  {
    "text": "their top 1 probability here so 99% it's",
    "start": "2319980",
    "end": "2326760"
  },
  {
    "text": "a company related text 99% it's an educational institution text okay which makes sense because you know",
    "start": "2326760",
    "end": "2334410"
  },
  {
    "text": "this one said Matt company so let's if we say firm and run that again all right",
    "start": "2334410",
    "end": "2341780"
  },
  {
    "text": "yeah not telling you to company actually drops the probability yes",
    "start": "2341780",
    "end": "2349789"
  },
  {
    "text": "yes sure so the question is what if I",
    "start": "2357630",
    "end": "2365050"
  },
  {
    "text": "have some very specific custom pre-processing that I need to run before predictions so here obviously you know",
    "start": "2365050",
    "end": "2371170"
  },
  {
    "text": "it's a toy example we're not doing that so - two things two options so if you're",
    "start": "2371170",
    "end": "2377290"
  },
  {
    "text": "already using spark and you're going to do you can do ETL on spark so you're",
    "start": "2377290",
    "end": "2382810"
  },
  {
    "text": "going to do each other for training but you could do live ETL for prediction as well and you could invoke a stage maker",
    "start": "2382810",
    "end": "2389470"
  },
  {
    "text": "endpoint directly from your spark cluster using the spark SDK okay that's actually one of the scenarios where that",
    "start": "2389470",
    "end": "2396190"
  },
  {
    "text": "combination is interesting the downside to this is you need to have a spark",
    "start": "2396190",
    "end": "2401920"
  },
  {
    "text": "cluster running all the time so if you have one if you need one all the time fine that's that's right that's okay if",
    "start": "2401920",
    "end": "2406990"
  },
  {
    "text": "you don't need it all the time then that inference pipeline feature is actually a good option so you can you can I'll talk",
    "start": "2406990",
    "end": "2414250"
  },
  {
    "text": "quickly about that at the end but in a nutshell you can train a pre-processing",
    "start": "2414250",
    "end": "2420340"
  },
  {
    "text": "I'll go in psychic learn or spark ml and that could be trained on spark cluster",
    "start": "2420340",
    "end": "2426090"
  },
  {
    "text": "okay if you own your existing EMR then you could grab the model it's set into M",
    "start": "2426090",
    "end": "2431980"
  },
  {
    "text": "leap format okay sorry this is really specific and and you can terminate the",
    "start": "2431980",
    "end": "2439120"
  },
  {
    "text": "ml cluster so you have this M leap container that can be the the early step",
    "start": "2439120",
    "end": "2445660"
  },
  {
    "text": "in your inference pipeline so run pre-processing on with that spark trained I'll go and then chain that with",
    "start": "2445660",
    "end": "2452890"
  },
  {
    "text": "a proper prediction using anything else but built-in algo or tensorflow etc okay",
    "start": "2452890",
    "end": "2458110"
  },
  {
    "text": "does that answer your question all right thank you a very good question thanks I couldn't fly fry that I happy I could",
    "start": "2458110",
    "end": "2463660"
  },
  {
    "text": "clarify it so alright and we could we could keep going and of course at the end we could decide to delete the",
    "start": "2463660",
    "end": "2470380"
  },
  {
    "text": "endpoint to stop paying okay so just to be really clear on the pricing model you pay for the notebook instance per second",
    "start": "2470380",
    "end": "2477550"
  },
  {
    "text": "depending on the instance type for training you pay per second x per second",
    "start": "2477550",
    "end": "2485590"
  },
  {
    "text": "for that instance type multiplied by the number of instances that you use for the training job and for the endpoint it's the same",
    "start": "2485590",
    "end": "2491900"
  },
  {
    "text": "right the only difference is the end point stays up until you delete it so",
    "start": "2491900",
    "end": "2496970"
  },
  {
    "text": "the only way to waste money with sage maker is to actually forget to delete your endpoint okay",
    "start": "2496970",
    "end": "2503000"
  },
  {
    "text": "so let's not forget right and it goes away okay all right",
    "start": "2503000",
    "end": "2511820"
  },
  {
    "text": "I was a long example let's keep going",
    "start": "2511820",
    "end": "2517520"
  },
  {
    "text": "so what about deep learning so deep learning like I said we support built-in",
    "start": "2517520",
    "end": "2524510"
  },
  {
    "text": "environments for tensorflow MX net PI",
    "start": "2524510",
    "end": "2530420"
  },
  {
    "text": "torch chain or etc but you also get optimized versions so this is an example",
    "start": "2530420",
    "end": "2536510"
  },
  {
    "text": "of training tensorflow on image classification models so the",
    "start": "2536510",
    "end": "2541670"
  },
  {
    "text": "blue bar is the vanilla version the one you could get in a in the tense of a repo okay and the higher is better as",
    "start": "2541670",
    "end": "2549350"
  },
  {
    "text": "you can imagine so that's the number of images you train to a second and the",
    "start": "2549350",
    "end": "2554480"
  },
  {
    "text": "orange bar is the tensorflow version in sage maker and the deep learning am i okay thanks to optimizations that our",
    "start": "2554480",
    "end": "2562340"
  },
  {
    "text": "teams do you get 5x - 7 X speed-up okay this is from about a little less",
    "start": "2562340",
    "end": "2569360"
  },
  {
    "text": "than a year ago and this is really significant because if you have the training job that takes six hours nine takes one hour so you can train five or",
    "start": "2569360",
    "end": "2576650"
  },
  {
    "text": "six times a day each rate faster this is a more recent benchmark on again",
    "start": "2576650",
    "end": "2584510"
  },
  {
    "text": "training on c5 and it's 11 times faster",
    "start": "2584510",
    "end": "2589610"
  },
  {
    "text": "so we keep accelerating okay so the versions the tensorflow versions and it's true for the other deep learning",
    "start": "2589610",
    "end": "2595490"
  },
  {
    "text": "libraries the other versions that you get the other the deep learning libraries that you get in sage maker are",
    "start": "2595490",
    "end": "2602180"
  },
  {
    "text": "super super optimized so it's not just manage notebooks like I hear sometimes there's a lot of value in using those so",
    "start": "2602180",
    "end": "2609500"
  },
  {
    "text": "let's look at a chance to flow example and I will actually run everything on the on the notebook instance and I will",
    "start": "2609500",
    "end": "2616310"
  },
  {
    "text": "use elastic France is that no where is it",
    "start": "2616310",
    "end": "2623380"
  },
  {
    "text": "oh that's doing okay okay so in the previous example I actually fired up",
    "start": "2623380",
    "end": "2628989"
  },
  {
    "text": "training instances and prediction instances okay here I'm going to use something called local mode which is",
    "start": "2628989",
    "end": "2635319"
  },
  {
    "text": "very very nice for experimentation where I'll do everything locally on the notebook instance so it's saving me time",
    "start": "2635319",
    "end": "2642729"
  },
  {
    "text": "because I don't have to wait for those training or prediction instances to come up I can enter it faster and of course I",
    "start": "2642729",
    "end": "2648729"
  },
  {
    "text": "save the cost of using those manage instances so it's a cool way to experiment when you have you know a",
    "start": "2648729",
    "end": "2654699"
  },
  {
    "text": "subset of your data so here we're going to build a simple convolution neural",
    "start": "2654699",
    "end": "2661539"
  },
  {
    "text": "network that learns - I know it's not a convolution remark sorry example this is",
    "start": "2661539",
    "end": "2668709"
  },
  {
    "text": "a simple fully connected Network learning to classify the iris dataset",
    "start": "2668709",
    "end": "2674170"
  },
  {
    "text": "okay so it's it might be one of the oldest image data sets out there all",
    "start": "2674170",
    "end": "2680739"
  },
  {
    "text": "these are images but it's actually a text dataset sorry getting confuse now it's a small one it",
    "start": "2680739",
    "end": "2686019"
  },
  {
    "text": "has 150 samples it was built in 1936 right so somebody went in the forest or",
    "start": "2686019",
    "end": "2692109"
  },
  {
    "text": "I don't know where and they measured 150 flowers so you know sepal length and",
    "start": "2692109",
    "end": "2697839"
  },
  {
    "text": "petal length etc etc so for features and and they wrote down what type of iris",
    "start": "2697839",
    "end": "2704019"
  },
  {
    "text": "that was and there are three types okay zero one and two okay so let's try to see how we can do something with this",
    "start": "2704019",
    "end": "2711449"
  },
  {
    "text": "okay so it starts the same put the data",
    "start": "2711449",
    "end": "2717190"
  },
  {
    "text": "in s3 right this time a bit of tensorflow code so this code is vanilla",
    "start": "2717190",
    "end": "2724989"
  },
  {
    "text": "tensorflow code that you could run on your own machine and no modification if you used two tensorflow you you know",
    "start": "2724989",
    "end": "2730869"
  },
  {
    "text": "about those functions right serving input a fan trained input F an eval",
    "start": "2730869",
    "end": "2736239"
  },
  {
    "text": "input offense of functions loading the training set loading the validation set serving predictions etc those are",
    "start": "2736239",
    "end": "2742959"
  },
  {
    "text": "standard functions that you're usually writing when you when you build a tensorflow code and here's it actually",
    "start": "2742959",
    "end": "2750940"
  },
  {
    "text": "the function that built the network so like I said it's a simple fully connected Network three hidden layers",
    "start": "2750940",
    "end": "2756940"
  },
  {
    "text": "ten neurons 20 neurons 10 min rounds and on the output layer three classes right",
    "start": "2756940",
    "end": "2763809"
  },
  {
    "text": "three neurons because we have really three classes so as basic as it gets okay if you don't know tensorflow no",
    "start": "2763809",
    "end": "2770260"
  },
  {
    "text": "worries just remember just grab your code put it in there and if it's proper",
    "start": "2770260",
    "end": "2775839"
  },
  {
    "text": "chance of flow code it's gonna work okay there we go",
    "start": "2775839",
    "end": "2781470"
  },
  {
    "text": "these detailing all those functions now configuring the training job okay so",
    "start": "2781470",
    "end": "2787030"
  },
  {
    "text": "this time we use the tensor flow object yes yeah so yeah sure it's it needs to",
    "start": "2787030",
    "end": "2800200"
  },
  {
    "text": "be it needs to be a separate file because the container will load it okay",
    "start": "2800200",
    "end": "2805539"
  },
  {
    "text": "and I think it's good practice to separate the actual learning code from the experimentation code especially if",
    "start": "2805539",
    "end": "2812710"
  },
  {
    "text": "you want to reuse it and deploy it in production I hope no one is actually running notebook code in production okay",
    "start": "2812710",
    "end": "2819490"
  },
  {
    "text": "but I'm not gonna listen to the answer okay yeah I hope not okay don't this is",
    "start": "2819490",
    "end": "2825880"
  },
  {
    "text": "for experimentation okay then you actually write proper code and you do unit testing and so on yeah of course of",
    "start": "2825880",
    "end": "2832510"
  },
  {
    "text": "course okay so same story right we're going to",
    "start": "2832510",
    "end": "2837789"
  },
  {
    "text": "train on the local instance type okay so again not firing up anything just",
    "start": "2837789",
    "end": "2844210"
  },
  {
    "text": "running locally okay and we train okay",
    "start": "2844210",
    "end": "2849520"
  },
  {
    "text": "and as you can see it starts immediately because we don't have to wait for that instance to come up okay so this is",
    "start": "2849520",
    "end": "2855099"
  },
  {
    "text": "super fast trains for a few seconds and then I want to deploy that model and",
    "start": "2855099",
    "end": "2862000"
  },
  {
    "text": "and see how it how well it does and this is where the so I still use the deploy",
    "start": "2862000",
    "end": "2867970"
  },
  {
    "text": "API instance type is local again and I",
    "start": "2867970",
    "end": "2873099"
  },
  {
    "text": "just say hey you know what when I created the notebook in this notebook instance I attached an accelerator to it",
    "start": "2873099",
    "end": "2879630"
  },
  {
    "text": "so why don't you go and use that okay and so the local prediction",
    "start": "2879630",
    "end": "2884730"
  },
  {
    "text": "that I will do now will be accelerated okay keep in mind this is only for",
    "start": "2884730",
    "end": "2889950"
  },
  {
    "text": "prediction okay elastic inference is not is elastic inference it is for prediction only we are not accelerating",
    "start": "2889950",
    "end": "2896670"
  },
  {
    "text": "training we're accelerating predictions okay all right so deploying that model",
    "start": "2896670",
    "end": "2902610"
  },
  {
    "text": "locally and then predicting things and this is extremely verbose",
    "start": "2902610",
    "end": "2909590"
  },
  {
    "text": "okay so passing those four features all right so we went in in the forest find",
    "start": "2909590",
    "end": "2915480"
  },
  {
    "text": "an iris flower measured some things and now we want to predict okay and we see",
    "start": "2915480",
    "end": "2922080"
  },
  {
    "text": "that without much of a doubt this is classified as iris class one okay all of",
    "start": "2922080",
    "end": "2928830"
  },
  {
    "text": "that stuff happened locally but still if we had bigger data and more complex data",
    "start": "2928830",
    "end": "2935310"
  },
  {
    "text": "you could still use acceleration locally to experiment and then of course when you want to train at scale you can just",
    "start": "2935310",
    "end": "2942360"
  },
  {
    "text": "go and you can just go and fire up proper instances to train at full scale",
    "start": "2942360",
    "end": "2949950"
  },
  {
    "text": "but for experimentation this is as fast as it gets right and this is a better option than training on your laptop that",
    "start": "2949950",
    "end": "2956310"
  },
  {
    "text": "doesn't have a GPU for example okay we're not done take a deep breath",
    "start": "2956310",
    "end": "2964820"
  },
  {
    "text": "alright so okay now we have a model that we like but is it the optimal model we",
    "start": "2966050",
    "end": "2972090"
  },
  {
    "text": "picked some hyper parameters right remember for blazing text they looked weird how do we do that so that there's",
    "start": "2972090",
    "end": "2980640"
  },
  {
    "text": "a technical automatic model tuning and it's about finding the optimal set of",
    "start": "2980640",
    "end": "2987240"
  },
  {
    "text": "hyper parameters so there are four ways you can go here the first one is manual",
    "start": "2987240",
    "end": "2992550"
  },
  {
    "text": "search I know what I'm doing yeah",
    "start": "2992550",
    "end": "2997580"
  },
  {
    "text": "yep so I'm running a little late so I'm",
    "start": "3013990",
    "end": "3025810"
  },
  {
    "text": "going to take the short shortcut to that end so I'm gonna answer it but come talk to me later on so short the short answer",
    "start": "3025810",
    "end": "3032470"
  },
  {
    "text": "is scalability right maybe you're facing really really big",
    "start": "3032470",
    "end": "3038770"
  },
  {
    "text": "training jobs and and it would not be cost efficient to scale your am your EMR",
    "start": "3038770",
    "end": "3044920"
  },
  {
    "text": "cluster right the second thing is maybe you need an algo that is not available in spark ml okay and deep learning is",
    "start": "3044920",
    "end": "3053080"
  },
  {
    "text": "typically not there so any deep learning things should go there and maybe you want to be smart about picking the right",
    "start": "3053080",
    "end": "3059980"
  },
  {
    "text": "instance type for ETL and it's a different instance type of training so",
    "start": "3059980",
    "end": "3065740"
  },
  {
    "text": "when you work with this park on EMR you have to make a compromise right so you need lots of RAM for ETL and lots of",
    "start": "3065740",
    "end": "3072400"
  },
  {
    "text": "computing power for machine learning so where did you set that threshold so the",
    "start": "3072400",
    "end": "3081099"
  },
  {
    "text": "best way is to actually separate those concerns and say I'm gonna do ETL on SPARC and I'll take you know the RAM",
    "start": "3081099",
    "end": "3087490"
  },
  {
    "text": "optimized instances and I'll fire up training jobs on Sage maker using GPUs",
    "start": "3087490",
    "end": "3092859"
  },
  {
    "text": "or c5 you know the compute optimized instances and I get the best of both worlds and communication between the two is",
    "start": "3092859",
    "end": "3099700"
  },
  {
    "text": "completely seamless so you pass the data frame to Sage maker so no weird",
    "start": "3099700",
    "end": "3104740"
  },
  {
    "text": "processing and sage maker returns data frame to you so it's completely transparent",
    "start": "3104740",
    "end": "3111838"
  },
  {
    "text": "yeah we have some samples so the question is what about pre trend models we have some notebooks I'll point to you",
    "start": "3127700",
    "end": "3133920"
  },
  {
    "text": "at the end okay the the pre turn examples so back to model tuning so I",
    "start": "3133920",
    "end": "3139260"
  },
  {
    "text": "know what I'm doing except I'm not so you're gonna spend a lot of time testing",
    "start": "3139260",
    "end": "3144570"
  },
  {
    "text": "combinations that don't work so the scientific way so let's do grid search",
    "start": "3144570",
    "end": "3149880"
  },
  {
    "text": "let's systematically explore the space of hyper parameters thing is you're going to spend a lot of time training",
    "start": "3149880",
    "end": "3156960"
  },
  {
    "text": "you're going to train hundreds if not thousands of models hoping to zoom in on that hot spot in the hyper parameter",
    "start": "3156960",
    "end": "3164130"
  },
  {
    "text": "space so super slow super expensive a lot of people use a random search so",
    "start": "3164130",
    "end": "3171720"
  },
  {
    "text": "spray-and-pray just pick 100 or 200 combinations of hyper parameters in the in the upper",
    "start": "3171720",
    "end": "3177420"
  },
  {
    "text": "parameter space and you know just try your luck right funny enough this",
    "start": "3177420",
    "end": "3183420"
  },
  {
    "text": "generally works better than great search better and faster so it's not it's not",
    "start": "3183420",
    "end": "3188430"
  },
  {
    "text": "intellectually satisfying but I would recommend that instead or you use",
    "start": "3188430",
    "end": "3195270"
  },
  {
    "text": "actually HBO which is really using machine learning to predict the right",
    "start": "3195270",
    "end": "3200340"
  },
  {
    "text": "parameters for the job so we train a few times look picking random values look at the",
    "start": "3200340",
    "end": "3207060"
  },
  {
    "text": "accuracy that we get and then apply optimization on that it's called a",
    "start": "3207060",
    "end": "3212310"
  },
  {
    "text": "Gaussian process regression and biasion optimization try saying that ten times I",
    "start": "3212310",
    "end": "3218490"
  },
  {
    "text": "recommend it if you can't sleep tonight and and using these machine learning optimization you you actually pick the",
    "start": "3218490",
    "end": "3225960"
  },
  {
    "text": "next two or three jobs and you do that again and you have those new data points you know parameters versus accuracy and",
    "start": "3225960",
    "end": "3232650"
  },
  {
    "text": "optimize again and again and again and again and gradually you tend to converge really quickly - pretty optimal",
    "start": "3232650",
    "end": "3239820"
  },
  {
    "text": "parameters and typically random search grid search we're talking hundreds of models maybe thousands hpo it's an order",
    "start": "3239820",
    "end": "3248190"
  },
  {
    "text": "of magnitude less you can get two really good results in 50 trainings right it depends on many hyper parameters you",
    "start": "3248190",
    "end": "3254970"
  },
  {
    "text": "want to optimize that's probably the scale right we're talking tens of training not hundreds of",
    "start": "3254970",
    "end": "3260549"
  },
  {
    "text": "trainings so let's look at a quick demo",
    "start": "3260549",
    "end": "3266450"
  },
  {
    "text": "and here I'm going to run yeah that's the one so I'm going to run learning",
    "start": "3268819",
    "end": "3276719"
  },
  {
    "text": "rate optimization on a simple convolution neural network intensive",
    "start": "3276719",
    "end": "3282630"
  },
  {
    "text": "flow okay so we're trying to learn the amnesty data set the handwritten digits",
    "start": "3282630",
    "end": "3287749"
  },
  {
    "text": "so we download that upload them to a3 you know same old same old this is the",
    "start": "3287749",
    "end": "3294119"
  },
  {
    "text": "convolution the run Network in terms of flow so separate script you again you could run this on the laptop two",
    "start": "3294119",
    "end": "3301259"
  },
  {
    "text": "convolutional layers very very typical it's it's actually the vanilla",
    "start": "3301259",
    "end": "3307349"
  },
  {
    "text": "tensorflow example I think all right and",
    "start": "3307349",
    "end": "3313009"
  },
  {
    "text": "so we set up the estimator for tensorflow and this is the exact same",
    "start": "3313009",
    "end": "3318509"
  },
  {
    "text": "one as we saw before no difference and instead of calling fit as we did in the",
    "start": "3318509",
    "end": "3326009"
  },
  {
    "text": "previous example we're gonna say hey let's explore learning rates between 0.01 and point - that's my range so go",
    "start": "3326009",
    "end": "3334440"
  },
  {
    "text": "and explore that train a number of models and find the best one okay so I",
    "start": "3334440",
    "end": "3340109"
  },
  {
    "text": "need to define the metric that I want to optimize on so here I want to minimize loss of prediction error okay and I",
    "start": "3340109",
    "end": "3348359"
  },
  {
    "text": "configure the hyper parameter tuning jobs in K using this estimator using this metric yielding this range please",
    "start": "3348359",
    "end": "3355559"
  },
  {
    "text": "run nine training jobs three by three okay so we'll run three optimize run",
    "start": "3355559",
    "end": "3362069"
  },
  {
    "text": "three optimized run three that's nine right think okay and then we call fit",
    "start": "3362069",
    "end": "3370519"
  },
  {
    "text": "all right and if we look at the console now we actually see there it is okay so",
    "start": "3370519",
    "end": "3381150"
  },
  {
    "text": "we train nine jobs here they are and we see that the loss",
    "start": "3381150",
    "end": "3388119"
  },
  {
    "text": "the metric want to optimize on okay and the best of the nine is whoops I didn't",
    "start": "3388119",
    "end": "3396790"
  },
  {
    "text": "click on the right thing",
    "start": "3396790",
    "end": "3399750"
  },
  {
    "text": "no sorry got lost okay here we go again okay so the nine training jobs and the",
    "start": "3403070",
    "end": "3410000"
  },
  {
    "text": "best one is your okay and this one has a point 52 loss whatever and the learning",
    "start": "3410000",
    "end": "3417560"
  },
  {
    "text": "rate is point away 22 whatever that is that's not a value that I've tried right",
    "start": "3417560",
    "end": "3422930"
  },
  {
    "text": "and it's interesting if you compare this to other training jobs a tiny tiny",
    "start": "3422930",
    "end": "3428600"
  },
  {
    "text": "variation in the learning rate which is the single parameter we're looking at here gives a really really as a really",
    "start": "3428600",
    "end": "3434570"
  },
  {
    "text": "big impact on the training loss so these are not values you would have tried you would have tried point a one point two",
    "start": "3434570",
    "end": "3439730"
  },
  {
    "text": "the typical stuff so imagine with three four five six hyper parameters okay so manual search trust me you don't",
    "start": "3439730",
    "end": "3447260"
  },
  {
    "text": "know what what you're doing and neither am I so hpo is the way to go here okay",
    "start": "3447260",
    "end": "3452810"
  },
  {
    "text": "super simple to set up all right the last thing I want to show you before",
    "start": "3452810",
    "end": "3458090"
  },
  {
    "text": "they kick me out is compilation okay so",
    "start": "3458090",
    "end": "3463100"
  },
  {
    "text": "now you have a model you are you trained it you optimized it and you want it to",
    "start": "3463100",
    "end": "3468260"
  },
  {
    "text": "run as fast as possible on your own machine right so this is what neo does",
    "start": "3468260",
    "end": "3474380"
  },
  {
    "text": "okay so it supports tensorflow MMX net PI torch when an X an extra boost it",
    "start": "3474380",
    "end": "3481460"
  },
  {
    "text": "optimizes for these architectures more to come and as I've said earlier this will be open sourced for other hardware",
    "start": "3481460",
    "end": "3488390"
  },
  {
    "text": "vendors to to run so how difficult is it",
    "start": "3488390",
    "end": "3493720"
  },
  {
    "text": "not very so you can figure the training job okay so defining where your model is",
    "start": "3493720",
    "end": "3502910"
  },
  {
    "text": "an s3 okay and I might switch to that that's probably a better option here",
    "start": "3502910",
    "end": "3509210"
  },
  {
    "text": "switching to my PI try to keep both things on the screen yeah it's super",
    "start": "3509210",
    "end": "3516200"
  },
  {
    "text": "tiny okay",
    "start": "3516200",
    "end": "3523880"
  },
  {
    "text": "still alive great okay so I would train a model on",
    "start": "3524150",
    "end": "3529260"
  },
  {
    "text": "stage maker grab it from s3 and it would look something like this okay so here",
    "start": "3529260",
    "end": "3535770"
  },
  {
    "text": "it's a resonant model here in MX net so I would have a parameters file and a JSON file describing the model okay so",
    "start": "3535770",
    "end": "3542850"
  },
  {
    "text": "that's what you would have in that tar archive here okay it's really the output",
    "start": "3542850",
    "end": "3548790"
  },
  {
    "text": "artifact from your sage maker training job you need to define the input shape so it's an image classification model so",
    "start": "3548790",
    "end": "3556140"
  },
  {
    "text": "actually I think I had a file here so it predicts where is it it predicts no it's",
    "start": "3556140",
    "end": "3566430"
  },
  {
    "text": "not here sir here consider it predicts one image with three channels red green",
    "start": "3566430",
    "end": "3574500"
  },
  {
    "text": "and blue 224 224 pixels that's that's the the shape it was trained on it's an",
    "start": "3574500",
    "end": "3581910"
  },
  {
    "text": "MX net model so please do whatever you have to do to optimize it for Raspberry Pi and I would put something in that",
    "start": "3581910",
    "end": "3589830"
  },
  {
    "text": "bucket okay anyone can write that okay and then",
    "start": "3589830",
    "end": "3595440"
  },
  {
    "text": "I need to run that command all right let's run that command",
    "start": "3595440",
    "end": "3602990"
  },
  {
    "text": "okay so that's it and I could run this one probably not",
    "start": "3604880",
    "end": "3618519"
  },
  {
    "text": "okay oh it failed okay all right I messed something up",
    "start": "3623710",
    "end": "3629690"
  },
  {
    "text": "again okay not a gzip file all right fine yeah I know what that is okay my fault",
    "start": "3629690",
    "end": "3636230"
  },
  {
    "text": "no worry so you compile it within seconds either it fails because",
    "start": "3636230",
    "end": "3642559"
  },
  {
    "text": "I'm stupid or you are you get a compiled",
    "start": "3642559",
    "end": "3648710"
  },
  {
    "text": "version okay and in that compiled version that I",
    "start": "3648710",
    "end": "3654680"
  },
  {
    "text": "copied to the PI you get the optimized version of the model so first let's try",
    "start": "3654680",
    "end": "3661880"
  },
  {
    "text": "to run the vanilla model so I'm trying to predict a dog image using the vanilla",
    "start": "3661880",
    "end": "3670279"
  },
  {
    "text": "version of the MX net model okay so takes a bit to warm up it needs to load",
    "start": "3670279",
    "end": "3675619"
  },
  {
    "text": "MX Nettie needs to load the model and then I read that dog image from yes if I",
    "start": "3675619",
    "end": "3682640"
  },
  {
    "text": "touch the right name I load that dog image from disk and I predict it and let's see how much time",
    "start": "3682640",
    "end": "3688069"
  },
  {
    "text": "it takes it's a Raspberry Pi so you have to be patient",
    "start": "3688069",
    "end": "3693579"
  },
  {
    "text": "come on buddy all right so we predicted",
    "start": "3693579",
    "end": "3698869"
  },
  {
    "text": "it correctly but it took about eight seconds okay that's way too slow for any",
    "start": "3698869",
    "end": "3703880"
  },
  {
    "text": "kind of application okay so now using the compiled version which",
    "start": "3703880",
    "end": "3712759"
  },
  {
    "text": "looks like this okay so I get you know",
    "start": "3712759",
    "end": "3718339"
  },
  {
    "text": "in the archive I get that same model that I got before and I got a shared object library that actually contains",
    "start": "3718339",
    "end": "3725920"
  },
  {
    "text": "Hardware optimized versions of some of the operators used in the model okay and",
    "start": "3725920",
    "end": "3732109"
  },
  {
    "text": "so that's what I need to load and I can load it very easily using the here it is",
    "start": "3732109",
    "end": "3741559"
  },
  {
    "text": "right using that deep learning runtime okay so here I'm not even loading MX net",
    "start": "3741559",
    "end": "3748249"
  },
  {
    "text": "on the PI right I'm just loading the model on the Neo runtime and predicting so you also save space",
    "start": "3748249",
    "end": "3755369"
  },
  {
    "text": "by not installing MX net or tensorflow to your device those can take up to you",
    "start": "3755369",
    "end": "3762510"
  },
  {
    "text": "know one gigabytes of storage okay so here again loading the model loading be",
    "start": "3762510",
    "end": "3770940"
  },
  {
    "text": "optimized symbols and and predicting the same image and I think it's gonna do it",
    "start": "3770940",
    "end": "3778560"
  },
  {
    "text": "ten times or something and obviously this is not taking six seconds right",
    "start": "3778560",
    "end": "3785570"
  },
  {
    "text": "okay so let's look at the log all right takes one point three seconds so like I",
    "start": "3789710",
    "end": "3797010"
  },
  {
    "text": "said five six x speed-up which is huge okay because this is so tiny right that",
    "start": "3797010",
    "end": "3803430"
  },
  {
    "text": "the minute you start using Hardware optimization you get a massive performance boost I don't think you'll",
    "start": "3803430",
    "end": "3808590"
  },
  {
    "text": "get that on the other platforms but this is really significant okay so there you go neo and you can apply that to see",
    "start": "3808590",
    "end": "3817800"
  },
  {
    "text": "five instances P three instances so even there you should see a nice speed-up",
    "start": "3817800",
    "end": "3823849"
  },
  {
    "text": "okay I talked about inference pipelines already so in a nutshell a sequence of containers that let you run different",
    "start": "3823849",
    "end": "3833910"
  },
  {
    "text": "models to do pre-processing post-processing etc there's a there's a",
    "start": "3833910",
    "end": "3838950"
  },
  {
    "text": "good notebook on that and so go go and run that using scikit-learn and there's",
    "start": "3838950",
    "end": "3844589"
  },
  {
    "text": "a spark version as well that you might like if you so if you're looking for",
    "start": "3844589",
    "end": "3850560"
  },
  {
    "text": "resources so a lot of them are already mentioned in previous talks I just want to point out this if you if you want to",
    "start": "3850560",
    "end": "3858540"
  },
  {
    "text": "have a long and deep workshop on sage maker this is actually the workshop I",
    "start": "3858540",
    "end": "3865890"
  },
  {
    "text": "ran at reinvent a couple of weeks ago people tend people seem to like it and",
    "start": "3865890",
    "end": "3871349"
  },
  {
    "text": "you can grab the code and the slides from that repo and run it and it uses XJ boost but it's gonna take you through",
    "start": "3871349",
    "end": "3877400"
  },
  {
    "text": "the full thing batch transform hpo etc etc okay so if you runner if you want to",
    "start": "3877400",
    "end": "3885570"
  },
  {
    "text": "get really really deep on sage maker I reckon and again the best referee the best",
    "start": "3885570",
    "end": "3890710"
  },
  {
    "text": "information source here is this collection of notebooks okay on github",
    "start": "3890710",
    "end": "3896890"
  },
  {
    "text": "so the repo is called sage Amazon sage maker examples this is the best way to learn read the docs and dive into those",
    "start": "3896890",
    "end": "3905430"
  },
  {
    "text": "to start maybe with the built-in algos and if you want to go crazy on spark",
    "start": "3905430",
    "end": "3911410"
  },
  {
    "text": "you'll find spark examples as well okay and if you you were curious about remind",
    "start": "3911410",
    "end": "3919210"
  },
  {
    "text": "me are pre-trained models you'll find",
    "start": "3919210",
    "end": "3925450"
  },
  {
    "text": "examples where are they advanced yes here they are okay so byom",
    "start": "3925450",
    "end": "3937120"
  },
  {
    "text": "means bring your own model yeah it took me a while to figure it out so they show",
    "start": "3937120",
    "end": "3942130"
  },
  {
    "text": "you how to import the tensorflow model or other kinds of models there's even an hour version if people like to use our",
    "start": "3942130",
    "end": "3948510"
  },
  {
    "text": "so again lots of notebooks this is a hugely interesting resource and it",
    "start": "3948510",
    "end": "3954430"
  },
  {
    "text": "covers really really everything and it keeps evolving alright I'm late again so",
    "start": "3954430",
    "end": "3962110"
  },
  {
    "text": "it's the next speaker here oh yeah all right so I'm late again hey was late in London I'm late in",
    "start": "3962110",
    "end": "3967900"
  },
  {
    "text": "Dublin right there's some consistency here so I fully apologize for that I hope you appear and and so I'm gonna",
    "start": "3967900",
    "end": "3976120"
  },
  {
    "text": "take a break and I'm going to prepare for the last session focusing on reinforcement learning right so go have",
    "start": "3976120",
    "end": "3983080"
  },
  {
    "text": "coffee before that alright thanks a lot thank you very much [Applause]",
    "start": "3983080",
    "end": "3992569"
  }
]