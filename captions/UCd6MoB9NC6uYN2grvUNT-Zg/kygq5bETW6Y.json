[
  {
    "start": "0",
    "end": "31000"
  },
  {
    "text": "good morning everyone so welcome to AWS love everyone who is here and everyone who is joining us on the stream I'm",
    "start": "30",
    "end": "7049"
  },
  {
    "text": "going to talk about deploying your data warehouse on AWS so I'm promoting das I'm especially the",
    "start": "7049",
    "end": "13679"
  },
  {
    "text": "Solutions Architect for data analytics for AWS work with customers across a lot",
    "start": "13679",
    "end": "18869"
  },
  {
    "text": "of verticals but very keen to discuss your options on data warehouse on a double yesterday right so how do you",
    "start": "18869",
    "end": "27689"
  },
  {
    "text": "deploy a data warehouse on AWS not this way right this is a very",
    "start": "27689",
    "end": "33420"
  },
  {
    "text": "traditional approach where you set up in a standby staging servers deploy your",
    "start": "33420",
    "end": "40290"
  },
  {
    "text": "data warehouse set up your bi application servers etc however there is",
    "start": "40290",
    "end": "46829"
  },
  {
    "text": "a better less expensive more efficient faster you know with less management",
    "start": "46829",
    "end": "54059"
  },
  {
    "text": "headache approach and that is going through the fundamentals of a data warehouse all the different stages so",
    "start": "54059",
    "end": "61739"
  },
  {
    "text": "the ingestion stage so here are options you start looking at how how is the data",
    "start": "61739",
    "end": "70860"
  },
  {
    "text": "coming in is it streaming data is the data stored in your relational database",
    "start": "70860",
    "end": "77340"
  },
  {
    "text": "is the data stored in existing data warehouse or is all the data is in a",
    "start": "77340",
    "end": "85320"
  },
  {
    "text": "file server something and you have your own code existing code you want to use and Adrian talked about using AWS batch",
    "start": "85320",
    "end": "92460"
  },
  {
    "text": "where you can use you know parallely hydrate your data warehouse from various",
    "start": "92460",
    "end": "97920"
  },
  {
    "text": "sources and also you can use services like AWS glue which is a fully managed",
    "start": "97920",
    "end": "103380"
  },
  {
    "text": "in a ETL service we're gonna talk to in in details in a second the best practice",
    "start": "103380",
    "end": "109320"
  },
  {
    "text": "on AWS is store the initial load and data to s3 independent of that they",
    "start": "109320",
    "end": "115229"
  },
  {
    "text": "dictate the temperature the speed at which data is coming in whatever source it is and you kind of start building",
    "start": "115229",
    "end": "120360"
  },
  {
    "text": "something like a data lag and the next step for you is to work out how would",
    "start": "120360",
    "end": "127860"
  },
  {
    "start": "123000",
    "end": "276000"
  },
  {
    "text": "you prepare that data for analysis at that stage you are considering",
    "start": "127860",
    "end": "133450"
  },
  {
    "text": "something called ETL and probably the most commonly used term in in computer",
    "start": "133450",
    "end": "138820"
  },
  {
    "text": "science but so you you look at how complex is your ETL process is that",
    "start": "138820",
    "end": "145240"
  },
  {
    "text": "going to run less than five minutes is a small transformations then you can use the AWS lambda if you want in a big data",
    "start": "145240",
    "end": "153610"
  },
  {
    "text": "style fully managed ETL you can use AWS glue or if it's extremely complex we",
    "start": "153610",
    "end": "159910"
  },
  {
    "text": "doing lots of aggregation in a lots of in a combining different data sources",
    "start": "159910",
    "end": "165760"
  },
  {
    "text": "together then you want to use our managed hadoop with Amazon EMR that's a",
    "start": "165760",
    "end": "171730"
  },
  {
    "text": "choice so you got various choices to do that and then the idea is to store the",
    "start": "171730",
    "end": "177580"
  },
  {
    "text": "data again on s3 in a query optimized format ready for self-service so here",
    "start": "177580",
    "end": "185080"
  },
  {
    "text": "you choose once the data is in s3 you can decide how you can analyze it so let's look at there are our options",
    "start": "185080",
    "end": "191280"
  },
  {
    "text": "right so immediately the data is queryable through Amazon attina",
    "start": "191280",
    "end": "198549"
  },
  {
    "text": "and you can run SQL query straightaway analyze the data or you can think of a",
    "start": "198549",
    "end": "204220"
  },
  {
    "text": "data warehouse tile implementation using Amazon redshift you recently launched",
    "start": "204220",
    "end": "211480"
  },
  {
    "text": "Amazon spectrum we can talk a little bit more details on this and once the data is there you can decide how you want to",
    "start": "211480",
    "end": "216580"
  },
  {
    "text": "visualize it you can use quick site or there are other partner solutions like tabular and click but and also you can",
    "start": "216580",
    "end": "224709"
  },
  {
    "text": "do predictive analytics but the key advantage of this this structure is you",
    "start": "224709",
    "end": "230260"
  },
  {
    "text": "have options you have choices based on how the data is coming in on how you want to analyze it and this kind of",
    "start": "230260",
    "end": "237370"
  },
  {
    "text": "pattern kind of cover most majority of the data warehousing use cases you would",
    "start": "237370",
    "end": "242380"
  },
  {
    "text": "ever have right so I'm going to talk a little bit about Amazon redshift",
    "start": "242380",
    "end": "249420"
  },
  {
    "text": "architecture so we launched redshift in on Valentine's Day 2013 so a little over",
    "start": "249420",
    "end": "256900"
  },
  {
    "text": "four years and since then we've been inventing like crazy we were launching new features on red ship every cup",
    "start": "256900",
    "end": "264770"
  },
  {
    "text": "weeks if you take a look at Amazon is damage that comes / redshifts lat what's",
    "start": "264770",
    "end": "271490"
  },
  {
    "text": "new you'll see new features being published constantly so what what is",
    "start": "271490",
    "end": "276919"
  },
  {
    "start": "276000",
    "end": "400000"
  },
  {
    "text": "redshift is is is our data warehouse so choice you can start very small at 160",
    "start": "276919",
    "end": "284449"
  },
  {
    "text": "gig in a single node SSD cluster to scale up to like two petabyte using the",
    "start": "284449",
    "end": "293360"
  },
  {
    "text": "console we have some customers running about 6 better by data warehouse NTT DoCoMo on AWS using redshift it's simple",
    "start": "293360",
    "end": "302449"
  },
  {
    "text": "you can connect through JDBC ODBC using your favorite BI to wrap shipped receive",
    "start": "302449",
    "end": "309199"
  },
  {
    "text": "is optimized for scans of terabytes and petabytes size because we leverage",
    "start": "309199",
    "end": "314710"
  },
  {
    "text": "columnist storage compression when shared nothing MPP architecture",
    "start": "314710",
    "end": "320289"
  },
  {
    "text": "Security's built in redshift you can encrypt your data at rest or in transit",
    "start": "320289",
    "end": "325520"
  },
  {
    "text": "or isolated or using Amazon V PC you can manage your own keys using kms Amazon",
    "start": "325520",
    "end": "332419"
  },
  {
    "text": "kms or hardware security modules as well",
    "start": "332419",
    "end": "337750"
  },
  {
    "text": "right so let's quickly take a look at the red ship architecture at the bit that you work with redshift is called",
    "start": "337780",
    "end": "346069"
  },
  {
    "text": "the leading edge and that's where you connect through your application through SQL tool for your bi to over JDBC ODBC",
    "start": "346069",
    "end": "352909"
  },
  {
    "text": "drivers you can obviously download Amazon redshift driver for Ana's website",
    "start": "352909",
    "end": "358550"
  },
  {
    "text": "or you can also connect by Postgres jdb JDBC drivers as well behind the leader",
    "start": "358550",
    "end": "364969"
  },
  {
    "text": "node is the compute nodes so leader node gets your sequel compiles into C++ code",
    "start": "364969",
    "end": "371180"
  },
  {
    "text": "and throat throws to the compute nodes and we are continuously backing up the",
    "start": "371180",
    "end": "376430"
  },
  {
    "text": "cluster to s3 as we receive yes as I says as we receive your query generate the code and",
    "start": "376430",
    "end": "382729"
  },
  {
    "text": "distributing to the compute nodes and and also I want to say in the leading",
    "start": "382729",
    "end": "388279"
  },
  {
    "text": "node has got things like the Postgres catalog tables and we also add additional cataloguing tables for you to",
    "start": "388279",
    "end": "395000"
  },
  {
    "text": "understand how so for you to query the metadata as well so let's look at some of the common",
    "start": "395000",
    "end": "402710"
  },
  {
    "start": "400000",
    "end": "479000"
  },
  {
    "text": "use cases how red ship is used by various customers one of the most common",
    "start": "402710",
    "end": "407840"
  },
  {
    "text": "use cases the main thing we are talking about traditional data warehousing and so you'd want to do business reporting",
    "start": "407840",
    "end": "415010"
  },
  {
    "text": "advanced analytics pipeline in a secure and compliant way you'd you'd be because",
    "start": "415010",
    "end": "421490"
  },
  {
    "text": "red shift you know is MPP so data load scales linearly with the size the",
    "start": "421490",
    "end": "429350"
  },
  {
    "text": "cluster and so we have customs we may I mentioned entity DoCoMo Nasdaq also who",
    "start": "429350",
    "end": "436340"
  },
  {
    "text": "uses redshift for their data warehousing the other use cases you see is for log",
    "start": "436340",
    "end": "442700"
  },
  {
    "text": "analysis Pinterest uses redshift for interactive data analysis for KPIs for",
    "start": "442700",
    "end": "449210"
  },
  {
    "text": "recommendations lift also uses redshift and Yelp for this purposes the other use",
    "start": "449210",
    "end": "457880"
  },
  {
    "text": "case is building business applications on top of redshift and having many users",
    "start": "457880",
    "end": "463940"
  },
  {
    "text": "on that so Accenture has a platform called a IP which uses redshift for its data warehouse infancy says God",
    "start": "463940",
    "end": "470060"
  },
  {
    "text": "amplitude has got as well and that uses redshift for many users and they expose",
    "start": "470060",
    "end": "475700"
  },
  {
    "text": "it to many users to use that as well so this is a whole list of redshift",
    "start": "475700",
    "end": "481310"
  },
  {
    "start": "479000",
    "end": "615000"
  },
  {
    "text": "customer the only reason I would put this slide for you to see that you know it kind of matches with any vertical you are you have customers you know from in",
    "start": "481310",
    "end": "488960"
  },
  {
    "text": "a retail from banking from pretty much any industry you can think of using redshift you know in public sector",
    "start": "488960",
    "end": "495550"
  },
  {
    "text": "education and governments you see so let's quickly look at NTT DoCoMo so they",
    "start": "495550",
    "end": "503479"
  },
  {
    "text": "were running on premises greenplum and they migrated to redshift and they have",
    "start": "503479",
    "end": "510410"
  },
  {
    "text": "68 million customer tens of terabytes of data coming in every day they were experiencing challenges with scaling",
    "start": "510410",
    "end": "517339"
  },
  {
    "text": "performance issues and they needed a level of security so this is this is the kind of architecture they built with 125",
    "start": "517339",
    "end": "525380"
  },
  {
    "text": "nodes ds2 8xl 4500 V CPUs and",
    "start": "525380",
    "end": "531080"
  },
  {
    "text": "terabyte or 30 terabyte of RAM to to provide compressed data and they kind of",
    "start": "531080",
    "end": "539360"
  },
  {
    "text": "got 10 times faster analytics queries with you know 50% reduction in time for",
    "start": "539360",
    "end": "545750"
  },
  {
    "text": "new BI application deployments and you know they're an extremely happy customer and their use case is publicly available",
    "start": "545750",
    "end": "552290"
  },
  {
    "text": "our website as well and the other one I wanted to showcase is Nasdaq they move",
    "start": "552290",
    "end": "558500"
  },
  {
    "text": "from Microsoft sequel server on Prem and to use redshift as their data warehouse",
    "start": "558500",
    "end": "565180"
  },
  {
    "text": "and you kind of did that in in I think seven seven man months and yes a they",
    "start": "565180",
    "end": "575149"
  },
  {
    "text": "kind of get seven billion to seven sorry seven billion rows of data every day they analyze market shares client",
    "start": "575149",
    "end": "583760"
  },
  {
    "text": "activity surveillance billing etc they needed to in a lower down there TCR and",
    "start": "583760",
    "end": "589610"
  },
  {
    "text": "this is the kind of architecture they're built well they uses HSM to encrypt the data",
    "start": "589610",
    "end": "594860"
  },
  {
    "text": "load from on-premises to s3 and then they load it back to redshift cluster so",
    "start": "594860",
    "end": "600140"
  },
  {
    "text": "that their cluster has got 23 node ds2 8 Excel and yes that's some of some of the",
    "start": "600140",
    "end": "606260"
  },
  {
    "text": "matrix of it yeah yeah seven months they finish the entire migration and yeah",
    "start": "606260",
    "end": "612470"
  },
  {
    "text": "they're happy with the performance they get so we're looking at data warehouse",
    "start": "612470",
    "end": "618020"
  },
  {
    "start": "615000",
    "end": "660000"
  },
  {
    "text": "is one of the common things that we look at is performance and how do you how do you tune redshift for performance so in",
    "start": "618020",
    "end": "626329"
  },
  {
    "text": "the in redshift world throughput is not about concurrency but the effective",
    "start": "626329",
    "end": "632440"
  },
  {
    "text": "usage of the MPP architecture so the key things remember is don't skew the work",
    "start": "632440",
    "end": "638420"
  },
  {
    "text": "just to a few slices choose the right distribution key do the minimum work",
    "start": "638420",
    "end": "644300"
  },
  {
    "text": "don't pull out more blocks of the destined you need to and assign just",
    "start": "644300",
    "end": "649579"
  },
  {
    "text": "enough memory to your queue or to your query and otherwise you're wasting valuable resources so let's dig into",
    "start": "649579",
    "end": "656630"
  },
  {
    "text": "each of those right and so doing how do",
    "start": "656630",
    "end": "662300"
  },
  {
    "start": "660000",
    "end": "759000"
  },
  {
    "text": "you do equal amount of work and past lives a slice is a very important concept in redshift say each",
    "start": "662300",
    "end": "669370"
  },
  {
    "text": "of the compute nodes has got to eight is 32 slices based on the notes type you're",
    "start": "669370",
    "end": "676630"
  },
  {
    "text": "using and slice is a unit of computation and storage put together and that is",
    "start": "676630",
    "end": "681940"
  },
  {
    "text": "what gives redshift parallelism and when I say that the data load you know scales",
    "start": "681940",
    "end": "687940"
  },
  {
    "text": "linearly that's because their number of every time you add a node number of nodes number of slices increases that",
    "start": "687940",
    "end": "693730"
  },
  {
    "text": "means more parallelism increased from data load from s3 and so let's look at",
    "start": "693730",
    "end": "699340"
  },
  {
    "text": "the different and types of keys we have because that sort of that kind of leads",
    "start": "699340",
    "end": "705700"
  },
  {
    "text": "nicely to the discussion of distribution and so key distribution is the first",
    "start": "705700",
    "end": "711220"
  },
  {
    "text": "type this is mainly used for your large fact tables tables with billions tens of",
    "start": "711220",
    "end": "718240"
  },
  {
    "text": "billions and trillions rows we use a modular operator on a particular column",
    "start": "718240",
    "end": "724270"
  },
  {
    "text": "and generate a key and that decides which slice the data we land in then is",
    "start": "724270",
    "end": "730720"
  },
  {
    "text": "you'd use you'd also use it where you're using that mostly for the predicate also",
    "start": "730720",
    "end": "735970"
  },
  {
    "text": "for those columns that are used in your group by queries next is even here we",
    "start": "735970",
    "end": "741400"
  },
  {
    "text": "just do round robin and put the data to various slices and then you know there's",
    "start": "741400",
    "end": "747160"
  },
  {
    "text": "another option called all here and I is mainly used for small tables less than",
    "start": "747160",
    "end": "752950"
  },
  {
    "text": "five million where you want the data to appear in every slice for faster performance so now let's talk about how",
    "start": "752950",
    "end": "761290"
  },
  {
    "start": "759000",
    "end": "820000"
  },
  {
    "text": "does redshift what does redshift due to allow you to do the minimum amount of work to get the best performance out so",
    "start": "761290",
    "end": "768660"
  },
  {
    "text": "we talked about columnists storage ready block size is one Meg which is more more",
    "start": "768660",
    "end": "777730"
  },
  {
    "text": "efficient and further reduces the number of i/o requests and and because it's",
    "start": "777730",
    "end": "784270"
  },
  {
    "text": "stored column in a columnar fashion is the same data type so it gets very good compression types so we launched a new",
    "start": "784270",
    "end": "792300"
  },
  {
    "text": "encoding type of jet standard which gives great compression for char varchar",
    "start": "792300",
    "end": "797560"
  },
  {
    "text": "and Jason strings where she also has got an in-memory data data type called zone",
    "start": "797560",
    "end": "805900"
  },
  {
    "text": "maps which kind of contains them min and Max value of every block so when you",
    "start": "805900",
    "end": "812110"
  },
  {
    "text": "were running a predicate filter down it automatically knows not to touch a particular block and hence gives us",
    "start": "812110",
    "end": "818110"
  },
  {
    "text": "great performance right so that's what redshift does for you and what can you do to get and give redshift the best",
    "start": "818110",
    "end": "824920"
  },
  {
    "start": "820000",
    "end": "882000"
  },
  {
    "text": "performance so here let's talk first thing we're going to talk about it skews redshift comes with something called wlm",
    "start": "824920",
    "end": "831790"
  },
  {
    "text": "workload management here you can configure your queues effectively so I",
    "start": "831790",
    "end": "837760"
  },
  {
    "text": "think you come you you are allowed 8q groups or query groups and you can",
    "start": "837760",
    "end": "845770"
  },
  {
    "text": "assign a certain amount of memory and parallelism to each of those and you can",
    "start": "845770",
    "end": "852940"
  },
  {
    "text": "do things like you can you can put some complex logic as well you can say that like any long query with a very high i/o",
    "start": "852940",
    "end": "861490"
  },
  {
    "text": "skew in your query with a segment execution time greater than two minutes",
    "start": "861490",
    "end": "867070"
  },
  {
    "text": "and this cue ratio greater than two I went to hop from that particular queue",
    "start": "867070",
    "end": "872380"
  },
  {
    "text": "to another queue or I just want to log it or I just want to terminate it so you got very good granularity in terms of",
    "start": "872380",
    "end": "879190"
  },
  {
    "text": "what controls you can put in place so how do you do actual performance tuning",
    "start": "879190",
    "end": "884710"
  },
  {
    "start": "882000",
    "end": "952000"
  },
  {
    "text": "so that's a whole lecture on zones so I'm going to skip and give you guys this",
    "start": "884710",
    "end": "889930"
  },
  {
    "text": "URL definitely take a note if we are in the field and we are doing performance analysis for performance tuning for a",
    "start": "889930",
    "end": "897850"
  },
  {
    "text": "customer these are the five steps we follow and so it's a great great five",
    "start": "897850",
    "end": "903730"
  },
  {
    "text": "five step playbook anyone can use so now",
    "start": "903730",
    "end": "908820"
  },
  {
    "text": "optimizing redshift by using schema conversion to SCT or Amazon AWS city",
    "start": "908820",
    "end": "916900"
  },
  {
    "text": "schema conversion tool is a great resource and it does allow to collect",
    "start": "916900",
    "end": "922180"
  },
  {
    "text": "statistics on if you point a city to in class trade uses all the statistics",
    "start": "922180",
    "end": "927370"
  },
  {
    "text": "there and actually suggests two different distribution keys if it thinks",
    "start": "927370",
    "end": "933280"
  },
  {
    "text": "that you can improve performance by changing anything it will suggest distribution keys it will sort keys and",
    "start": "933280",
    "end": "939700"
  },
  {
    "text": "you also generate a PDF report so that's that's very powerful definitely use if",
    "start": "939700",
    "end": "945730"
  },
  {
    "text": "you have a redshift cluster on a regular basis to see if there is a scope for improvement so I want to quickly talk to",
    "start": "945730",
    "end": "954130"
  },
  {
    "start": "952000",
    "end": "1083000"
  },
  {
    "text": "recive spectrum so this is a new functionality or new service that we recently launched and so it's fast at",
    "start": "954130",
    "end": "961450"
  },
  {
    "text": "exabyte scale and that's pretty less a lot lots of data right and is lasting",
    "start": "961450",
    "end": "968230"
  },
  {
    "text": "highly highly available on demand and we cannot say minimum ETL it supports",
    "start": "968230",
    "end": "975520"
  },
  {
    "text": "standard redshift sequels and gives you high concurrency so let's let's quickly",
    "start": "975520",
    "end": "981850"
  },
  {
    "text": "look through how does that work so same process as the previous redshift",
    "start": "981850",
    "end": "988240"
  },
  {
    "text": "architecture diagram we saw and so you'd throw your queries from your bi tool or",
    "start": "988240",
    "end": "994570"
  },
  {
    "text": "your SQL application and the query then gets compiled at the leader node C++",
    "start": "994570",
    "end": "1002820"
  },
  {
    "text": "code is generated query plan is then sent to all the compute nodes here the",
    "start": "1002820",
    "end": "1008760"
  },
  {
    "text": "compute nodes obtained partition inverters find out if the data is amiss 3 or is the data is on on the ratchet",
    "start": "1008760",
    "end": "1016980"
  },
  {
    "text": "itself each compute node issues multiple requests to spectrum layer so that's",
    "start": "1016980",
    "end": "1023040"
  },
  {
    "text": "that's the spectrum layer and for each slice multiple spectrum clusters are",
    "start": "1023040",
    "end": "1028890"
  },
  {
    "text": "assigned here spectrum node scans through your data on",
    "start": "1028890",
    "end": "1034050"
  },
  {
    "text": "s3 and then at this stage it does lots",
    "start": "1034050",
    "end": "1039480"
  },
  {
    "text": "of aggregation predicates filter and does things like joins and projections",
    "start": "1039480",
    "end": "1047100"
  },
  {
    "text": "as well and then finally it gets reduces the data set it was working on and",
    "start": "1047100",
    "end": "1053550"
  },
  {
    "text": "brings it back to redshift cluster because we have less data now and that's the best place within",
    "start": "1053550",
    "end": "1058850"
  },
  {
    "text": "to do things like GroupWise and etc and then finally and the results are",
    "start": "1058850",
    "end": "1065450"
  },
  {
    "text": "generated and finally aggregations happens in the cluster and the results",
    "start": "1065450",
    "end": "1070700"
  },
  {
    "text": "are sent back and to the client application now so that's a quick the",
    "start": "1070700",
    "end": "1075919"
  },
  {
    "text": "only only difference was the extra recive spectrum cluster that gives you that capability to choir files on s3 say",
    "start": "1075919",
    "end": "1084820"
  },
  {
    "start": "1083000",
    "end": "1437000"
  },
  {
    "text": "we're trying to do a quick demo here on exabyte scale data and only joking so",
    "start": "1084820",
    "end": "1091309"
  },
  {
    "text": "this is a screen shot recording of when we did when did you draw on the query so",
    "start": "1091309",
    "end": "1097580"
  },
  {
    "text": "let's let's say and JK Rowling's is going to launch the thirteenth novel and",
    "start": "1097580",
    "end": "1103760"
  },
  {
    "text": "and in a bookstore and work out how many they should stock right so let's say you",
    "start": "1103760",
    "end": "1111140"
  },
  {
    "text": "are a your a bookstore of Amazon scales you know billions and billions of products in your catalog so this may be",
    "start": "1111140",
    "end": "1117980"
  },
  {
    "text": "the first search you're going to do where product title looks like Harry Potter and author is this and then you",
    "start": "1117980",
    "end": "1125900"
  },
  {
    "text": "want to do something you want your now going to join with your daily orders",
    "start": "1125900",
    "end": "1132230"
  },
  {
    "text": "right so here we used a like 140",
    "start": "1132230",
    "end": "1138289"
  },
  {
    "text": "terabytes of data generated every day over a period of twenty years as that",
    "start": "1138289",
    "end": "1145010"
  },
  {
    "text": "kind of get gave exabytes so that s3 dot d underscore customer order item details",
    "start": "1145010",
    "end": "1150380"
  },
  {
    "text": "table is an exabyte table partitioned on s3 and so right so you want to do this",
    "start": "1150380",
    "end": "1158030"
  },
  {
    "text": "and then you want to say that right so I don't I don't want to get all the",
    "start": "1158030",
    "end": "1166159"
  },
  {
    "text": "results but I want to find out my reorder point maybe the first three days is where the things run out and that's",
    "start": "1166159",
    "end": "1171440"
  },
  {
    "text": "where I work out how many I want to reorder and say you you put some some date clauses in you add in you know some",
    "start": "1171440",
    "end": "1180590"
  },
  {
    "text": "other attributes other dimensions to it and then finally you say I'm going to",
    "start": "1180590",
    "end": "1185960"
  },
  {
    "text": "restrict to my region maybe maybe I only care about London or in this this example you use the at all so yeah so",
    "start": "1185960",
    "end": "1194610"
  },
  {
    "text": "that's it and then you kind of run your query so this this is recording and see",
    "start": "1194610",
    "end": "1199660"
  },
  {
    "text": "if it works yeah yeah so and this is a recording simulation of running this on",
    "start": "1199660",
    "end": "1208000"
  },
  {
    "text": "a screen and let's see how long that takes so just just so the comparison so to run this query with exabyte worth of data",
    "start": "1208000",
    "end": "1217500"
  },
  {
    "text": "using a thousand node hive cluster is going to take five years or more and",
    "start": "1217500",
    "end": "1224140"
  },
  {
    "text": "that's because we didn't account for the extra shuffle event and so so let's see",
    "start": "1224140",
    "end": "1231070"
  },
  {
    "text": "how long this takes but it let's let's let's think about do to get result in a",
    "start": "1231070",
    "end": "1238030"
  },
  {
    "text": "certain amount of time where we could still be patient is it needs to be in in a matter of minutes and in order to get",
    "start": "1238030",
    "end": "1244690"
  },
  {
    "text": "a result that quickly what do we need we need billion-fold reduction in the data",
    "start": "1244690",
    "end": "1251200"
  },
  {
    "text": "at least so so there's some magic that",
    "start": "1251200",
    "end": "1256960"
  },
  {
    "text": "not magic not not that's rare spectrum classic does for you but it's more",
    "start": "1256960",
    "end": "1262180"
  },
  {
    "text": "science and engineering so the first thing is the data is compressed right",
    "start": "1262180",
    "end": "1268180"
  },
  {
    "text": "straightaway we get five times compression is columnar format even though the orders table had got 124",
    "start": "1268180",
    "end": "1275680"
  },
  {
    "text": "columns we're only selecting five columns so straightaway we get a good",
    "start": "1275680",
    "end": "1280960"
  },
  {
    "text": "reduction there and this query automatically spectrum decides is going",
    "start": "1280960",
    "end": "1287650"
  },
  {
    "text": "to launch a to not launch is going to use 2500 and node spectrum cluster great",
    "start": "1287650",
    "end": "1294580"
  },
  {
    "text": "so it kind of coming to a million full reduction already now this clever engineering comes into place saying okay",
    "start": "1294580",
    "end": "1301750"
  },
  {
    "text": "we can't reduce static partition elimination because we are only deciding",
    "start": "1301750",
    "end": "1307180"
  },
  {
    "text": "to choose a particular state in us rather than the whole thing so us versus rest of the world and so that gives a",
    "start": "1307180",
    "end": "1314430"
  },
  {
    "text": "good amount of reduction and then we do some dynamic partition elimination and",
    "start": "1314430",
    "end": "1320890"
  },
  {
    "text": "the red shift optimizer works very well as guess what that gives 3.5 billion fold",
    "start": "1320890",
    "end": "1327100"
  },
  {
    "text": "reduction and let's see how our query is doing so quick query is still running as you can see and so you might be thinking",
    "start": "1327100",
    "end": "1335740"
  },
  {
    "text": "why do I need spectrum I don't have exabyte worth of data or you may actually do have X&Y today that's very",
    "start": "1335740",
    "end": "1341830"
  },
  {
    "text": "good and then let's take it so on average a data warehouse grows or",
    "start": "1341830",
    "end": "1347470"
  },
  {
    "text": "doubles this volume and grows ten times every five years we see an average",
    "start": "1347470",
    "end": "1353559"
  },
  {
    "text": "redshift customer doubles data each year and other advantages of spectrum is",
    "start": "1353559",
    "end": "1361389"
  },
  {
    "text": "because you're acquiring data directly on s3 and you are actually reducing some",
    "start": "1361389",
    "end": "1366490"
  },
  {
    "text": "ETL requirements so you don't have to load the data back to a ship to it to conform to a particular schema and then",
    "start": "1366490",
    "end": "1373629"
  },
  {
    "text": "the other advantages various teams can use different tools some one can use",
    "start": "1373629",
    "end": "1379720"
  },
  {
    "text": "Athena someone can use EMR and someone could good use register spectrum all looking at the same data well the",
    "start": "1379720",
    "end": "1386379"
  },
  {
    "text": "results have come back and guess what it came back just less than three minutes yeah yeah so that was pretty",
    "start": "1386379",
    "end": "1395590"
  },
  {
    "text": "incredible and yes a is live recording of Abhishek's screen and doing this",
    "start": "1395590",
    "end": "1403029"
  },
  {
    "text": "query so we basically we had 190 million finds 120 140 terabyte each over 20",
    "start": "1403029",
    "end": "1411730"
  },
  {
    "text": "years that gave you exabyte so as that's very powerful and so we're talking about",
    "start": "1411730",
    "end": "1417879"
  },
  {
    "text": "do we really need spectrum and so yeah and the other things it also gives you a",
    "start": "1417879",
    "end": "1423429"
  },
  {
    "text": "level of concurrency because you can have multiple teams using you know",
    "start": "1423429",
    "end": "1428740"
  },
  {
    "text": "different small recive clusters looking at the same data set and doing the data science querying etc etc right so that",
    "start": "1428740",
    "end": "1436720"
  },
  {
    "text": "other element of a data warehouse is ingestion ETL and bi let's quickly run",
    "start": "1436720",
    "end": "1443649"
  },
  {
    "start": "1437000",
    "end": "1662000"
  },
  {
    "text": "through those so if your data existing data that you need to bring to your data",
    "start": "1443649",
    "end": "1450039"
  },
  {
    "text": "warehouse is in a relational database that DMS or Amazon database migration",
    "start": "1450039",
    "end": "1456129"
  },
  {
    "text": "service is the tool of choice and a simple to use in a minimal down down time you can",
    "start": "1456129",
    "end": "1463309"
  },
  {
    "text": "run in multiple layers EADS is supports most widely used source databases locos",
    "start": "1463309",
    "end": "1472009"
  },
  {
    "text": "fast and easy to set up and reliable and you can do a one-off load you can do",
    "start": "1472009",
    "end": "1477590"
  },
  {
    "text": "things like change data capture as well so every block changes in your source database and this database so Stata base",
    "start": "1477590",
    "end": "1483950"
  },
  {
    "text": "could be your on-premises relational database or on AWS or on RDS anywhere so",
    "start": "1483950",
    "end": "1492590"
  },
  {
    "text": "you know it supports Oracle my sequel sequel server so there's a whole list of it kind of on top of my head and",
    "start": "1492590",
    "end": "1498529"
  },
  {
    "text": "definitely take a look however if you are doing something like extending your",
    "start": "1498529",
    "end": "1504950"
  },
  {
    "text": "on-premises data warehouse to the cloud or you are bringing data from an",
    "start": "1504950",
    "end": "1511220"
  },
  {
    "text": "existing on-premises data warehouse to AWS to redshift this is a very good",
    "start": "1511220",
    "end": "1517190"
  },
  {
    "text": "stretch strategy as well and here you'd use a city schema conversion to schema",
    "start": "1517190",
    "end": "1523129"
  },
  {
    "text": "conversion tools have got has got agents that you will deploy and that is in parallel with you know do small Delta's",
    "start": "1523129",
    "end": "1529759"
  },
  {
    "text": "and load it to s3 it also manages the load from s3 and to redshift as well the",
    "start": "1529759",
    "end": "1538009"
  },
  {
    "text": "copy command leverages redshifts massively parallel MPP architecture to",
    "start": "1538009",
    "end": "1543919"
  },
  {
    "text": "read and load data in parallel from files in s3 bucket you can take the maximum advantage of parallel processing",
    "start": "1543919",
    "end": "1550190"
  },
  {
    "text": "by splitting your data into multiple files and by setting distribution keys",
    "start": "1550190",
    "end": "1555440"
  },
  {
    "text": "on your tables so that's another way to learn data right and then let's quickly",
    "start": "1555440",
    "end": "1563389"
  },
  {
    "text": "talk about AWS glue it's a fully managed ETL service that makes it easy to move",
    "start": "1563389",
    "end": "1568549"
  },
  {
    "text": "data between your data stores AWS glue integrates with s 3 RDS Amazon redshift",
    "start": "1568549",
    "end": "1575230"
  },
  {
    "text": "you can connect to any JDBC compliant data source and glue automatically",
    "start": "1575230",
    "end": "1581119"
  },
  {
    "text": "crawls your data source provided you give you the permission to crawl your data the data source and creates a",
    "start": "1581119",
    "end": "1587869"
  },
  {
    "text": "catalog it identified that if data formers different data types and it",
    "start": "1587869",
    "end": "1593179"
  },
  {
    "text": "also suggests different transformations you may want to run on it it runs the",
    "start": "1593179",
    "end": "1598790"
  },
  {
    "text": "code it generates is Python but it's all visual but you can you know flip back to look at the code itself you can edit the",
    "start": "1598790",
    "end": "1605780"
  },
  {
    "text": "code your own IDE you can combine it with a notebook like Zeppelin notebook",
    "start": "1605780",
    "end": "1611500"
  },
  {
    "text": "and yeah and the other thing you also provides it provides a scheduler and",
    "start": "1611500",
    "end": "1617900"
  },
  {
    "text": "when to run the job you know how to get a trigger etc and say here I have we",
    "start": "1617900",
    "end": "1626030"
  },
  {
    "text": "recently published this blog where to manage up source into redshift using AWS",
    "start": "1626030",
    "end": "1631220"
  },
  {
    "text": "glue and sneaker which is an open-source sequel engine and so definitely take a look at that and yeah so this session",
    "start": "1631220",
    "end": "1639980"
  },
  {
    "text": "recommend using your phone's so yeah they go quite a few things that you may want to take note of and then we also",
    "start": "1639980",
    "end": "1646490"
  },
  {
    "text": "have a very very vibrant partner ecosystem with lots of ETL tools right",
    "start": "1646490",
    "end": "1652150"
  },
  {
    "text": "like informatica snap logic material so definitely take a look at that and if that's something you",
    "start": "1652150",
    "end": "1658340"
  },
  {
    "text": "want to use or you're already using it you may want to continue using it as well so in the when it comes to BI",
    "start": "1658340",
    "end": "1665830"
  },
  {
    "start": "1662000",
    "end": "1748000"
  },
  {
    "text": "visualization quick site is a very good solution you can immediately this",
    "start": "1665830",
    "end": "1671590"
  },
  {
    "text": "advantage of using quick side is you can get started immediately it's like a SAS service and in a go-ahead it's ready for",
    "start": "1671590",
    "end": "1679580"
  },
  {
    "text": "you you point your quick site to your redshift cluster if it's in you know in",
    "start": "1679580",
    "end": "1685940"
  },
  {
    "text": "the same account then it'll automatically this discovery provider you give the permissions and then one of",
    "start": "1685940",
    "end": "1694100"
  },
  {
    "text": "the other options is maybe you can use the in-memory engine of a quick side",
    "start": "1694100",
    "end": "1700460"
  },
  {
    "text": "like spice to bring in some of the datasets for faster performance as well so yeah that's that's quite popular and",
    "start": "1700460",
    "end": "1707510"
  },
  {
    "text": "also you have a very vibrant ecosystem of be honest solutions available in",
    "start": "1707510",
    "end": "1714500"
  },
  {
    "text": "Tabler typical MicroStrategy Luka you name it as well so when comes to",
    "start": "1714500",
    "end": "1722090"
  },
  {
    "text": "advanced analytics where you want to do things like in a UDF's spider python UDF's machine learnings of data science",
    "start": "1722090",
    "end": "1729260"
  },
  {
    "text": "we have a quite a few tools that connect allows you to do things like those like",
    "start": "1729260",
    "end": "1734809"
  },
  {
    "text": "our SAS but if there is a particular programming language you think could benefit and to integrate directly with",
    "start": "1734809",
    "end": "1741980"
  },
  {
    "text": "worshipped apart from Python which we currently support to our UDF's let us know we'll definitely kinter and it's a",
    "start": "1741980",
    "end": "1749330"
  },
  {
    "start": "1748000",
    "end": "1803000"
  },
  {
    "text": "quick quick look at the partner ecosystem so four types of partners we talked about the ETL data integration",
    "start": "1749330",
    "end": "1755210"
  },
  {
    "text": "partners we talked about the BI partners we also have a whole list of system integration and consulting partners who",
    "start": "1755210",
    "end": "1761390"
  },
  {
    "text": "can work with you to deliver those data warehouses on our native yes some of some of them are here in meeting as well",
    "start": "1761390",
    "end": "1767360"
  },
  {
    "text": "and then we also have tools and products partner products to do your query and",
    "start": "1767360",
    "end": "1772929"
  },
  {
    "text": "data modelling as well on redshift just",
    "start": "1772929",
    "end": "1778130"
  },
  {
    "text": "just wanted to show this once on June 15 for us to publish the big data warehouse",
    "start": "1778130",
    "end": "1785120"
  },
  {
    "text": "q2 2017 in which AWS is positioned as a leader according to the Forrester with",
    "start": "1785120",
    "end": "1791750"
  },
  {
    "text": "more than 5,000 deployments Amazon where she's the largest data warehouse deployments in the cloud so yeah it's",
    "start": "1791750",
    "end": "1799429"
  },
  {
    "text": "something to keep in mind and if you want to read the report there's small links to that as well so now a little",
    "start": "1799429",
    "end": "1805580"
  },
  {
    "start": "1803000",
    "end": "1902000"
  },
  {
    "text": "bit of focus on migrations so take a look at this blog we wrote where we can",
    "start": "1805580",
    "end": "1812630"
  },
  {
    "text": "do like Oracle migrations to redshifts oracle data warehouse redshift migrations if that's something you want",
    "start": "1812630",
    "end": "1818360"
  },
  {
    "text": "to do or if you want to extend your existing on-prem cloud oracle data warehouse to redshift and you know take",
    "start": "1818360",
    "end": "1825980"
  },
  {
    "text": "care of the scalability challenges that you currently have and this this may be a very good blog to read and then I",
    "start": "1825980",
    "end": "1836750"
  },
  {
    "text": "would say that yeah this is a I recently",
    "start": "1836750",
    "end": "1842600"
  },
  {
    "text": "wrote this one actually it's the terror data to redshift is very good you know on premises you can run Tara",
    "start": "1842600",
    "end": "1849350"
  },
  {
    "text": "later on premises or you can run Tara data on AWS cloud if you want to extend that to redshift or you know if you want",
    "start": "1849350",
    "end": "1856100"
  },
  {
    "text": "to take advantage of redshift and performance cost etc and then this is",
    "start": "1856100",
    "end": "1862160"
  },
  {
    "text": "this is a good place to start as well it goes to step by step of how you would want to set up the migration and then I",
    "start": "1862160",
    "end": "1872410"
  },
  {
    "text": "I would say this this is another one where you want to converge data silos to",
    "start": "1872410",
    "end": "1878840"
  },
  {
    "text": "your data warehouse so you can have multiple in a relational data sources",
    "start": "1878840",
    "end": "1883880"
  },
  {
    "text": "and you are you can bring all of those using DMS to a single redshift cluster",
    "start": "1883880",
    "end": "1889100"
  },
  {
    "text": "where you kind of gain insights which you couldn't do previously because you couldn't combine all those databases",
    "start": "1889100",
    "end": "1894590"
  },
  {
    "text": "together so that's a good good block to read and then finally I would love to",
    "start": "1894590",
    "end": "1901610"
  },
  {
    "text": "invite you guys to come and join and redshift meetup community the next",
    "start": "1901610",
    "end": "1907520"
  },
  {
    "start": "1902000",
    "end": "2155000"
  },
  {
    "text": "session is next month 26th of October RSVP opens on the 6th of October and you",
    "start": "1907520",
    "end": "1915260"
  },
  {
    "text": "know normally have a very good exciting set of speakers from our customers and partners we're going to talk kind of",
    "start": "1915260",
    "end": "1923120"
  },
  {
    "text": "have an agenda yen Robinson's Moe's presents the first 20 minutes talking about new features since the last meter",
    "start": "1923120",
    "end": "1929300"
  },
  {
    "text": "which we had I think in July and we talked we and Cibola has also agreed to",
    "start": "1929300",
    "end": "1935960"
  },
  {
    "text": "present about the data science platform and that being for their customers and",
    "start": "1935960",
    "end": "1940970"
  },
  {
    "text": "we still looking for another presenter we got a few proposals well if you want to propose talk and that please get in",
    "start": "1940970",
    "end": "1948110"
  },
  {
    "text": "touch and so yeah I mean that's pretty much what I wanted to cover and ready to",
    "start": "1948110",
    "end": "1953720"
  },
  {
    "text": "take questions ready any question on Twitter",
    "start": "1953720",
    "end": "1960610"
  },
  {
    "text": "so DBA work is not gone it's reduced and we're DBA can focus on in a performance",
    "start": "1975309",
    "end": "1983030"
  },
  {
    "text": "and supporting business cases business requirements etc by by getting rid of",
    "start": "1983030",
    "end": "1989390"
  },
  {
    "text": "like you know when it when a node fails and all those things have been taken out because it's a managed service those",
    "start": "1989390",
    "end": "1995299"
  },
  {
    "text": "things are taken care of but you know statistics are always generated and you can always want to improve performance",
    "start": "1995299",
    "end": "2001570"
  },
  {
    "text": "new business cases comes in and how do you support the business with you sir you know DBA functionality is still",
    "start": "2001570",
    "end": "2007030"
  },
  {
    "text": "absolutely there slightly reduced you don't want to do for example patching",
    "start": "2007030",
    "end": "2012220"
  },
  {
    "text": "and patching a carnal things like that that's kind of disappeared but it's kind",
    "start": "2012220",
    "end": "2018280"
  },
  {
    "text": "of more gears to supporting the business and innovations spectrum so you can load",
    "start": "2018280",
    "end": "2041980"
  },
  {
    "text": "and Orsi data remember talking I had you glue supports it in a moment I think it",
    "start": "2041980",
    "end": "2049570"
  },
  {
    "text": "does let me check on that and then you can use obviously glue to ETL that back to redshift you can do that however if",
    "start": "2049570",
    "end": "2057040"
  },
  {
    "text": "you have a spectrum table which is an external table so you just say create",
    "start": "2057040",
    "end": "2062440"
  },
  {
    "text": "external table this and point to the bucket s3 bucket on redshift and that's",
    "start": "2062440",
    "end": "2068710"
  },
  {
    "text": "creates the spectrum table and then you can directly query the orc I think or Parque average Jason all these are",
    "start": "2068710",
    "end": "2077679"
  },
  {
    "text": "supported but just double check Park it definitely several different is I just want to see I think it is spectrum is",
    "start": "2077679",
    "end": "2087280"
  },
  {
    "text": "not available in UK yet coming soon and",
    "start": "2087280",
    "end": "2092378"
  },
  {
    "text": "we don't normally talk about many gates etc but let us take this offline router",
    "start": "2092379",
    "end": "2098280"
  },
  {
    "text": "until you soon",
    "start": "2098280",
    "end": "2101700"
  },
  {
    "text": "yeah good good question and say the",
    "start": "2104540",
    "end": "2111120"
  },
  {
    "text": "equation changed slightly with the introduction of spectrum so let's talk about pre spectrum or post spectrum on",
    "start": "2111120",
    "end": "2117810"
  },
  {
    "text": "peas peas pea Priya spectrum one when you have a data warehouse redshift data warehouse and see a case you know your",
    "start": "2117810",
    "end": "2125970"
  },
  {
    "text": "database users have grown your load has grown your bi queries have bred you now",
    "start": "2125970",
    "end": "2132480"
  },
  {
    "text": "need to scale it right so remember that playbook I showed you guys the first",
    "start": "2132480",
    "end": "2138360"
  },
  {
    "text": "step would be to absolutely step-by-step",
    "start": "2138360",
    "end": "2145230"
  },
  {
    "text": "go through where's it going I think it's been one of the early slides go through",
    "start": "2145230",
    "end": "2152970"
  },
  {
    "text": "all of these steps absolutely step by step before you go ahead and do like add nodes and all that stuff and and then I",
    "start": "2152970",
    "end": "2161220"
  },
  {
    "start": "2155000",
    "end": "2235000"
  },
  {
    "text": "at the end of the exercise you may work out okay so I actually I would get a linear performance improvement if I add",
    "start": "2161220",
    "end": "2168740"
  },
  {
    "text": "three more nodes and you've worked it out so that's kind of the pre pre",
    "start": "2168740",
    "end": "2175260"
  },
  {
    "text": "spectrum world post spectrum world if you have in your region spectrum then you can start thinking things like okay",
    "start": "2175260",
    "end": "2181980"
  },
  {
    "text": "so let's look at this additional node that I can't satisfy with my existing",
    "start": "2181980",
    "end": "2187950"
  },
  {
    "text": "cluster can i isolate it can I spin up another small red shift cluster with",
    "start": "2187950",
    "end": "2194400"
  },
  {
    "text": "spectrum tables looking at the same data right so now you increased you",
    "start": "2194400",
    "end": "2199790"
  },
  {
    "text": "concurrency and added more users but",
    "start": "2199790",
    "end": "2205170"
  },
  {
    "text": "looking at the same data and producing different even though it's the same data but you're producing different kind of",
    "start": "2205170",
    "end": "2210750"
  },
  {
    "text": "analysis so that definitely increases so yes it think through this but whatever",
    "start": "2210750",
    "end": "2216600"
  },
  {
    "text": "you do please go through all these steps and then make make a decision what you",
    "start": "2216600",
    "end": "2221700"
  },
  {
    "text": "want to do",
    "start": "2221700",
    "end": "2224089"
  },
  {
    "text": "good question but not necessarily that may well be the case but not necessarily",
    "start": "2235230",
    "end": "2240370"
  },
  {
    "text": "and you could you'd have to do that from trial and error and see how that works and because you know as you as we saw",
    "start": "2240370",
    "end": "2247900"
  },
  {
    "text": "the breakdown of their processes they're certain certain sort of processing happens and spectrum nerds certain",
    "start": "2247900",
    "end": "2254440"
  },
  {
    "text": "processing happens on and on the red shift note so the best practice that I",
    "start": "2254440",
    "end": "2260080"
  },
  {
    "text": "recommend when I'm doing a performance tuning exercise is to think backwards don't think cluster at all don't think",
    "start": "2260080",
    "end": "2267280"
  },
  {
    "text": "of the anything like a cluster exists first thing what is your query who is the consumer what is the query you're",
    "start": "2267280",
    "end": "2274030"
  },
  {
    "text": "trying to solve what are the isolate all the queries you normally see 90% of the",
    "start": "2274030",
    "end": "2279700"
  },
  {
    "text": "queries are actually not 90% of the load is coming from probably 10% of the queries right so isolate those queries",
    "start": "2279700",
    "end": "2286390"
  },
  {
    "text": "and see how do you satisfy those using distribution keys and sort keys etc",
    "start": "2286390",
    "end": "2293890"
  },
  {
    "text": "first and then decide what you need to do with the cluster this is second step",
    "start": "2293890",
    "end": "2300760"
  },
  {
    "text": "to do the answer question any other question I know we already I think I did",
    "start": "2300760",
    "end": "2306820"
  },
  {
    "text": "run a bit too fast sorry gentlemen the back first",
    "start": "2306820",
    "end": "2314520"
  },
  {
    "text": "yeah",
    "start": "2319520",
    "end": "2322520"
  },
  {
    "start": "2337000",
    "end": "2434000"
  },
  {
    "text": "so your challenge is mainly on the ETL pipeline right definitely can recommend",
    "start": "2337310",
    "end": "2345290"
  },
  {
    "text": "lots of things can we take this offline because I think that we have to go to step by step looking at where the",
    "start": "2345290",
    "end": "2350660"
  },
  {
    "text": "contentions are and then work it out and what tool said for example you are currently using to bring the data data",
    "start": "2350660",
    "end": "2356660"
  },
  {
    "text": "together so we even though we recommend SCT and what we would say is you'd use",
    "start": "2356660",
    "end": "2363110"
  },
  {
    "text": "SCT to have multiple agents looking at",
    "start": "2363110",
    "end": "2368900"
  },
  {
    "text": "different schemas and different tables so you split the entire pipeline into s3",
    "start": "2368900",
    "end": "2375290"
  },
  {
    "text": "first load then split the workload on your source Terra data into different",
    "start": "2375290",
    "end": "2383780"
  },
  {
    "text": "SCT agents managing in each individual schemas and tables and break it down and",
    "start": "2383780",
    "end": "2389600"
  },
  {
    "text": "bring entire thing to s3 and once the data is in s3 then we choose right toolset because",
    "start": "2389600",
    "end": "2397010"
  },
  {
    "text": "then you get the advantage of in a distributed object store where you can do lots of tuning in terms of how you",
    "start": "2397010",
    "end": "2403340"
  },
  {
    "text": "load the data direction but it's quite a detailed analysis not that complex but quite detailed you have to dig down into",
    "start": "2403340",
    "end": "2410120"
  },
  {
    "text": "each of the steps let's take this offline this gentleman sorry because",
    "start": "2410120",
    "end": "2418690"
  },
  {
    "text": "actually",
    "start": "2427089",
    "end": "2430089"
  },
  {
    "text": "no no a city does all of it the a city agents are deployed as you",
    "start": "2433520",
    "end": "2440130"
  },
  {
    "start": "2434000",
    "end": "2493000"
  },
  {
    "text": "can have multiple of those agents so let's I think so there are multiple SCT",
    "start": "2440130",
    "end": "2450869"
  },
  {
    "text": "agents you install and all of those yeah",
    "start": "2450869",
    "end": "2459960"
  },
  {
    "text": "I don't know where is the diagram gone and so all of those who work in parallel and it will orchestrate the entire",
    "start": "2459960",
    "end": "2466770"
  },
  {
    "text": "process so it will get those changes instead of in many batches micro batches",
    "start": "2466770",
    "end": "2473940"
  },
  {
    "text": "and put it to the s3 and once it's near three it knows what's in history and it'll load back that to those wretched",
    "start": "2473940",
    "end": "2479609"
  },
  {
    "text": "it does the entire process",
    "start": "2479609",
    "end": "2483019"
  },
  {
    "text": "interesting question M so what rather than benefit what what",
    "start": "2492560",
    "end": "2499830"
  },
  {
    "start": "2493000",
    "end": "2589000"
  },
  {
    "text": "is what I would like you to look at is your usage pattern so if you're doing an",
    "start": "2499830",
    "end": "2505920"
  },
  {
    "text": "ad hoc expiratory nature you just use the athena first because out of the box",
    "start": "2505920",
    "end": "2512700"
  },
  {
    "text": "it's just don't there's no concept of cluster is server less sequel immediately grow and explore the data",
    "start": "2512700",
    "end": "2519120"
  },
  {
    "text": "and your next stage you'd be alright there is some gem in this data I",
    "start": "2519120",
    "end": "2524340"
  },
  {
    "text": "actually need to bring it to my data warehouse and then I don't expose it to my dashboard and get my CEO to look at",
    "start": "2524340",
    "end": "2531390"
  },
  {
    "text": "it and you know do whatever decisions today that could be a process flow so Athena could be the first thing that",
    "start": "2531390",
    "end": "2537720"
  },
  {
    "text": "does the first exploration and then your next step is using spectrum and redshift",
    "start": "2537720",
    "end": "2543090"
  },
  {
    "text": "to give that stuff so kind of a bow tags coexist they would never be in a",
    "start": "2543090",
    "end": "2549990"
  },
  {
    "text": "situation where this I don't need this because you know Tina gives you that agility and spectrum will give you that",
    "start": "2549990",
    "end": "2556830"
  },
  {
    "text": "familiarity with your data warehouse and you see Google search Hey",
    "start": "2556830",
    "end": "2566260"
  },
  {
    "text": "paper query spectrum is paper query as well so both the same same cost same charge model yeah sir say you'd you say",
    "start": "2566260",
    "end": "2591220"
  },
  {
    "start": "2589000",
    "end": "2667000"
  },
  {
    "text": "the first one the key distribution you definitely use for your larger fax tables say in a star schema world right",
    "start": "2591220",
    "end": "2599080"
  },
  {
    "text": "and so in in a redshift world have completely flat table is totally fine",
    "start": "2599080",
    "end": "2604420"
  },
  {
    "text": "right you don't always need to think of a star schema it's totally fine either either is fine and so so if you have",
    "start": "2604420",
    "end": "2613120"
  },
  {
    "text": "those you know multi-billion ma you know or trillion row tables that's where you",
    "start": "2613120",
    "end": "2618340"
  },
  {
    "text": "kind of think of key for example that's where you need that performance it's the medium-size tables dimension",
    "start": "2618340",
    "end": "2626440"
  },
  {
    "text": "tables you'd think of the round-robin approach but if you have very small",
    "start": "2626440",
    "end": "2632560"
  },
  {
    "text": "lookup table is less than 5 million raised and that you use in most queries and then you might as well you get extra",
    "start": "2632560",
    "end": "2640390"
  },
  {
    "text": "advantage of redshift having respected having the compression so you might have to put that in all across all slices and",
    "start": "2640390",
    "end": "2646270"
  },
  {
    "text": "that's where you would use even even distribution keys",
    "start": "2646270",
    "end": "2651060"
  },
  {
    "text": "you know normally don't answer competitive questions like that however you think red versus advantages spectrum",
    "start": "2666610",
    "end": "2674510"
  },
  {
    "start": "2667000",
    "end": "2733000"
  },
  {
    "text": "and sorry the question was apologists the question was how do you",
    "start": "2674510",
    "end": "2680330"
  },
  {
    "text": "differentiate red shift from another cloud data warehouse like snowflake so",
    "start": "2680330",
    "end": "2688000"
  },
  {
    "text": "snowflakes key advantage is be able to run queries on s3 and with redshift with",
    "start": "2688000",
    "end": "2694700"
  },
  {
    "text": "spectrum gives you the same advantage as well and so it's not about advantages whatever you are familiar with how",
    "start": "2694700",
    "end": "2701620"
  },
  {
    "text": "snowflake is a very good partner of ours we have customers in a you respect",
    "start": "2701620",
    "end": "2706960"
  },
  {
    "text": "snowflake we have customers using redshift spectrum so hopefully that",
    "start": "2706960",
    "end": "2713360"
  },
  {
    "text": "answers your question you need to do a bake off into you need to do a matrix and see what works best",
    "start": "2713360",
    "end": "2718430"
  },
  {
    "text": "for you and there are certain advantages in a redshift and obviously certain",
    "start": "2718430",
    "end": "2725180"
  },
  {
    "text": "bunch of other product as well",
    "start": "2725180",
    "end": "2728500"
  },
  {
    "start": "2733000",
    "end": "2758000"
  },
  {
    "text": "we don't have dates yet so is there a release date for as spectrum in the UK",
    "start": "2733760",
    "end": "2740190"
  },
  {
    "text": "and all I can tell you is soon we don't we don't have exact days and we don't",
    "start": "2740190",
    "end": "2746490"
  },
  {
    "text": "publicly announce days we can talk offline and really no gluey is publicly",
    "start": "2746490",
    "end": "2759630"
  },
  {
    "start": "2758000",
    "end": "2792000"
  },
  {
    "text": "generally available is is available in most US regions I think four of those",
    "start": "2759630",
    "end": "2765590"
  },
  {
    "text": "and and light spectrum is coming soon to",
    "start": "2765590",
    "end": "2771990"
  },
  {
    "text": "the EU regions as well very very very",
    "start": "2771990",
    "end": "2780600"
  },
  {
    "text": "very good question and can I take this offline",
    "start": "2780600",
    "end": "2785030"
  },
  {
    "text": "sorry let me take you question I think you deserve a little bit more answer because of that differences between data",
    "start": "2791970",
    "end": "2799900"
  },
  {
    "start": "2792000",
    "end": "2853000"
  },
  {
    "text": "pipeline and glue right so as glue stands has been releasing GA the in the",
    "start": "2799900",
    "end": "2807520"
  },
  {
    "text": "solution you'd see data pipeline has got certain advantages because it does certain things better but if you look at",
    "start": "2807520",
    "end": "2815230"
  },
  {
    "text": "glue how is you know the users open source Python how the pipeline's is a more visual and stuff so that there are",
    "start": "2815230",
    "end": "2820810"
  },
  {
    "text": "certain advantages that as well but you know I'll you know you'd see",
    "start": "2820810",
    "end": "2826660"
  },
  {
    "text": "what what if you can cover your use case can't use case with glue I'll recommend you use glue if you if you can wait",
    "start": "2826660",
    "end": "2833230"
  },
  {
    "text": "until a certain Delta feature that you need that comes into glue you might as well wait that will that make sense",
    "start": "2833230",
    "end": "2840099"
  },
  {
    "text": "apologies young repeat the question last question by the way",
    "start": "2840099",
    "end": "2845940"
  },
  {
    "text": "so is spectrum query 2 s 3 regions limited to that particular region great",
    "start": "2852380",
    "end": "2859430"
  },
  {
    "start": "2853000",
    "end": "2883000"
  },
  {
    "text": "question currently as it stands you can",
    "start": "2859430",
    "end": "2864680"
  },
  {
    "text": "only run queries in that particular region obviously think of the cross region in a query and stuff weather is",
    "start": "2864680",
    "end": "2871789"
  },
  {
    "text": "going to be supported in the future or not something we can discuss later thank you everyone I think I can take any",
    "start": "2871789",
    "end": "2878690"
  },
  {
    "text": "questions offline and thank you for listening bye prestina [Applause]",
    "start": "2878690",
    "end": "2885550"
  }
]