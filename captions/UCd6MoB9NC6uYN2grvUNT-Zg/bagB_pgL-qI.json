[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "can you guys hear now all right great cool all right all right sorry about",
    "start": "709",
    "end": "7290"
  },
  {
    "text": "that folks but uh welcome to the Big Data meets machine learning session my name is Bruno Faria and I'm a senior EMR",
    "start": "7290",
    "end": "13889"
  },
  {
    "text": "Solutions Architect at AWS and in this session you're going to learn how to architect a a spark",
    "start": "13889",
    "end": "21150"
  },
  {
    "text": "environment for data scientists so before I actually get started there are some things that I would like to talk",
    "start": "21150",
    "end": "26279"
  },
  {
    "text": "about such as the challenge now I really do thank everyone for coming here especially because this is",
    "start": "26279",
    "end": "32040"
  },
  {
    "text": "one of the later sessions or the last session of the day and hopefully we're making this exciting and fun enough and",
    "start": "32040",
    "end": "37290"
  },
  {
    "text": "that way no one of a no one falls asleep here today so let's go ahead and get started and the first thing that I like",
    "start": "37290",
    "end": "44280"
  },
  {
    "text": "to talk about really is the machine learning of assumption versus reality",
    "start": "44280",
    "end": "49860"
  },
  {
    "text": "type of thing this is something that we hear very often when talking to our customers and one of the first",
    "start": "49860",
    "end": "55410"
  },
  {
    "start": "54000",
    "end": "74000"
  },
  {
    "text": "assumption about machine learning pipelines today is that most of the work is actually spent on the model",
    "start": "55410",
    "end": "61890"
  },
  {
    "text": "architecture or actually with the data scientist configuring and apply their machine learning models or model",
    "start": "61890",
    "end": "68100"
  },
  {
    "text": "specific data processing that's the assumption however this this actual",
    "start": "68100",
    "end": "73170"
  },
  {
    "text": "assumptions is far from the reality and the reality of things is that where most of the work in a big data pipeline takes",
    "start": "73170",
    "end": "80759"
  },
  {
    "start": "74000",
    "end": "170000"
  },
  {
    "text": "is actually during the processing phase so nowadays a real-life machine learning",
    "start": "80759",
    "end": "86729"
  },
  {
    "text": "application doesn't just require more than predicting with a single model so for example your data may need some kind",
    "start": "86729",
    "end": "92790"
  },
  {
    "text": "of pre-processing you may need to do feature engineering normalization of the data and you also may need to do things",
    "start": "92790",
    "end": "99270"
  },
  {
    "text": "such as dimensionality reduction so all of these things is done on the data",
    "start": "99270",
    "end": "104700"
  },
  {
    "text": "pre-processing or data processing side of things and this is where we have this assumption vs. reality where we see",
    "start": "104700",
    "end": "111810"
  },
  {
    "text": "customers often thinking that most of the work is gonna be spent with the data scientist team however you need to have",
    "start": "111810",
    "end": "118560"
  },
  {
    "text": "good data in order to have a successful machine learning algorithm or machine learning application and most of you",
    "start": "118560",
    "end": "124799"
  },
  {
    "text": "probably already heard about this so it's not enough just to have the data to back up but oftentimes that data is",
    "start": "124799",
    "end": "130709"
  },
  {
    "text": "gonna be raw is gonna be kind of format that does not apply for you does not work with your machine learning algorithm so there's going to",
    "start": "130709",
    "end": "137900"
  },
  {
    "text": "be a lot of a pre-processing involved so this also takes us to another kind of",
    "start": "137900",
    "end": "144200"
  },
  {
    "text": "work where nowadays we have sort of or both of these data engineering and data scientists they overlap where oftentimes",
    "start": "144200",
    "end": "151940"
  },
  {
    "text": "they need to do similar kind of work in order to have a successful machine learning application and this is the",
    "start": "151940",
    "end": "158870"
  },
  {
    "text": "challenge that we're going to be talking about today and how can you overcome this challenges and have this kind of",
    "start": "158870",
    "end": "165260"
  },
  {
    "text": "workflow where you have big data as well as being apply with your machine learning so part of the agenda today the",
    "start": "165260",
    "end": "172220"
  },
  {
    "start": "170000",
    "end": "217000"
  },
  {
    "text": "first thing that I'm going to be talking about is a little bit about spark apache spark then i'm gonna move on to talk",
    "start": "172220",
    "end": "178670"
  },
  {
    "text": "about amazon EMR and sage maker and then I'll show you how of these services can",
    "start": "178670",
    "end": "183799"
  },
  {
    "text": "be integrated together to create a machine learning workflow or a pipeline and then lastly I'll be showing this",
    "start": "183799",
    "end": "190630"
  },
  {
    "text": "finishing this off with a demo so I do have a devil that's about 15 minutes long where I'll be putting everything",
    "start": "190630",
    "end": "196940"
  },
  {
    "text": "into action and showing you everything that I've talked about today so I know a lot of times you might be talking over",
    "start": "196940",
    "end": "202489"
  },
  {
    "text": "here all day but if you don't see a demo or something in action it might not be real so in this case I'm going to be",
    "start": "202489",
    "end": "208609"
  },
  {
    "text": "showing that everything that I am talking about Israel and it is something that customers are used in production",
    "start": "208609",
    "end": "213650"
  },
  {
    "text": "today so the demo is going to be at the end of this so let's go ahead and get started and the first thing that I would",
    "start": "213650",
    "end": "220100"
  },
  {
    "start": "217000",
    "end": "321000"
  },
  {
    "text": "like to talk about is spark so Apache spark is an open source distributed",
    "start": "220100",
    "end": "225370"
  },
  {
    "text": "framework for data processing so in other words you can use spark in order",
    "start": "225370",
    "end": "230959"
  },
  {
    "text": "to make your application parallel or distributed in order to process massive amount of data now spark also has a",
    "start": "230959",
    "end": "238700"
  },
  {
    "text": "memory compiler computing abilities but not only that it is also optimized for",
    "start": "238700",
    "end": "243769"
  },
  {
    "text": "high performance now when I say high performance just to give an idea of how performance Park really is a typical",
    "start": "243769",
    "end": "251989"
  },
  {
    "text": "spark application can be up to a hundred times faster than a Hadoop MapReduce application so this kind of really show",
    "start": "251989",
    "end": "259370"
  },
  {
    "text": "you where spark high performance comes from and also spark is not only used for a",
    "start": "259370",
    "end": "264889"
  },
  {
    "text": "single use case there are various use cases that spark can be applied with for example we can use Park for batch data",
    "start": "264889",
    "end": "272000"
  },
  {
    "text": "processing as well as streaming data for use cases where you have to do near real-time computations in a spark also",
    "start": "272000",
    "end": "280100"
  },
  {
    "text": "offers machine learning libraries as well as graph databases and a sequel interface now the best thing about SPARC",
    "start": "280100",
    "end": "286970"
  },
  {
    "text": "is that it also has native api's for Java Scala Python sequel in art so",
    "start": "286970",
    "end": "293060"
  },
  {
    "text": "chances are that a lot of data scientists or most data scientists they're familiar with one of these languages so that means that a data",
    "start": "293060",
    "end": "299930"
  },
  {
    "text": "scientist can easily pick up on SPARC and start developing a spark application right away it's very easy to use not",
    "start": "299930",
    "end": "307220"
  },
  {
    "text": "only that but the SPARC API is very concise and that means that oftentimes you can reduce the amount of line the",
    "start": "307220",
    "end": "314840"
  },
  {
    "text": "amount of lines on your machine learning application or even reduce the complexity of that application and there",
    "start": "314840",
    "end": "321740"
  },
  {
    "start": "321000",
    "end": "413000"
  },
  {
    "text": "is one particular component of spark that I like to focus on for the next couple of sessions and that's going to",
    "start": "321740",
    "end": "327949"
  },
  {
    "text": "be the spark data frames now the data frames there they are a distributed",
    "start": "327949",
    "end": "333380"
  },
  {
    "text": "collection of data organized into named columns so what do I mean by this is that you can think of a data frame as",
    "start": "333380",
    "end": "339949"
  },
  {
    "text": "the equivalent of a table within a database it has your schema it has your columns as well as your rows and also",
    "start": "339949",
    "end": "347840"
  },
  {
    "text": "the data frame can be constructed from various different sources including external databases you can also use",
    "start": "347840",
    "end": "354560"
  },
  {
    "text": "structure files and even exist in rdd's now rdd's stands for resilient",
    "start": "354560",
    "end": "359900"
  },
  {
    "text": "distributed data set and that's also another component of spark that i'm not going to talk too much about it today",
    "start": "359900",
    "end": "364940"
  },
  {
    "text": "but I just know that rdd's is one of the lower levels api of spark is what spark",
    "start": "364940",
    "end": "370190"
  },
  {
    "text": "had available when initially was developed but nowadays most of the work",
    "start": "370190",
    "end": "375620"
  },
  {
    "text": "with spark is actually done using the data frame API instead of rdd's but just",
    "start": "375620",
    "end": "381830"
  },
  {
    "text": "know that you can still construct data frames from an RDD also a data frame",
    "start": "381830",
    "end": "387650"
  },
  {
    "text": "supports various different file formats that includes text CSV JSON or our CIO",
    "start": "387650",
    "end": "393680"
  },
  {
    "text": "parkade many more it's you an exemple during the demo I do have an example I'm loading data from lib spm",
    "start": "393680",
    "end": "401330"
  },
  {
    "text": "format and that's the format that's really it's optimized for machine learning certain machine learning",
    "start": "401330",
    "end": "407269"
  },
  {
    "text": "algorithms and this is also another format that's natively supported by SPARC now let's talk about another",
    "start": "407269",
    "end": "415849"
  },
  {
    "start": "413000",
    "end": "508000"
  },
  {
    "text": "component of spark which this is one of the core components of this session today and that sparked ml lib or machine",
    "start": "415849",
    "end": "422689"
  },
  {
    "text": "learning library and now my live is sparks machine learning library on top of the data frame API now there was also",
    "start": "422689",
    "end": "430579"
  },
  {
    "text": "a spark does support ml Lib with RTD however the spark documentation stated",
    "start": "430579",
    "end": "436639"
  },
  {
    "text": "that it's currently in maintenance mode so they're not adding any new features to it and on top of that the spark",
    "start": "436639",
    "end": "442819"
  },
  {
    "text": "documentation also mentions that starting with spark 3 it looks like they planned on deprecating RTD altogether so",
    "start": "442819",
    "end": "450589"
  },
  {
    "text": "this is yet another reason why I'm focusing primarily on using data frames here today so with the ml lib or Sparks",
    "start": "450589",
    "end": "458209"
  },
  {
    "text": "machine learning library you also have the machine the ml pipeline's API which",
    "start": "458209",
    "end": "463729"
  },
  {
    "text": "is an API that allows you to create an entire machine learning workflow from a single application so during the",
    "start": "463729",
    "end": "470149"
  },
  {
    "text": "workflow you can include stages that would transform your data convert your data to some kind of different format or",
    "start": "470149",
    "end": "476029"
  },
  {
    "text": "add columns to it and you can also use that for performance tuning evaluation",
    "start": "476029",
    "end": "481489"
  },
  {
    "text": "so there's various things and useful features that you can do with the ml",
    "start": "481489",
    "end": "486529"
  },
  {
    "text": "pipeline API which I'll talk about some of them here as we go along on top of",
    "start": "486529",
    "end": "492349"
  },
  {
    "text": "that with the ml ml Lib you also have the ability to do model selection as well as tuning your application your",
    "start": "492349",
    "end": "498800"
  },
  {
    "text": "machine learning my application with a hyper parameter tuning you can also do cross-validation and train split",
    "start": "498800",
    "end": "505779"
  },
  {
    "text": "validation for this so let's talk still on the components of ML Lib another",
    "start": "505779",
    "end": "512719"
  },
  {
    "start": "508000",
    "end": "545000"
  },
  {
    "text": "thing about it is that it provides various different algorithms so it's farc ml lab you can use algorithms",
    "start": "512719",
    "end": "518059"
  },
  {
    "text": "including for classification regression clustering as well as collaborate",
    "start": "518059",
    "end": "523099"
  },
  {
    "text": "filtering now one of the useful features for MLM is also gives you the ability to do",
    "start": "523099",
    "end": "529440"
  },
  {
    "text": "so if you need to do something such as feature extraction or transformer transformation or dimensionality",
    "start": "529440",
    "end": "535500"
  },
  {
    "text": "reduction these are all very common tests for a data scientist it is possible to do that all using the MLM or",
    "start": "535500",
    "end": "542850"
  },
  {
    "text": "sparks machine learning library and one of the particular components that I'm talking about now is also the ml",
    "start": "542850",
    "end": "550080"
  },
  {
    "start": "545000",
    "end": "760000"
  },
  {
    "text": "pipelines and the ml pipeline's provides you with a set of tools to construct and evaluate that entire workflow now an ml",
    "start": "550080",
    "end": "558270"
  },
  {
    "text": "pipeline and spark consists of stages now a stage can be a transformer stage",
    "start": "558270",
    "end": "563610"
  },
  {
    "text": "and this is a stage that's actually going to transform that data into a data frame into a new data frame so for",
    "start": "563610",
    "end": "570839"
  },
  {
    "text": "example you could do some kind of feature extraction or feature engineering in order to create a data frame containing a column with new",
    "start": "570839",
    "end": "578550"
  },
  {
    "text": "features or containing a prediction column and the other stage whenever",
    "start": "578550",
    "end": "584010"
  },
  {
    "text": "using ml lib is also the estimator stage and this is the stage where you're going to actually apply your model and the",
    "start": "584010",
    "end": "591839"
  },
  {
    "text": "algorithm that trains on the data and you have the fit method to perform this so when you execute the method you're in",
    "start": "591839",
    "end": "598500"
  },
  {
    "text": "essence pretty much training on that data set so let's talk about an example",
    "start": "598500",
    "end": "604080"
  },
  {
    "text": "spark application so to make this a little bit more clear here's an example application of a spark ml cold and this",
    "start": "604080",
    "end": "612060"
  },
  {
    "text": "is directly from with some minor changes but this is directly from the spark and documentation now this is an entire",
    "start": "612060",
    "end": "618470"
  },
  {
    "text": "machine learning application written in Scala and this is a four spark so this",
    "start": "618470",
    "end": "624300"
  },
  {
    "text": "is native spark it's not you integrating with sage maker yet that example will be shown later but let's cover what each of",
    "start": "624300",
    "end": "631500"
  },
  {
    "text": "these means in a little bit more detail so you can have a better understanding of how spark ml lib or a spark ml",
    "start": "631500",
    "end": "637860"
  },
  {
    "text": "pipelines work so in the very first slide we have over here our training variable and this is where we're loading",
    "start": "637860",
    "end": "643650"
  },
  {
    "text": "our training data set in this case we're loading from a park a file format and we're just simply creating a data frame",
    "start": "643650",
    "end": "650130"
  },
  {
    "text": "for this this is what's going to be using as our training data set and next we configure the ml pipeline by",
    "start": "650130",
    "end": "656730"
  },
  {
    "text": "specifying the stages for the pipeline and here we have a pipeline consisting of three stages the",
    "start": "656730",
    "end": "662610"
  },
  {
    "text": "first one is a tokenizer stage and that stage is pretty much going to take a tax",
    "start": "662610",
    "end": "667740"
  },
  {
    "text": "line and is going to contain eyes that into an array or a list of words instead",
    "start": "667740",
    "end": "673019"
  },
  {
    "text": "of being a single line that's where the tokenizer stage is doing and here's where we specify input and output",
    "start": "673019",
    "end": "678899"
  },
  {
    "text": "columns the next thing about that we have our hashing stage and this is",
    "start": "678899",
    "end": "684029"
  },
  {
    "text": "pretty much a hashing function though we can also specify the amount of features for this data and if you notice one of",
    "start": "684029",
    "end": "690390"
  },
  {
    "text": "the things about this is that the input column for our second stage is actually the output column of the previous stages",
    "start": "690390",
    "end": "697350"
  },
  {
    "text": "so this kind of gives you an idea of how all of the stages within an ml pipeline",
    "start": "697350",
    "end": "702630"
  },
  {
    "text": "they're linked together and they're chained together in order to form this workflow for your machine learning and",
    "start": "702630",
    "end": "708149"
  },
  {
    "text": "then we have our estimator stage here which is our logistic regression stage we're using logistic regression to apply",
    "start": "708149",
    "end": "715529"
  },
  {
    "text": "the model over I dated so here we specify some parameters for this particular algorithm and then we just",
    "start": "715529",
    "end": "722220"
  },
  {
    "text": "create a pipeline by specifying those three stages now once we want to fit the",
    "start": "722220",
    "end": "727470"
  },
  {
    "text": "pipeline into training documents we just specify that we run the execute the fit method and specifying the training data",
    "start": "727470",
    "end": "734190"
  },
  {
    "text": "and this is going to create our model and once we have that model we can actually test predictions by applying",
    "start": "734190",
    "end": "740970"
  },
  {
    "text": "the model into our testing data and this is what the last two lines and since in essence does we'll load our test data",
    "start": "740970",
    "end": "748110"
  },
  {
    "text": "and then we'll run a transformation to where we're doing predictions and applying the model over the test data to",
    "start": "748110",
    "end": "754589"
  },
  {
    "text": "verify how efficient that algorithm where that model really is so moving",
    "start": "754589",
    "end": "761100"
  },
  {
    "start": "760000",
    "end": "900000"
  },
  {
    "text": "forward that was it for SPARC I would like to talk a little bit about Amazon EMR which is a stands for elastic",
    "start": "761100",
    "end": "769230"
  },
  {
    "text": "MapReduce an EMR is our managed service for running Hadoop ecosystem applications in AWS and the applications",
    "start": "769230",
    "end": "776670"
  },
  {
    "text": "that EMR includes it includes presto spark Hadoop hive and many other",
    "start": "776670",
    "end": "782100"
  },
  {
    "text": "applications so EMR is up to 20 over 20 different applications available for you to launch and run those applications but",
    "start": "782100",
    "end": "789959"
  },
  {
    "text": "not only that but with EMR you can also process massive amounts of data in barrier from barriers that",
    "start": "789959",
    "end": "796080"
  },
  {
    "text": "sources including s3 dynamodb redshift Kafka and many others now one other",
    "start": "796080",
    "end": "802500"
  },
  {
    "text": "thing about EMR is that it provides various features for you to run this kind of job in very low cost for example",
    "start": "802500",
    "end": "809910"
  },
  {
    "text": "with EMR we have the concept of transient clusters and with transient coasters by default an EMR you're going",
    "start": "809910",
    "end": "816630"
  },
  {
    "text": "to keep your storage in s3 and if you're keeping your storage in s3 you can have this concept of transient clusters which",
    "start": "816630",
    "end": "823140"
  },
  {
    "text": "is when you launch a cluster that cluster will run for the duration of a particular job and once that job",
    "start": "823140",
    "end": "828900"
  },
  {
    "text": "completes the cluster will terminate itself and at that point you're saving costs because you don't have to keep a",
    "start": "828900",
    "end": "835170"
  },
  {
    "text": "cluster running at all times and since your data is an s3 you no longer have to worry about any sort of data loss EMR",
    "start": "835170",
    "end": "841920"
  },
  {
    "text": "also has a auto scaling feature so you can figure that cluster to auto scale based on particular load events that",
    "start": "841920",
    "end": "849000"
  },
  {
    "text": "happen in the cluster and also EMR provides you with the ability to run spa instances directly on that cluster so",
    "start": "849000",
    "end": "855480"
  },
  {
    "text": "all of these are features that will allow you to run your SPARC application in very low costs so this is the reason",
    "start": "855480",
    "end": "862620"
  },
  {
    "text": "why I'm using EMR today to run our spark cluster so part of the demo I will be",
    "start": "862620",
    "end": "867840"
  },
  {
    "text": "using I'll be launching an EMR cluster and that's what we'll be using for SPARC",
    "start": "867840",
    "end": "872990"
  },
  {
    "text": "in the last in the next feature of EMR is that it's secure it's integrated with",
    "start": "872990",
    "end": "878580"
  },
  {
    "text": "all the AWS services for security including I am as well as the AWS kms",
    "start": "878580",
    "end": "884790"
  },
  {
    "text": "and with the simple click of a button on your mark you can also configure the cluster to encrypt your data at arrest",
    "start": "884790",
    "end": "890850"
  },
  {
    "text": "as well as data in transit so you can configure your your SPARC workload as",
    "start": "890850",
    "end": "896010"
  },
  {
    "text": "well as build your machine learning pipeline in a very secure way so having",
    "start": "896010",
    "end": "901350"
  },
  {
    "start": "900000",
    "end": "1033000"
  },
  {
    "text": "said this about Amara let's talk about the next service that would be part of our compassion here today which is",
    "start": "901350",
    "end": "908100"
  },
  {
    "text": "Amazon sage maker now what is sage maker sage maker is a fully managed service",
    "start": "908100",
    "end": "913260"
  },
  {
    "text": "for machine learning now this is a pretty broad statement so I know a lot of you are wondering well what do you mean by managed machine learning service",
    "start": "913260",
    "end": "919950"
  },
  {
    "text": "so let me talk quickly cover some of the features and main components of sage maker which one",
    "start": "919950",
    "end": "926730"
  },
  {
    "text": "sage maker provides you with a prig no books or you can also use sage maker",
    "start": "926730",
    "end": "932839"
  },
  {
    "text": "notebooks and these are Jupiter notebooks in order for you to do data exploration visualization and so on",
    "start": "932839",
    "end": "940089"
  },
  {
    "text": "say Jamaica also comes with built in machine learning algorithms these are",
    "start": "940089",
    "end": "945379"
  },
  {
    "text": "cloud scale algorithms and in essence these are infinitely scalable algorithms",
    "start": "945379",
    "end": "951889"
  },
  {
    "text": "these are very efficient algorithms that sage maker comes with a built in to it",
    "start": "951889",
    "end": "957439"
  },
  {
    "text": "and also with the click of a button or a simple API call you can configure sage maker to execute",
    "start": "957439",
    "end": "964519"
  },
  {
    "text": "a training job so if you want to train over a data set you can simply do a click of a button and sage maker will",
    "start": "964519",
    "end": "970879"
  },
  {
    "text": "create what we call a training job to train that data stage maker is also",
    "start": "970879",
    "end": "975920"
  },
  {
    "text": "optimized so you can use sage maker to train and tune your machine learning pipeline and also the other thing that",
    "start": "975920",
    "end": "983360"
  },
  {
    "text": "it has is one-click deployment for you to host that machine learning model directly on Sage maker so once you have",
    "start": "983360",
    "end": "990379"
  },
  {
    "text": "created your model you can simple click of a button or run an API call and that's what we'll be doing from SPARC",
    "start": "990379",
    "end": "995929"
  },
  {
    "text": "and the end result is that you have your model hosted on sage maker and then you",
    "start": "995929",
    "end": "1001059"
  },
  {
    "text": "can simply make API calls to make predictions using that particular model they have hosted them and all of this is",
    "start": "1001059",
    "end": "1008769"
  },
  {
    "text": "fully managed so sage makers taking care of auto scaling your cluster for your",
    "start": "1008769",
    "end": "1014259"
  },
  {
    "text": "training jobs or for your endpoints sage maker is going to do handle things such as security checks as well as handle",
    "start": "1014259",
    "end": "1021189"
  },
  {
    "text": "node failures and various other things so you can almost look at sage maker as",
    "start": "1021189",
    "end": "1026350"
  },
  {
    "text": "a one-stop shop for all your machine or any type of tasks that you're doing today so now let's talk a little bit",
    "start": "1026350",
    "end": "1034449"
  },
  {
    "start": "1033000",
    "end": "1143000"
  },
  {
    "text": "about the sage maker spark SDK and this is really what's the core of this talk",
    "start": "1034449",
    "end": "1040480"
  },
  {
    "text": "or this session here today is how can we integrate spark and sdk stage maker and",
    "start": "1040480",
    "end": "1046630"
  },
  {
    "text": "spark together so the stage maker spark sdk it is an open source sdk available",
    "start": "1046630",
    "end": "1052480"
  },
  {
    "text": "directly on github and it's actually free installed on EMR 5.11 and above so",
    "start": "1052480",
    "end": "1058419"
  },
  {
    "text": "now that doesn't mean that you can only use the stage makers Park SDK from um are you can if you're running your own spark",
    "start": "1058419",
    "end": "1065390"
  },
  {
    "text": "distribution you can still use it it just means that you have to install it and configure accordingly to your",
    "start": "1065390",
    "end": "1071810"
  },
  {
    "text": "application so again this is also another example is why I chose here mark to do this today is because iam are",
    "start": "1071810",
    "end": "1077630"
  },
  {
    "text": "already has all these things built in and it's easier to just run with it also",
    "start": "1077630",
    "end": "1083450"
  },
  {
    "text": "the stage maker spark SDK provides ap is for both Scala and Python and it allows",
    "start": "1083450",
    "end": "1089570"
  },
  {
    "text": "you it gives you the ability to train and pour and predict with Amazon sage maker all directly from your spark",
    "start": "1089570",
    "end": "1096380"
  },
  {
    "text": "applications so and you can configure your application to do all these things and at no point in time you have to",
    "start": "1096380",
    "end": "1102770"
  },
  {
    "text": "configure spark to launch another application or to trigger some kind of different workflow and this is where we",
    "start": "1102770",
    "end": "1109160"
  },
  {
    "text": "have this is the great thing about how it is is that you get to do this entire pipeline all within your spark",
    "start": "1109160",
    "end": "1115760"
  },
  {
    "text": "application and have this entire integration seamlessly to users and the",
    "start": "1115760",
    "end": "1122870"
  },
  {
    "text": "other component that the sparks each mate has 2k provides is data frames in",
    "start": "1122870",
    "end": "1127940"
  },
  {
    "text": "and data frames out in other words sage maker the sage maker spark SDK is built",
    "start": "1127940",
    "end": "1133880"
  },
  {
    "text": "to work with data frames and it's going to automatically serialize and deserialize your data to and from the",
    "start": "1133880",
    "end": "1140240"
  },
  {
    "text": "protobuf format so let's talk a little bit about the benefits of spark with",
    "start": "1140240",
    "end": "1147110"
  },
  {
    "text": "stage makers so that's a question that comes up often so as you heard me talking about earlier spark is great at",
    "start": "1147110",
    "end": "1154310"
  },
  {
    "text": "doing machine learning things and so is sage makers so the question that does come up is why can't I just use sage",
    "start": "1154310",
    "end": "1160640"
  },
  {
    "text": "maker or why can't I just use spark well the first benefit of integrating spark",
    "start": "1160640",
    "end": "1166490"
  },
  {
    "start": "1163000",
    "end": "1247000"
  },
  {
    "text": "with sage maker is that you have the ability to decouple ETL and machine learning so take for example your ETL",
    "start": "1166490",
    "end": "1173810"
  },
  {
    "text": "workload may be completely different may require a completely set of different",
    "start": "1173810",
    "end": "1178820"
  },
  {
    "text": "hardware then your machine learning workload for example maybe for your retail workload you would like to run",
    "start": "1178820",
    "end": "1185270"
  },
  {
    "text": "our for instance types while for your training job you like to use speed 3 or some kind of a GPU instance",
    "start": "1185270",
    "end": "1191940"
  },
  {
    "text": "now you can certainly run a spark cluster in GPU instance but that's not going to be ideal and it can be also",
    "start": "1191940",
    "end": "1198419"
  },
  {
    "text": "something that's very costly therefore whenever you're scaling these independently of one another you have",
    "start": "1198419",
    "end": "1204840"
  },
  {
    "text": "the ability to really configure each type of workload separately and this",
    "start": "1204840",
    "end": "1210210"
  },
  {
    "text": "also avoids you from having to size your spark cluster in order to run your",
    "start": "1210210",
    "end": "1215940"
  },
  {
    "text": "training job again if you're a decoupling them you no longer have to be",
    "start": "1215940",
    "end": "1221100"
  },
  {
    "text": "worrying about if you're gonna be running your spark application directly from a GPU instance and also have the",
    "start": "1221100",
    "end": "1228419"
  },
  {
    "text": "ability to avoid a time consuming EMR if you need to scale your EMR cluster you",
    "start": "1228419",
    "end": "1233610"
  },
  {
    "text": "no longer have to be thinking about how long it's gonna take to scale and you can just let sage maker handle that",
    "start": "1233610",
    "end": "1239370"
  },
  {
    "text": "particular workload for you and also you run your ETL once and then you can run",
    "start": "1239370",
    "end": "1245039"
  },
  {
    "text": "various models in parallel now the second benefit that you have with the",
    "start": "1245039",
    "end": "1250320"
  },
  {
    "start": "1247000",
    "end": "1301000"
  },
  {
    "text": "integration of sage maker and spark is also the ability to run machine learning algorithm in any language",
    "start": "1250320",
    "end": "1256919"
  },
  {
    "text": "so the SPARC machine learning library is great however you might need something else in fact you might need to run some",
    "start": "1256919",
    "end": "1263820"
  },
  {
    "text": "kind of ML algorithm that's not available directly in SPARC and if you are using spark with sage maker you have",
    "start": "1263820",
    "end": "1269490"
  },
  {
    "text": "the ability to use a different algorithm and you also have the ability to use deep learning frameworks such as MX net",
    "start": "1269490",
    "end": "1277049"
  },
  {
    "text": "or tensorflow the both of these frameworks they're also natively available both on Sage",
    "start": "1277049",
    "end": "1283440"
  },
  {
    "text": "maker as well as EMR so again you have the ability to use these frameworks if",
    "start": "1283440",
    "end": "1289230"
  },
  {
    "text": "you like to do so and you can also write your own custom code in any language so",
    "start": "1289230",
    "end": "1294360"
  },
  {
    "text": "you're not just tied in to sparks native API that's supported you can reuse your custom application in any code you like",
    "start": "1294360",
    "end": "1301610"
  },
  {
    "start": "1301000",
    "end": "1394000"
  },
  {
    "text": "now this is a particularly interesting benefit that we have is that you get the best prediction performance so what I",
    "start": "1301610",
    "end": "1308730"
  },
  {
    "text": "mean by this is whenever you can you have the ability to perform your machine learning predictions without using spark",
    "start": "1308730",
    "end": "1315720"
  },
  {
    "text": "so again spark where you're really shines is whenever you're doing ETL and you're",
    "start": "1315720",
    "end": "1321000"
  },
  {
    "text": "processing massive amount of data however often you might have some kind of lateness or",
    "start": "1321000",
    "end": "1326910"
  },
  {
    "text": "you might have some kind of latency requirements in order to process the",
    "start": "1326910",
    "end": "1331920"
  },
  {
    "text": "data and uh excuse me you know if you want to improve the latency for running",
    "start": "1331920",
    "end": "1338520"
  },
  {
    "text": "small-batch predictions that's not going to be possible if you're using spark because of the overhead of the spark",
    "start": "1338520",
    "end": "1344400"
  },
  {
    "text": "framework which again is really useful for doing ETL so you wouldn't be difficult if you're trying to do let's",
    "start": "1344400",
    "end": "1350760"
  },
  {
    "text": "say a prediction for a single image or a single record you would be almost not to",
    "start": "1350760",
    "end": "1356970"
  },
  {
    "text": "say pointless however you would be just an overkill for you to run a small",
    "start": "1356970",
    "end": "1362220"
  },
  {
    "text": "prediction for a small batch using spark and you can also achieve real-time",
    "start": "1362220",
    "end": "1368430"
  },
  {
    "text": "predictions by using sage maker models that are hosting sage maker without using spark and lastly you also get to",
    "start": "1368430",
    "end": "1375960"
  },
  {
    "text": "optimize your own prediction instance so whenever you're hosting the modern sage",
    "start": "1375960",
    "end": "1381030"
  },
  {
    "text": "maker you also specify the particular instance that you like to use for your",
    "start": "1381030",
    "end": "1386220"
  },
  {
    "text": "endpoints and all these things so this goes back to the point where I mentioned earlier of decoupling your ETL from your",
    "start": "1386220",
    "end": "1392760"
  },
  {
    "text": "machine learning now that's it for the some of the benefits now let's talk about some of the common use cases that",
    "start": "1392760",
    "end": "1399630"
  },
  {
    "start": "1394000",
    "end": "1485000"
  },
  {
    "text": "we see customers using spark along with sage maker and the first use case is of",
    "start": "1399630",
    "end": "1406500"
  },
  {
    "text": "course for data preparation as well as feature engineering before training so if you recall back to one of the first",
    "start": "1406500",
    "end": "1413070"
  },
  {
    "text": "slides that I showed you which was the machine learning assumption versus",
    "start": "1413070",
    "end": "1418350"
  },
  {
    "text": "reality and with this you have now the ability of do using spark to prepare",
    "start": "1418350",
    "end": "1423450"
  },
  {
    "text": "your data or do feature engineering and use spark in order to achieve those that",
    "start": "1423450",
    "end": "1428850"
  },
  {
    "text": "part of the pipeline and this also ties in with using spark to do data transformation as well as using spark",
    "start": "1428850",
    "end": "1435900"
  },
  {
    "text": "for batch prediction so as I mentioned before the sage maker spark SDK supports",
    "start": "1435900",
    "end": "1441060"
  },
  {
    "text": "data frames and you can use those data frames in order to apply your predictions to let's say a large amount",
    "start": "1441060",
    "end": "1448020"
  },
  {
    "text": "of data you might have terabytes or petabytes of data somewhere and if you like to apply a prediction to all of",
    "start": "1448020",
    "end": "1453720"
  },
  {
    "text": "that data then that would be a really good case where you're using spark along with stage maker in order to",
    "start": "1453720",
    "end": "1458830"
  },
  {
    "text": "perform this large batch prediction and of course this ties in with the data enrichment and predictions and also the",
    "start": "1458830",
    "end": "1466840"
  },
  {
    "text": "last use case that we see doing often is to deploy spark machine learning models",
    "start": "1466840",
    "end": "1471940"
  },
  {
    "text": "at any scale so in other words you can create your spark your model using spark",
    "start": "1471940",
    "end": "1476980"
  },
  {
    "text": "machine learning and then you can simply host that model on stage maker afterwards and that model can be at any",
    "start": "1476980",
    "end": "1483310"
  },
  {
    "text": "skill so let's talk a little bit about developing an Amazon stage maker",
    "start": "1483310",
    "end": "1489070"
  },
  {
    "start": "1485000",
    "end": "1582000"
  },
  {
    "text": "application all right animals on stage make her a spark say Jamaica applications so there are various ways",
    "start": "1489070",
    "end": "1494770"
  },
  {
    "text": "to do that for example you can download the spark stage Maker SDK and you can",
    "start": "1494770",
    "end": "1500140"
  },
  {
    "text": "use your IDE of choice and then create your own application however for the",
    "start": "1500140",
    "end": "1505570"
  },
  {
    "text": "demo purposes that I'll be showing you today I like to create a spark stage Maker spark application in an",
    "start": "1505570",
    "end": "1511210"
  },
  {
    "text": "interactive way so for this I'm going to be using notebooks in this case the stage maker notebooks with Jupiter and",
    "start": "1511210",
    "end": "1518440"
  },
  {
    "text": "from that Jupiter notebook we're going to run our spark application or that spark machine learning pipeline that's",
    "start": "1518440",
    "end": "1525850"
  },
  {
    "text": "integrated with stage maker now one thing to know about this is that whenever you need to run spark from a",
    "start": "1525850",
    "end": "1532690"
  },
  {
    "text": "notebook like Jupiter the way that Jupiter will communicate with spark is through an application called Livi and",
    "start": "1532690",
    "end": "1539680"
  },
  {
    "text": "Apache levy is ultimately it's a rest server for SPARC so that allows you to",
    "start": "1539680",
    "end": "1545890"
  },
  {
    "text": "make REST API calls to spark but not only that but that's the actual application that our notebooks like",
    "start": "1545890",
    "end": "1552850"
  },
  {
    "text": "Jupiter here as well as that plan we use in order for you to create this gives me",
    "start": "1552850",
    "end": "1559630"
  },
  {
    "text": "in order for you to create spark notebooks and now the other good thing about EMR is that Livi is one of the",
    "start": "1559630",
    "end": "1566110"
  },
  {
    "text": "application choices that you can have installed on EMR so whenever you launch the EMR cluster you simply also select",
    "start": "1566110",
    "end": "1572440"
  },
  {
    "text": "spark and Libby and at that point you have a cluster that's fully ready to be",
    "start": "1572440",
    "end": "1577660"
  },
  {
    "text": "your processing back hand for your machine learning application so this really takes us to now the portion that",
    "start": "1577660",
    "end": "1584620"
  },
  {
    "start": "1582000",
    "end": "1879000"
  },
  {
    "text": "I think a lot of you might be interest which is seeing the demo for this so I",
    "start": "1584620",
    "end": "1590170"
  },
  {
    "text": "do have a demo that I pre-recorded in advance now the reason why I pre-recorded this demo it was because it does take a few",
    "start": "1590170",
    "end": "1597880"
  },
  {
    "text": "minutes for you to launch an EMR cluster as well as a few minutes for you to run your training jobs so therefore we would",
    "start": "1597880",
    "end": "1604240"
  },
  {
    "text": "be losing some medics here therefore I decided to pre-record the demo however as we go along I'll be",
    "start": "1604240",
    "end": "1610660"
  },
  {
    "text": "explaining everything here as I'm doing with the demo so let's go ahead and get",
    "start": "1610660",
    "end": "1615670"
  },
  {
    "text": "started on this demo now the first part",
    "start": "1615670",
    "end": "1622810"
  },
  {
    "text": "of this demo as you can see here I am on the AWS management console and the first step is to actually launch an EMR",
    "start": "1622810",
    "end": "1630280"
  },
  {
    "text": "cluster so from the AWS console I can simply go to the EMR console and from there I'm going to create our spark",
    "start": "1630280",
    "end": "1636850"
  },
  {
    "text": "cluster for application so here we just go to create cluster and the first step",
    "start": "1636850",
    "end": "1642880"
  },
  {
    "text": "when you're creating an EMR cluster is to select the different applications that you like to have installed so in",
    "start": "1642880",
    "end": "1649750"
  },
  {
    "text": "this case as I mentioned before we need to use both SPARC as well as livvie in order for us to communicate with spark",
    "start": "1649750",
    "end": "1657580"
  },
  {
    "text": "from Jupiter notebook so once we select the applications the second step is to",
    "start": "1657580",
    "end": "1662650"
  },
  {
    "text": "actually specify the V PC and subnet as well as the instance type that you'd like to use from that for that",
    "start": "1662650",
    "end": "1669310"
  },
  {
    "text": "particular cluster so this ties in back to our example where you want to make",
    "start": "1669310",
    "end": "1674680"
  },
  {
    "text": "sure that you have different hardware for your specific workloads so in this",
    "start": "1674680",
    "end": "1680020"
  },
  {
    "text": "case for our spark cluster this is going to be our processing close to our ETL cluster we're going to be using and for",
    "start": "1680020",
    "end": "1686770"
  },
  {
    "text": "the large instance so I have one master node and two coordinates for this particular cluster and then on the third",
    "start": "1686770",
    "end": "1694060"
  },
  {
    "text": "step these are just general settings for EMR which includes the cluster name as",
    "start": "1694060",
    "end": "1699130"
  },
  {
    "text": "well as logging locations so not much over here Italian to our demo but the",
    "start": "1699130",
    "end": "1705700"
  },
  {
    "text": "next important part is going to be the security of this cluster so that's going",
    "start": "1705700",
    "end": "1710800"
  },
  {
    "text": "to be on our next step over here and let's go ahead and just move over to the next step and let me move to so here it",
    "start": "1710800",
    "end": "1719650"
  },
  {
    "text": "is this is the actual security step of whenever you're launching EMR cluster so",
    "start": "1719650",
    "end": "1725500"
  },
  {
    "text": "the important thing to know over here is that for the security options I did select an ec2 keeper and this is an",
    "start": "1725500",
    "end": "1731540"
  },
  {
    "text": "SSH key and simply - something that will allow me to SSH to the cluster if I like",
    "start": "1731540",
    "end": "1737090"
  },
  {
    "text": "- now the reason why I'm selecting an SSH key here is because I will be logging into the cluster and to show an",
    "start": "1737090",
    "end": "1743840"
  },
  {
    "text": "example what the logs will look like whenever you're running your application however for a lot of workloads many",
    "start": "1743840",
    "end": "1750920"
  },
  {
    "text": "customers they will just launch them our clusters to run their jobs without even using an SSH key so if you have a use",
    "start": "1750920",
    "end": "1758150"
  },
  {
    "text": "case where you don't even need to log into that cluster at all you can simply launch the cluster as is without an SSH",
    "start": "1758150",
    "end": "1764450"
  },
  {
    "text": "key now the other important part on this particular step is the ec2 SS profile",
    "start": "1764450",
    "end": "1770690"
  },
  {
    "text": "that EMR is using so in this case I have a PC to roll call um Ric to default row",
    "start": "1770690",
    "end": "1776090"
  },
  {
    "text": "now the one thing that this row does need to have in order for it to communicate with Sage maker is that this",
    "start": "1776090",
    "end": "1782990"
  },
  {
    "text": "row needs to have a policy that allows it to make API calls to Sage maker because in essence whenever you're using",
    "start": "1782990",
    "end": "1790160"
  },
  {
    "text": "the sage maker spark SDK what SPARC is going to be doing is going to be making",
    "start": "1790160",
    "end": "1795919"
  },
  {
    "text": "API calls to Sage Maker therefore you need to allow the ec2 cluster role to",
    "start": "1795919",
    "end": "1802340"
  },
  {
    "text": "make those API calls to Sage Maker now the other thing that I would like to show you over here that's important is",
    "start": "1802340",
    "end": "1808640"
  },
  {
    "text": "going to be the security groups now the security groups I get to select the security groups for our cluster here the",
    "start": "1808640",
    "end": "1815150"
  },
  {
    "text": "master node as well as the core and task no security groups and the important thing to note here is that our in order",
    "start": "1815150",
    "end": "1822440"
  },
  {
    "text": "for Jupiter to communicate with SPARC your security group does need to allow communication to the Livi server I don't",
    "start": "1822440",
    "end": "1830600"
  },
  {
    "text": "remember the port number outs up my head but I will be showing the port number here in a second on how the configuration is done but the main",
    "start": "1830600",
    "end": "1837320"
  },
  {
    "text": "takeaway here is that your security group for your EMR cluster does need to have allow connectivity from your",
    "start": "1837320",
    "end": "1844370"
  },
  {
    "text": "Jupiter notebook in order to access living so I'm gonna go ahead and this is",
    "start": "1844370",
    "end": "1850510"
  },
  {
    "text": "next I just create the cluster and once the cluster is being created it's going to be in a starting state and once the",
    "start": "1850510",
    "end": "1857300"
  },
  {
    "text": "cluster is fully done I've been created it's going to go into a way State and that means that the cluster is",
    "start": "1857300",
    "end": "1862340"
  },
  {
    "text": "ready to run a job so while the cluster is creating the next step is going to be",
    "start": "1862340",
    "end": "1867470"
  },
  {
    "text": "to launch a sage maker notebook instance and this is going to be Jupiter that we'll be using to develop our spark",
    "start": "1867470",
    "end": "1874640"
  },
  {
    "text": "application so here I am at the AWS management console and the sage maker part of it thing and here I go to the",
    "start": "1874640",
    "end": "1881750"
  },
  {
    "start": "1879000",
    "end": "2024000"
  },
  {
    "text": "stage maker notebook instance and create a notebook instance now you simply give your notebook",
    "start": "1881750",
    "end": "1887120"
  },
  {
    "text": "instance a name and one of the things that you should know over here is the notebook instance type so in this case",
    "start": "1887120",
    "end": "1893240"
  },
  {
    "text": "I'm just using a small instance a t2 medium and that's completely okay because remember our spark cluster or",
    "start": "1893240",
    "end": "1900770"
  },
  {
    "text": "EMR spark and EMR is really what's going to be doing the processing or the heavy lifting for our application so in",
    "start": "1900770",
    "end": "1908810"
  },
  {
    "text": "general your your Jupiter your notebook instance doesn't need to be that powerful or have that much resources",
    "start": "1908810",
    "end": "1914780"
  },
  {
    "text": "because that's where a spark is going to come in and then we also have our particular instance that we'll be using",
    "start": "1914780",
    "end": "1920720"
  },
  {
    "text": "for our trading job as well as the endpoints so again for the notebooks we can have a small instance type to use",
    "start": "1920720",
    "end": "1928190"
  },
  {
    "text": "for that now the other thing that you should know over here while you're creating a notebook is going to be the",
    "start": "1928190",
    "end": "1935120"
  },
  {
    "text": "permissions for the notebook or the networking portion of it and then that work is where I specified the subnet as",
    "start": "1935120",
    "end": "1942470"
  },
  {
    "text": "well as the security group for my stage maker notebook now if you recall earlier I mentioned to you that you need to have",
    "start": "1942470",
    "end": "1949370"
  },
  {
    "text": "connectivity to EMR to the security groups therefore for this particular",
    "start": "1949370",
    "end": "1954410"
  },
  {
    "text": "demo I'm using the same security group and subnet as EMR and this by default I",
    "start": "1954410",
    "end": "1959900"
  },
  {
    "text": "have a rule configuring there that allows that stage maker notebook security group it allows it to connect",
    "start": "1959900",
    "end": "1966320"
  },
  {
    "text": "to the EMR master node now one other cool feature over here is that the github repositories for stage maker",
    "start": "1966320",
    "end": "1973730"
  },
  {
    "text": "notebooks so in this case I already created some notebooks in advance and I",
    "start": "1973730",
    "end": "1978770"
  },
  {
    "text": "would like to say maker to clone that a repository automatically when I'm launching a notebook so here I can",
    "start": "1978770",
    "end": "1984560"
  },
  {
    "text": "specify the URL for my github repository and by doing that stage Maker will",
    "start": "1984560",
    "end": "1990440"
  },
  {
    "text": "automatically clone that repository are with my notebooks automatically once",
    "start": "1990440",
    "end": "1995560"
  },
  {
    "text": "it's launched so after this I can specify tags and that's optional and the",
    "start": "1995560",
    "end": "2001770"
  },
  {
    "text": "next step is just to create that notebook instance now once the notebook instance has been created it just takes a couple minutes",
    "start": "2001770",
    "end": "2008160"
  },
  {
    "text": "for it to be ready so its first and pending status but once it's ready it's going to go into in-service status and",
    "start": "2008160",
    "end": "2014850"
  },
  {
    "text": "now we can actually click on open Jupiter and this is actually going to open up our Jupiter notebook where we'll",
    "start": "2014850",
    "end": "2021450"
  },
  {
    "text": "develop our application from so let's go ahead and do that so let me go ahead and",
    "start": "2021450",
    "end": "2026490"
  },
  {
    "start": "2024000",
    "end": "2064000"
  },
  {
    "text": "open up the Jupiter notebook and now over here you can see that I already have some notebooks automatically when I",
    "start": "2026490",
    "end": "2032970"
  },
  {
    "text": "open up this Jupiter console and this is because of you remember earlier I had",
    "start": "2032970",
    "end": "2038460"
  },
  {
    "text": "specify a github repository containing my notebooks so this is already in place",
    "start": "2038460",
    "end": "2043950"
  },
  {
    "text": "because of that now before I can actually use spark from Jupiter there's",
    "start": "2043950",
    "end": "2049860"
  },
  {
    "text": "one thing that I need to do first which is configure spark magic and specify",
    "start": "2049860",
    "end": "2055550"
  },
  {
    "text": "where is my spark cluster the location of my EMR closer that will be used so in",
    "start": "2055550",
    "end": "2061440"
  },
  {
    "text": "order to do that I can simply pull up the terminal directly from Jupiter and I",
    "start": "2061440",
    "end": "2066570"
  },
  {
    "start": "2064000",
    "end": "2109000"
  },
  {
    "text": "just need to I download in this case I downloaded the example spark magic",
    "start": "2066570",
    "end": "2071850"
  },
  {
    "text": "configuration from now our spark magic is a Python library that allows the",
    "start": "2071850",
    "end": "2077760"
  },
  {
    "text": "Jupiter notebooks to communicate with living or spark and here I'm downloading",
    "start": "2077760",
    "end": "2082950"
  },
  {
    "text": "the simple spark magic config that JSON file and there's only a couple of property changes that I need to do which",
    "start": "2082950",
    "end": "2089669"
  },
  {
    "text": "is you see here in a second so here I just I'm downloading the example config",
    "start": "2089669",
    "end": "2096000"
  },
  {
    "text": "let me go ahead and fast forward this and once the spark magic configurations",
    "start": "2096000",
    "end": "2101460"
  },
  {
    "text": "download it I'm gonna edit that file and specify my EMR cluster URL or my spark",
    "start": "2101460",
    "end": "2107250"
  },
  {
    "text": "cluster URL so in this case when we open up the spark Magic configuration we can",
    "start": "2107250",
    "end": "2112500"
  },
  {
    "start": "2109000",
    "end": "2172000"
  },
  {
    "text": "see the different configurations for the Python kernel PI spark as well as our Scala and so on now if you notice over",
    "start": "2112500",
    "end": "2119610"
  },
  {
    "text": "there it's pointed for the URL it's pointed to local host and port and 18",
    "start": "2119610",
    "end": "2124650"
  },
  {
    "text": "98 and 89 98 is the levy server port number and that's the port number that needs to be open within your security",
    "start": "2124650",
    "end": "2132180"
  },
  {
    "text": "group so here the only thing we need to do is replace the localhost with the",
    "start": "2132180",
    "end": "2138300"
  },
  {
    "text": "actual URL of the EMR cluster so to get the Year mark close to URL I simply I",
    "start": "2138300",
    "end": "2143730"
  },
  {
    "text": "can go back to the AWS management console and here we can see that the EMR cluster is now ready to go it's in",
    "start": "2143730",
    "end": "2150300"
  },
  {
    "text": "waiting waiting state and here we have the master Public DNS and that's the DNS",
    "start": "2150300",
    "end": "2155700"
  },
  {
    "text": "that we want to replace localhost with so in essence what you're telling right",
    "start": "2155700",
    "end": "2160740"
  },
  {
    "text": "now Jupiter is that use this EMR cluster to run your spark applications so let's go",
    "start": "2160740",
    "end": "2167310"
  },
  {
    "text": "ahead and do the replacement and then we save the file and that's it that's all",
    "start": "2167310",
    "end": "2173400"
  },
  {
    "start": "2172000",
    "end": "2186000"
  },
  {
    "text": "it is for spark magic configuration and now we're ready to actually pull up a spark kernel and run our spark",
    "start": "2173400",
    "end": "2180420"
  },
  {
    "text": "application so in this case I already have a example notebook that's using the spark kernel and this will be the",
    "start": "2180420",
    "end": "2187170"
  },
  {
    "start": "2186000",
    "end": "2289000"
  },
  {
    "text": "notebook that I'm mentioning here before now as we can see over here it's using spark magic the spark kernel and the",
    "start": "2187170",
    "end": "2195090"
  },
  {
    "text": "problem that I'm doing with this application the problem that I'm trying to solve just to give you an idea this",
    "start": "2195090",
    "end": "2200670"
  },
  {
    "text": "is a Multi multi class classification for spark problems so I'm using the XG",
    "start": "2200670",
    "end": "2206580"
  },
  {
    "text": "boost algorithm in order to classify the mn ist data step and that's a data set",
    "start": "2206580",
    "end": "2214410"
  },
  {
    "text": "of images containing handwritten algo handwritten digits from 0 through 9 so",
    "start": "2214410",
    "end": "2221010"
  },
  {
    "text": "what this park what this application is doing is trying to predict what is the",
    "start": "2221010",
    "end": "2226110"
  },
  {
    "text": "number that's been written on that image so if you have an image with the number one written on it this application will should predict",
    "start": "2226110",
    "end": "2232770"
  },
  {
    "text": "that that's the number that's written on that particular image so this is the the problem that I'm demonstrating with this",
    "start": "2232770",
    "end": "2239250"
  },
  {
    "text": "particular application so the first thing that I'm doing over here I'm going to specify my training as well as test",
    "start": "2239250",
    "end": "2247350"
  },
  {
    "text": "data set and if you remember earlier I'm using the Lib SVM format so I'm simply",
    "start": "2247350",
    "end": "2252900"
  },
  {
    "text": "using spark that read that format and I'm doing a live SVM I'm specifying the",
    "start": "2252900",
    "end": "2258420"
  },
  {
    "text": "number of features so in this case this data stack contains ten one hundred and eighty four features that's because the",
    "start": "2258420",
    "end": "2264270"
  },
  {
    "text": "images there are 28 by 28 pics so images so 28 times 28 784 and that's the amount",
    "start": "2264270",
    "end": "2271260"
  },
  {
    "text": "of features that we have for this particular image so once we have our data set that's loaded our training as",
    "start": "2271260",
    "end": "2277770"
  },
  {
    "text": "well as our test data set it's time to actually create and invoke the model however as you can see before I get to",
    "start": "2277770",
    "end": "2284910"
  },
  {
    "text": "that the first thing that happens when I actually run this first paragraph it says starting a spark application so now",
    "start": "2284910",
    "end": "2292470"
  },
  {
    "start": "2289000",
    "end": "2303000"
  },
  {
    "text": "this is me I've gone back to the EMR cluster and I've SSH directly to the",
    "start": "2292470",
    "end": "2298440"
  },
  {
    "text": "master node and here what I want to show you is what the levy server log looks like whenever you're running the spark",
    "start": "2298440",
    "end": "2304920"
  },
  {
    "start": "2303000",
    "end": "2421000"
  },
  {
    "text": "application so here it's starting up the application we can see over here that",
    "start": "2304920",
    "end": "2310200"
  },
  {
    "text": "it's using levy as a default application and once the application is actually",
    "start": "2310200",
    "end": "2315300"
  },
  {
    "text": "ready to run we can see over here that is go you're going to be returned an application ID as well as the state and",
    "start": "2315300",
    "end": "2322140"
  },
  {
    "text": "you can see that it says spark session available so that means that our spark",
    "start": "2322140",
    "end": "2327330"
  },
  {
    "text": "application is good to go and we can go ahead and get started on actually creating the model and invoking the",
    "start": "2327330",
    "end": "2333510"
  },
  {
    "text": "model so before I actually execute this particular paragraph just to give an",
    "start": "2333510",
    "end": "2339030"
  },
  {
    "text": "idea of what this some of the lines of code here means so this is where I have",
    "start": "2339030",
    "end": "2344130"
  },
  {
    "text": "that core integration between stage maker and spark so this is a spark application that's actually using a",
    "start": "2344130",
    "end": "2351360"
  },
  {
    "text": "couple of stages for the our ml pipeline so here we're using the XG boost",
    "start": "2351360",
    "end": "2357060"
  },
  {
    "text": "estimator and this is the stage where we're actually doing the training on the data now if you notice the difference",
    "start": "2357060",
    "end": "2363900"
  },
  {
    "text": "between this and the previous example spark application that I had shown you guys is that for this I'm actually",
    "start": "2363900",
    "end": "2370260"
  },
  {
    "text": "letting off setting everything for the training job over to stage maker so here",
    "start": "2370260",
    "end": "2375540"
  },
  {
    "text": "I specify the instance type for stage maker to use for this training job as well as the instance cow for that",
    "start": "2375540",
    "end": "2382050"
  },
  {
    "text": "cluster the initial instance cow type as well as IIM role so what to the takeaway",
    "start": "2382050",
    "end": "2388260"
  },
  {
    "text": "here is that saij maker is going to be running this training job and then we're going to actually use our we're going to do a",
    "start": "2388260",
    "end": "2396950"
  },
  {
    "text": "transformation here to where we're going to predict the first the first five rows on our training data set so this is",
    "start": "2396950",
    "end": "2404180"
  },
  {
    "text": "really what that application is doing and then I'm executing a show just to show that data frame the scheme of the",
    "start": "2404180",
    "end": "2410119"
  },
  {
    "text": "data frame and to make sure whether or not our predictions are correct for the test data so let me go ahead and go",
    "start": "2410119",
    "end": "2418640"
  },
  {
    "text": "through this part over here so whenever we initiate this application here is",
    "start": "2418640",
    "end": "2424010"
  },
  {
    "text": "what you're going to see from the spark cluster or from the live II logs you're going to see that it's starting the",
    "start": "2424010",
    "end": "2429410"
  },
  {
    "text": "application and the one thing that you should notice is that you're going to see that it's now making API calls to",
    "start": "2429410",
    "end": "2435590"
  },
  {
    "text": "Sage Maker so here we see that's using the XG boost estimator say Jamaica estimator and it's actually making a",
    "start": "2435590",
    "end": "2443000"
  },
  {
    "text": "creating trainee job request for this and here we have all the configurations",
    "start": "2443000",
    "end": "2448010"
  },
  {
    "text": "for this training job including the job name as well as the input data output",
    "start": "2448010",
    "end": "2453109"
  },
  {
    "text": "data location now one of the things to note is that now we see that the training job is in progress this is",
    "start": "2453109",
    "end": "2459830"
  },
  {
    "text": "what's being reported directly from the spark application now we can also see this directly from the stage maker",
    "start": "2459830",
    "end": "2465710"
  },
  {
    "text": "console so if we go back to Sage Maker we can go directly to training jobs and",
    "start": "2465710",
    "end": "2472040"
  },
  {
    "text": "see that the training jobs that our spark application has created in this case it has created one that's currently",
    "start": "2472040",
    "end": "2478460"
  },
  {
    "text": "in progress and this is what's executing the training portion of our spark",
    "start": "2478460",
    "end": "2483710"
  },
  {
    "text": "application so here we can see that it's starting the application it's still in progress and from the stage maker",
    "start": "2483710",
    "end": "2489530"
  },
  {
    "text": "console you also see information such as the training image is the xgb boost so",
    "start": "2489530",
    "end": "2494990"
  },
  {
    "text": "all the information can find directly here so let's go ahead and fast forward a little bit and once that training job",
    "start": "2494990",
    "end": "2502640"
  },
  {
    "start": "2501000",
    "end": "2552000"
  },
  {
    "text": "actually gets completed the next step for that spark is going to do is going",
    "start": "2502640",
    "end": "2508430"
  },
  {
    "text": "to be to actually create an endpoint for our job so it's going to create a a",
    "start": "2508430",
    "end": "2514099"
  },
  {
    "text": "model and then it's going to create an endpoint in order to host that model",
    "start": "2514099",
    "end": "2519200"
  },
  {
    "text": "that we have just created so here where it's making a create endpoint request so it is",
    "start": "2519200",
    "end": "2525200"
  },
  {
    "text": "making an endpoint to create an endpoint of stage maker and this is where our this machine learning model will be",
    "start": "2525200",
    "end": "2531140"
  },
  {
    "text": "hosted at so the same thing as before we can go directly to the stage maker",
    "start": "2531140",
    "end": "2536240"
  },
  {
    "text": "console and look at the endpoint that's been created so this is the endpoint that's being created by SPARC and that's",
    "start": "2536240",
    "end": "2542809"
  },
  {
    "text": "the endpoint that later on we can use to make API calls and you use that in order",
    "start": "2542809",
    "end": "2548960"
  },
  {
    "text": "to make predictions on our data so let me go ahead and skip this - once the",
    "start": "2548960",
    "end": "2554150"
  },
  {
    "start": "2552000",
    "end": "2584000"
  },
  {
    "text": "endpoint creation is completed so here now once this is completed now we're",
    "start": "2554150",
    "end": "2559849"
  },
  {
    "text": "actually running the transformation of the data so this is the part where I'm running on the application the actual",
    "start": "2559849",
    "end": "2566329"
  },
  {
    "text": "transformation in order to predict the first five rows of that so here we can",
    "start": "2566329",
    "end": "2571609"
  },
  {
    "text": "see that it's starting the job to show where I have that directly on my drooper",
    "start": "2571609",
    "end": "2576950"
  },
  {
    "text": "application and here in a bit that application is going to be complete so let's go ahead and see what does that",
    "start": "2576950",
    "end": "2583519"
  },
  {
    "text": "look like once it's completed so here after I executed this particular",
    "start": "2583519",
    "end": "2588769"
  },
  {
    "start": "2584000",
    "end": "2748000"
  },
  {
    "text": "paragraph this is the output of that stage maker application and the interesting thing to know over here this",
    "start": "2588769",
    "end": "2596269"
  },
  {
    "text": "is our label and predictions for the data frame and here we can see that the label these are the numbers that",
    "start": "2596269",
    "end": "2602450"
  },
  {
    "text": "actually are with those images so we can see that the first image has number 7 in there and our predictions for it based",
    "start": "2602450",
    "end": "2609349"
  },
  {
    "text": "on the features for this particular image was actually 7 in fact we can see",
    "start": "2609349",
    "end": "2614779"
  },
  {
    "text": "that they actually be XG boost algorithm is really good for this kind of problem because every single one of our",
    "start": "2614779",
    "end": "2620869"
  },
  {
    "text": "predictions it was actually correct and it matches or at least the first 5 rows it matches correctly based on our labels",
    "start": "2620869",
    "end": "2627680"
  },
  {
    "text": "so here we have number 4 and based on the features for number 4 it predicted",
    "start": "2627680",
    "end": "2632930"
  },
  {
    "text": "that it is indeed number 4 so now what if you want to simply use an existing",
    "start": "2632930",
    "end": "2637940"
  },
  {
    "text": "sage maker endpoint and make an API call to that without having to start over by",
    "start": "2637940",
    "end": "2645499"
  },
  {
    "text": "creating a new endpoint or do everything that we did earlier that's also possible",
    "start": "2645499",
    "end": "2650779"
  },
  {
    "text": "so you can simply already leverage this existing endpoint even from your spark application to do",
    "start": "2650779",
    "end": "2657100"
  },
  {
    "text": "your batch prediction so here the next example I'm showing you how you can make a batch prediction using an existing",
    "start": "2657100",
    "end": "2664120"
  },
  {
    "text": "sage maker endpoint so I'm also using the same XG boost algorithm as I have",
    "start": "2664120",
    "end": "2669730"
  },
  {
    "text": "used before now one of the things about the they actually boost algorithm is it",
    "start": "2669730",
    "end": "2675070"
  },
  {
    "text": "does need to have the input format as CSV or a live SVM but here the thing to",
    "start": "2675070",
    "end": "2681190"
  },
  {
    "text": "know is that I'm specifying do not create an endpoint policy meaning use",
    "start": "2681190",
    "end": "2686380"
  },
  {
    "text": "the existing endpoint that I have in there and this time around I'm actually running a prediction on a data frame",
    "start": "2686380",
    "end": "2693400"
  },
  {
    "text": "containing ten rows so let's see what is the results that I got returned by",
    "start": "2693400",
    "end": "2698590"
  },
  {
    "text": "running this prediction on ten rows and let's see if we also get that right so here are specified the training data and",
    "start": "2698590",
    "end": "2705160"
  },
  {
    "text": "I specify transformation on that to predict the first ten rows that we can",
    "start": "2705160",
    "end": "2710500"
  },
  {
    "text": "also see that every single one of our predictions was actually correct so number nine number nine number five and",
    "start": "2710500",
    "end": "2717190"
  },
  {
    "text": "so on so this really showed that this algorithm is working well so at this point you can move forward and pretty",
    "start": "2717190",
    "end": "2724720"
  },
  {
    "text": "much having that model hosted in stage maker now the next question would be alright this is how we did it from SPARC",
    "start": "2724720",
    "end": "2731410"
  },
  {
    "text": "and now we have that model on stage maker it's being available for us whenever we want to make predictions so",
    "start": "2731410",
    "end": "2738130"
  },
  {
    "text": "what if you want to make a prediction on a single record so as I mentioned before you can still make your predictions",
    "start": "2738130",
    "end": "2743830"
  },
  {
    "text": "without having to use SPARC and that's what the next example is so here I'm",
    "start": "2743830",
    "end": "2749200"
  },
  {
    "start": "2748000",
    "end": "2766000"
  },
  {
    "text": "running a test our prediction by just using a single record for our image and",
    "start": "2749200",
    "end": "2755650"
  },
  {
    "text": "I'm making this prediction without using spark so here I'm using an example Lib",
    "start": "2755650",
    "end": "2762490"
  },
  {
    "text": "SVM format data set they want to look in our training sample this is what the",
    "start": "2762490",
    "end": "2768340"
  },
  {
    "start": "2766000",
    "end": "2842000"
  },
  {
    "text": "live SVM format looks like for that image so this is a single row and it contains the label as well as the",
    "start": "2768340",
    "end": "2774940"
  },
  {
    "text": "different features for that so let's go ahead and run this and once we run this prediction and this is using a Python we",
    "start": "2774940",
    "end": "2782680"
  },
  {
    "text": "should be able to determine what what is the the prediction for that",
    "start": "2782680",
    "end": "2787900"
  },
  {
    "text": "number which is label number five if you look at our record here again let's see",
    "start": "2787900",
    "end": "2793240"
  },
  {
    "text": "here so we can see that our labels number five so this the prediction for here should be number five as well now",
    "start": "2793240",
    "end": "2799569"
  },
  {
    "text": "let's see if that matches and here it is label is number five and our prediction",
    "start": "2799569",
    "end": "2804999"
  },
  {
    "text": "is also number five now one of the cool things for you to note here is that this prediction took 178 milliseconds that's",
    "start": "2804999",
    "end": "2812980"
  },
  {
    "text": "because was the first time that I execute it if you ran it again is actually gonna be even faster so this would be the example where if you're",
    "start": "2812980",
    "end": "2819279"
  },
  {
    "text": "trying to run your application without using spark and you can do these inference or predictions for single",
    "start": "2819279",
    "end": "2825490"
  },
  {
    "text": "records based on this so this concludes the demo for everything that I wanted to",
    "start": "2825490",
    "end": "2831069"
  },
  {
    "text": "show you so let me go in if you can go back to the slides really quick just to finalize let's see",
    "start": "2831069",
    "end": "2839140"
  },
  {
    "text": "here so I just had a couple of slides but now there you go",
    "start": "2839140",
    "end": "2844960"
  },
  {
    "text": "so we got down with the demo but that's that concludes for the demo and the main the main takeaway that I would like for",
    "start": "2844960",
    "end": "2852039"
  },
  {
    "text": "you guys to take today is for really for you to look over our documentation so",
    "start": "2852039",
    "end": "2858489"
  },
  {
    "start": "2853000",
    "end": "2904000"
  },
  {
    "text": "the spark say the our stage maker documentation as well as the spark documentation we have various notebooks",
    "start": "2858489",
    "end": "2865960"
  },
  {
    "text": "so all of the examples that I showed here today is available online so you can go and there and start using some",
    "start": "2865960",
    "end": "2872559"
  },
  {
    "text": "example notebooks in order to test spark integration with sage maker and and",
    "start": "2872559",
    "end": "2878529"
  },
  {
    "text": "really that's concludes here for the session today and once again I appreciate all of you for coming and",
    "start": "2878529",
    "end": "2883779"
  },
  {
    "text": "staying out throughout the session despite being late and I hope you really got something useful here that you can",
    "start": "2883779",
    "end": "2889480"
  },
  {
    "text": "take back with you and use it for your daily use cases so once again thank you",
    "start": "2889480",
    "end": "2895239"
  },
  {
    "text": "for coming and please don't forget to complete the survey at the end of the session thank you everyone",
    "start": "2895239",
    "end": "2900420"
  },
  {
    "text": "[Applause] [Music] [Applause]",
    "start": "2900420",
    "end": "2905889"
  }
]