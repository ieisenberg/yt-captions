[
  {
    "start": "0",
    "end": "90000"
  },
  {
    "text": "all right I think we're gonna go ahead and get started my name is Ben Schneider I'm a specialist architect at AWS I",
    "start": "0",
    "end": "6629"
  },
  {
    "text": "focus on the data and analytic services as well as the AI and AML services in",
    "start": "6629",
    "end": "12450"
  },
  {
    "text": "today's session we're gonna be covering big data best practices and and how to",
    "start": "12450",
    "end": "19800"
  },
  {
    "text": "structure the architecture when you're building big data and analytical systems the way we're actually going to be",
    "start": "19800",
    "end": "24990"
  },
  {
    "text": "stepping through this is we're gonna be stepping through first framing analytical systems by some common",
    "start": "24990",
    "end": "30480"
  },
  {
    "text": "challenges and constraints how our customers facing these big data systems and what are some common characteristics",
    "start": "30480",
    "end": "37920"
  },
  {
    "text": "of those systems we're going to step through a set of reference architectures and doing so we're going to map various",
    "start": "37920",
    "end": "43980"
  },
  {
    "text": "tools both AWS tools as well as open source tools to see where they fit into",
    "start": "43980",
    "end": "49289"
  },
  {
    "text": "those reference architectures we have a couple demos throughout to show some",
    "start": "49289",
    "end": "54300"
  },
  {
    "text": "real systems running you know they're all live demos it's not recorded so we're gonna switch to some demos",
    "start": "54300",
    "end": "60420"
  },
  {
    "text": "throughout here and then finish out with a couple customer references I should",
    "start": "60420",
    "end": "65970"
  },
  {
    "text": "let everyone know this is a two hundred level session meaning we're gonna talk a lot about reference architectures we're",
    "start": "65970",
    "end": "71729"
  },
  {
    "text": "gonna give a few demonstrations we're not going to dive 400 level deep on any single service like Kinesis or EMR or",
    "start": "71729",
    "end": "78299"
  },
  {
    "text": "redshift we have separate sessions for that but this is meant to be a overview of the different types of architectures",
    "start": "78299",
    "end": "85110"
  },
  {
    "text": "and where the services fit into those architectures and as we get started here",
    "start": "85110",
    "end": "90630"
  },
  {
    "text": "it's really good to frame not every big data system is the same so there's not a",
    "start": "90630",
    "end": "96090"
  },
  {
    "text": "single one size fits all for these analytical systems what we find is there's really different types of",
    "start": "96090",
    "end": "102000"
  },
  {
    "text": "systems so some customers are looking at doing batch and interactive analytics",
    "start": "102000",
    "end": "107759"
  },
  {
    "text": "with their data other customers are looking at being able to take real-time data feeds and being able to not only",
    "start": "107759",
    "end": "114570"
  },
  {
    "text": "ingest that data and store it in a repository but also be able to drive",
    "start": "114570",
    "end": "119850"
  },
  {
    "text": "insights in real time and take actions on those insights and then of course being able to take data build models",
    "start": "119850",
    "end": "126360"
  },
  {
    "text": "power inferencing and do machine learning with that data so as we're stepping through these reference",
    "start": "126360",
    "end": "132060"
  },
  {
    "text": "architectures we're going to show architectures for stream processing for data lakes and batching interaction for machine",
    "start": "132060",
    "end": "138000"
  },
  {
    "text": "learning and I should note that not you don't have to pick one of these these aren't mutually exclusive so you don't",
    "start": "138000",
    "end": "143069"
  },
  {
    "text": "have to boil the ocean you don't have to say I want to do stream processing instead of batching or interactive processing you can start small",
    "start": "143069",
    "end": "150080"
  },
  {
    "text": "architected and weights start building on top of those and then you know incrementally add to the capability as",
    "start": "150080",
    "end": "156840"
  },
  {
    "text": "as you're building it it's really good to be able to frame these services when",
    "start": "156840",
    "end": "163110"
  },
  {
    "start": "160000",
    "end": "260000"
  },
  {
    "text": "it comes to how the delivery model is so most people in the room are probably familiar with virtualized servers ec2 so",
    "start": "163110",
    "end": "171690"
  },
  {
    "text": "when customers are running things like Kafka and we'll talk about what this is on ec2 that's a virtualized service",
    "start": "171690",
    "end": "178590"
  },
  {
    "text": "meaning you as a customer would manage the Kafka and all the other components",
    "start": "178590",
    "end": "184320"
  },
  {
    "text": "above the OS we have services that fit into the managed category so things like",
    "start": "184320",
    "end": "189540"
  },
  {
    "text": "EMR which is our Hadoop platform as a service things like RDS these services",
    "start": "189540",
    "end": "195540"
  },
  {
    "text": "you don't necessarily have to manage the service your self will do that on your behalf so you can set up a Hadoop",
    "start": "195540",
    "end": "201360"
  },
  {
    "text": "cluster tell us you want it to be ten nodes scale to a hundred nodes scale back down to 50 and you can configure",
    "start": "201360",
    "end": "208320"
  },
  {
    "text": "all this but you as a customer are still thinking about servers so you're still taking a look at what you need",
    "start": "208320",
    "end": "214350"
  },
  {
    "text": "setting up the auto scaling policies insane I want this number of servers here's how I want to scale it when we",
    "start": "214350",
    "end": "220890"
  },
  {
    "text": "look at server lists or cluster lists things like lambda Athena glue these are",
    "start": "220890",
    "end": "226140"
  },
  {
    "text": "services that really abstract out the servers away from you we're going to give a couple",
    "start": "226140",
    "end": "231570"
  },
  {
    "text": "demonstrations of things like Athena where we're gonna be performing sequel based analytics on data even though",
    "start": "231570",
    "end": "237630"
  },
  {
    "text": "we're going to be using sequel we're not going to be actually storing the data in a relational database so we're gonna be querying the data under the covers",
    "start": "237630",
    "end": "243810"
  },
  {
    "text": "that's using presto which is part of the big data ecosystem and what you do is",
    "start": "243810",
    "end": "249030"
  },
  {
    "text": "you just issue you look query you don't have to worry about if it took 10 servers 100 servers whatever it took to actually execute that query so the",
    "start": "249030",
    "end": "255720"
  },
  {
    "text": "servers are completely abstracted away you just write the code you write the analytic that matters and this is",
    "start": "255720",
    "end": "261780"
  },
  {
    "start": "260000",
    "end": "285000"
  },
  {
    "text": "important when you're looking at the different types of tools out there so as we're looking at these architectures we're going to be looking",
    "start": "261780",
    "end": "267190"
  },
  {
    "text": "open-source products we're going to be looking at partner products and AWS services and fitting them into these",
    "start": "267190",
    "end": "272830"
  },
  {
    "text": "architectures and as we're finding them into the architectures it's good to analyze you know does it make sense to",
    "start": "272830",
    "end": "280120"
  },
  {
    "text": "roll your own on ec2 does it make sense to use a managed service or service service so as we're stepping through",
    "start": "280120",
    "end": "286960"
  },
  {
    "text": "here we're gonna be looking at what are the various reference architectures what are the tools that help me implement",
    "start": "286960",
    "end": "293560"
  },
  {
    "text": "that reference architecture you know how how would I put together those tools we're gonna give a couple demonstrations",
    "start": "293560",
    "end": "299860"
  },
  {
    "text": "later on towards the end of this and really why am i putting these tools together specifically and before we dive",
    "start": "299860",
    "end": "306790"
  },
  {
    "text": "into that it's really good to set some tenants set some principles as we're looking at these best practices and",
    "start": "306790",
    "end": "311920"
  },
  {
    "text": "architectures this first tenant isn't exclusive to big data analytics it's really building loosely coupled or",
    "start": "311920",
    "end": "318760"
  },
  {
    "text": "decoupled systems what that really means here in the Big Data space is the way I analyze my data shouldn't be dependent",
    "start": "318760",
    "end": "326080"
  },
  {
    "text": "on the way I collect my data meaning I could change the way I collect my data the tool maybe it's streaming maybe it's",
    "start": "326080",
    "end": "331900"
  },
  {
    "text": "a batch oriented but I want to decouple the way I collect my data the way I",
    "start": "331900",
    "end": "337150"
  },
  {
    "text": "store my data the way I process and consume the data and that really lets you future proof that lets you iterate",
    "start": "337150",
    "end": "342730"
  },
  {
    "text": "and build a system that lets you really migrate it over time and we'll take a",
    "start": "342730",
    "end": "348190"
  },
  {
    "text": "good look at a couple of those examples and if you have that loose coupling another one of the tenants is being able",
    "start": "348190",
    "end": "354790"
  },
  {
    "start": "350000",
    "end": "391000"
  },
  {
    "text": "to pick the right tool for the right job so rather than trying to pick one tool that does absolutely everything if you",
    "start": "354790",
    "end": "361060"
  },
  {
    "text": "have streaming data and you need to real-time analytics maybe we'll use something like a spark streaming or Kinesis analytics or flink to be able to",
    "start": "361060",
    "end": "368380"
  },
  {
    "text": "do that if you really want to be able to do machine learning a durp deep learning you're gonna use different libraries",
    "start": "368380",
    "end": "374200"
  },
  {
    "text": "like I'm mixing that or tensorflow or one of those tools to be able to do that so the key is being able to architect it",
    "start": "374200",
    "end": "381040"
  },
  {
    "text": "so that the data stays in one spot you have a logical data model or data set of that data and are able to use these",
    "start": "381040",
    "end": "387970"
  },
  {
    "text": "various tools to be able to ingest and process that data this isn't really an",
    "start": "387970",
    "end": "393730"
  },
  {
    "start": "391000",
    "end": "425000"
  },
  {
    "text": "architectural tenant but really lets you have speed and agility but anytime you",
    "start": "393730",
    "end": "398919"
  },
  {
    "text": "could leverage those managed services or service services it really lets you focus on what matters it lets you focus on the",
    "start": "398919",
    "end": "405510"
  },
  {
    "text": "analytic the transformation the ETL and rather than loading software in those",
    "start": "405510",
    "end": "411990"
  },
  {
    "text": "other pieces so this is a really good aspect it's not really a architecture so you get swap out you know for example at",
    "start": "411990",
    "end": "417900"
  },
  {
    "text": "Kafka to a Kinesis data streams depending on how you're architecting it but it's really good principle that we",
    "start": "417900",
    "end": "424260"
  },
  {
    "text": "work with customers on event journaling what this means is as you're collecting",
    "start": "424260",
    "end": "430110"
  },
  {
    "start": "425000",
    "end": "480000"
  },
  {
    "text": "your data and these big data systems it's really really good best practice to not overwrite your data so if you're",
    "start": "430110",
    "end": "436770"
  },
  {
    "text": "getting data records and some of those data records are getting corrected rather than replacing those records keep",
    "start": "436770",
    "end": "442320"
  },
  {
    "text": "appending to your data set keep adding to that data set and what happens in these big data systems and why that is",
    "start": "442320",
    "end": "448590"
  },
  {
    "text": "is if you have large volumes of data and if there's ever an issue an air in your",
    "start": "448590",
    "end": "453960"
  },
  {
    "text": "ETL process if there's an issue on somebody accidentally going in and deleting something or something like",
    "start": "453960",
    "end": "459750"
  },
  {
    "text": "that you always have your source of record in its original form you could go back to and be able to then analyze and",
    "start": "459750",
    "end": "466349"
  },
  {
    "text": "process and then rehydrate or regenerate your datasets and we'll look at how that looks when it comes to de lakes and s3",
    "start": "466349",
    "end": "472950"
  },
  {
    "text": "we have a couple other sessions dedicated today like so we're not going to spend a lot of time on that but we'll",
    "start": "472950",
    "end": "478740"
  },
  {
    "text": "give them a few examples be cost conscious costs conscious excuse me you",
    "start": "478740",
    "end": "485700"
  },
  {
    "start": "480000",
    "end": "516000"
  },
  {
    "text": "know a lot of times big data doesn't have to be big costs if you architect it correctly you know let's say you're building a",
    "start": "485700",
    "end": "491370"
  },
  {
    "text": "Hadoop system if you're decoupling compute and storage and scaling those independently you could build a very",
    "start": "491370",
    "end": "497400"
  },
  {
    "text": "very cost effective and performance system and keep that cost down and lastly adding machine learning this is",
    "start": "497400",
    "end": "505020"
  },
  {
    "text": "definitely a rapid trend we see more and more customers as they're collecting the data wants to be able to leverage",
    "start": "505020",
    "end": "510889"
  },
  {
    "text": "application level services platform services these different services to be able to build their models so as we're",
    "start": "510889",
    "end": "518130"
  },
  {
    "start": "516000",
    "end": "604000"
  },
  {
    "text": "framing this we're gonna frame it around this simplified data processing pipeline and this is definitely not necessarily",
    "start": "518130",
    "end": "525570"
  },
  {
    "text": "every pipeline we see but it's good to be able to set context around some logical constructs here so if you take a",
    "start": "525570",
    "end": "531420"
  },
  {
    "text": "look at this we separate out into collecting how am I capture and collect this information and",
    "start": "531420",
    "end": "537330"
  },
  {
    "text": "the key thing is if I have different datasets I may not want to collect the same type of data or a different",
    "start": "537330",
    "end": "543089"
  },
  {
    "text": "datasets the same way so if I have GPS data or clickstream data I might want to",
    "start": "543089",
    "end": "548339"
  },
  {
    "text": "collect that very very differently than I'm if I'm collecting imagery or satellite data so as we're stepping",
    "start": "548339",
    "end": "554820"
  },
  {
    "text": "through these architectures will categorize these services into what they're good at in these different",
    "start": "554820",
    "end": "559980"
  },
  {
    "text": "phases being able to collect store process analyzing consume the data and",
    "start": "559980",
    "end": "565320"
  },
  {
    "text": "you'll notice their cycles here you know it's not necessarily waterfall you know oftentimes you collect and stored the raw data that raw data is oftentimes in",
    "start": "565320",
    "end": "573420"
  },
  {
    "text": "the original form so it might be CSV JSON whatever form it may be then you",
    "start": "573420",
    "end": "578790"
  },
  {
    "text": "oftentimes really want to be able to take that data and create curated datasets query optimized datasets to be",
    "start": "578790",
    "end": "585060"
  },
  {
    "text": "able to very rapidly access that data it could be through interactive queries",
    "start": "585060",
    "end": "590100"
  },
  {
    "text": "through machine learning through your data warehousing so you know this is a really iterative process once you",
    "start": "590100",
    "end": "596550"
  },
  {
    "text": "collect the data start getting into a canonic or or normalized form and then ultimately consume it by different",
    "start": "596550",
    "end": "602190"
  },
  {
    "text": "stakeholders so as we're talking about this oftentimes we talk about temperature you know high velocity of",
    "start": "602190",
    "end": "609450"
  },
  {
    "text": "your data high velocity of your queries high velocity of the analytics and when",
    "start": "609450",
    "end": "614640"
  },
  {
    "text": "we're stepping through this we're gonna be talking about the temperature across those those spectrums so let's go ahead",
    "start": "614640",
    "end": "620760"
  },
  {
    "text": "and step through I need to remember stays behind here so if you look at the",
    "start": "620760",
    "end": "625920"
  },
  {
    "start": "622000",
    "end": "647000"
  },
  {
    "text": "collection face so oftentimes you have different data sources so you might have",
    "start": "625920",
    "end": "631010"
  },
  {
    "text": "records coming in from transactional databases you might have a relational database you might have a or a",
    "start": "631010",
    "end": "637620"
  },
  {
    "text": "different type of knows sequel database and these oftentimes are records coming in that you need to be able to analyze",
    "start": "637620",
    "end": "642990"
  },
  {
    "text": "in process and we could call those transactions similar you might have log",
    "start": "642990",
    "end": "648870"
  },
  {
    "start": "647000",
    "end": "655000"
  },
  {
    "text": "data application logs media file is these large file objects that you need",
    "start": "648870",
    "end": "654120"
  },
  {
    "text": "to be able to store and then streaming data and the reason we like to show this a lot of times is each of those",
    "start": "654120",
    "end": "659760"
  },
  {
    "start": "655000",
    "end": "677000"
  },
  {
    "text": "collection methods when we're helping customers architect our solution oftentimes require a different way of",
    "start": "659760",
    "end": "665940"
  },
  {
    "text": "collecting that data so we're talking about things like how to do streaming analytics and collect streaming data versus how to",
    "start": "665940",
    "end": "673010"
  },
  {
    "text": "collect data from a relational database as an example and when you're looking at",
    "start": "673010",
    "end": "678120"
  },
  {
    "text": "being able to store this data if we take a look at storing the day oftentimes transactional data goes into one form of",
    "start": "678120",
    "end": "684480"
  },
  {
    "text": "a database it might be a no sequel database it might be a relational database and we'll step through some of",
    "start": "684480",
    "end": "690240"
  },
  {
    "text": "a track trade criteria of that media and log file is really the de facto standard",
    "start": "690240",
    "end": "696150"
  },
  {
    "text": "there is really s3 if you're doing this on premises that would be similar to you know doing something like HDFS with your",
    "start": "696150",
    "end": "703020"
  },
  {
    "text": "Hadoop cluster where you want a big file store or a big object store a data like that sort of thing and we stream storage",
    "start": "703020",
    "end": "709830"
  },
  {
    "text": "it's really data streams coming in to some source stream storage and really what that means is different sets of",
    "start": "709830",
    "end": "717330"
  },
  {
    "start": "713000",
    "end": "866000"
  },
  {
    "text": "tools you could use so the first one is Apache Kafka this is a open source project pretty well-established we have",
    "start": "717330",
    "end": "725880"
  },
  {
    "text": "lots of customers going from hybrid solutions hybrid architectures or built these type of solutions on premises that",
    "start": "725880",
    "end": "732030"
  },
  {
    "text": "are running Kafka today and you can very easily bring that into AWS run it on",
    "start": "732030",
    "end": "737160"
  },
  {
    "text": "your ec2 instances run in a virtual virtual hosting an area to be able to",
    "start": "737160",
    "end": "743160"
  },
  {
    "text": "bring that into AWS you have Amazon data streams so data streams is a little",
    "start": "743160",
    "end": "749160"
  },
  {
    "text": "different where data streams is really at the highest level so we need to find a data stream what you define is a",
    "start": "749160",
    "end": "755460"
  },
  {
    "text": "number of shards let's say you want to be able to process a thousand records per second that's a single shard if you",
    "start": "755460",
    "end": "762300"
  },
  {
    "text": "want to scale up to a hundred thousand records you can very easily just change the shard numbers you don't have to",
    "start": "762300",
    "end": "767970"
  },
  {
    "text": "worry about is it taking ten servers 100 servers how many servers under the covers across all the different",
    "start": "767970",
    "end": "773010"
  },
  {
    "text": "availability zones to make it highly available to be able to do that you just configure based on sharding how much",
    "start": "773010",
    "end": "779760"
  },
  {
    "text": "capacity you want out of that pipe and what we did is we actually release that first out of the Kinesis platform and",
    "start": "779760",
    "end": "786870"
  },
  {
    "text": "then we saw customers using that for a very very common use case what if I have streaming data IOT sensors GPS click",
    "start": "786870",
    "end": "794760"
  },
  {
    "text": "stream any of that streaming data sources and really what I want to do is instead of doing real-time insights I",
    "start": "794760",
    "end": "800040"
  },
  {
    "text": "want to capture that data and do sort of advanced processing or offline processing of that data and that's where",
    "start": "800040",
    "end": "806700"
  },
  {
    "text": "Kinesis data firehose comes in what that allows you to do is configure an endpoint to be able to consume store the",
    "start": "806700",
    "end": "814320"
  },
  {
    "text": "data so you could configure things like s3 if you're populating a data lake if you want to populate a an elk stack you",
    "start": "814320",
    "end": "822090"
  },
  {
    "text": "could configure Amazon Elastic search you configure these various destinations to be able to store the data as the data",
    "start": "822090",
    "end": "829710"
  },
  {
    "text": "is flowing in and that is actually priced a little bit differently than",
    "start": "829710",
    "end": "835170"
  },
  {
    "text": "things like Kinesis data streams so in data streams you actually configure our number of shards with data firehose it's",
    "start": "835170",
    "end": "841170"
  },
  {
    "text": "purely based on the amount of data that's sent through that that pipe so you don't have the pre provision there's",
    "start": "841170",
    "end": "846540"
  },
  {
    "text": "some soft limits of records per second within your account so I always take a look at those soft limits but you don't",
    "start": "846540",
    "end": "853470"
  },
  {
    "text": "have to actually pre provision for five thousand records all the way down to you know 300 or whatever it may be it's",
    "start": "853470",
    "end": "860370"
  },
  {
    "text": "really build per megabyte or per gigabyte sent through that pipe and then store it into your back-end repository",
    "start": "860370",
    "end": "866870"
  },
  {
    "start": "866000",
    "end": "911000"
  },
  {
    "text": "so we have a few of these throughout this presentation don't worry these are all going to be on SlideShare as well",
    "start": "866870",
    "end": "873270"
  },
  {
    "text": "but here's some of the trade criteria of what type of stream service to use and",
    "start": "873270",
    "end": "879120"
  },
  {
    "text": "one question I like I like to ask a lot of times when customers are looking at things like for example sqs versus",
    "start": "879120",
    "end": "884520"
  },
  {
    "text": "Kinesis sqs is a really really great service it allows you to have reliable",
    "start": "884520",
    "end": "889620"
  },
  {
    "text": "queuing within AWS but if you really need to be able to have a one-to-many fan-out if you have streaming real-time",
    "start": "889620",
    "end": "895950"
  },
  {
    "text": "data that needs to be able to be able to process and reprocess later on that's",
    "start": "895950",
    "end": "901170"
  },
  {
    "text": "really work stream storage comes in versus have one consumer take a message off the queue have a visibility timeout",
    "start": "901170",
    "end": "907890"
  },
  {
    "text": "that sort of thing so if we then shift",
    "start": "907890",
    "end": "913320"
  },
  {
    "start": "911000",
    "end": "970000"
  },
  {
    "text": "over and look at object storage now s3 is really the de facto standard method",
    "start": "913320",
    "end": "919470"
  },
  {
    "text": "of storing objects or storing files out into the cloud here and what it allows",
    "start": "919470",
    "end": "925050"
  },
  {
    "text": "you to do is really build your data Lake or build that object storage very very",
    "start": "925050",
    "end": "930990"
  },
  {
    "text": "robustly and what I mean by that is you could very easily run lots of different analytics on on s3 it's really natively supported",
    "start": "930990",
    "end": "939329"
  },
  {
    "text": "by a wide number of tools including the Hadoop ecosystem including our service",
    "start": "939329",
    "end": "945240"
  },
  {
    "text": "called EMR or her new platform but also our partner products clatter I'm at bar horn works they're all able to talk to",
    "start": "945240",
    "end": "951449"
  },
  {
    "text": "s3 natively to be able to read and process that data we're actually getting",
    "start": "951449",
    "end": "957569"
  },
  {
    "text": "give a couple demonstrations here in a moment processing data directly out of s3 through EMR our magic aduke platform",
    "start": "957569",
    "end": "964679"
  },
  {
    "text": "using sage maker Jupiter environment to be able to show a couple of demonstrations the other really powerful",
    "start": "964679",
    "end": "972480"
  },
  {
    "start": "970000",
    "end": "1109000"
  },
  {
    "text": "thing and it's interesting I came from environment where before joining AWS about four years ago I actually built a",
    "start": "972480",
    "end": "978629"
  },
  {
    "text": "very set of large Hadoop clusters on premises and if you would have asked me before if you decoupled compute and",
    "start": "978629",
    "end": "985619"
  },
  {
    "text": "storage like back when Hadoop was coming out you know people would look at you like you're crazy because that was a",
    "start": "985619",
    "end": "990689"
  },
  {
    "text": "whole premise of MapReduce right you want to bring you the compute to the storage and be able to process that what",
    "start": "990689",
    "end": "996149"
  },
  {
    "text": "we're really finding is both through this evolution of these types of analytics things like spark with the",
    "start": "996149",
    "end": "1001910"
  },
  {
    "text": "rdd's and the data frames presto these different types of analytics as well as how did it's architected be able to talk",
    "start": "1001910",
    "end": "1008809"
  },
  {
    "text": "to s3 you could really very very performant ly decouple this this compute",
    "start": "1008809",
    "end": "1014029"
  },
  {
    "text": "to the storage and still run these Hadoop based workloads really really effectively we have customers like FINRA",
    "start": "1014029",
    "end": "1020540"
  },
  {
    "text": "are running very large H bases purely backed by s3 so they're not using HTTP",
    "start": "1020540",
    "end": "1025970"
  },
  {
    "text": "for their for their different log log files and stuff like that for their HBase they're talking directly to s3",
    "start": "1025970",
    "end": "1031990"
  },
  {
    "text": "for all those artifacts so you could architect it so that is very very performant we do have some edge cases",
    "start": "1031990",
    "end": "1039230"
  },
  {
    "text": "working with customers where they want to cache that data be able to store that data locally s3 oftentimes though",
    "start": "1039230",
    "end": "1045110"
  },
  {
    "text": "becomes the source of record and we have services like s3 disk copy it's a library you could run to create caches",
    "start": "1045110",
    "end": "1050690"
  },
  {
    "text": "of that data on your cluster to be able to have if you really do have the",
    "start": "1050690",
    "end": "1055760"
  },
  {
    "text": "repetitive breeding off of disk over and over again maybe it's an old pig or hive job that is constantly running on that",
    "start": "1055760",
    "end": "1063020"
  },
  {
    "text": "on that data set with us three you get a crazy amount of number 911",
    "start": "1063020",
    "end": "1069380"
  },
  {
    "text": "durability I think that means if you store 10,000 objects you might might on average lose one in every like million",
    "start": "1069380",
    "end": "1075620"
  },
  {
    "text": "years so tremendous data reliability I'd never achieved that when I was building on premises Hadoop clusters you also",
    "start": "1075620",
    "end": "1083750"
  },
  {
    "text": "don't have to worry about data replication within the region so if",
    "start": "1083750",
    "end": "1088820"
  },
  {
    "text": "you're actually building these systems a lot of times you have to worry about replicas or copies of the data within within the region with s3 you don't have",
    "start": "1088820",
    "end": "1096740"
  },
  {
    "text": "to worry about that that's all handled under the covers that's one of the reasons you achieve that are we achieve",
    "start": "1096740",
    "end": "1102350"
  },
  {
    "text": "that through our ability number it's very very secure and low cost so let's",
    "start": "1102350",
    "end": "1110180"
  },
  {
    "start": "1109000",
    "end": "1190000"
  },
  {
    "text": "talk a little bit about data tearing so when it comes to data tearing we actually just announced it doesn't have",
    "start": "1110180",
    "end": "1117320"
  },
  {
    "text": "it on the slide we actually just announced new intelligence tearing I don't know if you guys saw today so",
    "start": "1117320",
    "end": "1122710"
  },
  {
    "text": "imagine there's a line on this slide that talks about that as well but with data turning you could actually tear",
    "start": "1122710",
    "end": "1128630"
  },
  {
    "text": "your s3 storage based on different classes so the very frequent data that",
    "start": "1128630",
    "end": "1133880"
  },
  {
    "text": "you're always accessing is usually in standard you have infrequent access you have glacier we have a report you can",
    "start": "1133880",
    "end": "1142070"
  },
  {
    "text": "run called the s3 Analytics report that would tell you you know which objects it recommends to be able to shift between",
    "start": "1142070",
    "end": "1150890"
  },
  {
    "text": "the storage tiers I should mention we do have customers still leveraging things like HDFS when you launch a EMR cluster",
    "start": "1150890",
    "end": "1158360"
  },
  {
    "text": "which is that Hadoop managed platform or when you launch one of our partner products running Hadoop on AWS that is",
    "start": "1158360",
    "end": "1165050"
  },
  {
    "text": "still there's still HDFS there's still scratch space you know there's still that that location to be able to have to",
    "start": "1165050",
    "end": "1172100"
  },
  {
    "text": "be able to very rapidly read and write from s3 just becomes that source of record that is always referred back to",
    "start": "1172100",
    "end": "1178510"
  },
  {
    "text": "so we've talked a little bit about streaming we've talked about object storage we're gonna dive it a little bit",
    "start": "1178510",
    "end": "1185180"
  },
  {
    "text": "more into databases and then give a few demonstrations of of these things here",
    "start": "1185180",
    "end": "1190390"
  },
  {
    "start": "1190000",
    "end": "1252000"
  },
  {
    "text": "so when it comes to databases there's really a wide number of databases that are out there that you could use the top",
    "start": "1190390",
    "end": "1197450"
  },
  {
    "text": "two really are caching caching services so ElastiCache allows you to write us",
    "start": "1197450",
    "end": "1202730"
  },
  {
    "text": "from kache D we have Dax which is really a dynamodb front end that has a right",
    "start": "1202730",
    "end": "1208160"
  },
  {
    "text": "right through cash to be able to cash your dynamodb Neptune one of my one of",
    "start": "1208160",
    "end": "1214130"
  },
  {
    "text": "my many favorite new services to be able to have property graphs or RDF graphs",
    "start": "1214130",
    "end": "1219470"
  },
  {
    "text": "and be able to do very very rich graph queries against against your data set",
    "start": "1219470",
    "end": "1224860"
  },
  {
    "text": "dynamodb which really allows you to store key values or JSON documents within it and then RDS both kind of",
    "start": "1224860",
    "end": "1232720"
  },
  {
    "text": "vanilla RDS that lets you run a wide number of different engines as well as aura and you might ask you know which",
    "start": "1232720",
    "end": "1239840"
  },
  {
    "text": "tool should I use you know if if I'm building a system should I use a graph database or should I use a document",
    "start": "1239840",
    "end": "1245960"
  },
  {
    "text": "storage or should I use a traditional RDBMS and really what we find here is is",
    "start": "1245960",
    "end": "1252970"
  },
  {
    "start": "1252000",
    "end": "1451000"
  },
  {
    "text": "really choosing the best tool for the for the job and the way we help customers choose that is through a set",
    "start": "1252970",
    "end": "1259580"
  },
  {
    "text": "of questions so if we take a look at some of the questions that we oftentimes ask customers it really stems from",
    "start": "1259580",
    "end": "1266840"
  },
  {
    "text": "things like what is your data structure do I have a very fixed schema am i doing",
    "start": "1266840",
    "end": "1274370"
  },
  {
    "text": "key value lookups so if you know a great example is if I'm storing tons of session data and I want to store that in",
    "start": "1274370",
    "end": "1280280"
  },
  {
    "text": "a dynamo or a a key value store it's very very easy to be able to store that and retrieve it based on the key is it",
    "start": "1280280",
    "end": "1286940"
  },
  {
    "text": "very very relational data and I'm constantly traversing the graphs and doing different properties on that graph so the data structure itself will help",
    "start": "1286940",
    "end": "1294800"
  },
  {
    "text": "you very very easily dictate kind of what the right tool for the right job is but also the data access pattern I have",
    "start": "1294800",
    "end": "1302570"
  },
  {
    "text": "a lot of discussions with customers on should I use their aura or should I use redshift and really the way the data",
    "start": "1302570",
    "end": "1309110"
  },
  {
    "text": "gets ingested and the way you're using that data will help you answer that pretty quickly if it's a very very high",
    "start": "1309110",
    "end": "1315290"
  },
  {
    "text": "throughput transactional that data need then or is a great option if it if you really need that analytical capability",
    "start": "1315290",
    "end": "1321650"
  },
  {
    "text": "you really need a all ap rather than a transactional database then that's where",
    "start": "1321650",
    "end": "1327320"
  },
  {
    "text": "we see customers use redshift and got click I'm gonna put the clicker",
    "start": "1327320",
    "end": "1336430"
  },
  {
    "text": "down so here's a one my one of my favorite tables I promise you guys will",
    "start": "1336430",
    "end": "1342220"
  },
  {
    "text": "have a ton of throughout the session here so within here you know I highlighted two different rows just to",
    "start": "1342220",
    "end": "1348310"
  },
  {
    "text": "call out so if you're looking at these different options one of the key things is what's the use case the use case in",
    "start": "1348310",
    "end": "1354010"
  },
  {
    "text": "my opinion should really drive the major decision here if it's log data you know",
    "start": "1354010",
    "end": "1359170"
  },
  {
    "text": "you notice will call out elasticsearch if it's you know OLTP very transactional",
    "start": "1359170",
    "end": "1364240"
  },
  {
    "text": "something like aurora so the use case will really drive a lot of that but also that shape of that data what what does",
    "start": "1364240",
    "end": "1370030"
  },
  {
    "text": "that data look like that's needed it gets stored the other thing I should mention is a lot of these services",
    "start": "1370030",
    "end": "1376210"
  },
  {
    "text": "either runs at the regional level or runs within your VPC and when you're",
    "start": "1376210",
    "end": "1382030"
  },
  {
    "text": "looking at architecting your solution it's good to be able to look at these services and say this service is really",
    "start": "1382030",
    "end": "1387930"
  },
  {
    "text": "one that runs in my AZ for example or in my PPC for example RDS RDS runs you know",
    "start": "1387930",
    "end": "1395110"
  },
  {
    "text": "you get an e ni a network interface within your V PC versus something like a DynamoDB DynamoDB operates at the",
    "start": "1395110",
    "end": "1401500"
  },
  {
    "text": "regional level you don't have to worry about replicating that across a ZZZ we do that for you you can set up endpoints",
    "start": "1401500",
    "end": "1407500"
  },
  {
    "text": "with in your V PC but as you're looking at these options as good as you're architecting out and and looking at that",
    "start": "1407500",
    "end": "1414610"
  },
  {
    "text": "diagram which ones are the services run within the V pcs which ones have kind of",
    "start": "1414610",
    "end": "1419890"
  },
  {
    "text": "that the endpoints into that V PC even things like s/3 s/3 happens s3 is a",
    "start": "1419890",
    "end": "1426340"
  },
  {
    "text": "regional service but that doesn't mean you can't lock it down what I mean by that is you can create s3 buckets and",
    "start": "1426340",
    "end": "1432010"
  },
  {
    "text": "say that this bucket it is only accessible by this V PC and nobody else could access the data within that bucket",
    "start": "1432010",
    "end": "1438460"
  },
  {
    "text": "so even if it's at a regional level that doesn't mean it's not secure within your V PC you still lock things down but it's",
    "start": "1438460",
    "end": "1444730"
  },
  {
    "text": "good to be cognizant that way you're asking the right questions when you're agreeing that architecture so let's dive",
    "start": "1444730",
    "end": "1452770"
  },
  {
    "start": "1451000",
    "end": "1568000"
  },
  {
    "text": "into the processing phase here and we take a look at the processing the first",
    "start": "1452770",
    "end": "1458320"
  },
  {
    "text": "thing we'll look at is the interactive and batch processing and really what we'll find here is things like",
    "start": "1458320",
    "end": "1464530"
  },
  {
    "text": "elasticsearch allows to search is really fantastic at that log data the the textual data redshift and Russia's",
    "start": "1464530",
    "end": "1470770"
  },
  {
    "text": "spectrum for your really data warehousing needs Athena when my other",
    "start": "1470770",
    "end": "1476740"
  },
  {
    "text": "favorite relatively new services not that new anymore where you get actually perform sequel based querying against",
    "start": "1476740",
    "end": "1483130"
  },
  {
    "text": "your data as it lies on s3 one of the differences between something like Athena and using s3 select from there",
    "start": "1483130",
    "end": "1490300"
  },
  {
    "text": "with that is Athena is really operating at a logical data set level within the catalog of glue rather than doing a",
    "start": "1490300",
    "end": "1497740"
  },
  {
    "text": "selecting of data out of a particular object out of s3 so you could actually",
    "start": "1497740",
    "end": "1503020"
  },
  {
    "text": "have a suite of objects that you're querying over do complex queries over using something like Athena versus",
    "start": "1503020",
    "end": "1509170"
  },
  {
    "text": "another way of doing different types of gits s3 select really fantastic but that's one of the differences between",
    "start": "1509170",
    "end": "1514510"
  },
  {
    "text": "Athena and and that and of course the EMR it's really our Hadoop and Sparky",
    "start": "1514510",
    "end": "1520240"
  },
  {
    "text": "ecosystem as a service lets you run various platforms applications you could",
    "start": "1520240",
    "end": "1525580"
  },
  {
    "text": "actually bootstrap it to run a wide number of tools it's it it's actually interesting EMR is one of the services",
    "start": "1525580",
    "end": "1531100"
  },
  {
    "text": "we offer that's really the managed service you configure how many servers do you want we'll install the Hadoop if",
    "start": "1531100",
    "end": "1536530"
  },
  {
    "text": "one notes goes down we'll relaunch it all that jazz what's really interesting about it though is you actually have",
    "start": "1536530",
    "end": "1541570"
  },
  {
    "text": "root access up to the ec2 instances that EMR is using most of our managed",
    "start": "1541570",
    "end": "1546760"
  },
  {
    "text": "services you actually don't have that access so most of our like things like RDS you have a uni endpoint that you",
    "start": "1546760",
    "end": "1552580"
  },
  {
    "text": "could connect into you could perform database functions but you don't necessarily have SSH access into that service EMR is different where you",
    "start": "1552580",
    "end": "1559450"
  },
  {
    "text": "actually have and you actually see the ec2 instances that that service is using and launching and managing on your",
    "start": "1559450",
    "end": "1566260"
  },
  {
    "text": "behalf when it comes to real-time analytics really if you're looking at",
    "start": "1566260",
    "end": "1572770"
  },
  {
    "start": "1568000",
    "end": "1683000"
  },
  {
    "text": "the Hadoop ecosystem the number of services that do real-time analytics keeps growing you have spark streaming",
    "start": "1572770",
    "end": "1579220"
  },
  {
    "text": "flink you know things like storm to be able to run real-time analytics what's",
    "start": "1579220",
    "end": "1585850"
  },
  {
    "text": "interesting is Kinesis data analytics allows you it's very similar to how athena lets you query your your data",
    "start": "1585850",
    "end": "1592870"
  },
  {
    "text": "objects at rest in s three use in sequel even though it's not a relational database data analytics lets you perform",
    "start": "1592870",
    "end": "1599049"
  },
  {
    "text": "based queries on your real-time data even though it's not really in a database so it lets you connect write",
    "start": "1599049",
    "end": "1605529"
  },
  {
    "text": "sequel things like tumbling Windows random cut forests different source of expressions to be able to analyze that",
    "start": "1605529",
    "end": "1612700"
  },
  {
    "text": "data it's really easy to write because most people know is sequel but then under the covers it's gonna be",
    "start": "1612700",
    "end": "1618730"
  },
  {
    "text": "processing the data in real time off of your feed and then sending it somewhere else one thing that I should mention",
    "start": "1618730",
    "end": "1625889"
  },
  {
    "text": "sometimes I work with customers and with real-time analytics it's a very valid",
    "start": "1625889",
    "end": "1632109"
  },
  {
    "text": "question to ask you know what's my end-to-end latency from when I publish a message to when I start processing and",
    "start": "1632109",
    "end": "1637960"
  },
  {
    "text": "then get the final insights out and the tool that you use here might that's a",
    "start": "1637960",
    "end": "1643090"
  },
  {
    "text": "really important question to ask as you're picking the tool that you're using and let me give you a real example so today as it stands",
    "start": "1643090",
    "end": "1649210"
  },
  {
    "text": "aw slammed is really fantastic it lets you write your functions you can set up an event trigger for Kinesis off of that",
    "start": "1649210",
    "end": "1655299"
  },
  {
    "text": "but what happens with that is they actually pulls every second off of the event queue what that means if your",
    "start": "1655299",
    "end": "1661659"
  },
  {
    "text": "end-to-end lynsee needs to be less than a second you really shouldn't be using something like lambda today because it's pulling every second it could be more",
    "start": "1661659",
    "end": "1668889"
  },
  {
    "text": "than that so that's where you'd use something like a KCl or another approach and in those examples so that's why",
    "start": "1668889",
    "end": "1675489"
  },
  {
    "text": "there's these different services different ways of writing your analytics different Lane C requirements for those",
    "start": "1675489",
    "end": "1682799"
  },
  {
    "text": "and then a predictive analytics there's a lot of sessions and a lot of updated",
    "start": "1682859",
    "end": "1688059"
  },
  {
    "start": "1683000",
    "end": "1784000"
  },
  {
    "text": "diagrams from the one that you see here that lets you build different types of machine learning algorithms on on the",
    "start": "1688059",
    "end": "1694450"
  },
  {
    "text": "platform and the keep you don't want to point out here is when you're looking at the architecture of your data system is really just making sure the data is",
    "start": "1694450",
    "end": "1701139"
  },
  {
    "text": "easily consumable using open standards so it doesn't necessarily matter if",
    "start": "1701139",
    "end": "1707830"
  },
  {
    "text": "you're using Park Hae vs. Arce for a lot of these systems as long as it's an open standard those are two column-oriented",
    "start": "1707830",
    "end": "1713919"
  },
  {
    "text": "formats or if the data is in CSV that versus JSON versus different formats the",
    "start": "1713919",
    "end": "1719200"
  },
  {
    "text": "key is really to have the data in a in a form that's very easily consumable and used by a lot of these different tools",
    "start": "1719200",
    "end": "1726480"
  },
  {
    "text": "including the machine learning libraries the platform for folks aren't aware",
    "start": "1726480",
    "end": "1732980"
  },
  {
    "text": "at the very highest level we have application services these are really built to be able to consume by anyone",
    "start": "1732980",
    "end": "1738679"
  },
  {
    "text": "they're very very simple API so if I want to do a transcription of a of a of",
    "start": "1738679",
    "end": "1744410"
  },
  {
    "text": "a feed and I want to get the transcription of that I could just do an API call point it to my audio file and",
    "start": "1744410",
    "end": "1750650"
  },
  {
    "text": "get the transcription out these are powered by machine learning deep learning they're always getting better over time",
    "start": "1750650",
    "end": "1755900"
  },
  {
    "text": "and it's really really easy to empower my system with these intelligent capabilities and there's really a suite",
    "start": "1755900",
    "end": "1763280"
  },
  {
    "text": "of these and this number is constantly growing and then at the lower level we have things like sage maker this is",
    "start": "1763280",
    "end": "1769190"
  },
  {
    "text": "really where a lot the data scientists live being able to build their own machine learning models have a managed",
    "start": "1769190",
    "end": "1775400"
  },
  {
    "text": "platform such as the Jupiter environment to build those models plug into the catalog and be able to you know run",
    "start": "1775400",
    "end": "1781790"
  },
  {
    "text": "model and points that sort of thing so if we take a look at here you'll notice",
    "start": "1781790",
    "end": "1787370"
  },
  {
    "start": "1784000",
    "end": "1817000"
  },
  {
    "text": "we kind of color-coded you know the real time data is definitely the fastest when",
    "start": "1787370",
    "end": "1792620"
  },
  {
    "text": "we're looking at things like elasticsearch Athena presto spark those are all very very",
    "start": "1792620",
    "end": "1798110"
  },
  {
    "text": "fast if you're literally looking at things like hive and pig those were some",
    "start": "1798110",
    "end": "1803330"
  },
  {
    "text": "of the original analytical tools out of the ecosystem that got developed those tend to be a little bit slower they're",
    "start": "1803330",
    "end": "1808669"
  },
  {
    "text": "actually under the covers still running kind of old-school MapReduce to be able to run those analytics and then write",
    "start": "1808669",
    "end": "1814580"
  },
  {
    "text": "the results out so here's a couple of",
    "start": "1814580",
    "end": "1820330"
  },
  {
    "start": "1817000",
    "end": "1825000"
  },
  {
    "text": "slides here these are going to be on SlideShare so I'm going to skip over a couple of them and then talk a little",
    "start": "1820330",
    "end": "1827660"
  },
  {
    "start": "1825000",
    "end": "1914000"
  },
  {
    "text": "bit about ETL and then after this side we're gonna dive into the demonstration so so what why do we want to do ETL so",
    "start": "1827660",
    "end": "1835460"
  },
  {
    "text": "one of the best practices is really to consume the data as is coming in the raw form right because if you're",
    "start": "1835460",
    "end": "1841309"
  },
  {
    "text": "transforming the data and there's everything anything ever wrong it's really hard to recover from that along",
    "start": "1841309",
    "end": "1847040"
  },
  {
    "text": "with other reasons so what we find and this is a very very well-established pattern from you know folks like NASA",
    "start": "1847040",
    "end": "1854090"
  },
  {
    "text": "Tech and Dow Jones and FINRA and lots of customers doing this pattern but the data is coming in in its raw form what",
    "start": "1854090",
    "end": "1861620"
  },
  {
    "text": "that means is it's coming in and then often times you want to create carotid datasets or canonical datasets that",
    "start": "1861620",
    "end": "1867040"
  },
  {
    "text": "represent a normalized view of that data and that's where the ETL or the e-elt",
    "start": "1867040",
    "end": "1872930"
  },
  {
    "text": "process comes in and that normalized data set oftentimes is in a different",
    "start": "1872930",
    "end": "1878030"
  },
  {
    "text": "format so you might have different data sets coming in one JSON one CSV and your",
    "start": "1878030",
    "end": "1883250"
  },
  {
    "text": "output often times will be something like a column oriented format like RKO or RC or if you have constantly",
    "start": "1883250",
    "end": "1889100"
  },
  {
    "text": "changing schemas you might do something like Avro because it just handles changing so schemas a little bit better",
    "start": "1889100",
    "end": "1894190"
  },
  {
    "text": "and that's actually the corrida data sets that that get consumed even farther",
    "start": "1894190",
    "end": "1899690"
  },
  {
    "text": "downstream so to show that I think we'll go ahead and actually dive to a demonstration what we're going to do in",
    "start": "1899690",
    "end": "1906470"
  },
  {
    "text": "this demonstration is show some raw data that we've brought in and if we could go",
    "start": "1906470",
    "end": "1913520"
  },
  {
    "text": "ahead and switch over thank you so what we have here is we actually have",
    "start": "1913520",
    "end": "1919430"
  },
  {
    "start": "1914000",
    "end": "2377000"
  },
  {
    "text": "the glue data catalog and if we take a look here we're actually going to look at this database and you can't see me do",
    "start": "1919430",
    "end": "1928460"
  },
  {
    "text": "my air quotes this isn't a real database this is really a logical grouping of data sets and this is in my data Kellog",
    "start": "1928460",
    "end": "1935420"
  },
  {
    "text": "I'll make that bigger since this is a nice big room so this is actually a",
    "start": "1935420",
    "end": "1942050"
  },
  {
    "text": "logical grouping of data sets that I have I want to be able to analyze so if I actually take a look at that those",
    "start": "1942050",
    "end": "1948110"
  },
  {
    "text": "data sets here what you'll notice is some of these data sets are CSV and some of them are parkade and this is the",
    "start": "1948110",
    "end": "1955850"
  },
  {
    "text": "first data set we're going to start analyzing and this came in the raw form so this came from New York City Taxi",
    "start": "1955850",
    "end": "1961310"
  },
  {
    "text": "Commission it's two gigs of data that we're gonna start analyzing and it's gonna work really really well it's only",
    "start": "1961310",
    "end": "1967520"
  },
  {
    "text": "two gigs of data we're gonna we don't need a transform and it's going to be pretty performing we're gonna start analyzing it so to that what we could do",
    "start": "1967520",
    "end": "1975350"
  },
  {
    "text": "is we could actually go back over here and right now we're in the glue data catalog and we can actually go over here",
    "start": "1975350",
    "end": "1982970"
  },
  {
    "text": "and say I want to view the data and what this is doing this this is shifting to",
    "start": "1982970",
    "end": "1988460"
  },
  {
    "text": "Athena and you'll notice we're actually issuing select statements as if it's in",
    "start": "1988460",
    "end": "1994520"
  },
  {
    "text": "a relational database even though it's not the data still at rest on s3 the data",
    "start": "1994520",
    "end": "2004660"
  },
  {
    "text": "still at rest on s3 we start looking at the data and start querying the data here and I happen to be using Athena",
    "start": "2004660",
    "end": "2012100"
  },
  {
    "text": "here but you could actually use Hadoop you could plug this into the sage maker and start doing machine learning models",
    "start": "2012100",
    "end": "2017170"
  },
  {
    "text": "I could start doing a wide number of analytics on this data oh you know this",
    "start": "2017170",
    "end": "2022780"
  },
  {
    "text": "is actually doing just a select limit 10 that's you know kind of quite frankly pretty boring you'll notice they only",
    "start": "2022780",
    "end": "2028600"
  },
  {
    "text": "scanned 2 megabytes that's actually because I actually was limiting the",
    "start": "2028600",
    "end": "2033970"
  },
  {
    "text": "query by doing that limit 10 if I actually do a count here in the average trip distance this is one month of data",
    "start": "2033970",
    "end": "2039730"
  },
  {
    "text": "you'll notice it's about 140 million records 14 million records so we just",
    "start": "2039730",
    "end": "2045340"
  },
  {
    "text": "queried over 14 million records in four seconds pretty interactive right being able to start looking at this data",
    "start": "2045340",
    "end": "2050669"
  },
  {
    "text": "analyzing it see what are the outliers do some data cleaning that sort of thing",
    "start": "2050669",
    "end": "2055830"
  },
  {
    "text": "what what gets interesting is instead of one month so data what if I brought the",
    "start": "2055830",
    "end": "2062679"
  },
  {
    "text": "data set in and it instead I brought in about eight years of data instead so I'm",
    "start": "2062679",
    "end": "2069730"
  },
  {
    "text": "gonna actually query that data set next oops and this is going to take a little",
    "start": "2069730",
    "end": "2079240"
  },
  {
    "text": "bit more time so we're actually limiting it to 2016 and 15 in the query you'll notice but because the data is actually",
    "start": "2079240",
    "end": "2086648"
  },
  {
    "text": "structured here we just took the raw data we loaded in s3 we didn't do much with the data we just loaded it in and",
    "start": "2086649",
    "end": "2093398"
  },
  {
    "text": "we're starting to query it if we take a look at this data what it what it has is",
    "start": "2093399",
    "end": "2098800"
  },
  {
    "text": "each file is actually a month of data each file is about 2.4 gigs and what",
    "start": "2098800",
    "end": "2104350"
  },
  {
    "text": "this is doing is this is actually scanning over all this data it's not using like the file name you know doing",
    "start": "2104350",
    "end": "2110200"
  },
  {
    "text": "like a red X underscore and only looking for those years scanning over all the data finding those two years of data and",
    "start": "2110200",
    "end": "2117400"
  },
  {
    "text": "aggregating those results back to me and let's see if that was the right timing for stalling it was 28 seconds it took",
    "start": "2117400",
    "end": "2125470"
  },
  {
    "text": "to query that data and get a resolved back so you know it's it's doable and like if I needed to do a",
    "start": "2125470",
    "end": "2132280"
  },
  {
    "text": "couple queries and I'm I'm fine waiting 30 seconds but this if this is a system",
    "start": "2132280",
    "end": "2137829"
  },
  {
    "text": "empowering data scientist data engineers maybe an application you can't really be responding within 30 seconds here and",
    "start": "2137829",
    "end": "2144119"
  },
  {
    "text": "this is where we start going through that ETL process so can we structure the data in a better way so it's much easier",
    "start": "2144119",
    "end": "2150910"
  },
  {
    "text": "and more cost-effective than the query of this data you'll notice under the data scanned it says 207 gigs what that",
    "start": "2150910",
    "end": "2158559"
  },
  {
    "text": "means is it cost me about $1 to do this query just now to be able to scan over it because what's happening is it's",
    "start": "2158559",
    "end": "2164500"
  },
  {
    "text": "scanning over the entire data set if I would have done really any query here it's still gonna say 207 gigs because",
    "start": "2164500",
    "end": "2170200"
  },
  {
    "text": "it's always scanning over the entire data set what I could do though is I could actually structure the data a lot",
    "start": "2170200",
    "end": "2176349"
  },
  {
    "text": "better if I actually structured the data so that each prefix on my s3 location",
    "start": "2176349",
    "end": "2182440"
  },
  {
    "text": "was split out by year you'll notice I'm actually creating a slightly it's a very similar query I'm just acquiring a",
    "start": "2182440",
    "end": "2188710"
  },
  {
    "text": "slightly different table here and if I",
    "start": "2188710",
    "end": "2194970"
  },
  {
    "text": "type it in right if I actually now query this what's gonna happen is it's",
    "start": "2195299",
    "end": "2204640"
  },
  {
    "text": "actually gonna do partition elimination so at the way this data is actually stored on s3 now just just to show",
    "start": "2204640",
    "end": "2211119"
  },
  {
    "text": "everyone is if we actually go back over here now and look at the partition data",
    "start": "2211119",
    "end": "2216640"
  },
  {
    "text": "set and this data you'll notice that in s3 there's actually a prefix where it",
    "start": "2216640",
    "end": "2222040"
  },
  {
    "text": "says year is equal to two thousand nine years equal to two thousand ten and each of those prefixes or directories its",
    "start": "2222040",
    "end": "2227740"
  },
  {
    "text": "objects or not file system but directories when I do that query I don't need to be cognizant that the data",
    "start": "2227740",
    "end": "2233920"
  },
  {
    "text": "stored in here I just do my query saying years one of these two and the system's if it's Athena if it's you know retro",
    "start": "2233920",
    "end": "2240730"
  },
  {
    "text": "spectrum if it's EMR automatically knows how that data stored through the catalog and is able to query that data so over",
    "start": "2240730",
    "end": "2249250"
  },
  {
    "text": "here you'll notice this took fifteen seconds to run now so about half the time it's can nine gigabytes so that",
    "start": "2249250",
    "end": "2255880"
  },
  {
    "text": "cost me about four and a half cents so same query before cost me a dollar now cost me four and a half cents what I",
    "start": "2255880",
    "end": "2263740"
  },
  {
    "text": "could also do is like if I actually structured the data as parquet so parquet is our",
    "start": "2263740",
    "end": "2271150"
  },
  {
    "text": "column-oriented storage what that means is rather than storing rows of data together actually stores column blocks",
    "start": "2271150",
    "end": "2278140"
  },
  {
    "text": "together and the reason why that's more performant is a lot of these queries isn't actually aren't actually pulling a",
    "start": "2278140",
    "end": "2283810"
  },
  {
    "text": "single row out for like a database would if it's like a transactional database well it's doing is is aggregating over a",
    "start": "2283810",
    "end": "2290470"
  },
  {
    "text": "lot of different data sets it's doing group buys over different columns and it's doing a subset of the columns and",
    "start": "2290470",
    "end": "2295869"
  },
  {
    "text": "by storing it column-oriented you'll actually notice usually a very very order magnitude performance difference",
    "start": "2295869",
    "end": "2302500"
  },
  {
    "text": "here so if I actually run this query now you'll actually notice a couple things",
    "start": "2302500",
    "end": "2308380"
  },
  {
    "text": "is actually gonna return in four seconds now none of this is pre-staged it's all kind of live hitting us east you'll",
    "start": "2308380",
    "end": "2315250"
  },
  {
    "text": "notice it returned in four seconds the other thing you'll notice same exact data set just structured differently on",
    "start": "2315250",
    "end": "2321520"
  },
  {
    "text": "s3 it actually scanned over 349 megabytes so that literally cost me less",
    "start": "2321520",
    "end": "2327760"
  },
  {
    "text": "than a tenth about a tenth of a cent to just do so we just went from $1 to a",
    "start": "2327760",
    "end": "2333160"
  },
  {
    "text": "tenth of a cent just by structuring the data slightly different on s3 and that's why if we actually switch back to the",
    "start": "2333160",
    "end": "2339160"
  },
  {
    "text": "presentation now that's actually why we go through this ETL process and create",
    "start": "2339160",
    "end": "2345670"
  },
  {
    "text": "these corrida data set it not only increases the performance for all the analytics we happen to be using Athena",
    "start": "2345670",
    "end": "2351430"
  },
  {
    "text": "here to be able to consume that data but also decrease our cost and really makes the system more performant overall and",
    "start": "2351430",
    "end": "2357640"
  },
  {
    "text": "then all those old files that we have out there you know if this was if this is a system that was used in production",
    "start": "2357640",
    "end": "2364450"
  },
  {
    "text": "I could actually archive all those old CSV stew glaciar I have them if I ever need to rehydrate them and then all the",
    "start": "2364450",
    "end": "2370330"
  },
  {
    "text": "users would actually be querying that park a port partition data sets to be able to analyze their data we have more",
    "start": "2370330",
    "end": "2378670"
  },
  {
    "start": "2377000",
    "end": "2461000"
  },
  {
    "text": "cool demos I promise in 20 minutes we'll see if I get there so here's a some of",
    "start": "2378670",
    "end": "2385660"
  },
  {
    "text": "the different trade of which e-elt ETL tool to use in different scenarios",
    "start": "2385660",
    "end": "2390930"
  },
  {
    "text": "really glues fantastic if you have data like requirements if your query and",
    "start": "2390930",
    "end": "2397510"
  },
  {
    "text": "external data source moving large data sets we have data pipeline that many of our customers are",
    "start": "2397510",
    "end": "2403059"
  },
  {
    "text": "still using a lot of questions I get is get a lot of times should I use something like DMS with schema",
    "start": "2403059",
    "end": "2409240"
  },
  {
    "text": "conversion tool or should I use something like glue if I have a relational database and a lot of times that that that becomes a question on how",
    "start": "2409240",
    "end": "2416650"
  },
  {
    "text": "much data is getting transformed how much manipulation of the data has to",
    "start": "2416650",
    "end": "2422770"
  },
  {
    "text": "happen and also over time how you want to populate the destination so things",
    "start": "2422770",
    "end": "2428289"
  },
  {
    "text": "like glue are really fantastic you could take the data set you could create you know different fuse data sets and store",
    "start": "2428289",
    "end": "2433839"
  },
  {
    "text": "it back to a redshift or one of those data data repositories but if you need to do CDC change data capture things",
    "start": "2433839",
    "end": "2442089"
  },
  {
    "text": "like DMS or one of our partner products is really where those shine glue today doesn't actually do something like a",
    "start": "2442089",
    "end": "2448630"
  },
  {
    "text": "change detection you're issuing queries and you could do your own where clause but it's not doing the binary logs of",
    "start": "2448630",
    "end": "2454299"
  },
  {
    "text": "the change detection like a lot of the partner products and DMS is so going",
    "start": "2454299",
    "end": "2462460"
  },
  {
    "start": "2461000",
    "end": "2557000"
  },
  {
    "text": "going a little bit to the consumption model now you know I get asked sometimes",
    "start": "2462460",
    "end": "2469569"
  },
  {
    "text": "you know what type of bi or what type of UI should I use in this type of solution and what we find is really depending on",
    "start": "2469569",
    "end": "2478029"
  },
  {
    "text": "what the user is and what function that they want to perform oftentimes they'll use different tools so if you tried to",
    "start": "2478029",
    "end": "2484690"
  },
  {
    "text": "put a business user in front of a Jupiter notebook I'd hate to see what happens there and likewise if you",
    "start": "2484690",
    "end": "2490900"
  },
  {
    "text": "actually put a data scientist in front of like some whiz-bang dashboard and didn't give them access to the data that",
    "start": "2490900",
    "end": "2497049"
  },
  {
    "text": "would actually probably be just as bad so in these solutions you'll actually find a suite of different UI is being",
    "start": "2497049",
    "end": "2503589"
  },
  {
    "text": "used to be able to perform these analytics so Jupiter's use quite a bit",
    "start": "2503589",
    "end": "2508869"
  },
  {
    "text": "within the data science space there's really two common patterns that we have customers used Jupiter today sage maker",
    "start": "2508869",
    "end": "2516520"
  },
  {
    "text": "which we're going to demo here in a moment allows you to have a managed Jupiter environment running both Jupiter as well",
    "start": "2516520",
    "end": "2522640"
  },
  {
    "text": "as uber lab which we're going to show in a moment you can also run manage Jupiter or managed notebooks right now on EMR so",
    "start": "2522640",
    "end": "2530410"
  },
  {
    "text": "if you're really just looking at the Hadoop system we actually launched a feature rate within EMR or managed Hadoop",
    "start": "2530410",
    "end": "2537099"
  },
  {
    "text": "platform you could also run those notebooks of course there's like the out stack and cabana to be able to do log",
    "start": "2537099",
    "end": "2544750"
  },
  {
    "text": "analysis you know we have partner products like Splunk and tableau and MicroStrategy and sorry if there's any",
    "start": "2544750",
    "end": "2551650"
  },
  {
    "text": "partners in the room that I missed you but a wide number of different partner products to be able to perform that capability so for putting all this",
    "start": "2551650",
    "end": "2559750"
  },
  {
    "start": "2557000",
    "end": "2597000"
  },
  {
    "text": "together I know it's a chart but we kind",
    "start": "2559750",
    "end": "2565210"
  },
  {
    "text": "of have this data flow where you have the various data sources flowing in different repositories based on the",
    "start": "2565210",
    "end": "2570970"
  },
  {
    "text": "requirements that are needed to store that data when the key call also make",
    "start": "2570970",
    "end": "2576099"
  },
  {
    "text": "that wasn't on the previous diagram is that ETL process how can I take my data create it in the most consumed herbal",
    "start": "2576099",
    "end": "2582700"
  },
  {
    "text": "method for different types of users to be able to process and analyze that data and then ultimately getting it in you",
    "start": "2582700",
    "end": "2591339"
  },
  {
    "text": "know to drive insights to be able to answer business questions that sort of thing so we're gonna step through some",
    "start": "2591339",
    "end": "2599680"
  },
  {
    "text": "design patterns now and kind of shifting back to that data temperature and if we",
    "start": "2599680",
    "end": "2606040"
  },
  {
    "text": "take a look at that data temperature you know we actually have real-time data so over on the left hand side the things",
    "start": "2606040",
    "end": "2612069"
  },
  {
    "text": "like Kinesis we have databases transactional databases and all the way",
    "start": "2612069",
    "end": "2617500"
  },
  {
    "text": "to data lakes cold storage that sort of thing if we take a look at the streaming versus interactive versus batch what we",
    "start": "2617500",
    "end": "2624910"
  },
  {
    "text": "have is we actually have streaming analytics done off of a streaming data source I just like to show this because it's it's intuitive when you say it but",
    "start": "2624910",
    "end": "2631809"
  },
  {
    "text": "it's nice you know oftentimes it takes saying that where you have streaming analytics obviously off of a streaming",
    "start": "2631809",
    "end": "2637329"
  },
  {
    "text": "source you wouldn't necessarily do streaming and lakes off of s3 per se what what you have is you could actually",
    "start": "2637329",
    "end": "2644049"
  },
  {
    "text": "have interactive Analects off of law and these data sources if you have a streaming data source and you want to",
    "start": "2644049",
    "end": "2649900"
  },
  {
    "text": "really perform interactive analytics the most common practice is to actually take",
    "start": "2649900",
    "end": "2655210"
  },
  {
    "text": "it the Kinesis stream or take the stream pipe the data to s3 and then using the suite of different tools that you have",
    "start": "2655210",
    "end": "2661150"
  },
  {
    "text": "available to you to be able to process that data once it hits s3 and we'll talk about a couple customers",
    "start": "2661150",
    "end": "2666940"
  },
  {
    "text": "in a moment that are doing exactly that and then finally kind of the batch processing that might be using hive and",
    "start": "2666940",
    "end": "2673570"
  },
  {
    "text": "might be using AWS batch if that sort of technology so let's step through kind of",
    "start": "2673570",
    "end": "2680859"
  },
  {
    "start": "2678000",
    "end": "2755000"
  },
  {
    "text": "examples of all these so data streaming in very often you want to be able to run",
    "start": "2680859",
    "end": "2686500"
  },
  {
    "text": "real-time analytics on that data so you could use Kinesis data analytics you",
    "start": "2686500",
    "end": "2692140"
  },
  {
    "text": "could use the KCl of the lamp lambda EMR with SPARC streaming those sorts of",
    "start": "2692140",
    "end": "2697330"
  },
  {
    "text": "things from there you might want to be able to run real-time predictions on",
    "start": "2697330",
    "end": "2702550"
  },
  {
    "text": "that data maybe through like a sage maker endpoint sage maker endpoints allow you to build these models have an",
    "start": "2702550",
    "end": "2708490"
  },
  {
    "text": "endpoint that these services could call and then once you actually call that endpoint it that if it's outside certain",
    "start": "2708490",
    "end": "2714190"
  },
  {
    "text": "thresholds you could actually then alert based on certain those notifications",
    "start": "2714190",
    "end": "2720810"
  },
  {
    "text": "send all that data to s3 and then finally create these materialized views",
    "start": "2720810",
    "end": "2726190"
  },
  {
    "text": "or app States and this is an area where if you have a need to be able to create",
    "start": "2726190",
    "end": "2731650"
  },
  {
    "text": "kpi's performance indicators this app state is really one of the core kind of",
    "start": "2731650",
    "end": "2737410"
  },
  {
    "text": "architectural components I see a lot of customers use where it might be a caching like a Redis or memcache or",
    "start": "2737410",
    "end": "2743410"
  },
  {
    "text": "might be dynamo will actually show an example of a customer using dynamo DB with API gateway with a real-time flow",
    "start": "2743410",
    "end": "2750580"
  },
  {
    "text": "here in a moment actually will do that next year so the first of two examples",
    "start": "2750580",
    "end": "2757690"
  },
  {
    "start": "2755000",
    "end": "2824000"
  },
  {
    "text": "that we're going to run through these are real customer examples Ashurst there are a media and information company they",
    "start": "2757690",
    "end": "2763599"
  },
  {
    "text": "have a wide number of different channels they have magazines newspapers things like ESPN ane lots of different channels",
    "start": "2763599",
    "end": "2770740"
  },
  {
    "text": "out there under the Hearst umbrella and this is actually a real flow that they have so they have various information",
    "start": "2770740",
    "end": "2777280"
  },
  {
    "text": "coming from their web sites going through Kinesis data streams once it hits data streams then since it's a",
    "start": "2777280",
    "end": "2783369"
  },
  {
    "text": "stream you could have multiple consumers write one of those consumers could store my data out back into a data Lake or a",
    "start": "2783369",
    "end": "2789130"
  },
  {
    "text": "data repository and another consumer could actually build a real-time dashboard or build a real-time analysis",
    "start": "2789130",
    "end": "2795460"
  },
  {
    "text": "of that data and this is exactly what they're doing so the data is actually flowing to a firehose which is storing the data",
    "start": "2795460",
    "end": "2801010"
  },
  {
    "text": "the s3 which then you could really use any of these tools on and then another consumer is going through a lambda",
    "start": "2801010",
    "end": "2807250"
  },
  {
    "text": "pipeline hitting DynamoDB to be able to store that app state or store that analysis state which is exposed through",
    "start": "2807250",
    "end": "2814930"
  },
  {
    "text": "an API gateway which allows you to have a restful endpoint defined through swagger to be able to get get the",
    "start": "2814930",
    "end": "2822370"
  },
  {
    "text": "current state of that data another example that we're going to step through is actually yield mo this is",
    "start": "2822370",
    "end": "2828460"
  },
  {
    "start": "2824000",
    "end": "2875000"
  },
  {
    "text": "there have thousands of devices out there that mobile devices that is",
    "start": "2828460",
    "end": "2833860"
  },
  {
    "text": "getting sent through a data stream what they're running is data and Alixe now so",
    "start": "2833860",
    "end": "2838900"
  },
  {
    "text": "in the first example through hearse they're using lambda what that means is their developers wrote function codes",
    "start": "2838900",
    "end": "2845100"
  },
  {
    "text": "little functions to be able to process the data the real-time data and then store the data into a dynamo table in",
    "start": "2845100",
    "end": "2852070"
  },
  {
    "text": "this example what what this customer is doing is actually writing sequel now to be able to run the same sort of",
    "start": "2852070",
    "end": "2857440"
  },
  {
    "text": "processing on the real-time feeds that they have and then do aggregations do filtering on that data to then store it",
    "start": "2857440",
    "end": "2864010"
  },
  {
    "text": "out into their data warehouse through a firehose and s3 channel there so I just",
    "start": "2864010",
    "end": "2869680"
  },
  {
    "text": "like to show kind of two real examples as we're stepping through these these architectures here through the batch",
    "start": "2869680",
    "end": "2876880"
  },
  {
    "text": "analytics and interactive analytics you still might have streaming data coming in so just because I have clickstream",
    "start": "2876880",
    "end": "2882130"
  },
  {
    "text": "data I might want to start analyzing the data interactively on what certain users are doing what different profiles of",
    "start": "2882130",
    "end": "2888250"
  },
  {
    "text": "users are doing that sort of thing I also might had other data sources",
    "start": "2888250",
    "end": "2893260"
  },
  {
    "text": "coming in s3 on a wide variety of methods it might be through snowball and might through s3 accelerate transfer",
    "start": "2893260",
    "end": "2899400"
  },
  {
    "text": "different data sets coming in at s3 I might want to integrate different data sets from organizations that might be",
    "start": "2899400",
    "end": "2906460"
  },
  {
    "text": "within my company or different companies we have a lot of data sharing happening on s3 through things like open data",
    "start": "2906460",
    "end": "2913090"
  },
  {
    "text": "program a public data set so there's Landsat data out there there's genomics data lots of different data sets being",
    "start": "2913090",
    "end": "2919420"
  },
  {
    "text": "shared through s3 and then analyzed downstream by a wide number of different companies obviously you can completely",
    "start": "2919420",
    "end": "2926740"
  },
  {
    "text": "lock down the data with an s3 as well so as soon as the data is coming in",
    "start": "2926740",
    "end": "2932650"
  },
  {
    "text": "the streaming data could actually get flow flown flows through s3 but also",
    "start": "2932650",
    "end": "2938589"
  },
  {
    "text": "could get load into a redshift or elasticsearch but also for the processing you use Athena you could use",
    "start": "2938589",
    "end": "2944980"
  },
  {
    "text": "which we actually just demonstrated you could use EMR to be able to do that interactive processing for the batch",
    "start": "2944980",
    "end": "2953319"
  },
  {
    "text": "processing we actually see SPARC used quite a bit now first batch processing it actually used to be hive and pig",
    "start": "2953319",
    "end": "2959289"
  },
  {
    "text": "where the predominant language is used a lot of times we're seeing more and more",
    "start": "2959289",
    "end": "2964750"
  },
  {
    "text": "SPARC used for the ETL and the batch processing as well and that really kind of is tied together through the machine",
    "start": "2964750",
    "end": "2971380"
  },
  {
    "text": "learning and that is what makes up that interactive layer as well as the batch layer that we see to show an example",
    "start": "2971380",
    "end": "2978430"
  },
  {
    "text": "here FINRA they ingest sometimes about seventy five billion events every day",
    "start": "2978430",
    "end": "2984190"
  },
  {
    "text": "what they're doing is they're actually taking stock information financial trade information and once the what they are",
    "start": "2984190",
    "end": "2991390"
  },
  {
    "text": "really looking at doing is being able to find things like market manipulators or other anomalous behavior on the stock",
    "start": "2991390",
    "end": "2998319"
  },
  {
    "text": "markets so what they have is they have all this data flowing into s3 they actually have that canonical data issue",
    "start": "2998319",
    "end": "3005309"
  },
  {
    "text": "so they actually have different brokerage firms reporting data in different formats it goes in the raw",
    "start": "3005309",
    "end": "3010529"
  },
  {
    "text": "form it gets into a canonical form and then they're using a wide number of different EMR clusters as well as",
    "start": "3010529",
    "end": "3015660"
  },
  {
    "text": "redshift clusters to be able to process this data and the reason I want to show this is just showing the flexibility of",
    "start": "3015660",
    "end": "3022170"
  },
  {
    "text": "as you're architecting your solution storing the data in a format that is",
    "start": "3022170",
    "end": "3027270"
  },
  {
    "text": "easily consumed by not only multiple types of the same service like multiple EMR clusters but also consumed by your",
    "start": "3027270",
    "end": "3034619"
  },
  {
    "text": "data warehouse by your data like analytics by your machine learnings that sort of thing so if we take a look at",
    "start": "3034619",
    "end": "3043349"
  },
  {
    "start": "3041000",
    "end": "3088000"
  },
  {
    "text": "the data like architectures and there's some really good sessions on data leaks throughout the conference here so definitely recommend those sessions if",
    "start": "3043349",
    "end": "3050609"
  },
  {
    "text": "you're looking at building a data like but really it's tying in the real time processing the interactive and batch",
    "start": "3050609",
    "end": "3057089"
  },
  {
    "text": "processing you'll notice s3 is really the center of the lake here but another",
    "start": "3057089",
    "end": "3062339"
  },
  {
    "text": "really important aspect of the data Lake is really metadata management sometimes when I talk about data leaks",
    "start": "3062339",
    "end": "3068280"
  },
  {
    "text": "to customers when they ask ask them you know what are you doing for metadata for the governance for the Providence those",
    "start": "3068280",
    "end": "3074340"
  },
  {
    "text": "sorts of things it's really good to have answers for that so the data in s3 is a",
    "start": "3074340",
    "end": "3079800"
  },
  {
    "text": "really really good start but also having a common data language a common metod in a store so you can use all these",
    "start": "3079800",
    "end": "3086160"
  },
  {
    "text": "different tools to be able to process your data so I think next what we'll do is we're actually going to shift over to",
    "start": "3086160",
    "end": "3092340"
  },
  {
    "text": "another demonstration here showing the flexibility of how to use these different tools we're actually going to",
    "start": "3092340",
    "end": "3099300"
  },
  {
    "text": "be leveraging the same data set that we were just querying so if we could go",
    "start": "3099300",
    "end": "3105150"
  },
  {
    "text": "ahead and shift over thank you to the laptop what we're going to show here is this data set that we're just querying",
    "start": "3105150",
    "end": "3111930"
  },
  {
    "text": "over what if I actually wanted to take this data set and do things like extend",
    "start": "3111930",
    "end": "3117750"
  },
  {
    "text": "my data warehouse into this data set start doing machine learning in this data set to show the flexibility right",
    "start": "3117750",
    "end": "3123090"
  },
  {
    "text": "show the future-proofing I started with a very small use case analyzing the data but I want to add on top of this so what",
    "start": "3123090",
    "end": "3129750"
  },
  {
    "text": "we're going to do here is we're actually going to launch into this redshift cluster so I have this redshift cluster",
    "start": "3129750",
    "end": "3136170"
  },
  {
    "text": "currently running here and what we're going to show here is I'm gonna actually",
    "start": "3136170",
    "end": "3142020"
  },
  {
    "text": "connect into the redshift cluster and",
    "start": "3142020",
    "end": "3147500"
  },
  {
    "text": "show you guys the screen here so I actually just connect it into this",
    "start": "3147500",
    "end": "3153480"
  },
  {
    "text": "cluster and I'll make it a lot bigger for folks I happen to be using piece equal here but really I could be using a",
    "start": "3153480",
    "end": "3160770"
  },
  {
    "text": "bi tool or another type of sequel tool DB visualizer of C core workbench that sort of thing and what you'll notice",
    "start": "3160770",
    "end": "3166770"
  },
  {
    "text": "here in my data in my data warehouse is I have my tables here but I also have no",
    "start": "3166770",
    "end": "3173970"
  },
  {
    "text": "other schemas defined so I just have my one table defined here what I'm gonna do",
    "start": "3173970",
    "end": "3178980"
  },
  {
    "text": "here is I'm actually going to define so this is leveraging redshift spectrum now so at first we were demoing with Athena",
    "start": "3178980",
    "end": "3184730"
  },
  {
    "text": "now we're actually doing some stuff with retro spectrum and I'm actually going create this schema if we actually take a",
    "start": "3184730",
    "end": "3191490"
  },
  {
    "text": "look now we actually see the schema that we just created and the nice thing about this now is iock I could actually query",
    "start": "3191490",
    "end": "3198630"
  },
  {
    "text": "the same data that I was just queering so if we actually take a look at these data sets you know it probably",
    "start": "3198630",
    "end": "3203900"
  },
  {
    "text": "looks from there we're just clearing the the yellow single yellow yellow partition I could actually take this",
    "start": "3203900",
    "end": "3208970"
  },
  {
    "text": "data and use my bi tools to start querying this data not only that I could also fuse the data with data that's",
    "start": "3208970",
    "end": "3214970"
  },
  {
    "text": "already in the data warehouse with data that's in the data like using retro spectrum so some really good patterns",
    "start": "3214970",
    "end": "3220700"
  },
  {
    "text": "there to be able to start extending that data across the different lakes and the warehouses but what we could do is",
    "start": "3220700",
    "end": "3228350"
  },
  {
    "start": "3226000",
    "end": "3290000"
  },
  {
    "text": "taking the same exact data set we could also have this EMR cluster so I actually",
    "start": "3228350",
    "end": "3233870"
  },
  {
    "text": "launched just an EMR cluster I launched it with Hughes Epling I have different",
    "start": "3233870",
    "end": "3239390"
  },
  {
    "text": "web interfaces into this so I could use this cluster I could run my jobs hit that same data repository the reason I",
    "start": "3239390",
    "end": "3246260"
  },
  {
    "text": "know that is I could actually go over to the configurations of this cluster and you'll notice a couple things one is I",
    "start": "3246260",
    "end": "3253250"
  },
  {
    "text": "actually told it to optimize it for spark this is really good to do if you're using EMR and you're just using",
    "start": "3253250",
    "end": "3258410"
  },
  {
    "text": "spark definitely set this configuration it sets the the properties for spark to use the full memory and the different",
    "start": "3258410",
    "end": "3265610"
  },
  {
    "text": "settings out of yarn but it you'll also notice it's using the glue data catalog here for things like hive and spark I've",
    "start": "3265610",
    "end": "3274450"
  },
  {
    "text": "but what I could do now is I'm actually instead of running things directly on that cluster I'm actually gonna launch",
    "start": "3274450",
    "end": "3281270"
  },
  {
    "text": "into sage maker now and we're going to go into the sage maker and we're going",
    "start": "3281270",
    "end": "3291260"
  },
  {
    "start": "3290000",
    "end": "3437000"
  },
  {
    "text": "to go into a Jupiter notebook and within this notebook what we're going to do is we're actually going to start querying",
    "start": "3291260",
    "end": "3297050"
  },
  {
    "text": "that same exact data sets that we had so because it's in the open format it's in",
    "start": "3297050",
    "end": "3302120"
  },
  {
    "text": "csv park' these different formats i could actually go in here and start analyzing this data so what I'm going to",
    "start": "3302120",
    "end": "3309710"
  },
  {
    "text": "show here first is is you know within here I can actually reacts acute these",
    "start": "3309710",
    "end": "3316040"
  },
  {
    "text": "cells you know I could actually query this dataset you'll notice this is actually the same data catalog so that",
    "start": "3316040",
    "end": "3321530"
  },
  {
    "text": "metadata management is really really key on this how am i cataloguing my data so",
    "start": "3321530",
    "end": "3326600"
  },
  {
    "text": "in here I can actually query my different data sets I could actually go in here and start bringing this into a",
    "start": "3326600",
    "end": "3332780"
  },
  {
    "text": "panda's data frame now so I actually just through the EMR cluster through spark I queried",
    "start": "3332780",
    "end": "3338980"
  },
  {
    "text": "earlier I actually queried all the taxi data and then through the lipping",
    "start": "3338980",
    "end": "3344690"
  },
  {
    "text": "integration I actually brought it into a Penta data frame and over here I could actually do all types of different pandas things within the spark info or",
    "start": "3344690",
    "end": "3351890"
  },
  {
    "text": "within the sage maker environment now are really the Jupiter environment that sage maker is providing not only that is",
    "start": "3351890",
    "end": "3359319"
  },
  {
    "text": "if I actually want to build my own model maybe I want to build a XG boost's model",
    "start": "3359319",
    "end": "3364819"
  },
  {
    "text": "to be able to you know do regression or",
    "start": "3364819",
    "end": "3370130"
  },
  {
    "text": "something like that here what I could do here is through the Jupiter environment",
    "start": "3370130",
    "end": "3375470"
  },
  {
    "text": "I can actually trigger this this job to run you'll notice I'm actually specifying different node types so",
    "start": "3375470",
    "end": "3381500"
  },
  {
    "text": "definitely if you're looking at doing machine learning attend some sessions on sage maker lets you run these Jupiter",
    "start": "3381500",
    "end": "3387200"
  },
  {
    "text": "environments that have very low footprint and then spin up these training jobs that could be very very",
    "start": "3387200",
    "end": "3392329"
  },
  {
    "text": "large and auto scale up and down or really scale up based on your definition",
    "start": "3392329",
    "end": "3397539"
  },
  {
    "text": "so what I just did those through that sage maker Jupiter environment hitting",
    "start": "3397539",
    "end": "3402829"
  },
  {
    "text": "that livvie server I actually just triggered the sage maker job so if I actually go over the",
    "start": "3402829",
    "end": "3407960"
  },
  {
    "text": "training jobs now what we'll see is we actually just triggered this training job all through the integration of the",
    "start": "3407960",
    "end": "3414380"
  },
  {
    "text": "platform and we happen to be using AWS products now you know I could be using other partner products on some of these",
    "start": "3414380",
    "end": "3421099"
  },
  {
    "text": "I could have actually shown a tableau or jaspersoft MicroStrategy one of the BI tools in front of the redshifts rather",
    "start": "3421099",
    "end": "3427220"
  },
  {
    "text": "than the P sequel so there's a lot of nice integration through their partner ecosystem as well so if we can actually",
    "start": "3427220",
    "end": "3434420"
  },
  {
    "text": "switch back to the presentation we're gonna wrap up here with like two minutes",
    "start": "3434420",
    "end": "3441500"
  },
  {
    "start": "3437000",
    "end": "3530000"
  },
  {
    "text": "to spare it's kind of how we roll so just to wrap up you know as you're",
    "start": "3441500",
    "end": "3447109"
  },
  {
    "text": "building these systems from building that decoupled architecture the way I'm adjusting data or way customers are",
    "start": "3447109",
    "end": "3452839"
  },
  {
    "text": "ingesting data versus storing it versus analyzing it really good to decouple all those different steps so that you can",
    "start": "3452839",
    "end": "3459289"
  },
  {
    "text": "use these different tools you can swap out the best tool for the best job also",
    "start": "3459289",
    "end": "3465140"
  },
  {
    "text": "leveraging managed services service Shen's really lets you focus on what matters and I will highlight the",
    "start": "3465140",
    "end": "3472800"
  },
  {
    "text": "cost-conscious aspect if you're actually pricing out architecture and it becomes very very large costs usually the way we",
    "start": "3472800",
    "end": "3481540"
  },
  {
    "text": "price our services are based on the way that that service is intended to be used so you know for example if I actually",
    "start": "3481540",
    "end": "3488530"
  },
  {
    "text": "try to write out a lot of really small files to s3 and I cost that out the put cost for us three actually go very very",
    "start": "3488530",
    "end": "3494950"
  },
  {
    "text": "very high rather than using something like a fire hose or like a dynamo DB which is meant for that very very high",
    "start": "3494950",
    "end": "3500470"
  },
  {
    "text": "throughput low payload right right type",
    "start": "3500470",
    "end": "3506290"
  },
  {
    "text": "of architecture so just go through that price in our exercise get involved with your ABI be at AWS teams to help you",
    "start": "3506290",
    "end": "3513340"
  },
  {
    "text": "with that exercise and that really helps validate that architecture so definitely I want to thank everyone for their time",
    "start": "3513340",
    "end": "3519220"
  },
  {
    "text": "please remember to fill out your reviews and you know hope everyone enjoys the rest of the day in the conference",
    "start": "3519220",
    "end": "3526060"
  },
  {
    "text": "[Applause]",
    "start": "3526060",
    "end": "3532650"
  }
]