[
  {
    "text": "one of the things we hear from customers",
    "start": "30",
    "end": "1079"
  },
  {
    "text": "is how can we make our own algorithms",
    "start": "1079",
    "end": "3570"
  },
  {
    "text": "algorithms that our data scientists and",
    "start": "3570",
    "end": "5130"
  },
  {
    "text": "developers are building for our own",
    "start": "5130",
    "end": "6390"
  },
  {
    "text": "business as fast as the ones that are",
    "start": "6390",
    "end": "8550"
  },
  {
    "text": "inside stage maker and the truth is",
    "start": "8550",
    "end": "10800"
  },
  {
    "text": "today this is kind of cumbersome you can",
    "start": "10800",
    "end": "12870"
  },
  {
    "text": "do it but it's very very difficult to",
    "start": "12870",
    "end": "15330"
  },
  {
    "text": "get right it's very hard to get the",
    "start": "15330",
    "end": "17190"
  },
  {
    "text": "performance and it involves getting",
    "start": "17190",
    "end": "18359"
  },
  {
    "text": "really into the weeds on a whole bunch",
    "start": "18359",
    "end": "19770"
  },
  {
    "text": "of low level low lying C++ code nobody's",
    "start": "19770",
    "end": "23939"
  },
  {
    "text": "got time for that and so today we're",
    "start": "23939",
    "end": "26070"
  },
  {
    "text": "introducing a new feature which we call",
    "start": "26070",
    "end": "27750"
  },
  {
    "text": "Amazon sage maker streaming algorithms",
    "start": "27750",
    "end": "31200"
  },
  {
    "text": "[Music]",
    "start": "31200",
    "end": "34289"
  },
  {
    "text": "streaming algorithms allow you to",
    "start": "34850",
    "end": "36989"
  },
  {
    "text": "accelerate your own algorithms by taking",
    "start": "36989",
    "end": "39390"
  },
  {
    "text": "the same approach we use for the sage",
    "start": "39390",
    "end": "41010"
  },
  {
    "text": "maker algorithms and stream large",
    "start": "41010",
    "end": "42809"
  },
  {
    "text": "volumes of training data from Amazon s3",
    "start": "42809",
    "end": "44850"
  },
  {
    "text": "through your model this is great because",
    "start": "44850",
    "end": "48149"
  },
  {
    "text": "it allows you to stream it into your own",
    "start": "48149",
    "end": "49620"
  },
  {
    "text": "algorithm it dramatically reduces by up",
    "start": "49620",
    "end": "53370"
  },
  {
    "text": "to 90% the amount of time it takes for",
    "start": "53370",
    "end": "55469"
  },
  {
    "text": "your training to start because you don't",
    "start": "55469",
    "end": "56730"
  },
  {
    "text": "have to do that copy step up front it",
    "start": "56730",
    "end": "58920"
  },
  {
    "text": "also means you can just drive more data",
    "start": "58920",
    "end": "61170"
  },
  {
    "text": "onto your GPUs and your CPUs to drive",
    "start": "61170",
    "end": "63600"
  },
  {
    "text": "your training that means they operate",
    "start": "63600",
    "end": "65128"
  },
  {
    "text": "more efficiently and it can cut down the",
    "start": "65129",
    "end": "66810"
  },
  {
    "text": "time that it takes to run your training",
    "start": "66810",
    "end": "68700"
  },
  {
    "text": "models and because on AWS everything is",
    "start": "68700",
    "end": "71220"
  },
  {
    "text": "metered and only pay-as-you-go every",
    "start": "71220",
    "end": "73650"
  },
  {
    "text": "second counts so if you can reduce the",
    "start": "73650",
    "end": "75720"
  },
  {
    "text": "training time you can dramatically",
    "start": "75720",
    "end": "77040"
  },
  {
    "text": "reduce the training costs so we're",
    "start": "77040",
    "end": "79229"
  },
  {
    "text": "bringing this today to tensorflow and",
    "start": "79229",
    "end": "81210"
  },
  {
    "text": "we'll be your prying it to additional",
    "start": "81210",
    "end": "82860"
  },
  {
    "text": "frameworks coming soon so once you have",
    "start": "82860",
    "end": "86340"
  },
  {
    "text": "trained your model add another step in",
    "start": "86340",
    "end": "89340"
  },
  {
    "text": "machine learning is the optimization",
    "start": "89340",
    "end": "90840"
  },
  {
    "text": "step and again just from the end of an",
    "start": "90840",
    "end": "93689"
  },
  {
    "text": "API you can run high powered machine",
    "start": "93689",
    "end": "96360"
  },
  {
    "text": "learning driven parameter optimization",
    "start": "96360",
    "end": "97920"
  },
  {
    "text": "which continually searches for the best",
    "start": "97920",
    "end": "100470"
  },
  {
    "text": "possible model for your training data",
    "start": "100470",
    "end": "102869"
  },
  {
    "text": "and once you've optimized most customers",
    "start": "102869",
    "end": "105090"
  },
  {
    "text": "want to be able to host their model they",
    "start": "105090",
    "end": "106890"
  },
  {
    "text": "wanna be able to put it into production",
    "start": "106890",
    "end": "107820"
  },
  {
    "text": "and so we provide a B testing for those",
    "start": "107820",
    "end": "110310"
  },
  {
    "text": "algorithms so you can deploy them safely",
    "start": "110310",
    "end": "112020"
  },
  {
    "text": "and we provide robust elastic hosting",
    "start": "112020",
    "end": "114899"
  },
  {
    "text": "for those models right inside stage",
    "start": "114899",
    "end": "116579"
  },
  {
    "text": "maker now the elastic hosting is as you",
    "start": "116579",
    "end": "120240"
  },
  {
    "text": "would expect very easy to integrate",
    "start": "120240",
    "end": "122640"
  },
  {
    "text": "we take your model we deploy it onto a",
    "start": "122640",
    "end": "124979"
  },
  {
    "text": "fully managed infrastructure and we give",
    "start": "124979",
    "end": "127020"
  },
  {
    "text": "you an easy to integrate API endpoint",
    "start": "127020",
    "end": "128940"
  },
  {
    "text": "then you can drive low latency real time",
    "start": "128940",
    "end": "131700"
  },
  {
    "text": "predictions using that model",
    "start": "131700",
    "end": "133470"
  },
  {
    "text": "under the hood will autoscale to make",
    "start": "133470",
    "end": "135450"
  },
  {
    "text": "sure that we're providing the right",
    "start": "135450",
    "end": "136680"
  },
  {
    "text": "level of capacity and we'll do that",
    "start": "136680",
    "end": "138480"
  },
  {
    "text": "across multiple aziz to provide fault",
    "start": "138480",
    "end": "140490"
  },
  {
    "text": "tolerance to the model so this is",
    "start": "140490",
    "end": "142770"
  },
  {
    "text": "perfect for real-time low latency",
    "start": "142770",
    "end": "145050"
  },
  {
    "text": "predictions but there are some",
    "start": "145050",
    "end": "147060"
  },
  {
    "text": "situations where customers want to be",
    "start": "147060",
    "end": "148560"
  },
  {
    "text": "able to process a whole bunch of data",
    "start": "148560",
    "end": "150420"
  },
  {
    "text": "just all at once upfront in batch or",
    "start": "150420",
    "end": "153060"
  },
  {
    "text": "where they have very very large files",
    "start": "153060",
    "end": "155250"
  },
  {
    "text": "such as those medical imaging files that",
    "start": "155250",
    "end": "157020"
  },
  {
    "text": "I was talking about before or 5k video",
    "start": "157020",
    "end": "159210"
  },
  {
    "text": "where they want to be able to process a",
    "start": "159210",
    "end": "161100"
  },
  {
    "text": "big file all at once",
    "start": "161100",
    "end": "162450"
  },
  {
    "text": "so today what we're well set up for real",
    "start": "162450",
    "end": "165570"
  },
  {
    "text": "time customers want to be able to have",
    "start": "165570",
    "end": "167490"
  },
  {
    "text": "more flexibility to Train batch and",
    "start": "167490",
    "end": "169740"
  },
  {
    "text": "large files and so today we're",
    "start": "169740",
    "end": "171600"
  },
  {
    "text": "introducing a new feature of sage maker",
    "start": "171600",
    "end": "173040"
  },
  {
    "text": "which we call batch transform sage make",
    "start": "173040",
    "end": "180780"
  },
  {
    "text": "a batch transform provides fully managed",
    "start": "180780",
    "end": "183120"
  },
  {
    "text": "high-throughput batch transform jobs",
    "start": "183120",
    "end": "185130"
  },
  {
    "text": "with a simple API call so this means",
    "start": "185130",
    "end": "187980"
  },
  {
    "text": "that you can process data dumps in a",
    "start": "187980",
    "end": "189959"
  },
  {
    "text": "batch if you have a new set of billing",
    "start": "189959",
    "end": "192240"
  },
  {
    "text": "inventory if you have a whole bunch of",
    "start": "192240",
    "end": "193740"
  },
  {
    "text": "product sales you can just take that at",
    "start": "193740",
    "end": "195600"
  },
  {
    "text": "the end of the day and just throw it",
    "start": "195600",
    "end": "196950"
  },
  {
    "text": "against your model and start doing your",
    "start": "196950",
    "end": "198480"
  },
  {
    "text": "predictions",
    "start": "198480",
    "end": "199110"
  },
  {
    "text": "you can also process large files five",
    "start": "199110",
    "end": "202080"
  },
  {
    "text": "gigabytes and up much more easily",
    "start": "202080",
    "end": "203610"
  },
  {
    "text": "without having to chunk them into pieces",
    "start": "203610",
    "end": "204930"
  },
  {
    "text": "and what a lot of customers are doing",
    "start": "204930",
    "end": "206880"
  },
  {
    "text": "are using this approach to test their",
    "start": "206880",
    "end": "208830"
  },
  {
    "text": "models so once they've trained a model",
    "start": "208830",
    "end": "210600"
  },
  {
    "text": "they have a test set which is well",
    "start": "210600",
    "end": "212010"
  },
  {
    "text": "understood and they just run through",
    "start": "212010",
    "end": "213510"
  },
  {
    "text": "that as a sanity check to make sure it",
    "start": "213510",
    "end": "215190"
  },
  {
    "text": "works before they start rolling in into",
    "start": "215190",
    "end": "216959"
  },
  {
    "text": "production through a/b tests and that",
    "start": "216959",
    "end": "219330"
  },
  {
    "text": "works if you want to run it in the cloud",
    "start": "219330",
    "end": "220800"
  },
  {
    "text": "or if you're deploying those models say",
    "start": "220800",
    "end": "223290"
  },
  {
    "text": "to an MRI machine which sits in a",
    "start": "223290",
    "end": "224820"
  },
  {
    "text": "hospital to run at the edge so the joy",
    "start": "224820",
    "end": "227940"
  },
  {
    "text": "here is that you can use the exact same",
    "start": "227940",
    "end": "229890"
  },
  {
    "text": "models to your training in Sage Maker",
    "start": "229890",
    "end": "232019"
  },
  {
    "text": "today for real-time predictions and run",
    "start": "232019",
    "end": "234690"
  },
  {
    "text": "the exact same models for batch",
    "start": "234690",
    "end": "236070"
  },
  {
    "text": "predictions and you can take all of the",
    "start": "236070",
    "end": "238650"
  },
  {
    "text": "pre-processing and post-processing steps",
    "start": "238650",
    "end": "240510"
  },
  {
    "text": "and bring those into your batch",
    "start": "240510",
    "end": "242549"
  },
  {
    "text": "processing workloads as well and again",
    "start": "242549",
    "end": "244260"
  },
  {
    "text": "this is all fully managed if you've ever",
    "start": "244260",
    "end": "246750"
  },
  {
    "text": "run a training job in sage maker this is",
    "start": "246750",
    "end": "248700"
  },
  {
    "text": "virtually the same API and so through",
    "start": "248700",
    "end": "251489"
  },
  {
    "text": "all these capabilities the training the",
    "start": "251489",
    "end": "254130"
  },
  {
    "text": "optimization the hosting we see a",
    "start": "254130",
    "end": "256109"
  },
  {
    "text": "dramatic decrease in time that it takes",
    "start": "256109",
    "end": "258720"
  },
  {
    "text": "customers to take their machine learning",
    "start": "258720",
    "end": "260459"
  },
  {
    "text": "models train them up and put them into",
    "start": "260459",
    "end": "262260"
  },
  {
    "text": "production Intuit are running close to",
    "start": "262260",
    "end": "264630"
  },
  {
    "text": "real-time fraud analytic",
    "start": "264630",
    "end": "265920"
  },
  {
    "text": "and driven by machine learning on their",
    "start": "265920",
    "end": "267840"
  },
  {
    "text": "platform routinely and they were able to",
    "start": "267840",
    "end": "269970"
  },
  {
    "text": "reduce their deployment time down by 90",
    "start": "269970",
    "end": "272100"
  },
  {
    "text": "percent using sage maker they used to",
    "start": "272100",
    "end": "274350"
  },
  {
    "text": "take them six months to build a model",
    "start": "274350",
    "end": "276030"
  },
  {
    "text": "and get the first version into",
    "start": "276030",
    "end": "277560"
  },
  {
    "text": "production now they can do it routinely",
    "start": "277560",
    "end": "279300"
  },
  {
    "text": "in less than a week",
    "start": "279300",
    "end": "282440"
  }
]