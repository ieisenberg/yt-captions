[
  {
    "text": "- Hi. Hey, folks. My name's Emily Webber. I'm a machine learning\nspecialist solutions architect",
    "start": "1050",
    "end": "6720"
  },
  {
    "text": "at Amazon Web Services. And today we are gonna learn how to pretrain foundation\nmodels on AWS, my friends.",
    "start": "6720",
    "end": "13950"
  },
  {
    "text": "Yes, that's right. In this session, you're gonna learn just the first couple steps about how to create your own\nfoundation models on AWS.",
    "start": "13950",
    "end": "22140"
  },
  {
    "text": "So you can imagine, obviously\nthis is a complex topic. So this is the start of our\nlevel 400 in this class.",
    "start": "22140",
    "end": "30150"
  },
  {
    "text": "The first couple of YouTube\nvideos we're at a level 300, arguably 200 maybe in the first ones,",
    "start": "30150",
    "end": "36180"
  },
  {
    "text": "and that was to get you introduced to some of the easier topics,\nsome of the higher-level,",
    "start": "36180",
    "end": "41789"
  },
  {
    "text": "you know, more simple topics that are related to some\nof the larger trends.",
    "start": "41790",
    "end": "48660"
  },
  {
    "text": "And right now, we're\ngonna super dive deep. We're gonna learn about\npretraining foundation models.",
    "start": "48660",
    "end": "54780"
  },
  {
    "text": "And in this class,\nwe're mostly gonna learn about how to pretrain foundation models on AWS using SageMaker",
    "start": "54780",
    "end": "61737"
  },
  {
    "text": "and we're gonna learn about distributed training on SageMaker. And then in the next YouTube video,",
    "start": "61737",
    "end": "67050"
  },
  {
    "text": "we are gonna learn how to prepare datasets and then how to literally train at scale.",
    "start": "67050",
    "end": "72600"
  },
  {
    "text": "So this one introduces you to the concept of pretraining foundation models,",
    "start": "72600",
    "end": "79020"
  },
  {
    "text": "experimental results for what you need before you pull the trigger",
    "start": "79020",
    "end": "84420"
  },
  {
    "text": "and go crazy with all your accelerators. And then in the next\none we'll learn about,",
    "start": "84420",
    "end": "89490"
  },
  {
    "text": "again, preparing data at scale and then actually executing large training runs on SageMaker. So let's dive in.",
    "start": "89490",
    "end": "95493"
  },
  {
    "text": "All right, so in the session right now, we are going to learn\nwhen you should pretrain",
    "start": "97290",
    "end": "105570"
  },
  {
    "text": "a new foundation model. So remember, pretraining\nis how to literally create the foundation model from scratch.",
    "start": "105570",
    "end": "111570"
  },
  {
    "text": "So not just fine-tuning the thing, but actually pretraining a new foundation model from scratch.",
    "start": "111570",
    "end": "117117"
  },
  {
    "text": "And so we're gonna learn when to do this, so when it's actually a good idea,",
    "start": "117117",
    "end": "123660"
  },
  {
    "text": "and then what you need to\ndo to do it effectively. How to do it on AWS,",
    "start": "123660",
    "end": "128700"
  },
  {
    "text": "and then, again, the distributed\ntraining fundamentals, and we're gonna close out\nwith a notebook walk through",
    "start": "128700",
    "end": "133860"
  },
  {
    "text": "of pretraining a 30 billion\nparameter LLM on SageMaker. So let's go.",
    "start": "133860",
    "end": "139053"
  },
  {
    "text": "All right, so we've covered\na lot in this class so far. And in particular before\nyou actually approach",
    "start": "140250",
    "end": "147390"
  },
  {
    "text": "creating a new foundation model, I want you to have\nclosed these four things.",
    "start": "147390",
    "end": "152519"
  },
  {
    "text": "So four items I want you to do. So first, you should've tested many different foundation models.",
    "start": "152520",
    "end": "159690"
  },
  {
    "text": "So you should've tried\na few foundation models and you should've done prompt engineering",
    "start": "159690",
    "end": "165030"
  },
  {
    "text": "with those foundation models. So you should've learned\nabout instruction-tuning,",
    "start": "165030",
    "end": "170850"
  },
  {
    "text": "you should've put\ninstructions in your prompts, you should've tried few-shot prompting, and you should've gotten\nsome results with this.",
    "start": "170850",
    "end": "177330"
  },
  {
    "text": "You should feel pretty comfortable\nwith prompt engineering. Second thing I need you to do is I want you to have tried",
    "start": "177330",
    "end": "184080"
  },
  {
    "text": "a variety of fine-tuning techniques. So you should've already done PEFT,",
    "start": "184080",
    "end": "189780"
  },
  {
    "text": "you should've already done a larger, more classic fine-tuning. Maybe you even did the continued,\nunsupervised pretraining",
    "start": "189780",
    "end": "198900"
  },
  {
    "text": "on a couple tens of GBs, but definitely you want\nexperience with fine-tuning.",
    "start": "198900",
    "end": "204333"
  },
  {
    "text": "You should've exposed those both models to your end consumers",
    "start": "205260",
    "end": "210630"
  },
  {
    "text": "and gotten feedback on their performance. So you want your customers\nto have looked at the results",
    "start": "210630",
    "end": "217460"
  },
  {
    "text": "that came out of the prompt engineering and to have looked at the results that came out of the fine-tuning. And you should have not just anecdotal",
    "start": "218790",
    "end": "227190"
  },
  {
    "text": "or qualitative evidence, you want empirical, like, numbers that you can nail down like,",
    "start": "227190",
    "end": "233047"
  },
  {
    "text": "\"Yes, okay, customers\nrespond well to this, they don't respond well to that.",
    "start": "233047",
    "end": "239220"
  },
  {
    "text": "You know, this model\nresponse is more accurate, that model is better suited\nto different use cases.\"",
    "start": "239220",
    "end": "244620"
  },
  {
    "text": "You wanna be able to lock this down. That is because, basically,\nyou're gonna use this baseline",
    "start": "244620",
    "end": "252380"
  },
  {
    "text": "as the justification for\nyour pretraining project. Let's explain how to do that.",
    "start": "252390",
    "end": "258460"
  },
  {
    "text": "So really, if you're actually considering a pretraining project, if\nyou're serious about thinking,",
    "start": "259440",
    "end": "264787"
  },
  {
    "text": "\"Hey, maybe I want to create\na new foundation model,\" here's what you need. So, basically, I want you\nto build a chart like this.",
    "start": "264787",
    "end": "272100"
  },
  {
    "text": "I want you to say, \"Okay, on my Y-axis, here is my model accuracy,\"\nright, \"from 0 to 100,\"",
    "start": "272100",
    "end": "278879"
  },
  {
    "text": "or another KPI, right? It doesn't have to be accuracy, but you want some indicator of the quality",
    "start": "278880",
    "end": "285210"
  },
  {
    "text": "of your model results. And then I want you to show me all of the different types\nof techniques that you tried.",
    "start": "285210",
    "end": "291720"
  },
  {
    "text": "So you wanna be able to say, okay, so zero-shot got me\nhere, maybe got me to 10%,",
    "start": "291720",
    "end": "298820"
  },
  {
    "text": "single-shot maybe got me to 15, few-shot prompting got me maybe to 20,",
    "start": "299670",
    "end": "306510"
  },
  {
    "text": "parameter-efficient\nfine-tuning got me to 25, classic fine-tuning got me to maybe 30,",
    "start": "306510",
    "end": "316250"
  },
  {
    "text": "And then this area north of this, like the place you're gonna grow,",
    "start": "316410",
    "end": "321723"
  },
  {
    "text": "we are pretty sure, we hope\npretraining will fill this gap. And I say hope because pretraining\nis a big project, right?",
    "start": "322620",
    "end": "331340"
  },
  {
    "text": "You're gonna run a lot of GPUs\nand a lot of accelerators. And before I sign off on your project,",
    "start": "331410",
    "end": "337169"
  },
  {
    "text": "I want you to show me that\nyou have reason to believe",
    "start": "337170",
    "end": "342170"
  },
  {
    "text": "that it's going to improve your model and not just, you know, \"Oh,\nI heard someone else do it,\"",
    "start": "342570",
    "end": "348480"
  },
  {
    "text": "but like empirically,\n\"Here's what happens.\" So what does it actually take?",
    "start": "348480",
    "end": "354750"
  },
  {
    "text": "You know, you come in and you say, \"Emily, you know, I'm interested in pretraining a new foundation model because I'm ambitious,",
    "start": "354750",
    "end": "360150"
  },
  {
    "text": "and I have datasets and I think it's fun. What does it take? What do I need?\"",
    "start": "360150",
    "end": "365670"
  },
  {
    "text": "So let's look at three\ndifferent foundation models that were actually all\ntrained on SageMaker,",
    "start": "365670",
    "end": "371430"
  },
  {
    "text": "happily all trained on AWS, and let's explore what it\ntook to pretrain those.",
    "start": "371430",
    "end": "376920"
  },
  {
    "text": "Now, all of the data that I'm sharing here is from the papers that were released",
    "start": "376920",
    "end": "383850"
  },
  {
    "text": "with all of these models. So let's take a look at this. So Stable Diffusion, we\nknow Stable Diffusion,",
    "start": "383850",
    "end": "390330"
  },
  {
    "text": "2.1, five billion images. So that's a lot, right?",
    "start": "390330",
    "end": "396120"
  },
  {
    "text": "That's a massive dataset. And when you unzip that, it\nturns out to be 240 terabytes.",
    "start": "396120",
    "end": "404570"
  },
  {
    "text": "Now, again, this is from the lion dataset, and so that's basically\ncrawled from the internet and then you can download it",
    "start": "404910",
    "end": "411150"
  },
  {
    "text": "based on the licensing of those images. But, so that's a lot of data, right? That's multiple hundreds of terabytes.",
    "start": "411150",
    "end": "418413"
  },
  {
    "text": "But interestingly, the model\nsize of Stable Diffusion,",
    "start": "419340",
    "end": "424340"
  },
  {
    "text": "again, is quite small. It actually fits on a single accelerator. So you're looking at less\nthan one billion parameters",
    "start": "424410",
    "end": "431400"
  },
  {
    "text": "in the neural network\nof that model, right? And that's actually the\nself-attention blocks and the multi-head attention blocks",
    "start": "431400",
    "end": "438420"
  },
  {
    "text": "that are constituting the\nbody of the neural network, so the encoders and the decoders.",
    "start": "438420",
    "end": "444990"
  },
  {
    "text": "And in the case of Stable Diffusion, actually it's not even just one model, it's actually four different models",
    "start": "444990",
    "end": "450960"
  },
  {
    "text": "that are operating\ntogether to train this unit where the unit is taking in the text",
    "start": "450960",
    "end": "458730"
  },
  {
    "text": "as rendered through an LLM for embeddings",
    "start": "458730",
    "end": "463730"
  },
  {
    "text": "and then it's responding with the image that it's generating. So it's generating this image\non the other side of the unit.",
    "start": "464370",
    "end": "472560"
  },
  {
    "text": "And then, again, it's\ntaking steps in diffusion, so it's actually generating this image,",
    "start": "472560",
    "end": "479400"
  },
  {
    "text": "it's iteratively generating images and then it's using this denoising process",
    "start": "479400",
    "end": "485670"
  },
  {
    "text": "to compare the image that\nit generated randomly with the ground truth image",
    "start": "485670",
    "end": "490860"
  },
  {
    "text": "because it's a labeled\ndataset for starters and then this updates the unit",
    "start": "490860",
    "end": "495930"
  },
  {
    "text": "to then train a better generative model. So that's a long way of saying\nthat it's a small model.",
    "start": "495930",
    "end": "501270"
  },
  {
    "text": "And so because it's a small model, it fits on fewer instances.",
    "start": "501270",
    "end": "507449"
  },
  {
    "text": "So as stability reports, when they trained Stable Diffusion 2.1,",
    "start": "507450",
    "end": "513479"
  },
  {
    "text": "they used a cluster size\nof 37 p4d instances. And remember, the p4d instance\nis an AWS instance family.",
    "start": "513480",
    "end": "521610"
  },
  {
    "text": "You're looking at eight NVIDIA A100 GPUs. The p4d version, each of\nthem, has 40 GB of memory,",
    "start": "521610",
    "end": "531330"
  },
  {
    "text": "they're backed with NVMe local storage, you're looking at RDMA intra-GPU connect,",
    "start": "531330",
    "end": "538440"
  },
  {
    "text": "and lots of other fun things. And so, again, the cluster\nsize is 37 p4d instances,",
    "start": "538440",
    "end": "545390"
  },
  {
    "text": "so times eight, to give\nyou all of the GPUs that are in this cluster\nsize, all of the accelerators.",
    "start": "546720",
    "end": "554730"
  },
  {
    "text": "And then this entire job\ntook 28 days to train, which, if you think about how\nlong it would take a person",
    "start": "554730",
    "end": "564390"
  },
  {
    "text": "to look at five billion images, 28 days is actually not that bad, right,",
    "start": "564390",
    "end": "569700"
  },
  {
    "text": "relative to the amount of data this thing is crunching through. So that's 28 days. And then Falcon is a new LLM that came in",
    "start": "569700",
    "end": "578990"
  },
  {
    "text": "just in the last month,\nthe last couple months, from the TII institute in Abu Dhabi,",
    "start": "579480",
    "end": "586410"
  },
  {
    "text": "and they built this incredible model. And in the 40 billion\nparameter model size,",
    "start": "586410",
    "end": "595440"
  },
  {
    "text": "they used one trillion tokens. And so this is using\nthe updated scaling laws",
    "start": "595440",
    "end": "601530"
  },
  {
    "text": "from the Chinchilla paper that indicates that LLMs are mostly\nundertrained actually,",
    "start": "601530",
    "end": "607589"
  },
  {
    "text": "and so they recommend significantly increasing the dataset size, which means the model, at a smaller level,",
    "start": "607590",
    "end": "615570"
  },
  {
    "text": "is actually going to\nhave more information. And so that actually is lower pressure,",
    "start": "615570",
    "end": "621149"
  },
  {
    "text": "that's pressure to decrease\nthe size of the models. So in any case, so they trained Falcon",
    "start": "621150",
    "end": "627149"
  },
  {
    "text": "on one trillion tokens. If you average it out, that's\njust under three terabytes,",
    "start": "627150",
    "end": "634320"
  },
  {
    "text": "which some of you are\nsitting out there thinking, \"Hey, that's a lot,\" and other of you are thinking,\n\"That's pretty accessible.\"",
    "start": "634320",
    "end": "641310"
  },
  {
    "text": "Like, for what people do on AWS, like three terabytes is\nactually not that big of a deal.",
    "start": "641310",
    "end": "648760"
  },
  {
    "text": "You can pretty easily build\nand manage systems on AWS",
    "start": "652140",
    "end": "657140"
  },
  {
    "text": "for three terabytes. That's actually not that much data, speaking from a cloud perspective.",
    "start": "657480",
    "end": "663690"
  },
  {
    "text": "So yeah, so that's all\nit is, not that bad. And then you're looking\nat 40 billion parameters",
    "start": "663690",
    "end": "671130"
  },
  {
    "text": "in the model size, in the\nneural network itself. And then the cluster they\nused was 48 p4d instances.",
    "start": "671130",
    "end": "678630"
  },
  {
    "text": "So again, on SageMaker,\nsame instance type, 40 GB of memory in those A100s,\nand then they just have 48.",
    "start": "678630",
    "end": "687260"
  },
  {
    "text": "So in that SageMaker training job, they just set number\nof instances equals 48.",
    "start": "687270",
    "end": "692279"
  },
  {
    "text": "Beautiful thing. And then this took two months to train. And so you'll notice that\nthey report two months",
    "start": "692280",
    "end": "699000"
  },
  {
    "text": "and not 60 days precisely, so we can assume that\nthere's some start and stop",
    "start": "699000",
    "end": "704520"
  },
  {
    "text": "in the two-month report,\nbut just about two months. And then the last model\nhere is BloombergGPT.",
    "start": "704520",
    "end": "712290"
  },
  {
    "text": "BloombergGPT is another\nwonderful case study, obviously from Bloomberg. And as they report this,",
    "start": "712290",
    "end": "719430"
  },
  {
    "text": "this model was trained on SageMaker. And BloombergGPT is really interesting because they built a large language model",
    "start": "719430",
    "end": "727230"
  },
  {
    "text": "for financial services. So they noticed that most NLP models",
    "start": "727230",
    "end": "732990"
  },
  {
    "text": "were sort of general purpose\nfor the general domain and they wanted to build an LLM that was exclusively\nfor financial services.",
    "start": "732990",
    "end": "740400"
  },
  {
    "text": "And so they leveraged 700 billion tokens.",
    "start": "740400",
    "end": "745400"
  },
  {
    "text": "And as you can read in the\npaper, they split it between, so half of their dataset\nwas from the Common Crawl,",
    "start": "745620",
    "end": "754139"
  },
  {
    "text": "or the internet basically. The Common Crawl is a\nwell understood dataset that's used to create\nfoundation models for language.",
    "start": "754140",
    "end": "762240"
  },
  {
    "text": "And so half of their dataset\nis from the Common Crawl. And then the second half of their dataset",
    "start": "762240",
    "end": "767339"
  },
  {
    "text": "was Bloomberg's specific\nfinancial services, like I think articles,\nheadlines, the Bloomberg News,",
    "start": "767340",
    "end": "775970"
  },
  {
    "text": "so half of it was their own proprietary Bloomberg\nfinancial services dataset.",
    "start": "776250",
    "end": "781317"
  },
  {
    "text": "And so they had this nice mix of the two. And then they went for 50\nbillion parameter model.",
    "start": "781317",
    "end": "788370"
  },
  {
    "text": "Again, following the\nChinchilla Scaling Laws, and snuck in just under two terabytes.",
    "start": "788370",
    "end": "795269"
  },
  {
    "text": "So just under two terabytes\nfor their dataset size. And they had a...",
    "start": "795270",
    "end": "801420"
  },
  {
    "text": "Quite aggressive on the cluster size. So that's 64 p4d instances,\nwhich is a natural step up",
    "start": "801420",
    "end": "808010"
  },
  {
    "text": "from the Falcon proposed 40B. And then the BloombergGPT,\nagain just under two months,",
    "start": "809370",
    "end": "814800"
  },
  {
    "text": "so about 53 days, to train the\nmodel, again, on SageMaker.",
    "start": "814800",
    "end": "819800"
  },
  {
    "text": "And so this should give you a nice summary of what it takes to pretrain\na new foundation model",
    "start": "820020",
    "end": "828360"
  },
  {
    "text": "or to create a new foundation\nmodel from scratch. In summary, it takes three things.",
    "start": "828360",
    "end": "834420"
  },
  {
    "text": "It takes data in terabytes. Fortunately, not in petabytes.",
    "start": "834420",
    "end": "840779"
  },
  {
    "text": "So you're looking at a few terabytes like one, two, or three.",
    "start": "840780",
    "end": "845133"
  },
  {
    "text": "And then you're looking\nat tens of compute nodes. So you're gonna want\nobviously accelerators,",
    "start": "845970",
    "end": "854780"
  },
  {
    "text": "quite a few of them, you\nwant a lot of accelerators, and you want tens of compute nodes.",
    "start": "855090",
    "end": "860790"
  },
  {
    "text": "So you don't really need to\ngo north of 100 compute nodes, you certainly can.",
    "start": "860790",
    "end": "866430"
  },
  {
    "text": "And obviously if you are training a model that's much larger than the\n40 or 50 billion parameter,",
    "start": "866430",
    "end": "873930"
  },
  {
    "text": "and if you're training a model that's north of 100 billion parameters, then certainly you might pop",
    "start": "873930",
    "end": "880829"
  },
  {
    "text": "into more than 100 compute nodes. But more than anything, again, I want you to have a super\nstrong business case.",
    "start": "880830",
    "end": "887400"
  },
  {
    "text": "Like, I want you to be able to say like, \"Emily, if we take this foundation\nmodel and we put it here,",
    "start": "887400",
    "end": "894330"
  },
  {
    "text": "our business is going\nto go up.\" (chuckles) Like, when you can tell that\nto me really concretely,",
    "start": "894330",
    "end": "900029"
  },
  {
    "text": "I'm gonna be so excited, and\nI'm gonna say like, \"Let's go. Let's get started on your\npretraining project.\"",
    "start": "900030",
    "end": "906210"
  },
  {
    "text": "Because if you have these three things, then you have what it takes to create your own foundation model",
    "start": "906210",
    "end": "912029"
  },
  {
    "text": "that saves you from legal potential risk that makes the data licensing",
    "start": "912030",
    "end": "918449"
  },
  {
    "text": "and data usage a lot more clear. And it also makes the rest of\nyour application development extremely clear because\nthen you're not locked in",
    "start": "918450",
    "end": "925829"
  },
  {
    "text": "to any vendor. You can create your own foundation model and then put it wherever you want to.",
    "start": "925830",
    "end": "930870"
  },
  {
    "text": "So there's a lot of\nflexibility in this approach,",
    "start": "930870",
    "end": "935373"
  },
  {
    "text": "and so that's why it's\nmy personal favorite. All right, and so, again,\nbefore you actually hit go",
    "start": "936247",
    "end": "943020"
  },
  {
    "text": "on all of those hundreds and\nthousands of accelerators, I want you to be able to have",
    "start": "943020",
    "end": "949230"
  },
  {
    "text": "this, like, last final chart basically where you want to prove empirically that, as a result of your experiments,",
    "start": "949230",
    "end": "957300"
  },
  {
    "text": "you saw that using a pretrained model improved the performance,",
    "start": "957300",
    "end": "962970"
  },
  {
    "text": "specifically the model accuracy or another statistic that\nyou're tracking as a KPI,",
    "start": "962970",
    "end": "968130"
  },
  {
    "text": "that using a pretrained model, like, improved the performance\nof your open source model",
    "start": "968130",
    "end": "974310"
  },
  {
    "text": "and this will do X to your business case. And once you have all of\nthose cleanly defined,",
    "start": "974310",
    "end": "980850"
  },
  {
    "text": "then you are ready to hit go\non your pretraining project. Now, remember that both of\nthese are actually fine-tuned.",
    "start": "980850",
    "end": "988199"
  },
  {
    "text": "So you would take an open source BERT, or you would take maybe open\nsource Falcon if you want to,",
    "start": "988200",
    "end": "996149"
  },
  {
    "text": "the base Falcon, and then you'd\nfine-tune it on your data. So maybe you're fine-tuning base Falcon",
    "start": "996150",
    "end": "1004580"
  },
  {
    "text": "on your 30 GB of data for your prompts,",
    "start": "1004580",
    "end": "1009580"
  },
  {
    "text": "your instruction fine-tuning\nthat you want to do. Or you're doing one step of\nunsupervised pretraining,",
    "start": "1011120",
    "end": "1019190"
  },
  {
    "text": "so you do one round of\nunsupervised pretraining and then do the fine-tuning. But in any case, what I'm saying is",
    "start": "1019190",
    "end": "1026089"
  },
  {
    "text": "the accuracy of both of these\nis on your downstream task. And you wanna say that, compared\nto this open source model",
    "start": "1026090",
    "end": "1034310"
  },
  {
    "text": "or possibly a proprietary\nmodel, we are able to show that pretraining on our data",
    "start": "1034310",
    "end": "1042230"
  },
  {
    "text": "actually improved the\naccuracy of our model. And so once you've done this,",
    "start": "1042230",
    "end": "1047360"
  },
  {
    "text": "then you are well on\nyour way to a really cool and really successful pretraining project.",
    "start": "1047360",
    "end": "1053723"
  },
  {
    "text": "So how do we do this? Literally, how do we pretrain or create our own\nfoundation models on AWS?",
    "start": "1054800",
    "end": "1061250"
  },
  {
    "text": "It's actually not that bad. You might think it's terrifying,\nit's not. It's doable. We've been helping\ncustomers, as you can see,",
    "start": "1061250",
    "end": "1067850"
  },
  {
    "text": "do this for a long time. We have an awesome orchestration stack, we have a lot of expertise\nbuilt up in this area,",
    "start": "1067850",
    "end": "1074029"
  },
  {
    "text": "and so it's not too bad. So the first thing you're gonna do, again,",
    "start": "1074030",
    "end": "1079630"
  },
  {
    "text": "is just gather your datasets, store these, have these ready to go.",
    "start": "1079630",
    "end": "1085070"
  },
  {
    "text": "Many different ways of doing that, which we'll learn about\nactually in the next class. So first, you're gonna gather your data",
    "start": "1085070",
    "end": "1090710"
  },
  {
    "text": "and park that probably in an S3 bucket. Then you're gonna process\nyour data for training.",
    "start": "1090710",
    "end": "1097010"
  },
  {
    "text": "As we learn in the next lecture, many different ways of doing that on AWS.",
    "start": "1097010",
    "end": "1101400"
  },
  {
    "text": "My personal favorite is SageMaker so we'll tend to go the SageMaker route, but there's a lot of other\nways you can do this.",
    "start": "1102530",
    "end": "1108830"
  },
  {
    "text": "Then we're gonna basically\noptimize the data storage for our training runs.",
    "start": "1108830",
    "end": "1115850"
  },
  {
    "text": "So I enjoy using FSx for Lustre. So we'll actually create\nan FSx for Lustre volume,",
    "start": "1115850",
    "end": "1122630"
  },
  {
    "text": "which we can then mount\nto our training instances for really rapid iteration.",
    "start": "1122630",
    "end": "1128390"
  },
  {
    "text": "So we'll optimize our\ndata storage for the runs, and then we'll run a notebook somewhere",
    "start": "1128390",
    "end": "1134360"
  },
  {
    "text": "to do develop and test\nbasically training scripts. So if you're in the cloud,\nyou'll use small like CPU,",
    "start": "1134360",
    "end": "1142340"
  },
  {
    "text": "maybe small older GPUs just to develop and test\nyour training scripts,",
    "start": "1142340",
    "end": "1147409"
  },
  {
    "text": "or you can just run this\nlocally on your laptop. Really up to you. And so you'll develop and test",
    "start": "1147410",
    "end": "1152750"
  },
  {
    "text": "these training scripts, right, that are pointing to the Falcon, for example, model config,\nloading the model config,",
    "start": "1152750",
    "end": "1162019"
  },
  {
    "text": "doing some testing there with the PyTorch forward\nand backward pass. Alternatively, maybe you're just using",
    "start": "1162020",
    "end": "1168890"
  },
  {
    "text": "the Hugging Face Trainer API and maybe you're building it into a distributed training framework.",
    "start": "1168890",
    "end": "1175220"
  },
  {
    "text": "So you're using Accelerate,\nor you're using DeepSpeed, or using SageMaker\ndistributed training libraries",
    "start": "1175220",
    "end": "1181400"
  },
  {
    "text": "that we'll learn about here. And so, again, you're gonna\nstart really small here.",
    "start": "1181400",
    "end": "1187160"
  },
  {
    "text": "So you're gonna start with\nsmall CPUs, small GPUs,",
    "start": "1187160",
    "end": "1192160"
  },
  {
    "text": "older GPUs, small models, small datasets, and then, incrementally,\nyou're gonna scale this up.",
    "start": "1192440",
    "end": "1198919"
  },
  {
    "text": "So over the course of maybe\nthree, four, five months",
    "start": "1198920",
    "end": "1203920"
  },
  {
    "text": "you'll take a number of scaling jumps. So you'll start on just a notebook,",
    "start": "1204410",
    "end": "1209419"
  },
  {
    "text": "just running the basic model config on 100 samples from your\ndataset, or something like this.",
    "start": "1209420",
    "end": "1216370"
  },
  {
    "text": "And then you'll increase that. Then you'll broaden the scope,",
    "start": "1216530",
    "end": "1220882"
  },
  {
    "text": "maybe using a larger\nGPU, larger instances, larger volumes of data, and\nthen you'll take another jump.",
    "start": "1221900",
    "end": "1228710"
  },
  {
    "text": "Then you go to distributed\ntrainings or using multiple hosts and then on and on\nuntil you're ready to go",
    "start": "1228710",
    "end": "1235309"
  },
  {
    "text": "for one, like, massive run. And then throughout that, you\nevaluate the model artifact.",
    "start": "1235310",
    "end": "1242059"
  },
  {
    "text": "So after each training run, you're gonna download the model\ncheckpoints and poke at it,",
    "start": "1242060",
    "end": "1248090"
  },
  {
    "text": "like load it into a framework, see how well it does in PyTorch locally",
    "start": "1248090",
    "end": "1253160"
  },
  {
    "text": "or with Hugging Face, and just compare the performance,\nright, and just test it",
    "start": "1253160",
    "end": "1259010"
  },
  {
    "text": "and then that leads you back\ninto to the rest of your flow.",
    "start": "1259010",
    "end": "1262943"
  },
  {
    "text": "So what does this look like in action? Again, I like to think\nof a pretraining project as having different phases.",
    "start": "1265520",
    "end": "1271640"
  },
  {
    "text": "Actually I talk about this in my book. You can definitely look\nstraight in my book, \"Pretrain Vision and\nLarge Language Models\"",
    "start": "1271640",
    "end": "1277910"
  },
  {
    "text": "to check this out. But yeah, so I think about\nthis in a number of phases.",
    "start": "1277910",
    "end": "1283190"
  },
  {
    "text": "So really your first phase\nis a 1% sample of your data.",
    "start": "1283190",
    "end": "1288190"
  },
  {
    "text": "And I say this heuristically\nlike, just use a little bit, use a little data, use the small size,",
    "start": "1288530",
    "end": "1295130"
  },
  {
    "text": "the smallest size of the model and run on a single\naccelerator, so just one GPU,",
    "start": "1295130",
    "end": "1300927"
  },
  {
    "text": "and that should not\ntake more than an hour, like that should be one fast\nlittle mini job that you run.",
    "start": "1302030",
    "end": "1309053"
  },
  {
    "text": "Development time on that, I\nwould count that in hours. Again, you're using Managed Services,",
    "start": "1310370",
    "end": "1316220"
  },
  {
    "text": "you're mostly stepping through notebooks that are already built, and so the development time\nfor that shouldn't be massive.",
    "start": "1316220",
    "end": "1324020"
  },
  {
    "text": "And then you're gonna\nincrease the dataset, right? So then phase two is a bump.",
    "start": "1324020",
    "end": "1330350"
  },
  {
    "text": "So you'll use more of the dataset, you'll marry that with\na larger model size,",
    "start": "1330350",
    "end": "1336770"
  },
  {
    "text": "increase the number of accelerators. So instead of just\nusing one Trainium chip,",
    "start": "1336770",
    "end": "1342440"
  },
  {
    "text": "now you're using eight Trainium chips. And then your development\nand compute time,",
    "start": "1342440",
    "end": "1348200"
  },
  {
    "text": "that's gonna take you a couple days, like that's more complex. And so it'll take you a\ncouple days to build this out,",
    "start": "1348200",
    "end": "1357200"
  },
  {
    "text": "to develop it, test it, make sure it's running appropriately, optimize a couple things, troubleshoot it,",
    "start": "1357200",
    "end": "1363320"
  },
  {
    "text": "submit a ticket, read a blog\npost, all the normal things.",
    "start": "1363320",
    "end": "1367372"
  },
  {
    "text": "Yeah, and so that's phase two, is like, \"Can I actually use data parallel,\"",
    "start": "1368420",
    "end": "1373460"
  },
  {
    "text": "which we'll learn about in a minute, \"Can I use data parallel to park my model",
    "start": "1373460",
    "end": "1378559"
  },
  {
    "text": "on all eight of these accelerators,\" and then run my forward and backward pass.",
    "start": "1378560",
    "end": "1382700"
  },
  {
    "text": "Once you've closed phase two, then things start to get interesting. So then you're looking at a\nmuch larger sample of your data.",
    "start": "1383570",
    "end": "1392903"
  },
  {
    "text": "And depending on how large you wanna go, you could just go full throttle, right? You could just go straight\nfor the full data sample.",
    "start": "1394160",
    "end": "1402080"
  },
  {
    "text": "Or you could go with just\nthe 50%, use a larger model,",
    "start": "1402080",
    "end": "1407080"
  },
  {
    "text": "and then maybe you're running\non multiple boxes actually. So maybe you have, yeah, eight,",
    "start": "1407810",
    "end": "1415160"
  },
  {
    "text": "I'm sorry, yeah, eight instances. Is that eight? No,\nthat'd be two. (chuckles) 16 divided by 8 is 2. So\nyou've got 2 instances.",
    "start": "1415160",
    "end": "1424340"
  },
  {
    "text": "Two instances with each\nhaving eight accelerators. So you have two instances.",
    "start": "1424340",
    "end": "1429890"
  },
  {
    "text": "And it'll take you a little bit. Honestly, two instances probably won't take you a couple weeks. That'll probably take\nyou a week and a half.",
    "start": "1429890",
    "end": "1436250"
  },
  {
    "text": "But, so you have two boxes\nnow instead of just one, and you're using a larger data sample,",
    "start": "1436250",
    "end": "1442970"
  },
  {
    "text": "and you're using a larger\nversion of your model, and now you know concretely that you're developing on two instances,",
    "start": "1442970",
    "end": "1449210"
  },
  {
    "text": "whereas before you were\njust developing on one. So the jump here in accelerators, like you see first we were\nrunning on a single GPU",
    "start": "1449210",
    "end": "1456409"
  },
  {
    "text": "or a single Trainium chip, then we increase that out to eight GPUs,",
    "start": "1456410",
    "end": "1462110"
  },
  {
    "text": "eight Trainium chips, all\nsitting in a single box. So now we just have one instance\nwith all eight of those.",
    "start": "1463082",
    "end": "1469370"
  },
  {
    "text": "Then we double that. So then in this phase\nthree, we have two boxes. Now we have two p4d instances,\nor two Trainium boxes,",
    "start": "1469370",
    "end": "1478990"
  },
  {
    "text": "and we are essentially training on a larger version of our model",
    "start": "1479480",
    "end": "1485870"
  },
  {
    "text": "and we're doing this for\na larger period of time, and then finally we'll go crazy, right,",
    "start": "1485870",
    "end": "1491537"
  },
  {
    "text": "and then we'll hit the\ntrigger for the whole thing and max out our dataset size.",
    "start": "1491537",
    "end": "1497389"
  },
  {
    "text": "We're running on the entire data sample. We're using the largest model that we can. We're maxing out our accelerator runs.",
    "start": "1497390",
    "end": "1504950"
  },
  {
    "text": "So maybe we're going up to 64 instances. If we wanna train a\nfully generative model,",
    "start": "1504950",
    "end": "1512870"
  },
  {
    "text": "maybe we're training 100\nbillion parameters, maybe more, or if we're just doing\nclassification, you know,",
    "start": "1512870",
    "end": "1519770"
  },
  {
    "text": "or like small targeted generation, maybe we're only doing that\n40 or 50 billion range.",
    "start": "1519770",
    "end": "1525169"
  },
  {
    "text": "But, so in any case, at each phase, again, we're gonna download\nthat model checkpoint",
    "start": "1525170",
    "end": "1531860"
  },
  {
    "text": "and we're gonna test it to\nmake sure that it's valid. So we'll just download\nthe model checkpoint,",
    "start": "1531860",
    "end": "1537860"
  },
  {
    "text": "load it into our Hugging Face framework, run some local development on that,",
    "start": "1537860",
    "end": "1544010"
  },
  {
    "text": "and make sure we like it. And so when you think about\nyour project in these phases,",
    "start": "1544010",
    "end": "1549170"
  },
  {
    "text": "it makes it much easier to\nmitigate the risks, right, and mitigate the complexity",
    "start": "1549170",
    "end": "1557029"
  },
  {
    "text": "and the frustration and the stress because then you can just easily focus on each of the solvable steps.",
    "start": "1557030",
    "end": "1563870"
  },
  {
    "text": "And then by the end of it,\nyou've got an awesome model. And so we come to find out",
    "start": "1563870",
    "end": "1570710"
  },
  {
    "text": "that there's this thing called\ndistributed training, right? And as you may have guessed, in order to train a model on hundreds",
    "start": "1570710",
    "end": "1579650"
  },
  {
    "text": "or thousands of accelerators, you need a framework to\nactually distribute this, right,",
    "start": "1579650",
    "end": "1585020"
  },
  {
    "text": "to manage all of the copies of the model and then to synchronize\nthe copies of those model",
    "start": "1585020",
    "end": "1592490"
  },
  {
    "text": "across the forward and the backward pass that the neural network is taking, and we're gonna learn how to do that.",
    "start": "1592490",
    "end": "1597590"
  },
  {
    "text": "But before we even distribute a model, there's something I like to talk about,",
    "start": "1597590",
    "end": "1603260"
  },
  {
    "text": "which is called job parallelism. And so this is actually\nreally common on SageMaker",
    "start": "1603260",
    "end": "1608870"
  },
  {
    "text": "because we have this elastic training API we have this ephemeral training API",
    "start": "1608870",
    "end": "1613970"
  },
  {
    "text": "where you can invoke SageMaker and SageMaker will\ncreate this training job. We'll spin up instances,",
    "start": "1613970",
    "end": "1620450"
  },
  {
    "text": "we'll copy your data\nfrom your data location, copy in your image, invoke your image,",
    "start": "1620450",
    "end": "1625940"
  },
  {
    "text": "stream everything to CloudWatch, and then pop your model\nin S3 when it's done.",
    "start": "1625940",
    "end": "1631280"
  },
  {
    "text": "So we create a cluster on demand, we actually spin up CPU",
    "start": "1631280",
    "end": "1636290"
  },
  {
    "text": "or accelerator-based instances on demand, interact with your data.",
    "start": "1636290",
    "end": "1642620"
  },
  {
    "text": "And by that, I mean mount it\non the training volume for you, on the training instance,",
    "start": "1642620",
    "end": "1648173"
  },
  {
    "text": "and give you this really nice\nmanaged training experience. It's elastic because you don't\nneed to literally provision",
    "start": "1649121",
    "end": "1655279"
  },
  {
    "text": "all of the boxes ahead of time. You're just calling\nthem when you need them, like precisely to train the model.",
    "start": "1655280",
    "end": "1662990"
  },
  {
    "text": "So job parallelism is handy when you need to train multiple models. I like to use it when I'm processing data.",
    "start": "1662990",
    "end": "1669890"
  },
  {
    "text": "Honestly, I'm so used to\nthe SageMaker training API, in the SageMaker way. I just use it for all\nof my data processing",
    "start": "1669890",
    "end": "1676610"
  },
  {
    "text": "'cause it's very\nefficient for me to do so. So I'll actually show you how to do data processing\nin SageMaker, just on CPUs.",
    "start": "1676610",
    "end": "1685090"
  },
  {
    "text": "So we'll take a look at that here and then in the next video as well. So job parallelism is the first kind",
    "start": "1685760",
    "end": "1691910"
  },
  {
    "text": "of distributed training we'll learn about. So job parallelism refers to literally running multiple\njobs at the same time",
    "start": "1691910",
    "end": "1701400"
  },
  {
    "text": "but using different inputs and outputs. So in this case, I'm running\nthree separate jobs, right?",
    "start": "1702290",
    "end": "1709010"
  },
  {
    "text": "I have my S3 bucket where\nall my data is stored and then I'm running these different jobs,",
    "start": "1709010",
    "end": "1714860"
  },
  {
    "text": "and each of these is using the same image. They could be using different images.",
    "start": "1714860",
    "end": "1720289"
  },
  {
    "text": "They can be using different scripts. And then all of them are, the metadata is captured in\nthe SageMaker control plane,",
    "start": "1720290",
    "end": "1729020"
  },
  {
    "text": "the logs are sent to CloudWatch, and then my model artifact is stored in S3",
    "start": "1729020",
    "end": "1734300"
  },
  {
    "text": "after the end of the job. Now, I like job parallelism\nbecause it lets me scale",
    "start": "1734300",
    "end": "1739909"
  },
  {
    "text": "up to as many models as I need to train. So, for example, I might write a really basic like Python loop",
    "start": "1739910",
    "end": "1747529"
  },
  {
    "text": "where I'm just gonna list\nthrough all of my models or all of my jobs that I need to run.",
    "start": "1747530",
    "end": "1753290"
  },
  {
    "text": "For each job, I'll point\nto my input location, so some S3 path,",
    "start": "1753290",
    "end": "1759020"
  },
  {
    "text": "I'll set an output location, also an S3, and then I'll get the estimators.",
    "start": "1759020",
    "end": "1764930"
  },
  {
    "text": "Remember, the estimator is this syntax, this Python Syntax that we have",
    "start": "1764930",
    "end": "1770630"
  },
  {
    "text": "that is a wrapper around\nthe training job API. So it lets you call the training\njob API but just in Python.",
    "start": "1770630",
    "end": "1779200"
  },
  {
    "text": "And so you'll get the estimator. And then from this,\nyou'll set your estimator",
    "start": "1779600",
    "end": "1786170"
  },
  {
    "text": "and you'll call model .fit, right? So estimator.fit, you\nfit this model in Python.",
    "start": "1786170",
    "end": "1791630"
  },
  {
    "text": "And then with this handy\nparameter here, wait=False, what that means is it frees Jupiter.",
    "start": "1791630",
    "end": "1798200"
  },
  {
    "text": "So when you use the default settings, Jupiter will sort of sit there with,",
    "start": "1798200",
    "end": "1803510"
  },
  {
    "text": "and it'll show a little process running. And so that will halt your notebook",
    "start": "1803510",
    "end": "1809600"
  },
  {
    "text": "or halt your Python process because it thinks that there's\na variable that's running",
    "start": "1809600",
    "end": "1815210"
  },
  {
    "text": "or that there's a process that's running. In this case, when you\nsaid wait equals to false,",
    "start": "1815210",
    "end": "1822260"
  },
  {
    "text": "basically that saves you from that, like that completes the\nlocal Python process,",
    "start": "1822260",
    "end": "1829309"
  },
  {
    "text": "so it completes the local Python execution after it's called the API.",
    "start": "1829310",
    "end": "1835610"
  },
  {
    "text": "So then your local Python executor stops, it halts, it exits.",
    "start": "1835610",
    "end": "1841970"
  },
  {
    "text": "But in the back-end, you've actually turned on\nthese SageMaker instances. So you're turning on the\nSageMaker training instance,",
    "start": "1841970",
    "end": "1848510"
  },
  {
    "text": "you've got this cluster\nthat's now running, and then this is how you can literally",
    "start": "1848510",
    "end": "1853909"
  },
  {
    "text": "loop through it, right? So you loop through your list of models and then, for each of these,\nyou create this estimator,",
    "start": "1853910",
    "end": "1860390"
  },
  {
    "text": "you call estimator.fit. And then when my wait is set to false, again, that just runs the job.",
    "start": "1860390",
    "end": "1866897"
  },
  {
    "text": "And so this way I can\nhave basically three jobs that I'm invoking, almost\nnearly at the same time.",
    "start": "1866897",
    "end": "1874850"
  },
  {
    "text": "Like, if you're doing a lot\nof data processing here, then obviously it won't\nrun at the same time. But if you're not doing\nit in data processing,",
    "start": "1874850",
    "end": "1882050"
  },
  {
    "text": "then these jobs will basically execute at nearly the same second\n'cause it's a pretty fast loop.",
    "start": "1882050",
    "end": "1887543"
  },
  {
    "text": "So this is how you can do job parallelism. Using the SageMaker training API,",
    "start": "1888860",
    "end": "1893870"
  },
  {
    "text": "you can use this to train models or you can use this to process data.",
    "start": "1893870",
    "end": "1899360"
  },
  {
    "text": "Another handy feature to use\nhere is called warm pools. And so warm pools is another parameter",
    "start": "1899360",
    "end": "1906320"
  },
  {
    "text": "you add to the estimator and it keeps the instance warm actually. So when you run a job,\nthe instance turns on",
    "start": "1906320",
    "end": "1914060"
  },
  {
    "text": "and then let's say I forgot a parentheses or I typed in like one\nletter that was wrong",
    "start": "1914060",
    "end": "1921500"
  },
  {
    "text": "and my Python script breaks, what do I do? So when warm pools are enabled,",
    "start": "1921500",
    "end": "1927470"
  },
  {
    "text": "the instance will stay there. So you can just do a one-line code update, run the training job again\nand it will start in seconds,",
    "start": "1927470",
    "end": "1936350"
  },
  {
    "text": "instead of needing to wait up\nto eight minutes to restart. So warm pools are great,",
    "start": "1936350",
    "end": "1942140"
  },
  {
    "text": "and you can use those here\nif you're looping through. So, like, lots of models,",
    "start": "1942140",
    "end": "1948680"
  },
  {
    "text": "and you wanna reuse those instances. So job parallelism is the first kind",
    "start": "1948680",
    "end": "1955250"
  },
  {
    "text": "of distributed training I'd\nlike you to think about. And then the second kind\nis called data parallelism.",
    "start": "1955250",
    "end": "1962330"
  },
  {
    "text": "So let's try and understand\ndata parallelism. So distributed gradient\ndescent has evolved over time.",
    "start": "1962330",
    "end": "1970580"
  },
  {
    "text": "So let's unpack this. So gradient descent, remember, is the core mathematical operation",
    "start": "1970580",
    "end": "1977690"
  },
  {
    "text": "that inspires all of the\noptimizers in neural networks. So neural network optimization",
    "start": "1977690",
    "end": "1984470"
  },
  {
    "text": "is how the neural network learns, right? You define your layers in neural networks,",
    "start": "1984470",
    "end": "1989990"
  },
  {
    "text": "you pass data through those layers, be that images, be that text,",
    "start": "1989990",
    "end": "1995510"
  },
  {
    "text": "they get all of this\nmath computed on them, and then at the end of the\nlayer there's this output.",
    "start": "1995510",
    "end": "2001750"
  },
  {
    "text": "And that can be generation,\nit can be classification, it's whatever you define it. And then there's a comparison",
    "start": "2001750",
    "end": "2009370"
  },
  {
    "text": "with the output of the neural\nnetwork and the ground truth. And the ground truth can be created",
    "start": "2009370",
    "end": "2016570"
  },
  {
    "text": "artificially through pretraining, which if you want to know more about, read my book on pretraining.",
    "start": "2016570",
    "end": "2022210"
  },
  {
    "text": "But, so yeah, so it creates it through this unsupervised pretraining loss",
    "start": "2022210",
    "end": "2027279"
  },
  {
    "text": "that essentially is\ndefined through masking. And so that's how it's\ncreated in language.",
    "start": "2027280",
    "end": "2032860"
  },
  {
    "text": "And then in vision, if you're\ndoing Stable Diffusion, remember, you have captions. So the training dataset for\nStable Diffusion is labeled.",
    "start": "2032860",
    "end": "2041440"
  },
  {
    "text": "In any case, they're all\nusing gradient descent. Gradient descent is an algorithm,",
    "start": "2041440",
    "end": "2049000"
  },
  {
    "text": "is, again, a mathematical operation that you learned in your\nfirst calculus class,",
    "start": "2049000",
    "end": "2054370"
  },
  {
    "text": "believe it or not, when you\nwere solving derivatives because gradient actually\nrefers to derivative, right?",
    "start": "2054370",
    "end": "2061419"
  },
  {
    "text": "There are two ways of\ndescribing the same topic. And so when you're descending\nthrough the gradient,",
    "start": "2061420",
    "end": "2068230"
  },
  {
    "text": "you're computing the partial derivatives. And so we compute the partial derivatives of your neural network with\nrespect to your loss function",
    "start": "2068230",
    "end": "2077379"
  },
  {
    "text": "because we want to know\nhow those parameters are impacting the loss. And then to minimize the loss,",
    "start": "2077380",
    "end": "2084250"
  },
  {
    "text": "we update the neural network parameters in the other direction. So anyway, that's how\nneural networks learn.",
    "start": "2084250",
    "end": "2089570"
  },
  {
    "text": "And it's easier if it's\nall just on one box, right? If everything is just on\none GPU or one instance,",
    "start": "2090910",
    "end": "2098890"
  },
  {
    "text": "one accelerator, it's a lot easier. Once you distribute this and\nonce you need to run this,",
    "start": "2098890",
    "end": "2104470"
  },
  {
    "text": "not just on multiple GPUs\nor multiple accelerators",
    "start": "2104470",
    "end": "2109470"
  },
  {
    "text": "but multiple hosts and multiple instances, it's a lot more challenging.",
    "start": "2109690",
    "end": "2114700"
  },
  {
    "text": "So one of the classic ways of doing this is called the parameter server. So a parameter server literally maintains",
    "start": "2114700",
    "end": "2121180"
  },
  {
    "text": "one copy of the model. It's one sort of global copy of the model that's also interacting\nwith all of these hosts,",
    "start": "2121180",
    "end": "2129700"
  },
  {
    "text": "all of these worker hosts. And so this sort of leader rank here",
    "start": "2129700",
    "end": "2134710"
  },
  {
    "text": "will keep track of all of the\nother instances in the cluster",
    "start": "2134710",
    "end": "2139710"
  },
  {
    "text": "and we'll keep track of the gradients that are being computed, so\nthe forward and backward pass.",
    "start": "2140920",
    "end": "2147460"
  },
  {
    "text": "And so this worker, this master node, this leader node will send the data",
    "start": "2147460",
    "end": "2155020"
  },
  {
    "text": "out to all of these workers. They compute the forward\nand the backward pass.",
    "start": "2155020",
    "end": "2161500"
  },
  {
    "text": "They compute the forward pass rather because they're all\nusing copies of the data.",
    "start": "2161500",
    "end": "2168040"
  },
  {
    "text": "They send the forward pass\nback into the central one. This averages that and\nthen sends it back out",
    "start": "2168040",
    "end": "2174760"
  },
  {
    "text": "to update the parameters\nwith the backward pass. So parameter servers are somewhat slow.",
    "start": "2174760",
    "end": "2183040"
  },
  {
    "text": "They're not the fastest thing in the world because when you have\nall of these accelerators",
    "start": "2183040",
    "end": "2188140"
  },
  {
    "text": "and all of these hosts that\nare attacking just one spot, it leads to conflicts and\nit slows down communication.",
    "start": "2188140",
    "end": "2195850"
  },
  {
    "text": "And so, yeah, so parameter\nservers are a little bit slow. But surprisingly, they're\nsomewhat bandwidth,",
    "start": "2195850",
    "end": "2203410"
  },
  {
    "text": "they're somewhat low bandwidth. The communication hops are not massive.",
    "start": "2203410",
    "end": "2209353"
  },
  {
    "text": "This is compared with the\nMPI AllReduce algorithm",
    "start": "2210250",
    "end": "2215250"
  },
  {
    "text": "that is common in\nhigh-performance computing where you see all of the nodes",
    "start": "2217060",
    "end": "2222340"
  },
  {
    "text": "are actually communicating\nwith each other. So instead of having\none centralized leader",
    "start": "2222340",
    "end": "2228220"
  },
  {
    "text": "as in the parameter\nserver, in this approach, you see all of the nodes, again,",
    "start": "2228220",
    "end": "2233290"
  },
  {
    "text": "communicating with each other like a ring. So this is called a ring-based topology",
    "start": "2233290",
    "end": "2240040"
  },
  {
    "text": "because the nodes, again, create this ring where they're interacting with each other.",
    "start": "2240040",
    "end": "2246460"
  },
  {
    "text": "And so this is more common, this is the updated version\nof the parameter server,",
    "start": "2246460",
    "end": "2251650"
  },
  {
    "text": "and this is common in both Horovod and PyTorch DistributedDataParallel.",
    "start": "2251650",
    "end": "2256483"
  },
  {
    "text": "And this distributed\ngradient descent version, this AllReduce is faster,",
    "start": "2257394",
    "end": "2264579"
  },
  {
    "text": "it's certainly much faster\nthan the parameter server, but it's also incredibly\nbandwidth-intensive.",
    "start": "2264580",
    "end": "2270370"
  },
  {
    "text": "And in fact, as you add more and more and more and more accelerators\nto this overall job,",
    "start": "2270370",
    "end": "2277540"
  },
  {
    "text": "the communication and the\nbandwidth specifically becomes the upper bound, like you become,",
    "start": "2277540",
    "end": "2283600"
  },
  {
    "text": "basically you max out your\nability just to communicate and then adding extra\ninstances to your job",
    "start": "2283600",
    "end": "2291790"
  },
  {
    "text": "doesn't do anything for you. You're not scaling efficiently. So we built a thing for that.",
    "start": "2291790",
    "end": "2298645"
  },
  {
    "text": "So SageMaker Distributed Data Parallel provides a custom algorithm for this.",
    "start": "2298645",
    "end": "2303730"
  },
  {
    "text": "So this is a custom communication\ncollective algorithm on AWS that is purpose-built",
    "start": "2303730",
    "end": "2310599"
  },
  {
    "text": "for distributed gradient descent. So this runs on SageMaker.",
    "start": "2310600",
    "end": "2315640"
  },
  {
    "text": "Basically, we use the CPUs\non all of your instances",
    "start": "2315640",
    "end": "2320640"
  },
  {
    "text": "to operate like parameter servers. So they're communicating\nwith all of the other CPUs",
    "start": "2321250",
    "end": "2326620"
  },
  {
    "text": "and then the GPUs, or the accelerators, are used for the majority",
    "start": "2326620",
    "end": "2331780"
  },
  {
    "text": "of the lion's share of the work, right. They're actually doing the\nforward and the backward pass.",
    "start": "2331780",
    "end": "2336970"
  },
  {
    "text": "The CPUs, again, are communicating with the other instances\nand passing the gradients.",
    "start": "2336970",
    "end": "2344950"
  },
  {
    "text": "And then we overlap the backward pass with the GPU health checks basically.",
    "start": "2344950",
    "end": "2351070"
  },
  {
    "text": "So in order to communicate\nwith all of the accelerators,",
    "start": "2351070",
    "end": "2354313"
  },
  {
    "text": "yeah, we'll overlap that with the backward pass communication.",
    "start": "2356680",
    "end": "2360673"
  },
  {
    "text": "And it's really fast. It's 20 to 40% faster, and therefore less expensive than NCCL",
    "start": "2362475",
    "end": "2368390"
  },
  {
    "text": "and other MPI-based solutions on AWS. And so this is how to get\nthe best performance on AWS",
    "start": "2369520",
    "end": "2375079"
  },
  {
    "text": "for extremely large cluster sizes. And so data parallel in a nutshell, again,",
    "start": "2375940",
    "end": "2381160"
  },
  {
    "text": "is actually making copies of your model. What's going on in a data parallel world",
    "start": "2381160",
    "end": "2387670"
  },
  {
    "text": "is we're making multiple\ncopies of your model on each accelerator. So if you have a box with eight GPUs",
    "start": "2387670",
    "end": "2396010"
  },
  {
    "text": "and you have data parallel\nand your data parallel degree, maybe you're running eight\ncopies of your model,",
    "start": "2396010",
    "end": "2403780"
  },
  {
    "text": "then each GPU is gonna\nhave one copy of the model. And we'll split the data\nacross all of those,",
    "start": "2403780",
    "end": "2410440"
  },
  {
    "text": "run forward pass, compute the gradients, run the backward pass, then actually constantly rematerialize",
    "start": "2410440",
    "end": "2418630"
  },
  {
    "text": "the version of the model\ninside the accelerators. It's crazy, but that's actually faster",
    "start": "2418630",
    "end": "2424450"
  },
  {
    "text": "than keeping one global copy\nas in the parameter server, again, just because of the communication.",
    "start": "2424450",
    "end": "2430210"
  },
  {
    "text": "So that's data parallel. And then data parallel is\ngreat for large datasets,",
    "start": "2430210",
    "end": "2435369"
  },
  {
    "text": "like when you're working\non many hundreds of, honestly, even more than 50GB",
    "start": "2435370",
    "end": "2442090"
  },
  {
    "text": "data parallel will do a lot for you. But even 100GB, a couple hundred GB,",
    "start": "2442090",
    "end": "2447100"
  },
  {
    "text": "once you get in the terabytes, data parallelism is really critical, and it's a way to just speed up your job.",
    "start": "2447100",
    "end": "2454150"
  },
  {
    "text": "So if your training job\ntakes you a couple days, a couple weeks, I know\nit does for some of you,",
    "start": "2454150",
    "end": "2462700"
  },
  {
    "text": "then data parallelism is a way that you can just speed up your job. You just add more instances\nto this training job config",
    "start": "2462700",
    "end": "2472110"
  },
  {
    "text": "and use a data parallel\ndistribution software and your job just runs faster.",
    "start": "2472330",
    "end": "2477640"
  },
  {
    "text": "It's a beautiful thing, works\nlike a charm. (chuckles) You just add data parallel\nto all of your instances",
    "start": "2477640",
    "end": "2484270"
  },
  {
    "text": "and your job just runs faster. It's just pure math and it's wonderful. So data parallelism is great.",
    "start": "2484270",
    "end": "2489349"
  },
  {
    "text": "But data parallelism isn't enough. So particularly when\nyour large language model",
    "start": "2490450",
    "end": "2497230"
  },
  {
    "text": "doesn't fit on a single GPU\nor a single accelerator, which is the case for really any model",
    "start": "2497230",
    "end": "2505119"
  },
  {
    "text": "that's north of about\none billion parameters, so anything in the six, seven billion parameter family and more,",
    "start": "2505120",
    "end": "2512890"
  },
  {
    "text": "you're gonna want to split\nthat over multiple GPUs, multiple accelerators. So there are two ways\nof thinking about this.",
    "start": "2512890",
    "end": "2518680"
  },
  {
    "text": "One is called pipeline parallelism where we take the layers\nof your neural network and, like, literally put the\nlayers of your neural network",
    "start": "2518680",
    "end": "2525910"
  },
  {
    "text": "on different accelerators. So back to our handy-dandy\nlittle demo here.",
    "start": "2525910",
    "end": "2532950"
  },
  {
    "text": "So let's say I've got this\nGeForce GTX 700, which I do,",
    "start": "2533650",
    "end": "2538650"
  },
  {
    "text": "let's hold it the right\nway this time, here we go. So let's say I've got this\nhandy-dandy accelerator",
    "start": "2538720",
    "end": "2545020"
  },
  {
    "text": "and I have two GPUs. In a data parallel world,\nI might have two copies,",
    "start": "2545020",
    "end": "2551110"
  },
  {
    "text": "I have copy one, and copy two. And then I'm running my\nforward and my backward pass",
    "start": "2551110",
    "end": "2556480"
  },
  {
    "text": "and I'm splitting the data. So half the data sees is GPU One, half the data it sees is GPU Two",
    "start": "2556480",
    "end": "2562570"
  },
  {
    "text": "and I'm averaging the gradients. And so I get one model at the end, which is the exact weighted\naverage of GPU One and GPU Two.",
    "start": "2562570",
    "end": "2571890"
  },
  {
    "text": "If you have doubts\nabout this, don't worry. The Adam optimizer itself is\njust an averager in any case.",
    "start": "2572050",
    "end": "2578890"
  },
  {
    "text": "So commutative property\nis great. (chuckles) Whether you're averaging\nacross batches or across GPUs,",
    "start": "2578890",
    "end": "2585430"
  },
  {
    "text": "the difference is the same. So that's data parallelism. In a model parallel world,",
    "start": "2585430",
    "end": "2591849"
  },
  {
    "text": "my model is too big to fit\non a single one of these so I'm gonna take layer one",
    "start": "2591850",
    "end": "2597130"
  },
  {
    "text": "and then layer two goes over here. So I might have half of my neural network sitting on one accelerator",
    "start": "2597130",
    "end": "2603640"
  },
  {
    "text": "and then I have the other\nhalf of my neural network sitting on my second accelerator. So that's pipeline parallelism.",
    "start": "2603640",
    "end": "2609700"
  },
  {
    "text": "Pipeline parallelism takes\nlayer one and layer two and then runs a communication framework.",
    "start": "2609700",
    "end": "2616330"
  },
  {
    "text": "So my data first goes into GPU\nOne, I get my forward pass; then it goes into GPU Two,\nI get my forward pass here;",
    "start": "2616330",
    "end": "2625599"
  },
  {
    "text": "and then I compute the loss and then I do the backward\npass on both of those.",
    "start": "2625600",
    "end": "2631810"
  },
  {
    "text": "The second type of parallelism\nis called tensor parallelism. Tensor parallelism basically goes",
    "start": "2631810",
    "end": "2637630"
  },
  {
    "text": "one layer deeper than that,\nwhich is if I have one, or however many, really\nlarge attention heads,",
    "start": "2637630",
    "end": "2644110"
  },
  {
    "text": "let's say I have a\nmulti-head attention block that is so large, I need two GPUs.",
    "start": "2644110",
    "end": "2651700"
  },
  {
    "text": "Maybe this whole box here,\nthis whole little card,",
    "start": "2651700",
    "end": "2656700"
  },
  {
    "text": "this set of two cards, this might just be one layer\nin my neural network, right?",
    "start": "2657730",
    "end": "2663040"
  },
  {
    "text": "If I have a huge gigantic attention head that's parked on two of these, then I'm using tensor parallelism.",
    "start": "2663040",
    "end": "2670060"
  },
  {
    "text": "So just know that there are lots of ways of splitting your model. And one way of splitting your model",
    "start": "2670060",
    "end": "2676809"
  },
  {
    "text": "is called pipeline parallelism and the larger way is\ncalled tensor parallelism.",
    "start": "2676810",
    "end": "2681130"
  },
  {
    "text": "So SageMaker model parallel is a fully managed\ndistributed training library that you can use to easily\npartition your own models.",
    "start": "2682540",
    "end": "2690880"
  },
  {
    "text": "So you can use SageMaker model parallel to easily point to, again, PyTorch.",
    "start": "2690880",
    "end": "2697980"
  },
  {
    "text": "And we stabilize the GPU\nutilization for you actually. So you don't even have to, like, grapple",
    "start": "2698020",
    "end": "2705400"
  },
  {
    "text": "with the pipeline and tensor parallelism if you don't want to. We have automatic plugins to find the best partitioning\nstrategy for you.",
    "start": "2705400",
    "end": "2714220"
  },
  {
    "text": "And then we actually\ninterleave the execution pass. So we'll execute the forward\nand the backward pass",
    "start": "2714220",
    "end": "2722050"
  },
  {
    "text": "on different accelerators\nat different points in time to give you a really nice and stable GPU",
    "start": "2722050",
    "end": "2728695"
  },
  {
    "text": "or accelerator utilization, helping you scale more effectively.",
    "start": "2728696",
    "end": "2733753"
  },
  {
    "text": "Obviously runs nicely on SageMaker and, yeah, works nicely with PyTorch.",
    "start": "2734950",
    "end": "2741823"
  },
  {
    "text": "So again, SageMaker model\nparallel, to split this visually, splits your model over\nmultiple accelerators.",
    "start": "2742900",
    "end": "2749380"
  },
  {
    "text": "So literally, we'll take your minibatch and we'll split your\nminibatch into microbatches.",
    "start": "2749380",
    "end": "2755980"
  },
  {
    "text": "So if your global batch size is eight",
    "start": "2755980",
    "end": "2760960"
  },
  {
    "text": "and your per GPU batch size is two, that would mean your\nmicrobatch size is two, right?",
    "start": "2762160",
    "end": "2768340"
  },
  {
    "text": "So microbatch is the\nsame thing as a per GPU. And so what's interesting is let's say,",
    "start": "2768340",
    "end": "2774880"
  },
  {
    "text": "again, here I have my four GPUs and I have one layer per accelerator,",
    "start": "2774880",
    "end": "2780220"
  },
  {
    "text": "so layer one, layer two,\nlayer three, layer four, and then I'm piping in my data from S3.",
    "start": "2780220",
    "end": "2787483"
  },
  {
    "text": "I can execute these at\ndifferent points in time. You can use this algorithm\nin this framework",
    "start": "2788470",
    "end": "2794890"
  },
  {
    "text": "to just easily execute\nonly sort of the forward",
    "start": "2794890",
    "end": "2799890"
  },
  {
    "text": "or the backward pass at a time that keeps the overall GPU utilization more stable.",
    "start": "2800260",
    "end": "2806590"
  },
  {
    "text": "So this minimizes idle\ntime on the accelerators and is eventually much\nmore cost effective.",
    "start": "2806590",
    "end": "2812323"
  },
  {
    "text": "And then we went and\ndeveloped something else. So this other project from\nlast year is called MiCS.",
    "start": "2814090",
    "end": "2821440"
  },
  {
    "text": "It's from our Amazon Science team, who like to minimize the\ncommunication scale on the cloud.",
    "start": "2821440",
    "end": "2826930"
  },
  {
    "text": "And in particular this hits a\n99% linear scaling efficiency,",
    "start": "2826930",
    "end": "2831930"
  },
  {
    "text": "and a jump from 128 to 512 GPUs, and is almost three times\nfaster than DeepSpeed",
    "start": "2832690",
    "end": "2840340"
  },
  {
    "text": "because we used something called hierarchical GPU communication. So MiCS develops a number of technologies,",
    "start": "2840340",
    "end": "2846670"
  },
  {
    "text": "and the most impactful one is a novel GPU communication framework",
    "start": "2846670",
    "end": "2854490"
  },
  {
    "text": "that first communicates\ninside of data parallel groups and then communicates\nacross data parallel groups.",
    "start": "2854860",
    "end": "2861520"
  },
  {
    "text": "So basically, a look\ninside of the model first to communicate and then\naggregate across models.",
    "start": "2861520",
    "end": "2868120"
  },
  {
    "text": "This is useful because, again, it reduces the total number\nof communication hops,",
    "start": "2868120",
    "end": "2873760"
  },
  {
    "text": "letting you just be so much\nfaster with your model training. So how do you get started",
    "start": "2873760",
    "end": "2879040"
  },
  {
    "text": "with SageMaker distributed training? We have a lot of example notebooks. So we have example notebooks on GitHub",
    "start": "2879040",
    "end": "2884890"
  },
  {
    "text": "that we're gonna look at in just a minute. You can also just plug in to our distributed training\nlibraries as your back-end.",
    "start": "2884890",
    "end": "2893380"
  },
  {
    "text": "So especially SageMaker DDP, you can easily point to this\nfrom Hugging Face Accelerates,",
    "start": "2893380",
    "end": "2900730"
  },
  {
    "text": "from PyTorch through a\nnumber of other options, and then you can keep all of your same",
    "start": "2900730",
    "end": "2905799"
  },
  {
    "text": "distributed training software and just set SMDDP as your back-end and enable it in the job config",
    "start": "2905800",
    "end": "2913029"
  },
  {
    "text": "and then you don't have to, like, rewrite all of your code, for example. You can also just add\nit to your Docker files.",
    "start": "2913030",
    "end": "2918790"
  },
  {
    "text": "So you can use the AWS\nDeep Learning Containers and then add the SageMaker DDP,",
    "start": "2918790",
    "end": "2925423"
  },
  {
    "text": "or SageMaker model\nparallel, I believe, code to that Docker container,\nbuild it and use it,",
    "start": "2926500",
    "end": "2932500"
  },
  {
    "text": "and, yeah, then you get\nthe performance boosts. And with that, let's\ntake a look at the demo.",
    "start": "2932500",
    "end": "2938593"
  },
  {
    "text": "So in this notebook, we are\ngoing to pretrain GPT-2. That is correct.",
    "start": "2939970",
    "end": "2945273"
  },
  {
    "text": "We're gonna train a 30 billion\nparameter GPT-2-based model",
    "start": "2945273",
    "end": "2950190"
  },
  {
    "text": "here in this very notebook. This is gonna be on SageMaker. And if you like, you are\nwelcome to follow along with me.",
    "start": "2951100",
    "end": "2959110"
  },
  {
    "text": "So for those of you who are familiar with these SageMaker examples, this is a notebook from\nthe examples repository.",
    "start": "2959110",
    "end": "2966819"
  },
  {
    "text": "We're in this handy training\ndirectory in particular inside of distributed training.",
    "start": "2966820",
    "end": "2972010"
  },
  {
    "text": "So this includes some PyTorch\nand TensorFlow examples, especially in model and data parallel,",
    "start": "2972010",
    "end": "2978700"
  },
  {
    "text": "and you'll see we're gonna work on GPT-2. So feel free to use the QR code\nif you like to get the link.",
    "start": "2978700",
    "end": "2986410"
  },
  {
    "text": "Otherwise, you can go\nstraight to the Bitly sm-nb-4.",
    "start": "2986410",
    "end": "2991260"
  },
  {
    "text": "So let's check this thing out here. All right.",
    "start": "2992110",
    "end": "2998230"
  },
  {
    "text": "So as promised, here is the\ntraining GPT-2 notebook.",
    "start": "2998230",
    "end": "3003063"
  },
  {
    "text": "If you're running this\nat home, couple pro tips. First off, as usual in SageMaker Studio,",
    "start": "3003960",
    "end": "3010350"
  },
  {
    "text": "start with the smallest instance you can. So this t3.medium should be\na handy place to get rollin'.",
    "start": "3010350",
    "end": "3018830"
  },
  {
    "text": "And then this notebook\nis extremely verbose. The training job is gonna\noutput so many logs.",
    "start": "3019380",
    "end": "3026339"
  },
  {
    "text": "It's actually gonna kill your kernel if you don't turn off the log. So before you run this,",
    "start": "3026340",
    "end": "3032820"
  },
  {
    "text": "scroll down here to the\ntraining job definition. And then where it says smp_estimator.fit,",
    "start": "3032820",
    "end": "3041880"
  },
  {
    "text": "feel free to set wait equal\nFalse and set logs to False.",
    "start": "3041880",
    "end": "3046473"
  },
  {
    "text": "So the logs equal to False will certainly stop\nthe logs from emitting. And then this wait equal False",
    "start": "3049382",
    "end": "3055200"
  },
  {
    "text": "gives you the notebook back actually. So it's not sitting here idle,",
    "start": "3055200",
    "end": "3060960"
  },
  {
    "text": "or rather sitting here in use thinking the Python program is running. So let's take it from the top.",
    "start": "3060960",
    "end": "3067023"
  },
  {
    "text": "So this notebook is, as\npromised, pretraining GPT-2.",
    "start": "3069030",
    "end": "3074030"
  },
  {
    "text": "So first off, you're\nthinking, \"Why not GPT-3? Why not something more recent?\"",
    "start": "3074130",
    "end": "3079590"
  },
  {
    "text": "So as it happens, GPT-3 is a\npretty near proxy to GPT-2.",
    "start": "3079590",
    "end": "3084590"
  },
  {
    "text": "There were very little\nlow-level updates to the model. The biggest update",
    "start": "3086280",
    "end": "3091680"
  },
  {
    "text": "was a 10X increase in size, pretty nearly. And so starting with GPT-2",
    "start": "3091680",
    "end": "3097260"
  },
  {
    "text": "and then increasing the size of the model is a great place to get started. And in particular we're using\nHugging Face Transformers",
    "start": "3097260",
    "end": "3106380"
  },
  {
    "text": "and something called\nSharded Data Parallel. So Sharded Data Parallel\nis a nice framework on AWS",
    "start": "3106380",
    "end": "3113670"
  },
  {
    "text": "that implements a custom\ncommunication collective. So this actually overlaps the\nbackward pass of your model",
    "start": "3113670",
    "end": "3121370"
  },
  {
    "text": "with the GPU communication to give you this hierarchical\nGPU communication",
    "start": "3121950",
    "end": "3128280"
  },
  {
    "text": "that is available through\nthe model parallel library. So we're gonna learn how to do this here.",
    "start": "3128280",
    "end": "3134670"
  },
  {
    "text": "The notebook comes with\nall sorts of things. The entry point here\nis the train.py script.",
    "start": "3134670",
    "end": "3141480"
  },
  {
    "text": "You also have a data pipeline to run on a much larger dataset.",
    "start": "3141480",
    "end": "3146910"
  },
  {
    "text": "So this notebook ships\nwith a tiny subset of data. It's gonna be SST2 dataset.",
    "start": "3146910",
    "end": "3154530"
  },
  {
    "text": "And it is quite small, but you can still use it to\njust get things up and running.",
    "start": "3154530",
    "end": "3160050"
  },
  {
    "text": "And then once you've done that, you're welcome to change that out to the much larger data pipeline",
    "start": "3160050",
    "end": "3166110"
  },
  {
    "text": "that's gonna download the web dataset, which is multiple hundreds of GBs. So it's a pretty hefty job.",
    "start": "3166110",
    "end": "3173520"
  },
  {
    "text": "And then the learning rate scheduler, feel free to take this, copy\nthis, use it, very helpful.",
    "start": "3173520",
    "end": "3179910"
  },
  {
    "text": "And then all sorts of other items.",
    "start": "3179910",
    "end": "3183333"
  },
  {
    "text": "So first, as usual, we'll\ninstall our SageMaker SDK,",
    "start": "3184950",
    "end": "3188672"
  },
  {
    "text": "all the normal configs here. We're grabbing our execution role, pointing to various things.",
    "start": "3189990",
    "end": "3196590"
  },
  {
    "text": "And then, as promised, we're gonna download that SST2 dataset. First, installing the\ntransformers datasets",
    "start": "3196590",
    "end": "3205890"
  },
  {
    "text": "and then the transformers library itself. And then importing from\ndataset, couple functions here,",
    "start": "3205890",
    "end": "3213192"
  },
  {
    "text": "load dataset, load from\ndisc, load the metric. And then this AutoModelForCausalLM,",
    "start": "3213193",
    "end": "3218553"
  },
  {
    "text": "so remember CausalLM stands\nfor causal language model",
    "start": "3219660",
    "end": "3224660"
  },
  {
    "text": "and that refers to the\nleft to right prediction that GPT-based models do. So they have that causal\nlanguage modeling loss",
    "start": "3224670",
    "end": "3232800"
  },
  {
    "text": "and then they're gonna use that left to right autoregressive behavior to predict new tokens and new words.",
    "start": "3232800",
    "end": "3240809"
  },
  {
    "text": "And so in the notebook, you're gonna define some hyperparameters, notably the dataset that you're\nusing, the config for this.",
    "start": "3240810",
    "end": "3248130"
  },
  {
    "text": "I remember in Hugging Face you add these extra parameters. Yes, indeed. I do want to train. I do want to eval.",
    "start": "3248130",
    "end": "3254460"
  },
  {
    "text": "And then we'll load the datasets. And then we'll load the tokenizer.",
    "start": "3254460",
    "end": "3260703"
  },
  {
    "text": "And do some pre-processing\non my humble t3. So we're doing a\npre-processing right here,",
    "start": "3262080",
    "end": "3269162"
  },
  {
    "text": "using this mapper. All right, and then we're\nsetting these as hyperparameters.",
    "start": "3270780",
    "end": "3278093"
  },
  {
    "text": "And then we're putting them back in S3. So remember, your handy command here is always just AWS s3 copy.",
    "start": "3282142",
    "end": "3289830"
  },
  {
    "text": "So fun thing about notebooks is that the Boto3 and the AWS\nCLI library is already set.",
    "start": "3289830",
    "end": "3297440"
  },
  {
    "text": "So you can always do\nsomething like an aws s3 ls and then this will just show\nyou what notebooks your,",
    "start": "3298020",
    "end": "3305520"
  },
  {
    "text": "or rather what buckets, what S3 buckets your\nnotebook can point to.",
    "start": "3307980",
    "end": "3313920"
  },
  {
    "text": "And then you can of course copy, and you can copy two and copy\nfrom all sorts of things.",
    "start": "3313920",
    "end": "3321380"
  },
  {
    "text": "Great, so that's your s3 ls. And then here we're storing\nthese just as variables.",
    "start": "3323040",
    "end": "3331020"
  },
  {
    "text": "Great, so we've got our\nS3 training location and then our validation dataset.",
    "start": "3331020",
    "end": "3336093"
  },
  {
    "text": "Later on in this YouTube series we're gonna show you how\nto use FSx for Lustre.",
    "start": "3337560",
    "end": "3342597"
  },
  {
    "text": "And so we'll set up FSx for Lustre and then actually run this\nsame notebook actually",
    "start": "3342597",
    "end": "3348870"
  },
  {
    "text": "on FSx for Lustre. So that's gonna be in\nthe next YouTube video. But this one just focuses on the notebook",
    "start": "3348870",
    "end": "3354839"
  },
  {
    "text": "and letting you understand how to use this notebook appropriately.",
    "start": "3354840",
    "end": "3358083"
  },
  {
    "text": "Yeah, and so with Lustre, you have all these other\nlovely configs you get to set and we'll learn how to do\nthat in the next session.",
    "start": "3362160",
    "end": "3369570"
  },
  {
    "text": "And here's some hyperparameters. So our max steps, seed,\nfacts that we're using, bf16,",
    "start": "3370680",
    "end": "3378920"
  },
  {
    "text": "and onward, and then the model config. So unless you have access\nto 16 p4d instances",
    "start": "3383370",
    "end": "3391280"
  },
  {
    "text": "in your account right now, then you're gonna wanna\npick a smaller GPT-2 config.",
    "start": "3392310",
    "end": "3398250"
  },
  {
    "text": "I do have 16 p4d, so I'm gonna show you myself launching on that\njust so you can see it",
    "start": "3398250",
    "end": "3404099"
  },
  {
    "text": "'cause it's really exciting. But if not, then feel free\nto start with this one.",
    "start": "3404100",
    "end": "3411059"
  },
  {
    "text": "So most of you should start with either the GPT-2\nxl or the GPT-2 small,",
    "start": "3411060",
    "end": "3417119"
  },
  {
    "text": "and then you'll update\nthe configs down below,",
    "start": "3417120",
    "end": "3422120"
  },
  {
    "text": "and I'll show you how to do that. Great, yeah, so more information.",
    "start": "3422160",
    "end": "3428400"
  },
  {
    "text": "Oh, excellent. Okay, so remember the world size here, that just refers to the total\nnumber of GPUs or accelerators",
    "start": "3428400",
    "end": "3435030"
  },
  {
    "text": "and your overall cluster. So if you have one p4d, then\nyou have a world size of eight.",
    "start": "3435030",
    "end": "3442430"
  },
  {
    "text": "If you have two p4ds, you\nhave a world size of 16, and so on and so forth.",
    "start": "3443400",
    "end": "3448890"
  },
  {
    "text": "And so the recommendation here is that for GPT-2 with\n30 billion parameters,",
    "start": "3448890",
    "end": "3454170"
  },
  {
    "text": "we're actually saying you need\nat least 16 p4d instances. And then for GPT-2 xl,",
    "start": "3454170",
    "end": "3461369"
  },
  {
    "text": "you're gonna want at least one p4d. And for GPT-2 small, then you can get away with a p3.16 xl.",
    "start": "3461370",
    "end": "3470643"
  },
  {
    "text": "All right, so make any\nchanges that you need to make based on the recommendations up here. I'm gonna leave this at 16 times 8",
    "start": "3473100",
    "end": "3482070"
  },
  {
    "text": "to give us that world size. And then for fun, we'll\nset the base job name,",
    "start": "3482070",
    "end": "3488309"
  },
  {
    "text": "not necessary, but handy. And then the Lustre configs that we'll set",
    "start": "3488310",
    "end": "3494660"
  },
  {
    "text": "in the next video. And then here we're gonna\ncreate the PyTorch estimator. So this is, again, a wrapper around the,",
    "start": "3494660",
    "end": "3503340"
  },
  {
    "text": "both the PyTorch Deep Learning Container, which is the base image,\nand the training job API.",
    "start": "3503340",
    "end": "3510560"
  },
  {
    "text": "So we're invoking SageMaker train model create a training job.",
    "start": "3510570",
    "end": "3516000"
  },
  {
    "text": "And then we're passing all of this stuff. So we're passing the main entry_point.",
    "start": "3516000",
    "end": "3521640"
  },
  {
    "text": "So this script here. And then the source directory, you can set that to really\nanywhere referential",
    "start": "3521640",
    "end": "3529929"
  },
  {
    "text": "to this object, to this path. We're setting it as the\nworking directory here.",
    "start": "3529929",
    "end": "3536490"
  },
  {
    "text": "And then SageMaker will look\ninside of the source directory for your entry point.",
    "start": "3536490",
    "end": "3542400"
  },
  {
    "text": "It will also look for a requirements.txt. Then we're gonna pass our instance type,",
    "start": "3542400",
    "end": "3548520"
  },
  {
    "text": "instance count, SageMaker session. And then this bundle of joy here, so the distribution path here,",
    "start": "3548520",
    "end": "3558470"
  },
  {
    "text": "so we're setting our MPI,\nmessage passing interface.",
    "start": "3558660",
    "end": "3563660"
  },
  {
    "text": "So that's a sort of base library just to communicate across instances,",
    "start": "3563820",
    "end": "3568829"
  },
  {
    "text": "it enables distributed\ncomputing generally. And then we're telling MPI,",
    "start": "3568830",
    "end": "3573877"
  },
  {
    "text": "\"Hey, here's how many processes per host,\" so eight GPUs per instance up there.",
    "start": "3573877",
    "end": "3582090"
  },
  {
    "text": "And then those other custom MPI options that we passed actually just to sort of set the\nMPI config to work nicely",
    "start": "3582090",
    "end": "3590309"
  },
  {
    "text": "with the SageMaker distributed\ntraining framework. And then here are the SM\ndistributed parameters.",
    "start": "3590310",
    "end": "3597869"
  },
  {
    "text": "So again, in this case,\nwe're using model parallel because we want to shard the\nmodel across multiple GPUs.",
    "start": "3597870",
    "end": "3605700"
  },
  {
    "text": "And in fact, again, we're using that Sharded\nData Parallel framework. And so it's a little confusing,",
    "start": "3605700",
    "end": "3611352"
  },
  {
    "text": "but Sharded Data Parallel is\nindeed a part of model parallel because it's a way to\nwork with large models.",
    "start": "3611352",
    "end": "3616946"
  },
  {
    "text": "And so we're just enabling this. As we say, model parallel is enabled.",
    "start": "3616947",
    "end": "3621963"
  },
  {
    "text": "DDP, so DDP is distributed data parallel, and so we're just telling the job here",
    "start": "3622830",
    "end": "3629100"
  },
  {
    "text": "that we do indeed need to use\na data parallel framework. And so that is useful for having",
    "start": "3629100",
    "end": "3635790"
  },
  {
    "text": "multiple copies of the model\nin your overall cluster. If you have just one copy of the model,",
    "start": "3635790",
    "end": "3641880"
  },
  {
    "text": "DDP would be set to false. You might be doing that for fine-tuning. But certainly if you're pretraining",
    "start": "3641880",
    "end": "3647789"
  },
  {
    "text": "on a very large cluster size, then you'll want to set DDP to true.",
    "start": "3647790",
    "end": "3655020"
  },
  {
    "text": "Couple other parameters here. Again, bf16 is a helpful\ndata representation",
    "start": "3655020",
    "end": "3661289"
  },
  {
    "text": "that increases stability in pretraining and large scale training.",
    "start": "3661290",
    "end": "3665523"
  },
  {
    "text": "All right, framework versions. So we're running on PyTorch 1.13,",
    "start": "3667170",
    "end": "3673320"
  },
  {
    "text": "which obviously you can update this. So certainly you can use a\nmore recent version of PyTorch.",
    "start": "3673320",
    "end": "3680100"
  },
  {
    "text": "This is the one that we've\nused for this example, and then you're setting the\nversion of Python here as well.",
    "start": "3680100",
    "end": "3685560"
  },
  {
    "text": "You can also skip this and\npoint to just a base image. So if you have your own\ndeep learning container,",
    "start": "3685560",
    "end": "3691410"
  },
  {
    "text": "your own image that you'd\nlike to use as a base, you're welcome to send that here as well.",
    "start": "3691410",
    "end": "3696780"
  },
  {
    "text": "Then we're gonna point\nto S3 on the output side. So that is where the model will go",
    "start": "3696780",
    "end": "3703350"
  },
  {
    "text": "at the completion of training. And then we're setting checkpoints. So you can set your\ncheckpoints to either S3",
    "start": "3703350",
    "end": "3711660"
  },
  {
    "text": "or it'll default to FSx for\nLustre, if Lustre is there. And then checkpointing is useful",
    "start": "3711660",
    "end": "3717900"
  },
  {
    "text": "because if and when your job fails, you want to, of course, capture\nthe most recent checkpoint",
    "start": "3717900",
    "end": "3725550"
  },
  {
    "text": "and then restart from that checkpoint in the next version of your job. So checkpointing is useful.",
    "start": "3725550",
    "end": "3731163"
  },
  {
    "text": "All of this we'll dive into more detail throughout the rest of the course. And then metric definitions,\nhyperparameters,",
    "start": "3732030",
    "end": "3738750"
  },
  {
    "text": "so on and so forth, great. And then we'll call\nestimator.fit, model.fit.",
    "start": "3738750",
    "end": "3744540"
  },
  {
    "text": "And then, again, this is spinning\nup those 16 p4d instances,",
    "start": "3744540",
    "end": "3749540"
  },
  {
    "text": "pointing to my data that's in S3. And then remember just set logs to false.",
    "start": "3749940",
    "end": "3755489"
  },
  {
    "text": "And then if you like, you can set wait equal to false as well to give you this notebook back.",
    "start": "3755489",
    "end": "3761370"
  },
  {
    "text": "And so that lets us do,\nagain, more programming.",
    "start": "3761370",
    "end": "3765273"
  },
  {
    "text": "And so now that that's wrapped, let's see if we can check this out. So we'll go back into\nthe SageMaker console,",
    "start": "3766860",
    "end": "3774333"
  },
  {
    "text": "and we'll go out here to Training, and then we'll look at Training Jobs.",
    "start": "3775890",
    "end": "3783240"
  },
  {
    "text": "So we'll click on Training Jobs and then, lo and behold, we do\nhave a couple jobs completed.",
    "start": "3783240",
    "end": "3790650"
  },
  {
    "text": "And then, remember, because this is a small\nsample of the data,",
    "start": "3790650",
    "end": "3796320"
  },
  {
    "text": "because this isn't a web scale dataset, this is just a a small demo,",
    "start": "3796320",
    "end": "3800973"
  },
  {
    "text": "the job completes quite quickly, right? Obviously, 11 to 12 minutes is great.",
    "start": "3802290",
    "end": "3808710"
  },
  {
    "text": "In the real world, if you're actually pretraining\na 30 billion parameter model, that's gonna take a solid\n30 days, maybe more,",
    "start": "3808710",
    "end": "3818660"
  },
  {
    "text": "very likely maybe 45 days. And so, in any case, 10 minutes isn't bad.",
    "start": "3818670",
    "end": "3825213"
  },
  {
    "text": "And so here we can see the job metadata. This is the name of my job. Indeed, it completed just today.",
    "start": "3826290",
    "end": "3834093"
  },
  {
    "text": "Tiny volume size, which is fine, again, because I was\ncheckpointing my model in S3.",
    "start": "3836280",
    "end": "3844993"
  },
  {
    "text": "In other cases, you'll\nwanna just increase this as a function of the model size actually.",
    "start": "3846210",
    "end": "3851223"
  },
  {
    "text": "Yeah, the volume size because this will let you\nstore larger and larger models",
    "start": "3852060",
    "end": "3857740"
  },
  {
    "text": "on the instance itself. Otherwise, you can store\nit on Lustre and then,",
    "start": "3858668",
    "end": "3865230"
  },
  {
    "text": "or you can just save it to S3 directly, in which case the size doesn't matter. And then, yeah, p4d, 24\nxl times 16. (chuckles)",
    "start": "3865230",
    "end": "3874170"
  },
  {
    "text": "So we just passed those\nto the training job and then, again, SageMaker spins up this massive distributed training cluster",
    "start": "3874170",
    "end": "3882450"
  },
  {
    "text": "that lets us train our\nmodel quite effectively. So we'll point to our input data config.",
    "start": "3882450",
    "end": "3889010"
  },
  {
    "text": "So we set this train channel,\nand I'll go back and... Yeah, actually let's just do this now.",
    "start": "3889010",
    "end": "3894750"
  },
  {
    "text": "So the train channel is\ncreated through this thing that's called a data channel.",
    "start": "3894750",
    "end": "3900090"
  },
  {
    "text": "And so the data channel is really a way to send data to your\nmodel in some capacity.",
    "start": "3900090",
    "end": "3907470"
  },
  {
    "text": "And so in this case, we passed an S3 path. Let's see here.",
    "start": "3907470",
    "end": "3913080"
  },
  {
    "text": "So when we set the data channel,\nyeah, which is right here,",
    "start": "3913080",
    "end": "3918080"
  },
  {
    "text": "then let's see if we can just find this. So we'll look for data\nchannel in our notebook here.",
    "start": "3920370",
    "end": "3928983"
  },
  {
    "text": "Yeah, here we go. So that's under the Lustre config. And then there should be\nanother one, not data Chanel.",
    "start": "3930450",
    "end": "3938640"
  },
  {
    "text": "Here we go, data channel. There we go.",
    "start": "3938640",
    "end": "3941942"
  },
  {
    "text": "Hmm, yeah, so if we're not using Lustre, then the data channel is\nreally just a pointer to S3.",
    "start": "3946440",
    "end": "3952650"
  },
  {
    "text": "So this is using this training input, which lets you set other\nconfigs for that pointer in S3",
    "start": "3952650",
    "end": "3960810"
  },
  {
    "text": "or you can just set the path directly. And then it's just this\nlittle object here. And so that goes into your training job.",
    "start": "3960810",
    "end": "3968550"
  },
  {
    "text": "And then let's unpack the script briefly. So out here we'll take\na look at the train.py.",
    "start": "3968550",
    "end": "3976100"
  },
  {
    "text": "So that's this one. So this is the main entry point\nthat we set for this model.",
    "start": "3976410",
    "end": "3984260"
  },
  {
    "text": "And it is a long script. So for those of you\nwho enjoy long scripts, I hope you enjoy this one.",
    "start": "3984270",
    "end": "3990779"
  },
  {
    "text": "For those of you who don't, feel free to go onto to the next video because this one's gonna\nbe a long script here.",
    "start": "3990780",
    "end": "3997290"
  },
  {
    "text": "So let's unpack this. All right, so smdistributed, so again those are the SageMaker",
    "start": "3997290",
    "end": "4003710"
  },
  {
    "text": "distributed training libraries and we're gonna import\nthose into this script.",
    "start": "4003710",
    "end": "4008750"
  },
  {
    "text": "So we're importing them as smp. And so for SageMaker model\nparallel, you can use that",
    "start": "4008750",
    "end": "4015710"
  },
  {
    "text": "sort of as distributed\ntraining software directly.",
    "start": "4015710",
    "end": "4020710"
  },
  {
    "text": "So you can write new models\nthat interact with smp using this smp.step.",
    "start": "4020780",
    "end": "4028640"
  },
  {
    "text": "If you have a different model, like say you have something\nfrom Hugging Face Accelerate",
    "start": "4028640",
    "end": "4034220"
  },
  {
    "text": "and you want, or PyTorch\nFully Sharded Data Parallel, then you can plug into\nour optimized back-ends",
    "start": "4034220",
    "end": "4042800"
  },
  {
    "text": "just by updating a couple\nlines of code actually. But this script, it's\ngonna use smp directly.",
    "start": "4042800",
    "end": "4048230"
  },
  {
    "text": "And then we're using obviously\nPyTorch, Transformers, and then all of those other\nPython scripts in this package.",
    "start": "4048230",
    "end": "4057070"
  },
  {
    "text": "Right, so data pipeline,\nlearning rates, memory tracker, all of those we're importing right here.",
    "start": "4057230",
    "end": "4063070"
  },
  {
    "text": "All right, SMDDP, couple objects here and then let's roll.",
    "start": "4065030",
    "end": "4071723"
  },
  {
    "text": "So the learning rate\nscheduler, handy thing. Make sure you have a\nlearning rate scheduler.",
    "start": "4074000",
    "end": "4079310"
  },
  {
    "text": "@smp.step. So this @smp.step, this decorator here,",
    "start": "4080757",
    "end": "4086450"
  },
  {
    "text": "what this is doing is\nparallelizing everything inside of this function actually.",
    "start": "4086450",
    "end": "4092870"
  },
  {
    "text": "So every time your model takes a step through another minibatch,",
    "start": "4092870",
    "end": "4098150"
  },
  {
    "text": "you can add this decorator\nto distribute that step. So the distribution is mostly useful",
    "start": "4098150",
    "end": "4106560"
  },
  {
    "text": "in the forward and backward pass. In the case of model parallel,",
    "start": "4108200",
    "end": "4113333"
  },
  {
    "text": "it's interesting because the\ndifferent layers in your model are actually gonna be\nput on different GPUs",
    "start": "4114260",
    "end": "4119600"
  },
  {
    "text": "or different cards. So you might have layer one on one card and layer two on another card.",
    "start": "4119600",
    "end": "4126200"
  },
  {
    "text": "And then this smp.step, like, orchestrates the communication across those cards",
    "start": "4126200",
    "end": "4133880"
  },
  {
    "text": "at the same time as sending\nthe minibatches through.",
    "start": "4133880",
    "end": "4138880"
  },
  {
    "text": "So it's gonna send the\nminibatches to those cards, give you the forward pass,",
    "start": "4138890",
    "end": "4144980"
  },
  {
    "text": "and then compute the loss\nrelative to your label data",
    "start": "4144980",
    "end": "4149980"
  },
  {
    "text": "and relative to the waits in those layers, and then it'll backprop that\nloss to those same layers.",
    "start": "4150380",
    "end": "4158929"
  },
  {
    "text": "And so all of that happens right here. And then basically\nSageMaker model parallel",
    "start": "4158930",
    "end": "4165080"
  },
  {
    "text": "is handing that distributed orchestration. So we deal with the, like, tracking,",
    "start": "4165080",
    "end": "4171409"
  },
  {
    "text": "where the different layers\nare on the different GPUs, and keeping the GPU utilization stable",
    "start": "4171410",
    "end": "4177859"
  },
  {
    "text": "while you just get to\nwrite your PyTorch code. So you deal with the PyTorch, and then we deal with the sort\nof distributed orchestration.",
    "start": "4177860",
    "end": "4186980"
  },
  {
    "text": "And so that's the case for the\ntrain step and the test step. And then the model\nevaluation works as well.",
    "start": "4186980",
    "end": "4195143"
  },
  {
    "text": "And then to summarize, 'cause this is actually an\nintroduction to pretraining,",
    "start": "4196970",
    "end": "4202532"
  },
  {
    "text": "let's see here. So there's an arg that's\ncalled .fine_tune,",
    "start": "4203720",
    "end": "4208450"
  },
  {
    "text": "and that's just in this example because the same training script can be used both just to\nfine-tune a GPT-2 model",
    "start": "4209300",
    "end": "4218530"
  },
  {
    "text": "or to literally pretrain it from scratch. And I wanna show you the difference because it's not that big actually.",
    "start": "4218900",
    "end": "4226790"
  },
  {
    "text": "So this is model config, great. So if args.fine_tune, basically\nif you are fine-tuning,",
    "start": "4226790",
    "end": "4235990"
  },
  {
    "text": "actually I don't think that's the one. So basically if you are fine-tuning,",
    "start": "4238201",
    "end": "4246023"
  },
  {
    "text": "then you're going to pick\nup this model, here we go.",
    "start": "4247100",
    "end": "4250043"
  },
  {
    "text": "Where, is this, yeah,\nthis is the one, line 952.",
    "start": "4259040",
    "end": "4264040"
  },
  {
    "text": "So if you are fine-tuning this model, then you're gonna pick\nit up from pretrained.",
    "start": "4264440",
    "end": "4271250"
  },
  {
    "text": "So you'll pick up a pretrained model such as one that's sitting\non your local EFS volume,",
    "start": "4271250",
    "end": "4280180"
  },
  {
    "text": "in which case you'll\npull, or your local disc, so you'll point to args.model_dir",
    "start": "4280370",
    "end": "4286190"
  },
  {
    "text": "which we'll just read\nthat pretrained model. Otherwise, you can download\nit from the Hugging Face Hub",
    "start": "4286190",
    "end": "4294260"
  },
  {
    "text": "using this syntax right here. So just .from_pretrained and\nit'll download this model.",
    "start": "4294260",
    "end": "4300890"
  },
  {
    "text": "If you're not fine-tuning\nand you wanna pretrain because, like me, you think it's cool,",
    "start": "4300890",
    "end": "4306620"
  },
  {
    "text": "then you're gonna load\nfrom config actually. So literally, the biggest\ndifference in the training script",
    "start": "4306620",
    "end": "4313530"
  },
  {
    "text": "is how you're loading the model. In this case, we're gonna load the model from the model config to\npretrain this from scratch.",
    "start": "4314510",
    "end": "4324110"
  },
  {
    "text": "I will say the tokenization\nis different for pretraining than it is for fine-tuning.",
    "start": "4324110",
    "end": "4329660"
  },
  {
    "text": "Typically, you'll use\nslightly different tokens because instead of doing, say, BERT",
    "start": "4329660",
    "end": "4336990"
  },
  {
    "text": "or normal GPT-2 just generation, you need to actually do\nthe masking or do the CLM.",
    "start": "4338420",
    "end": "4345770"
  },
  {
    "text": "So the tokenization is different, but the actual training script is literally, nearly identical.",
    "start": "4345770",
    "end": "4353810"
  },
  {
    "text": "And so that's how to get\nstarted with pretraining. And then let me show you\nthe logs to this job.",
    "start": "4353810",
    "end": "4361640"
  },
  {
    "text": "So here we pointed to our train channel, so again that's in S3, our test channel.",
    "start": "4361640",
    "end": "4368240"
  },
  {
    "text": "And then the model checkpoints get uploaded to S3 on completion.",
    "start": "4368240",
    "end": "4372413"
  },
  {
    "text": "And remember, so SageMaker training captures all of the\nhyperparameters for you. It's capturing the number\nof epochs we go through,",
    "start": "4373370",
    "end": "4381170"
  },
  {
    "text": "the fact that we're using flash attention, the bf16, the number of heads,",
    "start": "4381170",
    "end": "4388370"
  },
  {
    "text": "the number of checkpoints we're storing, the number of layers, that we\nwanna save the final model,",
    "start": "4388370",
    "end": "4394789"
  },
  {
    "text": "yeah, just set that tool one\nif you do wanna save the model, which most of you will wanna do. And then here we go, so\nwe're gonna view the logs.",
    "start": "4394790",
    "end": "4403880"
  },
  {
    "text": "So when your jobs run for longer, these graphs will be more exciting.",
    "start": "4403880",
    "end": "4409190"
  },
  {
    "text": "In this case, the jobs\nran for just a few steps, so we're barely seeing\nanything in the charts,",
    "start": "4409190",
    "end": "4417050"
  },
  {
    "text": "which is fine 'cause we're\ngonna view it in the logs. So now we're gonna go out to CloudWatch.",
    "start": "4417050",
    "end": "4422063"
  },
  {
    "text": "And in CloudWatch, we're\nable to see the logs for, of course, all of our AWS resources",
    "start": "4424370",
    "end": "4431540"
  },
  {
    "text": "and in particular our\nSageMaker training jobs. And so this is nice\nbecause you can see yes,",
    "start": "4431540",
    "end": "4437270"
  },
  {
    "text": "indeed, I do have 16 instances. And so each instance has\nits own CloudWatch stream.",
    "start": "4437270",
    "end": "4445327"
  },
  {
    "text": "So here we'll see these 16 instances.",
    "start": "4445700",
    "end": "4450047"
  },
  {
    "text": "And then let's start with the top one. So we'll start at the top one and then...",
    "start": "4450950",
    "end": "4458950"
  },
  {
    "text": "I'm gonna scroll up here just a bit. What I'm looking for right\nnow, more than anything,",
    "start": "4460160",
    "end": "4465920"
  },
  {
    "text": "is to make sure that I have\nthe right instance actually.",
    "start": "4465920",
    "end": "4470920"
  },
  {
    "text": "So you see it says\nalgo-1, algo-2, and so on. That actually refers to the...",
    "start": "4471020",
    "end": "4478400"
  },
  {
    "text": "Right, the instances is\njust denoted by this algo-2. And all of the non-leader nodes,",
    "start": "4478400",
    "end": "4485390"
  },
  {
    "text": "they'll all point to the primary node, and you'll see which one typically.",
    "start": "4485390",
    "end": "4494543"
  },
  {
    "text": "So in any case, they're\npointing to the main one, which can be algo-1.",
    "start": "4500090",
    "end": "4506723"
  },
  {
    "text": "Great, okay. So they're pointing to the main.",
    "start": "4509960",
    "end": "4515030"
  },
  {
    "text": "And then we can go to the\ntop and we can display,",
    "start": "4515030",
    "end": "4520030"
  },
  {
    "text": "we can view in plain\ntext, which is quite nice, and then you can search",
    "start": "4520580",
    "end": "4526040"
  },
  {
    "text": "for different points\nin time in CloudWatch. So you can go to a custom display",
    "start": "4526040",
    "end": "4531350"
  },
  {
    "text": "and it's going to look\nfor a moment in time. So you can say an\nabsolute day, for example.",
    "start": "4531350",
    "end": "4538460"
  },
  {
    "text": "So if you ran this job in May, then you'll actually, like,\npick the date and the time",
    "start": "4538460",
    "end": "4545119"
  },
  {
    "text": "and you'll sort of set this\nwindow to do the search. Otherwise, you can set it\nrelative to where you are now",
    "start": "4545120",
    "end": "4551150"
  },
  {
    "text": "and then you'll go back and then that will bring the logs up. But if you just run it,\nit's quite easy to see.",
    "start": "4551150",
    "end": "4557659"
  },
  {
    "text": "And so these logs here, the\nfirst thing your job is gonna do",
    "start": "4557660",
    "end": "4562660"
  },
  {
    "text": "is just pip install everything\nin that requirements.txt.",
    "start": "4563030",
    "end": "4568030"
  },
  {
    "text": "So it's gonna pip install\neverything in that list. And again, you can avoid that\nby prebuilding a container",
    "start": "4568160",
    "end": "4576050"
  },
  {
    "text": "and then just using the image in your job, otherwise you can just put\neverything in this list here",
    "start": "4576050",
    "end": "4581327"
  },
  {
    "text": "and the requirements. So it downloads it, and\nthen we run the job.",
    "start": "4581327",
    "end": "4587663"
  },
  {
    "text": "Oh, one other point to\ncall out here is that, so SageMaker will print the\nenvironment variables actually.",
    "start": "4588800",
    "end": "4597173"
  },
  {
    "text": "Most commonly, it'll print\nthe environment variables, which tells you where\nall of the resources are",
    "start": "4598100",
    "end": "4604640"
  },
  {
    "text": "and the different environment\nvariables that you were using. And then once you see that,",
    "start": "4604640",
    "end": "4610579"
  },
  {
    "text": "it's a handy way to get sort of oriented with how the framework works.",
    "start": "4610580",
    "end": "4616730"
  },
  {
    "text": "And so with that, I hope you\nenjoyed this introduction to pretraining your own large models.",
    "start": "4616730",
    "end": "4625239"
  },
  {
    "text": "In the next video, we're gonna learn how to set up FSx for Lustre and then how to actually\ntrain by using FSx for Lustre.",
    "start": "4625240",
    "end": "4634580"
  },
  {
    "text": "So we'll check that out\nin the very next video. Thanks.",
    "start": "4634580",
    "end": "4638743"
  }
]