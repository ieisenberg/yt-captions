[
  {
    "text": "hello and welcome to a new video on",
    "start": "1020",
    "end": "3179"
  },
  {
    "text": "Amazon comprehend or as we call it",
    "start": "3179",
    "end": "5460"
  },
  {
    "text": "comprehend video snacks",
    "start": "5460",
    "end": "7859"
  },
  {
    "text": "Amazon comprehend is a fully managed and",
    "start": "7859",
    "end": "10500"
  },
  {
    "text": "continuously trained AI service from",
    "start": "10500",
    "end": "12599"
  },
  {
    "text": "Amazon web services that can derive and",
    "start": "12599",
    "end": "15000"
  },
  {
    "text": "understand valuable insights from text",
    "start": "15000",
    "end": "17220"
  },
  {
    "text": "within documents",
    "start": "17220",
    "end": "19320"
  },
  {
    "text": "this is a two-part video series where we",
    "start": "19320",
    "end": "22199"
  },
  {
    "text": "will talk about a new feature in Amazon",
    "start": "22199",
    "end": "23939"
  },
  {
    "text": "comprehend which will simplify document",
    "start": "23939",
    "end": "26519"
  },
  {
    "text": "classification and entity extraction for",
    "start": "26519",
    "end": "29820"
  },
  {
    "text": "intelligent document processing",
    "start": "29820",
    "end": "31320"
  },
  {
    "text": "workflows",
    "start": "31320",
    "end": "32520"
  },
  {
    "text": "my name is Rick talibder and I'm a",
    "start": "32520",
    "end": "35399"
  },
  {
    "text": "senior language architect with the",
    "start": "35399",
    "end": "36840"
  },
  {
    "text": "Amazon comprehend product team and I'll",
    "start": "36840",
    "end": "39059"
  },
  {
    "text": "be joined today by Andrew and biswas a",
    "start": "39059",
    "end": "41280"
  },
  {
    "text": "senior specialist Solutions architect in",
    "start": "41280",
    "end": "43680"
  },
  {
    "text": "the intelligent document processing team",
    "start": "43680",
    "end": "47340"
  },
  {
    "text": "companies spend significant time and",
    "start": "47340",
    "end": "49500"
  },
  {
    "text": "effort manually or digitally",
    "start": "49500",
    "end": "51180"
  },
  {
    "text": "pre-processing documents to make them",
    "start": "51180",
    "end": "53579"
  },
  {
    "text": "usable for their applications",
    "start": "53579",
    "end": "56039"
  },
  {
    "text": "Legacy document processing can work fine",
    "start": "56039",
    "end": "58379"
  },
  {
    "text": "for pristine documents but when document",
    "start": "58379",
    "end": "61020"
  },
  {
    "text": "quality varies the performance of Legacy",
    "start": "61020",
    "end": "63780"
  },
  {
    "text": "systems does not meet business needs",
    "start": "63780",
    "end": "67020"
  },
  {
    "text": "documents have different formats types",
    "start": "67020",
    "end": "69720"
  },
  {
    "text": "and layouts manually processing these",
    "start": "69720",
    "end": "72360"
  },
  {
    "text": "makes it a time consuming error prone",
    "start": "72360",
    "end": "74760"
  },
  {
    "text": "and costly process on top for customers",
    "start": "74760",
    "end": "78060"
  },
  {
    "text": "with high and low seasons and extra",
    "start": "78060",
    "end": "80640"
  },
  {
    "text": "complication is managing the stuffing",
    "start": "80640",
    "end": "82920"
  },
  {
    "text": "level to make sure the document pipeline",
    "start": "82920",
    "end": "85380"
  },
  {
    "text": "capacity doesn't slow down the ability",
    "start": "85380",
    "end": "87960"
  },
  {
    "text": "to grow",
    "start": "87960",
    "end": "89340"
  },
  {
    "text": "the most important factor is the ability",
    "start": "89340",
    "end": "91979"
  },
  {
    "text": "to automate the identification of",
    "start": "91979",
    "end": "94140"
  },
  {
    "text": "documents and get the key information",
    "start": "94140",
    "end": "96240"
  },
  {
    "text": "from them for business decision making",
    "start": "96240",
    "end": "99360"
  },
  {
    "text": "for example in the insurance industry in",
    "start": "99360",
    "end": "102420"
  },
  {
    "text": "a claims package the key is finding the",
    "start": "102420",
    "end": "105240"
  },
  {
    "text": "few dozen most impactful data points",
    "start": "105240",
    "end": "108000"
  },
  {
    "text": "that primarily Drive the claim approval",
    "start": "108000",
    "end": "110700"
  },
  {
    "text": "process",
    "start": "110700",
    "end": "111860"
  },
  {
    "text": "overall the biggest impact on customers",
    "start": "111860",
    "end": "115020"
  },
  {
    "text": "is the ability to make high quality",
    "start": "115020",
    "end": "117720"
  },
  {
    "text": "decisions faster based on accurate",
    "start": "117720",
    "end": "120360"
  },
  {
    "text": "information",
    "start": "120360",
    "end": "121439"
  },
  {
    "text": "speeding up this cycle allows you to",
    "start": "121439",
    "end": "124439"
  },
  {
    "text": "serve more customers and redeploy people",
    "start": "124439",
    "end": "127140"
  },
  {
    "text": "to higher value tasks",
    "start": "127140",
    "end": "130819"
  },
  {
    "text": "as per Gartner intelligent document",
    "start": "130860",
    "end": "133260"
  },
  {
    "text": "processing Solutions extract data to",
    "start": "133260",
    "end": "136140"
  },
  {
    "text": "support automation of high volume",
    "start": "136140",
    "end": "137580"
  },
  {
    "text": "repetitive document processing tasks for",
    "start": "137580",
    "end": "140580"
  },
  {
    "text": "analysis and insight",
    "start": "140580",
    "end": "142500"
  },
  {
    "text": "intelligent document processing",
    "start": "142500",
    "end": "144180"
  },
  {
    "text": "Solutions uses natural language",
    "start": "144180",
    "end": "146040"
  },
  {
    "text": "Technologies and computer vision to",
    "start": "146040",
    "end": "148739"
  },
  {
    "text": "extract data from structured and",
    "start": "148739",
    "end": "150780"
  },
  {
    "text": "unstructured content especially from",
    "start": "150780",
    "end": "153120"
  },
  {
    "text": "documents to support Automation and",
    "start": "153120",
    "end": "155819"
  },
  {
    "text": "augmentation",
    "start": "155819",
    "end": "157319"
  },
  {
    "text": "in today's world it is highly critical",
    "start": "157319",
    "end": "159780"
  },
  {
    "text": "for businesses to stay competitive the",
    "start": "159780",
    "end": "162660"
  },
  {
    "text": "rapid Innovation and transformation with",
    "start": "162660",
    "end": "164580"
  },
  {
    "text": "cloud-based Technologies is touching",
    "start": "164580",
    "end": "166800"
  },
  {
    "text": "every industry verticals",
    "start": "166800",
    "end": "168840"
  },
  {
    "text": "inclusion document processing is one",
    "start": "168840",
    "end": "171480"
  },
  {
    "text": "such area which is inevitably becoming",
    "start": "171480",
    "end": "173400"
  },
  {
    "text": "essential for every business",
    "start": "173400",
    "end": "176459"
  },
  {
    "text": "Amazon comprehend helps you automate",
    "start": "176459",
    "end": "179220"
  },
  {
    "text": "document processing with no machine",
    "start": "179220",
    "end": "181620"
  },
  {
    "text": "learning experience you can use",
    "start": "181620",
    "end": "183900"
  },
  {
    "text": "classification and extraction",
    "start": "183900",
    "end": "185580"
  },
  {
    "text": "capabilities to rapidly process a",
    "start": "185580",
    "end": "188400"
  },
  {
    "text": "variety of document types and accurately",
    "start": "188400",
    "end": "190800"
  },
  {
    "text": "extract insights to help business",
    "start": "190800",
    "end": "193140"
  },
  {
    "text": "decisions",
    "start": "193140",
    "end": "194459"
  },
  {
    "text": "you can also access capabilities to",
    "start": "194459",
    "end": "197099"
  },
  {
    "text": "detect and protect sensitive data and",
    "start": "197099",
    "end": "199379"
  },
  {
    "text": "help meet compliance requirements",
    "start": "199379",
    "end": "203099"
  },
  {
    "text": "to set the context let's take a look at",
    "start": "203099",
    "end": "205739"
  },
  {
    "text": "an intelligent document processing use",
    "start": "205739",
    "end": "207480"
  },
  {
    "text": "case in the insurance industry",
    "start": "207480",
    "end": "210120"
  },
  {
    "text": "a typical Insurance claim process",
    "start": "210120",
    "end": "211920"
  },
  {
    "text": "involves a claim package that may",
    "start": "211920",
    "end": "214080"
  },
  {
    "text": "contain multiple documents when an",
    "start": "214080",
    "end": "216420"
  },
  {
    "text": "insurance claim is filed it includes",
    "start": "216420",
    "end": "218819"
  },
  {
    "text": "documents like Insurance claim form",
    "start": "218819",
    "end": "221159"
  },
  {
    "text": "incident reports identity documents and",
    "start": "221159",
    "end": "224760"
  },
  {
    "text": "third-party claim documents to name a",
    "start": "224760",
    "end": "226980"
  },
  {
    "text": "few",
    "start": "226980",
    "end": "228060"
  },
  {
    "text": "this step is followed by document",
    "start": "228060",
    "end": "229980"
  },
  {
    "text": "validation to check if necessary",
    "start": "229980",
    "end": "232560"
  },
  {
    "text": "documents are present in the claims",
    "start": "232560",
    "end": "234420"
  },
  {
    "text": "package",
    "start": "234420",
    "end": "235379"
  },
  {
    "text": "followed by information gathering and",
    "start": "235379",
    "end": "237720"
  },
  {
    "text": "cross document validation",
    "start": "237720",
    "end": "239940"
  },
  {
    "text": "Insurance claim representatives and",
    "start": "239940",
    "end": "242400"
  },
  {
    "text": "adjudicators typically spend hundreds of",
    "start": "242400",
    "end": "245459"
  },
  {
    "text": "hours manually sifting sorting and",
    "start": "245459",
    "end": "249000"
  },
  {
    "text": "extracting information from hundreds or",
    "start": "249000",
    "end": "252000"
  },
  {
    "text": "even thousands of claim filing documents",
    "start": "252000",
    "end": "255299"
  },
  {
    "text": "this process can be extremely time",
    "start": "255299",
    "end": "257880"
  },
  {
    "text": "consuming prone to human errors and",
    "start": "257880",
    "end": "260340"
  },
  {
    "text": "doesn't scale",
    "start": "260340",
    "end": "262380"
  },
  {
    "text": "with inclusion document processing a",
    "start": "262380",
    "end": "265199"
  },
  {
    "text": "bulk of this document processing steps",
    "start": "265199",
    "end": "267479"
  },
  {
    "text": "can be automated with minimal human",
    "start": "267479",
    "end": "270060"
  },
  {
    "text": "intervention and faster turnaround times",
    "start": "270060",
    "end": "273419"
  },
  {
    "text": "speeding up the overall Insurance",
    "start": "273419",
    "end": "275100"
  },
  {
    "text": "adjudication process",
    "start": "275100",
    "end": "278400"
  },
  {
    "text": "using Amazon comprehend you can increase",
    "start": "278400",
    "end": "281040"
  },
  {
    "text": "productivity by quickly and accurately",
    "start": "281040",
    "end": "283500"
  },
  {
    "text": "processing and extracting insights from",
    "start": "283500",
    "end": "286380"
  },
  {
    "text": "a variety of document types",
    "start": "286380",
    "end": "288479"
  },
  {
    "text": "you can highly customize your solutions",
    "start": "288479",
    "end": "290580"
  },
  {
    "text": "by categorizing documents and extracting",
    "start": "290580",
    "end": "293639"
  },
  {
    "text": "data that is specific to your business",
    "start": "293639",
    "end": "296040"
  },
  {
    "text": "domain",
    "start": "296040",
    "end": "297600"
  },
  {
    "text": "you can automate your document",
    "start": "297600",
    "end": "299460"
  },
  {
    "text": "processing Pipeline and manage machine",
    "start": "299460",
    "end": "301860"
  },
  {
    "text": "learning models at scale with no prior",
    "start": "301860",
    "end": "304979"
  },
  {
    "text": "machine learning expertise",
    "start": "304979",
    "end": "307259"
  },
  {
    "text": "you can also discover and protect",
    "start": "307259",
    "end": "309419"
  },
  {
    "text": "personally identifiable information or",
    "start": "309419",
    "end": "312240"
  },
  {
    "text": "pii in your documents to help meet",
    "start": "312240",
    "end": "314820"
  },
  {
    "text": "privacy and compliance requirements",
    "start": "314820",
    "end": "318060"
  },
  {
    "text": "with that let me hand it over to anjan",
    "start": "318060",
    "end": "321000"
  },
  {
    "text": "to walk us through an intelligent",
    "start": "321000",
    "end": "322860"
  },
  {
    "text": "document processing or IDP workflow for",
    "start": "322860",
    "end": "326340"
  },
  {
    "text": "the insurance industry",
    "start": "326340",
    "end": "328199"
  },
  {
    "text": "but you can Envision a similar",
    "start": "328199",
    "end": "329940"
  },
  {
    "text": "conceptual IDP workflow irrespective of",
    "start": "329940",
    "end": "332880"
  },
  {
    "text": "your business domain",
    "start": "332880",
    "end": "335220"
  },
  {
    "text": "an IDP workflow typically starts with",
    "start": "335220",
    "end": "337740"
  },
  {
    "text": "storing the documents in a Secure",
    "start": "337740",
    "end": "339539"
  },
  {
    "text": "Storage such as Amazon simple storage",
    "start": "339539",
    "end": "341759"
  },
  {
    "text": "service also known as Amazon S3",
    "start": "341759",
    "end": "345300"
  },
  {
    "text": "in our example Insurance use case we",
    "start": "345300",
    "end": "348300"
  },
  {
    "text": "store a claims package containing",
    "start": "348300",
    "end": "350039"
  },
  {
    "text": "documents such as claim forms claim",
    "start": "350039",
    "end": "352259"
  },
  {
    "text": "codes and identity documents in an S3",
    "start": "352259",
    "end": "354840"
  },
  {
    "text": "bucket",
    "start": "354840",
    "end": "356160"
  },
  {
    "text": "the next step is document classification",
    "start": "356160",
    "end": "358380"
  },
  {
    "text": "which helps us identify or categorize",
    "start": "358380",
    "end": "360900"
  },
  {
    "text": "these documents",
    "start": "360900",
    "end": "363360"
  },
  {
    "text": "once the documents are identified they",
    "start": "363360",
    "end": "365759"
  },
  {
    "text": "can be routed appropriately for",
    "start": "365759",
    "end": "367800"
  },
  {
    "text": "information extraction and further",
    "start": "367800",
    "end": "370020"
  },
  {
    "text": "document enrichment",
    "start": "370020",
    "end": "372600"
  },
  {
    "text": "the extracted data can be validated",
    "start": "372600",
    "end": "375300"
  },
  {
    "text": "using business rules and finally the",
    "start": "375300",
    "end": "377639"
  },
  {
    "text": "validated data can be sent out to",
    "start": "377639",
    "end": "379440"
  },
  {
    "text": "Downstream systems",
    "start": "379440",
    "end": "382020"
  },
  {
    "text": "in this video we are going to focus",
    "start": "382020",
    "end": "384240"
  },
  {
    "text": "mainly on document classification using",
    "start": "384240",
    "end": "386520"
  },
  {
    "text": "Amazon comprehend and its latest",
    "start": "386520",
    "end": "388199"
  },
  {
    "text": "features",
    "start": "388199",
    "end": "390300"
  },
  {
    "text": "you can use Amazon comprehend custom",
    "start": "390300",
    "end": "392280"
  },
  {
    "text": "classification to organize documents",
    "start": "392280",
    "end": "394500"
  },
  {
    "text": "into categories or classes that you",
    "start": "394500",
    "end": "396720"
  },
  {
    "text": "define",
    "start": "396720",
    "end": "398900"
  },
  {
    "text": "custom classification is a two-step",
    "start": "399180",
    "end": "401039"
  },
  {
    "text": "process",
    "start": "401039",
    "end": "402780"
  },
  {
    "text": "first you train a custom classification",
    "start": "402780",
    "end": "404699"
  },
  {
    "text": "model also called a classifier to",
    "start": "404699",
    "end": "407580"
  },
  {
    "text": "recognize the classes that are of",
    "start": "407580",
    "end": "409199"
  },
  {
    "text": "interest to you",
    "start": "409199",
    "end": "411840"
  },
  {
    "text": "secondly you use your model to classify",
    "start": "411840",
    "end": "414479"
  },
  {
    "text": "any number of unlabeled document sets",
    "start": "414479",
    "end": "418699"
  },
  {
    "text": "let's take a closer look at this process",
    "start": "419300",
    "end": "421919"
  },
  {
    "text": "at a high level",
    "start": "421919",
    "end": "424560"
  },
  {
    "text": "your document processing workflow may",
    "start": "424560",
    "end": "426840"
  },
  {
    "text": "involve documents of various formats",
    "start": "426840",
    "end": "429000"
  },
  {
    "text": "like PDF Tiff PNG and JPEG files the",
    "start": "429000",
    "end": "433440"
  },
  {
    "text": "most common formats are multi-page or",
    "start": "433440",
    "end": "435720"
  },
  {
    "text": "single page scanned or digital PDF and",
    "start": "435720",
    "end": "438479"
  },
  {
    "text": "Tiff files",
    "start": "438479",
    "end": "439680"
  },
  {
    "text": "single page images and jpeg or PNG",
    "start": "439680",
    "end": "443160"
  },
  {
    "text": "format",
    "start": "443160",
    "end": "444780"
  },
  {
    "text": "in order to be able to perform",
    "start": "444780",
    "end": "446340"
  },
  {
    "text": "classification on these documents you",
    "start": "446340",
    "end": "448740"
  },
  {
    "text": "can train a custom document classifier",
    "start": "448740",
    "end": "450539"
  },
  {
    "text": "with Amazon comprehend using an existing",
    "start": "450539",
    "end": "453360"
  },
  {
    "text": "set of label documents in one of two",
    "start": "453360",
    "end": "455400"
  },
  {
    "text": "ways",
    "start": "455400",
    "end": "456720"
  },
  {
    "text": "the first method is using a comma",
    "start": "456720",
    "end": "459360"
  },
  {
    "text": "separated value or CSV file which",
    "start": "459360",
    "end": "461880"
  },
  {
    "text": "contains the labels for each of these",
    "start": "461880",
    "end": "463740"
  },
  {
    "text": "classes and the corresponding plain text",
    "start": "463740",
    "end": "466199"
  },
  {
    "text": "from the document",
    "start": "466199",
    "end": "468180"
  },
  {
    "text": "the second method is using an augmented",
    "start": "468180",
    "end": "470400"
  },
  {
    "text": "manifest file generated by Amazon",
    "start": "470400",
    "end": "472680"
  },
  {
    "text": "sagemaker ground truth which is a data",
    "start": "472680",
    "end": "475319"
  },
  {
    "text": "labeling service",
    "start": "475319",
    "end": "477979"
  },
  {
    "text": "once the training is complete a trained",
    "start": "478080",
    "end": "480539"
  },
  {
    "text": "model will be created along with its",
    "start": "480539",
    "end": "482460"
  },
  {
    "text": "corresponding metrics the trained model",
    "start": "482460",
    "end": "484800"
  },
  {
    "text": "can then be used to classify documents",
    "start": "484800",
    "end": "488880"
  },
  {
    "text": "until now in order to classify a scan",
    "start": "488880",
    "end": "491819"
  },
  {
    "text": "document in PDF or image format you",
    "start": "491819",
    "end": "494520"
  },
  {
    "text": "would first have to extract the plain",
    "start": "494520",
    "end": "496020"
  },
  {
    "text": "text out of the document using optical",
    "start": "496020",
    "end": "498360"
  },
  {
    "text": "character recognition also known as OCR",
    "start": "498360",
    "end": "501120"
  },
  {
    "text": "or a text parsing technology",
    "start": "501120",
    "end": "505080"
  },
  {
    "text": "you would then use the plain text to",
    "start": "505080",
    "end": "506759"
  },
  {
    "text": "classify the document using a trained",
    "start": "506759",
    "end": "508740"
  },
  {
    "text": "Amazon comprehend classifier either in",
    "start": "508740",
    "end": "511080"
  },
  {
    "text": "real time or asynchronously for large",
    "start": "511080",
    "end": "513479"
  },
  {
    "text": "document volumes",
    "start": "513479",
    "end": "515700"
  },
  {
    "text": "this two-step process added complexity",
    "start": "515700",
    "end": "517800"
  },
  {
    "text": "to document processing workflows and we",
    "start": "517800",
    "end": "520500"
  },
  {
    "text": "heard from our customers that this was",
    "start": "520500",
    "end": "522479"
  },
  {
    "text": "an added complexity to their document",
    "start": "522479",
    "end": "524399"
  },
  {
    "text": "processing Pipelines",
    "start": "524399",
    "end": "526080"
  },
  {
    "text": "so we are excited to announce the",
    "start": "526080",
    "end": "528180"
  },
  {
    "text": "support for documents in Native formats",
    "start": "528180",
    "end": "530220"
  },
  {
    "text": "for document classification using Amazon",
    "start": "530220",
    "end": "532500"
  },
  {
    "text": "comprehend",
    "start": "532500",
    "end": "535220"
  },
  {
    "text": "you can now simply call Amazon",
    "start": "535320",
    "end": "537060"
  },
  {
    "text": "comprehend document classification apis",
    "start": "537060",
    "end": "539519"
  },
  {
    "text": "and pass any document in Native formats",
    "start": "539519",
    "end": "542399"
  },
  {
    "text": "such as PDF and images and let Amazon",
    "start": "542399",
    "end": "545279"
  },
  {
    "text": "comprehend do both the text extraction",
    "start": "545279",
    "end": "547740"
  },
  {
    "text": "as well as classification for you",
    "start": "547740",
    "end": "551100"
  },
  {
    "text": "this one step process means that all the",
    "start": "551100",
    "end": "553500"
  },
  {
    "text": "heavy lifting of linearizing or",
    "start": "553500",
    "end": "555839"
  },
  {
    "text": "converting native documents to UDF 8",
    "start": "555839",
    "end": "558660"
  },
  {
    "text": "text format is automatically performed",
    "start": "558660",
    "end": "561480"
  },
  {
    "text": "by Amazon comprehend behind the scenes",
    "start": "561480",
    "end": "563820"
  },
  {
    "text": "simply by specifying a document read",
    "start": "563820",
    "end": "566459"
  },
  {
    "text": "mode",
    "start": "566459",
    "end": "567959"
  },
  {
    "text": "internally Amazon comprehend can use",
    "start": "567959",
    "end": "570839"
  },
  {
    "text": "either a digital PDF parser or Amazon",
    "start": "570839",
    "end": "573600"
  },
  {
    "text": "extract which is a machine learning",
    "start": "573600",
    "end": "575760"
  },
  {
    "text": "service that automatically extracts text",
    "start": "575760",
    "end": "578240"
  },
  {
    "text": "handwriting and data from scanned",
    "start": "578240",
    "end": "580620"
  },
  {
    "text": "documents as a document read mode",
    "start": "580620",
    "end": "584040"
  },
  {
    "text": "you can specify which document read mode",
    "start": "584040",
    "end": "586200"
  },
  {
    "text": "to use depending on the type of document",
    "start": "586200",
    "end": "588240"
  },
  {
    "text": "you wish to classify",
    "start": "588240",
    "end": "591000"
  },
  {
    "text": "with this new feature you can now",
    "start": "591000",
    "end": "593040"
  },
  {
    "text": "perform document classification in real",
    "start": "593040",
    "end": "595080"
  },
  {
    "text": "time or asynchronously for large set of",
    "start": "595080",
    "end": "598080"
  },
  {
    "text": "documents in their native formats",
    "start": "598080",
    "end": "600779"
  },
  {
    "text": "we will now look at a quick walkthrough",
    "start": "600779",
    "end": "602880"
  },
  {
    "text": "of how to use a trained custom",
    "start": "602880",
    "end": "604740"
  },
  {
    "text": "classification model to classify some",
    "start": "604740",
    "end": "607080"
  },
  {
    "text": "sample documents",
    "start": "607080",
    "end": "608760"
  },
  {
    "text": "in the AWS console we will first",
    "start": "608760",
    "end": "611220"
  },
  {
    "text": "navigate over to Amazon comprehend",
    "start": "611220",
    "end": "615319"
  },
  {
    "text": "once in the Amazon comprehend console",
    "start": "616440",
    "end": "619080"
  },
  {
    "text": "we'll expand the left menu",
    "start": "619080",
    "end": "621959"
  },
  {
    "text": "and click on custom classification",
    "start": "621959",
    "end": "624959"
  },
  {
    "text": "in the custom classification screen you",
    "start": "624959",
    "end": "627480"
  },
  {
    "text": "can see a list of trained custom",
    "start": "627480",
    "end": "629519"
  },
  {
    "text": "classification models in our case we",
    "start": "629519",
    "end": "632519"
  },
  {
    "text": "have already trained a model using a",
    "start": "632519",
    "end": "634380"
  },
  {
    "text": "sample set of documents and this model",
    "start": "634380",
    "end": "636420"
  },
  {
    "text": "is capable of classifying Insurance",
    "start": "636420",
    "end": "638760"
  },
  {
    "text": "claim documents",
    "start": "638760",
    "end": "640440"
  },
  {
    "text": "in order to do real-time classification",
    "start": "640440",
    "end": "642540"
  },
  {
    "text": "we also need a real-time endpoint so",
    "start": "642540",
    "end": "645420"
  },
  {
    "text": "let's navigate to the endpoint screen",
    "start": "645420",
    "end": "647040"
  },
  {
    "text": "from the left menu",
    "start": "647040",
    "end": "649079"
  },
  {
    "text": "in the endpoint screen you can see a",
    "start": "649079",
    "end": "651540"
  },
  {
    "text": "list of deployed real-time endpoints",
    "start": "651540",
    "end": "654240"
  },
  {
    "text": "we have already deployed our",
    "start": "654240",
    "end": "655800"
  },
  {
    "text": "classification model with a real-time",
    "start": "655800",
    "end": "657720"
  },
  {
    "text": "endpoint and are now ready to classify",
    "start": "657720",
    "end": "660300"
  },
  {
    "text": "some sample documents",
    "start": "660300",
    "end": "662160"
  },
  {
    "text": "so let's navigate to the real-time",
    "start": "662160",
    "end": "663899"
  },
  {
    "text": "analysis screen from the left menu",
    "start": "663899",
    "end": "666300"
  },
  {
    "text": "the Amazon comprehend real-time analysis",
    "start": "666300",
    "end": "669000"
  },
  {
    "text": "screen allows us to test our trained",
    "start": "669000",
    "end": "671940"
  },
  {
    "text": "custom classifier model",
    "start": "671940",
    "end": "674279"
  },
  {
    "text": "under the input data section we are",
    "start": "674279",
    "end": "676680"
  },
  {
    "text": "going to select custom",
    "start": "676680",
    "end": "678360"
  },
  {
    "text": "and for model type we are going to",
    "start": "678360",
    "end": "680579"
  },
  {
    "text": "select custom classification",
    "start": "680579",
    "end": "683700"
  },
  {
    "text": "under endpoint we will select the",
    "start": "683700",
    "end": "685860"
  },
  {
    "text": "endpoint that we created earlier",
    "start": "685860",
    "end": "689600"
  },
  {
    "text": "we will upload a sample file to classify",
    "start": "689820",
    "end": "694079"
  },
  {
    "text": "we have selected a document which is a",
    "start": "694079",
    "end": "696300"
  },
  {
    "text": "standard CMS 1500 document used in",
    "start": "696300",
    "end": "699180"
  },
  {
    "text": "health insurance claims",
    "start": "699180",
    "end": "702440"
  },
  {
    "text": "our model is trained to identify these",
    "start": "702600",
    "end": "705000"
  },
  {
    "text": "type of documents correctly",
    "start": "705000",
    "end": "708320"
  },
  {
    "text": "we'll choose the file",
    "start": "708480",
    "end": "712040"
  },
  {
    "text": "under the advanced document input",
    "start": "713459",
    "end": "715680"
  },
  {
    "text": "section we're going to specify a",
    "start": "715680",
    "end": "718019"
  },
  {
    "text": "document read mode",
    "start": "718019",
    "end": "720240"
  },
  {
    "text": "we're going to select Force document",
    "start": "720240",
    "end": "722040"
  },
  {
    "text": "read action",
    "start": "722040",
    "end": "724140"
  },
  {
    "text": "and under document read action we can",
    "start": "724140",
    "end": "726899"
  },
  {
    "text": "use either text track detect document",
    "start": "726899",
    "end": "728880"
  },
  {
    "text": "text or textract analyze document",
    "start": "728880",
    "end": "733560"
  },
  {
    "text": "we'll select extract detect document",
    "start": "733560",
    "end": "735540"
  },
  {
    "text": "text",
    "start": "735540",
    "end": "737040"
  },
  {
    "text": "and then click analyze",
    "start": "737040",
    "end": "740660"
  },
  {
    "text": "we can see that our model has correctly",
    "start": "743300",
    "end": "745920"
  },
  {
    "text": "identified this document as a CMS 1500",
    "start": "745920",
    "end": "748680"
  },
  {
    "text": "file",
    "start": "748680",
    "end": "750180"
  },
  {
    "text": "with 99 confidence score",
    "start": "750180",
    "end": "754320"
  },
  {
    "text": "we can also see the Json that was",
    "start": "754320",
    "end": "757079"
  },
  {
    "text": "generated by the endpoint",
    "start": "757079",
    "end": "759120"
  },
  {
    "text": "real-time analysis using endpoints is",
    "start": "759120",
    "end": "761760"
  },
  {
    "text": "best suited for use cases that require",
    "start": "761760",
    "end": "763920"
  },
  {
    "text": "low latency and involve single page",
    "start": "763920",
    "end": "766980"
  },
  {
    "text": "documents if your use case involves",
    "start": "766980",
    "end": "769560"
  },
  {
    "text": "processing a large number of multi-page",
    "start": "769560",
    "end": "771779"
  },
  {
    "text": "documents then it is recommended that",
    "start": "771779",
    "end": "774120"
  },
  {
    "text": "you use asynchronous analysis jobs for",
    "start": "774120",
    "end": "776700"
  },
  {
    "text": "classification",
    "start": "776700",
    "end": "778200"
  },
  {
    "text": "in the create analysis job screen we",
    "start": "778200",
    "end": "780899"
  },
  {
    "text": "will give our job a name for analysis",
    "start": "780899",
    "end": "783420"
  },
  {
    "text": "type we will select custom",
    "start": "783420",
    "end": "784560"
  },
  {
    "text": "classification",
    "start": "784560",
    "end": "786300"
  },
  {
    "text": "under the classifier model we will",
    "start": "786300",
    "end": "788220"
  },
  {
    "text": "select our model and the later version",
    "start": "788220",
    "end": "790620"
  },
  {
    "text": "of the model",
    "start": "790620",
    "end": "793279"
  },
  {
    "text": "under input data we'll provide the S3",
    "start": "793320",
    "end": "796019"
  },
  {
    "text": "bucket location where our documents are",
    "start": "796019",
    "end": "798420"
  },
  {
    "text": "stored that we wish to classify",
    "start": "798420",
    "end": "802040"
  },
  {
    "text": "under Advanced document input as we have",
    "start": "803160",
    "end": "805740"
  },
  {
    "text": "seen before we will select document",
    "start": "805740",
    "end": "808620"
  },
  {
    "text": "read mode as Force document read action",
    "start": "808620",
    "end": "812040"
  },
  {
    "text": "and subsequently select text track",
    "start": "812040",
    "end": "813959"
  },
  {
    "text": "detect document text as a mode which",
    "start": "813959",
    "end": "816540"
  },
  {
    "text": "comprehend will use to read our",
    "start": "816540",
    "end": "818700"
  },
  {
    "text": "documents",
    "start": "818700",
    "end": "821300"
  },
  {
    "text": "finally we will specify an output",
    "start": "821339",
    "end": "824040"
  },
  {
    "text": "location in Amazon S3 where the output",
    "start": "824040",
    "end": "826740"
  },
  {
    "text": "of the analysis job will be stored",
    "start": "826740",
    "end": "830100"
  },
  {
    "text": "we'll then click create job",
    "start": "830100",
    "end": "834120"
  },
  {
    "text": "we saw how easy it was to use a custom",
    "start": "834120",
    "end": "836639"
  },
  {
    "text": "classification model with Amazon",
    "start": "836639",
    "end": "838680"
  },
  {
    "text": "comprehend to classify documents",
    "start": "838680",
    "end": "841380"
  },
  {
    "text": "now let's take a look at an end-to-end",
    "start": "841380",
    "end": "843660"
  },
  {
    "text": "intelligent document processing",
    "start": "843660",
    "end": "845160"
  },
  {
    "text": "conceptual workflow that is capable of",
    "start": "845160",
    "end": "847620"
  },
  {
    "text": "classifying and extracting information",
    "start": "847620",
    "end": "849540"
  },
  {
    "text": "from large number of documents",
    "start": "849540",
    "end": "853079"
  },
  {
    "text": "we first begin by storing all the",
    "start": "853079",
    "end": "855120"
  },
  {
    "text": "documents into an Amazon S3 bucket",
    "start": "855120",
    "end": "858060"
  },
  {
    "text": "once the documents are stored an AWS",
    "start": "858060",
    "end": "861120"
  },
  {
    "text": "Lambda function is invoked using S3",
    "start": "861120",
    "end": "863579"
  },
  {
    "text": "triggers to start an asynchronous",
    "start": "863579",
    "end": "865920"
  },
  {
    "text": "document classification job with Amazon",
    "start": "865920",
    "end": "868320"
  },
  {
    "text": "comprehend",
    "start": "868320",
    "end": "871019"
  },
  {
    "text": "once the job completes successfully the",
    "start": "871019",
    "end": "873959"
  },
  {
    "text": "outputs are stored in an output Amazon",
    "start": "873959",
    "end": "876180"
  },
  {
    "text": "S3 location specified",
    "start": "876180",
    "end": "879420"
  },
  {
    "text": "and AWS Lambda function post processes",
    "start": "879420",
    "end": "882600"
  },
  {
    "text": "the output and labels the documents",
    "start": "882600",
    "end": "884760"
  },
  {
    "text": "appropriately using the classification",
    "start": "884760",
    "end": "886980"
  },
  {
    "text": "results of the classifier and stores",
    "start": "886980",
    "end": "889800"
  },
  {
    "text": "them in an Amazon S3 bucket",
    "start": "889800",
    "end": "892920"
  },
  {
    "text": "we now have classified a neatly labeled",
    "start": "892920",
    "end": "896100"
  },
  {
    "text": "documents that can be routed",
    "start": "896100",
    "end": "897959"
  },
  {
    "text": "appropriately for extraction of",
    "start": "897959",
    "end": "899820"
  },
  {
    "text": "information with named entity",
    "start": "899820",
    "end": "901740"
  },
  {
    "text": "recognition",
    "start": "901740",
    "end": "903060"
  },
  {
    "text": "using any defined business logic",
    "start": "903060",
    "end": "905940"
  },
  {
    "text": "for example extracting Insurance claim",
    "start": "905940",
    "end": "908820"
  },
  {
    "text": "amount from an insurance claim form",
    "start": "908820",
    "end": "911639"
  },
  {
    "text": "the output of each of these extraction",
    "start": "911639",
    "end": "914040"
  },
  {
    "text": "steps is then stored into an Amazon S3",
    "start": "914040",
    "end": "917279"
  },
  {
    "text": "bucket",
    "start": "917279",
    "end": "918360"
  },
  {
    "text": "and any low confidence values can be",
    "start": "918360",
    "end": "920760"
  },
  {
    "text": "sent to human reviewer for manual",
    "start": "920760",
    "end": "922740"
  },
  {
    "text": "validation",
    "start": "922740",
    "end": "925279"
  },
  {
    "text": "finally the extracted and verified data",
    "start": "925740",
    "end": "929279"
  },
  {
    "text": "can be stored in a database or sent to",
    "start": "929279",
    "end": "931980"
  },
  {
    "text": "Downstream systems",
    "start": "931980",
    "end": "933899"
  },
  {
    "text": "such as insurance claims processing",
    "start": "933899",
    "end": "936180"
  },
  {
    "text": "systems CRM systems to name a few",
    "start": "936180",
    "end": "940139"
  },
  {
    "text": "that wraps up our comprehend video snack",
    "start": "940139",
    "end": "942480"
  },
  {
    "text": "on document classification using Amazon",
    "start": "942480",
    "end": "944820"
  },
  {
    "text": "comprehend",
    "start": "944820",
    "end": "946079"
  },
  {
    "text": "as a next step we encourage you to check",
    "start": "946079",
    "end": "948779"
  },
  {
    "text": "out part two of this video which Dives",
    "start": "948779",
    "end": "951060"
  },
  {
    "text": "deep into information extraction from",
    "start": "951060",
    "end": "953040"
  },
  {
    "text": "documents using named entity recognition",
    "start": "953040",
    "end": "956160"
  },
  {
    "text": "you can also check out the links for use",
    "start": "956160",
    "end": "958500"
  },
  {
    "text": "cases technical documentation and our",
    "start": "958500",
    "end": "961139"
  },
  {
    "text": "open source GitHub repository for full",
    "start": "961139",
    "end": "963480"
  },
  {
    "text": "code samples",
    "start": "963480",
    "end": "966180"
  },
  {
    "text": "thanks for watching",
    "start": "966180",
    "end": "969139"
  }
]