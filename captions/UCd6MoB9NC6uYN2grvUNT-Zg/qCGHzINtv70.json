[
  {
    "start": "0",
    "end": "25000"
  },
  {
    "text": "hello everyone I'm just here to talk about a joint pilot project that we did",
    "start": "30",
    "end": "5160"
  },
  {
    "text": "at the Gavin and genome one with AWS so the purpose of the entire thing was",
    "start": "5160",
    "end": "12330"
  },
  {
    "text": "basically just to see if we could build some in-house cloud expertise and customize our pipeline for use in the",
    "start": "12330",
    "end": "19410"
  },
  {
    "text": "clouds so the pipeline we would use at Garvin in the cloud simple as that the team that put this all together",
    "start": "19410",
    "end": "27150"
  },
  {
    "start": "25000",
    "end": "25000"
  },
  {
    "text": "the real powerhouse behind it was of course Aaron Statham our chief bioinformatics officer and he's in",
    "start": "27150",
    "end": "33360"
  },
  {
    "text": "charge of the pipeline generally that delivers results to patients we also",
    "start": "33360",
    "end": "38370"
  },
  {
    "text": "have mark beaneasy David Thomas and Andrew stone on the research side getting the cohorts together and doing",
    "start": "38370",
    "end": "45690"
  },
  {
    "text": "some great work on the actual research and getting the funding all of that and of course has been up nanu and myself",
    "start": "45690",
    "end": "51510"
  },
  {
    "text": "are the DevOps team for genome one so the two of us did a lot of work actually",
    "start": "51510",
    "end": "57000"
  },
  {
    "text": "building the system with a lot of reliance on our excellent AWS team Ben",
    "start": "57000",
    "end": "63090"
  },
  {
    "text": "Thurgood and Jamie Nelson who worked with us directly as partners which was",
    "start": "63090",
    "end": "68280"
  },
  {
    "text": "excellent and a great experience for all of us so a very quick run down of genome",
    "start": "68280",
    "end": "74280"
  },
  {
    "start": "71000",
    "end": "71000"
  },
  {
    "text": "one who are we so this is the initial batch of X 10 sequencing sites around",
    "start": "74280",
    "end": "80460"
  },
  {
    "text": "the world these are the early adopters of the Illumina X 10 sequencing technology and as you can see on the",
    "start": "80460",
    "end": "87930"
  },
  {
    "text": "southern hemisphere where basically that lonely little dot on Australia but the",
    "start": "87930",
    "end": "93720"
  },
  {
    "text": "map today looks a bit more like this so a lot more players have joined the team",
    "start": "93720",
    "end": "98729"
  },
  {
    "text": "a lot more people have bought into the Illumina solution nevertheless genome",
    "start": "98729",
    "end": "104250"
  },
  {
    "text": "one remains the largest genome sequencing facility in the southern hemisphere we've got a lot of great",
    "start": "104250",
    "end": "111060"
  },
  {
    "text": "strengths in the research and science side of it some of the best researchers",
    "start": "111060",
    "end": "116100"
  },
  {
    "text": "in Australia and we have some great ski strengths in software development and",
    "start": "116100",
    "end": "121469"
  },
  {
    "text": "data mining which forms the great deal of this genomics work thanks to the",
    "start": "121469",
    "end": "127020"
  },
  {
    "text": "Illumina solution in-house we can now sequence a genome in about a week that's end-to-end the entire DNA strain",
    "start": "127020",
    "end": "133800"
  },
  {
    "text": "of a person for about 1.5 K Australian so that leads me to the problem of scale",
    "start": "133800",
    "end": "141360"
  },
  {
    "start": "140000",
    "end": "140000"
  },
  {
    "text": "so genome 1 as you can see on the slide has 12 high-throughput next-generation sequencers which produce about 300",
    "start": "141360",
    "end": "149460"
  },
  {
    "text": "genomes per week which we need to analyze every genome has to go through an analysis pipeline and really we can't",
    "start": "149460",
    "end": "157800"
  },
  {
    "text": "compromise on the core values of that pipeline accuracy a traceability documentation consistency functionality",
    "start": "157800",
    "end": "165420"
  },
  {
    "text": "quality obviously these are patients their lives are impacted we can't compromise there but good science costs",
    "start": "165420",
    "end": "172230"
  },
  {
    "text": "money and we need to save money somewhere which is rare AWS comes in and where we have to of course cut down our",
    "start": "172230",
    "end": "180150"
  },
  {
    "text": "own internal processes go lean go agile with everything we've built a lot of software development expertise in-house",
    "start": "180150",
    "end": "186750"
  },
  {
    "text": "as a result to kind of build all of our own systems put our own stamp on everything and continually improve",
    "start": "186750",
    "end": "192360"
  },
  {
    "text": "everything we do in-house and up until recently that hasn't included our",
    "start": "192360",
    "end": "197460"
  },
  {
    "text": "infrastructure and this projects that have to change that so software first",
    "start": "197460",
    "end": "203190"
  },
  {
    "start": "200000",
    "end": "200000"
  },
  {
    "text": "Hardware last is III guess keynote gave",
    "start": "203190",
    "end": "208230"
  },
  {
    "text": "me a good way of talking about it until now we've kind of been building the plane while flying it and software first",
    "start": "208230",
    "end": "216840"
  },
  {
    "text": "Hardware last is kind of us changing that around a little bit now hardware as you all know is a big investment up",
    "start": "216840",
    "end": "222900"
  },
  {
    "text": "front and it's Agis quite quickly and we are no strangers to buying large amounts",
    "start": "222900",
    "end": "228180"
  },
  {
    "text": "of hardware so our by informatics pipeline used to all run in-house on our systems and by treating Hardware the way",
    "start": "228180",
    "end": "237660"
  },
  {
    "text": "we do the rest of our code by creating essentially infrastructure as code we're",
    "start": "237660",
    "end": "243630"
  },
  {
    "text": "able to actually change the entire model of how we view Hardware how we build it so infrastructure as code means we",
    "start": "243630",
    "end": "249989"
  },
  {
    "text": "define all of our infrastructure and code we commit it to a github repository and we're able to actually build on that",
    "start": "249989",
    "end": "257870"
  },
  {
    "text": "continuously improving the infrastructure as well as the software which leads to some great cost",
    "start": "257870",
    "end": "264000"
  },
  {
    "text": "reductions faster analysis speed and reduces risk because every time we build something we",
    "start": "264000",
    "end": "270750"
  },
  {
    "text": "spin up a whole new fresh infrastructure for it but what you're probably looking",
    "start": "270750",
    "end": "275970"
  },
  {
    "text": "for is this last point the 22 times cost reduction and compute that we observed",
    "start": "275970",
    "end": "281010"
  },
  {
    "text": "as part of this project and that really is is a very key takeaway from this so",
    "start": "281010",
    "end": "287669"
  },
  {
    "start": "287000",
    "end": "287000"
  },
  {
    "text": "how did we get there first of all our bioinformatics processing pipeline like I said used to run in-house we sequence",
    "start": "287669",
    "end": "294990"
  },
  {
    "text": "whole human genomes about 18,000 per year and they all have to go through this pipeline the genome produces about",
    "start": "294990",
    "end": "301860"
  },
  {
    "text": "80 gigabytes of raw data in fast queue format when it comes out of the sequencer and so on that data size just",
    "start": "301860",
    "end": "308400"
  },
  {
    "text": "about doubles when it moves into the BAM format and as it passes through all",
    "start": "308400",
    "end": "314280"
  },
  {
    "text": "these applications and finally it comes out as AG VCF which is a listing essentially of the variance between our",
    "start": "314280",
    "end": "322889"
  },
  {
    "text": "genome the one that we're looking at and the gold standard now I'm not going to",
    "start": "322889",
    "end": "328530"
  },
  {
    "text": "go into the details because Dennis has done a really good job already explaining that so if you didn't catch",
    "start": "328530",
    "end": "334020"
  },
  {
    "text": "her talk it'll be available on YouTube but suffice to say that these genomes",
    "start": "334020",
    "end": "340430"
  },
  {
    "text": "can each be treated as an individual unit and they can all be processed independently without the processes",
    "start": "340430",
    "end": "346349"
  },
  {
    "text": "talking to each other so it's a great place to use parallel computing and",
    "start": "346349",
    "end": "352110"
  },
  {
    "text": "massively parallel eyes the whole process to do them all at once what we built to facilitate that is a docker",
    "start": "352110",
    "end": "358320"
  },
  {
    "text": "image so if you run anything in-house you know that your HPC typically will",
    "start": "358320",
    "end": "363810"
  },
  {
    "text": "have the issue of compatibility everything you run has to be essentially",
    "start": "363810",
    "end": "369810"
  },
  {
    "text": "built into that HPC architecture in so that well essentially you have to make",
    "start": "369810",
    "end": "378210"
  },
  {
    "text": "sure that your code matches up with the infrastructure you plan to run on docker changes that by adding a docker",
    "start": "378210",
    "end": "385020"
  },
  {
    "text": "container into the mix that docker container can contain everything you need to develop your software to run it",
    "start": "385020",
    "end": "392250"
  },
  {
    "text": "on any compute and as such we kind of containerized everything we needed put",
    "start": "392250",
    "end": "397620"
  },
  {
    "text": "it into this container and what it does is really quite simple downloads the raw data of the genome",
    "start": "397620",
    "end": "404309"
  },
  {
    "text": "from NCI the supercomputing facility that is partnered with us or from",
    "start": "404309",
    "end": "412019"
  },
  {
    "text": "Amazon's s3 and then we run it through a sequence of applications one after the other which forms our bioinformatics",
    "start": "412019",
    "end": "417899"
  },
  {
    "text": "pipeline which we developed to the Broad Institute standard and then finally that goes back into Amazon s3 or NCI it was",
    "start": "417899",
    "end": "427589"
  },
  {
    "text": "optimized for a certain kind of instance and I'll go into that but suffice to say",
    "start": "427589",
    "end": "432599"
  },
  {
    "text": "for now that instance is a particularly beefy instance and we use its",
    "start": "432599",
    "end": "438419"
  },
  {
    "text": "capabilities to the max the docker container was shaped and configured to",
    "start": "438419",
    "end": "444079"
  },
  {
    "text": "specifically match those optimized compute capabilities of that machine and that machine costs us about forty",
    "start": "444079",
    "end": "451079"
  },
  {
    "text": "dollars per genome on AWS but by making our infrastructure as code essentially",
    "start": "451079",
    "end": "458669"
  },
  {
    "text": "able to be spun up directly anytime we want we can do we can take advantage of",
    "start": "458669",
    "end": "465209"
  },
  {
    "text": "Amazon's spot pricing we can take excess compute from Amazon run our docker",
    "start": "465209",
    "end": "470939"
  },
  {
    "text": "container there and we observe a cost about of about seven to ten dollars per",
    "start": "470939",
    "end": "476309"
  },
  {
    "text": "genome even including the potential for losing the spot instance and having to run again now this pipelines first research",
    "start": "476309",
    "end": "485219"
  },
  {
    "start": "483000",
    "end": "483000"
  },
  {
    "text": "application was in sarcoma of course someone has to pay for it something has",
    "start": "485219",
    "end": "491699"
  },
  {
    "text": "to be done with the pipeline for us to be able to test it so what we tested it with was this study sarcoma if you're",
    "start": "491699",
    "end": "499709"
  },
  {
    "text": "not familiar with it is a terrifying form of cancer not least because it's",
    "start": "499709",
    "end": "505319"
  },
  {
    "text": "hard to diagnose in that sarcoma affects",
    "start": "505319",
    "end": "512309"
  },
  {
    "text": "a lot of young Australians specifically and those young Australians we can't",
    "start": "512309",
    "end": "518219"
  },
  {
    "text": "screen everybody for it and we don't really know which part of the population to screen in order to determine who's at",
    "start": "518219",
    "end": "525300"
  },
  {
    "text": "risk of sarcoma which means that right now mostly sarcoma is only found when",
    "start": "525300",
    "end": "530429"
  },
  {
    "text": "it's too late to do much about it the study seeks out it seeks to change",
    "start": "530429",
    "end": "535629"
  },
  {
    "text": "that by trying to find in the genome a sort of fingerprint for sarcoma and",
    "start": "535629",
    "end": "542079"
  },
  {
    "text": "here's how we were doing it essentially was taking two data sets one which is 4,000 elderly healthy people and one",
    "start": "542079",
    "end": "550209"
  },
  {
    "text": "which consists of 1,000 young adults who are affected by sarcoma",
    "start": "550209",
    "end": "555639"
  },
  {
    "text": "now as Denis said in her excellent talk and went into more detail about",
    "start": "555639",
    "end": "560879"
  },
  {
    "text": "essentially what we need to do in order to find a way of diagnosing sarcoma is",
    "start": "560879",
    "end": "568079"
  },
  {
    "start": "563000",
    "end": "563000"
  },
  {
    "text": "to compare the two right so essentially what we're doing is part of the study",
    "start": "568079",
    "end": "573449"
  },
  {
    "text": "the MGR B data set of elderly healthy Australians is used to assess the",
    "start": "573449",
    "end": "578740"
  },
  {
    "text": "genomic patterns associated with healthy old age you can see this is what a genome of a healthy person looks like",
    "start": "578740",
    "end": "585069"
  },
  {
    "text": "and do data mining on that to kind of get a fingerprint in the computer and",
    "start": "585069",
    "end": "590079"
  },
  {
    "text": "you can provide that study as a sort of control for any other study that seeks",
    "start": "590079",
    "end": "597129"
  },
  {
    "text": "to compare to those healthy genomes with an unhealthy genome such as these is KS",
    "start": "597129",
    "end": "605439"
  },
  {
    "text": "genomes which are young people affected by sarcoma we're going and so the point",
    "start": "605439",
    "end": "612639"
  },
  {
    "text": "of the research study is to then take that information compared to and see how that fingerprinting matches up what's in",
    "start": "612639",
    "end": "619990"
  },
  {
    "text": "the affected genomes but not ah but also in the MGR be the healthy",
    "start": "619990",
    "end": "627339"
  },
  {
    "text": "elderly is easy to be discounted we can say this is unlikely to be causative and",
    "start": "627339",
    "end": "633879"
  },
  {
    "text": "everything that's not forms our fingerprint of what in person's genome",
    "start": "633879",
    "end": "640569"
  },
  {
    "text": "might predispose them to the condition so our process in doing the actual study",
    "start": "640569",
    "end": "648299"
  },
  {
    "start": "644000",
    "end": "644000"
  },
  {
    "text": "step by step looks a bit like this we didn't set out to run this pipeline 24/7",
    "start": "648299",
    "end": "654399"
  },
  {
    "text": "we set out with the pipeline so as to",
    "start": "654399",
    "end": "659490"
  },
  {
    "text": "basically take what we had internally push it into the cloud in in a way",
    "start": "659490",
    "end": "667600"
  },
  {
    "text": "we could replicate automatically rather than manually we actually wanted to be",
    "start": "667600",
    "end": "673480"
  },
  {
    "text": "able to throw this up and then get rid of it when we're not using it and throw it up again with as many compute",
    "start": "673480",
    "end": "679540"
  },
  {
    "text": "instances as we need anytime and you'll find that we actually were quite",
    "start": "679540",
    "end": "686440"
  },
  {
    "text": "successful at that and really the way we did it was to start by first optimizing our docker container to run very nicely",
    "start": "686440",
    "end": "693190"
  },
  {
    "text": "on one particular sort of Amazon instance then we set up a queueing",
    "start": "693190",
    "end": "698410"
  },
  {
    "text": "system the queuing system had the job simply of funneling jobs through that",
    "start": "698410",
    "end": "705610"
  },
  {
    "text": "pipeline and then we set up a data mover architecture machines to take data up",
    "start": "705610",
    "end": "711490"
  },
  {
    "text": "and down when it's done as well as when it's about to be processed and then we set up a layer of monitoring to create",
    "start": "711490",
    "end": "718660"
  },
  {
    "text": "visibility accounting and billing information now at every step rather",
    "start": "718660",
    "end": "724690"
  },
  {
    "text": "than simply build on that we took that and put it into Amazon's cloud formation",
    "start": "724690",
    "end": "730600"
  },
  {
    "text": "service AWS CloudFormation allowed us to then make that's replicable so that",
    "start": "730600",
    "end": "736680"
  },
  {
    "text": "looks a little bit like this a bunch of JSON files I should note you can also use yamo but",
    "start": "736680",
    "end": "743790"
  },
  {
    "start": "738000",
    "end": "738000"
  },
  {
    "text": "basically our infrastructure rather than be servers in a box somewhere",
    "start": "743790",
    "end": "749490"
  },
  {
    "text": "looks like these files simple code I've cut it off here because the actual code",
    "start": "749490",
    "end": "755560"
  },
  {
    "text": "isn't that important that you can see that I've selected here a particular definition right the ECS cluster what I",
    "start": "755560",
    "end": "762250"
  },
  {
    "text": "can do in cloud formation is simply to define every instance and every",
    "start": "762250",
    "end": "768400"
  },
  {
    "text": "parameter of every Amazon piece of hardware or software that we use in our",
    "start": "768400",
    "end": "773680"
  },
  {
    "text": "studies and turn it into just a code definition but I can also create a",
    "start": "773680",
    "end": "780040"
  },
  {
    "text": "definition of my own like ECS cluster and build another file that defines in",
    "start": "780040",
    "end": "785860"
  },
  {
    "text": "detail how that ECS cluster runs what kind of machines are in it how many of",
    "start": "785860",
    "end": "791020"
  },
  {
    "text": "them what the rules are for growing the system shrinking the system and also how",
    "start": "791020",
    "end": "796750"
  },
  {
    "text": "these behave so when I start up an instance what do I run on it so you can see in",
    "start": "796750",
    "end": "802800"
  },
  {
    "text": "the code here I'm not gonna be able to point to it oh maybe I can so you can",
    "start": "802800",
    "end": "808050"
  },
  {
    "text": "basically see that we are specifying what each of these instances will run so",
    "start": "808050",
    "end": "813480"
  },
  {
    "text": "when an instance comes up it will run that piece of script and it's all nicely put together in cloud formation for us",
    "start": "813480",
    "end": "820399"
  },
  {
    "text": "such that we can change exactly how the things behave how many cores they have and so forth on fly now how do we",
    "start": "820399",
    "end": "828630"
  },
  {
    "start": "826000",
    "end": "826000"
  },
  {
    "text": "optimize to the instance that part is simple enough what you're looking at is a series of applications so every",
    "start": "828630",
    "end": "836310"
  },
  {
    "text": "colored bar represents one application in our research pipeline each of these applications has different memory needs",
    "start": "836310",
    "end": "843270"
  },
  {
    "text": "different computing needs and each of them uses certain amount of RAM or CPU",
    "start": "843270",
    "end": "848520"
  },
  {
    "text": "etc and simply put what we did in order to optimize to our compute instance was",
    "start": "848520",
    "end": "854399"
  },
  {
    "text": "to tweak the parameters of our pipeline and our hardware such that we observed",
    "start": "854399",
    "end": "859589"
  },
  {
    "text": "full capacity and full utilization of that instance with a good genome",
    "start": "859589",
    "end": "865529"
  },
  {
    "text": "sequencing time overall and on those less compute intensive steps that we",
    "start": "865529",
    "end": "872070"
  },
  {
    "text": "found full Ram utilization but didn't end up going into virtual memory because",
    "start": "872070",
    "end": "877830"
  },
  {
    "text": "a genome is very large and a lot of it should be processed in memory to get the fastest possible result so what this",
    "start": "877830",
    "end": "884730"
  },
  {
    "text": "gave us was essentially a pipeline perfectly optimized for that one type of instance after that we built our queuing",
    "start": "884730",
    "end": "892500"
  },
  {
    "start": "890000",
    "end": "890000"
  },
  {
    "text": "system we took Amazon ECS which is the container service and what we did was",
    "start": "892500",
    "end": "898380"
  },
  {
    "text": "essentially upload our docker image into ECS automatically and you can do this as",
    "start": "898380",
    "end": "905070"
  },
  {
    "text": "part of a bamboo deployment so we can actually change out the container that",
    "start": "905070",
    "end": "910620"
  },
  {
    "text": "we use anytime and we decided to run one container per instance and that makes it",
    "start": "910620",
    "end": "917880"
  },
  {
    "text": "very easy for us ECS just spins up computing hardware for us whenever we need to run a container",
    "start": "917880",
    "end": "923750"
  },
  {
    "text": "so we pick one container per genome and that's just how it goes create we also create an auto scaling",
    "start": "923750",
    "end": "930750"
  },
  {
    "text": "group which essentially just means as the queue fills up the compute cluster scales up to allow",
    "start": "930750",
    "end": "937850"
  },
  {
    "text": "us to do it all in parallel now we have an Amazon RDS database and the point of",
    "start": "937850",
    "end": "943070"
  },
  {
    "text": "the Amazon RDS database is simply to assign one genome to one task in the",
    "start": "943070",
    "end": "948140"
  },
  {
    "text": "database and to keep track of the jobs that are running so we can say if there",
    "start": "948140",
    "end": "953480"
  },
  {
    "text": "is a genome inside the RDS database and it's not been sequenced yet then we can",
    "start": "953480",
    "end": "960620"
  },
  {
    "text": "go one step further and say this genome is now a running job so we have the",
    "start": "960620",
    "end": "968030"
  },
  {
    "text": "Amazon SQS service simply watches that RDS looks for genomes that haven't been",
    "start": "968030",
    "end": "974720"
  },
  {
    "text": "processed and aren't running pulls them out of the RDS assigns that genome to an",
    "start": "974720",
    "end": "980930"
  },
  {
    "text": "instance outer scales up the Amazon ECS in order to pull down that genome",
    "start": "980930",
    "end": "988340"
  },
  {
    "text": "and start processing it that becomes a job in the audience and then the RDS takes well the RDS watches the ECS takes",
    "start": "988340",
    "end": "996170"
  },
  {
    "text": "it from there if the job fails it'll go back into the queue if it passes it gets ticked off in",
    "start": "996170",
    "end": "1002350"
  },
  {
    "text": "the RDS until there's no more genomes left to be sequenced so this is what the",
    "start": "1002350",
    "end": "1007870"
  },
  {
    "text": "architecture looks like now on the left you can see NCI that's really just a data point it's it's where we stage our",
    "start": "1007870",
    "end": "1017380"
  },
  {
    "text": "genomes and then garvin on the right is where we do our development on our",
    "start": "1017380",
    "end": "1023700"
  },
  {
    "text": "containers and we push them into the cloud system in the center you see one",
    "start": "1023700",
    "end": "1029230"
  },
  {
    "text": "big Amazon VPC and basically what you",
    "start": "1029230",
    "end": "1034510"
  },
  {
    "text": "can see on the right is just our continuous improvement architecture we have all of our containers moving into",
    "start": "1034510",
    "end": "1042100"
  },
  {
    "text": "Garvin's bamboo system now Garvin's bamboo system is basically just keeping",
    "start": "1042100",
    "end": "1048730"
  },
  {
    "text": "versions of all of our containers and allowing us to deploy a particular version at any given time which is",
    "start": "1048730",
    "end": "1054820"
  },
  {
    "text": "helpful if you need to well because we keep track of the exact pipeline that",
    "start": "1054820",
    "end": "1060580"
  },
  {
    "text": "was used for each patient and we need to be able to go back and replicate that so this pipeline results in",
    "start": "1060580",
    "end": "1068200"
  },
  {
    "text": "essentially from the right the versions of our docker container pouring into",
    "start": "1068200",
    "end": "1073539"
  },
  {
    "text": "amazon's ECR their container registry and then that container registry puts",
    "start": "1073539",
    "end": "1079240"
  },
  {
    "text": "them into evcs which is from where they go into the main loop of this and the",
    "start": "1079240",
    "end": "1085809"
  },
  {
    "text": "main loop is simple enough you can see on the left the genomes coming in through the internet gateway we have a",
    "start": "1085809",
    "end": "1092260"
  },
  {
    "text": "set of NAT machines which whose entire job is to pull data down and then push",
    "start": "1092260",
    "end": "1097809"
  },
  {
    "text": "it into the process and then you have a private VPC or rather a private subnet",
    "start": "1097809",
    "end": "1104860"
  },
  {
    "text": "within the VPC which is an auto scaling group runs our container at scale and",
    "start": "1104860",
    "end": "1111820"
  },
  {
    "text": "you can see the RDS is in that same bundle and then there's a simple queuing service on the bottom right that's X how",
    "start": "1111820",
    "end": "1119230"
  },
  {
    "text": "I've just described so once they're all done any given instance will push its data into an Amazon s3 bucket through",
    "start": "1119230",
    "end": "1126880"
  },
  {
    "text": "that s3 endpoint it's in there and then from there the s3 bucket simply fills up",
    "start": "1126880",
    "end": "1134500"
  },
  {
    "text": "over time and once it's filled up sufficiently we can pull it down over Direct Connect or put it on an AWS",
    "start": "1134500",
    "end": "1140380"
  },
  {
    "text": "snowball and push it to garvin directly so to keep track of this we've have used",
    "start": "1140380",
    "end": "1148450"
  },
  {
    "start": "1145000",
    "end": "1145000"
  },
  {
    "text": "a set of information radiators that we basically built ourselves so on the left",
    "start": "1148450",
    "end": "1153880"
  },
  {
    "text": "you can see the colorful wall board that we used and that's really just a very simple HTML setup with squares using",
    "start": "1153880",
    "end": "1161880"
  },
  {
    "text": "Amazon's rest interface to query for the amount of spot instances we're using the",
    "start": "1161880",
    "end": "1167500"
  },
  {
    "text": "pricing that we've observed on each spot instance how many jobs are running how many are done all of those simple",
    "start": "1167500",
    "end": "1173529"
  },
  {
    "text": "metrics are viewable at any given time on a big wall board in the middle of garvin and we never lose sight of those",
    "start": "1173529",
    "end": "1180210"
  },
  {
    "text": "as a side note we used to handle this with a simple chicken interface which",
    "start": "1180210",
    "end": "1186820"
  },
  {
    "text": "you can see on the bottom right the chicken was simply squeezed every time a genome went through and the entire team",
    "start": "1186820",
    "end": "1193240"
  },
  {
    "text": "would be alerted we've just sequenced the genome we've gone well beyond the chicken since then but the actual",
    "start": "1193240",
    "end": "1200289"
  },
  {
    "text": "process the same we're going into the statistics in a readable and human friendly way and",
    "start": "1200289",
    "end": "1207809"
  },
  {
    "text": "at the top there you can see a little tiny snippet from our bamboo logs you",
    "start": "1207809",
    "end": "1215380"
  },
  {
    "text": "can see bamboos keeping track of every build of our a Dockers container you can",
    "start": "1215380",
    "end": "1220840"
  },
  {
    "text": "see that bamboo is also tracking whenever we push code what versions we're running what versions we put the",
    "start": "1220840",
    "end": "1227050"
  },
  {
    "text": "patient's through and so on so we can keep things consistent auditable and clear and finally on the right you have",
    "start": "1227050",
    "end": "1234309"
  },
  {
    "text": "the billing and we can keep very nice track of billing because we don't have to tag everything manually since all the",
    "start": "1234309",
    "end": "1241420"
  },
  {
    "text": "infrastructure is built by cloud formation we can set cloud formation to simply tag everything for us with the",
    "start": "1241420",
    "end": "1248440"
  },
  {
    "text": "appropriate billing tags so that when it comes time for someone to pay for it we know exactly who used what compute and",
    "start": "1248440",
    "end": "1255730"
  },
  {
    "text": "what data came from whom and so on on top of that our pipeline has now run",
    "start": "1255730",
    "end": "1262270"
  },
  {
    "start": "1260000",
    "end": "1260000"
  },
  {
    "text": "about four and a half thousand whole genomes which is pretty impressive that",
    "start": "1262270",
    "end": "1268450"
  },
  {
    "text": "raw data is currently staged on NCI and you can already go online to SGC Gavin",
    "start": "1268450",
    "end": "1273820"
  },
  {
    "text": "or bu to see that data for yourself it's all browsable publicly obviously",
    "start": "1273820",
    "end": "1279309"
  },
  {
    "text": "de-identified but it's all available to you and all Australian researchers data",
    "start": "1279309",
    "end": "1285550"
  },
  {
    "text": "has been processed on spot and we've been using a max spot bid of about 2.9",
    "start": "1285550",
    "end": "1292120"
  },
  {
    "text": "dollars and I'll go into the details of the costing for the first phase of this",
    "start": "1292120",
    "end": "1297850"
  },
  {
    "text": "over the next couple of slides now after the project is finished the results are pulled back down to NCI",
    "start": "1297850",
    "end": "1304200"
  },
  {
    "text": "and we have the following egress options the direct connect from AWS",
    "start": "1304200",
    "end": "1310890"
  },
  {
    "text": "to NCI is already set up so if you're using NCI alongside AWS the two can talk",
    "start": "1310890",
    "end": "1317620"
  },
  {
    "text": "directly and you get a much better egress cost coming down direct connect between AWS and garvin flows through",
    "start": "1317620",
    "end": "1325630"
  },
  {
    "text": "UNSW so we can also pull our own data into garvin at a fraction of the cost",
    "start": "1325630",
    "end": "1332380"
  },
  {
    "text": "and finally the AWS snowball we found has about the same costing as",
    "start": "1332380",
    "end": "1337480"
  },
  {
    "text": "the Direct Connect does so we can have the data physically shipped to us for",
    "start": "1337480",
    "end": "1342850"
  },
  {
    "text": "about the same price finally there's an",
    "start": "1342850",
    "end": "1347980"
  },
  {
    "text": "egress waiver available for research dot electus which was covered in the keynote so if you're a research institution you",
    "start": "1347980",
    "end": "1355720"
  },
  {
    "text": "to have access to improved pricing when it comes to pulling your data back down now now phase two metrics so for the",
    "start": "1355720",
    "end": "1364510"
  },
  {
    "text": "most recent phase of the pipeline are that we put through a staggering two",
    "start": "1364510",
    "end": "1370630"
  },
  {
    "text": "thousand eight hundred and fifty nine high-quality genomes roughly 70 million low side genotyped in each and we found",
    "start": "1370630",
    "end": "1377830"
  },
  {
    "text": "that there was about three million CPU hours used with to process about 1.1",
    "start": "1377830",
    "end": "1383919"
  },
  {
    "text": "petabytes of data so the pipeline is working beautifully at scale and I can",
    "start": "1383919",
    "end": "1390460"
  },
  {
    "start": "1389000",
    "end": "1389000"
  },
  {
    "text": "show you here for phase one for the first eight hundred and fifty six of those genomes how the actual spot price",
    "start": "1390460",
    "end": "1398890"
  },
  {
    "text": "went for us so we found that every genome went through in about twenty two hours the spot price sorry the spot",
    "start": "1398890",
    "end": "1406780"
  },
  {
    "text": "instances took a maximum of four tries to run the thing if you lost the spot",
    "start": "1406780",
    "end": "1412660"
  },
  {
    "text": "instance for instance as you were going but we found that a lot of our losses in terms of losing instances came from out",
    "start": "1412660",
    "end": "1419919"
  },
  {
    "text": "of memory errors because we'd under provisioned the RAM in some cases and also from accidental terminations so",
    "start": "1419919",
    "end": "1427570"
  },
  {
    "text": "beautifully it only took us about 1040 jobs and attempts to get through the",
    "start": "1427570",
    "end": "1434950"
  },
  {
    "text": "entire data set at a much better price so this is the fluctuation we observed",
    "start": "1434950",
    "end": "1440169"
  },
  {
    "start": "1437000",
    "end": "1437000"
  },
  {
    "text": "in spot price we found that the pricing stayed constant if we pushed it even as",
    "start": "1440169",
    "end": "1446770"
  },
  {
    "text": "high as about a hundred and fifty SC 3/8 X large instances and 200 are 3/8 X",
    "start": "1446770",
    "end": "1451840"
  },
  {
    "text": "Lodge which are huge that's 10,000 cores we can utilize before Amazon even starts",
    "start": "1451840",
    "end": "1458350"
  },
  {
    "text": "charging us more so as long as you keep a maximum maximum spot price you'll find",
    "start": "1458350",
    "end": "1463990"
  },
  {
    "start": "1462000",
    "end": "1462000"
  },
  {
    "text": "that there's no problem getting the exact same costs deleted one thing that we observed cost us a bit",
    "start": "1463990",
    "end": "1471470"
  },
  {
    "text": "more than we expected was the GNAT instances and all I can say to that is the larger instances have much bigger",
    "start": "1471470",
    "end": "1478160"
  },
  {
    "text": "bandwidth but of course they'll cost you more and we've worked with our AWS team",
    "start": "1478160",
    "end": "1483679"
  },
  {
    "text": "since and they've suggested a few nice fixes along along with the possibility",
    "start": "1483679",
    "end": "1489110"
  },
  {
    "text": "of using squid proxy of course if you go straight across Amazon you don't need the data movers at all but since we used",
    "start": "1489110",
    "end": "1495920"
  },
  {
    "text": "the data movers that came to about 2.4 k out of our total budget you'll find that",
    "start": "1495920",
    "end": "1502940"
  },
  {
    "text": "for 856 samples the first 856 we've spent about 15,000 on compute it's about",
    "start": "1502940",
    "end": "1509360"
  },
  {
    "text": "seventeen point six dollars per genome our data egress costs came to about",
    "start": "1509360",
    "end": "1514550"
  },
  {
    "text": "eight point seven dollars for genomes and then stashing everything on s3 gave us an extra two point two so that's in",
    "start": "1514550",
    "end": "1522170"
  },
  {
    "text": "the US dollars as Amazon reports in so that comes to about thirty seven point three three",
    "start": "1522170",
    "end": "1528770"
  },
  {
    "text": "Australian dollars per genome which is quite an great price from our",
    "start": "1528770",
    "end": "1533900"
  },
  {
    "text": "perspective so that's basically it that's our pipeline and you'll find that",
    "start": "1533900",
    "end": "1539179"
  },
  {
    "text": "it's basically already optimized for the cloud it's already doing everything it's supposed to but there's already there's",
    "start": "1539179",
    "end": "1544550"
  },
  {
    "text": "still more that we can do with it we're able to actually adjust our data metrics and also have the possibility of using",
    "start": "1544550",
    "end": "1553760"
  },
  {
    "text": "more instance types and that one is something I really want to talk to I mean first of all data metrics were a",
    "start": "1553760",
    "end": "1560150"
  },
  {
    "text": "mistake on our part caused a lot of out of memory errors which ended certain",
    "start": "1560150",
    "end": "1565700"
  },
  {
    "text": "jobs before that time we'll find much better results having fixed that problem but using more",
    "start": "1565700",
    "end": "1571700"
  },
  {
    "text": "instance types allows us to use spot to the fullest if we develop more docker containers we're able to use those",
    "start": "1571700",
    "end": "1577760"
  },
  {
    "text": "docker containers optimize them for different instance types and have a",
    "start": "1577760",
    "end": "1583190"
  },
  {
    "text": "double your spot fleet pick out the particular types of instances that would be best for us and just assign the",
    "start": "1583190",
    "end": "1589280"
  },
  {
    "text": "appropriate container to each which could save us even more money finally phase four we could optimize out the NAP",
    "start": "1589280",
    "end": "1596240"
  },
  {
    "text": "instances you squid proxy or get rid of them altogether and finally there's an exciting new",
    "start": "1596240",
    "end": "1602899"
  },
  {
    "text": "option available called AWS batch so I'll have Ben come up and speak to that",
    "start": "1602899",
    "end": "1608289"
  },
  {
    "text": "but first I'd just like to give a quick special thanks shout out to our sponsors so New South Wales o H Jamar gave us a",
    "start": "1608289",
    "end": "1616850"
  },
  {
    "text": "great deal of funding for this research study Asprey provided us samples and so",
    "start": "1616850",
    "end": "1623210"
  },
  {
    "text": "did the 45 and up study oops and of course I'd like to thank Amazon for",
    "start": "1623210",
    "end": "1629269"
  },
  {
    "text": "giving us special pricing as a research entity and for these invaluable assistance of Ben and Jamie over to Ben",
    "start": "1629269",
    "end": "1637269"
  },
  {
    "text": "thanks Liv you okay mister well I'm",
    "start": "1637269",
    "end": "1645470"
  },
  {
    "text": "gonna give you an introduction very quickly to batch the AWS batch was a new solution from Amazon for batch",
    "start": "1645470",
    "end": "1651919"
  },
  {
    "text": "processing very quickly in the last few minutes so first of all can I get an",
    "start": "1651919",
    "end": "1658399"
  },
  {
    "text": "indication from anyone in the room who's actually put together some kind of batch workload themselves before yeah quite a",
    "start": "1658399",
    "end": "1666440"
  },
  {
    "text": "few of you and you would know as Liv you kind of alluded to it's been a real",
    "start": "1666440",
    "end": "1672259"
  },
  {
    "text": "pleasure working with genome one in the garbin Institute because they kind of make really complex things look easy but",
    "start": "1672259",
    "end": "1678730"
  },
  {
    "text": "these things are actually still quite complicated even though that well understood and we've done it so many times in AWS themselves have built so",
    "start": "1678730",
    "end": "1686419"
  },
  {
    "text": "many different batch computing engines to create the services that we use you",
    "start": "1686419",
    "end": "1692679"
  },
  {
    "text": "know there's you know using spot fleet and SNS and SQS",
    "start": "1692679",
    "end": "1698379"
  },
  {
    "text": "s3 DynamoDB and creating all of the automations around this stuff is",
    "start": "1698379",
    "end": "1703639"
  },
  {
    "text": "something you'll have to create and then also maintain so a device has released a",
    "start": "1703639",
    "end": "1711080"
  },
  {
    "start": "1708000",
    "end": "1708000"
  },
  {
    "text": "diverse batch which is a managed service for taking that all of that undifferentiated heavy lifting off your",
    "start": "1711080",
    "end": "1716690"
  },
  {
    "text": "hands and making it much simpler to do it will handle installing all of the",
    "start": "1716690",
    "end": "1722359"
  },
  {
    "text": "server setting up all the servers managing the compute managing the the",
    "start": "1722359",
    "end": "1728149"
  },
  {
    "text": "cost optimization for you managing spot market and also integrating in with",
    "start": "1728149",
    "end": "1733610"
  },
  {
    "text": "lots of AWS services like s3 DynamoDB and even recognition to create quite",
    "start": "1733610",
    "end": "1740270"
  },
  {
    "text": "complex pipelines are all with all with a managed service so just a few concepts",
    "start": "1740270",
    "end": "1748040"
  },
  {
    "start": "1745000",
    "end": "1745000"
  },
  {
    "text": "to go over with AWS batch you create jobs so you create a job definition that",
    "start": "1748040",
    "end": "1753230"
  },
  {
    "text": "defines what vcp use it's going to use what image it needs to use what storage",
    "start": "1753230",
    "end": "1759679"
  },
  {
    "text": "is going to mount and other sort of characteristics of the job and then when",
    "start": "1759679",
    "end": "1765559"
  },
  {
    "text": "you submit a job - a job queue it'll be automatically taken up by the scheduler and and run in the appropriate computer",
    "start": "1765559",
    "end": "1775130"
  },
  {
    "text": "environment so this is an example architecture it's pretty pretty pretty",
    "start": "1775130",
    "end": "1782210"
  },
  {
    "start": "1777000",
    "end": "1777000"
  },
  {
    "text": "common we'll see input files being put into s3 triggering a lambda function that calls submit - creating the job in",
    "start": "1782210",
    "end": "1790160"
  },
  {
    "text": "batch which puts it on a runnable queue and the scheduler in batch will take up those jobs and put them into compute",
    "start": "1790160",
    "end": "1796490"
  },
  {
    "text": "environments that match those computing needs the results being put into s3 where they can then be used to for",
    "start": "1796490",
    "end": "1802460"
  },
  {
    "text": "analysis and those types of things now one of the great chances for optimization in genome one at the",
    "start": "1802460",
    "end": "1809990"
  },
  {
    "text": "government Institute and the genomics pipeline that olivia was talking about he's taking these each application as he",
    "start": "1809990",
    "end": "1817400"
  },
  {
    "text": "said has specific compute requirements so one needs lots of CPU one needs lots of memory ones leads lots of i/o and by",
    "start": "1817400",
    "end": "1824929"
  },
  {
    "text": "breaking those down into individual jobs we can use AWS batch to schedule them",
    "start": "1824929",
    "end": "1830480"
  },
  {
    "text": "into compute environments that match the computing needs of each of those jobs so there's going to be some great",
    "start": "1830480",
    "end": "1835669"
  },
  {
    "text": "opportunities for improvements into the future so just finishing up there that",
    "start": "1835669",
    "end": "1841340"
  },
  {
    "text": "the batch database batch is going to be able to give you a managed service to",
    "start": "1841340",
    "end": "1847850"
  },
  {
    "text": "manage your batch workloads there can be cost optimized they can either be resource optimized or can be time",
    "start": "1847850",
    "end": "1854750"
  },
  {
    "text": "optimized if you've got a deadline and it'll take away a lot of that work do you need otherwise need to do thank you",
    "start": "1854750",
    "end": "1860450"
  },
  {
    "text": "very much [Applause]",
    "start": "1860450",
    "end": "1866020"
  }
]