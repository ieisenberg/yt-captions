[
  {
    "start": "0",
    "end": "138000"
  },
  {
    "text": "hey everybody thanks so much for joining us today we have a really cool topic to share with you and I would love to",
    "start": "11590",
    "end": "18220"
  },
  {
    "text": "introduce Tom Lane who's gonna talk about the Karis MX net topic that we",
    "start": "18220",
    "end": "24100"
  },
  {
    "text": "have today so Tom why don't you go ahead and take it away we will have more attendees join here in the next couple",
    "start": "24100",
    "end": "29140"
  },
  {
    "text": "of minutes it usually starts ramping up right about now all right thanks for the quick introduction there yeah sure my is Tom",
    "start": "29140",
    "end": "36490"
  },
  {
    "text": "Lane and I work as a machine learning scientists at AWS specifically I've been",
    "start": "36490",
    "end": "41829"
  },
  {
    "text": "working with the MX net deep learning framework and work with both internal",
    "start": "41829",
    "end": "47200"
  },
  {
    "text": "and external customers to help them use MX net and I've helped a few customers",
    "start": "47200",
    "end": "52360"
  },
  {
    "text": "use Karis MX net and that's the topic of our conversation today so some of you",
    "start": "52360",
    "end": "59020"
  },
  {
    "text": "may be familiar with Karis already so Karis is a really user-friendly Python",
    "start": "59020",
    "end": "65710"
  },
  {
    "text": "package for deep learning and so deep",
    "start": "65710",
    "end": "72310"
  },
  {
    "text": "learning that's got a whole host of of different applications so we've got",
    "start": "72310",
    "end": "78789"
  },
  {
    "text": "natural language processing is a really important component and so just an",
    "start": "78789",
    "end": "84640"
  },
  {
    "text": "example of that is for question answering this is one thing in too many different things in NLP so if you've",
    "start": "84640",
    "end": "91299"
  },
  {
    "text": "given a body of text and you have a question then you can identify the section of the text that answers that",
    "start": "91299",
    "end": "98229"
  },
  {
    "text": "question the best we've also got",
    "start": "98229",
    "end": "103749"
  },
  {
    "text": "computer vision as a whole new section as well and so this is an example of instance segmentation so we take in an",
    "start": "103749",
    "end": "111189"
  },
  {
    "text": "image and then we're trying to identify different objects and the regions around each and then lastly reinforcement",
    "start": "111189",
    "end": "121240"
  },
  {
    "text": "learning is to another area so it's got a lot of Attraction recently especially deep reinforcement learning and we can",
    "start": "121240",
    "end": "127869"
  },
  {
    "text": "use carrots for for reinforcement learning to create a network that can decide the best actions to take",
    "start": "127869",
    "end": "135990"
  },
  {
    "start": "138000",
    "end": "138000"
  },
  {
    "text": "and so as I mentioned before it's a really user friendly API so we have this",
    "start": "145670",
    "end": "152060"
  },
  {
    "text": "is just going to take an example of a really simple neural network and it's got a few different layers so these are",
    "start": "152060",
    "end": "157610"
  },
  {
    "text": "just fully connected layers and with carrots really easy to do so we're just going to look at the piping code for creating",
    "start": "157610",
    "end": "164120"
  },
  {
    "text": "this network we've got a few different imports that we need to do so we're going to create a second sequential",
    "start": "164120",
    "end": "171110"
  },
  {
    "text": "network because we've got lots of layers stacked upon each other and then each of the layers is what's called a dense",
    "start": "171110",
    "end": "177050"
  },
  {
    "text": "layer and so to create this fully connected Network we start by creating",
    "start": "177050",
    "end": "182300"
  },
  {
    "text": "the sequential blog and then we add a number of dense layers each with different number of units we've got",
    "start": "182300",
    "end": "188210"
  },
  {
    "text": "three units followed by another layer 3 units and finally the outer layer has 2 and then we can apply different",
    "start": "188210",
    "end": "194420"
  },
  {
    "text": "activation so this is really typical neural network you're not going to be able to apply this network to the",
    "start": "194420",
    "end": "200900"
  },
  {
    "text": "examples we saw previously but it's showing you for the most symbol character we can do with carrots and",
    "start": "200900",
    "end": "206180"
  },
  {
    "text": "then the the final steps for training a network firstly we've got the compile",
    "start": "206180",
    "end": "211670"
  },
  {
    "text": "stage and it's just gonna build the graph all these operators and then",
    "start": "211670",
    "end": "217400"
  },
  {
    "text": "lastly apply a fit function this is similar to what you find with SCI can learn for example and we have input data",
    "start": "217400",
    "end": "225890"
  },
  {
    "text": "is a extra layer Y training and then we",
    "start": "225890",
    "end": "231020"
  },
  {
    "text": "run this for 5 epochs we passed through this dataset five times and there we go",
    "start": "231020",
    "end": "236210"
  },
  {
    "text": "with carrots we've already trained the first neural network and so today's",
    "start": "236210",
    "end": "242420"
  },
  {
    "start": "242000",
    "end": "242000"
  },
  {
    "text": "presentation we're talking about caris MX net and so what exactly is this",
    "start": "242420",
    "end": "247930"
  },
  {
    "text": "so in the last code example we're working with a front-end language a",
    "start": "247930",
    "end": "253760"
  },
  {
    "text": "front-end Python package which is called chaos but then under the hood all the",
    "start": "253760",
    "end": "258799"
  },
  {
    "text": "operations have been performed on a back-end and so this is where you have the optimized operations and so when",
    "start": "258799",
    "end": "265790"
  },
  {
    "text": "Carol started one of the most common backends was Theano but it's no longer",
    "start": "265790",
    "end": "271160"
  },
  {
    "text": "being developed and so the the typical default most people using with carrots now is tensorflow but if we actually",
    "start": "271160",
    "end": "279850"
  },
  {
    "text": "transition the backend to MX net there's a number of key advantages and so they're the kind of things I want to run",
    "start": "279850",
    "end": "285760"
  },
  {
    "text": "through with you today and this",
    "start": "285760",
    "end": "291610"
  },
  {
    "text": "presentation is going to be broken into two sections so the first section is",
    "start": "291610",
    "end": "296680"
  },
  {
    "text": "model training and then we'll be looking at deployment so the key benefits when",
    "start": "296680",
    "end": "303310"
  },
  {
    "text": "it comes to model training of using the MX net back-end for Karis is firstly it really speeds up training significantly",
    "start": "303310",
    "end": "310240"
  },
  {
    "text": "and I'll show you some benchmarks of how much that's been improvement we can get from that really good scaling",
    "start": "310240",
    "end": "317260"
  },
  {
    "text": "characteristics as well if you want to speed up your training even more you can use multiple GPUs to do this at multiple",
    "start": "317260",
    "end": "323440"
  },
  {
    "text": "instances and then really really easy to get started so I'll show you some",
    "start": "323440",
    "end": "328810"
  },
  {
    "text": "installation steps here we have a number of tools on AWS where carousel makes",
    "start": "328810",
    "end": "334090"
  },
  {
    "text": "notes pre-installed so you don't need to even do this yourself and then for deployment we'll look at how to deploy",
    "start": "334090",
    "end": "340720"
  },
  {
    "text": "with Amazon Sage maker we've also got a number of inference api's for MX now and",
    "start": "340720",
    "end": "347230"
  },
  {
    "text": "lastly we'll look at a few different open source packages that are separate from MX now but one is dedicated",
    "start": "347230",
    "end": "352690"
  },
  {
    "text": "specifically to serving models the MX net model server and by using MX net we",
    "start": "352690",
    "end": "359500"
  },
  {
    "text": "unlock a whole host of different tools in the MX net ecosystem for chaos so now",
    "start": "359500",
    "end": "366010"
  },
  {
    "text": "let's take a look at the training I mentioned really easy to get started",
    "start": "366010",
    "end": "372040"
  },
  {
    "start": "369000",
    "end": "369000"
  },
  {
    "text": "so with AWS typically you could either be using ec2 instances there is a DLR",
    "start": "372040",
    "end": "380620"
  },
  {
    "text": "meets for deep learning ami image that you can start for easy to and it has a",
    "start": "380620",
    "end": "385900"
  },
  {
    "text": "number of things pre-installed so it's got a whole host of deep learning frameworks including MX net10 Spallone",
    "start": "385900",
    "end": "392530"
  },
  {
    "text": "fly torch and it also has the cuda libraries install which are generally",
    "start": "392530",
    "end": "398770"
  },
  {
    "text": "quite difficult to set up yourself but if you set up a GPU instance you have a lab ready for you ready to go and then",
    "start": "398770",
    "end": "406540"
  },
  {
    "text": "another alternative is to use Amazon sage maker so here you have the likes of notebook instances and there's a similar thing",
    "start": "406540",
    "end": "413430"
  },
  {
    "text": "you have a number of environments Condor environments that have been set up for",
    "start": "413430",
    "end": "419009"
  },
  {
    "text": "you with all the deep learning packages and for carat MX now we want to be using",
    "start": "419009",
    "end": "424080"
  },
  {
    "text": "the MX net Condor environment and we can activate this on the ec2 instance or",
    "start": "424080",
    "end": "431039"
  },
  {
    "text": "taking maker instance by activating it with source activate MX net - p36 as that's just",
    "start": "431039",
    "end": "438900"
  },
  {
    "text": "noting that we want to be using Python 3.6 with MX net and now when we start",
    "start": "438900",
    "end": "445289"
  },
  {
    "text": "and run Python and import caris so this is as you would do normally it's gonna",
    "start": "445289",
    "end": "450690"
  },
  {
    "text": "by default we use the MX net back-end so there's been some configuration that's done when you've activated the",
    "start": "450690",
    "end": "456150"
  },
  {
    "text": "environment to make sure Kaos uses MX net and this as simple as that if you're using DL armies or sage maker but if",
    "start": "456150",
    "end": "464729"
  },
  {
    "text": "you've got the case where you're running your own machine maybe you want to try this out on your laptop you can go",
    "start": "464729",
    "end": "470639"
  },
  {
    "text": "through the install steps yourself but it's relatively easy so the first thing we need to do is install MX net because",
    "start": "470639",
    "end": "478110"
  },
  {
    "text": "that's the backend engine we can do that with pip install and then there's two variants that I'd recommend looking into",
    "start": "478110",
    "end": "485039"
  },
  {
    "text": "so we can optimize this install a little bit more if you're running an Intel CPU",
    "start": "485039",
    "end": "490560"
  },
  {
    "text": "then there's a package called mkl that will optimize so you just change the pip",
    "start": "490560",
    "end": "495750"
  },
  {
    "text": "install slightly and then if you've got a NVIDIA GPU you can use that to the full potential by pip installing and",
    "start": "495750",
    "end": "502650"
  },
  {
    "text": "makes net that's EU 92 and the number here denotes the version of CUDA Evo",
    "start": "502650",
    "end": "508199"
  },
  {
    "text": "installed so if you're running a different version of computer you need to change out slightly that was the eve back-end installs now",
    "start": "508199",
    "end": "515760"
  },
  {
    "text": "you want Karis MX net so again we can just go through install Karis MX net now",
    "start": "515760",
    "end": "521789"
  },
  {
    "text": "get everything configured for you there's a lot more details on the MX net website for more spoke installations if",
    "start": "521789",
    "end": "529020"
  },
  {
    "text": "you want to build from source for example we don't typically need to do that you can just if install and so once",
    "start": "529020",
    "end": "537540"
  },
  {
    "text": "you've Pippin still Karis MX net everything gets set up and there's one really important file for care",
    "start": "537540",
    "end": "543180"
  },
  {
    "text": "it's a configuration file after we've done the install this is what the the",
    "start": "543180",
    "end": "549660"
  },
  {
    "text": "default would look like the key thing to know here is the backend parameter has",
    "start": "549660",
    "end": "554820"
  },
  {
    "text": "been set to MX now so when we import chaos we're going to use the MX net",
    "start": "554820",
    "end": "560160"
  },
  {
    "text": "back-end and essentially that's all we need to do so if you've got a previous Kerris code that you've written and you",
    "start": "560160",
    "end": "566820"
  },
  {
    "text": "want to try out the MX net out version just follow those either source activate",
    "start": "566820",
    "end": "572340"
  },
  {
    "text": "the MX net environment or pip install Kerris MX net and then you can run your care asset code and just give things a",
    "start": "572340",
    "end": "578370"
  },
  {
    "text": "try and see how much it improves your performance I just want to flag one potential issue",
    "start": "578370",
    "end": "585540"
  },
  {
    "start": "583000",
    "end": "583000"
  },
  {
    "text": "that you may come across so it's in terms of the operators that are supported so the vast majority of",
    "start": "585540",
    "end": "592440"
  },
  {
    "text": "operators are supported by Karis MX now so there's the likes of the convolutional operators with current",
    "start": "592440",
    "end": "598350"
  },
  {
    "text": "operators LST ohms and a number of sparse operators but occasionally you may find an operator these are more of",
    "start": "598350",
    "end": "604890"
  },
  {
    "text": "the niche operators that aren't fully supported right now things like convalesce TMS and the map function and",
    "start": "604890",
    "end": "614810"
  },
  {
    "text": "typically if you've written Karis code and you've used the backends functions",
    "start": "614810",
    "end": "620520"
  },
  {
    "text": "quite a lot so if you're using carrots with tensorflow and you've used tensorflow operations",
    "start": "620520",
    "end": "625920"
  },
  {
    "text": "when you switch to Karis MX net that's not necessarily gonna work but there's you there's more often than not",
    "start": "625920",
    "end": "632330"
  },
  {
    "text": "workarounds with MX net equivalents but then what may mean that you can't just",
    "start": "632330",
    "end": "638280"
  },
  {
    "text": "hit install before first majority of cases this as possible and then there's",
    "start": "638280",
    "end": "643560"
  },
  {
    "text": "an issue tracking the current limited supported operators and with potential",
    "start": "643560",
    "end": "648660"
  },
  {
    "text": "workarounds I mentioned you get a huge speed-up in training performance and one",
    "start": "648660",
    "end": "657090"
  },
  {
    "start": "652000",
    "end": "652000"
  },
  {
    "text": "of the standard ways to measure this is on the image net classification challenge so this is a day set of",
    "start": "657090",
    "end": "664230"
  },
  {
    "text": "million images across 8,000 classes and you have to classify each of those",
    "start": "664230",
    "end": "669930"
  },
  {
    "text": "images to achieve a given class and one of the most common networks neural",
    "start": "669930",
    "end": "675060"
  },
  {
    "text": "networks to you for this problem is a ResNet ResNet 50",
    "start": "675060",
    "end": "680170"
  },
  {
    "text": "and if we look at the performance statistics with chaos with tensorflow on a GPU compared to chaos with MX net on",
    "start": "680170",
    "end": "688550"
  },
  {
    "text": "GPU we can see we get a huge increase in performance so we're looking here at the seconds it takes to process a thousand",
    "start": "688550",
    "end": "696290"
  },
  {
    "text": "images and we've gone now from just under 20 seconds to around about 7 seconds with carousel mix now just by",
    "start": "696290",
    "end": "702800"
  },
  {
    "text": "switching it would be the same caris code for these two cases just to",
    "start": "702800",
    "end": "709399"
  },
  {
    "text": "know here we're using one GPU on a p38 X large we could just have a p3 2x large",
    "start": "709399",
    "end": "716779"
  },
  {
    "text": "because it has one GPU but this hardware was used and we'll see a bit later on for multiple GPU benchmarking and",
    "start": "716779",
    "end": "724959"
  },
  {
    "text": "another benefit of Karis MX now in addition to the speed-up is the fact",
    "start": "724959",
    "end": "731300"
  },
  {
    "text": "that it's more memory efficient so tensorflow typically takes the whole of a memory allocation of your GPU + MX",
    "start": "731300",
    "end": "739220"
  },
  {
    "text": "Ness default is to take only the slice that it needs but you can also fit bigger batch sizes in memory for Karis",
    "start": "739220",
    "end": "746149"
  },
  {
    "text": "MX net which then further improves training but for the benchmarks you're seeing here we use the fixed batch size",
    "start": "746149",
    "end": "752269"
  },
  {
    "text": "of 32 it's a similar thing on other benchmarks so that was for GPU and now",
    "start": "752269",
    "end": "759290"
  },
  {
    "text": "this case for CPU a slightly different data set here so we're working with",
    "start": "759290",
    "end": "765290"
  },
  {
    "text": "smaller images as called see for 10 again same concept of image classification and we see an improvement",
    "start": "765290",
    "end": "772130"
  },
  {
    "text": "within carrots and looks now but it's not just on convolutional networks that",
    "start": "772130",
    "end": "780170"
  },
  {
    "text": "we see this increase so also for NLP you might be using LS TMS with GPU we see a",
    "start": "780170",
    "end": "788410"
  },
  {
    "text": "significant improvement with Karis MMX net and this is for text generation challenge but there are some cases where",
    "start": "788410",
    "end": "796670"
  },
  {
    "text": "Cara semak's net could potentially be slower this is usually limited to running lsdm networks on training lsdm",
    "start": "796670",
    "end": "805640"
  },
  {
    "text": "networks on CPU as just two operators that that are currently being worked on to",
    "start": "805640",
    "end": "811710"
  },
  {
    "text": "improve that performance but the vast majority of training typically takes place on GPU so this isn't a huge issue",
    "start": "811710",
    "end": "817470"
  },
  {
    "text": "for training and then because we use in MX nap we can make use of all the",
    "start": "817470",
    "end": "825570"
  },
  {
    "start": "821000",
    "end": "821000"
  },
  {
    "text": "tooling provided for training on MX that typically and one of the useful tools is more for advanced users is the MX net",
    "start": "825570",
    "end": "833790"
  },
  {
    "text": "profiler and so some of you may have profiled your code previously and this",
    "start": "833790",
    "end": "839070"
  },
  {
    "text": "is the deep learning equivalent to your profiling your code you can start the",
    "start": "839070",
    "end": "844950"
  },
  {
    "text": "profiler in a number of ways but one of the simplest ways is just set an environment variable before you run your",
    "start": "844950",
    "end": "850920"
  },
  {
    "text": "code here we're just setting auto start to one and then run your your script as",
    "start": "850920",
    "end": "857070"
  },
  {
    "text": "you normally would and when your script completes you usually get an extra file which is called profile Jason and inside",
    "start": "857070",
    "end": "865890"
  },
  {
    "text": "this file is all the information about how long each of the operators has taken and how everything fits together on a",
    "start": "865890",
    "end": "871740"
  },
  {
    "text": "timeline and the format of this file is in the same format that chrome uses for",
    "start": "871740",
    "end": "878190"
  },
  {
    "text": "its internal tracing so you can open up Chrome go to the tracing tool and then",
    "start": "878190",
    "end": "883740"
  },
  {
    "text": "open this file and you get a really nice user interface which shows you a timeline of all the operators and so",
    "start": "883740",
    "end": "889980"
  },
  {
    "text": "just in the background you can see a caption or a screenshot sorry of one",
    "start": "889980",
    "end": "896850"
  },
  {
    "text": "potential operator so here we're seeing convolution followed by a batch norm and then an activation that's a typical",
    "start": "896850",
    "end": "902760"
  },
  {
    "text": "block you see in lots of neural networks will you see the convolution is taking the longest out of the operators but",
    "start": "902760",
    "end": "908670"
  },
  {
    "text": "this is a really good way of identifying potential bottlenecks so it could be in data loading or it could be in model",
    "start": "908670",
    "end": "914940"
  },
  {
    "text": "training the forward of the backward pass it's just really good for the advanced users now this is a really",
    "start": "914940",
    "end": "923490"
  },
  {
    "start": "922000",
    "end": "922000"
  },
  {
    "text": "important point to make about model performance on training the default with",
    "start": "923490",
    "end": "929940"
  },
  {
    "text": "carrots is to use something called channel last and you may have noticed",
    "start": "929940",
    "end": "935760"
  },
  {
    "text": "that in the config file that that was specified what that means is if we have a number",
    "start": "935760",
    "end": "942420"
  },
  {
    "text": "of images they're going to be stored in an array in a in a certain order so",
    "start": "942420",
    "end": "948330"
  },
  {
    "text": "let's take the case where we have a hundred color images and the height and width of these images is 32 pixels by 32",
    "start": "948330",
    "end": "955950"
  },
  {
    "text": "pixels and so the default array will save this in n HWC format these are",
    "start": "955950",
    "end": "963810"
  },
  {
    "text": "common abbreviations but what it stands for is the N is for the number the batch size we have a hundred H and W our",
    "start": "963810",
    "end": "971430"
  },
  {
    "text": "height and width so we see 32 and then C is the channel so we're working with",
    "start": "971430",
    "end": "976890"
  },
  {
    "text": "color images which typically we have a red green and blue channels which are three and so the data shape will be a",
    "start": "976890",
    "end": "984810"
  },
  {
    "text": "hundred by 32 by 32 by 3 but this isn't the most efficient way of storing our",
    "start": "984810",
    "end": "991410"
  },
  {
    "text": "data so especially for NVIDIA GPUs it's much more efficient to have what's",
    "start": "991410",
    "end": "996600"
  },
  {
    "text": "called channel first it's not quite the first because we have the batch size first we have n CHW this time and so we",
    "start": "996600",
    "end": "1005060"
  },
  {
    "text": "just move that channel axis a little bit further up and the operators on the",
    "start": "1005060",
    "end": "1010130"
  },
  {
    "text": "nvidia gpus are much more optimized for this and so we can improve training performance substantially so I said this",
    "start": "1010130",
    "end": "1018710"
  },
  {
    "text": "wasn't the default for and Kara semak's now but if you're using sage maker or",
    "start": "1018710",
    "end": "1025310"
  },
  {
    "text": "the DLR me when you activate the environment that you need it will",
    "start": "1025310",
    "end": "1031069"
  },
  {
    "text": "actually default to channel first so you don't need to do anything so it's only the case where you've installed Kara so",
    "start": "1031070",
    "end": "1038150"
  },
  {
    "text": "mixing it yourself so this could be on your laptop that you need to go into the config file and just change that from",
    "start": "1038150",
    "end": "1044089"
  },
  {
    "text": "channel last to channel first and then you're going to be able to make use of all these improved operators it is",
    "start": "1044090",
    "end": "1052910"
  },
  {
    "text": "possible as well to change this option from inside the code you don't need to do it through the config file so if you",
    "start": "1052910",
    "end": "1060200"
  },
  {
    "text": "import the back end for Kerris you can then set the data format to channel",
    "start": "1060200",
    "end": "1066050"
  },
  {
    "text": "first and that's just an alternative way to do it",
    "start": "1066050",
    "end": "1070240"
  },
  {
    "text": "and we've changed how the operators are going to be processed but the next thing we need to do is convert our data to",
    "start": "1071440",
    "end": "1078530"
  },
  {
    "text": "that format may already be in the format but if you've got it in the NH WC format",
    "start": "1078530",
    "end": "1085370"
  },
  {
    "text": "with the channel last there's a utility just to change the axis to get it to the channel first as you can see that at the",
    "start": "1085370",
    "end": "1092480"
  },
  {
    "text": "top line and then the only other change we need to make is to really the first layer in your network where you provide",
    "start": "1092480",
    "end": "1100490"
  },
  {
    "text": "the input shape for channel last the input shape of this convolution would",
    "start": "1100490",
    "end": "1105650"
  },
  {
    "text": "have the 3 at the end but now we have channel first we move the threat to the start and with those two changes we can",
    "start": "1105650",
    "end": "1112370"
  },
  {
    "text": "see the improved performance now for scaling we see really good scaling",
    "start": "1112370",
    "end": "1119390"
  },
  {
    "start": "1116000",
    "end": "1116000"
  },
  {
    "text": "characteristics across multiple GPUs so we've already seen the case of imagenet",
    "start": "1119390",
    "end": "1124669"
  },
  {
    "text": "training on a resonate 50 on it 1 GPU but then we see a similar thing for for",
    "start": "1124669",
    "end": "1130730"
  },
  {
    "text": "GPUs and 80 views where Kara semak's net is much quicker than carrots tons of load for these cases we go from around",
    "start": "1130730",
    "end": "1139580"
  },
  {
    "text": "about 7 seconds per thousand images for one GPU down to one to two seconds it's",
    "start": "1139580",
    "end": "1145340"
  },
  {
    "text": "good scaling with a GPU and if we've got faster training that just means we can it's very much quicker we can run more",
    "start": "1145340",
    "end": "1152330"
  },
  {
    "text": "experiments and get to a better model ultimately in the code we just need to",
    "start": "1152330",
    "end": "1162590"
  },
  {
    "text": "add one extra line it's as simple as that there's a utility again called",
    "start": "1162590",
    "end": "1168559"
  },
  {
    "text": "multi-gpu model where we provide the existing model specify the number of GPUs to use and then we can compile the",
    "start": "1168559",
    "end": "1176360"
  },
  {
    "text": "model after that as you want to choose the number of GPUs that the machine has",
    "start": "1176360",
    "end": "1181790"
  },
  {
    "text": "no more otherwise you will get there's",
    "start": "1181790",
    "end": "1186280"
  },
  {
    "start": "1187000",
    "end": "1187000"
  },
  {
    "text": "one other tool in the MX in the ecosystem is really useful for training is something called MX board so separate",
    "start": "1187720",
    "end": "1195620"
  },
  {
    "text": "from the MX net project it's an open source project and this is an",
    "start": "1195620",
    "end": "1201080"
  },
  {
    "text": "integration for 10 suppor so some of you may have used 10 support before a really great way to visualize",
    "start": "1201080",
    "end": "1207310"
  },
  {
    "text": "all those metrics MX board is a pison package to save MX net data in a format",
    "start": "1207310",
    "end": "1215020"
  },
  {
    "text": "that can be read by tens aboard so in this case we're just writing data in a",
    "start": "1215020",
    "end": "1220690"
  },
  {
    "text": "histogram format we import MX board and specifically in the summary writer and",
    "start": "1220690",
    "end": "1226420"
  },
  {
    "text": "then we add histogram on that summary writer with it could be a loss data for",
    "start": "1226420",
    "end": "1232780"
  },
  {
    "text": "example look at the distribution of the loss while we're training and to use this with Karis MX now there's something",
    "start": "1232780",
    "end": "1240820"
  },
  {
    "text": "called a callback which is a function that's called at certain points in your training you can have a callback at the",
    "start": "1240820",
    "end": "1248140"
  },
  {
    "text": "end of an epoch for example that would then call the summary writer and save the lost distributions and then the",
    "start": "1248140",
    "end": "1260290"
  },
  {
    "start": "1259000",
    "end": "1259000"
  },
  {
    "text": "final step of training is for the model export so after we've done our training",
    "start": "1260290",
    "end": "1269020"
  },
  {
    "text": "with the fit function we can then go caris models and save MX net model the",
    "start": "1269020",
    "end": "1274600"
  },
  {
    "text": "two things that we specify here are firstly the Train model and secondly a prefix it's just it gets added to the",
    "start": "1274600",
    "end": "1282760"
  },
  {
    "text": "start of the model files and so after we run that function we'll have created two files so the",
    "start": "1282760",
    "end": "1290920"
  },
  {
    "text": "first one is a symbol dot JSON file and that describes the architecture of the",
    "start": "1290920",
    "end": "1296290"
  },
  {
    "text": "network so which operations are performed in different orders and then",
    "start": "1296290",
    "end": "1301390"
  },
  {
    "text": "there's parameters file so inside the network as the number of parameters that are learnt and the parameter file",
    "start": "1301390",
    "end": "1307600"
  },
  {
    "text": "contains all that information so let's just have a quick code walk through of",
    "start": "1307600",
    "end": "1315370"
  },
  {
    "start": "1312000",
    "end": "1312000"
  },
  {
    "text": "doing this on a stage maker notebook and we're gonna run a Karass MX net code to",
    "start": "1315370",
    "end": "1321340"
  },
  {
    "text": "train a convolutional neural network just in a really simple case I'm just",
    "start": "1321340",
    "end": "1326470"
  },
  {
    "text": "going to escape the presentation for a second",
    "start": "1326470",
    "end": "1331200"
  },
  {
    "text": "okay so we've moved across two and this",
    "start": "1338440",
    "end": "1345919"
  },
  {
    "text": "is a sage maker notebook just like Jupiter so we have Jupiter notebooks",
    "start": "1345919",
    "end": "1351620"
  },
  {
    "text": "that we can run and each notebook is running in a certain environment so I've activated the MX net environment and so",
    "start": "1351620",
    "end": "1360080"
  },
  {
    "text": "my configuration file for Kaos has been set with channel first which as I",
    "start": "1360080",
    "end": "1365179"
  },
  {
    "text": "mentioned is the more optimal choice we have a number of imports here so we have",
    "start": "1365179",
    "end": "1371960"
  },
  {
    "text": "a we're going to use the dataset M mist and so that's just handwriting recognition and then the layers so we're",
    "start": "1371960",
    "end": "1380539"
  },
  {
    "text": "going to be using convolutional net work so we import layers such as comm 2d it's",
    "start": "1380539",
    "end": "1385669"
  },
  {
    "text": "gonna be a two-dimensional convolution and then as mentioned we have a number",
    "start": "1385669",
    "end": "1390710"
  },
  {
    "text": "of utils so here two channel first because we're going to switch the dimensions of data we can create the",
    "start": "1390710",
    "end": "1400490"
  },
  {
    "text": "dataset this is just because it's a prebate dataset for Kara so you can just load the data we have the training split",
    "start": "1400490",
    "end": "1409159"
  },
  {
    "text": "and the test split we want to train on the training data but if we look at the",
    "start": "1409159",
    "end": "1414200"
  },
  {
    "text": "shape of the data we have 60,000 images 28 by 28 height and width but there's no",
    "start": "1414200",
    "end": "1421399"
  },
  {
    "text": "channel this is a grayscale image so we haven't got three in this case it's not a color image but to use the",
    "start": "1421399",
    "end": "1428450"
  },
  {
    "text": "convolutions in the most optimal way we do need to add this channel axis we're",
    "start": "1428450",
    "end": "1434029"
  },
  {
    "text": "dealing with number arrays so we can just do it in a typical number fashion of adding a new axis but then we need to",
    "start": "1434029",
    "end": "1440600"
  },
  {
    "text": "make sure that this channel is added channel first so we want the one before",
    "start": "1440600",
    "end": "1446179"
  },
  {
    "text": "the height and the width rather than at the end and so that's what I'm calling to channel first to be able to do this",
    "start": "1446179",
    "end": "1451419"
  },
  {
    "text": "so now we have the data in the right format we can do things such as normalized the data because we're",
    "start": "1451419",
    "end": "1458000"
  },
  {
    "text": "working when your networks want to have our data standardized so we're just going to convert to floats and then",
    "start": "1458000",
    "end": "1464389"
  },
  {
    "text": "divide by 250 because that's the typical maximum for images so now we have our data in the",
    "start": "1464389",
    "end": "1470520"
  },
  {
    "text": "range between 0 and 1 all those channels",
    "start": "1470520",
    "end": "1475370"
  },
  {
    "text": "oh.why variables is at the label what we're trying to do is predict the",
    "start": "1476090",
    "end": "1481590"
  },
  {
    "text": "class of a handwritten digit between 0 and 9 and so we one-hot encode those",
    "start": "1481590",
    "end": "1487260"
  },
  {
    "text": "class variables which we call two categorical then the model we're going",
    "start": "1487260",
    "end": "1495210"
  },
  {
    "text": "to be working with convolutional network so we create a sequential network and",
    "start": "1495210",
    "end": "1500549"
  },
  {
    "text": "add convolutional layers with different kernel sizes and filter sizes the next",
    "start": "1500549",
    "end": "1509640"
  },
  {
    "text": "part is to compile the network we're going to train this using the atom optimizer and use categorical cross",
    "start": "1509640",
    "end": "1516240"
  },
  {
    "text": "entropy locks which is used for categorical variables and then we fit a",
    "start": "1516240",
    "end": "1522150"
  },
  {
    "text": "model with the data we're going to run this for 5 epochs and then we get the logs of our training so each step took",
    "start": "1522150",
    "end": "1530340"
  },
  {
    "text": "in the order of about 3 seconds really really quick because we're working on a GPU with tiny data set and because",
    "start": "1530340",
    "end": "1537480"
  },
  {
    "text": "carrots that makes things quite quick as well we run a quick evaluation on this model we see the accuracy is very high",
    "start": "1537480",
    "end": "1544380"
  },
  {
    "text": "or a 99% and the last step would be to save the model so we're using the same",
    "start": "1544380",
    "end": "1550350"
  },
  {
    "text": "model function with a prefix of M missed CNN and then we can see we have the two",
    "start": "1550350",
    "end": "1557190"
  },
  {
    "text": "files that have been saved as mentioned we also get some useful information for deployment so the first point is",
    "start": "1557190",
    "end": "1564539"
  },
  {
    "text": "something called data names we're going to use this later and that's going to tell us which part of our graph is used",
    "start": "1564539",
    "end": "1571650"
  },
  {
    "text": "for the input and then we can see what the output is and for that input it also",
    "start": "1571650",
    "end": "1576720"
  },
  {
    "text": "tells us what shape the data should be so this is all going to be useful later and I'll show you what ok let's go back",
    "start": "1576720",
    "end": "1583230"
  },
  {
    "text": "to the presentation",
    "start": "1583230",
    "end": "1586370"
  },
  {
    "text": "so we're looking at now a model deployment typically the model training",
    "start": "1604790",
    "end": "1615120"
  },
  {
    "start": "1609000",
    "end": "1609000"
  },
  {
    "text": "will be done by a scientists but when it comes to model deployment there's a number of other components we need to think about so we've got engineering and",
    "start": "1615120",
    "end": "1623070"
  },
  {
    "text": "we've got operations as well so you may in your company have different teams working on each of these aspects but if",
    "start": "1623070",
    "end": "1631260"
  },
  {
    "text": "you're just a scientist and you want to have some of these deployment options supportive for you there are tools",
    "start": "1631260",
    "end": "1637500"
  },
  {
    "text": "within AWS to help you on the engineering operation side just to be able to deploy the models but we'll see",
    "start": "1637500",
    "end": "1643080"
  },
  {
    "text": "cases if your company has an engineering team operations team we'll have tools that fit with those as well and when it",
    "start": "1643080",
    "end": "1655740"
  },
  {
    "start": "1654000",
    "end": "1654000"
  },
  {
    "text": "comes to model serving there's a number of areas where we have undifferentiated",
    "start": "1655740",
    "end": "1662640"
  },
  {
    "text": "heavy lifting that AWS is going to do for you so when it comes to training",
    "start": "1662640",
    "end": "1668970"
  },
  {
    "text": "your model that's really where you can differentiate in terms of the performance of your model and what the model is doing but there are certain",
    "start": "1668970",
    "end": "1675960"
  },
  {
    "text": "areas that we always want to have good performance good availability and good",
    "start": "1675960",
    "end": "1681180"
  },
  {
    "text": "networking monitoring tools are really important as well to be able to see how",
    "start": "1681180",
    "end": "1686340"
  },
  {
    "text": "the model is scaling to see whether we need to increase resources or not and",
    "start": "1686340",
    "end": "1691350"
  },
  {
    "text": "then we want to be agnostic to a number of different things such as the model",
    "start": "1691350",
    "end": "1696570"
  },
  {
    "text": "itself we don't want to reinvent the wheel for just a slightly different variant of a model we want this to work",
    "start": "1696570",
    "end": "1701820"
  },
  {
    "text": "across frameworks no not just for tend to blow a mix nail height or example and we want to be able",
    "start": "1701820",
    "end": "1709350"
  },
  {
    "text": "to work cross-platform so that we could look at deployments on the cloud and we can also look for deployments on edge",
    "start": "1709350",
    "end": "1718520"
  },
  {
    "text": "and sage make is a real good way to go for the whole ml pipeline so we have",
    "start": "1719180",
    "end": "1727830"
  },
  {
    "start": "1720000",
    "end": "1720000"
  },
  {
    "text": "tools starting with labeling your data we've got stage maker ground truth after",
    "start": "1727830",
    "end": "1733230"
  },
  {
    "text": "doing active learning and labeling in the most efficient way as we saw before we had the notebooks and for for building your neural network",
    "start": "1733230",
    "end": "1742720"
  },
  {
    "text": "models and then doing the training which we can do on the notebooks or in sage maitreya mode and docker containers then",
    "start": "1742720",
    "end": "1749139"
  },
  {
    "text": "we have the tuner for automatic model tuning but the areas that I'm going to",
    "start": "1749139",
    "end": "1754240"
  },
  {
    "text": "focus on today are around compiling the model and optimizing that and then",
    "start": "1754240",
    "end": "1760149"
  },
  {
    "text": "deploying on either the cloud or edge so for edge deployments we're going to look at something called green Russ and ours",
    "start": "1760149",
    "end": "1770409"
  },
  {
    "start": "1769000",
    "end": "1769000"
  },
  {
    "text": "and say Jamaican neo is a model compiler that will take those model files that we",
    "start": "1770409",
    "end": "1776169"
  },
  {
    "text": "exported in the previous step and then compile it specifically for a type of hardware so we can choose between Intel",
    "start": "1776169",
    "end": "1784539"
  },
  {
    "text": "so if we're running on the cloud on an Intel optimized device we can optimize for this if we have a GPU we can do the",
    "start": "1784539",
    "end": "1791830"
  },
  {
    "text": "same for the video but if we're looking at edge deployments or maybe it's better if we target devices such as arm and",
    "start": "1791830",
    "end": "1799120"
  },
  {
    "text": "Qualcomm and we can do this with sage make India and why do we wanna optimize",
    "start": "1799120",
    "end": "1804820"
  },
  {
    "text": "well again it speeds up inference this time we like to speeds up for training but if we're using Neos just for",
    "start": "1804820",
    "end": "1811360"
  },
  {
    "start": "1805000",
    "end": "1805000"
  },
  {
    "text": "inference purposes this is a case for image net image classification again",
    "start": "1811360",
    "end": "1816490"
  },
  {
    "text": "same network on CPU and we can see before and after using sage making here",
    "start": "1816490",
    "end": "1822309"
  },
  {
    "text": "we got a huge improvements in the latency this is for a batch size of once we're just passing a single sample",
    "start": "1822309",
    "end": "1829470"
  },
  {
    "text": "through the network and we've gone from 180 milliseconds down to 20 milliseconds",
    "start": "1829470",
    "end": "1837120"
  },
  {
    "text": "and so how is this possible so just want to run quickly through a few different",
    "start": "1837120",
    "end": "1843370"
  },
  {
    "start": "1839000",
    "end": "1839000"
  },
  {
    "text": "types of optimizations that have been applied here they break down into two categories so firstly we've got graph",
    "start": "1843370",
    "end": "1849840"
  },
  {
    "text": "optimizations and then tensor optimizations and some of the most simple graph optimizations let's say",
    "start": "1849840",
    "end": "1857409"
  },
  {
    "text": "we've got a number of operators in a row so this is what the computation of graph might look like convolution activation",
    "start": "1857409",
    "end": "1864370"
  },
  {
    "text": "and then drop out before inference purposes we don't need some of these or operators so the likes of drop out we",
    "start": "1864370",
    "end": "1872529"
  },
  {
    "text": "can actually remove directly for inference which will improve performance we don't need to check whether we're training or are an inference and then we",
    "start": "1872529",
    "end": "1880570"
  },
  {
    "text": "can combine operators together so if we have a convolution and an activation",
    "start": "1880570",
    "end": "1886269"
  },
  {
    "text": "applied one after another we don't need to store the intermediate results and between these two we can apply them both",
    "start": "1886269",
    "end": "1893679"
  },
  {
    "text": "at the same time which will save time allocating memory and things like that and this is called diffusing and then if",
    "start": "1893679",
    "end": "1902710"
  },
  {
    "text": "we look at the individual operators so we saw how changing the channel order to",
    "start": "1902710",
    "end": "1908980"
  },
  {
    "text": "channel first had a huge improvement in speed or we can actually do similar things but changing the number that",
    "start": "1908980",
    "end": "1916840"
  },
  {
    "text": "we've got in each of these dimensions so this is called tiling where you just have a tiling factor to shift data and",
    "start": "1916840",
    "end": "1922929"
  },
  {
    "text": "this uses the cache in a more optimal way but this is all done for you don't you know exactly how to you're doing",
    "start": "1922929",
    "end": "1929380"
  },
  {
    "text": "these operations and then lastly vectorization is instead of having a",
    "start": "1929380",
    "end": "1934539"
  },
  {
    "text": "full loop through your data through different arrays for example we can just apply a single operator on two different",
    "start": "1934539",
    "end": "1941169"
  },
  {
    "text": "arrays together which is much more quicker in terms of processing time so",
    "start": "1941169",
    "end": "1948580"
  },
  {
    "text": "that was the compile stage is totally optional to compile your model you don't need to do it but if you do you get a",
    "start": "1948580",
    "end": "1955120"
  },
  {
    "text": "huge improvement in performance and then we can do this for cloud deployment and edge deployment so let's take a look now",
    "start": "1955120",
    "end": "1961600"
  },
  {
    "text": "at cloud deployment using sage maker and",
    "start": "1961600",
    "end": "1967330"
  },
  {
    "text": "we can do this using the sage maker Python SDK and these are kind of the main components that you'll see with",
    "start": "1967330",
    "end": "1974260"
  },
  {
    "text": "sage maker code so the first thing you create is something called an estimator and an estimator you then fit a model",
    "start": "1974260",
    "end": "1983139"
  },
  {
    "text": "with training data you can perform this in the notebook directly but more often",
    "start": "1983139",
    "end": "1990190"
  },
  {
    "text": "than not you want to do the training in a dedicated training environment and sage maker creates this environment for",
    "start": "1990190",
    "end": "1996039"
  },
  {
    "text": "you using docker containers where you can run training much more efficiently",
    "start": "1996039",
    "end": "2001129"
  },
  {
    "text": "so now you have your train model after cooling fit and",
    "start": "2001129",
    "end": "2006450"
  },
  {
    "text": "the train model is thought in s3 then when it comes to deployment you can take",
    "start": "2006450",
    "end": "2011520"
  },
  {
    "text": "the estimator and call dot deploy and this will convert your estimator into something called a predictor which is",
    "start": "2011520",
    "end": "2018360"
  },
  {
    "text": "ready to make inference calls and when we want to make a prediction we call the",
    "start": "2018360",
    "end": "2024840"
  },
  {
    "text": "predict function on the predictor and this is going to run at the model that",
    "start": "2024840",
    "end": "2029850"
  },
  {
    "text": "was saved in s3 in the prediction environment and again this is to between dock containers you don't have to cool",
    "start": "2029850",
    "end": "2038010"
  },
  {
    "text": "dot predict through sex age maker because an endpoint is created you could",
    "start": "2038010",
    "end": "2043140"
  },
  {
    "text": "use typical REST API clients and",
    "start": "2043140",
    "end": "2048600"
  },
  {
    "text": "services you could use curl from the command line for example you can do this for your browser and in the last diagram",
    "start": "2048600",
    "end": "2057929"
  },
  {
    "text": "our when we had the estimators are just here we have a script D Y so inside the",
    "start": "2057930",
    "end": "2063450"
  },
  {
    "text": "script is going to be an your training scripts but also some inference code and",
    "start": "2063450",
    "end": "2069149"
  },
  {
    "text": "so the key components for cloud employment there are defaults that are",
    "start": "2069150",
    "end": "2074550"
  },
  {
    "start": "2071000",
    "end": "2071000"
  },
  {
    "text": "specified for all these functions but you need to specify two ones in",
    "start": "2074550",
    "end": "2079740"
  },
  {
    "text": "particular and born so we've got the model function and this just tells sage maker how to load your model so we are",
    "start": "2079740",
    "end": "2087300"
  },
  {
    "text": "saving in MX net model so we can use the symbol block from blue one this is",
    "start": "2087300",
    "end": "2092730"
  },
  {
    "text": "pretty standard you can take this as is for most cases and then we pass in a few",
    "start": "2092730",
    "end": "2098490"
  },
  {
    "text": "different parameters so we have the two files that were exported by Karis MX now",
    "start": "2098490",
    "end": "2103710"
  },
  {
    "text": "we have the architecture file in parameters file and then we also specify the input node which was printed out",
    "start": "2103710",
    "end": "2111060"
  },
  {
    "text": "when we did these same model comparison it's now and that's all we need to do so now we have a network in MX now there's",
    "start": "2111060",
    "end": "2120240"
  },
  {
    "text": "also an input function if you don't define it it uses the default this just",
    "start": "2120240",
    "end": "2126230"
  },
  {
    "text": "pauses the input requests so we have a Jason input usually from the endpoint",
    "start": "2126230",
    "end": "2132540"
  },
  {
    "text": "that contains our data that we want prediction honest Kabir image for example but the input function by",
    "start": "2132540",
    "end": "2138940"
  },
  {
    "text": "default will process this for you and then after that we'll call the predict function and predict is just going to",
    "start": "2138940",
    "end": "2146230"
  },
  {
    "text": "apply the model to input data so here we provided an input that didn't have the",
    "start": "2146230",
    "end": "2154030"
  },
  {
    "text": "batch dimension and well this is just creating a batch of one and then we passed this batch with one sample",
    "start": "2154030",
    "end": "2160720"
  },
  {
    "text": "through the model to get our prediction and then the output function will be called at the end but again if you don't",
    "start": "2160720",
    "end": "2166900"
  },
  {
    "text": "specify this a default will be used and this just serializes the output",
    "start": "2166900",
    "end": "2173950"
  },
  {
    "text": "prediction and it means we can pass it through the endpoint you get our prediction and then the last thing we",
    "start": "2173950",
    "end": "2182080"
  },
  {
    "text": "just want to touch on for stage maker is the edge deployment so we've got a really good integration with a tool",
    "start": "2182080",
    "end": "2188920"
  },
  {
    "text": "called a WS IOT green grass and green grass is a good service for managing",
    "start": "2188920",
    "end": "2195370"
  },
  {
    "text": "edge devices or collections of edge devices where you once you've done your training on the cloud you can then",
    "start": "2195370",
    "end": "2202210"
  },
  {
    "text": "define something called a green grass group and inside the group you have a number of devices you designate one of",
    "start": "2202210",
    "end": "2209170"
  },
  {
    "text": "these devices as your core and this is the thing that's going to interact with",
    "start": "2209170",
    "end": "2214180"
  },
  {
    "text": "the cloud so once you've trained your than your network model and you've exported it you can deploy this as a",
    "start": "2214180",
    "end": "2220240"
  },
  {
    "text": "resource to the core and then the core can use this model as part of the lambda",
    "start": "2220240",
    "end": "2226060"
  },
  {
    "text": "function to interact with the devices so you may have a camera in one of these devices which will interact with the",
    "start": "2226060",
    "end": "2232570"
  },
  {
    "text": "core it will run the neural network model on the device on edge without ever",
    "start": "2232570",
    "end": "2237910"
  },
  {
    "text": "having to go back to the cloud and then in turn you can use maybe run object detection on that on that camera to then",
    "start": "2237910",
    "end": "2245350"
  },
  {
    "text": "decide to control other devices and this is all done on edge you don't need to do that and interact out of the cloud but",
    "start": "2245350",
    "end": "2252310"
  },
  {
    "text": "when you need to for updates and things like that and model deployments with",
    "start": "2252310",
    "end": "2257500"
  },
  {
    "text": "green grass the whole connection is secured and works with shadowing when the devices go offline",
    "start": "2257500",
    "end": "2265980"
  },
  {
    "text": "and so now let's have a quick walkthrough of some code of deployment just going to show you with Amazon sage",
    "start": "2267240",
    "end": "2273990"
  },
  {
    "text": "maker how easy this can be of the same model that we were working with before",
    "start": "2273990",
    "end": "2280130"
  },
  {
    "text": "so I mentioned there was this script py file a lot of this code will look",
    "start": "2292760",
    "end": "2298310"
  },
  {
    "text": "familiar to what we used before for training because I have training an inference code in the same file these",
    "start": "2298310",
    "end": "2305810"
  },
  {
    "text": "are the inference functions that we saw on the previous slide and then we have",
    "start": "2305810",
    "end": "2310970"
  },
  {
    "text": "the training from the notebook and so this is all just what in one single file",
    "start": "2310970",
    "end": "2316570"
  },
  {
    "text": "now from another notebook we can use the sage maker Python SDK so import from",
    "start": "2316570",
    "end": "2325340"
  },
  {
    "text": "Sage main Bursar's another Python package the MX net estimator that's the",
    "start": "2325340",
    "end": "2330440"
  },
  {
    "text": "important thing to note there there's a little bit set up we need to create a session and we get a role",
    "start": "2330440",
    "end": "2336110"
  },
  {
    "text": "so role is going to manage the permissions or running these containers the doctor containers and now we create",
    "start": "2336110",
    "end": "2343610"
  },
  {
    "text": "an MX net estimator we specify the script file they were interested in as",
    "start": "2343610",
    "end": "2350150"
  },
  {
    "text": "our training and inference code then we choose the instance type to do our",
    "start": "2350150",
    "end": "2355970"
  },
  {
    "text": "training so here I'm using a new view instance p32 x-large and specifying no one use MX net 1.3",
    "start": "2355970",
    "end": "2363350"
  },
  {
    "text": "with - version 3 and our script can take a number of parameters so I'm using",
    "start": "2363350",
    "end": "2370840"
  },
  {
    "text": "argument pausing in my character m-miss the py and so it's looking for",
    "start": "2370840",
    "end": "2376760"
  },
  {
    "text": "batch size and E box this is really general we can pass them whatever the media and this is how our automatic",
    "start": "2376760",
    "end": "2382490"
  },
  {
    "text": "model tuner integrates as well by bearing these high parameters find the best recreates the estimator and then we",
    "start": "2382490",
    "end": "2390560"
  },
  {
    "text": "can go ahead and call dot fit this is going to start the training job loading",
    "start": "2390560",
    "end": "2397640"
  },
  {
    "text": "the data in this case we have our data download defined in the script files for",
    "start": "2397640",
    "end": "2402860"
  },
  {
    "text": "you to pass in the data it's going to run we've got quite a lot of logging",
    "start": "2402860",
    "end": "2408950"
  },
  {
    "text": "output but it's going to run through the training and see it's just as quick as before and now we have our model saved in s3",
    "start": "2408950",
    "end": "2416690"
  },
  {
    "text": "and so for model deployment we can simply take our estimator",
    "start": "2416690",
    "end": "2422420"
  },
  {
    "text": "call the doc deploy and then choose the types of instance is that we want to deploy I'm not a lot",
    "start": "2422420",
    "end": "2427940"
  },
  {
    "text": "and here choosing a CPR instance lugging GPS now we get a predictor we can go",
    "start": "2427940",
    "end": "2435500"
  },
  {
    "text": "ahead and run our prediction so just the",
    "start": "2435500",
    "end": "2442370"
  },
  {
    "text": "distance you take a number for example and then we can cool a predictor with",
    "start": "2442370",
    "end": "2450290"
  },
  {
    "text": "dot predict and pass in the data that we've just used we're taking an Arg max",
    "start": "2450290",
    "end": "2455480"
  },
  {
    "text": "Hicks we want the most likely number we get probabilities of every number and its prediction from the model was",
    "start": "2455480",
    "end": "2461870"
  },
  {
    "text": "seventh which is what we just drew when you finish with an endpoint you should",
    "start": "2461870",
    "end": "2468170"
  },
  {
    "text": "go ahead and delete it just so it spins down this prediction environment",
    "start": "2468170",
    "end": "2475210"
  },
  {
    "text": "so that was a fine if you want to be working with a wsh maker but maybe you",
    "start": "2488760",
    "end": "2495210"
  },
  {
    "start": "2489000",
    "end": "2489000"
  },
  {
    "text": "have an engineering team that already has a deployment process already in a certain language so MX nap provides a",
    "start": "2495210",
    "end": "2502410"
  },
  {
    "text": "number of inference api's for training you'll be using the different interfaces",
    "start": "2502410",
    "end": "2508860"
  },
  {
    "text": "such as Python are in Julia but then the preferred API is for inference we have",
    "start": "2508860",
    "end": "2515370"
  },
  {
    "text": "inference API for C++ Scala and Java and in each of these we have customized",
    "start": "2515370",
    "end": "2524660"
  },
  {
    "text": "deployment options for different types of neural network tasks and these are",
    "start": "2524660",
    "end": "2531360"
  },
  {
    "text": "much more optimized so let's take scarlett as an example for object",
    "start": "2531360",
    "end": "2538500"
  },
  {
    "text": "detection maybe in Python at Sage maker we've trained using Cara semuc snap and",
    "start": "2538500",
    "end": "2544290"
  },
  {
    "text": "object detection model we save the model and we get those two files the architecture file and the prams file and",
    "start": "2544290",
    "end": "2551880"
  },
  {
    "text": "now using Scala we can and create an object detection so on line 11 we create",
    "start": "2551880",
    "end": "2559260"
  },
  {
    "text": "a new instance of an object detector and we provide a path through our model with",
    "start": "2559260",
    "end": "2565080"
  },
  {
    "text": "things like the input descriptor which we got from the save model output which",
    "start": "2565080",
    "end": "2570180"
  },
  {
    "text": "shows the data format that we need for an image so in this case would pass me",
    "start": "2570180",
    "end": "2575190"
  },
  {
    "text": "an image of two to four by two to four line width and it's a color image and then we choose a image that we want to",
    "start": "2575190",
    "end": "2583500"
  },
  {
    "text": "perform this object detection on so we have the path at the very top and lastly",
    "start": "2583500",
    "end": "2588510"
  },
  {
    "text": "in the bottom line we call the image object detect function on our object detector we pass in our image and we",
    "start": "2588510",
    "end": "2596070"
  },
  {
    "text": "specify that we're interested in the top three predictions and then the example would be on the right-hand side where",
    "start": "2596070",
    "end": "2602250"
  },
  {
    "text": "we've identified the dog for biking the car and so this is just using sage maker",
    "start": "2602250",
    "end": "2607350"
  },
  {
    "text": "API this is using the skull API for MX net to be running the front another",
    "start": "2607350",
    "end": "2615780"
  },
  {
    "text": "great open source tools this is a separate open source project is the MX",
    "start": "2615780",
    "end": "2621150"
  },
  {
    "start": "2616000",
    "end": "2616000"
  },
  {
    "text": "net model server and this just provides a lot of functionality that's typical for model",
    "start": "2621150",
    "end": "2627110"
  },
  {
    "text": "serving so we'll create a web server dedicated for doing model inference with",
    "start": "2627110",
    "end": "2633590"
  },
  {
    "text": "MX net models but it's not just MX now models that you can work with it also",
    "start": "2633590",
    "end": "2638600"
  },
  {
    "text": "supports the open neural network exchange format onyx and we can load in",
    "start": "2638600",
    "end": "2644119"
  },
  {
    "text": "onyx models through here and the core component of MX model server to make it",
    "start": "2644119",
    "end": "2650510"
  },
  {
    "text": "agnostic to different house and model and framework is to create something called a model archive and then when we",
    "start": "2650510",
    "end": "2656720"
  },
  {
    "text": "have a model archive we can then deploy that on our server and we can create an MX net model server using this archive",
    "start": "2656720",
    "end": "2663790"
  },
  {
    "text": "so let's take an example of creating an image classification endpoint we're",
    "start": "2663790",
    "end": "2671840"
  },
  {
    "text": "going to use a network called squeeze net in this case so once we've trained our squeeze net model we have the",
    "start": "2671840",
    "end": "2677630"
  },
  {
    "text": "architecture file with a parameters file but there's a few other things that we need for the archive firstly it's a",
    "start": "2677630",
    "end": "2684500"
  },
  {
    "text": "Python file that's just going to control how we process our input and obtain the",
    "start": "2684500",
    "end": "2690980"
  },
  {
    "text": "data in the right format for the model and then something called the signature of JSON file and that just specifies the",
    "start": "2690980",
    "end": "2698680"
  },
  {
    "text": "data format itself from the endpoint we can combine all this into an archive",
    "start": "2698680",
    "end": "2705680"
  },
  {
    "text": "using the model archive a command specifying the past the models and a handler which is the Python file that's",
    "start": "2705680",
    "end": "2712550"
  },
  {
    "text": "going to be run first and then when we have our dot ma are the model archive",
    "start": "2712550",
    "end": "2717619"
  },
  {
    "text": "file we can start an MX net model server with the mixed model server command and",
    "start": "2717619",
    "end": "2724220"
  },
  {
    "text": "specify which model our hope you want to use and after you've run this command so you might be running this on an ec2",
    "start": "2724220",
    "end": "2730609"
  },
  {
    "text": "instance for example it's going to start the web server and then we can make REST",
    "start": "2730609",
    "end": "2736430"
  },
  {
    "text": "API calls so it's going to create an endpoint by default at slash predictions slashed",
    "start": "2736430",
    "end": "2744020"
  },
  {
    "text": "squeeze net because that was what I'm always called and we can pass an image in this case to that endpoint and get",
    "start": "2744020",
    "end": "2752540"
  },
  {
    "text": "back our predictions and here we've got the classes of this particular image the",
    "start": "2752540",
    "end": "2761660"
  },
  {
    "text": "recommended approach for working with MMS is to use something called a WS Fargate it's just a really nice way to",
    "start": "2761660",
    "end": "2768140"
  },
  {
    "start": "2762000",
    "end": "2762000"
  },
  {
    "text": "manage specific tasks so here similar to elastic container service where you",
    "start": "2768140",
    "end": "2775490"
  },
  {
    "text": "specify a task that you want to run but with fog a you don't need to specify the instances that are going to be used to",
    "start": "2775490",
    "end": "2782510"
  },
  {
    "text": "run your model you just specify the requirements for your task in terms of CPU and memory and then AWS Fargate will",
    "start": "2782510",
    "end": "2789920"
  },
  {
    "text": "manage what instances to use and scale up efficiently so in summary we looked",
    "start": "2789920",
    "end": "2796190"
  },
  {
    "text": "at model training we saw how Karis MX nets speeds up training substantially communities for multiple GPUs and is",
    "start": "2796190",
    "end": "2803450"
  },
  {
    "start": "2797000",
    "end": "2797000"
  },
  {
    "text": "really easy to get started and then for model deployment we looked at sage maker",
    "start": "2803450",
    "end": "2809200"
  },
  {
    "text": "go to the MX now inference api's and lastly an x net model server so thanks",
    "start": "2809200",
    "end": "2816560"
  },
  {
    "text": "everyone for joining today and then yeah having to take some questions have a look through",
    "start": "2816560",
    "end": "2824770"
  }
]