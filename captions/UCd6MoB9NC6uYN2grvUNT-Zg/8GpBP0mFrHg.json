[
  {
    "start": "0",
    "end": "120000"
  },
  {
    "text": "hi there guys um my name's tim darby i'm with sony computer entertainment europe",
    "start": "2879",
    "end": "8559"
  },
  {
    "text": "and i'm going to be talking a little bit about rebuilding killzone servers just a tiny bit about",
    "start": "8559",
    "end": "16400"
  },
  {
    "text": "myself um i'm a senior server engineer with sony's online technology group",
    "start": "16400",
    "end": "24400"
  },
  {
    "text": "we provide help to the game teams implementing online features to",
    "start": "24400",
    "end": "30800"
  },
  {
    "text": "sony's first party games i've been working with guerrilla games",
    "start": "30800",
    "end": "36079"
  },
  {
    "text": "on various of the killzone titles on and off ever since the first killzone",
    "start": "36079",
    "end": "41520"
  },
  {
    "text": "game in 2004. most recently i've been working with gorilla cambridge on killzone mercenary",
    "start": "41520",
    "end": "49360"
  },
  {
    "text": "for the playstation vita so what am i going to be talking about",
    "start": "49360",
    "end": "55039"
  },
  {
    "text": "today well let's say it's rebuilding kill zone service so i'm going to be talking a bit about",
    "start": "55039",
    "end": "60559"
  },
  {
    "text": "how we originally built the server set for killzone one way back in 2004 and appears to",
    "start": "60559",
    "end": "66080"
  },
  {
    "text": "and then how we grew our server architecture as we added features and iterated through on that title and",
    "start": "66080",
    "end": "73119"
  },
  {
    "text": "then i'm going to be talking about how we rebuilt the whole set on amazon for the latest set of titles",
    "start": "73119",
    "end": "78560"
  },
  {
    "text": "in the killzone franchise and how we made use of amazon services to kind of get more bang for our buck to be",
    "start": "78560",
    "end": "85680"
  },
  {
    "text": "able to react and change and be more dynamic and flexible in how we did things",
    "start": "85680",
    "end": "91040"
  },
  {
    "text": "i'm going to be talking a bit in detail on how we use those aws services for specific use cases and then",
    "start": "91040",
    "end": "98880"
  },
  {
    "text": "finishing up with a few of the issues we had and lessons learned and hopefully a couple of minutes for q",
    "start": "98880",
    "end": "104159"
  },
  {
    "text": "a um i've fueled myself up with plenty of coffee before this session to try and get through this quickly so we've got some q a time",
    "start": "104159",
    "end": "110320"
  },
  {
    "text": "though so first of all though what is killzone",
    "start": "110320",
    "end": "117040"
  },
  {
    "text": "to help with this i've just got a short",
    "start": "117040",
    "end": "120719"
  },
  {
    "text": "video",
    "start": "126840",
    "end": "129840"
  },
  {
    "text": "so",
    "start": "154840",
    "end": "157840"
  },
  {
    "start": "210000",
    "end": "337000"
  },
  {
    "text": "so uh yeah obviously killzone is a first person shooter it's a playstation exclusive franchise",
    "start": "212319",
    "end": "219040"
  },
  {
    "text": "um and we've been going through iterations on the title all the way back since 2004. so",
    "start": "219040",
    "end": "227040"
  },
  {
    "text": "killzone serves a history of them starts way back in the ps2 we started out on killzone one on the",
    "start": "227040",
    "end": "234159"
  },
  {
    "text": "playstation 2 really just serving the core multiplayer experience getting people into games playing and",
    "start": "234159",
    "end": "242159"
  },
  {
    "text": "finding a new game after that so quite a straightforward experience back in that day and just really using the the minimum",
    "start": "242159",
    "end": "250080"
  },
  {
    "text": "server back end to be able to give that good experience with uh you know client",
    "start": "250080",
    "end": "255200"
  },
  {
    "text": "server networking and this is you know pre the amazon days for us",
    "start": "255200",
    "end": "260720"
  },
  {
    "text": "the architecture we used really it's you know consisted of using our hosting facilities that sony had in the",
    "start": "260720",
    "end": "266320"
  },
  {
    "text": "different regions with groups of real-time game servers that were deployed it was all centrally written technology",
    "start": "266320",
    "end": "273280"
  },
  {
    "text": "and nothing actually specific to the killzone title in that back end so we used this tech stack on several",
    "start": "273280",
    "end": "280160"
  },
  {
    "text": "games including killzone and there was just a little bit of configuration so you had multiple game",
    "start": "280160",
    "end": "285199"
  },
  {
    "text": "servers in each region so you've got low latency connection for playing a real-time game and then a centralized server stack in",
    "start": "285199",
    "end": "292400"
  },
  {
    "text": "one region providing in uh lobbies and then you know you go into the",
    "start": "292400",
    "end": "298320"
  },
  {
    "text": "lobbies and you find a game and play it limited in terms of the amount of persistent date that we stored",
    "start": "298320",
    "end": "304479"
  },
  {
    "text": "we had a rocking 256 bytes of data for each user's profile stored in the db",
    "start": "304479",
    "end": "310800"
  },
  {
    "text": "back end and actually the the storage for where all the players were what the",
    "start": "310800",
    "end": "317680"
  },
  {
    "text": "current servers were that are actually connected to the system that was all stored in this one central management server",
    "start": "317680",
    "end": "325039"
  },
  {
    "text": "that was around the critical point of failure because if that server fell over you had to restart the whole set for everything to be picked up again",
    "start": "325039",
    "end": "331440"
  },
  {
    "text": "which meant you'd have all your users kicked out of their games which was less than ideal moving on from",
    "start": "331440",
    "end": "338400"
  },
  {
    "start": "337000",
    "end": "483000"
  },
  {
    "text": "there a couple of years on now kills and liberation on the psp well by this time we ought to actually",
    "start": "338400",
    "end": "344240"
  },
  {
    "text": "have a bit more of a a persistent experience so uh the iron tech group took on a",
    "start": "344240",
    "end": "350800"
  },
  {
    "text": "service stack that was actually written by the san diego studios for some sports games originally um and brought that in as a shared",
    "start": "350800",
    "end": "358560"
  },
  {
    "text": "technology stack that was used by killzone amongst other games and that was a very classic kind of three-tier architecture",
    "start": "358560",
    "end": "364720"
  },
  {
    "text": "you got the presentation tier servers uh behind a hardware load balancer and they're running tomcat providing",
    "start": "364720",
    "end": "371840"
  },
  {
    "text": "customized jsp for each title uh you know providing game specific",
    "start": "371840",
    "end": "376960"
  },
  {
    "text": "markup that the game uses to show its you know leaderboards you know all the all the community",
    "start": "376960",
    "end": "382639"
  },
  {
    "text": "features profiles uh medals and badges that kind of thing so that provided all the online game",
    "start": "382639",
    "end": "387840"
  },
  {
    "text": "screens beneath that presentation tier we had uh the kind of business tier being accessed",
    "start": "387840",
    "end": "395120"
  },
  {
    "text": "via java rmi um and that was just kind of java standalone apps",
    "start": "395120",
    "end": "401520"
  },
  {
    "text": "and those two top tier servers were running session lists and they weren't in what i",
    "start": "401520",
    "end": "408319"
  },
  {
    "text": "would actually really call a cluster because there was no cross-cluster communication at all they weren't totally independent and all",
    "start": "408319",
    "end": "414240"
  },
  {
    "text": "of the orchestration is done down in the database layer through a connection using jdbc",
    "start": "414240",
    "end": "420319"
  },
  {
    "text": "peel sql calls to store procedures for all of the management of anything that needed uh let's say any kind of",
    "start": "420319",
    "end": "427360"
  },
  {
    "text": "transactionality orchestration and then actually for things like school",
    "start": "427360",
    "end": "432960"
  },
  {
    "text": "posting they are managed through an asynchronous batch processor where we'd use one of the database",
    "start": "432960",
    "end": "439520"
  },
  {
    "text": "tables as a queue and have the batch processing server sat there constantly checking for things to",
    "start": "439520",
    "end": "445199"
  },
  {
    "text": "come in so that would be dealing with posting schools putting them into game history pushing them into lead boards",
    "start": "445199",
    "end": "451280"
  },
  {
    "text": "and updating anything that needs to be done there's any kind of scheduled tasks as well and things like the leaderboards were a",
    "start": "451280",
    "end": "458960"
  },
  {
    "text": "big load on the database because you got a system there in uh",
    "start": "458960",
    "end": "464080"
  },
  {
    "text": "the sql database and you've got constant updates to it on a table which",
    "start": "464080",
    "end": "470240"
  },
  {
    "text": "has got quite a lot of indexed columns for all the sorted columns on the leaderboards but at the same time that's being read",
    "start": "470240",
    "end": "476319"
  },
  {
    "text": "as well so there's a bit of a battle going on between the database for reason database for rights there",
    "start": "476319",
    "end": "482160"
  },
  {
    "text": "so when we came onto killzone 2 on the ps3 we're talking like another couple of",
    "start": "482160",
    "end": "487440"
  },
  {
    "start": "483000",
    "end": "738000"
  },
  {
    "text": "years later on from that 2009 still using essentially the same real-time game server back-end with the",
    "start": "487440",
    "end": "494000"
  },
  {
    "text": "regional servers but we'd switched over on the java side of things",
    "start": "494000",
    "end": "499280"
  },
  {
    "text": "to use materialized views for the leaderboard system actually trading off latency in read",
    "start": "499280",
    "end": "505280"
  },
  {
    "text": "access to the database views of the leaderboards you know and meaning we weren't loading",
    "start": "505280",
    "end": "511759"
  },
  {
    "text": "the kind of leaderboard table as we're reading it with the rights as well",
    "start": "511759",
    "end": "517839"
  },
  {
    "text": "and also there was some shared services that were implemented in that",
    "start": "517839",
    "end": "523360"
  },
  {
    "text": "main server set that we used for features like battle replay which is a",
    "start": "523360",
    "end": "528399"
  },
  {
    "text": "a real-time player playback of multiplayer missions and a kind of a",
    "start": "528399",
    "end": "534000"
  },
  {
    "text": "top-down view and that actually started to muddy the waters up at the presentation tier then",
    "start": "534000",
    "end": "539360"
  },
  {
    "text": "because the presentation tier servers relied on an nfs mount to a shared file system for storing those uh replay files that",
    "start": "539360",
    "end": "546640"
  },
  {
    "text": "the clients were sending up whilst they're playing multiplayer games so as you see here is request on the",
    "start": "546640",
    "end": "552320"
  },
  {
    "text": "server set has kind of accreted functionality to a certain extent and by the time we got through in 2011",
    "start": "552320",
    "end": "560880"
  },
  {
    "text": "with killzone three um once again we still had problems with the leaderboards the leaderboards",
    "start": "560880",
    "end": "567200"
  },
  {
    "text": "even in the materialized view they mvs are actually causing more processing to go on on the database",
    "start": "567200",
    "end": "573120"
  },
  {
    "text": "because whenever it refreshes the materialized view there's a lot of db activity going on and with the reads being quite heavily",
    "start": "573120",
    "end": "581040"
  },
  {
    "text": "done from the website for the game the game you know we thought well what we'll do is we'll actually offload some of",
    "start": "581040",
    "end": "587040"
  },
  {
    "text": "those read activities by replicating the leaderboard database tables among other things",
    "start": "587040",
    "end": "593200"
  },
  {
    "text": "over to the website's db schema and actually transform the data at the same time into the schema that was most",
    "start": "593200",
    "end": "598880"
  },
  {
    "text": "convenient for the website to use also by this time we'd actually got",
    "start": "598880",
    "end": "604560"
  },
  {
    "text": "uh we'd moved on from this old style view of the the gaming side of",
    "start": "604560",
    "end": "610160"
  },
  {
    "text": "finding a lobby looking at the games in the lobby trying to find a game that's not already full by the time you",
    "start": "610160",
    "end": "615600"
  },
  {
    "text": "time you try and join it and we moved over to a system where we did matchmaking you pick your game mode and",
    "start": "615600",
    "end": "621680"
  },
  {
    "text": "you rely on the server back end to match you to a good game and because all the matching and the real-time game",
    "start": "621680",
    "end": "627519"
  },
  {
    "text": "side of things was done on the c plus side of the servers we had to actually add in another server to that",
    "start": "627519",
    "end": "633519"
  },
  {
    "text": "real time cluster for the matchmaking and for the first time we had to actually add in",
    "start": "633519",
    "end": "639040"
  },
  {
    "text": "uh title specific logic there because it is quite specific to title you can't just",
    "start": "639040",
    "end": "644640"
  },
  {
    "text": "configure in the rules for whether a game has it more important to match people against the same skill",
    "start": "644640",
    "end": "650399"
  },
  {
    "text": "versus people who are a close latency distance to you and that system was a bit problematic",
    "start": "650399",
    "end": "656720"
  },
  {
    "text": "even then because the real-time game system for the matching side of things",
    "start": "656720",
    "end": "662240"
  },
  {
    "text": "relied on pushing the request and the client just waits and waits and",
    "start": "662240",
    "end": "668880"
  },
  {
    "text": "eventually gets told here's your game so there's little feedback to the end user of what's going on during the",
    "start": "668880",
    "end": "673920"
  },
  {
    "text": "matchmaking process and also that matching service was a significant bottleneck in the system",
    "start": "673920",
    "end": "679760"
  },
  {
    "text": "and it wasn't something that could have been easily sharded out in this case and so when we had problems and the game",
    "start": "679760",
    "end": "685600"
  },
  {
    "text": "team wanted to be able to see what was going on in the matchmaking server it was quite tricky and the act of",
    "start": "685600",
    "end": "691360"
  },
  {
    "text": "turning up the logging levels on the matching server that in itself could tip the matching server over the edge",
    "start": "691360",
    "end": "698000"
  },
  {
    "text": "and between it being written in the system it was quite hard to debug hard to actually see what was going on",
    "start": "698000",
    "end": "704480"
  },
  {
    "text": "and tricky for the game team to update really had been getting to the breaking",
    "start": "704480",
    "end": "710399"
  },
  {
    "text": "point of being able to use this architecture and by this point also other game teams",
    "start": "710399",
    "end": "715680"
  },
  {
    "text": "that we worked with within sony had already moved over to their own server back ends and we'd moved from a situation where",
    "start": "715680",
    "end": "722800"
  },
  {
    "text": "back in 2004 none of the game teams really had their own server engineers",
    "start": "722800",
    "end": "728639"
  },
  {
    "text": "and by the time we got to kill zone three guerrilla games had their own uh pretty significant server team",
    "start": "728639",
    "end": "734320"
  },
  {
    "text": "available to start implementing these features themselves",
    "start": "734320",
    "end": "738880"
  },
  {
    "text": "brings us to 2013 which was uh guaranteed to be a pretty damn",
    "start": "739360",
    "end": "744880"
  },
  {
    "text": "busy year for us we're going to be releasing killzone mercenary in september and then kill zone shadow fall in",
    "start": "744880",
    "end": "753120"
  },
  {
    "text": "oh yeah tomorrow no pressure",
    "start": "753120",
    "end": "759839"
  },
  {
    "text": "on the one hand this was actually good timing because we had a bit of an opportunity",
    "start": "760560",
    "end": "767360"
  },
  {
    "text": "and a bit of impetus to change how we did things on the other hand is a lot of time pressure with killzone mercenary on the",
    "start": "767360",
    "end": "774480"
  },
  {
    "text": "playstation vita we had some significant departures from the functionality we wanted to provide for the game because we're on a",
    "start": "774480",
    "end": "780800"
  },
  {
    "text": "different platform on a mobile platform but we still wanted the offline experience to be actually integrated to",
    "start": "780800",
    "end": "787839"
  },
  {
    "text": "your overall user profile and your kind of whole community status",
    "start": "787839",
    "end": "794160"
  },
  {
    "text": "with shadowfall on the playstation 4 being a launch title there was going to be significant focus on it",
    "start": "794160",
    "end": "800480"
  },
  {
    "text": "and also the game team were keen to address some of the fundamental issues in the server back end that impacted",
    "start": "800480",
    "end": "807680"
  },
  {
    "text": "what they could do on the client side of things also the game team wanted to be able to",
    "start": "807680",
    "end": "814079"
  },
  {
    "text": "actually actively work a lot more post launch to integrate new features and actually engage with the community",
    "start": "814079",
    "end": "820000"
  },
  {
    "text": "to give them what they want after the original launch so we really had a lot of things that",
    "start": "820000",
    "end": "826160"
  },
  {
    "text": "meant we wanted to make a bit of a change and we had this old surfer architecture for nearly 10 years but this time",
    "start": "826160",
    "end": "833360"
  },
  {
    "text": "so working with online technology group and gorilla together we re-architected the whole server stack",
    "start": "833360",
    "end": "840880"
  },
  {
    "text": "we took a rather monolithic build and actually split it out from the ground up as separate shared",
    "start": "840880",
    "end": "847040"
  },
  {
    "text": "components so we had maven artifacts an api level",
    "start": "847040",
    "end": "853600"
  },
  {
    "text": "for the kind of underlying low-level functions leaderboards profiles",
    "start": "853600",
    "end": "859120"
  },
  {
    "text": "monitoring that kind of thing and then implementations of those maven components dependent on the api",
    "start": "859120",
    "end": "866079"
  },
  {
    "text": "artifacts for different back-ends so we can actually swap around back ends and try things out",
    "start": "866079",
    "end": "871519"
  },
  {
    "text": "and then to go with that we actually kept our game projects for mercenary and shadowfall independent from each other",
    "start": "871519",
    "end": "877120"
  },
  {
    "text": "but sharing you know potentially different versions of those underlying components",
    "start": "877120",
    "end": "882399"
  },
  {
    "text": "but also able to build their own things specific to title as well we also moved from uh having a kind of",
    "start": "882399",
    "end": "889839"
  },
  {
    "text": "an ant based uh build for the big monolithic server set",
    "start": "889839",
    "end": "894959"
  },
  {
    "text": "over to using maven and from a rather custom build system for both the real time the",
    "start": "894959",
    "end": "901600"
  },
  {
    "text": "c-plus plus side of things and also the java side moved over to jenkins for both of those components",
    "start": "901600",
    "end": "907680"
  },
  {
    "text": "on the c plus side that i don't actually cover here that was a total rebuild and we actually",
    "start": "907680",
    "end": "913279"
  },
  {
    "text": "still kept in c plus the real time side but rebuilt that from the ground up to actually reduce anywhere we could any",
    "start": "913279",
    "end": "919920"
  },
  {
    "text": "of the latencies in the system we also wanted to move over from a rather static",
    "start": "919920",
    "end": "925120"
  },
  {
    "text": "kind of deployment methodology over to dynamic scaling and this was to actually deal with a few",
    "start": "925120",
    "end": "930800"
  },
  {
    "text": "of the issues we wanted to try and work out any of the single points of failure from our system",
    "start": "930800",
    "end": "936720"
  },
  {
    "text": "and actually move over to a much more generally resilient architecture we also wanted to get away from",
    "start": "936720",
    "end": "944720"
  },
  {
    "text": "a kind of a working practice that we got caught up in where we predict before launch",
    "start": "944720",
    "end": "951440"
  },
  {
    "text": "what kind of concurrency what kind of active user base we're going to have statically build and statically load",
    "start": "951440",
    "end": "957120"
  },
  {
    "text": "test the servers out to that and then scramble like crazy at launch if we'd underpredicted",
    "start": "957120",
    "end": "964079"
  },
  {
    "text": "and if we hadn't under predicted if we'd over predicted we'd have to scramble to justify why we",
    "start": "964079",
    "end": "969600"
  },
  {
    "text": "were spending money we didn't need to and having many servers so idle so",
    "start": "969600",
    "end": "974959"
  },
  {
    "text": "we wanted to move to a much more dynamic system where the system itself would actually scale up and down with the load",
    "start": "974959",
    "end": "981839"
  },
  {
    "text": "and we also wanted to use aws services but we didn't just want to use them for the sake of using aws",
    "start": "981839",
    "end": "988240"
  },
  {
    "text": "because it was kind of sexy we wanted to use it because we wanted to spend as much of our time as possible",
    "start": "988240",
    "end": "993759"
  },
  {
    "text": "implementing game features you know we're not huge teams actually working on the games",
    "start": "993759",
    "end": "999600"
  },
  {
    "text": "and we want to actually write game features and not actually spend all our time",
    "start": "999600",
    "end": "1005360"
  },
  {
    "text": "putting together queuing systems building our own",
    "start": "1005360",
    "end": "1011279"
  },
  {
    "text": "monitoring back ends um building continuous deployment systems all that kind of thing",
    "start": "1011279",
    "end": "1017440"
  },
  {
    "text": "we wanted to actually be implementing you know lead boards and award systems and stack history",
    "start": "1017440",
    "end": "1024640"
  },
  {
    "text": "graphs and all that kind of thing and the only way could do that is if we spent much less of our resource",
    "start": "1024640",
    "end": "1031280"
  },
  {
    "text": "on the structural elements of the game servers so having taken those principles",
    "start": "1031280",
    "end": "1039360"
  },
  {
    "start": "1034000",
    "end": "1234000"
  },
  {
    "text": "what we ended up with was first of all killzone mercenary and the back end for that actually ended",
    "start": "1039360",
    "end": "1046240"
  },
  {
    "text": "up quite straightforward and simple compared to what we've done before the actual api used by the client",
    "start": "1046240",
    "end": "1052880"
  },
  {
    "text": "straightforward rest used jersey inside the tomcat servers that are inside",
    "start": "1052880",
    "end": "1058720"
  },
  {
    "text": "a beanstalk environment and used amazon alb for the ssl termination that was an auto",
    "start": "1058720",
    "end": "1066480"
  },
  {
    "text": "scaled environment so we could pick up with the load and add new servers as necessary and then scale down automatically",
    "start": "1066480",
    "end": "1073440"
  },
  {
    "text": "and within those servers that we built using maven and use spring for the kind of runtime",
    "start": "1073440",
    "end": "1079039"
  },
  {
    "text": "configuration we use spring to actually add in instances",
    "start": "1079039",
    "end": "1084559"
  },
  {
    "text": "of those underlying services in such a way as we could swap them out nice and easily with a quick redeploy",
    "start": "1084559",
    "end": "1090960"
  },
  {
    "text": "so we could take our leaderboard service originally on simply b and actually swap",
    "start": "1090960",
    "end": "1096720"
  },
  {
    "text": "it out for dynamo during development without actually impacting the api used by the game client",
    "start": "1096720",
    "end": "1102640"
  },
  {
    "text": "or actually even most of the internal apis used by the game code it was only a change of constructor that",
    "start": "1102640",
    "end": "1108080"
  },
  {
    "text": "was needed and then we had many of the other components mixing in use of",
    "start": "1108080",
    "end": "1113600"
  },
  {
    "text": "dynamo s3 for things which are kind of higher latency storage but we're going to be big",
    "start": "1113600",
    "end": "1119600"
  },
  {
    "text": "and then quite a lot of use of sqs and we used cloud formation to deploy it",
    "start": "1119600",
    "end": "1126080"
  },
  {
    "text": "so that we could get away from kind of having a huge worry about whether we were going to be deploying",
    "start": "1126080",
    "end": "1132000"
  },
  {
    "text": "a new server set and whether we you know could afford the time to spend to build up a new environment for e3 or another",
    "start": "1132000",
    "end": "1138640"
  },
  {
    "text": "event like that i wanted to be able to just quickly spin up a new environment",
    "start": "1138640",
    "end": "1144080"
  },
  {
    "text": "and then when we're done with it just take it away quickly we also use the same build setup for",
    "start": "1144080",
    "end": "1152320"
  },
  {
    "text": "the other part components of the server set the admin server just the same beanstalk based sat on",
    "start": "1152320",
    "end": "1158400"
  },
  {
    "text": "amazon using gwt to build it because we had most of our experience on the java side",
    "start": "1158400",
    "end": "1163440"
  },
  {
    "text": "of things and you know switching between java and javascript for coding is a way to get",
    "start": "1163440",
    "end": "1169440"
  },
  {
    "text": "serious migraines so we kept with that same kind of build and then the final component we had",
    "start": "1169440",
    "end": "1175600"
  },
  {
    "text": "there the scheduler which is one place we differ from uh kills and shadowful that again",
    "start": "1175600",
    "end": "1182640"
  },
  {
    "text": "straightforward simple beanstalk but in that case and actually something slightly wrong on that diagram when the amazon",
    "start": "1182640",
    "end": "1189600"
  },
  {
    "text": "in beanstalk single instance type came out who switched over for that so we no",
    "start": "1189600",
    "end": "1195919"
  },
  {
    "text": "longer were paying for an elb that we weren't actually using within that scheduler app but we were able to keep using the same",
    "start": "1195919",
    "end": "1202640"
  },
  {
    "text": "consistent build and deployment setup that we had for everything so",
    "start": "1202640",
    "end": "1207679"
  },
  {
    "text": "unlike previously where you had a significantly different deployment step for the different parts of the app",
    "start": "1207679",
    "end": "1213679"
  },
  {
    "text": "everything was nice and straightforward and homogeneous one thing we actually simplified for ourselves as well was",
    "start": "1213679",
    "end": "1220000"
  },
  {
    "text": "that on killzone mercenary rather than using our own real-time game servers we actually use psn functionality for",
    "start": "1220000",
    "end": "1226240"
  },
  {
    "text": "that which is why you don't see anything relating to matching nat reversal you know peer-to-peer client server networking on there",
    "start": "1226240",
    "end": "1233039"
  },
  {
    "text": "so that's mercenary killzone shadowful takes that same kind of basis and just",
    "start": "1233039",
    "end": "1239600"
  },
  {
    "start": "1234000",
    "end": "1418000"
  },
  {
    "text": "grows it out quite a lot shadowfall runs with dedicated servers",
    "start": "1239600",
    "end": "1245520"
  },
  {
    "text": "and actually goes to a hybrid deployment topology we actually have as you might guess",
    "start": "1245520",
    "end": "1250799"
  },
  {
    "text": "within sony we've got our own hosting facilities in a number of regions so we have clusters of kind of",
    "start": "1250799",
    "end": "1257200"
  },
  {
    "text": "statically provisioned game servers in the regions where we've got sony for her hosting",
    "start": "1257200",
    "end": "1262480"
  },
  {
    "text": "and some of those other reasons where sony doesn't have its internal hosting facilities we use aws for",
    "start": "1262480",
    "end": "1268640"
  },
  {
    "text": "deploying the real-time game servers let's say they've been rebuilt from scratch and in the aws reasons we use",
    "start": "1268640",
    "end": "1275120"
  },
  {
    "text": "the auto scaler so we can actually scale up on demand and the auto scaler side of things",
    "start": "1275120",
    "end": "1280400"
  },
  {
    "text": "manages resilience so the auto scaler automatically swaps out and replaces any server instances that go",
    "start": "1280400",
    "end": "1286320"
  },
  {
    "text": "bang at three in the morning also on the shadowful side of things",
    "start": "1286320",
    "end": "1292320"
  },
  {
    "text": "because they wanted to address the um user feedback issue",
    "start": "1292320",
    "end": "1298400"
  },
  {
    "text": "on things like matchmaking whilst they had the straight rest api at the front of it",
    "start": "1298400",
    "end": "1303440"
  },
  {
    "text": "they actually also have a comic connection that the client opens so they can push messages down from the",
    "start": "1303440",
    "end": "1308640"
  },
  {
    "text": "server and they not only use that for things like matching progress updates but also have",
    "start": "1308640",
    "end": "1313919"
  },
  {
    "text": "the facility to do dynamic rebalancing so they can look",
    "start": "1313919",
    "end": "1319520"
  },
  {
    "text": "from the analytic side of things at you know a potential misbalance of a cone of fire of a weapon or magazine",
    "start": "1319520",
    "end": "1326640"
  },
  {
    "text": "size and they can actually through the admin interface push down a server update with the system settings stored in",
    "start": "1326640",
    "end": "1332559"
  },
  {
    "text": "dynamodb and push changes for that all the way through to the game clients so that new system setting gets applied",
    "start": "1332559",
    "end": "1339679"
  },
  {
    "text": "as soon as all the people who are in game currently finish their round obviously you wouldn't want to apply it",
    "start": "1339679",
    "end": "1344799"
  },
  {
    "text": "straight away part way through around because that would be really confusing",
    "start": "1344799",
    "end": "1349600"
  },
  {
    "text": "so as you see here with shadowfall we started out with the same kind of basis but then expanded out and actually added",
    "start": "1350000",
    "end": "1356640"
  },
  {
    "text": "an extra level of complexity with things like the hybrid deployment meaning that we can't just use cloud",
    "start": "1356640",
    "end": "1362480"
  },
  {
    "text": "formation on its own we've actually got the beanstalk and the autoscaler elements been",
    "start": "1362480",
    "end": "1368159"
  },
  {
    "text": "deployed through cloud formation but the wrapper that generates those cloud formation templates",
    "start": "1368159",
    "end": "1373360"
  },
  {
    "text": "and pushes those when we do server updates also doing deployments to the static elements as well",
    "start": "1373360",
    "end": "1379120"
  },
  {
    "text": "and i do hope it's not a terrible pun but i think it is because they named the deployment framework they use for that",
    "start": "1379120",
    "end": "1385360"
  },
  {
    "text": "which is chef-based they called it burrito and if that's a wrapper i'm",
    "start": "1385360",
    "end": "1391200"
  },
  {
    "text": "well it's devops for you so that gives you a little flavor of the general",
    "start": "1391200",
    "end": "1396320"
  },
  {
    "text": "architecture of the servers but what i want to do now is talk to a few specific",
    "start": "1396320",
    "end": "1401440"
  },
  {
    "text": "use cases on how we've used aws services within that",
    "start": "1401440",
    "end": "1406799"
  },
  {
    "text": "and i'll be honest i'm going to be slightly killzone mercenary centric here because being the day before launch i",
    "start": "1406799",
    "end": "1412640"
  },
  {
    "text": "don't want to tempt fate too much famous last words so",
    "start": "1412640",
    "end": "1420158"
  },
  {
    "start": "1418000",
    "end": "1549000"
  },
  {
    "text": "one area that we had to put in there very early on for mercenary schedule jobs there are a number of things we need to actually manage",
    "start": "1420400",
    "end": "1426799"
  },
  {
    "text": "periodically so we knew we had to have that as quite a basic part of the system but we wanted to keep it simple so",
    "start": "1426799",
    "end": "1434960"
  },
  {
    "text": "we very early on dropped in a straight simple scheduler web app and to avoid a lot of",
    "start": "1434960",
    "end": "1441200"
  },
  {
    "text": "the clustered scheduling issues that you get if you look at the internals of spring quartz for clustered",
    "start": "1441200",
    "end": "1446640"
  },
  {
    "text": "scheduling it's quite tricky so we just had a separate server instance",
    "start": "1446640",
    "end": "1451679"
  },
  {
    "text": "minimax one server and all that does is run spring quartz with a defined job list and whenever any of",
    "start": "1451679",
    "end": "1458799"
  },
  {
    "text": "those jobs get triggered they just push an sqs message into a scheduled jobs queue",
    "start": "1458799",
    "end": "1465360"
  },
  {
    "text": "we then have our scheduled job receiver threads sat on every one of our front-end server",
    "start": "1465360",
    "end": "1470640"
  },
  {
    "text": "nodes we actually tried out a number of different versions of that deployment setup",
    "start": "1470640",
    "end": "1476000"
  },
  {
    "text": "with separate uh auto scaler stacks for the job processors i even tried for a short",
    "start": "1476000",
    "end": "1483600"
  },
  {
    "text": "while having those scheduled job processes all on the scheduler node as well but we",
    "start": "1483600",
    "end": "1489279"
  },
  {
    "text": "kind of knew that was going to be horrible because that was replicating the old architecture where this job schedule was a big",
    "start": "1489279",
    "end": "1494960"
  },
  {
    "text": "big bottleneck in the system but what we ended up with was putting those receivers and the job processor worker",
    "start": "1494960",
    "end": "1502000"
  },
  {
    "text": "threads all on the main nodes and that was because when we load tested it we found that a lot of the same things",
    "start": "1502000",
    "end": "1508320"
  },
  {
    "text": "that required us to scale up on the worker threads were the same",
    "start": "1508320",
    "end": "1514000"
  },
  {
    "text": "things with the uh scaling triggers for the rest api as well and also by actually keeping",
    "start": "1514000",
    "end": "1520400"
  },
  {
    "text": "that homogeneous blob of server that does almost everything",
    "start": "1520400",
    "end": "1525520"
  },
  {
    "text": "we also gain the opportunity to do some nice little debug things um particularly things like being able",
    "start": "1525520",
    "end": "1531600"
  },
  {
    "text": "to synchronously trigger jobs in a way that we could debug them and you know when you're running with a",
    "start": "1531600",
    "end": "1537360"
  },
  {
    "text": "large cluster of servers and you want to debug something that goes bang halfway through a job",
    "start": "1537360",
    "end": "1543520"
  },
  {
    "text": "life gets really hard when you've got a search through 30 or 40 boxes before you can find the one that actually had the job failure",
    "start": "1543520",
    "end": "1550559"
  },
  {
    "start": "1549000",
    "end": "1609000"
  },
  {
    "text": "and that was how we started out and that kind of worked in dev for a while but then we realized we were actually",
    "start": "1550559",
    "end": "1556720"
  },
  {
    "text": "tailing logs quite a lot when anything went wrong and that was never going to fly in production so",
    "start": "1556720",
    "end": "1563600"
  },
  {
    "text": "first of all what we did was we added wrappers around those job workers to provide cloud watch monitoring so we",
    "start": "1563600",
    "end": "1570240"
  },
  {
    "text": "have a cloudwatch data point for each different name job that just tracks the",
    "start": "1570240",
    "end": "1575600"
  },
  {
    "text": "elapsed time that the job took to run so we can then see whether the jobs actually got triggered",
    "start": "1575600",
    "end": "1581120"
  },
  {
    "text": "making sure their frequency is correct and actually tracking if a job's starting to get outside of the bounds of",
    "start": "1581120",
    "end": "1586880"
  },
  {
    "text": "what you can actually practically run the job in whether that's because you've not got enough provision throughput on dynamo",
    "start": "1586880",
    "end": "1592799"
  },
  {
    "text": "or if you've got a bug summary in the job and that goes a bit further and we also",
    "start": "1592799",
    "end": "1598240"
  },
  {
    "text": "added data point tracking and cloudwatch just actually chucking up a one if the job through an exception when it",
    "start": "1598240",
    "end": "1604720"
  },
  {
    "text": "ran and that was a nice straightforward uh alert for us",
    "start": "1604720",
    "end": "1610159"
  },
  {
    "start": "1609000",
    "end": "1787000"
  },
  {
    "text": "but that only got so far because what we actually found was that in a particular case",
    "start": "1610159",
    "end": "1615600"
  },
  {
    "text": "with uh leaderboard validation that was quite a long-running job and as",
    "start": "1615600",
    "end": "1620880"
  },
  {
    "text": "we got to higher data volumes particularly when we didn't have enough throughput configured on dynamodb",
    "start": "1620880",
    "end": "1627520"
  },
  {
    "text": "that hourly job that we were running could take an hour and ten minutes and an unfortunate side effect of this",
    "start": "1627520",
    "end": "1633360"
  },
  {
    "text": "topology we built was that the job scheduler had no idea that the job was already running and would merely chuck another job into",
    "start": "1633360",
    "end": "1639679"
  },
  {
    "text": "the queue to do the same validation task a second time and another node would pick up that job",
    "start": "1639679",
    "end": "1646320"
  },
  {
    "text": "and so all of a sudden you'd have two nodes part way through a leadable validation job that would eat most of the read through",
    "start": "1646320",
    "end": "1652559"
  },
  {
    "text": "put on the database table and that only got worse until you actually have all your nodes running the same job and nobody able to access the",
    "start": "1652559",
    "end": "1659039"
  },
  {
    "text": "leaderboards which wasn't a good thing so what we then added on was a layer of distributed",
    "start": "1659039",
    "end": "1664399"
  },
  {
    "text": "job management where we'd use a dynamodb table as a whiteboard to track the status of all",
    "start": "1664399",
    "end": "1669440"
  },
  {
    "text": "the running jobs in the system and an admin view such that we could",
    "start": "1669440",
    "end": "1674480"
  },
  {
    "text": "see the running status of all the jobs we got configured we could see through that one single unified view",
    "start": "1674480",
    "end": "1680240"
  },
  {
    "text": "without having to go to cloudwatch um how long those jobs are taking to run we could also provide more detailed",
    "start": "1680240",
    "end": "1686960"
  },
  {
    "text": "feedback on the failure status of any jobs if they weren't banged during their run and we could also",
    "start": "1686960",
    "end": "1694640"
  },
  {
    "text": "manage the distributed job so we could actually set certain jobs as being run once so that if anywhere",
    "start": "1694640",
    "end": "1701120"
  },
  {
    "text": "else in the cluster that job was running when a new copy of that job got picked up from sqs we could actually not",
    "start": "1701120",
    "end": "1707600"
  },
  {
    "text": "run a second copy of that same job but instead just increment a flag on the dynamo table for",
    "start": "1707600",
    "end": "1714080"
  },
  {
    "text": "that white board so as we could see through yapping app oh yeah that job got triggered five",
    "start": "1714080",
    "end": "1719360"
  },
  {
    "text": "times in the time it took to run this is not a good thing so with that that actually gave us a nice",
    "start": "1719360",
    "end": "1725600"
  },
  {
    "text": "kind of middle ground between doing complete distributed job scheduling and management but also",
    "start": "1725600",
    "end": "1730960"
  },
  {
    "text": "with a lot of the good tools to be able to monitor and actually track what's going on and remedy issues",
    "start": "1730960",
    "end": "1736880"
  },
  {
    "text": "with the final step being that through the admin interface we added the ability to outside of the",
    "start": "1736880",
    "end": "1742399"
  },
  {
    "text": "normal schedule just push a new copy of that job into sqs to be run immediately or fire it synchronously so that the",
    "start": "1742399",
    "end": "1750240"
  },
  {
    "text": "server node that actually got that rest api call would run that job",
    "start": "1750240",
    "end": "1755279"
  },
  {
    "text": "in process on the execution thread of that servlet so that you could very easily breakpoint",
    "start": "1755279",
    "end": "1760320"
  },
  {
    "text": "it and debug it and an interesting fact though is that on the shadowful side",
    "start": "1760320",
    "end": "1767360"
  },
  {
    "text": "they actually set themselves up a little earlier by simplifying slightly on how they could",
    "start": "1767360",
    "end": "1772880"
  },
  {
    "text": "configure those scheduled jobs rather than having all of the power of spring courts on their hands and actually ran",
    "start": "1772880",
    "end": "1779600"
  },
  {
    "text": "the job scheduling distributed in the first place which i think ultimately something that we'd like to pull into",
    "start": "1779600",
    "end": "1785279"
  },
  {
    "text": "the mercenary back end as well so from the scheduler side of things to",
    "start": "1785279",
    "end": "1791360"
  },
  {
    "start": "1787000",
    "end": "1838000"
  },
  {
    "text": "school posting once again it's quite sqs based we start out",
    "start": "1791360",
    "end": "1796880"
  },
  {
    "text": "with in mercenary it can actually be quite a lot of games coming in because of being on a mobile device but",
    "start": "1796880",
    "end": "1804480"
  },
  {
    "text": "because we want to actually store all your community data and be able to show it to you on your web profile you can actually be",
    "start": "1804480",
    "end": "1811440"
  },
  {
    "text": "offline for a matter of days or weeks and playing campaign missions and each mission is stored individually",
    "start": "1811440",
    "end": "1817679"
  },
  {
    "text": "we don't aggregate them together because then you'd lose a lot of the detail in what you did",
    "start": "1817679",
    "end": "1823279"
  },
  {
    "text": "so school post comes in with a lot of games goes into the rest api and they're split",
    "start": "1823279",
    "end": "1829279"
  },
  {
    "text": "up so that each individual multiplayer game round or campaign mission you played since the last push",
    "start": "1829279",
    "end": "1836240"
  },
  {
    "text": "gets put in as a separate job in sqs and that game jobs sqsu is being read by a separate set of",
    "start": "1836240",
    "end": "1842559"
  },
  {
    "start": "1838000",
    "end": "1968000"
  },
  {
    "text": "listeners to what we use for the scheduled job listeners and the first action on receipt of those",
    "start": "1842559",
    "end": "1848159"
  },
  {
    "text": "jobs is that they're pulled off the queue and we write an entry into the user's game",
    "start": "1848159",
    "end": "1854159"
  },
  {
    "text": "history and that goes into dynamo now it's a",
    "start": "1854159",
    "end": "1859600"
  },
  {
    "text": "transactional right within the context of that one dynamo table but then we have individual listeners",
    "start": "1859600",
    "end": "1866240"
  },
  {
    "text": "when that riot has completed a number of listeners are kicked off in parallel for",
    "start": "1866240",
    "end": "1872000"
  },
  {
    "text": "a lot of secondary jobs that happen as a result of each game score post so we update stats because we're quite",
    "start": "1872000",
    "end": "1878320"
  },
  {
    "text": "keen on our analytics and we want to actually track our game balancing and that ends up in another sqs cue we",
    "start": "1878320",
    "end": "1884240"
  },
  {
    "text": "also write into a number of services a numeric profile for efficient tracking of your overall scores",
    "start": "1884240",
    "end": "1890320"
  },
  {
    "text": "stack history so we can get a time series graph of what you've been doing uh leaderboard service does what it says",
    "start": "1890320",
    "end": "1897440"
  },
  {
    "text": "on the tin and then a few other things such as valor leaderboards profiles badges that kind of stuff but",
    "start": "1897440",
    "end": "1903840"
  },
  {
    "text": "all of those secondary jobs have a nasty side effect of how they fail",
    "start": "1903840",
    "end": "1909039"
  },
  {
    "text": "if the original game history right fails then you get an exception that throws up to the sqs receiver",
    "start": "1909039",
    "end": "1914799"
  },
  {
    "text": "and it doesn't actually confirm that message in sqs so it will get represented if the secondary jobs fail to run though",
    "start": "1914799",
    "end": "1922559"
  },
  {
    "text": "particularly if some of them fail to run there isn't a good way to recover that in the system as we originally wrote it",
    "start": "1922559",
    "end": "1931120"
  },
  {
    "text": "and as we found when we actually had our dynamo tables accidentally reconfigured",
    "start": "1931120",
    "end": "1936320"
  },
  {
    "text": "in production to five reads and five rights per second and we're getting a lot of those failure",
    "start": "1936320",
    "end": "1942080"
  },
  {
    "text": "cases we had a situation where when you looked at your profile on the web you had",
    "start": "1942080",
    "end": "1948320"
  },
  {
    "text": "one total score when you looked at it on the time series graph a totally different one when you looked",
    "start": "1948320",
    "end": "1953440"
  },
  {
    "text": "at on the profile and a third different value for your overall score when you looked it on the leaderboards",
    "start": "1953440",
    "end": "1960080"
  },
  {
    "text": "and this was clearly a big problem that's a bit of an understatement uh nothing so angry as a user community",
    "start": "1960080",
    "end": "1966640"
  },
  {
    "text": "whose stats don't add up but what we then did and we could do",
    "start": "1966640",
    "end": "1971760"
  },
  {
    "start": "1968000",
    "end": "2082000"
  },
  {
    "text": "this very quickly and i can't understand how long this would have taken us in the old architecture",
    "start": "1971760",
    "end": "1976880"
  },
  {
    "text": "we actually just re-jig things a little so that the listeners on those game history jobs instead of",
    "start": "1976880",
    "end": "1983360"
  },
  {
    "text": "actually trying to do those secondary jobs they just punted another message into yet another sqs queue",
    "start": "1983360",
    "end": "1989200"
  },
  {
    "text": "so for every school post that went in you'd actually have seven or eight messages being punted",
    "start": "1989200",
    "end": "1995039"
  },
  {
    "text": "into sqs with sqs quite happy to handle that extra load and then each one of those jobs is",
    "start": "1995039",
    "end": "2001760"
  },
  {
    "text": "processed by another set of receivers and executors and when they fail if you're having you know issues with your back end",
    "start": "2001760",
    "end": "2009600"
  },
  {
    "text": "then the failure state for those is that they can be retried they will still sit in sqs until they're",
    "start": "2009600",
    "end": "2015600"
  },
  {
    "text": "actually confirmed and removed from the system and you've got all the nice cloud watch monitoring data points that you get with",
    "start": "2015600",
    "end": "2021279"
  },
  {
    "text": "sqs so you can tell when your queue is going horribly wrong because of the jobs not being processed",
    "start": "2021279",
    "end": "2026960"
  },
  {
    "text": "and we've actually also got all the job management stuff in those receiver threads so we can see",
    "start": "2026960",
    "end": "2032880"
  },
  {
    "text": "which elements are going wrong and if we've got particular services which are going horribly wrong we can actually split things out further",
    "start": "2032880",
    "end": "2040559"
  },
  {
    "text": "in a very short process such that you can actually run something like the numeric profile update",
    "start": "2040559",
    "end": "2045600"
  },
  {
    "text": "into its own queue just for that also as a developer we have the nice little sneaky thing that we can go in use the",
    "start": "2045600",
    "end": "2052560"
  },
  {
    "text": "aws console and just have a quick peek of what messages are in the queue to see what's going on to see if there's",
    "start": "2052560",
    "end": "2058878"
  },
  {
    "text": "any particular pattern about the messages that keep being represented without actually causing any uh",
    "start": "2058879",
    "end": "2064638"
  },
  {
    "text": "destructive behavior on the queue but by doing this and actually having another set of receivers",
    "start": "2064639",
    "end": "2070638"
  },
  {
    "text": "we uh we resolved that kind of unrecoverable error state within the",
    "start": "2070639",
    "end": "2076398"
  },
  {
    "text": "system would have been nicer if you hadn't put their estate in the first place",
    "start": "2076399",
    "end": "2082720"
  },
  {
    "start": "2082000",
    "end": "2159000"
  },
  {
    "text": "and then finally here on the leaderboard side of things a few people have mentioned leaderboards",
    "start": "2082879",
    "end": "2088480"
  },
  {
    "text": "for the last couple of days we actually implemented a leaderboard component originally on simply b which",
    "start": "2088480",
    "end": "2094720"
  },
  {
    "text": "was simple and straightforward and quite slow though so as soon as",
    "start": "2094720",
    "end": "2100480"
  },
  {
    "text": "dynamodb came out we actually ran really quickly to translate that implementation",
    "start": "2100480",
    "end": "2105920"
  },
  {
    "text": "over to dynamodb so that we could actually swap out our components very easily",
    "start": "2105920",
    "end": "2110960"
  },
  {
    "text": "and in fact we actually ran for a little while mirrored with both lead wall back ends running in dev not",
    "start": "2110960",
    "end": "2116720"
  },
  {
    "text": "only so that we could actually mirror the data across but also so we can actually just validate the performance was how he",
    "start": "2116720",
    "end": "2122480"
  },
  {
    "text": "expected it however the downside to going to dynamo was we couldn't use",
    "start": "2122480",
    "end": "2128720"
  },
  {
    "text": "a simple one table structure for our leaderboards we have to actually have a secondary table",
    "start": "2128720",
    "end": "2133760"
  },
  {
    "text": "for all of the sort columns so if you've got a lead board sorted on i know",
    "start": "2133760",
    "end": "2140560"
  },
  {
    "text": "shots hits accuracy points kills deaths dozens of other things for each one of",
    "start": "2140560",
    "end": "2146480"
  },
  {
    "text": "those sort columns you actually have to have an extra row in the leaderboard sort table",
    "start": "2146480",
    "end": "2152079"
  },
  {
    "text": "for every row on the main table so actually get quite a lot of bloat on that lead board table and",
    "start": "2152079",
    "end": "2160160"
  },
  {
    "start": "2159000",
    "end": "2228000"
  },
  {
    "text": "there's actually some transactionality issues here when you do updates because with each right and each update",
    "start": "2160160",
    "end": "2166320"
  },
  {
    "text": "on the leaderboard being usually one read and one right onto the main table and then a delete of the old value in",
    "start": "2166320",
    "end": "2173920"
  },
  {
    "text": "the sorted view and then a right to add the new value in so you can actually scan through using range queries within",
    "start": "2173920",
    "end": "2181440"
  },
  {
    "text": "dynamo to look at the sorted view you've got problems where if the instance gets terminated halfway through that job",
    "start": "2181440",
    "end": "2187680"
  },
  {
    "text": "you can actually have the delete been done on the secondary table but not the insert and also with the",
    "start": "2187680",
    "end": "2195040"
  },
  {
    "text": "eventual consistency on the reads of the lead board views even if nothing had go",
    "start": "2195040",
    "end": "2200480"
  },
  {
    "text": "wrong there were short times when people's data is kind of in an",
    "start": "2200480",
    "end": "2205839"
  },
  {
    "text": "intermediate state of being updated on that secondary table so we had to have systems where we",
    "start": "2205839",
    "end": "2211280"
  },
  {
    "text": "resolved uh user scores appearing twice temporarily when they'd actually just upped their",
    "start": "2211280",
    "end": "2216720"
  },
  {
    "text": "score a little bit but the eventual consistency view of that leaderboard",
    "start": "2216720",
    "end": "2222160"
  },
  {
    "text": "showed both of the values at the same time even though the delete had gone through and the right had gone through of the",
    "start": "2222160",
    "end": "2227599"
  },
  {
    "text": "new data we also tried out various things with single table versus multiple table implementations",
    "start": "2227599",
    "end": "2233760"
  },
  {
    "start": "2228000",
    "end": "2285000"
  },
  {
    "text": "um one nice thing you can do with dynamo which is very kind of anti the old sql view where dbas really don't like",
    "start": "2233760",
    "end": "2240720"
  },
  {
    "text": "you deleting sql tables at runtime if you've got a daily or a weekly leaderboard if you actually have your",
    "start": "2240720",
    "end": "2247839"
  },
  {
    "text": "leaderboard structured so that each leaderboard is in its own table you can actually purge out last week's",
    "start": "2247839",
    "end": "2254000"
  },
  {
    "text": "data very easily just by deleting a table whereas if you've got everything stored in a single table you might have",
    "start": "2254000",
    "end": "2262319"
  },
  {
    "text": "a hundred thousand rows of data stored in that leaderboard from the last couple of days and deleting it then is a lot of reads",
    "start": "2262560",
    "end": "2270079"
  },
  {
    "text": "to actually scan through that data on a range query and a lot of deletes so actually using multiple tables can be",
    "start": "2270079",
    "end": "2276000"
  },
  {
    "text": "quite handy for that on the other hand operationally dynamically creating tables is still a",
    "start": "2276000",
    "end": "2281520"
  },
  {
    "text": "bit of a problem if you want to be able to back up your data and finally with the",
    "start": "2281520",
    "end": "2286560"
  },
  {
    "start": "2285000",
    "end": "2323000"
  },
  {
    "text": "leaderboards we've still had the issue that we'd had way back on the sql side of things and never really got away from",
    "start": "2286560",
    "end": "2292400"
  },
  {
    "text": "which is that fast ranking is still a problem so you can find your schools on the leaderboard through a one read operation",
    "start": "2292400",
    "end": "2299839"
  },
  {
    "text": "you can page through the leaderboard via any of the sorted columns very easily but actually finding out",
    "start": "2299839",
    "end": "2306079"
  },
  {
    "text": "what rank you are well you can actually count how many read operations that's going to cost",
    "start": "2306079",
    "end": "2311599"
  },
  {
    "text": "because if you use the simple uh you know size operation on a range query",
    "start": "2311599",
    "end": "2317280"
  },
  {
    "text": "that is quite clearly going to be reading through all the table entries above your score and",
    "start": "2317280",
    "end": "2324400"
  },
  {
    "start": "2323000",
    "end": "2404000"
  },
  {
    "text": "in fact whilst that is more obvious when you're using something like dynamo or simply b actually often a lot of the same",
    "start": "2324400",
    "end": "2331280"
  },
  {
    "text": "operation is going on in your sql database when you're trying to rank query but that doesn't show up when you're",
    "start": "2331280",
    "end": "2336560"
  },
  {
    "text": "doing dev with small volumes of data but when you're in production and if you're like me and you're a hundred",
    "start": "2336560",
    "end": "2342079"
  },
  {
    "text": "thousandths in the overall leaderboard and not in the top ten um it's quite an intensive operation to",
    "start": "2342079",
    "end": "2347920"
  },
  {
    "text": "find your your kind of perfect perfect rank within the system so for that we actually implemented um a",
    "start": "2347920",
    "end": "2355440"
  },
  {
    "text": "couple of different ways to get around that problem um if you want to know more details you'll have to ask me later because it's",
    "start": "2355440",
    "end": "2361440"
  },
  {
    "text": "a big and fussy topic but we had a fast ranking component which used a reverse index",
    "start": "2361440",
    "end": "2368079"
  },
  {
    "text": "that we'd calculate and update on the fly as people's scores came in so that would actually give you a table which ranked",
    "start": "2368079",
    "end": "2374079"
  },
  {
    "text": "through from score to rank and we also had a statistical predictor that would periodically on a",
    "start": "2374079",
    "end": "2379680"
  },
  {
    "text": "scheduled job scan through all of the leaderboards and actually generate a",
    "start": "2379680",
    "end": "2386400"
  },
  {
    "text": "a curve on a number of control points so if you had 100 000 row leaderboard it might actually compute 100 control",
    "start": "2386880",
    "end": "2393200"
  },
  {
    "text": "points to be able to predict what rank you'd have for a given score but each of those ones has its own",
    "start": "2393200",
    "end": "2398480"
  },
  {
    "text": "pros and cons and neither of them is a perfect answer to the fast ranking problem that we have",
    "start": "2398480",
    "end": "2404880"
  },
  {
    "start": "2404000",
    "end": "2420000"
  },
  {
    "text": "so from those few specific examples you'll see how we actually made use of aws services a little bit",
    "start": "2405520",
    "end": "2411520"
  },
  {
    "text": "and also from the overall topology how we made use of the kind of application level services but we did",
    "start": "2411520",
    "end": "2417760"
  },
  {
    "text": "have a number of issues first of all very quickly old style",
    "start": "2417760",
    "end": "2424079"
  },
  {
    "start": "2420000",
    "end": "2594000"
  },
  {
    "text": "monitoring we within sony have got a network operation center they've got their own monitoring tools",
    "start": "2424079",
    "end": "2429760"
  },
  {
    "text": "and they've got people in there 24 7. um and initially how we set things up",
    "start": "2429760",
    "end": "2435040"
  },
  {
    "text": "there is actually monitoring on each individual server instance on these auto scaled servers what we found very rapidly was that with",
    "start": "2435040",
    "end": "2442480"
  },
  {
    "text": "the topology only being updated uh maybe every 15 minutes of those auto",
    "start": "2442480",
    "end": "2447839"
  },
  {
    "text": "scale environments oftentimes the old-school monitoring",
    "start": "2447839",
    "end": "2452880"
  },
  {
    "text": "systems would actually throw up a bunch of red lights oh my god that server's gone horribly wrong well no actually the autoscaler",
    "start": "2452880",
    "end": "2459040"
  },
  {
    "text": "terminated it 10 minutes ago you have haven't updated your topology yet so",
    "start": "2459040",
    "end": "2464240"
  },
  {
    "text": "i'm very keen on something that's been mentioned yesterday around getting away from monitoring",
    "start": "2464240",
    "end": "2470400"
  },
  {
    "text": "individual instances so as well as monitoring the built-in things like sqs queues",
    "start": "2470400",
    "end": "2476160"
  },
  {
    "text": "monitoring elb latency on your overall rest apis we also have a lot of more",
    "start": "2476160",
    "end": "2483200"
  },
  {
    "text": "detailed custom cloudwatch endpoints where actually are monitoring things like how long are the",
    "start": "2483200",
    "end": "2490319"
  },
  {
    "text": "lead board requests taken to get answered how long is it taking to log in all those kind of things",
    "start": "2490319",
    "end": "2495760"
  },
  {
    "text": "and then drilling into the underlying services so we've got that same kind of onion skin that we can look into",
    "start": "2495760",
    "end": "2502319"
  },
  {
    "text": "what's going on in the system whereas the old style monitoring was often a source of uh",
    "start": "2502319",
    "end": "2508079"
  },
  {
    "text": "unneeded call outs we also had automatic scaling of server",
    "start": "2508079",
    "end": "2513680"
  },
  {
    "text": "instances but manual scaling management of dynamodb which resulted in",
    "start": "2513680",
    "end": "2518800"
  },
  {
    "text": "um how can i put this um me running as a bot when we were in",
    "start": "2518800",
    "end": "2524720"
  },
  {
    "text": "beta going through the hosting guys and looking at the cloud watch metrics for all our dynamo tables",
    "start": "2524720",
    "end": "2530640"
  },
  {
    "text": "and doing this kind of head scratching chin scratching exercise do we need another 50 100 reads on that",
    "start": "2530640",
    "end": "2537359"
  },
  {
    "text": "table and i think it only took us a couple of three days to realize that this was a really stupid way of doing things",
    "start": "2537359",
    "end": "2544240"
  },
  {
    "text": "and so what we actually implemented then was another scheduled job on the game servers that we've been quite hesitant",
    "start": "2544240",
    "end": "2550319"
  },
  {
    "text": "to turn on fully but which in a in a scheduled job",
    "start": "2550319",
    "end": "2555839"
  },
  {
    "text": "looks at each table looks at the cloudops metrics you know does in a piece of code what",
    "start": "2555839",
    "end": "2560880"
  },
  {
    "text": "we're doing manually and being kind of bot like and that actually increases the throughput during the day",
    "start": "2560880",
    "end": "2567040"
  },
  {
    "text": "and then once a day it will decrease it because with dynamite db whilst you have got some auto scaling",
    "start": "2567040",
    "end": "2572480"
  },
  {
    "text": "capabilities there if you use them you can't actually just freely scale things up and down constantly",
    "start": "2572480",
    "end": "2578000"
  },
  {
    "text": "you can only do i think four damn scales per day but that gives us a certain amount of",
    "start": "2578000",
    "end": "2584640"
  },
  {
    "text": "distance from the day-to-day grunt work of managing our dynamodb tables but with a little bit of",
    "start": "2584640",
    "end": "2591200"
  },
  {
    "text": "configuration around it we can actually automate that we also have a problem with dynamite db",
    "start": "2591200",
    "end": "2597440"
  },
  {
    "start": "2594000",
    "end": "2652000"
  },
  {
    "text": "and the leaderboards we have a few actually first of all",
    "start": "2597440",
    "end": "2602800"
  },
  {
    "text": "we pre predated the local secondary indexes feature and had already implemented our",
    "start": "2602880",
    "end": "2609599"
  },
  {
    "text": "leaderboards when that came in we did look at actually switching over to using local secondary indices",
    "start": "2609599",
    "end": "2615440"
  },
  {
    "text": "but i think some of our tables the the number of rows and the number of attributes",
    "start": "2615440",
    "end": "2620720"
  },
  {
    "text": "and the weather in alaska combined together to work out how much",
    "start": "2620720",
    "end": "2626000"
  },
  {
    "text": "volume of data there was going to be in that table was going to be above the limit of what we could have used with a single hash",
    "start": "2626000",
    "end": "2631680"
  },
  {
    "text": "key which is limited to i think 10 gig of data overall so we had to stick with that existing",
    "start": "2631680",
    "end": "2637760"
  },
  {
    "text": "system and also with our leaderboards not all leaderboards are as active as each other with killzone shadowful",
    "start": "2637760",
    "end": "2644560"
  },
  {
    "text": "you've got user-generated game modes that users can create a new custom game",
    "start": "2644560",
    "end": "2649760"
  },
  {
    "text": "mode and each game mode has its own leaderboard but also you've got developer created",
    "start": "2649760",
    "end": "2654960"
  },
  {
    "start": "2652000",
    "end": "2735000"
  },
  {
    "text": "leaderboard developer created game modes that are in there from the start and the activity level on those ones is",
    "start": "2654960",
    "end": "2661359"
  },
  {
    "text": "much much higher than you're going to see on the user generated ones where a lot of those",
    "start": "2661359",
    "end": "2667359"
  },
  {
    "text": "game modes will be only played a few times and the problem with those actually all being in the same dynamodb",
    "start": "2667359",
    "end": "2672800"
  },
  {
    "text": "tables is that you have a number of hash keys which are active like orders of magnitude higher",
    "start": "2672800",
    "end": "2680000"
  },
  {
    "text": "than others and that leads to hotspots in that dynamodb table and so you have to kind of over",
    "start": "2680000",
    "end": "2686319"
  },
  {
    "text": "provision the throughput quite a lot to scale up the shards that underlie the dynamodb service",
    "start": "2686319",
    "end": "2691440"
  },
  {
    "text": "that are providing access to those particular hash keys and whilst we tried looking at whether",
    "start": "2691440",
    "end": "2697760"
  },
  {
    "text": "we could split those out into separate instances the leaderboard service it actually turns out that it's not that",
    "start": "2697760",
    "end": "2704160"
  },
  {
    "text": "simple from the game point of view because post launch some users will create uh game modes that everyone loves",
    "start": "2704160",
    "end": "2711359"
  },
  {
    "text": "and that will be recommended and they're on the front page so it's not like we can actually just statically configure",
    "start": "2711359",
    "end": "2716720"
  },
  {
    "text": "one table for the really popular leaderboards and another table for the kind of general run-of-the-mill leaderboards",
    "start": "2716720",
    "end": "2725040"
  },
  {
    "text": "those things would have to actually be dynamically moved from one set of data to another and we haven't actually yet resolved",
    "start": "2725040",
    "end": "2730560"
  },
  {
    "text": "that issue so we're just having to monitor very carefully the status of those tables",
    "start": "2730560",
    "end": "2735920"
  },
  {
    "start": "2735000",
    "end": "2757000"
  },
  {
    "text": "we also have problems with surprise changes the console we've gone from a land where we had control we had to build our own",
    "start": "2735920",
    "end": "2741440"
  },
  {
    "text": "deployment systems and all our monitoring systems and everything but now we use aws console",
    "start": "2741440",
    "end": "2746960"
  },
  {
    "text": "and that's great until the day comes when the beanstalk page is all changed to a",
    "start": "2746960",
    "end": "2752400"
  },
  {
    "text": "wonderful new new ui and the cloud watch pages all change again and whilst for us as developers",
    "start": "2752400",
    "end": "2759280"
  },
  {
    "text": "that's not such a bad problem because it only takes a half an hour to get your head around the change but one of the other titles that we work",
    "start": "2759280",
    "end": "2765520"
  },
  {
    "text": "with they just implemented a runbook so the network operations guys could actually restart servers and",
    "start": "2765520",
    "end": "2771920"
  },
  {
    "text": "play around with configs and things like that at two in the morning without waking up the game team and that included a whole load of",
    "start": "2771920",
    "end": "2778319"
  },
  {
    "text": "detailed information on what was where on the aws console and what to change and then within a couple of days",
    "start": "2778319",
    "end": "2784560"
  },
  {
    "text": "of their title coming out and the but being published out to the knock those beanstalk pages changed",
    "start": "2784560",
    "end": "2790960"
  },
  {
    "text": "completely and so they had to rewrite their run book and it's it's uh",
    "start": "2790960",
    "end": "2796640"
  },
  {
    "text": "you're taking away a certain amount of control because you're no longer actually owning that system that is somebody else's system and it",
    "start": "2796640",
    "end": "2803440"
  },
  {
    "text": "would be lovely if we could actually opt in to those updates you know give us",
    "start": "2803440",
    "end": "2808480"
  },
  {
    "text": "a month to pick up the new version but as it stands you sometimes get a strange call from a not guy",
    "start": "2808480",
    "end": "2814960"
  },
  {
    "text": "who can't actually make the changes themselves because they're not entirely sure of how the system works anymore and finally we hit some limits of cloud",
    "start": "2814960",
    "end": "2821440"
  },
  {
    "text": "formation um you know we had these hybrid deployments that couldn't just use cloud formation",
    "start": "2821440",
    "end": "2826960"
  },
  {
    "text": "on their own but we also had some things with the beanstalk particularly where some of those cloud formation",
    "start": "2826960",
    "end": "2832960"
  },
  {
    "text": "resources aren't updatable currently so if you're trying to follow a best practice of",
    "start": "2832960",
    "end": "2838400"
  },
  {
    "text": "you deploy using cloud formation you reconfigure using cloud formation you update the web app version user",
    "start": "2838400",
    "end": "2843680"
  },
  {
    "text": "cloud formation you can't do that so if you have a disaster recovery situation where you want to deploy to a",
    "start": "2843680",
    "end": "2849440"
  },
  {
    "text": "new region very quickly if you use the cloud information template that's currently in prod and redeploy that to a new region you",
    "start": "2849440",
    "end": "2856160"
  },
  {
    "text": "actually end up deploying the service from if you're lucky a couple of weeks ago but it could be months so",
    "start": "2856160",
    "end": "2862160"
  },
  {
    "text": "we're actually quite keen to see updates on the cloud formation side such that we can actually update those",
    "start": "2862160",
    "end": "2867440"
  },
  {
    "text": "elements and keep to kind of i hate to say the word enterprise but i think it's an enterprise best practice",
    "start": "2867440",
    "end": "2874640"
  },
  {
    "start": "2874000",
    "end": "2969000"
  },
  {
    "text": "overall though i think we have actually learned some some pretty good lessons you know we've moved from sql to nosql",
    "start": "2874880",
    "end": "2882000"
  },
  {
    "text": "and you know there's been some road bumps along the way such as the school management side of things but",
    "start": "2882000",
    "end": "2887680"
  },
  {
    "text": "it actually allowed us to get that flexibility we never had before and actually having that flexibility",
    "start": "2887680",
    "end": "2894400"
  },
  {
    "text": "added options we had some late decisions from the game design to adding new features on mercenary and we could actually add whole new",
    "start": "2894400",
    "end": "2900960"
  },
  {
    "text": "services in ways we couldn't do before also as i mentioned with the dynamodb",
    "start": "2900960",
    "end": "2906400"
  },
  {
    "text": "the dynamic scaling and management which gives you resilient scaling and management doesn't stop at just server boxes you",
    "start": "2906400",
    "end": "2912160"
  },
  {
    "text": "have to look at everything that's in your server stack and everything you're dependent on a lot of the aws services manage that",
    "start": "2912160",
    "end": "2918000"
  },
  {
    "text": "scaling and manage that resiliency without you needing to look at them but not everything so you have to actually",
    "start": "2918000",
    "end": "2923440"
  },
  {
    "text": "appreciate everything in your topology and finally we've still not found a silver bullet for leaderboards",
    "start": "2923440",
    "end": "2929920"
  },
  {
    "text": "um i'm quite keen uh i mean we do use elastic cash from memcache at the moment but i'm very keen to look into redis and",
    "start": "2929920",
    "end": "2937119"
  },
  {
    "text": "some of the in-memory solutions for leaderboard management and you know maybe we'll swap out post",
    "start": "2937119",
    "end": "2942640"
  },
  {
    "text": "launch to a faster leaderboard solution and one which doesn't hit the dynamo side quite so heavily",
    "start": "2942640",
    "end": "2948400"
  },
  {
    "text": "because we have to have some quite high throughputs configured on dynamo and like i say we've also got that hotspot issue overall though i think we",
    "start": "2948400",
    "end": "2955599"
  },
  {
    "text": "have actually transitioned to a much more flexible system and one that we've",
    "start": "2955599",
    "end": "2960640"
  },
  {
    "text": "implemented using smaller teams than we've had before",
    "start": "2960640",
    "end": "2964800"
  },
  {
    "text": "i've got a few minutes now for any questions if you've got any",
    "start": "2966160",
    "end": "2971039"
  }
]