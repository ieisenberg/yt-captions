[
  {
    "start": "0",
    "end": "68000"
  },
  {
    "text": "all right you guys can hear me yes yes okay I see heads nodding okay so my name",
    "start": "0",
    "end": "5430"
  },
  {
    "text": "is Kyle I am the leader of our systems engineering group over at bill comm we're going to talk a little bit about",
    "start": "5430",
    "end": "11550"
  },
  {
    "text": "bill comm if you haven't heard of us but we are essentially the number one small",
    "start": "11550",
    "end": "17340"
  },
  {
    "text": "business bill payment provider we make it simple to pay and get paid so we are",
    "start": "17340",
    "end": "22769"
  },
  {
    "text": "technically a start-up we you know privately funded but we've been around for over a decade the company has been",
    "start": "22769",
    "end": "29460"
  },
  {
    "text": "established been growing very rapidly and so essentially we're moving from our",
    "start": "29460",
    "end": "34829"
  },
  {
    "text": "on-premises data centers and coalos and all of the stuff that comes with that",
    "start": "34829",
    "end": "39989"
  },
  {
    "text": "moving into the cloud this talk is all about our journey to the cloud",
    "start": "39989",
    "end": "45079"
  },
  {
    "text": "specifically around sage maker and glue and s3 and how we leverage AWS services",
    "start": "45079",
    "end": "50760"
  },
  {
    "text": "to build what we call our data platform which is essentially a data Lake but also the sage maker components they get",
    "start": "50760",
    "end": "57149"
  },
  {
    "text": "added on top of that okay everyone at the right session everyone happy okay this is what we're gonna go all right",
    "start": "57149",
    "end": "65629"
  },
  {
    "start": "68000",
    "end": "177000"
  },
  {
    "text": "all right great so the agenda today is we're gonna",
    "start": "68659",
    "end": "74190"
  },
  {
    "text": "essentially talk about why we need machine learning you're just a bill payments company why'd it why is machine",
    "start": "74190",
    "end": "79710"
  },
  {
    "text": "learning important why what is the bill com use case we're gonna go through the old ways how we sort of did machine",
    "start": "79710",
    "end": "86130"
  },
  {
    "text": "learning in the past on-prem and then how we built the secure data Lake inside of AWS and then we're going to go",
    "start": "86130",
    "end": "92670"
  },
  {
    "text": "through our stage maker pipeline as well as Mikey mentioned we also have the slide oh please ask questions this is a",
    "start": "92670",
    "end": "99900"
  },
  {
    "text": "fairly short presentation hoping that we can go through it but then also interact",
    "start": "99900",
    "end": "105540"
  },
  {
    "text": "so if you have questions about our architecture I also plan on explaining some of our learnings some of the things",
    "start": "105540",
    "end": "111090"
  },
  {
    "text": "that we discovered along the way because this is definitely new I mean it was really fun he would and he went up there",
    "start": "111090",
    "end": "117810"
  },
  {
    "text": "today's ten thousand people have a data Lake you know but if you don't have one it's actually not that easy to just turn",
    "start": "117810",
    "end": "124110"
  },
  {
    "text": "on the new services they announced today to actually help with that didn't exist so we had to build it ourselves we had",
    "start": "124110",
    "end": "130890"
  },
  {
    "text": "AWS help to try to build it the right way but ultimately I come from a systems background I am a former rack hugger",
    "start": "130890",
    "end": "138420"
  },
  {
    "text": "you know self reclaim former rack cover you know recovering sysadmin right so I",
    "start": "138420",
    "end": "143700"
  },
  {
    "text": "come from a sysadmin background I am NOT a machine learning engineer I am NOT a data scientist my job is to keep bill",
    "start": "143700",
    "end": "150330"
  },
  {
    "text": "calm customer data safe are my job is to keep it operating I'm in charge of the site reliability operations production",
    "start": "150330",
    "end": "158040"
  },
  {
    "text": "all of the systems and network so my number one goal in building a lake is making sure it's secure so this talk is",
    "start": "158040",
    "end": "164190"
  },
  {
    "text": "going to focus a lot on the security aspects of the data Lake as well as a little bit about how the machine",
    "start": "164190",
    "end": "169770"
  },
  {
    "text": "learning pipeline works but ultimately we're going to dig into just how we actually secured",
    "start": "169770",
    "end": "175940"
  },
  {
    "start": "177000",
    "end": "425000"
  },
  {
    "text": "awesome okay why does Bill calm need machine learning we are actually a",
    "start": "177010",
    "end": "184670"
  },
  {
    "text": "company in it if you've ever used like if you have a personal bill payment service it's fairly simple right if you",
    "start": "184670",
    "end": "191510"
  },
  {
    "text": "have chase you go on to the chase website you pay a bill right you go on there you say okay I want to pay a bill",
    "start": "191510",
    "end": "197750"
  },
  {
    "text": "recurring payments are done small business owners don't get to do that they have a lot more complexity around",
    "start": "197750",
    "end": "203570"
  },
  {
    "text": "their bill payment process in order to either pay or get paid a lot of times they're either receiving invoices or",
    "start": "203570",
    "end": "210110"
  },
  {
    "text": "they're paying invoices as a result we import all of those invoices into our",
    "start": "210110",
    "end": "215330"
  },
  {
    "text": "system for tracking sometimes I joke that bill becomes a lot more like Dropbox than PayPal because of the",
    "start": "215330",
    "end": "222410"
  },
  {
    "text": "amount of documents that we actually store we have terabytes of documents within our system in order to track you",
    "start": "222410",
    "end": "228440"
  },
  {
    "text": "know everything from invoices to contracts to any kind of thing that's uploaded these documents are random",
    "start": "228440",
    "end": "235760"
  },
  {
    "text": "unstructured completely formed in whatever way a customer ended up publishing it into the system as a",
    "start": "235760",
    "end": "244220"
  },
  {
    "text": "result like if you ever have a invoice they're all different formats QuickBooks has a different invoice format than say",
    "start": "244220",
    "end": "251420"
  },
  {
    "text": "like your Amazon bill that you get from AWS every every invoice is different our system needs to not only recognize the",
    "start": "251420",
    "end": "258019"
  },
  {
    "text": "type of invoice that comes in but then even an unstructured format if you had an invoice that comes in what is the due",
    "start": "258020",
    "end": "264560"
  },
  {
    "text": "like if you what is the total due amount on the invoice what is the due date we can't have invoices paid late so if the",
    "start": "264560",
    "end": "271880"
  },
  {
    "text": "system recognizes the document that's inserted we need to be able to use machine learning to actually recognize",
    "start": "271880",
    "end": "278650"
  },
  {
    "text": "what the import what is inside the invoice and when it needs to be paid also need to determine the vendor who",
    "start": "278650",
    "end": "285080"
  },
  {
    "text": "needs to get paid you don't want to pay the wrong person that would be bad",
    "start": "285080",
    "end": "290349"
  },
  {
    "text": "the second part about Bill Kong is that we've managed over a million transactions a month through our",
    "start": "291529",
    "end": "298229"
  },
  {
    "text": "payments platform and we have over a sixty billion dollar payment volume that flows to our platform the reason I say",
    "start": "298229",
    "end": "304559"
  },
  {
    "text": "this is essentially to emphasize that security is our number one goal we have a security team we take security system",
    "start": "304559",
    "end": "311219"
  },
  {
    "text": "seriously and we have dedicated engineers on it it is important for us",
    "start": "311219",
    "end": "316289"
  },
  {
    "text": "as we went through the machine learning exercise we had that mindset a lot of times when you're building a data",
    "start": "316289",
    "end": "322139"
  },
  {
    "text": "platform in a data Lake it's something that you don't have to worry as much about but for us it was key so we're",
    "start": "322139",
    "end": "328110"
  },
  {
    "text": "gonna talk a little bit about that later customers expect all the data entry to",
    "start": "328110",
    "end": "333149"
  },
  {
    "text": "be automatic ok so who uses something like concur Expensify who's gonna do that thing when",
    "start": "333149",
    "end": "338550"
  },
  {
    "text": "they go home yes ok I have to do that if you ever do that you upload a document",
    "start": "338550",
    "end": "344009"
  },
  {
    "text": "into Expensify you want it to be accurate you want it to be fast and you",
    "start": "344009",
    "end": "349319"
  },
  {
    "text": "don't want to type anything small business owners are the same way when a small business owner is using our",
    "start": "349319",
    "end": "355199"
  },
  {
    "text": "platform although we have an awesome platform and we want them to use it they want to get in and out they want to",
    "start": "355199",
    "end": "361379"
  },
  {
    "text": "process their bills so they can go back and actually run their business they're not they're not there to go do all this",
    "start": "361379",
    "end": "367139"
  },
  {
    "text": "that's why they have delcom is to spend less time on managing their invoice 'men invoices and payments so auto data entry",
    "start": "367139",
    "end": "374699"
  },
  {
    "text": "was super important manual entry by humans is expensive you may know that a",
    "start": "374699",
    "end": "380639"
  },
  {
    "text": "lot of folks in this field if there's a lot of automated processing OCR has been solved a long time ago right a lot of",
    "start": "380639",
    "end": "387689"
  },
  {
    "text": "companies can do OCR but what customers need is something a lot more advanced than that so some companies will",
    "start": "387689",
    "end": "393870"
  },
  {
    "text": "outsource the you know actual manual data entry of their documents into a",
    "start": "393870",
    "end": "399809"
  },
  {
    "text": "third party outsourcer that's like maybe onshore or offshore there's different providers that can help do that and AWS",
    "start": "399809",
    "end": "406199"
  },
  {
    "text": "provides you know stuff like Mechanical Turk to help with that sort of thing but that is expensive that option is very",
    "start": "406199",
    "end": "411779"
  },
  {
    "text": "expensive and then the other issue with that is that it's slow and we can't have processing of these documents to be slow",
    "start": "411779",
    "end": "418860"
  },
  {
    "text": "okay everyone with me so far everyone gets it okay awesome all right we're",
    "start": "418860",
    "end": "426150"
  },
  {
    "start": "425000",
    "end": "826000"
  },
  {
    "text": "gonna talk a little bit about the journey before machine learning was performed entirely on premise we would",
    "start": "426150",
    "end": "434550"
  },
  {
    "text": "lock all the data away so yeah you know so we have all our data centers all the data is all protected it's secure none",
    "start": "434550",
    "end": "442020"
  },
  {
    "text": "of the bad guys can get it which is awesome however if you have spent 10 minutes working with machine learning",
    "start": "442020",
    "end": "447270"
  },
  {
    "text": "engineers they want all the data right they're like I need all the data it's",
    "start": "447270",
    "end": "452370"
  },
  {
    "text": "actually more data than the traditional I would say the business analytics people would have required right in that",
    "start": "452370",
    "end": "458280"
  },
  {
    "text": "in the past you had a data warehouse and you're like oh I'll just pull some data out of the data warehouse for machine",
    "start": "458280",
    "end": "463350"
  },
  {
    "text": "learning engineers and data scientists they need all the raw data so for us one of the big challenges is around how do",
    "start": "463350",
    "end": "470370"
  },
  {
    "text": "we take our on-premise Oracle databases and actually provide data science and",
    "start": "470370",
    "end": "475830"
  },
  {
    "text": "machine learning on top of them so what we ended up having to do our manual extracts so we actually had to look at",
    "start": "475830",
    "end": "482250"
  },
  {
    "text": "the tape actually look at Oracle extracted into another Oracle database that was also on-premise and then give",
    "start": "482250",
    "end": "488460"
  },
  {
    "text": "the data scientists access to that that process was super slow and if you asked our data scientists how that process",
    "start": "488460",
    "end": "495330"
  },
  {
    "text": "went he would not have a very happy response they say it took too long I",
    "start": "495330",
    "end": "500550"
  },
  {
    "text": "there's too many security approvals required there's too many exceptions required and not not good at all so what",
    "start": "500550",
    "end": "508380"
  },
  {
    "text": "do we do our journey that cloud was essentially to take these manual extracts that we were performing on",
    "start": "508380",
    "end": "513810"
  },
  {
    "text": "premise and say okay well we've built a model we're actually on our third generation of machine learning now and",
    "start": "513810",
    "end": "520080"
  },
  {
    "text": "we've already built a model so how do we leverage AWS and sage maker to actually host that model so we actually started",
    "start": "520080",
    "end": "527220"
  },
  {
    "text": "there we started using sage maker on the inference side we didn't actually start using it to build our models we said",
    "start": "527220",
    "end": "534240"
  },
  {
    "text": "well we already have it so we took the model threw it into Sage maker and actually we're able to execute moving",
    "start": "534240",
    "end": "541170"
  },
  {
    "text": "that into production fairly quickly on the order of a couple months as opposed to many months to actually if we were to",
    "start": "541170",
    "end": "546930"
  },
  {
    "text": "build our own service and actually create our own Python extraction micro",
    "start": "546930",
    "end": "552000"
  },
  {
    "text": "service actually do all the inference and all the processing for the machine learning then it would have taken a lot longer",
    "start": "552000",
    "end": "558900"
  },
  {
    "text": "and as an Operations engineer I'd have to set up services had to complete all the dependency all of monitoring",
    "start": "558900",
    "end": "564720"
  },
  {
    "text": "everything for us it was a lot easier to just host it directly in the cloud and I'm going to show the architecture later",
    "start": "564720",
    "end": "570390"
  },
  {
    "text": "so you can see you know sort of how that works okay as I mentioned I'm an",
    "start": "570390",
    "end": "577470"
  },
  {
    "text": "Operations guy security is important I focused a lot on how do we actually protect the data I'm sure if you've been",
    "start": "577470",
    "end": "583800"
  },
  {
    "text": "at this conference now in two or three days you hear everyone says s3 is secure right s3 is secure and I actually agree",
    "start": "583800",
    "end": "591450"
  },
  {
    "text": "with that AWS provides a very secure platform with s3 but out of the box it is not perfect right and so what we did",
    "start": "591450",
    "end": "598500"
  },
  {
    "text": "is we worked with AWS to provide the best security around those s3 buckets that we could we decided from the",
    "start": "598500",
    "end": "604530"
  },
  {
    "text": "beginning it wasn't sufficient to just say Oh throw the data in a data Lake and call it done so these are some of the",
    "start": "604530",
    "end": "611010"
  },
  {
    "text": "policies we implemented the first is a bucket policy requiring only specific",
    "start": "611010",
    "end": "616110"
  },
  {
    "text": "users to have access by default in s3 bucket just gives everyone in the account access for us we wanted",
    "start": "616110",
    "end": "621840"
  },
  {
    "text": "fine-grained access control for us we have both machine learning engineers data scientists operations people and",
    "start": "621840",
    "end": "628140"
  },
  {
    "text": "different roles within the company so for us we utilize these roles and then",
    "start": "628140",
    "end": "633690"
  },
  {
    "text": "we say only allow these specific role IDs to have access to this bucket that was the first thing the second was we",
    "start": "633690",
    "end": "640920"
  },
  {
    "text": "had specific roles that were created around sage maker and glue and gave those roles access to the bucket that",
    "start": "640920",
    "end": "646530"
  },
  {
    "text": "allowed us to give separate so sage maker needs certain things to actually build the model and glue need certain",
    "start": "646530",
    "end": "652680"
  },
  {
    "text": "things in order to actually extract as a part of the data Lake the next part is",
    "start": "652680",
    "end": "658230"
  },
  {
    "text": "around kms and encryption encryption is really important by default s3 has a default encryption",
    "start": "658230",
    "end": "664110"
  },
  {
    "text": "encryption through the SSE a kms you can just check a box and it's on however the",
    "start": "664110",
    "end": "669330"
  },
  {
    "text": "default encryption provided by AWS was not sufficient for us we wanted to have more fine gain control about who was",
    "start": "669330",
    "end": "676500"
  },
  {
    "text": "allowed to encrypt and decrypt data so what we did was not only require kms",
    "start": "676500",
    "end": "681540"
  },
  {
    "text": "encryption and SSL encryption on the bucket but we required a specific KMS key for that bucket and",
    "start": "681540",
    "end": "687710"
  },
  {
    "text": "said this key is required to actually access this bucket and then we also have",
    "start": "687710",
    "end": "692780"
  },
  {
    "text": "to provide if you've ever worked with kms in addition to a bucket policy present permissions you have to provide",
    "start": "692780",
    "end": "698390"
  },
  {
    "text": "kms permissions so that we have specific permissions around what users are allowed to encrypt and decrypt data in",
    "start": "698390",
    "end": "704870"
  },
  {
    "text": "the bucket as well as who are the admins on those keys having your own kms allows you to get a little bit more fine-grain",
    "start": "704870",
    "end": "711410"
  },
  {
    "text": "about your keys who you know the rotation policies and other things that you wouldn't be able to do on the",
    "start": "711410",
    "end": "716570"
  },
  {
    "text": "default policy okay the next is evaluating fields so for us",
    "start": "716570",
    "end": "721820"
  },
  {
    "text": "a lot of the time that was spent in building this was building all the fields building all of the data that was",
    "start": "721820",
    "end": "728660"
  },
  {
    "text": "required to actually place in the data Lake so we spent a lot of time building our own scripts to actually extract the",
    "start": "728660",
    "end": "734990"
  },
  {
    "text": "data and one by one field by or column by column determine which which columns",
    "start": "734990",
    "end": "740630"
  },
  {
    "text": "are required to actually insert into the data Lake we didn't just willy-nilly say oh I just throw the whole thing in there",
    "start": "740630",
    "end": "746900"
  },
  {
    "text": "and just assume so for us we actually started small we said only non PII non",
    "start": "746900",
    "end": "753050"
  },
  {
    "text": "sensitive data is allowed into the data Lake and we'll start there we're gonna get comfortable with it and then as we",
    "start": "753050",
    "end": "759200"
  },
  {
    "text": "get more comfort then we're gonna go ahead and add more sensitive data once we feel like it's good to go so remember",
    "start": "759200",
    "end": "765470"
  },
  {
    "text": "you know we're on this journey right we didn't when you start in the cloud it's a different story than when you're trying to move on premise to the cloud",
    "start": "765470",
    "end": "772070"
  },
  {
    "text": "we have a lot of compliance regulations where any money laundering oh fak we",
    "start": "772070",
    "end": "777590"
  },
  {
    "text": "have to be money transmitted or licensed in all 50 states you know PCI certified internal audits",
    "start": "777590",
    "end": "783380"
  },
  {
    "text": "through saag external audits through banks it's very important to us that we not only protect the data but we protect",
    "start": "783380",
    "end": "788960"
  },
  {
    "text": "our partnerships in our brand last is octa so octa we uses our single",
    "start": "788960",
    "end": "795680"
  },
  {
    "text": "sign-on is super awesome we like octa and we use it for all the federated roles we said that only the",
    "start": "795680",
    "end": "802490"
  },
  {
    "text": "stage maker in blue rolls are great but that's only within the AWS services the",
    "start": "802490",
    "end": "807770"
  },
  {
    "text": "actual octave sam'l is used for any I so say humans so if you're a human you should be using octa even if you're if",
    "start": "807770",
    "end": "814250"
  },
  {
    "text": "no matter what administration function that you're doing we use octa and separate accounts in order to determine",
    "start": "814250",
    "end": "819950"
  },
  {
    "text": "whether your pre-production or production alright everyone with me awesome ok we're going to go through the",
    "start": "819950",
    "end": "828920"
  },
  {
    "start": "826000",
    "end": "1106000"
  },
  {
    "text": "the first part of the platform first part of the platform is all about how did we manage the data Lake this is the",
    "start": "828920",
    "end": "835220"
  },
  {
    "text": "date the actual data Lake data platform that we built there are three different",
    "start": "835220",
    "end": "842990"
  },
  {
    "text": "entities involved here there's a corporate data center that's the thing all the way to the left this middle",
    "start": "842990",
    "end": "848240"
  },
  {
    "text": "section is our AWS account that we call the data Lake and then we have on the",
    "start": "848240",
    "end": "853430"
  },
  {
    "text": "right we have an AWS account that's actually running for production so if you're Ahmet and well we'll kind of",
    "start": "853430",
    "end": "858590"
  },
  {
    "text": "start from the left and move over to the right to kind of understand what we're doing so on premise on premise we have",
    "start": "858590",
    "end": "870830"
  },
  {
    "text": "two is two primary databases our Oracle database and our my sequel database",
    "start": "870830",
    "end": "875870"
  },
  {
    "text": "Oracle's our primary transaction database my sequel is used for secondary micro services what we do is we use an",
    "start": "875870",
    "end": "881720"
  },
  {
    "text": "extraction from these databases into a master CSV file we spent a lot of effort",
    "start": "881720",
    "end": "888290"
  },
  {
    "text": "making sure that we did this right and then we found out today that there's just a service that can do it you know",
    "start": "888290",
    "end": "893870"
  },
  {
    "text": "out of the box but that's okay so but for now we built this extract everything",
    "start": "893870",
    "end": "900230"
  },
  {
    "text": "into a CSV file each CSV file represents a table so every table has its own CSV",
    "start": "900230",
    "end": "906230"
  },
  {
    "text": "and then we extract it every day we then take that extraction and we use the AWS",
    "start": "906230",
    "end": "911870"
  },
  {
    "text": "CLI to actually import all of that data into s3 so remember none of this data is",
    "start": "911870",
    "end": "917120"
  },
  {
    "text": "in the cloud we have to actually get it to the cloud right so that's the first step managing through the CLI we send it to",
    "start": "917120",
    "end": "924339"
  },
  {
    "text": "the cloud once it's in what we call the landing zone that's the s3 sort of left",
    "start": "924339",
    "end": "929470"
  },
  {
    "text": "side here we have our CSV files that encompass both a full extract and on",
    "start": "929470",
    "end": "934809"
  },
  {
    "text": "some of our tables we can actually do incremental extracts this landing zone is for operations use only there's no",
    "start": "934809",
    "end": "942549"
  },
  {
    "text": "data sciences snow machine learning people access this this is just a staging area as far as the first step to",
    "start": "942549",
    "end": "949449"
  },
  {
    "text": "actually get into the cloud once it's there we use glue so in the past it used",
    "start": "949449",
    "end": "955449"
  },
  {
    "text": "to be data pipeline now we have glue and lambda so that is the function that we decided to use glue then helps us",
    "start": "955449",
    "end": "962409"
  },
  {
    "text": "extract all of the data that we have in CSV into a format that's more AWS I",
    "start": "962409",
    "end": "968259"
  },
  {
    "text": "would call friendly okay so number one is we extract it into park' so if you",
    "start": "968259",
    "end": "974019"
  },
  {
    "text": "ever use the CSV right it's essentially role-based so every row like a database is like oh you have all these different",
    "start": "974019",
    "end": "980439"
  },
  {
    "text": "fields that are in the row the park' format essentially turns that into a column based format that's more",
    "start": "980439",
    "end": "986169"
  },
  {
    "text": "consumable for data platform for a data analysis once converted into park' and",
    "start": "986169",
    "end": "993189"
  },
  {
    "text": "then we also use blue in order to extract for certain fields that are not",
    "start": "993189",
    "end": "998439"
  },
  {
    "text": "necessarily that we like actually transformation of the fields we can extract into other CSV or park' as we",
    "start": "998439",
    "end": "1005009"
  },
  {
    "text": "need it Glu is what takes care of all of the catalogs it takes care of all the crawlers and the scheduling around this",
    "start": "1005009",
    "end": "1011699"
  },
  {
    "text": "piece there's a little bit more here around we actually use cloud walks on glue to send alerts so that we can",
    "start": "1011699",
    "end": "1018689"
  },
  {
    "text": "actually see the status of these jobs inside the data Lake we then need to",
    "start": "1018689",
    "end": "1024720"
  },
  {
    "text": "actually use the data so now we actually have the data it's in a usable format so what we do is we can attach the AWS",
    "start": "1024720",
    "end": "1031288"
  },
  {
    "text": "services on top of it we use redshift in Athena Athena was the first that we use",
    "start": "1031289",
    "end": "1036449"
  },
  {
    "text": "just because Athena is just like a sledgehammer you're going to just point Athena at a park case you know a park a",
    "start": "1036449",
    "end": "1043678"
  },
  {
    "text": "formatted bucket and now you can actually just read your data it's not fine-grain there's no importing you just",
    "start": "1043679",
    "end": "1049710"
  },
  {
    "text": "go at it and then we use red shift register just starting to use now but essentially",
    "start": "1049710",
    "end": "1056190"
  },
  {
    "text": "we're using both a combination of red shift and red stuff spectrum in order to determine you know we can actually push",
    "start": "1056190",
    "end": "1061860"
  },
  {
    "text": "data in then we can also access the files directly the reason that reason red stuff is a full on so we can have",
    "start": "1061860",
    "end": "1067230"
  },
  {
    "text": "the full data warehouse solution on the right side or it's actually what we do",
    "start": "1067230",
    "end": "1073409"
  },
  {
    "text": "is we take our machine learning engineers so our machine learning engineers inside of sage maker notebook",
    "start": "1073409",
    "end": "1080279"
  },
  {
    "text": "will then access either red shift or athina we also inside of our data Blake",
    "start": "1080279",
    "end": "1085889"
  },
  {
    "text": "use this data to do all of our analytics the analytics component is still maturing for us are you number one use",
    "start": "1085889",
    "end": "1092580"
  },
  {
    "text": "case to start is all in the machine learning side but in the future it's going to be all the ant analytics is",
    "start": "1092580",
    "end": "1097889"
  },
  {
    "text": "going to be in there for tableau and any other data analysis that's required on that data suite okay",
    "start": "1097889",
    "end": "1108869"
  },
  {
    "start": "1106000",
    "end": "1466000"
  },
  {
    "text": "last slide and then we're gonna go into questions this is all about our stage maker pipeline so the sage maker",
    "start": "1108869",
    "end": "1115230"
  },
  {
    "text": "pipeline if you remember on the last slide the very right side was this sage maker thing so who's used sage maker",
    "start": "1115230",
    "end": "1121950"
  },
  {
    "text": "who's logged into a state a sage maker notebook nobody okay you gotta be honest that'll",
    "start": "1121950",
    "end": "1127739"
  },
  {
    "text": "house if you've already done it you're gonna be bored when I talk about it okay sage maker essentially is a full service",
    "start": "1127739",
    "end": "1134220"
  },
  {
    "text": "Jupiter notebook hosting platform in order for you to be able to build and train and host your models even though",
    "start": "1134220",
    "end": "1142470"
  },
  {
    "text": "we had done machine learning in the past we actually found this incredibly useful because it takes care of all of the",
    "start": "1142470",
    "end": "1147779"
  },
  {
    "text": "lifecycle around the actual machine learning that we need the machine learning engineers log in from their",
    "start": "1147779",
    "end": "1154019"
  },
  {
    "text": "desktop into this stage maker notebook inside of the sage maker notebook we have secured it inside of a V PC this V",
    "start": "1154019",
    "end": "1161460"
  },
  {
    "text": "PC has no direct internet access so it is not on a public subnet a sage maker",
    "start": "1161460",
    "end": "1167879"
  },
  {
    "text": "has the ability to actually spin up notebook instances that are inside a V",
    "start": "1167879",
    "end": "1173070"
  },
  {
    "text": "PC and do not have internet access so that is what we've done however data",
    "start": "1173070",
    "end": "1178950"
  },
  {
    "text": "scientists need the Internet you can't just cut them off like it'd be nice but they don't like that the",
    "start": "1178950",
    "end": "1184479"
  },
  {
    "text": "problem is they need to use things like Conda they need to pull in different Python packages although sage maker",
    "start": "1184479",
    "end": "1191259"
  },
  {
    "text": "comes with a set and pre-built functions of you know a whole bunch of like tensorflow and all the different things",
    "start": "1191259",
    "end": "1197190"
  },
  {
    "text": "least my data scientists are like no no no I want this this this this this they're gonna bring in all their own",
    "start": "1197190",
    "end": "1203320"
  },
  {
    "text": "packages and they need the ability to get to the internet so what we do is we provide a proxy the proxy then provides",
    "start": "1203320",
    "end": "1210309"
  },
  {
    "text": "the whitelist to say what services that sage maker is allowed to access that way",
    "start": "1210309",
    "end": "1215499"
  },
  {
    "text": "they can't just randomly hit some URL that they shouldn't be accessing so it's a little bit more secure than just",
    "start": "1215499",
    "end": "1221769"
  },
  {
    "text": "simply the default sage maker which puts them directly on the internet in addition we provide encryption on the",
    "start": "1221769",
    "end": "1227409"
  },
  {
    "text": "sage maker notebook you can have encryption on site on your notebook and you can specify lifecycle policies to",
    "start": "1227409",
    "end": "1233200"
  },
  {
    "text": "specify your proxy on the red shift and Athena side I talked about a little bit",
    "start": "1233200",
    "end": "1239169"
  },
  {
    "text": "before but essentially resident Athena are then being accessed into from sage",
    "start": "1239169",
    "end": "1244899"
  },
  {
    "text": "makers so the data Sciences will be able to go through there actually through Jupiter then connect directly into thena",
    "start": "1244899",
    "end": "1251859"
  },
  {
    "text": "and run queries one of the learnings we had in this process like I said before data scientists want all of the data so",
    "start": "1251859",
    "end": "1259869"
  },
  {
    "text": "I provided initially all the data in a CSV file and I said here's all the data you kept saying you want all the data so",
    "start": "1259869",
    "end": "1266889"
  },
  {
    "text": "I gave it to them they tried to load it into Sage maker and the Jupiter notebook exceptionally exploded right it's too",
    "start": "1266889",
    "end": "1273789"
  },
  {
    "text": "much data too much inside of the single Python instance it ran on a memory the",
    "start": "1273789",
    "end": "1279099"
  },
  {
    "text": "data Sciences solution to the problem was oh I'll just create a bigger stage maker instance right let's AWS right so",
    "start": "1279099",
    "end": "1286269"
  },
  {
    "text": "okay we'll have a pea for 24 extra large like you know half a terabyte of memory",
    "start": "1286269",
    "end": "1292539"
  },
  {
    "text": "instance and then it worked right so that was a good workaround initially but",
    "start": "1292539",
    "end": "1298389"
  },
  {
    "text": "very expensive so what we did is we are moving towards Athena and redshift so",
    "start": "1298389",
    "end": "1303909"
  },
  {
    "text": "that they can actually get filtered data a lot easier into what they're doing because a lot of times you know we have",
    "start": "1303909",
    "end": "1310179"
  },
  {
    "text": "certain tables that are millions and millions of rows they don't necessarily need all of that data at all times to",
    "start": "1310179",
    "end": "1316559"
  },
  {
    "text": "actually build their models and just play with the data because there's the big learning around machine learning is",
    "start": "1316559",
    "end": "1322619"
  },
  {
    "text": "first you don't even know what you're doing right you have to play with it you have to learn so a lot of times you're just",
    "start": "1322619",
    "end": "1327690"
  },
  {
    "text": "wanting to do quick things grab certain subsets of the data figure out what you want and then you can actually do the training later all of",
    "start": "1327690",
    "end": "1334559"
  },
  {
    "text": "the training is provided inside of sage maker sage anchor kit and then load up from ECR can load the docker containers",
    "start": "1334559",
    "end": "1341399"
  },
  {
    "text": "required in order to actually do the training and then any there's a temporary bucket in s3 either for static",
    "start": "1341399",
    "end": "1348029"
  },
  {
    "text": "snapshots of data that we can provide the machine learning engineers so they can actually build it as well as for",
    "start": "1348029",
    "end": "1353999"
  },
  {
    "text": "temporary storage each of our buckets are all encrypted all using the policies",
    "start": "1353999",
    "end": "1359039"
  },
  {
    "text": "I said before okay now we built our model everything is working that's fantastic but now we need to host it so",
    "start": "1359039",
    "end": "1366119"
  },
  {
    "text": "the way our hosting platform works is we take the model and then we take the docker container and then we throw it",
    "start": "1366119",
    "end": "1371820"
  },
  {
    "text": "into a stage maker instance in production that production instance is inside of a private V PC this V PC has",
    "start": "1371820",
    "end": "1378960"
  },
  {
    "text": "no internet access in or out it is completely locked away it is like a like",
    "start": "1378960",
    "end": "1384480"
  },
  {
    "text": "a little shell in like its own little cocoon for us this was a great way to",
    "start": "1384480",
    "end": "1389519"
  },
  {
    "text": "just get on the platform as an Operations person I don't have to worry about any bad guys getting in or",
    "start": "1389519",
    "end": "1394889"
  },
  {
    "text": "anything weird about this system getting out it is just a complete conclude within us inside of sage maker if we",
    "start": "1394889",
    "end": "1401429"
  },
  {
    "text": "ever need to update we can just update the docker container and deploy it okay that's great but then how do you",
    "start": "1401429",
    "end": "1406980"
  },
  {
    "text": "actually use it the sage maker API is allow us to execute and infer on our",
    "start": "1406980",
    "end": "1413039"
  },
  {
    "text": "model directly from our data center the way that that works is we create an iam user you can't use a role because it's",
    "start": "1413039",
    "end": "1418859"
  },
  {
    "text": "not inside at the AWS platform we have a user in a user access key within our data center that allows access directly",
    "start": "1418859",
    "end": "1425460"
  },
  {
    "text": "into the stage maker execution API so it can run inference on the model it does",
    "start": "1425460",
    "end": "1430619"
  },
  {
    "text": "not need direct access from the joopa notebook in order or the sorry not jupiter inside of the actual inference",
    "start": "1430619",
    "end": "1436769"
  },
  {
    "text": "model the Python code that's running in here to actually reach out it can go directly from Sage Maker",
    "start": "1436769",
    "end": "1443100"
  },
  {
    "text": "and that's using the AWS SDK that way we don't need to actually we a lot of",
    "start": "1443100",
    "end": "1449830"
  },
  {
    "text": "patterns you see or using API gateway in front of sage maker which is perfectly fine we actually just don't need it for now",
    "start": "1449830",
    "end": "1455680"
  },
  {
    "text": "it just runs perfectly fine just using the execution API that's it",
    "start": "1455680",
    "end": "1461350"
  },
  {
    "text": "okay done all right I need questions",
    "start": "1461350",
    "end": "1466680"
  },
  {
    "start": "1466000",
    "end": "1905000"
  },
  {
    "text": "cool Thank You Kyle let's give them a hand so we'll turn a slide oh if you",
    "start": "1466680",
    "end": "1475300"
  },
  {
    "text": "have any questions go to slide Oh calm use the hash tag start up as a password type it in there and then you can upload",
    "start": "1475300",
    "end": "1481210"
  },
  {
    "text": "other questions you want answered to the top one it says do you evaluate the storage gateway to move data from",
    "start": "1481210",
    "end": "1487480"
  },
  {
    "text": "on-premise AWS yes so if you don't know storage gateway there's like different formats of its like cache gateway volume",
    "start": "1487480",
    "end": "1494470"
  },
  {
    "text": "gateways all these different things it's great for us in order for those to work you need to have a nice cozy volume and",
    "start": "1494470",
    "end": "1500260"
  },
  {
    "text": "a target to put it in there so we evaluate it the problem is we're not we're it's so much data that it we're",
    "start": "1500260",
    "end": "1506020"
  },
  {
    "text": "not quite ready for that so eventually this is a part of just the overall migration plan for us but not yet so",
    "start": "1506020",
    "end": "1513040"
  },
  {
    "text": "mostly on it we're looking for that for our corporate services not for a production looks like a new one popped",
    "start": "1513040",
    "end": "1520480"
  },
  {
    "text": "up it says how are the versions of model deployments managed oh okay this is good",
    "start": "1520480",
    "end": "1528070"
  },
  {
    "text": "yeah so the way that we manage the models is inside of that docker container directly so initially I didn't",
    "start": "1528070",
    "end": "1534190"
  },
  {
    "text": "explain removal but inside of the sage maker is both the actual docker container and the model and so we can",
    "start": "1534190",
    "end": "1540790"
  },
  {
    "text": "actually cut and paste a new model without actually touching the docker container when sage maker starts it",
    "start": "1540790",
    "end": "1546730"
  },
  {
    "text": "fires up docker it pulls in the model so for us right now our models do not change very frequently which is awesome",
    "start": "1546730",
    "end": "1553240"
  },
  {
    "text": "so we don't really have to do that that's a manual process which sucks eventually the right way to do this is",
    "start": "1553240",
    "end": "1558700"
  },
  {
    "text": "essentially we'll use the entire stage maker pipeline in flow all the way through and then that way the versioning",
    "start": "1558700",
    "end": "1564340"
  },
  {
    "text": "will come just as a part of that I guess let's",
    "start": "1564340",
    "end": "1569679"
  },
  {
    "text": "keep going on the sage maker stuff since we're on the topic where do you output the data produced from sage maker oh",
    "start": "1569679",
    "end": "1576690"
  },
  {
    "text": "yeah so okay so on that slide so the when bill comm accesses the sage maker",
    "start": "1576690",
    "end": "1584200"
  },
  {
    "text": "execution API it is both running the inference and receiving the output from Sage Maker it",
    "start": "1584200",
    "end": "1589539"
  },
  {
    "text": "gets the response from the machine from the model it then stores all the output so we store it all on print it keeps the",
    "start": "1589539",
    "end": "1595659"
  },
  {
    "text": "output it keeps the input that way we can improve the training of the model in the future and then it goes through that",
    "start": "1595659",
    "end": "1601509"
  },
  {
    "text": "cycle because we already have the data in our database it can then flow back into the data lake then this then the",
    "start": "1601509",
    "end": "1606970"
  },
  {
    "text": "data scientist can then use that data okay let's go to the top do you a great",
    "start": "1606970",
    "end": "1615999"
  },
  {
    "text": "and share the learning from all your customers or models on a customer basis",
    "start": "1615999",
    "end": "1623399"
  },
  {
    "text": "okay so in general with machine learning it's across everything we do it",
    "start": "1623399",
    "end": "1628989"
  },
  {
    "text": "generally doesn't work as well specific for customer so for us it's across everything like it is all customers all",
    "start": "1628989",
    "end": "1635259"
  },
  {
    "text": "data in order to actually provide the best model and we only use a single model for all customers I can imagine in",
    "start": "1635259",
    "end": "1642879"
  },
  {
    "text": "the future that could change but that's how it is them go to the top how do you",
    "start": "1642879",
    "end": "1648129"
  },
  {
    "text": "plan to manage security around your p2 at you know redshift I'm not sure what",
    "start": "1648129",
    "end": "1656559"
  },
  {
    "text": "specific around so Everett security the management of the security is around all of it right so a lot of it is about encryption it's about lease privileges",
    "start": "1656559",
    "end": "1663070"
  },
  {
    "text": "and access it's a lot of rounds you know making sure that the right data is there",
    "start": "1663070",
    "end": "1668139"
  },
  {
    "text": "but it's all of I mean that is such a huge I mean there's this like talks here",
    "start": "1668139",
    "end": "1674710"
  },
  {
    "text": "all day about this I mean I I'm happy to talk to you in person about all of it but it's huge it all depends on you know",
    "start": "1674710",
    "end": "1681340"
  },
  {
    "text": "because we're looking at data loss prevention we're looking at actual encryption we're looking at you know who",
    "start": "1681340",
    "end": "1687940"
  },
  {
    "text": "has access how do you keep the bad guys out how do you keep the good guys from getting out it's all different angles so",
    "start": "1687940",
    "end": "1693940"
  },
  {
    "text": "so ever ask that question you could also follow up talk if you want to I'm sure we'll give you somebody's time so how do",
    "start": "1693940",
    "end": "1701170"
  },
  {
    "text": "you transfer data from your data center to AWS yeah so for now that's ultimately",
    "start": "1701170",
    "end": "1706540"
  },
  {
    "text": "using the AWS CLI so we do just the rs3 it has a sink command so that's how it",
    "start": "1706540",
    "end": "1713350"
  },
  {
    "text": "is it's just a WSS we sink sink all of this stuff throw it in the cloud that works efficient for now they announced",
    "start": "1713350",
    "end": "1719890"
  },
  {
    "text": "today that they actually have the service that will actually help me do that which is fantastic there's also",
    "start": "1719890",
    "end": "1725110"
  },
  {
    "text": "data migration service that can help you do that we don't use that but that's also a lot of people do that you know",
    "start": "1725110",
    "end": "1732190"
  },
  {
    "text": "the talk you're talking about proxy access to the Internet is that via Internet gateway by MB AWS or is it",
    "start": "1732190",
    "end": "1739300"
  },
  {
    "text": "something you build custom or yeah so there are two ways to get to the Internet inside of the inside of a V PC",
    "start": "1739300",
    "end": "1745720"
  },
  {
    "text": "you can have an Internet gateway or you can have an app gateway we do not use an Internet gateway Internet gateways where",
    "start": "1745720",
    "end": "1751360"
  },
  {
    "text": "you just put a public subnet so you put your EOB s in this case that is not what we're doing we use the nack gateway and",
    "start": "1751360",
    "end": "1757300"
  },
  {
    "text": "a put that with the NAT gateway sits on a public subnet and then our actual app instances sit on a private subnet that",
    "start": "1757300",
    "end": "1764410"
  },
  {
    "text": "app instance only connects to the NAT gateway and then that's how it gets out",
    "start": "1764410",
    "end": "1770020"
  },
  {
    "text": "the Internet the NAT gateway provides the actual access these probably go",
    "start": "1770020",
    "end": "1776890"
  },
  {
    "text": "together so let's start with how often you retrain your model and is that an automated process and also just how",
    "start": "1776890",
    "end": "1783160"
  },
  {
    "text": "accurate is your is your data capture model okay so the first one so how often",
    "start": "1783160",
    "end": "1789130"
  },
  {
    "text": "so we retrain manually today I think now because we're still on the journey for a",
    "start": "1789130",
    "end": "1794200"
  },
  {
    "text": "sage maker however what I can see happening is that trading having a lot more frequently because as the",
    "start": "1794200",
    "end": "1800380"
  },
  {
    "text": "iterations get better it's become a lot because it's in Sage maker it's really easy to automate so for now we just",
    "start": "1800380",
    "end": "1806290"
  },
  {
    "text": "don't have to do it as much I would say that I mean I guess like ninety ninety-five percent of the time is just",
    "start": "1806290",
    "end": "1812740"
  },
  {
    "text": "playing with the data like it's not even like training it's like trying to figure out what are we going to do with the",
    "start": "1812740",
    "end": "1818620"
  },
  {
    "text": "data what data is useful what is the right model to use and then actually execute the training",
    "start": "1818620",
    "end": "1824490"
  },
  {
    "text": "and that's less of the time what's accuracy this is the excellent",
    "start": "1824490",
    "end": "1832200"
  },
  {
    "text": "question I don't know yeah so it's definitely helped the the accuracy of",
    "start": "1832200",
    "end": "1837330"
  },
  {
    "text": "the models especially on the document processing side has gone up tremendously",
    "start": "1837330",
    "end": "1842429"
  },
  {
    "text": "if you talk to our customers like I said we're on our third generation of machine learning the initial version was rough it was",
    "start": "1842429",
    "end": "1849809"
  },
  {
    "text": "like you know 50 percent or so accuracy where we're aiming though to get 90% accuracy and so we're somewhere on that",
    "start": "1849809",
    "end": "1856470"
  },
  {
    "text": "than that spectrum but we're not there yet but it's really hard like I said",
    "start": "1856470",
    "end": "1861690"
  },
  {
    "text": "because the documents are all different but it's it is actually becoming a lot more helpful come on last mush and it",
    "start": "1861690",
    "end": "1869490"
  },
  {
    "text": "looks like it says do you capture line-item details as well or just invoice headers we capture all line",
    "start": "1869490",
    "end": "1875280"
  },
  {
    "text": "items we capture line it we capture we essentially use you know we do the OCR",
    "start": "1875280",
    "end": "1881040"
  },
  {
    "text": "to actually cap so there's digital documents like a PDF and then you have like a scan document which is just a",
    "start": "1881040",
    "end": "1886350"
  },
  {
    "text": "picture so on both of those kinds of documents will extract all of the data and then will use it using the",
    "start": "1886350",
    "end": "1892830"
  },
  {
    "text": "coordinates on the document and then including all data so it's all the data that's in there it's all used as a part",
    "start": "1892830",
    "end": "1898590"
  },
  {
    "text": "of the model boom well thank you Kyle let's give them a hand that was great [Applause]",
    "start": "1898590",
    "end": "1907380"
  }
]