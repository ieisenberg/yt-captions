[
  {
    "start": "0",
    "end": "80000"
  },
  {
    "text": "(bright airy rock music)",
    "start": "17",
    "end": "1308"
  },
  {
    "text": "(computer keyboard clicking)",
    "start": "1308",
    "end": "5058"
  },
  {
    "text": "- Hi, I'm Aditya, a Cloud Support Engineer",
    "start": "12140",
    "end": "14420"
  },
  {
    "text": "here at the AWS headquarters in Seattle.",
    "start": "14420",
    "end": "17710"
  },
  {
    "text": "Sometimes customers ask me,",
    "start": "17710",
    "end": "19286"
  },
  {
    "text": "\"What is Amazon EMR",
    "start": "19287",
    "end": "20747"
  },
  {
    "text": "\"and how can I use it\nfor processing data?\"",
    "start": "20747",
    "end": "24220"
  },
  {
    "text": "I tell them Amazon EMR is\na managed cluster platform",
    "start": "24220",
    "end": "27269"
  },
  {
    "text": "that simplifies running\nbig data frameworks,",
    "start": "27270",
    "end": "30020"
  },
  {
    "text": "such as Apache Hadoop and Apache Spark,",
    "start": "30020",
    "end": "32669"
  },
  {
    "text": "on AWS to process and\nanalyze vast amounts of data.",
    "start": "32670",
    "end": "36100"
  },
  {
    "text": "to process and analyze\nvast amounts of data.",
    "start": "36100",
    "end": "39710"
  },
  {
    "text": "such as Apache Hive and Apache Pig,",
    "start": "41063",
    "end": "43730"
  },
  {
    "text": "you can process data\nfor analytics purposes",
    "start": "43730",
    "end": "46400"
  },
  {
    "text": "and business intelligence workloads.",
    "start": "46400",
    "end": "48750"
  },
  {
    "text": "Additionally, you can use Amazon EMR",
    "start": "48750",
    "end": "51140"
  },
  {
    "text": "to transform and move\nlarge amounts of data",
    "start": "51140",
    "end": "53710"
  },
  {
    "text": "into and out of other AWS\ndata stores and databases,",
    "start": "53710",
    "end": "58040"
  },
  {
    "text": "such as Amazon Simple\nStorage Services, Amazon S3,",
    "start": "58040",
    "end": "62290"
  },
  {
    "text": "and Amazon DynamoDB.",
    "start": "62290",
    "end": "64930"
  },
  {
    "text": "Today, we'll discuss the\narchitecture of Amazon EMR",
    "start": "64930",
    "end": "67470"
  },
  {
    "text": "and the step-by-step process\nto launch an EMR cluster",
    "start": "67470",
    "end": "70850"
  },
  {
    "text": "using the AWS Management Console",
    "start": "70850",
    "end": "73330"
  },
  {
    "text": "and then run a sample Hive application.",
    "start": "73330",
    "end": "76080"
  },
  {
    "text": "So let's start with talking about",
    "start": "76080",
    "end": "77707"
  },
  {
    "text": "the Amazon EMR architecture.",
    "start": "77707",
    "end": "80290"
  },
  {
    "start": "80000",
    "end": "97000"
  },
  {
    "text": "There are several types of storage.",
    "start": "80290",
    "end": "82090"
  },
  {
    "text": "Hadoop Distributed File System",
    "start": "82090",
    "end": "83859"
  },
  {
    "text": "is a distributed, scalable\nfile system for Hadoop.",
    "start": "83860",
    "end": "87460"
  },
  {
    "text": "EMR File System uses HDFS or Amazon S3",
    "start": "87460",
    "end": "91120"
  },
  {
    "text": "as the file system in your cluster.",
    "start": "91120",
    "end": "93420"
  },
  {
    "text": "Local File System refers to\na locally connected disk.",
    "start": "93420",
    "end": "97360"
  },
  {
    "start": "97000",
    "end": "124000"
  },
  {
    "text": "Cluster Resource Management\nmanages cluster resources",
    "start": "97360",
    "end": "100440"
  },
  {
    "text": "and scheduling the jobs\nfor processing data.",
    "start": "100440",
    "end": "103110"
  },
  {
    "text": "Uses YARN as the default.",
    "start": "103110",
    "end": "105630"
  },
  {
    "text": "Data Processing Frameworks.",
    "start": "105630",
    "end": "107409"
  },
  {
    "text": "Different frameworks are available",
    "start": "107410",
    "end": "109090"
  },
  {
    "text": "for different kinds of processing needs.",
    "start": "109090",
    "end": "111350"
  },
  {
    "text": "For example, Hadoop\nMapReduce, Tez, and Spark.",
    "start": "111350",
    "end": "116080"
  },
  {
    "text": "Applications and Programs\nsupports many applications,",
    "start": "116080",
    "end": "119300"
  },
  {
    "text": "such as Hive, Pig, and the\nSpark Streaming library.",
    "start": "119300",
    "end": "122763"
  },
  {
    "start": "124000",
    "end": "150000"
  },
  {
    "text": "Now let me walk you through the process",
    "start": "124010",
    "end": "126310"
  },
  {
    "text": "of creating a sample EMR cluster",
    "start": "126310",
    "end": "128700"
  },
  {
    "text": "by using the AWS Management Console.",
    "start": "128700",
    "end": "131489"
  },
  {
    "text": "Then we'll run a Hive script as a step",
    "start": "131490",
    "end": "134380"
  },
  {
    "text": "to process sample data that\nis stored in Amazon S3.",
    "start": "134380",
    "end": "138010"
  },
  {
    "text": "But before you do that,",
    "start": "138010",
    "end": "139230"
  },
  {
    "text": "there are some prerequisites\nfor your sample cluster:",
    "start": "139230",
    "end": "142239"
  },
  {
    "text": "sign up for AWS,",
    "start": "142240",
    "end": "143740"
  },
  {
    "text": "create an S3 bucket to store output data,",
    "start": "143740",
    "end": "146600"
  },
  {
    "text": "and create an EC2 key pair.",
    "start": "146600",
    "end": "148833"
  },
  {
    "text": "Once you have completed the prerequisites,",
    "start": "149970",
    "end": "152200"
  },
  {
    "start": "150000",
    "end": "541000"
  },
  {
    "text": "sign in to the console.",
    "start": "152200",
    "end": "153770"
  },
  {
    "text": "As you see, I'm already signed\nin to AWS Management Console.",
    "start": "153770",
    "end": "157490"
  },
  {
    "text": "Now let's select the region.",
    "start": "157490",
    "end": "159510"
  },
  {
    "text": "For demonstration purposes,",
    "start": "159510",
    "end": "160980"
  },
  {
    "text": "I'll just leave Ireland as the default.",
    "start": "160980",
    "end": "163150"
  },
  {
    "text": "Now let's navigate to EMR service.",
    "start": "163150",
    "end": "165463"
  },
  {
    "text": "Click on Create cluster.",
    "start": "168420",
    "end": "169923"
  },
  {
    "text": "We'll give it a meaningful name.",
    "start": "171010",
    "end": "173069"
  },
  {
    "text": "And we leave the Logging as checkmarked",
    "start": "173070",
    "end": "175390"
  },
  {
    "text": "and the Launch mode as Cluster.",
    "start": "175390",
    "end": "177660"
  },
  {
    "text": "And for the release version,",
    "start": "177660",
    "end": "178980"
  },
  {
    "text": "we're gonna use the default\none, which is the latest one.",
    "start": "178980",
    "end": "181810"
  },
  {
    "text": "And then for Applications,\nlet's select the Core Hadoop,",
    "start": "181810",
    "end": "184700"
  },
  {
    "text": "which includes Hive as well.",
    "start": "184700",
    "end": "187239"
  },
  {
    "text": "Now for Hardware configuration,\nwe can leave it as default.",
    "start": "187240",
    "end": "190840"
  },
  {
    "text": "And for EC2 key pair,",
    "start": "190840",
    "end": "192950"
  },
  {
    "text": "we can select the EC2 key\npair which we created earlier.",
    "start": "192950",
    "end": "196563"
  },
  {
    "text": "And Permissions, leave it as Default.",
    "start": "197770",
    "end": "200060"
  },
  {
    "text": "And you will see there\nwill be default EMR role",
    "start": "200060",
    "end": "203000"
  },
  {
    "text": "and the EC2 instance profile\nalready listed there.",
    "start": "203000",
    "end": "206730"
  },
  {
    "text": "Now let's click on Create cluster.",
    "start": "206730",
    "end": "208693"
  },
  {
    "text": "It will take some time for\nthis cluster to spin up.",
    "start": "214590",
    "end": "217900"
  },
  {
    "text": "While the EMR cluster is spinning up,",
    "start": "217900",
    "end": "220049"
  },
  {
    "text": "let's prepare your sample data and script.",
    "start": "220050",
    "end": "222730"
  },
  {
    "text": "The sample data is a series",
    "start": "222730",
    "end": "224379"
  },
  {
    "text": "of Amazon CloudFront web\ndistribution log files.",
    "start": "224380",
    "end": "228080"
  },
  {
    "text": "The data is stored in Amazon\nS3 at the URL that is shown,",
    "start": "228080",
    "end": "232440"
  },
  {
    "text": "and always be sure to change the region",
    "start": "232440",
    "end": "235290"
  },
  {
    "text": "to the same as where your\nEMR cluster is spinning up.",
    "start": "235290",
    "end": "238799"
  },
  {
    "text": "The sample script",
    "start": "238800",
    "end": "239840"
  },
  {
    "text": "calculates the number of\nrequests per operating system",
    "start": "239840",
    "end": "243180"
  },
  {
    "text": "over a specified timeframe.",
    "start": "243180",
    "end": "245290"
  },
  {
    "text": "The script uses HiveQL,",
    "start": "245290",
    "end": "247019"
  },
  {
    "text": "which is a SQL-like scripting language",
    "start": "247020",
    "end": "249180"
  },
  {
    "text": "for data warehousing and analysis.",
    "start": "249180",
    "end": "251950"
  },
  {
    "text": "Again, the script is stored in Amazon S3",
    "start": "251950",
    "end": "254319"
  },
  {
    "text": "at the URL that is shown,",
    "start": "254320",
    "end": "256180"
  },
  {
    "text": "and, as always, be sure\nto change the region",
    "start": "256180",
    "end": "259030"
  },
  {
    "text": "to the same one as where your\nEMR cluster is spinning up.",
    "start": "259030",
    "end": "262550"
  },
  {
    "text": "Process your sample data by\nrunning a Hive script as step",
    "start": "262550",
    "end": "266509"
  },
  {
    "text": "via the AWS Management Console.",
    "start": "266510",
    "end": "269300"
  },
  {
    "text": "In Amazon EMR, a step is a unit of work",
    "start": "269300",
    "end": "272210"
  },
  {
    "text": "that contains one or more Hadoop jobs.",
    "start": "272210",
    "end": "275520"
  },
  {
    "text": "You can submit steps when\nyou create the cluster",
    "start": "275520",
    "end": "278580"
  },
  {
    "text": "or when the cluster is running",
    "start": "278580",
    "end": "280460"
  },
  {
    "text": "if it is a long-running cluster.",
    "start": "280460",
    "end": "282370"
  },
  {
    "text": "The Hive script and sample\ndata used by the script",
    "start": "282370",
    "end": "285699"
  },
  {
    "text": "have been uploaded to Amazon S3 for you.",
    "start": "285700",
    "end": "288463"
  },
  {
    "text": "To submit your Hive script\nas a step, select Steps.",
    "start": "289460",
    "end": "293449"
  },
  {
    "text": "Click on Add step.",
    "start": "293450",
    "end": "295300"
  },
  {
    "text": "Now here we'll select the\nStep type as Hive program.",
    "start": "295300",
    "end": "298883"
  },
  {
    "text": "Now go ahead and add Script S3\nlocation, Input S3 location,",
    "start": "300730",
    "end": "305300"
  },
  {
    "text": "and Output S3 location.",
    "start": "305300",
    "end": "307379"
  },
  {
    "text": "In this case, we're gonna use the bucket",
    "start": "307380",
    "end": "309860"
  },
  {
    "text": "which we created as a prerequisite",
    "start": "309860",
    "end": "311860"
  },
  {
    "text": "for our Output S3 location.",
    "start": "311860",
    "end": "314250"
  },
  {
    "text": "So here we can see, for\nthe Output S3 location,",
    "start": "314250",
    "end": "317320"
  },
  {
    "text": "I've used the bucket",
    "start": "317320",
    "end": "318560"
  },
  {
    "text": "which was created as a\npart of the prerequisite,",
    "start": "318560",
    "end": "321160"
  },
  {
    "text": "and I have given the\nfolder name as output.",
    "start": "321160",
    "end": "324320"
  },
  {
    "text": "Now let's click on Add.",
    "start": "324320",
    "end": "325793"
  },
  {
    "text": "So now the step has been added,",
    "start": "328200",
    "end": "330040"
  },
  {
    "text": "and if you refresh this,",
    "start": "330040",
    "end": "331670"
  },
  {
    "text": "you can see now it's\nin the Running status.",
    "start": "331670",
    "end": "334120"
  },
  {
    "text": "Now we'll wait until the steps complete.",
    "start": "334120",
    "end": "336949"
  },
  {
    "text": "As you can see, this Hive\nprogram is now completed.",
    "start": "336950",
    "end": "340320"
  },
  {
    "text": "Now let's try to submit our Hive script",
    "start": "340320",
    "end": "342930"
  },
  {
    "text": "directly on the master node wire SSH.",
    "start": "342930",
    "end": "345979"
  },
  {
    "text": "But before we do that,",
    "start": "345980",
    "end": "347770"
  },
  {
    "text": "we need to open the ports for Port 22.",
    "start": "347770",
    "end": "351169"
  },
  {
    "text": "To do the same,",
    "start": "351170",
    "end": "352230"
  },
  {
    "text": "let's click on the Security\ngroups for Master node.",
    "start": "352230",
    "end": "355653"
  },
  {
    "text": "This will take you to the\nEC2 Management Console.",
    "start": "359220",
    "end": "361943"
  },
  {
    "text": "Now let's select the\nElasticMapReduce-master security group.",
    "start": "362900",
    "end": "367780"
  },
  {
    "text": "Click on Inbound.",
    "start": "367780",
    "end": "368993"
  },
  {
    "text": "Next, click on Edit.",
    "start": "370360",
    "end": "371663"
  },
  {
    "text": "Now click on Add Rule.",
    "start": "372780",
    "end": "374003"
  },
  {
    "text": "Then we'll select the SSH.",
    "start": "375330",
    "end": "377452"
  },
  {
    "text": "And the Source, I'm\njust selecting as My IP.",
    "start": "379670",
    "end": "382513"
  },
  {
    "text": "Click on Save.",
    "start": "383700",
    "end": "384663"
  },
  {
    "text": "So now that we have added\nPort 22 in the security group,",
    "start": "386190",
    "end": "389910"
  },
  {
    "text": "let's go back to EMR AWS console.",
    "start": "389910",
    "end": "393170"
  },
  {
    "text": "Here you will see the Master public DNS.",
    "start": "393170",
    "end": "396450"
  },
  {
    "text": "Now click on SSH.",
    "start": "396450",
    "end": "397723"
  },
  {
    "text": "Now this will give you\nthe complete command",
    "start": "399010",
    "end": "401560"
  },
  {
    "text": "to SSH into the master node\nof this Amazon EMR cluster.",
    "start": "401560",
    "end": "406060"
  },
  {
    "text": "Now let's copy this.",
    "start": "406060",
    "end": "407800"
  },
  {
    "text": "Once you have this copied,",
    "start": "407800",
    "end": "409280"
  },
  {
    "text": "now let's move over to the terminal.",
    "start": "409280",
    "end": "411889"
  },
  {
    "text": "Now let's go ahead and\npaste the command here,",
    "start": "411890",
    "end": "414793"
  },
  {
    "text": "and then press Enter.",
    "start": "415840",
    "end": "416923"
  },
  {
    "text": "We select yes.",
    "start": "418550",
    "end": "419913"
  },
  {
    "text": "And now you will see",
    "start": "420980",
    "end": "422460"
  },
  {
    "text": "you're logged in to the master\nnode of this EMR cluster.",
    "start": "422460",
    "end": "426289"
  },
  {
    "text": "So now let's go back to the EMR Console.",
    "start": "426290",
    "end": "429023"
  },
  {
    "text": "Let's navigate to Steps and\nthen select this Hive Program.",
    "start": "430140",
    "end": "434700"
  },
  {
    "text": "And here you will see the complete command",
    "start": "434700",
    "end": "437750"
  },
  {
    "text": "to run this script.",
    "start": "437750",
    "end": "438990"
  },
  {
    "text": "Now let's copy this complete command.",
    "start": "438990",
    "end": "441789"
  },
  {
    "text": "And now we'll go back to the terminal",
    "start": "441790",
    "end": "444700"
  },
  {
    "text": "and paste this command.",
    "start": "444700",
    "end": "446260"
  },
  {
    "text": "And before we press Enter,",
    "start": "446260",
    "end": "447880"
  },
  {
    "text": "let's go ahead and change\nthe output folder name.",
    "start": "447880",
    "end": "450800"
  },
  {
    "text": "This is just to make sure",
    "start": "450800",
    "end": "452099"
  },
  {
    "text": "that we can differentiate\nbetween the output",
    "start": "452100",
    "end": "454480"
  },
  {
    "text": "of two different jobs.",
    "start": "454480",
    "end": "456020"
  },
  {
    "text": "Now let's press Enter.",
    "start": "456020",
    "end": "457432"
  },
  {
    "text": "Now you will see the Hive job is starting,",
    "start": "458610",
    "end": "460550"
  },
  {
    "text": "and it will take some\ntime to complete this job.",
    "start": "460550",
    "end": "463263"
  },
  {
    "text": "As you can see, the job is completed now.",
    "start": "465030",
    "end": "467550"
  },
  {
    "text": "To view the results,",
    "start": "467550",
    "end": "468770"
  },
  {
    "text": "let's navigate to the Amazon S3 console.",
    "start": "468770",
    "end": "471623"
  },
  {
    "text": "We'll search for Amazon S3.",
    "start": "474730",
    "end": "476623"
  },
  {
    "text": "And now we'll select the bucket.",
    "start": "481330",
    "end": "483712"
  },
  {
    "text": "Now select the output folder.",
    "start": "487370",
    "end": "489003"
  },
  {
    "text": "And this is the output file.",
    "start": "492370",
    "end": "494650"
  },
  {
    "text": "Let's download this.",
    "start": "494650",
    "end": "496103"
  },
  {
    "text": "And now let's open this file.",
    "start": "500410",
    "end": "502810"
  },
  {
    "text": "And here you can see the\nresults of this Hive job.",
    "start": "502810",
    "end": "505773"
  },
  {
    "text": "Now reset your environment.",
    "start": "506960",
    "end": "509009"
  },
  {
    "text": "After you complete this tutorial,",
    "start": "509010",
    "end": "511210"
  },
  {
    "text": "remove your Amazon S3 bucket",
    "start": "511210",
    "end": "513260"
  },
  {
    "text": "and terminate your Amazon EMR cluster",
    "start": "513260",
    "end": "516050"
  },
  {
    "text": "to avoid incurring additional charges.",
    "start": "516050",
    "end": "518849"
  },
  {
    "text": "Congratulations, you have\ncreated an EMR cluster",
    "start": "518850",
    "end": "522000"
  },
  {
    "text": "and ran a sample Hive job.",
    "start": "522000",
    "end": "524210"
  },
  {
    "text": "For more information on analyzing\nbig data with Amazon EMR,",
    "start": "524210",
    "end": "528740"
  },
  {
    "text": "please access this link.",
    "start": "528740",
    "end": "531010"
  },
  {
    "text": "Thank you for watching,",
    "start": "531010",
    "end": "532260"
  },
  {
    "text": "and happy cloud computing\nfrom all of us here at AWS.",
    "start": "532260",
    "end": "535367"
  }
]