[
  {
    "text": "good afternoon everybody this presentation is developing classification and recommendation engines with Amazon EMR and Apache spark",
    "start": "539",
    "end": "7470"
  },
  {
    "text": "with Zillow group my name is John fritz I'm the senior product manager for Amazon EMR and I'm joined with jasjeet",
    "start": "7470",
    "end": "14190"
  },
  {
    "text": "in who's a senior director data science and engineering at Zillow group actually",
    "start": "14190",
    "end": "19320"
  },
  {
    "text": "by a quick show of hands how many folks here are running spark ml today so a fair amount how many of you guys are on",
    "start": "19320",
    "end": "25289"
  },
  {
    "text": "spark too or a spark one and then how many of you guys are running it on EMR",
    "start": "25289",
    "end": "30619"
  },
  {
    "text": "okay so good show of hands it's real quick just the agenda for the session",
    "start": "30619",
    "end": "37620"
  },
  {
    "text": "we'll talk real quick about just Apache spark and spark ml it might be a little bit of a refresher for some of you of",
    "start": "37620",
    "end": "42690"
  },
  {
    "text": "why we see this being adopted as one of the frameworks of choice to run large scale machine learning jobs and data",
    "start": "42690",
    "end": "48450"
  },
  {
    "text": "science and then a little bit of slides on running spark ml on Amazon EMR some",
    "start": "48450",
    "end": "54120"
  },
  {
    "text": "ways to do that with low cost and some options that you have show a quick notebook just of a demo that goes back",
    "start": "54120",
    "end": "61199"
  },
  {
    "text": "to some of the classification algorithms that were mentioned in previous slides and then hand it over for about half the",
    "start": "61199",
    "end": "67020"
  },
  {
    "text": "presentation for Zillow group to talk about how they've built classification recommendation engines for their",
    "start": "67020",
    "end": "73950"
  },
  {
    "text": "products so you know as many of you know",
    "start": "73950",
    "end": "79560"
  },
  {
    "text": "spark over the last couple years has been a de-facto engine to use for running machine learning jobs and",
    "start": "79560",
    "end": "85170"
  },
  {
    "text": "actually data processing in general you know many of you might have used Hadoop",
    "start": "85170",
    "end": "90270"
  },
  {
    "text": "MapReduce before and despite this being a massively parallel system you know",
    "start": "90270",
    "end": "95490"
  },
  {
    "text": "there were some limitations and so we found people adopting spar customers using spark on EMR for the first reason",
    "start": "95490",
    "end": "101400"
  },
  {
    "text": "from performance just like a do MapReduce it's massively parallel but instead of say running multiple",
    "start": "101400",
    "end": "107430"
  },
  {
    "text": "MapReduce jobs like you might have used for my job which are some machine learning libraries over MapReduce or",
    "start": "107430",
    "end": "113460"
  },
  {
    "text": "something like hive SPARC uses a highly optimized add to more effectively create",
    "start": "113460",
    "end": "119670"
  },
  {
    "text": "a query plan and does some also full stage code generation to make these queries and these machine learning jobs",
    "start": "119670",
    "end": "125430"
  },
  {
    "text": "efficient also SPARC minimizes IO a lot with Hadoop you know between map and reduce phases",
    "start": "125430",
    "end": "131870"
  },
  {
    "text": "you're dumping a lot to local disk you're also shuffling things around between nodes with spark you can cache",
    "start": "131870",
    "end": "137690"
  },
  {
    "text": "your data set as go data frame in memory and distributed around all the nodes in your cluster and then run out things",
    "start": "137690",
    "end": "144170"
  },
  {
    "text": "over and over and over again for iterative processing also sparks petitioning aware it won't shuffle data",
    "start": "144170",
    "end": "149450"
  },
  {
    "text": "in between nodes unless it has to which is different than some of the previous frameworks before so you know we've seen",
    "start": "149450",
    "end": "155390"
  },
  {
    "text": "customers who are adopting SPARC getting a large degree of performance versus some other older open source frameworks",
    "start": "155390",
    "end": "161830"
  },
  {
    "text": "but another main thing besides performance is SPARC is just easier to",
    "start": "161830",
    "end": "167000"
  },
  {
    "text": "use and we found that it really to some degree speaks your language whether you're more proficient and say Scala or",
    "start": "167000",
    "end": "172490"
  },
  {
    "text": "Python or you want to write our there's a really either native interface or a very tightly coupled library that allows",
    "start": "172490",
    "end": "179600"
  },
  {
    "text": "you to write code in the language that you want your UDF's or or whatever to go",
    "start": "179600",
    "end": "184820"
  },
  {
    "text": "easily construct these jobs but beyond that no typically machine learning is one part of maybe your data processing",
    "start": "184820",
    "end": "191030"
  },
  {
    "text": "pipeline but as you can see you need to extract features you need to clean up data and that might mess you know be",
    "start": "191030",
    "end": "197630"
  },
  {
    "text": "other parts in your processing pipeline SPARC really allows you to standardize everything onto one core framework",
    "start": "197630",
    "end": "203750"
  },
  {
    "text": "versus before you might have used MapReduce and storm and other different libraries together here you have your",
    "start": "203750",
    "end": "210290"
  },
  {
    "text": "SPARC course which is familiar framework for all these higher-level libraries and then you can run sequel you can run",
    "start": "210290",
    "end": "216260"
  },
  {
    "text": "streaming jobs you can run ml Lib which is what we'll talk about today or create graph databases so besides having you",
    "start": "216260",
    "end": "225710"
  },
  {
    "text": "know great performance and the ability to you know use a variety different languages to construct jobs obviously",
    "start": "225710",
    "end": "232160"
  },
  {
    "text": "SPARC is being adopted for machine learning because of the SPARC ml libraries they're built on top of the",
    "start": "232160",
    "end": "237770"
  },
  {
    "text": "data frame API so once again this concept that permeates all of the different SPARC architectures you can",
    "start": "237770",
    "end": "244640"
  },
  {
    "text": "build kind of a collection of columns like a relational table and but SPARC ml",
    "start": "244640",
    "end": "250700"
  },
  {
    "text": "is a bunch of libraries that make it easy to extract transform and select features out of these data frames so",
    "start": "250700",
    "end": "255890"
  },
  {
    "text": "easily transforming into a way to create your vectors and then has a variety of distributed algorithms for",
    "start": "255890",
    "end": "261260"
  },
  {
    "text": "classification and regression clustering and collaborated filtering it also has a couple of tools that make it",
    "start": "261260",
    "end": "267240"
  },
  {
    "text": "easy to do model selection and you know because it's scale out you can try many different models at once on a cluster",
    "start": "267240",
    "end": "273840"
  },
  {
    "text": "and then you know in the Omar's case you can shut down the cluster when you're not using it and then finally it has the",
    "start": "273840",
    "end": "279000"
  },
  {
    "text": "ml pipeline's tool which makes it easy to stitch all these things together as a and save it and rerun it later using it",
    "start": "279000",
    "end": "286110"
  },
  {
    "text": "on for testing and training as well there's a couple of different groups of",
    "start": "286110",
    "end": "291360"
  },
  {
    "text": "kind of feature extraction tools that spark ml has and you know this is just a small selection there's if you go into",
    "start": "291360",
    "end": "297660"
  },
  {
    "text": "the spark documentation you'll see a full list of all the different things ranging from you know easily extracting",
    "start": "297660",
    "end": "303030"
  },
  {
    "text": "words normalizing features easily scooping things up and creating vectors",
    "start": "303030",
    "end": "308220"
  },
  {
    "text": "and there's a lot of things out of the box where you can just use these libraries to manipulate the data that",
    "start": "308220",
    "end": "313230"
  },
  {
    "text": "you've loaded you know out of data frames into your machine learning jobs and you really can get this data from",
    "start": "313230",
    "end": "320040"
  },
  {
    "text": "anywhere we see customers on EMR you know leveraging a lot of s3 is your from a source of truth and data Lake but you",
    "start": "320040",
    "end": "326580"
  },
  {
    "text": "might have customer data in many different data stores based on your use cases in your full architecture and you",
    "start": "326580",
    "end": "331860"
  },
  {
    "text": "can pull data out of DynamoDB with the recently open source DMR hive dynamodb connector but using with SPARC you can",
    "start": "331860",
    "end": "338010"
  },
  {
    "text": "pull data out of RDS you can use the SPARC elasticsearch connector stream data in from Amazon Kinesis or Apache",
    "start": "338010",
    "end": "343320"
  },
  {
    "text": "Kafka use the spark redshift connector or use e/m RFS which is EMRs library",
    "start": "343320",
    "end": "350160"
  },
  {
    "text": "basically to interact with data in s3 to pull data out but you can create data frames and enrich data from all of these",
    "start": "350160",
    "end": "355770"
  },
  {
    "text": "and utilize data in these disparate sources to train and develop your machine learning models you know this is",
    "start": "355770",
    "end": "362970"
  },
  {
    "text": "just a example you know you can have a variety different training data you create the feature vector using some of",
    "start": "362970",
    "end": "368250"
  },
  {
    "text": "these tools which was easy and then create the models so there's a several",
    "start": "368250",
    "end": "373650"
  },
  {
    "text": "classification models in spark ml and you know logistic regression is a common",
    "start": "373650",
    "end": "380520"
  },
  {
    "text": "one if your dataset lends itself and is more simple oftentimes just trying out logistic regression and seeing how it",
    "start": "380520",
    "end": "385590"
  },
  {
    "text": "fits is usually a good choice but also decision trees as well which",
    "start": "385590",
    "end": "390840"
  },
  {
    "text": "you know they're easier to implement that can be very efficient you know they can handle both numeric and categorical",
    "start": "390840",
    "end": "397199"
  },
  {
    "text": "attributes and in some cases it deals a little bit better with say your edge case data an example of working down the",
    "start": "397199",
    "end": "406470"
  },
  {
    "text": "decision tree to see whether you know what you're trying to find is in the class or not and oftentimes once once",
    "start": "406470",
    "end": "413220"
  },
  {
    "text": "it's done you know then you'll have you know partitions to be able to then use in further downstream processes so SPARC",
    "start": "413220",
    "end": "419610"
  },
  {
    "text": "ml can take you know these are variety of two of the many algorithms that are just there you can just start using them",
    "start": "419610",
    "end": "424889"
  },
  {
    "text": "testing them and seeing which ones might fit your models the best but then building these pipelines and",
    "start": "424889",
    "end": "430050"
  },
  {
    "text": "saving them also as easy as I brought up before there's the SPARC ml pipeline's API where you can take a flow like this",
    "start": "430050",
    "end": "439169"
  },
  {
    "text": "and save it where you might start out with raw text you're gonna use the tokenizer to chop it up into words then",
    "start": "439169",
    "end": "444600"
  },
  {
    "text": "create your feature vectors and then choose one of many models in this case and this diagram logistic regression and",
    "start": "444600",
    "end": "450090"
  },
  {
    "text": "then fit that model and then you have the fitted model but you can then use the same pipeline for your testing phase",
    "start": "450090",
    "end": "457350"
  },
  {
    "text": "where you get your your test dataset you've more raw text into vectors and",
    "start": "457350",
    "end": "462870"
  },
  {
    "text": "then run that same model you had before that was saved with the pipeline to create your predictions and then see how",
    "start": "462870",
    "end": "467880"
  },
  {
    "text": "the predictions have have worked and whether you might want to change some things upstream or try out different",
    "start": "467880",
    "end": "473460"
  },
  {
    "text": "models then creating a pipeline really is that easy in this case you can see this is a simple pipeline with three",
    "start": "473460",
    "end": "480000"
  },
  {
    "text": "components and then you fit it and then get your predictions but you can make far more robust pipelines but the point",
    "start": "480000",
    "end": "486599"
  },
  {
    "text": "here is that you know with this API you can save you know fully fitted models and have a lot of interplay between",
    "start": "486599",
    "end": "492500"
  },
  {
    "text": "creating them there's a couple tools as well to train many different models take",
    "start": "492500",
    "end": "499830"
  },
  {
    "text": "chunks out of your tests and your training data set and kind of have SPARC figure out what's the most accurate one",
    "start": "499830",
    "end": "506400"
  },
  {
    "text": "to use this cross validator and the train validation split you know they basically iterate through many different",
    "start": "506400",
    "end": "512490"
  },
  {
    "text": "models and they'll get the fitted models and evaluate the performance so there's",
    "start": "512490",
    "end": "517620"
  },
  {
    "text": "other ways to do this as well but these are handy tools where if you don't have your own framework to go do so you can",
    "start": "517620",
    "end": "522719"
  },
  {
    "text": "lever things that are needed with inspark ml and get a lot of leverage out of them so",
    "start": "522719",
    "end": "528240"
  },
  {
    "text": "that's a little bit of background on spark ml and just eat we'll go into a little bit more on some of the algorithms they use at Zillow to build",
    "start": "528240",
    "end": "534550"
  },
  {
    "text": "out these classification recommendation engines but I want to switch gears a little bit to talk about how we see",
    "start": "534550",
    "end": "539769"
  },
  {
    "text": "customers running these applications in the cloud and we see many customers adopting Amazon EMR to do it for a",
    "start": "539769",
    "end": "546309"
  },
  {
    "text": "couple main reasons surrounding on the slide one is ease of use so if you're",
    "start": "546309",
    "end": "552279"
  },
  {
    "text": "you know spending all your time creating these models investigating your data you don't want to spend a lot of time configuring spark you want to spend a",
    "start": "552279",
    "end": "558550"
  },
  {
    "text": "lot of time managing clusters and you also you know don't want to deal with you know dependency mismatches these",
    "start": "558550",
    "end": "564759"
  },
  {
    "text": "sorts of things so with EMR within a couple clicks in the console you can have a fully functioning spark cluster ready to run whatever investigations you",
    "start": "564759",
    "end": "572199"
  },
  {
    "text": "want to do within a couple of minutes in several minutes so very easy to get it up and running fully configured with",
    "start": "572199",
    "end": "577930"
  },
  {
    "text": "dynamic allocation and notebooks and you can just start writing you know whatever jobs you have against data in s3 it's",
    "start": "577930",
    "end": "584800"
  },
  {
    "text": "also low cost you pay for what you use if you're not actively running a machine learning model why do you need to pay",
    "start": "584800",
    "end": "590079"
  },
  {
    "text": "for the cluster you don't you can shut it down so any EMR and we'll talk about this in a minute but decoupling storage",
    "start": "590079",
    "end": "595660"
  },
  {
    "text": "and compute you can spin a cluster up when you need it process the data and then shut it down and when you're not",
    "start": "595660",
    "end": "602199"
  },
  {
    "text": "using open source variety is important as many of you know spark codebase is",
    "start": "602199",
    "end": "607269"
  },
  {
    "text": "moving very very fast it's incredibly important to have the option to run the latest version as soon as possible after",
    "start": "607269",
    "end": "613029"
  },
  {
    "text": "it comes out you get new features new improvements oftentimes new improvements that things in spark are and spark ml",
    "start": "613029",
    "end": "619920"
  },
  {
    "text": "EMR has been shipping spark very close after the open source GA we were a couple days behind when spark 2 when",
    "start": "619920",
    "end": "626769"
  },
  {
    "text": "open source GA was available in EMR within a few days and minor versions as well so you know we're committed to",
    "start": "626769",
    "end": "633129"
  },
  {
    "text": "delivering the latest and greatest spark bits as soon as possible after their open source GA generally available",
    "start": "633129",
    "end": "639569"
  },
  {
    "text": "there's some management things behind the scenes and you know replacing bad nodes and scaling Friday security",
    "start": "639569",
    "end": "646360"
  },
  {
    "text": "features ranging from the encryption and if all the kind of automated defaults",
    "start": "646360",
    "end": "651670"
  },
  {
    "text": "aren't meeting your use case you also route access can add on other libraries all",
    "start": "651670",
    "end": "656800"
  },
  {
    "text": "sometimes we find that if you wanted to augment the spark ml machine learning libraries say with custom ones that",
    "start": "656800",
    "end": "662350"
  },
  {
    "text": "you've written or third-party vendor libraries you can bootstrap those on a cluster creation time and utilize them",
    "start": "662350",
    "end": "668140"
  },
  {
    "text": "along with all of the you know bits and components that are packaged we have a",
    "start": "668140",
    "end": "674320"
  },
  {
    "text": "few notebooks Jupiters natively there or sorry Zeppelin is natively there which is born of the Apache ecosystem it is a",
    "start": "674320",
    "end": "680230"
  },
  {
    "text": "very tight integration with SPARC the latest version Zeppelin has some interesting authentication features if",
    "start": "680230",
    "end": "685420"
  },
  {
    "text": "you have multiple users logging in at once but also you can interact with things like hive and other components in",
    "start": "685420",
    "end": "690490"
  },
  {
    "text": "the Hadoop ecosystem as well so it's a very powerful notebook you can save things to get you can save files you can",
    "start": "690490",
    "end": "695709"
  },
  {
    "text": "back the notebooks and s3 so there's a lot of interesting features with Zeppelin but if you don't want to use",
    "start": "695709",
    "end": "700750"
  },
  {
    "text": "that one you can use via bootstrap actions install things like Jupiter or studio that can communicate with SPARC",
    "start": "700750",
    "end": "707680"
  },
  {
    "text": "jupiter with apache tori as well we found works well and we expose a lot of these bootstrap actions on the AWS Big",
    "start": "707680",
    "end": "713560"
  },
  {
    "text": "Data blog and so I highly recommend taking a look we had a blog post a couple of weeks back about installing",
    "start": "713560",
    "end": "718959"
  },
  {
    "text": "our studio and sparkly R which is the interface for spark R and some other things as well we're constantly adding",
    "start": "718959",
    "end": "725380"
  },
  {
    "text": "those those things on Emaar run spark on yarn so for folks who are running spark",
    "start": "725380",
    "end": "731260"
  },
  {
    "text": "not an EMR say using standalone or mezzos mode EMR uses yarn it plays",
    "start": "731260",
    "end": "737589"
  },
  {
    "text": "nicely with a lot of the other Hadoop components but also is really great for dynamic allocation of executors to where",
    "start": "737589",
    "end": "744100"
  },
  {
    "text": "by default you can log in start writing a job you don't have to worry about specifying the number of executors and",
    "start": "744100",
    "end": "749680"
  },
  {
    "text": "you know did I specify enough ram EMR will utilizes or yarn will utilize as",
    "start": "749680",
    "end": "755649"
  },
  {
    "text": "much as it can to spread out the executors to run to run whatever workloads you put in also we have",
    "start": "755649",
    "end": "762850"
  },
  {
    "text": "available the SPARC UI you can access it through the resource manager UI",
    "start": "762850",
    "end": "767980"
  },
  {
    "text": "basically click through and if you're running in cluster mode it'll proxy to where the SPARC drivers running it's",
    "start": "767980",
    "end": "774459"
  },
  {
    "text": "very very useful you can see the dag of the job executors kicking in configuration settings at runtime you",
    "start": "774459",
    "end": "781990"
  },
  {
    "text": "can see if there's an error typically it will bubble up the stack trace so you can see exactly what happened and",
    "start": "781990",
    "end": "787510"
  },
  {
    "text": "quickly go either figure out yarn container log the errors in or sometimes it even gives you oftentimes",
    "start": "787510",
    "end": "793880"
  },
  {
    "text": "actually gives you enough context to immediately know what the air was in your job also historical jobs are",
    "start": "793880",
    "end": "799730"
  },
  {
    "text": "available in the spark history server which is available on the master node as well a couple tips and tricks for you",
    "start": "799730",
    "end": "808370"
  },
  {
    "text": "know basically increasing performance in trying to reduce cost it was just important kind of components when",
    "start": "808370",
    "end": "814460"
  },
  {
    "text": "designing any big data architecture the first is auto scaling and this feature shipped in EMR a couple weeks back and",
    "start": "814460",
    "end": "821260"
  },
  {
    "text": "we found that there's a variety of use cases for auto scaling one is if you're a resource-constrained while running a",
    "start": "821260",
    "end": "826520"
  },
  {
    "text": "job programmatically add nodes up to a certain amount and with yarn dynamic allocation enabled for spark spark we'll",
    "start": "826520",
    "end": "834410"
  },
  {
    "text": "start using those extra nodes and putting executors on them and scale your job out but on the flip side also",
    "start": "834410",
    "end": "840740"
  },
  {
    "text": "scaling the cluster down and not paying for it when nodes aren't being used like for instance if you have a data",
    "start": "840740",
    "end": "846020"
  },
  {
    "text": "scientist you come in you create a cluster you run some things and then leave for a couple hours that clusters",
    "start": "846020",
    "end": "851090"
  },
  {
    "text": "idle during that period of time you can set auto scaling policies to say look if this clusters idle at the end of the",
    "start": "851090",
    "end": "857630"
  },
  {
    "text": "hourly boundary because you already paid for the hour shrink things down in the background and kind of dynamically scale",
    "start": "857630",
    "end": "862910"
  },
  {
    "text": "out when more work has been added so there's some pretty you can save some serious costs by doing that but also",
    "start": "862910",
    "end": "868340"
  },
  {
    "text": "increased performance by dealing with with scaling what EMR does and how it works is that we pump a bunch of metrics",
    "start": "868340",
    "end": "874250"
  },
  {
    "text": "into cloud watch based on yarn and actually we have a couple new ones one on the yarn percentage memory used so",
    "start": "874250",
    "end": "879890"
  },
  {
    "text": "you know eighty percent of my yarn capacity is used it's a fraction of two yarn metrics so it's a custom one that we pump and also just the containers",
    "start": "879890",
    "end": "886970"
  },
  {
    "text": "pending yarn runs executors and yarn containers and the ratio of the containers pending over running it gives",
    "start": "886970",
    "end": "893450"
  },
  {
    "text": "you good indication of if you put on in another node would you be able to put anything on it so you can utilize those",
    "start": "893450",
    "end": "899930"
  },
  {
    "text": "two metrics also variety other ones we push or even custom metrics based on things that spark and you have to",
    "start": "899930",
    "end": "905780"
  },
  {
    "text": "institute figure out an instrument and push it to cloud watch but EMR will take that information and behind the scenes",
    "start": "905780",
    "end": "911210"
  },
  {
    "text": "create cloud watch alarms and user application auto scaling to dynamically add and remove nodes and actually in the",
    "start": "911210",
    "end": "916760"
  },
  {
    "text": "console under the cost of details page you'll see the events of auto scaling events so you can keep track",
    "start": "916760",
    "end": "922580"
  },
  {
    "text": "if you wonder hey why is my cluster three nodes you can see the events that kind of said hey you should scale down",
    "start": "922580",
    "end": "928640"
  },
  {
    "text": "now one other thing to add is that we find customers leveraging spot to save",
    "start": "928640",
    "end": "933830"
  },
  {
    "text": "significant amounts on their processing jobs for those who are not familiar ec2 spot is a way to bid on excess capacity",
    "start": "933830",
    "end": "939490"
  },
  {
    "text": "it's almost like a market where if you bid above above the current market price you get that instance however if the",
    "start": "939490",
    "end": "946519"
  },
  {
    "text": "market price goes above what you bid spot will reclaim that instance back so it's very good for workflows and",
    "start": "946519",
    "end": "951620"
  },
  {
    "text": "obviously sparks the distributed system so it can typically weather some nodes being taken away and if you decoupled",
    "start": "951620",
    "end": "956779"
  },
  {
    "text": "your storage and compute and s3 your cluster theoretically could go you would just rerun the job in your data set with",
    "start": "956779",
    "end": "962149"
  },
  {
    "text": "all of your training data or test data or whatever you're making predictions from will be safe in s3 so today we see",
    "start": "962149",
    "end": "968839"
  },
  {
    "text": "customers creating a lot of our clusters using spot typically for the task",
    "start": "968839",
    "end": "974570"
  },
  {
    "text": "instance type which doesn't have HDFS data node although SPARC doesn't heavily utilize the HDFS but just it's easier to",
    "start": "974570",
    "end": "980810"
  },
  {
    "text": "scale out and scale in without having HDFS do any rebalancing but we found is",
    "start": "980810",
    "end": "986269"
  },
  {
    "text": "that customers depending on the spot market price might want to have say a variety of different instance types",
    "start": "986269",
    "end": "991730"
  },
  {
    "text": "based on what's the cheapest spot price poor you know GB RAM or core available",
    "start": "991730",
    "end": "998329"
  },
  {
    "text": "and can you give us that or not know what AZ you want to launch and because spot price is different AZ's it could be",
    "start": "998329",
    "end": "1004149"
  },
  {
    "text": "that one availability zone is having a spot price spike while other ones are low and you know it's it's sometimes",
    "start": "1004149",
    "end": "1010810"
  },
  {
    "text": "hard to know exactly what AZ you should launch the cluster so coming soon will have the ability to an EMR you can",
    "start": "1010810",
    "end": "1017529"
  },
  {
    "text": "specify a list of different instance types and different bid prices and also a list of different AZ's so if you want",
    "start": "1017529",
    "end": "1024100"
  },
  {
    "text": "to run a machine learning job at cost EMR will do a lot of the lifting on the back end to figure out what is the",
    "start": "1024100",
    "end": "1030069"
  },
  {
    "text": "cheapest AZ and the cheapest set of instance types to run to give you that cluster at the lowest possible cost and",
    "start": "1030069",
    "end": "1037418"
  },
  {
    "text": "then finally once you've you know done a bunch of investigation you've utilized the notebooks you have a job that you",
    "start": "1037419",
    "end": "1043030"
  },
  {
    "text": "like it's time to take it out of your sandbox environment put it into production and there's a couple ways",
    "start": "1043030",
    "end": "1048610"
  },
  {
    "text": "that we see customers leveraging the AWS ecosystem to go do this the first and easiest one is using an EMR step EMR is",
    "start": "1048610",
    "end": "1056350"
  },
  {
    "text": "an API our step API and you can add a step which is a discrete unit of work in",
    "start": "1056350",
    "end": "1061840"
  },
  {
    "text": "this case a spark application under the hood will use spark submit and submit it to your cluster and that's interacting",
    "start": "1061840",
    "end": "1067000"
  },
  {
    "text": "with an EMR API you specify where your jar is and s3n while we'll take the take",
    "start": "1067000",
    "end": "1072130"
  },
  {
    "text": "the jar application run it and then write out whatever output you've specified in that and that's easy you",
    "start": "1072130",
    "end": "1077919"
  },
  {
    "text": "can keep writing adding steps and it'll execute them in sequence and then when",
    "start": "1077919",
    "end": "1083320"
  },
  {
    "text": "you're done you can even have a step say terminate my cluster when this is finished work when you create a cluster run these steps and then terminate it",
    "start": "1083320",
    "end": "1089460"
  },
  {
    "text": "for more advanced use cases around steps you can use something like AWS lambda and say well on this event trigger",
    "start": "1089460",
    "end": "1096730"
  },
  {
    "text": "running the step trigger creating this cluster and it could be time-based it could be you know new data comes into an",
    "start": "1096730",
    "end": "1102370"
  },
  {
    "text": "s3 bucket that's watching it you know go launch the job and go run the job for more advanced use cases around say",
    "start": "1102370",
    "end": "1108040"
  },
  {
    "text": "creating DAGs of jobs or things like that you can use AWS data pipeline which is a service in AWS that you know can",
    "start": "1108040",
    "end": "1114820"
  },
  {
    "text": "provision clusters run many jobs and shut down cluster and create new ones you can create really complicated",
    "start": "1114820",
    "end": "1119830"
  },
  {
    "text": "workflows building with both AWS services and jobs within those services but we also see lots of customers",
    "start": "1119830",
    "end": "1126040"
  },
  {
    "text": "leveraging things like Apache airflow which was open source by Airbnb which is another robust scheduler Luigi or even",
    "start": "1126040",
    "end": "1133990"
  },
  {
    "text": "Izzy on the cluster you can choose to install who Xeon EMR and use SPARC actions to create tags of jobs I'm so",
    "start": "1133990",
    "end": "1142240"
  },
  {
    "text": "with that I'll hand it over to Josje will talk a little bit how they leverage this technology at Zilla",
    "start": "1142240",
    "end": "1148320"
  },
  {
    "text": "hello everybody so thanks for the introduction so my name is Justin senior",
    "start": "1152570",
    "end": "1159860"
  },
  {
    "text": "director of data science engineering and zillow group and today I will talk about",
    "start": "1159860",
    "end": "1165550"
  },
  {
    "text": "recommendation systems at Zillah group so I'll cover I'll first give an",
    "start": "1165550",
    "end": "1172190"
  },
  {
    "text": "introduction on Zillow and I'll go through some of the use cases related to",
    "start": "1172190",
    "end": "1177560"
  },
  {
    "text": "recommendation systems I'll go over a high-level architecture of a",
    "start": "1177560",
    "end": "1183680"
  },
  {
    "text": "recommendation platform and I'm spend most of our time on going deep diving",
    "start": "1183680",
    "end": "1189380"
  },
  {
    "text": "into our machine learning algorithms I'll talk about our training a scoring pipeline and finally I'll look we'll",
    "start": "1189380",
    "end": "1194480"
  },
  {
    "text": "talk about the evaluation metrics we look at to make sure our models are",
    "start": "1194480",
    "end": "1200540"
  },
  {
    "text": "working properly so Zillow group is a",
    "start": "1200540",
    "end": "1205850"
  },
  {
    "text": "conglomerate of various real estate sites mobile applications and some Amara",
    "start": "1205850",
    "end": "1213950"
  },
  {
    "text": "here on the slide deck Trulia which started on in 2005 was among the first",
    "start": "1213950",
    "end": "1220820"
  },
  {
    "text": "to provide home related data such as home fax prior sell information online",
    "start": "1220820",
    "end": "1228160"
  },
  {
    "text": "to make it available for everybody it was quite revolutionary at the time you couldn't get this data easily publicly",
    "start": "1228160",
    "end": "1235060"
  },
  {
    "text": "Zillow which started around the same time was the first to provide home",
    "start": "1235060",
    "end": "1240320"
  },
  {
    "text": "valuation model for free it was really kind of the first time where you can just go to a home look at a number and",
    "start": "1240320",
    "end": "1248150"
  },
  {
    "text": "say oh this is how much I should buy this home for or this is how much I should sell it for Street easy and naked",
    "start": "1248150",
    "end": "1255830"
  },
  {
    "text": "apartments specialize in the New York City market and they are the dominant",
    "start": "1255830",
    "end": "1262010"
  },
  {
    "text": "players their hot pads is among the leaders in rentals more tech provides",
    "start": "1262010",
    "end": "1268090"
  },
  {
    "text": "loan pricing information for multiple lenders rest Lee provides developer API",
    "start": "1268090",
    "end": "1275750"
  },
  {
    "text": "is for real estate data and then dot Lu is doing something that I think will benefit all of us which is about making",
    "start": "1275750",
    "end": "1282690"
  },
  {
    "text": "the real estate transaction paperless I'm sure many of you have gone through a real estate transaction it's fairly",
    "start": "1282690",
    "end": "1289310"
  },
  {
    "text": "still a fairly painful process so now",
    "start": "1289310",
    "end": "1295560"
  },
  {
    "text": "we'll go into recommendation systems at Zillow group and here are some the courts some of the core use cases",
    "start": "1295560",
    "end": "1302220"
  },
  {
    "text": "include email where we sell recommended homes for sale and recommended homeless",
    "start": "1302220",
    "end": "1309390"
  },
  {
    "text": "for rent on email personalized for a user and then when you go to the home details page say on Trulia on Zillow and",
    "start": "1309390",
    "end": "1317690"
  },
  {
    "text": "you'll see similar homes power modules",
    "start": "1317690",
    "end": "1323160"
  },
  {
    "text": "and that's also a recommendation based algorithms behind the scenes personalized search is about taking your",
    "start": "1323160",
    "end": "1330630"
  },
  {
    "text": "explicit query and modifying that with an inferred query that we compute",
    "start": "1330630",
    "end": "1336090"
  },
  {
    "text": "offline using machine learning algorithms so that everyone gets a",
    "start": "1336090",
    "end": "1341670"
  },
  {
    "text": "personalized recommendations on mobile what we're doing is providing contextual",
    "start": "1341670",
    "end": "1348210"
  },
  {
    "text": "and location-aware based recommendations to buy a mobile app notifications and on",
    "start": "1348210",
    "end": "1354630"
  },
  {
    "text": "Zillow we have a feature called home claims where you can go and claim your home and you can go and update your your",
    "start": "1354630",
    "end": "1359850"
  },
  {
    "text": "facts directly and in order to increase coverage there we we do predictions on",
    "start": "1359850",
    "end": "1364860"
  },
  {
    "text": "who is the home owner of every single home and we also do predictions on who",
    "start": "1364860",
    "end": "1370560"
  },
  {
    "text": "we think is going to be selling selling their home soon we have algorithms around lender selection and we have",
    "start": "1370560",
    "end": "1377820"
  },
  {
    "text": "algorithms around similar photographs and in similar videos so here's our",
    "start": "1377820",
    "end": "1385460"
  },
  {
    "text": "high-level architecture at the top there are the recommendation API so there's",
    "start": "1385460",
    "end": "1392370"
  },
  {
    "text": "both there's a real time scoring aspect of that as well which I won't go through today but maybe in another session and",
    "start": "1392370",
    "end": "1399420"
  },
  {
    "text": "on the right hand side you have property feature feature ization where you take",
    "start": "1399420",
    "end": "1405590"
  },
  {
    "text": "properties and extract signal out of those properties and convert them to a",
    "start": "1405590",
    "end": "1411030"
  },
  {
    "text": "feature space that I recommend understands now going too deep into that later on property a great features are",
    "start": "1411030",
    "end": "1418640"
  },
  {
    "text": "features on a property but leverage additional data such as user signals for",
    "start": "1418640",
    "end": "1424530"
  },
  {
    "text": "example there's a feature to determine how popular new property is for example using user signals which counting is a",
    "start": "1424530",
    "end": "1433159"
  },
  {
    "text": "in-house built collaborative filtering algorithm and we'll go into shortly at the core of all",
    "start": "1433159",
    "end": "1441870"
  },
  {
    "text": "of this as John mentioned earlier is our data Lake which is based on s3 all the",
    "start": "1441870",
    "end": "1450450"
  },
  {
    "text": "data for our recommendation lives there and then Zillow and Trulia we have a lot",
    "start": "1450450",
    "end": "1458730"
  },
  {
    "text": "of so-called legacy systems where there's their status there's Microsoft",
    "start": "1458730",
    "end": "1464220"
  },
  {
    "text": "CEO servers Redis there's my sequel databases and we need to get the data",
    "start": "1464220",
    "end": "1469530"
  },
  {
    "text": "from these databases and put it into the cloud so we have custom data collection",
    "start": "1469530",
    "end": "1475110"
  },
  {
    "text": "systems to get the data into the cloud and we actually have a nice blog article",
    "start": "1475110",
    "end": "1480840"
  },
  {
    "text": "on how we do this on our data science blogs if you want to read later on these are profiles are about building user",
    "start": "1480840",
    "end": "1489510"
  },
  {
    "text": "preferences so that we can recommend personalized content to users and I'll",
    "start": "1489510",
    "end": "1495419"
  },
  {
    "text": "talk about that and rankings puts it all together ranking takes that user profiles takes kinda collaborative",
    "start": "1495419",
    "end": "1501240"
  },
  {
    "text": "filtering takes property features and puts it all together okay so let's go",
    "start": "1501240",
    "end": "1509669"
  },
  {
    "text": "deeper now okay so fund them and the fundamental concept through all - for all that",
    "start": "1509669",
    "end": "1517080"
  },
  {
    "text": "recommendation api's is defining what is a like in the dislike and the goal here is we want to predict homes for a user",
    "start": "1517080",
    "end": "1525150"
  },
  {
    "text": "using behaviors to other other similar home rooms similar users and to explain that let's take this example so Spencer",
    "start": "1525150",
    "end": "1531480"
  },
  {
    "text": "the CEO of Zillow group really likes the 19 million dollar home and the 22",
    "start": "1531480",
    "end": "1537900"
  },
  {
    "text": "million dollar home and Stan likes the",
    "start": "1537900",
    "end": "1545350"
  },
  {
    "text": "and Spencer doesn't like the 664 thousand dollar home more modestly price home Stan likes the more modestly priced",
    "start": "1545350",
    "end": "1552010"
  },
  {
    "text": "home but doesn't like the twenty two million dollar home so the goal here is to predict the Stan like or does he",
    "start": "1552010",
    "end": "1558040"
  },
  {
    "text": "dislike the 19 million dollar home and our definition of like is the user",
    "start": "1558040",
    "end": "1563230"
  },
  {
    "text": "actively engaged with the property and a dislike means that user didn't act lineage so what does that mean look at",
    "start": "1563230",
    "end": "1569410"
  },
  {
    "text": "the right there the candidate set are all visits to a property where they're",
    "start": "1569410",
    "end": "1579160"
  },
  {
    "text": "the time stems not equal to zero zero means the user never visited property and at times I mean the user has at",
    "start": "1579160",
    "end": "1584470"
  },
  {
    "text": "least one visit to the property now we don't want to use zero visits set to",
    "start": "1584470",
    "end": "1590890"
  },
  {
    "text": "zero because that will highly skew the algorithms because most users don't",
    "start": "1590890",
    "end": "1596080"
  },
  {
    "text": "visit most properties and we also look",
    "start": "1596080",
    "end": "1601210"
  },
  {
    "text": "at we defined like by looking at for example the number of views time spent",
    "start": "1601210",
    "end": "1607630"
  },
  {
    "text": "on a property the number of leads sent number of saves number of shares and",
    "start": "1607630",
    "end": "1614290"
  },
  {
    "text": "there's all certain thresholds to all of these and and these are constantly being learned as well and now that we have the",
    "start": "1614290",
    "end": "1624100"
  },
  {
    "text": "definition like in dislike so I'll first talk about kelabra labra filtering up so which counting was it was invented by",
    "start": "1624100",
    "end": "1630700"
  },
  {
    "text": "really talented data scientist at Trulia Joseph Kong and that's his actually",
    "start": "1630700",
    "end": "1636370"
  },
  {
    "text": "paper it was submitted in the kdd Cup in 2011 it came up among the top algorithms",
    "start": "1636370",
    "end": "1644410"
  },
  {
    "text": "and let me go over that a little bit what that is so in which County the idea",
    "start": "1644410",
    "end": "1651460"
  },
  {
    "text": "is to take common items and common users and figure out and their interactions",
    "start": "1651460",
    "end": "1658690"
  },
  {
    "text": "with the pertinent item that you're trying to pick predict on in this case a property and determine and then compute",
    "start": "1658690",
    "end": "1666340"
  },
  {
    "text": "these wedges so you can see in the in",
    "start": "1666340",
    "end": "1671890"
  },
  {
    "text": "the top left UTG represents the target user and iit-jee represent the target property",
    "start": "1671890",
    "end": "1678400"
  },
  {
    "text": "we're try make a prediction on so in our example UTG would be Stan and then itg would be the 19 million dollar home and then IOT",
    "start": "1678400",
    "end": "1686199"
  },
  {
    "text": "represents another item and you ot represents another user so in the first",
    "start": "1686199",
    "end": "1692079"
  },
  {
    "text": "word you see that utg like doesn't like I owe to and I and and you ot doesn't",
    "start": "1692079",
    "end": "1698379"
  },
  {
    "text": "like IOT and you ot doesn't like i io t so if you count these all up there's",
    "start": "1698379",
    "end": "1703389"
  },
  {
    "text": "actually eight configurations and taking an example in the previous slide there's like like like the ninety million dollar",
    "start": "1703389",
    "end": "1708879"
  },
  {
    "text": "home so there's two edges it's wedge number three and wedge number five in which number three Stan doesn't like the",
    "start": "1708879",
    "end": "1716440"
  },
  {
    "text": "twenty two million dollar home Spencer likes it and Spencer likes the 19 million dollar home so you increment",
    "start": "1716440",
    "end": "1722709"
  },
  {
    "text": "wedge three by one these are all counts and then ever try this Stan likes the modesty price home Spencer doesn't like",
    "start": "1722709",
    "end": "1729759"
  },
  {
    "text": "it and Spencer likes the 19 mil at home so when you comment wedge five by one and then there's eight of these features",
    "start": "1729759",
    "end": "1737979"
  },
  {
    "text": "and you count them all up and there's you actually notice there's actually 16",
    "start": "1737979",
    "end": "1743889"
  },
  {
    "text": "features so there's actually eight additional normalized features so one of the challenges in recommendation system",
    "start": "1743889",
    "end": "1748929"
  },
  {
    "text": "is a very popular properties let's say the White House get a lot of engagement",
    "start": "1748929",
    "end": "1755559"
  },
  {
    "text": "and that skews your results so what we do is you compute eight additional wedge",
    "start": "1755559",
    "end": "1760899"
  },
  {
    "text": "count features by dividing that by the degree product G of U and King high",
    "start": "1760899",
    "end": "1766779"
  },
  {
    "text": "which represents Jerry represents the number of users that have interacted",
    "start": "1766779",
    "end": "1773249"
  },
  {
    "text": "with an item and the item the item target item represent number of user interactivity with it so you just",
    "start": "1773249",
    "end": "1778359"
  },
  {
    "text": "multiply it and divide by that number so you get total of sixteen features and we use a gradient boosted classifier which",
    "start": "1778359",
    "end": "1785739"
  },
  {
    "text": "John went over briefly and then what you do is you make a prediction on all the",
    "start": "1785739",
    "end": "1792969"
  },
  {
    "text": "user and property pairs that that you want to make prediction on we filtered",
    "start": "1792969",
    "end": "1798789"
  },
  {
    "text": "in order to reduce costs and compute time we filter by top ten zip codes and",
    "start": "1798789",
    "end": "1804849"
  },
  {
    "text": "by a certain number of properties per user so in our example the goal again is the user ID is stand and",
    "start": "1804849",
    "end": "1811970"
  },
  {
    "text": "Brady's 90 million and the features on the right so you create this data set",
    "start": "1811970",
    "end": "1817190"
  },
  {
    "text": "use the gradient boosted classifier to you to train your model and then you score new user property pairs with that",
    "start": "1817190",
    "end": "1824300"
  },
  {
    "text": "model so now we'll move on to the user profile and the user profile again",
    "start": "1824300",
    "end": "1831260"
  },
  {
    "text": "represents the particular interests of every single user and the key signals",
    "start": "1831260",
    "end": "1839270"
  },
  {
    "text": "that we use include traffic data on other sites",
    "start": "1839270",
    "end": "1845480"
  },
  {
    "text": "what what users are viewing and what their interactions those events and we",
    "start": "1845480",
    "end": "1851030"
  },
  {
    "text": "use mobile app data interaction of the app and we also leverage the search",
    "start": "1851030",
    "end": "1856430"
  },
  {
    "text": "queries and just like in our watch county algorithm the labels are liked",
    "start": "1856430",
    "end": "1863990"
  },
  {
    "text": "and disliked so there's your one and each property will have we convert each",
    "start": "1863990",
    "end": "1872570"
  },
  {
    "text": "property to a set of categorical features so you can see on the right that for example bathroom you have 0",
    "start": "1872570",
    "end": "1878990"
  },
  {
    "text": "bath 0.5 bath 1 bath and so on the price you have the price there's 100k to 125 K",
    "start": "1878990",
    "end": "1884870"
  },
  {
    "text": "so everything is this categorical in nature and so properties have a thousand",
    "start": "1884870",
    "end": "1890360"
  },
  {
    "text": "features and they're either set to 0 1 so you know properties either could be",
    "start": "1890360",
    "end": "1896120"
  },
  {
    "text": "if your property is a 2 bathroom then it's not a zero zero by zero point five baths those will be set to zero and as I",
    "start": "1896120",
    "end": "1904070"
  },
  {
    "text": "mentioned before on the bottom here you said you have kind of the bedroom features and this user profile",
    "start": "1904070",
    "end": "1909290"
  },
  {
    "text": "specifically this user likes the scores are between 0 and 1 and this user likes",
    "start": "1909290",
    "end": "1915380"
  },
  {
    "text": "a two-bedroom and also likes 3 bedrooms but doesn't like a studio doesn't like a",
    "start": "1915380",
    "end": "1921590"
  },
  {
    "text": "one-bedroom so your model your again it's a gradient boosted classifier but your model outputs these scores per user",
    "start": "1921590",
    "end": "1932050"
  },
  {
    "text": "and I mentioned earlier ranking puts this all together and you know that's a ten topic that's a major topic on its",
    "start": "1932050",
    "end": "1939140"
  },
  {
    "text": "own but I'll cover some important parts of that essentially what we do is you",
    "start": "1939140",
    "end": "1944150"
  },
  {
    "text": "take your prop matrix which are set of categorical",
    "start": "1944150",
    "end": "1950090"
  },
  {
    "text": "features either 0 1 and you take your user vector which are those preference",
    "start": "1950090",
    "end": "1955130"
  },
  {
    "text": "scores for the same feature space multiplied together and you essentially get a single score at the end and",
    "start": "1955130",
    "end": "1964570"
  },
  {
    "text": "there's also additional features such as at the bottom that I have H decay",
    "start": "1964570",
    "end": "1970030"
  },
  {
    "text": "populated base scores quality scores that are part of this feature set and but ultimately it's a very simple",
    "start": "1970030",
    "end": "1977150"
  },
  {
    "text": "operation to to once once you compete the user vector and they do the feature",
    "start": "1977150",
    "end": "1982850"
  },
  {
    "text": "I use feature ization now here's our",
    "start": "1982850",
    "end": "1988160"
  },
  {
    "text": "training and scoring pipeline and a high level what this is all about is taking raw used raw signals and collecting that",
    "start": "1988160",
    "end": "1997670"
  },
  {
    "text": "data put and filtering that data to what we care about and creating the training",
    "start": "1997670",
    "end": "2005170"
  },
  {
    "text": "and scoring datasets then training your models score your score your predictions",
    "start": "2005170",
    "end": "2013930"
  },
  {
    "text": "and then put it into some data store so you can actually serve versus those recommendations so I'll go let's go into",
    "start": "2013930",
    "end": "2020470"
  },
  {
    "text": "this a little bit further so in the recommendation use case there are three types of data sets we have the user",
    "start": "2020470",
    "end": "2028720"
  },
  {
    "text": "behavior for example what users are interacting with on your site mobile app",
    "start": "2028720",
    "end": "2034270"
  },
  {
    "text": "actions so we have a custom event API that is invoked and that data then gets",
    "start": "2034270",
    "end": "2039630"
  },
  {
    "text": "pushed into a Kinesis stream so it's it",
    "start": "2039630",
    "end": "2045310"
  },
  {
    "text": "and then it lands on s3 all in near-real-time for the property data public record data",
    "start": "2045310",
    "end": "2052240"
  },
  {
    "text": "which comes from the counties in general that we have a producer that pushes the",
    "start": "2052240",
    "end": "2059290"
  },
  {
    "text": "data also into a Kenisha stream and for listening data a similar idea where we push that into a Kinesis stream then for",
    "start": "2059290",
    "end": "2067120"
  },
  {
    "text": "the user behavior data what we do is we use spark EMR to filter that data to",
    "start": "2067120",
    "end": "2074320"
  },
  {
    "text": "what we care about a lot of events we don't care about and we store in our Jesus star which is hive back by s3 and",
    "start": "2074320",
    "end": "2082360"
  },
  {
    "text": "there's also a Redis store which stores the reverse index which is for the witch",
    "start": "2082360",
    "end": "2087908"
  },
  {
    "text": "candy algorithm it's important that you need to look up all the users that interact with the property so the key here is the property ID and then you",
    "start": "2087909",
    "end": "2093790"
  },
  {
    "text": "have a vector of users so that's also computed then the then we use Sparky",
    "start": "2093790",
    "end": "2101500"
  },
  {
    "text": "amar to generate the training data sets taking the data from the user store taking the data from list listing data",
    "start": "2101500",
    "end": "2108310"
  },
  {
    "text": "and the property data and we create the training set again in hive back is three we then train our models in using spark",
    "start": "2108310",
    "end": "2117490"
  },
  {
    "text": "those green boosted classifiers we store that into s3 who actually I'm actually I",
    "start": "2117490",
    "end": "2123400"
  },
  {
    "text": "heard about the the key value editable key value pairs on s3 that I think will",
    "start": "2123400",
    "end": "2129070"
  },
  {
    "text": "be very useful because it allow us to easily find all you know there's",
    "start": "2129070",
    "end": "2135750"
  },
  {
    "text": "thousands tens of thousands as a model so allows us to to kind of mark that with metadata so I'm very excited about",
    "start": "2135750",
    "end": "2140950"
  },
  {
    "text": "that feature that was announced this morning then we use Sparky M artists for",
    "start": "2140950",
    "end": "2146650"
  },
  {
    "text": "and make those predictions and that ends up into a reddish database for offline",
    "start": "2146650",
    "end": "2153910"
  },
  {
    "text": "evaluation a fairly common set of metrics we look at we have a validation",
    "start": "2153910",
    "end": "2159040"
  },
  {
    "text": "set where we optimize the hyper parameter through the hyper partner tuning and then we have training and",
    "start": "2159040",
    "end": "2165190"
  },
  {
    "text": "test data sets it's essentially you you go back in time you take from today the",
    "start": "2165190",
    "end": "2172660"
  },
  {
    "text": "past weeks of data it would be your test set and the week before will be your training set and then you keep going back in time as as far as you want to go",
    "start": "2172660",
    "end": "2179530"
  },
  {
    "text": "and key off metrics we look at your standard precision recall and freshness",
    "start": "2179530",
    "end": "2186970"
  },
  {
    "text": "is very important because we want to surface fresh listing predictions for",
    "start": "2186970",
    "end": "2193210"
  },
  {
    "text": "users and and especially listings that have important key changes like this",
    "start": "2193210",
    "end": "2199060"
  },
  {
    "text": "such as the list price dropped or increased so we want to make sure we so",
    "start": "2199060",
    "end": "2206080"
  },
  {
    "text": "we look at how fresh are listing eyes are they one day old two days old you know things like that and then coverage",
    "start": "2206080",
    "end": "2211570"
  },
  {
    "text": "we want sure two surface listings that might not have a lot of use interaction but we",
    "start": "2211570",
    "end": "2217819"
  },
  {
    "text": "want to surface those listings to that to users that are relevant so future",
    "start": "2217819",
    "end": "2225829"
  },
  {
    "text": "work we are looking at the building classifiers for the listing description",
    "start": "2225829",
    "end": "2231380"
  },
  {
    "text": "there's a lot of interesting data in the description is doing text mining around",
    "start": "2231380",
    "end": "2237050"
  },
  {
    "text": "that looking at images and extracting sigell out of images to to to to to make",
    "start": "2237050",
    "end": "2244940"
  },
  {
    "text": "better predictions and then also we are very excited about structures streaming on spark and to move that fairly batch",
    "start": "2244940",
    "end": "2254750"
  },
  {
    "text": "as fast batch is happening often but we want to get to more of a real-time scenario so we're very excited about",
    "start": "2254750",
    "end": "2260599"
  },
  {
    "text": "that leveraging more signals and then doing more around real time scoring using in",
    "start": "2260599",
    "end": "2265910"
  },
  {
    "text": "session behavior to make predictions thank you very much and there's our blog",
    "start": "2265910",
    "end": "2275930"
  },
  {
    "text": "on the right side so please check it out and also we're hiring so please come",
    "start": "2275930",
    "end": "2281299"
  },
  {
    "text": "join us as well take some questions we'll take some questions",
    "start": "2281299",
    "end": "2286540"
  },
  {
    "text": "we'll take some questions down here thanks everybody for your time [Applause]",
    "start": "2292580",
    "end": "2301150"
  }
]