[
  {
    "start": "0",
    "end": "22000"
  },
  {
    "text": "everyone on my caliber a partner solution architect with AWS you saw my",
    "start": "920",
    "end": "6150"
  },
  {
    "text": "partner Ando Khan Mishra he's a principal solutions architect lady ABS today when we talk about optimizing red",
    "start": "6150",
    "end": "12509"
  },
  {
    "text": "shift on AWS and at the end of my slide presentation out of palm will give a",
    "start": "12509",
    "end": "17910"
  },
  {
    "text": "demonstration to show kind of what those best practices are in the real world so a quick overview like I just mentioned",
    "start": "17910",
    "end": "24539"
  },
  {
    "start": "22000",
    "end": "38000"
  },
  {
    "text": "we're gonna do a very high-level overview of red shift the typical node architecture often and then we'll hop",
    "start": "24539",
    "end": "30449"
  },
  {
    "text": "into the optimization tips then the demonstration and then hopefully I have enough time left over to do a question",
    "start": "30449",
    "end": "36270"
  },
  {
    "text": "and answer for you guys this is just some of the customers that we have that are using Amazon redshift one I want to",
    "start": "36270",
    "end": "43680"
  },
  {
    "start": "38000",
    "end": "89000"
  },
  {
    "text": "call out specifically a sportswear Foursquare had an issue a while ago where they wanted to kind of have a cost",
    "start": "43680",
    "end": "49200"
  },
  {
    "text": "savings with their existing database system so what they went to is Amazon",
    "start": "49200",
    "end": "55890"
  },
  {
    "text": "redshift with the the problems that they had was the they had yearly licensing costs with their existing database",
    "start": "55890",
    "end": "61109"
  },
  {
    "text": "system and it was very hard to keep up so you had to have a dedicated staff to sit there and maintain it and they wanted to kind of shift those people to",
    "start": "61109",
    "end": "67320"
  },
  {
    "text": "kind of a different area and use their time a little more optimally so what they did is they introduced Amazon",
    "start": "67320",
    "end": "73290"
  },
  {
    "text": "redshift and in conjunction with Amazon redshift and tableau they were to get those cost savings down in terms of not",
    "start": "73290",
    "end": "79110"
  },
  {
    "text": "having a licensing cost anymore and they're allowed they could get the team to focus on other things in terms of",
    "start": "79110",
    "end": "84960"
  },
  {
    "text": "optimization and other projects that they wanted to push forward now we'll",
    "start": "84960",
    "end": "90869"
  },
  {
    "start": "89000",
    "end": "129000"
  },
  {
    "text": "just start with a quick service overview of Amazon redshift Amazon redshift is our fully managed data warehouse it's",
    "start": "90869",
    "end": "99119"
  },
  {
    "text": "simple cost effective sequel using sequel standard sequel and your you can",
    "start": "99119",
    "end": "104939"
  },
  {
    "text": "use your existing business intelligence tools no upfront costs there's also Amazon spectrum Amazon",
    "start": "104939",
    "end": "110939"
  },
  {
    "text": "redshift spectrum which we'll be showing a demo later where you can query petabytes of data across s3 and can and",
    "start": "110939",
    "end": "117270"
  },
  {
    "text": "it supports formats like CSV grok part cat and other formats so you can see on",
    "start": "117270",
    "end": "122790"
  },
  {
    "text": "the Amazon redshift website",
    "start": "122790",
    "end": "125840"
  },
  {
    "start": "129000",
    "end": "172000"
  },
  {
    "text": "and then this is just a typical known architecture so there's two important parts to the Amazon redshift cluster",
    "start": "129170",
    "end": "134880"
  },
  {
    "text": "there's a leader node and the compute nodes leader node obviously as it sounds is the leader it orchestrates the",
    "start": "134880",
    "end": "140100"
  },
  {
    "text": "communication between the compute nodes it parses and develops the execution plans to carry out the operations and",
    "start": "140100",
    "end": "146940"
  },
  {
    "text": "sends those optimized query plans to the compute nodes and the compute nodes are the ones that actually do the data",
    "start": "146940",
    "end": "153209"
  },
  {
    "text": "crunching so they have there's two different types there's dense compute and then dense storage the dc2 is I",
    "start": "153209",
    "end": "161640"
  },
  {
    "text": "don't know if you're familiar just launched in October it's opposed to the DC ones so I think it's 30% more",
    "start": "161640",
    "end": "168090"
  },
  {
    "text": "performance at the same price and then this is just another example of leader",
    "start": "168090",
    "end": "173940"
  },
  {
    "start": "172000",
    "end": "197000"
  },
  {
    "text": "node showing how it's partitioned and sending the data the compute node so the compute nodes have slices so for example",
    "start": "173940",
    "end": "180450"
  },
  {
    "text": "at DC to large will have less slices than a DC to 8xl and you have to choose",
    "start": "180450",
    "end": "185820"
  },
  {
    "text": "how many slices you want based on your your workload and you know not one size",
    "start": "185820",
    "end": "190950"
  },
  {
    "text": "fits all but it just depends upon you know what you're doing and and your cost savings and stuff like that and then",
    "start": "190950",
    "end": "197940"
  },
  {
    "start": "197000",
    "end": "253000"
  },
  {
    "text": "we're diving right now into the optimization part of it so this the",
    "start": "197940",
    "end": "203010"
  },
  {
    "text": "first part is the look we want you to load your data efficiently so when you're using the copy command once you load all the data load all the data in",
    "start": "203010",
    "end": "210750"
  },
  {
    "text": "parallel we highly recommend compressing the data if you guys use the copy command by default it will use a default",
    "start": "210750",
    "end": "216989"
  },
  {
    "text": "compression in the slow the data a little more quickly sometimes the",
    "start": "216989",
    "end": "222540"
  },
  {
    "text": "default compression isn't optimal so sometimes you'll have to use whoops sorry about that analyze compress and",
    "start": "222540",
    "end": "229230"
  },
  {
    "text": "play with a sample set of data to see which compression algorithm works for your data and then obviously we want you",
    "start": "229230",
    "end": "234900"
  },
  {
    "text": "to split the files into multiple files so that you can leverage the parallel",
    "start": "234900",
    "end": "240480"
  },
  {
    "text": "processing of the slices in the compute to input there to slurp up that data",
    "start": "240480",
    "end": "247700"
  },
  {
    "start": "253000",
    "end": "285000"
  },
  {
    "text": "next we have a table design up front table design is critical the distribution key and sort key obviously",
    "start": "253730",
    "end": "259709"
  },
  {
    "text": "in influence performance primary key four primary key foreign key unique key",
    "start": "259709",
    "end": "265440"
  },
  {
    "text": "the unique key is not the unique key constraint is not enforced data",
    "start": "265440",
    "end": "270479"
  },
  {
    "text": "description he has two primary goals those are the distribution did you sorry excuse like distribute the workload",
    "start": "270479",
    "end": "276330"
  },
  {
    "text": "uniformly and minimize movement I'm",
    "start": "276330",
    "end": "283470"
  },
  {
    "text": "sorry we might sorry dated movement during a query and then also we want to compress",
    "start": "283470",
    "end": "290250"
  },
  {
    "start": "285000",
    "end": "301000"
  },
  {
    "text": "data for faster speed and lower cost because you don't want unrest data on the disk because that's obviously more expensive for you to to spend next we'll",
    "start": "290250",
    "end": "302250"
  },
  {
    "start": "301000",
    "end": "336000"
  },
  {
    "text": "talk about sort keys so a sort keys we want to use these on where clauses this",
    "start": "302250",
    "end": "307680"
  },
  {
    "text": "is this is just sorry one second no this",
    "start": "307680",
    "end": "314790"
  },
  {
    "text": "is just an example of sort keys this is just a little more data different types of sort keys and where they're most applicable compound sort keys are good",
    "start": "314790",
    "end": "320789"
  },
  {
    "text": "for no one query patterns like time series data interleaved sort keys give equal weight to each column in the sort",
    "start": "320789",
    "end": "327419"
  },
  {
    "text": "key the interleaved sort key is also good against very large tables for example something with a billion rows",
    "start": "327419",
    "end": "334910"
  },
  {
    "text": "and then that's just an example of how you can see uneven distribution Keys just visually so with uneven",
    "start": "335720",
    "end": "342360"
  },
  {
    "start": "336000",
    "end": "367000"
  },
  {
    "text": "distribution keys in the data being unevenly distributed across your nodes and slices you're going to have pretty",
    "start": "342360",
    "end": "348840"
  },
  {
    "text": "bad performance if one slice and one node is taking longer because you're only going to be as fast as your slowest slice so that's why we stress making",
    "start": "348840",
    "end": "355680"
  },
  {
    "text": "sure upfront that you have the right distribution keys to get that data evenly distributed across your nodes and",
    "start": "355680",
    "end": "361169"
  },
  {
    "text": "slices",
    "start": "361169",
    "end": "363560"
  },
  {
    "text": "and then this is just some the optimization you can do for the skewed distribution there's a github repo with",
    "start": "366970",
    "end": "376630"
  },
  {
    "text": "some scripts that will all share at the end of the slide that has various tools and automated things you can run against your redshift cluster to optimize it and",
    "start": "376630",
    "end": "386260"
  },
  {
    "text": "though so when choosing optimization sorry when choosing distribution key you",
    "start": "386260",
    "end": "393580"
  },
  {
    "text": "want to make sure it has high cardinality so that's something that's rather unique and the values are very uncommon so for example if you don't if",
    "start": "393580",
    "end": "400510"
  },
  {
    "text": "you can't find it distribution key that's a good fit you can try using even and what even distribution key does is",
    "start": "400510",
    "end": "406420"
  },
  {
    "text": "it distributes the data across all the nodes evenly basically in a round-robin style and then there's also another",
    "start": "406420",
    "end": "412720"
  },
  {
    "text": "option for smaller tables where you can use dist aisle all and if it's a small enough subset that will put it on the first slice of every compute node so it",
    "start": "412720",
    "end": "419680"
  },
  {
    "text": "doesn't have to go out and get that data to another node for something that's you know that you would query a bunch or",
    "start": "419680",
    "end": "425620"
  },
  {
    "text": "join next I'm going to talk about is",
    "start": "425620",
    "end": "431020"
  },
  {
    "start": "429000",
    "end": "478000"
  },
  {
    "text": "vark are columns some of the best practices we have there some people think that it's best practice to use the",
    "start": "431020",
    "end": "436840"
  },
  {
    "text": "maximum size just so you have to do anything later use the smallest possible size you can we compress data pretty",
    "start": "436840",
    "end": "443740"
  },
  {
    "text": "well in redshift so that you don't have to worry about the large size it has",
    "start": "443740",
    "end": "450490"
  },
  {
    "text": "minimal impact on the size of the data tables during processing for complex queries however intermediate queries",
    "start": "450490",
    "end": "455640"
  },
  {
    "text": "need to be stored in temporary tables or potentially on disks so if you have larger var card columns they're not",
    "start": "455640",
    "end": "462970"
  },
  {
    "text": "compressed so it can use that disk thus slowing down your queries so like I said you don't want to use the largest of our",
    "start": "462970",
    "end": "468550"
  },
  {
    "text": "card column you have you want to use what makes sense for you guys next we",
    "start": "468550",
    "end": "479290"
  },
  {
    "start": "478000",
    "end": "530000"
  },
  {
    "text": "have just bakes queries obviously disk is slower than memory so there's a script that I or a query that I've",
    "start": "479290",
    "end": "486040"
  },
  {
    "text": "written up there that will identify any of the queries that you have that are displace and obviously you want to",
    "start": "486040",
    "end": "491140"
  },
  {
    "text": "optimize those and make sure that either increase the memory in the wmq which I'll talk about a little later or maybe",
    "start": "491140",
    "end": "497890"
  },
  {
    "text": "it's just not an optimized query and you can take a look and see to optimize it there's also Q Simon rules that I just mentioned and then",
    "start": "497890",
    "end": "504970"
  },
  {
    "text": "there's dynamic memory allocation you can do so you can crease memory on specific cues for specific sessions if",
    "start": "504970",
    "end": "510039"
  },
  {
    "text": "you have a end of your report or end of a quarter report that's using more resources than necessary you can kind of",
    "start": "510039",
    "end": "516279"
  },
  {
    "text": "tweak that on the fly without having to restart the cluster you can also use new add new cues to wmq the only problem is",
    "start": "516279",
    "end": "522219"
  },
  {
    "text": "if you add a new cue you'll have to restart the cluster and then next we",
    "start": "522219",
    "end": "531880"
  },
  {
    "start": "530000",
    "end": "556000"
  },
  {
    "text": "have to commit queue usage so the create queue usage are expensive it there's a script that we have called",
    "start": "531880",
    "end": "539410"
  },
  {
    "text": "commit stat sequel you can run it and it'll show various queries and how long they've spent in the commit queue and",
    "start": "539410",
    "end": "545680"
  },
  {
    "text": "then you can analyze that and see what you need to do to optimize it in terms of its loading data too long or you're",
    "start": "545680",
    "end": "552430"
  },
  {
    "text": "pulling from the wrong data source and then this is just example of what the",
    "start": "552430",
    "end": "558550"
  },
  {
    "start": "556000",
    "end": "584000"
  },
  {
    "text": "the script looks like so you'll see this is just the the query that you see up",
    "start": "558550",
    "end": "564580"
  },
  {
    "text": "there it just shows the the commit time it's not necessarily the prettiest thing you can see you know which which queries",
    "start": "564580",
    "end": "571240"
  },
  {
    "text": "on which nodes are taking the longest and then kind of dig into what what's going on in those and next we'll talk",
    "start": "571240",
    "end": "579520"
  },
  {
    "text": "about the wmq so there's two queues",
    "start": "579520",
    "end": "585940"
  },
  {
    "start": "584000",
    "end": "639000"
  },
  {
    "text": "created by default there's a super user queue this is only when you want to run queries affect the system or for",
    "start": "585940",
    "end": "591520"
  },
  {
    "text": "troubleshooting purposes and then there's a default user the default user queue is initially query or there's neutrally configured for five",
    "start": "591520",
    "end": "599290"
  },
  {
    "text": "concurrency but you can add you can tweak that depending upon your workload I don't remember what we recommend but",
    "start": "599290",
    "end": "606160"
  },
  {
    "text": "it depends on your workload I think we had it one time said we didn't want to go over 24 concurrency but it just depends on your workload",
    "start": "606160",
    "end": "615029"
  },
  {
    "text": "and then again with the wmq you want identify short and long running queries to prioritize them obviously the shorter",
    "start": "621270",
    "end": "627400"
  },
  {
    "text": "ones you can schedule around the long running queries like I mentioned that the end of your reports the quarterly",
    "start": "627400",
    "end": "635020"
  },
  {
    "text": "reports that those types of things and then there's another script that I just",
    "start": "635020",
    "end": "642580"
  },
  {
    "start": "639000",
    "end": "662000"
  },
  {
    "text": "mentioned to tune peak concurrency so it's a script that you can see to make sure your queues are configured properly",
    "start": "642580",
    "end": "647860"
  },
  {
    "text": "where you're not running into collision and waiting for stuff to queue up now I'm it's it's listed here and I have another link at the end of the the",
    "start": "647860",
    "end": "653830"
  },
  {
    "text": "slides to show we can get the the files and the tools to kind of look at the queues to make sure you've scheduled",
    "start": "653830",
    "end": "659920"
  },
  {
    "text": "everything properly and setup the queues properly and then this is just an example of a wmq and how it works through the process so if a user's",
    "start": "659920",
    "end": "667930"
  },
  {
    "start": "662000",
    "end": "705000"
  },
  {
    "text": "logged in as a super user and runs a query the Cordy group labels super user the quarry is obviously assigned to the super user queue and then if it's user",
    "start": "667930",
    "end": "678130"
  },
  {
    "text": "if the user is assigned to a listed user group it's assigned to the the query the",
    "start": "678130",
    "end": "685330"
  },
  {
    "text": "the queue that this assigned to that it's assigned to that user group and then if nothing if they it doesn't match anything it just calls to just go to",
    "start": "685330",
    "end": "691660"
  },
  {
    "text": "that default queue and it's executed across the the right ship cluster",
    "start": "691660",
    "end": "697680"
  },
  {
    "start": "705000",
    "end": "813000"
  },
  {
    "text": "next we have table maintenance so with table maintenance you know like other",
    "start": "705870",
    "end": "712769"
  },
  {
    "text": "day justice requires statistics about tables in composition of data blocks being stored in order to make good",
    "start": "712769",
    "end": "718980"
  },
  {
    "text": "decisions so we recommend depends on your workload analyzed to get those",
    "start": "718980",
    "end": "724290"
  },
  {
    "text": "statistics updated and to show to so that the lead the leader node can optimize those queries for you across",
    "start": "724290",
    "end": "730439"
  },
  {
    "text": "the the compute nodes and then you also want to run analyze when loading popular columns and then vacuum is another one",
    "start": "730439",
    "end": "737550"
  },
  {
    "text": "so when you delete rows or add rows you there's unnecessary space that's there",
    "start": "737550",
    "end": "743059"
  },
  {
    "text": "so you want to use vacuum to kind of reclaim that space so if you delete like",
    "start": "743059",
    "end": "749069"
  },
  {
    "text": "I mentioned if you delete rows it doesn't really delete them there's empty space in there so that's just empty space you can reclaim using the vacuum",
    "start": "749069",
    "end": "754949"
  },
  {
    "text": "and again vacuum tables there's a script that's there that will based on threshold you can configure to say if",
    "start": "754949",
    "end": "761220"
  },
  {
    "text": "it's over a certain percentage to to run that to run a vacuum against it you want",
    "start": "761220",
    "end": "766709"
  },
  {
    "text": "to run it off hours because that could potentially cause issues if it's a long-running vacuumed query and again",
    "start": "766709",
    "end": "773339"
  },
  {
    "text": "it's these are all just kind of common things I've seen customers use it's it depends upon what you wanted what your",
    "start": "773339",
    "end": "778439"
  },
  {
    "text": "workload looks like to kind of really tune the vacuum in some of the maintenance stuff you know off hours make sure it doesn't collide in",
    "start": "778439",
    "end": "784259"
  },
  {
    "text": "maintance window to make sure it's not blocking other queries from running and",
    "start": "784259",
    "end": "792449"
  },
  {
    "text": "with that I'll turn it over on Oh pom for the demo",
    "start": "792449",
    "end": "797480"
  },
  {
    "text": "thanks Mike yep hi everyone",
    "start": "808430",
    "end": "814490"
  },
  {
    "text": "so let's great recommendations by Mike so let's put some of those things into practice a lot of things which you might",
    "start": "814490",
    "end": "821839"
  },
  {
    "text": "talked about are how to optimize your redshift cluster but last year we announced a new product called redshift",
    "start": "821839",
    "end": "827839"
  },
  {
    "text": "spectrum how many of you are aware of redshift spectrum just raise your hands oh great how many of you use it in your work or",
    "start": "827839",
    "end": "834790"
  },
  {
    "text": "probably one or two okay awesome how many of you use glue here three or four",
    "start": "834790",
    "end": "841610"
  },
  {
    "text": "yeah okay so so what I'm gonna show today is how redshift spectrum can be",
    "start": "841610",
    "end": "846920"
  },
  {
    "text": "used to combine the power of redshift with the freeform queries which you can",
    "start": "846920",
    "end": "852200"
  },
  {
    "text": "do on s3 so traditionally the way data warehousing world works is you have some",
    "start": "852200",
    "end": "858050"
  },
  {
    "text": "data coming into either your s3 or your data bases through various import jobs",
    "start": "858050",
    "end": "863959"
  },
  {
    "text": "and then they goes into there's an ETL pipeline which puts it into redshift and that's where you query and there were",
    "start": "863959",
    "end": "871010"
  },
  {
    "text": "two parallel paths earlier like redshift was one path and querying s3 was one but with spectrum both of them combined",
    "start": "871010",
    "end": "877130"
  },
  {
    "text": "together and you can query or s3 files from redshift as if the data was stored",
    "start": "877130",
    "end": "882860"
  },
  {
    "text": "in redshift even though it is not stored so you're directly sending your queries to s3 and I'm going to show you how that",
    "start": "882860",
    "end": "889130"
  },
  {
    "text": "works and also share some best practices when you work on a use case like that",
    "start": "889130",
    "end": "894440"
  },
  {
    "text": "how can you optimize those use cases so let's let's jump into that",
    "start": "894440",
    "end": "899470"
  },
  {
    "start": "896000",
    "end": "1069000"
  },
  {
    "text": "awesome so I'm gonna use my browser cool",
    "start": "907130",
    "end": "914000"
  },
  {
    "text": "so first thing so what I want to do is I want to start with the redshift cluster",
    "start": "917360",
    "end": "925470"
  },
  {
    "text": "and show you what are the different things we can do with spectrum and how do we start using spectrums so let's put",
    "start": "925470",
    "end": "931410"
  },
  {
    "text": "that to use so first I am logged into my AWS console I've already created a few",
    "start": "931410",
    "end": "937020"
  },
  {
    "text": "databases so I'll use one which I have not used before I create a bunch of test databases so I'll probably choose 501 to",
    "start": "937020",
    "end": "942840"
  },
  {
    "text": "start with and then I'll take the endpoint and try to login into this or",
    "start": "942840",
    "end": "949740"
  },
  {
    "text": "connect to this",
    "start": "949740",
    "end": "952370"
  },
  {
    "text": "okay so I just I already have PC coal here so which allows me to send queries",
    "start": "960470",
    "end": "967700"
  },
  {
    "text": "from my console to okay hope this is the visible yeah I think anybody not should",
    "start": "967700",
    "end": "975650"
  },
  {
    "text": "I increase the font size are pretty good pretty good awesome so let's let's connect to this database okay so we have",
    "start": "975650",
    "end": "984500"
  },
  {
    "text": "in our new redshift 501 database now our data warehouse what we want to do is",
    "start": "984500",
    "end": "990830"
  },
  {
    "text": "first thing I want to check what version it is so so I created a list of scripts which will be which I will be running",
    "start": "990830",
    "end": "995840"
  },
  {
    "text": "and also also explaining you what each one of them is doing and by the end of this exercise you will know how redshift",
    "start": "995840",
    "end": "1003010"
  },
  {
    "text": "and spectrum work together how you can create automated pipelines which create",
    "start": "1003010",
    "end": "1008710"
  },
  {
    "text": "pocket files how do you compress files so if you want to like just take the gist of it",
    "start": "1008710",
    "end": "1014440"
  },
  {
    "text": "they have three things which you want to do when you are dealing with files which are being queried so you're talking",
    "start": "1014440",
    "end": "1019900"
  },
  {
    "text": "about s3 files one is you want to compress the data second one is you want to have a columnar storage so there are",
    "start": "1019900",
    "end": "1027160"
  },
  {
    "text": "a lot of formats we are going to use power K for today and then the third piece is making sure you keep your data",
    "start": "1027160",
    "end": "1033670"
  },
  {
    "text": "partition so you don't scan the whole storage but those are the three things I'm gonna show all three of them step by",
    "start": "1033670",
    "end": "1039970"
  },
  {
    "text": "step and hopefully you will make some good use of that so so first we have",
    "start": "1039970",
    "end": "1045160"
  },
  {
    "text": "checked the version I spectrum is supported only above a certain version so it's a 1-point let me increase the",
    "start": "1045160",
    "end": "1052960"
  },
  {
    "text": "font size of this as well okay awesome so so one point one two Oh point one",
    "start": "1052960",
    "end": "1058870"
  },
  {
    "text": "point zero point one two nine four if you're if you upgrade your and shift to about this then you can use spectrum",
    "start": "1058870",
    "end": "1064180"
  },
  {
    "text": "otherwise it's not allowed so so with that being clear let's let's go to the",
    "start": "1064180",
    "end": "1070120"
  },
  {
    "start": "1069000",
    "end": "1146000"
  },
  {
    "text": "sample data set which I have here so and",
    "start": "1070120",
    "end": "1075910"
  },
  {
    "text": "increase the font size here as well okay let's make it a little smaller cool so I",
    "start": "1075910",
    "end": "1084310"
  },
  {
    "text": "will make it so what I'm doing is I'm",
    "start": "1084310",
    "end": "1089320"
  },
  {
    "text": "going to a s3 bucket and checking what data I have in this folder there is a folder ticket spectrum sales so there's some",
    "start": "1089320",
    "end": "1096820"
  },
  {
    "text": "sample data which a we have is called sales data I've already downloaded this file so what I'm going to show is how",
    "start": "1096820",
    "end": "1102820"
  },
  {
    "text": "this file looks like I'll just look at some of the top records of this file",
    "start": "1102820",
    "end": "1109140"
  },
  {
    "text": "okay so if you look at it they're a bunch of numbers here like it's hard to",
    "start": "1112470",
    "end": "1117760"
  },
  {
    "text": "make a sense of what these numbers mean but you can assume it's a tab-separated file with a bunch of columns and each",
    "start": "1117760",
    "end": "1124690"
  },
  {
    "text": "column probably has some meaning so so given given this data in s3 what we want",
    "start": "1124690",
    "end": "1130990"
  },
  {
    "text": "to do is we want to query this data using redshift this so this is this is the data sitting in s3 how do we query",
    "start": "1130990",
    "end": "1136030"
  },
  {
    "text": "this file you can you can have a CSV instead of this you can have a log data which is generated by your Apache logs",
    "start": "1136030",
    "end": "1142450"
  },
  {
    "text": "or something else so so next we got what we're going to do is we're gonna create",
    "start": "1142450",
    "end": "1147610"
  },
  {
    "start": "1146000",
    "end": "1199000"
  },
  {
    "text": "a schema so apologies for having underscore five here I was doing a lot of experiments and I keep kept changing",
    "start": "1147610",
    "end": "1154720"
  },
  {
    "text": "the number so we reached number five I hope my demo will work so let's start",
    "start": "1154720",
    "end": "1159970"
  },
  {
    "text": "with creating a new schema so my schema got created says create schema my",
    "start": "1159970",
    "end": "1167170"
  },
  {
    "text": "successful awesome so so once the so as you can see here I'm creating a database name spectrum db5 and this the data the",
    "start": "1167170",
    "end": "1176260"
  },
  {
    "text": "schema name is spectrum five and then I have a specific role which I have created here this role has access to s3",
    "start": "1176260",
    "end": "1183250"
  },
  {
    "text": "as well as redshifts because this red ship now needs to go to my s3 bucket as well so this this has both the things",
    "start": "1183250",
    "end": "1190990"
  },
  {
    "text": "English in the interest of time I'm not showing how to create this row but if people have questions I will show it now",
    "start": "1190990",
    "end": "1197500"
  },
  {
    "text": "let's let's move to the next step once we have the schema let's create a table from the data which we just checked so I",
    "start": "1197500",
    "end": "1204100"
  },
  {
    "start": "1199000",
    "end": "1290000"
  },
  {
    "text": "showed you some of the file which we have in s3 so let me create this table here so so just just go let's quickly go",
    "start": "1204100",
    "end": "1212260"
  },
  {
    "text": "through this statement so what we are saying is create external table external table means the file which is not on",
    "start": "1212260",
    "end": "1218770"
  },
  {
    "text": "redshift that will assume as if the fire the table exists locally but the table data actually is still in s3",
    "start": "1218770",
    "end": "1225730"
  },
  {
    "text": "there's no happening here nothing is being transferred it is just a logical statement saying from now on whenever",
    "start": "1225730",
    "end": "1233740"
  },
  {
    "text": "somebody refers to spectrum underscore v dot sales this is the schema which we",
    "start": "1233740",
    "end": "1240070"
  },
  {
    "text": "need to comply with and the data is stored here so so the so there's a",
    "start": "1240070",
    "end": "1245590"
  },
  {
    "text": "pointer to this file and there's a there's a catalog which we create for this so so now what we have done is we",
    "start": "1245590",
    "end": "1252820"
  },
  {
    "text": "created an external table external table is a text file it's a tab separated file and then it's",
    "start": "1252820",
    "end": "1259960"
  },
  {
    "text": "located here so now now let's go to the next step so now that we have created a table let's do a simple count star query",
    "start": "1259960",
    "end": "1266080"
  },
  {
    "text": "just to see how many records we have so that was it so when I I'm firing a count",
    "start": "1266080",
    "end": "1272200"
  },
  {
    "text": "star what it is doing is it is going to my node and this node is sending the",
    "start": "1272200",
    "end": "1277480"
  },
  {
    "text": "query to s3 it's checking which file to download what to do with the file and then sending the records back so it's",
    "start": "1277480",
    "end": "1284740"
  },
  {
    "text": "everything which I'm doing here is going to s3 and coming where s3 now let's do",
    "start": "1284740",
    "end": "1290650"
  },
  {
    "start": "1290000",
    "end": "1387000"
  },
  {
    "text": "one more statement a little bit just to see what kind of data we have here just",
    "start": "1290650",
    "end": "1298120"
  },
  {
    "text": "showing the top 10 records so so there's",
    "start": "1298120",
    "end": "1303220"
  },
  {
    "text": "a sales idealist ID seller ID and a bunch of things and because these are",
    "start": "1303220",
    "end": "1309880"
  },
  {
    "text": "the names we gave we said that the first value which you see in the file its sales ID the second value which you see",
    "start": "1309880",
    "end": "1315610"
  },
  {
    "text": "in the file is list ID so so we created a schema and everything in the file now",
    "start": "1315610",
    "end": "1321610"
  },
  {
    "text": "aligns with our schema so so far so good so we have our scheme already we have",
    "start": "1321610",
    "end": "1327460"
  },
  {
    "text": "our table ready now one of the things we can do here is let's say when we are doing this if you do explain plan of",
    "start": "1327460",
    "end": "1333730"
  },
  {
    "text": "this you can see it's it's gonna do I'm",
    "start": "1333730",
    "end": "1340540"
  },
  {
    "text": "highlighting this piece s3 query scan so it's going to actually it's gonna do a scan there and then it's telling which",
    "start": "1340540",
    "end": "1347950"
  },
  {
    "text": "file it will go how many potential rows it's gonna scan so it's telling you that this query goes to s3 and what",
    "start": "1347950",
    "end": "1353710"
  },
  {
    "text": "operations will be done on s3 now now let's go to the next one so so so far we",
    "start": "1353710",
    "end": "1359140"
  },
  {
    "text": "have created a file or created a table which is a spectrum table it is going to s3 it",
    "start": "1359140",
    "end": "1365650"
  },
  {
    "text": "allows us to query the data from s3 without anything being done on redshift side now let's create a normal s3",
    "start": "1365650",
    "end": "1371260"
  },
  {
    "text": "redshift table so I I I hope everybody here has probably created a table some",
    "start": "1371260",
    "end": "1377110"
  },
  {
    "text": "other times so it's pretty simple I'm calling it an event table and they're a",
    "start": "1377110",
    "end": "1382600"
  },
  {
    "text": "bunch of IDs here event ID venue ID start time and few other things so once",
    "start": "1382600",
    "end": "1388059"
  },
  {
    "start": "1387000",
    "end": "1478000"
  },
  {
    "text": "we create this we're gonna check what this file is because I'm gonna copy data",
    "start": "1388059",
    "end": "1396970"
  },
  {
    "text": "from this file into this so it's a standard red shift copy all of us who",
    "start": "1396970",
    "end": "1402010"
  },
  {
    "text": "done it some other time so this is s3 file if I want to do check what kind of things it has all events",
    "start": "1402010",
    "end": "1409530"
  },
  {
    "text": "cool so all events is a file which is or",
    "start": "1409530",
    "end": "1415740"
  },
  {
    "text": "which which which also has bunch of values separated by a delimiter and there's no header here so what we will",
    "start": "1415740",
    "end": "1422710"
  },
  {
    "text": "do is we'll load this data into the new table which we just created so these are events so just to give you the context",
    "start": "1422710",
    "end": "1428140"
  },
  {
    "text": "about data the data which we have in s3 is called sales so all the sales which have happened are coming into s3 let's",
    "start": "1428140",
    "end": "1435100"
  },
  {
    "text": "assume there are multiple systems which are pushing the sales data into s3 now we have created a table called event",
    "start": "1435100",
    "end": "1440980"
  },
  {
    "text": "event is just a metadata or a catalog data so it's telling us these are the potential events we have one of the best",
    "start": "1440980",
    "end": "1446590"
  },
  {
    "text": "practices here is whenever your data is just a dimension table it's good to just",
    "start": "1446590",
    "end": "1452710"
  },
  {
    "text": "keep it here like because it's not too much data so keep your dimension tables",
    "start": "1452710",
    "end": "1458260"
  },
  {
    "text": "here and if it's fact table you can put it on s3 some people also do something",
    "start": "1458260",
    "end": "1463990"
  },
  {
    "text": "called rotate old data into s3 so if you're redshift cluster becomes too big and you know that old data is not",
    "start": "1463990",
    "end": "1469660"
  },
  {
    "text": "queried as much as I want you can retire your old data into s3 but it is still queryable so I'm gonna show that piece",
    "start": "1469660",
    "end": "1476320"
  },
  {
    "text": "also but let's let's quickly get to this redshift piece so we created a table Parliament now we're going to load this",
    "start": "1476320",
    "end": "1482740"
  },
  {
    "start": "1478000",
    "end": "1620000"
  },
  {
    "text": "table with the data which I just showed you so it's a copy command what we are",
    "start": "1482740",
    "end": "1487840"
  },
  {
    "text": "saying is copy data from this place in redshift in s3",
    "start": "1487840",
    "end": "1493090"
  },
  {
    "text": "and we are telling the time format we are telling some region where this data exists it's free like 10 rows of data so",
    "start": "1493090",
    "end": "1500020"
  },
  {
    "text": "I think it will do it quickly let's see and by the time it does it see",
    "start": "1500020",
    "end": "1511230"
  },
  {
    "text": "maybe I'll just okay so awesome so there",
    "start": "1511230",
    "end": "1524530"
  },
  {
    "text": "about eight thousand seven hundred ninety eight records loaded so there are a lot of events here and then which are",
    "start": "1524530",
    "end": "1532000"
  },
  {
    "text": "loaded into redshift so this is a local table even though the data was in s3 we copied all the data into redshift so so",
    "start": "1532000",
    "end": "1538330"
  },
  {
    "text": "far we have two use cases sales data was in s3 it is still in s3 but it is queryable by redshift now we have events",
    "start": "1538330",
    "end": "1545650"
  },
  {
    "text": "data it was in s3 but we have copied into redshift because redshift queries are generally faster than the spectrum",
    "start": "1545650",
    "end": "1552160"
  },
  {
    "text": "queries so so we have done those two things now let's say we want to combine those two together like we have some",
    "start": "1552160",
    "end": "1558040"
  },
  {
    "text": "data in red shape we have some data in s3 how do we combine them together like one of the use cases which I see very",
    "start": "1558040",
    "end": "1563200"
  },
  {
    "text": "commonly is a lot of companies get their access logs so how people are using their websites like which IP addresses",
    "start": "1563200",
    "end": "1569500"
  },
  {
    "text": "are using it which browser people are using things like that and that generally comes as a log format Apache",
    "start": "1569500",
    "end": "1576610"
  },
  {
    "text": "log or some standard comma-separated format and a lot of companies want to use that data along with the other",
    "start": "1576610",
    "end": "1583390"
  },
  {
    "text": "analytical data they have you they want to see for a specific geography what was about how many people were browsing my",
    "start": "1583390",
    "end": "1590560"
  },
  {
    "text": "website combine it with a lot of other data you might have in red ship so you can do like dynamic queries so now we",
    "start": "1590560",
    "end": "1598180"
  },
  {
    "text": "have recreated that use case we have events here and we have sales there now let's let's just run a join query so",
    "start": "1598180",
    "end": "1604090"
  },
  {
    "text": "what I'm gonna do next is I'm gonna so before we run this query let's let's understand what this query is doing we",
    "start": "1604090",
    "end": "1610720"
  },
  {
    "text": "are saying select top 10 spectrum dot sales dot event ID comma sum of sales",
    "start": "1610720",
    "end": "1617140"
  },
  {
    "text": "dot price field so what we are doing is we are saying tell me the top 10 events where the",
    "start": "1617140",
    "end": "1624310"
  },
  {
    "start": "1620000",
    "end": "1704000"
  },
  {
    "text": "maximum sales happened so it's gonna tell us the most successful events to us and event data",
    "start": "1624310",
    "end": "1630830"
  },
  {
    "text": "is in a redshift and sales data is in s3 so and we are joining them as if they",
    "start": "1630830",
    "end": "1636260"
  },
  {
    "text": "are both local so we are saying sales dot event ID is equal to n dot CSID and price paid greater than 30 so we are",
    "start": "1636260",
    "end": "1642290"
  },
  {
    "text": "interested only in events or sales where at least 30 dollars were paid to us and",
    "start": "1642290",
    "end": "1649720"
  },
  {
    "text": "let's let's run it so we got our top 10",
    "start": "1649720",
    "end": "1660650"
  },
  {
    "text": "events where things happen and as you can see it's pretty quick depending on how you store I think your",
    "start": "1660650",
    "end": "1667730"
  },
  {
    "text": "query preference performance will improve or degrade so far we have not done much performance things we have",
    "start": "1667730",
    "end": "1674300"
  },
  {
    "text": "data in s3 we are just using it your data in redshift we have used it we are using it now let's jump into the performance side how do we so far it",
    "start": "1674300",
    "end": "1681710"
  },
  {
    "text": "works but how do we make it more scalable more optimized so so what can we do in this use case when you have",
    "start": "1681710",
    "end": "1687190"
  },
  {
    "text": "exabytes of data so the redshift spectrum can support very very large",
    "start": "1687190",
    "end": "1692300"
  },
  {
    "text": "files and a huge number of files so we we can do a lot of optimizations to",
    "start": "1692300",
    "end": "1697370"
  },
  {
    "text": "improve the performance of a spectrum so let's go through the performance things I talked about one by one so first thing",
    "start": "1697370",
    "end": "1703760"
  },
  {
    "text": "we're gonna start with is partitions before I jump into partitions let me just quickly show the explained plan of",
    "start": "1703760",
    "end": "1709910"
  },
  {
    "start": "1704000",
    "end": "1819000"
  },
  {
    "text": "this so you you'll see it very similar to the explained plan which I saw before so let's not look into everything but I",
    "start": "1709910",
    "end": "1716360"
  },
  {
    "text": "just wanted to show you it's doing a hash join and joining using s3 queries",
    "start": "1716360",
    "end": "1721550"
  },
  {
    "text": "cancer's again going to the sales is doing a s3 query scan is doing a hash aggregate in s3",
    "start": "1721550",
    "end": "1726860"
  },
  {
    "text": "it's telling how many approximate rows it's gonna scan and it will do s3",
    "start": "1726860",
    "end": "1733370"
  },
  {
    "text": "sequence scan so it's just to get an idea now what we gonna do is we gonna",
    "start": "1733370",
    "end": "1740320"
  },
  {
    "text": "create a similar use case but with partitioned data so let's say if we have partitions how many of you use",
    "start": "1740320",
    "end": "1747890"
  },
  {
    "text": "partitions already raise your hands if you use okay so it seems there are a lot of people who have not used it so so the",
    "start": "1747890",
    "end": "1753980"
  },
  {
    "text": "way partitions work is generally you like in our real-world use cases we have",
    "start": "1753980",
    "end": "1759830"
  },
  {
    "text": "lot of data it could be terabytes of data which are stored in file if you create a single file which has all the",
    "start": "1759830",
    "end": "1765919"
  },
  {
    "text": "data stored in it and you query through redshift redshift what red will do is first it will download the file parse",
    "start": "1765919",
    "end": "1772549"
  },
  {
    "text": "it and then try to make some decisions so the downloading itself will take a lot of time in a lot of cases we don't",
    "start": "1772549",
    "end": "1777710"
  },
  {
    "text": "need to download the full file especially for examples where you want to are interested only on a specific date you are saying hey tell me what",
    "start": "1777710",
    "end": "1784490"
  },
  {
    "text": "happened yesterday you don't want to download the full year if you want to are interested only in yesterday so a",
    "start": "1784490",
    "end": "1789860"
  },
  {
    "text": "lot of lot of people partition by date they will say they have the first partition by year then month then date",
    "start": "1789860",
    "end": "1797000"
  },
  {
    "text": "and then within those partitions you have different files so redshift will intelligently identify what files I",
    "start": "1797000",
    "end": "1804140"
  },
  {
    "text": "should download and then it will minimize the data sets so that's where partitions are useful so now let me show",
    "start": "1804140",
    "end": "1809630"
  },
  {
    "text": "you how partitions can be implemented so if you have large number of files in s3 or you can so you can improve the",
    "start": "1809630",
    "end": "1816320"
  },
  {
    "text": "performance of the use cases like this so let's first look at the data so",
    "start": "1816320",
    "end": "1821480"
  },
  {
    "text": "there's another data set which I have first I'm showing you the data so I just",
    "start": "1821480",
    "end": "1827030"
  },
  {
    "text": "did our three lists and I'm going to a location called spectrum sales partition",
    "start": "1827030",
    "end": "1832309"
  },
  {
    "text": "so the same data but it's partitioned this time so let me show you a gist of how this data looks like so as you can",
    "start": "1832309",
    "end": "1838130"
  },
  {
    "text": "see here the first first value or the name of the folder which we have is say",
    "start": "1838130",
    "end": "1844490"
  },
  {
    "text": "is rate equal to 2008 - 1 so this is the first partition key it's saying all the",
    "start": "1844490",
    "end": "1849590"
  },
  {
    "text": "data corresponding to the first of or January of 2008 is gonna be in this",
    "start": "1849590",
    "end": "1855320"
  },
  {
    "text": "folder all the data for February of 2008 is gonna be in this folder so let's look into one of those folders first so I'll",
    "start": "1855320",
    "end": "1861740"
  },
  {
    "text": "say that's great like you have done a good job segregating this but let's look",
    "start": "1861740",
    "end": "1867530"
  },
  {
    "text": "into what is inside one of them so so within each month we have 6 files so it",
    "start": "1867530",
    "end": "1872750"
  },
  {
    "text": "doesn't need to be 6 it could be any number maybe you have daily level files maybe you have files generated every 5",
    "start": "1872750",
    "end": "1878330"
  },
  {
    "text": "minutes it doesn't matter but you have some files and you want to see and let's",
    "start": "1878330",
    "end": "1884900"
  },
  {
    "text": "look into one of the files so we understand what kind of structure they have so I'll do ahead zero zero zero I okay cool",
    "start": "1884900",
    "end": "1893150"
  },
  {
    "text": "so I had already downloaded this file but this looks very similar to the file we saw earlier it's again a delimited",
    "start": "1893150",
    "end": "1899360"
  },
  {
    "text": "file and lot of columns here so structurally it is same just that the data was splitted into multiple files",
    "start": "1899360",
    "end": "1905660"
  },
  {
    "text": "and those files are stored in a way such that we know which for a given date",
    "start": "1905660",
    "end": "1912320"
  },
  {
    "text": "where are the files for that date so that's that's the simple partition here now let's move to the next step so once",
    "start": "1912320",
    "end": "1922040"
  },
  {
    "text": "we have the we know that the partitions let's let's go to the next step so what we gonna do is we're gonna create",
    "start": "1922040",
    "end": "1927170"
  },
  {
    "text": "another table which is very similar to the first table we created but let's look at the difference so we are again",
    "start": "1927170",
    "end": "1932780"
  },
  {
    "text": "saying create external table we are telling the name of the table we are defining the schema and then finally we",
    "start": "1932780",
    "end": "1938780"
  },
  {
    "text": "are saying fields are terminated what is the delimiter stored as text file and then this is where my",
    "start": "1938780",
    "end": "1945650"
  },
  {
    "text": "partitioned data is and table properties here so so one of our unique things here",
    "start": "1945650",
    "end": "1951830"
  },
  {
    "text": "you which is important to notice is partitioned by so when we create a table we tell the table hey table your",
    "start": "1951830",
    "end": "1958160"
  },
  {
    "text": "partition by this specific column and it it could be partition by multiple",
    "start": "1958160",
    "end": "1963830"
  },
  {
    "text": "columns because you can have maybe different product lines and within those product lines you have different dates",
    "start": "1963830",
    "end": "1969020"
  },
  {
    "text": "so you can say first product number will come and then the date will come or it could be a lot of hierarchical",
    "start": "1969020",
    "end": "1975070"
  },
  {
    "text": "information but partition by is one of the columns so you just say which",
    "start": "1975070",
    "end": "1980420"
  },
  {
    "text": "problem it is partition or it could be multiple columns so so let's run this so it created very quick it's the same same",
    "start": "1980420",
    "end": "1987530"
  },
  {
    "text": "concept applies here this query doesn't do anything it's just telling it hey the data is stored here and this is the",
    "start": "1987530",
    "end": "1994630"
  },
  {
    "text": "metadata which we expect or we call it data catalog so this is the catalog and the data stores here so now now we have",
    "start": "1994630",
    "end": "2001630"
  },
  {
    "text": "very similar table as before just that the raw data is partitioned here so let's let's run the similar query here",
    "start": "2001630",
    "end": "2007510"
  },
  {
    "text": "right now when we created this it is not loaded data so as you if you remember",
    "start": "2007510",
    "end": "2014230"
  },
  {
    "text": "when we went to this location as 3 this it has it had multiple folders so by default it has not added",
    "start": "2014230",
    "end": "2020620"
  },
  {
    "text": "any partitions into it so we will need to add it so what we will do is we will",
    "start": "2020620",
    "end": "2026350"
  },
  {
    "text": "say hey these are the dates you should be aware of so what people do is every",
    "start": "2026350",
    "end": "2031840"
  },
  {
    "text": "night or every day or every 5-minute whatever the cadence is every time you see a new data file come or every time",
    "start": "2031840",
    "end": "2038230"
  },
  {
    "text": "there's a logical interval of partition you just add a new partition so it knows",
    "start": "2038230",
    "end": "2043510"
  },
  {
    "text": "that this is the partition I need to scale it scan you can also do it automatically I think for for this",
    "start": "2043510",
    "end": "2050020"
  },
  {
    "text": "example what I want to do is I will add all the partitions we had and I'll just",
    "start": "2050020",
    "end": "2056770"
  },
  {
    "text": "jump into one of those statements so we know what what it is doing so so let's look okay so it's running some of them",
    "start": "2056770",
    "end": "2062888"
  },
  {
    "text": "but let it finish and then I'll so what it is doing is let me show you ok cool",
    "start": "2062889",
    "end": "2072970"
  },
  {
    "text": "so it's finished so we I had like repetition of statements like this multiple times but let's jump into let's",
    "start": "2072970",
    "end": "2079510"
  },
  {
    "text": "look into the highlighted section I'll I probably highlight one other one let's let's highlight this one so what I am",
    "start": "2079510",
    "end": "2085060"
  },
  {
    "text": "saying is alter table the sales underscore part so we just renamed the table to or create a new table called",
    "start": "2085060",
    "end": "2090790"
  },
  {
    "text": "sales underscore part part means partition and then we are adding our partition sales data equal to 10 so we",
    "start": "2090790",
    "end": "2099280"
  },
  {
    "text": "are saying October 2008 and then we are saying the location of this partition is",
    "start": "2099280",
    "end": "2104500"
  },
  {
    "text": "here we are just telling the location and after that it knows that this is the data it should be aware of so so we just",
    "start": "2104500",
    "end": "2111280"
  },
  {
    "text": "made our partition type table aware of all the partitions which it has so let's",
    "start": "2111280",
    "end": "2117910"
  },
  {
    "text": "do the next thing here what we want to do is actually we can do some queries here for example we can say just to keep",
    "start": "2117910",
    "end": "2124030"
  },
  {
    "text": "it simple select star from this table which we just created",
    "start": "2124030",
    "end": "2130170"
  },
  {
    "text": "oops I think I loaded it again let's close it okay go back",
    "start": "2130170",
    "end": "2137970"
  },
  {
    "text": "ok let's login again [Music]",
    "start": "2139200",
    "end": "2144260"
  },
  {
    "text": "awesome so we are again in the other terminal I wanted to do a select star",
    "start": "2144620",
    "end": "2149880"
  },
  {
    "text": "from the file the table which we just created okay and let's do some limit ten",
    "start": "2149880",
    "end": "2159510"
  },
  {
    "text": "so we'll just see the top ten and what it is gonna do is it's going to the partitions checking what kind of file it",
    "start": "2159510",
    "end": "2166320"
  },
  {
    "text": "has so so it's giving us a consistent view across the all the partitions so it's we can we can apply like partition",
    "start": "2166320",
    "end": "2173970"
  },
  {
    "text": "filters and as you would imagine those queries will be much faster than our normal queries because if if you say",
    "start": "2173970",
    "end": "2179430"
  },
  {
    "text": "where did the partition which we had here was a sale date and and if if that",
    "start": "2179430",
    "end": "2186600"
  },
  {
    "text": "filter is applied the query will be much faster because it's gonna look at very small set of data but if the filter is",
    "start": "2186600",
    "end": "2192240"
  },
  {
    "text": "something outside of those partitions it's gonna look at all the data and",
    "start": "2192240",
    "end": "2197520"
  },
  {
    "text": "think think think in terms of what what different filters are applied and try to reduce the set of data so let's so I",
    "start": "2197520",
    "end": "2206490"
  },
  {
    "text": "just ran this query now now one of the things we can do to understand how our partitioned tables is performing is we",
    "start": "2206490",
    "end": "2214410"
  },
  {
    "start": "2213000",
    "end": "2261000"
  },
  {
    "text": "can create an external we can there's a table which where we store this it's called svv external partition so so",
    "start": "2214410",
    "end": "2222360"
  },
  {
    "text": "let's let me run this query and then I'll show you what this query is doing so let's scroll down so there are a few",
    "start": "2222360",
    "end": "2229740"
  },
  {
    "text": "things I am doing saying select schema name table name values location from svv",
    "start": "2229740",
    "end": "2235800"
  },
  {
    "text": "external partitions very schema name equal to spectrum underscore five so this is the schema I just created it's",
    "start": "2235800",
    "end": "2241110"
  },
  {
    "text": "telling us what partitions this table is aware of right now so it's telling for a",
    "start": "2241110",
    "end": "2246510"
  },
  {
    "text": "given table at a given time you can ask what partitions exist so it's telling us it probably has some eight or nine",
    "start": "2246510",
    "end": "2252810"
  },
  {
    "text": "partitions actually it's yeah it's I think twelve partitions and Stelling where each of the partition data resides",
    "start": "2252810",
    "end": "2259710"
  },
  {
    "text": "so so that's that's good to know now we can do a query just to see how",
    "start": "2259710",
    "end": "2265800"
  },
  {
    "start": "2261000",
    "end": "2303000"
  },
  {
    "text": "this table is performing okay let's do",
    "start": "2265800",
    "end": "2271340"
  },
  {
    "text": "query here so so in this case we are we did a similar query again",
    "start": "2271340",
    "end": "2277360"
  },
  {
    "text": "tell me top ten customers where the sale was greater than $30 and tell me the top",
    "start": "2277360",
    "end": "2284380"
  },
  {
    "text": "ten events I mean I'm not running timer here but if you have a use case you can imagine this query will be much faster",
    "start": "2284380",
    "end": "2289960"
  },
  {
    "text": "than another if you're hitting a partition in this case we are not but if you apply aware where a specific",
    "start": "2289960",
    "end": "2295870"
  },
  {
    "text": "partition is being hit the partitioned data queries will be much faster than others and cool so I think one more",
    "start": "2295870",
    "end": "2304810"
  },
  {
    "text": "thing to understand here there's another table which stores how how s3 is",
    "start": "2304810",
    "end": "2311350"
  },
  {
    "text": "performing for all the queries you are doing as you become a more more advanced spectrum user you would want to see how",
    "start": "2311350",
    "end": "2318850"
  },
  {
    "text": "spectrum is performing like how many time I am hitting s3 what kind of performance issues I'm having so so it's",
    "start": "2318850",
    "end": "2324940"
  },
  {
    "text": "telling us for each query how much time s3 took so it has like bunch of things",
    "start": "2324940",
    "end": "2330340"
  },
  {
    "text": "things like how much time was elapsed in s3 what was the table name how many rows",
    "start": "2330340",
    "end": "2335350"
  },
  {
    "text": "did we scan in s3 what how many scan bytes we had one thing to remember is it is spectrum is an on-demand service",
    "start": "2335350",
    "end": "2342820"
  },
  {
    "text": "which means you don't pay any fees if you don't run a query you pay only if you run a query and the way query is",
    "start": "2342820",
    "end": "2349330"
  },
  {
    "text": "charged is how much data it is scanning so it's good to when you have a use case which is repeating it's good to check",
    "start": "2349330",
    "end": "2354760"
  },
  {
    "text": "how much data are we scanning based on the how much data you scan your price will vary and and it's telling us how",
    "start": "2354760",
    "end": "2362470"
  },
  {
    "text": "many rows are returned and a lot of other interesting things so this is a very interesting query to be aware of if",
    "start": "2362470",
    "end": "2369250"
  },
  {
    "text": "you want to check how my s3 interactions are happening how much time it is taking and other things now let's jump into the",
    "start": "2369250",
    "end": "2375790"
  },
  {
    "start": "2375000",
    "end": "2399000"
  },
  {
    "text": "next piece so so far so good it's it's doing partitions now I want to",
    "start": "2375790",
    "end": "2381490"
  },
  {
    "text": "show you how do you automate all this like one of the things which a lot of people have is you have a lot of data",
    "start": "2381490",
    "end": "2386770"
  },
  {
    "text": "coming in how can you create a compressed file out of the raw data which is coming how can you make sure it",
    "start": "2386770",
    "end": "2392530"
  },
  {
    "text": "is park a parquet or something else which is fast how can you compress it so let me show you how you can do it using parquet so",
    "start": "2392530",
    "end": "2399820"
  },
  {
    "start": "2399000",
    "end": "2444000"
  },
  {
    "text": "we have a service called glue it does a lot of things for you so I'm going to show you some of them so so first first",
    "start": "2399820",
    "end": "2408190"
  },
  {
    "text": "thing first I want to show you a lab this is just good to remember because for some of you who are",
    "start": "2408190",
    "end": "2414530"
  },
  {
    "text": "interested in this you can just repeat this lab in your home or in your office this this is this contains all the",
    "start": "2414530",
    "end": "2421730"
  },
  {
    "text": "instructions of what I am doing so if you want to repeat this just go to this link some of my colleagues have created",
    "start": "2421730",
    "end": "2427130"
  },
  {
    "text": "this it allows you to use glue to create a server less data pipeline or ETL",
    "start": "2427130",
    "end": "2435350"
  },
  {
    "text": "pipeline which is taking the data and it's converted into park' it's compressing it and then all the things",
    "start": "2435350",
    "end": "2441980"
  },
  {
    "text": "happen from there so so this is just a good thing to be aware of now let's let's move to our normal exercise so",
    "start": "2441980",
    "end": "2450230"
  },
  {
    "text": "first thing I want to go to my console I'll go to my ad base console first",
    "start": "2450230",
    "end": "2455870"
  },
  {
    "text": "thing let's let's create a new s3 bucket so this will be bucket where I would put",
    "start": "2455870",
    "end": "2462200"
  },
  {
    "text": "my data so so let's call it um - re",
    "start": "2462200",
    "end": "2469060"
  },
  {
    "text": "re-invent let's say 17 cool and I say",
    "start": "2469060",
    "end": "2475370"
  },
  {
    "text": "create so I just created it and within that file they were within that bucket",
    "start": "2475370",
    "end": "2481880"
  },
  {
    "text": "I'll create two folders one is the temporary folder and one is a target",
    "start": "2481880",
    "end": "2487670"
  },
  {
    "text": "folder and I will explain you why so temporary folder is for temporary files which glue is going to create and target",
    "start": "2487670",
    "end": "2495590"
  },
  {
    "text": "is where I want my final data to come so so what do is gonna do is it's gonna take the source data it's gonna run its",
    "start": "2495590",
    "end": "2502970"
  },
  {
    "text": "a spark engine actually so it uses spark to convert your raw data into a compressed data which is which could be",
    "start": "2502970",
    "end": "2509660"
  },
  {
    "text": "snappy or could be anything else and it also does power K power K is a columnar storage so when you have a filters in",
    "start": "2509660",
    "end": "2515840"
  },
  {
    "text": "your career how many of you know about parquet here just raise your hands if you know okay probably half of you don't know so I will quickly tell you there",
    "start": "2515840",
    "end": "2522620"
  },
  {
    "text": "are lot of columnar formats these days parquet is pretty popular but there are a lot of others but what they do is when",
    "start": "2522620",
    "end": "2529130"
  },
  {
    "text": "you have a query where you say select column a column B column C it's taking only those three columns it's not",
    "start": "2529130",
    "end": "2535130"
  },
  {
    "text": "looking at all the columns so if you have very wide columns columnar storage is very very",
    "start": "2535130",
    "end": "2540920"
  },
  {
    "text": "of efficient it saves a lot of money it's also very fast and compression as usual is great because you are scanning",
    "start": "2540920",
    "end": "2547340"
  },
  {
    "text": "less amount of data if it is compressed so so I created these two now let me go to glue and you're going to do some cool",
    "start": "2547340",
    "end": "2553820"
  },
  {
    "text": "stuff there so glue is a very new service I think we generate in last New",
    "start": "2553820",
    "end": "2561230"
  },
  {
    "text": "York summit a few months back so first thing first thing first what I'll do is",
    "start": "2561230",
    "end": "2566510"
  },
  {
    "text": "I'll add a new database let's call it so",
    "start": "2566510",
    "end": "2572870"
  },
  {
    "text": "so for this use case I'm using a taxi data the reason I'm using taxi data is a",
    "start": "2572870",
    "end": "2577880"
  },
  {
    "text": "it's a pretty big sized data and a lot of things I have already burnt being done of that so I just want to keep it",
    "start": "2577880",
    "end": "2583310"
  },
  {
    "text": "simple and easy so let's call it my text editor 5 that's my database name so I",
    "start": "2583310",
    "end": "2591860"
  },
  {
    "text": "created that and what I will do there is now now first thing I what I want to do",
    "start": "2591860",
    "end": "2598370"
  },
  {
    "text": "is there's a concept of crawler so what crawler does is you can tell a",
    "start": "2598370",
    "end": "2604100"
  },
  {
    "text": "blue crawler to go to a specific location and check what data exists",
    "start": "2604100",
    "end": "2609770"
  },
  {
    "text": "there and then whenever a new data file comes it can automatically run a series of steps and do some ETL for you so let",
    "start": "2609770",
    "end": "2617780"
  },
  {
    "text": "me show you how it works so so first we have to give a crawler name so let's call it reinvent 17 next so there are",
    "start": "2617780",
    "end": "2630200"
  },
  {
    "text": "two type of data store it can go to it could go to a JDBC connection so it can go to there and say hey tell me what",
    "start": "2630200",
    "end": "2635600"
  },
  {
    "text": "data you have or it can go to is 3 location and then sorry let's go to use s3 here and then you can either crawl",
    "start": "2635600",
    "end": "2642770"
  },
  {
    "text": "the data from your own account a bucket in your account or you can crawl from another account so in this use case I'm",
    "start": "2642770",
    "end": "2648320"
  },
  {
    "text": "crawling from another account there's a public bucket which is already created so I'm gonna use that bucket so let me",
    "start": "2648320",
    "end": "2654530"
  },
  {
    "text": "go to my notes and these are there's a bucket which already has some New York",
    "start": "2654530",
    "end": "2660320"
  },
  {
    "text": "taxi data this is the open source data which a lot of companies use for testings like this so so now it's asking",
    "start": "2660320",
    "end": "2667190"
  },
  {
    "text": "do I need another data store I will say no one minute let me make sure there are no",
    "start": "2667190",
    "end": "2673589"
  },
  {
    "text": "patterns cool so I'm not using any patterns and then I am role you can",
    "start": "2673589",
    "end": "2679049"
  },
  {
    "text": "create a new role or you can use an existing I already have already created a role so I'll just use that it's pretty",
    "start": "2679049",
    "end": "2684420"
  },
  {
    "text": "simple now there are two three options here do you want to run it hourly daily weekly monthly or on-demand so to keep",
    "start": "2684420",
    "end": "2692130"
  },
  {
    "text": "it simple I am running on-demand and then it's saying what do I want to do with the output so this crawler will do",
    "start": "2692130",
    "end": "2698640"
  },
  {
    "text": "a bunch of things it comes with the weather result what do you want to do with the result there they are such",
    "start": "2698640",
    "end": "2704010"
  },
  {
    "text": "telling do you want to put it in a database yes I think we just created I",
    "start": "2704010",
    "end": "2711289"
  },
  {
    "text": "think one second one yeah okay sorry",
    "start": "2711289",
    "end": "2716910"
  },
  {
    "text": "forgot prefix added to the tables so when I create the tables here so what is",
    "start": "2716910",
    "end": "2722069"
  },
  {
    "text": "going to do is it's it's going to discover the data once it finds it it's going to put it in this database",
    "start": "2722069",
    "end": "2727200"
  },
  {
    "text": "do you want to add a prefix to the table yes we would love to so let's call it tree infant maybe underscore seventeen",
    "start": "2727200",
    "end": "2735710"
  },
  {
    "text": "underscore so what we wanna do is every table it creates it'll add this prefix next it says okay finish it once we do",
    "start": "2735710",
    "end": "2745230"
  },
  {
    "text": "that this crawler is ready it's saying do you want to run it now yes we will run it now so so it's gone it's running",
    "start": "2745230",
    "end": "2751349"
  },
  {
    "text": "right now I think it takes about thirty forty seconds so let it run by the time it's running let me show you the",
    "start": "2751349",
    "end": "2756960"
  },
  {
    "text": "architecture of how this thing works so a quick quick show of the",
    "start": "2756960",
    "end": "2765089"
  },
  {
    "text": "architecture here so let me make it a little bigger so so there's s3 bucket",
    "start": "2765089",
    "end": "2770220"
  },
  {
    "text": "and s3 bucket has some data this is the source data it could come from your web",
    "start": "2770220",
    "end": "2776039"
  },
  {
    "text": "servers it could come from your third party which are pushing data to you it could be anything so you're getting a lot of data once you get the data blue",
    "start": "2776039",
    "end": "2783270"
  },
  {
    "text": "crawler will go there and discover what does this data look like so it's gonna interpret some things but you can also",
    "start": "2783270",
    "end": "2789450"
  },
  {
    "text": "override some of the things that interpret you can say okay whenever you see this column even though it's called ABC I want to call it XYZ so you can",
    "start": "2789450",
    "end": "2797099"
  },
  {
    "text": "change those things and then it creates a catalog based on the data so it creates a just",
    "start": "2797099",
    "end": "2802410"
  },
  {
    "text": "the it's called catalogs which which is metadata so it knows what does the",
    "start": "2802410",
    "end": "2807480"
  },
  {
    "text": "schema of this data look like and then and then we run a job look at the",
    "start": "2807480",
    "end": "2813839"
  },
  {
    "text": "bottommost thing here with the bottom most bucket this is another bucket or it could be same bucket but what we are",
    "start": "2813839",
    "end": "2819900"
  },
  {
    "text": "saying is that the origin source data will come in CSV format on the top crawler should go there every hour or",
    "start": "2819900",
    "end": "2827190"
  },
  {
    "text": "every day or whatever frequency you choose and it should take that data and put it in a parka format in the lower",
    "start": "2827190",
    "end": "2834000"
  },
  {
    "text": "bucket and once it is in lower that ship you should use this data redshift should not talk to the source data so because",
    "start": "2834000",
    "end": "2840750"
  },
  {
    "text": "then you're compressing the data it's power K so it's fast so everything every good thing is happening by using glue",
    "start": "2840750",
    "end": "2846720"
  },
  {
    "text": "and redshift is talking to this redshift spectrum is talking to this so hopefully our job would everyone finished by now",
    "start": "2846720",
    "end": "2853520"
  },
  {
    "start": "2850000",
    "end": "2905000"
  },
  {
    "text": "okay so yep it says done so if you look at this crawler what it does is so I I",
    "start": "2853520",
    "end": "2863670"
  },
  {
    "text": "think the crawler was called reinvent 17 yep so if we go to this it says it added",
    "start": "2863670",
    "end": "2870539"
  },
  {
    "text": "three tables so now if we go to our tables we should see some three tables to make it simple I think we added a",
    "start": "2870539",
    "end": "2875940"
  },
  {
    "text": "prefix here let's refresh this is showing okay wow yeah so different when",
    "start": "2875940",
    "end": "2882480"
  },
  {
    "text": "I refreshed I think my new table showed up I call them reinvent underscore 17 underscore whatever that is so so let's",
    "start": "2882480",
    "end": "2889740"
  },
  {
    "text": "look at one of them so let's go to yellow one so it's saying that by looking at the data I thought there",
    "start": "2889740",
    "end": "2895589"
  },
  {
    "text": "there's a vendor ID which is actually let me increase the font size here it's saying there's a vendor and Eve it is",
    "start": "2895589",
    "end": "2900660"
  },
  {
    "text": "big in there few other things things like that and then once we have that so",
    "start": "2900660",
    "end": "2906359"
  },
  {
    "text": "it has created a table the next thing we can do is we can create a job so what I",
    "start": "2906359",
    "end": "2914970"
  },
  {
    "text": "will do is I will go to the jobs here so now the now it understands the table now let me quickly create a job so I'll call",
    "start": "2914970",
    "end": "2921450"
  },
  {
    "text": "it Reno and underscore Park underscore -",
    "start": "2921450",
    "end": "2927029"
  },
  {
    "text": "job underscore job let's call it and then I am role I already have a row do I",
    "start": "2927029",
    "end": "2933660"
  },
  {
    "text": "want to use a proposed script generated by aw you already had a lot of scripts we used glue script rather than writing our own",
    "start": "2933660",
    "end": "2939569"
  },
  {
    "text": "but we can always ruin our own so you can have your custom code which is a spark compatible Java code and then you",
    "start": "2939569",
    "end": "2948059"
  },
  {
    "text": "can put it here but let's use the one which we have and then saying what's what is the file name that's okay",
    "start": "2948059",
    "end": "2954359"
  },
  {
    "text": "s3 pathway script is stored so we created a new table today I think we called it reinvent something so here it",
    "start": "2954359",
    "end": "2960599"
  },
  {
    "text": "is okay so I select that bucket s3 pathway script is stored and then temperate",
    "start": "2960599",
    "end": "2966660"
  },
  {
    "text": "because this script doesn't exist it's gonna put its script there temporary directory so this is this is where the",
    "start": "2966660",
    "end": "2972240"
  },
  {
    "text": "temporary directory we created comes handy so we'll say this is the temporary directory you can use and advanced by",
    "start": "2972240",
    "end": "2979769"
  },
  {
    "start": "2979000",
    "end": "3089000"
  },
  {
    "text": "default job is disabled so we gonna say enable next and then choose your data",
    "start": "2979769",
    "end": "2985289"
  },
  {
    "text": "source so we are saying what are the what are the data sources you are",
    "start": "2985289",
    "end": "2991109"
  },
  {
    "text": "interested in so in this case we're gonna choose just the yellow taxi just to keep it simple and then choose your",
    "start": "2991109",
    "end": "2997380"
  },
  {
    "text": "data targets so so so target could be an existing table or it could be a new",
    "start": "2997380",
    "end": "3002809"
  },
  {
    "text": "table so let's create a new table we'll say hey we don't every time you find a set of data as create a new file in s3",
    "start": "3002809",
    "end": "3010160"
  },
  {
    "text": "and the format of that should be parking so as you can see they're a bunch of formats we support JSON CSV Avro Parkway",
    "start": "3010160",
    "end": "3017029"
  },
  {
    "text": "and oversee so for this one we'll use parking and we'll say where where should you store this file once you have done",
    "start": "3017029",
    "end": "3023420"
  },
  {
    "text": "all the work so we're gonna say store in the same folder but store in target so",
    "start": "3023420",
    "end": "3029029"
  },
  {
    "text": "you can have a different bucket here if you want and then finally it's showing do you want any customization on top of",
    "start": "3029029",
    "end": "3035450"
  },
  {
    "text": "what I discovered so it's saying there's a vendor ID which is big int so we can probably change some other things if we",
    "start": "3035450",
    "end": "3040910"
  },
  {
    "text": "want like we can call it pickup date can be called pickup underscore date let's",
    "start": "3040910",
    "end": "3045920"
  },
  {
    "text": "say so you can clean up things you can change data type if you want so in interest of time I'll keep it simple and then finally I saying do you",
    "start": "3045920",
    "end": "3053809"
  },
  {
    "text": "want to run a finish and once we finish it shows the code it it has generated so",
    "start": "3053809",
    "end": "3059599"
  },
  {
    "text": "it is saying it has done it's doing this mapping it is doing some dynamic mapping so this is the simple code but as you",
    "start": "3059599",
    "end": "3066170"
  },
  {
    "text": "can see it's doing a lot of spark jobs and then finally I'll run this so once I",
    "start": "3066170",
    "end": "3071750"
  },
  {
    "text": "run for the data which I have chosen it takes about 30 minutes so I'm not going to ask you to wait here I've already",
    "start": "3071750",
    "end": "3077420"
  },
  {
    "text": "done it in a different database but just to show you what the way you can use it",
    "start": "3077420",
    "end": "3084829"
  },
  {
    "text": "is let's go here you can do a so so what will this will do is the output of this",
    "start": "3084829",
    "end": "3090829"
  },
  {
    "text": "will be if you go to your s3 file the file that bucket which we created you",
    "start": "3090829",
    "end": "3098539"
  },
  {
    "text": "will see so this is this is one which I created earlier if I go to target you'll",
    "start": "3098539",
    "end": "3103700"
  },
  {
    "text": "see things like this so it will create a lot of snap eparchy files these are the files which it's creating based on the",
    "start": "3103700",
    "end": "3108769"
  },
  {
    "text": "data which are discovered and then you'll point your spectrum to this and this spectrum will be much faster than",
    "start": "3108769",
    "end": "3114109"
  },
  {
    "text": "what it would be on the raw table because it is it is a columnar or it is compressed and just to complete the",
    "start": "3114109",
    "end": "3122329"
  },
  {
    "text": "story let me just create this I'll go back to my console ok let me see it oops",
    "start": "3122329",
    "end": "3133250"
  },
  {
    "text": "oops ok oops ok so control",
    "start": "3133250",
    "end": "3142568"
  },
  {
    "text": "okay so I already have so I haven't created the schema that's okay so I",
    "start": "3148010",
    "end": "3153620"
  },
  {
    "text": "think I can create it here okay it",
    "start": "3153620",
    "end": "3160320"
  },
  {
    "text": "created a schema and then I'll just so as you can see this is a very similar command we have done it like bunch of",
    "start": "3160320",
    "end": "3166230"
  },
  {
    "text": "times now we are again creating external table we are just saying stored as power K that is the only difference it's very",
    "start": "3166230",
    "end": "3171600"
  },
  {
    "text": "telling that it's a party data it had created external table the only difference you will see is this will be",
    "start": "3171600",
    "end": "3176850"
  },
  {
    "text": "much faster table much cheaper because the data is compressed columnar storage",
    "start": "3176850",
    "end": "3182040"
  },
  {
    "text": "and you just got everything by clicking bunch of clicks no no scripting done",
    "start": "3182040",
    "end": "3187050"
  },
  {
    "text": "everything is automated you can have scheduled your jobs you can queue them you can do a lot of things so so that",
    "start": "3187050",
    "end": "3192720"
  },
  {
    "text": "was the demo I wanted to show you Ghana we have like five minutes or so so also we're gonna use that for questions just",
    "start": "3192720",
    "end": "3199680"
  },
  {
    "text": "I can quickly show you that this table indeed works just to a select count star",
    "start": "3199680",
    "end": "3206640"
  },
  {
    "text": "on this so it will probably show us they are probably a million or so a 10 million or so rows in this and it's",
    "start": "3206640",
    "end": "3214980"
  },
  {
    "text": "doing pretty fast so that's the end of the demo what to summarize what we did",
    "start": "3214980",
    "end": "3220560"
  },
  {
    "text": "war we started with a file which was stored in in s3 we queried it using redshift then we created a redshift",
    "start": "3220560",
    "end": "3227880"
  },
  {
    "text": "native table then we combine both of them and then finally we showed once you have the files in s3 how can you use",
    "start": "3227880",
    "end": "3233790"
  },
  {
    "text": "glue to automatically create compressed files and column with stories like",
    "start": "3233790",
    "end": "3239370"
  },
  {
    "text": "parquet or something else using glue and everything can can be automated so one of the frequent use cases I see is a lot",
    "start": "3239370",
    "end": "3246270"
  },
  {
    "text": "of customers who have big clusters they do they do a redshift unload which means they remove some data from redshift they",
    "start": "3246270",
    "end": "3252960"
  },
  {
    "text": "put it in s3 and then they run power key on it so it becomes fast but still they",
    "start": "3252960",
    "end": "3258750"
  },
  {
    "text": "get a lot of cost advantages they got a lot of performance advantages so you can think about use cases where you can use",
    "start": "3258750",
    "end": "3264330"
  },
  {
    "text": "it with that mic you wanna come on the stage let's take a couple of questions if you have any questions we would love",
    "start": "3264330",
    "end": "3271620"
  },
  {
    "text": "to answer them yep",
    "start": "3271620",
    "end": "3274790"
  },
  {
    "text": "yeah so if they were like anomalies let's say there's things which don't match there's some way for you to say",
    "start": "3289290",
    "end": "3297220"
  },
  {
    "text": "ignore it so for example you need to come up with a fallback right if the column doesn't exist to put a null there or things like it's",
    "start": "3297220",
    "end": "3303130"
  },
  {
    "text": "a relational database so so you can do all that but you just need to be aware what is the fallback you are going to",
    "start": "3303130",
    "end": "3308440"
  },
  {
    "text": "because some fallback some people are very strict that I always want this column for example if it's a identity column or things like that but if there",
    "start": "3308440",
    "end": "3314950"
  },
  {
    "text": "are some mismatches you need to decide what do you want to do when a mismatch happens because you have the full access",
    "start": "3314950",
    "end": "3320590"
  },
  {
    "text": "to the script it will be mostly customization through other script this first spark script which we have in the",
    "start": "3320590",
    "end": "3327490"
  },
  {
    "text": "glue so it auto generates the base script for you in most cases you don't need to change it but if you know there",
    "start": "3327490",
    "end": "3332620"
  },
  {
    "text": "are data quality issues you will have access to each row there you can like say do some custom things on top of it",
    "start": "3332620",
    "end": "3339960"
  },
  {
    "text": "so sales data was in the file but the name the path is very important there",
    "start": "3353580",
    "end": "3359320"
  },
  {
    "text": "that that's how it knows which files to go to so you're when you say partitions you can have like a high number of layer",
    "start": "3359320",
    "end": "3366450"
  },
  {
    "text": "what is called slash delimited values and we'll go first partition second",
    "start": "3366450",
    "end": "3373150"
  },
  {
    "text": "partition third partition so the the the format is partition name equal to partition name comes first equal to what",
    "start": "3373150",
    "end": "3379450"
  },
  {
    "text": "is the value of that partition for the data which is stored within that folder yeah that's required yeah as far as I",
    "start": "3379450",
    "end": "3386890"
  },
  {
    "text": "know yeah",
    "start": "3386890",
    "end": "3389640"
  },
  {
    "text": "yep oh the the the New York taxi which we",
    "start": "3395890",
    "end": "3407750"
  },
  {
    "text": "created yeah so in this case it's not partition so if you go to the source data I can quickly show you there's no",
    "start": "3407750",
    "end": "3413930"
  },
  {
    "text": "partition there it's come first and power case so there are three things here one is partitioning which means you",
    "start": "3413930",
    "end": "3419300"
  },
  {
    "text": "have a logical structure to your files we are not doing that in the last step we are doing two other things though we",
    "start": "3419300",
    "end": "3425359"
  },
  {
    "text": "are compressing it with using snappy compression and we are also using power K if you've opened the file it will look",
    "start": "3425359",
    "end": "3431540"
  },
  {
    "text": "like a very it's a binary file we cannot like there ways to read power K but you cannot directly read it so",
    "start": "3431540",
    "end": "3437840"
  },
  {
    "text": "partition is not happening here it's it's power K format and it's compressed",
    "start": "3437840",
    "end": "3443560"
  },
  {
    "text": "what oh yeah actually we should in the interest of time I didn't do that but",
    "start": "3443560",
    "end": "3449840"
  },
  {
    "text": "you can have a step where you say you should put the file in the partition name so yeah so that will make kill two",
    "start": "3449840",
    "end": "3458480"
  },
  {
    "text": "words insulin shots right awesome",
    "start": "3458480",
    "end": "3463119"
  },
  {
    "text": "compression of sort keys yeah do you know about that not for the compound interleaved yeah because I know the",
    "start": "3486960",
    "end": "3493540"
  },
  {
    "text": "primary key shouldn't be compressed but I don't know about this from the top of",
    "start": "3493540",
    "end": "3498550"
  },
  {
    "text": "the head so we'll discuss offline and see if we can find an answer yep is it",
    "start": "3498550",
    "end": "3505660"
  },
  {
    "text": "high syntax so a spectrum is when you use spectrum from redshift it's it's",
    "start": "3505660",
    "end": "3511060"
  },
  {
    "text": "exactly like redshift so you don't have to decide oh it's backstrom so I do my query should be different your interface",
    "start": "3511060",
    "end": "3517540"
  },
  {
    "text": "the spectrum is exactly same as the redshift that's the best benefit actually because there's another option called Athena which you didn't go to",
    "start": "3517540",
    "end": "3523450"
  },
  {
    "text": "today but you can query as three files using a Pina as well but the advantages of doing",
    "start": "3523450",
    "end": "3528700"
  },
  {
    "text": "through a shape is if you have existing data in redshift you can join very easily it's it's very seamless to your",
    "start": "3528700",
    "end": "3535510"
  },
  {
    "text": "user experience because your the queries will automatically go to s3 or redshift",
    "start": "3535510",
    "end": "3540730"
  },
  {
    "text": "depending on where it is stored but the you the way people are interacting is going to be same yeah let's take one",
    "start": "3540730",
    "end": "3548200"
  },
  {
    "text": "last question",
    "start": "3548200",
    "end": "3550470"
  },
  {
    "text": "yeah so that's a good question like if you have a lot of data in redshift how should you unload the best way people do",
    "start": "3567140",
    "end": "3573930"
  },
  {
    "text": "it is you you think look at your query patterns and see how much of data you generally scan like look at your daily",
    "start": "3573930",
    "end": "3580200"
  },
  {
    "text": "query patterns and if you think that more than two years like wonder than two years has never query that thing is a",
    "start": "3580200",
    "end": "3586140"
  },
  {
    "text": "good idea to put that in s3 because the performance of data in s3 would not be same as this but it's much more scalable",
    "start": "3586140",
    "end": "3592470"
  },
  {
    "text": "you can have like exabytes of data and still fire a query with a very low cost",
    "start": "3592470",
    "end": "3598110"
  },
  {
    "text": "so so what people do is they generally have a time data yeah so the table will",
    "start": "3598110",
    "end": "3603450"
  },
  {
    "text": "have some time component in it it could be here it could be month and then they they have a rolling window kind of thing",
    "start": "3603450",
    "end": "3609000"
  },
  {
    "text": "like key floor last 12 months in redshift and everything older retires to s3 and then you add in the new partition",
    "start": "3609000",
    "end": "3615870"
  },
  {
    "text": "in the file so you have like access to both of them cool so we'll be here and",
    "start": "3615870",
    "end": "3623670"
  },
  {
    "text": "we'll take some questions offline but thank you very much for coming here",
    "start": "3623670",
    "end": "3628130"
  }
]