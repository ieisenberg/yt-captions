[
  {
    "text": "hi my name is on the rug gonna be talking to you on the deep dive about",
    "start": "620",
    "end": "5879"
  },
  {
    "text": "Amazon Aurora you know this is kind of an intimidating room from this direction I've got to tell you I feel like I",
    "start": "5879",
    "end": "12809"
  },
  {
    "text": "should be singing but don't worry you're not gonna see that I'm good for both of us so what I'll be talking about here is",
    "start": "12809",
    "end": "20810"
  },
  {
    "text": "basically just the fundamentals of what is in Aurora and you know some of the",
    "start": "20810",
    "end": "26279"
  },
  {
    "text": "recent improvements and some of the stuff that's coming soon at least a few",
    "start": "26279",
    "end": "31289"
  },
  {
    "text": "of those items were talked about in the keynote earlier today so let's just jump into it and I'll try to leave ample time",
    "start": "31289",
    "end": "38010"
  },
  {
    "text": "for questions I tend to go fast through these things so Aurora is really a relational database reimagined for the",
    "start": "38010",
    "end": "44820"
  },
  {
    "text": "cloud and our goal here was to say that hey if we were really designing the relational database today as opposed to",
    "start": "44820",
    "end": "51300"
  },
  {
    "text": "40 years ago what would we do and how would we build it and the answer ends up becoming somewhat different and we",
    "start": "51300",
    "end": "57870"
  },
  {
    "text": "really want to build something that has the speed and availability of a high-end commercial database the simplicity and",
    "start": "57870",
    "end": "63780"
  },
  {
    "text": "the cost-effectiveness of open source no one actually wants in that new database so we want to make ours compatible with",
    "start": "63780",
    "end": "69900"
  },
  {
    "text": "my sequel and Postgres thought two more most popular open source relational databases that are out there and you",
    "start": "69900",
    "end": "75840"
  },
  {
    "text": "know we want to provide all of the things you expect from a database from AWS you know pay-as-you-go pricing",
    "start": "75840",
    "end": "82350"
  },
  {
    "text": "managed service etc etc so relational",
    "start": "82350",
    "end": "87509"
  },
  {
    "text": "databases were you know came about a long time ago in the mainframe era and you know you basically end up with this",
    "start": "87509",
    "end": "94650"
  },
  {
    "text": "block big blue box that's you know sequel transactions caching logging and no matter what if you look at most",
    "start": "94650",
    "end": "101400"
  },
  {
    "text": "systems today they continue to do that and the difficulty that that creates is",
    "start": "101400",
    "end": "107189"
  },
  {
    "text": "is that it has a large failure but blast radius right it tends to be a scale up architecture",
    "start": "107189",
    "end": "112920"
  },
  {
    "text": "you do there are systems that scale out but not a ton of them and even with those the failure blast radius is you",
    "start": "112920",
    "end": "120689"
  },
  {
    "text": "know quite real and so one of the things that we do in Aurora is is that we move",
    "start": "120689",
    "end": "126360"
  },
  {
    "text": "storage which is really what you care about from you know state is what you care about from a blast radius",
    "start": "126360",
    "end": "132690"
  },
  {
    "text": "perspective we move that out into a purpose-built log structure distributed storage system",
    "start": "132690",
    "end": "138750"
  },
  {
    "text": "and the storage volume may be striped across hundreds of storage nodes they're",
    "start": "138750",
    "end": "144210"
  },
  {
    "text": "distributed across three availability zones there's six copies of data two copies of each easy and if any of these",
    "start": "144210",
    "end": "151800"
  },
  {
    "text": "instances fail we can certainly go and replace them quickly they're stateless",
    "start": "151800",
    "end": "157200"
  },
  {
    "text": "it doesn't much matter the caching has been moved out of process so if you have to do a restart of your database let's",
    "start": "157200",
    "end": "163620"
  },
  {
    "text": "say because someone's running a query and it's this thing is hung the cache will still be hot when it comes up and",
    "start": "163620",
    "end": "169770"
  },
  {
    "text": "so that also helps so you know that's a slide I've used many times over the last",
    "start": "169770",
    "end": "174990"
  },
  {
    "text": "three years here's a new one so why are six copies necessary so in the first in",
    "start": "174990",
    "end": "182010"
  },
  {
    "text": "the top picture we're showing an example with a two out of three quorum you know two out of three reads two out of three",
    "start": "182010",
    "end": "187920"
  },
  {
    "text": "writes and you know basically you're quorum is fine as long as you only have",
    "start": "187920",
    "end": "193550"
  },
  {
    "text": "you know one failure at a time and failures are reasonably rare let's say there's a one in ten thousand or one",
    "start": "193550",
    "end": "201660"
  },
  {
    "text": "than a hundred thousand chance of a failure to have two failures if they're independent events you multiply those",
    "start": "201660",
    "end": "207780"
  },
  {
    "text": "numbers and it feels pretty rare but the problem is in a large fleet if you know",
    "start": "207780",
    "end": "213989"
  },
  {
    "text": "there are always some failures that are going on right disks fails nodes fails network paths fail you know in a large",
    "start": "213989",
    "end": "221640"
  },
  {
    "text": "in the fleet there's always something that's broken and you know that needs to get repaired now the problem is is that",
    "start": "221640",
    "end": "227760"
  },
  {
    "text": "AC failures availability zone failures have shared fate if you lose an AZ you",
    "start": "227760",
    "end": "234270"
  },
  {
    "text": "will have a quorum break on each and every quorum until the ones where you've got a red break up there that breaks the",
    "start": "234270",
    "end": "242370"
  },
  {
    "text": "quorum and if that's a you know a long term destructive failure you're lost so we believe an Aurora that you need to",
    "start": "242370",
    "end": "250170"
  },
  {
    "text": "tolerate AZ plus one so you can have one failure in the system at any given time",
    "start": "250170",
    "end": "256019"
  },
  {
    "text": "and if you lose in AZ or you know you will end up basically being able to",
    "start": "256019",
    "end": "262830"
  },
  {
    "text": "tolerate it from a read perspective so in the R or have four out of six quorum for right three out of six four reads and the",
    "start": "262830",
    "end": "270340"
  },
  {
    "text": "quorum will survive an easy failure we've actually built a bunch of code to be able to tolerate even long-lasting",
    "start": "270340",
    "end": "277810"
  },
  {
    "text": "failures so for example a couple of years ago there was a snowstorm in",
    "start": "277810",
    "end": "284130"
  },
  {
    "text": "Washington DC and you know there are some concerns about the flat roof that happened to be on one of our data",
    "start": "284130",
    "end": "290590"
  },
  {
    "text": "centers and whether it was gonna survive that blizzard and you know it did everything was fine but you know just if",
    "start": "290590",
    "end": "297790"
  },
  {
    "text": "you lose a building in a long-term kind of way which can happen right I mean we all pay insurance for fire and you know",
    "start": "297790",
    "end": "303820"
  },
  {
    "text": "so forth you know that's because these events are possible right they're not impossible then you know that's a",
    "start": "303820",
    "end": "310120"
  },
  {
    "text": "long-term failure so in those cases we drop down from four out of six to three out of four of the remaining notes and",
    "start": "310120",
    "end": "318100"
  },
  {
    "text": "so if you have three Z's that requires six copies if you have five AZ's which",
    "start": "318100",
    "end": "323340"
  },
  {
    "text": "not that many people do then you can you know drop down to three out of five and",
    "start": "323340",
    "end": "329350"
  },
  {
    "text": "still have AZ plus one that makes sense so why is that one failure okay so if",
    "start": "329350",
    "end": "336940"
  },
  {
    "text": "that really turns into a question of how long does it take to repair and we use",
    "start": "336940",
    "end": "343150"
  },
  {
    "text": "10 Giga Byte segments there's a 10 gigabit Network and so you know you can",
    "start": "343150",
    "end": "349450"
  },
  {
    "text": "do the math on how long that repair takes you know from detection to repair",
    "start": "349450",
    "end": "354610"
  },
  {
    "text": "and so that really reduces your window of vulnerability right because you're now talking about a handful of seconds",
    "start": "354610",
    "end": "361570"
  },
  {
    "text": "in which time in order to for to actually lose data you need two of those",
    "start": "361570",
    "end": "367270"
  },
  {
    "text": "to happen simultaneously plus an AZ failure to you know lose four out of six",
    "start": "367270",
    "end": "372610"
  },
  {
    "text": "and no longer have three out of six for repair or you need you know for independent events and that's a awfully",
    "start": "372610",
    "end": "378520"
  },
  {
    "text": "bad day right now the nice thing about doing things with where you have a high",
    "start": "378520",
    "end": "384100"
  },
  {
    "text": "fault tolerance threshold is is that you can use it not just for these",
    "start": "384100",
    "end": "389260"
  },
  {
    "text": "catastrophic events not just for you know the ongoing problems that occur in",
    "start": "389260",
    "end": "394330"
  },
  {
    "text": "the large and the fleet but you can also use this as a mechanism to just rip",
    "start": "394330",
    "end": "399460"
  },
  {
    "text": "hairier system itself like so for example it's generally in most storage",
    "start": "399460",
    "end": "405009"
  },
  {
    "text": "systems it's a pretty big deal to patch it because you're injecting a failure and there are it's not as long as I've",
    "start": "405009",
    "end": "411970"
  },
  {
    "text": "make sure that I've got you know six colors for the various segments of",
    "start": "411970",
    "end": "418120"
  },
  {
    "text": "something then I can basically patch any one color at a time and you know as long as and the system is fine right because",
    "start": "418120",
    "end": "424479"
  },
  {
    "text": "I've injected a failure for a subset of the system but that's fine I can also use it to deal with heat management so",
    "start": "424479",
    "end": "431740"
  },
  {
    "text": "if I've got a hot node or hot disc I can just kill one of the segments on it and it'll automatically get repopulated on",
    "start": "431740",
    "end": "439240"
  },
  {
    "text": "some other disk on some other node you know that's just how our repairs work and that's cool right because it just it",
    "start": "439240",
    "end": "446199"
  },
  {
    "text": "takes what would normally be a complex hard challenging problem and it makes it straightforward one of the things I",
    "start": "446199",
    "end": "453310"
  },
  {
    "text": "often talk about within my team is is that latency is just a short-term",
    "start": "453310",
    "end": "460960"
  },
  {
    "text": "version of availability and availability is just a short-term version of durability right and if you have a",
    "start": "460960",
    "end": "467259"
  },
  {
    "text": "system that properly deals with the",
    "start": "467259",
    "end": "472449"
  },
  {
    "text": "performance and jitter and so forth using quorums that also the same exact model can be used for doing availability",
    "start": "472449",
    "end": "481030"
  },
  {
    "text": "the same exact model can be used for doing durability so yeah just for you know any distributed systems geeks in",
    "start": "481030",
    "end": "488080"
  },
  {
    "text": "the audience you know I think that I would encourage you to think about that statement and how you might apply it in your own and the system's you build",
    "start": "488080",
    "end": "495479"
  },
  {
    "text": "another interesting thing that we I think that's novel with Aurora I'm not",
    "start": "495479",
    "end": "500770"
  },
  {
    "text": "sure is that we you can do membership changes without stalls one of the problems that would occur when they",
    "start": "500770",
    "end": "507250"
  },
  {
    "text": "start doing all of these small segments and I'm talking about failures happening",
    "start": "507250",
    "end": "512578"
  },
  {
    "text": "failures become more common so I can't really manage changes to the membership in my quorum easily right in most",
    "start": "512579",
    "end": "519520"
  },
  {
    "text": "systems you're doing some things like a Paxos protocol to deal with a membership change we don't we use quorum sets and",
    "start": "519520",
    "end": "526750"
  },
  {
    "text": "so let's just walk through a failure of node F inside my quorum set that's ABCDE F",
    "start": "526750",
    "end": "533150"
  },
  {
    "text": "and so what happens there is is that I go from all nodes healthy to a quorum",
    "start": "533150",
    "end": "539960"
  },
  {
    "text": "change which is basically hey you can't write to this thing unless you're writing to the right quorum and then if",
    "start": "539960",
    "end": "545780"
  },
  {
    "text": "you're on the old version you know you basically have to write to every button member of it and say update the epoch",
    "start": "545780",
    "end": "551630"
  },
  {
    "text": "and see update the epoch and then they're not gonna accept to write unless somebody's got the right notion of the",
    "start": "551630",
    "end": "557540"
  },
  {
    "text": "quorum set so what we're doing here is is we're saying okay I want to go from",
    "start": "557540",
    "end": "563650"
  },
  {
    "text": "ABCDEF to ABCDE G and so I'm gonna",
    "start": "563650",
    "end": "569000"
  },
  {
    "text": "basically say that you need to write to a quorum set which is a b c d e f and",
    "start": "569000",
    "end": "575680"
  },
  {
    "text": "ABCDE g now note that in this particular case writing to ABCD which is a quorum",
    "start": "575680",
    "end": "582350"
  },
  {
    "text": "four out of six actually satisfies that right and from that point i can move",
    "start": "582350",
    "end": "587390"
  },
  {
    "text": "forward and say do a second epic change and go to ABC deg right and remove a b c",
    "start": "587390",
    "end": "594230"
  },
  {
    "text": "PDF from my quorum set and that works fine the neat thing about this is is",
    "start": "594230",
    "end": "599480"
  },
  {
    "text": "that you don't have these funny edge conditions the way you do and these other membership change protocols where",
    "start": "599480",
    "end": "606170"
  },
  {
    "text": "you can't tolerate multiple changes you can't tolerate rights and all of this and during this entire process i can",
    "start": "606170",
    "end": "612590"
  },
  {
    "text": "continue to talk you know accept rights accept reads and accept additional",
    "start": "612590",
    "end": "617630"
  },
  {
    "text": "failures right like if after while i'm in that second state if ii went down i",
    "start": "617630",
    "end": "623930"
  },
  {
    "text": "can just go and add h and then I have just a more complex quorum so right and",
    "start": "623930",
    "end": "629300"
  },
  {
    "text": "this is just boolean math right and so you know math is math math works and what's cool about this is is that if you",
    "start": "629300",
    "end": "638000"
  },
  {
    "text": "don't have a penalty for membership changes then you can be aggressive about",
    "start": "638000",
    "end": "643900"
  },
  {
    "text": "projecting a failure right so I can say like hey node F seems to be off I'm",
    "start": "643900",
    "end": "651170"
  },
  {
    "text": "gonna claim it's down and start to bring G in I can just as easily go back to F if that was just jitter in the network",
    "start": "651170",
    "end": "657980"
  },
  {
    "text": "or go back over to G and that's fine right both of those are perfectly legitimate cases to go to",
    "start": "657980",
    "end": "664130"
  },
  {
    "text": "and the permits the segment itself is small so getting chi in place is also at",
    "start": "664130",
    "end": "669680"
  },
  {
    "text": "name expensive operation so you know that's a little bit of new",
    "start": "669680",
    "end": "675139"
  },
  {
    "text": "content that I haven't talked about before but hopefully it's interesting",
    "start": "675139",
    "end": "680170"
  },
  {
    "text": "one of the other reasons that people tend not to use quorums is that the they're great for writing they kind of",
    "start": "680170",
    "end": "686630"
  },
  {
    "text": "suck for reading because you have to you know read if you're writing to four you have to read from three in the quorum",
    "start": "686630",
    "end": "692930"
  },
  {
    "text": "set in the quorum of six so we avoid that why because we can say like I know",
    "start": "692930",
    "end": "698740"
  },
  {
    "text": "who I wrote to and who act back to me and as long as I know that this block is",
    "start": "698740",
    "end": "706550"
  },
  {
    "text": "available on this note why would I do anything other than just read from that note so on then you know so the point is",
    "start": "706550",
    "end": "713120"
  },
  {
    "text": "is that most distributed systems of horse state but in a you know database systems are all about state and so this",
    "start": "713120",
    "end": "720589"
  },
  {
    "text": "is transient state so in the case of a crash recovery I may not have it but and",
    "start": "720589",
    "end": "726380"
  },
  {
    "text": "I may have to re-establish all of this and we'll talk about how to do that a little bit later but from the",
    "start": "726380",
    "end": "732139"
  },
  {
    "text": "perspective of an ongoing running system you know this is I can cache which nodes",
    "start": "732139",
    "end": "737269"
  },
  {
    "text": "are up-to-date and even I can cache what the latency of each you know the writes",
    "start": "737269",
    "end": "742579"
  },
  {
    "text": "and the reads were from that node so I can go and understand you know sort of the storage whether if you will inside",
    "start": "742579",
    "end": "748970"
  },
  {
    "text": "the system because it's a multi-tenant system in the storage layer and it doesn't have to be that I'm hot some",
    "start": "748970",
    "end": "754939"
  },
  {
    "text": "other you know caller to that node may be hot on some completely different activity on that disk and so I might",
    "start": "754939",
    "end": "762170"
  },
  {
    "text": "just go and you know move to some other thing so the read quorum is actually a repair quorum the only time that I",
    "start": "762170",
    "end": "768170"
  },
  {
    "text": "actually need three out of six is during a repair or a crash recovery if I've you",
    "start": "768170",
    "end": "773509"
  },
  {
    "text": "know so unfortunate as to reach all of this so I'm gonna grunt through fast and",
    "start": "773509",
    "end": "779439"
  },
  {
    "text": "you know there'll be time for questions either at the end of this talk or afterwards I can see a lot of confused",
    "start": "779439",
    "end": "785060"
  },
  {
    "text": "faces so you know let's move away from distributed systems and just talk about",
    "start": "785060",
    "end": "791750"
  },
  {
    "text": "you know why do we care right we care because it's faster right and so here's some numbers on Aurora compared",
    "start": "791750",
    "end": "799649"
  },
  {
    "text": "to my sequel 5/6 based on the suspension smart using the are 416 Excel you know",
    "start": "799649",
    "end": "805919"
  },
  {
    "text": "I've shown the variant of the slide in the past it you know it used to show five hundred thousand right one hundred",
    "start": "805919",
    "end": "812369"
  },
  {
    "text": "thousand sorry five hundred thousand reads per second and one hundred",
    "start": "812369",
    "end": "817919"
  },
  {
    "text": "thousand right per second what I mean by that is a DML operation or select not an",
    "start": "817919",
    "end": "823229"
  },
  {
    "text": "i/o and you know the nice news here is is that where we by going to a sixteen",
    "start": "823229",
    "end": "829679"
  },
  {
    "text": "excel we've doubled the right traffic so we can run two hundred thousand the rate is about the same on that that's",
    "start": "829679",
    "end": "837509"
  },
  {
    "text": "basically a packets per second limitation if we're running local we'd go higher so how do we achieve this at",
    "start": "837509",
    "end": "845639"
  },
  {
    "text": "core you know we achieve it because we do let's work and you know at v--",
    "start": "845639",
    "end": "851629"
  },
  {
    "text": "finally at the end of the day these systems databases are all about i/o",
    "start": "851629",
    "end": "857089"
  },
  {
    "text": "network systems you know where the you separated storage from compute it's all",
    "start": "857089",
    "end": "863339"
  },
  {
    "text": "about packets per second and network bandwidth and so on so you know it's",
    "start": "863339",
    "end": "868410"
  },
  {
    "text": "really about reducing the i/os minimizing the packets processing asynchronously and so what does that",
    "start": "868410",
    "end": "874649"
  },
  {
    "text": "turn to so let's compare RTS my sequel to Aurora in terms of the traffic so if",
    "start": "874649",
    "end": "880019"
  },
  {
    "text": "your there are five different types of rights that go on in my sequel there's",
    "start": "880019",
    "end": "885769"
  },
  {
    "text": "the redo log there's the bin log there's the data block the data block actually",
    "start": "885769",
    "end": "892619"
  },
  {
    "text": "gets written twice because it's not the same size as a atomic right to SSD and",
    "start": "892619",
    "end": "899009"
  },
  {
    "text": "you can deal you can end up in torn pages otherwise and there are these abominations called frm files which",
    "start": "899009",
    "end": "905549"
  },
  {
    "text": "represent to the metadata state and so",
    "start": "905549",
    "end": "911279"
  },
  {
    "text": "let's say I was writing a so I'm gonna have end up writing to an EPS volume",
    "start": "911279",
    "end": "917220"
  },
  {
    "text": "it's mirrored on the local side that I'm gonna let's say it's multi a-z so I'm gonna go and process that over to a",
    "start": "917220",
    "end": "924359"
  },
  {
    "text": "replica instance and it's gonna write a step four in step 5 now the problem with",
    "start": "924359",
    "end": "929399"
  },
  {
    "text": "this is that step one three and four are sequential and synchronous that means",
    "start": "929399",
    "end": "936360"
  },
  {
    "text": "that all of the latency is you know additive all of the jitter is additive",
    "start": "936360",
    "end": "943230"
  },
  {
    "text": "there are lots of different types of writes for any user operation and you the data blocks themselves which are the",
    "start": "943230",
    "end": "949110"
  },
  {
    "text": "largest portion of it have to be right written twice so on the 30 minutes this bench write only workload relatively",
    "start": "949110",
    "end": "956940"
  },
  {
    "text": "small data set with the largest number of Pi ops that were available at that time we were able to run about three",
    "start": "956940",
    "end": "963450"
  },
  {
    "text": "quarters of a million transactions and which averaged out to about seven point",
    "start": "963450",
    "end": "969270"
  },
  {
    "text": "four IOS per transaction let's compare that to Aurora so in the Aurora the only",
    "start": "969270",
    "end": "975930"
  },
  {
    "text": "thing I write over the network is the redo lock and so I have to write it two more places but you know the six copy",
    "start": "975930",
    "end": "983520"
  },
  {
    "text": "amplification but I'm write the redo logs are way smaller than data blocks",
    "start": "983520",
    "end": "988740"
  },
  {
    "text": "right like a hundred to one and so what we have to do is we boxcar the lot redo",
    "start": "988740",
    "end": "995430"
  },
  {
    "text": "log records together and you know ship them off using you know to the appropriate segments of partial order",
    "start": "995430",
    "end": "1001940"
  },
  {
    "text": "and then we you know issue the write all of those steps are asynchronous so we're",
    "start": "1001940",
    "end": "1006980"
  },
  {
    "text": "doing asynchronous Network processing interestingly because the data pages are",
    "start": "1006980",
    "end": "1013070"
  },
  {
    "text": "instantiated in the storage system there's we don't have this notion of checkpoints or cache replacements and",
    "start": "1013070",
    "end": "1019610"
  },
  {
    "text": "that's a really big improvement because the problem is is that there's a positive correlation in a you know in",
    "start": "1019610",
    "end": "1026420"
  },
  {
    "text": "the typical database between the activity that's happening in the foreground and the number of cache replacements and the amount of",
    "start": "1026420",
    "end": "1032449"
  },
  {
    "text": "checkpointing that's going on there it's a negative correlation in the R or if you're really hitting the cache shard",
    "start": "1032449",
    "end": "1037938"
  },
  {
    "text": "you can just throw blocks away because they're getting instantiated in the storage node so in this case even though",
    "start": "1037939",
    "end": "1043730"
  },
  {
    "text": "there's six times or more log right there's nine times less than that worked traffic and you know since it's all",
    "start": "1043730",
    "end": "1049670"
  },
  {
    "text": "asynchronous it's very tolerant and it's formed it's tolerant of network and",
    "start": "1049670",
    "end": "1054980"
  },
  {
    "text": "storage outliers so in this particular benchmark we were able to run 35 more 35",
    "start": "1054980",
    "end": "1061429"
  },
  {
    "text": "times more transactions and we were doing despite the 6x",
    "start": "1061429",
    "end": "1066780"
  },
  {
    "text": "amplification 7.7 times less iOS it's a big difference so where did all those",
    "start": "1066780",
    "end": "1074040"
  },
  {
    "text": "iOS go so they went to the storage node so what happens in the storage node is",
    "start": "1074040",
    "end": "1079530"
  },
  {
    "text": "is that we get a log record we persisted onto a disk and then we up you know put",
    "start": "1079530",
    "end": "1085200"
  },
  {
    "text": "into what we call an update queue and we acknowledge it back at that point it's persistent and we're done as far as the",
    "start": "1085200",
    "end": "1090780"
  },
  {
    "text": "head node is concerned right well all the rest of the work is background and so what we have to do after that is is",
    "start": "1090780",
    "end": "1096930"
  },
  {
    "text": "that we have to sort and groupid it since it was asynchronous they might be appearing out of order I may not have",
    "start": "1096930",
    "end": "1103830"
  },
  {
    "text": "received one of those rights because it's a network and networks have you know funny issues and so I we run the",
    "start": "1103830",
    "end": "1109440"
  },
  {
    "text": "gossip protocol amongst the peers to make sure that we fill in all the holes we coalesce those that's basically redo",
    "start": "1109440",
    "end": "1116370"
  },
  {
    "text": "generating a new version of the data page we write out of place so you know",
    "start": "1116370",
    "end": "1122340"
  },
  {
    "text": "you'll have the old version of the page and the new version of the page that removes the need to do things like",
    "start": "1122340",
    "end": "1127470"
  },
  {
    "text": "double write buffers it does mean that we have to garbage collect in the background and you know every so often",
    "start": "1127470",
    "end": "1134430"
  },
  {
    "text": "you know if we have space we'll scrub them to make sure that our CRC checks are valid and then we'll do backups of",
    "start": "1134430",
    "end": "1142380"
  },
  {
    "text": "both the log and the data blocks to s3 before we can garbage collect so the",
    "start": "1142380",
    "end": "1149970"
  },
  {
    "text": "observations here is is that all of these steps are asynchronous only steps one and two are in the foreground",
    "start": "1149970",
    "end": "1156090"
  },
  {
    "text": "latency path the input cube you know dividing that prior number seven point",
    "start": "1156090",
    "end": "1161520"
  },
  {
    "text": "seven by six because every storage ánotá is only seeing a sixth of the rights right it's the input queue is 46 times",
    "start": "1161520",
    "end": "1169710"
  },
  {
    "text": "less than my sequel right and it's we're favoring the latency-sensitive operations and we're basically doing",
    "start": "1169710",
    "end": "1177630"
  },
  {
    "text": "everything else in the background which gives you this trade-off between latency",
    "start": "1177630",
    "end": "1183240"
  },
  {
    "text": "and space so if we're running out of space we have to favor garbage collection and do standard you know",
    "start": "1183240",
    "end": "1190410"
  },
  {
    "text": "queue processing to sort of put back pressure on the input queue but if we're not we can let it build",
    "start": "1190410",
    "end": "1196560"
  },
  {
    "text": "and you know every workload has some amount of spiciness to it and so you know this lets us smooth it out with",
    "start": "1196560",
    "end": "1203010"
  },
  {
    "text": "background processing the other big",
    "start": "1203010",
    "end": "1208380"
  },
  {
    "text": "value that you get is is that since we're doing the redo processing in the storage node there's no crash recovery I",
    "start": "1208380",
    "end": "1215400"
  },
  {
    "text": "mean there is crash recovery but it's super fast because normally what you have to do in the do a traditional",
    "start": "1215400",
    "end": "1220470"
  },
  {
    "text": "database is that you know there's this classic five-minute rule where by every 5 minutes you have to write all blocks",
    "start": "1220470",
    "end": "1226790"
  },
  {
    "text": "in the O that have been modified out to disk so that you minimize the amount of log processing that would be happening",
    "start": "1226790",
    "end": "1232980"
  },
  {
    "text": "if you have to do a crash recovery now the one of the problems in my sequel is",
    "start": "1232980",
    "end": "1238290"
  },
  {
    "text": "is that that processing a single-threaded and so and of course your cache is cold so even though it was",
    "start": "1238290",
    "end": "1244770"
  },
  {
    "text": "five minutes in the foreground that might have been happening across a hundred threads and it was it was",
    "start": "1244770",
    "end": "1249960"
  },
  {
    "text": "happening it very likely across blocks that were hot in cache whereas here you",
    "start": "1249960",
    "end": "1255990"
  },
  {
    "text": "know so it might take an hour right you know I think we've all probably watched our databases take far longer than five",
    "start": "1255990",
    "end": "1263580"
  },
  {
    "text": "minutes to come up and even five minutes is a super long time to be out if you're running an internet site so what we do",
    "start": "1263580",
    "end": "1271470"
  },
  {
    "text": "in Aurora is is that the underlying storage is replaying redo log records all the time if you ask for a data page",
    "start": "1271470",
    "end": "1278040"
  },
  {
    "text": "either the page is current because we've done the coalescing or we'll make it current by basically processing the redo",
    "start": "1278040",
    "end": "1284670"
  },
  {
    "text": "log records from the last version of the page plus the log just like any log structured storage system and so we just",
    "start": "1284670",
    "end": "1292290"
  },
  {
    "text": "do that model all the time as part of a disk read and so we can also do that after a crash recovery and the log",
    "start": "1292290",
    "end": "1298920"
  },
  {
    "text": "application is of course parallel distributed asynchronous but there's no replay for startup and that ends up becoming a big deal so let's look at",
    "start": "1298920",
    "end": "1308220"
  },
  {
    "text": "what we have to do during a crash recovery so we let's say on an individual volume we might have a bunch",
    "start": "1308220",
    "end": "1315390"
  },
  {
    "text": "of writes that happen a gap where something hasn't you we've seen the write for entry 1,000 but we haven't",
    "start": "1315390",
    "end": "1322410"
  },
  {
    "text": "seen the write for you know some entry before it that was dependent upon and so there may be some gaps and so forth and",
    "start": "1322410",
    "end": "1328440"
  },
  {
    "text": "so it's kind of what I a ragged edge to the log so there's this",
    "start": "1328440",
    "end": "1333720"
  },
  {
    "text": "portion where you know it everything has gotten four out of six and then there's some stuff that's in flight the way",
    "start": "1333720",
    "end": "1339809"
  },
  {
    "text": "other systems sort of deal with that is is that they do two-phase commits two-phase commit take a long time",
    "start": "1339809",
    "end": "1346320"
  },
  {
    "text": "they're heavyweight it's kind of sucky so what we do instead is is that we push",
    "start": "1346320",
    "end": "1351809"
  },
  {
    "text": "and rather than to doing two-face commits in foreground processing we do the trimming of the ragged edge during",
    "start": "1351809",
    "end": "1360179"
  },
  {
    "text": "crash recovery so what we have to look for here is what what's the consistency point so the consistency so actually",
    "start": "1360179",
    "end": "1367139"
  },
  {
    "text": "first what is the volume complete LSN what is the LS n log sequence number you",
    "start": "1367139",
    "end": "1372809"
  },
  {
    "text": "know redo log record identifier such that there are no gaps past that point",
    "start": "1372809",
    "end": "1377879"
  },
  {
    "text": "so you know that the volume is complete past that so that's the point that's the ragged edge and you're not actually",
    "start": "1377879",
    "end": "1384059"
  },
  {
    "text": "interested in that point you're interested in the last commit before that point because anything that's not",
    "start": "1384059",
    "end": "1389429"
  },
  {
    "text": "committed is gone right and so so that's the consistency point Ellison so the",
    "start": "1389429",
    "end": "1394980"
  },
  {
    "text": "last commit right the last commit record you know before the volume come commits",
    "start": "1394980",
    "end": "1401519"
  },
  {
    "text": "LSM and so basically what we do after a crash recovery is we have to bring",
    "start": "1401519",
    "end": "1407669"
  },
  {
    "text": "everything up we have to compute these two numbers and we have to you know trim those off from the log and then we construct the database so and that's it",
    "start": "1407669",
    "end": "1415169"
  },
  {
    "text": "so it's basically some reads towards the tail of the log opening all the volumes etc so that's a summary of sort of like",
    "start": "1415169",
    "end": "1424649"
  },
  {
    "text": "basic or or all of the stuff I described what it must true three years ago when we launched it let's look at some more",
    "start": "1424649",
    "end": "1431730"
  },
  {
    "text": "recent improvements so a few months ago",
    "start": "1431730",
    "end": "1438419"
  },
  {
    "text": "we launched fast database cloning and so you know an advanced database volume",
    "start": "1438419",
    "end": "1444919"
  },
  {
    "text": "manager you know say EMC or NetApp filer or what have you will basically do a",
    "start": "1444919",
    "end": "1452370"
  },
  {
    "text": "copy-on-write clone of a volume and what and so we do as well and so what that means is you can clone the database",
    "start": "1452370",
    "end": "1458879"
  },
  {
    "text": "without copying the data and that since you don't have to copy the state and database the creation of the clone is",
    "start": "1458879",
    "end": "1465809"
  },
  {
    "text": "nearly instantaneous you know certainly sub minute and what happens then is that I've created this clone volume and as I",
    "start": "1465809",
    "end": "1473910"
  },
  {
    "text": "write either on the originals database or on the new database we end up creating copies right and if we",
    "start": "1473910",
    "end": "1480480"
  },
  {
    "text": "haven't done that we can both read the share data so that saves both space and it also saves time in terms of creating",
    "start": "1480480",
    "end": "1488190"
  },
  {
    "text": "the copy and this I think is going to be a really powerful feature for everyone because think of about the the value of",
    "start": "1488190",
    "end": "1496710"
  },
  {
    "text": "being able to clone the production database to run your tests right and so you've got some high fidelity data set",
    "start": "1496710",
    "end": "1502890"
  },
  {
    "text": "you can just run it and you know or maybe you want to do a reorganization",
    "start": "1502890",
    "end": "1507990"
  },
  {
    "text": "like I want to see whether if I add this index and against the actual data and",
    "start": "1507990",
    "end": "1513000"
  },
  {
    "text": "run the same workload against it it is better or worse right because I've added an index so the inserts are gonna be",
    "start": "1513000",
    "end": "1518970"
  },
  {
    "text": "slower but the queries are gonna be faster how do i trade that off and you know it's much better to be able to do",
    "start": "1518970",
    "end": "1524550"
  },
  {
    "text": "that against your actual data and your actual database and to be able to create that index without impacting your",
    "start": "1524550",
    "end": "1530550"
  },
  {
    "text": "production database right and you can do things like yeah push a bin log from one",
    "start": "1530550",
    "end": "1536100"
  },
  {
    "text": "database to the other to make sure that you get those updates going on you know sometimes you just want to save off a",
    "start": "1536100",
    "end": "1541559"
  },
  {
    "text": "point-in-time snapshot and without impacting something so someone says I think there's something weird going on",
    "start": "1541559",
    "end": "1546960"
  },
  {
    "text": "here well you know let me rather than shutting down the activity let it keep going forward but I've got a copy of",
    "start": "1546960",
    "end": "1552929"
  },
  {
    "text": "that system that I can use for forensic analysis right let's briefly review how",
    "start": "1552929",
    "end": "1561210"
  },
  {
    "text": "backup and restore works in the Roar so what we do in Aurora is is that we backup both the redo log and the data",
    "start": "1561210",
    "end": "1569520"
  },
  {
    "text": "blocks and you know as we reach a point where a point of consistency where we",
    "start": "1569520",
    "end": "1574650"
  },
  {
    "text": "can say all of the segments have all of the segments that comprise the data",
    "start": "1574650",
    "end": "1579720"
  },
  {
    "text": "volume have reached at least this point we can say that okay the snapshot has occurred to this point of truth right so",
    "start": "1579720",
    "end": "1586170"
  },
  {
    "text": "everything's happening in background all the time and what we're doing is we're just advancing the point in time and we can say like okay this is the back of",
    "start": "1586170",
    "end": "1592890"
  },
  {
    "text": "hello cell right very similar to the volume complete Ellis that I referred to earlier so we're just continually streaming that",
    "start": "1592890",
    "end": "1599010"
  },
  {
    "text": "stuff and you know that's just part of the system it there's no such thing as",
    "start": "1599010",
    "end": "1604049"
  },
  {
    "text": "like okay snapshot now and then you suddenly get a burst of iOS happening in the system that interfere with",
    "start": "1604049",
    "end": "1610049"
  },
  {
    "text": "foreground processing and at restore we just have to find the right stuff and pull it back down both data blocks and",
    "start": "1610049",
    "end": "1616139"
  },
  {
    "text": "redo log and that also happens in parallel synchronous so you know that",
    "start": "1616139",
    "end": "1621779"
  },
  {
    "text": "works it works fine it's a good deal faster than restoring a a traditional",
    "start": "1621779",
    "end": "1628409"
  },
  {
    "text": "database volume because it's happening so parallel but you know normally the",
    "start": "1628409",
    "end": "1634350"
  },
  {
    "text": "times when you're doing this operation are times that you're experiencing the outage usually due to something like I",
    "start": "1634350",
    "end": "1640019"
  },
  {
    "text": "dropped the wrong table or something like that or I you know ran this insert statement or query you know delete statement",
    "start": "1640019",
    "end": "1647010"
  },
  {
    "text": "without the right where clause you know not equal instead of equal we've all done it so you know we've also",
    "start": "1647010",
    "end": "1654779"
  },
  {
    "text": "introduced coming soon you know and generally everything that says coming soon is like inside the next three",
    "start": "1654779",
    "end": "1661019"
  },
  {
    "text": "months right backtrack which is",
    "start": "1661019",
    "end": "1666889"
  },
  {
    "text": "essentially near instantaneous restores and what we're doing here is we're saying you know we're not destructively",
    "start": "1666919",
    "end": "1674580"
  },
  {
    "text": "writing data on disk and we're garbage collecting it if you said you wanted to keep an extra gigabyte of data which",
    "start": "1674580",
    "end": "1685679"
  },
  {
    "text": "represents an hour of log creation and you're of a database volume you know you",
    "start": "1685679",
    "end": "1690720"
  },
  {
    "text": "should be able to pay for it and then be able to restore backwards in point in time right and be able to do that in the",
    "start": "1690720",
    "end": "1697139"
  },
  {
    "text": "volume without needing to write anything without meaning to read anything so this",
    "start": "1697139",
    "end": "1703139"
  },
  {
    "text": "is just a metadata operation where we're just moving pointers around and what we're doing is we're marking regions of",
    "start": "1703139",
    "end": "1708980"
  },
  {
    "text": "database those log processing as invisible so you know we still walk them",
    "start": "1708980",
    "end": "1714240"
  },
  {
    "text": "forward we just don't apply them and the nice thing about this model is is that",
    "start": "1714240",
    "end": "1719940"
  },
  {
    "text": "we also don't destructively overwrite when we're doing a backtrack so if you say I want to go back fifteen minutes",
    "start": "1719940",
    "end": "1726890"
  },
  {
    "text": "the gosh that was wrong let me go back to 13 minutes okay wait that was too far how about 14 minutes okay that's perfect",
    "start": "1726890",
    "end": "1733640"
  },
  {
    "text": "right and that's just happening in the space and you know just like a jog dial and so that works really pretty well I",
    "start": "1733640",
    "end": "1742700"
  },
  {
    "text": "think you know it's a feature that we're if you're running a production database you pretty much should just have on and",
    "start": "1742700",
    "end": "1748490"
  },
  {
    "text": "I think it's just adds a lot of value a lot of what we're focusing on thus far",
    "start": "1748490",
    "end": "1753650"
  },
  {
    "text": "in the talk is availability you can see that it's just different angles to availability and definitely one of the",
    "start": "1753650",
    "end": "1759919"
  },
  {
    "text": "issues that can arise is use their error right and so you want to be able to deal with those as well another thing that we",
    "start": "1759919",
    "end": "1768320"
  },
  {
    "text": "added just a couple of weeks ago is auto scaling of read replicas and so",
    "start": "1768320",
    "end": "1773780"
  },
  {
    "text": "basically if you have a lot of load you will just automatically add read replicas as you need all the read",
    "start": "1773780",
    "end": "1779390"
  },
  {
    "text": "replicas are attached to the same storage volume as the primary master they're all available as failover",
    "start": "1779390",
    "end": "1786470"
  },
  {
    "text": "targets for the master you can have up to 15 of them across your three AZ's",
    "start": "1786470",
    "end": "1792100"
  },
  {
    "text": "typically our re the Aurora read replicas use log processing redo log",
    "start": "1792100",
    "end": "1800450"
  },
  {
    "text": "processing to update their caches now you don't have to update the storage volumes right visit one storage volume",
    "start": "1800450",
    "end": "1805640"
  },
  {
    "text": "but you do have caches that are different in each of them and you just want to make sure that if I get a redo",
    "start": "1805640",
    "end": "1812450"
  },
  {
    "text": "record if either it's in my cache or it's not if it's not I just throw it away if it is inside my cache that I",
    "start": "1812450",
    "end": "1817880"
  },
  {
    "text": "need to update it and you know I'm focusing on data caches but there are also lots of other caches and my sequel",
    "start": "1817880",
    "end": "1823910"
  },
  {
    "text": "and any other database metadata etc so you have to deal with all of that generally we found that our replication",
    "start": "1823910",
    "end": "1831200"
  },
  {
    "text": "lag in Aurora tends to be in the 10 millisecond kind of range and you know",
    "start": "1831200",
    "end": "1836870"
  },
  {
    "text": "that's just I see somebody laughing here because I'm sure he's used to my sequel bin Laden",
    "start": "1836870",
    "end": "1841880"
  },
  {
    "text": "replication it can be a bit longer and you can imagine that with you know if",
    "start": "1841880",
    "end": "1848210"
  },
  {
    "text": "you're doing 200,000 writes per second you know you'd be waiting a long long time so you know I think that that's",
    "start": "1848210",
    "end": "1854330"
  },
  {
    "text": "pretty cool with a load balanced reader endpoint that does a load balancing across these things",
    "start": "1854330",
    "end": "1860179"
  },
  {
    "text": "and auto-scaling and so then you know you can just right and it'll automatically go to the one you need to",
    "start": "1860179",
    "end": "1867669"
  },
  {
    "text": "last reinvent I talked about online DDL where you know if you're familiar with my sequel it does do you know so to",
    "start": "1868330",
    "end": "1874940"
  },
  {
    "text": "speak online DDL but what it's doing underneath the covers is a full day to be a full table copy in the background",
    "start": "1874940",
    "end": "1880759"
  },
  {
    "text": "it rebuilds all the indices you know it can take a long time and then your run",
    "start": "1880759",
    "end": "1886940"
  },
  {
    "text": "basically the DML is going into a new table that basically gets up copied into that needs to get reapplied after you do",
    "start": "1886940",
    "end": "1894139"
  },
  {
    "text": "the switchover and while you're doing that switch over there's a table lock right so it's kind of slow so what we do",
    "start": "1894139",
    "end": "1904850"
  },
  {
    "text": "is something very similar to redo logging just schema logging and so we're",
    "start": "1904850",
    "end": "1910220"
  },
  {
    "text": "we introduced a new dictionary table where you know we do something like here's the table table one we're doing",
    "start": "1910220",
    "end": "1917179"
  },
  {
    "text": "an operation at column this is a column name and this is when we did it and then we're done and then we get you know and",
    "start": "1917179",
    "end": "1923509"
  },
  {
    "text": "it's got a timestamp against and when we inspect a page we have to say like okay is there a metadata change that's",
    "start": "1923509",
    "end": "1929419"
  },
  {
    "text": "happened to this page and if there has been we need to go and update it right and it's basically a modify on write",
    "start": "1929419",
    "end": "1936080"
  },
  {
    "text": "similar to the copy-on-write thing that was talking about earlier with cloning last year we added support for adding a",
    "start": "1936080",
    "end": "1944119"
  },
  {
    "text": "nullable column at the end of the table you know in the next few weeks we'll add",
    "start": "1944119",
    "end": "1949940"
  },
  {
    "text": "add a column anywhere and with or without a default and so that I think",
    "start": "1949940",
    "end": "1955399"
  },
  {
    "text": "should be useful to a lot of people there are a lot of other you know DDL operations that we should support you",
    "start": "1955399",
    "end": "1961789"
  },
  {
    "text": "know it's done a little bit slower than that have liked but you know we'll do it and if you look at the numbers I mean",
    "start": "1961789",
    "end": "1969289"
  },
  {
    "text": "it's ridiculous I mean you can see that by sevens a ton faster than five six in",
    "start": "1969289",
    "end": "1974450"
  },
  {
    "text": "this regard so if you're using five six you should really think about five seven but you know it's like many orders of",
    "start": "1974450",
    "end": "1981710"
  },
  {
    "text": "magnitude different right and it just doesn't make any sense you know what how different the approaches so one of the",
    "start": "1981710",
    "end": "1990619"
  },
  {
    "text": "you know we've often focused in general we folk stunt throughput for Aurora you know",
    "start": "1990619",
    "end": "1997190"
  },
  {
    "text": "like how many transactions per second can you push through this system or how many selects can you push through the",
    "start": "1997190",
    "end": "2003279"
  },
  {
    "text": "system sometimes we get people saying like hey I didn't get that 5x you promised and we all go and look at it",
    "start": "2003279",
    "end": "2009250"
  },
  {
    "text": "and it turns out that they're running one threat of sequential you know select statements or once the right of",
    "start": "2009250",
    "end": "2014350"
  },
  {
    "text": "consecutive insert statements and yo it's kind of hard to speed those things up right because there's just a basic",
    "start": "2014350",
    "end": "2022720"
  },
  {
    "text": "amount of work and we can't do any of our tricks with throughput processing de synchrony and you know batching and all",
    "start": "2022720",
    "end": "2028299"
  },
  {
    "text": "of that stuff to make stuff faster if you've only given me one unit of work to go do at a time but that doesn't mean",
    "start": "2028299",
    "end": "2035320"
  },
  {
    "text": "that's not a legitimate thing to go --speed those things up and so here's a set of things that we've been doing for",
    "start": "2035320",
    "end": "2041470"
  },
  {
    "text": "a single query or single you know operation performance and most of this",
    "start": "2041470",
    "end": "2048010"
  },
  {
    "text": "is so asynchronous keep free fetch so",
    "start": "2048010",
    "end": "2053560"
  },
  {
    "text": "one of the challenges we had in Aurora is is that if you're using a standard my",
    "start": "2053560",
    "end": "2059320"
  },
  {
    "text": "sequel you're going to make an i/o request through the Linux kernel and",
    "start": "2059320",
    "end": "2064419"
  },
  {
    "text": "it's going to try to recognize whether those iOS are happening in sequence and if there's i/o so it thinks are",
    "start": "2064419",
    "end": "2070240"
  },
  {
    "text": "happening in sequence it'll start to prefetch blocks out and so that's cool if your database is laid out that way",
    "start": "2070240",
    "end": "2077770"
  },
  {
    "text": "and you know it often starts out in a benchmark laid out that way but then you",
    "start": "2077770",
    "end": "2083230"
  },
  {
    "text": "know the way B trees work you end up with random access patterns and so it doesn't works super well and so what",
    "start": "2083230",
    "end": "2091240"
  },
  {
    "text": "we're doing now is is that we basically look at the B tree pattern and decide",
    "start": "2091240",
    "end": "2096629"
  },
  {
    "text": "dynamically whether you're likely to be accessing a lot more blocks and then pick the block - meaning to access and",
    "start": "2096630",
    "end": "2102609"
  },
  {
    "text": "you know we're doing Network calls about IO calls and we just start to load those into memory and so this is the latency",
    "start": "2102609",
    "end": "2111820"
  },
  {
    "text": "improvement factor versus what you know current my sequel batch key access joins using you know basically this is T PCH",
    "start": "2111820",
    "end": "2119560"
  },
  {
    "text": "you know slight variations and that T PCH it's got very formal rules so this is basically the same queries and data",
    "start": "2119560",
    "end": "2125800"
  },
  {
    "text": "but you know tcch formally running on our 3/8 excel and you know you can see that at least",
    "start": "2125800",
    "end": "2133210"
  },
  {
    "text": "on the cold data set we're getting you know anywhere from no improvement to 4 X",
    "start": "2133210",
    "end": "2139330"
  },
  {
    "text": "- 10 X - 14 X you know and so you know there are cases where it matters and",
    "start": "2139330",
    "end": "2144760"
  },
  {
    "text": "basic and these are all against cold buffers and so if you're running against a cached workload this obviously",
    "start": "2144760",
    "end": "2151540"
  },
  {
    "text": "prefetch isn't gonna help at all right but if you are running a large query",
    "start": "2151540",
    "end": "2157240"
  },
  {
    "text": "that has to do a scan it's a pretty big deal",
    "start": "2157240",
    "end": "2161760"
  },
  {
    "text": "speaking of batch scans you know after you get the iOS and the way my sequel",
    "start": "2162540",
    "end": "2167650"
  },
  {
    "text": "works around the slide is that you do a bunch of you know calls that are row at",
    "start": "2167650",
    "end": "2176200"
  },
  {
    "text": "a time repeated function calls locking latching cursor stores format changes from nodb",
    "start": "2176200",
    "end": "2183490"
  },
  {
    "text": "to my sequel formats you know there's just a ton of work and so we've started moving to a batch format for cable scans",
    "start": "2183490",
    "end": "2190780"
  },
  {
    "text": "index scans index range scans and you know it's been worse about anywhere from you know for the queries where that",
    "start": "2190780",
    "end": "2197940"
  },
  {
    "text": "matters it's been worth somewhere between 50 to 100 percent yeah so that's pretty cool as well hash joints you know",
    "start": "2197940",
    "end": "2206770"
  },
  {
    "text": "so any reasonable database should have hash joints so we've added those are coming soon again on T PCH you're seeing",
    "start": "2206770",
    "end": "2215680"
  },
  {
    "text": "you know pretty significant improvements on us the subset of queries where it makes sense to use that relative to",
    "start": "2215680",
    "end": "2221980"
  },
  {
    "text": "batch key access and if you're not familiar with hash joins the basic idea is that you know you have two sides to a",
    "start": "2221980",
    "end": "2228430"
  },
  {
    "text": "join one which you know your first I should say a joint is where you're saying like you know like something like",
    "start": "2228430",
    "end": "2234130"
  },
  {
    "text": "a equals B and you in one table and another you know just you know whatever",
    "start": "2234130",
    "end": "2242710"
  },
  {
    "text": "and what you do in this case is that you build a hash table of a smaller side",
    "start": "2242710",
    "end": "2248620"
  },
  {
    "text": "just ship it over to the other side and you probe the hash table to find matches it's a pretty standard approach and so",
    "start": "2248620",
    "end": "2255250"
  },
  {
    "text": "those are basically a completion of some of the near-term improvement that are happening you can see that",
    "start": "2255250",
    "end": "2261160"
  },
  {
    "text": "we're sort of trying to advance towards dealing with larger queries and so forth and so serverless so serverless is kind",
    "start": "2261160",
    "end": "2274180"
  },
  {
    "text": "of an odd thing to try to think about from a database perspective right because you know it's you can say like",
    "start": "2274180",
    "end": "2281230"
  },
  {
    "text": "okay I've got some lambda function I'd just like to run a request I willing to",
    "start": "2281230",
    "end": "2286839"
  },
  {
    "text": "eat some latency that's fine and you know in that context a request based service system makes sense but what",
    "start": "2286839",
    "end": "2293859"
  },
  {
    "text": "about a system which actually benefits from having a warm buffer cache and where you want to run you know a few",
    "start": "2293859",
    "end": "2300940"
  },
  {
    "text": "thousand a few hundred thousand requests per second but you might still want that to be server less to get the advantage",
    "start": "2300940",
    "end": "2307240"
  },
  {
    "text": "of scale up scale down you might have variable workloads you might have cyclical workloads you might have",
    "start": "2307240",
    "end": "2312490"
  },
  {
    "text": "unpredictable workloads you might just not want to deal with the workload right and so you know there are lots of I",
    "start": "2312490",
    "end": "2318250"
  },
  {
    "text": "would say the large majority of databases eighty ninety percent are not things that have a consistent level of",
    "start": "2318250",
    "end": "2324910"
  },
  {
    "text": "traffic that's always on all the time and for those things that maybe make sense to shut them down and shut them",
    "start": "2324910",
    "end": "2331869"
  },
  {
    "text": "down when not in use for those things that maybe make sense to go and say like oh it's spiking usage and I want to",
    "start": "2331869",
    "end": "2339250"
  },
  {
    "text": "rather than brown out or build a queue I want to actually just make my database bigger or smaller right so that's",
    "start": "2339250",
    "end": "2346029"
  },
  {
    "text": "basically what we're building here so the idea here is is that it starts up on",
    "start": "2346029",
    "end": "2351460"
  },
  {
    "text": "demand meaning a query hits a sequel request hits it it shuts down when not",
    "start": "2351460",
    "end": "2356529"
  },
  {
    "text": "in use it scales up and down automatically based on you know whether you're out of memory or CPU or whatever",
    "start": "2356529",
    "end": "2364079"
  },
  {
    "text": "interestingly no application impact when scaling and it's paper second one minute",
    "start": "2364079",
    "end": "2371410"
  },
  {
    "text": "minimum and so what were you what you see in this picture is you know you're very familiar application a database",
    "start": "2371410",
    "end": "2377559"
  },
  {
    "text": "endpoint basically an IP address underneath the covers a request router that's it we'll have we'll touch on that",
    "start": "2377559",
    "end": "2385269"
  },
  {
    "text": "a little bit and then scalable database capacity so you'll have one database node but it's going to either grow or",
    "start": "2385269",
    "end": "2391119"
  },
  {
    "text": "shrink based on the wormhole of instances that we manage and obviously have a database volume you",
    "start": "2391119",
    "end": "2396820"
  },
  {
    "text": "know the same six copies all the rest of it that's there all the time right you don't want to just because you want the",
    "start": "2396820",
    "end": "2403090"
  },
  {
    "text": "database instance to go down doesn't mean you want through data to go away right so so what happens when you",
    "start": "2403090",
    "end": "2411760"
  },
  {
    "text": "provision the instance so what we do is is that we give you a V PC endpoint",
    "start": "2411760",
    "end": "2417580"
  },
  {
    "text": "alright we give you an endpoint in each of your 3 V pcs and it's connected to an IP address and we set up a network load",
    "start": "2417580",
    "end": "2425770"
  },
  {
    "text": "balancer and we create a storage volume and you know that's basically it we don't create the database instance until",
    "start": "2425770",
    "end": "2432070"
  },
  {
    "text": "the first request arrives so the first",
    "start": "2432070",
    "end": "2437200"
  },
  {
    "text": "request triggers the instance provisioning because of the warm pool that typically takes I think that slide",
    "start": "2437200",
    "end": "2442930"
  },
  {
    "text": "may be wrong it's typically around three to five seconds which is pretty decent right you know and then when it so scale",
    "start": "2442930",
    "end": "2451270"
  },
  {
    "text": "up and down as your workload changes again that's usually one two three seconds and the instances hibernate",
    "start": "2451270",
    "end": "2457990"
  },
  {
    "text": "after a user-defined period of inactivity and the scaling operations are a transparent why are they why",
    "start": "2457990",
    "end": "2464680"
  },
  {
    "text": "scaling up and down transparent it's because inside the request layer we maintain your session state and so we'll",
    "start": "2464680",
    "end": "2472120"
  },
  {
    "text": "go and bring up another instance let's say you need to scale up and then we'll find a point when you don't have a",
    "start": "2472120",
    "end": "2478480"
  },
  {
    "text": "request active right now and then we'll transfer the sessions over to the new location okay and you know of course the",
    "start": "2478480",
    "end": "2485980"
  },
  {
    "text": "database storage is persisted until explicitly you delete it and so what you",
    "start": "2485980",
    "end": "2491080"
  },
  {
    "text": "basically specify in a service instance is this is the minimum size I want this",
    "start": "2491080",
    "end": "2496930"
  },
  {
    "text": "is the maximum size I want and this is how long I want to wait before I",
    "start": "2496930",
    "end": "2504400"
  },
  {
    "text": "hibernate this database right and so you can do something you know like that looks a lot like existing Aurora by",
    "start": "2504400",
    "end": "2510160"
  },
  {
    "text": "saying oh I want to are for you know two XL minimum maximum never hibernate right",
    "start": "2510160",
    "end": "2516820"
  },
  {
    "text": "that's basically the server version of serverless right we'll spin it up and we won't shut it down or you can say look I",
    "start": "2516820",
    "end": "2524050"
  },
  {
    "text": "want to it to go and from a t too small all the way up to an R 4:16 Excel and I want to you know just",
    "start": "2524050",
    "end": "2531720"
  },
  {
    "text": "shut it down after half an hour right and so that's useful you can use that",
    "start": "2531720",
    "end": "2537210"
  },
  {
    "text": "for dev test instances you can use that in your CI CD pipelines right because",
    "start": "2537210",
    "end": "2542400"
  },
  {
    "text": "someone makes a change to the application you want to run a bunch of tests pretty much always that involves",
    "start": "2542400",
    "end": "2547560"
  },
  {
    "text": "using a database right you could have cloned your database and have it run off of that and you know so all of those",
    "start": "2547560",
    "end": "2553830"
  },
  {
    "text": "sorts of things are valid as well as things like I have a game and you know the games have unpredictable workload I",
    "start": "2553830",
    "end": "2560849"
  },
  {
    "text": "don't want to deal with it just you know go and have it go up and down you know have games that have become less popular",
    "start": "2560849",
    "end": "2567869"
  },
  {
    "text": "but then eventually become you know maybe you know through some old reference and some movie you know become",
    "start": "2567869",
    "end": "2574470"
  },
  {
    "text": "suddenly more popular you know so you can't predict those things and then you also have cyclical apps right like I'm",
    "start": "2574470",
    "end": "2581340"
  },
  {
    "text": "responsible for performance reviews as a manager right at Amazon so you know the",
    "start": "2581340",
    "end": "2586400"
  },
  {
    "text": "HR team is always using our performance management application you know twice a",
    "start": "2586400",
    "end": "2591690"
  },
  {
    "text": "year every manager is suddenly storming onto that system and it's freaking slow right and during that time it would be",
    "start": "2591690",
    "end": "2598619"
  },
  {
    "text": "nice if it just could scale up and you know afford those periods without anyone dealing with it right you know and I'm",
    "start": "2598619",
    "end": "2604890"
  },
  {
    "text": "sure you all have those sorts of applications as well it's actually the majority use case by far so that's what",
    "start": "2604890",
    "end": "2612810"
  },
  {
    "text": "I've got on server list let me go a little bit fast through multi master",
    "start": "2612810",
    "end": "2619220"
  },
  {
    "text": "it's a multi master so there really have been two approaches to dealing with",
    "start": "2619220",
    "end": "2626070"
  },
  {
    "text": "multi master problems in the relational database world the first one is shared disk and so what",
    "start": "2626070",
    "end": "2632490"
  },
  {
    "text": "you do in shared disk is you say every instance of the database can access the common storage and what you do is you",
    "start": "2632490",
    "end": "2639180"
  },
  {
    "text": "fuse the caches together so all the data they all excuse me all the data is available to all nodes and it makes it",
    "start": "2639180",
    "end": "2645839"
  },
  {
    "text": "really easy to build an app right because you can just send any request anywhere and it's basically the same",
    "start": "2645839",
    "end": "2650940"
  },
  {
    "text": "kind of cache coherency protocol as you might be you know familiar with in running multi socket multi processors on",
    "start": "2650940",
    "end": "2657359"
  },
  {
    "text": "a motherboard you know and what the system is doing underneath the covers it's making sure",
    "start": "2657359",
    "end": "2662630"
  },
  {
    "text": "that if somebody's writing that nobody else has that thing dirty and it's writing you know you have to get it from",
    "start": "2662630",
    "end": "2668869"
  },
  {
    "text": "memory or you have to get a ship from the other socket or if somebody is reading then they need to if somebody",
    "start": "2668869",
    "end": "2674000"
  },
  {
    "text": "else has it dirty then they need to get the move from dirty to read only and",
    "start": "2674000",
    "end": "2679339"
  },
  {
    "text": "then you know I have to get that copy all of that kind of traffic so the cache coherence traffic is pretty much on a",
    "start": "2679339",
    "end": "2688130"
  },
  {
    "text": "per lock basis and so if you're running a two node cluster half the time you know your typical data item it will be",
    "start": "2688130",
    "end": "2693800"
  },
  {
    "text": "local half the time it'll be a remote and you'll be running a lot of traffic for those the networking can be expensive you know typically these",
    "start": "2693800",
    "end": "2700700"
  },
  {
    "text": "systems don't have a lot of nodes in them and the nodes that tend to be close together right and running you know",
    "start": "2700700",
    "end": "2707470"
  },
  {
    "text": "sophisticated stuff like InfiniBand and so forth and if you have a hot block it's very similar to running you know",
    "start": "2707470",
    "end": "2714200"
  },
  {
    "text": "high-end application you know system software on the multiprocessor you end up having a lot of cache heat and things",
    "start": "2714200",
    "end": "2721339"
  },
  {
    "text": "pinging back and forth a negative scale so that's one model it's very popular in",
    "start": "2721339",
    "end": "2727190"
  },
  {
    "text": "the broader world Oracle right looks a lot like this the other model is to do a",
    "start": "2727190",
    "end": "2734060"
  },
  {
    "text": "shared nothing system where you know the application can continue to write to any in one of these nodes and the database",
    "start": "2734060",
    "end": "2742160"
  },
  {
    "text": "takes responsibility for sharding the data across a multitude of database of data ranges and then basically",
    "start": "2742160",
    "end": "2749210"
  },
  {
    "text": "distributing the sequel statement and then converging it back so the linkage here is in the sequel tier so there's a",
    "start": "2749210",
    "end": "2756800"
  },
  {
    "text": "lot less coherence traffic here what it's really just for commits so because you need to make sure that if I wrote",
    "start": "2756800",
    "end": "2763190"
  },
  {
    "text": "this piece of data you wrote that piece of data and somebody else did the opposite it doesn't matter so that means",
    "start": "2763190",
    "end": "2768950"
  },
  {
    "text": "that you can scale to a lot of nodes the problem here is is that you know two-phase commit packs those commits are",
    "start": "2768950",
    "end": "2774589"
  },
  {
    "text": "very heavyweight the membership change algorithms are very heavyweight since its range partitioning you typically end",
    "start": "2774589",
    "end": "2782300"
  },
  {
    "text": "up with Hart Park you can easily end up with hot partitions because it's not just a hot block now I'm doing something",
    "start": "2782300",
    "end": "2789079"
  },
  {
    "text": "at a partition level so let's say partition the obvious way by date so that because my most of my queries",
    "start": "2789079",
    "end": "2794870"
  },
  {
    "text": "were by date it's very likely that the inserts are all happening into the last partition okay and cross partition",
    "start": "2794870",
    "end": "2801620"
  },
  {
    "text": "operations become expensive so these systems are much better at small request you see it very commonly in systems that",
    "start": "2801620",
    "end": "2808820"
  },
  {
    "text": "I would categorize as no sequel right and you know like Cassandra looks fair",
    "start": "2808820",
    "end": "2814850"
  },
  {
    "text": "bit like this I would say we're doing something different it's a complicated",
    "start": "2814850",
    "end": "2821480"
  },
  {
    "text": "page sorry about that and so what we're what we sort of say is is that you know",
    "start": "2821480",
    "end": "2827180"
  },
  {
    "text": "what there are OS ease of consistency in Aurora and so like the head node knows",
    "start": "2827180",
    "end": "2835120"
  },
  {
    "text": "exactly what order each transaction in that day yeah and its node should be",
    "start": "2835120",
    "end": "2840190"
  },
  {
    "text": "another head node same thing any given storage volume knows the partial order",
    "start": "2840190",
    "end": "2845990"
  },
  {
    "text": "of rights that it you know received and must be satisfied right and so if you",
    "start": "2845990",
    "end": "2852560"
  },
  {
    "text": "believe that then you know if you can know that those two things then the only time that you could possibly have a",
    "start": "2852560",
    "end": "2858740"
  },
  {
    "text": "conflict if a is you know given that all of these guys have observables individual state is when data has",
    "start": "2858740",
    "end": "2866210"
  },
  {
    "text": "changed both at multiple database nodes and at multiple storage nodes right so",
    "start": "2866210",
    "end": "2873080"
  },
  {
    "text": "if two guys are writing to the same storage volume it's ordered and that's fine if you know the same guy is writing",
    "start": "2873080",
    "end": "2883460"
  },
  {
    "text": "to two storage volumes well that's just regular old Aurora that's totally fine right and so since there's much you know",
    "start": "2883460",
    "end": "2890090"
  },
  {
    "text": "we're doing conflict resolution here it's there's much less coordination that's required and you know the way",
    "start": "2890090",
    "end": "2896780"
  },
  {
    "text": "what we do is is that when there is coordination it's basically hierarchically managed through ledger and like this is a case where you know",
    "start": "2896780",
    "end": "2905300"
  },
  {
    "text": "the blue guy is writing to page one than page two and the orange guy is writing to page two and the page one and you",
    "start": "2905300",
    "end": "2912170"
  },
  {
    "text": "know one write succeeded on page one and the other one was succeeded on page two and so wait a second now I'm not able to",
    "start": "2912170",
    "end": "2918380"
  },
  {
    "text": "achieve something and so both of these the storage node is aware that it",
    "start": "2918380",
    "end": "2925010"
  },
  {
    "text": "received something that it couldn't satisfied and so you can't be satisfied at an individual storage node it can't",
    "start": "2925010",
    "end": "2931970"
  },
  {
    "text": "be stopped by the individual head node so what we have done is as we elect a leader you know an uber master if you",
    "start": "2931970",
    "end": "2938150"
  },
  {
    "text": "will and if one of its responsibilities is to manage a ledger and all the ledger is is you know fancy word for",
    "start": "2938150",
    "end": "2943880"
  },
  {
    "text": "effectively are redo log by a different name a different kind of redo log but and",
    "start": "2943880",
    "end": "2949060"
  },
  {
    "text": "it's responsible for saying okay who's the victim right and so the idea here is",
    "start": "2949060",
    "end": "2955580"
  },
  {
    "text": "to you know that's very similar to like dealing with lock conflicts in a",
    "start": "2955580",
    "end": "2962510"
  },
  {
    "text": "traditional lock manager and picking a victim when he had of the deadlock and so you just basically have to do that",
    "start": "2962510",
    "end": "2967700"
  },
  {
    "text": "the cool thing about this is that you can make it hierarchical so in some sense the database and the storage node",
    "start": "2967700",
    "end": "2974570"
  },
  {
    "text": "are providing the base level of ordering their uber master is providing the next",
    "start": "2974570",
    "end": "2979820"
  },
  {
    "text": "level of ordering and you can imagine that if you're building a multi master multi region then if you have regional",
    "start": "2979820",
    "end": "2985400"
  },
  {
    "text": "conflicts where something is writing across multiple regions then you can just basically elect a Hoover master you",
    "start": "2985400",
    "end": "2991730"
  },
  {
    "text": "know across regions as well right let's quickly look at crash recovery here so",
    "start": "2991730",
    "end": "3000190"
  },
  {
    "text": "crash recovery you know we discussed what single master looks like multi master is actually more complicated and and that's because losing one database",
    "start": "3000190",
    "end": "3008760"
  },
  {
    "text": "node is doesn't mean the other ones gone right so in this case you've got again",
    "start": "3008760",
    "end": "3014230"
  },
  {
    "text": "the blue guy and the orange guy the blue guy went away so here we just need to go",
    "start": "3014230",
    "end": "3020110"
  },
  {
    "text": "and make sure you know they each individually have a volume complete LSN they each individually have a completion",
    "start": "3020110",
    "end": "3026860"
  },
  {
    "text": "point thing and so you have to do that's a ragged trim that I talked about earlier but just for one of the notes",
    "start": "3026860",
    "end": "3033400"
  },
  {
    "text": "and so you know that has implications on how we record Alison's Ellison's and",
    "start": "3033400",
    "end": "3038470"
  },
  {
    "text": "transaction IDs and all the rest of it they have to contain the you know the head node ID multi master multi-region",
    "start": "3038470",
    "end": "3049030"
  },
  {
    "text": "multi master is basically that same protocol what we do is we are doing that",
    "start": "3049030",
    "end": "3056410"
  },
  {
    "text": "same replication thing that I talked about within which is physical replication using redo",
    "start": "3056410",
    "end": "3062380"
  },
  {
    "text": "logs we ship it across regions so that'll actually pretty cool it's actually a separable feature we should",
    "start": "3062380",
    "end": "3067780"
  },
  {
    "text": "be launching that in q1 sometime and you know that gives you cross region physical replication and you know",
    "start": "3067780",
    "end": "3074260"
  },
  {
    "text": "something under ten seconds I would think because you're mostly gated there by the you know physical distance of the",
    "start": "3074260",
    "end": "3081280"
  },
  {
    "text": "wire and that's again I think a pretty cool thing and the you know the case of",
    "start": "3081280",
    "end": "3086410"
  },
  {
    "text": "doing that with multi-master what you're basically doing is I have a local partition you have a you know I have a",
    "start": "3086410",
    "end": "3091600"
  },
  {
    "text": "remote partition and I have to do the same kind of thing that I do in mult in",
    "start": "3091600",
    "end": "3099070"
  },
  {
    "text": "you know a shared nothing system about deciding where to send the writes and doing two-phase commit or doing a ledger",
    "start": "3099070",
    "end": "3107279"
  },
  {
    "text": "so this is something that didn't make the cut for the keynote but I think is a big deal so parallel query processing",
    "start": "3108690",
    "end": "3116890"
  },
  {
    "text": "for Aurora so wouldn't it be if we've got thousands of CPUs in the storage",
    "start": "3116890",
    "end": "3122800"
  },
  {
    "text": "nodes wouldn't it be cool if I could use those to do queries and you know I've got the data right what the heck not",
    "start": "3122800",
    "end": "3129160"
  },
  {
    "text": "you know exadata does something like this and so you know why wouldn't I be pushed down and paralyzed by query",
    "start": "3129160",
    "end": "3134680"
  },
  {
    "text": "processing across these thousands of nodes using the storage fleet and you know moving the processing also reduces",
    "start": "3134680",
    "end": "3140740"
  },
  {
    "text": "Network latency right and traffic which is important if that's my key concern",
    "start": "3140740",
    "end": "3147420"
  },
  {
    "text": "the one some of the challenges I face here is that the data storage and we",
    "start": "3147420",
    "end": "3154030"
  },
  {
    "text": "store in the storage know does not range partition because we want to avoid heat so you need to do full table scans for",
    "start": "3154030",
    "end": "3159400"
  },
  {
    "text": "this operation to work data may also be in flight right you have these read views and so forth about what data I'm",
    "start": "3159400",
    "end": "3165460"
  },
  {
    "text": "allowed to see what data is committed etcetera and you know you may not be allowed to see all of the data and not",
    "start": "3165460",
    "end": "3170680"
  },
  {
    "text": "all functions gonna be pushed down okay so yeah so we basically do here is that we",
    "start": "3170680",
    "end": "3177010"
  },
  {
    "text": "produce a plan to create a context we send it down the storage node is responsible for streaming stable rows",
    "start": "3177010",
    "end": "3183640"
  },
  {
    "text": "backup as well as a raw stream of unprocessed rows so the head node can do its magic with undo processing to get",
    "start": "3183640",
    "end": "3190240"
  },
  {
    "text": "the version of the data it's allowed to see then it you know combines that to do us",
    "start": "3190240",
    "end": "3195430"
  },
  {
    "text": "the final result so that's basically an overall view from a storage node perspective we run 16 peril query",
    "start": "3195430",
    "end": "3203990"
  },
  {
    "text": "processes on each node each one associated individual query and what it gets is here's the list of pages I need",
    "start": "3203990",
    "end": "3210619"
  },
  {
    "text": "you to scan here's the read view and the projections here's the expression that you need to go and run on that code and",
    "start": "3210619",
    "end": "3216640"
  },
  {
    "text": "we do two passes want to do filter evaluation once you do expression evaluation and then we send the data",
    "start": "3216640",
    "end": "3222920"
  },
  {
    "text": "back so here's an example of parallel hash join where we're basically pulling",
    "start": "3222920",
    "end": "3229910"
  },
  {
    "text": "the data we've run a bloom filter to you know pick out the values that are interesting we should pack up and you",
    "start": "3229910",
    "end": "3237319"
  },
  {
    "text": "know it's a fair bit faster so you know it's you know so that's you know point",
    "start": "3237319",
    "end": "3243170"
  },
  {
    "text": "query on an on an index column is a is basically a scan an aggregate query is a",
    "start": "3243170",
    "end": "3248720"
  },
  {
    "text": "scan followed by an Ag and you know an AG plus two table join you know so you",
    "start": "3248720",
    "end": "3254420"
  },
  {
    "text": "know you get advantages right and that's just because we're able to operate on the resources at one so I can take",
    "start": "3254420",
    "end": "3261049"
  },
  {
    "text": "questions for just about five minutes and would be happy to answer more questions outside the room after that",
    "start": "3261049",
    "end": "3267910"
  },
  {
    "text": "there are two mics over there on both the two spots if you want to ask any",
    "start": "3269829",
    "end": "3274970"
  },
  {
    "text": "questions sorry I had to go run really fast through the end of this presentation a question here when are",
    "start": "3274970",
    "end": "3283220"
  },
  {
    "text": "you planning on having Aurora available in all regions so right now it is",
    "start": "3283220",
    "end": "3290839"
  },
  {
    "text": "available in all of the regions that have 3a Z's except for Brazil and the",
    "start": "3290839",
    "end": "3298720"
  },
  {
    "text": "basically as soon as we get available capacity in any 3 AZ region you can",
    "start": "3298720",
    "end": "3306140"
  },
  {
    "text": "expect Aurora to be there people have you know we've gone and added a Z's two",
    "start": "3306140",
    "end": "3311390"
  },
  {
    "text": "regions like Frankfurt for example in order to support things like us and you",
    "start": "3311390",
    "end": "3316670"
  },
  {
    "text": "know EFS and things of that nature but you know we have every desire to go to 3",
    "start": "3316670",
    "end": "3322220"
  },
  {
    "text": "AZ pretty much anywhere that we a region and when we do so you can",
    "start": "3322220",
    "end": "3327709"
  },
  {
    "text": "expect that Aurora will be there how much of this will apply to a long post",
    "start": "3327709",
    "end": "3334039"
  },
  {
    "text": "Chris so everything that is storage based basically applies pretty quickly",
    "start": "3334039",
    "end": "3341170"
  },
  {
    "text": "and immediately things like the volume cloning and so forth things that require",
    "start": "3341170",
    "end": "3348019"
  },
  {
    "text": "some database work will take a little bit longer right and right now I mean",
    "start": "3348019",
    "end": "3353959"
  },
  {
    "text": "Aurora Postgres really just went out a few weeks ago I think the way I tend to",
    "start": "3353959",
    "end": "3359119"
  },
  {
    "text": "favor doing those things is is that usually you want to let things quiesce take customer load deal with your",
    "start": "3359119",
    "end": "3365539"
  },
  {
    "text": "tickets and just sort of deal with like the things that customers really want",
    "start": "3365539",
    "end": "3370789"
  },
  {
    "text": "that you know you maybe didn't quite get the fit and finish quite right before you start to add on and you know if",
    "start": "3370789",
    "end": "3376579"
  },
  {
    "text": "there is a bunch of stuff that we need to add in Aurora Postgres you know lambda functions you know load from s3 all that kind of",
    "start": "3376579",
    "end": "3383239"
  },
  {
    "text": "stuff we'll get there for sure is it is it possible to launch clones from",
    "start": "3383239",
    "end": "3389509"
  },
  {
    "text": "CloudFormation so it's just an API call so I think you",
    "start": "3389509",
    "end": "3395839"
  },
  {
    "text": "should be able to do so I'm not really familiar with cloud formation Evan yeah",
    "start": "3395839",
    "end": "3401599"
  },
  {
    "text": "so but as long as you can make a you know an AWS API call you should be fine",
    "start": "3401599",
    "end": "3409029"
  },
  {
    "text": "well today's multi master thing we were dealing with the right confliction so we",
    "start": "3409029",
    "end": "3416269"
  },
  {
    "text": "don't have to dealing with the rate and write completion anymore all the ways do you need to I didn't quite understand",
    "start": "3416269",
    "end": "3425089"
  },
  {
    "text": "the question could you repeat it yeah basically we are saying if we have multiple writer write if they conflict",
    "start": "3425089",
    "end": "3431779"
  },
  {
    "text": "with certain page we have to deal with that but for the real part we talk about",
    "start": "3431779",
    "end": "3436819"
  },
  {
    "text": "that cache coherence stuff right if you read and you read a very old version you",
    "start": "3436819",
    "end": "3441949"
  },
  {
    "text": "have dealing with that oh yeah we don't need to yes I I think the basic point you're making is is that if you have a",
    "start": "3441949",
    "end": "3447859"
  },
  {
    "text": "multi master configuration with reads and writes you don't really need replicas read replicas anymore and I",
    "start": "3447859",
    "end": "3453920"
  },
  {
    "text": "think that's fair yeah my point is you have multiple mosques and one master is",
    "start": "3453920",
    "end": "3460080"
  },
  {
    "text": "raised on page in its memory by its very old version do we need to handle that okay",
    "start": "3460080",
    "end": "3465780"
  },
  {
    "text": "that's correct so we have to deal with that right and so you know there's",
    "start": "3465780",
    "end": "3470940"
  },
  {
    "text": "heartbeating going on all the time there's fuzz about well how old what Reed view you're allowed to see and all",
    "start": "3470940",
    "end": "3479280"
  },
  {
    "text": "of those sorts of things so yes it definitely has to present the image of a",
    "start": "3479280",
    "end": "3484530"
  },
  {
    "text": "single database which really has to do with the same sort of Lamport clock mechanism is the same sort of you know",
    "start": "3484530",
    "end": "3491850"
  },
  {
    "text": "time-based notions of how up-to-date you can afford to be as anybody else you",
    "start": "3491850",
    "end": "3498480"
  },
  {
    "text": "know you have to have an understanding of what data you read last or right wrote last in your individual request to",
    "start": "3498480",
    "end": "3505080"
  },
  {
    "text": "decide what is readable in your next request so yeah there's a whole bunch of that kind of work of course thank you hey I might have",
    "start": "3505080",
    "end": "3515340"
  },
  {
    "text": "missed this update but is there any updates on 5.7 support yeah it should be out within the next month Thanks",
    "start": "3515340",
    "end": "3524930"
  },
  {
    "text": "any chance that at some point in the future we might get to have a Postgres replica from a my sequel variant master",
    "start": "3531830",
    "end": "3540020"
  },
  {
    "text": "oh I have not received that request before take my money yeah let me I love",
    "start": "3540020",
    "end": "3549620"
  },
  {
    "text": "think about that maybe I'll give you can show up afterwards then get a card and",
    "start": "3549620",
    "end": "3554840"
  },
  {
    "text": "you can you know send me an email with the use case in some detail with the",
    "start": "3554840",
    "end": "3561980"
  },
  {
    "text": "server lists will it bring just increase the size of the Aurora instance or will",
    "start": "3561980",
    "end": "3567020"
  },
  {
    "text": "it actually spawn multiple master masters or whatever you need so right now we grow and shrink we are not in an",
    "start": "3567020",
    "end": "3575630"
  },
  {
    "text": "initial release contemplating doing read replicas or multi master or any of those",
    "start": "3575630",
    "end": "3580820"
  },
  {
    "text": "sorts of things my sense is is that multi master and read replicas and multi a-z and all of",
    "start": "3580820",
    "end": "3588050"
  },
  {
    "text": "those sorts of things are generally more applicable to always-on systems I do think that if your intuition is is that",
    "start": "3588050",
    "end": "3595700"
  },
  {
    "text": "hey why doesn't don't those guys want things to scale up and scale down I think that's a legit point I need a bit",
    "start": "3595700",
    "end": "3603740"
  },
  {
    "text": "more experience with both of those features before I attempt to combine them I think that's everything uh I'm",
    "start": "3603740",
    "end": "3612620"
  },
  {
    "text": "just gonna unlike and then I'll be outside if anyone has any other questions right thank you guys",
    "start": "3612620",
    "end": "3620260"
  }
]