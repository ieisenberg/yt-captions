[
  {
    "start": "0",
    "end": "50000"
  },
  {
    "text": "hey guys I'll see you for the rest of my",
    "start": "30",
    "end": "6899"
  },
  {
    "text": "presentation it will be so welcome thank",
    "start": "6899",
    "end": "12000"
  },
  {
    "text": "you for taking the time I know we are standing between you and beer so we'll",
    "start": "12000",
    "end": "18150"
  },
  {
    "text": "try to make it make this quick this is about architecting a daily with a bunch",
    "start": "18150",
    "end": "25320"
  },
  {
    "text": "of different eight of our services my name is Abhishek I'm a stream park manager peanut butter dataverse for book",
    "start": "25320",
    "end": "32099"
  },
  {
    "text": "five plus years now we also have the honor of having Rowan who is from",
    "start": "32099",
    "end": "37829"
  },
  {
    "text": "Atlassian and and no one's going to talk about how the class Ian's build enabling",
    "start": "37829",
    "end": "43950"
  },
  {
    "text": "all the way from Sydney that's what 26 awesome so the way this presentations",
    "start": "43950",
    "end": "51980"
  },
  {
    "start": "50000",
    "end": "114000"
  },
  {
    "text": "what we're trying to do is try and land this deal awake right so if you've been",
    "start": "51980",
    "end": "57690"
  },
  {
    "text": "in the data industry for well we'll try and see what does this really mean and why are we talking about dealings right",
    "start": "57690",
    "end": "63390"
  },
  {
    "text": "now so I'll try and land the term the way I'll find you because I'll try and tell you why we think when we talk to a",
    "start": "63390",
    "end": "69780"
  },
  {
    "text": "lot of customers they're talking about data lakes I'll try and put together basic components of the daily and then",
    "start": "69780",
    "end": "76110"
  },
  {
    "text": "we will show you a couple of examples and these examples will grow in order of complexity the first one is a really",
    "start": "76110",
    "end": "82950"
  },
  {
    "text": "simple one that unifies real-time and batch data this is something that we've",
    "start": "82950",
    "end": "88170"
  },
  {
    "text": "also put on a blog with a CloudFormation template so after you kind of run through this you can go to the blog",
    "start": "88170",
    "end": "93840"
  },
  {
    "text": "click the clone permission template and deploy infrastructure required to build immediately and then we'll go on to a",
    "start": "93840",
    "end": "100590"
  },
  {
    "text": "more complex full foot the d-league that apply C&S and we will kind of peer into there",
    "start": "100590",
    "end": "105750"
  },
  {
    "text": "so if you're thinking of sleeping through the presentation right now the best because it grows in complexity great so",
    "start": "105750",
    "end": "114829"
  },
  {
    "start": "114000",
    "end": "150000"
  },
  {
    "text": "so what are reasons people are talking about today like I think there's no doubt that there is an exponential",
    "start": "114829",
    "end": "120719"
  },
  {
    "text": "growth in data that is happening so what we what we see customers tell us they",
    "start": "120719",
    "end": "127020"
  },
  {
    "text": "must capture data from different temporal scale sources and they want to break down traditional silos silos",
    "start": "127020",
    "end": "133739"
  },
  {
    "text": "between relational or old databases and new technologies SFP social data are",
    "start": "133739",
    "end": "141480"
  },
  {
    "text": "real-time feed coming from a b2c channel and your traditional data warehouses so",
    "start": "141480",
    "end": "147090"
  },
  {
    "text": "on so I think this is nothing new this has been happening for a while but I think what has happened and what has",
    "start": "147090",
    "end": "152610"
  },
  {
    "start": "150000",
    "end": "297000"
  },
  {
    "text": "changed in the recent time that there's now there is a diversified set of customers for your data earlier it used",
    "start": "152610",
    "end": "158489"
  },
  {
    "text": "to be an analyst which would write sequel produce reports and reports would go to a wide variety of people that a",
    "start": "158489",
    "end": "164519"
  },
  {
    "text": "look at where the quotes and make decisions based upon at what level in the organization they had now there are",
    "start": "164519",
    "end": "169890"
  },
  {
    "text": "a lot more people there are people who want to access this data by an API they want to run a wide variety of tools that",
    "start": "169890",
    "end": "175680"
  },
  {
    "text": "developers who want to run machine learning algorithms on this API who wanted to you want to do things like a",
    "start": "175680",
    "end": "182880"
  },
  {
    "text": "build model on this API there's also data scientists that want to explore",
    "start": "182880",
    "end": "188340"
  },
  {
    "text": "this data and find correlations that can be helpful to the business and these are not traditional analyst so the tools",
    "start": "188340",
    "end": "194579"
  },
  {
    "text": "that they use are also quite different from the tools traditional analysts of use so this is creating the completely",
    "start": "194579",
    "end": "202169"
  },
  {
    "text": "new way of looking at data so people are also asking us for different ways of",
    "start": "202169",
    "end": "208739"
  },
  {
    "text": "accessing this data it used to be a VI tool or a sequel engine that was good enough for you to captivity now it seems",
    "start": "208739",
    "end": "215250"
  },
  {
    "text": "like we need that they will in an organization and if you spell a large organization on one side of the",
    "start": "215250",
    "end": "221250"
  },
  {
    "text": "spectrum you have developers who love nothing but an API and on the other side of the spectrum you have business",
    "start": "221250",
    "end": "226920"
  },
  {
    "text": "analysts who couldn't be bothered with even writing sequel so you know and",
    "start": "226920",
    "end": "232140"
  },
  {
    "text": "there's a wide variety of these people trying to access the same data the commonality is that they do want to",
    "start": "232140",
    "end": "237660"
  },
  {
    "text": "access data at every granularity as possible so you will look like if you are a data warehouse admin you will all",
    "start": "237660",
    "end": "243959"
  },
  {
    "text": "constantly get requests that the granularity that you're aggregating the data right now is not good enough for me",
    "start": "243959",
    "end": "249690"
  },
  {
    "text": "give me the Royals data format so that I can join and do something else crime or you will find people who will tell you",
    "start": "249690",
    "end": "255900"
  },
  {
    "text": "that I run a Jupiter notebook and I want to access the raw data our iron our and",
    "start": "255900",
    "end": "261389"
  },
  {
    "text": "I have a wide variety of tools then there are also applications that are built into automation platforms that",
    "start": "261389",
    "end": "268950"
  },
  {
    "text": "want to access this data by an API there's also a lot of customers were trying to expose this data to external",
    "start": "268950",
    "end": "275310"
  },
  {
    "text": "customers and allowing them to run queries on this data which is also",
    "start": "275310",
    "end": "280410"
  },
  {
    "text": "completely new way of exposing their so I think this is what is kind of causing us to think about this data later to",
    "start": "280410",
    "end": "286440"
  },
  {
    "text": "kind of break down silos that doesn't mean the old traditional way of take your data ETL that put it into data",
    "start": "286440",
    "end": "292770"
  },
  {
    "text": "warehouse and put a bi on top of it it doesn't work it was fundamentally fine right so if I think about current",
    "start": "292770",
    "end": "299580"
  },
  {
    "start": "297000",
    "end": "348000"
  },
  {
    "text": "characteristics of a big of a daily these are the things that I think about if you need to have an ability to",
    "start": "299580",
    "end": "305039"
  },
  {
    "text": "collect everything you need to have an ability to dive in anywhere that means that any granularity you need to have",
    "start": "305039",
    "end": "311250"
  },
  {
    "text": "flexible access mechanisms BI tools bar ml algorithms are you know all different",
    "start": "311250",
    "end": "318510"
  },
  {
    "text": "types raw access API access and you need to future-proof it right so about two",
    "start": "318510",
    "end": "324300"
  },
  {
    "text": "years or five six years ago all machine learning you've been doing through was all sass then you have a bar and you",
    "start": "324300",
    "end": "330300"
  },
  {
    "text": "have spark now you have a mixin intensive glow and this technologies are gonna keep changing and they're gonna",
    "start": "330300",
    "end": "335520"
  },
  {
    "text": "keep coming with new and new ways of better improving these models so you need to build",
    "start": "335520",
    "end": "340880"
  },
  {
    "text": "add a delay in a way that it is future proof and allows you to incorporate newer ways of accessing the data we tank",
    "start": "340880",
    "end": "349250"
  },
  {
    "start": "348000",
    "end": "401000"
  },
  {
    "text": "s3 is a good place it is one of the places that you can build the daylight I",
    "start": "349250",
    "end": "354440"
  },
  {
    "text": "don't think any engineer will tell you there is only one and one way of doing something we think s3 is one of the ways",
    "start": "354440",
    "end": "361430"
  },
  {
    "text": "of doing that are during the day away the reason we do that is we when we talk to our customers who all wanna build",
    "start": "361430",
    "end": "367850"
  },
  {
    "text": "illegally one thing they all agree on is that SP can be a great place for putting the gear they disagree on how they want",
    "start": "367850",
    "end": "374780"
  },
  {
    "text": "to ingest the data they disagree on how they want to access the data some might prefer a red chip like to some prefer a",
    "start": "374780",
    "end": "381140"
  },
  {
    "text": "penis and prefer to run clusters Hadoop clusters on top of em are Sun protected run instances on top of this so there",
    "start": "381140",
    "end": "388520"
  },
  {
    "text": "are all variety of of things and there is no you know commonality but the only commonality we see is they all want to",
    "start": "388520",
    "end": "394910"
  },
  {
    "text": "be able to use s be able do it so what I'm talking about is how to use s to be",
    "start": "394910",
    "end": "400310"
  },
  {
    "text": "is in a way all this stuff is a very simple architecture so on and I think at",
    "start": "400310",
    "end": "408110"
  },
  {
    "start": "401000",
    "end": "467000"
  },
  {
    "text": "the end of this presentation not only will you be a master in date way but also in a tableau as icons so remove",
    "start": "408110",
    "end": "413450"
  },
  {
    "text": "that so but if you think about that there are three buckets of what you need",
    "start": "413450",
    "end": "418550"
  },
  {
    "text": "to do you need to build an injection mechanism you need have a you have a staging area or a lake where you can put",
    "start": "418550",
    "end": "425090"
  },
  {
    "text": "data let's say let's s3 you need to have processing mechanics and you need to have an ability consume that data there",
    "start": "425090",
    "end": "431180"
  },
  {
    "text": "are lots of injection tools like Kinesis Direct Connect snowball database",
    "start": "431180",
    "end": "436310"
  },
  {
    "text": "migration tools there third-party tools that will allow you to ingest data at different velocities and in different",
    "start": "436310",
    "end": "442400"
  },
  {
    "text": "formats from different sources in a way that they can all land into s3 their",
    "start": "442400",
    "end": "447470"
  },
  {
    "text": "different ways of processing this data set Athena redshift elasticsearch machine learning services",
    "start": "447470",
    "end": "453740"
  },
  {
    "text": "and then there are wide variety of ways to access this data like you can take something like quick site and can take",
    "start": "453740",
    "end": "459140"
  },
  {
    "text": "something like also an API stick an API on top of this exposure to an external world you have",
    "start": "459140",
    "end": "465150"
  },
  {
    "text": "people building applications on this data all right but there are also a",
    "start": "465150",
    "end": "470160"
  },
  {
    "start": "467000",
    "end": "494000"
  },
  {
    "text": "couple of other things that go and become a part of this architecture if you think about it that's because data",
    "start": "470160",
    "end": "476340"
  },
  {
    "text": "is not perfect all of us well let me say data is never perfect so you need ETL almost at any at any place",
    "start": "476340",
    "end": "485580"
  },
  {
    "text": "this could be a really small thing as you change the timestamp in a way that somebody is giving your data to a",
    "start": "485580",
    "end": "490680"
  },
  {
    "text": "timestamp in which let's say a downstream query engine understands it so there are lots of different tools on",
    "start": "490680",
    "end": "495960"
  },
  {
    "start": "494000",
    "end": "563000"
  },
  {
    "text": "AWS that allow you to ETL the data some of them are several s some of them are whole budget ways lambda will give you",
    "start": "495960",
    "end": "502710"
  },
  {
    "text": "trigger base code execution platform so that you can ETL your data you can also use in the bluest glue which is an even",
    "start": "502710",
    "end": "508770"
  },
  {
    "text": "base so will this engine you can run pi spark on top of it you can also run spark and hive on top of EMR but all",
    "start": "508770",
    "end": "516209"
  },
  {
    "text": "these are doing very very similar things they're trying to clean transform concatenate convert into better formats",
    "start": "516209",
    "end": "523560"
  },
  {
    "text": "they are doing transformations on this data sometimes even event-driven transformations on this data but",
    "start": "523560",
    "end": "529529"
  },
  {
    "text": "essentially what all these these technologies do is they allow you to express this transformation just code",
    "start": "529529",
    "end": "535680"
  },
  {
    "text": "right and lambda Express allows you to do that SPARC allows you to do that Lou allows you to do that so you can express",
    "start": "535680",
    "end": "541890"
  },
  {
    "text": "those transformations to code and this can actually be littered all across your you know engine it are all across your",
    "start": "541890",
    "end": "549690"
  },
  {
    "text": "architecture it can be right at the time when you're ingesting the data so that you put it in the right format in s3 it",
    "start": "549690",
    "end": "555720"
  },
  {
    "text": "could be at a time when you have ETL in the data but you are now pushing in this data into a format that it becomes",
    "start": "555720",
    "end": "561270"
  },
  {
    "text": "consumable right so there's one thing that also I feel is common to all of",
    "start": "561270",
    "end": "568650"
  },
  {
    "start": "563000",
    "end": "621000"
  },
  {
    "text": "these is the metadata right so I have all these different tools that I am allowing that are ingesting tools that",
    "start": "568650",
    "end": "575760"
  },
  {
    "text": "write data to history I have all these downstream query tools that read data from history but what I also need is a",
    "start": "575760",
    "end": "581940"
  },
  {
    "text": "common metadata store so we need something called the AWS blue data catalog which is",
    "start": "581940",
    "end": "587089"
  },
  {
    "text": "called metadata store that allows you to catalogue the data that you have in s3 and other JDBC sources like RDS and",
    "start": "587089",
    "end": "594529"
  },
  {
    "text": "redshift so it becomes your common metadata store that allows you to search get connection information do",
    "start": "594529",
    "end": "600829"
  },
  {
    "text": "classification of data a business annotations to data add versioning to",
    "start": "600829",
    "end": "605930"
  },
  {
    "text": "data it doesn't have any data it just has metadata but it allows all the",
    "start": "605930",
    "end": "611269"
  },
  {
    "text": "downstream engines to have a central place where they understand what is this data in history and that allows you to",
    "start": "611269",
    "end": "617360"
  },
  {
    "text": "use any of the engines replace ibly we also introduced something called",
    "start": "617360",
    "end": "622759"
  },
  {
    "start": "621000",
    "end": "648000"
  },
  {
    "text": "crawlers that crawlers is a way for you to automatically catalog your data that is sitting in s3 we have build",
    "start": "622759",
    "end": "629269"
  },
  {
    "text": "techniques that allow us to understand the schema of the data and automatically ingest tables in in the catalog it's",
    "start": "629269",
    "end": "636920"
  },
  {
    "text": "very simple it's based on something called classifiers which are basically draw patterns that are run in an ordered",
    "start": "636920",
    "end": "642230"
  },
  {
    "text": "fashion and based upon what we think this data set is we create a tables on",
    "start": "642230",
    "end": "647899"
  },
  {
    "text": "the data side here it looks like so if you can see on the column that I marked this CSV data data from read shape data",
    "start": "647899",
    "end": "654470"
  },
  {
    "start": "648000",
    "end": "664000"
  },
  {
    "text": "from an RDS data source data for K so the catalog is what it is but catalog of",
    "start": "654470",
    "end": "661459"
  },
  {
    "text": "all your data is sitting in s3 so for example here you can look at the table",
    "start": "661459",
    "end": "667129"
  },
  {
    "start": "664000",
    "end": "715000"
  },
  {
    "text": "properties you can add business annotations you can add data statistics to the table properties we also can find",
    "start": "667129",
    "end": "673670"
  },
  {
    "text": "things like a new version of the schema in the metadata catalog and what you can do with the downstream engine like",
    "start": "673670",
    "end": "679639"
  },
  {
    "text": "redshift or spa or athina depends upon how your data is actually structured so",
    "start": "679639",
    "end": "685639"
  },
  {
    "text": "some of this as - is this a new version of the table can I actually handle this mutation will actually depend upon the",
    "start": "685639",
    "end": "692809"
  },
  {
    "text": "downstream query intended use for example ratchet handles mutations really well Athena doesn't have mutations over",
    "start": "692809",
    "end": "699110"
  },
  {
    "text": "right now there are ways in which one tool will be good at one thing another tool will",
    "start": "699110",
    "end": "705090"
  },
  {
    "text": "not be good at that thing but you can decide but all them can share the same metadata catalog the same commissioning",
    "start": "705090",
    "end": "712170"
  },
  {
    "text": "model and the same annotations for example so here's here's a version",
    "start": "712170",
    "end": "717690"
  },
  {
    "start": "715000",
    "end": "736000"
  },
  {
    "text": "control for example this gave me a newer version of the of the table and it gives me how much can be evolved we also do",
    "start": "717690",
    "end": "725160"
  },
  {
    "text": "things like automatic partition detection punishing helps keep down",
    "start": "725160",
    "end": "730740"
  },
  {
    "text": "stream engines reduce the amount of data that they are wearing and that's making the query slightly faster so go back to",
    "start": "730740",
    "end": "738180"
  },
  {
    "start": "736000",
    "end": "755000"
  },
  {
    "text": "our diagram so we've added a new new item to the diagram it's metadata metadata stores so if you think about it",
    "start": "738180",
    "end": "744480"
  },
  {
    "text": "a variety of ingestion tools that allow you to write data a variety of tools to",
    "start": "744480",
    "end": "749670"
  },
  {
    "text": "create a dinner but probably a single s3 metadata store and a single catalog so",
    "start": "749670",
    "end": "755790"
  },
  {
    "start": "755000",
    "end": "790000"
  },
  {
    "text": "if you think about then there's also a need for doing real-time processing and",
    "start": "755790",
    "end": "761340"
  },
  {
    "text": "if you wanted to do real-time processing the tools there as well for example Kinesis I mean I don't like the term",
    "start": "761340",
    "end": "767340"
  },
  {
    "text": "real-time that's why the word industry because you are actually processing the daylight in stream instead of actually",
    "start": "767340",
    "end": "773850"
  },
  {
    "text": "processing the data once it's stored on some kind of a datastore so something like spark streaming or flink on EMR or",
    "start": "773850",
    "end": "780660"
  },
  {
    "text": "Kinesis analytics can actually run sequel queries on an stream of data and",
    "start": "780660",
    "end": "785970"
  },
  {
    "text": "give you real-time results of the DSN so",
    "start": "785970",
    "end": "791300"
  },
  {
    "start": "790000",
    "end": "828000"
  },
  {
    "text": "now if you think about all this that only commonality that we see you know all of this is here is s 3 s 3 is a",
    "start": "791300",
    "end": "799320"
  },
  {
    "text": "great place to store your data and here's a metadata catalog everything else is replaceable by one engine or",
    "start": "799320",
    "end": "805200"
  },
  {
    "text": "data based upon your use case so if you think about your data our data Lake strategy a data link strategy used says",
    "start": "805200",
    "end": "812100"
  },
  {
    "text": "depending upon the way you are ingesting your data use the source you want to depending upon the tool",
    "start": "812100",
    "end": "818720"
  },
  {
    "text": "you want to use it at query use the tool that you want to but I think these two will unify your data architecture the s3",
    "start": "818720",
    "end": "825680"
  },
  {
    "text": "based daily and a Dirac Allah so if I go back to whoops if I go back to you know",
    "start": "825680",
    "end": "834170"
  },
  {
    "start": "828000",
    "end": "879000"
  },
  {
    "text": "if you think about the data layer if you think about any of those things that I just said they kind of all matched within what we set out to build in daily",
    "start": "834170",
    "end": "841810"
  },
  {
    "text": "now let's take examples and we'll do two examples here we will run through an architecture",
    "start": "841810",
    "end": "847399"
  },
  {
    "text": "let's say a man I have an IOT device provider I have thermostats and I want to send",
    "start": "847399",
    "end": "853189"
  },
  {
    "text": "real-time thermostat information to the cloud and I want to get real-time",
    "start": "853189",
    "end": "858199"
  },
  {
    "text": "inefficiencies of termina I also want to get information about you know the daily",
    "start": "858199",
    "end": "863449"
  },
  {
    "text": "temperatures or averages so I need real-time data I need daily aggregations I need hourly",
    "start": "863449",
    "end": "869240"
  },
  {
    "text": "aggregations how would I do the data link on that so that's the first example and the second one is a much bigger one",
    "start": "869240",
    "end": "875209"
  },
  {
    "text": "which is what Rohan is going to talk about from an Atlassian spidery all right so here's our here's our first",
    "start": "875209",
    "end": "881029"
  },
  {
    "start": "879000",
    "end": "958000"
  },
  {
    "text": "example of what we what we're trying to do with this example this land this concept of a daily using some of the aid",
    "start": "881029",
    "end": "887180"
  },
  {
    "text": "of bluest pieces so I have some senses and I'm they've produced a record level data now what do I want to do with this",
    "start": "887180",
    "end": "893209"
  },
  {
    "text": "so I have a lot of business questions that I want to answer from that I want to ask this data like what is going on",
    "start": "893209",
    "end": "899720"
  },
  {
    "text": "with a specific sensor so that's real-time that's granularity the record level I want daily application that tell me",
    "start": "899720",
    "end": "906500"
  },
  {
    "text": "all the device inefficiencies that are there tell me the average temperatures across the device across particular",
    "start": "906500",
    "end": "911689"
  },
  {
    "text": "region or give me a real-time view of how many sensors that really fill in these are probably drive questions to",
    "start": "911689",
    "end": "917480"
  },
  {
    "text": "ask if you were an IOT company but these are only the business questions now the operational question is how I'm going to",
    "start": "917480",
    "end": "922699"
  },
  {
    "text": "scale this from a million sensors to tomorrow if I go and become really popular a billion sensors",
    "start": "922699",
    "end": "928790"
  },
  {
    "text": "how do I manage high availability how do I manage less management overhead and only pay",
    "start": "928790",
    "end": "934220"
  },
  {
    "text": "for what I need so what will it take will you take the concepts that we've just talked about and build an architecture about this all right so I",
    "start": "934220",
    "end": "941420"
  },
  {
    "text": "have this data what I can do is into something like a fire hose and that data can land indirectly to s3 they can",
    "start": "941420",
    "end": "948380"
  },
  {
    "text": "be some buffering of the data that allows me to create bigger files and then I can easily query them anakena",
    "start": "948380",
    "end": "953839"
  },
  {
    "text": "it's worth then so immediately I have raw access to the data that energy so",
    "start": "953839",
    "end": "958940"
  },
  {
    "text": "the way to do it would be create a delivery stream and pieces you you call",
    "start": "958940",
    "end": "964279"
  },
  {
    "text": "whatever they want to call it a loopy stream you you denote it with the necessary prefix here's where I want the",
    "start": "964279",
    "end": "969980"
  },
  {
    "text": "data to be loaded and use there's other configuration items for example if you want to do transformation on the data",
    "start": "969980",
    "end": "975649"
  },
  {
    "text": "you can hook up a lambda function to it let's say you don't like the way the timestamp comes out of the sensor data",
    "start": "975649",
    "end": "980990"
  },
  {
    "text": "you can change that there's also some interesting advanced things that you can",
    "start": "980990",
    "end": "986390"
  },
  {
    "text": "do with with Kinesis stream like you can back it up to a different stream you can also buffer this data for a for I think",
    "start": "986390",
    "end": "992480"
  },
  {
    "text": "I don't create megabyte if not and then it gets pushed on to x3 so once you push",
    "start": "992480",
    "end": "999800"
  },
  {
    "text": "this into once you push this into s3 you can either use a crawler that will",
    "start": "999800",
    "end": "1005019"
  },
  {
    "text": "automatically crawl the schema of this data because this data is in raw JSON and I can crawl JSON I can figure out a",
    "start": "1005019",
    "end": "1010990"
  },
  {
    "text": "schema and then you can write or you can also directly write a DDL statement a create table statement that'll create a",
    "start": "1010990",
    "end": "1017410"
  },
  {
    "text": "table and that's it you can start you can start query according the data the",
    "start": "1017410",
    "end": "1023020"
  },
  {
    "text": "raw data as it arrives from Kinesis fire goes onto s3 in a sparse format so now I",
    "start": "1023020",
    "end": "1028808"
  },
  {
    "text": "have a data and I have DNA in s3 I have Athena creating a table I can run these queries that's great",
    "start": "1028809",
    "end": "1034540"
  },
  {
    "text": "now so if I wanted to say tell me all the information about this particular sensor across a period of time that",
    "start": "1034540",
    "end": "1041290"
  },
  {
    "text": "could be a simple query on this raw day now my second question was I want daily applications now I can take this raw",
    "start": "1041290",
    "end": "1048069"
  },
  {
    "start": "1043000",
    "end": "1079000"
  },
  {
    "text": "data set and I can add a blue based event pipeline that allows me to ETL",
    "start": "1048069",
    "end": "1053590"
  },
  {
    "text": "this data and create an hourly the data and put it it has a new",
    "start": "1053590",
    "end": "1058659"
  },
  {
    "text": "different table on streams if you look at my diagram I have tables doesn't have",
    "start": "1058659",
    "end": "1064780"
  },
  {
    "text": "a pointer I have it I have tables the raw time series is my raw data when I abrogate the data it becomes a daily",
    "start": "1064780",
    "end": "1071650"
  },
  {
    "text": "average table that's a completely different s3 prefix and I can query the data as well and Athena is just a",
    "start": "1071650",
    "end": "1078100"
  },
  {
    "text": "different table right if you take out the blue job a blue job is a very simple",
    "start": "1078100",
    "end": "1083230"
  },
  {
    "start": "1079000",
    "end": "1114000"
  },
  {
    "text": "way of doing this you take this data set you aggregate it you run a sequel query and now you can run this every hour",
    "start": "1083230",
    "end": "1088690"
  },
  {
    "text": "there is no sir service to manage and a simple pipe or extract so I've done this",
    "start": "1088690",
    "end": "1094480"
  },
  {
    "text": "now I can done this cool like every last minute addition only thing I could find so what I do is I run this on a schedule",
    "start": "1094480",
    "end": "1103090"
  },
  {
    "text": "that means run it every hour so that my daily applications are here in our partition it also based upon a daily",
    "start": "1103090",
    "end": "1109570"
  },
  {
    "text": "average now I automatically have a daily application of the state now if I wanted to do the real-time",
    "start": "1109570",
    "end": "1116169"
  },
  {
    "start": "1114000",
    "end": "1138000"
  },
  {
    "text": "stuff what I could do is hook Canisius fired off to something like Kinesis analytics and there Canisius firewalls",
    "start": "1116169",
    "end": "1123280"
  },
  {
    "text": "can give the analytics can give me give me a daily and inefficiency count and",
    "start": "1123280",
    "end": "1128799"
  },
  {
    "text": "push this into s3 so now I have a table that gives me a real-time view of what",
    "start": "1128799",
    "end": "1133929"
  },
  {
    "text": "the daily efficiency was I can also build the dashboard and on top of this now so for example if I wanted to go to",
    "start": "1133929",
    "end": "1141940"
  },
  {
    "text": "my law table and get events by device ID I can get so I have three different tables now a row table a daily",
    "start": "1141940",
    "end": "1148630"
  },
  {
    "text": "aggregation table and real-time tables so when I query the raw table I can give",
    "start": "1148630",
    "end": "1153850"
  },
  {
    "text": "information like this give me the events by device ID give me the top 20 devices when I create a daily aggregation I can",
    "start": "1153850",
    "end": "1161140"
  },
  {
    "text": "say give me a KPI like give me a daily overall efficiency of the device or I can go to the result table and I can run",
    "start": "1161140",
    "end": "1168309"
  },
  {
    "text": "a simple query like top 10 most inefficient devices that I see on a per second or per hour basis this is what",
    "start": "1168309",
    "end": "1176559"
  },
  {
    "start": "1175000",
    "end": "1258000"
  },
  {
    "text": "the whole architecture looks like one of the things that you might have noticed about this architecture that it can scale from hundreds of thousands it",
    "start": "1176559",
    "end": "1184650"
  },
  {
    "text": "can move it has virtually infinite storage scalability it has real-time and bad processing",
    "start": "1184650",
    "end": "1190290"
  },
  {
    "text": "there because we had a real-time table a bad table or raw table we have interactive queries that can run on this",
    "start": "1190290",
    "end": "1196620"
  },
  {
    "text": "table and you only pay for what you need and all of this can be done without actually managing a single server across",
    "start": "1196620",
    "end": "1204240"
  },
  {
    "text": "this infrastructure now if you wanted to do this today you could do this today if you go to the AWS Big Data blog and",
    "start": "1204240",
    "end": "1211050"
  },
  {
    "text": "search for unite real and batch time analytics there is a cloud formation script that will take this raw data that",
    "start": "1211050",
    "end": "1218100"
  },
  {
    "text": "that has a data generator like the IOT sensor data that simulates it and has a CloudFormation template that can set",
    "start": "1218100",
    "end": "1224280"
  },
  {
    "text": "this entire architecture up for you including the crawling including this so from 0 to nothing you have built an",
    "start": "1224280",
    "end": "1231840"
  },
  {
    "text": "architecture that not only allows you to do a batch it also allows you to do real-time and allows you to build this",
    "start": "1231840",
    "end": "1238410"
  },
  {
    "text": "on this allow you to build it on s3 so that's probably a very simple example of",
    "start": "1238410",
    "end": "1244559"
  },
  {
    "text": "how you would want to build the data way now to talk about a lot more complex example and a lot more enterprise use",
    "start": "1244559",
    "end": "1251700"
  },
  {
    "text": "case let's let me invite roll thanks happy shake hi everyone",
    "start": "1251700",
    "end": "1259770"
  },
  {
    "start": "1258000",
    "end": "1472000"
  },
  {
    "text": "so I'm rowing to Pella I'm the analytics platform manager of Atlassian and I've",
    "start": "1259770",
    "end": "1265770"
  },
  {
    "text": "been with the company for about three years now and I managed a team there of",
    "start": "1265770",
    "end": "1270980"
  },
  {
    "text": "awesome data engineers and then full stack developers and together we've driven some awesome changes to Allison's",
    "start": "1270980",
    "end": "1276809"
  },
  {
    "text": "data landscape so today I'll be talking about Allison's data Lake how he handles",
    "start": "1276809",
    "end": "1282390"
  },
  {
    "text": "some of the across the core components of the lake and drive a soft-serve zero",
    "start": "1282390",
    "end": "1288270"
  },
  {
    "text": "friction approach to managing the lake as well as some of the challenges were faced along the way I'll start with a",
    "start": "1288270",
    "end": "1294630"
  },
  {
    "text": "bit of an introduction into what Allison is I don't know everyone here is aware of what Atlassian",
    "start": "1294630",
    "end": "1299739"
  },
  {
    "text": "is but we make software that helps teams of all sizes organize discuss and complete their work by providing a",
    "start": "1299739",
    "end": "1307509"
  },
  {
    "text": "shared open platform and tools I take advantage of that platform tools like",
    "start": "1307509",
    "end": "1312669"
  },
  {
    "text": "JIRA software for a project in pro for project issue and planning stride newest",
    "start": "1312669",
    "end": "1320470"
  },
  {
    "text": "product team messaging and communications tool trailer which helps individuals and teams organize them",
    "start": "1320470",
    "end": "1325899"
  },
  {
    "text": "prioritize their work as well as our confluence and bitbucket our data Lake",
    "start": "1325899",
    "end": "1331659"
  },
  {
    "text": "is pretty young it's only going on two years and that's been very much to our advantage we've been able to to really",
    "start": "1331659",
    "end": "1339009"
  },
  {
    "text": "learn from others in the industry and make some some good decisions up front even though it's so young it's been",
    "start": "1339009",
    "end": "1345999"
  },
  {
    "text": "widely adopted in the company and has become pretty much the single source of",
    "start": "1345999",
    "end": "1351039"
  },
  {
    "text": "analytics for Atlassian we call our data Lake Socrates and the name stuck pretty",
    "start": "1351039",
    "end": "1358450"
  },
  {
    "text": "well so far it's named in reference to the Socratic method a means of",
    "start": "1358450",
    "end": "1364409"
  },
  {
    "text": "cooperative argumentative dialogue between individuals based on asking and answering questions to streamline",
    "start": "1364409",
    "end": "1369849"
  },
  {
    "text": "critical thinking and we kind of hope that our platform allows our users to do the same thing essentially ask more and",
    "start": "1369849",
    "end": "1375519"
  },
  {
    "text": "more questions to get down to the real deep answers to give you an idea of scale we have plus of 500 de terabytes",
    "start": "1375519",
    "end": "1384070"
  },
  {
    "text": "of data and binary compressed format available for access on s3 we ingest",
    "start": "1384070",
    "end": "1390609"
  },
  {
    "text": "over 1 billion events every day and we have around 100 integrations into the",
    "start": "1390609",
    "end": "1397869"
  },
  {
    "text": "lake and we have 1,000 weekly active users of about 2,000 plus some around",
    "start": "1397869",
    "end": "1406479"
  },
  {
    "text": "2,000 employees at a lysine so it's mostly internal internal arm",
    "start": "1406479",
    "end": "1413720"
  },
  {
    "text": "analysts and whatnot the Donna Lake is managed by the ilex platform team a",
    "start": "1413720",
    "end": "1419030"
  },
  {
    "text": "subset of the data engineering team and we're about nine developers and we split",
    "start": "1419030",
    "end": "1424310"
  },
  {
    "text": "our focus across these four pillars here I've highlighted in blue there's ingests which is how can we get",
    "start": "1424310",
    "end": "1430220"
  },
  {
    "text": "data into the data Lake in a low friction low maintenance way there's prepare which is like how do we really",
    "start": "1430220",
    "end": "1437120"
  },
  {
    "text": "add value to the data by modeling it aggregating and deriving information out of it there's organized which is how can we",
    "start": "1437120",
    "end": "1444260"
  },
  {
    "text": "ensure data is secure but also structured to scale and lastly there's",
    "start": "1444260",
    "end": "1449720"
  },
  {
    "text": "discover which is like how can we enable our users to find understand and visualize data without having to ask a",
    "start": "1449720",
    "end": "1457040"
  },
  {
    "text": "data engineer for assistance so in the following slides I'll talk about how AWS",
    "start": "1457040",
    "end": "1462620"
  },
  {
    "text": "is offerings Havel helped us move up the value chain in each of these pillars and allowed us to focus on providing greater",
    "start": "1462620",
    "end": "1469160"
  },
  {
    "text": "value to our customers elsewhere starting with ingest so we consider this",
    "start": "1469160",
    "end": "1474620"
  },
  {
    "start": "1472000",
    "end": "1495000"
  },
  {
    "text": "to be one of the key pillars of the data like our goal here is to make the",
    "start": "1474620",
    "end": "1480070"
  },
  {
    "text": "management as low friction as possible the less time that I believe the last",
    "start": "1480070",
    "end": "1486440"
  },
  {
    "text": "time that my team and other Atlassian teams can spend focus on getting data into the lake the more time we can spend",
    "start": "1486440",
    "end": "1492290"
  },
  {
    "text": "focusing on on adding value elsewhere so some of the challenges we've had up to",
    "start": "1492290",
    "end": "1498530"
  },
  {
    "start": "1495000",
    "end": "1585000"
  },
  {
    "text": "now I guess to give you an idea like up until now I've always been doing my pool based ingestion so the challenges I'm",
    "start": "1498530",
    "end": "1506210"
  },
  {
    "text": "speaking about here are mostly centered around that when we started our data leak we were aiming to for a quick",
    "start": "1506210",
    "end": "1512210"
  },
  {
    "text": "Minimum Viable Product and we really wanted to prove value fast so we decided",
    "start": "1512210",
    "end": "1517220"
  },
  {
    "text": "to go to each individual system and extract data but whatever means is necessary but this led to a few problems",
    "start": "1517220",
    "end": "1523940"
  },
  {
    "text": "we had very brittle pipelines in that sourcing systems would change either the",
    "start": "1523940",
    "end": "1529340"
  },
  {
    "text": "data model or the API and all they would have downtime when we're trying to extract and this was quite problematic",
    "start": "1529340",
    "end": "1535910"
  },
  {
    "text": "for us there was complex interfaces in that like my team had to understand",
    "start": "1535910",
    "end": "1541520"
  },
  {
    "text": "various technologies like like a various forcing system technology and how they actually connect to those",
    "start": "1541520",
    "end": "1547060"
  },
  {
    "text": "sourcing systems and it was just made a harder for me to hire people and how-to harder for us to train up the newbies as",
    "start": "1547060",
    "end": "1553690"
  },
  {
    "text": "well and lastly the pipelines are disruptive on sourcing system teams so",
    "start": "1553690",
    "end": "1559560"
  },
  {
    "text": "like you know our pool base extracts would be like a performance hit for them and like cause all sorts of failures and",
    "start": "1559560",
    "end": "1566860"
  },
  {
    "text": "down on downtime on their side as well these problems combined started to discourage potential data producers from",
    "start": "1566860",
    "end": "1573220"
  },
  {
    "text": "actually contributing to our data leak and it caused a high barrier of entry as",
    "start": "1573220",
    "end": "1578470"
  },
  {
    "text": "often they would have to wait for our team to help them get the data onto the platform as opposed to being all the",
    "start": "1578470",
    "end": "1583480"
  },
  {
    "text": "self-serve and do it themselves so a quick glimpse into our ingestion",
    "start": "1583480",
    "end": "1588850"
  },
  {
    "start": "1585000",
    "end": "1661000"
  },
  {
    "text": "journey up until now when we started our data leak we were only pulling data from",
    "start": "1588850",
    "end": "1594970"
  },
  {
    "text": "our web assets our billing and CRM systems and our products and this essentially provided us will provide an",
    "start": "1594970",
    "end": "1601000"
  },
  {
    "text": "analyst with visibility from the moment there was a click on a website to when a product was purchased and firts you like",
    "start": "1601000",
    "end": "1607690"
  },
  {
    "text": "analytics around the behavior of our users of our users in product as well and this like really just helped",
    "start": "1607690",
    "end": "1614920"
  },
  {
    "text": "generate some people actually using their data like to begin with we then started like by early 2016 we saw an",
    "start": "1614920",
    "end": "1621610"
  },
  {
    "text": "explosion of micro services relates to coastal aciem and socrates was also proving to be very successful platform",
    "start": "1621610",
    "end": "1628050"
  },
  {
    "text": "within the company with more people wanting to take advantage of its offerings we continued to help teams on",
    "start": "1628050",
    "end": "1634840"
  },
  {
    "text": "board their data but whatever means necessary and we tried to accommodate for any means of integration really but",
    "start": "1634840",
    "end": "1641080"
  },
  {
    "text": "this led to a multitude of in this multitude of interfacing technologies started to lead to like a series of",
    "start": "1641080",
    "end": "1648370"
  },
  {
    "text": "problems and it really took a toll on the team we began to see reliability issues with numerous extracts failing",
    "start": "1648370",
    "end": "1654790"
  },
  {
    "text": "daily and like probably you know 20 or 30% of them would fail every day so",
    "start": "1654790",
    "end": "1662040"
  },
  {
    "start": "1661000",
    "end": "1735000"
  },
  {
    "text": "later in 2016 stream that we another Atlassian team",
    "start": "1662040",
    "end": "1667960"
  },
  {
    "text": "released a platform called stream hub which was a an enterprise bus this awesome little cylinder here and",
    "start": "1667960",
    "end": "1674850"
  },
  {
    "text": "producers could essentially send data to a single endpoint and consumers could subscribe to events",
    "start": "1674850",
    "end": "1681110"
  },
  {
    "text": "without having to worry about the complications of sourcing systems so we start to see our sourcing systems had",
    "start": "1681110",
    "end": "1687200"
  },
  {
    "text": "already started to push events to stream up and micro services and enterprise systems that start to subscribe to those",
    "start": "1687200",
    "end": "1693950"
  },
  {
    "text": "events as well so we saw this as a good opportunity for us to subscribe as well and take advantage of this vast amount",
    "start": "1693950",
    "end": "1700280"
  },
  {
    "text": "of data that was available there so what is stream hub essentially it's an",
    "start": "1700280",
    "end": "1706010"
  },
  {
    "text": "event-driven architecture which means essentially different producers could",
    "start": "1706010",
    "end": "1712280"
  },
  {
    "text": "integrate with the platform in different ways and downstream systems wouldn't even know how or care and secondly it",
    "start": "1712280",
    "end": "1719840"
  },
  {
    "text": "has a schema registration which means messages sent to stream hub were pre validated to a contract and we had a",
    "start": "1719840",
    "end": "1728150"
  },
  {
    "text": "schema available telling us how to read and materialize those events in our data Lake I'll just drive in to walk into",
    "start": "1728150",
    "end": "1737840"
  },
  {
    "start": "1735000",
    "end": "1779000"
  },
  {
    "text": "like how its kind of architected stream house specifically so its implementation is mostly on a micro service",
    "start": "1737840",
    "end": "1745160"
  },
  {
    "text": "architecture built on ec2 the journey for a publisher starts with the schema service down the bottom where by a",
    "start": "1745160",
    "end": "1752750"
  },
  {
    "text": "publisher can register a JSON schema for the event they tend to send to the",
    "start": "1752750",
    "end": "1757970"
  },
  {
    "text": "service and skim a registration like they do is quite self-service basically they just submit a pull request to the",
    "start": "1757970",
    "end": "1764900"
  },
  {
    "text": "service and you know upload the schema the API is the gateway for publishers to",
    "start": "1764900",
    "end": "1770300"
  },
  {
    "text": "submit events and it's at this stage that we also authenticate the publisher and we validate the event against the",
    "start": "1770300",
    "end": "1777350"
  },
  {
    "text": "schema service once publishes events have been validated and authenticated",
    "start": "1777350",
    "end": "1782360"
  },
  {
    "text": "they sent through to a Kinesis stream and then there are two things that kind of happened here",
    "start": "1782360",
    "end": "1787370"
  },
  {
    "text": "one is a subscriber would register their intent to subscribe consume that message",
    "start": "1787370",
    "end": "1793030"
  },
  {
    "text": "they would do this by creating a regular expression predicate and submitting it to the schema service and then the",
    "start": "1793030",
    "end": "1801980"
  },
  {
    "text": "second thing that would happen would be D marketing service would take these events off the Kinesis stream and decide",
    "start": "1801980",
    "end": "1809420"
  },
  {
    "text": "which subscriber needs those events based on the rhetoric and then fire it off essentially to",
    "start": "1809420",
    "end": "1815179"
  },
  {
    "text": "which our subscriber it is at the moment it currently supports sqs and Canisius as consumers a city a stream of service",
    "start": "1815179",
    "end": "1822200"
  },
  {
    "text": "maintains a contract with us publishers and subscribers they try and maintain 99.95 percent uptime SLA which is less",
    "start": "1822200",
    "end": "1830720"
  },
  {
    "text": "than twenty twenty one point five minutes downtime per month 99.95 percent success rate so one in every two",
    "start": "1830720",
    "end": "1838130"
  },
  {
    "text": "thousand calls fail and a five hundred millisecond latency between when the",
    "start": "1838130",
    "end": "1843679"
  },
  {
    "text": "eventers arrives at the API to actually sent to the consumer and and they also",
    "start": "1843679",
    "end": "1851870"
  },
  {
    "text": "offer at least once guarantee here so then how do we land this data in",
    "start": "1851870",
    "end": "1857570"
  },
  {
    "start": "1854000",
    "end": "1910000"
  },
  {
    "text": "Socrates our data leak it wasn't as easy as setting up a fire hose and learning the events what we like what what a fire",
    "start": "1857570",
    "end": "1865429"
  },
  {
    "text": "hose would do I guess would would consume one stream of events and and land those events in one location in in",
    "start": "1865429",
    "end": "1873080"
  },
  {
    "text": "our data Lake but what we wanted to do was consume one stream of events of multiple event types from multiple",
    "start": "1873080",
    "end": "1879500"
  },
  {
    "text": "publishers and land those events as multiple tables in our data Lake so varying tables based on like the",
    "start": "1879500",
    "end": "1885860"
  },
  {
    "text": "sourcing system and the event type on top of this we want to ensure that it",
    "start": "1885860",
    "end": "1891289"
  },
  {
    "text": "could do things like mask PII for any events they're coming through on the pipe and that we could land the data in",
    "start": "1891289",
    "end": "1898789"
  },
  {
    "text": "a formula it's easy to consume that's park' format it's not be compressed and in not just Jason strings",
    "start": "1898789",
    "end": "1905750"
  },
  {
    "text": "like so no a coma format essentially so we built a data landing framework and",
    "start": "1905750",
    "end": "1911169"
  },
  {
    "start": "1910000",
    "end": "2131000"
  },
  {
    "text": "what this framework does is essentially it takes events off the Kinesis stream using a lambda and lands ease event in",
    "start": "1911169",
    "end": "1919399"
  },
  {
    "text": "s3 as raw Jason the files in s3 are stored as compressed Jason in parts that",
    "start": "1919399",
    "end": "1926690"
  },
  {
    "text": "are based on the event type and the date of the event and it's now we could start",
    "start": "1926690",
    "end": "1932419"
  },
  {
    "text": "exposing these events in in our data Lake and Socrates but there would be",
    "start": "1932419",
    "end": "1937539"
  },
  {
    "text": "there would be very difficult for our users to consume stew as they store in Jason and they're not stored in a column",
    "start": "1937539",
    "end": "1943820"
  },
  {
    "text": "in the forum that would work best with our query engines so the next phase of processing",
    "start": "1943820",
    "end": "1949620"
  },
  {
    "text": "takes diesel and these landed events and using spark structured streaming we real",
    "start": "1949620",
    "end": "1955320"
  },
  {
    "text": "and these events in parque format and snappy compressed we used a schemer and richer service to basically read the",
    "start": "1955320",
    "end": "1962820"
  },
  {
    "text": "json schema that we get from stream hub and to determine the structure of the",
    "start": "1962820",
    "end": "1968070"
  },
  {
    "text": "park' files as well and we also allow for like things like enrichment so we",
    "start": "1968070",
    "end": "1974250"
  },
  {
    "text": "can you can mask particular fields if you want to or you can you can do custom partitioning or whatever you need to do",
    "start": "1974250",
    "end": "1980730"
  },
  {
    "text": "there as well and said this stage as well that we also expose the data to in our data Lake so now that's in Pocket",
    "start": "1980730",
    "end": "1986640"
  },
  {
    "text": "format we can expose in our Dalek essentially it starts to look a bit like this now so you have the data and park'",
    "start": "1986640",
    "end": "1992430"
  },
  {
    "text": "format it's and you can see that we also have a custom partitions here too so",
    "start": "1992430",
    "end": "1998370"
  },
  {
    "text": "while the events at this stage would be consumable they still wouldn't be",
    "start": "1998370",
    "end": "2004100"
  },
  {
    "text": "performing enough we still have the problem of having thousands of small files in every partition and and like",
    "start": "2004100",
    "end": "2012230"
  },
  {
    "text": "this would be problematic store for our analytical engines like that the spark the data that optimize a spark job runs",
    "start": "2012230",
    "end": "2021050"
  },
  {
    "text": "every five minutes so you know you're just going to get like a new file every five minutes essentially so what we've",
    "start": "2021050",
    "end": "2026300"
  },
  {
    "text": "done is we've created one last step one last step they go and - like it's",
    "start": "2026300",
    "end": "2033470"
  },
  {
    "text": "basically a daily compaction job and all it does is it takes the small files in",
    "start": "2033470",
    "end": "2038660"
  },
  {
    "text": "each partition and concatenate them together and then on top of that we're just basically alter the partition so",
    "start": "2038660",
    "end": "2046360"
  },
  {
    "text": "from that I'm concatenated path - like the now the new part that were created",
    "start": "2046360",
    "end": "2052399"
  },
  {
    "text": "with now that concatenated data and this starts to look just like this like basically one file in either every every",
    "start": "2052400",
    "end": "2059139"
  },
  {
    "text": "path every partition as opposed to having many thousands of files so this",
    "start": "2059140",
    "end": "2065000"
  },
  {
    "text": "is the entire landing framework for so far in the future there's a few things",
    "start": "2065000",
    "end": "2070220"
  },
  {
    "text": "would like to do for one we would like to remove the landing bucket that we have here and have the data optimizer",
    "start": "2070220",
    "end": "2076429"
  },
  {
    "text": "step read straight from the can the second thing would like to do ideally is eventually just replace this",
    "start": "2076430",
    "end": "2083720"
  },
  {
    "text": "data optimizer step entirely if and when we get things like parquet reading probably from fire hose or something",
    "start": "2083720",
    "end": "2090950"
  },
  {
    "text": "like that and just get rid of it all entirely at this stage we can't really do that though so everyone prepare",
    "start": "2090950",
    "end": "2098470"
  },
  {
    "text": "prepare so we talked about the importance of ingesting how to reduce",
    "start": "2098470",
    "end": "2103490"
  },
  {
    "text": "friction there but still that the true value really comes once you can aggregate or derive more meaning out of",
    "start": "2103490",
    "end": "2109609"
  },
  {
    "text": "data that's been landed in the lake so without providing an easier service in this area like we're basically forcing",
    "start": "2109609",
    "end": "2117470"
  },
  {
    "text": "our data analysts and our data scientists to learn complex Big Data technologies and that's not really what",
    "start": "2117470",
    "end": "2123170"
  },
  {
    "text": "we had to do so in this section I'll walk through some of the challenges we've faced and how we've tackled each",
    "start": "2123170",
    "end": "2129500"
  },
  {
    "text": "of those challenges so in the cheese we'll be running our data Lake the",
    "start": "2129500",
    "end": "2135470"
  },
  {
    "start": "2131000",
    "end": "2223000"
  },
  {
    "text": "following challenges have become quite apparent one is that there is a data engineering bottleneck",
    "start": "2135470",
    "end": "2141230"
  },
  {
    "text": "poteete by which I mean data analysts and data scientists and other data consumers",
    "start": "2141230",
    "end": "2146450"
  },
  {
    "text": "I typically heavily reliant on data engineering and they need help to do things like create tables scheduling",
    "start": "2146450",
    "end": "2154010"
  },
  {
    "text": "them or knowing how they run the second thing is cluster management is an issue for us particularly over cluster running",
    "start": "2154010",
    "end": "2161750"
  },
  {
    "text": "multiple jobs on them and if you want to it's very difficult to attribute cost factor to clusters back to jobs I mean",
    "start": "2161750",
    "end": "2169780"
  },
  {
    "text": "and it's also quite problematic to try and upgrade clusters so so if you have a",
    "start": "2169780",
    "end": "2175940"
  },
  {
    "text": "cluster with 100 jobs running on it and you want to upgrade it but 10% of those jobs fail you would be in a state where",
    "start": "2175940",
    "end": "2182210"
  },
  {
    "text": "you have to kind of run two clusters side by side for a period of time that's quite expensive and lastly reinventing",
    "start": "2182210",
    "end": "2189950"
  },
  {
    "text": "the wheel by which I mean data engineers spend a lot of time solving solved problems things like how do I land data",
    "start": "2189950",
    "end": "2197510"
  },
  {
    "text": "in s3 how do I do incremental loads of data and how do I do incremental loads of data where I want to reload a few",
    "start": "2197510",
    "end": "2204380"
  },
  {
    "text": "days of data every day like I can't really do updates and what",
    "start": "2204380",
    "end": "2209580"
  },
  {
    "text": "I see is a daughter engineers can solve these problems but like across the data engineering teams at our last year",
    "start": "2209580",
    "end": "2215090"
  },
  {
    "text": "everyone's has a different way of solving these problems and it's there's no there's no similarity there's no like",
    "start": "2215090",
    "end": "2220440"
  },
  {
    "text": "same same ways of doing these things so what we provide in the data Lake is raw",
    "start": "2220440",
    "end": "2226620"
  },
  {
    "start": "2223000",
    "end": "2334000"
  },
  {
    "text": "and unaltered data but the true value comes from that the prepared and",
    "start": "2226620",
    "end": "2233190"
  },
  {
    "text": "transformed data things like dimensional models whereby we can properly report on",
    "start": "2233190",
    "end": "2239490"
  },
  {
    "text": "business processes against performance form Krank conformed dimensions and attributes aggregated and derive views",
    "start": "2239490",
    "end": "2247230"
  },
  {
    "text": "that we can use to push on to reporting tools like tableau or or just make it easier for your users to query and",
    "start": "2247230",
    "end": "2253050"
  },
  {
    "text": "understand data and use it to find extracts that our users we use for machine learning models or for a",
    "start": "2253050",
    "end": "2260100"
  },
  {
    "text": "particular notebook or some analysis that they're working on so given the cluster management challenges I talked about earlier we've moved to a bottle of",
    "start": "2260100",
    "end": "2267360"
  },
  {
    "text": "having job scope clusters this means that we dedicate one short living",
    "start": "2267360",
    "end": "2272910"
  },
  {
    "text": "cluster for every job and we shot it down essentially after the job is complete so spin it up around your job",
    "start": "2272910",
    "end": "2280110"
  },
  {
    "text": "shut it down and and we use Airbnb open sourced air flow to manage the spin up",
    "start": "2280110",
    "end": "2286860"
  },
  {
    "text": "and shutdown of that so there are great benefits too there are many benefits to",
    "start": "2286860",
    "end": "2292980"
  },
  {
    "text": "job seeker clusters but I'll just list a few of them for one you can actually now like understand the cost per entity and",
    "start": "2292980",
    "end": "2300690"
  },
  {
    "text": "job or job and take advantage of things like per second billing in EMR and you",
    "start": "2300690",
    "end": "2307020"
  },
  {
    "text": "can also occur like charge back to your customers if you wanted to like your analysts like you can say you're using",
    "start": "2307020",
    "end": "2313110"
  },
  {
    "text": "20 bucks worth of compute every every month or whatever you can upscale",
    "start": "2313110",
    "end": "2318240"
  },
  {
    "text": "clusters for particular jobs that are more resource intensive and you can take advantage of laser later versions of",
    "start": "2318240",
    "end": "2324990"
  },
  {
    "text": "EMRs bark I've faster by just upgrading everything and keeping problematic ones",
    "start": "2324990",
    "end": "2331050"
  },
  {
    "text": "like quarantines an older version of EEMA so this is the example of a typical",
    "start": "2331050",
    "end": "2337330"
  },
  {
    "text": "dog I directed a colic graph that we have an airflow and the beauty of dogs",
    "start": "2337330",
    "end": "2342610"
  },
  {
    "text": "is they give you predefined steps and you can build predefined steps and standard actions that we can use across",
    "start": "2342610",
    "end": "2349150"
  },
  {
    "text": "all dogs and so this kind of helps us stop from reinventing the wheel we can we can reuse things that across all our",
    "start": "2349150",
    "end": "2356350"
  },
  {
    "text": "dogs and across our whole data engineering log in this dag we're doing a few things so we're basically spinning",
    "start": "2356350",
    "end": "2362260"
  },
  {
    "text": "up the cluster running a particular transformation and then copying the roads across for debugging test3 in case",
    "start": "2362260",
    "end": "2370180"
  },
  {
    "text": "anything happens and they're showing down that cluster afterwards so we kind",
    "start": "2370180",
    "end": "2376510"
  },
  {
    "start": "2375000",
    "end": "2416000"
  },
  {
    "text": "of solved jobs scoped EMI clusters were so we can we can avoid reinventing the",
    "start": "2376510",
    "end": "2382360"
  },
  {
    "text": "wheel by using things like DAGs with predefined steps but we haven't really solved the data engineering bottleneck",
    "start": "2382360",
    "end": "2389050"
  },
  {
    "text": "yet users still need like need help creating aggregated and derive views and",
    "start": "2389050",
    "end": "2396280"
  },
  {
    "text": "to solve this we created Taz transformation as a service and I guess like it's it's like this is us trying to",
    "start": "2396280",
    "end": "2404230"
  },
  {
    "text": "move up the value chain a little bit you know in my team so this allows our users",
    "start": "2404230",
    "end": "2409300"
  },
  {
    "text": "to self-serve and it's pretty much almost pretty much zero friction with our team at this stage and all it is is",
    "start": "2409300",
    "end": "2416350"
  },
  {
    "text": "just a UI where where you can basically",
    "start": "2416350",
    "end": "2421690"
  },
  {
    "text": "submit a spark SQL statement and in the",
    "start": "2421690",
    "end": "2426880"
  },
  {
    "text": "future we're looking to do something like add a CLI as well where whereby you can like you know then like use it on a",
    "start": "2426880",
    "end": "2432700"
  },
  {
    "text": "build plan or like a continuous deployment sort of thing I'd expect yeah",
    "start": "2432700",
    "end": "2437710"
  },
  {
    "text": "like I said it accepts a spark sequel you can do things like like date",
    "start": "2437710",
    "end": "2443020"
  },
  {
    "text": "parameters for incremental loads as well and users can run out how kind of like a",
    "start": "2443020",
    "end": "2449470"
  },
  {
    "text": "create table a select statement or they can do things like schedule the transformations are varying frequencies",
    "start": "2449470",
    "end": "2455140"
  },
  {
    "text": "be a daily weekly monthly or whatever behind the scenes we're still doing the same things with so we're still creating",
    "start": "2455140",
    "end": "2460720"
  },
  {
    "text": "tags and ephemeral clusters and so far so this has been pretty good in the three months since we've launched",
    "start": "2460720",
    "end": "2466450"
  },
  {
    "text": "we have around 75 scheduled transformations some being on this is",
    "start": "2466450",
    "end": "2473349"
  },
  {
    "start": "2471000",
    "end": "2492000"
  },
  {
    "text": "the third pillar organized so how do we structure secure and secure our data how",
    "start": "2473349",
    "end": "2479500"
  },
  {
    "text": "do we ensure the right people have the right access like this was pretty easy to do and like an IG BMS world but it's",
    "start": "2479500",
    "end": "2484930"
  },
  {
    "text": "it's proving to be a lot harder in a big data world and how to enable self-service and provision areas for",
    "start": "2484930",
    "end": "2490990"
  },
  {
    "text": "people to work freely the challenges we have with organizing r1 teams want",
    "start": "2490990",
    "end": "2496930"
  },
  {
    "start": "2492000",
    "end": "2528000"
  },
  {
    "text": "flexibility they they want to structure the work and it will and their work in a",
    "start": "2496930",
    "end": "2503950"
  },
  {
    "text": "way that's meaningful to them and they don't want us to dictate how entities are named or how how they should",
    "start": "2503950",
    "end": "2509829"
  },
  {
    "text": "structure their spaces security how can we ensure that data is secure on land",
    "start": "2509829",
    "end": "2515290"
  },
  {
    "text": "and still enable our users and their teams to team members to access what",
    "start": "2515290",
    "end": "2520900"
  },
  {
    "text": "they need to do easily and lastly categorizing data how do we structure our data like Cyril scale so I",
    "start": "2520900",
    "end": "2526960"
  },
  {
    "text": "start with our categories and in our data leak we essentially have four or four areas we have to land it which is",
    "start": "2526960",
    "end": "2533260"
  },
  {
    "start": "2528000",
    "end": "2587000"
  },
  {
    "text": "not exposed to our users not performing for query engines and but we keep it",
    "start": "2533260",
    "end": "2540010"
  },
  {
    "text": "around just in case we need to replay an ETL we have bra which is partitioned and",
    "start": "2540010",
    "end": "2546369"
  },
  {
    "text": "optimized data usually calmer formatted we mask any sensitive data here and we",
    "start": "2546369",
    "end": "2555099"
  },
  {
    "text": "exposing our data like at this stage as well and we have modeled which is certified and conformed entities",
    "start": "2555099",
    "end": "2562799"
  },
  {
    "text": "typically entities that have been built and maintained by the data engineering team and are usually entities that",
    "start": "2562799",
    "end": "2568540"
  },
  {
    "text": "contain like critical business metrics or core referential data that is used by",
    "start": "2568540",
    "end": "2574329"
  },
  {
    "text": "many users across the company and lastly we have self-service which is like a space where we can teams can build their",
    "start": "2574329",
    "end": "2581710"
  },
  {
    "text": "own entities upload their own data or perform a transformation on some of the raw data we've provided so I talked",
    "start": "2581710",
    "end": "2588670"
  },
  {
    "start": "2587000",
    "end": "2620000"
  },
  {
    "text": "about how we kind of promote self-service so we use our JIRA service",
    "start": "2588670",
    "end": "2594220"
  },
  {
    "text": "desk a little plug though and and basically you can a new schema and we do act a bit as a",
    "start": "2594220",
    "end": "2603390"
  },
  {
    "text": "gatekeeper here and we do like we don't want to have too many schemas in our",
    "start": "2603390",
    "end": "2608820"
  },
  {
    "text": "data leak but what we do do is like try and limit them to be like at a team",
    "start": "2608820",
    "end": "2615030"
  },
  {
    "text": "level or the organization departmental level at most when a user requests a",
    "start": "2615030",
    "end": "2622230"
  },
  {
    "text": "self-service schema a few things happen one is reprovision a s3 bucket for them we target to the user and we tag it to",
    "start": "2622230",
    "end": "2629460"
  },
  {
    "text": "their business unit as well so we can charge it back we create a schema in our meta stores and we create an Active",
    "start": "2629460",
    "end": "2636510"
  },
  {
    "text": "Directory group whereby they can maintain control and access to their buckets themselves we call these schemas",
    "start": "2636510",
    "end": "2643530"
  },
  {
    "text": "zones we used to call them playgrounds in the past but they were we found that people who start using quite a lot for",
    "start": "2643530",
    "end": "2650010"
  },
  {
    "text": "production loads so it was Owens was a bit more of a generic word there are keys for anything and we use vaults to",
    "start": "2650010",
    "end": "2656370"
  },
  {
    "text": "control access rights involved as essentially an open-source tool it's designed to manage secrets it creates a",
    "start": "2656370",
    "end": "2663420"
  },
  {
    "text": "temporary I am user with like a two-hour lifespan and passes those credentials on",
    "start": "2663420",
    "end": "2669210"
  },
  {
    "text": "to the user so this is pretty much how it works a user would sign in with the",
    "start": "2669210",
    "end": "2675330"
  },
  {
    "text": "LDAP authentication they pick the policy that they want to use so in this example we have like a zone marketing right in",
    "start": "2675330",
    "end": "2682110"
  },
  {
    "text": "his own marketing read and then they can read those credentials for that policy they can then use it for whatever they",
    "start": "2682110",
    "end": "2689400"
  },
  {
    "text": "want really so in this example they're basically listing an s3 bucket and then",
    "start": "2689400",
    "end": "2695640"
  },
  {
    "text": "uploading files today s3 bucket so this is a thing that's it for that one I'll",
    "start": "2695640",
    "end": "2703050"
  },
  {
    "start": "2698000",
    "end": "2707000"
  },
  {
    "text": "wait till I get mr. Slyde note yep okay so that's pretty much it for that that",
    "start": "2703050",
    "end": "2710310"
  },
  {
    "start": "2707000",
    "end": "2735000"
  },
  {
    "text": "pillar moving on this is discover this is our probably the most neglected pillar in our data Lake so far up until",
    "start": "2710310",
    "end": "2717630"
  },
  {
    "text": "recent months we believe that the other three pillars were of high criticality and more important before we could",
    "start": "2717630",
    "end": "2723240"
  },
  {
    "text": "really have a look at this here we try to enable our users to find what they need to understand it",
    "start": "2723240",
    "end": "2730510"
  },
  {
    "text": "and to be able to deep dive and find insights without having to interact with the data engineering team the core",
    "start": "2730510",
    "end": "2737140"
  },
  {
    "start": "2735000",
    "end": "2775000"
  },
  {
    "text": "challenges we have here are the teams want options we can't just offer one visualization",
    "start": "2737140",
    "end": "2742240"
  },
  {
    "text": "tool and her and say we're done different tools are different pros and cons and we need to provide as much",
    "start": "2742240",
    "end": "2748780"
  },
  {
    "text": "flexibility as we can here secondly managing query engines is time-consuming",
    "start": "2748780",
    "end": "2754680"
  },
  {
    "text": "career engines are the the main way that analysts interact with the platform but",
    "start": "2754680",
    "end": "2760240"
  },
  {
    "text": "they're also take a lot of time to get running right and lastly finding data is",
    "start": "2760240",
    "end": "2765760"
  },
  {
    "text": "hard we have over 3,000 tables and Socrates and it's difficult for like a newbie or like a well seasoned analyst",
    "start": "2765760",
    "end": "2773080"
  },
  {
    "text": "to find data and like so this is kind of a Madonna stored in s3 this is like",
    "start": "2773080",
    "end": "2780190"
  },
  {
    "start": "2775000",
    "end": "2847000"
  },
  {
    "text": "that's our storage area and the beauty of that is that we can spin up any type of compute that we want to access it so",
    "start": "2780190",
    "end": "2788050"
  },
  {
    "text": "this gives us flexibility and tools that we choose so for example if tableau doesn't really work very well with",
    "start": "2788050",
    "end": "2793630"
  },
  {
    "text": "presto on EMR we can easily switch now for something like spark or hive an EMR as well and it",
    "start": "2793630",
    "end": "2802990"
  },
  {
    "text": "also another thing like it gives us flexibility to make significant changes",
    "start": "2802990",
    "end": "2808060"
  },
  {
    "text": "so at the beginning this year if we decide to stop using presto and move to Athena I'll go into the reasons for I've",
    "start": "2808060",
    "end": "2815560"
  },
  {
    "text": "done this a bit later on but the point here is that the change was really simple to do we didn't have to like",
    "start": "2815560",
    "end": "2821440"
  },
  {
    "text": "migrate data anywhere around it's all based in s3 so it's just a matter of pointing it like getting the like the",
    "start": "2821440",
    "end": "2828340"
  },
  {
    "text": "meta State metadata up into the Athena and to make it work in recent months we've also looked at dropping our source",
    "start": "2828340",
    "end": "2834940"
  },
  {
    "text": "of self managed IDs hive meta store in favor of the glue meta store and this is",
    "start": "2834940",
    "end": "2841150"
  },
  {
    "text": "just basically a way that were you like we're trying to simplify our architecture as much as we can so before",
    "start": "2841150",
    "end": "2849190"
  },
  {
    "start": "2847000",
    "end": "2906000"
  },
  {
    "text": "we had presto we had several issues mostly due to our own poor management of",
    "start": "2849190",
    "end": "2855580"
  },
  {
    "text": "the cluster we just we would rather spend your time doing other things rather than imaging",
    "start": "2855580",
    "end": "2860920"
  },
  {
    "text": "Casta so we had queries failing all over the place and on top of this we had a",
    "start": "2860920",
    "end": "2866230"
  },
  {
    "text": "hard time of enabling like a table level security there was some work being done",
    "start": "2866230",
    "end": "2873490"
  },
  {
    "text": "by the open source community to enable this but we were having a hard time implementing this in our ecosystem so we",
    "start": "2873490",
    "end": "2881290"
  },
  {
    "text": "implemented Athena which gave us the ability to attribute cost to our users less infrastructure to manage so the",
    "start": "2881290",
    "end": "2888250"
  },
  {
    "text": "team could spend more time doing more value add stuff and we don't have to pay for we don't use so if there's no",
    "start": "2888250",
    "end": "2894700"
  },
  {
    "text": "queries running on the weekend we don't have to pay for it and also it uses s3",
    "start": "2894700",
    "end": "2899920"
  },
  {
    "text": "level buckets security policies so we don't need to worry about enabling authorization at the query engine layer",
    "start": "2899920",
    "end": "2906750"
  },
  {
    "start": "2906000",
    "end": "3008000"
  },
  {
    "text": "but there are challenges with Athena we",
    "start": "2906750",
    "end": "2912310"
  },
  {
    "text": "were an early adopter so we started we started migrating to Athena as soon as I went GA and we hit a number of hurdles",
    "start": "2912310",
    "end": "2919210"
  },
  {
    "text": "and teething issues in the service service along the way for one thing there wasn't parody with presto",
    "start": "2919210",
    "end": "2925840"
  },
  {
    "text": "they´ll birth they're worth running on the same tech but there's still some functions and some features that weren't working and presto that I'm that were",
    "start": "2925840",
    "end": "2931900"
  },
  {
    "text": "working at II know what they're working presto so we had to keep both clusters up and running or we had to keep pressed",
    "start": "2931900",
    "end": "2937360"
  },
  {
    "text": "up and running for a period of time well we had a thinner up secondly there's no ad authentication and by which I mean",
    "start": "2937360",
    "end": "2945160"
  },
  {
    "text": "like there's IM users and roles to authenticate with but this is not really",
    "start": "2945160",
    "end": "2950760"
  },
  {
    "text": "this this will work for lots of companies it didn't really work for us who would much prefer to have our own ad",
    "start": "2950760",
    "end": "2956500"
  },
  {
    "text": "authentication in front of Athena so what we have built here is a proxy in front of Athena that just basically",
    "start": "2956500",
    "end": "2962590"
  },
  {
    "text": "accepts 80 authentication and converts it to like I am user on the other side and lastly cost management in the time",
    "start": "2962590",
    "end": "2970150"
  },
  {
    "text": "since we've been using Athena we've been hit a few times by this whereby we haven't really been paying attention for",
    "start": "2970150",
    "end": "2976320"
  },
  {
    "text": "for a while and then at the end of the month comes and we get this surprisingly large bill and it's usually just because",
    "start": "2976320",
    "end": "2984010"
  },
  {
    "text": "I use has been querying it and it's been - higher frequency and like there's a",
    "start": "2984010",
    "end": "2989580"
  },
  {
    "text": "bit of monitoring lacking at the moment I'm sure improve in time but at the moments a bit lacking",
    "start": "2989580",
    "end": "2995250"
  },
  {
    "text": "so however overall at the moment we feel at the the the benefits outweigh the",
    "start": "2995250",
    "end": "3000440"
  },
  {
    "text": "costs and the challenges and the Athena is likely to improve it and and probably surpass anything that we can do with",
    "start": "3000440",
    "end": "3007100"
  },
  {
    "text": "presto so we provide a number of visualization tools as well the four",
    "start": "3007100",
    "end": "3013250"
  },
  {
    "start": "3008000",
    "end": "3077000"
  },
  {
    "text": "that we provide are primarily tableau which we use for exploration on core",
    "start": "3013250",
    "end": "3019130"
  },
  {
    "text": "datasets and corporate dashboards where are shiny are used for web apps and",
    "start": "3019130",
    "end": "3024890"
  },
  {
    "text": "mostly for quick sorry and for very niche requirements where Zeppelin",
    "start": "3024890",
    "end": "3032210"
  },
  {
    "text": "notebooks which is like we run an EMR it's a web-based notebooks usually for",
    "start": "3032210",
    "end": "3038030"
  },
  {
    "text": "quick analysis in Python and SPARC and lastly Reed ash this has been one of our probably most successful ones in in data",
    "start": "3038030",
    "end": "3045890"
  },
  {
    "text": "fields very similar to like a a client database visualization exploration tool",
    "start": "3045890",
    "end": "3053500"
  },
  {
    "text": "in that you can just run any kind of query you want get the results quickly get a visualization up and scheduled to",
    "start": "3053500",
    "end": "3061670"
  },
  {
    "text": "run whenever you want it's quite cool and but like just because we offer these",
    "start": "3061670",
    "end": "3066800"
  },
  {
    "text": "four tools doesn't mean we don't stuff for people from bringing their own like we want to provide as much flexibility",
    "start": "3066800",
    "end": "3071810"
  },
  {
    "text": "as we can we just managed these at the moment and lastly in an effort to kind",
    "start": "3071810",
    "end": "3079820"
  },
  {
    "start": "3077000",
    "end": "3087000"
  },
  {
    "text": "of solve the whole data discovery problem we've built a search functionality on top of our data portal",
    "start": "3079820",
    "end": "3087130"
  },
  {
    "text": "it's it's it's still a work in progress but the idea here is that like like what",
    "start": "3087280",
    "end": "3095000"
  },
  {
    "start": "3093000",
    "end": "3111000"
  },
  {
    "text": "we're where we want to get to is like a community of analysts and data scientists what country contributes to start a catalog and make it easier for",
    "start": "3095000",
    "end": "3101960"
  },
  {
    "text": "others in the community to find out a better understand but thought abettor without having to reach out to us so",
    "start": "3101960",
    "end": "3109010"
  },
  {
    "text": "that's pretty much it in the future I'm sorry key takeaways that I hope I've",
    "start": "3109010",
    "end": "3116180"
  },
  {
    "start": "3111000",
    "end": "3160000"
  },
  {
    "text": "kind of shed my sis is a 1 is that AWS helps you move up the value chain they provide services",
    "start": "3116180",
    "end": "3123420"
  },
  {
    "text": "for things that are becoming commodity and utility for example Kinesis versity",
    "start": "3123420",
    "end": "3128579"
  },
  {
    "text": "run it versus running your own Kafka cluster or Athena versus running your own EMR presto cluster and this is how",
    "start": "3128579",
    "end": "3137550"
  },
  {
    "text": "it helped us spend less time maintaining things and more time focusing on adding value elsewhere and secondly it's not",
    "start": "3137550",
    "end": "3144869"
  },
  {
    "text": "all that easy you can't just spin up a Kinesis stream of fire hose and have an instant data leak you often have to",
    "start": "3144869",
    "end": "3150300"
  },
  {
    "text": "build services around these things to make them work for you and like I guess",
    "start": "3150300",
    "end": "3155430"
  },
  {
    "text": "that's when you have to start thinking about like how can you make these things scale and how can you add flexibility so",
    "start": "3155430",
    "end": "3160800"
  },
  {
    "start": "3160000",
    "end": "3600000"
  },
  {
    "text": "that's it thank you I hope there was some takeaways in this for you if you're interested in finding out more about",
    "start": "3160800",
    "end": "3166710"
  },
  {
    "text": "what we're doing you can reach out to me afterwards we can have a chat now and otherwise you can catch me on LinkedIn",
    "start": "3166710",
    "end": "3171990"
  },
  {
    "text": "thanks [Applause]",
    "start": "3171990",
    "end": "3183300"
  },
  {
    "text": "ix developers in the like the analytics platform team it's like a mixture at the",
    "start": "3185400",
    "end": "3192060"
  },
  {
    "text": "moment so we have like we have data engineers but we're also most recently been hiring our full-stack developers to",
    "start": "3192060",
    "end": "3197730"
  },
  {
    "text": "kind of build like UIs and some of the applications are all about yeah like I",
    "start": "3197730",
    "end": "3205920"
  },
  {
    "text": "said like two years we started off yeah about two years ago pretty much to",
    "start": "3205920",
    "end": "3211460"
  },
  {
    "text": "the day",
    "start": "3211460",
    "end": "3213700"
  },
  {
    "text": "yeah we did we still do we still have some some clusters that do run with yarn",
    "start": "3223410",
    "end": "3228450"
  },
  {
    "text": "but the beauty of EMR and job scope clusters is mostly the chargeback that's",
    "start": "3228450",
    "end": "3234660"
  },
  {
    "text": "like what I'm most interested in so like being able to charge back those costs to our departments and things like that",
    "start": "3234660",
    "end": "3241430"
  },
  {
    "text": "yeah and yeah we like we do a bit of both so sometimes we use the ion sometimes we use job scope do",
    "start": "3255220",
    "end": "3263460"
  },
  {
    "text": "[Music]",
    "start": "3263460",
    "end": "3266539"
  },
  {
    "text": "so in the past we were mostly doing like extracts vai Vai JDBC so we'll hit we'll hit",
    "start": "3274110",
    "end": "3279900"
  },
  {
    "text": "there I do Bemis databases and extract we're moving more towards event-driven",
    "start": "3279900",
    "end": "3285870"
  },
  {
    "text": "so yeah so whereby they would firing events whenever something happens in their system",
    "start": "3285870",
    "end": "3292700"
  },
  {
    "text": "but I guess I mean let me think so like our aim is to have one pipe in like one",
    "start": "3318330",
    "end": "3325950"
  },
  {
    "text": "one means of ingestion so like you could have like a like the idea of using stream hub like the enterprise bus was",
    "start": "3325950",
    "end": "3333570"
  },
  {
    "text": "that it's already well established and we could have then we just have to one",
    "start": "3333570",
    "end": "3339270"
  },
  {
    "text": "the one link to worry about and it's like just less maintenance so you're",
    "start": "3339270",
    "end": "3368070"
  },
  {
    "text": "saying if you had like something like meal soft already as your enterprise bus then you would just use that I guess and",
    "start": "3368070",
    "end": "3373320"
  },
  {
    "text": "you just ingest from that now be your enterprise both yeah you would probably not do that the middle step down right",
    "start": "3373320",
    "end": "3379170"
  },
  {
    "text": "you'll just have your yourself to enterprise bus and you have some means of ingestion into your data like from",
    "start": "3379170",
    "end": "3384990"
  },
  {
    "text": "that one thing yep",
    "start": "3384990",
    "end": "3391280"
  },
  {
    "text": "okay good question so if an attribute changes how do we manage that change so",
    "start": "3402230",
    "end": "3407500"
  },
  {
    "text": "with a stream hub they do the schemas are registered but they're registered",
    "start": "3407500",
    "end": "3413119"
  },
  {
    "text": "against like they have a versioning in the registry so if a new if they're",
    "start": "3413119",
    "end": "3420320"
  },
  {
    "text": "breaking change happens to the event they register an entirely new schema with a new version and what we do is we",
    "start": "3420320",
    "end": "3426710"
  },
  {
    "text": "basically land that as a separate table in Socrates so so the breaking change",
    "start": "3426710",
    "end": "3431750"
  },
  {
    "text": "were in effect the the previous event plus version but it will like a basically build a new table and then",
    "start": "3431750",
    "end": "3437240"
  },
  {
    "text": "while we're hoping to do in the future is have like views and Athena over the top of that which kind of combines those",
    "start": "3437240",
    "end": "3443210"
  },
  {
    "text": "multiple schema versions for that one particular event type anytime you see",
    "start": "3443210",
    "end": "3451099"
  },
  {
    "text": "any kind of",
    "start": "3451099",
    "end": "3454119"
  },
  {
    "text": "so we try and keep the transformations like we try and keep the data as raw and",
    "start": "3475340",
    "end": "3480420"
  },
  {
    "text": "untouched in all those places so like the land is not actually doing anything other than just like splitting the field",
    "start": "3480420",
    "end": "3485550"
  },
  {
    "text": "of the files out so we're not actually doing anything we're not actually modifying the context of that file at",
    "start": "3485550",
    "end": "3490830"
  },
  {
    "text": "any stage there's only in the the data optimizer stage that I showed you that that that's where the only place we",
    "start": "3490830",
    "end": "3497010"
  },
  {
    "text": "actually where we're actually doing any sort of transformation to keep it like narrowed we are looking at how we're",
    "start": "3497010",
    "end": "3522180"
  },
  {
    "text": "going to do that at this stage which is",
    "start": "3522180",
    "end": "3544590"
  },
  {
    "text": "3 if you have you can encrypt using either your own encryption you put the",
    "start": "3544590",
    "end": "3550800"
  },
  {
    "text": "key VMs and then you can have individual users who could have different keys but",
    "start": "3550800",
    "end": "3557310"
  },
  {
    "text": "different prefixes so if I'm user a and I only have access to data in a certain prefix or a certain that's a directory I",
    "start": "3557310",
    "end": "3564750"
  },
  {
    "text": "only get access to a certain key that can decrypt that data I don't get access to all the other keys",
    "start": "3564750",
    "end": "3570590"
  },
  {
    "text": "so that's one way one way of doing that you provide commissioning on the key",
    "start": "3570590",
    "end": "3577859"
  },
  {
    "text": "level itself and change an azure data classification changes encrypt them",
    "start": "3577859",
    "end": "3583080"
  },
  {
    "text": "using different keys",
    "start": "3583080",
    "end": "3585800"
  },
  {
    "text": "generally you would use something like spark understands this",
    "start": "3589550",
    "end": "3595290"
  },
  {
    "text": "Athena understands this hi understands this all these systems have a way in which they can fill out the key and",
    "start": "3595290",
    "end": "3601650"
  },
  {
    "text": "decrypt the data and they're using a veneer processing ministry all of them support client-side encryption with keys",
    "start": "3601650",
    "end": "3607560"
  },
  {
    "text": "in kms so we like we basically could set",
    "start": "3607560",
    "end": "3629670"
  },
  {
    "text": "a limit on the cluster that we're running out at the moment like we are trying to move to a stage like I guess",
    "start": "3629670",
    "end": "3636240"
  },
  {
    "text": "what happens is they kind of get queued up a little bit so we're not really at the moment they're not entirely job",
    "start": "3636240",
    "end": "3642630"
  },
  {
    "text": "scope clusters at the moment behind traitors it's mostly one big cluster and",
    "start": "3642630",
    "end": "3648900"
  },
  {
    "text": "we have multiple jobs that we're moving towards like jobs goat clusters and it wouldn't really be a problem for us regardless right if you have multiple",
    "start": "3648900",
    "end": "3655620"
  },
  {
    "text": "jobs just spinning up we've just been up more and more nodes",
    "start": "3655620",
    "end": "3659930"
  },
  {
    "text": "yes III did a kind of elaborate I explained there a little bit that we are",
    "start": "3668079",
    "end": "3673490"
  },
  {
    "text": "trying to move towards the gloom in the store so yeah we have got birth up and running at the moment we're in the stage",
    "start": "3673490",
    "end": "3678890"
  },
  {
    "text": "of like shutting like with all our email clusters I think press there's only one that doesn't actually work with glue",
    "start": "3678890",
    "end": "3685810"
  },
  {
    "text": "okay good so now I think we can probably do it for everything almost out of time",
    "start": "3685810",
    "end": "3701290"
  },
  {
    "text": "yeah at the moment with the Kinesis stream of just over provisions the Kinesis dreams where I'm going is sort",
    "start": "3721510",
    "end": "3727490"
  },
  {
    "text": "of scaling that with the cluster as well I would just we've just over provisioned",
    "start": "3727490",
    "end": "3732920"
  },
  {
    "text": "but we will look at probably doing something there too",
    "start": "3732920",
    "end": "3737410"
  },
  {
    "text": "for for that part not so much no but we have looked at using glue for some of",
    "start": "3738190",
    "end": "3743869"
  },
  {
    "text": "the preparation work that we're looking at doing with amuro's killing it's",
    "start": "3743869",
    "end": "3753530"
  },
  {
    "text": "probably much easier if you scale up based upon your cpu load which will be a",
    "start": "3753530",
    "end": "3758900"
  },
  {
    "text": "function let's say you have a giant feed coming out of Kinesis so let's say after",
    "start": "3758900",
    "end": "3764420"
  },
  {
    "text": "one hour you see the P CPU load is still sustainably at 70% or 80% then you can",
    "start": "3764420",
    "end": "3770329"
  },
  {
    "text": "scale up the cluster to get your job done but for a second building you can actually aggressively scale it out so",
    "start": "3770329",
    "end": "3778069"
  },
  {
    "text": "instead of trying to do it on the Kinesis as to how quickly the stream",
    "start": "3778069",
    "end": "3783500"
  },
  {
    "text": "and monitoring them in cloud watch and then call the Iman cluster you might actually just do it in this because what",
    "start": "3783500",
    "end": "3789020"
  },
  {
    "text": "that will also do is it'll also help you with queues if your data is awfully skewed and but it's not that you're",
    "start": "3789020",
    "end": "3795980"
  },
  {
    "text": "getting a higher volume of niece's but it is awfully skewed your aggregations will take a longer time right so that",
    "start": "3795980",
    "end": "3801740"
  },
  {
    "text": "can actually help if you just do it on the CPU no thank you guys",
    "start": "3801740",
    "end": "3809770"
  }
]