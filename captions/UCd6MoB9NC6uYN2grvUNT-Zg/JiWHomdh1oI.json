[
  {
    "text": "Hi and welcome to 'This is My Architecture'.",
    "start": "7270",
    "end": "9370"
  },
  {
    "text": "I'm Andy, I'm here with Rishabh from DataCoral.",
    "start": "9420",
    "end": "11550"
  },
  {
    "text": "Hi, Rishabh.",
    "start": "11660",
    "end": "12069"
  },
  {
    "text": "- Hi.",
    "start": "12250",
    "end": "12520"
  },
  {
    "text": "It's great to be here, Andy.",
    "start": "12520",
    "end": "13489"
  },
  {
    "text": "- Excellent.",
    "start": "13570",
    "end": "13910"
  },
  {
    "text": "If you can tell me what is Datacoral.",
    "start": "13920",
    "end": "15890"
  },
  {
    "text": "- Of course.",
    "start": "15890",
    "end": "16260"
  },
  {
    "text": "So, Datacoral is a data engineering platform that allows our users",
    "start": "16619",
    "end": "19753"
  },
  {
    "text": "to extract data from 80 plus different data sources",
    "start": "19821",
    "end": "22602"
  },
  {
    "text": "and load that into the destination data warehouse of their choice,",
    "start": "22733",
    "end": "25788"
  },
  {
    "text": "like Redshift or Snowflake.",
    "start": "25947",
    "end": "27266"
  },
  {
    "text": "Now, the data sources can be databases, such as Postgres or MySQL,",
    "start": "27526",
    "end": "31654"
  },
  {
    "text": "they can be SaaS applications like Salesforce, Marketo,",
    "start": "31749",
    "end": "35066"
  },
  {
    "text": "or they can be generic file and object stores like S3.",
    "start": "35169",
    "end": "37949"
  },
  {
    "text": "- Excellent.",
    "start": "38300",
    "end": "38680"
  },
  {
    "text": "So, I see a largely serverless architecture in here,",
    "start": "39010",
    "end": "41940"
  },
  {
    "text": "if you wouldn't mind, can you tell us how you actually get the data",
    "start": "41956",
    "end": "44249"
  },
  {
    "text": "from your customers database into a different data store.",
    "start": "44250",
    "end": "47290"
  },
  {
    "text": "- Of course.",
    "start": "47429",
    "end": "47930"
  },
  {
    "text": "So, let's imagine a use case,",
    "start": "48120",
    "end": "49131"
  },
  {
    "text": "let's say that we want to move data from a MySQL database,",
    "start": "49155",
    "end": "52249"
  },
  {
    "text": "let's say it has a few 100 tables into a Redshift cluster right here.",
    "start": "52326",
    "end": "56980"
  },
  {
    "text": "And we want to be fetching data every five minutes",
    "start": "58950",
    "end": "61074"
  },
  {
    "text": "because data is changing rapidly.",
    "start": "61084",
    "end": "62390"
  },
  {
    "text": "So this is our architecture,",
    "start": "62710",
    "end": "64757"
  },
  {
    "text": "and one of the things that you'll notice here is that",
    "start": "64869",
    "end": "67318"
  },
  {
    "text": "it's fully deployed in our customer's VPC.",
    "start": "67357",
    "end": "69476"
  },
  {
    "text": "And this means that no data ever leaves their system.",
    "start": "70209",
    "end": "72580"
  },
  {
    "text": "In terms of the installation itself,",
    "start": "73260",
    "end": "74860"
  },
  {
    "text": "central to it is this orchestration service.",
    "start": "74970",
    "end": "77490"
  },
  {
    "text": "Now, this is two parts.",
    "start": "77740",
    "end": "78630"
  },
  {
    "text": "One, we use a Lambda which generates in response to all kinds of events.",
    "start": "78740",
    "end": "83470"
  },
  {
    "text": "And we have a DynamoDB, which basically acts as a config and state store.",
    "start": "83639",
    "end": "89360"
  },
  {
    "text": "Now coming back to this data source that we want to replicate data from.",
    "start": "89840",
    "end": "92648"
  },
  {
    "text": "So, because we wanted to fetch data every five minutes,",
    "start": "92930",
    "end": "95170"
  },
  {
    "text": "this orchestration service at the fifth minute will send an event",
    "start": "95311",
    "end": "98933"
  },
  {
    "text": "to our data connector, which, again, as you see will be a Lambda,",
    "start": "99041",
    "end": "102439"
  },
  {
    "text": "and it's using Lambda for processing, and it's fully serverless.",
    "start": "102440",
    "end": "104899"
  },
  {
    "text": "Now, this connector will do a few interesting things here.",
    "start": "105809",
    "end": "108289"
  },
  {
    "text": "So first, it will take this incoming request, and it will batch it up",
    "start": "108480",
    "end": "111756"
  },
  {
    "text": "into multiple pieces that can run independently and in parallel.",
    "start": "111803",
    "end": "115560"
  },
  {
    "text": "So you might imagine every single table being a separate batch,",
    "start": "116170",
    "end": "120238"
  },
  {
    "text": "and sometimes for large tables there can be multiple batches for the same table.",
    "start": "120309",
    "end": "124080"
  },
  {
    "text": "So these are going to be running independently in parallel.",
    "start": "125219",
    "end": "128100"
  },
  {
    "text": "And as these are in progress,",
    "start": "128180",
    "end": "129823"
  },
  {
    "text": "state information about how much progress has been made",
    "start": "129983",
    "end": "132788"
  },
  {
    "text": "on interesting metrics, such as how long a particular sync took,",
    "start": "132827",
    "end": "136724"
  },
  {
    "text": "how many records were fetched",
    "start": "136780",
    "end": "138669"
  },
  {
    "text": "whether schema changed, all of this gets saved into DynamoDB.",
    "start": "138700",
    "end": "142440"
  },
  {
    "text": "- So you mentioned batching your data before, I'm just curious,",
    "start": "143490",
    "end": "146340"
  },
  {
    "text": "what are some of the other reasons why you ended up doing that?",
    "start": "146363",
    "end": "148140"
  },
  {
    "text": "- So, there's two reasons.",
    "start": "148450",
    "end": "149640"
  },
  {
    "text": "One, because they could these can run in parallel,",
    "start": "149650",
    "end": "152077"
  },
  {
    "text": "we get very large speed ups in how quickly data gets fetched.",
    "start": "152219",
    "end": "155760"
  },
  {
    "text": "And the second is, if something were to go wrong,",
    "start": "155960",
    "end": "158266"
  },
  {
    "text": "it's only that particular batch that would need to be rerun,",
    "start": "158344",
    "end": "160950"
  },
  {
    "text": "because the rest of the data flows are all going fine.",
    "start": "161090",
    "end": "163729"
  },
  {
    "text": "- Excellent.",
    "start": "163990",
    "end": "164380"
  },
  {
    "text": "So after you have this initial Lambda, what happens next here?",
    "start": "164700",
    "end": "167910"
  },
  {
    "text": "- So the data that is being fetched as raw data",
    "start": "168220",
    "end": "170611"
  },
  {
    "text": "gets staged in S3.",
    "start": "170678",
    "end": "173030"
  },
  {
    "text": "And once the sync for a particular table is successful,",
    "start": "173429",
    "end": "177240"
  },
  {
    "text": "our connector will alert the orchestration service saying,",
    "start": "177420",
    "end": "181467"
  },
  {
    "text": "“Hey, the data has been written to S3, ready to go for the next step.”",
    "start": "181481",
    "end": "184850"
  },
  {
    "text": "And for the next step,",
    "start": "185004",
    "end": "186170"
  },
  {
    "text": "the orchestration service will then send an event to the Datacoral",
    "start": "186264",
    "end": "189146"
  },
  {
    "text": "Redshift Service, which you'll notice is yet another Lambda.",
    "start": "189170",
    "end": "191610"
  },
  {
    "text": "Then this service will pick this data up and issue a copy statement into",
    "start": "192400",
    "end": "196970"
  },
  {
    "text": "Redshift such that the data can be successfully loaded from S3 into Redshift.",
    "start": "196970",
    "end": "202190"
  },
  {
    "text": "- Can you give me a sense of how much data you can actually move with this?",
    "start": "203070",
    "end": "205600"
  },
  {
    "text": "- Yeah.",
    "start": "205910",
    "end": "206200"
  },
  {
    "text": "So, across all of our customers,",
    "start": "206200",
    "end": "208336"
  },
  {
    "text": "we are replicating over 500 billion records per month.",
    "start": "208422",
    "end": "211249"
  },
  {
    "text": "And as you can see, it's all done in a serverless fashion.",
    "start": "211469",
    "end": "213680"
  },
  {
    "text": "- Excellent.",
    "start": "213830",
    "end": "214180"
  },
  {
    "text": "Before, one of the things that you mentioned is",
    "start": "214449",
    "end": "216579"
  },
  {
    "text": "that you're able to take care of schema changes.",
    "start": "216590",
    "end": "219349"
  },
  {
    "text": "Can you walk me through how that actually happens?",
    "start": "219350",
    "end": "221090"
  },
  {
    "text": "- Yeah.",
    "start": "221140",
    "end": "221429"
  },
  {
    "text": "So, our data connector is essentially storing information",
    "start": "221430",
    "end": "224500"
  },
  {
    "text": "about whether schema for tables at source had changed.",
    "start": "224510",
    "end": "227599"
  },
  {
    "text": "And so, what will happen before this load is",
    "start": "227840",
    "end": "229980"
  },
  {
    "text": "we'll make sure that schema hasn't changed.",
    "start": "229980",
    "end": "231970"
  },
  {
    "text": "But if it has changed,",
    "start": "232049",
    "end": "233213"
  },
  {
    "text": "this Lambda will be sending certain DDL statements,",
    "start": "233283",
    "end": "235949"
  },
  {
    "text": "which will go ahead and modify the table at the destination.",
    "start": "235971",
    "end": "238616"
  },
  {
    "text": "and then the load from S3 into Redshift will be successful.",
    "start": "238753",
    "end": "241896"
  },
  {
    "text": "- Excellent Rishabh.",
    "start": "242120",
    "end": "242670"
  },
  {
    "text": "Thank you very much for walking through this.",
    "start": "242670",
    "end": "244410"
  },
  {
    "text": "It's great to see how Datacoral is using an entirely",
    "start": "244420",
    "end": "246590"
  },
  {
    "text": "serverless architecture to not only provide data pipelines,",
    "start": "246590",
    "end": "249629"
  },
  {
    "text": "but also metrics and information on the status of it as well.",
    "start": "249630",
    "end": "252510"
  },
  {
    "text": "- It's great to be here, Andy.",
    "start": "253470",
    "end": "254290"
  },
  {
    "text": "Thank you for having me.",
    "start": "254290",
    "end": "255069"
  },
  {
    "text": "- Excellent.",
    "start": "255250",
    "end": "255660"
  },
  {
    "text": "Thank you.",
    "start": "255670",
    "end": "256130"
  },
  {
    "text": "And thank you for watching 'This is My Architecture'.",
    "start": "256399",
    "end": "258379"
  }
]