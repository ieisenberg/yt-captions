[
  {
    "start": "0",
    "end": "239000"
  },
  {
    "text": "[Music]",
    "start": "360",
    "end": "4310"
  },
  {
    "text": "hi welcome back to aws supports you my name is clint whitehoff and i'm an enterprise support manager based out of",
    "start": "6319",
    "end": "12400"
  },
  {
    "text": "pittsburgh pennsylvania thanks for taking a few minutes out of your day to join us so today on aws supports you we're going",
    "start": "12400",
    "end": "18960"
  },
  {
    "text": "to be sharing best practices and troubleshooting tips from aws support and joining me i have raj from aws",
    "start": "18960",
    "end": "26400"
  },
  {
    "text": "support raj why don't you uh first of all welcome back to the show um thanks for thanks for joining us it's good good",
    "start": "26400",
    "end": "32960"
  },
  {
    "text": "to have you back why don't you introduce yourself hello my name is raj and i'm a principal",
    "start": "32960",
    "end": "38719"
  },
  {
    "text": "tam based out of new york and i've been with aws for about five years now and i",
    "start": "38719",
    "end": "44640"
  },
  {
    "text": "specialize on data analytics and machine learning fantastic so for those of you that have",
    "start": "44640",
    "end": "50719"
  },
  {
    "text": "joined aws support you before um again thanks for for coming back and for those",
    "start": "50719",
    "end": "55920"
  },
  {
    "text": "of you that are new be sure to go and check out raj's episode that i'll throw in the chat here in a second um where he",
    "start": "55920",
    "end": "61920"
  },
  {
    "text": "talked about uh an entitlement that enterprise support customers get which is called infrastructure event",
    "start": "61920",
    "end": "67280"
  },
  {
    "text": "management um but now to the to the topic for today and before we get there a note to the attendees online please",
    "start": "67280",
    "end": "74240"
  },
  {
    "text": "throw your questions and thoughts into the chat there um and also drop us a note and let us know where you're tuning",
    "start": "74240",
    "end": "80479"
  },
  {
    "text": "in from we definitely want to hear from you and as we go through the course of the presentation today",
    "start": "80479",
    "end": "85759"
  },
  {
    "text": "raj is going to have two demos and at the end of each one of those sections there he does the demos we'll leave time",
    "start": "85759",
    "end": "91920"
  },
  {
    "text": "for some questions and answers so the topic is using amazon sagemaker",
    "start": "91920",
    "end": "97119"
  },
  {
    "text": "data wrangler and aws glue data brew for exploratory data in in analysis",
    "start": "97119",
    "end": "104240"
  },
  {
    "text": "uh raj why don't you give us a rundown of what we're going to talk about today in detail perfect thank you so today we're going",
    "start": "104240",
    "end": "111280"
  },
  {
    "text": "to talk about exploratory data analysis so let me give a use case for context",
    "start": "111280",
    "end": "117280"
  },
  {
    "text": "before we get started in my current role as technical account manager i was",
    "start": "117280",
    "end": "122320"
  },
  {
    "text": "working on predicting escalations to specific teams based on past escalations",
    "start": "122320",
    "end": "128239"
  },
  {
    "text": "from cases that we receive and in this case i used sage maker studio sagemaker",
    "start": "128239",
    "end": "133840"
  },
  {
    "text": "data wrangler for data cleaning and transformation also carried out the model building for",
    "start": "133840",
    "end": "140560"
  },
  {
    "text": "prediction using sagemaker canvas i'm not a data scientist and",
    "start": "140560",
    "end": "146000"
  },
  {
    "text": "but in this case i was able to understand the trends in data visualize the pattern",
    "start": "146000",
    "end": "152080"
  },
  {
    "text": "and correlate between the data patterns and eventually prepare the data create a",
    "start": "152080",
    "end": "157760"
  },
  {
    "text": "model and deploy the model to production to make predictions all with sagemaker",
    "start": "157760",
    "end": "163120"
  },
  {
    "text": "features such as data wrangler canvas autopilot all of these are low code no code aws",
    "start": "163120",
    "end": "170480"
  },
  {
    "text": "services or service features this prompted me to work on this session to share my knowledge on how you can",
    "start": "170480",
    "end": "177280"
  },
  {
    "text": "quickly carry out data analysis and data transformations easily with point and",
    "start": "177280",
    "end": "183360"
  },
  {
    "text": "click using aws services the example we will use in our demo",
    "start": "183360",
    "end": "189360"
  },
  {
    "text": "today is a churn prediction use case i'm just giving a heads up we'll discuss about that as we move forward into the",
    "start": "189360",
    "end": "195680"
  },
  {
    "text": "demo okay what is eda as the name indicates eda is for exploring the data",
    "start": "195680",
    "end": "202640"
  },
  {
    "text": "and preparing the data for the next phase of model building and training all",
    "start": "202640",
    "end": "208159"
  },
  {
    "text": "within the machine learning pipeline eda is unavoidable and it is one of the",
    "start": "208159",
    "end": "213280"
  },
  {
    "text": "major steps to fine-tune the given data set in different form of analysis",
    "start": "213280",
    "end": "219760"
  },
  {
    "text": "that is to understand the insights of key characteristics of various entities",
    "start": "219760",
    "end": "225360"
  },
  {
    "text": "of the data set such as columns rows applying pandas numpy statistical",
    "start": "225360",
    "end": "233120"
  },
  {
    "text": "methods and data visualization packages",
    "start": "233120",
    "end": "237519"
  },
  {
    "start": "239000",
    "end": "307000"
  },
  {
    "text": "first let's talk about why data preparation has always been considered to be a tedious and resource intensive",
    "start": "239519",
    "end": "247040"
  },
  {
    "text": "job due to the inherent nature of data being dirty and it is not ready for",
    "start": "247040",
    "end": "253280"
  },
  {
    "text": "machine learning in its raw form uh dirty data that we are talking about here that could be uh missing or",
    "start": "253280",
    "end": "261359"
  },
  {
    "text": "erroneous values in the data or there could be a lot of outliers and so on",
    "start": "261359",
    "end": "266639"
  },
  {
    "text": "feature engineering is often indeed a way of transformation of inputs to",
    "start": "266639",
    "end": "273199"
  },
  {
    "text": "deliver a more accurate and efficient ml model forbes did a survey back in 2016 that",
    "start": "273199",
    "end": "279840"
  },
  {
    "text": "showed about 80 of ml engineering teams time is spent on preparing the data",
    "start": "279840",
    "end": "286160"
  },
  {
    "text": "consider the ml process and how the data exists here are some of the common challenges with data preparation in ml",
    "start": "286160",
    "end": "293199"
  },
  {
    "text": "it's time consuming it's resource intensive it requires multiple tools and",
    "start": "293199",
    "end": "298400"
  },
  {
    "text": "data preparation tools are typically not devops friendly and common data",
    "start": "298400",
    "end": "303520"
  },
  {
    "text": "preparation tasks require writing code now before we get into the exploratory",
    "start": "303520",
    "end": "311199"
  },
  {
    "start": "307000",
    "end": "506000"
  },
  {
    "text": "data analysis phase uh a brief intro of ml for beginners what is machine",
    "start": "311199",
    "end": "317360"
  },
  {
    "text": "learning machine learning is a subfield of artificial intelligence which is",
    "start": "317360",
    "end": "322400"
  },
  {
    "text": "broadly defined as the capability of the machine to imitate intelligent human",
    "start": "322400",
    "end": "327840"
  },
  {
    "text": "behavior so let's take an example right or maybe a brief overview of machine learning in",
    "start": "327840",
    "end": "334400"
  },
  {
    "text": "comparison to compute because we all know about compute so we just wanted to compare that with compute to get a good",
    "start": "334400",
    "end": "339600"
  },
  {
    "text": "understanding in compute um x and the function is given so in our",
    "start": "339600",
    "end": "344720"
  },
  {
    "text": "case y is equal to x plus one and what is unknown y is unknown and the focus is",
    "start": "344720",
    "end": "351440"
  },
  {
    "text": "on why computing the output whereas in machine learning we know x and we also",
    "start": "351440",
    "end": "358160"
  },
  {
    "text": "know what that is the input and output we feed that into the system so that it can learn the relationship between x and",
    "start": "358160",
    "end": "366080"
  },
  {
    "text": "y by itself and come up with the function to predict y given x",
    "start": "366080",
    "end": "372720"
  },
  {
    "text": "so here if you see the focus is on the function that's the key differentiator when you compare machine learning with",
    "start": "372720",
    "end": "379520"
  },
  {
    "text": "computing here is a typical machine learning process or pipeline that starts with",
    "start": "379520",
    "end": "386560"
  },
  {
    "text": "business understanding that is followed by understanding and analyzing the data then after the data is prepared we",
    "start": "386560",
    "end": "394240"
  },
  {
    "text": "proceed to the model evaluation phase model build phase and then deploy the model to production",
    "start": "394240",
    "end": "401039"
  },
  {
    "text": "as you see preparing the data is the first and crucial step in machine learning in our session today we're",
    "start": "401039",
    "end": "407520"
  },
  {
    "text": "going to focus uh completely on data understanding and data preparation phase",
    "start": "407520",
    "end": "412560"
  },
  {
    "text": "and which is going to be the first step within your machine learning pipeline and much of your success for your ml",
    "start": "412560",
    "end": "419280"
  },
  {
    "text": "projects depend on this particular face because this face determines what data",
    "start": "419280",
    "end": "425599"
  },
  {
    "text": "is presented and how is it presented to your ml training model build and",
    "start": "425599",
    "end": "430880"
  },
  {
    "text": "deployment if you look at what's powering the ml ecosystem",
    "start": "430880",
    "end": "436800"
  },
  {
    "text": "it's all about customers who are reinventing their businesses with data and data is the",
    "start": "436800",
    "end": "444479"
  },
  {
    "text": "underlying force that fuels the insights and predictions that feed to a better",
    "start": "444479",
    "end": "449840"
  },
  {
    "text": "decision making and innovation uh i would like to give you a holistic view of the data aspects within machine",
    "start": "449840",
    "end": "457199"
  },
  {
    "text": "learning and hence the slide uh what you see here is a typical data integration",
    "start": "457199",
    "end": "462840"
  },
  {
    "text": "ecosystem where you start with different data sources such as amazon rds",
    "start": "462840",
    "end": "469440"
  },
  {
    "text": "on-prem streaming data and other databases that's what you see on the left side",
    "start": "469440",
    "end": "475840"
  },
  {
    "text": "and then you connect to these data sources you catalog the data you analyze the",
    "start": "475840",
    "end": "482560"
  },
  {
    "text": "data to interpret meaningful insights from the data and then you carry out different transformations such as",
    "start": "482560",
    "end": "488479"
  },
  {
    "text": "ordinal transform one hot encoding removing columns and so on after all",
    "start": "488479",
    "end": "493680"
  },
  {
    "text": "these eda activities the data goes into your data lake house where it recites for other teams to consume on further",
    "start": "493680",
    "end": "500560"
  },
  {
    "text": "analytics and the data warehouse activities for the downstream and machine learning pipelines",
    "start": "500560",
    "end": "507199"
  },
  {
    "start": "506000",
    "end": "662000"
  },
  {
    "text": "to get ready for a machine learning model you first need to collect the data in which could be in different formats",
    "start": "507199",
    "end": "513919"
  },
  {
    "text": "from different sources those could be databases warehouses which may require creating complex",
    "start": "513919",
    "end": "520240"
  },
  {
    "text": "queries or sometimes this data might exist as just csv files in s3 bucket",
    "start": "520240",
    "end": "526320"
  },
  {
    "text": "after the data is collected it needs to be transformed into a usable format transforming your data requires you to",
    "start": "526320",
    "end": "532399"
  },
  {
    "text": "write code and complete tedious tasks after the data is transformed then you write more code to see the visualization",
    "start": "532399",
    "end": "540000"
  },
  {
    "text": "and that is primarily to inspect and analyze the data such as quickly detecting outliers",
    "start": "540000",
    "end": "546240"
  },
  {
    "text": "extreme values within the data set as you inspect and analyze the data you typically uncover more data that will be",
    "start": "546240",
    "end": "554000"
  },
  {
    "text": "helpful for your model such as mean median your growth rate",
    "start": "554000",
    "end": "560480"
  },
  {
    "text": "or rolling averages which requires more time on coding in other aspects after",
    "start": "560480",
    "end": "566000"
  },
  {
    "text": "you have prepared your data in your development environment you must take the data preparation work to protection",
    "start": "566000",
    "end": "572640"
  },
  {
    "text": "this is all tedious process which is like aggregating the data preparing the data and the data prep workflows can",
    "start": "572640",
    "end": "579440"
  },
  {
    "text": "take months to complete with sagemaker data wrangler or aws glue data brew this",
    "start": "579440",
    "end": "584640"
  },
  {
    "text": "process gets much simpler when we talk about eda we have so many options and you have the freedom to",
    "start": "584640",
    "end": "591440"
  },
  {
    "text": "select the options that is right for your use case and what type of prediction problems that",
    "start": "591440",
    "end": "596720"
  },
  {
    "text": "you work on pretty much determines what type of eda tools that you have to choose",
    "start": "596720",
    "end": "601920"
  },
  {
    "text": "broadly we have the following options to do eda on aws first stagemaker processing which is the",
    "start": "601920",
    "end": "608959"
  },
  {
    "text": "capability of sagemaker that lets you easily run your pre-processing",
    "start": "608959",
    "end": "615200"
  },
  {
    "text": "post processing and model evaluation workloads on a fully managed infrastructure",
    "start": "615200",
    "end": "621600"
  },
  {
    "text": "the middle two tiers that you see here which i have highlighted amazon sagemaker data wrangler and aws",
    "start": "621600",
    "end": "628480"
  },
  {
    "text": "glue data brew these two are considered to be like a low code no code solutions",
    "start": "628480",
    "end": "634079"
  },
  {
    "text": "for your eda activities and we're going to dive deep into these two options in our session today so i'll skip that for",
    "start": "634079",
    "end": "639600"
  },
  {
    "text": "now the fourth option that you see here is spark in emr many organizations use",
    "start": "639600",
    "end": "645760"
  },
  {
    "text": "spark for data processing and other purposes and in these situations spark clusters are typically run on amazon emr",
    "start": "645760",
    "end": "653519"
  },
  {
    "text": "a managed service for hadoop ecosystem clusters which eliminates the need to do",
    "start": "653519",
    "end": "658880"
  },
  {
    "text": "your own setup tuning and maintenance let's dive deep into amazon sagemaker",
    "start": "658880",
    "end": "665680"
  },
  {
    "start": "662000",
    "end": "1268000"
  },
  {
    "text": "data wrangler before we get into sagemaker data wrangler let us see what sagemaker is so if you look at sagemaker",
    "start": "665680",
    "end": "673440"
  },
  {
    "text": "it is a comprehensive machine learning service that enables business analysts",
    "start": "673440",
    "end": "679200"
  },
  {
    "text": "data scientists and ml ops engineers to build train and deploy machine learning",
    "start": "679200",
    "end": "685680"
  },
  {
    "text": "models for any use cases regardless of their ml expertise",
    "start": "685680",
    "end": "690880"
  },
  {
    "text": "business analyst can make ml predictions using a visual interface using sagemaker",
    "start": "690880",
    "end": "696320"
  },
  {
    "text": "canvas this is your point and click model creation service",
    "start": "696320",
    "end": "701680"
  },
  {
    "text": "analyst can easily prepare data and build train and deploy the machine",
    "start": "701680",
    "end": "706880"
  },
  {
    "text": "learning models using sagemaker studio mlaps engineers can deploy and manage",
    "start": "706880",
    "end": "712079"
  },
  {
    "text": "models at scale with sagemaker mlops sagemaker datawrangler is a feature of",
    "start": "712079",
    "end": "719200"
  },
  {
    "text": "amazon sagemaker enabled through sagemaker studio that makes it easy for data scientists and ml engineers to",
    "start": "719200",
    "end": "726959"
  },
  {
    "text": "aggregate and prepare the data for machine learning applications by using a",
    "start": "726959",
    "end": "732000"
  },
  {
    "text": "visual interface that is to accelerate the data cleaning exploration and visualization",
    "start": "732000",
    "end": "739040"
  },
  {
    "text": "data wrangler pretty much addresses all aspects of data preparations needed for machine",
    "start": "739040",
    "end": "745200"
  },
  {
    "text": "learning you can quickly inspect and import the data from multiple sources",
    "start": "745200",
    "end": "750399"
  },
  {
    "text": "leverage built-in data transformations which are built in pi spot or build your",
    "start": "750399",
    "end": "755440"
  },
  {
    "text": "own custom transformation in pi spark sql or pandas",
    "start": "755440",
    "end": "760560"
  },
  {
    "text": "visually analyze the features evaluate and iteratively improve the model accuracy and quickly deploy the data",
    "start": "760560",
    "end": "767920"
  },
  {
    "text": "preparation jobs um into your own production environment all using a visual interface and note that this is",
    "start": "767920",
    "end": "775680"
  },
  {
    "text": "all without having to write any code data scientists need to collect data in",
    "start": "775680",
    "end": "782240"
  },
  {
    "text": "various formats and those could be from different sources which requires creating complex queries",
    "start": "782240",
    "end": "787920"
  },
  {
    "text": "and the tools that you use that needs to import the data and you have to prepare the",
    "start": "787920",
    "end": "794160"
  },
  {
    "text": "data within a data prep environment after the data is imported you view the",
    "start": "794160",
    "end": "799200"
  },
  {
    "text": "statistics and you access a suite of built-in data transformations that are",
    "start": "799200",
    "end": "805120"
  },
  {
    "text": "designed to reduce the tedious tasks such as data cleaning and exploration visualization is a critical task in data",
    "start": "805120",
    "end": "812880"
  },
  {
    "text": "preparation workflow because data scientists need an intuitive understanding of the data to quickly",
    "start": "812880",
    "end": "819519"
  },
  {
    "text": "detect problems you can use built-in data transformation tools to transform the data into different formats and then",
    "start": "819519",
    "end": "827760"
  },
  {
    "text": "you can use that to build accurately the different ml models that is required",
    "start": "827760",
    "end": "834240"
  },
  {
    "text": "preparing high quality data that is for the training aspects requires collection of data from",
    "start": "834240",
    "end": "840480"
  },
  {
    "text": "different sources as we were talking about earlier because data is available in different sources and this often",
    "start": "840480",
    "end": "846480"
  },
  {
    "text": "requires you to write complex queries right so let's take sagemaker uh for",
    "start": "846480",
    "end": "851519"
  },
  {
    "text": "example like how do you choose the data sources you can use sagemaker data wrangler's data selection tool and you",
    "start": "851519",
    "end": "858560"
  },
  {
    "text": "can quickly select the data from different data sources such as athena",
    "start": "858560",
    "end": "864399"
  },
  {
    "text": "redshift lake formation s3 and also data from sagemaker feature",
    "start": "864399",
    "end": "870880"
  },
  {
    "text": "store and this also includes support for common",
    "start": "870880",
    "end": "876160"
  },
  {
    "text": "file formats so what are those file formats we're talking about csv files uh parquet files and",
    "start": "876160",
    "end": "882959"
  },
  {
    "text": "newly introduced within sagemaker data wrangler you could also get json files and orc files these",
    "start": "882959",
    "end": "890399"
  },
  {
    "text": "two are new and as it was present earlier you can also import the database table directly into",
    "start": "890399",
    "end": "896959"
  },
  {
    "text": "amazon sagemaker before importing large volumes of data for the data sources you could",
    "start": "896959",
    "end": "902399"
  },
  {
    "text": "query and inspect the data in place to understand its relevance and all from",
    "start": "902399",
    "end": "908160"
  },
  {
    "text": "the sagemaker data wrangler and as you can see the data visualizations can help you to",
    "start": "908160",
    "end": "914639"
  },
  {
    "text": "intuitively understand the data and we are talking about uh some of the graphs or templates like histogram uh scatter",
    "start": "914639",
    "end": "922399"
  },
  {
    "text": "plots uh box and visco plots line plots and bar charts all of this built in and",
    "start": "922399",
    "end": "929120"
  },
  {
    "text": "again we are not writing any code here this is all just point and click which we'll cover that within the demo",
    "start": "929120",
    "end": "934880"
  },
  {
    "text": "sagemaker data wrangler also helps you to interactively create and edit your own visualization",
    "start": "934880",
    "end": "941519"
  },
  {
    "text": "so that you can quickly detect outliers or extreme values for example you can select the histogram",
    "start": "941519",
    "end": "948959"
  },
  {
    "text": "template select the columns of data to plot and plot a histogram without",
    "start": "948959",
    "end": "954160"
  },
  {
    "text": "writing code data scientists can use the preview modeling capabilities to quickly",
    "start": "954160",
    "end": "959199"
  },
  {
    "text": "understand the ml model accuracy and diagnose any potential issues in the data preparation workflow",
    "start": "959199",
    "end": "966399"
  },
  {
    "text": "you could also quickly carry out diagnosis on common ml issues like",
    "start": "966399",
    "end": "971600"
  },
  {
    "text": "target leakage or multi-co-linearity all of this with data wranglers built-in",
    "start": "971600",
    "end": "977360"
  },
  {
    "text": "analysis you can easily understand how the analysis works from informative",
    "start": "977360",
    "end": "983360"
  },
  {
    "text": "explanations that come included with each analysis and that's that's the core so that you get an understanding of why",
    "start": "983360",
    "end": "990320"
  },
  {
    "text": "this particular analysis or what is the data pattern that you are observing sagemaker datawrangler also offers a",
    "start": "990320",
    "end": "997680"
  },
  {
    "text": "rich selection of pre-configured data transformations such as convert column type",
    "start": "997680",
    "end": "1004320"
  },
  {
    "text": "rename a column or maybe delete a column or in other cases you can transform your",
    "start": "1004320",
    "end": "1010560"
  },
  {
    "text": "data into formats that could be effectively used for machine learning models without writing a single line of",
    "start": "1010560",
    "end": "1016959"
  },
  {
    "text": "code for example let's say you uh easily convert a text field column into a",
    "start": "1016959",
    "end": "1024000"
  },
  {
    "text": "numerical column with just single click another example is that you can also create new columns of data such as",
    "start": "1024000",
    "end": "1031360"
  },
  {
    "text": "monthly summaries or purchasing behavior from the raw transaction data it also",
    "start": "1031360",
    "end": "1036400"
  },
  {
    "text": "provide an automatic data consistency check to ensure that the data is consistent such as date stamps using the",
    "start": "1036400",
    "end": "1044160"
  },
  {
    "text": "date month year format earlier this year we also announced a",
    "start": "1044160",
    "end": "1049600"
  },
  {
    "text": "new transformation that allows you to balance your data set easily and effectively for ml model training the",
    "start": "1049600",
    "end": "1056080"
  },
  {
    "text": "newly announced balancing operator is grouped under balance data transform type in the add",
    "start": "1056080",
    "end": "1062559"
  },
  {
    "text": "transform pane once you get into the transformations within data wrangler",
    "start": "1062559",
    "end": "1067679"
  },
  {
    "text": "you could also author custom transformations and when i say custom transformations you could do that in pi",
    "start": "1067679",
    "end": "1073360"
  },
  {
    "text": "spark sql and pandas and this provides kind of flexibility across your",
    "start": "1073360",
    "end": "1078480"
  },
  {
    "text": "organization in terms of what kind of a language you may want to use for your custom transformation",
    "start": "1078480",
    "end": "1084640"
  },
  {
    "text": "the new time series transforms that helps accelerate the preparation of time",
    "start": "1084640",
    "end": "1089760"
  },
  {
    "text": "series data with ml and these transformers include free sample",
    "start": "1089760",
    "end": "1094960"
  },
  {
    "text": "lag features rolling window features and also the feature ice date time format",
    "start": "1094960",
    "end": "1102559"
  },
  {
    "text": "datawrangler records all the steps of your data preparation workflow in a data",
    "start": "1102720",
    "end": "1108400"
  },
  {
    "text": "flow graph this helps you understand what are the different steps or the workflow that you have carried so far",
    "start": "1108400",
    "end": "1115360"
  },
  {
    "text": "and also you can visualize the order transformations and join and concatenate",
    "start": "1115360",
    "end": "1121440"
  },
  {
    "text": "operators you can easily navigate your data transformation pipeline and modify and",
    "start": "1121440",
    "end": "1127600"
  },
  {
    "text": "delete the steps as you iteratively make progress sagemaker data wrangler enables you to",
    "start": "1127600",
    "end": "1134960"
  },
  {
    "text": "quickly identify inconsistencies in your data preparation workflow and diagnose the issues before your models are",
    "start": "1134960",
    "end": "1141520"
  },
  {
    "text": "deployed into production and people who are working on model tuning you very well understand why this is important",
    "start": "1141520",
    "end": "1148160"
  },
  {
    "text": "because the cleaner the data you prepare and send it to your data models the",
    "start": "1148160",
    "end": "1153679"
  },
  {
    "text": "easier your pipeline is and and often it's a back and forth mechanism that you follow right so within the sagemaker",
    "start": "1153679",
    "end": "1159679"
  },
  {
    "text": "pipeline so it's very imperative that you identify if there are any issues within your data set pretty early at the",
    "start": "1159679",
    "end": "1165360"
  },
  {
    "text": "life cycle you can select a subset of your data validate your model against that data identify what could be some",
    "start": "1165360",
    "end": "1172559"
  },
  {
    "text": "possible errors you also can determine which features are contributing to the model performance relative to other",
    "start": "1172559",
    "end": "1178799"
  },
  {
    "text": "features and determine if additional feature engineering is needed to improve your model performance all of this that",
    "start": "1178799",
    "end": "1185120"
  },
  {
    "text": "we are talking about here is without having to deploy models into production that's the key pointer",
    "start": "1185120",
    "end": "1191200"
  },
  {
    "text": "this is a huge time saver right why is it a huge time saver because you're iteratively re refining the",
    "start": "1191200",
    "end": "1197520"
  },
  {
    "text": "features and improving the model accuracy and all within the tool without having to go through your model",
    "start": "1197520",
    "end": "1203520"
  },
  {
    "text": "deployment phase you can run all your data processing workloads with just a single click of a",
    "start": "1203520",
    "end": "1210480"
  },
  {
    "text": "button you can easily browse amazon s3 and specify your target output",
    "start": "1210480",
    "end": "1216080"
  },
  {
    "text": "destination you can also select the instance type number of nodes to process your particular workload",
    "start": "1216080",
    "end": "1223360"
  },
  {
    "text": "finally you can easily take code into production with data wrangler without translating hundreds of lines of data",
    "start": "1223360",
    "end": "1230799"
  },
  {
    "text": "preparation code you can export your data preparation workflow to a notebook or a code script with just a single",
    "start": "1230799",
    "end": "1237280"
  },
  {
    "text": "click and that is to easily bring your data preparation workflows to production",
    "start": "1237280",
    "end": "1243440"
  },
  {
    "text": "you could also integrate your data preparation workflow with sagemaker pipelines or",
    "start": "1243440",
    "end": "1249840"
  },
  {
    "text": "open source ml pipeline orchestration frameworks such as cube flow and airflow",
    "start": "1249840",
    "end": "1255280"
  },
  {
    "text": "that is to automate your model deployment and management or publish your features created to sagemaker",
    "start": "1255280",
    "end": "1261200"
  },
  {
    "text": "feature store for future reuse and syndication across your organization",
    "start": "1261200",
    "end": "1267919"
  },
  {
    "text": "now that we have a very good understanding or a theoretical overview of sagemaker data wrangler let's get",
    "start": "1267919",
    "end": "1274720"
  },
  {
    "start": "1268000",
    "end": "2132000"
  },
  {
    "text": "into sage maker data wrangler demo",
    "start": "1274720",
    "end": "1279200"
  },
  {
    "text": "so the use case as i indicated earlier we'll be using today is customer churn",
    "start": "1282559",
    "end": "1288400"
  },
  {
    "text": "prediction where the objective is to build a machine learning model for",
    "start": "1288400",
    "end": "1293679"
  },
  {
    "text": "automated identification of unhappy customers also known as customer churn",
    "start": "1293679",
    "end": "1299679"
  },
  {
    "text": "prediction we will use an example of churn that is familiar to all of us",
    "start": "1299679",
    "end": "1306799"
  },
  {
    "text": "leaving a mobile phone operator seems like i can always find fault with my provider and if my provider knows that",
    "start": "1306799",
    "end": "1312640"
  },
  {
    "text": "i'm thinking of leaving it can offer timely incentives and i can always use a phone upgrade or perhaps have a new",
    "start": "1312640",
    "end": "1319760"
  },
  {
    "text": "feature activated and i might just stick around we will also share a link to the",
    "start": "1319760",
    "end": "1324960"
  },
  {
    "text": "github repo for the example that we'll be using here is the reference to github where you can find customer churn",
    "start": "1324960",
    "end": "1331039"
  },
  {
    "text": "prediction python notebook and the data set references in the data set",
    "start": "1331039",
    "end": "1336159"
  },
  {
    "text": "you have a column churn which is true or false true means customer churn false",
    "start": "1336159",
    "end": "1341600"
  },
  {
    "text": "means customer did not churn or stay back now that we have a good understanding of",
    "start": "1341600",
    "end": "1346799"
  },
  {
    "text": "our use case uh let's dive deep into exploratory data analysis we got into sagemaker uh through getting to the aws",
    "start": "1346799",
    "end": "1353520"
  },
  {
    "text": "console and what you're viewing now is the sagemaker studio environment",
    "start": "1353520",
    "end": "1359440"
  },
  {
    "text": "as i mentioned earlier eda is a tedious and time consuming process uh let's assume that you have to do all by",
    "start": "1359440",
    "end": "1366159"
  },
  {
    "text": "yourself without sage make a data wrangler or blue data brew so in that case how will you do that you will do it",
    "start": "1366159",
    "end": "1372240"
  },
  {
    "text": "all with jupiter notebook environment right so i have loaded a notebook sample for",
    "start": "1372240",
    "end": "1378320"
  },
  {
    "text": "churn prediction in sagemaker notebook environment that you're seeing and as you can see for you to interpret",
    "start": "1378320",
    "end": "1385039"
  },
  {
    "text": "the trends and pattern in the data you have to import libraries convert the data to data frames write code to",
    "start": "1385039",
    "end": "1391600"
  },
  {
    "text": "visualize your data and then carry on the transformation using code so many lines of code for each transformation",
    "start": "1391600",
    "end": "1397440"
  },
  {
    "text": "and visualization this is all without sagemaker data wrangler or blue data book this is how you do it now",
    "start": "1397440",
    "end": "1405360"
  },
  {
    "text": "considering all of this is going to be tedious we're going to get into data wrangler how can we do this very simply",
    "start": "1405360",
    "end": "1410640"
  },
  {
    "text": "to go to data wrangler all you do is to go to file new data wrangler flow and",
    "start": "1410640",
    "end": "1415919"
  },
  {
    "text": "this is all within sagemaker studio and then first we got to load the data from",
    "start": "1415919",
    "end": "1420960"
  },
  {
    "text": "s3 the different sources that you can select and then we select load data from s3 and you select s3 here",
    "start": "1420960",
    "end": "1428640"
  },
  {
    "text": "and within s3 i have my data source in a specific bucket and i'm going to open that",
    "start": "1428640",
    "end": "1434480"
  },
  {
    "text": "bucket and also the data within the bucket and then once i click on the particular data set i'm going to say",
    "start": "1434480",
    "end": "1440799"
  },
  {
    "text": "import and as you could imagine we will do back and forth steps between analysis and",
    "start": "1440799",
    "end": "1447440"
  },
  {
    "text": "transformation and in real life that is how it works data wrangling is a back and forth activity",
    "start": "1447440",
    "end": "1452640"
  },
  {
    "text": "first let's say we want to view the data set and for viewing the data set you",
    "start": "1452640",
    "end": "1458159"
  },
  {
    "text": "select the data set and then you will say that let's do an analysis you have to just click on the data and then click",
    "start": "1458159",
    "end": "1465200"
  },
  {
    "text": "on add analysis and with the analysis the first step let's take a look at the",
    "start": "1465200",
    "end": "1471600"
  },
  {
    "text": "table summary so we just wanted to get a high level overview of the data set this is all in csv format right so we just",
    "start": "1471600",
    "end": "1477440"
  },
  {
    "text": "wanted to understand how the data set looks so to do table summary you just select the table summary and give the",
    "start": "1477440",
    "end": "1483200"
  },
  {
    "text": "analysis and what you see here is pretty much count mean standard deviation min",
    "start": "1483200",
    "end": "1488480"
  },
  {
    "text": "and max those are the different statistic statistical information about the table that's what you get from the",
    "start": "1488480",
    "end": "1494880"
  },
  {
    "text": "table summary um so this gives a high level overview of the data set right okay these are some of the statistical",
    "start": "1494880",
    "end": "1500960"
  },
  {
    "text": "behavior now let's dive deep a little more let's say i want to do a histogram plot for two columns or for two features",
    "start": "1500960",
    "end": "1508559"
  },
  {
    "text": "which is voicemail plan against churn and in this case um churn",
    "start": "1508559",
    "end": "1514480"
  },
  {
    "text": "needs to be numerical because histogram typically is a numerical plot right but if you see in our data set what do we",
    "start": "1514480",
    "end": "1520480"
  },
  {
    "text": "have churn is either true or false right which is not numerical it's true or false",
    "start": "1520480",
    "end": "1526240"
  },
  {
    "text": "so in this case what we should do is we need to convert the true or false to a numerical format which is going to be",
    "start": "1526240",
    "end": "1532880"
  },
  {
    "text": "zero or one which is like true is one and false means zero so for that i need",
    "start": "1532880",
    "end": "1538000"
  },
  {
    "text": "to carry out a transform uh to convert this so what do we do now that from analysis let us shift back to transform",
    "start": "1538000",
    "end": "1545840"
  },
  {
    "text": "so to do that you go back to the workflow and then you click on uh transform",
    "start": "1545840",
    "end": "1552080"
  },
  {
    "text": "as you see here within the transform you already have two steps there which is the source",
    "start": "1552080",
    "end": "1557279"
  },
  {
    "text": "as well as what are the different types that comes in as soon as you load the data set",
    "start": "1557279",
    "end": "1563120"
  },
  {
    "text": "now once we got the data set let us see how do we convert this to numerical to",
    "start": "1563120",
    "end": "1568799"
  },
  {
    "text": "change the churn to numerical value we're going to use categorical encode which is the ordinal transform within",
    "start": "1568799",
    "end": "1575200"
  },
  {
    "text": "the transformation window and here you go and select the input column as far as the input column is going to be churned",
    "start": "1575200",
    "end": "1581600"
  },
  {
    "text": "with a question mark right which has true or false you give a name to the output column and that is the new column",
    "start": "1581600",
    "end": "1587120"
  },
  {
    "text": "that we are going to come up with in the transformation and you say a preview as you see we now have a new column",
    "start": "1587120",
    "end": "1594400"
  },
  {
    "text": "which is churn and that is going to be float which is 0 or 1 which is a numerical value voila we now have a new column",
    "start": "1594400",
    "end": "1602000"
  },
  {
    "text": "with numerical values which we can go and do the analysis now once we are done",
    "start": "1602000",
    "end": "1607120"
  },
  {
    "text": "with this transformation let's get back to the histogram plot and that's where we left off right uh so this is the back",
    "start": "1607120",
    "end": "1613200"
  },
  {
    "text": "and forth that i was talking about now to for me to do this plot with an histogram i'm going to select a",
    "start": "1613200",
    "end": "1618640"
  },
  {
    "text": "vm plan in the x-axis and color by churn",
    "start": "1618640",
    "end": "1623760"
  },
  {
    "text": "is going to be on the other axis that is how i'm going to color it and say preview",
    "start": "1623760",
    "end": "1629279"
  },
  {
    "text": "what you see here is churn that is going to get bend against a voicemail plan and",
    "start": "1629279",
    "end": "1635200"
  },
  {
    "text": "which is nothing but the count of records and this basically says that we're going to have a good distribution",
    "start": "1635200",
    "end": "1641919"
  },
  {
    "text": "of voicemail plan against the churn values which is all going to be",
    "start": "1641919",
    "end": "1647360"
  },
  {
    "text": "0 or 1. so once this is going to load up you will see that how the distribution is",
    "start": "1647360",
    "end": "1654000"
  },
  {
    "text": "going to be within the chart value",
    "start": "1654000",
    "end": "1657840"
  },
  {
    "text": "once you select the two axes all we are going to do is just to click",
    "start": "1662480",
    "end": "1667600"
  },
  {
    "text": "on preview and then that loads up the chart",
    "start": "1667600",
    "end": "1672799"
  },
  {
    "text": "and we are still doing the histogram plot yeah and this is this is how you do it once you do the transformation you",
    "start": "1679600",
    "end": "1685440"
  },
  {
    "text": "just validate if the data is loading and then you see the binning value across zero and one cool now that we are done",
    "start": "1685440",
    "end": "1692159"
  },
  {
    "text": "with the histogram plot um where you see the data distribution let's do another",
    "start": "1692159",
    "end": "1697279"
  },
  {
    "text": "plot and let's say we save this for a later review uh let's save this analysis",
    "start": "1697279",
    "end": "1702799"
  },
  {
    "text": "and then let's create a new plot and this plot is going to be a scatter plot uh let's do a scatter plot um against",
    "start": "1702799",
    "end": "1711200"
  },
  {
    "text": "the churn versus the evening minutes or day minutes in this case we will just use",
    "start": "1711200",
    "end": "1718320"
  },
  {
    "text": "two axes like x and y and once you select these two axis which is churn against eve minutes or evening minutes",
    "start": "1718320",
    "end": "1725520"
  },
  {
    "text": "you just click on preview and once you click on preview that gives you the chart which pretty much uh says uh the",
    "start": "1725520",
    "end": "1733279"
  },
  {
    "text": "distribution across the church which are going to be the binary values 0 or 1. this gives you a visual representation",
    "start": "1733279",
    "end": "1739600"
  },
  {
    "text": "of how the evening minutes value is scattered across the binary value of 0",
    "start": "1739600",
    "end": "1744799"
  },
  {
    "text": "or what for chun and then you save this analysis for later review as required",
    "start": "1744799",
    "end": "1749919"
  },
  {
    "text": "basically you preview and then later you save the analysis now let's go ahead and do a quick model we saw some",
    "start": "1749919",
    "end": "1755919"
  },
  {
    "text": "visualization we did some transformation now let's build a quick model again all of this is happening within the",
    "start": "1755919",
    "end": "1761919"
  },
  {
    "text": "sagemaker data wrangler and we are not going out of data wrangler itself to do a quick model you select the analysis",
    "start": "1761919",
    "end": "1768799"
  },
  {
    "text": "type as quick model and give a name to the model and then you select the label that you would like to predict and what",
    "start": "1768799",
    "end": "1774399"
  },
  {
    "text": "is the label we are trying to predict it's the chart is this customer going to get churned or no",
    "start": "1774399",
    "end": "1779520"
  },
  {
    "text": "in this quick model we will also present um our what you see here is pretty much",
    "start": "1779520",
    "end": "1785279"
  },
  {
    "text": "the model and the accuracy score of the model right and here is a quick model that we built it directly within the",
    "start": "1785279",
    "end": "1792240"
  },
  {
    "text": "sagemaker data wrangler itself and consider this to be kind of a teaser for your next phase of model building and",
    "start": "1792240",
    "end": "1799520"
  },
  {
    "text": "training process i mean i'm i'm no way trying to reduce the complexity of model building but basically for you to get an",
    "start": "1799520",
    "end": "1806480"
  },
  {
    "text": "understanding of what type of data i have what features are really important to me versus what is not less important",
    "start": "1806480",
    "end": "1811840"
  },
  {
    "text": "and then let me try to bring a model or try to build a model automatically and see how the model building phase would",
    "start": "1811840",
    "end": "1817600"
  },
  {
    "text": "look like so this is how you do that to get a teaser and then here as you see uh you could easily determine what are your",
    "start": "1817600",
    "end": "1824080"
  },
  {
    "text": "important features like day minutes um evening minutes night calls vm messages",
    "start": "1824080",
    "end": "1829679"
  },
  {
    "text": "international calls and the features of less importance on the flip side that's going to be your phone because no one is going to switch their",
    "start": "1829679",
    "end": "1836799"
  },
  {
    "text": "mobile operator because their phone number is different right or the day charge evening charge night charge or",
    "start": "1836799",
    "end": "1843120"
  },
  {
    "text": "international charge so those are going to be the features of less importance and as you see the f1 score also that is",
    "start": "1843120",
    "end": "1848320"
  },
  {
    "text": "highlighted right on top um which is 0.851 on the test data set which is not a bad score at all for a quick model",
    "start": "1848320",
    "end": "1855200"
  },
  {
    "text": "bill that you did and as you can see we quickly imported the data set visualize the data you carried out transforms such",
    "start": "1855200",
    "end": "1862240"
  },
  {
    "text": "as converting a column to categorical dropping the columns as well as build a quick model",
    "start": "1862240",
    "end": "1868240"
  },
  {
    "text": "so as as what you see as a summary after you carry out the transformation you can",
    "start": "1868240",
    "end": "1873760"
  },
  {
    "text": "export the data transformation um into the following that could be s3 bucket or pipelines or feature stores or python",
    "start": "1873760",
    "end": "1880320"
  },
  {
    "text": "code now that we completed the sagemaker data wrangler i'm going to take a pause to see if there are any questions so far",
    "start": "1880320",
    "end": "1889760"
  },
  {
    "text": "yeah hey raj we did we covered a lot there and um you know i think the thing for the the",
    "start": "1889919",
    "end": "1895360"
  },
  {
    "text": "folks watching in in today is the the purpose of data wrangler is to really",
    "start": "1895360",
    "end": "1900720"
  },
  {
    "text": "help make the data preparation that scientists and analysts have been going through previously a lot easier whenever",
    "start": "1900720",
    "end": "1907840"
  },
  {
    "text": "it's whenever they're utilizing the sagemaker suite of ml products is that an accurate statement",
    "start": "1907840",
    "end": "1913440"
  },
  {
    "text": "yeah that's right uh clint so broadly we just want to give a simple interface",
    "start": "1913440",
    "end": "1918640"
  },
  {
    "text": "for users data scientists of people with different uh machine learning expertise right you could go from zero to ten",
    "start": "1918640",
    "end": "1926000"
  },
  {
    "text": "and you could be on any spectrum but we just want to give a point and click because machine learning process as we",
    "start": "1926000",
    "end": "1931120"
  },
  {
    "text": "all know it's a complex process and there's going to be a lot of back and forth and what is needed is a fully",
    "start": "1931120",
    "end": "1937279"
  },
  {
    "text": "integrated system so for that sagemaker studio environment or sagemaker as a whole is your one stop shop for your",
    "start": "1937279",
    "end": "1944320"
  },
  {
    "text": "entire machine learning pipeline of which we just took one component which is sagemaker datawrangler which is a",
    "start": "1944320",
    "end": "1950399"
  },
  {
    "text": "point and click or you may call it low code no code solution to wrangle your data or to prepare your data or to",
    "start": "1950399",
    "end": "1957039"
  },
  {
    "text": "transform your data all your data preparation before you feed the data to machine learning process this is how you",
    "start": "1957039",
    "end": "1963360"
  },
  {
    "text": "do all the data prep and um are there any specific questions yeah we did have one that came through",
    "start": "1963360",
    "end": "1969600"
  },
  {
    "text": "here it's sort of an umbrella question there's it goes with sagemaker it's from jibjab06",
    "start": "1969600",
    "end": "1975120"
  },
  {
    "text": "thanks for tuning back in again uh how does sagemaker data wrangler work",
    "start": "1975120",
    "end": "1980799"
  },
  {
    "text": "with sagemaker studio yeah i'll keep it crisp stage maker data wrangler is a feature within sagemaker",
    "start": "1980799",
    "end": "1987600"
  },
  {
    "text": "studio it is within sagemaker studio so as you already know sage maker studio is",
    "start": "1987600",
    "end": "1992799"
  },
  {
    "text": "a feature-rich environment where you do your end-to-end pipeline once you load",
    "start": "1992799",
    "end": "1998159"
  },
  {
    "text": "the sagemaker studio once you create a new domain and launch your sagemaker studio um you could go ahead and use the",
    "start": "1998159",
    "end": "2005440"
  },
  {
    "text": "visual interface within sagemaker studio to do your entire pipeline like you could do feature engineering with",
    "start": "2005440",
    "end": "2011519"
  },
  {
    "text": "feature store you could also do the integration work between model build and model train and you can access",
    "start": "2011519",
    "end": "2017600"
  },
  {
    "text": "your jupyter notebook basically through the sagemaker studio right and then on the left side or the left pane that's",
    "start": "2017600",
    "end": "2022640"
  },
  {
    "text": "where you integrate the github etc so one of the aspects within sagemaker",
    "start": "2022640",
    "end": "2027679"
  },
  {
    "text": "studio is data wrangler so as we saw in the demo you once you login into the sagemaker studio you go ahead and open a",
    "start": "2027679",
    "end": "2034880"
  },
  {
    "text": "new workflow within uh the studio and in that workflow one of the components is the data wrangle flow and then that's",
    "start": "2034880",
    "end": "2041600"
  },
  {
    "text": "where you start working on your data and the the key differentiator here is that",
    "start": "2041600",
    "end": "2047919"
  },
  {
    "text": "it's all within one integrated system so you don't have to go out of sagemaker at",
    "start": "2047919",
    "end": "2052960"
  },
  {
    "text": "all you can wrangle with your data prepare your data and then with the ease of integration with other tools like s3",
    "start": "2052960",
    "end": "2060560"
  },
  {
    "text": "or feature store you can quickly move around with your data or there are also third-party tools that are integrated",
    "start": "2060560",
    "end": "2066960"
  },
  {
    "text": "within data wrangler um and in those cases once you select the source to be the third party tools that you're",
    "start": "2066960",
    "end": "2072800"
  },
  {
    "text": "looking for and often as you see the products are iterative and we expand",
    "start": "2072800",
    "end": "2078158"
  },
  {
    "text": "based on customer use cases and feedback that we get and it is only going to get expanded in terms of support to new",
    "start": "2078159",
    "end": "2084398"
  },
  {
    "text": "tools etc so the idea here is sagemaker is going to be your one-stop shop for",
    "start": "2084399",
    "end": "2090240"
  },
  {
    "text": "your ml pipelines and within sagemaker datawrangler is your tool for wrangling",
    "start": "2090240",
    "end": "2096638"
  },
  {
    "text": "with your data if that makes sense yeah that makes that makes good sense and it was a good explanation so thanks",
    "start": "2096639",
    "end": "2102320"
  },
  {
    "text": "for that raj um for the for the attendees online please feel free to through your questions as we go through",
    "start": "2102320",
    "end": "2107839"
  },
  {
    "text": "here um today and then we'll get those over to raj um but that was the the question",
    "start": "2107839",
    "end": "2113520"
  },
  {
    "text": "that has come through for now so back to you awesome thanks glenn so we are like",
    "start": "2113520",
    "end": "2119599"
  },
  {
    "text": "through the halfway mark of our session today so as i said earlier we wanted to cover uh two tools right one is",
    "start": "2119599",
    "end": "2126800"
  },
  {
    "text": "sagemaker uh datawrangler and the second one is blue data brew so now that we've",
    "start": "2126800",
    "end": "2132640"
  },
  {
    "start": "2132000",
    "end": "2492000"
  },
  {
    "text": "covered sagemaker data wrangler with an overview and then a demo we'll follow the same methodology we're going to do a",
    "start": "2132640",
    "end": "2140000"
  },
  {
    "text": "glue data brew demo and then i mean first is going to be an overview",
    "start": "2140000",
    "end": "2145599"
  },
  {
    "text": "that is going to be followed by a glue data pro demo so before we get started just an",
    "start": "2145599",
    "end": "2151760"
  },
  {
    "text": "overview aws glue data group is a new visual data preparation tool that makes",
    "start": "2151760",
    "end": "2158079"
  },
  {
    "text": "it easy for data analysts and data scientists to clean and normalize the data to prepare it for data analytics",
    "start": "2158079",
    "end": "2165680"
  },
  {
    "text": "and machine learning and as i'm going to get through glue a common question that we normally uh get asked are for the",
    "start": "2165680",
    "end": "2173359"
  },
  {
    "text": "audiences here as well uh hey we have sagemaker data wrangler we also have blue data blue so what is",
    "start": "2173359",
    "end": "2179760"
  },
  {
    "text": "the value prop here right so there is uh one particular slide that i've put together where we will talk about that",
    "start": "2179760",
    "end": "2186320"
  },
  {
    "text": "so for now just uh so that we are all on same page uh these two are two different",
    "start": "2186320",
    "end": "2191599"
  },
  {
    "text": "tools within two ecosystems your sagemaker data wrangler is within your sagemaker ecosystem and your glue data",
    "start": "2191599",
    "end": "2198720"
  },
  {
    "text": "brew is another tool for data wrangling which is available within your blue ecosystem that is on the data analytics",
    "start": "2198720",
    "end": "2204560"
  },
  {
    "text": "so often data analytics and machine learning go hand in hand so we wanted to cover both the tools on the",
    "start": "2204560",
    "end": "2211920"
  },
  {
    "text": "complete spectrum and these two tools the commonality between these two tools is that they both are low code no code",
    "start": "2211920",
    "end": "2220960"
  },
  {
    "text": "visual interfaces and consider them to be point and click cool so before we dive into sagemaker",
    "start": "2220960",
    "end": "2227520"
  },
  {
    "text": "data blue i mean sorry i'm interchangeably using the uh features and the service name glue data",
    "start": "2227520",
    "end": "2234400"
  },
  {
    "text": "bro sagemaker data wrangler is done and now we are into glue data brew so let's first talk about glue aws glue is",
    "start": "2234400",
    "end": "2244000"
  },
  {
    "text": "serverless data integration service that makes it easy to discover prepare and",
    "start": "2244000",
    "end": "2250000"
  },
  {
    "text": "combine the data for analytics machine learning and application development aws",
    "start": "2250000",
    "end": "2255359"
  },
  {
    "text": "glue provides all the capabilities needed for data integration so that you can start analyzing your data and",
    "start": "2255359",
    "end": "2263200"
  },
  {
    "text": "putting it to use in minutes instead of months aws glue",
    "start": "2263200",
    "end": "2268320"
  },
  {
    "text": "provides a visual and code-based interface to make data integration",
    "start": "2268320",
    "end": "2273359"
  },
  {
    "text": "easier let's talk about glue data brew in detail now",
    "start": "2273359",
    "end": "2278560"
  },
  {
    "text": "data analyst and data scientist can use aws glue data brew to visually enrich",
    "start": "2278560",
    "end": "2285599"
  },
  {
    "text": "clean and normalize the data without writing any code and what you see here",
    "start": "2285599",
    "end": "2290960"
  },
  {
    "text": "is pretty much the glue data brew console we will dive deep into it as we",
    "start": "2290960",
    "end": "2296320"
  },
  {
    "text": "get into the demo section blue data bro auto discovers the",
    "start": "2296320",
    "end": "2301520"
  },
  {
    "text": "location schema and the runtime metrics of data from your aws glue data catalog",
    "start": "2301520",
    "end": "2307440"
  },
  {
    "text": "as i mentioned earlier it is within the aws glue ecosystem and it connects with",
    "start": "2307440",
    "end": "2312480"
  },
  {
    "text": "your aws data stores and it provides an interactive interface to explore combine",
    "start": "2312480",
    "end": "2319680"
  },
  {
    "text": "clean and transform the raw data without writing any code",
    "start": "2319680",
    "end": "2325359"
  },
  {
    "text": "data brew auto discovers the location schema your runtime metrics of the data",
    "start": "2325359",
    "end": "2330880"
  },
  {
    "text": "all from your data catalog and it connects with your aws data stores and",
    "start": "2330880",
    "end": "2336960"
  },
  {
    "text": "provides an interactive interface to explore combine clean and transform the",
    "start": "2336960",
    "end": "2342720"
  },
  {
    "text": "raw data without any code this is the slide that i was referring",
    "start": "2342720",
    "end": "2348800"
  },
  {
    "text": "to just before getting started with data brew now that we've discussed sagemaker data wrangler and an overview of glue",
    "start": "2348800",
    "end": "2356560"
  },
  {
    "text": "data blue a common question that i indicated earlier is that hey where does",
    "start": "2356560",
    "end": "2361680"
  },
  {
    "text": "these two tools fit into the equation of data prep right so i'll address it in",
    "start": "2361680",
    "end": "2366960"
  },
  {
    "text": "two folds or two faces one is persona so from a persona perspective data",
    "start": "2366960",
    "end": "2373520"
  },
  {
    "text": "engineers and data analysts can use glue studio and data brew for data",
    "start": "2373520",
    "end": "2379839"
  },
  {
    "text": "preparation on the other side your data analyst and your data scientist can use sagemaker",
    "start": "2379839",
    "end": "2386800"
  },
  {
    "text": "studio and sagemaker data wrangler for data preparation when it comes to the complexity this is the second aspect",
    "start": "2386800",
    "end": "2393839"
  },
  {
    "text": "that i was talking about the complexity and coding perspective consider glue data blue to be a tool for no code where",
    "start": "2393839",
    "end": "2400240"
  },
  {
    "text": "you don't have to bring in any custom code for data wrangling on the flip side let's say that you have custom code",
    "start": "2400240",
    "end": "2407040"
  },
  {
    "text": "you've already built the custom code right that could be in pi spark sql or pandas in those cases you can use",
    "start": "2407040",
    "end": "2414160"
  },
  {
    "text": "sagemaker data wrangler to run your custom code because you already have the code and you want to have a mechanism to",
    "start": "2414160",
    "end": "2419520"
  },
  {
    "text": "run the code again if you do not have the code you can still choose data brew or sagemaker data wrangler but for",
    "start": "2419520",
    "end": "2425839"
  },
  {
    "text": "custom code the way to do is through sagemaker datawrangler",
    "start": "2425839",
    "end": "2431119"
  },
  {
    "text": "it all depends on where your data resides and where in the pipeline you want the data analysis and",
    "start": "2431119",
    "end": "2438480"
  },
  {
    "text": "transformation to take place is it closer to your etl base or is it closer",
    "start": "2438480",
    "end": "2443839"
  },
  {
    "text": "to your ml pipeline base as you imagine it's not a simple one way or the other",
    "start": "2443839",
    "end": "2449200"
  },
  {
    "text": "answer but largely it depends on your use case what data analysis and transformations",
    "start": "2449200",
    "end": "2455680"
  },
  {
    "text": "like native built-in or custom transforms that you would like to carry on which data set what format are they",
    "start": "2455680",
    "end": "2462960"
  },
  {
    "text": "in and where does your data recite that's the source and then which pipeline are you focusing your data",
    "start": "2462960",
    "end": "2469200"
  },
  {
    "text": "transformation for also not to forget what is your downstream consumption is it analytics",
    "start": "2469200",
    "end": "2475119"
  },
  {
    "text": "or is it machine learning or maybe it's going to decide in a data lake all of these put together uh it's going to",
    "start": "2475119",
    "end": "2481680"
  },
  {
    "text": "decide what kind of tool option that you got to choose but the key pointer here is a visual",
    "start": "2481680",
    "end": "2488319"
  },
  {
    "text": "interface for you to do all your data wrangling now with the glue data brew let us",
    "start": "2488319",
    "end": "2495119"
  },
  {
    "start": "2492000",
    "end": "2695000"
  },
  {
    "text": "explore all the features of data brew we are going to cover all of this in detail in a demo so we'll quickly glance over",
    "start": "2495119",
    "end": "2502240"
  },
  {
    "text": "those features within data brew in glue data blue the project workspace presents you with a visual view of your data and",
    "start": "2502240",
    "end": "2510240"
  },
  {
    "text": "it also allows you to perform your data prep on a step-by-step basis and see the",
    "start": "2510240",
    "end": "2516079"
  },
  {
    "text": "results of change after each step so you can quickly trade between them i carry",
    "start": "2516079",
    "end": "2521520"
  },
  {
    "text": "out an analysis and then i see the results i carry out an analysis i see the result",
    "start": "2521520",
    "end": "2528079"
  },
  {
    "text": "in the schema view this is another view within your blue data brew you can see the",
    "start": "2528079",
    "end": "2533680"
  },
  {
    "text": "statistics about the data column in each column and you can rename a column by",
    "start": "2533680",
    "end": "2539920"
  },
  {
    "text": "entering a new name for the column name and you could also arrange the column order by dragging and dropping the",
    "start": "2539920",
    "end": "2547440"
  },
  {
    "text": "columns",
    "start": "2547440",
    "end": "2550440"
  },
  {
    "text": "data brew has a rich profiling functionality um it's uh just in few",
    "start": "2553359",
    "end": "2559119"
  },
  {
    "text": "clicks you could get a synopsis of your data",
    "start": "2559119",
    "end": "2565520"
  },
  {
    "text": "users can create data quality rules that brings customizable checks to your",
    "start": "2565520",
    "end": "2571359"
  },
  {
    "text": "data set and it also allows users to access the data quality based on unique",
    "start": "2571359",
    "end": "2578079"
  },
  {
    "text": "business use cases data brew provides options to help identify pii data personally",
    "start": "2578079",
    "end": "2585359"
  },
  {
    "text": "identifiable information data and that is through running a data profile",
    "start": "2585359",
    "end": "2590400"
  },
  {
    "text": "and as well as once you identify the data it also helps you carry out few transformations for",
    "start": "2590400",
    "end": "2597040"
  },
  {
    "text": "masking or reducting the sensitive data data scientists can use blue data brew",
    "start": "2597040",
    "end": "2604160"
  },
  {
    "text": "from within their jupiter notebook environment as well uh the glue data blue environment",
    "start": "2604160",
    "end": "2610480"
  },
  {
    "text": "tab or the jupiter extension will share a link to the github repo where you can",
    "start": "2610480",
    "end": "2615599"
  },
  {
    "text": "get the jupyter extension an outlier right this is an aspect that",
    "start": "2615599",
    "end": "2621359"
  },
  {
    "text": "is normally uh discussed during your data preparation phases an outlier is a",
    "start": "2621359",
    "end": "2626560"
  },
  {
    "text": "data point that is noticeably different from the rest and this represents an",
    "start": "2626560",
    "end": "2632720"
  },
  {
    "text": "error in measurement uh bad data that could be from the collection or simply",
    "start": "2632720",
    "end": "2639359"
  },
  {
    "text": "show variables not considered when collecting the data right so it is important to identify these outliers",
    "start": "2639359",
    "end": "2645359"
  },
  {
    "text": "pretty early in your data prep because your model build phase could be skewed because you did not outla find the",
    "start": "2645359",
    "end": "2652160"
  },
  {
    "text": "outlier pretty early and you presented the data with an outlier and that outlier could be a huge influencer for",
    "start": "2652160",
    "end": "2658880"
  },
  {
    "text": "your model outcome and you can remove outliers as required directly to the",
    "start": "2658880",
    "end": "2664160"
  },
  {
    "text": "point-and-click uh interface that you have with bluedatabro databrew also tracks your data in a",
    "start": "2664160",
    "end": "2671359"
  },
  {
    "text": "visual interface to determine its origin we call that data lineage and this view",
    "start": "2671359",
    "end": "2677040"
  },
  {
    "text": "shows uh how the data flows through different entities um from where it originally came from and you can also",
    "start": "2677040",
    "end": "2683920"
  },
  {
    "text": "see its origin uh other entities it was influenced by and what happened to it",
    "start": "2683920",
    "end": "2689680"
  },
  {
    "text": "over time and where it was stored finally",
    "start": "2689680",
    "end": "2694240"
  },
  {
    "start": "2695000",
    "end": "3448000"
  },
  {
    "text": "great now that we covered the um glue data brew as an overview let's uh",
    "start": "2695760",
    "end": "2702720"
  },
  {
    "text": "get into the demo and then finally we will wrap up with any questions you may",
    "start": "2702720",
    "end": "2708000"
  },
  {
    "text": "have",
    "start": "2708000",
    "end": "2710240"
  },
  {
    "text": "awesome so let's get into the glue data brew demo here",
    "start": "2714079",
    "end": "2719200"
  },
  {
    "text": "with your demo environment what you're seeing now is the glue data blue console and we're going to bring in the same",
    "start": "2719200",
    "end": "2725440"
  },
  {
    "text": "data set which is the churn data set and i've already loaded the same churn data",
    "start": "2725440",
    "end": "2730560"
  },
  {
    "text": "set to data blue and as you can see we have data sets projects recipes data",
    "start": "2730560",
    "end": "2737040"
  },
  {
    "text": "quality rules jobs and what's new on the left pane and the different data set",
    "start": "2737040",
    "end": "2743359"
  },
  {
    "text": "that is loaded in the console under the data set you can preview the data once it's",
    "start": "2743359",
    "end": "2750000"
  },
  {
    "text": "loaded and you can also see the data set details such as name of the data set",
    "start": "2750000",
    "end": "2755599"
  },
  {
    "text": "data size where is it coming from that is the source and are there any projects or",
    "start": "2755599",
    "end": "2761200"
  },
  {
    "text": "jobs associated with that particular data set right on top where it says data set details",
    "start": "2761200",
    "end": "2767280"
  },
  {
    "text": "and you can also see the different columns of the data",
    "start": "2767280",
    "end": "2772400"
  },
  {
    "text": "right at the bottom that's where you preview the data set data brew also offers a rich data",
    "start": "2772400",
    "end": "2780160"
  },
  {
    "text": "profile overview which we're going to see next and as i click on the next tab",
    "start": "2780160",
    "end": "2785680"
  },
  {
    "text": "that is where you see the data profile overview you will run a data profile job to get",
    "start": "2785680",
    "end": "2792000"
  },
  {
    "text": "to this data profile overview which we'll cover in the later part of the demo and you can also",
    "start": "2792000",
    "end": "2798000"
  },
  {
    "text": "understand the correlation matrix what you see here the that basically gives you a visual representation of how",
    "start": "2798000",
    "end": "2806000"
  },
  {
    "text": "two variables are related with each other um so in this case zero means there's no relationship between the",
    "start": "2806000",
    "end": "2811839"
  },
  {
    "text": "variables this will help you determine which features you may want to retain and which features you may want to drop",
    "start": "2811839",
    "end": "2818000"
  },
  {
    "text": "based on the relationship you don't want to have a highly correlated data information being fed into your data",
    "start": "2818000",
    "end": "2823280"
  },
  {
    "text": "model you can also see the value distribution comparison which is a numerical value that is scaled between 0 and 1 that",
    "start": "2823280",
    "end": "2829839"
  },
  {
    "text": "gives you a comparative view of the distribution across the columns",
    "start": "2829839",
    "end": "2836800"
  },
  {
    "text": "you can also get column statistics view and this is all coming out of the box",
    "start": "2836800",
    "end": "2841920"
  },
  {
    "text": "you can view the column headers how many records are valid let's say a hundred percent is valid as you see on the left",
    "start": "2841920",
    "end": "2848800"
  },
  {
    "text": "column and you can also see the top distinct values as you see on the right",
    "start": "2848800",
    "end": "2853839"
  },
  {
    "text": "side um in our case you are seeing what you're seeing is basically the state distribution of the top distinct",
    "start": "2853839",
    "end": "2860880"
  },
  {
    "text": "values and then you can also analyze the data",
    "start": "2860880",
    "end": "2865920"
  },
  {
    "text": "quality using data quality rules and that's what we're seeing right now and data quality rules are customizable",
    "start": "2865920",
    "end": "2872400"
  },
  {
    "text": "validation checks that define business requirements for specific data and finally the data lineage as you see here",
    "start": "2872400",
    "end": "2879760"
  },
  {
    "text": "where is your data coming from what is the source and what is your data set info on project is it related to a",
    "start": "2879760",
    "end": "2886400"
  },
  {
    "text": "particular project and in that case it will also tell me if there is any job that you ran against the data and where",
    "start": "2886400",
    "end": "2893040"
  },
  {
    "text": "the output of that job is getting stored",
    "start": "2893040",
    "end": "2898280"
  },
  {
    "text": "now that we took a a deep dive into the data set aspects within uh data brew the",
    "start": "2900000",
    "end": "2906960"
  },
  {
    "text": "next thing that we're going to talk about is projects and consider projects",
    "start": "2906960",
    "end": "2912000"
  },
  {
    "text": "as a place where you're going to carry out all your transformation and once you",
    "start": "2912000",
    "end": "2917200"
  },
  {
    "text": "get your data set you will create a project and that's why you want to carry out all the transforms",
    "start": "2917200",
    "end": "2923599"
  },
  {
    "text": "and to create a project uh what you will do is that you will first enter a",
    "start": "2923599",
    "end": "2928640"
  },
  {
    "text": "project name and then you will also select the data",
    "start": "2928640",
    "end": "2933839"
  },
  {
    "text": "set and then you can use tags as required and then select the role that i'm going to outline here and then all after that",
    "start": "2933839",
    "end": "2940160"
  },
  {
    "text": "you just click on a project and we i'm not going to click on a project here because i've already created a project just to save time let's open that",
    "start": "2940160",
    "end": "2947040"
  },
  {
    "text": "project and see what we could see within the project so you have three views within project grid schema and profile",
    "start": "2947040",
    "end": "2954079"
  },
  {
    "text": "view grid gives you an overall view of the data set that includes statistical headers um your schema view which is the",
    "start": "2954079",
    "end": "2962400"
  },
  {
    "text": "next view that gives you a detail on headers so let's move to schema view yeah this",
    "start": "2962400",
    "end": "2969440"
  },
  {
    "text": "gives you details on headers where the type of the data in the column and data quality and distribution and finally the",
    "start": "2969440",
    "end": "2974960"
  },
  {
    "text": "profile view we already saw about the profile view that gives you an overall profile view with statistical",
    "start": "2974960",
    "end": "2980640"
  },
  {
    "text": "information about the data set cool now that we have a good",
    "start": "2980640",
    "end": "2986559"
  },
  {
    "text": "understanding of the different views let's say we want to group the data set this is the transformation i was talking about in our case we want to aggregate",
    "start": "2986559",
    "end": "2994400"
  },
  {
    "text": "the state against churn count you could do anything but i'm just taking an example of aggregating the state against the",
    "start": "2994400",
    "end": "3001119"
  },
  {
    "text": "churn count you select the column as state and churn aggregate as count",
    "start": "3001119",
    "end": "3006240"
  },
  {
    "text": "and yeah the the next one is going to be churn aggregators count and what you see",
    "start": "3006240",
    "end": "3011680"
  },
  {
    "text": "here is that for each state how many counts we have for the target label and here",
    "start": "3011680",
    "end": "3019040"
  },
  {
    "text": "the target label is going to be chun cool and this is one other view that we",
    "start": "3019040",
    "end": "3024720"
  },
  {
    "text": "can get this is a different information that we could get and let's say that we want to do a categorical mapping of the",
    "start": "3024720",
    "end": "3030319"
  },
  {
    "text": "churn column right so this is going to be the same transformation we saw earlier within data wrangler converting",
    "start": "3030319",
    "end": "3036559"
  },
  {
    "text": "the true or false to one or zero it's pretty much the same i'm just going to do it with data brew and see how you do it you give",
    "start": "3036559",
    "end": "3043440"
  },
  {
    "text": "a name to the destination column and also select the values to be",
    "start": "3043440",
    "end": "3049440"
  },
  {
    "text": "you first say hey if it is true it is going to be 1 if it is going to be false it is going to",
    "start": "3049440",
    "end": "3055119"
  },
  {
    "text": "be 0 and then after that you also select what is the column header for",
    "start": "3055119",
    "end": "3061040"
  },
  {
    "text": "destination that you are going to select um in this case lets say that we are going to say churn mapped that is going",
    "start": "3061040",
    "end": "3066720"
  },
  {
    "text": "to be the target header and then now we uh click on the new column with this churn map",
    "start": "3066720",
    "end": "3072880"
  },
  {
    "text": "which is going to be the numerical value and now with the",
    "start": "3072880",
    "end": "3078960"
  },
  {
    "text": "value that is getting mapped we're just going to click on apply and that's going to load the data set with the new column",
    "start": "3078960",
    "end": "3084880"
  },
  {
    "text": "you see the churn map there which ones are zeros that just came out of this transform now that we have this column",
    "start": "3084880",
    "end": "3090559"
  },
  {
    "text": "we can drop the old column which is churn flag which is the 2r value the last thing i want to call out is how do",
    "start": "3090559",
    "end": "3097839"
  },
  {
    "text": "you go about creating a job yeah a job is nothing but",
    "start": "3097839",
    "end": "3104079"
  },
  {
    "text": "within data brew it is just like a work order and it performs the task that you",
    "start": "3104079",
    "end": "3109359"
  },
  {
    "text": "assign to it that could be like either scheduled or on demand so to do that",
    "start": "3109359",
    "end": "3115119"
  },
  {
    "text": "you get into jobs on the left side and you can create either a data profile job",
    "start": "3115119",
    "end": "3121440"
  },
  {
    "text": "or you could even create a recipe job whichever that you want and data profile job this is",
    "start": "3121440",
    "end": "3127599"
  },
  {
    "text": "exactly the one that we spoke about earlier uh within the data profile you are seeing an overview of the data right",
    "start": "3127599",
    "end": "3133839"
  },
  {
    "text": "so this is how you will run a job to get that view and to create a job you will say that hey create a profile job and",
    "start": "3133839",
    "end": "3141359"
  },
  {
    "text": "select the role as required and then create and run job and i'm not going to create a job here because i've already",
    "start": "3141359",
    "end": "3148000"
  },
  {
    "text": "run one and i'm going to open up that profile job here is the profile job that",
    "start": "3148000",
    "end": "3154079"
  },
  {
    "text": "we already ran yeah these are the profile jobs that we ran previously and this data profile",
    "start": "3154079",
    "end": "3160960"
  },
  {
    "text": "overview once you click on the job it's going to bring that overview and that is nothing but uh the first screen that we",
    "start": "3160960",
    "end": "3167680"
  },
  {
    "text": "saw that we saw before we got into the demo right so this is how you will run a",
    "start": "3167680",
    "end": "3173359"
  },
  {
    "text": "job to get that data profile overview i was just trying to work backwards like this is the output that you get first",
    "start": "3173359",
    "end": "3179599"
  },
  {
    "text": "and how do you get that output this is how you get it by running that particular job so just as a summary",
    "start": "3179599",
    "end": "3185760"
  },
  {
    "text": "right finally to wrap up with sagemaker data wrangler and glue data blue are two options for you to do",
    "start": "3185760",
    "end": "3192960"
  },
  {
    "text": "your transformation just on the glue data blue alone the data profile overview that is where you can visually",
    "start": "3192960",
    "end": "3199440"
  },
  {
    "text": "analyze your data set this is the analysis phase the pictorial representation visualization that is",
    "start": "3199440",
    "end": "3204480"
  },
  {
    "text": "where you do all the analysis the second page that we saw which is the profile",
    "start": "3204480",
    "end": "3210960"
  },
  {
    "text": "view that is where you do all the transformations within the project so when you get into the projects that's",
    "start": "3210960",
    "end": "3216720"
  },
  {
    "text": "where you do all the transformations awesome so that's that's a lot of information i get it but consider this",
    "start": "3216720",
    "end": "3223760"
  },
  {
    "text": "to be kind of a teaser right in terms of what the point and click tools can offer",
    "start": "3223760",
    "end": "3228880"
  },
  {
    "text": "you like sage maker data wrangler or blue data brew um and there is a lot",
    "start": "3228880",
    "end": "3234559"
  },
  {
    "text": "more we could do but as you know with the limited time uh these are the things that we can showcase so you you get a",
    "start": "3234559",
    "end": "3241040"
  },
  {
    "text": "feel of what you could do with a point and click tools and uh thereby you you can use it for your respective use cases",
    "start": "3241040",
    "end": "3247680"
  },
  {
    "text": "again as i said earlier it all depends on the use cases that you have cool now that um we are done with the demo let's",
    "start": "3247680",
    "end": "3256079"
  },
  {
    "text": "see if there are any questions yeah uh thanks raj and that was a really",
    "start": "3256079",
    "end": "3262079"
  },
  {
    "text": "good presentation two fantastic demos that we went through so we do have a question here from a first timer",
    "start": "3262079",
    "end": "3269640"
  },
  {
    "text": "whoopsie123 are the steps generated by point and click save somewhere for example in the form of the script",
    "start": "3269640",
    "end": "3276960"
  },
  {
    "text": "absolutely so um i think i covered this previously but let me uh rephrase that",
    "start": "3276960",
    "end": "3282400"
  },
  {
    "text": "so once you do all these activities right with regards to data wrangler or",
    "start": "3282400",
    "end": "3287520"
  },
  {
    "text": "data brew say for example data wrangler right you do all the point and click you create a new workflow you do an analysis",
    "start": "3287520",
    "end": "3294400"
  },
  {
    "text": "and then you go back to transformation etc once that is done you have an option",
    "start": "3294400",
    "end": "3299680"
  },
  {
    "text": "to export so there there is a particular feature to export and those exports",
    "start": "3299680",
    "end": "3305040"
  },
  {
    "text": "could be to different sources you may want to export it as a script and keep it for later reference you can store it",
    "start": "3305040",
    "end": "3311680"
  },
  {
    "text": "in s3 or you may want to take it into a feature store also so there are different targets where you can export",
    "start": "3311680",
    "end": "3318640"
  },
  {
    "text": "these steps and you can do that repetitively and and the point here is that data analytics data integration or",
    "start": "3318640",
    "end": "3325920"
  },
  {
    "text": "the entire data prep workflow is an iterative process right so you may want to do it for a sample data set and",
    "start": "3325920",
    "end": "3331839"
  },
  {
    "text": "iteratively apply to other datasets so that is why we have this feature to export awesome",
    "start": "3331839",
    "end": "3337280"
  },
  {
    "text": "nice that was uh i don't believe there's any other questions and we are at about time for today so for the folks tuning",
    "start": "3337280",
    "end": "3344319"
  },
  {
    "text": "in online thanks uh thanks for for for joining in glad that explanation was",
    "start": "3344319",
    "end": "3349440"
  },
  {
    "text": "clear for you there um today we looked at two distinctly uh two topics which was sagemaker data wrangler which is",
    "start": "3349440",
    "end": "3356079"
  },
  {
    "text": "going to help you reduce the time to simplify your data uh preparation processes as well as glue data brew",
    "start": "3356079",
    "end": "3362799"
  },
  {
    "text": "which is a visual data preparation tool uh which your different data teams can use to help clean and prepare the data",
    "start": "3362799",
    "end": "3370799"
  },
  {
    "text": "if there are questions that were not answered today please feel free to head over to repost.aws",
    "start": "3370799",
    "end": "3377280"
  },
  {
    "text": "where you can either ask your question or maybe you can help answer one of your fellow online colleagues questions there",
    "start": "3377280",
    "end": "3383680"
  },
  {
    "text": "and become an expert maybe dive a little deep and and try to to answer a question to help somebody",
    "start": "3383680",
    "end": "3389599"
  },
  {
    "text": "out and those could actually turn in to be topics for a future um aws supports",
    "start": "3389599",
    "end": "3394880"
  },
  {
    "text": "you uh discussion which is actually what we'll be doing next monday where we're going to answer a few questions",
    "start": "3394880",
    "end": "3401040"
  },
  {
    "text": "off of repost live and have a few demos to go along with it if you do have a topic of interest",
    "start": "3401040",
    "end": "3407520"
  },
  {
    "text": "please feel free to send us an email at aws support shoot at amazon.com we definitely want to hear from you and get",
    "start": "3407520",
    "end": "3413680"
  },
  {
    "text": "your feedback so for now that's all for today raj again thanks so much for joining us back",
    "start": "3413680",
    "end": "3419839"
  },
  {
    "text": "on aws supports you it's fantastic to have you here sir anytime thanks a lot glenn hope this is",
    "start": "3419839",
    "end": "3425760"
  },
  {
    "text": "all useful for you wishing you all the very best absolutely and everyone online um happy cloud computing",
    "start": "3425760",
    "end": "3432760"
  },
  {
    "text": "[Music]",
    "start": "3432760",
    "end": "3444719"
  },
  {
    "text": "you",
    "start": "3447119",
    "end": "3449200"
  }
]