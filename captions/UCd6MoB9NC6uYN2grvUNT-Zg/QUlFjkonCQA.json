[
  {
    "start": "0",
    "end": "32000"
  },
  {
    "text": "right so who here thinks s3 by itself by",
    "start": "0",
    "end": "5220"
  },
  {
    "text": "itself is the data Lake raise your hands please go as I did a lake by itself is",
    "start": "5220",
    "end": "13110"
  },
  {
    "text": "actually quite more than just s3 right because if you're just chucking objects into s3 you're really just creating a",
    "start": "13110",
    "end": "18390"
  },
  {
    "text": "day to look at the swamp right and we're here to talk about what actually you need to build on top of s3 and other say",
    "start": "18390",
    "end": "24630"
  },
  {
    "text": "object stores to really create a secure and compliant data Lake upon which you",
    "start": "24630",
    "end": "30210"
  },
  {
    "text": "can perform analytics so I'm trying to end I'm a big data consultant with Amazon Web Services and I'm gonna talk",
    "start": "30210",
    "end": "35460"
  },
  {
    "start": "32000",
    "end": "32000"
  },
  {
    "text": "to you about all that stuff and we're gonna break it down to four phases during this session so first we're gonna",
    "start": "35460",
    "end": "41670"
  },
  {
    "text": "understand the value prop for our data Lake and how AWS can help then we're gonna get a sense of what is required to",
    "start": "41670",
    "end": "47670"
  },
  {
    "text": "have a successful data Lake implementation we're going to then listen to a public sector customers vision around their daily Lake strategy",
    "start": "47670",
    "end": "53699"
  },
  {
    "text": "and their successful proof of concept on AWS and we're also going to learn one way to do it in AWS with a deep dive",
    "start": "53699",
    "end": "60449"
  },
  {
    "text": "into the proof of concept implementation we created for this customer so",
    "start": "60449",
    "end": "66030"
  },
  {
    "start": "65000",
    "end": "65000"
  },
  {
    "text": "traditionally analytics looks like this right you have a bunch of data silos typically disparate databases all",
    "start": "66030",
    "end": "71729"
  },
  {
    "text": "flowing into your data warehouse it might be relational data you snuck into",
    "start": "71729",
    "end": "77939"
  },
  {
    "text": "there typically it's terabytes to even possibly petabytes scale and size the schema is very rigid right it's defined",
    "start": "77939",
    "end": "85409"
  },
  {
    "text": "prior to data load and it's typically only ever used for operational reporting and ad hoc sort of query and the",
    "start": "85409",
    "end": "92640"
  },
  {
    "text": "downsides of this kind of architecture are that you have a really really large initial Catholics cost and you also have",
    "start": "92640",
    "end": "98100"
  },
  {
    "text": "continual optics costs which aren't cheap and this is the way things have",
    "start": "98100",
    "end": "103500"
  },
  {
    "start": "103000",
    "end": "103000"
  },
  {
    "text": "been going for quite some time right and as time the balls technology evolves requirements evolve and new",
    "start": "103500",
    "end": "110159"
  },
  {
    "text": "requirements now break the traditional approach upon which we need to look at our data so our requirements our",
    "start": "110159",
    "end": "116280"
  },
  {
    "text": "business customers are all this stuff kind of think about these new and innovative sort of use cases such as",
    "start": "116280",
    "end": "122369"
  },
  {
    "text": "capturing and storing new non-relational data at the exabyte scale securing and",
    "start": "122369",
    "end": "128009"
  },
  {
    "text": "combining data from new existing sources right no longer are we looking at data something that just happens as a",
    "start": "128009",
    "end": "133760"
  },
  {
    "text": "consequence of our business it's something that is actually a value and we want to combine that data with other",
    "start": "133760",
    "end": "139250"
  },
  {
    "text": "data sources so we can get even more value out of that data and we also want to do new kinds of analyses on this data",
    "start": "139250",
    "end": "144590"
  },
  {
    "text": "so machine learning big data and analytics real-time analytics and the issue of data warehousing right is that",
    "start": "144590",
    "end": "151760"
  },
  {
    "text": "it's not really optimized for these kinds of use cases that's not to say data warehousing is being replaced by data Lake but that's to say that there",
    "start": "151760",
    "end": "158900"
  },
  {
    "text": "are certain things that data lakes can do better than data warehouses and that's why we're all here right this is the packed room everyone's interested so",
    "start": "158900",
    "end": "166340"
  },
  {
    "start": "166000",
    "end": "166000"
  },
  {
    "text": "let's talk about exactly how data lakes extend the traditional approach right you can ingest in relational as well as",
    "start": "166340",
    "end": "172130"
  },
  {
    "text": "non-relational data in fact you can just into any data you want at all right you don't need to define the schema upon",
    "start": "172130",
    "end": "178340"
  },
  {
    "text": "ingest you can define it upon read you can look at it in the terabytes as well as the exabytes scale you can apply any",
    "start": "178340",
    "end": "185660"
  },
  {
    "text": "sorts of tools you want against the data itself right that's separation of storage at compute is really great here",
    "start": "185660",
    "end": "191090"
  },
  {
    "text": "no longer am I bound to that Nick Taylor particular technology stack that is associated for data warehouse I can now",
    "start": "191090",
    "end": "197930"
  },
  {
    "text": "leverage Hadoop I can now leverage sass I can now leverage whatever tool I want to analyze my data and the best part of",
    "start": "197930",
    "end": "205430"
  },
  {
    "text": "all I think for a lot of people is that it's really low cost storage right you can just start there and data in there today and start playing with it the the",
    "start": "205430",
    "end": "211700"
  },
  {
    "text": "time and cost for entry is very very low and I think this you can makes it a really appealing for a lot of customers",
    "start": "211700",
    "end": "217310"
  },
  {
    "text": "and just to hit on this point again s2k by itself is not data Lake okay it's not",
    "start": "217310",
    "end": "225140"
  },
  {
    "start": "218000",
    "end": "218000"
  },
  {
    "text": "enough to store data needs to be discoverable data it needs to be catalogued in some sort of way and needs",
    "start": "225140",
    "end": "232430"
  },
  {
    "text": "to be secured and it needs to have some sort of control over it such that you can actually manage your data by",
    "start": "232430",
    "end": "238310"
  },
  {
    "text": "throwing it into s3 by itself you aren't really enabling that you're just really the the analog is kind of you have a",
    "start": "238310",
    "end": "244100"
  },
  {
    "text": "child who you're saying okay I want you to sort all your stuff in in a logical way and well some people some kids might",
    "start": "244100",
    "end": "250459"
  },
  {
    "text": "want to just throw it all into the closet hey it's a dead Lake no this time you want some sort of way to organize",
    "start": "250459",
    "end": "256790"
  },
  {
    "text": "your data so how do we map this to AWS and so some of you may probably most if",
    "start": "256790",
    "end": "264320"
  },
  {
    "text": "you have seen this kind of diet where there there are some key sorts of pillars for a data Lake and by itself",
    "start": "264320",
    "end": "271710"
  },
  {
    "text": "it's kind of okay cool things can can point and ingest and be pointed at various things in a day lake but I want",
    "start": "271710",
    "end": "278819"
  },
  {
    "text": "to go deeper into it I want to show exactly how data flows in a data Lake so typically you what you have data and you",
    "start": "278819",
    "end": "285270"
  },
  {
    "start": "281000",
    "end": "281000"
  },
  {
    "text": "want answers in the data right that's really what your business is all about so there are four phases first is the",
    "start": "285270",
    "end": "290729"
  },
  {
    "text": "collect phase then there's a storage phase where you're actually storing the data he also have a process an analyze",
    "start": "290729",
    "end": "297120"
  },
  {
    "text": "phase where you're using tools like Hadoop in SAS to process against the data and then you eventually have that",
    "start": "297120",
    "end": "302430"
  },
  {
    "text": "the results of this processed end results into some form map and someone some can consume such as a a",
    "start": "302430",
    "end": "309030"
  },
  {
    "text": "visualization engine something like that and one thing to note here is that it's not a single continuous flow right there",
    "start": "309030",
    "end": "314310"
  },
  {
    "text": "there's an iterative process in the middle where you store you process",
    "start": "314310",
    "end": "319349"
  },
  {
    "text": "analyze the results of that process go back into the storage and you keep iterating over and over against your",
    "start": "319349",
    "end": "324840"
  },
  {
    "text": "data so you can gain more insights as more data comes in so a typical example would be a same machine learning right",
    "start": "324840",
    "end": "330000"
  },
  {
    "text": "you initially have an initial training set and as you have more data come in you can refine and further refine that",
    "start": "330000",
    "end": "335130"
  },
  {
    "text": "data as more comes in and there are a couple other key considerations here in this pipelines right there there's a",
    "start": "335130",
    "end": "340979"
  },
  {
    "text": "time to answer which is the latency the cost associated with the pipeline as well as the throughput right how fast",
    "start": "340979",
    "end": "346979"
  },
  {
    "text": "can this data actually go through so here is a reference architecture for how",
    "start": "346979",
    "end": "353190"
  },
  {
    "text": "how you can map actual a tip of services and third-party products to that flow I showed before and one thing you'll",
    "start": "353190",
    "end": "359940"
  },
  {
    "text": "notice here is that things go from left to right right you have things that are stored within you that are collected say",
    "start": "359940",
    "end": "365880"
  },
  {
    "text": "using your IR T sensors down there on the left and they can bubble into kafka",
    "start": "365880",
    "end": "371460"
  },
  {
    "text": "for storage and then this can go to s3 and then you can process analyze them and then you can consume the eventually and for a lot of people actually they",
    "start": "371460",
    "end": "379050"
  },
  {
    "text": "think that this is the authoritative answer to what a data leak is and it's not right this is a great diagram to",
    "start": "379050",
    "end": "385380"
  },
  {
    "text": "show how you can map technology to fulfill certain architectural requirements but it's not answering the",
    "start": "385380",
    "end": "390779"
  },
  {
    "text": "big questions right big questions for a lot of people in the room probably are how do I secure my data how do I search",
    "start": "390779",
    "end": "396839"
  },
  {
    "text": "on that data how do I ensure that I am fulfilling all my compliance requirements on my data while also you",
    "start": "396839",
    "end": "402930"
  },
  {
    "text": "know letting all my users and analysts and such be able to access my data so we need to take a step back we need to",
    "start": "402930",
    "end": "409260"
  },
  {
    "text": "establish it ADA strategy first right and one thing to consider is that in",
    "start": "409260",
    "end": "414690"
  },
  {
    "text": "fact eighty percent of what we consider analytics is not actually analytics right the the actual effort that has",
    "start": "414690",
    "end": "421290"
  },
  {
    "text": "taken say doing no clicks with Jupiter or doing some sort of insight gathering",
    "start": "421290",
    "end": "427140"
  },
  {
    "text": "from your data it's actually quite minimal a lot of the work you're doing is actually pre-processing storing",
    "start": "427140",
    "end": "432720"
  },
  {
    "text": "ensuring security controls all that stuff and this is frankly undifferentiated heavy lifting this is",
    "start": "432720",
    "end": "439170"
  },
  {
    "text": "stuff that everyone can do and has to do and it's not unique to your business so",
    "start": "439170",
    "end": "444900"
  },
  {
    "text": "we want to accelerate the 80% so that we can focus that 20% and so the key",
    "start": "444900",
    "end": "450090"
  },
  {
    "start": "449000",
    "end": "449000"
  },
  {
    "text": "components of a successful data strategy are as follows right you have one to ten there and we'll go through each of them",
    "start": "450090",
    "end": "455640"
  },
  {
    "text": "so don't don't feel the need to take a picture right now but all these map to those key tenets of a data Lake right",
    "start": "455640",
    "end": "462000"
  },
  {
    "text": "you need to have entitlements for your objects in s3 need the ability to catalog and search on those objects you",
    "start": "462000",
    "end": "467640"
  },
  {
    "text": "need to have some way of storing and processing streaming data all these map to those poor tenants so let's go ahead",
    "start": "467640",
    "end": "474120"
  },
  {
    "text": "and go through each one of those right and one thing that I really want to to make sure that people get from this",
    "start": "474120",
    "end": "480060"
  },
  {
    "text": "presentation is that I'm not just know through oh these are ten key components of a data strategy enjoy figure it out",
    "start": "480060",
    "end": "485070"
  },
  {
    "text": "what I'm going to do is map each one of those tenants to services as well as third-party tools that you can use so",
    "start": "485070",
    "end": "492090"
  },
  {
    "start": "486000",
    "end": "486000"
  },
  {
    "text": "first automated and reliable data ingestion right this is the initial data gathering this is the part where you",
    "start": "492090",
    "end": "497580"
  },
  {
    "text": "want to pull in data into your data Lake and so from the 80 resources we have Kinesis right our streaming service we",
    "start": "497580",
    "end": "503730"
  },
  {
    "text": "have direct connect the way for you to pipe from your arm from environment to AWS snowball in case you have say our",
    "start": "503730",
    "end": "509850"
  },
  {
    "text": "existing petabyte scale data that you just want to ship over right sneakernet is still a thing today as well as our",
    "start": "509850",
    "end": "516719"
  },
  {
    "text": "database migration service where let's say you have a bunch of databases on your on-prem environment today and you want to migrate them into AWS one that",
    "start": "516720",
    "end": "524400"
  },
  {
    "text": "interesting thing about the database migration service is that you don't have to point at a database for the target you can actually point in s3 for the",
    "start": "524400",
    "end": "531630"
  },
  {
    "text": "database and just my gracious service so in fact that's a very efficient way to actually pull data",
    "start": "531630",
    "end": "536730"
  },
  {
    "text": "from your on-prem databases today into s3 you also note that I have some services or rather some tools here that",
    "start": "536730",
    "end": "545100"
  },
  {
    "text": "are not eight of your services and what I want to drive home here is that AWS is a builders platform right we encourage",
    "start": "545100",
    "end": "552420"
  },
  {
    "text": "any whatever works best for your use case and your cuz your business is what",
    "start": "552420",
    "end": "557610"
  },
  {
    "text": "we want for you to use so if scoop is a better answer use scoop if knife eye is the better",
    "start": "557610",
    "end": "564089"
  },
  {
    "text": "answer use a knife I we enable that for you one thing to note is that this is an in particular endorsement it's",
    "start": "564089",
    "end": "570540"
  },
  {
    "text": "impossible to fit every single tool that exists out there into these things but these are just some ideas that you can",
    "start": "570540",
    "end": "576750"
  },
  {
    "text": "look at to get a sense of what had tools are out there to help you with these the",
    "start": "576750",
    "end": "582300"
  },
  {
    "text": "next tenant is the preservation of orginal stores data right being able to store data and have it be the golden",
    "start": "582300",
    "end": "589850"
  },
  {
    "text": "copy of data right the ground truth copy of data the raw data upon which it is",
    "start": "589850",
    "end": "595290"
  },
  {
    "text": "never erased and anything that is created from it needs to be written to a new data set right and this also entails",
    "start": "595290",
    "end": "602370"
  },
  {
    "text": "being able to manage the life cycle of that data right let's say you have a requirement to write only once ever in",
    "start": "602370",
    "end": "610649"
  },
  {
    "text": "read many times and you can never delete it right that's a very common requirement for some agencies out there",
    "start": "610649",
    "end": "615870"
  },
  {
    "text": "and so we can do that with glacier bolt and s3 capturing data change is also a",
    "start": "615870",
    "end": "622589"
  },
  {
    "text": "very important requirement as well right sometimes you need to look at how data is actually changing as you ingest it so",
    "start": "622589",
    "end": "630269"
  },
  {
    "text": "tools such as knife line stream sets help with that they can get to capture the change of data as an actual",
    "start": "630269",
    "end": "636889"
  },
  {
    "text": "construct and you can use that to help capture that change metadata capture is",
    "start": "636889",
    "end": "642089"
  },
  {
    "text": "very important right being able to capture the data about the data being able to say this data is owned by this",
    "start": "642089",
    "end": "648089"
  },
  {
    "text": "cost center and was created by this analyst and was used it was derived from this data set things like that and being",
    "start": "648089",
    "end": "655529"
  },
  {
    "text": "able to count that data as well as search on that data is one of the key constructs that you need to have for",
    "start": "655529",
    "end": "660540"
  },
  {
    "text": "today Tulelake managing governance security and privacy this is big one",
    "start": "660540",
    "end": "665790"
  },
  {
    "text": "right people want to know how do I secure my data how do I manage who has access to",
    "start": "665790",
    "end": "671010"
  },
  {
    "text": "my data how do I say have calm little security and my data cell level secured",
    "start": "671010",
    "end": "676230"
  },
  {
    "text": "in my data how do I say federate against my users in Active Directory and say",
    "start": "676230",
    "end": "681269"
  },
  {
    "text": "only this group has access to these columns and my data these tools can help with that self-service discovery search",
    "start": "681269",
    "end": "689519"
  },
  {
    "text": "and access right being able to then search and then access that data right searching by itself isn't that helpful",
    "start": "689519",
    "end": "696120"
  },
  {
    "text": "hey there's data out there it's somewhere I can access it this is where you can actually get that sort of",
    "start": "696120",
    "end": "702329"
  },
  {
    "text": "mechanism through which you can say I found this data now I want to put that data into say SAS for analysis imagine",
    "start": "702329",
    "end": "710040"
  },
  {
    "text": "data quality this is a big one for a lot of customers right how do I know my data is actually clean how do I know that this is actually a good data so there",
    "start": "710040",
    "end": "716940"
  },
  {
    "text": "are a huge suite of products out there that can help with that as well as our own services as well",
    "start": "716940",
    "end": "723769"
  },
  {
    "text": "preparing for analytics right I was talking about how the 80% of time is spent on analytics this these tools here",
    "start": "723769",
    "end": "731190"
  },
  {
    "text": "help with speeding up that process help with figuring out ok I need to process my data I need to make it into a format",
    "start": "731190",
    "end": "738089"
  },
  {
    "text": "that is consumable I need to harmonize my data how can I do that these tools can help an orchestration and job",
    "start": "738089",
    "end": "744240"
  },
  {
    "text": "scheduling right when you're starting off with this kind of stuff you you typically are playing around with a single cluster on EMR or you're playing",
    "start": "744240",
    "end": "750839"
  },
  {
    "text": "around with a couple of nodes here and there with some other tool but as you scale up this actually becomes kind of",
    "start": "750839",
    "end": "757740"
  },
  {
    "text": "management nightmare right because you need to figure out a way to automate this in a manageable way you need to",
    "start": "757740",
    "end": "762779"
  },
  {
    "text": "figure out a way to schedule jobs you need a way to say ok on the start of every business day I need to process",
    "start": "762779",
    "end": "769199"
  },
  {
    "text": "these data sets and they need to go here oh by the way I need to tag them with this metadata and ensure that all the",
    "start": "769199",
    "end": "775649"
  },
  {
    "text": "security controls are in place these tools can help so I'm now I'm going fairly quickly there's a lot and there's",
    "start": "775649",
    "end": "782279"
  },
  {
    "text": "also these two gentlemen here but I do want to go back to this part right here and note that every single thing I",
    "start": "782279",
    "end": "787980"
  },
  {
    "text": "talked about there does map to these services right and if you find that one of these services doesn't exactly fit",
    "start": "787980",
    "end": "794370"
  },
  {
    "text": "your business requirements today then there's a huge suite of partner products as well as third-party opens will",
    "start": "794370",
    "end": "799810"
  },
  {
    "text": "and source tool kits as well that can help you so one last thing before I go",
    "start": "799810",
    "end": "805270"
  },
  {
    "text": "is security compliance right everything comes back to security security should be your number one priority and I want",
    "start": "805270",
    "end": "812200"
  },
  {
    "text": "to cover that right now with regards to data leaks so I am in the identity access management is our service",
    "start": "812200",
    "end": "819520"
  },
  {
    "start": "814000",
    "end": "814000"
  },
  {
    "text": "database service that we provide to help users manage their a diverse",
    "start": "819520",
    "end": "825130"
  },
  {
    "text": "infrastructure so you can manage users groups and roles you can have Federation with open ID and Active Directory you",
    "start": "825130",
    "end": "831700"
  },
  {
    "text": "can have temporary credentials I implore anyone that's seriously looking at security the de Lake to understand this",
    "start": "831700",
    "end": "836710"
  },
  {
    "text": "very well because lots of great things can happen if you understand it quite well so for example if I have a",
    "start": "836710",
    "end": "842440"
  },
  {
    "start": "841000",
    "end": "841000"
  },
  {
    "text": "requirement for a HIPAA data right and I want to say I only want a certain subset",
    "start": "842440",
    "end": "848740"
  },
  {
    "text": "of users to access my data you can apply a simple policy just like this against",
    "start": "848740",
    "end": "854440"
  },
  {
    "text": "that prefix that you see down there under that resource condition to say if this data is tagged as HIPAA this person",
    "start": "854440",
    "end": "862240"
  },
  {
    "text": "can have access to it in terms of compliance we have a huge suite of",
    "start": "862240",
    "end": "868150"
  },
  {
    "start": "865000",
    "end": "865000"
  },
  {
    "text": "compliance that you can see here and if",
    "start": "868150",
    "end": "873730"
  },
  {
    "text": "you have a particular interest and one of these in the interest of time I do encourage you to look online on our website too to gain more information",
    "start": "873730",
    "end": "880210"
  },
  {
    "text": "around it a particular interest to let people here is probably FedRAMP so there's a huge welcome knowledge on our",
    "start": "880210",
    "end": "888100"
  },
  {
    "text": "public facing website around how we address FedRAMP and what which security I mean which services are under scope",
    "start": "888100",
    "end": "893260"
  },
  {
    "text": "for FedRAMP so what are some best practices for security so let's talk",
    "start": "893260",
    "end": "899500"
  },
  {
    "start": "898000",
    "end": "898000"
  },
  {
    "text": "about the classical question of multi-tenant versus single tenant right",
    "start": "899500",
    "end": "904680"
  },
  {
    "text": "historically most if not all organizations had a multi-tenant environment set up say Hadoop",
    "start": "904680",
    "end": "910510"
  },
  {
    "text": "environment set up where you had a bunch of teams touching a single Hadoop cluster and the key goals here are to",
    "start": "910510",
    "end": "917290"
  },
  {
    "text": "ensure that each user that touches that cluster only has specific access to their designated objects' in s3 the rest",
    "start": "917290",
    "end": "925360"
  },
  {
    "text": "of clusters they have access to email Fester's they have access to things like that and you also want to be able to",
    "start": "925360",
    "end": "930430"
  },
  {
    "text": "facilitate data sharing between those team in a secure way and so there are couple",
    "start": "930430",
    "end": "936550"
  },
  {
    "text": "ways you can do that so fine-grained data and resource Werner",
    "start": "936550",
    "end": "941559"
  },
  {
    "text": "trip is really what it all boils down to where you have teams that's your s3 buckets and clusters but",
    "start": "941559",
    "end": "946899"
  },
  {
    "text": "the issue with this approach is that it's actually quite complex to maintain any sort of access control in a",
    "start": "946899",
    "end": "954279"
  },
  {
    "text": "manageable way and the unfortunate reality is that as people come from on-prem to the cloud they're taking",
    "start": "954279",
    "end": "960730"
  },
  {
    "text": "along that that old way of thinking where everyone must have a multi-tenant sort of architecture and the issue here",
    "start": "960730",
    "end": "968199"
  },
  {
    "text": "is that the management it becomes a nightmare you end up with teams that you have specific access to resources but",
    "start": "968199",
    "end": "975369"
  },
  {
    "text": "you need to make sure that team y only has access to that little subset of data in that cluster in team Z can only have",
    "start": "975369",
    "end": "982209"
  },
  {
    "text": "that level of access in that cluster one thing to consider is that blast radius is a concern so for example if team Y",
    "start": "982209",
    "end": "988360"
  },
  {
    "text": "somehow got access to the rest of the environment they they have free rein over anything in that shared environment",
    "start": "988360",
    "end": "995040"
  },
  {
    "text": "the better approach in my opinion is to have a coarse-grained model of ownership",
    "start": "995040",
    "end": "1000059"
  },
  {
    "text": "where teams owned entire resources so in",
    "start": "1000059",
    "end": "1005189"
  },
  {
    "text": "this case a entire s3 buckets and clusters ownership should be segregated by AWS accounts and and by doing so by",
    "start": "1005189",
    "end": "1012149"
  },
  {
    "text": "minimizing the bust radius in this way by compartmentalizing users and groups in this way you you really make access",
    "start": "1012149",
    "end": "1018600"
  },
  {
    "text": "control easier to set and maintain and your security folks will be happier as well there's a lot less writing that you",
    "start": "1018600",
    "end": "1024538"
  },
  {
    "text": "have to do in your documentation to say that this group is controlled against this environment on these were resources",
    "start": "1024539",
    "end": "1030870"
  },
  {
    "text": "by these mechanisms whereas if you can just split it out into say coarse-grained cons models it's quite",
    "start": "1030870",
    "end": "1037438"
  },
  {
    "text": "easier so this is suitable for a time as James I'm not saying this is the best model but if you can do this another",
    "start": "1037439",
    "end": "1047339"
  },
  {
    "text": "thing to look at that's very important is configuring Amazon s3 permissions make sure that your policies on your",
    "start": "1047339",
    "end": "1054659"
  },
  {
    "text": "buckets are configured properly right the leading cause of data exfiltration from s3 is because someone didn't",
    "start": "1054659",
    "end": "1062399"
  },
  {
    "text": "configure their s3 bucket policy correctly and because you know your data is",
    "start": "1062399",
    "end": "1068230"
  },
  {
    "text": "important to you it's I want to stress very importantly that if you want to maintain secure controls on your data",
    "start": "1068230",
    "end": "1074440"
  },
  {
    "text": "you need to make sure that you are having very explicit Buckett policies on",
    "start": "1074440",
    "end": "1079990"
  },
  {
    "text": "your resources so one one last little bit of stuff one thing that a lot of",
    "start": "1079990",
    "end": "1089110"
  },
  {
    "text": "customers start off with is say spinning spinning up Hadoop clusters and ec2 and",
    "start": "1089110",
    "end": "1094500"
  },
  {
    "text": "that's great actually that's actually a very common model to process data in AWS so to do so in a secure manner first you",
    "start": "1094500",
    "end": "1102850"
  },
  {
    "text": "have to encrypt everything you want to encrypt everything don't settle for some someone telling you that oh we've",
    "start": "1102850",
    "end": "1108010"
  },
  {
    "text": "performance degradation on something because we're encrypting it you want to encrypt everything right",
    "start": "1108010",
    "end": "1113890"
  },
  {
    "text": "security is should be your number one concern and you can do this via a variety of mechanisms and actually EMR",
    "start": "1113890",
    "end": "1119500"
  },
  {
    "text": "makes it easier to do this so traditionally being able to secure data and your data leak via Hadoop based same",
    "start": "1119500",
    "end": "1127480"
  },
  {
    "text": "mechanisms is actually quite hard and what EMR does is it makes it easier to",
    "start": "1127480",
    "end": "1132880"
  },
  {
    "text": "secure daily we actually make it with a couple clicks or a very easy CLI command",
    "start": "1132880",
    "end": "1138760"
  },
  {
    "text": "to configure your cluster such that they are secure versus the old approach where you had to individually configure every",
    "start": "1138760",
    "end": "1145270"
  },
  {
    "text": "single application on your Hadoop cluster to make sure it was secure one new feature that came out just last week",
    "start": "1145270",
    "end": "1151420"
  },
  {
    "text": "is EMR 514 supports the ability to audit users who run queries against s3 through",
    "start": "1151420",
    "end": "1157450"
  },
  {
    "text": "EMR FS which is that the mechanism through which you query data on s3 and it pushes the user and group information",
    "start": "1157450",
    "end": "1163690"
  },
  {
    "text": "to audit logs on cloud trails so for any security folks in the room who want to audit actual transactions against s3",
    "start": "1163690",
    "end": "1170950"
  },
  {
    "text": "objects this is what you want to look at and besides auditing EMR provides a bunch of other features such as",
    "start": "1170950",
    "end": "1176050"
  },
  {
    "text": "consistent view manage this your security configurations which I talked about before and fine-grained",
    "start": "1176050",
    "end": "1181630"
  },
  {
    "text": "authorization at s3 on the ec2 roll level and optionally EMR does integrate",
    "start": "1181630",
    "end": "1188500"
  },
  {
    "text": "with frameworks such as Apache Ranger so if you do want to have an additional layer of control take a look at doing",
    "start": "1188500",
    "end": "1195790"
  },
  {
    "text": "that as well so and all together you notice that this is",
    "start": "1195790",
    "end": "1202050"
  },
  {
    "text": "actually that old side that I showed you before me there's a new layer here that shows additional tech stuff that you",
    "start": "1202050",
    "end": "1207960"
  },
  {
    "text": "need to have for the data lake so you need to have that security governance layer right concern number one and then",
    "start": "1207960",
    "end": "1215400"
  },
  {
    "text": "you also have the data catalog layer as well so I'm passing it on to now dr.",
    "start": "1215400",
    "end": "1221340"
  },
  {
    "text": "Naik for to talk about the US Census video hopefully the mics on so I think",
    "start": "1221340",
    "end": "1230790"
  },
  {
    "text": "Tony gave you a great sort of the ground of how you can achieve establishing a",
    "start": "1230790",
    "end": "1237720"
  },
  {
    "text": "data like in AWS environment now from an implementation side I think the question",
    "start": "1237720",
    "end": "1244140"
  },
  {
    "text": "always comes okay how do you can work convert your business folks okay many of you may be talking to your",
    "start": "1244140",
    "end": "1251010"
  },
  {
    "text": "business side to say hey we have all this capability how can I make it",
    "start": "1251010",
    "end": "1256380"
  },
  {
    "text": "possible so I engage with AWS envision engineering team to try to make",
    "start": "1256380",
    "end": "1262620"
  },
  {
    "text": "sure that what is possible and Tony",
    "start": "1262620",
    "end": "1268530"
  },
  {
    "start": "1266000",
    "end": "1266000"
  },
  {
    "text": "talked about the technical side let's talk about the business side so at census we get massive amounts of data",
    "start": "1268530",
    "end": "1277080"
  },
  {
    "text": "many of you may be aware of only about the decennial census which we conduct",
    "start": "1277080",
    "end": "1282450"
  },
  {
    "text": "once every 10 years but besides that we conduct about a hundred and three other surveys we have statistics which come",
    "start": "1282450",
    "end": "1291060"
  },
  {
    "text": "out about population economy businesses everything we feed to various",
    "start": "1291060",
    "end": "1298200"
  },
  {
    "text": "decision-makers in the government side and the private side so what that has",
    "start": "1298200",
    "end": "1304110"
  },
  {
    "text": "led to nothing wrong is just that we have done it individually we have done",
    "start": "1304110",
    "end": "1309990"
  },
  {
    "text": "it as single instances single surveys and this has led to a decentralized data",
    "start": "1309990",
    "end": "1316740"
  },
  {
    "text": "management process which means I have multiple copies because sometimes I need to share the data amongst different",
    "start": "1316740",
    "end": "1323520"
  },
  {
    "text": "groups I have a decentralized stewardship I don't have standardized",
    "start": "1323520",
    "end": "1329160"
  },
  {
    "text": "metadata and then there's difficulty to across the board the other problem is",
    "start": "1329160",
    "end": "1336080"
  },
  {
    "text": "processing an analytic so you have code intensive platforms I don't know how",
    "start": "1336080",
    "end": "1344490"
  },
  {
    "text": "many of you are used actually let's take a poll how many of you are using SAS in",
    "start": "1344490",
    "end": "1350670"
  },
  {
    "text": "your environment how many are using Oracle okay",
    "start": "1350670",
    "end": "1355970"
  },
  {
    "text": "have any of you tried transferring that data into some other platform that's",
    "start": "1355970",
    "end": "1363300"
  },
  {
    "text": "part of the problem so you have to have proper curation you have to make sure",
    "start": "1363300",
    "end": "1368430"
  },
  {
    "text": "that you understand what is SAS doing to the data and so Tony mentioned about",
    "start": "1368430",
    "end": "1373680"
  },
  {
    "text": "lineage he mentioned about the iterations well we need to keep track of that to make sure that we know what is",
    "start": "1373680",
    "end": "1380970"
  },
  {
    "text": "going on and this is also not just of the data but it's also of the code so that we can reproduce the results and",
    "start": "1380970",
    "end": "1388410"
  },
  {
    "text": "make sure that we are giving the right results in case there was a change in",
    "start": "1388410",
    "end": "1393630"
  },
  {
    "text": "the logic the second part a third part is the data this applies data storage",
    "start": "1393630",
    "end": "1401120"
  },
  {
    "text": "I'm sure many of you sure it's all in my data center I'm not saying that it's not",
    "start": "1401120",
    "end": "1407070"
  },
  {
    "text": "in my data center but they are separate buckets there are separate buckets they",
    "start": "1407070",
    "end": "1413550"
  },
  {
    "text": "are separate storage areas we are managing it all in the data center but",
    "start": "1413550",
    "end": "1419760"
  },
  {
    "text": "it is still decentralized we give storage to each entity",
    "start": "1419760",
    "end": "1425390"
  },
  {
    "text": "okay now AWS does the same thing we all multi-tenant each one of us customers we",
    "start": "1425390",
    "end": "1431310"
  },
  {
    "text": "have our own storage but now this is activated because each project needs its",
    "start": "1431310",
    "end": "1436320"
  },
  {
    "text": "own storage most data have file access control so census gets all types of data",
    "start": "1436320",
    "end": "1444300"
  },
  {
    "text": "we collect our own data we get tax data we get HIPAA data we get data from",
    "start": "1444300",
    "end": "1450570"
  },
  {
    "text": "various federal statistics organizations and so there are laws called sips here",
    "start": "1450570",
    "end": "1456060"
  },
  {
    "text": "which basically prevent us from giving access to data and therefore we create",
    "start": "1456060",
    "end": "1461880"
  },
  {
    "text": "these silos of having data sort of managed individually and that is",
    "start": "1461880",
    "end": "1469490"
  },
  {
    "text": "not scalable then I know many of you may be facing this is the no whole notion of",
    "start": "1469490",
    "end": "1476810"
  },
  {
    "text": "security and privacy security been in",
    "start": "1476810",
    "end": "1482210"
  },
  {
    "text": "this business for too long security is always at the end it is an after effect",
    "start": "1482210",
    "end": "1489070"
  },
  {
    "text": "okay privacy guess what the whole federal",
    "start": "1489070",
    "end": "1494690"
  },
  {
    "text": "government is based on this privacy because this privacy is the public trust that they we provide to the public when",
    "start": "1494690",
    "end": "1502400"
  },
  {
    "text": "they give us their data okay we are not impacted we're financially but we are",
    "start": "1502400",
    "end": "1508940"
  },
  {
    "text": "impacted from a public trust and point and that's a huge issue so and then the",
    "start": "1508940",
    "end": "1517430"
  },
  {
    "text": "last part is the technology constraints so the first four were more on the",
    "start": "1517430",
    "end": "1523010"
  },
  {
    "text": "business side but then there is the technology constraint which is actually shared both by the business and the IT",
    "start": "1523010",
    "end": "1530830"
  },
  {
    "text": "organization because the business is saying I want better tools I want newer",
    "start": "1530830",
    "end": "1535910"
  },
  {
    "text": "capabilities everyone has heard about Big Data it's the latest big buzzword but we've",
    "start": "1535910",
    "end": "1543260"
  },
  {
    "text": "been living it for quite some time and people are wanting more capabilities more avenues to apply technology so this",
    "start": "1543260",
    "end": "1551960"
  },
  {
    "text": "is a huge problem of getting access to the most current version of software or",
    "start": "1551960",
    "end": "1557960"
  },
  {
    "text": "tools for the business side so I present",
    "start": "1557960",
    "end": "1563570"
  },
  {
    "text": "it to AWS and one of the key points was the fact that and AWS has done a good",
    "start": "1563570",
    "end": "1570710"
  },
  {
    "text": "job I don't know how many of you are aware I think the first instance of a data Lake was built by FINRA but it was",
    "start": "1570710",
    "end": "1577490"
  },
  {
    "text": "built with all AWS capability so the challenge I presented to AWS was I don't",
    "start": "1577490",
    "end": "1584690"
  },
  {
    "text": "want to use only AWS capabilities we want to use other open-source big data",
    "start": "1584690",
    "end": "1590030"
  },
  {
    "text": "platforms we want to use existing cots products and why is that because that",
    "start": "1590030",
    "end": "1596180"
  },
  {
    "text": "gives us an easy mode of transition from where we are today to the new site you",
    "start": "1596180",
    "end": "1601970"
  },
  {
    "text": "cannot have a switch and tell the business or just start using the new tools they're going to say where's my",
    "start": "1601970",
    "end": "1607640"
  },
  {
    "text": "old data where's my old coal I need to get access to that and so we have to provide a transition mechanism for that",
    "start": "1607640",
    "end": "1614920"
  },
  {
    "text": "so we the proof of concept was to build a small interface we ran using some",
    "start": "1614920",
    "end": "1624980"
  },
  {
    "text": "custom products cots products we use some of the controls it was a very simple small scale capability but then",
    "start": "1624980",
    "end": "1632480"
  },
  {
    "text": "we used actual data these are actually data that we had published so there was",
    "start": "1632480",
    "end": "1637610"
  },
  {
    "text": "no sensitive data in this and then we also used the legacy SAS code that was",
    "start": "1637610",
    "end": "1643790"
  },
  {
    "text": "applied to that data and we said let's start using that using the tools that",
    "start": "1643790",
    "end": "1648980"
  },
  {
    "text": "Tony talked about and then here is the",
    "start": "1648980",
    "end": "1654470"
  },
  {
    "start": "1651000",
    "end": "1651000"
  },
  {
    "text": "beauty we did prove and we presented it and the AL is going to show you how we",
    "start": "1654470",
    "end": "1661340"
  },
  {
    "text": "did manage to show that we could centralize the data there would be only one instance of the data so even if",
    "start": "1661340",
    "end": "1668180"
  },
  {
    "text": "there are five uses of the data I don't have to make five copies I will make",
    "start": "1668180",
    "end": "1673460"
  },
  {
    "text": "only one copy and let the five users access that copy we will keep the data",
    "start": "1673460",
    "end": "1678920"
  },
  {
    "text": "lineage so as changes are made to data we will track it and we can share or",
    "start": "1678920",
    "end": "1684650"
  },
  {
    "text": "link the data between projects or research or our directorates provided",
    "start": "1684650",
    "end": "1690740"
  },
  {
    "text": "the security and privacy allows them to get access so the security and privacy",
    "start": "1690740",
    "end": "1695990"
  },
  {
    "text": "is built into it a processing this was",
    "start": "1695990",
    "end": "1701030"
  },
  {
    "text": "basically processing the data using code intensive so those who have you SAS you",
    "start": "1701030",
    "end": "1706940"
  },
  {
    "text": "know how intense it is to use SAS how much power you need to actually process",
    "start": "1706940",
    "end": "1711980"
  },
  {
    "text": "the data we managed to do that one of the other things we managed to do was we",
    "start": "1711980",
    "end": "1717830"
  },
  {
    "text": "separated the data from the code so SAS there are different implementations of",
    "start": "1717830",
    "end": "1724640"
  },
  {
    "text": "SAS one of the implementations of SAS is where SAS actually handles the data all",
    "start": "1724640",
    "end": "1730400"
  },
  {
    "text": "along and we separated it up so that the data could live by itself and be used by",
    "start": "1730400",
    "end": "1736840"
  },
  {
    "text": "other platforms and then the reproducibility of results was the fact that we had the data we had the code we",
    "start": "1736840",
    "end": "1745179"
  },
  {
    "text": "had all the lineage tracked in one place the next thing of course was the fact",
    "start": "1745179",
    "end": "1752139"
  },
  {
    "text": "that if we were using the cloud we were using the s3 or Hadoop in our case we",
    "start": "1752139",
    "end": "1758049"
  },
  {
    "text": "were using a Kim low that gave us the centralized data storage apache accumulo",
    "start": "1758049",
    "end": "1763840"
  },
  {
    "text": "and then it also gave us not only row and column but it gave us all the way to",
    "start": "1763840",
    "end": "1770830"
  },
  {
    "text": "the cell level control so we were mixing data we were mixing our census data with",
    "start": "1770830",
    "end": "1777190"
  },
  {
    "text": "tax data and Tony mentioned all the different Securities guess what we have",
    "start": "1777190",
    "end": "1784299"
  },
  {
    "text": "to apply all of them and so everything was encrypted in motion at rest and we",
    "start": "1784299",
    "end": "1790570"
  },
  {
    "text": "controlled access the best was the fact that we had centralized all the security",
    "start": "1790570",
    "end": "1798130"
  },
  {
    "text": "it was all in one place we didn't have to monitor it ok who's made copies where",
    "start": "1798130",
    "end": "1804039"
  },
  {
    "text": "is the copy of the data we keep track of all the data in one place now the data",
    "start": "1804039",
    "end": "1809740"
  },
  {
    "text": "Lake is not a centralized data warehouse in terms of the traditional sense it is",
    "start": "1809740",
    "end": "1815440"
  },
  {
    "text": "more of a centralized logical data warehouse it can handle all sorts of",
    "start": "1815440",
    "end": "1820450"
  },
  {
    "text": "data and that is what we were trying to achieve here and of course we have",
    "start": "1820450",
    "end": "1826000"
  },
  {
    "text": "auditing and usage monitoring because we are billing so we can audit and you monitor usage not only from an",
    "start": "1826000",
    "end": "1833620"
  },
  {
    "text": "operational standpoint from a performance standpoint but also from a security and access control standpoint",
    "start": "1833620",
    "end": "1839889"
  },
  {
    "text": "and the technology capabilities the beauty of the proof of concept was this",
    "start": "1839889",
    "end": "1846549"
  },
  {
    "text": "notion of providing on top of the FedRAMP certified environment that AWS",
    "start": "1846549",
    "end": "1852730"
  },
  {
    "text": "provides we could add a census certified platform as a service environment that",
    "start": "1852730",
    "end": "1859600"
  },
  {
    "text": "the users can use there's tighter integration between storage and compute",
    "start": "1859600",
    "end": "1864700"
  },
  {
    "text": "because my compute additional platform is very close to the data handling platform and I have the",
    "start": "1864700",
    "end": "1871710"
  },
  {
    "text": "ability to handle very large data sets and use some of the newer tools the",
    "start": "1871710",
    "end": "1877140"
  },
  {
    "text": "business gets to use newer tools like our Python or SPARC going forward and",
    "start": "1877140",
    "end": "1882330"
  },
  {
    "text": "then of course there's a cost charge back model so I don't know how many of",
    "start": "1882330",
    "end": "1888570"
  },
  {
    "start": "1886000",
    "end": "1886000"
  },
  {
    "text": "you are aware of the six ARS these are the different ways you can transition to the cloud the proof-of-concept enabled",
    "start": "1888570",
    "end": "1895680"
  },
  {
    "text": "us to basically monitor and validate three of the AHS of existing data so we",
    "start": "1895680",
    "end": "1904290"
  },
  {
    "text": "did not have to do major really tech thing we just did a reap lat form and a refactor and that was a huge win for the",
    "start": "1904290",
    "end": "1912180"
  },
  {
    "text": "business the mission we have is we are basically saying we want to build on the",
    "start": "1912180",
    "end": "1918630"
  },
  {
    "text": "cloud computing with big data and machine learning but we can take any data we can take many tools and it's",
    "start": "1918630",
    "end": "1925800"
  },
  {
    "text": "easier to operate with the notion that we can give better business results which is better analytics quality",
    "start": "1925800",
    "end": "1932520"
  },
  {
    "text": "product and faster time to publish I'll hand it over to Al will walk you through",
    "start": "1932520",
    "end": "1938700"
  },
  {
    "text": "for what we did in this proof-of-concept dr. Mike thank you so much for being",
    "start": "1938700",
    "end": "1945150"
  },
  {
    "text": "here today wonderful presentation today I get a",
    "start": "1945150",
    "end": "1950190"
  },
  {
    "text": "chance to join the video Lake pattern discussed by Tony with the actual",
    "start": "1950190",
    "end": "1955890"
  },
  {
    "text": "business requirements discussed by dr. Knight and we'll see how it all comes",
    "start": "1955890",
    "end": "1960930"
  },
  {
    "text": "together in a data Lake implementation that we developed as a proof of concept for the US Census Bureau what we find is",
    "start": "1960930",
    "end": "1971190"
  },
  {
    "text": "that the data Lake pattern is common it caused a lot of data lake implementations but what's interesting",
    "start": "1971190",
    "end": "1977490"
  },
  {
    "text": "what we really find interesting is that very lake implementations are all unique",
    "start": "1977490",
    "end": "1982590"
  },
  {
    "text": "in their own ways why are they are all unique this is because of specific requirements by the customer right all",
    "start": "1982590",
    "end": "1991470"
  },
  {
    "text": "customers have different requirements and that drives architecture towards certain architectural decisions",
    "start": "1991470",
    "end": "1998220"
  },
  {
    "text": "it could be data access it could be security it could be very lineage it",
    "start": "1998220",
    "end": "2005309"
  },
  {
    "text": "could be integrations with other systems all those specific requirements from the",
    "start": "2005309",
    "end": "2012330"
  },
  {
    "text": "customers drive their data lake implementations towards a certain uniqueness in them so let's take a look",
    "start": "2012330",
    "end": "2020130"
  },
  {
    "text": "at the specific requirements that we received for the daylight proof of concept for the US Census the first one",
    "start": "2020130",
    "end": "2027990"
  },
  {
    "start": "2026000",
    "end": "2026000"
  },
  {
    "text": "is column level access control with cell level capability I'm sure you're all",
    "start": "2027990",
    "end": "2033210"
  },
  {
    "text": "familiar with a cell level over there column level access control this is when you'll wipe down the whole column for",
    "start": "2033210",
    "end": "2040350"
  },
  {
    "text": "access to a particular user or maybe a group of users cell level capability",
    "start": "2040350",
    "end": "2046710"
  },
  {
    "text": "would mean that you would lock down a particular cell within that column this",
    "start": "2046710",
    "end": "2051929"
  },
  {
    "text": "means that if this is the social security number the president for",
    "start": "2051929",
    "end": "2057060"
  },
  {
    "text": "example you would want to lock down that access because it's very sensitive it",
    "start": "2057060",
    "end": "2062908"
  },
  {
    "text": "could be a social security number or in general it could be any other sensitive information but it needs to be locked",
    "start": "2062909",
    "end": "2069570"
  },
  {
    "text": "down on the cell level not just the column well so for the specific",
    "start": "2069570",
    "end": "2074908"
  },
  {
    "text": "requirements we'll looked around at solutions from the partner network as well as open source and we found that",
    "start": "2074909",
    "end": "2082500"
  },
  {
    "text": "apache accumulo gets our needs for this particular requirement and this is how our architecture was influenced by this",
    "start": "2082500",
    "end": "2089250"
  },
  {
    "text": "requirement the next requirement is essential storage now what could we use",
    "start": "2089250",
    "end": "2095908"
  },
  {
    "text": "for central storage in AWS anyway well",
    "start": "2095909",
    "end": "2101310"
  },
  {
    "text": "Amazon s3 right Amazon s3 is at the heart of each day awake and it's a good",
    "start": "2101310",
    "end": "2107760"
  },
  {
    "text": "choice for it the next one is video image at macro level it's really",
    "start": "2107760",
    "end": "2113070"
  },
  {
    "text": "important for businesses to understand who created the data set who uploaded",
    "start": "2113070",
    "end": "2118230"
  },
  {
    "text": "the files who changed the data set and uploaded new files who analyzed the data set and where the results are stored so",
    "start": "2118230",
    "end": "2127109"
  },
  {
    "text": "this is the kind of metadata that we need to store somewhere and a really good choice is dynamodb held up based user security",
    "start": "2127109",
    "end": "2136650"
  },
  {
    "text": "and permissions we have AWS directory service fullest so we can map to that we",
    "start": "2136650",
    "end": "2142890"
  },
  {
    "text": "also needed to provide a choice of the hadoop infrastructure for this particularly data way so we provided the",
    "start": "2142890",
    "end": "2150510"
  },
  {
    "text": "choice between AWS EMR which is elastic MapReduce and Hortonworks",
    "start": "2150510",
    "end": "2157039"
  },
  {
    "text": "we needed to provide the ability to run legacy SAS scripts on SAS 9.4 and so we",
    "start": "2157039",
    "end": "2164869"
  },
  {
    "text": "had to have SAS in the infrastructure as well the next requirement is on-demand",
    "start": "2164869",
    "end": "2172160"
  },
  {
    "text": "infrastructure of analytic jobs database platform is perfect for it right you can",
    "start": "2172160",
    "end": "2178380"
  },
  {
    "text": "scale up or down as you need and you can pay for only what you use and the last",
    "start": "2178380",
    "end": "2184650"
  },
  {
    "text": "specific requirement we had was cost tracking for each analytics job it's really important for businesses to do",
    "start": "2184650",
    "end": "2191130"
  },
  {
    "text": "cost tracking of how much it costs to run this particular analytics routine so",
    "start": "2191130",
    "end": "2197609"
  },
  {
    "text": "we could use AWS pricing list API is for this so this is how our solution was",
    "start": "2197609",
    "end": "2206539"
  },
  {
    "start": "2203000",
    "end": "2203000"
  },
  {
    "text": "influenced by these specific requirements so let's take a look at the solution architecture we have data sets",
    "start": "2206539",
    "end": "2213450"
  },
  {
    "text": "and analytics scripts in Amazon s3 which is typical for date away then we have a",
    "start": "2213450",
    "end": "2219119"
  },
  {
    "text": "service web application the UI is stored in Amazon s3 then we have REST API is",
    "start": "2219119",
    "end": "2226170"
  },
  {
    "text": "fulfilled by Amazon API Deathwing AWS lamda runs the business logic for us and",
    "start": "2226170",
    "end": "2233520"
  },
  {
    "text": "as we talked about it the data range and other metadata is stored in Amazon DynamoDB of course we also use",
    "start": "2233520",
    "end": "2240569"
  },
  {
    "text": "confirmation for automation of deployments AWS I am for restricting",
    "start": "2240569",
    "end": "2246750"
  },
  {
    "text": "access to AWS infrastructure as well as Kohath watch logs for logging and AWS",
    "start": "2246750",
    "end": "2254069"
  },
  {
    "text": "directory service to satisfy the need for LDAP security and permissions now",
    "start": "2254069",
    "end": "2259859"
  },
  {
    "text": "what is missing in this picture we need on-demand infrastruc",
    "start": "2259859",
    "end": "2265290"
  },
  {
    "text": "for the analytics job right so when the user selects the analyze job and runs it",
    "start": "2265290",
    "end": "2271280"
  },
  {
    "text": "here's what happens we spin up resources on demand for this particular analytics",
    "start": "2271280",
    "end": "2277740"
  },
  {
    "text": "job we need to run the Hadoop platform so based on the user choice we either",
    "start": "2277740",
    "end": "2285780"
  },
  {
    "text": "spin up Amazon EMR which is elastic MapReduce cluster or we make it call to",
    "start": "2285780",
    "end": "2292500"
  },
  {
    "text": "Hortonworks Cloudbreak to spin up a horde morris cluster with",
    "start": "2292500",
    "end": "2299550"
  },
  {
    "text": "either cluster we install accumulo on top of the cluster and a cumulative IDEs",
    "start": "2299550",
    "end": "2306120"
  },
  {
    "text": "that security control that we need to lock down the data for the user and then",
    "start": "2306120",
    "end": "2312060"
  },
  {
    "text": "we hydrate accumulo where the data set that the user selected to analyze and",
    "start": "2312060",
    "end": "2317720"
  },
  {
    "text": "then we label the data as we hydrate accumulo with security labels to know",
    "start": "2317720",
    "end": "2324300"
  },
  {
    "text": "what data is accessible for this user then we'll help hi Blair on top of it",
    "start": "2324300",
    "end": "2331320"
  },
  {
    "text": "hive has JDBC compliant endpoints so it's easy to connect other tools with it",
    "start": "2331320",
    "end": "2336810"
  },
  {
    "text": "and this makes it possible to run SAS",
    "start": "2336810",
    "end": "2343230"
  },
  {
    "text": "analytics against us endpoint as well as other analytics scripts at the end of",
    "start": "2343230",
    "end": "2350790"
  },
  {
    "text": "the net analytics routine the results are stored back to s3 and they can",
    "start": "2350790",
    "end": "2356460"
  },
  {
    "text": "become new datasets that are curated and s3 in the data lake and at the end of",
    "start": "2356460",
    "end": "2363840"
  },
  {
    "text": "the full completion of the job all the research is shut down so you don't have",
    "start": "2363840",
    "end": "2369810"
  },
  {
    "text": "to pay for them as you no longer need them so before we do a little demo I'd",
    "start": "2369810",
    "end": "2378810"
  },
  {
    "start": "2375000",
    "end": "2375000"
  },
  {
    "text": "like to talk about the functional flow the functional flow is implemented in an app store like concept where the user",
    "start": "2378810",
    "end": "2386310"
  },
  {
    "text": "gets a chance to select the data sets that the user would like to analyze then",
    "start": "2386310",
    "end": "2391350"
  },
  {
    "text": "select the analytics script to analyze those data sets select the analytics platform as we",
    "start": "2391350",
    "end": "2397470"
  },
  {
    "text": "discussed between I am wearing Hortonworks then hit the submit button the magic happens the",
    "start": "2397470",
    "end": "2404250"
  },
  {
    "text": "analyze job brands executes and stores there's a stress tree and then those could become new data sets so the",
    "start": "2404250",
    "end": "2412890"
  },
  {
    "text": "solution demo was excited yes cool so",
    "start": "2412890",
    "end": "2418290"
  },
  {
    "text": "this is the UI for the daily for the US Census let's go ahead and log in as we",
    "start": "2418290",
    "end": "2424950"
  },
  {
    "text": "login it will call add LDAP the directory service which is one of the",
    "start": "2424950",
    "end": "2430410"
  },
  {
    "text": "requirements yet the security and permissions here's the list of BSS that",
    "start": "2430410",
    "end": "2435750"
  },
  {
    "text": "are curated in this data way and we could add a data set from here upload",
    "start": "2435750",
    "end": "2443790"
  },
  {
    "text": "the files add the metadata to the data set but let's take a look at the existing data set so one of the",
    "start": "2443790",
    "end": "2450119"
  },
  {
    "text": "requirements was basically nish we can see who was created by when I was",
    "start": "2450119",
    "end": "2455130"
  },
  {
    "text": "created the size of the data set as well as the cost to stored per month and this",
    "start": "2455130",
    "end": "2461910"
  },
  {
    "text": "is using the pricing and price list API so it's updated owners and who's allowed",
    "start": "2461910",
    "end": "2469680"
  },
  {
    "text": "access we could start the job from here but let's set the restrictions on those",
    "start": "2469680",
    "end": "2474930"
  },
  {
    "text": "data set first here are the details of",
    "start": "2474930",
    "end": "2480750"
  },
  {
    "text": "the data set we selected we still see the same image the size of the data set the cost of a set per month who was",
    "start": "2480750",
    "end": "2488940"
  },
  {
    "text": "created by and when the owners and who's allowed access but down below we see a",
    "start": "2488940",
    "end": "2494280"
  },
  {
    "text": "list of files and permissions so let's select the first file and we'll see",
    "start": "2494280",
    "end": "2499859"
  },
  {
    "text": "columns inside that file that are available for permission settings you",
    "start": "2499859",
    "end": "2506250"
  },
  {
    "text": "can see the listing of the columns and you can see that all the locks are currently grayed out and unlocked which",
    "start": "2506250",
    "end": "2513869"
  },
  {
    "text": "means everyone is a lot access to this data everyone who runs analytics yes that will have access so let's lock down",
    "start": "2513869",
    "end": "2520829"
  },
  {
    "text": "a reach state column and allow only one user to access it Hutten how did was my",
    "start": "2520829",
    "end": "2527160"
  },
  {
    "text": "core work in my friend so I'm comfortable giving him access all right so we like the down",
    "start": "2527160",
    "end": "2534279"
  },
  {
    "text": "I'm not Hatem only Hutton can access this column at this point nobody else as",
    "start": "2534279",
    "end": "2540900"
  },
  {
    "text": "I will run this next analytics job I will not have access to this particular",
    "start": "2540900",
    "end": "2546279"
  },
  {
    "text": "or each state column so we're ready for the job workflow if you remember is the",
    "start": "2546279",
    "end": "2552069"
  },
  {
    "text": "App Store concept where you select the routine and the dataset and you confirm and start the job so let's select this",
    "start": "2552069",
    "end": "2558819"
  },
  {
    "text": "routine to run then we see the files",
    "start": "2558819",
    "end": "2565720"
  },
  {
    "text": "inside this data set and we select the path that we would like to analyze so",
    "start": "2565720",
    "end": "2571059"
  },
  {
    "text": "let's just pick the first one for the demo this is the file that will be",
    "start": "2571059",
    "end": "2576910"
  },
  {
    "text": "analyzed by the Analytics routine and the last screen in the wizard is we get",
    "start": "2576910",
    "end": "2583299"
  },
  {
    "text": "the platform choice between EMI important works let's do em our first and then we'll do Horton works in the",
    "start": "2583299",
    "end": "2588369"
  },
  {
    "text": "next job we can with the resources writing if we need to at the end of the",
    "start": "2588369",
    "end": "2594220"
  },
  {
    "text": "analytics routine but this time let's clean them up so that we wouldn't have to pay for those resources once the job",
    "start": "2594220",
    "end": "2600730"
  },
  {
    "text": "is done let's call this job restricted because I don't have access to one",
    "start": "2600730",
    "end": "2607210"
  },
  {
    "text": "particular column or you just a here's the routine we selected here's the",
    "start": "2607210",
    "end": "2612759"
  },
  {
    "text": "dataset we selected this is the file that will be analyzed based on our",
    "start": "2612759",
    "end": "2618039"
  },
  {
    "text": "selection and the hive table name is the right from that file name automatically",
    "start": "2618039",
    "end": "2624029"
  },
  {
    "text": "and now we're ready to submit the analyst job this will call the EMR API",
    "start": "2624029",
    "end": "2631029"
  },
  {
    "text": "and it will spin up the six node cluster for EMR this is on demand infrastructure",
    "start": "2631029",
    "end": "2640170"
  },
  {
    "text": "and we can see that the job was managed to set successfully it's starting now and while it's spinning up let's run",
    "start": "2640170",
    "end": "2650259"
  },
  {
    "text": "another job in parallel so that we could run multiple analytics in parallel to",
    "start": "2650259",
    "end": "2655660"
  },
  {
    "text": "use the power of the cloud so let's go ahead and pick the same data set and",
    "start": "2655660",
    "end": "2662289"
  },
  {
    "text": "this time we will remove the restriction on the column that we set it on so this column was",
    "start": "2662289",
    "end": "2669040"
  },
  {
    "text": "locked down we'll remove the user will disable the lock and we'll save the",
    "start": "2669040",
    "end": "2674710"
  },
  {
    "text": "change now all columns are unlocked and now I",
    "start": "2674710",
    "end": "2682300"
  },
  {
    "text": "have access to all the data so we'll",
    "start": "2682300",
    "end": "2689890"
  },
  {
    "text": "select the same routine we'll select the same data set and we'll select the same",
    "start": "2689890",
    "end": "2696520"
  },
  {
    "text": "file within the dataset to analyze the only difference is now I have access to this or each state column right and",
    "start": "2696520",
    "end": "2705030"
  },
  {
    "text": "let's pick Hortonworks for this job let's keep the cleanup",
    "start": "2705030",
    "end": "2712120"
  },
  {
    "text": "resources on so that what wouldn't pay for them when we don't need them the job",
    "start": "2712120",
    "end": "2717160"
  },
  {
    "text": "name will be unrestricted at this point because I will have access to each state",
    "start": "2717160",
    "end": "2722290"
  },
  {
    "text": "data set and then the same routine the same data set let's submit the job the",
    "start": "2722290",
    "end": "2729700"
  },
  {
    "text": "job submits successfully and we see Hortonworks as the platform that they will run on this will also spin up a six node",
    "start": "2729700",
    "end": "2736870"
  },
  {
    "text": "cluster for Hortonworks and since we",
    "start": "2736870",
    "end": "2742960"
  },
  {
    "text": "only have a few minutes left let's keep ahead to the successful completion of the job this is when the",
    "start": "2742960",
    "end": "2750040"
  },
  {
    "text": "job completed successfully and the first one is Hortonworks the second landmark",
    "start": "2750040",
    "end": "2755980"
  },
  {
    "text": "and we see when I was wrong who was run",
    "start": "2755980",
    "end": "2761110"
  },
  {
    "text": "by so that's we need we see the task name and the status as well as how long",
    "start": "2761110",
    "end": "2766720"
  },
  {
    "text": "each task took for the cluster we see the breakdown of the cluster the cost",
    "start": "2766720",
    "end": "2772570"
  },
  {
    "text": "per each instance the cost of the SAS instance and the type of the SAS instance and so all of it was executed",
    "start": "2772570",
    "end": "2781120"
  },
  {
    "text": "and shut down and the results were stored on s3 here's the link to s3 where",
    "start": "2781120",
    "end": "2786370"
  },
  {
    "text": "the results were stored and the costs distorted as well as links to Elias Council and EMR logs at this point all",
    "start": "2786370",
    "end": "2794920"
  },
  {
    "text": "the infrastructure shut down Lewis analytics job as the second one and we were random in parallel so we could use more analyze",
    "start": "2794920",
    "end": "2803640"
  },
  {
    "text": "jobs as needed and speaking of results",
    "start": "2803640",
    "end": "2809870"
  },
  {
    "text": "here's the result set from the the restricted job I don't have a rich Spade",
    "start": "2809870",
    "end": "2817140"
  },
  {
    "text": "output here I selected your each state output on the right and this is the",
    "start": "2817140",
    "end": "2823050"
  },
  {
    "text": "unrestricted output right so on the Left I just don't have it at all",
    "start": "2823050",
    "end": "2828210"
  },
  {
    "text": "because I didn't have access to each state on the right I do have the full reach state output and to wrap up the",
    "start": "2828210",
    "end": "2837240"
  },
  {
    "text": "demo I'd like to show you how easy it was to modify the SAS script to work against Hadoop platform so as we drill",
    "start": "2837240",
    "end": "2846270"
  },
  {
    "text": "down into the routine you see that there is a source code there and we passed in",
    "start": "2846270",
    "end": "2851970"
  },
  {
    "text": "a few variables and literally the only place we had to change was these couple",
    "start": "2851970",
    "end": "2860820"
  },
  {
    "text": "of lines we only had to point that from the local file to the Hadoop endpoint",
    "start": "2860820",
    "end": "2867330"
  },
  {
    "text": "behive endpoint and that was the only change we made we didn't modify the actual script at all the program in",
    "start": "2867330",
    "end": "2873270"
  },
  {
    "text": "logic we only pointed it to Hadoop and that made it work so a quick word about",
    "start": "2873270",
    "end": "2881880"
  },
  {
    "start": "2878000",
    "end": "2878000"
  },
  {
    "text": "the Camila accumulo is a distributed key-value store with cell level based access control it's a single system of",
    "start": "2881880",
    "end": "2889710"
  },
  {
    "text": "record which satisfies our business requirement and it's fine-grained access control which returns only the data that",
    "start": "2889710",
    "end": "2897330"
  },
  {
    "text": "the user has access to and nothing else as you saw in the demo it was installed",
    "start": "2897330",
    "end": "2903120"
  },
  {
    "text": "be a bootstrap on Amazon EMR and it can be installed on Hortonworks cluster be",
    "start": "2903120",
    "end": "2908880"
  },
  {
    "text": "Apache and barri blueprints we also developed a couple pieces of code to help us with the implementation",
    "start": "2908880",
    "end": "2915050"
  },
  {
    "text": "cosmically accumulate loader loads DSS from s3 inter-communal tables and as it",
    "start": "2915050",
    "end": "2921660"
  },
  {
    "text": "does that is at science column labels as security labels custom accumulo",
    "start": "2921660",
    "end": "2927570"
  },
  {
    "text": "authorization handler checks which labels user has access in LDAP and speaking of SAS we use the",
    "start": "2927570",
    "end": "2936310"
  },
  {
    "text": "Hadoop module SAS access for Hadoop module to access hive it needs Hadoop",
    "start": "2936310",
    "end": "2943089"
  },
  {
    "text": "platform specific files such as jar files on-site bus and since we have two platforms we had two sets of those files",
    "start": "2943089",
    "end": "2950800"
  },
  {
    "text": "on SAS and we needed to use environmental variables that you see on",
    "start": "2950800",
    "end": "2956080"
  },
  {
    "text": "the screen to point to either one set of files or the other to tell SAS which platform we're going to use for this",
    "start": "2956080",
    "end": "2962380"
  },
  {
    "text": "execution we installed SAS on ec2 instance created ami which is Amazon machine image and",
    "start": "2962380",
    "end": "2969160"
  },
  {
    "text": "you saw in the demo that there were only minimal changes required to the SAS",
    "start": "2969160",
    "end": "2974349"
  },
  {
    "text": "script just a point to the right data source and so once we have the mi we can",
    "start": "2974349",
    "end": "2980800"
  },
  {
    "start": "2977000",
    "end": "2977000"
  },
  {
    "text": "launch assess instances on demand from our AMI and those SAS instances helped",
    "start": "2980800",
    "end": "2987760"
  },
  {
    "text": "in an embedded shell script that drives all the executions as a controller",
    "start": "2987760",
    "end": "2993220"
  },
  {
    "text": "script for us we use the run command to trigger that shell script it downloads",
    "start": "2993220",
    "end": "2998950"
  },
  {
    "text": "the SAS script that the user selected to run against the dataset it executes the SAS script on SAS SAS accesses the high",
    "start": "2998950",
    "end": "3007290"
  },
  {
    "text": "end point the external tables increased from accumulo it stores the results",
    "start": "3007290",
    "end": "3012960"
  },
  {
    "text": "locally and then our controller shell script picks those results and stores them back into s3 which is our central",
    "start": "3012960",
    "end": "3020099"
  },
  {
    "text": "storage and then everything shuts down",
    "start": "3020099",
    "end": "3024980"
  },
  {
    "start": "3025000",
    "end": "3025000"
  },
  {
    "text": "so in summary would like to say that cloud and big data are a perfect match they provide agility and new",
    "start": "3025250",
    "end": "3032130"
  },
  {
    "text": "opportunities for your business as you saw in this presentation AWS provides comprehensive analytics security and",
    "start": "3032130",
    "end": "3038820"
  },
  {
    "text": "compliance capabilities data Lake requirements in use cases VA and then",
    "start": "3038820",
    "end": "3043830"
  },
  {
    "text": "please take a look at AWS partnering network and open source tools like we did for your specific needs",
    "start": "3043830",
    "end": "3050119"
  },
  {
    "text": "thank you [Applause]",
    "start": "3050119",
    "end": "3057420"
  }
]