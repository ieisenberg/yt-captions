[
  {
    "text": "so my name is Julian I am a tech evangelist and I focus on machine learning in this session we're going to",
    "start": "30",
    "end": "7049"
  },
  {
    "text": "dive deeper into our machine learning stack and we'll focus on Amazon sage",
    "start": "7049",
    "end": "14519"
  },
  {
    "text": "maker and a couple of new services around sage maker that help customers make the most of their machine learning",
    "start": "14519",
    "end": "22109"
  },
  {
    "text": "workloads and we're going to cover all kinds of different things so you've",
    "start": "22109",
    "end": "27300"
  },
  {
    "text": "heard this one before our mission my mission is to make sure all of you can do machine learning on",
    "start": "27300",
    "end": "34680"
  },
  {
    "text": "AWS and even if you're not a machine learning expert we think we can help you get started so that's what sage maker",
    "start": "34680",
    "end": "42180"
  },
  {
    "text": "does what this session is about is going to the next level which is now that we",
    "start": "42180",
    "end": "48000"
  },
  {
    "text": "all do and enjoy machine learning can we make it as fast as efficient and as an",
    "start": "48000",
    "end": "54289"
  },
  {
    "text": "expensive and cost effective as possible so we'll go over different building",
    "start": "54289",
    "end": "62820"
  },
  {
    "text": "blocks in the machine learning stack let's start with infrastructure and frameworks the low-level layer that you",
    "start": "62820",
    "end": "70080"
  },
  {
    "text": "saw on in bor√•s presentation so ad reinvent in December we introduced a new",
    "start": "70080",
    "end": "76710"
  },
  {
    "text": "type of GPU instance it's called the p3 DN so it's still based on the nvidia v",
    "start": "76710",
    "end": "83070"
  },
  {
    "text": "100 GPU which is still the fastest GPU available but we just took the p3 family",
    "start": "83070",
    "end": "88860"
  },
  {
    "text": "and we made it more powerful so what we did is add more of everything so on the",
    "start": "88860",
    "end": "97079"
  },
  {
    "text": "CPU side we added more cores more virtual CPUs because as you scale your",
    "start": "97079",
    "end": "103380"
  },
  {
    "text": "launch training jobs it's not just about having GPU power you also need to have CPU power to run your tensorflow app or",
    "start": "103380",
    "end": "111420"
  },
  {
    "text": "your PI torch app and this can be big models and they need to be to pump tons of data to the GPU so they need to run",
    "start": "111420",
    "end": "118649"
  },
  {
    "text": "fast ok it's not just about the GPUs the CPU can become a bottleneck so we added",
    "start": "118649",
    "end": "124170"
  },
  {
    "text": "a bunch of of virtual CPUs to those instances we also added more GPU Ram",
    "start": "124170",
    "end": "129869"
  },
  {
    "text": "because again as your jobs get bigger you need more GPU Ram to train faster so",
    "start": "129869",
    "end": "136990"
  },
  {
    "text": "we just double the amount of memory available to GPUs and more importantly I",
    "start": "136990",
    "end": "142720"
  },
  {
    "text": "think we made networking even faster because when you try when you train on",
    "start": "142720",
    "end": "149830"
  },
  {
    "text": "large drubbs first of all you need to load your data set to the training instances so",
    "start": "149830",
    "end": "155860"
  },
  {
    "text": "obviously the faster you bring the data to the instances the better right and the second thing is as you do",
    "start": "155860",
    "end": "163150"
  },
  {
    "text": "distributed training the different instances involved in the training job need to exchange tons of data right they",
    "start": "163150",
    "end": "169960"
  },
  {
    "text": "need to at each epoch each round of training they need to exchange tunnel form information so again the faster you",
    "start": "169960",
    "end": "178000"
  },
  {
    "text": "can do that the better okay so p.3d ends for the large",
    "start": "178000",
    "end": "183120"
  },
  {
    "text": "distributed training jobs out there we also improve the c-5 family so the c-5",
    "start": "183120",
    "end": "189520"
  },
  {
    "text": "family is the compute optimized family with the latest Intel architecture",
    "start": "189520",
    "end": "195760"
  },
  {
    "text": "called skylake which has specific instructions that speed up machine learning jobs and what we did here is we",
    "start": "195760",
    "end": "204220"
  },
  {
    "text": "brought 100 gigabit networking as well for the same reasons because some customers train large drops on c5 they",
    "start": "204220",
    "end": "212920"
  },
  {
    "text": "find that pretty effective but even for prediction if you have fast networking",
    "start": "212920",
    "end": "218400"
  },
  {
    "text": "the faster the network runs the more throughput you can get from your",
    "start": "218400",
    "end": "223480"
  },
  {
    "text": "instance so both from a training and prediction perspective more networking",
    "start": "223480",
    "end": "229210"
  },
  {
    "text": "speed is better ok and just so you know this is the fastest single core clock",
    "start": "229210",
    "end": "237010"
  },
  {
    "text": "speed you can get on ec2 instances you can get to up to 3.5 5 gigahertz single",
    "start": "237010",
    "end": "244810"
  },
  {
    "text": "core speed so if you need the fastest lowest latency single core prediction",
    "start": "244810",
    "end": "250270"
  },
  {
    "text": "this is the instance family to use but it's not just about hardware it's not",
    "start": "250270",
    "end": "256510"
  },
  {
    "text": "just about instances we're also working very hard on making the libraries as",
    "start": "256510",
    "end": "261669"
  },
  {
    "text": "efficient as possible so let's talk about tensorflow for a second so",
    "start": "261669",
    "end": "267130"
  },
  {
    "text": "this benchmark shows far far from a few months ago is that we took the vanilla",
    "start": "267130",
    "end": "273880"
  },
  {
    "text": "tensorflow version and we run an image classification training benchmark on the",
    "start": "273880",
    "end": "280780"
  },
  {
    "text": "c5 instance okay and then we optimize transfer flow using performance patches",
    "start": "280780",
    "end": "288340"
  },
  {
    "text": "and collaborating with Intel and we train the exact same job on the exact",
    "start": "288340",
    "end": "294040"
  },
  {
    "text": "same instance type and we made it 11 times faster okay so when you use our",
    "start": "294040",
    "end": "301750"
  },
  {
    "text": "tensorflow version running on AWS and the same applies to other libraries you just",
    "start": "301750",
    "end": "307600"
  },
  {
    "text": "don't use the vanilla version that's available out there okay you get the same API optimized okay so 11 times",
    "start": "307600",
    "end": "315490"
  },
  {
    "text": "faster is very significant because it literally means if you have a training job that lasts for 12 hours you know now",
    "start": "315490",
    "end": "322540"
  },
  {
    "text": "it's gonna last a little more than one hour so during the same day for the same budget you can train 11 times more and",
    "start": "322540",
    "end": "330280"
  },
  {
    "text": "machine running is a very iterative process so the more you train the more chances you have to get to the the best",
    "start": "330280",
    "end": "336760"
  },
  {
    "text": "model so this is available on sage maker and on the deep learning mi you have nothing to do just importance of low",
    "start": "336760",
    "end": "343420"
  },
  {
    "text": "enjoy speed we also worked on scaling chance of flow for large numbers of GPUs",
    "start": "343420",
    "end": "349960"
  },
  {
    "text": "so we noticed that for some customers training at very high scale they didn't",
    "start": "349960",
    "end": "356950"
  },
  {
    "text": "get the linear speed-up that they expected so specifically when training",
    "start": "356950",
    "end": "362050"
  },
  {
    "text": "with 256 GPUs they would only get 65 percent of the ideal linear speed-up",
    "start": "362050",
    "end": "369730"
  },
  {
    "text": "okay and that's pretty bad because it means the extra GPUs that you add to the problem are not efficient so you don't",
    "start": "369730",
    "end": "376930"
  },
  {
    "text": "spend your money well again we worked on this problem optimized tensorflow and now we have 90%",
    "start": "376930",
    "end": "384940"
  },
  {
    "text": "of the ideal linear speed-up so that means pretty much if you double the number of GPUs on your chance of road",
    "start": "384940",
    "end": "391450"
  },
  {
    "text": "training job you get almost twice the speed up all the way to 256 GPUs okay so",
    "start": "391450",
    "end": "399729"
  },
  {
    "text": "linear scaling again this is available in sage maker and the deep lining mi so to give you an",
    "start": "399729",
    "end": "405340"
  },
  {
    "text": "idea of what this means if we train the resin 850 image classifier on a large",
    "start": "405340",
    "end": "412150"
  },
  {
    "text": "image data set with the vanilla version and 256 GPUs this takes 30 minutes with",
    "start": "412150",
    "end": "418510"
  },
  {
    "text": "the new version it takes 14 minutes ok so imagine when you're working with 256",
    "start": "418510",
    "end": "425050"
  },
  {
    "text": "GPUs I guess you have a pretty large data set right so using this optimized version on GPUs",
    "start": "425050",
    "end": "431590"
  },
  {
    "text": "you can slash your training time in half so two weeks becomes one week that's a",
    "start": "431590",
    "end": "438400"
  },
  {
    "text": "very very significant saving especially with 256 GPUs we also did some work on",
    "start": "438400",
    "end": "444940"
  },
  {
    "text": "Apache on accident another open source library for deep running so the problem what we're solving here is when you do",
    "start": "444940",
    "end": "452440"
  },
  {
    "text": "distributed training you need to keep the same number of instances and GPUs if",
    "start": "452440",
    "end": "459070"
  },
  {
    "text": "you're training on GPUs from beginning to end okay so what if for a very long",
    "start": "459070",
    "end": "465850"
  },
  {
    "text": "training job you actually have extra instances lying around doing nothing",
    "start": "465850",
    "end": "471240"
  },
  {
    "text": "wouldn't it wouldn't it be nice if you could take those extra instances and add",
    "start": "471240",
    "end": "476320"
  },
  {
    "text": "them to the training job and maybe take them back when you need them again well this is exactly what we've done on MX",
    "start": "476320",
    "end": "482680"
  },
  {
    "text": "net it's called dynamic training and we can add and remove instances during one",
    "start": "482680",
    "end": "489400"
  },
  {
    "text": "training job okay and we do this without losing any accuracy so if you have",
    "start": "489400",
    "end": "494500"
  },
  {
    "text": "reserved instances for example lying around doing nothing why don't you add them to your long",
    "start": "494500",
    "end": "501490"
  },
  {
    "text": "lasting training job and and speed things up right we're trying we're",
    "start": "501490",
    "end": "507130"
  },
  {
    "text": "working on on implementing this in other libraries but for now this is only available in MX net so I guess that's",
    "start": "507130",
    "end": "517270"
  },
  {
    "text": "pretty good for infrastructure and libraries now we can train very fast we can predict very fast with the c-5 now",
    "start": "517270",
    "end": "525610"
  },
  {
    "text": "what about working on models can we make models more efficient and I'm gonna talk",
    "start": "525610",
    "end": "532060"
  },
  {
    "text": "about three things I'm gonna talk about automatic model tuning so figuring out automatically optimal hyperparameters",
    "start": "532060",
    "end": "539260"
  },
  {
    "text": "for models I'm going to talk about model compilation optimizing for the",
    "start": "539260",
    "end": "545770"
  },
  {
    "text": "underlying hardware which is not something where people talk about a lot about when it comes to machine learning",
    "start": "545770",
    "end": "551980"
  },
  {
    "text": "but hey we think it matters and model compression so shrinking models",
    "start": "551980",
    "end": "557160"
  },
  {
    "text": "automatically and this is uh this is my Tel Aviv topic this is the only place in the world I can cover this so there you",
    "start": "557160",
    "end": "564850"
  },
  {
    "text": "go right that's the hard core part of the hard core talk hopefully you'll like it so let's talk about hyper parameters",
    "start": "564850",
    "end": "571710"
  },
  {
    "text": "so hyper parameters are parameters for the algo itself okay so let's say we're",
    "start": "571710",
    "end": "579640"
  },
  {
    "text": "working with decision trees when we set up the training job we have to decide",
    "start": "579640",
    "end": "585100"
  },
  {
    "text": "what the maximum depth for the trees should be and what the maximum number of",
    "start": "585100",
    "end": "590320"
  },
  {
    "text": "leaves should be plus some very esoteric parameters like gamma ETA lambda alpha",
    "start": "590320",
    "end": "596650"
  },
  {
    "text": "and a few more okay and the list goes on so you have to know quite a lot about",
    "start": "596650",
    "end": "602200"
  },
  {
    "text": "the algo to get this right and when it comes to neural networks again we need",
    "start": "602200",
    "end": "607300"
  },
  {
    "text": "to figure out how many layers we should have and how wide each layer should be",
    "start": "607300",
    "end": "613150"
  },
  {
    "text": "and what the learning rate should be for for that training job and if we're working with embeddings then what should",
    "start": "613150",
    "end": "619720"
  },
  {
    "text": "be the configuration for that and how many dimensions and blah blah blah and the list goes on",
    "start": "619720",
    "end": "624940"
  },
  {
    "text": "right and this this is y al goes and deep learning are interesting because they can be tweaked to death but if",
    "start": "624940",
    "end": "633700"
  },
  {
    "text": "you're just starting with machine learning and even if you're experienced figuring out the right parameters is pretty difficult so there are a number",
    "start": "633700",
    "end": "641050"
  },
  {
    "text": "of techniques to do this the first one is manual search ok aka I",
    "start": "641050",
    "end": "647500"
  },
  {
    "text": "know what I'm doing ok so I have this 12 parameters for my algo and I know",
    "start": "647500",
    "end": "653440"
  },
  {
    "text": "exactly which ones I should use because I just know what I'm doing right or I read a blog about that or I saw someone",
    "start": "653440",
    "end": "660880"
  },
  {
    "text": "on stage who told me to use those values I saw this YouTube video etc so",
    "start": "660880",
    "end": "666910"
  },
  {
    "text": "trust me no one knows what they're really doing me especially so that's not",
    "start": "666910",
    "end": "672040"
  },
  {
    "text": "a really good technique right the second way of figuring out hyper parameters is called grid search so it",
    "start": "672040",
    "end": "678160"
  },
  {
    "text": "sounds more scientific so the way grid search works is you start by defining",
    "start": "678160",
    "end": "683490"
  },
  {
    "text": "hyper parameter ranges so again let's say you have 12 parameters okay so you",
    "start": "683490",
    "end": "689319"
  },
  {
    "text": "have reasonable values for those parameters you know some parameters are by design between 0 & 1 so you're gonna",
    "start": "689319",
    "end": "695709"
  },
  {
    "text": "explore that and some other parameters maybe may have a wider range but you can still figure out reasonable ranges to",
    "start": "695709",
    "end": "702310"
  },
  {
    "text": "explore okay so now you have this 12 dimension space okay and you start by",
    "start": "702310",
    "end": "708459"
  },
  {
    "text": "picking maybe a hundred random combinations for those twelve parameters",
    "start": "708459",
    "end": "713680"
  },
  {
    "text": "and you train okay so you train 100 times and you look at the accuracies that you get and the theory says some",
    "start": "713680",
    "end": "721029"
  },
  {
    "text": "part of that 12 dimension space will yield slightly higher accuracies than",
    "start": "721029",
    "end": "726699"
  },
  {
    "text": "the rest okay so you restrict the ranges to this hotspot and you train maybe you",
    "start": "726699",
    "end": "733810"
  },
  {
    "text": "hundred more times okay hoping that within that hotspot there is a another",
    "start": "733810",
    "end": "739269"
  },
  {
    "text": "tinier hotspot and you do that a few more times until you get to the accuracy",
    "start": "739269",
    "end": "744490"
  },
  {
    "text": "that you like or until you run out of budget okay and it's it's really similar",
    "start": "744490",
    "end": "749889"
  },
  {
    "text": "to exploring space right space is mostly empty with galaxies every now and then so find a galaxy zoom in on the galaxy",
    "start": "749889",
    "end": "757449"
  },
  {
    "text": "and the galaxy is mostly empty it has a few solar systems so find a solar system",
    "start": "757449",
    "end": "762730"
  },
  {
    "text": "and zoom in on that and within the solar system you have a few planets hopefully",
    "start": "762730",
    "end": "768610"
  },
  {
    "text": "one is more interesting than the others and you zoom in on that one right and then you zoom in on the planet and you",
    "start": "768610",
    "end": "775060"
  },
  {
    "text": "end up in Tel Aviv right so that's the theory unfortunately you're gonna train",
    "start": "775060",
    "end": "780339"
  },
  {
    "text": "thousands of models it's gonna take forever so it's slow and it's expensive so some people said hey we need to find",
    "start": "780339",
    "end": "789579"
  },
  {
    "text": "a faster way to get to that 2d a hyper parameters so let's try random search",
    "start": "789579",
    "end": "795910"
  },
  {
    "text": "okay I call that technique spray-and-pray and it feels not as efficient as the",
    "start": "795910",
    "end": "803380"
  },
  {
    "text": "previous one right the previous one felt scientific slow but scientific this one",
    "start": "803380",
    "end": "809250"
  },
  {
    "text": "not so much actually there's a research paper from 2012 written by the top researchers in",
    "start": "809250",
    "end": "815620"
  },
  {
    "text": "AI that show that on average this works better than research so just speak 100",
    "start": "815620",
    "end": "820900"
  },
  {
    "text": "or 200 values for the hyper parameters at random train just pick the best okay",
    "start": "820900",
    "end": "826540"
  },
  {
    "text": "on average you will get okay models so fine faster cheaper but it's random okay",
    "start": "826540",
    "end": "834790"
  },
  {
    "text": "so how do you explain to your boss or a customer that yes you select this model",
    "start": "834790",
    "end": "839890"
  },
  {
    "text": "because it's random good luck so the the proper way to do this is to use machine",
    "start": "839890",
    "end": "847270"
  },
  {
    "text": "learning okay and this is called a hyper parameter optimization and it is it's what sage maker does so you train a",
    "start": "847270",
    "end": "854680"
  },
  {
    "text": "first few models maybe three two three four models okay with random values and",
    "start": "854680",
    "end": "861450"
  },
  {
    "text": "then using those data points that map parameters to accuracies you apply",
    "start": "861450",
    "end": "869310"
  },
  {
    "text": "machine learning optimization and if you can't sleep tonight I highly recommend",
    "start": "869310",
    "end": "874750"
  },
  {
    "text": "Gaussian process regression and Bayesian optimization and those algos predict the",
    "start": "874750",
    "end": "881500"
  },
  {
    "text": "next three sets of parameters to try so you train those three additional jobs",
    "start": "881500",
    "end": "887830"
  },
  {
    "text": "now you have six and musics those six points maybe you predict the three next ones",
    "start": "887830",
    "end": "893190"
  },
  {
    "text": "okay so you don't decide at random what the next three jobs should be they are predicted they come out of those two",
    "start": "893190",
    "end": "900190"
  },
  {
    "text": "algos okay so that sounds insane let's look at a demo so here I'm going to use",
    "start": "900190",
    "end": "906970"
  },
  {
    "text": "sage maker and I'm going to use one of the built-in are goes in Sage maker and",
    "start": "906970",
    "end": "912310"
  },
  {
    "text": "this one is XJ boost right so popular open source I'll go for classification",
    "start": "912310",
    "end": "918300"
  },
  {
    "text": "so this gray cell here is what we would do on stage maker",
    "start": "918300",
    "end": "923500"
  },
  {
    "text": "use the estimator object in sage maker used the container storing the extra",
    "start": "923500",
    "end": "930400"
  },
  {
    "text": "boost implementation train on one c5 2x instance output the model to that",
    "start": "930400",
    "end": "936980"
  },
  {
    "text": "location set some hyper parameters because we know what we're doing right",
    "start": "936980",
    "end": "942950"
  },
  {
    "text": "so here we're trying to do binary classification on the data set and we think the trees should be no deeper than",
    "start": "942950",
    "end": "950360"
  },
  {
    "text": "four levels because we know what we're doing and we want to train for a thousand rounds with early stopping at",
    "start": "950360",
    "end": "956750"
  },
  {
    "text": "100 rounds because we know what we're doing okay so good luck with that you",
    "start": "956750",
    "end": "964010"
  },
  {
    "text": "can train a hundred of those manually or you can do grid search and you can you can explore space forever and you will",
    "start": "964010",
    "end": "970490"
  },
  {
    "text": "never get to the right place so what we do instead is this okay we define hyper",
    "start": "970490",
    "end": "980060"
  },
  {
    "text": "parameter ranges so here I'm optimizing for for high-power parameters so max the",
    "start": "980060",
    "end": "985910"
  },
  {
    "text": "maximum depth of the tree trees actually because we have multiple trees and those",
    "start": "985910",
    "end": "991910"
  },
  {
    "text": "three esoteric parameters ETA min child weight and alpha okay starting from",
    "start": "991910",
    "end": "998330"
  },
  {
    "text": "reasonable ranges so we can have integer ranges we can have continuous ranges so",
    "start": "998330",
    "end": "1003670"
  },
  {
    "text": "floats and we can have categorical ranges if you want to pass function",
    "start": "1003670",
    "end": "1010360"
  },
  {
    "text": "names optimizer names etc etc okay so I'm defining my ranges I'm defining the",
    "start": "1010360",
    "end": "1016750"
  },
  {
    "text": "metric I want to optimize on so here I want to minimize validation error on",
    "start": "1016750",
    "end": "1022840"
  },
  {
    "text": "this training okay and then still",
    "start": "1022840",
    "end": "1029260"
  },
  {
    "text": "setting up my estimator in the same way so using the same extra boost container training on one in four to excel",
    "start": "1029260",
    "end": "1036640"
  },
  {
    "text": "instance just setting the mandatory parameter which is objective what are",
    "start": "1036640",
    "end": "1043360"
  },
  {
    "text": "you trying to do here XJ boost can be used in different capabilities so here I want to build a binary classifier that's",
    "start": "1043360",
    "end": "1050590"
  },
  {
    "text": "the only one that I said and then i create my hyper parameter tuning object",
    "start": "1050590",
    "end": "1055840"
  },
  {
    "text": "with the estimator the metric i'm optimizing on the range is to explore",
    "start": "1055840",
    "end": "1061510"
  },
  {
    "text": "the number of jobs i want to train and how many i want to train in",
    "start": "1061510",
    "end": "1066780"
  },
  {
    "text": "go okay so I will train 3x3 until I've trained 20 and each time I've trained",
    "start": "1066780",
    "end": "1072390"
  },
  {
    "text": "three I will run hyper parameter optimization and predict the next ones",
    "start": "1072390",
    "end": "1077840"
  },
  {
    "text": "okay and then I just get the training going calling the feet API just as I would",
    "start": "1077840",
    "end": "1086340"
  },
  {
    "text": "write so this trains for a bit okay and if I switch to the sage maker console",
    "start": "1086340",
    "end": "1096770"
  },
  {
    "text": "okay I'll see my tuning job here so I'd",
    "start": "1096770",
    "end": "1102720"
  },
  {
    "text": "retrain those 20 jobs I can see the metric for each individual job and if I",
    "start": "1102720",
    "end": "1108000"
  },
  {
    "text": "click on the individual training jobs I will see the parameters that have been selected and predicted or I could just",
    "start": "1108000",
    "end": "1116010"
  },
  {
    "text": "say hey just give me the one give me the best one right I'm only interested in",
    "start": "1116010",
    "end": "1121080"
  },
  {
    "text": "the best one so the best one actually used max depth of five and alpha of one",
    "start": "1121080",
    "end": "1129630"
  },
  {
    "text": "point eight nine nine seven eight seven five which of course is a value you would have figured out manually etc etc",
    "start": "1129630",
    "end": "1137280"
  },
  {
    "text": "okay so now you can just grab this best model from sage maker and deploy it okay",
    "start": "1137280",
    "end": "1143090"
  },
  {
    "text": "and we also have a notebook where you can visualize the progress of a training",
    "start": "1143090",
    "end": "1149880"
  },
  {
    "text": "job so we can see all the tree all the individual training jobs and their accuracies and we can plot let's just",
    "start": "1149880",
    "end": "1157620"
  },
  {
    "text": "look at this one we can plot the jobs so here that's for the max depth parameter",
    "start": "1157620",
    "end": "1163340"
  },
  {
    "text": "okay and and then we see the validation error so obviously lower is better okay",
    "start": "1163340",
    "end": "1169110"
  },
  {
    "text": "so we can see the top job is actually that one okay and we can see there's a",
    "start": "1169110",
    "end": "1175110"
  },
  {
    "text": "bit of a trend right so if we have deeper trees we tend to increase validation error because probably we are",
    "start": "1175110",
    "end": "1181920"
  },
  {
    "text": "over fitting the data set okay so that kind of makes sense so it's also a good way to understand what the impact of",
    "start": "1181920",
    "end": "1188700"
  },
  {
    "text": "specific parameters is on your training job okay and the only thing that we've",
    "start": "1188700",
    "end": "1194340"
  },
  {
    "text": "done here is to train to Train just 20",
    "start": "1194340",
    "end": "1199500"
  },
  {
    "text": "times right which is very typical of what you would do train you know tens of time not",
    "start": "1199500",
    "end": "1205530"
  },
  {
    "text": "hundreds of times now let's move on to the next problem okay so now we have a",
    "start": "1205530",
    "end": "1210780"
  },
  {
    "text": "model that's optimized with respect to hyper parameters now how do we optimize",
    "start": "1210780",
    "end": "1215850"
  },
  {
    "text": "it with respect to the underlying hardware and again it's not a problem we usually work on okay maybe you're",
    "start": "1215850",
    "end": "1222750"
  },
  {
    "text": "surprised that oh yeah seriously we could optimize for their hardware unfortunately it's very",
    "start": "1222750",
    "end": "1228570"
  },
  {
    "text": "complex okay so it's possible you can do it but it's really manual work expert",
    "start": "1228570",
    "end": "1235830"
  },
  {
    "text": "work and every time you train a new model or every time you want to optimize for a new hardware platform you had to",
    "start": "1235830",
    "end": "1242340"
  },
  {
    "text": "you have to do it all over again so it's very time consuming very time intensive and the consequence is most people don't",
    "start": "1242340",
    "end": "1249690"
  },
  {
    "text": "do it so to solve this problem we've introduced a new service called neo and",
    "start": "1249690",
    "end": "1255170"
  },
  {
    "text": "neo is be two things neo is a model compiler so it will take an existing",
    "start": "1255170",
    "end": "1262710"
  },
  {
    "text": "model in s3 you just write a very simple config file I'll show you in a minute and you compile it with a simple API and",
    "start": "1262710",
    "end": "1270570"
  },
  {
    "text": "by the way it's free I know that's important so you know you don't get",
    "start": "1270570",
    "end": "1275790"
  },
  {
    "text": "charged for that service and what you get is the is an optimized version of",
    "start": "1275790",
    "end": "1281400"
  },
  {
    "text": "the model for the underlying hardware that you can then load on your platform",
    "start": "1281400",
    "end": "1286460"
  },
  {
    "text": "using the neo runtime which is a really tiny runtime much much smaller than",
    "start": "1286460",
    "end": "1291470"
  },
  {
    "text": "typical libraries like chancellor flow so in the IOT world yoke's",
    "start": "1291470",
    "end": "1296580"
  },
  {
    "text": "you also save space by not installing those libraries and you can just load",
    "start": "1296580",
    "end": "1301740"
  },
  {
    "text": "the model with the neo runtime and predict okay so we've actually opened sort open",
    "start": "1301740",
    "end": "1307140"
  },
  {
    "text": "source this so if you have a platform that's close to one of our supported platforms but not exactly the same one",
    "start": "1307140",
    "end": "1314460"
  },
  {
    "text": "you can probably port neo and if you work for a hardware manufacturer you're more than welcome to port neo to your",
    "start": "1314460",
    "end": "1321630"
  },
  {
    "text": "own architecture so here's an example for the Raspberry Pi I'm a big Raspberry",
    "start": "1321630",
    "end": "1327930"
  },
  {
    "text": "Pi fan so here I've got a it's a resin at 50 model so image classifier",
    "start": "1327930",
    "end": "1334020"
  },
  {
    "text": "that I that I trained or you could take an existing model from from a model Zoo",
    "start": "1334020",
    "end": "1339270"
  },
  {
    "text": "okay just put it in s3 okay just declare what that model lives",
    "start": "1339270",
    "end": "1347300"
  },
  {
    "text": "declare what the input shape is so that looks weird but if you train the model you would know okay so here this model",
    "start": "1347300",
    "end": "1355020"
  },
  {
    "text": "is an image classifier so it takes as input one image with three channels red",
    "start": "1355020",
    "end": "1360960"
  },
  {
    "text": "green and blue color images and the images are 2 24 by 2 24 pixels okay once",
    "start": "1360960",
    "end": "1367050"
  },
  {
    "text": "again you would know if you had trained that model what the input shape is it's an MX net model and I want to compile it",
    "start": "1367050",
    "end": "1373950"
  },
  {
    "text": "for the Raspberry Pi and I want the model to be stored in s3 again okay so",
    "start": "1373950",
    "end": "1379460"
  },
  {
    "text": "then I all have to do is run this simple API so I'm using the command line here you could use any SDK and it takes a few",
    "start": "1379460",
    "end": "1387210"
  },
  {
    "text": "seconds for this model and then I grab the compiled model from s3 okay and",
    "start": "1387210",
    "end": "1394950"
  },
  {
    "text": "inside them so I find inside the archive there's the original model and the",
    "start": "1394950",
    "end": "1401309"
  },
  {
    "text": "compiled version okay which is the dot ISO file so it's a shared object it's native code for the photo Raspberry Pi",
    "start": "1401309",
    "end": "1407940"
  },
  {
    "text": "with hardware optimized version of the math operators used by the model right",
    "start": "1407940",
    "end": "1415760"
  },
  {
    "text": "algebra convolution etc so then all I need to do is copy that to my Raspberry",
    "start": "1415760",
    "end": "1422760"
  },
  {
    "text": "Pi okay bottom box and use the deep learning runtime to load the model and",
    "start": "1422760",
    "end": "1428820"
  },
  {
    "text": "predict okay and when I did this demo on my PI I went from seven seconds per",
    "start": "1428820",
    "end": "1434370"
  },
  {
    "text": "image to about one second per image okay so 7x faster so I'm not promising",
    "start": "1434370",
    "end": "1440190"
  },
  {
    "text": "7x speed-up across the across the spectrum okay we tend to say on average",
    "start": "1440190",
    "end": "1447510"
  },
  {
    "text": "you get twice as fast but you know I see better results from time to time okay",
    "start": "1447510",
    "end": "1452640"
  },
  {
    "text": "once again one API call you get this done in you know five",
    "start": "1452640",
    "end": "1457950"
  },
  {
    "text": "minutes including reading the documentation and you speed up your model by 2x 3x and it's free so",
    "start": "1457950",
    "end": "1465930"
  },
  {
    "text": "if I were you I'd give it a try okay so let's see how we can do this with",
    "start": "1465930",
    "end": "1476390"
  },
  {
    "text": "transfer flow why not okay so here I'm training again with stage maker I'm",
    "start": "1476390",
    "end": "1483240"
  },
  {
    "text": "training a simple image classifier on the amnesty data set okay so you've all seen amnesty digits",
    "start": "1483240",
    "end": "1490740"
  },
  {
    "text": "from 0 to 9 and of course we need to learn how to classify in ten categories okay so this is a vanilla tensorflow",
    "start": "1490740",
    "end": "1498630"
  },
  {
    "text": "code okay and I'm training that model on",
    "start": "1498630",
    "end": "1503720"
  },
  {
    "text": "on sage maker I'm actually doing distributed training here which is a beast of a bit wasteful because it is a",
    "start": "1503720",
    "end": "1511500"
  },
  {
    "text": "tiny dataset but just to show you it's super easy to do distributed training just say you want to train on two instances that's all you have to do okay",
    "start": "1511500",
    "end": "1518730"
  },
  {
    "text": "so it trains for a while a short while okay then I can I can deploy it the",
    "start": "1518730",
    "end": "1526440"
  },
  {
    "text": "usual way just called deploy okay so here I'm deploying the unoptimized version the vanilla version okay and I",
    "start": "1526440",
    "end": "1534360"
  },
  {
    "text": "can invoke it predict some digits but what I really want to do is compile it",
    "start": "1534360",
    "end": "1541290"
  },
  {
    "text": "okay so I call that compile model API passing the target Hardware here I can",
    "start": "1541290",
    "end": "1548160"
  },
  {
    "text": "Optima I want to optimize for the c-5 family so the sky lake the intel skylake cpus the input shape in this case is one",
    "start": "1548160",
    "end": "1556290"
  },
  {
    "text": "sample 784 bytes because those images are 28 by 28 pixel and i flatten them",
    "start": "1556290",
    "end": "1562920"
  },
  {
    "text": "into an array so 28 by 28 is 784 okay and this is a tensor flow model okay so",
    "start": "1562920",
    "end": "1570030"
  },
  {
    "text": "this compiles just for a few seconds and then I can deploy it in exactly the same",
    "start": "1570030",
    "end": "1575190"
  },
  {
    "text": "way as before so dot deploy and voila as we say okay so obviously for a tiny",
    "start": "1575190",
    "end": "1581910"
  },
  {
    "text": "model like that the speed-up is not what we're you know I'm not benchmarking or anything but with bigger models you get",
    "start": "1581910",
    "end": "1590220"
  },
  {
    "text": "significant speed up by just doing this okay so it's not even breaking the deployment API all you have to do is",
    "start": "1590220",
    "end": "1597150"
  },
  {
    "text": "compile a model and then deployed okay so I don't see any reason why you would skip that step",
    "start": "1597150",
    "end": "1603600"
  },
  {
    "text": "honestly so easy to do",
    "start": "1603600",
    "end": "1607400"
  },
  {
    "text": "so now let's get crazy okay so now we have a model optimized for the hardware",
    "start": "1610190",
    "end": "1618210"
  },
  {
    "text": "and we're happy but you know maybe it's a little too big you know maybe I want",
    "start": "1618210",
    "end": "1624300"
  },
  {
    "text": "to deploy on a really small device or maybe you know I would like the model to still be faster so could I have the same",
    "start": "1624300",
    "end": "1634380"
  },
  {
    "text": "accuracy with a smaller model so you know less layers a fewer layers I should",
    "start": "1634380",
    "end": "1641070"
  },
  {
    "text": "say fewer layers and maybe narrower layers okay so intuitively it sounds",
    "start": "1641070",
    "end": "1648300"
  },
  {
    "text": "weird I said well no you know we trained that model it got to that accuracy now you want to remove layers on shrink",
    "start": "1648300",
    "end": "1654630"
  },
  {
    "text": "layers and of course we're gonna lose accuracy so you could try and do that",
    "start": "1654630",
    "end": "1660000"
  },
  {
    "text": "manually and spend your life figuring it out or you can go crazy and use a",
    "start": "1660000",
    "end": "1666390"
  },
  {
    "text": "technique or reinforcement learning so I'm not gonna go deep into reinforcement learning reinforcement learning is",
    "start": "1666390",
    "end": "1673380"
  },
  {
    "text": "basically a way to solve hard problems not by building a dataset and training",
    "start": "1673380",
    "end": "1680910"
  },
  {
    "text": "an algo but just by letting an agent experiment in an environment taking",
    "start": "1680910",
    "end": "1687420"
  },
  {
    "text": "actions okay and rewarding positive actions and punishing negative actions",
    "start": "1687420",
    "end": "1693690"
  },
  {
    "text": "so here my agent is gonna start from a trained deep learning model and",
    "start": "1693690",
    "end": "1698910"
  },
  {
    "text": "initially it's a stupid agent so it's going to do silly things it's gonna remove areas that will degrade accuracy",
    "start": "1698910",
    "end": "1705150"
  },
  {
    "text": "a lot et cetera but it's going to do that for a while and by looking at the actions that the",
    "start": "1705150",
    "end": "1710400"
  },
  {
    "text": "agent took and the results okay so did you improve accuracy yes or no we're",
    "start": "1710400",
    "end": "1716580"
  },
  {
    "text": "going to give positive or negative rewards and using a reinforcement learning algo we can learn from the",
    "start": "1716580",
    "end": "1722490"
  },
  {
    "text": "sequence of actions and the reward and we can train that algo and now our agent",
    "start": "1722490",
    "end": "1730140"
  },
  {
    "text": "is a little more intelligent the next time okay so it explores again we train again",
    "start": "1730140",
    "end": "1736630"
  },
  {
    "text": "and the agent becomes a little smarter and we do that a number of times okay and we get to something that works",
    "start": "1736630",
    "end": "1742840"
  },
  {
    "text": "pretty well so in reinforcement learning terms we have an objective which is find",
    "start": "1742840",
    "end": "1748840"
  },
  {
    "text": "the smallest possible model that maintains the original accuracy okay",
    "start": "1748840",
    "end": "1754750"
  },
  {
    "text": "that's the goal and that's the that's the goal that tells us how to reward",
    "start": "1754750",
    "end": "1759840"
  },
  {
    "text": "actions if you took actions that yield a smaller model that is as accurate you",
    "start": "1759840",
    "end": "1765250"
  },
  {
    "text": "get a positive reward anything else gets a negative reward the environment is basically just managing the managing the",
    "start": "1765250",
    "end": "1776230"
  },
  {
    "text": "updated network and training it a bit for a few a box to see what the accuracy",
    "start": "1776230",
    "end": "1781240"
  },
  {
    "text": "is for the modified network the state is just which layer did we keep how big are",
    "start": "1781240",
    "end": "1787570"
  },
  {
    "text": "they etc and the actions are you know remove layers or shrink layers and the",
    "start": "1787570",
    "end": "1792820"
  },
  {
    "text": "reward like I said is a combination of compression and accuracy okay and if it",
    "start": "1792820",
    "end": "1799210"
  },
  {
    "text": "sounds insane it is so we have a sample notebook in Sage maker which I'm not",
    "start": "1799210",
    "end": "1806320"
  },
  {
    "text": "gonna run because I run it yesterday I run it for six hours on five GPU",
    "start": "1806320",
    "end": "1812710"
  },
  {
    "text": "instances because it is a bit of a heavy problem and you can this is actually",
    "start": "1812710",
    "end": "1819760"
  },
  {
    "text": "backed by research paper so again if you want to go crazy this is the notebook to to look for in the sage maker examples",
    "start": "1819760",
    "end": "1826660"
  },
  {
    "text": "and so what this one does is it loads a pre trained ResNet Network and then if",
    "start": "1826660",
    "end": "1834040"
  },
  {
    "text": "that's an agent experiment with removing layers and and training it a little bit",
    "start": "1834040",
    "end": "1840400"
  },
  {
    "text": "and measuring accuracy okay and then we can still probably see this training job",
    "start": "1840400",
    "end": "1847480"
  },
  {
    "text": "in the console yes let's see six hours",
    "start": "1847480",
    "end": "1853660"
  },
  {
    "text": "yes that's the one and maybe we can look at some of the metrics",
    "start": "1853660",
    "end": "1860580"
  },
  {
    "text": "okay so we see we see the metrics but the one are more interested in is this",
    "start": "1865780",
    "end": "1870860"
  },
  {
    "text": "one episode reward mean so we can see the average reward that the the agent",
    "start": "1870860",
    "end": "1877190"
  },
  {
    "text": "gets improves over time so initially it's very low and then as it learns to",
    "start": "1877190",
    "end": "1883600"
  },
  {
    "text": "mess with the the existing network in a positive way it gets higher rewards okay",
    "start": "1883600",
    "end": "1890240"
  },
  {
    "text": "and you can see I could keep that thing going for a while but I felt six hours on five GPU instances that was nice",
    "start": "1890240",
    "end": "1897140"
  },
  {
    "text": "enough okay and so all the code is in",
    "start": "1897140",
    "end": "1902299"
  },
  {
    "text": "the notebook okay you can read all about it and so what kind of result do I get so",
    "start": "1902299",
    "end": "1909980"
  },
  {
    "text": "after so thirty GPU hours so the original model had an accuracy of point",
    "start": "1909980",
    "end": "1918919"
  },
  {
    "text": "81 okay 81 percent and so after those 30 GPU hours I get to a model that's that's",
    "start": "1918919",
    "end": "1928190"
  },
  {
    "text": "the lowest one here three points 66 times smaller okay so almost four times smaller with a",
    "start": "1928190",
    "end": "1935570"
  },
  {
    "text": "base accuracy of 0.7 okay knowing that this accuracy is only measured after a",
    "start": "1935570",
    "end": "1940669"
  },
  {
    "text": "few epochs so if I took that model and trained it properly for as long as I've",
    "start": "1940669",
    "end": "1947390"
  },
  {
    "text": "trained the original model I would get very close to point 81 okay so pretty",
    "start": "1947390",
    "end": "1954020"
  },
  {
    "text": "much same accuracy about three to four times smaller okay and if you train a little longer you can get to even",
    "start": "1954020",
    "end": "1960790"
  },
  {
    "text": "smaller networks okay all right so now if you have a headache it's my fault but",
    "start": "1960790",
    "end": "1967520"
  },
  {
    "text": "hey I have a headache too yeah flu at 3:00 a.m. I recommend it okay so",
    "start": "1967520",
    "end": "1977030"
  },
  {
    "text": "one last thing so we've seen how to optimize hyper parameters right we've",
    "start": "1977030",
    "end": "1983179"
  },
  {
    "text": "seen how to optimize for the underlying hardware so speeding prediction up and",
    "start": "1983179",
    "end": "1991070"
  },
  {
    "text": "we've seen how we could even shrink using that crazy reinforcement learning technique how we could shrink the model",
    "start": "1991070",
    "end": "1999320"
  },
  {
    "text": "without losing accuracy but coming back to more reasonable things we should look",
    "start": "1999320",
    "end": "2005450"
  },
  {
    "text": "at optimizing prediction because we got all excited about training right a lot",
    "start": "2005450",
    "end": "2012799"
  },
  {
    "text": "of people focus on training times optimizing training and why not but we",
    "start": "2012799",
    "end": "2018409"
  },
  {
    "text": "what our customers tell us is that 90% of their spend is actually on prediction",
    "start": "2018409",
    "end": "2024139"
  },
  {
    "text": "and that makes sense because you train once in a while and then you deploy and if you deploy at scale",
    "start": "2024139",
    "end": "2030049"
  },
  {
    "text": "you maybe have 10 20 50 100 instances running 24/7 doing prediction so those",
    "start": "2030049",
    "end": "2037339"
  },
  {
    "text": "costs will add up okay so the legitimate",
    "start": "2037339",
    "end": "2042710"
  },
  {
    "text": "question is since this is the bigger chunk of your machine learning spend are",
    "start": "2042710",
    "end": "2049970"
  },
  {
    "text": "you making the most out of that and we see some problems here when you deploy",
    "start": "2049970",
    "end": "2056599"
  },
  {
    "text": "to CPU or GPU instances sometimes the models are not complex enough to keep",
    "start": "2056599",
    "end": "2063230"
  },
  {
    "text": "the instance busy right the nvidia v 100 chip that i mentioned it has over 5,000",
    "start": "2063230",
    "end": "2069888"
  },
  {
    "text": "GPU cores okay so you'd need to have big models to keep those cores busy less",
    "start": "2069889",
    "end": "2077329"
  },
  {
    "text": "complex models will only use a fraction of that okay and the same goes for virtual CPUs on on CPU instances so it's",
    "start": "2077329",
    "end": "2085429"
  },
  {
    "text": "not obvious that you make the most of the hardware and then every model is",
    "start": "2085429",
    "end": "2090829"
  },
  {
    "text": "going to be a little different right so some models will run nicely on on a CPU",
    "start": "2090829",
    "end": "2096230"
  },
  {
    "text": "instance some will require a GPU instance some will require a GPU",
    "start": "2096230",
    "end": "2103250"
  },
  {
    "text": "instance while not making full use of it so you know that CPU versus GPU split is",
    "start": "2103250",
    "end": "2110510"
  },
  {
    "text": "a little restrictive so that's why we invented this new service called elastic",
    "start": "2110510",
    "end": "2115910"
  },
  {
    "text": "inference and it's very easy to understand so elastic inference lets you attach acceleration to any CPU",
    "start": "2115910",
    "end": "2125260"
  },
  {
    "text": "instance okay so this works on Amazon ec2 Amazon sage maker notebook instances",
    "start": "2125260",
    "end": "2131470"
  },
  {
    "text": "and prediction endpoints so you actually deploy to CPU instance but you attach",
    "start": "2131470",
    "end": "2139080"
  },
  {
    "text": "fractional acceleration between 1 to 32 teraflops okay so you have three sizes",
    "start": "2139080",
    "end": "2145570"
  },
  {
    "text": "medium/large X large so you can experiment with that so now you get to a",
    "start": "2145570",
    "end": "2151960"
  },
  {
    "text": "performance level that's close to what you would get with a full-fledged GPU",
    "start": "2151960",
    "end": "2157720"
  },
  {
    "text": "instance okay but you get a huge discount so if you combine a c5 and a c5",
    "start": "2157720",
    "end": "2166240"
  },
  {
    "text": "medium with the I think the Excel accelerator or the large accelerator",
    "start": "2166240",
    "end": "2173430"
  },
  {
    "text": "come and you compare that to a p2 Excel instance you you get pretty much to the",
    "start": "2173430",
    "end": "2181180"
  },
  {
    "text": "same level of performance as p2 Excel which is the cheapest GPU instance you get within that ten percent but it's",
    "start": "2181180",
    "end": "2188910"
  },
  {
    "text": "7075 percent cheaper okay so if you deploy to GPU instances today I would",
    "start": "2188910",
    "end": "2198010"
  },
  {
    "text": "highly recommend that you try this okay try the different sizes of acceleration deploy to c5 and instead plus an",
    "start": "2198010",
    "end": "2205960"
  },
  {
    "text": "accelerator try the three sizes make sure the performance is okay with your app and then compare the costs okay and",
    "start": "2205960",
    "end": "2213400"
  },
  {
    "text": "then yell at me on Twitter if you didn't save a ton of money right I'm used to it",
    "start": "2213400",
    "end": "2219120"
  },
  {
    "text": "okay let's take a look so here we're",
    "start": "2219120",
    "end": "2227350"
  },
  {
    "text": "going to look at an MX net example and I'm not going to train a model here I'm going to grab a model in onn X format so",
    "start": "2227350",
    "end": "2236860"
  },
  {
    "text": "as you may know onn X is an interoperability format for for MX net",
    "start": "2236860",
    "end": "2245110"
  },
  {
    "text": "and and pi torch and a few other libraries so libraries can exchange models so here I will just download rest",
    "start": "2245110",
    "end": "2253210"
  },
  {
    "text": "the ResNet 152 image classifier in an annex format from the web",
    "start": "2253210",
    "end": "2258480"
  },
  {
    "text": "I will just upload it to a3 okay and then I create it okay I register it so",
    "start": "2258480",
    "end": "2268050"
  },
  {
    "text": "to speak in sage maker and this is an MX net model okay so now I can deploy it",
    "start": "2268050",
    "end": "2275329"
  },
  {
    "text": "and well this is pretty disappointing",
    "start": "2275329",
    "end": "2280589"
  },
  {
    "text": "which is the way I like it because it means it's simple all you have to do is this okay model deploy instance count",
    "start": "2280589",
    "end": "2288089"
  },
  {
    "text": "instance type accelerator type okay so comparing to GPU deployment you just",
    "start": "2288089",
    "end": "2296520"
  },
  {
    "text": "need to change the instance type to CPU and specify an accelerator type okay",
    "start": "2296520",
    "end": "2301680"
  },
  {
    "text": "that's it so if you want to use the if you don't want to use sage maker if you",
    "start": "2301680",
    "end": "2306690"
  },
  {
    "text": "want to use the deep learning ami you'll find in the deep learning mi MX net and",
    "start": "2306690",
    "end": "2312660"
  },
  {
    "text": "tensorflow api's to use acceleration okay we there's a TF contrib API that",
    "start": "2312660",
    "end": "2319710"
  },
  {
    "text": "extends the tensor flow estimator to work with the elastic accelerator and we have a similar feature in the max net so",
    "start": "2319710",
    "end": "2326099"
  },
  {
    "text": "you can either use this managed in sage maker or build it yourself using the API",
    "start": "2326099",
    "end": "2335069"
  },
  {
    "text": "is that we added okay so that's pretty simple so we're trying to make machine",
    "start": "2335069",
    "end": "2344760"
  },
  {
    "text": "learning accessible to everybody and with those new services we think we make",
    "start": "2344760",
    "end": "2350280"
  },
  {
    "text": "it we make those workloads faster for training faster for inference so you",
    "start": "2350280",
    "end": "2356940"
  },
  {
    "text": "save time training time prediction time and time is money so you will save a lot",
    "start": "2356940",
    "end": "2362400"
  },
  {
    "text": "of money especially with that elastic inference and no plumbing right hyper",
    "start": "2362400",
    "end": "2369210"
  },
  {
    "text": "parameter optimization super is super simple to use neo one API call and it's",
    "start": "2369210",
    "end": "2375180"
  },
  {
    "text": "free elastic inference one extra parameter in",
    "start": "2375180",
    "end": "2380640"
  },
  {
    "text": "your deployment API calls on Sage maker okay so no reason not to use those if",
    "start": "2380640",
    "end": "2387300"
  },
  {
    "text": "you ask me but that's just me so you should go and try and tell us what you think",
    "start": "2387300",
    "end": "2392310"
  },
  {
    "text": "if you want to get started I would recommend going to the stage maker page where you'll find information on the",
    "start": "2392310",
    "end": "2399180"
  },
  {
    "text": "services and customer stories pricing documentation etc I highly recommend the",
    "start": "2399180",
    "end": "2405630"
  },
  {
    "text": "collection of notebook examples on github some of mine some of my demos",
    "start": "2405630",
    "end": "2411780"
  },
  {
    "text": "came from that today there are plenty more and this is really the best way to understand how sage maker works they",
    "start": "2411780",
    "end": "2418290"
  },
  {
    "text": "were available on github and are also automatically available on notebook instances when you create them and you",
    "start": "2418290",
    "end": "2425520"
  },
  {
    "text": "can take a look at my blog where I have quite a bit of stage maker and deep learning content so you might find that",
    "start": "2425520",
    "end": "2432300"
  },
  {
    "text": "useful too so thank you very much I hope this was useful you can stay in touch on",
    "start": "2432300",
    "end": "2438630"
  },
  {
    "text": "Twitter send me feedback tell me what you tried I'm happy to get your opinion",
    "start": "2438630",
    "end": "2446130"
  },
  {
    "text": "on those services if you build cool stuff let me know I'm again I'm more than happy to retweet and share with the community so thank you very much I'll",
    "start": "2446130",
    "end": "2454050"
  },
  {
    "text": "come back later this afternoon to talk about inference pipelines and spark so",
    "start": "2454050",
    "end": "2459720"
  },
  {
    "text": "if you want to even bigger headache please stick around thank you very much",
    "start": "2459720",
    "end": "2465320"
  }
]