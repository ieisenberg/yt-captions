[
  {
    "text": "good afternoon everybody I'm David Arvin I'm a data science manager with AWS as a",
    "start": "170",
    "end": "6720"
  },
  {
    "text": "professional services team and I'm very lucky to have with me today Prasad Prabhu a principal engineer added to it",
    "start": "6720",
    "end": "12509"
  },
  {
    "text": "on their data platform team we're gonna talk about build training and deploying machine learning models in sage maker so",
    "start": "12509",
    "end": "20039"
  },
  {
    "text": "thank you guys for all joining us this afternoon in this afternoon run-up last session of the day before replay so we",
    "start": "20039",
    "end": "27269"
  },
  {
    "text": "appreciate you diehards coming out and attending what we're gonna do today is give a brief view of Amazon sage maker",
    "start": "27269",
    "end": "34710"
  },
  {
    "text": "we'll talk about each of the three components build train and deploy we'll talk about a little bit of the additional functionality we've built",
    "start": "34710",
    "end": "41489"
  },
  {
    "text": "into Amazon Sage Maker so the customized algorithms pre-built deep learning framework containers the ability to",
    "start": "41489",
    "end": "47250"
  },
  {
    "text": "bring your own docker container algorithm and stage maker automatic model tuning and then we'll dig into the",
    "start": "47250",
    "end": "53670"
  },
  {
    "text": "kind of 400 level content that is real-time deployment at scale so this is an area that we have a lot of customers",
    "start": "53670",
    "end": "59969"
  },
  {
    "text": "using stage makers successfully on but it's also an area that we get questions from customers on how to do this better",
    "start": "59969",
    "end": "65430"
  },
  {
    "text": "and more effectively so we'll talk about creating and updating endpoints we'll talk about reducing risk deployments and",
    "start": "65430",
    "end": "71549"
  },
  {
    "text": "automatic scaling of sage maker and then we'll pass it off to Prasad and he'll talk through how they're using sage",
    "start": "71549",
    "end": "78210"
  },
  {
    "text": "maker add into it integrating it with their data science workflows and give an overview of their architecture that",
    "start": "78210",
    "end": "84689"
  },
  {
    "text": "they've created so let's start off by just talking about AWS is machine",
    "start": "84689",
    "end": "90329"
  },
  {
    "text": "learning stack so we really view machine learning is this three tiered stack where AI services are at the top ai",
    "start": "90329",
    "end": "95880"
  },
  {
    "text": "services are you know very simple easy to use API based ways that you can",
    "start": "95880",
    "end": "102030"
  },
  {
    "text": "implement machine learning and artificial intelligence into your workflows so whether that be computer",
    "start": "102030",
    "end": "107670"
  },
  {
    "text": "vision where you may be using recognition to do image classification or object detection whether you're in",
    "start": "107670",
    "end": "113880"
  },
  {
    "text": "the speech space where you're using poly for text-to-speech or transcribe to go the other way around or you're doing",
    "start": "113880",
    "end": "120420"
  },
  {
    "text": "language where you may use comprehend to analyze a large corpus of documents or translate to convert from one language",
    "start": "120420",
    "end": "126719"
  },
  {
    "text": "to another or Lex to build your own chat BOTS you can be assured that we have",
    "start": "126719",
    "end": "132000"
  },
  {
    "text": "done the heavy lifting for you to train and set up these services so that all you have to do is bring your",
    "start": "132000",
    "end": "138220"
  },
  {
    "text": "data and make a simple API call to get back results from machine learning and",
    "start": "138220",
    "end": "143500"
  },
  {
    "text": "go very deep in these domains with a lot of expertise here but a lot of times",
    "start": "143500",
    "end": "149830"
  },
  {
    "text": "people have machine learning challenges that are outside of any of these domains and that's why we've created that second",
    "start": "149830",
    "end": "155920"
  },
  {
    "text": "layer which is Amazon Sage Maker our machine learning platform that allows you to build and train your own machine",
    "start": "155920",
    "end": "161500"
  },
  {
    "text": "learning models whether they be in a domain that's outside of the ones of the AI services or whether they be just a",
    "start": "161500",
    "end": "168790"
  },
  {
    "text": "very customized version of something in the vision or speech or language domains",
    "start": "168790",
    "end": "175680"
  },
  {
    "text": "and then those are resting on the more foundational layer of ML frameworks and",
    "start": "175680",
    "end": "181150"
  },
  {
    "text": "infrastructure and it's a WSS goal to be the best place to run a variety of deep learning frameworks so whether you're",
    "start": "181150",
    "end": "187569"
  },
  {
    "text": "using tensorflow whether using MX net where they using PI torch or chain ER we want to be the best place to run those",
    "start": "187569",
    "end": "194650"
  },
  {
    "text": "frameworks the fastest place and that requires us to optimize these frameworks",
    "start": "194650",
    "end": "199930"
  },
  {
    "text": "for the specific hardware that AWS has so a p2 p3 GPU instances latest version",
    "start": "199930",
    "end": "205750"
  },
  {
    "text": "of CPU instances the c5 or FPGAs we want to be able to optimize that so that",
    "start": "205750",
    "end": "212890"
  },
  {
    "text": "you're you can be as effective as possible in using those deep learning frameworks and for a long time we heard",
    "start": "212890",
    "end": "220000"
  },
  {
    "text": "that ml is really complicated for everyday developers it's a very long",
    "start": "220000",
    "end": "225670"
  },
  {
    "text": "cycle that involves multiple teams it can be error-prone there can be gaps and",
    "start": "225670",
    "end": "231209"
  },
  {
    "text": "it requires you to collect and prepare your training data it requires you to",
    "start": "231209",
    "end": "236739"
  },
  {
    "text": "pick a machine learning model or algorithm that you want to use you have to setup and manage a training",
    "start": "236739",
    "end": "241900"
  },
  {
    "text": "environment then you have to scale that out train and to nuts scale and you know",
    "start": "241900",
    "end": "248230"
  },
  {
    "text": "all that requires a lot of work and a you know a lot of iteration a specialized skill set and then even",
    "start": "248230",
    "end": "254799"
  },
  {
    "text": "after you train the model your jobs not done you typically hand it off to another team maybe a team of scientists",
    "start": "254799",
    "end": "260169"
  },
  {
    "text": "is training the model they're gonna hand it off to a dev ops team that's going to go deploy that model into production and",
    "start": "260169",
    "end": "267150"
  },
  {
    "text": "once they have done that even that can sometimes require rewriting the whole process that can be error-prone but once",
    "start": "267150",
    "end": "273900"
  },
  {
    "text": "they've actually completed that they're not done they have to still manage and scale that production environment as the",
    "start": "273900",
    "end": "280620"
  },
  {
    "text": "service grows and traffic grows so that was really the driving force behind why we launched stage maker and why we we've",
    "start": "280620",
    "end": "287610"
  },
  {
    "text": "worked so hard on Sage Maker we really designed it to be a shared set of components that can be used by",
    "start": "287610",
    "end": "292979"
  },
  {
    "text": "scientists that can be used by developers to implement machine learning in production in a very efficient manner",
    "start": "292979",
    "end": "299100"
  },
  {
    "text": "and so we've designed it so that with one click or just a few lines of code you can actually create production grade",
    "start": "299100",
    "end": "306180"
  },
  {
    "text": "training and deployment environments we designed it so that we have customized",
    "start": "306180",
    "end": "311729"
  },
  {
    "text": "algorithms that in many cases are ten times more performant than their other counterparts that you'd find in the",
    "start": "311729",
    "end": "316979"
  },
  {
    "text": "marketplace there also you know once you have those two things you can deliver",
    "start": "316979",
    "end": "322349"
  },
  {
    "text": "predictive insights faster make better decisions and build better experiences",
    "start": "322349",
    "end": "327810"
  },
  {
    "text": "for your customers so how does Amazon Sage maker try and simplify machine",
    "start": "327810",
    "end": "333570"
  },
  {
    "text": "learning well when we think about sage maker we think about the three modules or components of the service and those",
    "start": "333570",
    "end": "339330"
  },
  {
    "text": "those are build train and deploy so the first component is build so stage maker",
    "start": "339330",
    "end": "345960"
  },
  {
    "text": "provides a very easy to use notebook instance experience so you want to spin up an instance that's running a jupiter",
    "start": "345960",
    "end": "352080"
  },
  {
    "text": "notebook server jupiter notebook is a really interactive easy way to visualize",
    "start": "352080",
    "end": "357479"
  },
  {
    "text": "your data explore your data run snippets of code document what you've done with markdown and it's a great interactive",
    "start": "357479",
    "end": "365310"
  },
  {
    "text": "way that a lot of data scientists have have come to you know experience and build machine learning models but you",
    "start": "365310",
    "end": "370710"
  },
  {
    "text": "don't have to use that you're not locked into using jupiter notebooks sage maker is modular these three modules can be",
    "start": "370710",
    "end": "376919"
  },
  {
    "text": "used independently of one another and so if you want to use just one or two of them you can always just pick one or two",
    "start": "376919",
    "end": "382770"
  },
  {
    "text": "and so if you don't want to use the jupiter notebook instances and sage maker you can always call those training",
    "start": "382770",
    "end": "388349"
  },
  {
    "text": "and deployment api's from your own device whether that be a laptop see two instance cloud9",
    "start": "388349",
    "end": "394470"
  },
  {
    "text": "wherever it works for you now i'm to",
    "start": "394470",
    "end": "399599"
  },
  {
    "text": "Sage maker training we really try to design this as a managed environment so that when you call a sage",
    "start": "399599",
    "end": "405000"
  },
  {
    "text": "maker create training API it will spin up provision separate training cluster",
    "start": "405000",
    "end": "411450"
  },
  {
    "text": "of instances it will load in an algorithm container it will load data in",
    "start": "411450",
    "end": "416970"
  },
  {
    "text": "from s3 it will train your algorithm take any outputs write them back to s3 and then tear down that cluster so you",
    "start": "416970",
    "end": "423030"
  },
  {
    "text": "having to think about any of that without you having to manage any of that and of course it works distributed setting so you can spin up multiple",
    "start": "423030",
    "end": "428940"
  },
  {
    "text": "instances at once and have them work together on training your model oh and you can also take advantage of things",
    "start": "428940",
    "end": "433950"
  },
  {
    "text": "like high-performance i/o so pipe mode which allows you to stream data directly from s3 into your training instances",
    "start": "433950",
    "end": "439950"
  },
  {
    "text": "that you can begin training immediately as soon as your instances are up and the community algorithm is ready now on the",
    "start": "439950",
    "end": "447780"
  },
  {
    "text": "deploy component we've tried to provide a lot of options there as well so real time endpoints is what we'll talk more",
    "start": "447780",
    "end": "453990"
  },
  {
    "text": "about later but really when you want to create a prediction from your model you",
    "start": "453990",
    "end": "460740"
  },
  {
    "text": "want to create this real time endpoint that gives you a REST API so that you can you do an HTTP POST request get back",
    "start": "460740",
    "end": "467820"
  },
  {
    "text": "a prediction in near real time from your machine learning model you want that endpoint to be persistent so that you",
    "start": "467820",
    "end": "473280"
  },
  {
    "text": "can build your service around it you want to be reliable and scalable and that's what stage maker real-time endpoints provide you can also use batch",
    "start": "473280",
    "end": "481650"
  },
  {
    "text": "transforms so if you have a large number of records that you need predictions for all at once in a batch setting you can",
    "start": "481650",
    "end": "489120"
  },
  {
    "text": "do that you can use stage maker batch transform it spins up a separate you know bat transform environment it",
    "start": "489120",
    "end": "495600"
  },
  {
    "text": "streams data in from s3 generates those predictions right to the results back out test 3 and then tears down that",
    "start": "495600",
    "end": "501000"
  },
  {
    "text": "hardware so everything again is managed for you so that you don't have to think about that and of course going to Sage",
    "start": "501000",
    "end": "506760"
  },
  {
    "text": "makers modularity you can also deploy to something like an AWS Greengrass or deep lens or even your own on-premise service",
    "start": "506760",
    "end": "513599"
  },
  {
    "text": "because the model artifacts that you get out of sage maker training are yours to keep and do with as you see fit some of",
    "start": "513599",
    "end": "521310"
  },
  {
    "text": "the additional features that we've built for Amazon Sage Maker include stage maker algorithms these are really",
    "start": "521310",
    "end": "526950"
  },
  {
    "text": "designed for speed and scale they've been rewritten with advancements in both science and engineering",
    "start": "526950",
    "end": "533640"
  },
  {
    "text": "to take advantage of things like GPU acceleration and train in a distributed manner and the science is different too",
    "start": "533640",
    "end": "540300"
  },
  {
    "text": "so that actual mathematical methodology that is used for them is different than",
    "start": "540300",
    "end": "546660"
  },
  {
    "text": "what you would find in other implementations of these algorithms in many cases and we have an eclectic mix",
    "start": "546660",
    "end": "552210"
  },
  {
    "text": "so we have some supervised learning algorithms that are a you know common",
    "start": "552210",
    "end": "557310"
  },
  {
    "text": "things like linear regression and classification I'm supervised things like k-means and principal components but we also have",
    "start": "557310",
    "end": "562710"
  },
  {
    "text": "computer vision algorithms like image detection and/or image classification",
    "start": "562710",
    "end": "568080"
  },
  {
    "text": "and object detection and also in the natural language processing space so word to vac sequence to sequence for",
    "start": "568080",
    "end": "575010"
  },
  {
    "text": "neuro machine translation neural topic modeling so we have this variety of different algorithms that you can use",
    "start": "575010",
    "end": "580890"
  },
  {
    "text": "for your use case we also have pre-built",
    "start": "580890",
    "end": "586620"
  },
  {
    "text": "deep learning framework containers in Sage Maker and these were designed so that data scientists who want to work in",
    "start": "586620",
    "end": "592140"
  },
  {
    "text": "tensorflow or PI torch or chain or MX net and write their own neural network or write their own algorithm are able to",
    "start": "592140",
    "end": "598560"
  },
  {
    "text": "do so in a very natural way they bring their 20 lines of Python or the Python that they would normally write in that",
    "start": "598560",
    "end": "603960"
  },
  {
    "text": "framework and they can send it to Sage maker without having to think about orchestrating containers or managing the",
    "start": "603960",
    "end": "611280"
  },
  {
    "text": "building of that container we've set up a process for them to do that so that's very easy to write your code locally and",
    "start": "611280",
    "end": "618380"
  },
  {
    "text": "send it to Sage maker and how to work in Sage makers managed production grade",
    "start": "618380",
    "end": "623640"
  },
  {
    "text": "training and deployment environments these are also open sourced so if you wanted to make changes if you want to",
    "start": "623640",
    "end": "629610"
  },
  {
    "text": "use what sage makers built as a starting point add additional libraries of functionality you can do that as well",
    "start": "629610",
    "end": "634830"
  },
  {
    "text": "and because they're open sourced you can use local mode and pull them locally onto your own environment iterate very",
    "start": "634830",
    "end": "641430"
  },
  {
    "text": "very rapidly and ensure that whatever you write locally is going to work when you push it to the larger scale in your",
    "start": "641430",
    "end": "649440"
  },
  {
    "text": "production environments and of course sage maker always provides the capability to bring your own so if you",
    "start": "649440",
    "end": "655710"
  },
  {
    "text": "have an algorithm that uses a library or a language like R or Java or Julia and",
    "start": "655710",
    "end": "661530"
  },
  {
    "text": "you want that detail still take advantage of sage maker training and",
    "start": "661530",
    "end": "666650"
  },
  {
    "text": "you can absolutely do that you can just",
    "start": "666650",
    "end": "671820"
  },
  {
    "text": "take that container or take that algorithm code bundle it up into a docker container and publish that to an ECR registry and and still use it in",
    "start": "671820",
    "end": "679080"
  },
  {
    "text": "Amazon sage maker sage maker also comes with automatic model tuning and hyper parameters are very important to machine",
    "start": "679080",
    "end": "685770"
  },
  {
    "text": "learning models and getting the right fits getting the right performance out of your algorithm and it's a lot of",
    "start": "685770",
    "end": "692490"
  },
  {
    "text": "undifferentiated heavy lifting for a pretty specialized skill set in a lot of cases data scientists spend a lot of",
    "start": "692490",
    "end": "698040"
  },
  {
    "text": "time iterating over to find the right hyper parameter values and it's it's costly it's expensive and we wanted to",
    "start": "698040",
    "end": "704940"
  },
  {
    "text": "make a very efficient way of finding the right hyper parameter value so whether you're trying to tune the number of",
    "start": "704940",
    "end": "710040"
  },
  {
    "text": "layers in your neural network or our regularization coefficient in a linear regression you can use this automatic",
    "start": "710040",
    "end": "716160"
  },
  {
    "text": "model tuning to do so it's gonna actually build a metal model another machine learning model on top of your",
    "start": "716160",
    "end": "721890"
  },
  {
    "text": "algorithm and it's going to try and predict where good hyper parameter values are and then explore that hyper",
    "start": "721890",
    "end": "729030"
  },
  {
    "text": "parameter space and when it finds good values exploit and really search tightly in within that space where it thinks",
    "start": "729030",
    "end": "734790"
  },
  {
    "text": "it's good and this is great because it works really flexibly it works with the sage maker tree bit out build algorithms",
    "start": "734790",
    "end": "741720"
  },
  {
    "text": "it works with the deep learning framework containers and it works with bring your own so now let's talk a",
    "start": "741720",
    "end": "749610"
  },
  {
    "text": "little bit about real time deployments that scale and as I mentioned before this is an area where we see a lot of",
    "start": "749610",
    "end": "756210"
  },
  {
    "text": "customers using sage maker very successfully but we also wanted to provide some good practices that others",
    "start": "756210",
    "end": "762330"
  },
  {
    "text": "could benefit from so the goal of sage maker is that makes it so easy to set up",
    "start": "762330",
    "end": "767790"
  },
  {
    "text": "a real time endpoint that has a REST API where you can HT post requests get back",
    "start": "767790",
    "end": "774240"
  },
  {
    "text": "a prediction from your machine learning model and that's single click or a couple lines of code and these endpoints",
    "start": "774240",
    "end": "782550"
  },
  {
    "text": "are really scalable they're high throughput and they are highly reliable so you can be confident that you can",
    "start": "782550",
    "end": "789030"
  },
  {
    "text": "build your your service around them so let's go through and see what actually",
    "start": "789030",
    "end": "795150"
  },
  {
    "text": "creating an endpoint looks like we've got AWS CLI code here you could use the Java SDK or boat o3 and Python",
    "start": "795150",
    "end": "803610"
  },
  {
    "text": "to do the same thing but in this case we'll just use the AWS CLI and show you",
    "start": "803610",
    "end": "808620"
  },
  {
    "text": "how easy it is to create a sage maker endpoint so after we've trained our model we can now kick off the create",
    "start": "808620",
    "end": "817079"
  },
  {
    "text": "model process so we've we've trained our model and sage maker we're gonna run AWS sage maker create",
    "start": "817079",
    "end": "822240"
  },
  {
    "text": "model we're gonna give it a model name we'll just call it model one in this case we're gonna point it to a image",
    "start": "822240",
    "end": "828360"
  },
  {
    "text": "this image is a container living in ECR and it has logic that can load in the",
    "start": "828360",
    "end": "835459"
  },
  {
    "text": "model that we've trained and generate a prediction that's all you need to have really set up into this container and",
    "start": "835459",
    "end": "842490"
  },
  {
    "text": "then we point it to a path in s3 and that SP path is where our model artifact",
    "start": "842490",
    "end": "847769"
  },
  {
    "text": "is living that could have come out of sage makers training or we could have trained independently of sage maker and put the model artifact in s3 as well and",
    "start": "847769",
    "end": "854160"
  },
  {
    "text": "we'll call that model artifact model one tar.gz and then the other thing we'll pass in is an I am role which will give",
    "start": "854160",
    "end": "862230"
  },
  {
    "text": "us permission to access that model artifact in s3 and create the sage maker resources so now that we've registered",
    "start": "862230",
    "end": "869490"
  },
  {
    "text": "this model to be hosted in Sage Maker we can go ahead and create an endpoint configuration with AWS sage maker create",
    "start": "869490",
    "end": "877290"
  },
  {
    "text": "endpoint config and this endpoint config most importantly points back at model 1 and says that's the model that we want",
    "start": "877290",
    "end": "883800"
  },
  {
    "text": "to host with our endpoint it also provides information like the hardware",
    "start": "883800",
    "end": "889769"
  },
  {
    "text": "setup so how many instances we're going to use and what type of instances we're going to use in this case 2 MLM for",
    "start": "889769",
    "end": "895140"
  },
  {
    "text": "x-large instances we have variant weight and variant name we'll show how those are used in more detail later and we'll",
    "start": "895140",
    "end": "901170"
  },
  {
    "text": "give that endpoint config a name called model 1 config now we actually want to",
    "start": "901170",
    "end": "906870"
  },
  {
    "text": "create the endpoint and creating the endpoint means actually provisioning the instances for a persistent endpoint so",
    "start": "906870",
    "end": "913199"
  },
  {
    "text": "when we run AWS create endpoint that's going to actually go out and spin up",
    "start": "913199",
    "end": "919139"
  },
  {
    "text": "those instances load our model load our container and make sure that it's up and ready to be a REST API and serve of",
    "start": "919139",
    "end": "926910"
  },
  {
    "text": "predictions from our machine learning model and so AWS age maker create",
    "start": "926910",
    "end": "933029"
  },
  {
    "text": "endpoint all we have to do is define the endpoint name we'll call it my end point and an",
    "start": "933029",
    "end": "938160"
  },
  {
    "text": "end point config name which will point back to that model 1 config that we just created in the step above well that's",
    "start": "938160",
    "end": "946050"
  },
  {
    "text": "great now we've got an end point up and running but a lot of times in machine learning people want to be able to update their end points they've gotten",
    "start": "946050",
    "end": "952470"
  },
  {
    "text": "new data and they've retrain their model or they think they've done made an improvement in how they've set up their",
    "start": "952470",
    "end": "958379"
  },
  {
    "text": "model well that's a different algorithm or just better hyper parameters or other",
    "start": "958379",
    "end": "963899"
  },
  {
    "text": "values stage maker again makes that really really easy for you to do so we",
    "start": "963899",
    "end": "969629"
  },
  {
    "text": "use Bluegreen deployments which means you don't have any scheduled downtime when you update a stage maker endpoint",
    "start": "969629",
    "end": "975149"
  },
  {
    "text": "you can be sure that you're not going to lose traffic because you have to take your endpoint out of service and then put it back up in service with the new",
    "start": "975149",
    "end": "981779"
  },
  {
    "text": "model no we handle all that behind the scenes for you with a Bluegreen deployment you can also deploy multiple",
    "start": "981779",
    "end": "987720"
  },
  {
    "text": "models behind the same endpoint we'll talk a little bit about that in the reduced risk deployment section as well",
    "start": "987720",
    "end": "994040"
  },
  {
    "text": "so to do that updating endpoint operation it looks very similar to our",
    "start": "994040",
    "end": "999870"
  },
  {
    "text": "last block of code we're gonna create a model in this case we're gonna call it model two we're gonna point out a",
    "start": "999870",
    "end": "1005059"
  },
  {
    "text": "different model artifact in s3 model two tar.gz we're gonna create a new endpoint",
    "start": "1005059",
    "end": "1010939"
  },
  {
    "text": "configuration we're gonna call that model two - config and we'll just point at the model - model name that we just",
    "start": "1010939",
    "end": "1016879"
  },
  {
    "text": "created in the previous step but that's the only thing that's different now you could also change some of the hardware",
    "start": "1016879",
    "end": "1021980"
  },
  {
    "text": "configurations perhaps you moved from a tree based method to a neural network and you want to instead use GPUs in this",
    "start": "1021980",
    "end": "1030649"
  },
  {
    "text": "case rather than CPU so you can do that that's that's fine in this case we've just kept two ml m4x large as our",
    "start": "1030649",
    "end": "1036980"
  },
  {
    "text": "endpoint setup and then instead of using a to be of stage maker create model",
    "start": "1036980",
    "end": "1042020"
  },
  {
    "text": "we're gonna use AWS age maker or sorry create endpoint we're gonna use AWS age maker update endpoint and will point at",
    "start": "1042020",
    "end": "1049010"
  },
  {
    "text": "that same endpoint name my endpoint but we'll use the model to - config instead",
    "start": "1049010",
    "end": "1054700"
  },
  {
    "text": "and that's all it takes to update but in a lot of cases although you know we may",
    "start": "1054700",
    "end": "1061429"
  },
  {
    "text": "test your scientists may test on a holdout sample of data and he may be confident that the",
    "start": "1061429",
    "end": "1067110"
  },
  {
    "text": "model is performing more effectively based on on that whole boat dataset that",
    "start": "1067110",
    "end": "1074970"
  },
  {
    "text": "may actually not be the case when you scale it out into the real world or there may still be unforeseen issues",
    "start": "1074970",
    "end": "1080220"
  },
  {
    "text": "that you might run into and so you probably want to reduce your risk of when you incremental Erie train your",
    "start": "1080220",
    "end": "1085950"
  },
  {
    "text": "model with new data and you want to update that endpoint or when you try and out a new algorithm and you think you've made improvements you want to reduce the",
    "start": "1085950",
    "end": "1092129"
  },
  {
    "text": "risk of updating that endpoint to switching 100% of the traffic over so",
    "start": "1092129",
    "end": "1097769"
  },
  {
    "text": "there's a very simple way in Sage Maker to do a limited test so that you're",
    "start": "1097769",
    "end": "1104070"
  },
  {
    "text": "still checking how the second model would do in production but you're not",
    "start": "1104070",
    "end": "1109529"
  },
  {
    "text": "sending all of your traffic there especially if you have a high value service that's driving this with you",
    "start": "1109529",
    "end": "1115590"
  },
  {
    "text": "know very important customers are a lot of financial impact you want to do these reduced risk deployments where what",
    "start": "1115590",
    "end": "1121259"
  },
  {
    "text": "we're gonna do now is again create an endpoint config but instead of just pointing out one production variant of",
    "start": "1121259",
    "end": "1128999"
  },
  {
    "text": "model one or model two we're gonna point at two of them model one and model two and here's where we give them different",
    "start": "1128999",
    "end": "1134700"
  },
  {
    "text": "variant names model 1 - traffic and model 2 - traffic so that we can understand which predictions went to",
    "start": "1134700",
    "end": "1141539"
  },
  {
    "text": "which model we also give them different - variant weights so we get model 195",
    "start": "1141539",
    "end": "1147840"
  },
  {
    "text": "and model 2 5 so that means 95% of the traffic is gonna go to our tested model",
    "start": "1147840",
    "end": "1154739"
  },
  {
    "text": "one that's already been in production that we're comfortable with and 5% of the traffic's gonna come in to that new",
    "start": "1154739",
    "end": "1160019"
  },
  {
    "text": "model 2 that we want to see how it performs when we actually scale it in the real world after that we just do do",
    "start": "1160019",
    "end": "1168600"
  },
  {
    "text": "the same update endpoint operation that we just talked about use that same update employee API and we you know use",
    "start": "1168600",
    "end": "1175409"
  },
  {
    "text": "the same endpoint named my endpoint and point out that both models config that we just created the next step is to swap",
    "start": "1175409",
    "end": "1184200"
  },
  {
    "text": "the weights so after we've had model 2 in production for a while we've watched",
    "start": "1184200",
    "end": "1190379"
  },
  {
    "text": "the traffic go in were confirmed that it's performing as we'd expected were comfortable with its behavior we may",
    "start": "1190379",
    "end": "1197429"
  },
  {
    "text": "want to say hey all of a sudden we could just which the end pointed using just model to or we could start scaling back and",
    "start": "1197429",
    "end": "1204030"
  },
  {
    "text": "give more traffic to model to and less traffic to model one and the sage maker makes it very easy to do that as well so",
    "start": "1204030",
    "end": "1210210"
  },
  {
    "text": "you can do update endpoint weights and capacities and that API will actually adjust model ones weight down to in this",
    "start": "1210210",
    "end": "1217800"
  },
  {
    "text": "case five so there's a weight of five on model one wait a 5ml two that means each",
    "start": "1217800",
    "end": "1223620"
  },
  {
    "text": "of them will get 50% of the traffic and you could continue decrementing model ones percent of traffic down further and",
    "start": "1223620",
    "end": "1229770"
  },
  {
    "text": "further and either remove it entirely or you could actually decide to always keep",
    "start": "1229770",
    "end": "1234870"
  },
  {
    "text": "a little bit of traffic going to model one in case you want to see how model one and model two vary in other future",
    "start": "1234870",
    "end": "1240570"
  },
  {
    "text": "use cases or if one drifts more than the other so it provides you a lot of flexibility to control traffic and",
    "start": "1240570",
    "end": "1246330"
  },
  {
    "text": "constantly monitor and maintain and make sure that you're comfortable with which model is in production now that you've",
    "start": "1246330",
    "end": "1255480"
  },
  {
    "text": "got your endpoint up and running Sage maker tries to make it very easy to scale that endpoint so there's a console",
    "start": "1255480",
    "end": "1262740"
  },
  {
    "text": "setting for automatic scaling and it's very easy to use you just navigate to your end point take a look at the",
    "start": "1262740",
    "end": "1268530"
  },
  {
    "text": "automatic scaling you can set the minimum number of instances and the maximum number of instances that you want behind that end point you specify a",
    "start": "1268530",
    "end": "1275730"
  },
  {
    "text": "threshold for target invocations per instance we recommend using that as the",
    "start": "1275730",
    "end": "1280890"
  },
  {
    "text": "value that you use for your automatic scaling we find that it does a pretty good job of finding those nonlinearities",
    "start": "1280890",
    "end": "1288630"
  },
  {
    "text": "in latency so they it relates pretty well it Maps pretty well to once you",
    "start": "1288630",
    "end": "1294060"
  },
  {
    "text": "find that threshold of invocations if you go above it you may see you higher Layton sees and below it you'll see a",
    "start": "1294060",
    "end": "1301950"
  },
  {
    "text": "the latency to desire so trying to figure that precise threshold out takes",
    "start": "1301950",
    "end": "1307620"
  },
  {
    "text": "a little bit of experimentation you can also set scaling cooldowns so this is the amount of time that will",
    "start": "1307620",
    "end": "1314400"
  },
  {
    "text": "have to be above the threshold or below the threshold in order to add or remove instances and this just prevents fluctuation if your traffic is highly",
    "start": "1314400",
    "end": "1321060"
  },
  {
    "text": "volatile so you know I think most people know why automatic scaling is a benefit",
    "start": "1321060",
    "end": "1326730"
  },
  {
    "text": "but just to reiterate most traffic is cyclical you have peaks you know maybe",
    "start": "1326730",
    "end": "1332730"
  },
  {
    "text": "it's daily you might have Peaks when people are awake and troughs when people are asleep",
    "start": "1332730",
    "end": "1338350"
  },
  {
    "text": "you may have a trend so over time you may see more traffic coming to the surface as it becomes more popular and",
    "start": "1338350",
    "end": "1343760"
  },
  {
    "text": "you may also have a large single days where you have a spike in traffic",
    "start": "1343760",
    "end": "1350290"
  },
  {
    "text": "because it's a holiday or because of some other unexpected event and in those",
    "start": "1350290",
    "end": "1356030"
  },
  {
    "text": "cases you really don't want to sit there and monitor your endpoint and maintain your endpoint and so automatic scan only",
    "start": "1356030",
    "end": "1361610"
  },
  {
    "text": "makes it much easier you don't want a provision to the max because if you provision to the max then you're",
    "start": "1361610",
    "end": "1367010"
  },
  {
    "text": "overpaying during the times when people are asleep or you're your timezone traffic are down and you want to still",
    "start": "1367010",
    "end": "1373700"
  },
  {
    "text": "be able to make sure that you have good SaaS and and good Layton sees for",
    "start": "1373700",
    "end": "1378770"
  },
  {
    "text": "prediction so you don't want a provision to the min either so automatic scaling allows you to kind of have the best of",
    "start": "1378770",
    "end": "1385340"
  },
  {
    "text": "both worlds in those cases so what does automatic scaling look like in action so",
    "start": "1385340",
    "end": "1390500"
  },
  {
    "text": "we have a plot here of a sage maker endpoint we've plotted both invocations in total and invocations per instance we",
    "start": "1390500",
    "end": "1397280"
  },
  {
    "text": "start that endpoint out with just a single instance behind it we kick off a load test we send a whole bunch of",
    "start": "1397280",
    "end": "1402730"
  },
  {
    "text": "invocations to that endpoint it acrosses the threshold past the scaling past the cooldown and",
    "start": "1402730",
    "end": "1410990"
  },
  {
    "text": "that initiates the auto scaling component that all of that adds an instance to the endpoint so you can see",
    "start": "1410990",
    "end": "1418430"
  },
  {
    "text": "at 20 135 in that graph the two lines split the invocations for instance go",
    "start": "1418430",
    "end": "1424100"
  },
  {
    "text": "down new instance have been added so the load balancer starts partitioning out you keep the same total level of",
    "start": "1424100",
    "end": "1430010"
  },
  {
    "text": "notations but invocation and for instance goes way down because you now have new instance it instances in your",
    "start": "1430010",
    "end": "1436070"
  },
  {
    "text": "endpoint and you can better manage your traffic or better respond to your",
    "start": "1436070",
    "end": "1441080"
  },
  {
    "text": "traffic with with better latency now we recommend using invocations per instance",
    "start": "1441080",
    "end": "1446720"
  },
  {
    "text": "as the threshold but we do have customers who have very specific scaling criteria that may not be met by",
    "start": "1446720",
    "end": "1453410"
  },
  {
    "text": "implications for instance they may have an algorithm that has very specific memory CPU or GPU requirements and in",
    "start": "1453410",
    "end": "1460460"
  },
  {
    "text": "those cases sage maker also allows you to do automatic scaling based on the",
    "start": "1460460",
    "end": "1467090"
  },
  {
    "text": "endpoint instances CloudWatch metrics so what",
    "start": "1467090",
    "end": "1472100"
  },
  {
    "text": "does that look like so we have to create an application auto scaling policy so we",
    "start": "1472100",
    "end": "1477799"
  },
  {
    "text": "use the AWS application auto scaling CLI and we say register scaleable target the",
    "start": "1477799",
    "end": "1482900"
  },
  {
    "text": "first thing we do is provide the service name which is sage maker we provide a resource which is our model to variant",
    "start": "1482900",
    "end": "1489080"
  },
  {
    "text": "and we provide the scalable dimension so how many instances in this case and give",
    "start": "1489080",
    "end": "1495650"
  },
  {
    "text": "it a minute a max amount then we provide the actual policy so we put the scaling",
    "start": "1495650",
    "end": "1502789"
  },
  {
    "text": "policy we give it a name model to scaling we re specify some of that information about the service and the",
    "start": "1502789",
    "end": "1509059"
  },
  {
    "text": "variant but the real important numbers here are in that target tracking scaling policy configuration and we define a",
    "start": "1509059",
    "end": "1517220"
  },
  {
    "text": "metric named CPU utilization and a target value of 50 so on average when",
    "start": "1517220",
    "end": "1522980"
  },
  {
    "text": "our instances CPU utilization goes above 50% we're gonna add instances to that",
    "start": "1522980",
    "end": "1528980"
  },
  {
    "text": "endpoint and we can see what that looks like under a load test so again we start",
    "start": "1528980",
    "end": "1535970"
  },
  {
    "text": "off top plot is invocations and invocations per instance you can see",
    "start": "1535970",
    "end": "1541130"
  },
  {
    "text": "those are tracking together we have one instance in the endpoint we kick up that",
    "start": "1541130",
    "end": "1547039"
  },
  {
    "text": "load test CP utilization in total for our endpoint spikes and then plateaus as",
    "start": "1547039",
    "end": "1553760"
  },
  {
    "text": "we've kind of exhausted the limits of our endpoint CPU and then we have a",
    "start": "1553760",
    "end": "1560990"
  },
  {
    "text": "split as the in the middle of the chart as the instances come up from auto",
    "start": "1560990",
    "end": "1566809"
  },
  {
    "text": "scaling and so that automatic scaling allows us to handle more invocations overall and fewer with fewer invocations",
    "start": "1566809",
    "end": "1574490"
  },
  {
    "text": "per instance and we get the CPU effectiveness of all those other instances added to the two values on the",
    "start": "1574490",
    "end": "1582470"
  },
  {
    "text": "chart so that you can see we're really able to meet better meet the invocation",
    "start": "1582470",
    "end": "1587659"
  },
  {
    "text": "and requests coming in so with that I will pass it off pass it off to Prasad",
    "start": "1587659",
    "end": "1593630"
  },
  {
    "text": "who's going to talk us through how sage maker has been integrated with Intuit",
    "start": "1593630",
    "end": "1599799"
  },
  {
    "text": "everyone I'm Prasad Prabhu I'm a principal engineer with into its central data platform this is what I'm going to",
    "start": "1600490",
    "end": "1607880"
  },
  {
    "text": "talk about today which is just a brief introduction to Intuit how we use machine learning a brief overview of our",
    "start": "1607880",
    "end": "1614899"
  },
  {
    "text": "data Lake functional architecture and which parts are served by Sage maker a typical model development workflow at",
    "start": "1614899",
    "end": "1622130"
  },
  {
    "text": "Intuit and the key benefits of Sage maker that we've seen and how we",
    "start": "1622130",
    "end": "1627440"
  },
  {
    "text": "standardized the model development process to get speed and then a brief",
    "start": "1627440",
    "end": "1633559"
  },
  {
    "text": "demo off of the stuff we've built right so the Intuit is a financial products",
    "start": "1633559",
    "end": "1639740"
  },
  {
    "text": "company in that we help individuals track their personal finance do their",
    "start": "1639740",
    "end": "1645830"
  },
  {
    "text": "taxes we have small businesses do their accounting we have products for self-employed because gig economy is big",
    "start": "1645830",
    "end": "1653090"
  },
  {
    "text": "nowadays we have tools used by accountants to manage books for small",
    "start": "1653090",
    "end": "1659330"
  },
  {
    "text": "businesses now across all of these we found machine learning is has cross-cutting applications we have a lot",
    "start": "1659330",
    "end": "1666440"
  },
  {
    "text": "of SAS offerings and mobile offerings so we have security based models we have",
    "start": "1666440",
    "end": "1673149"
  },
  {
    "text": "things around each of the products so for example in a tax group we use machine learning to predict to",
    "start": "1673149",
    "end": "1679639"
  },
  {
    "text": "personalize the product so that we give the best experience to each user the US tax code is fairly complex machine",
    "start": "1679639",
    "end": "1686570"
  },
  {
    "text": "learning allows us to simplify it for individual users in the small business group we help a V use machine learning",
    "start": "1686570",
    "end": "1693380"
  },
  {
    "text": "to help us gather data about these small businesses automatically scrape their",
    "start": "1693380",
    "end": "1699080"
  },
  {
    "text": "bank accounts and then kind of sort through which transactions belong to",
    "start": "1699080",
    "end": "1704269"
  },
  {
    "text": "which categories and in self-employed we use them for things like trying to",
    "start": "1704269",
    "end": "1710480"
  },
  {
    "text": "differentiate between personal and business expenses to help self-employed folks maximize their returns so this is",
    "start": "1710480",
    "end": "1718250"
  },
  {
    "text": "like an brief overview of our functional data Lake right we have a product that",
    "start": "1718250",
    "end": "1723260"
  },
  {
    "text": "has instrumentation that sends out beacons of the events that are happening",
    "start": "1723260",
    "end": "1728539"
  },
  {
    "text": "we have a real-time event bus which is Kafka based which all of those events land up in we",
    "start": "1728539",
    "end": "1736030"
  },
  {
    "text": "have a feature management platform that reads those events feature eise's them maybe extracts values embedded in a big",
    "start": "1736030",
    "end": "1743830"
  },
  {
    "text": "JSON object maybe does things like extract business fields out of them and",
    "start": "1743830",
    "end": "1749950"
  },
  {
    "text": "then stores it in a online feature store we also have things that read off of the",
    "start": "1749950",
    "end": "1755470"
  },
  {
    "text": "event bus and put it into a data Lake for large scale analytics and machine learning and then we have data users",
    "start": "1755470",
    "end": "1763120"
  },
  {
    "text": "which are our analysts and data scientists use things like sage maker notebooks and traditional hive clients",
    "start": "1763120",
    "end": "1770590"
  },
  {
    "text": "to connect with the data Lake run large queries get this data right once you",
    "start": "1770590",
    "end": "1776560"
  },
  {
    "text": "have the data we have a mechanism to store those models in a model repository",
    "start": "1776560",
    "end": "1783870"
  },
  {
    "text": "after we train them and then push them out into a REST API model hosting system",
    "start": "1783870",
    "end": "1789790"
  },
  {
    "text": "we have a control tier which gives us things like scheduling engine to run bad",
    "start": "1789790",
    "end": "1794800"
  },
  {
    "text": "jobs for either training or model execution an ml orchestration that helps",
    "start": "1794800",
    "end": "1800200"
  },
  {
    "text": "us track model versioning and I'll that's one of the parts I'll give a demo",
    "start": "1800200",
    "end": "1805450"
  },
  {
    "text": "about at the end a service catalog which gives us standardized implementations of",
    "start": "1805450",
    "end": "1811240"
  },
  {
    "text": "sage maker resources and other AWS resources and a data catalog to help us track what data is available where and",
    "start": "1811240",
    "end": "1819430"
  },
  {
    "text": "the reader catalog is also critical because we have a lot of data that's subject to compliance whether that's tax",
    "start": "1819430",
    "end": "1825250"
  },
  {
    "text": "compliance in the US or GDP are like requirements across in Europe or in",
    "start": "1825250",
    "end": "1832060"
  },
  {
    "text": "other countries so did the data catalog helps us kind of keep track of what what",
    "start": "1832060",
    "end": "1838780"
  },
  {
    "text": "regulation is applicable to which piece of data who can access it and things like that wonderful like given the data",
    "start": "1838780",
    "end": "1845470"
  },
  {
    "text": "that we store which is primarily tax and financial related we also have a lot of",
    "start": "1845470",
    "end": "1853300"
  },
  {
    "text": "data handling guidelines around who can access the data maintaining audit trails of who access that who modified it that",
    "start": "1853300",
    "end": "1861190"
  },
  {
    "text": "was one of the key requirements for us so model deployment workflow or development",
    "start": "1861190",
    "end": "1869620"
  },
  {
    "text": "workflow looks something like this it starts off with an analyst or a data scientist finding a gap in a product or",
    "start": "1869620",
    "end": "1878710"
  },
  {
    "text": "finding a place where we can optimize the user interface and we figure out",
    "start": "1878710",
    "end": "1883780"
  },
  {
    "text": "what are the KPIs that indicate or what the problem is and a potential solution of how we can personalize the particular",
    "start": "1883780",
    "end": "1891010"
  },
  {
    "text": "experience flow right once we have that our data scientists spin up a sage maker",
    "start": "1891010",
    "end": "1897970"
  },
  {
    "text": "notebook they get to wrangle the data they do data visualization they play",
    "start": "1897970",
    "end": "1903280"
  },
  {
    "text": "around with different algorithms to see which one fits the use case the best and the nature of the data once they have",
    "start": "1903280",
    "end": "1910300"
  },
  {
    "text": "figured that out a couple of things come out of that process one thing is which algorithm they want to use and then",
    "start": "1910300",
    "end": "1916870"
  },
  {
    "text": "another thing is a potentially data transformation job that needs to be run",
    "start": "1916870",
    "end": "1922360"
  },
  {
    "text": "on the data at a regular basis so I'll talk about the model deployment workflow",
    "start": "1922360",
    "end": "1928960"
  },
  {
    "text": "rather than the ETL process over here so once somebody's figured out this is",
    "start": "1928960",
    "end": "1934270"
  },
  {
    "text": "the algorithm I want to use they'll build a docker image and there's a",
    "start": "1934270",
    "end": "1940780"
  },
  {
    "text": "couple of ways they can do it they can use either one of the sage maker available default like RT box provided",
    "start": "1940780",
    "end": "1948820"
  },
  {
    "text": "gotham's or they can use other algorithms which might be a combination",
    "start": "1948820",
    "end": "1954400"
  },
  {
    "text": "of open source implementations and in that case they'll build their own docker",
    "start": "1954400",
    "end": "1959440"
  },
  {
    "text": "container we push that docker to Sage maker training once the training job is",
    "start": "1959440",
    "end": "1965320"
  },
  {
    "text": "run we have a model artifact one of the things that we've done is essentially improved the model development lifecycle",
    "start": "1965320",
    "end": "1972940"
  },
  {
    "text": "and we've brought in the rigorous software development to this workflow so once the model is trained we actually",
    "start": "1972940",
    "end": "1978970"
  },
  {
    "text": "want to put it through several phases of testing we have a dev testing workflow",
    "start": "1978970",
    "end": "1984400"
  },
  {
    "text": "where a data scientist just runs their testing to confirm the model looks okay then we put it into a QA environment",
    "start": "1984400",
    "end": "1990640"
  },
  {
    "text": "where we integrate it with these software products confirm it has access to all of the features that it needs and",
    "start": "1990640",
    "end": "1997270"
  },
  {
    "text": "the product is behaving the way it should integrated with the model we run a whole bunch of performance testing at this",
    "start": "1997270",
    "end": "2003820"
  },
  {
    "text": "point to make sure the model behaves as it needs to act at the scale it needs",
    "start": "2003820",
    "end": "2009690"
  },
  {
    "text": "that is needed and once everything looks good we publish the model to production",
    "start": "2009690",
    "end": "2014769"
  },
  {
    "text": "and the artifact that goes to production is essentially the algorithm or the",
    "start": "2014769",
    "end": "2021399"
  },
  {
    "text": "docker container to use the instance sizes and auto scaling groups that we",
    "start": "2021399",
    "end": "2027909"
  },
  {
    "text": "need for the traffic right and that's kind of what we promote to a production environment so the key benefits of",
    "start": "2027909",
    "end": "2036789"
  },
  {
    "text": "Amazon Sage maker as we've seen them is this a lot of algorithms are being",
    "start": "2036789",
    "end": "2042909"
  },
  {
    "text": "supported and new ones keep getting added all the time we originally had our",
    "start": "2042909",
    "end": "2048970"
  },
  {
    "text": "own framework which was more homegrown and built in-house but it doesn't scale",
    "start": "2048970",
    "end": "2055030"
  },
  {
    "text": "well when you want to keep adding new things into it doesn't want to spend its",
    "start": "2055030",
    "end": "2060908"
  },
  {
    "text": "resources trying to add support for open source things and translating them into",
    "start": "2060909",
    "end": "2066849"
  },
  {
    "text": "our in-house framework so amazon building it and providing these Dockers",
    "start": "2066849",
    "end": "2072608"
  },
  {
    "text": "out of the box is a huge advantage right we also have custom algorithms supported using docker this was a big win for us",
    "start": "2072609",
    "end": "2079240"
  },
  {
    "text": "because we have a lot of our own internal models which we are migrating so in the migration phase it it helps us",
    "start": "2079240",
    "end": "2087069"
  },
  {
    "text": "a lot to have custom support it's also highly customizable in the type of",
    "start": "2087069",
    "end": "2092108"
  },
  {
    "text": "instances that we run and when we run",
    "start": "2092109",
    "end": "2097930"
  },
  {
    "text": "our performance testing we actually find out there's some models which are bound by CPU or some by memory like they might",
    "start": "2097930",
    "end": "2105460"
  },
  {
    "text": "load a huge lookup table into memory or some might be IO bound if they're doing",
    "start": "2105460",
    "end": "2111160"
  },
  {
    "text": "a lot of feature lookup we can customize which instances to use and customize our",
    "start": "2111160",
    "end": "2119920"
  },
  {
    "text": "auto scaling groups this helps us like move really really quickly and out of",
    "start": "2119920",
    "end": "2126280"
  },
  {
    "text": "the box model hyper parameter tuning our data scientists love this feature right",
    "start": "2126280",
    "end": "2131670"
  },
  {
    "text": "another critical thing that we love is security given the kind of data that we have Amazon gives us really good tools",
    "start": "2131670",
    "end": "2140260"
  },
  {
    "text": "and really solid integration with I am and kms so we can do things like when",
    "start": "2140260",
    "end": "2147160"
  },
  {
    "text": "somebody spends up a sage maker notebook it's locked down typically to the individual data scientist who's spun it",
    "start": "2147160",
    "end": "2153490"
  },
  {
    "text": "up and nobody else can log in it when they fire queries we can see and log",
    "start": "2153490",
    "end": "2159580"
  },
  {
    "text": "what queries were fired right so all of these are super cool and a good model",
    "start": "2159580",
    "end": "2166270"
  },
  {
    "text": "for authentication and authorization this is a feature of sage maker plus",
    "start": "2166270",
    "end": "2171400"
  },
  {
    "text": "another AWS service called kms or EMR we use a lot of high for our data",
    "start": "2171400",
    "end": "2177820"
  },
  {
    "text": "exploration and we figured out that you can actually integrate high one EMR with",
    "start": "2177820",
    "end": "2183970"
  },
  {
    "text": "our Active Directory so when a user logs in we can track through across all these",
    "start": "2183970",
    "end": "2190180"
  },
  {
    "text": "systems which user is doing what action and obviously the scalability of Amazon",
    "start": "2190180",
    "end": "2195490"
  },
  {
    "text": "Cloud when we were in our own data center one of the biggest bottlenecks was getting time on our Hadoop cluster",
    "start": "2195490",
    "end": "2202830"
  },
  {
    "text": "so data scientists were always fighting for compute resources with Amazon it's",
    "start": "2202830",
    "end": "2208510"
  },
  {
    "text": "much easier for them to go and start experimenting on a data science use case they're not held back by lack of",
    "start": "2208510",
    "end": "2214660"
  },
  {
    "text": "capacity right so the things that we did to standardize model development right",
    "start": "2214660",
    "end": "2220780"
  },
  {
    "text": "we standardized the notebooks which means standardizing the security model arounds notebook instances based on the",
    "start": "2220780",
    "end": "2228670"
  },
  {
    "text": "IAM role that's available to it integration with our Active Directory and Kerberos and adding function and",
    "start": "2228670",
    "end": "2237880"
  },
  {
    "text": "integrations to hive and our data marts so what we did over here is we we built a custom lambda that can be launched",
    "start": "2237880",
    "end": "2245320"
  },
  {
    "text": "through AWS Service Catalog and when a data scientist wants to get a new instance they go to a service catalog",
    "start": "2245320",
    "end": "2251950"
  },
  {
    "text": "say give me a new instance they just need to choose what size they want and it will automatically launch it with the",
    "start": "2251950",
    "end": "2258940"
  },
  {
    "text": "I am role that is assigned to them so they don't need to do anything extra they have the permission that they need",
    "start": "2258940",
    "end": "2264220"
  },
  {
    "text": "when the sage maker notebook comes up once the sage make a notebook it has",
    "start": "2264220",
    "end": "2270609"
  },
  {
    "text": "come up we have custom actions that go in and do things like a point sage maker to our internal pi PI repo we don't want",
    "start": "2270609",
    "end": "2278859"
  },
  {
    "text": "them going out to the Internet integration with an establishing trust with our Kerberos instances and adding",
    "start": "2278859",
    "end": "2286390"
  },
  {
    "text": "like monitoring tools behind the scenes so we can monitor that sage maker instance for training we didn't do too",
    "start": "2286390",
    "end": "2294099"
  },
  {
    "text": "much we we have built a Python library that makes it really easy to build",
    "start": "2294099",
    "end": "2299800"
  },
  {
    "text": "talker images that are used for custom training otherwise we use the out-of-the-box bo2 API for launching",
    "start": "2299800",
    "end": "2307300"
  },
  {
    "text": "training jobs right when it comes to hosting we reuse the the Python library",
    "start": "2307300",
    "end": "2313780"
  },
  {
    "text": "I talked about earlier so that when somebody spins up a new hosting image we",
    "start": "2313780",
    "end": "2319900"
  },
  {
    "text": "ensure that the hosting image is exactly identical to the training image in terms",
    "start": "2319900",
    "end": "2326800"
  },
  {
    "text": "of having all of the prerequisites installed on board most of our data",
    "start": "2326800",
    "end": "2333190"
  },
  {
    "text": "scientists use Python so that's why we standardized on this approach and then",
    "start": "2333190",
    "end": "2339339"
  },
  {
    "text": "integration with our internal internal API gateway and services gateway by",
    "start": "2339339",
    "end": "2346000"
  },
  {
    "text": "default when you have a sage maker hosting instance that comes up it's actually authenticated and authorized",
    "start": "2346000",
    "end": "2352690"
  },
  {
    "text": "through I am and not all of our customers can rely on AWS I am roles so",
    "start": "2352690",
    "end": "2359980"
  },
  {
    "text": "we have our own services mesh that handles a lot of these authentication authorization and then we built a model",
    "start": "2359980",
    "end": "2367000"
  },
  {
    "text": "deployment tool that is kind of a CI ad tool to enforce a strict model",
    "start": "2367000",
    "end": "2372640"
  },
  {
    "text": "development lifecycle all of this helped Intuit cut down the time taken to build",
    "start": "2372640",
    "end": "2379270"
  },
  {
    "text": "a new model and deploy to production from something like six months to one week that's how easy it's been so this",
    "start": "2379270",
    "end": "2388089"
  },
  {
    "text": "part I'm going to give a brief demo of the tools that we built so I'll just",
    "start": "2388089",
    "end": "2394480"
  },
  {
    "text": "talk about the version of models that go into the online hosting world",
    "start": "2394480",
    "end": "2400030"
  },
  {
    "text": "and not the offline batch mode at this point right so we've essentially",
    "start": "2400030",
    "end": "2406180"
  },
  {
    "text": "extended pythons setup dot pi to provide an additional kind of input to the whole",
    "start": "2406180",
    "end": "2414370"
  },
  {
    "text": "thing so those of you who are familiar with Python know that there's something I'll set up that PI and it's a place",
    "start": "2414370",
    "end": "2421450"
  },
  {
    "text": "that a Python developer can declare all of their dependencies when they start building a model and here you can see",
    "start": "2421450",
    "end": "2427780"
  },
  {
    "text": "there is a section that says install underscore requires and there's a bunch of Python dependencies in there your",
    "start": "2427780",
    "end": "2434560"
  },
  {
    "text": "Python pandas tensorflow or if I using scikit-learn you can list",
    "start": "2434560",
    "end": "2439660"
  },
  {
    "text": "all of your dependencies over there and then you need to provide one additional thing over there which is entry points",
    "start": "2439660",
    "end": "2445000"
  },
  {
    "text": "and the two things over there are the set up to underscore tools docker dot",
    "start": "2445000",
    "end": "2451630"
  },
  {
    "text": "predict and then the other one is the Train if you look at the amazon sage maker documentation they actually say",
    "start": "2451630",
    "end": "2457720"
  },
  {
    "text": "when they spin up their docker instance they invoke a specific entry point on",
    "start": "2457720",
    "end": "2462970"
  },
  {
    "text": "that docker container what this allows us to do is provide a mapping from the",
    "start": "2462970",
    "end": "2469360"
  },
  {
    "text": "entry point that sage maker is looking for to a specific function within this repo where the model which needs to kick",
    "start": "2469360",
    "end": "2477190"
  },
  {
    "text": "off the specific job whether it's training or prediction once so the",
    "start": "2477190",
    "end": "2483340"
  },
  {
    "text": "process is essentially a data scientist comes in Forks our model template repo",
    "start": "2483340",
    "end": "2488980"
  },
  {
    "text": "which starts off with this basic template and then they can start adding",
    "start": "2488980",
    "end": "2494080"
  },
  {
    "text": "their own code in here once they're ready to actually run something they just need to run Python setup dot pi",
    "start": "2494080",
    "end": "2500350"
  },
  {
    "text": "train what that does in the back end is generates a new docker file and then",
    "start": "2500350",
    "end": "2506080"
  },
  {
    "text": "kicks off a dock or build which spits out a docker image ID once it's",
    "start": "2506080",
    "end": "2511870"
  },
  {
    "text": "successful you have to publish that that",
    "start": "2511870",
    "end": "2517330"
  },
  {
    "text": "particular docker image to ECR once you it's an ECR then you can use standard",
    "start": "2517330",
    "end": "2522630"
  },
  {
    "text": "station maker commands to kick off training jobs or to use it in hosting so",
    "start": "2522630",
    "end": "2528370"
  },
  {
    "text": "once the data scientist has built their image they're happy with it they come",
    "start": "2528370",
    "end": "2533380"
  },
  {
    "text": "into our internal tool that we call Damac or decision model automation console or you",
    "start": "2533380",
    "end": "2539349"
  },
  {
    "text": "see two big sections in there the top section is for our homegrown older",
    "start": "2539349",
    "end": "2545799"
  },
  {
    "text": "models and the new ones are for the sage maker based models and you can see",
    "start": "2545799",
    "end": "2553239"
  },
  {
    "text": "there's a whole bunch of different environments that are available an account of what models are available to",
    "start": "2553239",
    "end": "2559509"
  },
  {
    "text": "this specific data scientist whose log who's logged in right so let's say I'm a data scientist I've actually built my",
    "start": "2559509",
    "end": "2566079"
  },
  {
    "text": "model but run a training job and I have a model tar.gz sitting in s3 location",
    "start": "2566079",
    "end": "2573700"
  },
  {
    "text": "I'm gonna go in and say create a new endpoint for me I give this tool the",
    "start": "2573700",
    "end": "2580210"
  },
  {
    "text": "location of the github repo where the model code resides which branch or tag",
    "start": "2580210",
    "end": "2585309"
  },
  {
    "text": "that that needs to be built and then what's the path where the model artifact",
    "start": "2585309",
    "end": "2591190"
  },
  {
    "text": "lives once they say submit it behind the scenes it kicks off a Jenkins job that's",
    "start": "2591190",
    "end": "2598089"
  },
  {
    "text": "gonna pick up the get repo run this the door set up docker tools to create a the",
    "start": "2598089",
    "end": "2605349"
  },
  {
    "text": "image again and then push it to ECR and then run a sage maker then creates a",
    "start": "2605349",
    "end": "2611619"
  },
  {
    "text": "sage maker a hosting endpoint for it and then links it to this particular ECR image once that part happens it's",
    "start": "2611619",
    "end": "2619660"
  },
  {
    "text": "actually available to our data scientists to promote through different environments this for standardized",
    "start": "2619660",
    "end": "2626859"
  },
  {
    "text": "environments that that we host but there's a dev and e to e a puff and a",
    "start": "2626859",
    "end": "2633039"
  },
  {
    "text": "prod environment we give access to our data scientists to promote it all the way till the perf environment but not",
    "start": "2633039",
    "end": "2640150"
  },
  {
    "text": "production production updates are normally reserved for the engineering",
    "start": "2640150",
    "end": "2645279"
  },
  {
    "text": "teams and the operations folks who monitor these models so once you have",
    "start": "2645279",
    "end": "2652239"
  },
  {
    "text": "your model built you can publish to different environments we also snapshot a copy of the model",
    "start": "2652239",
    "end": "2659349"
  },
  {
    "text": "artifact and we snapshot of all of the performance characteristics of the model",
    "start": "2659349",
    "end": "2665950"
  },
  {
    "text": "whether they're operational characteristics in terms of latency and time taken to produce a prediction or the efficacy of",
    "start": "2665950",
    "end": "2674319"
  },
  {
    "text": "the model itself all of those are captured and linked to the specific version every time they retrain it",
    "start": "2674319",
    "end": "2680589"
  },
  {
    "text": "actually creates a new version of the model itself that you can promote all the way through at this point we haven't",
    "start": "2680589",
    "end": "2687760"
  },
  {
    "text": "integrated with sage makers ability to create a be tests but that's on our",
    "start": "2687760",
    "end": "2693160"
  },
  {
    "text": "roadmap so that you can start creating different versions of the model set up a B tests easily we have a feedback loop",
    "start": "2693160",
    "end": "2700599"
  },
  {
    "text": "that allows us to monitor the efficacy of the models and then switch traffic at at run time so in conclusion conclude",
    "start": "2700599",
    "end": "2715480"
  },
  {
    "text": "and wrap things up Amazon sage maker as you can see is a really versatile platform for building",
    "start": "2715480",
    "end": "2720760"
  },
  {
    "text": "training and deploying machine learning models at scale you know customers like intuitive at how really a lot of success",
    "start": "2720760",
    "end": "2727140"
  },
  {
    "text": "implementing it into their workflows with really impressive results and you're able to go out and explore sage",
    "start": "2727140",
    "end": "2733960"
  },
  {
    "text": "maker which is free tier eligible to build models of your own and with that we will take questions so thank you for",
    "start": "2733960",
    "end": "2741280"
  },
  {
    "text": "attending [Applause]",
    "start": "2741280",
    "end": "2748949"
  },
  {
    "text": "a question in the middle there I don't know if we have mics I don't seen him in that aisle sounds good",
    "start": "2757710",
    "end": "2766700"
  },
  {
    "text": "I said I think we went incrementally so a sage maker was announced at last vment",
    "start": "2776099",
    "end": "2782790"
  },
  {
    "text": "we started our development probably soon after that and then he started building",
    "start": "2782790",
    "end": "2788280"
  },
  {
    "text": "bits around integration with our own internal service mesh that probably came",
    "start": "2788280",
    "end": "2793530"
  },
  {
    "text": "soon then we built a set of tools docker and then the UI that I showed we had",
    "start": "2793530",
    "end": "2799470"
  },
  {
    "text": "that from our earlier system that we had built so we just repurposed it to also",
    "start": "2799470",
    "end": "2804930"
  },
  {
    "text": "work with sage maker so we went just like building incremental II and that took maybe four or five months",
    "start": "2804930",
    "end": "2814010"
  },
  {
    "text": "so that's a lot of data scientists who work on that they essentially look at",
    "start": "2826730",
    "end": "2832130"
  },
  {
    "text": "are we trying to like what's the kind of prediction we trying to do is it is it a",
    "start": "2832130",
    "end": "2837200"
  },
  {
    "text": "numerical or a regression value or maybe it's a more complex prediction so I'm",
    "start": "2837200",
    "end": "2845420"
  },
  {
    "text": "not as familiar with the data science algorithms myself just to just add on",
    "start": "2845420",
    "end": "2851779"
  },
  {
    "text": "what Prasad said right now automatic model tunings or type of parameter tuning but you know we kind of see",
    "start": "2851779",
    "end": "2859579"
  },
  {
    "text": "there's a lot of space there's no still a lot of research to be done in the auto machine learning space where you would",
    "start": "2859579",
    "end": "2866180"
  },
  {
    "text": "do things like pick between different algorithms and that's a space that we're kind of passionate about and thinking",
    "start": "2866180",
    "end": "2871940"
  },
  {
    "text": "about so it is a problem that you know we've seen some research papers on but",
    "start": "2871940",
    "end": "2878180"
  },
  {
    "text": "it's it's still an active area development so for right now what a lot of scientists do is you know they get to",
    "start": "2878180",
    "end": "2884510"
  },
  {
    "text": "the domain and they use some heuristics to understand which algorithms might be good and then they test them and with sage makers ability to spin up manage",
    "start": "2884510",
    "end": "2891920"
  },
  {
    "text": "training jobs in parallel it really enables that testing in a very quick",
    "start": "2891920",
    "end": "2897740"
  },
  {
    "text": "efficient way where you kick off multiple jobs at once to test lots of different algorithms against one another",
    "start": "2897740",
    "end": "2903230"
  },
  {
    "text": "on the same problem the question",
    "start": "2903230",
    "end": "2909309"
  },
  {
    "text": "so in terms of size we probably have somewhere between one and two petabytes",
    "start": "2914950",
    "end": "2920510"
  },
  {
    "text": "I think we have in terms of sources it's a lot probably more than a hundred from",
    "start": "2920510",
    "end": "2928820"
  },
  {
    "text": "different types of sources and then in terms of user base we might have around",
    "start": "2928820",
    "end": "2936100"
  },
  {
    "text": "for like a few hundred users across our analysts and data science community in",
    "start": "2936100",
    "end": "2941780"
  },
  {
    "text": "terms of our technology stack we use a lot of scoop and other open source",
    "start": "2941780",
    "end": "2946940"
  },
  {
    "text": "ingestion tools we have some tools that are built in houses well once it's once",
    "start": "2946940",
    "end": "2953630"
  },
  {
    "text": "we have ingested the data we have a lot of processing that happens using traditional MapReduce spark just regular",
    "start": "2953630",
    "end": "2961220"
  },
  {
    "text": "Python jobs for smaller data sets or loading into something like an Oracle or",
    "start": "2961220",
    "end": "2967400"
  },
  {
    "text": "a vertigo and then running transformations sequels on those and a lot of data that's available for",
    "start": "2967400",
    "end": "2973670"
  },
  {
    "text": "exploration is made available through hive as the primary query engine and there's also what occur that's used",
    "start": "2973670",
    "end": "2980300"
  },
  {
    "text": "heavily within our company",
    "start": "2980300",
    "end": "2984610"
  },
  {
    "text": "so just understand the question is around how we added like custom okay so",
    "start": "3007869",
    "end": "3014000"
  },
  {
    "text": "once we have the instance that spun up it's actually available as a rest",
    "start": "3014000",
    "end": "3019310"
  },
  {
    "text": "endpoint there this is available in the open source Jupiter world as well you",
    "start": "3019310",
    "end": "3025400"
  },
  {
    "text": "can invoke Jupiter to run custom scripts so what we do is once the instance is",
    "start": "3025400",
    "end": "3030619"
  },
  {
    "text": "booted up it actually emits lifecycle phases to say it's now ready or it's being booted up once we detect it's",
    "start": "3030619",
    "end": "3037550"
  },
  {
    "text": "ready we have a custom step that runs on it it downloads the artifacts that we need off of our s3 bucket where these",
    "start": "3037550",
    "end": "3044720"
  },
  {
    "text": "artifacts are stored and then runs those scripts to install our monitoring aging or modifying Kerberos config files",
    "start": "3044720",
    "end": "3053050"
  },
  {
    "text": "that's how we we do it docker image",
    "start": "3053050",
    "end": "3062750"
  },
  {
    "text": "during the hosting phase or",
    "start": "3062750",
    "end": "3067030"
  },
  {
    "text": "Kirk so for the training jobs itself we don't particularly have anything running",
    "start": "3079290",
    "end": "3085369"
  },
  {
    "text": "within the dark rip image in terms of authorization and security because when a training job is running it's virtually",
    "start": "3085369",
    "end": "3092060"
  },
  {
    "text": "inaccessible to the outside world there's no rest endpoint that that will that can't reach it so we just let it",
    "start": "3092060",
    "end": "3098360"
  },
  {
    "text": "run up and then spin down the only thing that can execute on the particular",
    "start": "3098360",
    "end": "3103520"
  },
  {
    "text": "darker image is stuff that gets executed so we do have a static monitoring tools",
    "start": "3103520",
    "end": "3110480"
  },
  {
    "text": "that kind of run a scan on the darker image to confirm there's nothing malicious embedded inside the container",
    "start": "3110480",
    "end": "3116150"
  },
  {
    "text": "itself",
    "start": "3116150",
    "end": "3118480"
  },
  {
    "text": "okay I said we have a very thin layer of UI that you saw and then behind the scenes",
    "start": "3130190",
    "end": "3137020"
  },
  {
    "text": "a lot of the heavy lifting is done using Jenkins because Jenkins provides a lot of the tools by default like monitoring",
    "start": "3137020",
    "end": "3142600"
  },
  {
    "text": "github repos pulling them down running something on them and then pushing to ECR Ivy runs Jenkins images in AWS so",
    "start": "3142600",
    "end": "3152520"
  },
  {
    "text": "that'll makes it easy to give it permissions to push to ECR or execute",
    "start": "3152520",
    "end": "3158980"
  },
  {
    "text": "commands and sage maker to create endpoints and things like that so Jenkins is the primary heavy lifter of",
    "start": "3158980",
    "end": "3164200"
  },
  {
    "text": "the CICE pipeline not particularly",
    "start": "3164200",
    "end": "3172990"
  },
  {
    "text": "because Jenkins is is pretty powerful and flexible like wherever we don't find",
    "start": "3172990",
    "end": "3179590"
  },
  {
    "text": "plugins we can always run a shell script to do what we need to do and just to add",
    "start": "3179590",
    "end": "3184660"
  },
  {
    "text": "to that we have a couple of github repositories in AWS that actually are on",
    "start": "3184660",
    "end": "3190720"
  },
  {
    "text": "CI CD pipelines so sage build is one of",
    "start": "3190720",
    "end": "3195850"
  },
  {
    "text": "them and so that's in the AWS samples repository I believe and that kind of",
    "start": "3195850",
    "end": "3201490"
  },
  {
    "text": "sets up a cloud for that has a confirmation template that would set up something to use code commit and code deploy that will allow you to kind of",
    "start": "3201490",
    "end": "3208450"
  },
  {
    "text": "have your own CI CD pipeline within stage maker that's a person able yes but we also see a lot of customers using",
    "start": "3208450",
    "end": "3213910"
  },
  {
    "text": "some site Jenkins and another external tools so it's really more about what works for your it add in in case of like",
    "start": "3213910",
    "end": "3222100"
  },
  {
    "text": "Jenkins we also it integrates with our other enterprise tools which help in like scanning and monitoring builds and",
    "start": "3222100",
    "end": "3229180"
  },
  {
    "text": "stuff like that",
    "start": "3229180",
    "end": "3231599"
  },
  {
    "text": "yep those are all safe so those are sage maker environments so what we do is if data scientist comes in and says I want",
    "start": "3237750",
    "end": "3244360"
  },
  {
    "text": "to create model a and I'm publishing this to a particular let's say dev",
    "start": "3244360",
    "end": "3249880"
  },
  {
    "text": "environment what we do is we've templatized the endpoint name to say there's the endpoint name that the data",
    "start": "3249880",
    "end": "3255820"
  },
  {
    "text": "scientist actually chose appended with a version number and append the environment to it so what gets spun up",
    "start": "3255820",
    "end": "3263410"
  },
  {
    "text": "in AWS is actually my endpoint - zero point one - dev right so yep so that",
    "start": "3263410",
    "end": "3273580"
  },
  {
    "text": "that's another thing that that we do behind the scenes to help us manage those versions that's right that's right",
    "start": "3273580",
    "end": "3285640"
  },
  {
    "text": "so we have I think that the functional tests are all automated at this point we",
    "start": "3285640",
    "end": "3291430"
  },
  {
    "text": "are in the process of automating the performance tests today the performance tests are run manually and then our perf",
    "start": "3291430",
    "end": "3296950"
  },
  {
    "text": "engineers look at the results they talk to the product team which is integrating with the particular model to see is the",
    "start": "3296950",
    "end": "3303610"
  },
  {
    "text": "latency good enough we also rely a lot on cloud watch metrics because cloud watch gives us us exactly what was the",
    "start": "3303610",
    "end": "3310060"
  },
  {
    "text": "overhead added by like sage maker has a very thin Network layer in between and",
    "start": "3310060",
    "end": "3316600"
  },
  {
    "text": "then the time taken by our docker container and the model code itself so we know what needs to be tuned we also",
    "start": "3316600",
    "end": "3323680"
  },
  {
    "text": "get logs printed out from a docker container I think they show up as cloud",
    "start": "3323680",
    "end": "3329020"
  },
  {
    "text": "watch logs as well so we can instrument inside the docker container a little bit",
    "start": "3329020",
    "end": "3334600"
  },
  {
    "text": "to see what's happening and give us clues on optimizing them the Queen's are the question",
    "start": "3334600",
    "end": "3341370"
  },
  {
    "text": "yes so we just launched a model search functionality the right before",
    "start": "3345440",
    "end": "3350900"
  },
  {
    "text": "Thanksgiving actually and so you can tag your models using AWS tags and that's",
    "start": "3350900",
    "end": "3356840"
  },
  {
    "text": "how we see a lot of people trying to you know track their provenance of which",
    "start": "3356840",
    "end": "3362120"
  },
  {
    "text": "data sets reused which training jobs created this model and so this this model search functionality is in beta",
    "start": "3362120",
    "end": "3368180"
  },
  {
    "text": "it's in the console so if you go to the AWS age maker console and click search you can see your past training jobs and",
    "start": "3368180",
    "end": "3375020"
  },
  {
    "text": "models and understand some provenance there",
    "start": "3375020",
    "end": "3379270"
  },
  {
    "text": "yeah I got so that's a good question I",
    "start": "3400510",
    "end": "3407510"
  },
  {
    "text": "didn't cover it in the slides the the setup tool start docker is made",
    "start": "3407510",
    "end": "3412730"
  },
  {
    "text": "available as a Python library so you can just when you start up your environment",
    "start": "3412730",
    "end": "3418250"
  },
  {
    "text": "you'll say pip install setup tools docker and then that's available in your local Python repository and then you can",
    "start": "3418250",
    "end": "3424280"
  },
  {
    "text": "start opening it up and and adding things on top of it and we publish that",
    "start": "3424280",
    "end": "3432080"
  },
  {
    "text": "to our own internal pi PI repository so it's available to all data scientists similar to how in the Java earlier our",
    "start": "3432080",
    "end": "3439849"
  },
  {
    "text": "Nexus repository that all the libraries are available you can just pull that in and then start using it",
    "start": "3439849",
    "end": "3447190"
  },
  {
    "text": "yeah",
    "start": "3456030",
    "end": "3459030"
  },
  {
    "text": "no like data wrangling is its own problem this is for use cases where we",
    "start": "3461990",
    "end": "3468780"
  },
  {
    "text": "already had data available and we just had to like earlier we were constrained",
    "start": "3468780",
    "end": "3474210"
  },
  {
    "text": "by hardware resources or training jobs would take too long to run things like",
    "start": "3474210",
    "end": "3481380"
  },
  {
    "text": "that",
    "start": "3481380",
    "end": "3483650"
  },
  {
    "text": "so with sage maker there's three parts to it right there's a note books on the",
    "start": "3498029",
    "end": "3504569"
  },
  {
    "text": "algorithms and hosting so the the note books are there wasn't too much of a",
    "start": "3504569",
    "end": "3510479"
  },
  {
    "text": "learning curve because Jupiter is a fairly well established on note book that's available in the open source community so that was pretty up what",
    "start": "3510479",
    "end": "3519119"
  },
  {
    "text": "took longer was for them to learn how to use our tools to actually launch those",
    "start": "3519119",
    "end": "3524689"
  },
  {
    "text": "and then how to integrate with our existing data sources because it's a",
    "start": "3524689",
    "end": "3530129"
  },
  {
    "text": "little bit different than how they still they used to do it in our data center versus in AWS so that was the learning",
    "start": "3530129",
    "end": "3537569"
  },
  {
    "text": "curve in terms of the algorithms I think a lot of the algorithms are available in",
    "start": "3537569",
    "end": "3544049"
  },
  {
    "text": "the open source so Amazon provides fairly decent documentation and how to use it so that was manageable to hosting",
    "start": "3544049",
    "end": "3552659"
  },
  {
    "text": "we don't expose to our data scientists too much our engineering teams tend to take care of those things engineering",
    "start": "3552659",
    "end": "3558539"
  },
  {
    "text": "and operations just to add that we do have some you know ways to get up and",
    "start": "3558539",
    "end": "3565379"
  },
  {
    "text": "running with sage maker when you stand up sage maker notebook instance it's pre-populated with a bunch of example",
    "start": "3565379",
    "end": "3570539"
  },
  {
    "text": "notebooks that walk you through kind of get you started you know now that it's been around for a year we've got some",
    "start": "3570539",
    "end": "3575879"
  },
  {
    "text": "good blog content out there that talks about how to do a variety of things in sage maker and the ml certification that",
    "start": "3575879",
    "end": "3583649"
  },
  {
    "text": "we we just announced could be another another way that you can learn more about that kind of thing all right I",
    "start": "3583649",
    "end": "3590309"
  },
  {
    "text": "think I'll be staying around for a few more minutes but our time is up so we'll we'll leave the stage now but we'll be",
    "start": "3590309",
    "end": "3595799"
  },
  {
    "text": "around in case you do have additional questions thank you guys thank you [Applause]",
    "start": "3595799",
    "end": "3603130"
  }
]