[
  {
    "text": "[Music]",
    "start": "0",
    "end": "13449"
  },
  {
    "text": "[Music]",
    "start": "15540",
    "end": "50880"
  },
  {
    "text": "[Music]",
    "start": "61770",
    "end": "111548"
  },
  {
    "text": "[Music]",
    "start": "116060",
    "end": "123030"
  },
  {
    "text": "[Applause] [Music]",
    "start": "124400",
    "end": "135780"
  },
  {
    "text": "just keep telling me",
    "start": "137390",
    "end": "141290"
  },
  {
    "text": "[Music]",
    "start": "144380",
    "end": "166569"
  },
  {
    "text": "leave it up to me and now blow [Music]",
    "start": "167680",
    "end": "197350"
  },
  {
    "text": "[Music] [Applause]",
    "start": "202800",
    "end": "209039"
  },
  {
    "text": "all right I think we can get going here sorry for the delay I'll quickly",
    "start": "214520",
    "end": "221340"
  },
  {
    "text": "introduce myself my name is Toni Gibbs I'm a data warehousing specialist solution architect here at Amazon Web",
    "start": "221340",
    "end": "226830"
  },
  {
    "text": "Services and we've taken you guys through these best practices with redshift I've added a little bit of",
    "start": "226830",
    "end": "233930"
  },
  {
    "text": "legacy migration twist to it but if you're not doing migration a legacy",
    "start": "233930",
    "end": "240570"
  },
  {
    "text": "migration this is fairly generalized so you'll still get something out of this",
    "start": "240570",
    "end": "245810"
  },
  {
    "text": "so let's just go over the agenda I'm gonna start out by going over architectural concepts this is just the",
    "start": "245810",
    "end": "251880"
  },
  {
    "text": "basics of redshift how it works so for some of you if you've been using redshift",
    "start": "251880",
    "end": "257190"
  },
  {
    "text": "this will just be review I'm gonna cover data storage ingestion and elt and",
    "start": "257190",
    "end": "263430"
  },
  {
    "text": "redshift we're gonna then talk about workload management I'll touch on our new Auto wlm that we are rolling out",
    "start": "263430",
    "end": "271320"
  },
  {
    "text": "across the fleet actually right now a little bit on cluster sizing resizing",
    "start": "271320",
    "end": "276870"
  },
  {
    "text": "our new elastic resize I'll cover that then I'm gonna just kind of do a quick summary of some of the tips and tricks",
    "start": "276870",
    "end": "283290"
  },
  {
    "text": "and things like that that I've seen on migrations give you guys some additional",
    "start": "283290",
    "end": "288600"
  },
  {
    "text": "resources there won't be open Q&A because of the format of this talk but I",
    "start": "288600",
    "end": "294510"
  },
  {
    "text": "will take Q&A out in the hallway afterwards so if you guys have questions I'll stand around and answer whatever it",
    "start": "294510",
    "end": "300930"
  },
  {
    "text": "is that you have out in the hall all right I always like to just kind of see",
    "start": "300930",
    "end": "306540"
  },
  {
    "text": "where you guys are at how many of you here use redshift today okay reasonable",
    "start": "306540",
    "end": "313830"
  },
  {
    "text": "number of you how many of you are looking at migrating to redshift because",
    "start": "313830",
    "end": "320910"
  },
  {
    "text": "maybe you have some sort of legacy on-premise system awesome okay cool well",
    "start": "320910",
    "end": "330050"
  },
  {
    "text": "start out here with the architectural concepts redshift really kind of starts out from post graphs so if you are",
    "start": "330050",
    "end": "338880"
  },
  {
    "text": "familiar with Postgres sequel you're gonna feel right at home with redshift it uses almost the exact same sequel",
    "start": "338880",
    "end": "344820"
  },
  {
    "text": "syntax we obviously rewrote the storage engine to be entirely column ER so we",
    "start": "344820",
    "end": "351120"
  },
  {
    "text": "store the data on disk column by column rather than row by row we also made redshift MPP that's massively parallel",
    "start": "351120",
    "end": "358890"
  },
  {
    "text": "processing so it allows redshift to horizontally scale out up to 128 compute",
    "start": "358890",
    "end": "366120"
  },
  {
    "text": "nodes and storing two petabytes of raw storage we added a lot of OLAP",
    "start": "366120",
    "end": "371640"
  },
  {
    "text": "functionality as well to redshift so these are things like windowing functions approximate functions that",
    "start": "371640",
    "end": "376650"
  },
  {
    "text": "sort of thing the types of functions that you use in an analytics data warehouse we wrapped all of this up in",
    "start": "376650",
    "end": "384540"
  },
  {
    "text": "the AWS ecosystem so it's in tight integration with things like ec2 or s3 I",
    "start": "384540",
    "end": "391919"
  },
  {
    "text": "am that sort of thing and it really is this combination of all of these components that make redshift what it is",
    "start": "391919",
    "end": "401180"
  },
  {
    "text": "we release redshift on February 2013 on Valentine's Day to be specific and since",
    "start": "401180",
    "end": "408960"
  },
  {
    "text": "that time we've continued to innovate and add new features we add these features every two weeks typically so",
    "start": "408960",
    "end": "415740"
  },
  {
    "text": "every two weeks there's a maintenance window we roll out the latest version of redshift you have a choice to either use the",
    "start": "415740",
    "end": "422100"
  },
  {
    "text": "current version or to lag one version behind if you want to do your own testing and such so that's we call that",
    "start": "422100",
    "end": "428310"
  },
  {
    "text": "the trailing patch of the current redshift like I mentioned is a classic",
    "start": "428310",
    "end": "434940"
  },
  {
    "text": "MPP architecture I'm going to start out by explaining this from that green box up there at the top you can connect to",
    "start": "434940",
    "end": "440700"
  },
  {
    "text": "redshift using JDBC or ODBC drivers which we supply or optionally you can",
    "start": "440700",
    "end": "447419"
  },
  {
    "text": "also use the open source Postgres drivers they still work with redshift so if say you have some Python code that",
    "start": "447419",
    "end": "453990"
  },
  {
    "text": "you want to connect to redshift maybe you're doing some work with R or that sort of thing you can use the open",
    "start": "453990",
    "end": "459660"
  },
  {
    "text": "source Postgres drivers to make those connections and work with those applications you connect to something",
    "start": "459660",
    "end": "465570"
  },
  {
    "text": "that we call the leader node the leader node really acts as just it's a query coordinator it holds a bunch of metadata the catalog",
    "start": "465570",
    "end": "472260"
  },
  {
    "text": "resides on the leader node and what will happen is is when a query hits redshift that leader node will send that the code",
    "start": "472260",
    "end": "478470"
  },
  {
    "text": "down to the compute nodes and every one of those compute nodes will execute that query in parallel so",
    "start": "478470",
    "end": "485370"
  },
  {
    "text": "that's the MPP share nothing part of this architecture all of the data obviously resides across all of these",
    "start": "485370",
    "end": "490920"
  },
  {
    "text": "compute nodes hopefully spread evenly out I'll explain about 15 slides from",
    "start": "490920",
    "end": "497640"
  },
  {
    "text": "now how we get data across those compute nodes underneath the compute nodes",
    "start": "497640",
    "end": "503870"
  },
  {
    "text": "redshift typically talks to s3 you're going to typically load data off of s3 you're also going to unload data if you",
    "start": "503870",
    "end": "510420"
  },
  {
    "text": "want to remove data from your cluster because you want to do something else with it behind the scenes were constantly",
    "start": "510420",
    "end": "516210"
  },
  {
    "text": "backing data up in redshift it's a fully managed service so the data there's these automatic backups and we're always",
    "start": "516210",
    "end": "522330"
  },
  {
    "text": "flushing that data to s3 you can also if you wanted to restore a cluster or spin",
    "start": "522330",
    "end": "527430"
  },
  {
    "text": "up an entirely new cluster based on the backup about two years ago we release a",
    "start": "527430",
    "end": "533340"
  },
  {
    "text": "feature to redshift that we call Amazon redshift spectrum spectrum is this extra",
    "start": "533340",
    "end": "538860"
  },
  {
    "text": "layer of compute nodes that sits between your cluster and s3 and allows you to",
    "start": "538860",
    "end": "545780"
  },
  {
    "text": "query s3 directly so think of this like external tables where the data resides",
    "start": "545780",
    "end": "552330"
  },
  {
    "text": "on s3 and you can execute queries directly against it some of our customers also use this spectrum",
    "start": "552330",
    "end": "559110"
  },
  {
    "text": "functionality to load data through it as well so it's basically for querying and",
    "start": "559110",
    "end": "564510"
  },
  {
    "text": "loading data so the first piece of terminology I'm going to introduce you",
    "start": "564510",
    "end": "570180"
  },
  {
    "text": "to is column er it's pretty simple it's really just storing the disk data on disk column by column and the reason",
    "start": "570180",
    "end": "576780"
  },
  {
    "text": "redshift uses this architecture is because when you're running analytics queries we're typically gonna be reading",
    "start": "576780",
    "end": "583770"
  },
  {
    "text": "many rows only across a subset of your columns so you're gonna be aggregating maybe you know millions if not billions",
    "start": "583770",
    "end": "591180"
  },
  {
    "text": "of records and you're just gonna do that across you know maybe a fraction of the number of columns in your table just to",
    "start": "591180",
    "end": "597810"
  },
  {
    "text": "illustrate this say if we have this really simple example I have a simple three column table this deep dive table",
    "start": "597810",
    "end": "604620"
  },
  {
    "text": "some data in it and if I wanted to just execute this very simple sequel query I'm just selecting the minimum date here",
    "start": "604620",
    "end": "611010"
  },
  {
    "text": "in a row based database assuming I don't have an index to help out I would end up having to read all of",
    "start": "611010",
    "end": "617430"
  },
  {
    "text": "the data off disk in this table in order to find that minimum date because you have to read each row go to the seek to",
    "start": "617430",
    "end": "624600"
  },
  {
    "text": "the column you're looking for and extract that value and figure out what the min is in redshift on the other hand",
    "start": "624600",
    "end": "631110"
  },
  {
    "text": "we can just scan down that single column so we can just read that we only need to read the date column off disk and that's",
    "start": "631110",
    "end": "638009"
  },
  {
    "text": "one of the ways that redshift reduces the amount of i/o that needs to be done to when executing queries so the next",
    "start": "638009",
    "end": "648029"
  },
  {
    "text": "piece of terminology compression compression is largely handled for you now in redshift basically the first time",
    "start": "648029",
    "end": "655649"
  },
  {
    "text": "you load data in you end up getting some compression settings applied to your tables that's automatic you can always",
    "start": "655649",
    "end": "662430"
  },
  {
    "text": "apply compression manually to redshift as well and we have a command built-in",
    "start": "662430",
    "end": "668220"
  },
  {
    "text": "called analyze compression that will analyze all of the data in your table or a subset of it actually and what it'll",
    "start": "668220",
    "end": "675300"
  },
  {
    "text": "do is it'll pick it up it'll compress it with every one of our encoding types and it'll give you back the smallest what's",
    "start": "675300",
    "end": "683940"
  },
  {
    "text": "going to result in the smallest amount of data on disk we usually recommend compression pretty much on every table",
    "start": "683940",
    "end": "690449"
  },
  {
    "text": "it has two benefits one it allows you to store more data in your cluster and the second benefit is it actually improves",
    "start": "690449",
    "end": "697110"
  },
  {
    "text": "performance because we're reducing i/o",
    "start": "697110",
    "end": "701810"
  },
  {
    "text": "so just as an example of how compression works in redshift we have our deep dive",
    "start": "702589",
    "end": "707730"
  },
  {
    "text": "table up there you can manually apply compression settings there so with the",
    "start": "707730",
    "end": "713100"
  },
  {
    "text": "encode statement which you can see that I've written up there the other important thing is is that the encoding",
    "start": "713100",
    "end": "718319"
  },
  {
    "text": "is column by column in redshift so different columns can have different compression types and that means each",
    "start": "718319",
    "end": "725220"
  },
  {
    "text": "column can grow and shrink independently so our best practices on compression",
    "start": "725220",
    "end": "731839"
  },
  {
    "text": "basically apply it or attempt to to apply it to every table if the analyze",
    "start": "731839",
    "end": "737850"
  },
  {
    "text": "compression command ends up spitting out raw as the recommendation it's probably",
    "start": "737850",
    "end": "744029"
  },
  {
    "text": "because the table is really small and it doesn't benefit compression or maybe that column",
    "start": "744029",
    "end": "749180"
  },
  {
    "text": "contains mostly nulls sometimes the sparse columns will end up coming back",
    "start": "749180",
    "end": "754620"
  },
  {
    "text": "with raw as well I'm a little bit of sequel written out here that I added that's where you get one of the system",
    "start": "754620",
    "end": "760680"
  },
  {
    "text": "tables where you can find the compression that's applied to an existing table next piece of terminology",
    "start": "760680",
    "end": "766889"
  },
  {
    "text": "blocks our blocks are really big and redshift they're one Meg blocks every",
    "start": "766889",
    "end": "772379"
  },
  {
    "text": "block is encoded with one of twelve different encoding types one of those encoding types being raw when we apply",
    "start": "772379",
    "end": "779430"
  },
  {
    "text": "compression to redshift you can technically have millions of values within a single block usually I'll see",
    "start": "779430",
    "end": "787459"
  },
  {
    "text": "250,000 values or so of you if I were to look in the system table and see how many values in there but if they you're",
    "start": "787459",
    "end": "793259"
  },
  {
    "text": "getting really good compression yeah definitely squeeze over a million values in a single block so the next piece of",
    "start": "793259",
    "end": "801569"
  },
  {
    "text": "terminology is own Maps this is really just metadata about the blocks the",
    "start": "801569",
    "end": "806939"
  },
  {
    "text": "important piece of metadata in here is the minimum and maximum values so redshift automatically stores the min",
    "start": "806939",
    "end": "814350"
  },
  {
    "text": "and Max value for every single block in the entire system we use that",
    "start": "814350",
    "end": "819689"
  },
  {
    "text": "information to reduce i/o and a little straight how that works in about three",
    "start": "819689",
    "end": "825029"
  },
  {
    "text": "slides from now the next piece of terminology is data sorting data sorting",
    "start": "825029",
    "end": "831389"
  },
  {
    "text": "is the actual physically physical sort of the data on disk and the primary",
    "start": "831389",
    "end": "836879"
  },
  {
    "text": "purpose of sorting data is to leverage the zone maps or make them more effective so the zone Maps I was talking",
    "start": "836879",
    "end": "842970"
  },
  {
    "text": "about the last slide we sort data to make the zone Maps more effective just",
    "start": "842970",
    "end": "849809"
  },
  {
    "text": "to illustrate sorting this is very simple we have a same table those rows you can see I'm gonna manually add the",
    "start": "849809",
    "end": "857370"
  },
  {
    "text": "sort key there so that you see the date location that's how I would sort this table by those and if I were to sort",
    "start": "857370",
    "end": "864269"
  },
  {
    "text": "that its first gonna sort by the date and then it's gonna sort by the location it's a very simple concept just",
    "start": "864269",
    "end": "870180"
  },
  {
    "text": "physically sorting the data by those two columns now tying this together the zone maps and the sort keys say if I have",
    "start": "870180",
    "end": "877079"
  },
  {
    "text": "these four blocks on disk and I have the zone maps printed out this each of the blocks there so the min and",
    "start": "877079",
    "end": "882270"
  },
  {
    "text": "Max values if I were to execute this very simple query just counting the number of Records on a particular day",
    "start": "882270",
    "end": "887880"
  },
  {
    "text": "what redshift is going to do it's going to check those own Maps first before reading blocks off disk and it's going",
    "start": "887880",
    "end": "894750"
  },
  {
    "text": "to be able to eliminate some of these blocks so in this case we've reduced Samael now if I were to sort this exact",
    "start": "894750",
    "end": "903120"
  },
  {
    "text": "same table by the date I'm gonna end up with the zone maps being in much better shape you can see that they all run in",
    "start": "903120",
    "end": "910410"
  },
  {
    "text": "sequential order now now in this case when I execute the query I know I only need to read just a single block so",
    "start": "910410",
    "end": "917250"
  },
  {
    "text": "there's sorting the tables makes the zone Maps more effective and thus reduces i/o that is the primary purpose",
    "start": "917250",
    "end": "923550"
  },
  {
    "text": "of sorting data and redshift our best practices usually what you're gonna do",
    "start": "923550",
    "end": "931680"
  },
  {
    "text": "is you're gonna add the sort key to the column or columns that you primarily filter on your the predicates and the",
    "start": "931680",
    "end": "937560"
  },
  {
    "text": "where clause because this is data warehousing it's almost always going to",
    "start": "937560",
    "end": "942750"
  },
  {
    "text": "be the time stamp on your fact table or a date so and that's because we're usually filtering or we have some sort",
    "start": "942750",
    "end": "949920"
  },
  {
    "text": "of where date between this value and this value so that's usually the sort key and if you don't not sure what to do",
    "start": "949920",
    "end": "956220"
  },
  {
    "text": "probably just apply to that one caches is if you do are going to be applying",
    "start": "956220",
    "end": "961950"
  },
  {
    "text": "sort keys to multiple columns as soon as you have a high cardinality value it doesn't make sense to add additional",
    "start": "961950",
    "end": "968610"
  },
  {
    "text": "columns after that usually as a rule of thumb I find three or four columns is",
    "start": "968610",
    "end": "974310"
  },
  {
    "text": "about the maximum that you also want to sort by so this one is I added this",
    "start": "974310",
    "end": "981959"
  },
  {
    "text": "slide actually primarily for migrations from legacy systems this top is sequel",
    "start": "981959",
    "end": "988410"
  },
  {
    "text": "example which I'll walk through is one that I usually see that customers kind of a hit when they're doing migrations",
    "start": "988410",
    "end": "994980"
  },
  {
    "text": "so we want to try and put our predicate columns on our fact tables which means a",
    "start": "994980",
    "end": "1001070"
  },
  {
    "text": "little bit more DD normalization than we may be used to so one of those is time",
    "start": "1001070",
    "end": "1007640"
  },
  {
    "text": "or date dimension tables which are very common in data warehousing in redshift",
    "start": "1007640",
    "end": "1013070"
  },
  {
    "text": "ideally we wanted that into our fact table so we can leverage the zone maps on the fact table",
    "start": "1013070",
    "end": "1019640"
  },
  {
    "text": "so in this these middle two lines of sequel here you can see that I'm how I",
    "start": "1019640",
    "end": "1025400"
  },
  {
    "text": "would query that if I had a date dimension table if I were to take that those values put them in the fact table",
    "start": "1025400",
    "end": "1030890"
  },
  {
    "text": "and run that second line is sequel it will run significantly faster than doing",
    "start": "1030890",
    "end": "1036770"
  },
  {
    "text": "that join even even if we've set the distribution keys and everything up perfectly that still will run much",
    "start": "1036770",
    "end": "1043819"
  },
  {
    "text": "faster because the zone maps are more effective the second one kind of applies to all data warehousing this is",
    "start": "1043820",
    "end": "1051470"
  },
  {
    "text": "basically try and materialize filtered columns don't run run run through",
    "start": "1051470",
    "end": "1058160"
  },
  {
    "text": "functions and that sort of thing and well with that obviously saves a work the main purpose again is is we want to",
    "start": "1058160",
    "end": "1064550"
  },
  {
    "text": "have the zone maps on the table so we can leverage those and that's what that second query will do so this concept is",
    "start": "1064550",
    "end": "1073160"
  },
  {
    "text": "slices this next one here is really important one it's how we get parallelism within a redshift cluster so",
    "start": "1073160",
    "end": "1081230"
  },
  {
    "text": "we divvy up each one of our compute nodes into either 2 or 16 slices and",
    "start": "1081230",
    "end": "1087950"
  },
  {
    "text": "those slices are where the data resides and there's a collection of processes that work against the data on each of",
    "start": "1087950",
    "end": "1094100"
  },
  {
    "text": "the slices so I'm going to kind of explain how we spread data out across",
    "start": "1094100",
    "end": "1100220"
  },
  {
    "text": "the redshift cluster we have three mechanisms sort of fournette now with our new disc style auto which I'll talk",
    "start": "1100220",
    "end": "1106910"
  },
  {
    "text": "a little bit about the first is is distribution style key and what key does",
    "start": "1106910",
    "end": "1112280"
  },
  {
    "text": "is is you pick one of the columns in the table to distribute by and what we do is",
    "start": "1112280",
    "end": "1118370"
  },
  {
    "text": "for each row we take the value we hash it that hash will correspond to one of",
    "start": "1118370",
    "end": "1124790"
  },
  {
    "text": "the slices in the cluster and the entire row then goes to that slice so let's",
    "start": "1124790",
    "end": "1130580"
  },
  {
    "text": "disc I'll key the next is a disk style even and what even does is maybe if you don't have a good key to distribute on",
    "start": "1130580",
    "end": "1137240"
  },
  {
    "text": "it's like just saying redshift please just distribute this data evenly for me and we just round-robin the data through",
    "start": "1137240",
    "end": "1143090"
  },
  {
    "text": "the cluster so that's kind of that was the way that I used to recommend customers to use dist I'll even talk",
    "start": "1143090",
    "end": "1150649"
  },
  {
    "text": "about Otto after I'm done with all all is a special case sort of it's meant for",
    "start": "1150649",
    "end": "1158570"
  },
  {
    "text": "small tables dimension tables primarily and what it does is it makes a complete",
    "start": "1158570",
    "end": "1163880"
  },
  {
    "text": "copy of the table on each node in the cluster so primarily small tables our",
    "start": "1163880",
    "end": "1170450"
  },
  {
    "text": "rule of thumb is usually three million records or less for dis style all what Otto does and it's the new default and",
    "start": "1170450",
    "end": "1177590"
  },
  {
    "text": "redshift we released it last December what it does is it basically will switch",
    "start": "1177590",
    "end": "1184580"
  },
  {
    "text": "automatically between all and even today so if you use dis style auto which is",
    "start": "1184580",
    "end": "1190159"
  },
  {
    "text": "the default and you have say like a few thousand rows that table will be dis tile all you continue to add data that",
    "start": "1190159",
    "end": "1196460"
  },
  {
    "text": "table and it passes a threshold it will flip to dis style even automatically in",
    "start": "1196460",
    "end": "1202279"
  },
  {
    "text": "the background so that's the new distribution style which is our new default so I'm going to walk you through",
    "start": "1202279",
    "end": "1208929"
  },
  {
    "text": "even key and all and explain how these work so let's just say are the same",
    "start": "1208929",
    "end": "1214639"
  },
  {
    "text": "table we've been working with we have four records here and I want to just",
    "start": "1214639",
    "end": "1219649"
  },
  {
    "text": "distribute these records with this style even I got this very simple insert statement here with even the records are",
    "start": "1219649",
    "end": "1226220"
  },
  {
    "text": "just going to evenly spread across the slices in the cluster here we have to",
    "start": "1226220",
    "end": "1231409"
  },
  {
    "text": "compute nodes and we have four slices so I'm very simple example now let's try",
    "start": "1231409",
    "end": "1239360"
  },
  {
    "text": "dis style key in this particular example what I've done is is you can see the DDL",
    "start": "1239360",
    "end": "1244519"
  },
  {
    "text": "syntax here i picked the location column there's a SFO and JFK SFO JFK those four",
    "start": "1244519",
    "end": "1250100"
  },
  {
    "text": "records there what I'm gonna do is I'm going to take that first SFO and I'm gonna it's going to get hashed and maybe",
    "start": "1250100",
    "end": "1256309"
  },
  {
    "text": "it corresponds to a slice zero here JFK maybe it happens to hash out to",
    "start": "1256309",
    "end": "1262039"
  },
  {
    "text": "slice one JFK is going to go back to slice zero and SFO obviously the same",
    "start": "1262039",
    "end": "1267799"
  },
  {
    "text": "location this is an example of a bad distribution key in redshift if I were",
    "start": "1267799",
    "end": "1273049"
  },
  {
    "text": "to execute a sequel query against this data what would end up happening is obviously node number two there has no",
    "start": "1273049",
    "end": "1280279"
  },
  {
    "text": "data it do any work so we want to that's not what we want to have happen we want to have something that looks more like what",
    "start": "1280279",
    "end": "1286710"
  },
  {
    "text": "happened when we use dis style even so I'm going to pick instead the audience",
    "start": "1286710",
    "end": "1292560"
  },
  {
    "text": "ID there and obviously the value is one two three four very high cardinality we",
    "start": "1292560",
    "end": "1298380"
  },
  {
    "text": "have looks like it's a primary key or something like that so in this particular case one might slice hash",
    "start": "1298380",
    "end": "1305280"
  },
  {
    "text": "there two might hash over to slice two may be that value goes there and the",
    "start": "1305280",
    "end": "1311040"
  },
  {
    "text": "fourth one goes there obviously I made the example with four rows work perfectly but if you have a large number",
    "start": "1311040",
    "end": "1316620"
  },
  {
    "text": "of Records we're talking millions of Records statistically the data is going to end up evenly spread across the",
    "start": "1316620",
    "end": "1323310"
  },
  {
    "text": "cluster so that's dis style key dis style all on the other hand what this",
    "start": "1323310",
    "end": "1331110"
  },
  {
    "text": "distribution style does is again we make a complete copy on each node in the cluster and what we do is each row comes",
    "start": "1331110",
    "end": "1337650"
  },
  {
    "text": "in is we write it actually to the first slice on each node so that's how this",
    "start": "1337650",
    "end": "1343380"
  },
  {
    "text": "style all works this is again for optimizing small tables and making joins",
    "start": "1343380",
    "end": "1348660"
  },
  {
    "text": "with them more efficient so just to recap I de style key we use",
    "start": "1348660",
    "end": "1355110"
  },
  {
    "text": "this with joints so the idea here is is if I have to really large tables we want",
    "start": "1355110",
    "end": "1361680"
  },
  {
    "text": "to co-locate the rows that we're joining on onto the same slice so with the on clause in your",
    "start": "1361680",
    "end": "1367890"
  },
  {
    "text": "sequel where you have that on and you're those two columns that you're going to be doing the joint on if those both of",
    "start": "1367890",
    "end": "1374490"
  },
  {
    "text": "those are the disk keys that will make the join significantly faster now",
    "start": "1374490",
    "end": "1380820"
  },
  {
    "text": "one catch is is we don't want to cause skew in the of the data so I have this",
    "start": "1380820",
    "end": "1386130"
  },
  {
    "text": "equal query written up here that is where you can see if there is skew happening with that table what that row",
    "start": "1386130",
    "end": "1394590"
  },
  {
    "text": "ratio is is it's the ratio between the slice with the most amount of data and",
    "start": "1394590",
    "end": "1399900"
  },
  {
    "text": "the slice with the least amount of data and ideally it should be close to 1.0 my",
    "start": "1399900",
    "end": "1405540"
  },
  {
    "text": "recommendation is a maximum of around 1.4 maybe 1.5 would be an absolute max",
    "start": "1405540",
    "end": "1413180"
  },
  {
    "text": "think of it like a skew of two that would mean there'd be twice as much data on one slice as another dish style all",
    "start": "1413180",
    "end": "1421700"
  },
  {
    "text": "my rule of thumb is three million rows or less it's usually safe to use dis style all as long as you're not doing a",
    "start": "1421700",
    "end": "1428240"
  },
  {
    "text": "large number of writes to that table dist I'll even I probably wouldn't use",
    "start": "1428240",
    "end": "1433760"
  },
  {
    "text": "that anymore unless you know for sure that it's gonna the tables going to end up being or needing to be dis style even",
    "start": "1433760",
    "end": "1440090"
  },
  {
    "text": "the new default dis style auto does a great job like I said it flips back and",
    "start": "1440090",
    "end": "1445280"
  },
  {
    "text": "forth or between all and even we have it set fairly low right now it's just 80k",
    "start": "1445280",
    "end": "1450710"
  },
  {
    "text": "records we've been changing that value behind the scenes as we get more data we're being cautious obviously with it",
    "start": "1450710",
    "end": "1457340"
  },
  {
    "text": "but if so if you know that you have a dimension table and it is under three million you can still safely use this",
    "start": "1457340",
    "end": "1463190"
  },
  {
    "text": "style so just a handful of this is",
    "start": "1463190",
    "end": "1469250"
  },
  {
    "text": "mostly summarization here of what I've already talked about obviously add compression to your tables add a sort",
    "start": "1469250",
    "end": "1475730"
  },
  {
    "text": "key to the columns that you've primarily filter on I talked about the materializing values in your fact table",
    "start": "1475730",
    "end": "1483070"
  },
  {
    "text": "just because I've seen it happen quite a few times avoid distribution keys on time base",
    "start": "1483070",
    "end": "1488570"
  },
  {
    "text": "things of some customers that will apply it to a date seems like it should work",
    "start": "1488570",
    "end": "1493880"
  },
  {
    "text": "you check the skew and you get nice even distribution of data and then you run a",
    "start": "1493880",
    "end": "1499880"
  },
  {
    "text": "query that maybe only is running on a single day and all of that day's data resides on a single slice and it ends up",
    "start": "1499880",
    "end": "1506720"
  },
  {
    "text": "causing what we call in-flight skew so it's just as a rule of thumb I usually don't add it on temporal columns the",
    "start": "1506720",
    "end": "1513560"
  },
  {
    "text": "last one here not really a summary item varchars chars I'm trying to keep them",
    "start": "1513560",
    "end": "1520190"
  },
  {
    "text": "as narrow as they need to be you don't have to go crazy with this it just uses a little bit more memory when we execute",
    "start": "1520190",
    "end": "1526760"
  },
  {
    "text": "queries if they're declared really long so as an example don't do varchar' you",
    "start": "1526760",
    "end": "1532940"
  },
  {
    "text": "know 1024 on you know a state abbreviation for example it's just wasteful you know if you make a bar",
    "start": "1532940",
    "end": "1538670"
  },
  {
    "text": "chart an or something like that totally fine so let's go into data storage so we",
    "start": "1538670",
    "end": "1547120"
  },
  {
    "text": "don't really advertise this one in our Docs or anything like that but the compute nodes are significantly larger",
    "start": "1547120",
    "end": "1552549"
  },
  {
    "text": "than what we advertise we have a lot more storage on them and the reason we",
    "start": "1552549",
    "end": "1557710"
  },
  {
    "text": "do that is the space we advertise is what you actually get for your data we",
    "start": "1557710",
    "end": "1564730"
  },
  {
    "text": "mirror all the data in the cluster so when you write data to one node it gets written to another node in the cluster",
    "start": "1564730",
    "end": "1570990"
  },
  {
    "text": "we obviously have an operating system on redshift there's temp and all of that kind of stuff carved a aside so that's",
    "start": "1570990",
    "end": "1578440"
  },
  {
    "text": "why the compute nodes have a lot more space than what we say once the data is",
    "start": "1578440",
    "end": "1585970"
  },
  {
    "text": "committed like I said it is on two nodes in the cluster we also asynchronously then back that",
    "start": "1585970",
    "end": "1592539"
  },
  {
    "text": "data up to s3 so every five gigs of changed data which is actually a very small amount of data in redshift or",
    "start": "1592539",
    "end": "1599110"
  },
  {
    "text": "eight hours whichever happens first that will trigger the flush to s3",
    "start": "1599110",
    "end": "1604169"
  },
  {
    "text": "one exception this is temporary tables and I like to call this one out because they you can write data to them",
    "start": "1604169",
    "end": "1611440"
  },
  {
    "text": "significantly faster so if you're needing staging tables things like that",
    "start": "1611440",
    "end": "1616659"
  },
  {
    "text": "or things that really are temporary in nature definitely leverage temp tables because they rights way faster so",
    "start": "1616659",
    "end": "1625090"
  },
  {
    "text": "redshift is an acid compliant it is fully transactional are we really only",
    "start": "1625090",
    "end": "1630940"
  },
  {
    "text": "have one isolation level in redshift though it's serializable you can request the other ones you'll just end up getting a serializable isolation level",
    "start": "1630940",
    "end": "1639010"
  },
  {
    "text": "our commits are there's two phases to them well obviously it's an MPP data warehouse we have many nodes so there's",
    "start": "1639010",
    "end": "1646179"
  },
  {
    "text": "a local phase to the commit and a global phase because of that commits are more",
    "start": "1646179",
    "end": "1652120"
  },
  {
    "text": "expensive in redshift than they are in say a regular just database with SMP",
    "start": "1652120",
    "end": "1657159"
  },
  {
    "text": "database one thing that customers also don't realize is that the DDL is",
    "start": "1657159",
    "end": "1662530"
  },
  {
    "text": "transactional in redshift so you can actually begin a transaction do DDL and roll it back if you just if you don't",
    "start": "1662530",
    "end": "1669850"
  },
  {
    "text": "declare a transaction though what will end up happening is as redshift if you have auto commit on will end up doing",
    "start": "1669850",
    "end": "1676060"
  },
  {
    "text": "transaction for every DDL statement that can get expensive with a lot if there's",
    "start": "1676060",
    "end": "1682600"
  },
  {
    "text": "a large number of transactions so what we usually will recommend and you probably want this for correctness anyways is to wrap workflows in an",
    "start": "1682600",
    "end": "1689980"
  },
  {
    "text": "explicit transaction the copy statement",
    "start": "1689980",
    "end": "1696400"
  },
  {
    "text": "and redshift is the primary way of ingesting data it's gonna walk you",
    "start": "1696400",
    "end": "1702040"
  },
  {
    "text": "through kind of how it works let's just say we have one of these compute nodes this is our dc28 Excel compute node it",
    "start": "1702040",
    "end": "1709120"
  },
  {
    "text": "is 16 slices it's one of the bigger ones and if say we have a single file here on",
    "start": "1709120",
    "end": "1714700"
  },
  {
    "text": "disk if you were to issue or that file will say as on s3 you were to issue a",
    "start": "1714700",
    "end": "1720850"
  },
  {
    "text": "copy statement against this cluster what's going to end up happening is red chips gonna look at s3 it's gonna go",
    "start": "1720850",
    "end": "1726100"
  },
  {
    "text": "okay I got one file and it's gonna have in this case the slice zero is going to reach out pull that file up it's gonna",
    "start": "1726100",
    "end": "1732520"
  },
  {
    "text": "stream it it's going to parse it and it's going to distribute the data across all the slices it's not really that",
    "start": "1732520",
    "end": "1737650"
  },
  {
    "text": "efficient if we wanted to get some parallelism out of this and we broke that same file up into sixteen chunks",
    "start": "1737650",
    "end": "1744550"
  },
  {
    "text": "because we have sixteen slices now what's going to happen is every one of the slices in the cluster is gonna reach",
    "start": "1744550",
    "end": "1751810"
  },
  {
    "text": "out in parallel to s3 and it's going to pull that file up and then it's going to parse it distribute it and get it",
    "start": "1751810",
    "end": "1757780"
  },
  {
    "text": "written to disk and this actually scales linearly so you'll get a close very close to a 16x performance improvement",
    "start": "1757780",
    "end": "1765970"
  },
  {
    "text": "by splitting the files our rule of thumb one Meg files to one gig somewhere in",
    "start": "1765970",
    "end": "1772720"
  },
  {
    "text": "that range after you apply gzip compression we do now have Z standard compression I",
    "start": "1772720",
    "end": "1778180"
  },
  {
    "text": "haven't actually tried that out but I'll just kind of throw it out it has a higher compression rate then gzip and",
    "start": "1778180",
    "end": "1785020"
  },
  {
    "text": "it's also technically faster to decompress I don't have numbers on it it's just a brand new thing that we just",
    "start": "1785020",
    "end": "1790960"
  },
  {
    "text": "released so as a these are kind of some",
    "start": "1790960",
    "end": "1796630"
  },
  {
    "text": "things that I've encountered when helping customers work through actually mostly migrations from other systems",
    "start": "1796630",
    "end": "1803070"
  },
  {
    "text": "pick a simple delimiter if you're using CSV files which is what most customers use don't",
    "start": "1803070",
    "end": "1808809"
  },
  {
    "text": "try and use crazy on printable characters picking bells or utf-8",
    "start": "1808809",
    "end": "1814509"
  },
  {
    "text": "characters just pick something to let simple like a pipe or a comma you can escape everything that you need to wrap",
    "start": "1814509",
    "end": "1820960"
  },
  {
    "text": "strings and double quotes and that sort of thing pick a simple null character usually you",
    "start": "1820960",
    "end": "1828370"
  },
  {
    "text": "know backslash capital n works well I've seen some customers for some reason use",
    "start": "1828370",
    "end": "1833649"
  },
  {
    "text": "backslash little n that means a newline and that doesn't work so again just pick simple stuff",
    "start": "1833649",
    "end": "1840490"
  },
  {
    "text": "one thing I'll call out is a utf-8 characters they take up four bytes in redshift and when you declare a varchar'",
    "start": "1840490",
    "end": "1848220"
  },
  {
    "text": "that actually is the length in bytes so if you are going to be storing utf-8",
    "start": "1848220",
    "end": "1853690"
  },
  {
    "text": "characters you'll have to extend the the length of your varchars to compensate",
    "start": "1853690",
    "end": "1860850"
  },
  {
    "text": "redshift spectrum I mentioned when we were kind of covering the architecture that customers use this for ingesting",
    "start": "1861389",
    "end": "1870009"
  },
  {
    "text": "data one nice thing about this is especially if you have parquet data on s3 and you only want to ingest a subset",
    "start": "1870009",
    "end": "1877330"
  },
  {
    "text": "of the columns that's actually a great way to ingest data so you can do an insert into with a select statement and",
    "start": "1877330",
    "end": "1883809"
  },
  {
    "text": "that select can do all sorts of things you could select only some of the columns you can filter data because you",
    "start": "1883809",
    "end": "1889840"
  },
  {
    "text": "can put a where clause you can aggregate data as it's coming in on so it gives a lot of flexibility a lot more",
    "start": "1889840",
    "end": "1896019"
  },
  {
    "text": "flexibility in the copy statement so if you're wanting to do some sort of transformations or filtering definitely",
    "start": "1896019",
    "end": "1902499"
  },
  {
    "text": "leverage spectrum for that redshift is",
    "start": "1902499",
    "end": "1908440"
  },
  {
    "text": "designed for big data it is a batch processing it primarily does batch processing so because of that a small",
    "start": "1908440",
    "end": "1916029"
  },
  {
    "text": "right in redshift because of our large blocks is actually just as expensive as",
    "start": "1916029",
    "end": "1921279"
  },
  {
    "text": "writing a large amount of data so micro batching is completely fine and",
    "start": "1921279",
    "end": "1926590"
  },
  {
    "text": "redshift a lot of customers will do like 1-minute micro batching increments which is fine but just be aware of that if",
    "start": "1926590",
    "end": "1933249"
  },
  {
    "text": "single row inserts for example isn't something you want to do in redshift you want to leverage the copy statement in",
    "start": "1933249",
    "end": "1938769"
  },
  {
    "text": "bulk load updates and deletes are covered when you delete data and redshift we",
    "start": "1938769",
    "end": "1945609"
  },
  {
    "text": "actually just tagged records for deletion we don't actually remove it right away updates what updates do is they'll tag",
    "start": "1945609",
    "end": "1953409"
  },
  {
    "text": "the record for deletion and they'll add a new record to the end of the table okay redshift uses multi-version",
    "start": "1953409",
    "end": "1959739"
  },
  {
    "text": "concurrency control so that's what the reason it works like that we do need to clean those up auto vacuum is in red",
    "start": "1959739",
    "end": "1967299"
  },
  {
    "text": "shift for deletes and so the auto vacuum system will clean up the deleted records",
    "start": "1967299",
    "end": "1973440"
  },
  {
    "text": "this is a very frequent question I have on how to do basically merging of data",
    "start": "1973440",
    "end": "1980499"
  },
  {
    "text": "or up certs especially when migrations are in flight and you have some existing",
    "start": "1980499",
    "end": "1985629"
  },
  {
    "text": "logic so I'll kind of walk through was the same deep dive table same four rows and let's just say we have some CSV file",
    "start": "1985629",
    "end": "1992679"
  },
  {
    "text": "the CSV file is on s3 and you're loading that pot and in this file we find we",
    "start": "1992679",
    "end": "1998379"
  },
  {
    "text": "want to update to records chests changing a couple values and we're going to add two new records in the CSV file",
    "start": "1998379",
    "end": "2005159"
  },
  {
    "text": "so how do we do that the copy statement is append only so we have to kind of work around this the workflow is to load",
    "start": "2005159",
    "end": "2013619"
  },
  {
    "text": "that data into a staging table then what we want to do is we want to delete the data out of the production table that",
    "start": "2013619",
    "end": "2020879"
  },
  {
    "text": "matches and then insert all of the data over from our staging table into our product Abel so I mentioned the",
    "start": "2020879",
    "end": "2029639"
  },
  {
    "text": "transactions and wrapping things in a transaction so I'm going to start out with the beginning statement create a new transaction then I'm going to create",
    "start": "2029639",
    "end": "2037259"
  },
  {
    "text": "a temporary table so the reason why is remember temporary tables twice as fast",
    "start": "2037259",
    "end": "2043009"
  },
  {
    "text": "then I'm going to use this like keyword I'm gonna copy over all of the settings",
    "start": "2043009",
    "end": "2049888"
  },
  {
    "text": "from the previous deep dive table so this is gonna copy over the compression settings it's also going to copy over",
    "start": "2049889",
    "end": "2056128"
  },
  {
    "text": "the distribution key when we insert data from one table to another they have the same distribution key it's gonna be a",
    "start": "2056129",
    "end": "2062128"
  },
  {
    "text": "faster copy of data I'm going to use the copy statement here I've turned off",
    "start": "2062129",
    "end": "2068309"
  },
  {
    "text": "compression or analyzing compression automatically because we've already said it from the with a light command then",
    "start": "2068309",
    "end": "2075450"
  },
  {
    "text": "what I'm going to do is I'm going to execute this delete statement I'm going to delete all the data out of my prod table where the IDS match but it's okay",
    "start": "2075450",
    "end": "2082800"
  },
  {
    "text": "everyone else is not going to see this because this is all in a transaction then I'm going to insert the records",
    "start": "2082800",
    "end": "2088919"
  },
  {
    "text": "over I can drop my staging table safely now and now I can finish the transaction",
    "start": "2088919",
    "end": "2095010"
  },
  {
    "text": "and everyone gets a consistent view of the data even if they are querying it while this workflow is happening so just",
    "start": "2095010",
    "end": "2105570"
  },
  {
    "text": "to summarize and wrap this up here I mentioned wrapping things in explicit transaction consider using drop table or",
    "start": "2105570",
    "end": "2113580"
  },
  {
    "text": "truncate instead of delete it's just in case that's instead of doing say we wouldn't want to delete data out of that",
    "start": "2113580",
    "end": "2119940"
  },
  {
    "text": "staging table it's just more work for vacuum if we were to drop it as the data is just gone no vacuum necessary if we",
    "start": "2119940",
    "end": "2126810"
  },
  {
    "text": "are trying to use temporary tables if you have to use a permanent table you",
    "start": "2126810",
    "end": "2132000"
  },
  {
    "text": "can also consider using the backup no option what that does is it says hey don't flush that table to s3 make sure",
    "start": "2132000",
    "end": "2140430"
  },
  {
    "text": "both the staging and the prod table have the same distribution key if they can the reason for that is that insert into",
    "start": "2140430",
    "end": "2147480"
  },
  {
    "text": "with a select will be much faster because they're both both tables data resides on the same slice the comp",
    "start": "2147480",
    "end": "2155820"
  },
  {
    "text": "update one is basically we don't want to be figuring out compression repeatedly over and over again on those staging",
    "start": "2155820",
    "end": "2162660"
  },
  {
    "text": "tables if you're creating them and dropping them without compression that like keyword that I used that'll take",
    "start": "2162660",
    "end": "2167820"
  },
  {
    "text": "care of that and the last one there",
    "start": "2167820",
    "end": "2172920"
  },
  {
    "text": "this ultra tape pen is a little bit if you've used a swap partition it's a",
    "start": "2172920",
    "end": "2178620"
  },
  {
    "text": "little bit like that command a lot of times I don't talk about this one but it is if you're going to be have a staging",
    "start": "2178620",
    "end": "2185070"
  },
  {
    "text": "table with million probably more like hundreds of millions if not billions of records and you want to quickly move",
    "start": "2185070",
    "end": "2191130"
  },
  {
    "text": "move that data to another table it'll be almost instant it's like a DDL command",
    "start": "2191130",
    "end": "2197670"
  },
  {
    "text": "and what it does is it basically rewrites the metadata so that tables data now belongs to another table this",
    "start": "2197670",
    "end": "2205200"
  },
  {
    "text": "is a very powerful command but you only use it if you're going to be needing to move very large portions of data if",
    "start": "2205200",
    "end": "2211500"
  },
  {
    "text": "you're not just stick to the insert into with a select so vacuum and analyze I've",
    "start": "2211500",
    "end": "2218480"
  },
  {
    "text": "kind of mentioned them a little bit vacuum delete we now have Auto vacuum",
    "start": "2218480",
    "end": "2223830"
  },
  {
    "text": "for deletes we also have Auto analyze so what these two vacuum does two things it",
    "start": "2223830",
    "end": "2229800"
  },
  {
    "text": "one it it removes the deleted records and it also resources the table so the",
    "start": "2229800",
    "end": "2235140"
  },
  {
    "text": "table is globally sorted the delete portion of it is fully automated now",
    "start": "2235140",
    "end": "2240480"
  },
  {
    "text": "it's not something you need to worry about analyzes for collecting statistics on tables so this is where it samples a",
    "start": "2240480",
    "end": "2246300"
  },
  {
    "text": "table it figures out things like cardinality and stuff that is also has also been automated so again not",
    "start": "2246300",
    "end": "2252780"
  },
  {
    "text": "something you need to worry about so those two completely done vacuum sort unfortunately you still need to schedule",
    "start": "2252780",
    "end": "2260400"
  },
  {
    "text": "that most customers run it like once a day maybe once a week if you're mostly",
    "start": "2260400",
    "end": "2265830"
  },
  {
    "text": "or loading data in sorted order so you unfortunately do need to do the vacuum sort we have announced though",
    "start": "2265830",
    "end": "2273030"
  },
  {
    "text": "that that actually is coming it was announced at reinvent so we are still continuing to work on vac auto vacuum",
    "start": "2273030",
    "end": "2279359"
  },
  {
    "text": "sort so wlm workload management the",
    "start": "2279359",
    "end": "2285030"
  },
  {
    "text": "purpose of workload management is to separate different types of query workloads the reason isolating the",
    "start": "2285030",
    "end": "2292560"
  },
  {
    "text": "resources for them so a lot of times we have different competing workloads that",
    "start": "2292560",
    "end": "2298410"
  },
  {
    "text": "run on a cluster and some of them might be more important to us than others for example we usually want to prioritize",
    "start": "2298410",
    "end": "2305119"
  },
  {
    "text": "dashboards maybe our tableau queries that sort of thing and the ETL or the",
    "start": "2305119",
    "end": "2310200"
  },
  {
    "text": "ingestion of data is maybe less important so we might separate those two concerns so how we do this in redshift",
    "start": "2310200",
    "end": "2319950"
  },
  {
    "text": "is we have we can create what are called queues every single query in a redshift cluster executes in a queue by default",
    "start": "2319950",
    "end": "2329060"
  },
  {
    "text": "sort of by default now I'll talk a little bit about auto wlm in a couple slides by default we have a single queue",
    "start": "2329060",
    "end": "2337560"
  },
  {
    "text": "with five slots that's kind of so you can heck throw five queries that redshift and",
    "start": "2337560",
    "end": "2343170"
  },
  {
    "text": "after that they will queue a lot of customers set up wlm to expand it beyond",
    "start": "2343170",
    "end": "2349319"
  },
  {
    "text": "that most customers will set up somewhere to run somewhere between 12 and 15 queries is typically what they'll",
    "start": "2349319",
    "end": "2355650"
  },
  {
    "text": "set up and then they'll enable this feature that we call sqa or short query",
    "start": "2355650",
    "end": "2361140"
  },
  {
    "text": "acceleration what sqa does is it analyzes incoming queries if the queue",
    "start": "2361140",
    "end": "2366869"
  },
  {
    "text": "is full what we do is is it will look at the query and say we know we can run this in a couple seconds we will",
    "start": "2366869",
    "end": "2373950"
  },
  {
    "text": "immediately run it anyways even though your main queue is full so it kind of gets hopped into a different into the",
    "start": "2373950",
    "end": "2380670"
  },
  {
    "text": "short query queue which is kind of a hidden off and that the idea here is if",
    "start": "2380670",
    "end": "2385859"
  },
  {
    "text": "you have short running queries that only take like a second or two to run we just want to get them out of the system make",
    "start": "2385859",
    "end": "2391829"
  },
  {
    "text": "this as make the end user feel like things are fairly snappy still if we're slowing up long-running queries that",
    "start": "2391829",
    "end": "2397589"
  },
  {
    "text": "take minutes or hours to run and they take a few extra minutes it's probably not a big deal so the idea is keep short",
    "start": "2397589",
    "end": "2404190"
  },
  {
    "text": "running queries running quickly so each",
    "start": "2404190",
    "end": "2409829"
  },
  {
    "text": "one of our cues and redshift is split up into something we call slots it",
    "start": "2409829",
    "end": "2415049"
  },
  {
    "text": "corresponds to the concurrency in the system and each one of these slots has a",
    "start": "2415049",
    "end": "2420240"
  },
  {
    "text": "certain amount of memory that is assigned to it concurrency scaling what",
    "start": "2420240",
    "end": "2427200"
  },
  {
    "text": "this is is this was a feature that we recently released at the end of March",
    "start": "2427200",
    "end": "2432299"
  },
  {
    "text": "and what it does is if a queue is full",
    "start": "2432299",
    "end": "2437450"
  },
  {
    "text": "you can we can spin up additional redshift clusters and send the queries",
    "start": "2437450",
    "end": "2443220"
  },
  {
    "text": "over to those additional clusters and leverage the backup and I'll talk about that quite a bit more in depth in a few",
    "start": "2443220",
    "end": "2450900"
  },
  {
    "text": "slides so the first thing here I'm gonna walk through just an example of how typical",
    "start": "2450900",
    "end": "2457470"
  },
  {
    "text": "customer sets up workload management I've created this scenario that is very generic light ingestion peak reporting",
    "start": "2457470",
    "end": "2466760"
  },
  {
    "text": "you know during the middle of the day and then a heavy ingestion overnight and just your typical spread of types of",
    "start": "2466760",
    "end": "2473940"
  },
  {
    "text": "users and in this scenario fine we're just set up at wlm SQ i would",
    "start": "2473940",
    "end": "2480500"
  },
  {
    "text": "probably set it up something like this I'd have an ingestion queue I keep a fairly low number of slots on that",
    "start": "2480500",
    "end": "2486860"
  },
  {
    "text": "ingestion queue a reasonable amount of memory per slot so I've configured with 20% memory to concurrency that means",
    "start": "2486860",
    "end": "2493340"
  },
  {
    "text": "each slot has 10% of the clusters memory my dashboard there I'm gonna give it",
    "start": "2493340",
    "end": "2499250"
  },
  {
    "text": "quite a bit a larger number of concurrency so I've set it up to ten or ten slots and I've enabled concurrency",
    "start": "2499250",
    "end": "2506630"
  },
  {
    "text": "scaling which allows it to leverage additional redshift clusters then I have",
    "start": "2506630",
    "end": "2513050"
  },
  {
    "text": "this default queue here at the end we'll say that's kind of like my queue for",
    "start": "2513050",
    "end": "2518180"
  },
  {
    "text": "everyone else maybe my data science users business analyst that are gonna run ad-hoc queries and I've set the concurrency or",
    "start": "2518180",
    "end": "2524510"
  },
  {
    "text": "the number of slots on this quite a bit lower I've also allowed them to use concurrency scaling as well I'm assuming",
    "start": "2524510",
    "end": "2531350"
  },
  {
    "text": "these guys are gonna be running more beefy queries than what the dashboard is so that's why they have like 8% memory",
    "start": "2531350",
    "end": "2536810"
  },
  {
    "text": "each one thing that I've kind of done here is is this is hidden this super",
    "start": "2536810",
    "end": "2543860"
  },
  {
    "text": "user queue so we do have in redshift it's not really meant to be hidden it just doesn't show up in the console it's",
    "start": "2543860",
    "end": "2549170"
  },
  {
    "text": "in the documentation there is an additional cue and redshift that we call the super user queue that queue any",
    "start": "2549170",
    "end": "2555140"
  },
  {
    "text": "super user can switch to that and use it it's meant for administration tasks so if say someone you just wanted to do",
    "start": "2555140",
    "end": "2561980"
  },
  {
    "text": "some administrative work maybe dropping some tables things like that you can definitely leverage that queue you",
    "start": "2561980",
    "end": "2567350"
  },
  {
    "text": "probably don't want to be using it for executing long-running queries or things like that though so concurrency scaling",
    "start": "2567350",
    "end": "2574010"
  },
  {
    "text": "I mentioned I would talk about how this works so say we have our single cluster here and we start sending a ton of",
    "start": "2574010",
    "end": "2582440"
  },
  {
    "text": "queries to this thing and the queue start filling up what we end up doing is as we flush the data to s3 out of the",
    "start": "2582440",
    "end": "2589010"
  },
  {
    "text": "cluster so our backup there and we end up spinning up an additional redshift",
    "start": "2589010",
    "end": "2595520"
  },
  {
    "text": "cluster that are additional redshift cluster instead of having the queries",
    "start": "2595520",
    "end": "2600680"
  },
  {
    "text": "queuing and waiting on the main cluster we instead send it over to the other cluster it needs to hit s3 to pull that",
    "start": "2600680",
    "end": "2609350"
  },
  {
    "text": "data and pulling that data off of s3 but what we found is is for a lot of queries it's",
    "start": "2609350",
    "end": "2614690"
  },
  {
    "text": "beneficial to use this second cluster it's faster than waiting you know for",
    "start": "2614690",
    "end": "2620059"
  },
  {
    "text": "your turn to run and so we end up alleviating the load on the main cluster",
    "start": "2620059",
    "end": "2625819"
  },
  {
    "text": "and getting that query running immediately now if say more queries continue to come",
    "start": "2625819",
    "end": "2631970"
  },
  {
    "text": "in we can also spin up even more clusters by default we allow up to 10 additional redshift clusters to be spun",
    "start": "2631970",
    "end": "2640339"
  },
  {
    "text": "up you can select and limit the number of it if you when you enable concurrency scaling so that's done per queue in the",
    "start": "2640339",
    "end": "2647240"
  },
  {
    "text": "wlm section to enable it we did a lot of analysis on the fleet and seeing how",
    "start": "2647240",
    "end": "2655099"
  },
  {
    "text": "many customers were going to benefit from this one thing that we did is for",
    "start": "2655099",
    "end": "2660319"
  },
  {
    "text": "every 24 hours that your cluster runs your main cluster we give you one hour",
    "start": "2660319",
    "end": "2665450"
  },
  {
    "text": "of free concurrency scaling we found that 97% of our class customers if they",
    "start": "2665450",
    "end": "2672589"
  },
  {
    "text": "were to enable concurrency scaling this feature would be free for them so we",
    "start": "2672589",
    "end": "2678170"
  },
  {
    "text": "wanted to make this feature mostly free so I'm gonna talk briefly about qmr qmr",
    "start": "2678170",
    "end": "2686049"
  },
  {
    "text": "was a feature that we added for those of you that have people that use redshift",
    "start": "2686049",
    "end": "2692170"
  },
  {
    "text": "that you wish weren't connected to your redshift cluster these are customers",
    "start": "2692170",
    "end": "2697309"
  },
  {
    "text": "that maybe write Cartesian joins and just do bad stuff to try and drag a",
    "start": "2697309",
    "end": "2702619"
  },
  {
    "text": "billion records out of the cluster what qmr does is allows you to write",
    "start": "2702619",
    "end": "2708019"
  },
  {
    "text": "rules to detect these poorly written queries and abort them automatically so",
    "start": "2708019",
    "end": "2713509"
  },
  {
    "text": "this way someone's not getting paged and needn't find the bad query and killing it manually that was the primary purpose",
    "start": "2713509",
    "end": "2720049"
  },
  {
    "text": "for this feature another portion of it though is you can also log the queries I",
    "start": "2720049",
    "end": "2728839"
  },
  {
    "text": "highly recommend that lot run logging long-running queries in redshift so",
    "start": "2728839",
    "end": "2734119"
  },
  {
    "text": "that's one rule I always just recommend just set up a rule and log your long-running ones so just to recap the bennet best",
    "start": "2734119",
    "end": "2742640"
  },
  {
    "text": "practices for w them keep the number of queues you have to a minimum",
    "start": "2742640",
    "end": "2748430"
  },
  {
    "text": "don't try and set up like six queues things like that what ends up happening is is you don't use them all use wlm to",
    "start": "2748430",
    "end": "2757220"
  },
  {
    "text": "limit your ingestion usually to two or three is a good rule of thumb keep your wlm limited to about fifteen",
    "start": "2757220",
    "end": "2763340"
  },
  {
    "text": "and I mentioned the long running queries already and usage of the superuser queue",
    "start": "2763340",
    "end": "2769780"
  },
  {
    "text": "so jumping quickly into cluster sizing we have two instance types in redshift",
    "start": "2769780",
    "end": "2774980"
  },
  {
    "text": "RDC two which is SSD and ds2 which is magnetic disk basically up the SSDs are",
    "start": "2774980",
    "end": "2784460"
  },
  {
    "text": "obviously faster they do cost more and each of these instance types comes in",
    "start": "2784460",
    "end": "2790370"
  },
  {
    "text": "two different sizes we have a smaller instance and an 8xl which is the big instance type you can see the",
    "start": "2790370",
    "end": "2796790"
  },
  {
    "text": "information about it up there so when sizing and Amazon redshift cluster I",
    "start": "2796790",
    "end": "2803840"
  },
  {
    "text": "always recommend starting with how much uncompressed data you have in your legacy system assume you're gonna get a",
    "start": "2803840",
    "end": "2811250"
  },
  {
    "text": "3/8 x compression ratio and redshift it's probably going to end up being closer to 4 times but I think 3 is just",
    "start": "2811250",
    "end": "2817430"
  },
  {
    "text": "a it's a good rule of thumb to start out with and target having around thirty to forty percent free disk space we usually",
    "start": "2817430",
    "end": "2824540"
  },
  {
    "text": "always recommend you know keeping it under 80 percent as the red line and then pick HDD or SSD depending on what",
    "start": "2824540",
    "end": "2833000"
  },
  {
    "text": "kind of performance you require usually some of our you know if you're coming",
    "start": "2833000",
    "end": "2838880"
  },
  {
    "text": "from a high-performing legacy system you obviously want to pick the SSDs so just",
    "start": "2838880",
    "end": "2844100"
  },
  {
    "text": "as an example of how I would do this work but customer came to me and said you don't have 20 terabytes of data in",
    "start": "2844100",
    "end": "2849590"
  },
  {
    "text": "my legacy system I would take that and assume it's about 6.7 terabytes then",
    "start": "2849590",
    "end": "2855920"
  },
  {
    "text": "depending on the performance requirements I would either you pick four of our dc28 xl's or five of our ds2",
    "start": "2855920",
    "end": "2863210"
  },
  {
    "text": "extra-large which would give you ten terabytes of capacity and enough free space to do work so that's how I would",
    "start": "2863210",
    "end": "2870110"
  },
  {
    "text": "do that sizing now there's two ways if you get it wrong to resize a reg",
    "start": "2870110",
    "end": "2877290"
  },
  {
    "text": "cluster there's our classic and there's what we call elastic resize our classic",
    "start": "2877290",
    "end": "2882660"
  },
  {
    "text": "resize what it does is so if we have these three compute nodes here we want",
    "start": "2882660",
    "end": "2888300"
  },
  {
    "text": "to resize this to four we spin up an entirely new cluster in the background we then drop the main cluster and",
    "start": "2888300",
    "end": "2895710"
  },
  {
    "text": "read-only we do a binary stream of the data over once the data is fully",
    "start": "2895710",
    "end": "2901500"
  },
  {
    "text": "streamed over we redirect the DNS and we bounce the connections and you're up and running on the new cluster now how we do",
    "start": "2901500",
    "end": "2909540"
  },
  {
    "text": "it the new elastic resize say if we have the same three nodes and we want to",
    "start": "2909540",
    "end": "2915480"
  },
  {
    "text": "incrementally add one we just want to add some storage maybe what we do is when you click the button to resize this",
    "start": "2915480",
    "end": "2921990"
  },
  {
    "text": "it quite literally is just a type a new number and click we spin up or we attach",
    "start": "2921990",
    "end": "2927000"
  },
  {
    "text": "an extra node to this cluster we kick off the back up to s3 this usually takes",
    "start": "2927000",
    "end": "2933210"
  },
  {
    "text": "around 10-ish minutes or so once it's complete what ends up happening is we",
    "start": "2933210",
    "end": "2940520"
  },
  {
    "text": "basically redistribute the metadata and this portion of it takes around four",
    "start": "2940520",
    "end": "2946350"
  },
  {
    "text": "minutes it's actually really fast what as we redistribute the metadata we hold",
    "start": "2946350",
    "end": "2951960"
  },
  {
    "text": "all the queries so all in flight queries are put on pause and once that step is",
    "start": "2951960",
    "end": "2958560"
  },
  {
    "text": "done we begin to restore the data and the cluster is fully available now at",
    "start": "2958560",
    "end": "2963870"
  },
  {
    "text": "this point for both reads and for writes so you get about a four minute ish outage where you can't do reads or",
    "start": "2963870",
    "end": "2971040"
  },
  {
    "text": "writes in this step so where you would pick one or this is actually there's",
    "start": "2971040",
    "end": "2977130"
  },
  {
    "text": "some limitations with it on the small instance types with the small ones you can only either double or cut in half",
    "start": "2977130",
    "end": "2983540"
  },
  {
    "text": "because there's just not enough slices to work with we're redistributing metadata on the nodes on our 8x cells",
    "start": "2983540",
    "end": "2990990"
  },
  {
    "text": "however because we have so many slices we can incrementally add nodes so if say",
    "start": "2990990",
    "end": "2996570"
  },
  {
    "text": "we started with a four node cluster I could incremental ego five six seven eight or all the way down to two so all",
    "start": "2996570",
    "end": "3002390"
  },
  {
    "text": "the way up to doubling and all the way to in half",
    "start": "3002390",
    "end": "3006759"
  },
  {
    "text": "so when you would use one versus the other you would use elastic resize maybe",
    "start": "3007410",
    "end": "3013840"
  },
  {
    "text": "scaling up for spikey writing right workloads incrementally just adding and removing storage the classic one",
    "start": "3013840",
    "end": "3021310"
  },
  {
    "text": "you can switch Hardware types so if you started out on magnetic disks you wanted to move to SSDs classic resize allows",
    "start": "3021310",
    "end": "3027640"
  },
  {
    "text": "you to switch instance types as well obviously if the limitations of elastic",
    "start": "3027640",
    "end": "3034060"
  },
  {
    "text": "resize are restricting you you would obviously also use classic resize and",
    "start": "3034060",
    "end": "3039130"
  },
  {
    "text": "you can see the difference in the amount of time it takes while the cluster is kind of doing these the classic resize",
    "start": "3039130",
    "end": "3046180"
  },
  {
    "text": "usually an overnight operation so with",
    "start": "3046180",
    "end": "3052180"
  },
  {
    "text": "cluster sizing the one thing that I always say if you're doing a prod workload make sure you have two notes or",
    "start": "3052180",
    "end": "3058770"
  },
  {
    "text": "more and the reason why is you want the redundancy from the mirrored copy",
    "start": "3058770",
    "end": "3063810"
  },
  {
    "text": "maintain at least 20 min per centum in among space and if you happen to be a",
    "start": "3063810",
    "end": "3069400"
  },
  {
    "text": "customer using DC ones migrate off of those we're allow you to even migrate",
    "start": "3069400",
    "end": "3075130"
  },
  {
    "text": "your reserved instances we don't want you running those at all really we want you have the best experience with",
    "start": "3075130",
    "end": "3081040"
  },
  {
    "text": "redshift the DC 2's in a lot of cases are 2 times faster so it's a free",
    "start": "3081040",
    "end": "3086200"
  },
  {
    "text": "migration no just to hit the summary here I kind of put this together to",
    "start": "3086200",
    "end": "3094180"
  },
  {
    "text": "summarize those of you who are looking for legacy moving from legacy systems the row based data if you're coming from",
    "start": "3094180",
    "end": "3101020"
  },
  {
    "text": "a row based database which many people are denormalize further remember don't",
    "start": "3101020",
    "end": "3107800"
  },
  {
    "text": "use those date dimension tables get the whatever you're going to be filtering on the predicates into your fact table",
    "start": "3107800",
    "end": "3115350"
  },
  {
    "text": "redshift is column ER it does well with really wide fact tables I you I have",
    "start": "3115350",
    "end": "3121090"
  },
  {
    "text": "customers that run like 300 plus columns on a single table and that's fine",
    "start": "3121090",
    "end": "3127870"
  },
  {
    "text": "if you're coming from an SMP system you do have to think about distribution keys",
    "start": "3127870",
    "end": "3133450"
  },
  {
    "text": "it's not something you have to think about in SMP um so if you have those an SMP system you will need 2 Colo",
    "start": "3133450",
    "end": "3141010"
  },
  {
    "text": "data for your large joints so that's really the one thing you have to kind of think about that's different",
    "start": "3141010",
    "end": "3147640"
  },
  {
    "text": "redshift like I mentioned before in the very beginning it is meant for big data if you're we're talking like you know 10",
    "start": "3147640",
    "end": "3155620"
  },
  {
    "text": "10 20 gigs of data it might make more sense to use RDS or Aurora Postgres and",
    "start": "3155620",
    "end": "3162370"
  },
  {
    "text": "then when you reach sufficient size the switch over to redshift is really easy because the sequel syntax is almost the",
    "start": "3162370",
    "end": "3169990"
  },
  {
    "text": "exact same we also added stored procedures a lot of people don't realize",
    "start": "3169990",
    "end": "3175510"
  },
  {
    "text": "that we put this in there's only launched last month we have PL PG sequel which is the Postgres",
    "start": "3175510",
    "end": "3181620"
  },
  {
    "text": "language stored procs now in redshift and we added that for those of you who",
    "start": "3181620",
    "end": "3187810"
  },
  {
    "text": "are doing migrations making them easier so just some additional resources with a",
    "start": "3187810",
    "end": "3196690"
  },
  {
    "text": "bunch of links here on github there are some scripts and things that we find customers find useful check those out we",
    "start": "3196690",
    "end": "3207370"
  },
  {
    "text": "also have a handful of blogs here the top one is really great today I'm a design playbook top 10 performance",
    "start": "3207370",
    "end": "3216040"
  },
  {
    "text": "tuning that is a great blog as well so just extra tips and tricks thank you",
    "start": "3216040",
    "end": "3222250"
  },
  {
    "text": "guys very much [Applause]",
    "start": "3222250",
    "end": "3228849"
  }
]