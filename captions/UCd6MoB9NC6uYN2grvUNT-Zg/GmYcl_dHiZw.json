[
  {
    "text": "in this video you'll see how to",
    "start": "80",
    "end": "1560"
  },
  {
    "text": "accelerate ETL or extract transfer and",
    "start": "1560",
    "end": "4720"
  },
  {
    "text": "load processes for Amazon redshift with",
    "start": "4720",
    "end": "7240"
  },
  {
    "text": "AWS glue with this serverless solution",
    "start": "7240",
    "end": "10840"
  },
  {
    "text": "you can reduce costs by avoiding",
    "start": "10840",
    "end": "12759"
  },
  {
    "text": "third-party ETL fees increase data",
    "start": "12759",
    "end": "15639"
  },
  {
    "text": "integration efficiency and simplify",
    "start": "15639",
    "end": "17880"
  },
  {
    "text": "common data loading operations into",
    "start": "17880",
    "end": "20279"
  },
  {
    "text": "redshift AWS glue is a serverless data",
    "start": "20279",
    "end": "24000"
  },
  {
    "text": "integration service that makes it easier",
    "start": "24000",
    "end": "25800"
  },
  {
    "text": "to discover prepare move and integrate",
    "start": "25800",
    "end": "28800"
  },
  {
    "text": "data from multiple sources for analytics",
    "start": "28800",
    "end": "31320"
  },
  {
    "text": "machine learning or ML and application",
    "start": "31320",
    "end": "35040"
  },
  {
    "text": "development Amazon redshift is a fully",
    "start": "35040",
    "end": "37600"
  },
  {
    "text": "managed petabyte scale data warehouse",
    "start": "37600",
    "end": "39960"
  },
  {
    "text": "service in the cloud that uses SQL to",
    "start": "39960",
    "end": "42480"
  },
  {
    "text": "analyze structured and semi-structured",
    "start": "42480",
    "end": "44520"
  },
  {
    "text": "data across data warehouses operational",
    "start": "44520",
    "end": "47360"
  },
  {
    "text": "databases and data",
    "start": "47360",
    "end": "49320"
  },
  {
    "text": "Lakes together these AWS native services",
    "start": "49320",
    "end": "53160"
  },
  {
    "text": "offer a modern data architecture that's",
    "start": "53160",
    "end": "55399"
  },
  {
    "text": "easy to manage and offers performance at",
    "start": "55399",
    "end": "57719"
  },
  {
    "text": "scale in seconds in this video we'll",
    "start": "57719",
    "end": "61079"
  },
  {
    "text": "look at two scenarios in the first",
    "start": "61079",
    "end": "63559"
  },
  {
    "text": "scenario we want to update customer data",
    "start": "63559",
    "end": "65720"
  },
  {
    "text": "stored in Amazon redshift based on daily",
    "start": "65720",
    "end": "68240"
  },
  {
    "text": "account transactions stored in Apache",
    "start": "68240",
    "end": "70280"
  },
  {
    "text": "Iceberg tables on Amazon S3 we'll use",
    "start": "70280",
    "end": "73680"
  },
  {
    "text": "glue Studio to build an end-to-end job",
    "start": "73680",
    "end": "76000"
  },
  {
    "text": "that will extract the Amazon S3 data",
    "start": "76000",
    "end": "78640"
  },
  {
    "text": "join it with the Amazon red shift tables",
    "start": "78640",
    "end": "81240"
  },
  {
    "text": "Aggregate and join transactions and",
    "start": "81240",
    "end": "83439"
  },
  {
    "text": "merge the updated records back to Amazon",
    "start": "83439",
    "end": "85799"
  },
  {
    "text": "redshift we'll then use Amazon quick",
    "start": "85799",
    "end": "88439"
  },
  {
    "text": "site to help analyze the result results",
    "start": "88439",
    "end": "90240"
  },
  {
    "text": "by ranking customers based on total",
    "start": "90240",
    "end": "92200"
  },
  {
    "text": "daily",
    "start": "92200",
    "end": "93399"
  },
  {
    "text": "transactions let's get",
    "start": "93399",
    "end": "96399"
  },
  {
    "text": "started we'll begin in AWS glue Studio",
    "start": "96399",
    "end": "100119"
  },
  {
    "text": "where we'll open the visual ETL",
    "start": "100119",
    "end": "102040"
  },
  {
    "text": "interface and create a new job starting",
    "start": "102040",
    "end": "104040"
  },
  {
    "text": "with a blank",
    "start": "104040",
    "end": "105280"
  },
  {
    "text": "canvas first we'll give the job a",
    "start": "105280",
    "end": "108960"
  },
  {
    "text": "name next we'll add an S3 data source",
    "start": "108960",
    "end": "113719"
  },
  {
    "text": "node next we'll configure the S3 data",
    "start": "115920",
    "end": "118920"
  },
  {
    "text": "source properties",
    "start": "118920",
    "end": "120399"
  },
  {
    "text": "let's use AWS glue's data catalog to",
    "start": "120399",
    "end": "123039"
  },
  {
    "text": "select the database and tables we want",
    "start": "123039",
    "end": "124680"
  },
  {
    "text": "to work",
    "start": "124680",
    "end": "127119"
  },
  {
    "text": "with based on our selection the format",
    "start": "130160",
    "end": "133160"
  },
  {
    "text": "is automatically set to a patchy",
    "start": "133160",
    "end": "135760"
  },
  {
    "text": "Iceberg now that our data source is",
    "start": "135760",
    "end": "137959"
  },
  {
    "text": "configured let's add the aggregate",
    "start": "137959",
    "end": "139959"
  },
  {
    "text": "transform to our job so we can compute",
    "start": "139959",
    "end": "142400"
  },
  {
    "text": "aggregated values for fields in our data",
    "start": "142400",
    "end": "145040"
  },
  {
    "text": "set we can select the fields we want to",
    "start": "145040",
    "end": "147519"
  },
  {
    "text": "group rows",
    "start": "147519",
    "end": "148760"
  },
  {
    "text": "by",
    "start": "148760",
    "end": "151760"
  },
  {
    "text": "next we'll select the fields and",
    "start": "153120",
    "end": "154480"
  },
  {
    "text": "functions to",
    "start": "154480",
    "end": "157040"
  },
  {
    "text": "aggregate we can select from a number of",
    "start": "160959",
    "end": "163200"
  },
  {
    "text": "built-in",
    "start": "163200",
    "end": "165720"
  },
  {
    "text": "functions to make our data easier to",
    "start": "166080",
    "end": "168400"
  },
  {
    "text": "work with let's rename the field we just",
    "start": "168400",
    "end": "172439"
  },
  {
    "text": "created",
    "start": "178760",
    "end": "181440"
  },
  {
    "text": "now that the iceberg Source data from S3",
    "start": "181440",
    "end": "183799"
  },
  {
    "text": "is prepped we want to join that data",
    "start": "183799",
    "end": "185840"
  },
  {
    "text": "with our redshift",
    "start": "185840",
    "end": "187400"
  },
  {
    "text": "tables to do so the first step is to add",
    "start": "187400",
    "end": "190519"
  },
  {
    "text": "redshift as a",
    "start": "190519",
    "end": "193440"
  },
  {
    "text": "source we can choose to access our red",
    "start": "193519",
    "end": "196200"
  },
  {
    "text": "shift data through a direct data",
    "start": "196200",
    "end": "197720"
  },
  {
    "text": "connection or through AWS glue data",
    "start": "197720",
    "end": "200519"
  },
  {
    "text": "catalog",
    "start": "200519",
    "end": "201599"
  },
  {
    "text": "tables we'll use the direct data",
    "start": "201599",
    "end": "203920"
  },
  {
    "text": "connection and select our data from the",
    "start": "203920",
    "end": "206480"
  },
  {
    "text": "drop-down now we'll choose how we'd like",
    "start": "206480",
    "end": "208599"
  },
  {
    "text": "to access the data",
    "start": "208599",
    "end": "210920"
  },
  {
    "text": "the single table option allows us to",
    "start": "210920",
    "end": "212680"
  },
  {
    "text": "select a single schema and a single",
    "start": "212680",
    "end": "214680"
  },
  {
    "text": "table from our redshift data the custom",
    "start": "214680",
    "end": "217799"
  },
  {
    "text": "query option allows us to define a",
    "start": "217799",
    "end": "219640"
  },
  {
    "text": "custom data set using SQL from multiple",
    "start": "219640",
    "end": "222080"
  },
  {
    "text": "red shift tables found in one or more",
    "start": "222080",
    "end": "225040"
  },
  {
    "text": "schemas we'll leave the single table",
    "start": "225040",
    "end": "227239"
  },
  {
    "text": "option",
    "start": "227239",
    "end": "228280"
  },
  {
    "text": "selected next we'll select the schema",
    "start": "228280",
    "end": "230799"
  },
  {
    "text": "and table we want to work",
    "start": "230799",
    "end": "234000"
  },
  {
    "text": "with let's join our data sources",
    "start": "236959",
    "end": "241680"
  },
  {
    "text": "the join transform requires two parent",
    "start": "244959",
    "end": "248879"
  },
  {
    "text": "nodes next we'll add a join condition",
    "start": "251040",
    "end": "254360"
  },
  {
    "text": "we'll select a field from each parent",
    "start": "254360",
    "end": "258079"
  },
  {
    "text": "node the tables are now joined next",
    "start": "262240",
    "end": "265560"
  },
  {
    "text": "let's add two derived columns so we can",
    "start": "265560",
    "end": "267600"
  },
  {
    "text": "structure our data to show the last days",
    "start": "267600",
    "end": "269320"
  },
  {
    "text": "trans transactions and total",
    "start": "269320",
    "end": "272560"
  },
  {
    "text": "valuation for our first derived column",
    "start": "272560",
    "end": "275120"
  },
  {
    "text": "we'll use a simple SQL statement to",
    "start": "275120",
    "end": "277120"
  },
  {
    "text": "update the total valuation column with",
    "start": "277120",
    "end": "279199"
  },
  {
    "text": "the last day's data let's add the second",
    "start": "279199",
    "end": "282240"
  },
  {
    "text": "derived",
    "start": "282240",
    "end": "284720"
  },
  {
    "text": "column in this case we'll create a new",
    "start": "289840",
    "end": "292280"
  },
  {
    "text": "column that contains the last day's",
    "start": "292280",
    "end": "293960"
  },
  {
    "text": "transaction flow plus the total",
    "start": "293960",
    "end": "296919"
  },
  {
    "text": "valuation next let's add a transform",
    "start": "296919",
    "end": "299560"
  },
  {
    "text": "form that allows us to select which",
    "start": "299560",
    "end": "301160"
  },
  {
    "text": "fields to work with in our data",
    "start": "301160",
    "end": "304800"
  },
  {
    "text": "set we'll select the stock symbol field",
    "start": "306199",
    "end": "308720"
  },
  {
    "text": "from our red shift data and the two",
    "start": "308720",
    "end": "310560"
  },
  {
    "text": "fields that we just",
    "start": "310560",
    "end": "313280"
  },
  {
    "text": "created now that our data is prepped",
    "start": "313280",
    "end": "315840"
  },
  {
    "text": "let's write it back to Amazon",
    "start": "315840",
    "end": "317680"
  },
  {
    "text": "redshift to do so we'll first add a",
    "start": "317680",
    "end": "320479"
  },
  {
    "text": "redshift Target",
    "start": "320479",
    "end": "323520"
  },
  {
    "text": "node next we'll configure the target",
    "start": "326440",
    "end": "329039"
  },
  {
    "text": "node",
    "start": "329039",
    "end": "330039"
  },
  {
    "text": "properties as we did earlier with the",
    "start": "330039",
    "end": "332080"
  },
  {
    "text": "red shift Source node we'll access our",
    "start": "332080",
    "end": "334360"
  },
  {
    "text": "data using a direct data",
    "start": "334360",
    "end": "336280"
  },
  {
    "text": "connection we'll select the same schema",
    "start": "336280",
    "end": "338800"
  },
  {
    "text": "that we used in our source",
    "start": "338800",
    "end": "341960"
  },
  {
    "text": "node now we'll select our destination",
    "start": "343400",
    "end": "347840"
  },
  {
    "text": "table let's see what we can do with the",
    "start": "350360",
    "end": "352560"
  },
  {
    "text": "data in our table the glue Studio visual",
    "start": "352560",
    "end": "355880"
  },
  {
    "text": "editor offers out of the box options",
    "start": "355880",
    "end": "357919"
  },
  {
    "text": "that help simplify common data loading",
    "start": "357919",
    "end": "359960"
  },
  {
    "text": "operations into Amazon redshift we'll",
    "start": "359960",
    "end": "362759"
  },
  {
    "text": "select",
    "start": "362759",
    "end": "363919"
  },
  {
    "text": "merge this selection provides two",
    "start": "363919",
    "end": "366280"
  },
  {
    "text": "choices we can specify matching keys and",
    "start": "366280",
    "end": "369080"
  },
  {
    "text": "choose what happens to rows that match",
    "start": "369080",
    "end": "371039"
  },
  {
    "text": "or don't match the key or we can enter a",
    "start": "371039",
    "end": "373960"
  },
  {
    "text": "custom merge statement let's look at",
    "start": "373960",
    "end": "376120"
  },
  {
    "text": "that",
    "start": "376120",
    "end": "378319"
  },
  {
    "text": "option for our purposes we'll select the",
    "start": "380160",
    "end": "383039"
  },
  {
    "text": "simple option which allows us to enter",
    "start": "383039",
    "end": "385479"
  },
  {
    "text": "conditions without writing any special",
    "start": "385479",
    "end": "387720"
  },
  {
    "text": "code first we'll select the field to use",
    "start": "387720",
    "end": "390240"
  },
  {
    "text": "as a matching",
    "start": "390240",
    "end": "392880"
  },
  {
    "text": "key next we'll specify the actions to",
    "start": "394800",
    "end": "398039"
  },
  {
    "text": "perform when records match or do not",
    "start": "398039",
    "end": "400120"
  },
  {
    "text": "match between the source and Target in",
    "start": "400120",
    "end": "403039"
  },
  {
    "text": "this case we'll leave the default",
    "start": "403039",
    "end": "404960"
  },
  {
    "text": "actions",
    "start": "404960",
    "end": "406280"
  },
  {
    "text": "selected finally we'll select the IM am",
    "start": "406280",
    "end": "409400"
  },
  {
    "text": "rooll that can write to the Amazon S3",
    "start": "409400",
    "end": "411680"
  },
  {
    "text": "staging",
    "start": "411680",
    "end": "414199"
  },
  {
    "text": "directory next we'll go to job details",
    "start": "416319",
    "end": "419960"
  },
  {
    "text": "here we'll also select an AM roll as",
    "start": "419960",
    "end": "422160"
  },
  {
    "text": "well as a glue",
    "start": "422160",
    "end": "424800"
  },
  {
    "text": "version let's save and run our",
    "start": "428879",
    "end": "432919"
  },
  {
    "text": "job next let's switch to Quick site and",
    "start": "435879",
    "end": "438599"
  },
  {
    "text": "look at an analysis built from the data",
    "start": "438599",
    "end": "440599"
  },
  {
    "text": "we just",
    "start": "440599",
    "end": "443039"
  },
  {
    "text": "gathered quick site is a fast Cloud",
    "start": "443160",
    "end": "446280"
  },
  {
    "text": "powerered business intelligence service",
    "start": "446280",
    "end": "448280"
  },
  {
    "text": "that delivers insight to the people in",
    "start": "448280",
    "end": "450240"
  },
  {
    "text": "your organization you can easily create",
    "start": "450240",
    "end": "452960"
  },
  {
    "text": "and publish interactive dashboards like",
    "start": "452960",
    "end": "455199"
  },
  {
    "text": "this one the fields list on the left",
    "start": "455199",
    "end": "458039"
  },
  {
    "text": "shows the schema we created in AWS glue",
    "start": "458039",
    "end": "461560"
  },
  {
    "text": "Studio the chart shows a list of company",
    "start": "461560",
    "end": "464120"
  },
  {
    "text": "funds ranked by valuation as of the last",
    "start": "464120",
    "end": "466599"
  },
  {
    "text": "day's transaction flow we can drill down",
    "start": "466599",
    "end": "469440"
  },
  {
    "text": "to see the last day's transactions for",
    "start": "469440",
    "end": "471560"
  },
  {
    "text": "each",
    "start": "471560",
    "end": "472479"
  },
  {
    "text": "company this scenario Illustrated how",
    "start": "472479",
    "end": "475039"
  },
  {
    "text": "easy it is to get started with ETL for",
    "start": "475039",
    "end": "477240"
  },
  {
    "text": "redshift by directly browsing red shift",
    "start": "477240",
    "end": "479720"
  },
  {
    "text": "schemas and tables from the glue studio",
    "start": "479720",
    "end": "483960"
  },
  {
    "text": "interface in the second scenario we'll",
    "start": "484720",
    "end": "487000"
  },
  {
    "text": "use some Advanced Transformations and",
    "start": "487000",
    "end": "488759"
  },
  {
    "text": "built-in pattern detection in AWS glue",
    "start": "488759",
    "end": "491720"
  },
  {
    "text": "to update and enrich our data",
    "start": "491720",
    "end": "494199"
  },
  {
    "text": "specifically we'll use Bank Account",
    "start": "494199",
    "end": "496039"
  },
  {
    "text": "Details to enrich customer profiles and",
    "start": "496039",
    "end": "498280"
  },
  {
    "text": "deepen our analysis while filtering out",
    "start": "498280",
    "end": "500639"
  },
  {
    "text": "personally identifiable information or",
    "start": "500639",
    "end": "503120"
  },
  {
    "text": "pii let's get",
    "start": "503120",
    "end": "506520"
  },
  {
    "text": "started this time we'll look at a job",
    "start": "506919",
    "end": "509120"
  },
  {
    "text": "that has already been created this job",
    "start": "509120",
    "end": "511599"
  },
  {
    "text": "uses the same data sources as the",
    "start": "511599",
    "end": "513479"
  },
  {
    "text": "previous job but includes updates and",
    "start": "513479",
    "end": "516000"
  },
  {
    "text": "additional elements that will help us",
    "start": "516000",
    "end": "517680"
  },
  {
    "text": "extract the information we need our data",
    "start": "517680",
    "end": "521080"
  },
  {
    "text": "source node is connected to the same",
    "start": "521080",
    "end": "523000"
  },
  {
    "text": "Amazon S3 data source a detect sensitive",
    "start": "523000",
    "end": "526519"
  },
  {
    "text": "data node has been added for this",
    "start": "526519",
    "end": "528800"
  },
  {
    "text": "scenario this transform has two data",
    "start": "528800",
    "end": "531320"
  },
  {
    "text": "scanning options you can find sensitive",
    "start": "531320",
    "end": "533800"
  },
  {
    "text": "data in each row by scanning the entire",
    "start": "533800",
    "end": "536200"
  },
  {
    "text": "data set or find columns that represent",
    "start": "536200",
    "end": "538680"
  },
  {
    "text": "sensitive information by scanning a",
    "start": "538680",
    "end": "541200"
  },
  {
    "text": "sample in this case we'll look for",
    "start": "541200",
    "end": "543720"
  },
  {
    "text": "sensitive data in each row we can also",
    "start": "543720",
    "end": "546880"
  },
  {
    "text": "Define the types of sensitive",
    "start": "546880",
    "end": "548320"
  },
  {
    "text": "information to detect in this case we're",
    "start": "548320",
    "end": "551399"
  },
  {
    "text": "selecting specific patterns to",
    "start": "551399",
    "end": "553519"
  },
  {
    "text": "detect we can choose from a number of",
    "start": "553519",
    "end": "555800"
  },
  {
    "text": "out of the- boox patterns for this job",
    "start": "555800",
    "end": "558120"
  },
  {
    "text": "we'll look for bank account patterns in",
    "start": "558120",
    "end": "559839"
  },
  {
    "text": "two geographic",
    "start": "559839",
    "end": "561480"
  },
  {
    "text": "regions finally we'll choose actions to",
    "start": "561480",
    "end": "564079"
  },
  {
    "text": "take on detected entities we're going to",
    "start": "564079",
    "end": "566320"
  },
  {
    "text": "enrich our data with the results let's",
    "start": "566320",
    "end": "568959"
  },
  {
    "text": "look look at the next",
    "start": "568959",
    "end": "570279"
  },
  {
    "text": "node this transform extracts a new",
    "start": "570279",
    "end": "572720"
  },
  {
    "text": "column from the pii detection",
    "start": "572720",
    "end": "575000"
  },
  {
    "text": "results we've connected the same",
    "start": "575000",
    "end": "576959"
  },
  {
    "text": "redshift data source that we used in the",
    "start": "576959",
    "end": "578880"
  },
  {
    "text": "previous scenario with the same schema",
    "start": "578880",
    "end": "581040"
  },
  {
    "text": "and",
    "start": "581040",
    "end": "581839"
  },
  {
    "text": "table next let's look at the join",
    "start": "581839",
    "end": "584839"
  },
  {
    "text": "transform the join is configured as",
    "start": "584839",
    "end": "587160"
  },
  {
    "text": "before but this time we've selected our",
    "start": "587160",
    "end": "588959"
  },
  {
    "text": "pii column extraction as the second",
    "start": "588959",
    "end": "591240"
  },
  {
    "text": "parent",
    "start": "591240",
    "end": "592160"
  },
  {
    "text": "node in the aggregate transform we've",
    "start": "592160",
    "end": "594959"
  },
  {
    "text": "chosen two new fields to group",
    "start": "594959",
    "end": "598120"
  },
  {
    "text": "by in the rename field transform a field",
    "start": "598120",
    "end": "601800"
  },
  {
    "text": "was renamed to help keep our data clean",
    "start": "601800",
    "end": "604920"
  },
  {
    "text": "the pivot transform will help",
    "start": "604920",
    "end": "606399"
  },
  {
    "text": "restructure our data for aggregation by",
    "start": "606399",
    "end": "608839"
  },
  {
    "text": "converting rows into columns allowing us",
    "start": "608839",
    "end": "611440"
  },
  {
    "text": "to extract data by",
    "start": "611440",
    "end": "613360"
  },
  {
    "text": "regions this information will be written",
    "start": "613360",
    "end": "615600"
  },
  {
    "text": "to our redshift data Target let's take a",
    "start": "615600",
    "end": "618560"
  },
  {
    "text": "look as before we're using a direct data",
    "start": "618560",
    "end": "621360"
  },
  {
    "text": "connection to access redshift",
    "start": "621360",
    "end": "624880"
  },
  {
    "text": "data we're using the same schema but",
    "start": "625800",
    "end": "628360"
  },
  {
    "text": "have selected a new new table to work",
    "start": "628360",
    "end": "630079"
  },
  {
    "text": "with again we're using the merge",
    "start": "630079",
    "end": "632560"
  },
  {
    "text": "function let's quickly review the",
    "start": "632560",
    "end": "634600"
  },
  {
    "text": "remaining",
    "start": "634600",
    "end": "636480"
  },
  {
    "text": "details now we'll save and run this new",
    "start": "636480",
    "end": "640880"
  },
  {
    "text": "job let's take a look at the new quick",
    "start": "643480",
    "end": "645800"
  },
  {
    "text": "site",
    "start": "645800",
    "end": "648040"
  },
  {
    "text": "analysis as before our data is ranked by",
    "start": "648399",
    "end": "651240"
  },
  {
    "text": "last day's transaction flow this time we",
    "start": "651240",
    "end": "654000"
  },
  {
    "text": "can drill down to see how the",
    "start": "654000",
    "end": "655360"
  },
  {
    "text": "transaction flow is broken down by",
    "start": "655360",
    "end": "658040"
  },
  {
    "text": "region",
    "start": "658040",
    "end": "661040"
  },
  {
    "text": "you've just seen how to accelerate ETL",
    "start": "667200",
    "end": "669360"
  },
  {
    "text": "processes for Amazon redshift with AWS",
    "start": "669360",
    "end": "672720"
  },
  {
    "text": "glue you can learn more about this topic",
    "start": "672720",
    "end": "675240"
  },
  {
    "text": "in the description and links for this",
    "start": "675240",
    "end": "677440"
  },
  {
    "text": "video thanks for watching now it's your",
    "start": "677440",
    "end": "680040"
  },
  {
    "text": "turn to",
    "start": "680040",
    "end": "681800"
  },
  {
    "text": "try",
    "start": "681800",
    "end": "684800"
  }
]