[
  {
    "start": "0",
    "end": "74000"
  },
  {
    "text": "good afternoon my name is Peter Sirota and I'm the general manager of Amazon elastic mapreduce service",
    "start": "2100",
    "end": "9240"
  },
  {
    "text": "before we start our presentation which is scaling your analytics with Amazon elastic mappers I want to take a quick",
    "start": "9240",
    "end": "15179"
  },
  {
    "text": "poll can you please raise your hand if you've used uh Hadoop before",
    "start": "15179",
    "end": "20880"
  },
  {
    "text": "so a lot of hands like 60 room can you raise your hand if you've used elastic",
    "start": "20880",
    "end": "26580"
  },
  {
    "text": "mapper juice before huh maybe half of you that's a pretty",
    "start": "26580",
    "end": "32398"
  },
  {
    "text": "exciting so uh maybe some of that is going to be a repeat of the information you already know but maybe not so what",
    "start": "32399",
    "end": "38340"
  },
  {
    "text": "we're going to talk today about we're going to talk about some of the Amazon elastic mappers features and how we help Hadoop customers take full advantage of",
    "start": "38340",
    "end": "46260"
  },
  {
    "text": "the AWS web services Amazon web services that want to use kadoo we'll talk a",
    "start": "46260",
    "end": "51660"
  },
  {
    "text": "little bit about Hadoop ecosystem on Amazon EMR and then we'll have two of our customers presenting their use cases",
    "start": "51660",
    "end": "59160"
  },
  {
    "text": "and their info and their architecture and how the museum are to accomplish their big data analytics",
    "start": "59160",
    "end": "66799"
  },
  {
    "text": "so just to recap slide is Hadoop the right system for big data processing",
    "start": "68820",
    "end": "74600"
  },
  {
    "start": "74000",
    "end": "74000"
  },
  {
    "text": "some of our customers are asking us well look now you have redshift and you have other Technologies why why Hadoop and uh",
    "start": "74600",
    "end": "82200"
  },
  {
    "text": "what is it what does it do for uh for the data processing and so you know there's really several several reasons",
    "start": "82200",
    "end": "87840"
  },
  {
    "text": "why we believe that Hadoop is an excellent general purpose uh data processing tool",
    "start": "87840",
    "end": "93360"
  },
  {
    "text": "first of all the application itself Apache Hadoop is scalable and fault",
    "start": "93360",
    "end": "98579"
  },
  {
    "text": "tolerant so that you can add more machines to process more data you can distribute the data across multiple",
    "start": "98579",
    "end": "105000"
  },
  {
    "text": "machines and process it effectively it's fault taller in a sense that if you have a machine that goes out of service",
    "start": "105000",
    "end": "111920"
  },
  {
    "text": "Hadoop can continue processing the data it's very flexible system in the sense",
    "start": "111920",
    "end": "118140"
  },
  {
    "text": "that you can have multiple languages you can use to process big data for example you can have",
    "start": "118140",
    "end": "125579"
  },
  {
    "text": "a Java program that you can write across data in Hadoop or you can have a SQL statement or you can have a an R packet",
    "start": "125579",
    "end": "132780"
  },
  {
    "text": "or R program that you'd use so variety of data a variety of languages and a variety of data formats from",
    "start": "132780",
    "end": "139020"
  },
  {
    "text": "semi-structured log files to completely non-structured binary data formats Hadoop is open source which obviously",
    "start": "139020",
    "end": "147180"
  },
  {
    "text": "you know makes sense which which makes it easier for you to to um to stay vendor neutral and not have",
    "start": "147180",
    "end": "153060"
  },
  {
    "text": "lock-ins but another aspect of the open source Community is that it's a really vibrant ecosystem there's a ton going on",
    "start": "153060",
    "end": "158819"
  },
  {
    "text": "with Hadoop a lot of interesting Technologies are coming up literally every couple of months and so we'll talk",
    "start": "158819",
    "end": "164099"
  },
  {
    "text": "about some of them later Hadoop has built there's a vibrant ecosystem of vendors that provide paid",
    "start": "164099",
    "end": "170700"
  },
  {
    "text": "for services and software that makes your life easy to build applications or",
    "start": "170700",
    "end": "175860"
  },
  {
    "text": "to troubleshoot your applications and provide a security features encryption features things like that",
    "start": "175860",
    "end": "180900"
  },
  {
    "text": "could you traditionally start as a batch oriented system so essentially you have batch programs that write that run",
    "start": "180900",
    "end": "187739"
  },
  {
    "text": "periodically however what's happening as of late is Hadoop is moving into",
    "start": "187739",
    "end": "193319"
  },
  {
    "text": "real-time processing meaning that customers that run Hadoop queries they want results faster and as a result",
    "start": "193319",
    "end": "199560"
  },
  {
    "text": "there's several technologies that now become available in Hadoop it makes it easier",
    "start": "199560",
    "end": "205099"
  },
  {
    "text": "their challenges with Hadoop if you use Hadoop on premise you have to figure out how to manage your hdfs which is Hadoop",
    "start": "205440",
    "end": "212459"
  },
  {
    "text": "distributed file system how to make the disaster recovery work a story work you",
    "start": "212459",
    "end": "217920"
  },
  {
    "text": "have to figure out how to manage your upgrades there's a significant system administration if you know it doesn't work you have to replace it so all that",
    "start": "217920",
    "end": "224640"
  },
  {
    "text": "complexity comes with with Hadoop on premise also you have to pay for expensive support contracts if you want",
    "start": "224640",
    "end": "230700"
  },
  {
    "text": "support um now one of the more important reasons why Hadoop on premise is so inflexible",
    "start": "230700",
    "end": "237060"
  },
  {
    "text": "is that you have to figure out in advance what Hardware do you want and then stick with that Hardware",
    "start": "237060",
    "end": "243239"
  },
  {
    "text": "what happens is that in fact Hadoop jobs are very diverse and so you know some Hadoop jobs and memory in terms of some",
    "start": "243239",
    "end": "249180"
  },
  {
    "start": "247000",
    "end": "247000"
  },
  {
    "text": "CPU intensive and so um picking the hardware up front sometimes could be a wrong choice",
    "start": "249180",
    "end": "255299"
  },
  {
    "text": "you can also run Hadoop on ec2 directly however if you do that there's still",
    "start": "255299",
    "end": "261060"
  },
  {
    "text": "some difficulties you have to figure out how to integrate with AWS storage Services you have to still independently",
    "start": "261060",
    "end": "266220"
  },
  {
    "text": "manage and monitor Hadoop clusters replace Hardware Etc so we really",
    "start": "266220",
    "end": "271320"
  },
  {
    "text": "believe that Amazon EMR is sort of the easiest and the best way to run Hadoop in the cloud",
    "start": "271320",
    "end": "277380"
  },
  {
    "text": "so why use EMR what sort of interesting features do we see our customers using on email first of all it's a fully",
    "start": "277380",
    "end": "282720"
  },
  {
    "text": "managed service right so you can issue an API command or uh through command through command line client or console",
    "start": "282720",
    "end": "289199"
  },
  {
    "text": "you can launch a Hadoop job Hadoop cluster now once the cluster is set up and running you can actually",
    "start": "289199",
    "end": "296220"
  },
  {
    "text": "um we manage that cluster while it's in progress so for instance if you have a a",
    "start": "296220",
    "end": "301500"
  },
  {
    "text": "problem with the disk or a problem with the faulty Hardware that Hardware gets replaced automatically behind the scenes",
    "start": "301500",
    "end": "307380"
  },
  {
    "text": "so you don't have to even worry about it we provide various tuning mechanisms for clusters for clusters and we enable you",
    "start": "307380",
    "end": "315540"
  },
  {
    "text": "to use the variety of pricing payment options that make you trim your costs",
    "start": "315540",
    "end": "322380"
  },
  {
    "text": "we support multiple data stores and we'll talk more about that in detail but so but you know your data originates in",
    "start": "322380",
    "end": "328560"
  },
  {
    "text": "multiple places and we believe that you should be able to use one system to access the data no matter where it",
    "start": "328560",
    "end": "333900"
  },
  {
    "text": "resides and Hadoop is a great choice for that you'll have some interesting unique features in ecosystem support that makes",
    "start": "333900",
    "end": "339900"
  },
  {
    "text": "really a nice package solution so let's start with the sort of basic uh",
    "start": "339900",
    "end": "346199"
  },
  {
    "text": "basic idea of how to use Amazon EMR so you have a variety of different data",
    "start": "346199",
    "end": "351780"
  },
  {
    "text": "stores so so you probably have the data already in S3 you probably have data and dynamodb or redshift you might be",
    "start": "351780",
    "end": "358080"
  },
  {
    "text": "generating data on application servers and then you need to push the data somewhere and typically you input that",
    "start": "358080",
    "end": "363600"
  },
  {
    "text": "data into some of those services then when you write your code your application it can be you know in r or",
    "start": "363600",
    "end": "370680"
  },
  {
    "text": "SQL statement or whatever the application you write and then you launch a Hadoop cluster so",
    "start": "370680",
    "end": "375780"
  },
  {
    "text": "you essentially issue an API command the elastic mappers and say launch a cluster and so what elastic mappers would do in",
    "start": "375780",
    "end": "381539"
  },
  {
    "text": "the end it'll actually take the it'll provision the name node and it'll provision the slates for the cluster",
    "start": "381539",
    "end": "387300"
  },
  {
    "text": "it'll install software on it you actually can configure Hadoop in tune",
    "start": "387300",
    "end": "393000"
  },
  {
    "text": "Hadoop on the EMR so we have this concept called bootstrap action that allows you to",
    "start": "393000",
    "end": "398160"
  },
  {
    "text": "essentially run arbitrary pieces of code in your cluster before the class before a node is checked in into the cluster so",
    "start": "398160",
    "end": "404880"
  },
  {
    "text": "for example you can tune the parameters you can install additional software uh you can you pretty much you know you",
    "start": "404880",
    "end": "410699"
  },
  {
    "text": "have root access to that to that cluster and the bootstrap actions are used widely by you know for tuning for",
    "start": "410699",
    "end": "417120"
  },
  {
    "text": "installing of software Etc once your cluster is running you have direct access to that cluster through",
    "start": "417120",
    "end": "424020"
  },
  {
    "text": "the apis but also through um so directly accessing um Hadoop so",
    "start": "424020",
    "end": "430380"
  },
  {
    "text": "you can have bi tools for example directly interfacing with Hadoop over jdbc or odbc drivers you can submit pig",
    "start": "430380",
    "end": "437580"
  },
  {
    "text": "or Hive interactive queries and then you have a choice to optionally Output your data in some other in some",
    "start": "437580",
    "end": "444539"
  },
  {
    "text": "other third-party system so for example you can keep the data on hdfs in a continuously running Hadoop but you can",
    "start": "444539",
    "end": "449940"
  },
  {
    "text": "choose to Output the data back into S3 or into dynamodb or some other system and if you do that",
    "start": "449940",
    "end": "456840"
  },
  {
    "text": "you can essentially kill your cluster and you can scale data separately from",
    "start": "456840",
    "end": "462780"
  },
  {
    "text": "CPU so essentially you can have all your data petabytes of data stored in S3 and",
    "start": "462780",
    "end": "468300"
  },
  {
    "text": "then you can have multiple clusters that run against that same data set that are in the EMR",
    "start": "468300",
    "end": "475400"
  },
  {
    "text": "so the Clusters are elastic you can customize the Clusters and you can reduce the cost and so so how some of",
    "start": "475560",
    "end": "481979"
  },
  {
    "text": "our customers do that so first you can choose your instance steps and you can and you can to some degree mix and match",
    "start": "481979",
    "end": "487979"
  },
  {
    "text": "instance types so if you want to run always on Hadoop cluster and store all",
    "start": "487979",
    "end": "493199"
  },
  {
    "text": "of your data on hdfs just like menu on-premise systems we have a great note",
    "start": "493199",
    "end": "498360"
  },
  {
    "text": "for that which is a hs1 8 extra large it has 48 terabytes of memory 48 terabytes",
    "start": "498360",
    "end": "504000"
  },
  {
    "text": "of disk against 24 uh 24 disks which delivers about three gigabytes per",
    "start": "504000",
    "end": "509879"
  },
  {
    "text": "second to the CPU and it has a 10 gigabit interconnect",
    "start": "509879",
    "end": "515459"
  },
  {
    "text": "so that is a system that you know a lot of our customers choose to go with if they want to have full list of hdfs",
    "start": "515459",
    "end": "522000"
  },
  {
    "text": "always on cluster now if you decide to use more of an S3 approach where you",
    "start": "522000",
    "end": "527040"
  },
  {
    "text": "store a lot of your data in S3 in your scan S3 with your EMR processing you might want to go with some of the other",
    "start": "527040",
    "end": "533459"
  },
  {
    "text": "instance types where you know the the disk is not as important as IO or memory",
    "start": "533459",
    "end": "539459"
  },
  {
    "text": "so we have memory intensive instances and we have CPU intensive instances and",
    "start": "539459",
    "end": "544560"
  },
  {
    "text": "so you know depending on your workload you might choose to to go with either M1 or M2 families or the CPU intensity C1",
    "start": "544560",
    "end": "552300"
  },
  {
    "text": "XL or cc2 families so now for example we have customers that",
    "start": "552300",
    "end": "558540"
  },
  {
    "text": "do image recognition on top of elastic mappers and those guys use something",
    "start": "558540",
    "end": "563940"
  },
  {
    "text": "like cc2 and then we have customers that doing log scan Pro log processing ETL a",
    "start": "563940",
    "end": "570540"
  },
  {
    "text": "great instance for that is M1 Excel so you can tune and and by the way you don't have to have a single cluster and",
    "start": "570540",
    "end": "577680"
  },
  {
    "text": "that's it you can have clusters that you can create for different job types so you optimize for the jobs that you have running",
    "start": "577680",
    "end": "586380"
  },
  {
    "text": "the other sort of access pattern is essentially you need to make a decision whether you want to run the transient clusters so you essentially have a badge",
    "start": "586380",
    "end": "594180"
  },
  {
    "text": "job that runs every day or every couple hours so you just spin a cluster up and then once it's done you persisted back",
    "start": "594180",
    "end": "600540"
  },
  {
    "text": "in the screen you shut it down or you can run 24x7 clusters where the load is",
    "start": "600540",
    "end": "606300"
  },
  {
    "text": "predictable and you have um you know and it's continuous in reality many of our customers choose",
    "start": "606300",
    "end": "613440"
  },
  {
    "text": "both of those options so they will take the always-on use case where they continues to run in some ETL processes for example",
    "start": "613440",
    "end": "620100"
  },
  {
    "text": "and they'd run an always-on cluster and then they would use the transient clusters for some one-off jobs for",
    "start": "620100",
    "end": "625860"
  },
  {
    "text": "example the other aspect you don't you know you don't have to just choose one cluster",
    "start": "625860",
    "end": "631500"
  },
  {
    "text": "size and stick with it elastic mappers enables you to resize the cluster while it's running so for example if you have",
    "start": "631500",
    "end": "638580"
  },
  {
    "text": "a job that takes 10 hours to run on four nodes you can halfway through that job",
    "start": "638580",
    "end": "644519"
  },
  {
    "text": "decide to increase the cluster by doubling it for example and so you'll run faster",
    "start": "644519",
    "end": "650220"
  },
  {
    "text": "one other aspect of this is that you know in practice a lot of our customers they they have clusters that run during",
    "start": "650220",
    "end": "656279"
  },
  {
    "text": "the day you know maybe for query processing but then at night they'll run complex ctls which the size of the",
    "start": "656279",
    "end": "662940"
  },
  {
    "text": "cluster had to increase for that complexityl processing so that would expand the cluster during the night and",
    "start": "662940",
    "end": "668459"
  },
  {
    "text": "then they shrink it during the day so you can manage the capacity you can match compute demands with the cluster",
    "start": "668459",
    "end": "674519"
  },
  {
    "text": "sizing now the other interesting aspect is you can use elastic mapreduce with all of",
    "start": "674519",
    "end": "681060"
  },
  {
    "text": "the available pricing that is available Institute and specifically spot NRI are very effective for um sort of your",
    "start": "681060",
    "end": "687240"
  },
  {
    "text": "operational uh cluster setup so let's talk about spot so elastic mapper to",
    "start": "687240",
    "end": "693000"
  },
  {
    "text": "split the Hadoop nodes into two categories there's a task note and there's a core node the task nodes the",
    "start": "693000",
    "end": "698940"
  },
  {
    "text": "ones that run the jobs but they don't hold any data on them they don't hold hdfs on them and in the core notes they",
    "start": "698940",
    "end": "705720"
  },
  {
    "text": "do the data processing but they also hold data so they hold hdfs so typically customers run clusters with the data",
    "start": "705720",
    "end": "713700"
  },
  {
    "text": "with hdfs on them with on demand so they have clusters that are running on demand instances and then they would augment",
    "start": "713700",
    "end": "719820"
  },
  {
    "text": "the cluster with spot what's interesting about spot is is it's about 90 percent of ec2 price it's very very inexpensive",
    "start": "719820",
    "end": "727620"
  },
  {
    "text": "but the opposite part of it is that sometimes these nodes go away and they have to be reclaimed by the service so",
    "start": "727620",
    "end": "734399"
  },
  {
    "text": "EMR manages all that complexity you just have to say how many machines you want in a spot and how many and how much",
    "start": "734399",
    "end": "740339"
  },
  {
    "text": "you're willing to pay for it EMR will regulate that number based on uh whether",
    "start": "740339",
    "end": "745500"
  },
  {
    "text": "or not the spot service has the capacity and of course if you have a predictable load and you know what instant steps you",
    "start": "745500",
    "end": "752399"
  },
  {
    "text": "want a number of those instances you can greatly reduce your cost by subscribing to reserved instances which is about 65",
    "start": "752399",
    "end": "757800"
  },
  {
    "text": "percent off of on-demand pricing so you can use a spot you can use our",
    "start": "757800",
    "end": "763320"
  },
  {
    "text": "eyes and you can use on demand all in the context of the same cluster",
    "start": "763320",
    "end": "769680"
  },
  {
    "text": "one other interesting fact about the task and core notes you can mix and match different instance types so for",
    "start": "769680",
    "end": "775139"
  },
  {
    "text": "example you can run the core nodes with the hs1 and then augment the task notes with you know C1 family for example",
    "start": "775139",
    "end": "783920"
  },
  {
    "text": "now Amazon as I mentioned elastic mappers integrates with multiple data stores and we believe that it's",
    "start": "784800",
    "end": "792180"
  },
  {
    "text": "really important to not just provide hdfs access which we obviously provide but provide access to all of the other",
    "start": "792180",
    "end": "798480"
  },
  {
    "text": "stores that you might use already for your data needs so you very likely use",
    "start": "798480",
    "end": "803519"
  },
  {
    "text": "S3 to store your logs or images or any other information you might want to you know crunch with the with Amazon EMR",
    "start": "803519",
    "end": "810180"
  },
  {
    "text": "you're likely using hdfs there's other access points like a Dynamo GB and the",
    "start": "810180",
    "end": "815760"
  },
  {
    "text": "redshift and Amazon Glacier and the relational database Services all of which has reliable connectors to EMR and",
    "start": "815760",
    "end": "824639"
  },
  {
    "text": "access through the applications that you write so for instance you can write a hive script that uses that queries the",
    "start": "824639",
    "end": "831240"
  },
  {
    "text": "data in S3 query the data and dynamodb and the frankly hdfs and the one single",
    "start": "831240",
    "end": "836940"
  },
  {
    "text": "SQL statement so most of the time people end up doing",
    "start": "836940",
    "end": "843480"
  },
  {
    "start": "840000",
    "end": "840000"
  },
  {
    "text": "some hybrid architectures where you have multiple data stores using in the same cluster so for instance you might have",
    "start": "843480",
    "end": "849839"
  },
  {
    "text": "some data stored in S3 and then you might have transient clusters running against that data store",
    "start": "849839",
    "end": "855300"
  },
  {
    "text": "but we also might want to run the always on cluster against the same bucket that",
    "start": "855300",
    "end": "860339"
  },
  {
    "text": "you have and periodically sync to the bucket so you have hdfs and S3 access pattern",
    "start": "860339",
    "end": "866639"
  },
  {
    "text": "so the reason why it's really a good approach is because S3 provides 11 9's durability it's a very durable store you",
    "start": "866639",
    "end": "873480"
  },
  {
    "text": "also can can prevent your customers from shooting themselves in the foot you're using S3 so for example some of our",
    "start": "873480",
    "end": "879899"
  },
  {
    "text": "customers complain to us hey look you know I accidentally deleted data off of hdfs can we help restore with that you",
    "start": "879899",
    "end": "886260"
  },
  {
    "text": "know it's really hard problem and on-premise typically customers deal with that by doing backups or replications",
    "start": "886260",
    "end": "892260"
  },
  {
    "text": "off of hdfs cluster sampler cells well if you have if you use S3 you can enable",
    "start": "892260",
    "end": "898019"
  },
  {
    "text": "versioning on your S3 bucket and so none of the deletes will actually happen I just increment the version of the files",
    "start": "898019",
    "end": "903959"
  },
  {
    "text": "and then you can set up live policy on top of those files so that you know versions with the you know old versions",
    "start": "903959",
    "end": "910500"
  },
  {
    "text": "could expire in 90 days or you can archive them to Glacier 90 days so you have this really great Disaster Recovery",
    "start": "910500",
    "end": "916560"
  },
  {
    "text": "story with S3 the other benefit is you can run multiple clusters against the same data store you can partition us",
    "start": "916560",
    "end": "922139"
  },
  {
    "text": "through data using permissions and Echo so that some departments for example don't go and look at the data of the",
    "start": "922139",
    "end": "929040"
  },
  {
    "text": "other departments if you have a an Enterprise company the other pattern that we see used a lot",
    "start": "929040",
    "end": "935760"
  },
  {
    "text": "is Amazon EMR in conjunction with redshift when redshift came out folks were saying",
    "start": "935760",
    "end": "941519"
  },
  {
    "start": "937000",
    "end": "937000"
  },
  {
    "text": "well you know is redshift a replacement for EMR are there competitive technologists they're really in reality",
    "start": "941519",
    "end": "946620"
  },
  {
    "text": "a highly complementary Technologies EMR is a one use case is a big ETL into",
    "start": "946620",
    "end": "953160"
  },
  {
    "text": "redshift so for example if you have log files you need to scrub them before you load them into redshift you might as used a lot for that if you use you",
    "start": "953160",
    "end": "959820"
  },
  {
    "text": "usually Define functions for example you can actually grab data from redshift and run these user-defined functions inside",
    "start": "959820",
    "end": "965579"
  },
  {
    "text": "Emi and join some other data sets you can run our programs on top of redshift",
    "start": "965579",
    "end": "970620"
  },
  {
    "text": "data for example by exporting data from redshift into EMR and there's a really fast interconnect that we recently",
    "start": "970620",
    "end": "976079"
  },
  {
    "text": "launched it enable enables you to load data from EMR hdfs directly into",
    "start": "976079",
    "end": "982440"
  },
  {
    "text": "redshare",
    "start": "982440",
    "end": "985160"
  },
  {
    "text": "so Amazon elastic mappers provides you know a significant vibrant ecosystem on top of on top of just standard Hadoop",
    "start": "987839",
    "end": "995880"
  },
  {
    "text": "we recently launched Hadoop 2.0 or actually Hadoop 2.2 which is a ga version of Hadoop second generation it",
    "start": "995880",
    "end": "1002959"
  },
  {
    "text": "has a an interesting framework called yarn that allows you to launch non-maproduced applications inside",
    "start": "1002959",
    "end": "1009079"
  },
  {
    "text": "Hadoop which will unleash all sorts of use cases now but that that",
    "start": "1009079",
    "end": "1014420"
  },
  {
    "text": "um that version comes with a lot of other interesting goodies and obviously it has mapreduce and Avro support and Thrift support mahad which is a",
    "start": "1014420",
    "end": "1021259"
  },
  {
    "text": "statistical Library um open machine learning machine learning there's databases that come",
    "start": "1021259",
    "end": "1026780"
  },
  {
    "text": "with Hadoop which is age base we'll talk a little bit about in a second um there's diverse queer languages such",
    "start": "1026780",
    "end": "1031880"
  },
  {
    "text": "as Pig and Hive so there are a lot of Open Source tools that are being built",
    "start": "1031880",
    "end": "1037880"
  },
  {
    "text": "so let's review you know one of them which is a hive right so Hive is a SQL like component that allows you to write",
    "start": "1037880",
    "end": "1044240"
  },
  {
    "start": "1041000",
    "end": "1041000"
  },
  {
    "text": "SQL against EMR right so you can run a query in Hive against your EMR cluster",
    "start": "1044240",
    "end": "1050299"
  },
  {
    "text": "it's you can build a sort of a data warehouse on top of Hadoop we integrated Hive with S3 and with dynamodb so you",
    "start": "1050299",
    "end": "1058820"
  },
  {
    "text": "actually can as I mentioned earlier but I want to repeat it because it's really quite uh interesting uh feature you can",
    "start": "1058820",
    "end": "1064760"
  },
  {
    "text": "run Hive query that joins tables across all of the data sources",
    "start": "1064760",
    "end": "1070580"
  },
  {
    "text": "that I mentioned so for example if you have a table in dynamodb that has customer clicks and you have table in",
    "start": "1070580",
    "end": "1076640"
  },
  {
    "text": "hdfs it has customer account information and you have table and S3 that might",
    "start": "1076640",
    "end": "1082340"
  },
  {
    "text": "have some order information you can write a single statement in Hive that",
    "start": "1082340",
    "end": "1087559"
  },
  {
    "text": "will join all those threes all those three storages to all those three tables together",
    "start": "1087559",
    "end": "1092780"
  },
  {
    "text": "and it uses fast interconnect that we've built that utilizes the full power of your capacity in Dynamo for example for",
    "start": "1092780",
    "end": "1099679"
  },
  {
    "text": "reads well full cap full capacity or however much you you want to allocate for that query because it's a flexible",
    "start": "1099679",
    "end": "1105860"
  },
  {
    "text": "query you can actually say look I want to use only five percent of my available Dynamo capacity but I'll use the capacity very efficiently to do that to",
    "start": "1105860",
    "end": "1113000"
  },
  {
    "text": "do that integration so it's a really powerful feature if you have multiple if you have data in multiple form in",
    "start": "1113000",
    "end": "1118039"
  },
  {
    "text": "multiple stores now as I mentioned earlier a lot of our",
    "start": "1118039",
    "end": "1125419"
  },
  {
    "start": "1124000",
    "end": "1124000"
  },
  {
    "text": "customers are pushing us and the Hadoop ecosystem to drive faster and faster into sort of near real-time",
    "start": "1125419",
    "end": "1133600"
  },
  {
    "text": "operations so they want their queries to return faster at least in some data sets",
    "start": "1133600",
    "end": "1138679"
  },
  {
    "text": "and so Amazon um EMR essentially has a integration with age base which is a nosql database",
    "start": "1138679",
    "end": "1146500"
  },
  {
    "text": "columnar database which allows you you know to store billions of rows across",
    "start": "1146500",
    "end": "1151520"
  },
  {
    "text": "millions of columns it's really good at range queries for example and it's fully",
    "start": "1151520",
    "end": "1157400"
  },
  {
    "text": "integrated with EMR one interesting feature we've built in that so in that solution is we actually have a backup",
    "start": "1157400",
    "end": "1162559"
  },
  {
    "text": "and restore from Dynamo into from age base into S3 so you can have scheduled",
    "start": "1162559",
    "end": "1167600"
  },
  {
    "text": "backup mechanism that automatically backs up your data into S3 and then you can recover from that backup on another",
    "start": "1167600",
    "end": "1173780"
  },
  {
    "text": "availability zone or in case a disaster recovery just restart that uh that cluster",
    "start": "1173780",
    "end": "1180919"
  },
  {
    "text": "another um solution that is available now in the",
    "start": "1180919",
    "end": "1186679"
  },
  {
    "start": "1186000",
    "end": "1186000"
  },
  {
    "text": "market and is available on Amazon EMR is a spark and shock which is in memory mapreduce for faster queries so shock is",
    "start": "1186679",
    "end": "1193760"
  },
  {
    "text": "essentially a SQL like language that the hive qlik language that enables you to",
    "start": "1193760",
    "end": "1199220"
  },
  {
    "text": "uh run SQL queries on the data that's in memory so for example you can have a you can spin up an EMR cluster load data",
    "start": "1199220",
    "end": "1206840"
  },
  {
    "text": "from hdfs into spark memory database and then you can have queries running really",
    "start": "1206840",
    "end": "1213740"
  },
  {
    "text": "really fast Berkeley actually did a benchmark test against spark Impala",
    "start": "1213740",
    "end": "1220100"
  },
  {
    "text": "dynamodb and some and some other systems and actually there's a you know there's",
    "start": "1220100",
    "end": "1225500"
  },
  {
    "text": "a good write-up you guys can look it up today we're pronouncing that we're actually going to launch Impala which is",
    "start": "1225500",
    "end": "1232460"
  },
  {
    "text": "another database product on top of VMR so the Impala product will allow you to",
    "start": "1232460",
    "end": "1238280"
  },
  {
    "text": "run parallel database queries on top of Hadoop again you can use a SQL interface and query data in hdfs on your cluster",
    "start": "1238280",
    "end": "1248679"
  },
  {
    "text": "this this morning Vernon announced uh streaming streaming service which allows",
    "start": "1249020",
    "end": "1255020"
  },
  {
    "text": "you to store stream data or stream data into the cloud which is called Amazon Kinesis",
    "start": "1255020",
    "end": "1260660"
  },
  {
    "text": "um so elastic mappers will provide a connector into that service so you'll be",
    "start": "1260660",
    "end": "1265760"
  },
  {
    "text": "able to use Hive Pig and mapreduce jobs so all of the infrastructure that you already have with EMR against that data",
    "start": "1265760",
    "end": "1272600"
  },
  {
    "text": "store as well so you can build for example things like micro batching where you can have you know if you have logs",
    "start": "1272600",
    "end": "1280520"
  },
  {
    "text": "arriving to Kinesis say you know continuously every five minutes you can have a mapreduce job that would run grab",
    "start": "1280520",
    "end": "1287720"
  },
  {
    "text": "those logs do some aggregation and push the results into your Dynamo table for",
    "start": "1287720",
    "end": "1293000"
  },
  {
    "text": "example or someplace else so you know this is a you know a really exciting um connector that will help you build",
    "start": "1293000",
    "end": "1299840"
  },
  {
    "text": "additional use cases on top of EMR so to wrap things up we have",
    "start": "1299840",
    "end": "1307400"
  },
  {
    "text": "um you know elastic mappers really helps you operate Hadoop clusters in the cloud so",
    "start": "1307400",
    "end": "1314000"
  },
  {
    "text": "it provides elastic clusters and enables you to optimize your costs to save money on data processing it has rapid tuned in",
    "start": "1314000",
    "end": "1321080"
  },
  {
    "text": "provisioning of clusters so you can launch as many as you need for your applications and it integrates with a",
    "start": "1321080",
    "end": "1326659"
  },
  {
    "text": "lot of data stores in AWS which makes it really easy to experiment and to build",
    "start": "1326659",
    "end": "1332059"
  },
  {
    "text": "Innovative database Innovative data products so we have",
    "start": "1332059",
    "end": "1337539"
  },
  {
    "text": "significant number of partners that that build on top of elastic mappers so you",
    "start": "1337539",
    "end": "1343700"
  },
  {
    "text": "know we have bi tools from the top vendors like microstrategy and sap and just soft and Tableau we have",
    "start": "1343700",
    "end": "1351080"
  },
  {
    "text": "alternative distributions in EMR because we believe that there should be a choice and if you want to be able to run map r",
    "start": "1351080",
    "end": "1357440"
  },
  {
    "text": "on top of EMR you shouldn't sacrifice all the features that that EMR provides",
    "start": "1357440",
    "end": "1363200"
  },
  {
    "text": "so we actually work with these vendors to make sure that distributions are available",
    "start": "1363200",
    "end": "1368559"
  },
  {
    "text": "we have thousands of thousands of customers that use EMR today and we'll have couple of customers present right",
    "start": "1370940",
    "end": "1377539"
  },
  {
    "text": "now on their use case and how they're using EMR so first I'd like to introduce introduce Eva Tech who is the director",
    "start": "1377539",
    "end": "1383120"
  },
  {
    "text": "of Big Data platform and Netflix and she's going to talk about how they're using EMR and Netflix",
    "start": "1383120",
    "end": "1390039"
  },
  {
    "text": "thank you",
    "start": "1393039",
    "end": "1396039"
  },
  {
    "text": "hi everybody my name is Eva say and I manage the data Big Data platform team at Netflix",
    "start": "1399140",
    "end": "1406658"
  },
  {
    "text": "so at Netflix we have one unified hadoop-based data analytics platform that is shared by the whole company and",
    "start": "1407480",
    "end": "1414740"
  },
  {
    "text": "we build it on top of EMR and today I'm going to walk through some of the key decisions that we make and",
    "start": "1414740",
    "end": "1420980"
  },
  {
    "text": "things that we do to make it scalable for our use cases",
    "start": "1420980",
    "end": "1426100"
  },
  {
    "text": "so before I do that I'm just going to give a very short history of why we're in the cloud",
    "start": "1426559",
    "end": "1432919"
  },
  {
    "text": "so in 2008 we have a database corruption in our data center that disrupt DVD",
    "start": "1432919",
    "end": "1439159"
  },
  {
    "text": "deliveries for our customer for a few days so that clearly indicates that we need a more fault tolerant system",
    "start": "1439159",
    "end": "1446539"
  },
  {
    "text": "and in 2009 our streaming services expanding rapidly and we are also",
    "start": "1446539",
    "end": "1452299"
  },
  {
    "text": "planning for Global expansion so we want to have a globally scalable system",
    "start": "1452299",
    "end": "1458000"
  },
  {
    "text": "going to the cloud is The Logical next step and AWS is the most mature Cloud infrastructure that we pick",
    "start": "1458000",
    "end": "1464600"
  },
  {
    "text": "and it's worthwhile to mention that because we're moving all the Netflix services to the cloud it is only logical",
    "start": "1464600",
    "end": "1470120"
  },
  {
    "text": "for us to build our data analytics platform in the cloud also where the data is being generated",
    "start": "1470120",
    "end": "1477340"
  },
  {
    "text": "so how do we exactly achieve the scalability that we need when I mention scalability I don't only",
    "start": "1478159",
    "end": "1484580"
  },
  {
    "text": "mean infrastructure scalability I also mean how do we scale our team better",
    "start": "1484580",
    "end": "1490400"
  },
  {
    "text": "so if there is one single most important key architectural choice that we make is",
    "start": "1490400",
    "end": "1495919"
  },
  {
    "text": "this one we decided to separate compute and storage layer specifically we decided that S3 is our",
    "start": "1495919",
    "end": "1503000"
  },
  {
    "text": "data warehouse it is our source of truth and that's where we store all our data",
    "start": "1503000",
    "end": "1509020"
  },
  {
    "text": "if we choose hdfs as our storage layer we will need to do a lot more work to replicate the Clusters and to make keep",
    "start": "1509659",
    "end": "1516620"
  },
  {
    "text": "them in sync so we decided to use S3 directly which is highly available scalable and in level 9 durable and we",
    "start": "1516620",
    "end": "1524960"
  },
  {
    "text": "only use hdfs as temporary storage when we spin off Hive and pick jobs that spin",
    "start": "1524960",
    "end": "1530240"
  },
  {
    "text": "up multiple Hadoop jobs so how does S3 throughput compare to",
    "start": "1530240",
    "end": "1535700"
  },
  {
    "text": "hdms that's usually a questions that we always get um at Netflix we are using sequence file",
    "start": "1535700",
    "end": "1541640"
  },
  {
    "text": "right now so with that file format when we Benchmark read throughput S3 is about",
    "start": "1541640",
    "end": "1546919"
  },
  {
    "text": "five to ten percent slower than hdfs but given our nature of having long-running",
    "start": "1546919",
    "end": "1552260"
  },
  {
    "text": "job and multi-state jobs that's not really significant and for right throughput at times it is inconclusive",
    "start": "1552260",
    "end": "1559340"
  },
  {
    "text": "but at times we find that S3 is even faster than hdfs and that could be because S3 is eventually consistent and",
    "start": "1559340",
    "end": "1565700"
  },
  {
    "text": "S3 has a much bigger Fleet of servers so what we're going to do next is to",
    "start": "1565700",
    "end": "1570980"
  },
  {
    "text": "upgrade our file format to either orc or parquet file format which is one of the two latest column and file storage",
    "start": "1570980",
    "end": "1578000"
  },
  {
    "text": "format and we will Benchmark that on S3 and see the results these two file",
    "start": "1578000",
    "end": "1583640"
  },
  {
    "text": "format has very different read write and C characteristics and we'll we'll see how that goes and being able to work",
    "start": "1583640",
    "end": "1589940"
  },
  {
    "text": "well in S3 is a key consideration for us so the next questions or the next prop",
    "start": "1589940",
    "end": "1597080"
  },
  {
    "text": "uh kind of issue that we need to deal with is S3 being eventually consistent so by the way of convention we mitigated",
    "start": "1597080",
    "end": "1604220"
  },
  {
    "text": "a lot of the inconsistent listing Behavior the two things we do is we do",
    "start": "1604220",
    "end": "1609500"
  },
  {
    "text": "not when we run jobs we do not overwrite into the same key space and we also avoid writing a lot of small files by",
    "start": "1609500",
    "end": "1616700"
  },
  {
    "text": "doing that we mitigate most of the inconsistent listing Behavior but we",
    "start": "1616700",
    "end": "1622340"
  },
  {
    "text": "also built a library called sampert in the past couple of months we built that so that we could kind of have visibility",
    "start": "1622340",
    "end": "1630200"
  },
  {
    "text": "into the problem as it happens what it is is a library that we inject into our Hadoop clusters",
    "start": "1630200",
    "end": "1636580"
  },
  {
    "text": "we wrap around all the file system calls that we're going to S3 when we create a",
    "start": "1636580",
    "end": "1642200"
  },
  {
    "text": "key Nursery we put a key in dynamodb when we do a listing on s3b compared to what we see in dynamodb and what we",
    "start": "1642200",
    "end": "1649640"
  },
  {
    "text": "found out actually we give user a choice when inconsistent listing happen the",
    "start": "1649640",
    "end": "1654860"
  },
  {
    "text": "user can either decide to fail or continue the job after X minutes of retry what we found out is that but when",
    "start": "1654860",
    "end": "1662360"
  },
  {
    "text": "we do within a day for the last month or so we have about tens of inconsistent",
    "start": "1662360",
    "end": "1668960"
  },
  {
    "text": "listing that happened and after or within 15 minutes of retry most of them",
    "start": "1668960",
    "end": "1674419"
  },
  {
    "text": "or 70 of them are gone and for the remaining thirty percent it could take up to hours to become consistent but the",
    "start": "1674419",
    "end": "1681620"
  },
  {
    "text": "key point is with sample the user have visibility and they could decide what to do when the issue happened",
    "start": "1681620",
    "end": "1689620"
  },
  {
    "text": "so the next key decision we make is that we have multiple clusters we have an SLA cluster for prot job at",
    "start": "1690200",
    "end": "1697159"
  },
  {
    "text": "Hall cluster for ad hoc job they're physically isolated so that these are two different job types that we run so",
    "start": "1697159",
    "end": "1703340"
  },
  {
    "text": "that they're in two different environments and we also put them in two different zones so that they are they",
    "start": "1703340",
    "end": "1709159"
  },
  {
    "text": "are more thought tolerant we also spin up bonus clusters in three",
    "start": "1709159",
    "end": "1714200"
  },
  {
    "text": "different zones doing 12 hours of Netflix traffic in the US so the trough",
    "start": "1714200",
    "end": "1720080"
  },
  {
    "text": "time is during night time in the U.S at that time we have a lot of excess capacity across the zones because we do",
    "start": "1720080",
    "end": "1726740"
  },
  {
    "text": "Reserve capacity so the asgs for our Netflix site traffic would shrink down",
    "start": "1726740",
    "end": "1732200"
  },
  {
    "text": "we would leverage these excess capacity spin up EMR clusters and run ETL and",
    "start": "1732200",
    "end": "1738080"
  },
  {
    "text": "data analytics jobs so that we could get the data by the morning",
    "start": "1738080",
    "end": "1744100"
  },
  {
    "text": "the third thing we do is that we have a unified and global data pipelines the",
    "start": "1745100",
    "end": "1751340"
  },
  {
    "text": "two main pipelines there's events Pipeline and dimension pipelines they're both hadoop-based pipeline in some",
    "start": "1751340",
    "end": "1758299"
  },
  {
    "text": "fashion the event pipelines span across all the Netflix services and spent",
    "start": "1758299",
    "end": "1763700"
  },
  {
    "text": "across all the device types we have and we also have one unified semi-format",
    "start": "1763700",
    "end": "1770020"
  },
  {
    "text": "data format for all across all the data sources and we have over 100 data",
    "start": "1770020",
    "end": "1775640"
  },
  {
    "text": "sources for the dimension pipeline is called a goodness we have tens of them running it",
    "start": "1775640",
    "end": "1781580"
  },
  {
    "text": "does bulk extraction for more Cassandra online data store what it does is basically restore the SS tables run",
    "start": "1781580",
    "end": "1788720"
  },
  {
    "text": "Hadoop jobs on top and convert them into high dimension tables so with these two unified pipelines for",
    "start": "1788720",
    "end": "1795320"
  },
  {
    "text": "an engineer to add in data source it becomes a lot easier and Genie is something that I'll mention",
    "start": "1795320",
    "end": "1801679"
  },
  {
    "text": "in the next slide so last but not least we innovate on top",
    "start": "1801679",
    "end": "1807020"
  },
  {
    "text": "of the Hadoop layers to build services and tools so that our team can scale better so that our system can become a",
    "start": "1807020",
    "end": "1813620"
  },
  {
    "text": "lot more self-serve",
    "start": "1813620",
    "end": "1816520"
  },
  {
    "text": "so on top of hadoot layer we have three services that we built Genie is an",
    "start": "1820159",
    "end": "1825200"
  },
  {
    "text": "abstraction layer on top of Hadoop a user could actually issue a high pick or Hadoop job by doing a rest API call and",
    "start": "1825200",
    "end": "1833720"
  },
  {
    "text": "Genie is going to decide which clusters to run it on based on text and based on",
    "start": "1833720",
    "end": "1838940"
  },
  {
    "text": "statuses of the cluster which means Hadoop administrators can actually pull a particular Hadoop cluster out of the",
    "start": "1838940",
    "end": "1845480"
  },
  {
    "text": "fleet or we could do red black pushes to upgrade our Hadoop cluster without impacting the users",
    "start": "1845480",
    "end": "1852020"
  },
  {
    "text": "Franklin is a metadata service that we're currently building the goal of it is to become a data catalog across all",
    "start": "1852020",
    "end": "1859039"
  },
  {
    "text": "the data sources that we have including Hive pick RDS S3 files that are just",
    "start": "1859039",
    "end": "1864860"
  },
  {
    "text": "landed on S3 teradata and redshift Event Services and orchestration layer",
    "start": "1864860",
    "end": "1871399"
  },
  {
    "text": "that is used to kick off ETL job flows when certain event or conditions is",
    "start": "1871399",
    "end": "1878120"
  },
  {
    "text": "satisfied so we also manage a couple of CLI gateways for the users to run ad-hoc",
    "start": "1878120",
    "end": "1885140"
  },
  {
    "text": "queries and to access the ad hoc cluster specifically on top of these Services we build tools",
    "start": "1885140",
    "end": "1891980"
  },
  {
    "text": "lipstick is a pig is a is a graphical representation of Peg workflow that show",
    "start": "1891980",
    "end": "1898399"
  },
  {
    "text": "users job information as it executes sting is a visualization and reporting tool that we built on top of a system",
    "start": "1898399",
    "end": "1904700"
  },
  {
    "text": "that run a set of schedule Hive jobs based on user specification",
    "start": "1904700",
    "end": "1911000"
  },
  {
    "text": "forklift is a data movement too that moves data between S3 and teradata for example we have data that we want to",
    "start": "1911000",
    "end": "1917299"
  },
  {
    "text": "download from S3 into teradata for reporting using microstrategy Looper is a ETL backfield tool that we",
    "start": "1917299",
    "end": "1924620"
  },
  {
    "text": "run ETL back few jobs ATL developer would probably want to run ETA backfield job for the past X weeks or X number of",
    "start": "1924620",
    "end": "1932000"
  },
  {
    "text": "days and it would run the job in higher parallelism to get it done Sherlock and Visa are data analytics",
    "start": "1932000",
    "end": "1939380"
  },
  {
    "text": "tools that we built on top to to let us look at Hadoop metrics within a cluster",
    "start": "1939380",
    "end": "1945380"
  },
  {
    "text": "so specifically Sherlock allowed us to look at aggregated Hadoop metrics within",
    "start": "1945380",
    "end": "1950779"
  },
  {
    "text": "a cluster for example the number of S3 bytes read for day in total and in miso",
    "start": "1950779",
    "end": "1956960"
  },
  {
    "text": "basically allow us to discover a specific job based on tax based on attributes and dive deep into that",
    "start": "1956960",
    "end": "1962899"
  },
  {
    "text": "specific Hadoop job and see how it ran so it's worthwhile to mention that both Genie and lipsticks are Netflix open",
    "start": "1962899",
    "end": "1969320"
  },
  {
    "text": "source project our goal is to open source the components that we built so that we could be more scalable and",
    "start": "1969320",
    "end": "1975260"
  },
  {
    "text": "Leverage The Open Source community so putting all into perspective this is",
    "start": "1975260",
    "end": "1982520"
  },
  {
    "text": "the current scale of our system as we more add more subscribers and viewing",
    "start": "1982520",
    "end": "1988100"
  },
  {
    "text": "hours the amount of data and footprint of the Clusters will continue to increase",
    "start": "1988100",
    "end": "1995320"
  },
  {
    "start": "1996000",
    "end": "1996000"
  },
  {
    "text": "so I'm going to run through some of the use cases of how we use our system it's obvious that we allow users to access",
    "start": "1996260",
    "end": "2002140"
  },
  {
    "text": "our ad hoc cluster to do ad hoc jobs for data exploration or maybe they want to",
    "start": "2002140",
    "end": "2007419"
  },
  {
    "text": "do some testing of their ETL jobs like I mentioned we have SIMPLE reporting tool built on top of our",
    "start": "2007419",
    "end": "2014140"
  },
  {
    "start": "2014000",
    "end": "2014000"
  },
  {
    "text": "system and this is just a screenshot of sting which is a simple visualization",
    "start": "2014140",
    "end": "2019240"
  },
  {
    "text": "and reporting to that is based on a set of Hive jobs that users specified",
    "start": "2019240",
    "end": "2025240"
  },
  {
    "text": "and we also run a lot of ETL jobs on our system pig is our ETL language of choice we already migrated most about ETL jobs",
    "start": "2025240",
    "end": "2033100"
  },
  {
    "text": "from the data center to the cloud and we are building a lot more on it and I'm going to run through a few of the ETL",
    "start": "2033100",
    "end": "2039640"
  },
  {
    "text": "use cases as well so we use our system to build and train",
    "start": "2039640",
    "end": "2044679"
  },
  {
    "text": "analytics and statistical models and with this model and combining with",
    "start": "2044679",
    "end": "2049960"
  },
  {
    "text": "hundreds of signals That We Gather from the site we do scoring the scoring results would tell us",
    "start": "2049960",
    "end": "2057220"
  },
  {
    "text": "what recommendations we should give to each user what are the search results we should service based on the search terms",
    "start": "2057220",
    "end": "2064000"
  },
  {
    "text": "and for example the predicted lifetime value of new users for marketing purposes these are just a few examples",
    "start": "2064000",
    "end": "2070540"
  },
  {
    "text": "and scoring can be done on the Hadoop clusters or the system that we build or sometimes data scientists has their own",
    "start": "2070540",
    "end": "2076898"
  },
  {
    "text": "R cluster to run it a b testing so we run up to 100 test",
    "start": "2076899",
    "end": "2083138"
  },
  {
    "text": "cases on our site at any given time and each of them has a number of test cells",
    "start": "2083139",
    "end": "2089500"
  },
  {
    "text": "and all these test cells and test cases we measure them based on a set of standard metrics and against a set of",
    "start": "2089500",
    "end": "2096158"
  },
  {
    "text": "standard dimensions so for example we will be running a lot of user experience related a b tests on our site",
    "start": "2096159",
    "end": "2102300"
  },
  {
    "text": "a common metric that we always look at is play duration and we will look at it against Dimension like user segmentation",
    "start": "2102300",
    "end": "2110760"
  },
  {
    "text": "the geolocation device types things like that so what we do is we have regular we",
    "start": "2110760",
    "end": "2118300"
  },
  {
    "text": "have jobs running in our system regularly to calculate all these metrics and an internal team a vertical team",
    "start": "2118300",
    "end": "2125020"
  },
  {
    "text": "that actually built a two called ignite that actually pulled these data from our system and actually display them for our",
    "start": "2125020",
    "end": "2131859"
  },
  {
    "text": "product managers and Engineers to actually compare the results and it's a",
    "start": "2131859",
    "end": "2136960"
  },
  {
    "text": "pretty interactive UI so openconnect is Netflix CDN Solutions",
    "start": "2136960",
    "end": "2143500"
  },
  {
    "text": "we provide to our to ISP providers who participate in this program a storage",
    "start": "2143500",
    "end": "2151000"
  },
  {
    "text": "appliance that cache the popular movies and TV shows in order for them to save transit cost",
    "start": "2151000",
    "end": "2158820"
  },
  {
    "text": "so we have jobs in our system to determine what are the most popular movies and TV shows based on different",
    "start": "2158820",
    "end": "2165520"
  },
  {
    "text": "locations and we use that results to populate the cash for these appliances",
    "start": "2165520",
    "end": "2172560"
  },
  {
    "text": "so I guess I'm going to summarize by saying what what makes EMR scale for us it allow us to build a very scalable",
    "start": "2173560",
    "end": "2180460"
  },
  {
    "text": "platform on top and it allow our team to be a lot more agile in the process of",
    "start": "2180460",
    "end": "2186280"
  },
  {
    "text": "doing so um emo does all the heavy lifting of integrating Hadoop on EMR on ec2 and AWS",
    "start": "2186280",
    "end": "2194579"
  },
  {
    "text": "specifically they have a ruby client that allow us to spin up clusters on different node types so that so that we",
    "start": "2194579",
    "end": "2200380"
  },
  {
    "start": "2196000",
    "end": "2196000"
  },
  {
    "text": "can do experiment and do Benchmark a lot easier they have expense rank S3",
    "start": "2200380",
    "end": "2205720"
  },
  {
    "text": "integration and cloudwatch these are the features that we Leverage",
    "start": "2205720",
    "end": "2210540"
  },
  {
    "text": "so it let us focus on Innovation and build a more robust platform and",
    "start": "2211020",
    "end": "2216160"
  },
  {
    "text": "solution and also we work very closely with the EMR team for tactical issues when it happens and we also have a very",
    "start": "2216160",
    "end": "2222700"
  },
  {
    "text": "collaborative relationship with them to discuss strategic roadmap items so to",
    "start": "2222700",
    "end": "2228160"
  },
  {
    "text": "bring our infrastructure to the next level",
    "start": "2228160",
    "end": "2233099"
  },
  {
    "start": "2233000",
    "end": "2233000"
  },
  {
    "text": "so these are the up and coming um EMR enhancements hopefully that we could",
    "start": "2233320",
    "end": "2239560"
  },
  {
    "text": "leverage and these are the outcome of some of the collaborative discussions we have with EMR team and I'm just picking",
    "start": "2239560",
    "end": "2245680"
  },
  {
    "text": "out a few to show here heterogeneous nail cluster is important to us because that way if we have",
    "start": "2245680",
    "end": "2252099"
  },
  {
    "text": "multiple different node type in a costume it would allow us more effectively leverage excess capacity",
    "start": "2252099",
    "end": "2258660"
  },
  {
    "text": "during 12 hours Auto expansion so we currently use",
    "start": "2258660",
    "end": "2263920"
  },
  {
    "text": "expense rank already we expand our SLA cluster when we need to do catch up on ETL jobs and Shrink them back but Auto",
    "start": "2263920",
    "end": "2271119"
  },
  {
    "text": "expansion could allow the cluster to automatically expand and Shrink based on load and based on rules that we",
    "start": "2271119",
    "end": "2277540"
  },
  {
    "text": "specified that would be ideal and Richard monitoring infrastructure is something like cluster heat map or being",
    "start": "2277540",
    "end": "2283960"
  },
  {
    "text": "able to identify any problematic notes on the cluster would actually help us and monitor administer the cluster a lot",
    "start": "2283960",
    "end": "2290079"
  },
  {
    "text": "easier so finally what we're trying to do is Strive to build the best of class Big",
    "start": "2290079",
    "end": "2296800"
  },
  {
    "text": "Data platform in the cloud and we want to be the reference architecture of the industry by open sourcing some of the",
    "start": "2296800",
    "end": "2302380"
  },
  {
    "text": "components that we built thank you",
    "start": "2302380",
    "end": "2309660"
  },
  {
    "text": "thank you Eva it's exciting to see our customers building interesting Technologies on top of VMR and open",
    "start": "2311920",
    "end": "2319780"
  },
  {
    "text": "sourcing them and Netflix definitely is leading the way they are open sourcing Genie and lipstick we also have another",
    "start": "2319780",
    "end": "2327160"
  },
  {
    "text": "customer Yelp who's actually going to present tomorrow if you guys are around you should check it out they have a Mr",
    "start": "2327160",
    "end": "2334119"
  },
  {
    "text": "job library that they have open source which is essentially python on top of VMR really exciting exciting tool as",
    "start": "2334119",
    "end": "2340540"
  },
  {
    "text": "well so now I'd like to invite Bob Harris who is going to talk about how of",
    "start": "2340540",
    "end": "2346660"
  },
  {
    "text": "Channel 4 who's going to talk about how they use EMR in conjunction with the",
    "start": "2346660",
    "end": "2352060"
  },
  {
    "text": "infrastructure they already have in the Enterprise case",
    "start": "2352060",
    "end": "2357119"
  },
  {
    "text": "thank you Peter hi uh good uh good afternoon and thanks for",
    "start": "2362560",
    "end": "2368619"
  },
  {
    "text": "this opportunity to come and tell you about what we're doing on EMR at Channel",
    "start": "2368619",
    "end": "2374440"
  },
  {
    "text": "4. now unlike Netflix I guess I ought to tell you a",
    "start": "2374440",
    "end": "2380800"
  },
  {
    "text": "little bit about who Channel 4 are first because unless you're from the UK you you've probably not heard for a heard of us we are a public service",
    "start": "2380800",
    "end": "2387940"
  },
  {
    "start": "2383000",
    "end": "2383000"
  },
  {
    "text": "commercially funded not-for-profit broadcaster and let's just get it out of the way straight away we're not the BBC",
    "start": "2387940",
    "end": "2393820"
  },
  {
    "text": "everybody knows the BBC we are the other public service broadcaster we have a",
    "start": "2393820",
    "end": "2399040"
  },
  {
    "text": "Remick to innovate when it comes to content and I like to think of CTO but",
    "start": "2399040",
    "end": "2404320"
  },
  {
    "text": "we also innovate when it comes to leveraging technology we are funded predominantly by advertising with a",
    "start": "2404320",
    "end": "2410140"
  },
  {
    "text": "turnover today of probably about 1.6 billion dollars um and all the profit is effectively",
    "start": "2410140",
    "end": "2416700"
  },
  {
    "text": "folded back into into making more content so we're a company that is very",
    "start": "2416700",
    "end": "2421839"
  },
  {
    "text": "very very mindful of what things cost and that's really why we're we're",
    "start": "2421839",
    "end": "2426940"
  },
  {
    "text": "sitting using sort of AWS and EMR today and finally our content suits across the",
    "start": "2426940",
    "end": "2433660"
  },
  {
    "text": "portfolio of about a dozen channels and more importantly across about 30 or so",
    "start": "2433660",
    "end": "2439359"
  },
  {
    "text": "video on demand platforms similar to Netflix and across a multitude of",
    "start": "2439359",
    "end": "2445060"
  },
  {
    "text": "devices today and it's really the data that comes back off those platforms that drove us towards needing to do uh the",
    "start": "2445060",
    "end": "2453460"
  },
  {
    "text": "analytics we do today but what really started it all off was back in 2010 we got a new CEO",
    "start": "2453460",
    "end": "2461740"
  },
  {
    "start": "2461000",
    "end": "2461000"
  },
  {
    "text": "and the first thing David announced was Channel 4 was going to become a data",
    "start": "2461740",
    "end": "2467200"
  },
  {
    "text": "driven company so sitting somewhere down in the basement I thought this is this is going",
    "start": "2467200",
    "end": "2472839"
  },
  {
    "text": "to be interesting what is becoming a data-driven company mean well obviously it goes without saying we're going to be",
    "start": "2472839",
    "end": "2479140"
  },
  {
    "text": "analyzing a lot more data than we were at the time and I said about thinking what is it",
    "start": "2479140",
    "end": "2485140"
  },
  {
    "text": "what is it going to mean to process you know one two three orders of magnitude",
    "start": "2485140",
    "end": "2491440"
  },
  {
    "text": "more data than we were than we were already processing especially when the processing platform",
    "start": "2491440",
    "end": "2498640"
  },
  {
    "text": "we were using was based on these sorts of Technologies now I guess many of you in the room",
    "start": "2498640",
    "end": "2504520"
  },
  {
    "start": "2504000",
    "end": "2504000"
  },
  {
    "text": "we've certainly all heard of these Technologies some of you will actually be be running them I mean this is a",
    "start": "2504520",
    "end": "2511420"
  },
  {
    "text": "well-established business intelligence capability we had in channel four based on proprietary products",
    "start": "2511420",
    "end": "2518460"
  },
  {
    "text": "we led the certainly in the advertising industry with things like real-time data",
    "start": "2518460",
    "end": "2524800"
  },
  {
    "text": "processing back at the turn of the century sorry real-time data warehousing so we're pretty used to doing reporting",
    "start": "2524800",
    "end": "2531460"
  },
  {
    "text": "at Large Scale we're pretty used to doing it doing it near real time good skills in that team but",
    "start": "2531460",
    "end": "2538000"
  },
  {
    "text": "realistically you know two major issues from for me as CTO scaling that platform by an order of",
    "start": "2538000",
    "end": "2544119"
  },
  {
    "text": "magnitude was going to cost more money than I could even begin to think about throwing at it",
    "start": "2544119",
    "end": "2551079"
  },
  {
    "text": "and also you know one of the first things I knew CEO did was put a new analytics team in place as I said we've",
    "start": "2551079",
    "end": "2557079"
  },
  {
    "text": "been doing reporting for a long time but now we're going to have to sort of support analytics and when I thought about that it really came to me that",
    "start": "2557079",
    "end": "2564040"
  },
  {
    "text": "reporting is all about sort of rear view mirror it's what's happened analytics feels like sort of head-up display",
    "start": "2564040",
    "end": "2570520"
  },
  {
    "text": "looking forward these were going to be very different individuals and have very different requirements",
    "start": "2570520",
    "end": "2577240"
  },
  {
    "text": "so income Hadoop or incomes Hadoop would actually realize we were going to see",
    "start": "2577240",
    "end": "2582579"
  },
  {
    "start": "2581000",
    "end": "2581000"
  },
  {
    "text": "this data explosion coming down the road you know some time ago an order already started experimenting",
    "start": "2582579",
    "end": "2588760"
  },
  {
    "text": "with with open source Hadoop and when actually run an in-house trial",
    "start": "2588760",
    "end": "2595180"
  },
  {
    "text": "we ran that in-house trial on about a cluster of about 10 Dell machines and",
    "start": "2595180",
    "end": "2601000"
  },
  {
    "text": "over a period of about two weeks we know we used one of the one of the common distros out there over a two-week period",
    "start": "2601000",
    "end": "2607480"
  },
  {
    "text": "we've spent more time trying to make the tin work trying to make the interconnects work trying to get the jvm",
    "start": "2607480",
    "end": "2613180"
  },
  {
    "text": "sorted out than we ever did running any hive what it did to sorry running any any",
    "start": "2613180",
    "end": "2618339"
  },
  {
    "text": "Hadoop what it did tell us this was definitely the right technology but this really didn't feel like an easy way to",
    "start": "2618339",
    "end": "2624220"
  },
  {
    "text": "be going about it we're a very small team we're talking about you know single digit technicians not 20 30 40 50 or",
    "start": "2624220",
    "end": "2631900"
  },
  {
    "text": "whatever uh and I think it was in 2011 around October I stopped by Seattle because",
    "start": "2631900",
    "end": "2637359"
  },
  {
    "text": "we've been using Amazon since 2008 so we were a relatively early adopter of the platform from a compute point of view",
    "start": "2637359",
    "end": "2643180"
  },
  {
    "text": "and I saw Peter in Seattle and I think it was 2011 October and he ridiculed me",
    "start": "2643180",
    "end": "2649740"
  },
  {
    "text": "forever having put this stuff on real tin and said you've got to go away and",
    "start": "2649740",
    "end": "2655000"
  },
  {
    "text": "and drive EMR test drive EMR which we did we actually achieved more in two days",
    "start": "2655000",
    "end": "2661480"
  },
  {
    "text": "test drive and EMR than we had in the sort of previous months doing it on real 10 so we flipped across pretty quickly",
    "start": "2661480",
    "end": "2668619"
  },
  {
    "text": "to to running our new work on EMR ran a lot of it in parallel with the",
    "start": "2668619",
    "end": "2675280"
  },
  {
    "text": "conventional bi you've got to understand you've got a big investment in bi you've got a set of skills you've got a set of",
    "start": "2675280",
    "end": "2681099"
  },
  {
    "text": "people that understand those tools there's a bit of a learning curve with this stuff and it's it's very very different from perhaps if you're used to",
    "start": "2681099",
    "end": "2687760"
  },
  {
    "text": "writing reports in things like business objects uh but we took a big step in 2012 and we",
    "start": "2687760",
    "end": "2694720"
  },
  {
    "text": "put it straight into the hands of the end users so we we bypassed as it were all the teams all the report Port",
    "start": "2694720",
    "end": "2701140"
  },
  {
    "text": "writing teams and we gave Hive to the end to the end users to the analysts and in the same way Netflix has we've had to",
    "start": "2701140",
    "end": "2708040"
  },
  {
    "text": "develop a whole set of tools to allow them to do that we've got something called the Big Data control panel we're",
    "start": "2708040",
    "end": "2714099"
  },
  {
    "text": "not marketeers we don't come up with sexy names for things but the big data control panel is basically a Django app",
    "start": "2714099",
    "end": "2720060"
  },
  {
    "text": "that that allows the analysts to stop start clusters stock clusters submit",
    "start": "2720060",
    "end": "2727240"
  },
  {
    "text": "um Hive queries Etc get results back and if they remember when they go home at",
    "start": "2727240",
    "end": "2733060"
  },
  {
    "text": "night shut the Clusters down again that's become a bit of an issue we have to go and kill them every evening that's",
    "start": "2733060",
    "end": "2739540"
  },
  {
    "text": "the tool that we do think occasionally about open sourcing and when I when I see Netflix present and show all their",
    "start": "2739540",
    "end": "2745540"
  },
  {
    "text": "open sort of all their different open source tools I do feel a little bit guilty so maybe we get ran for that one",
    "start": "2745540",
    "end": "2752079"
  },
  {
    "text": "day and later later in 2012 we basically put all those EML workflows into production",
    "start": "2752079",
    "end": "2758260"
  },
  {
    "text": "so now channel 4 as a business is relying on EMR to write to run the business every day",
    "start": "2758260",
    "end": "2764619"
  },
  {
    "text": "uh realistically it just grows and grows now and more recently we've been starting to use things like mahut and R",
    "start": "2764619",
    "end": "2771520"
  },
  {
    "text": "and H space Etc so pretty much the standard road map that this this takes you on once you start using this it just",
    "start": "2771520",
    "end": "2778000"
  },
  {
    "text": "becomes so compelling it's uh it just just grows so",
    "start": "2778000",
    "end": "2783520"
  },
  {
    "text": "you know why are we doing this what sorts of problems are we solving well largely",
    "start": "2783520",
    "end": "2789040"
  },
  {
    "start": "2786000",
    "end": "2786000"
  },
  {
    "text": "it's all about giving exactly the same in Netflix it's all about giving our end users our viewers a better experience as",
    "start": "2789040",
    "end": "2796420"
  },
  {
    "text": "I said they currently consume our content across a range of services a range of devices and the first thing we",
    "start": "2796420",
    "end": "2803560"
  },
  {
    "text": "want to do is try and give them a joined up experience so we want to we want to recognize that person on whatever",
    "start": "2803560",
    "end": "2809020"
  },
  {
    "text": "platform they they turn up on as I said it might be a digital TV set it might be",
    "start": "2809020",
    "end": "2815020"
  },
  {
    "text": "an increasingly a connected TV set but more often now it's on an iPad a tablet",
    "start": "2815020",
    "end": "2820300"
  },
  {
    "text": "an iPhone an Android phone wherever they happen to be",
    "start": "2820300",
    "end": "2826079"
  },
  {
    "text": "so I certainly realize these two points that are on my way around you know so what data are we using uh largely web",
    "start": "2829480",
    "end": "2835680"
  },
  {
    "start": "2831000",
    "end": "2831000"
  },
  {
    "text": "website click stream logs these currently come to us via typical log",
    "start": "2835680",
    "end": "2842200"
  },
  {
    "text": "collection platforms one of the issues I've still got and we'll talk about it in a little while I don't often get",
    "start": "2842200",
    "end": "2848319"
  },
  {
    "text": "those logs for 24 hours you know until 24 hours after the event but a huge volume of Click stream logs coming in",
    "start": "2848319",
    "end": "2855060"
  },
  {
    "text": "increasingly four four OD I should explain four on demand is our video on",
    "start": "2855060",
    "end": "2860079"
  },
  {
    "text": "demand platform uh a lot of video on demand activity data coming in",
    "start": "2860079",
    "end": "2865720"
  },
  {
    "text": "we've got over 9 million registered users that's uh something around 18 of the UK",
    "start": "2865720",
    "end": "2873160"
  },
  {
    "text": "population I think registered and a huge number of the views something around 60",
    "start": "2873160",
    "end": "2878800"
  },
  {
    "text": "today so a huge number of the interactions on my platform are people who are logged in so it means I",
    "start": "2878800",
    "end": "2885040"
  },
  {
    "text": "recognize who they are we don't enforce that the majority of you know being a public service broadcaster free-to-air",
    "start": "2885040",
    "end": "2890920"
  },
  {
    "text": "broadcaster arethos is very much we're not going to force you to to tell us who you are but if you do",
    "start": "2890920",
    "end": "2897819"
  },
  {
    "text": "tell us who you are we will strive to give you a better experience in terms of General General reach I",
    "start": "2897819",
    "end": "2904780"
  },
  {
    "text": "think something like 53 million people a month 53 million individuals a month consume some bit of Channel 4 content",
    "start": "2904780",
    "end": "2912579"
  },
  {
    "text": "during that month so that's something like 92 percent of the UK population oh",
    "start": "2912579",
    "end": "2918700"
  },
  {
    "text": "back again and the principal tasks largely audience segmentation",
    "start": "2918700",
    "end": "2924819"
  },
  {
    "text": "allows us to feed better adverts we really want you to enjoy those efforts",
    "start": "2924819",
    "end": "2930940"
  },
  {
    "text": "so we try and feed you ones that are relevant personalize the experience when you arrive at our site it's really nicely we",
    "start": "2930940",
    "end": "2938079"
  },
  {
    "text": "recognize who you are welcome back whoever and increasingly recommendations today",
    "start": "2938079",
    "end": "2944800"
  },
  {
    "text": "something like 10 of our views we think come come from recommendations and holding you and making you view more and",
    "start": "2944800",
    "end": "2951640"
  },
  {
    "text": "more and more content obviously desperately important to us",
    "start": "2951640",
    "end": "2956099"
  },
  {
    "text": "um again we've put a pipeline together I'm not going to go through every detail of this and apologize it's not really",
    "start": "2956680",
    "end": "2961960"
  },
  {
    "text": "meant to be readable it's just really meant to show this sort of left to right flow we go through uh most of this",
    "start": "2961960",
    "end": "2968319"
  },
  {
    "text": "content is generated on on web platforms",
    "start": "2968319",
    "end": "2973420"
  },
  {
    "text": "so this data lives in the web already so the first thing we do with it we put it straight into S3",
    "start": "2973420",
    "end": "2979780"
  },
  {
    "text": "put it into S3 for a couple of reasons one is I can put it into S3 at very low cost I still get analysts come and tell",
    "start": "2979780",
    "end": "2986740"
  },
  {
    "text": "me so by that I mean external analysts the storing data is very expensive and you have to really be careful about what",
    "start": "2986740",
    "end": "2992859"
  },
  {
    "text": "it is you collect rubbish it's really cheap collect anything that moves put it into S3 if",
    "start": "2992859",
    "end": "3000540"
  },
  {
    "text": "you're not going to use it straight away put it into glacier",
    "start": "3000540",
    "end": "3005480"
  },
  {
    "text": "I won't say which company told me that but they are no it wouldn't be fair to say who um the other reason we put into S3 as",
    "start": "3005880",
    "end": "3012960"
  },
  {
    "text": "you heard earlier when it's in S3 I can run multiple clusters off it and as I've said we've already let the uses the",
    "start": "3012960",
    "end": "3019740"
  },
  {
    "text": "users loose on running their own clusters then we effectively put that through a left to right all written in",
    "start": "3019740",
    "end": "3025680"
  },
  {
    "text": "pig I'm certainly going to be taking a look at lipstick pig is a language I happen to Love Actually but all written",
    "start": "3025680",
    "end": "3032280"
  },
  {
    "text": "in pig python Java but a lot of pig and we put it through and we basically process and you know add to the quality",
    "start": "3032280",
    "end": "3038700"
  },
  {
    "text": "of that data as we go through really dupe it we improve the quality of it we had Flags to it to to to to assist with",
    "start": "3038700",
    "end": "3045839"
  },
  {
    "text": "the analysis Etc certainly a lot of work we've done there is is using things like elasticash and",
    "start": "3045839",
    "end": "3052619"
  },
  {
    "text": "redis on our mapper nodes Etc to improve the performance and allow us to do",
    "start": "3052619",
    "end": "3059880"
  },
  {
    "text": "things like work out dwell times between two different transactions Etc how long were you watching a piece of video for",
    "start": "3059880",
    "end": "3065880"
  },
  {
    "text": "did you watch this piece of video for longer than you watched the last piece of video that sort of thing and finally out of that pipeline as we go along",
    "start": "3065880",
    "end": "3072359"
  },
  {
    "text": "various points spit the data out to to the end users",
    "start": "3072359",
    "end": "3077460"
  },
  {
    "text": "uh again not meant to see this but that sort of orangey section on the right hand side there of that diagram that's",
    "start": "3077460",
    "end": "3084359"
  },
  {
    "start": "3079000",
    "end": "3079000"
  },
  {
    "text": "the chunk that we do in in EMR and it's we've already used the term ETL here a",
    "start": "3084359",
    "end": "3091319"
  },
  {
    "text": "few times today and I always talk about you know EMR as being ETL on steroids",
    "start": "3091319",
    "end": "3096720"
  },
  {
    "text": "we process billions of rows in a typical run sometimes anything up to 20 billion",
    "start": "3096720",
    "end": "3101940"
  },
  {
    "text": "rows what comes out the end are a few million rows of results and those",
    "start": "3101940",
    "end": "3107040"
  },
  {
    "text": "million Rosa results we put into an rdbms usually into our Oracle database that means it's immediately available to",
    "start": "3107040",
    "end": "3113940"
  },
  {
    "text": "all those folks writing reports who are used for those tools Etc but the point I really like to make is",
    "start": "3113940",
    "end": "3120240"
  },
  {
    "text": "this stuff is totally complementary with what you've already got people come to me and say you know I can't use EMR I",
    "start": "3120240",
    "end": "3125880"
  },
  {
    "text": "can't use Hadoop because we've already got this sunk investment in pick the you know technology of your",
    "start": "3125880",
    "end": "3132180"
  },
  {
    "text": "choice or a cool teradata whatever uh that's that's incorrect you absolutely",
    "start": "3132180",
    "end": "3137880"
  },
  {
    "text": "can these things are totally complementary there is no need to rip and replace that existing technology",
    "start": "3137880",
    "end": "3144359"
  },
  {
    "text": "I'm not saying I'm going to buy any more of that existing technology but you don't need to throw it away",
    "start": "3144359",
    "end": "3149520"
  },
  {
    "text": "uh and going forward EMR will continue to underpin all our growth Etc so very",
    "start": "3149520",
    "end": "3155640"
  },
  {
    "text": "quickly because I may have time uh use of EMR is just going to grow in Channel 4 and I'm really excited by some of",
    "start": "3155640",
    "end": "3162119"
  },
  {
    "text": "those new new announcements coming along we've got to get ourselves migrated to to Hadoop too",
    "start": "3162119",
    "end": "3169260"
  },
  {
    "text": "um certainly for just so we can get access to things like yarn redshift is a no-brainer to be to begin to replace",
    "start": "3169260",
    "end": "3176040"
  },
  {
    "text": "some of that traditional data warehousing as we go forward again we don't have to rip it out but as it ages",
    "start": "3176040",
    "end": "3182220"
  },
  {
    "text": "out we'll move things towards redshift we're putting in things like Direct Connect so it means I can actually Plumb",
    "start": "3182220",
    "end": "3188760"
  },
  {
    "text": "data between my on-prem and my VPC based you know Cloud systems and the thing",
    "start": "3188760",
    "end": "3195480"
  },
  {
    "text": "that I was most excited about this morning because we've been doing a lot of work around real time analysis I",
    "start": "3195480",
    "end": "3202200"
  },
  {
    "text": "don't want to base my segmentation on what you did last time you came to see me I want to space my segmentation on",
    "start": "3202200",
    "end": "3208020"
  },
  {
    "text": "what you're doing today maybe last time you're in a bad mood and you were looking for drama but today you",
    "start": "3208020",
    "end": "3213960"
  },
  {
    "text": "want comedy if I can actually do in session analytics when you press the",
    "start": "3213960",
    "end": "3219420"
  },
  {
    "text": "play button I've got 23 minutes on average to come back and make another recommendation to you that's a lifetime",
    "start": "3219420",
    "end": "3225660"
  },
  {
    "text": "if I could get the data as I said it takes 24 hours to arrive today using",
    "start": "3225660",
    "end": "3231059"
  },
  {
    "text": "kinesis um you know we've been playing with things like storm Kafka and things like",
    "start": "3231059",
    "end": "3236700"
  },
  {
    "text": "that Kinesis for me was the most exciting thing of this uh of this conference and to hear as I've just",
    "start": "3236700",
    "end": "3243960"
  },
  {
    "text": "tweeted there is going to be a Kinesis to email connector um that is going to be tremendously",
    "start": "3243960",
    "end": "3249839"
  },
  {
    "text": "useful for us so that's what we do thank you very much",
    "start": "3249839",
    "end": "3255200"
  }
]