[
  {
    "text": "wonderful well hello everybody thank you very much for coming along and hello everybody else who's online my name is",
    "start": "0",
    "end": "7049"
  },
  {
    "text": "Ian Robinson I'm a specialist Solutions Architect for data and analytics at AWS and in this session I'm gonna be",
    "start": "7049",
    "end": "14160"
  },
  {
    "text": "covering serverless big data analytics with Amazon Athena and quick site about",
    "start": "14160",
    "end": "19289"
  },
  {
    "text": "- inferring a couple of other services as well so I'll have a brief look at glue and I'm also using s3 I'm then",
    "start": "19289",
    "end": "25109"
  },
  {
    "text": "running the follow-up session where we'll do a deeper dive into glue so we'll just have a quick look at gluing",
    "start": "25109",
    "end": "30269"
  },
  {
    "text": "this session deeper dive in the next session I thought I'd frame this the outset with a",
    "start": "30269",
    "end": "37829"
  },
  {
    "text": "brief discussion of agile analytics so you've probably all seen a diagram like",
    "start": "37829",
    "end": "42989"
  },
  {
    "text": "this in the past probably quite familiar with this you know kind of traditional data pipeline we're collecting data",
    "start": "42989",
    "end": "49320"
  },
  {
    "text": "storing it doing some analysis potentially doing some transformations to a more query optimized model",
    "start": "49320",
    "end": "56210"
  },
  {
    "text": "performing some aggregations some analysis consuming that data very traditional data pipeline now in the",
    "start": "56210",
    "end": "64018"
  },
  {
    "text": "past the time it takes to complete this pipeline could be in the order of days",
    "start": "64019",
    "end": "69270"
  },
  {
    "text": "or even weeks we have to identify the data and then we have to acquire that data we might have to think about",
    "start": "69270",
    "end": "75659"
  },
  {
    "text": "procuring or provisioning compute resource or storage or actually acquiring some infrastructure some",
    "start": "75659",
    "end": "81960"
  },
  {
    "text": "hardware and so on so the time it takes to go from an idea to insight can be in",
    "start": "81960",
    "end": "88439"
  },
  {
    "text": "the order of days or weeks that's pretty slow pretty burdensome so ideally what",
    "start": "88439",
    "end": "94619"
  },
  {
    "text": "our end users would like is to be able to reduce these cycle times to be able",
    "start": "94619",
    "end": "99869"
  },
  {
    "text": "to run lots and lots of these data pipelines one after another or even interleave them this is a more agile",
    "start": "99869",
    "end": "107610"
  },
  {
    "text": "approach to developing an analytics capability if we can reduce those cycle times it gives us a lot more freedom in",
    "start": "107610",
    "end": "114360"
  },
  {
    "text": "terms of being able to experiment with our data we can throw things together",
    "start": "114360",
    "end": "119450"
  },
  {
    "text": "bringing existing sources of data that we might have acquired choose an appropriate technology begin some",
    "start": "119450",
    "end": "125549"
  },
  {
    "text": "analysis begin to experiment if that experiment looks fruitful we can",
    "start": "125549",
    "end": "130709"
  },
  {
    "text": "continue to invest in it but if it looks as though it's leading to a dead end then well we just want to",
    "start": "130709",
    "end": "135730"
  },
  {
    "text": "get rid of it kill it cancel it and stop paying for any of the resources that we were using to do that processing and with this kind",
    "start": "135730",
    "end": "144040"
  },
  {
    "text": "of setup as well we can react far more quickly you know if the market and competitors force us to reevaluate our",
    "start": "144040",
    "end": "150489"
  },
  {
    "text": "understanding of our data to conduct fresh analyses and so on again with this more kind of an agile approach to be",
    "start": "150489",
    "end": "157299"
  },
  {
    "text": "able to reduce these cycle times go from idea to insight we can react far more",
    "start": "157299",
    "end": "163659"
  },
  {
    "text": "quickly from the far more agile far more powerful with regards to the ways in which we process our data so what I'm",
    "start": "163659",
    "end": "171819"
  },
  {
    "text": "going to show today is a way in which we compose several of the AWS services in",
    "start": "171819",
    "end": "177250"
  },
  {
    "text": "order to build the foundations for an agile analytics platform and in this",
    "start": "177250",
    "end": "183879"
  },
  {
    "text": "we're going to take advantage of predominantly serverless services and the ones where we don't really have to",
    "start": "183879",
    "end": "189970"
  },
  {
    "text": "think about capacity planning or sizing infrastructure or provisioning that infrastructure these are the kind of",
    "start": "189970",
    "end": "196359"
  },
  {
    "text": "services where AWS is taking on all of those responsibilities allows you to focus on the stuff that's really",
    "start": "196359",
    "end": "201790"
  },
  {
    "text": "important to you whether that's transforming your data or actually querying the data or generating",
    "start": "201790",
    "end": "207940"
  },
  {
    "text": "visualizations on top of it you get to focus on all of that and let us worry about standing up the infrastructure and",
    "start": "207940",
    "end": "213400"
  },
  {
    "text": "doing the processing in the background on your behalf so this is the kind of",
    "start": "213400",
    "end": "219879"
  },
  {
    "text": "setup that we're going to look at today we're going to take some raw data that's stored in s3 and the simple object",
    "start": "219879",
    "end": "226299"
  },
  {
    "text": "service and in this case we've actually got some data from the New York transportation system so we've got some",
    "start": "226299",
    "end": "233079"
  },
  {
    "text": "taxi data sand limo data and some data from uber so three different data sets",
    "start": "233079",
    "end": "238180"
  },
  {
    "text": "which we've already acquired and we're storing in s3 so that's our raw data up",
    "start": "238180",
    "end": "243430"
  },
  {
    "text": "here we're then going to catalogue that data so we're catalog it means that at",
    "start": "243430",
    "end": "250060"
  },
  {
    "text": "some point in the future it's going to be very easy to discover that data find out where it resides what formats it it",
    "start": "250060",
    "end": "255849"
  },
  {
    "text": "it's in what schema is applicable to that data so we'll catalog it and then",
    "start": "255849",
    "end": "261278"
  },
  {
    "text": "we'll do some transformations on that date right so we're going to build a model an analytics model that is optimized for a",
    "start": "261279",
    "end": "267880"
  },
  {
    "text": "set of queries around this transportation data so we can take three different sources with slightly",
    "start": "267880",
    "end": "273460"
  },
  {
    "text": "different fields slight different schemas and we're going to transform it and write the output of those",
    "start": "273460",
    "end": "278770"
  },
  {
    "text": "transformations to another bucket in s3 so we're going to change the schema slightly you know canonicalize all of",
    "start": "278770",
    "end": "285430"
  },
  {
    "text": "the column names but we're actually also going to transform the data format so our raw data is CSV our own output data",
    "start": "285430",
    "end": "293860"
  },
  {
    "text": "is going to be Parker are going to Colin a query optimize format for large aggregations large kind of large-scale",
    "start": "293860",
    "end": "300160"
  },
  {
    "text": "analytics so once we've done that once we've got our canonical data or our",
    "start": "300160",
    "end": "305560"
  },
  {
    "text": "canonical model in s3 we're then going to start querying it using Athena an",
    "start": "305560",
    "end": "310810"
  },
  {
    "text": "Athena is our managed or Server less interactive sequel service it allows you",
    "start": "310810",
    "end": "317020"
  },
  {
    "text": "to write sequel against data at rest in s 3 no need to spin up any other",
    "start": "317020",
    "end": "322120"
  },
  {
    "text": "infrastructure you can just start writing sequel against that data in s3 you could actually write it against the raw data but we'll have actually gone",
    "start": "322120",
    "end": "329169"
  },
  {
    "text": "through some process of canonicalizing it transforming it optimizing it for our analysis purposes so we'll see some",
    "start": "329169",
    "end": "335830"
  },
  {
    "text": "sequel queries against this data and then on top of that we're going to put Amazon quick site which is another",
    "start": "335830",
    "end": "342370"
  },
  {
    "text": "manager service that gives you bi or a visualization tool in the browser you can create dashboards and you can share",
    "start": "342370",
    "end": "348850"
  },
  {
    "text": "those dashboards with your users within your organization ok so let's go through",
    "start": "348850",
    "end": "358330"
  },
  {
    "text": "some of these moving parts in a bit more detail I mentioned that we're going to catalog our data and we're going to",
    "start": "358330",
    "end": "364930"
  },
  {
    "text": "apply some transformations to it now traditionally we do those transformations easily using a",
    "start": "364930",
    "end": "370090"
  },
  {
    "text": "third-party tool such as talend or we would spin up something like EMR and manage to do cluster",
    "start": "370090",
    "end": "376720"
  },
  {
    "text": "perhaps install SPARC and use that to apply some transformations to add a to calculate some aggregations transform",
    "start": "376720",
    "end": "382479"
  },
  {
    "text": "the format and so on what we're going to use is a new service from AWS called",
    "start": "382479",
    "end": "387490"
  },
  {
    "text": "blue it's a service that was announced reinvent last year and was made generally available in the US",
    "start": "387490",
    "end": "394760"
  },
  {
    "text": "a couple of months ago and we'll talk a little bit later or at the end of this session about release dates elsewhere",
    "start": "394760",
    "end": "401290"
  },
  {
    "text": "okay so we're going to use that it's a managed or a server less ETL capability",
    "start": "401290",
    "end": "406430"
  },
  {
    "text": "and it also exposes a highly available hive meta store compatible data catalog",
    "start": "406430",
    "end": "411860"
  },
  {
    "text": "so we're going to use these two different parts of glue in order to catalog and transform our data so this",
    "start": "411860",
    "end": "418400"
  },
  {
    "text": "is Glu Glu simply has three core capabilities it has a data catalog so as",
    "start": "418400",
    "end": "424400"
  },
  {
    "text": "I mentioned this is a highly available hive meta store compatible catalog that",
    "start": "424400",
    "end": "429680"
  },
  {
    "text": "stores metadata describing all of your sources of data in AWS now we've added",
    "start": "429680",
    "end": "435800"
  },
  {
    "text": "some additional features to that catalog some additional api's without accrual errs that will actually go discovering",
    "start": "435800",
    "end": "442910"
  },
  {
    "text": "data within your estate and uploading that metadata into the catalog once that",
    "start": "442910",
    "end": "449000"
  },
  {
    "text": "data is in the catalog once that metadata is in the catalog describing your sources of data describing the schemas and so on that metadata can be",
    "start": "449000",
    "end": "455990"
  },
  {
    "text": "consumed by a number of other services that we expose so the table definitions that are stored in the catalog can be",
    "start": "455990",
    "end": "462500"
  },
  {
    "text": "used by Athena for example and that's what we'll be doing today Athena can consume those table definitions and we allows us to write",
    "start": "462500",
    "end": "468830"
  },
  {
    "text": "sequel against the data wherever it resides we can use it in redshift spectrum we can use the catalog in EMR",
    "start": "468830",
    "end": "476420"
  },
  {
    "text": "we can use spark and hi Vinnie anwar and point that about our catalog as well so the very same table definitions in the",
    "start": "476420",
    "end": "482480"
  },
  {
    "text": "catalog we can reuse via several services will point it at the same sources of underlying data so that's one",
    "start": "482480",
    "end": "489980"
  },
  {
    "text": "thing that glue does very well the other two things are really around ETL jobs",
    "start": "489980",
    "end": "495010"
  },
  {
    "text": "glue will allow you to compose transformations given a source and a",
    "start": "495010",
    "end": "500780"
  },
  {
    "text": "target from the catalog create for me a transformation we take data from the source transform it and push it into the",
    "start": "500780",
    "end": "506900"
  },
  {
    "text": "target if we create a transformation through Gloup Gloup will actually",
    "start": "506900",
    "end": "512479"
  },
  {
    "text": "behind-the-scenes create for us some code some Python code that addresses sparks a spice bar code we can open up",
    "start": "512479",
    "end": "519560"
  },
  {
    "text": "that code in our own IDE or in the browser and we can further modify it further enrich it and so on so we can",
    "start": "519560",
    "end": "525590"
  },
  {
    "text": "actually modify this trips that we then won't glue to execute and the last thing that glues",
    "start": "525590",
    "end": "530650"
  },
  {
    "text": "responsible for is job execution itself we can specify the schedule when we want these jobs executed whether it's on a",
    "start": "530650",
    "end": "537040"
  },
  {
    "text": "periodic basis whether it's in reaction to certain events you know data lands in",
    "start": "537040",
    "end": "542350"
  },
  {
    "text": "s3 I want to use that event as the trigger for a glue job we can specify all of that in glue and then we leave",
    "start": "542350",
    "end": "548980"
  },
  {
    "text": "glue to be responsible for actually scheduling and executing those transformations on our behalf so what",
    "start": "548980",
    "end": "557830"
  },
  {
    "text": "I've done prior to this session is actually take two of those sources of data the taxi data and the limo data and",
    "start": "557830",
    "end": "564460"
  },
  {
    "text": "I've already run some glue jobs on top of them to transform them into a canonical model and to transform the",
    "start": "564460",
    "end": "570910"
  },
  {
    "text": "format into park' right so with those two sources of data we had just over",
    "start": "570910",
    "end": "578380"
  },
  {
    "text": "one-and-a-half gigs of taxi data and 220 Meg's of limo data so this is not a huge",
    "start": "578380",
    "end": "584320"
  },
  {
    "text": "data set I think this is actually data only covering a few months from 2016",
    "start": "584320",
    "end": "590500"
  },
  {
    "text": "but in applying those transformations and converting to parkade to this query optimized column of format we've",
    "start": "590500",
    "end": "596950"
  },
  {
    "text": "actually significantly reduced the data set sizes so we've gone down from just over one and a half gig to just under a",
    "start": "596950",
    "end": "603940"
  },
  {
    "text": "hundred Meg from 220 Meg down to just 18 Meg so it got much smaller files in our",
    "start": "603940",
    "end": "610320"
  },
  {
    "text": "canonical store on s3 again I'll talk a little later on about",
    "start": "610320",
    "end": "616270"
  },
  {
    "text": "these columnar formats in more detail and about some of the benefits that they offer us so this leaves us with the uber",
    "start": "616270",
    "end": "623500"
  },
  {
    "text": "data that we want to transform and push into that canonical model so what we're",
    "start": "623500",
    "end": "630190"
  },
  {
    "text": "going to do here is actually look at glue look at the glue console I just got",
    "start": "630190",
    "end": "638140"
  },
  {
    "text": "a mirror mine",
    "start": "638140",
    "end": "641160"
  },
  {
    "text": "can you see that okay good all right so this is clues console and like a lot of",
    "start": "644720",
    "end": "650180"
  },
  {
    "text": "services glue also exposes an API so we can consume it via the clan line or via",
    "start": "650180",
    "end": "656360"
  },
  {
    "text": "Java SDK but we're going to do everything today through the console so you can see on the left hand side we've",
    "start": "656360",
    "end": "661820"
  },
  {
    "text": "effectively divided up between those two-bit capabilities the data catalog and the ETL and within the catalog we",
    "start": "661820",
    "end": "667580"
  },
  {
    "text": "have this concept of databases and a database is simply a collection of table definitions they don't have to be",
    "start": "667580",
    "end": "672680"
  },
  {
    "text": "relational databases and relational tables it's just effectively a metaphor for describing sources of data these are",
    "start": "672680",
    "end": "679910"
  },
  {
    "text": "the databases that I've currently cataloged in my account one of them I'll",
    "start": "679910",
    "end": "685160"
  },
  {
    "text": "be using in my next demo a couple of them a couple of them that were using in this they're the NYC transportation",
    "start": "685160",
    "end": "691340"
  },
  {
    "text": "database is our raw data NYC transportation canonical is our",
    "start": "691340",
    "end": "696740"
  },
  {
    "text": "target is that canonical model that we're building so this is the NYC",
    "start": "696740",
    "end": "703090"
  },
  {
    "text": "transportation database it includes three different sources of data data for those limo rides data for taxis and data",
    "start": "703090",
    "end": "709430"
  },
  {
    "text": "for uber and we can actually see that the underlying data is stored in an s3 bucket it's not necessary bucket I can",
    "start": "709430",
    "end": "716210"
  },
  {
    "text": "control its so this is actually cross account access in order to access that data so we've got these three sources of",
    "start": "716210",
    "end": "726050"
  },
  {
    "text": "data here then I have a second database which is my canonical database I'm not",
    "start": "726050",
    "end": "734630"
  },
  {
    "text": "actually created a definition for this yet in a little while when we look at Athena the first thing I'm going to do",
    "start": "734630",
    "end": "739760"
  },
  {
    "text": "is actually create a definition a table definition for Athena so that we can start querying it just trying to think",
    "start": "739760",
    "end": "747650"
  },
  {
    "text": "what I need to do here and now I think the section I was simply going to review",
    "start": "747650",
    "end": "752960"
  },
  {
    "text": "what's available in glue okay that's good",
    "start": "752960",
    "end": "757900"
  },
  {
    "text": "so we're using glue in a minute to actually do some additional transformation on that uber data to get into the canonical model and we'll also",
    "start": "762360",
    "end": "768809"
  },
  {
    "text": "be running a crawler one of these little things that can go discovering sources of data and that will create a table definition that we can start querying",
    "start": "768809",
    "end": "774779"
  },
  {
    "text": "through Athena so this brings us to Athena Athena is our interactive query service you can",
    "start": "774779",
    "end": "781110"
  },
  {
    "text": "think of it as serverless or managed sequel queries instead of having to worry about standing up a data warehouse",
    "start": "781110",
    "end": "788119"
  },
  {
    "text": "redshift or having to provision an EMR cluster and install presto or SPARC all",
    "start": "788119",
    "end": "793499"
  },
  {
    "text": "you really want to concentrate on is writing sequel against data at rest in s 3 and that's exactly what Athena allows",
    "start": "793499",
    "end": "799860"
  },
  {
    "text": "you to do so there's no infrastructure no administration on your behalf we're just maintaining a warm pool of of",
    "start": "799860",
    "end": "806699"
  },
  {
    "text": "instances that become available to you the moment you execute a query so there's no not even any spin up time the",
    "start": "806699",
    "end": "813720"
  },
  {
    "text": "moment you execute a query we start running it we start executing it because it's a managed service and we're",
    "start": "813720",
    "end": "821160"
  },
  {
    "text": "constantly adding features to all of our services the moment those features become available within Athena they",
    "start": "821160",
    "end": "827160"
  },
  {
    "text": "become available to you so over the last few months we've added the ability to be able to read encrypted data in s3 we've",
    "start": "827160",
    "end": "834989"
  },
  {
    "text": "added the ability to read several different file formats so we released supporting quite a wide range of file",
    "start": "834989",
    "end": "840449"
  },
  {
    "text": "formats and over time we're adding more and more file formats that we support",
    "start": "840449",
    "end": "845569"
  },
  {
    "text": "several extensibility mechanisms it's very easy to use Athena we're going to",
    "start": "845569",
    "end": "851850"
  },
  {
    "text": "use it again through the browser through the console so it gives you a very simple sequel editor like experience but",
    "start": "851850",
    "end": "859350"
  },
  {
    "text": "it also exposes a JDBC endpoint so you can connect your own sequel client to it if you want and we also expose endpoints",
    "start": "859350",
    "end": "867749"
  },
  {
    "text": "that allow you to control and query Athena by way of our API is in our SDKs",
    "start": "867749",
    "end": "874790"
  },
  {
    "text": "so the process for querying data in Athena if you haven't already created a database and a table definition is what",
    "start": "874790",
    "end": "880860"
  },
  {
    "text": "we need to do that so you can do that in the console you can create some hive-like DDL to describe your external",
    "start": "880860",
    "end": "887579"
  },
  {
    "text": "table definition and all we're doing is creating metadata that describes the data that's the only thing that's being",
    "start": "887579",
    "end": "892769"
  },
  {
    "text": "stored in the catalog so we can do that in the console if we prefer there's a little wizard that we",
    "start": "892769",
    "end": "898600"
  },
  {
    "text": "can walk through that will allow us to create a table definition or and this is my preferred approach now we can use",
    "start": "898600",
    "end": "904899"
  },
  {
    "text": "glue and glues ability to crawl sources of data to create an external table",
    "start": "904899",
    "end": "909999"
  },
  {
    "text": "definition and then we can consume that from Athina once that's done we start querying a couple of other things just",
    "start": "909999",
    "end": "919779"
  },
  {
    "text": "to mention that it's highly available so we're maintaining this warm pool of server instances across availability",
    "start": "919779",
    "end": "925239"
  },
  {
    "text": "zones we expose an endpoint to you they're the only things that you know about or the endpoints the only thing that you really know about when you",
    "start": "925239",
    "end": "931629"
  },
  {
    "text": "execute a query we will choose appropriate resource in an availability zone in order to service that query",
    "start": "931629",
    "end": "937470"
  },
  {
    "text": "because Athena is designed to query data in s3 well s3 provides very very strong",
    "start": "937470",
    "end": "944619"
  },
  {
    "text": "high availability guarantees 11:9 of durability so all the objects that are written to s3",
    "start": "944619",
    "end": "950109"
  },
  {
    "text": "are replicated across availability zones within that region so we've got very",
    "start": "950109",
    "end": "955269"
  },
  {
    "text": "very high availability of durability guarantees for your data benefits here",
    "start": "955269",
    "end": "962169"
  },
  {
    "text": "we don't actually need to load data once it's a VAP once it's in s3 it's available for querying Athena allows us to query data in its",
    "start": "962169",
    "end": "969189"
  },
  {
    "text": "raw format so if you're landing data as CSV or as Jason as we have in our you",
    "start": "969189",
    "end": "974410"
  },
  {
    "text": "know raw bucket we can actually query that in its raw state my preference is",
    "start": "974410",
    "end": "980259"
  },
  {
    "text": "to convert that to a query optimized format if we regularly running large queries over the same body of data then",
    "start": "980259",
    "end": "986769"
  },
  {
    "text": "we can enormous ly improve latencies you know reduce latencies and reduce costs by converting to a query optimized",
    "start": "986769",
    "end": "993160"
  },
  {
    "text": "format that's not absolutely necessary Jason you've got CSV you can go query it",
    "start": "993160",
    "end": "998709"
  },
  {
    "text": "the moment it's there in s3 so in that respect there's no ETL absolutely",
    "start": "998709",
    "end": "1004589"
  },
  {
    "text": "necessary when we start querying your data we'll start streaming the results back",
    "start": "1004589",
    "end": "1010949"
  },
  {
    "text": "to the client okay so you're not having first I want to wait for us to generate the entire result set which for an",
    "start": "1010949",
    "end": "1016079"
  },
  {
    "text": "analytics like query might be quite a large result set instead we'll start streaming those results back to the client as soon as we start generating",
    "start": "1016079",
    "end": "1021959"
  },
  {
    "text": "the first result but we'll also put the entire result set to another location in this that you can control under the hood",
    "start": "1021959",
    "end": "1031949"
  },
  {
    "text": "Athena is using presto so presto is an open source distributed",
    "start": "1031950",
    "end": "1037660"
  },
  {
    "text": "in memory sequel engine okay now you can kind of reproduce the Athena like",
    "start": "1037660",
    "end": "1043720"
  },
  {
    "text": "experience by standing up presto on EMR but if you were to do that you would have to think about sizing your EMR",
    "start": "1043720",
    "end": "1050170"
  },
  {
    "text": "cluster choosing the appropriate instance types and so on and maintaining its its availability we're doing all of",
    "start": "1050170",
    "end": "1057580"
  },
  {
    "text": "that on your behalf so we're maintaining a large fleet of warm presto instances but this is the technology that we're",
    "start": "1057580",
    "end": "1063280"
  },
  {
    "text": "using under the hood and then for creating those external table definitions we're using hive DDL so",
    "start": "1063280",
    "end": "1070120"
  },
  {
    "text": "again a well-understood familiar technology for describing table structures over external sources of data",
    "start": "1070120",
    "end": "1078510"
  },
  {
    "text": "presto has its own ANSI sequel dialect this has support for complex joins so we",
    "start": "1079770",
    "end": "1087010"
  },
  {
    "text": "can join tables in our Athena queries it allows for nested queries for windowed",
    "start": "1087010",
    "end": "1094450"
  },
  {
    "text": "functions so we can query over window data supports complex data types or",
    "start": "1094450",
    "end": "1100540"
  },
  {
    "text": "arrays and structs and importantly it also allows us to partition our data so",
    "start": "1100540",
    "end": "1106900"
  },
  {
    "text": "by partitioning data we effectively supply in our table definition one or more partitioning keys so we're saying",
    "start": "1106900",
    "end": "1114340"
  },
  {
    "text": "that our data in s3 is laid out perhaps month by month in different folders all",
    "start": "1114340",
    "end": "1119560"
  },
  {
    "text": "the data for this month will go in one folder or the data for last month will go in another and we might specify month",
    "start": "1119560",
    "end": "1125350"
  },
  {
    "text": "as a partitioning key in our table definition if we do that and then if in our sequel queries we reuse a predicate",
    "start": "1125350",
    "end": "1131920"
  },
  {
    "text": "aware clause that includes that partitioning key then the query itself",
    "start": "1131920",
    "end": "1137590"
  },
  {
    "text": "will only go and look at the data that's absolutely necessary for resolving that query we can exclude all the partitions",
    "start": "1137590",
    "end": "1145240"
  },
  {
    "text": "that don't immediately apply so this is supported in presto and it's supported",
    "start": "1145240",
    "end": "1150460"
  },
  {
    "text": "in Athena it's a very very efficient and very very useful way again of reducing",
    "start": "1150460",
    "end": "1157150"
  },
  {
    "text": "latencies and as we'll see also reducing costs if we're choosing to use a thing so I",
    "start": "1157150",
    "end": "1165000"
  },
  {
    "text": "mentioned the multiple different data formats very simple in the CSV TSV we",
    "start": "1165000",
    "end": "1170970"
  },
  {
    "text": "can supply custom delimiters we already have classifiers for patchy web logs for",
    "start": "1170970",
    "end": "1176940"
  },
  {
    "text": "cloud trail logs very recently I think within the last week or so we introduced",
    "start": "1176940",
    "end": "1182190"
  },
  {
    "text": "support for log stash grok expressions so if you're familiar with crock it's a nice way of composing regular",
    "start": "1182190",
    "end": "1188910"
  },
  {
    "text": "expressions building up composed regular expressions so if you've got your own",
    "start": "1188910",
    "end": "1195060"
  },
  {
    "text": "proprietary text-based file format you can write a block expression that can",
    "start": "1195060",
    "end": "1200790"
  },
  {
    "text": "parse that and generate schema and a table definition based on those crock expressions we also support jason with",
    "start": "1200790",
    "end": "1209460"
  },
  {
    "text": "simple JSON structures and nested data jason structures and Avro and we support",
    "start": "1209460",
    "end": "1214980"
  },
  {
    "text": "two columnar format spark a and O are see Athena can also read compressed",
    "start": "1214980",
    "end": "1222150"
  },
  {
    "text": "files so again it's really good practice when you're querying data at rest in s 3",
    "start": "1222150",
    "end": "1227490"
  },
  {
    "text": "to have reasonably large objects to compress those objects and potentially",
    "start": "1227490",
    "end": "1234060"
  },
  {
    "text": "to use a query optimized format Athena can read snappy said lib gz l 0",
    "start": "1234060",
    "end": "1240540"
  },
  {
    "text": "compressed objects in s3 and as I mentioned earlier we've introduced a",
    "start": "1240540",
    "end": "1245970"
  },
  {
    "text": "feature whereby it can read encrypted data at rest in s 3",
    "start": "1245970",
    "end": "1252110"
  },
  {
    "text": "[Music] what else to say here it's fast I mean it's designed as an interactive query service you might be familiar with other",
    "start": "1254290",
    "end": "1261710"
  },
  {
    "text": "kind of MapReduce technologies where it may be that you end up waiting many minutes or even hours for that job to complete and very often that's because",
    "start": "1261710",
    "end": "1268430"
  },
  {
    "text": "those jobs are executed serially we have to do the map and then the reduce in the",
    "start": "1268430",
    "end": "1273830"
  },
  {
    "text": "next map then the next reduce and we have to wait for each of these stages to complete presto actually has a far more",
    "start": "1273830",
    "end": "1279680"
  },
  {
    "text": "interesting model it creates a directed acyclic graph of all the steps involved",
    "start": "1279680",
    "end": "1285440"
  },
  {
    "text": "in executing a query so it can actually pipeline steps it can actually start a",
    "start": "1285440",
    "end": "1290960"
  },
  {
    "text": "subsequent step even while an earlier one is underway so this can significantly reduce the query execution",
    "start": "1290960",
    "end": "1297170"
  },
  {
    "text": "times so we're taking advantage of that in Athena",
    "start": "1297170",
    "end": "1301870"
  },
  {
    "text": "[Music] this is the important stuff how do we",
    "start": "1302480",
    "end": "1309630"
  },
  {
    "text": "charge for it how much does it cost this is a server less offering so you're not",
    "start": "1309630",
    "end": "1315330"
  },
  {
    "text": "having to stand up in the infrastructure you're not having to pay on a per server basis instead with Athena we're charging",
    "start": "1315330",
    "end": "1320760"
  },
  {
    "text": "on a per query basis and the costs are five dollars per terabyte of data",
    "start": "1320760",
    "end": "1326940"
  },
  {
    "text": "scanned in order to compute your result set all right now if you're generating",
    "start": "1326940",
    "end": "1332659"
  },
  {
    "text": "external table definitions through Athena using that hive DDL that's free if a query fails that also it's free if",
    "start": "1332659",
    "end": "1340679"
  },
  {
    "text": "you cancel a query halfway through you'll be charged for the amount of data that we've scanned up to that point the",
    "start": "1340679",
    "end": "1347669"
  },
  {
    "text": "important point here is we're charging for the amount of data that we actually have to scan in order to compute the",
    "start": "1347669",
    "end": "1353490"
  },
  {
    "text": "results if we have to scan the entirety of your results of your data set that's",
    "start": "1353490",
    "end": "1358620"
  },
  {
    "text": "potentially got to be quite expensive but if you adopt some techniques such as compressing your data partitioning it",
    "start": "1358620",
    "end": "1366240"
  },
  {
    "text": "and using a format such as parquet or Orci then the amount of data that we",
    "start": "1366240",
    "end": "1371429"
  },
  {
    "text": "actually have to scan when executing your query can be significantly reduced by orders of magnitude and that's going",
    "start": "1371429",
    "end": "1378840"
  },
  {
    "text": "to improve the query latencies we're going to go from potentially several seconds down to just a couple of seconds but it's also significantly going to",
    "start": "1378840",
    "end": "1386100"
  },
  {
    "text": "reduce the costs of executing those queries and we'll see an example in a moment how we can drive the costs down",
    "start": "1386100",
    "end": "1391770"
  },
  {
    "text": "from five dollars down to effectively just over a cent per query so I",
    "start": "1391770",
    "end": "1399240"
  },
  {
    "text": "mentioned several times and in our example we're going to be doing exactly this the opportunity for converting our data to a query optimized format such as",
    "start": "1399240",
    "end": "1406440"
  },
  {
    "text": "park' or oh I see so these are storage formats they're very both very similar",
    "start": "1406440",
    "end": "1412010"
  },
  {
    "text": "and we support both in Athena they are columnar storage formats which",
    "start": "1412010",
    "end": "1417840"
  },
  {
    "text": "means that the data on disk is stored in blocks within a file there can be many",
    "start": "1417840",
    "end": "1423090"
  },
  {
    "text": "blocks and the data within individual blocks is data related to a specific column so all the data within a",
    "start": "1423090",
    "end": "1429630"
  },
  {
    "text": "particular block is a homogeneous data type okay so this has some immediate",
    "start": "1429630",
    "end": "1434669"
  },
  {
    "text": "benefits if the blocks are homogeneous data type it means that we can apply on a block",
    "start": "1434669",
    "end": "1439890"
  },
  {
    "text": "basis or a column by column basis very specific encodings we compress within",
    "start": "1439890",
    "end": "1445020"
  },
  {
    "text": "the body of those files on a column by column basis so that significantly reduces the size of these things on disk",
    "start": "1445020",
    "end": "1452570"
  },
  {
    "text": "both formats also include a significant amount of metadata describing the contents of those blocks and things such",
    "start": "1452570",
    "end": "1459480"
  },
  {
    "text": "as the men max values for all of their the entries within those blocks now this",
    "start": "1459480",
    "end": "1465540"
  },
  {
    "text": "means that we can take advantage of something called predicate push down when we're interrogating these files we",
    "start": "1465540",
    "end": "1471780"
  },
  {
    "text": "can actually interrogate first of all the metadata and determine which blocks we actually really need to pull off disk",
    "start": "1471780",
    "end": "1477300"
  },
  {
    "text": "so we don't have to bring the entire file into memory and scan it in order to retrieve that data potentially we can",
    "start": "1477300",
    "end": "1483900"
  },
  {
    "text": "identify just portions of those files and stream those into the processor okay",
    "start": "1483900",
    "end": "1490170"
  },
  {
    "text": "so this is a you know by using this query optimized format using predicates pushdowns and the fact that a lot of the",
    "start": "1490170",
    "end": "1496770"
  },
  {
    "text": "kind of analytics queries that we're conducting are going to be ones where we're only interrogating specific",
    "start": "1496770",
    "end": "1503340"
  },
  {
    "text": "columns within the set rather the entire set of columns within any particular table all these things combined means",
    "start": "1503340",
    "end": "1510960"
  },
  {
    "text": "that we can actually reduce the amount of data that we have to scan in order to compute a result so here's an example of",
    "start": "1510960",
    "end": "1518910"
  },
  {
    "text": "the you know paper query so we set up a little test where we had some log data think they had a tera died a terabyte",
    "start": "1518910",
    "end": "1526020"
  },
  {
    "text": "worth of log data and that's just in text format and then we've got this",
    "start": "1526020",
    "end": "1532350"
  },
  {
    "text": "little query up here on the right-hand side where we're doing some aggregations",
    "start": "1532350",
    "end": "1537420"
  },
  {
    "text": "over that log data so doing some analysis on top of it if we query via",
    "start": "1537420",
    "end": "1543240"
  },
  {
    "text": "Athena directly against that raw log data which were entirely entitled to do you know very easy lamech data in s3",
    "start": "1543240",
    "end": "1549300"
  },
  {
    "text": "start query if we do that against the raw log data we have to scan the",
    "start": "1549300",
    "end": "1554430"
  },
  {
    "text": "entirety of that one terabyte of data so that's effectively going to cost us five dollars irrespective of the size of the",
    "start": "1554430",
    "end": "1561690"
  },
  {
    "text": "result set because we had to scan the entirety of that one terabyte when we converted that to park' we boiled it",
    "start": "1561690",
    "end": "1568980"
  },
  {
    "text": "down to was it say here about 130 gigs so I went from a terabyte to 130 gig but because we're now taking",
    "start": "1568980",
    "end": "1576720"
  },
  {
    "text": "advantage of that columnar format and we're only looking for specific column",
    "start": "1576720",
    "end": "1582360"
  },
  {
    "text": "values within those objects on s3 and because we're taking advantage of predicates pushdown the amount of data",
    "start": "1582360",
    "end": "1588180"
  },
  {
    "text": "we actually had to scan when we executed the very same query was just over well was you know two point six nine gig",
    "start": "1588180",
    "end": "1595020"
  },
  {
    "text": "alright so over one terabyte for the first query just two point six nine gig",
    "start": "1595020",
    "end": "1600360"
  },
  {
    "text": "for the second query so that reduced our costs down from five dollars down to",
    "start": "1600360",
    "end": "1605610"
  },
  {
    "text": "just over a cent for the very same query and we've got the additional benefit that we went from several minutes for",
    "start": "1605610",
    "end": "1612090"
  },
  {
    "text": "the first query to just five seconds for the second query right so reduced latencies and reduced costs in our",
    "start": "1612090",
    "end": "1621510"
  },
  {
    "text": "documentation we provide several ways in which you can convert your stuff to parquet or IRC you can do it through",
    "start": "1621510",
    "end": "1627780"
  },
  {
    "text": "hive you can stand up an EMR cluster our preferred solution once glue becomes available in the regions where your",
    "start": "1627780",
    "end": "1633870"
  },
  {
    "text": "operating is to use glue it makes it very very easy to take CSV data and",
    "start": "1633870",
    "end": "1639030"
  },
  {
    "text": "rewrite it as parking all right we're actually going to it to finally get down",
    "start": "1639030",
    "end": "1644580"
  },
  {
    "text": "and do some work so we are going to look at the Athena console",
    "start": "1644580",
    "end": "1650540"
  },
  {
    "text": "but that was glue this is Athena you can see it looks like a kind of typical",
    "start": "1657740",
    "end": "1662860"
  },
  {
    "text": "sequel client browser-based sequel client top right we've got there the window where we can actually enter our",
    "start": "1662860",
    "end": "1668720"
  },
  {
    "text": "our queries either our DDL to create those table definitions or the actual select statements and so on we've got a",
    "start": "1668720",
    "end": "1674929"
  },
  {
    "text": "window below that for the results set remember that's being streamed out to us and also written to another location in",
    "start": "1674929",
    "end": "1681559"
  },
  {
    "text": "s3 and then down the left hand side we have another view on our data catalog",
    "start": "1681559",
    "end": "1686890"
  },
  {
    "text": "now there's actually one thing to point out here Athena is already available in",
    "start": "1686890",
    "end": "1692030"
  },
  {
    "text": "Dublin region so Athena already has a data catalog when glue becomes available",
    "start": "1692030",
    "end": "1699200"
  },
  {
    "text": "in the region where Athena is also present then you convert from using glues Athena's data catalog to using",
    "start": "1699200",
    "end": "1706640"
  },
  {
    "text": "glues data catalog so I think over time we're going to see glues data catalog you know this highly available account",
    "start": "1706640",
    "end": "1712910"
  },
  {
    "text": "wide data catalog being foundational for a lot of these technologies but even today you can do a lot of this stuff in",
    "start": "1712910",
    "end": "1718280"
  },
  {
    "text": "Athena in the Dublin region it has its own catalog I'm actually running it in",
    "start": "1718280",
    "end": "1723950"
  },
  {
    "text": "you se so I can take advantage of glue as well and I'm already pointed at glues data catalog so we can see here all the",
    "start": "1723950",
    "end": "1732020"
  },
  {
    "text": "different databases the very same list of databases that we saw in the glue console including my NYC transportation",
    "start": "1732020",
    "end": "1739940"
  },
  {
    "text": "canonical database which currently has no table definitions right I've actually created some date data I've converted",
    "start": "1739940",
    "end": "1746480"
  },
  {
    "text": "the taxi data and the limo data to parquet I've stored that in a bucket in s3 but I haven't created a table",
    "start": "1746480",
    "end": "1752240"
  },
  {
    "text": "definition over it so that's exactly what I'm going to do now so here I have",
    "start": "1752240",
    "end": "1760270"
  },
  {
    "text": "several callers that are created these crawlers are part of glue you can create",
    "start": "1760270",
    "end": "1765830"
  },
  {
    "text": "a crawler and then you can schedule it so that crawler can run on periodic basis and those crawlers can go looking",
    "start": "1765830",
    "end": "1772220"
  },
  {
    "text": "for sources of data within your AWS estate they cannot put into an s3 and you can point them at redshift or a JDBC",
    "start": "1772220",
    "end": "1778400"
  },
  {
    "text": "endpoint and they can infer the metadata regarding those relational schemas as well upload all of that into the catalog",
    "start": "1778400",
    "end": "1785260"
  },
  {
    "text": "so I've got a crawler here which is pointed at this particular",
    "start": "1785260",
    "end": "1791299"
  },
  {
    "text": "key space within s3 well this is not the",
    "start": "1791299",
    "end": "1796700"
  },
  {
    "text": "caller we want it's this one my",
    "start": "1796700",
    "end": "1802039"
  },
  {
    "text": "canonical crawler which is pointed at a bucket that I own where I've actually started out putting some of that Parque",
    "start": "1802039",
    "end": "1807559"
  },
  {
    "text": "de jure I'm going to run that crawler",
    "start": "1807559",
    "end": "1812258"
  },
  {
    "text": "okay this hopefully is only just going to take a few seconds to run I'm doing this through the console but we can",
    "start": "1817870",
    "end": "1825400"
  },
  {
    "text": "schedule the execution of these crawlers we also have an SDK and a command-line tool also allows you to invoke the",
    "start": "1825400",
    "end": "1831040"
  },
  {
    "text": "caller's that you've created thank you create crawlers through those tools as well and then you can invoke me through those tools so it's going to",
    "start": "1831040",
    "end": "1841120"
  },
  {
    "text": "take a few seconds but effectively it's going away looking in my bucket in s3 it's discovering some files if it",
    "start": "1841120",
    "end": "1849040"
  },
  {
    "text": "recognizes the file format then it's going to open that file and scan the first link might be the first 10,000",
    "start": "1849040",
    "end": "1854620"
  },
  {
    "text": "rows or so it's going to try and apply a classifier to infer the schema for those for the contents of that file oh and we",
    "start": "1854620",
    "end": "1864100"
  },
  {
    "text": "can see here that it's actually added one table to my database so if I look at my canonical database I now have a table",
    "start": "1864100",
    "end": "1873490"
  },
  {
    "text": "definition over that park' data you can see the schema there and if we flick back to Athena and",
    "start": "1873490",
    "end": "1881250"
  },
  {
    "text": "refresh the view we now see that we've got my transportation data there as well",
    "start": "1881250",
    "end": "1888570"
  },
  {
    "text": "all right so I've created a table",
    "start": "1888570",
    "end": "1895300"
  },
  {
    "text": "definition over my data I could have done that by creating some DDL here I can now actually start running some",
    "start": "1895300",
    "end": "1900309"
  },
  {
    "text": "queries so we've got taxi data and limo data at",
    "start": "1900309",
    "end": "1907650"
  },
  {
    "text": "the moment in our canonical data set and we can see here the number of entries we've got for each of those so that's",
    "start": "1907650",
    "end": "1914960"
  },
  {
    "text": "about 10 million rows for the taxi data and just under one-and-a-half million",
    "start": "1914960",
    "end": "1921240"
  },
  {
    "text": "rows of limo data oh I knew there was",
    "start": "1921240",
    "end": "1927630"
  },
  {
    "text": "something else that I wanted to do okay so when a winged glue actually infers the schema for the stuff that it's found",
    "start": "1927630",
    "end": "1934950"
  },
  {
    "text": "in s3 it's going to create a table definition and it's going to attach specific data types to all those columns",
    "start": "1934950",
    "end": "1941460"
  },
  {
    "text": "but at the moment glue only applies very very low-level data types so you'll",
    "start": "1941460",
    "end": "1946560"
  },
  {
    "text": "notice here that my pickup date time and drop-off date/time have been identified as big integers actually what I want to",
    "start": "1946560",
    "end": "1954540"
  },
  {
    "text": "do is to convert those to time stamps in",
    "start": "1954540",
    "end": "1963120"
  },
  {
    "text": "the future blue will also infer some of these high-level data types as well so",
    "start": "1963120",
    "end": "1968310"
  },
  {
    "text": "it will recognize time stamps and will actually add those to the schema definitions at the moment we sometimes have to make some of these additional",
    "start": "1968310",
    "end": "1974220"
  },
  {
    "text": "modifications so flick back to Athena",
    "start": "1974220",
    "end": "1980460"
  },
  {
    "text": "what other queries can we run ok we can start running some more interesting analyses so let's look at some average",
    "start": "1980460",
    "end": "1988410"
  },
  {
    "text": "distances average prices for some of these taxi and limo journeys so we can",
    "start": "1988410",
    "end": "1995580"
  },
  {
    "text": "see here that limo journeys are just under three miles that's the average",
    "start": "1995580",
    "end": "2001040"
  },
  {
    "text": "distance and it's costing around about $10 per mile average taxi journeys are",
    "start": "2001040",
    "end": "2009010"
  },
  {
    "text": "out just over four and a half miles and the cost there the average cost is about $8 per mile okay and we can get more",
    "start": "2009010",
    "end": "2020150"
  },
  {
    "text": "complex I've got data in s3 I've created an",
    "start": "2020150",
    "end": "2025930"
  },
  {
    "text": "external table definition once that table definition has been created once yeah it's persisted in the catalog so",
    "start": "2025930",
    "end": "2031480"
  },
  {
    "text": "you can come back to Athena find your existing table definitions and just start writing sequel so we don't have to",
    "start": "2031480",
    "end": "2037000"
  },
  {
    "text": "go through the entire entering process every time we want to write queries all",
    "start": "2037000",
    "end": "2042490"
  },
  {
    "text": "right so the next thing I'm going to do very briefly we're gonna the last",
    "start": "2042490",
    "end": "2048129"
  },
  {
    "text": "section we're actually going to put some visualization on top of this but whilst we're doing that I'm also going to be",
    "start": "2048130",
    "end": "2053200"
  },
  {
    "text": "eating all of that uber data into my bucket as well all right so I'm going to",
    "start": "2053200",
    "end": "2058360"
  },
  {
    "text": "create a job in Glu",
    "start": "2058360",
    "end": "2062280"
  },
  {
    "text": "it's too okay we have to choose a role",
    "start": "2064780",
    "end": "2072470"
  },
  {
    "text": "because glue actually has to access some of our resources on our behalf so we have to supply role allows it to do that",
    "start": "2072470",
    "end": "2078559"
  },
  {
    "text": "with glue we can let glue create for us a script we can actually upload our own scripts",
    "start": "2078559",
    "end": "2083569"
  },
  {
    "text": "as well so if you've developed your own PI smart scripts elsewhere you can actually use glue to execute those scripts but we're going to have glue",
    "start": "2083569",
    "end": "2090648"
  },
  {
    "text": "create for us a script I need to supply a temporary bucket if there are any",
    "start": "2090649",
    "end": "2097579"
  },
  {
    "text": "intermediate results that are generated and then I want to find the table at",
    "start": "2097579",
    "end": "2105200"
  },
  {
    "text": "that acting is my source okay so I'm",
    "start": "2105200",
    "end": "2111950"
  },
  {
    "text": "going to take my uber data which is in an account that belongs elsewhere and I",
    "start": "2111950",
    "end": "2117799"
  },
  {
    "text": "am going to choose a table",
    "start": "2117799",
    "end": "2122049"
  },
  {
    "text": "as a target all right so even as we're",
    "start": "2126390",
    "end": "2131849"
  },
  {
    "text": "actually creating this job glue is showing us the kind of transformation is going to execute and we can actually",
    "start": "2131849",
    "end": "2137910"
  },
  {
    "text": "modify some of these mappings if we so wish I'm just gonna leave that as is for now",
    "start": "2137910",
    "end": "2145369"
  },
  {
    "text": "there's my job and what we can actually see here great we'll actually see that",
    "start": "2147289",
    "end": "2155099"
  },
  {
    "text": "glue has created for us some PI spark code now actually what I can do if I",
    "start": "2155099",
    "end": "2160559"
  },
  {
    "text": "want is modify that code further and",
    "start": "2160559",
    "end": "2165690"
  },
  {
    "text": "that is exactly what I'm going to do here I'll very briefly tell you what I'm doing I'm just adding in an additional",
    "start": "2165690",
    "end": "2170789"
  },
  {
    "text": "column which represents the type of data so we've got limo date a taxi data so",
    "start": "2170789",
    "end": "2175829"
  },
  {
    "text": "we've got a column in our canonical data set called type with values limo and taxi for this data set we're going to",
    "start": "2175829",
    "end": "2182309"
  },
  {
    "text": "add in the value bus all the rows that come from Aruba data will be flagged as or tanked as uber data and so I'm going",
    "start": "2182309",
    "end": "2189059"
  },
  {
    "text": "to save that and then run that job what let's just point out this bit here this",
    "start": "2189059",
    "end": "2197190"
  },
  {
    "text": "line here is a line of Pais barcode",
    "start": "2197190",
    "end": "2202950"
  },
  {
    "text": "we're actually writing to that target to that s3 target and here we're specifying the format includes done Glu knows",
    "start": "2202950",
    "end": "2209220"
  },
  {
    "text": "already that the data in that target is park' so I didn't even have to ask me if you create a job and glue doesn't know",
    "start": "2209220",
    "end": "2216329"
  },
  {
    "text": "what format you're expected to to output it will actually give you a choice button it's already alright so we're",
    "start": "2216329",
    "end": "2223680"
  },
  {
    "text": "gonna run that job in the background great so got Athena for writing sequel",
    "start": "2223680",
    "end": "2230339"
  },
  {
    "text": "directly against our data and now we've got quick site so quick site is our",
    "start": "2230339",
    "end": "2235440"
  },
  {
    "text": "manage BI and visualization tool allows you to create dashboards create charts assemble them into dashboards share",
    "start": "2235440",
    "end": "2242160"
  },
  {
    "text": "those dashboards within groups within your organisation if you want to use",
    "start": "2242160",
    "end": "2248309"
  },
  {
    "text": "quick site you have to sign up you just you know create an account once you've done that you can access it much as any",
    "start": "2248309",
    "end": "2253890"
  },
  {
    "text": "other AWS in order to use quick sight with Athena",
    "start": "2253890",
    "end": "2260619"
  },
  {
    "text": "[Music] you do need to tell quick sight you're",
    "start": "2261040",
    "end": "2269390"
  },
  {
    "text": "prepared to use Athena so I think there are account permissions here I've done",
    "start": "2269390",
    "end": "2275330"
  },
  {
    "text": "this already but I'm just just making you aware of it you need to check this box here that would allow your quick",
    "start": "2275330",
    "end": "2281870"
  },
  {
    "text": "sight instance to access Athena on your behalf all right I've already done all",
    "start": "2281870",
    "end": "2287630"
  },
  {
    "text": "of them so what I'm gonna do what I'm going to do is create a new data set so",
    "start": "2287630",
    "end": "2299750"
  },
  {
    "text": "quick sight allows me to connect to lots and lots of different sources of data to an AWS to s3 to relational databases in",
    "start": "2299750",
    "end": "2306260"
  },
  {
    "text": "RDS we in this instance we're going to connect by a way of Athena so we're just",
    "start": "2306260",
    "end": "2311570"
  },
  {
    "text": "going to delegate a lot of the heavy lifting to Athena and allow Athena to do all the aggregations on our behalf we",
    "start": "2311570",
    "end": "2316760"
  },
  {
    "text": "can connect to redshift and so on in this instance we're just going to use quick sight to connect to Athena and",
    "start": "2316760",
    "end": "2322190"
  },
  {
    "text": "let's call this my C so I'm giving a name to my data source and again quick",
    "start": "2322190",
    "end": "2333950"
  },
  {
    "text": "sight is aware of all of the different databases that I could I could conceivably use through Athena so we're",
    "start": "2333950",
    "end": "2340550"
  },
  {
    "text": "going to use that canonical one and this particular table",
    "start": "2340550",
    "end": "2345760"
  },
  {
    "text": "I want to directly query the data afire",
    "start": "2352119",
    "end": "2357140"
  },
  {
    "text": "athena so quick site also has an in-memory computation engine called",
    "start": "2357140",
    "end": "2362210"
  },
  {
    "text": "spice it allows you to copy data from all those different sources into AWS into this in memory computation engine",
    "start": "2362210",
    "end": "2368180"
  },
  {
    "text": "which can hugely speed up a lot of the kind of visualization queries that you",
    "start": "2368180",
    "end": "2373250"
  },
  {
    "text": "want to run what we're doing is delegating to Athena to in order to be able to scan potentially terabytes or",
    "start": "2373250",
    "end": "2378859"
  },
  {
    "text": "petabytes of data at rest in s 3 I'm",
    "start": "2378859",
    "end": "2389420"
  },
  {
    "text": "also going to create a new calculated field let's call it power of de and",
    "start": "2389420",
    "end": "2402160"
  },
  {
    "text": "we're going to use the extract function we're going to take the hours within those time stamps from I think it's the",
    "start": "2402160",
    "end": "2410740"
  },
  {
    "text": "pickup",
    "start": "2410740",
    "end": "2413740"
  },
  {
    "text": "all right so I've created the connection to a data set in fact it's a connection to Athena which is then pointed at an underlying data set and as part of that",
    "start": "2427020",
    "end": "2435360"
  },
  {
    "text": "setting up that data set I've also created a new calculated field so I'm gonna save that data set and actually",
    "start": "2435360",
    "end": "2441150"
  },
  {
    "text": "then create a visualization on top of it all right so let's just have a look at",
    "start": "2441150",
    "end": "2448290"
  },
  {
    "text": "just look at our data based on the type",
    "start": "2448290",
    "end": "2454770"
  },
  {
    "text": "limo taxi see at the moment we're still only got the limo and the taxi data our",
    "start": "2454770",
    "end": "2461100"
  },
  {
    "text": "glue job is still running in the background converting all that movie data to parkade when it becomes available once it's actually been output",
    "start": "2461100",
    "end": "2467130"
  },
  {
    "text": "is parking in that destination folder it will be media variable for querying via",
    "start": "2467130",
    "end": "2472410"
  },
  {
    "text": "a quick site as well but the moment you know you'll see that we've got far more taxi news than we do have limo data",
    "start": "2472410",
    "end": "2479980"
  },
  {
    "text": "[Music]",
    "start": "2479980",
    "end": "2483190"
  },
  {
    "text": "what are you doing here",
    "start": "2490359",
    "end": "2493960"
  },
  {
    "text": "so here",
    "start": "2500150",
    "end": "2503079"
  },
  {
    "text": "so quick site is is trying to make sense of your intentions trying to make it as",
    "start": "2506040",
    "end": "2511980"
  },
  {
    "text": "easy as possible for you to create visualizations as quickly as possible so you notice just by clicking on a few of these different columns here on the left",
    "start": "2511980",
    "end": "2518730"
  },
  {
    "text": "hand side we've actually already started to get some visualizations now we can tweak them we can choose the visualization type we can further modify",
    "start": "2518730",
    "end": "2525810"
  },
  {
    "text": "them but all I've had to do here is to choose pickup date time and the type",
    "start": "2525810",
    "end": "2532170"
  },
  {
    "text": "taxi or uber and already we've got a simple chart here I'm going to slider at",
    "start": "2532170",
    "end": "2539160"
  },
  {
    "text": "the bottom that allows me to to scan or to grow and shrink the amount of data",
    "start": "2539160",
    "end": "2544530"
  },
  {
    "text": "that I'm visualizing I could choose to go all the way down to the hour and",
    "start": "2544530",
    "end": "2554220"
  },
  {
    "text": "again",
    "start": "2554220",
    "end": "2556550"
  },
  {
    "text": "we can see here that we can actually expand and contract the range over which we're querying and whenever we execute",
    "start": "2561620",
    "end": "2568550"
  },
  {
    "text": "one of these queries or the visualization execute a query in this instance is delegating to Athena in",
    "start": "2568550",
    "end": "2574970"
  },
  {
    "text": "order to query against that data sitting at rest in s 3 ok let's just see if our",
    "start": "2574970",
    "end": "2582020"
  },
  {
    "text": "glue job has completed this one he",
    "start": "2582020",
    "end": "2588380"
  },
  {
    "text": "wasn't ok it's still running we're 6",
    "start": "2588380",
    "end": "2595040"
  },
  {
    "text": "minutes in so there was a bit of spin up time there in order to provision some of",
    "start": "2595040",
    "end": "2601430"
  },
  {
    "text": "the resources dedicated to servicing our job once all that's been spun up they",
    "start": "2601430",
    "end": "2606890"
  },
  {
    "text": "could be reused for other jobs that we're running within our environment under the hood glue is also using spark",
    "start": "2606890",
    "end": "2613580"
  },
  {
    "text": "in order to do these conversions now behalf once that data is output to our target directory we would actually see",
    "start": "2613580",
    "end": "2619310"
  },
  {
    "text": "our visualizations in quick sight enriched with that uber data as well ok",
    "start": "2619310",
    "end": "2625580"
  },
  {
    "text": "that pretty much brings me to the end I'll take some questions and I've already got one question online which is",
    "start": "2625580",
    "end": "2631760"
  },
  {
    "text": "you know could I comment on the difference between Athena and redshift or specifically redshift spectrum so",
    "start": "2631760",
    "end": "2639050"
  },
  {
    "text": "earlier this year we announced a new feature for redshift for the managed data warehouse that features called",
    "start": "2639050",
    "end": "2645710"
  },
  {
    "text": "redshift spectrum and spectrum also allows you to query directly against data sitting at rest in s 3 well that's",
    "start": "2645710",
    "end": "2653480"
  },
  {
    "text": "what I think you do as well I think in many ways these are kind of complementary services the great thing",
    "start": "2653480",
    "end": "2659930"
  },
  {
    "text": "about theater is there is nothing to setup whatsoever other than that external table definition remember",
    "start": "2659930",
    "end": "2666200"
  },
  {
    "text": "you're sharing those definitions across several different services with Athena you just start writing queries so Athena",
    "start": "2666200",
    "end": "2672590"
  },
  {
    "text": "is great for interactive ad-hoc an exploratory analysis of your data alright so again going back to that",
    "start": "2672590",
    "end": "2680240"
  },
  {
    "text": "agile analytics scenario where we want to conduct experiments we want to discover something interesting about the",
    "start": "2680240",
    "end": "2686420"
  },
  {
    "text": "data that we have sitting potential in the data link Athena makes it very easy for us to start querying that within a",
    "start": "2686420",
    "end": "2692750"
  },
  {
    "text": "matter of minutes redshift spectrum is a feature that's more appropriate where we're building up",
    "start": "2692750",
    "end": "2699030"
  },
  {
    "text": "potentially again in an agile fashion but building out a real data warehousing",
    "start": "2699030",
    "end": "2704910"
  },
  {
    "text": "capability where we want to analyze our structured data where we have well-understood queries well-understood",
    "start": "2704910",
    "end": "2711090"
  },
  {
    "text": "reports that we want to run regularly against that data and where potentially we may have very large fact tables",
    "start": "2711090",
    "end": "2717710"
  },
  {
    "text": "petabytes or even exabytes sitting at rest in s 3 and we want to join that",
    "start": "2717710",
    "end": "2722880"
  },
  {
    "text": "against dimension data that's sitting in redshift but it may be in order to identify some of those candidate data",
    "start": "2722880",
    "end": "2729240"
  },
  {
    "text": "sets in the first instance we're going to use Athena very quick very easy to get up and running we can discover some",
    "start": "2729240",
    "end": "2735390"
  },
  {
    "text": "of those data sets start writing some exploratory queries and if we think this is worth production izing as it were we",
    "start": "2735390",
    "end": "2741960"
  },
  {
    "text": "can bring some of those queries into redshift and take advantage of redshift spectrum but again spectrum and Athena",
    "start": "2741960",
    "end": "2748740"
  },
  {
    "text": "and hive on EMR and spark on NMR they can all be pointed at the same",
    "start": "2748740",
    "end": "2753810"
  },
  {
    "text": "underlying table definition it's the same underlying data sitting in s3 any",
    "start": "2753810",
    "end": "2761400"
  },
  {
    "text": "other questions",
    "start": "2761400",
    "end": "2764089"
  },
  {
    "text": "yes sir the questions around glue and it's crawlers and in fact this gentleman you know it's previously created a",
    "start": "2776019",
    "end": "2782539"
  },
  {
    "text": "crawler and it's done exactly what I did after the crawler had run had changed one of those datatypes and then had rerun the",
    "start": "2782539",
    "end": "2789380"
  },
  {
    "text": "crawler and guess what glue changed that datatype back I think that's a feature",
    "start": "2789380",
    "end": "2794660"
  },
  {
    "text": "of glue today so nice thing about glues when you rerun those crawlers if they",
    "start": "2794660",
    "end": "2799969"
  },
  {
    "text": "discover that the the sources of data have changed so it may be that you're",
    "start": "2799969",
    "end": "2805160"
  },
  {
    "text": "calling JSON data for example and you know in between times you've added new",
    "start": "2805160",
    "end": "2810259"
  },
  {
    "text": "fields you modify the structure of some of those sources glue will actually Immersion the metadata in the catalog so",
    "start": "2810259",
    "end": "2816529"
  },
  {
    "text": "what you were seeing your catalog is actually some version metadata but I think unfortunate today when you rerun",
    "start": "2816529",
    "end": "2822079"
  },
  {
    "text": "that crawler it's going to revert to some of those low-level data types",
    "start": "2822079",
    "end": "2827709"
  },
  {
    "text": "right yeah",
    "start": "2837330",
    "end": "2842230"
  },
  {
    "text": "so the question here is about being able to query across different data sources you know in Glu i can actually point",
    "start": "2849930",
    "end": "2855900"
  },
  {
    "text": "glue at my sequel or redshift or other you know so i've got relational metadata and metadata regarding file sitting in",
    "start": "2855900",
    "end": "2862319"
  },
  {
    "text": "s3 can i query across them i think well firstly athena itself is today dedicated",
    "start": "2862319",
    "end": "2870270"
  },
  {
    "text": "only to querying data sitting at rest in s 3 if we stood up presto on EMR that",
    "start": "2870270",
    "end": "2875520"
  },
  {
    "text": "will actually allow us to join the Presto connectors allow us to join from several different data sources but",
    "start": "2875520",
    "end": "2882119"
  },
  {
    "text": "typically one of the strategies that we would employ in terms of building out a kind of data lake platform is to take",
    "start": "2882119",
    "end": "2891030"
  },
  {
    "text": "all of our raw sources of data including some of that relational data and create",
    "start": "2891030",
    "end": "2896400"
  },
  {
    "text": "entities entities within s3 that would allow us to join across those entities so it may be that we take a raw data and",
    "start": "2896400",
    "end": "2903180"
  },
  {
    "text": "in a separate bucket as i'm doing in this example building out some more query optimized models and those models",
    "start": "2903180",
    "end": "2910380"
  },
  {
    "text": "may actually take data from several different sources from relational from third-party api's you know from other",
    "start": "2910380",
    "end": "2916950"
  },
  {
    "text": "file formats and so on but once we've built that canonical model that would allow us to join across those several different sources yeah yeah so will glue",
    "start": "2916950",
    "end": "2932010"
  },
  {
    "text": "does glue today supports karla today no but it's on the roadmap yeah so under",
    "start": "2932010",
    "end": "2940559"
  },
  {
    "text": "the hood glue is running spark in order to perform those transformations and as you may know spark you know supports a",
    "start": "2940559",
    "end": "2946680"
  },
  {
    "text": "number of different programming languages for our first release we chose Python but we know that there are many developers who want to use other",
    "start": "2946680",
    "end": "2952530"
  },
  {
    "text": "languages and that will come I've got one at the back first and then I'll uh",
    "start": "2952530",
    "end": "2960440"
  },
  {
    "text": "so the question is if I'm you know isolated my data and only connecting to",
    "start": "2973970",
    "end": "2979280"
  },
  {
    "text": "it even the data in s3 volume up my V PC am I breaking that encapsulation no you",
    "start": "2979280",
    "end": "2985670"
  },
  {
    "text": "can set up V PCM points when you're configuring glue so that it can go directly to those V PC endpoints so the",
    "start": "2985670",
    "end": "2991069"
  },
  {
    "text": "data never has to translate over there at the Internet and yeah so the other",
    "start": "2991069",
    "end": "3003730"
  },
  {
    "text": "part of the question is when we're actually provisioning those glue resources are they outside the V PC to",
    "start": "3003730",
    "end": "3009819"
  },
  {
    "text": "end up translating data outside the V PC my understanding is we provision those resources in your V PC we create elastic",
    "start": "3009819",
    "end": "3016180"
  },
  {
    "text": "network interfaces within your V PC they're dedicated only to your account",
    "start": "3016180",
    "end": "3021329"
  },
  {
    "text": "so nobody else can access that data there's a forthcoming feature whereby we",
    "start": "3021329",
    "end": "3026619"
  },
  {
    "text": "will actually encrypt that data whilst it's actually being processed within those resources within your frequency and that brings me to the end and you",
    "start": "3026619",
    "end": "3034210"
  },
  {
    "text": "know catch up with me in the next few minutes or after the next session the next session he's on glue but we'll wrap",
    "start": "3034210",
    "end": "3039250"
  },
  {
    "text": "up there hope that's useful [Applause]",
    "start": "3039250",
    "end": "3043299"
  }
]