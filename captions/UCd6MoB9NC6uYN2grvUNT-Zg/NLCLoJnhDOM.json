[
  {
    "text": "so hi this is BD t40 for large scale ETL data flows with data pipeline and data",
    "start": "530",
    "end": "6960"
  },
  {
    "text": "not so thank you for coming out of this session I'm sorry I work as an engineer",
    "start": "6960",
    "end": "12809"
  },
  {
    "text": "on the analytics team at Coursera and today I'll be talking about data dr. in-house ETL framework that was built on",
    "start": "12809",
    "end": "19980"
  },
  {
    "text": "top of data pipeline and we open sourced about 10 year ago so in this session",
    "start": "19980",
    "end": "26189"
  },
  {
    "text": "I'll be covering how we use data pipeline to orchestrate and manage our ETL and then how why and how we develop",
    "start": "26189",
    "end": "33360"
  },
  {
    "text": "data to develop these ETL more easily some and like how do you deduct has",
    "start": "33360",
    "end": "38910"
  },
  {
    "text": "enabled every developer in our organization to write their own ETLs now and some of the best practices",
    "start": "38910",
    "end": "44399"
  },
  {
    "text": "that we have found in turning hundreds of pipelines and at the end you should get a very good preview of how you can",
    "start": "44399",
    "end": "50640"
  },
  {
    "text": "use data to write and develop your own ideas so let's get started I'm sure many",
    "start": "50640",
    "end": "58320"
  },
  {
    "text": "of you have taken classes on Coursera before or have heard of Coursera but for people who aren't familiar cursory eyes",
    "start": "58320",
    "end": "64739"
  },
  {
    "text": "and online education startup and our mission is to provide universal access to the world's best education and to do",
    "start": "64739",
    "end": "72600"
  },
  {
    "text": "so we partner with best universities across the globe and I'm sure you recognize a lot of the names up here and",
    "start": "72600",
    "end": "78950"
  },
  {
    "text": "we work with them to put their best courses online from the best instructors",
    "start": "78950",
    "end": "84150"
  },
  {
    "text": "and then make them accessible to millions of learners to see the impact of this let's talk about like some of",
    "start": "84150",
    "end": "89939"
  },
  {
    "text": "the scale that we have achieved at Coursera currently we have 15 million learners on our platform which",
    "start": "89939",
    "end": "96240"
  },
  {
    "text": "participate in 1,300 college-level courses and these courses have been created by 120 of our partners and all",
    "start": "96240",
    "end": "104490"
  },
  {
    "text": "of these courses have been completed 2.5 million times by our learners and with all this learner activity on our",
    "start": "104490",
    "end": "110490"
  },
  {
    "text": "platform we see collect we collect and see a lot of data and like most",
    "start": "110490",
    "end": "115590"
  },
  {
    "text": "companies our data stored in like various different source systems in various different formats and we need to",
    "start": "115590",
    "end": "121680"
  },
  {
    "text": "like aggregate all this data into a single warehouse so that our analyst can analyze this more data more easily and",
    "start": "121680",
    "end": "127079"
  },
  {
    "text": "the you can make it accessible to everyone in the company for choice of warehouse",
    "start": "127079",
    "end": "132450"
  },
  {
    "text": "we use Amazon redshift 1 because of like accessibility using sequel so most of",
    "start": "132450",
    "end": "137970"
  },
  {
    "text": "our company was familiar with sequel so it's very easy for them to like start using the warehouse and read ship saw a",
    "start": "137970",
    "end": "144180"
  },
  {
    "text": "tremendous performance when we compared it to other warehouse solutions such as hive so we ended up choosing redshift",
    "start": "144180",
    "end": "150920"
  },
  {
    "text": "currently a fewer than 200 employees at the company 167 have written at least",
    "start": "150920",
    "end": "156630"
  },
  {
    "text": "one query in the last one year so they're like really proud of that fact because as we strive to become more",
    "start": "156630",
    "end": "162480"
  },
  {
    "text": "data-driven as an organization we want to make data more accessible to everyone in the company so to get a sense of what",
    "start": "162480",
    "end": "170640"
  },
  {
    "text": "ETL at Coursera looks like let's go through a brief flow of how data is stored in the stored at Coursera and how",
    "start": "170640",
    "end": "177990"
  },
  {
    "text": "it reaches they checked and then how we used that further so on the left side you see like four primary data sources",
    "start": "177990",
    "end": "184230"
  },
  {
    "text": "so on the top you have the clickstream data that we collect with our internal eventing system and this data is then",
    "start": "184230",
    "end": "190950"
  },
  {
    "text": "pushed to s3 which is later ETL done and then we run my sequel on Amazon RDS",
    "start": "190950",
    "end": "196250"
  },
  {
    "text": "which powers our relay which works as a relational data store we also run Cassandra no sequel database and on ec2",
    "start": "196250",
    "end": "204150"
  },
  {
    "text": "we manage this ourselves and then we also pull data from some third-party services such as Facebook",
    "start": "204150",
    "end": "209310"
  },
  {
    "text": "because we have facebook login available and then CRM tools if you if your company uses that and then support",
    "start": "209310",
    "end": "215430"
  },
  {
    "text": "systems etc this data is then transformed using either ec2 machines",
    "start": "215430",
    "end": "220560"
  },
  {
    "text": "where you can run a single machine job or using EMR and we use carding as a framework for writing MapReduce jobs",
    "start": "220560",
    "end": "227220"
  },
  {
    "text": "because most of Coursera's back-end is written in scala so our developers didn't have to learn a new language when they wanted to write",
    "start": "227220",
    "end": "233580"
  },
  {
    "text": "MapReduce jobs so we could do it directly your Scala and write our MapReduce jobs using scoldings carding",
    "start": "233580",
    "end": "238890"
  },
  {
    "text": "is an open source project by Twitter for those who aren't familiar then we load",
    "start": "238890",
    "end": "244140"
  },
  {
    "text": "this data in then EMR jobs basically push this data into s3 which is then loaded into redshift using the copy",
    "start": "244140",
    "end": "250500"
  },
  {
    "text": "command and redshift now that we have data in this way in a single warehouse and a single at a single place we can",
    "start": "250500",
    "end": "257910"
  },
  {
    "text": "run applications such as periscope or looker or Amazon quick side once we get access",
    "start": "257910",
    "end": "264260"
  },
  {
    "text": "to that and then use that to make it more accessible and analyze data directly we can we also further",
    "start": "264260",
    "end": "270800"
  },
  {
    "text": "transform this data to build things like recommendation systems or make analyze",
    "start": "270800",
    "end": "276830"
  },
  {
    "text": "AP tests that we have run and this data is then pushed into RDS or Cassandra based on like latency requirements so we",
    "start": "276830",
    "end": "283190"
  },
  {
    "text": "don't want to serve production traffic out of redshift so we load this data into like Cassandra or RDS based on",
    "start": "283190",
    "end": "288380"
  },
  {
    "text": "requirements there so with all this like for all these data sources and us",
    "start": "288380",
    "end": "294919"
  },
  {
    "text": "building new features every week we currently run around 150 active",
    "start": "294919",
    "end": "299930"
  },
  {
    "text": "pipelines so these pipelines run daily or hourly cadence and then have been",
    "start": "299930",
    "end": "305870"
  },
  {
    "text": "developed by 44 different developers in the company and to give you a good sense of like what this 44 number means we",
    "start": "305870",
    "end": "312710"
  },
  {
    "text": "currently have a total of 70 developers in our organization so more than 50 percent have contributed to ETL in some",
    "start": "312710",
    "end": "319070"
  },
  {
    "text": "fashion so before we start building some of the pipeline let's talk about some",
    "start": "319070",
    "end": "325340"
  },
  {
    "text": "requirements that we have out of our ETL system so fault tolerance",
    "start": "325340",
    "end": "330440"
  },
  {
    "text": "in case a jobs fail or in case we have a resource failure during running the ETL",
    "start": "330440",
    "end": "336470"
  },
  {
    "text": "so you're running a MapReduce job when no dies we want to automatically restart the job and basically not have to go",
    "start": "336470",
    "end": "343280"
  },
  {
    "text": "through the whole pipeline again and we want and data pipeline does amazing job providing default retries so we use that",
    "start": "343280",
    "end": "349820"
  },
  {
    "text": "feature it provides three tries on a resource level or at an activity level which we use here then you want the",
    "start": "349820",
    "end": "357080"
  },
  {
    "text": "ability to schedule pipelines at different cadence and different frequency at different time of the day",
    "start": "357080",
    "end": "363110"
  },
  {
    "text": "so you can run some pipelines at like 12:00 in the night or you want about my you might want to run some pipelines at",
    "start": "363110",
    "end": "369200"
  },
  {
    "text": "8:00 in the morning and similarly you have some pipelines that run on a weekly schedule so if you have received the",
    "start": "369200",
    "end": "376030"
  },
  {
    "text": "recommendations he recommended courses email from Coursera that runs the weekly schedule because we and for like if you",
    "start": "376030",
    "end": "383389"
  },
  {
    "text": "are aggregating data from our payment processing system we try to do that every our so that we can reconcile up",
    "start": "383389",
    "end": "389639"
  },
  {
    "text": "and then other data sources might be okay with like a Monday latency and similarly then instead of having one",
    "start": "389639",
    "end": "397650"
  },
  {
    "text": "large pipeline that take takes care of all this data we want different pipelines so we want dependencies",
    "start": "397650",
    "end": "403830"
  },
  {
    "text": "between pipelines so let's say a recommendation model should not start training before like course you like",
    "start": "403830",
    "end": "410189"
  },
  {
    "text": "learner activity data has been loaded so we want the dependency across different pipelines and similarly within a",
    "start": "410189",
    "end": "415530"
  },
  {
    "text": "pipeline you don't want a single but you don't want the other all pipeline to be just one step because you want that to",
    "start": "415530",
    "end": "422789"
  },
  {
    "text": "be more modular saying extract data and then do some transformation then do maybe do some more transformation and",
    "start": "422789",
    "end": "428550"
  },
  {
    "text": "then load that then resource management so every pipeline let's say it's running",
    "start": "428550",
    "end": "435569"
  },
  {
    "text": "at 20 node cluster and then similarly other pipelines require 30 node cluster we don't want all pipelines blocking on",
    "start": "435569",
    "end": "441419"
  },
  {
    "text": "single large cluster to run them so we want some isolation between one pipelines through a silly sources should",
    "start": "441419",
    "end": "447360"
  },
  {
    "text": "not be blocked by other pipelines and similarly we want some amount of like we don't want and congestion on a single",
    "start": "447360",
    "end": "454349"
  },
  {
    "text": "resource across our pipeline so we every pipeline should start and terminate its own resources and then take care of that",
    "start": "454349",
    "end": "462139"
  },
  {
    "text": "the ability to monitor an alert so in case you have a job failure or you have",
    "start": "462139",
    "end": "467430"
  },
  {
    "text": "like a complete pipeline failure you want to like trigger either a pager duty or an SMS alert around that the other",
    "start": "467430",
    "end": "474569"
  },
  {
    "text": "things you want to monitor is like a health of a pipeline so how much disk space are you using or how much data are",
    "start": "474569",
    "end": "480389"
  },
  {
    "text": "you processing for a single pipeline is definitely crucial because like let's say you had a pipeline that you wrote in",
    "start": "480389",
    "end": "486150"
  },
  {
    "text": "a year ago and that time it was using 50 percent this phase but as more user activity comes in here like reaching",
    "start": "486150",
    "end": "492150"
  },
  {
    "text": "that cliff so you want to get alerted about that in advance rather than seeing the pipeline fail and then you realizing",
    "start": "492150",
    "end": "497819"
  },
  {
    "text": "that you overflow and then as we talked about like 50 percent of the developers",
    "start": "497819",
    "end": "503849"
  },
  {
    "text": "in the organization have contributed to ETL so we definitely want this process to be as seamless as possible so like",
    "start": "503849",
    "end": "509580"
  },
  {
    "text": "not everybody who needs to know all like everything that is required to write an ETL so we want the development",
    "start": "509580",
    "end": "515900"
  },
  {
    "text": "meant to be as smooth as possible and then ability to test these pipelines in dev as easy as possible as well so data",
    "start": "515900",
    "end": "523370"
  },
  {
    "text": "pipeline provides all of these features very seamlessly you can use the web UI to develop your ETL but there were some",
    "start": "523370",
    "end": "529610"
  },
  {
    "text": "additional things that we wanted to make our development more easy so we ended up building data duct which basically wraps",
    "start": "529610",
    "end": "535640"
  },
  {
    "text": "around the data pipeline API and make things more easy to work with and then some of the features that data",
    "start": "535640",
    "end": "541640"
  },
  {
    "text": "dock provides basically code reusability so we can reuse the same code across different pipelines so if you were",
    "start": "541640",
    "end": "547940"
  },
  {
    "text": "pulling data from the dam say same data source we don't want to like write different pipelines that basically have",
    "start": "547940",
    "end": "553850"
  },
  {
    "text": "a replica of that same code so we want to reuse the same code across different pipelines and then we want extensibility",
    "start": "553850",
    "end": "559100"
  },
  {
    "text": "so once we develop a new feature we want the same pipeline framework to be more extendable so if when we initially wrote",
    "start": "559100",
    "end": "567620"
  },
  {
    "text": "our ETL framework we were only pulling data from IDs so when we wrote Cassandra we wanted that to be more seamless as",
    "start": "567620",
    "end": "573920"
  },
  {
    "text": "well that we could integrate with Cassandra just like we had integrated with RDS we wanted a command line",
    "start": "573920",
    "end": "579470"
  },
  {
    "text": "interface to manage and develop pipelines more easily and then I talked about testing so we want the ability to",
    "start": "579470",
    "end": "587240"
  },
  {
    "text": "basically test pipelines in depth before we actually push them to production and the test environment should basically mimic prod as close as possible and then",
    "start": "587240",
    "end": "595730"
  },
  {
    "text": "dynamically update is something let's say we changed some source system data model we don't have to like redeploy the",
    "start": "595730",
    "end": "601790"
  },
  {
    "text": "pipeline again it's like that change should automatically be reflected in the next one of the pipeline and here we",
    "start": "601790",
    "end": "608840"
  },
  {
    "text": "assume that we are testing our pipelines carefully so that when you push in prod it's like hopefully like guaranteed to",
    "start": "608840",
    "end": "614750"
  },
  {
    "text": "work and then they deduct is open source you can definitely find it at like",
    "start": "614750",
    "end": "619820"
  },
  {
    "text": "Coursera's github account and then the documentation is hosted underneath the docs i'll put these slides up later and you",
    "start": "619820",
    "end": "625730"
  },
  {
    "text": "can definitely check the link out from SlideShare when these slides are uploaded the installation is very straightforward the first thing you only",
    "start": "625730",
    "end": "632450"
  },
  {
    "text": "need to do pay per install data duct and their example pipelines to get you up to speed so now let's try and build some",
    "start": "632450",
    "end": "638240"
  },
  {
    "text": "pipelines to get a feel for what like what writing pipelines with data can look like so the first pipeline we",
    "start": "638240",
    "end": "644680"
  },
  {
    "text": "are going to try to write is basically pull data from a relational store and load that into a different into redshift",
    "start": "644680",
    "end": "650350"
  },
  {
    "text": "which is also relational so we're going to extract the like user data from an RDS database and then load that into",
    "start": "650350",
    "end": "656950"
  },
  {
    "text": "like a mirror table in red shift and this data in this pipeline would be",
    "start": "656950",
    "end": "662230"
  },
  {
    "text": "orchestrated by data pipeline and then we basically have an extract step where we load pull data from RDS and then",
    "start": "662230",
    "end": "669010"
  },
  {
    "text": "probably run some transformation if we need to and then load this into redshift so on the left side you see a younger",
    "start": "669010",
    "end": "676150"
  },
  {
    "text": "file which is basically our entire pipeline definition so we say what the we're gonna parse this out one by one so",
    "start": "676150",
    "end": "683050"
  },
  {
    "text": "in case it's not completely visible you can reference this later when the you have these lights and on the right side",
    "start": "683050",
    "end": "688930"
  },
  {
    "text": "you basically see your dag of what the pipeline flow looks like so these are",
    "start": "688930",
    "end": "693940"
  },
  {
    "text": "all these steps that are going to be executed some of these steps are automatically inferred based on the",
    "start": "693940",
    "end": "699250"
  },
  {
    "text": "pipeline definition and I'm going to talk about each of these later so as we",
    "start": "699250",
    "end": "704770"
  },
  {
    "text": "saw the definition of a pipeline is specified as a Hamel file there's a notion of steps which is basically one",
    "start": "704770",
    "end": "711280"
  },
  {
    "text": "action within a single pipeline and then we are going to talk about the stuff like configuration overrides and reuse",
    "start": "711280",
    "end": "717880"
  },
  {
    "text": "of code as we parse this pipeline out so let's talk about steps first",
    "start": "717880",
    "end": "723310"
  },
  {
    "text": "so steps is basically a single step is composed of different data pipeline",
    "start": "723310",
    "end": "729160"
  },
  {
    "text": "activities within data pipeline and activity is a certain action and within data doctor step can be one or more of",
    "start": "729160",
    "end": "736030"
  },
  {
    "text": "these activities so if you have a thing if you have a common pattern of like multiple activities that need to be",
    "start": "736030",
    "end": "741910"
  },
  {
    "text": "changed in a certain fashion you can create a step out and writing a new step",
    "start": "741910",
    "end": "747190"
  },
  {
    "text": "is basically extending the current base step class and then you basically get a new step out so our first step is",
    "start": "747190",
    "end": "753100"
  },
  {
    "text": "basically extract IDs so we want to extract data from an RDS database and you specify a sequel query that you want",
    "start": "753100",
    "end": "760390"
  },
  {
    "text": "to use to extract this data so in this case we will basically get the user ID name email from our users table and you",
    "start": "760390",
    "end": "768269"
  },
  {
    "text": "specify the hostname so the hostname is basically if it specifies what host it needs to go to and figure out like",
    "start": "768269",
    "end": "774619"
  },
  {
    "text": "things like what credentials it needs to use the credentials are stored as a single config and then database is",
    "start": "774619",
    "end": "781049"
  },
  {
    "text": "basically within that host what database it needs to use if you specified your query ID like user DV dot users you",
    "start": "781049",
    "end": "786839"
  },
  {
    "text": "could have just not specified the database name so this step would",
    "start": "786839",
    "end": "791849"
  },
  {
    "text": "basically pull data from RDS and load it into s3 so the next thing that we want",
    "start": "791849",
    "end": "801929"
  },
  {
    "text": "to do is now we have all this data that is loaded extracted from an RDS database and we want to load that into redshift",
    "start": "801929",
    "end": "807419"
  },
  {
    "text": "so if you know about the copy command in redshift it doesn't directly create tables so you need your table to exist",
    "start": "807419",
    "end": "814259"
  },
  {
    "text": "before you can actually start loading data so create load redshift is basically just like a generic if statement so it basically checks if the",
    "start": "814259",
    "end": "821069"
  },
  {
    "text": "table is existed or not if it doesn't exist will basically create the table and then grant permissions based on like",
    "start": "821069",
    "end": "830789"
  },
  {
    "text": "how you have defined your user groups so if you have a developer group it basically grant more insert statements",
    "start": "830789",
    "end": "836489"
  },
  {
    "text": "and the insert permission on that if you only have a consumer group then ill grant like a select access and then all",
    "start": "836489",
    "end": "843059"
  },
  {
    "text": "you need to specify is that this is like the staging redshift table that we want to load data in and everything else is",
    "start": "843059",
    "end": "848819"
  },
  {
    "text": "like automatically inferred then the third step is basically now we have data in the staging table in redshift and we",
    "start": "848819",
    "end": "855419"
  },
  {
    "text": "want to write this to another schema which is basically our production schema where absurd is basically you update any",
    "start": "855419",
    "end": "862589"
  },
  {
    "text": "entries that need to be reflected based on the primary key and then you insert the remaining statements so the",
    "start": "862589",
    "end": "870059"
  },
  {
    "text": "unloading in this step we are basically loading data from a staging table into a production table where the self says",
    "start": "870059",
    "end": "875699"
  },
  {
    "text": "like if you have data loaded ETL where you only load like part of some time window of data instead of loading the",
    "start": "875699",
    "end": "881669"
  },
  {
    "text": "full table you basically keep your production table as it is load data into a staging table and then copy and copy",
    "start": "881669",
    "end": "889289"
  },
  {
    "text": "and insert the updated entry into the production table so these were the three steps that we specified in the",
    "start": "889289",
    "end": "896040"
  },
  {
    "text": "UML file directly but there were more things that happened in the background when we executed the pipeline sorry",
    "start": "896040",
    "end": "901800"
  },
  {
    "text": "about that so the first thing that happened was bootstrap so bootstrap is",
    "start": "901800",
    "end": "907920"
  },
  {
    "text": "basically one activity that would run based on what resource you are using so if you were using an ec2 resource we",
    "start": "907920",
    "end": "914040"
  },
  {
    "text": "would run basically a bootstrap or an ec2 resource and if you were running an EMR job you would basically bootstrap the",
    "start": "914040",
    "end": "920459"
  },
  {
    "text": "EMR cluster so what happens in the bootstrap step is like this is out of my automated so every time you specify a",
    "start": "920459",
    "end": "926130"
  },
  {
    "text": "pipeline that automatically gets the bootstrap step where it'll basically fetch the latest binaries of the code",
    "start": "926130",
    "end": "931829"
  },
  {
    "text": "that needs to be executed for that pipeline from s3 so you every time you change the code in the library you",
    "start": "931829",
    "end": "938010"
  },
  {
    "text": "basically push an updated version to s3 and whenever the pipeline runs next is basically pull the latest version so",
    "start": "938010",
    "end": "944130"
  },
  {
    "text": "which basically enables the dynamic update of pipelines that we talked about and it will if you have like",
    "start": "944130",
    "end": "952019"
  },
  {
    "text": "dependencies say you were using some version of boto and you updated that version so instead",
    "start": "952019",
    "end": "957750"
  },
  {
    "text": "of you having to update the ami and going across every pipeline you can basically make a bootstrap step update",
    "start": "957750",
    "end": "963720"
  },
  {
    "text": "your dependencies this will take some time - like if you were installing like soup libraries that were much more",
    "start": "963720",
    "end": "968910"
  },
  {
    "text": "complicated that time you can make Babli once they take some amount of time you",
    "start": "968910",
    "end": "974279"
  },
  {
    "text": "can go back and update your ami but you don't need to do it every time you man like install and your dependency and",
    "start": "974279",
    "end": "981199"
  },
  {
    "text": "then it basically makes sure that all your pipelines are running the latest version of code so you don't run into a",
    "start": "981199",
    "end": "987390"
  },
  {
    "text": "stage where some of your pipelines are running like the old code because you forgot to redeploy them and then some",
    "start": "987390",
    "end": "993420"
  },
  {
    "text": "pipelines are running new code so you only see errors in like half the pipeline's and it's hard to debug the",
    "start": "993420",
    "end": "1000079"
  },
  {
    "text": "other thing that happened was when we wrote the absurd step because the",
    "start": "1000079",
    "end": "1005149"
  },
  {
    "text": "pipeline definition specifies that data is being pulled from an RDS database we",
    "start": "1005149",
    "end": "1010250"
  },
  {
    "text": "can probably build some amount of sanity checks around our ETL so the first thing is HF does not",
    "start": "1010250",
    "end": "1017000"
  },
  {
    "text": "enforce primary keys but the if you specify primary Keys there",
    "start": "1017000",
    "end": "1022730"
  },
  {
    "text": "houston like query optimization so you have to manually enforce that your data has primary there has no primary key",
    "start": "1022730",
    "end": "1029000"
  },
  {
    "text": "violations so whenever you do an absurd is basically checks that your production table didn't have a primary key",
    "start": "1029000",
    "end": "1034220"
  },
  {
    "text": "violation anymore if it has it can fire off an SMS alert based on how you configure it and then you can count the",
    "start": "1034220",
    "end": "1041270"
  },
  {
    "text": "number of rows that you see in your staging table with the number of rows you expected in your source system to",
    "start": "1041270",
    "end": "1046339"
  },
  {
    "text": "basically do that you didn't drop rows between the ETL and then you can check for corrupted rows by matching data",
    "start": "1046340",
    "end": "1053000"
  },
  {
    "text": "between the source system and your production schema where things get",
    "start": "1053000",
    "end": "1059360"
  },
  {
    "text": "slightly tricky is like if you have multiple transformations so if you had transformations you can specify that you",
    "start": "1059360",
    "end": "1064490"
  },
  {
    "text": "don't want the kind of yeah you can write your own sanity check instead of using like the default sanity check",
    "start": "1064490",
    "end": "1070610"
  },
  {
    "text": "which basically assumes that you're mirroring or an IDs table into your HF and all this is like automatically done",
    "start": "1070610",
    "end": "1077810"
  },
  {
    "text": "when you specify a absurd step the next thing that is automatically done is",
    "start": "1077810",
    "end": "1083300"
  },
  {
    "text": "basically teardown so whenever your pipeline finishes it will basically run a teardown step so if you see a failure",
    "start": "1083300",
    "end": "1090290"
  },
  {
    "text": "on your pipeline you would get an SNS notification saying hey this pipeline filled this was the error message and",
    "start": "1090290",
    "end": "1096740"
  },
  {
    "text": "that would be fired off and similarly it also does logging or bhatia pipelines so",
    "start": "1096740",
    "end": "1101780"
  },
  {
    "text": "so that you can monitor like we talked about monitoring the health of the pipeline so in this case you can monitor",
    "start": "1101780",
    "end": "1107150"
  },
  {
    "text": "how many retries do you have or did like what what what was like health of the",
    "start": "1107150",
    "end": "1112280"
  },
  {
    "text": "pipeline saying how many talked about how much time did your query take or how much time did each of these steps take",
    "start": "1112280",
    "end": "1117920"
  },
  {
    "text": "so you can monitor all of this and basically lock it into like an RDS database or back into redshift itself",
    "start": "1117920",
    "end": "1123650"
  },
  {
    "text": "and then use that to basically build dashboards about your health of your pipeline so that you can catch alert",
    "start": "1123650",
    "end": "1129230"
  },
  {
    "text": "before like things actually fail and then we talked about like visualization",
    "start": "1129230",
    "end": "1136340"
  },
  {
    "text": "of the pipeline so we saw this diagram earlier this is automatically generated when you specify the amyl file so if you",
    "start": "1136340",
    "end": "1142130"
  },
  {
    "text": "execute it you can basically generate the same diagram and this helps in like debugging data flows and like",
    "start": "1142130",
    "end": "1148190"
  },
  {
    "text": "your dependencies are specified between steps and basically the blue boxes here",
    "start": "1148190",
    "end": "1153860"
  },
  {
    "text": "are like actual actions and then the gray boxes specify like data nodes which are like intermediate data that is",
    "start": "1153860",
    "end": "1161419"
  },
  {
    "text": "stored in different places in s3 then you have like a single config so if you",
    "start": "1161419",
    "end": "1167450"
  },
  {
    "text": "saw that in our pipeline definition we didn't specify what ami we wanted to use or what security groups did we want to",
    "start": "1167450",
    "end": "1173509"
  },
  {
    "text": "run this pipeline in so all this is like stored in a single config that you can override all of these in a pipeline we",
    "start": "1173509",
    "end": "1179840"
  },
  {
    "text": "will see that in the next example we talked about but basically by default we would use this ami what security group a",
    "start": "1179840",
    "end": "1186950"
  },
  {
    "text": "we want to run in how many retries do we want what treat idle a do we want all",
    "start": "1186950",
    "end": "1191990"
  },
  {
    "text": "this is like abstracted away from not every developer having to specify these config for emitters then the other thing",
    "start": "1191990",
    "end": "1200600"
  },
  {
    "text": "is we talked about extensibility so custom steps is basically an action",
    "start": "1200600",
    "end": "1206149"
  },
  {
    "text": "where you can either extend upon a currently existing step within the library or you can build your own steps",
    "start": "1206149",
    "end": "1212149"
  },
  {
    "text": "when we write our next ETL which basically pulls data from Cassandra we'll see how we basically extend upon",
    "start": "1212149",
    "end": "1218179"
  },
  {
    "text": "the our existing steps to build new steps out of it and to deploy this pipeline is very straightforward",
    "start": "1218179",
    "end": "1224690"
  },
  {
    "text": "all you have to run is basically data duct pipeline activate and then you basically give the path to the ml file",
    "start": "1224690",
    "end": "1230450"
  },
  {
    "text": "and will automatically sheduled that pipeline there are other options like",
    "start": "1230450",
    "end": "1235460"
  },
  {
    "text": "delaying the pipeline for like to be executed start executing in the future or you want to go back in time and say I",
    "start": "1235460",
    "end": "1241340"
  },
  {
    "text": "want to start this pipeline like 10 days ago so all that is like specified as a config parameter so basically your",
    "start": "1241340",
    "end": "1248539"
  },
  {
    "text": "developers have like the DEM config specified and then they can just activate the pipeline using a",
    "start": "1248539",
    "end": "1253669"
  },
  {
    "text": "command-line interface so let's jump into a more complex pipeline where we",
    "start": "1253669",
    "end": "1259639"
  },
  {
    "text": "have to like actually think about data modeling and then you have certain more constraints compared to like what we saw",
    "start": "1259639",
    "end": "1264679"
  },
  {
    "text": "in RDS so here we are trying to pull data from Cassandra which is a no sequel store and yeah as you we can assume that",
    "start": "1264679",
    "end": "1273980"
  },
  {
    "text": "you're storing like blobs such as a JSON object and then we want to parse this out into like multiple tables so that we can",
    "start": "1273980",
    "end": "1280460"
  },
  {
    "text": "create a relational schema out of this JSON object which can have like lists and things like that and again this",
    "start": "1280460",
    "end": "1286220"
  },
  {
    "text": "pipeline would be orchestrated using data pipelines so what our data flow would look like as we'll pull data from Cassandra will push it to s3 then we'll",
    "start": "1286220",
    "end": "1293810"
  },
  {
    "text": "run multiple transformations using EMR and then once we have like tabular data which may mix what are what individual",
    "start": "1293810",
    "end": "1300740"
  },
  {
    "text": "tables in redshift is going to look like we're gonna load this up into redshift so here we'll talk about basically shell",
    "start": "1300740",
    "end": "1307550"
  },
  {
    "text": "command activity so she'll command activity is a data pipeline action which basically lets you run any command or",
    "start": "1307550",
    "end": "1313280"
  },
  {
    "text": "any shell script so once we can assume that our EMR cluster has our library",
    "start": "1313280",
    "end": "1318530"
  },
  {
    "text": "installed and like a latest version of the jar which specifies the Hadoop job and then we'll use shell command",
    "start": "1318530",
    "end": "1324050"
  },
  {
    "text": "activity to invoke these so some of the constraints that we see with like a no",
    "start": "1324050",
    "end": "1330020"
  },
  {
    "text": "sequence store is one you're not allowed row scans so in our previous ETL we just said select user ID from users table and",
    "start": "1330020",
    "end": "1336530"
  },
  {
    "text": "dump the whole table out that we don't want to affect like production traffic so if the same cluster is running",
    "start": "1336530",
    "end": "1342590"
  },
  {
    "text": "production traffic we don't want ETL to basically spike up our latency and destroy the user experience and so these are like one of",
    "start": "1342590",
    "end": "1351020"
  },
  {
    "text": "few of the challenges that you can actually face when it's like trying to ETL so here what we do is basically",
    "start": "1351020",
    "end": "1356390"
  },
  {
    "text": "Netflix as an open source project called priam which basically lets you take a backup or a snapshot of your Cassandra",
    "start": "1356390",
    "end": "1362930"
  },
  {
    "text": "cluster so we would take a backup of a concurrent Cassandra cluster and load",
    "start": "1362930",
    "end": "1367970"
  },
  {
    "text": "this data into s3 and then we are going to use the backups to actually run the ETL instead of actually touching the",
    "start": "1367970",
    "end": "1374180"
  },
  {
    "text": "broad cluster and then now this backup is stored in a Cassandra specific format",
    "start": "1374180",
    "end": "1380750"
  },
  {
    "text": "which is called SS tables so you instead of every us having to write our own ETL",
    "start": "1380750",
    "end": "1387530"
  },
  {
    "text": "for parsing this SS table format out there's another open source project which we forced for called a guess test",
    "start": "1387530",
    "end": "1394100"
  },
  {
    "text": "this was also by Netflix where our version of the four basically takes in a",
    "start": "1394100",
    "end": "1399290"
  },
  {
    "text": "Cassandra schema so we specify what schema do we want to use parse and then we would read the SS tables out and",
    "start": "1399290",
    "end": "1406040"
  },
  {
    "text": "don't load Avro dumps so we basically take every row in cassandra and create a corresponding Avro row so",
    "start": "1406040",
    "end": "1413070"
  },
  {
    "text": "that we can work with this more easily instead of us having to learn about the SS table format and aggregating",
    "start": "1413070",
    "end": "1418440"
  },
  {
    "text": "different nodes because they have different versions of the data then we're going to use carding as we talked",
    "start": "1418440",
    "end": "1424860"
  },
  {
    "text": "about to parse this Avro files into multiple tables that each look like one",
    "start": "1424860",
    "end": "1430200"
  },
  {
    "text": "of the tables in redshift so here we are going to like basically build upon the",
    "start": "1430200",
    "end": "1435600"
  },
  {
    "text": "base steps that we specified earlier in our pipeline and then you convert them into like Augustus and scalding steps so",
    "start": "1435600",
    "end": "1443820"
  },
  {
    "text": "what the pipeline definition here for this pipeline would look like is basically we are pulling data from this words KVS which basically has entries",
    "start": "1443820",
    "end": "1451860"
  },
  {
    "text": "for all foreign words so every time someone votes would upwards in question",
    "start": "1451860",
    "end": "1457050"
  },
  {
    "text": "or an upwards an answer or downward said we're basically we create an entry in this table and then we're going to parse",
    "start": "1457050",
    "end": "1463200"
  },
  {
    "text": "this scheme out so the begginers will talk about this whole pipeline so steps again we will",
    "start": "1463200",
    "end": "1471180"
  },
  {
    "text": "talk about the two steps here so we had a guess this step and a discarding step so all your developer has to specify for",
    "start": "1471180",
    "end": "1477870"
  },
  {
    "text": "any guess test step is that he wants to run against this and then he wants this Cassandra schema to be parsed out and",
    "start": "1477870",
    "end": "1483750"
  },
  {
    "text": "internally the step definition basically specified where it needs to go for the backups or date of backups it needs to",
    "start": "1483750",
    "end": "1490440"
  },
  {
    "text": "use based on like time of running the pipeline and then he'll basically create a profile and these are basically are",
    "start": "1490440",
    "end": "1496320"
  },
  {
    "text": "put it into an intermediate s3 location which our next step is going to pick up so then we have this carding step here",
    "start": "1496320",
    "end": "1504030"
  },
  {
    "text": "it's going to use the data from the previous step and then run a here my job so this we had a single char and then",
    "start": "1504030",
    "end": "1511530"
  },
  {
    "text": "the Emer job is basically it's specifying a function which basically transformed the JSON object that was",
    "start": "1511530",
    "end": "1517650"
  },
  {
    "text": "loaded in cassandra into like multiple relational entities so here you're gonna just map over the JSON object and create",
    "start": "1517650",
    "end": "1524580"
  },
  {
    "text": "like different outputs into like different flat files which each correspond to one of the tables in their",
    "start": "1524580",
    "end": "1529890"
  },
  {
    "text": "chip so here we're going to output different nodes so when we want layout put a different questions table and",
    "start": "1529890",
    "end": "1535290"
  },
  {
    "text": "we're going to put a different answers table now if you looked at the pipeline",
    "start": "1535290",
    "end": "1540660"
  },
  {
    "text": "carefully you instead of as having two spaces like if you want to override only certain portions of the",
    "start": "1540660",
    "end": "1546630"
  },
  {
    "text": "config for this pipeline so we want to change the number of instances that are used by this cluster so instead of every",
    "start": "1546630",
    "end": "1553470"
  },
  {
    "text": "developer having to number number of tasks Norden to want to use number of like what size of the master node is in",
    "start": "1553470",
    "end": "1559170"
  },
  {
    "text": "the cluster we only want to override certain fields so here we are only overriding number of instances that we",
    "start": "1559170",
    "end": "1564990"
  },
  {
    "text": "want number of tasks instances that we want in our cluster and the tasks and sin size and everything else basically",
    "start": "1564990",
    "end": "1570900"
  },
  {
    "text": "falls back to the defaults which is like iam our roles that we want to use or SSH",
    "start": "1570900",
    "end": "1576150"
  },
  {
    "text": "keys that you want to specify on that cluster the next thing we want to talk about is like multiple output nodes so a",
    "start": "1576150",
    "end": "1582780"
  },
  {
    "text": "single step can basically create multiple outputs that each flow in two",
    "start": "1582780",
    "end": "1588090"
  },
  {
    "text": "different following steps so this scalding step is basically creating two outputs here one is the",
    "start": "1588090",
    "end": "1593160"
  },
  {
    "text": "question stable one is the answer stable and then we would say use the same old create loader a chip step it basically",
    "start": "1593160",
    "end": "1599820"
  },
  {
    "text": "takes in data from these questions output node and loads that into like the questions table and redshift so with",
    "start": "1599820",
    "end": "1606000"
  },
  {
    "text": "your gonna use existing steps that we also had already built and now we have like these two new custom steps which",
    "start": "1606000",
    "end": "1611520"
  },
  {
    "text": "are like scalding and I guess this to build our pipeline in like a more modular fashion we talked about",
    "start": "1611520",
    "end": "1618870"
  },
  {
    "text": "bootstrap so the same thing would happen again so this saves like every pipeline definition to having to write this like",
    "start": "1618870",
    "end": "1624780"
  },
  {
    "text": "additional step because that is automatically inferred and in case of EMR what would happen is basically in to",
    "start": "1624780",
    "end": "1631680"
  },
  {
    "text": "fetch the latest gr that we build for our Hadoop jobs and then one use case",
    "start": "1631680",
    "end": "1637230"
  },
  {
    "text": "before we moved to redshift was we also had a hive warehouse so in this case you",
    "start": "1637230",
    "end": "1643740"
  },
  {
    "text": "would want like if you have multiple clusters you want to talk them like make them talk to the same meta store so your",
    "start": "1643740",
    "end": "1650040"
  },
  {
    "text": "bootstrap step can basically configure your cluster to talk to an external meta store and that way not every cluster",
    "start": "1650040",
    "end": "1656430"
  },
  {
    "text": "needs to specify like hive external tables that this has three location corresponds to this hive table so these",
    "start": "1656430",
    "end": "1663960"
  },
  {
    "text": "are some of the use cases of like how you could build like pipelines that bring data into redshift so yeah so",
    "start": "1663960",
    "end": "1672810"
  },
  {
    "text": "we've talked about like bringing all this data into red chips so similarly we could the scalding step that put our parses",
    "start": "1672810",
    "end": "1678179"
  },
  {
    "text": "clickstream data and you could write other like basically poll Facebook for getting data and now all of your data is",
    "start": "1678179",
    "end": "1685950"
  },
  {
    "text": "in the warehouse so how can you use your existing ETL framework to build more",
    "start": "1685950",
    "end": "1691080"
  },
  {
    "text": "features that go beyond just pulling data into the warehouse is what we are going to talk about next so here we are",
    "start": "1691080",
    "end": "1697320"
  },
  {
    "text": "trying to build some data products such as recommendations or dashboards out of our data within redshift so what a",
    "start": "1697320",
    "end": "1704760"
  },
  {
    "text": "common pattern pipeline would look like is basically we wait for dependent pipelines so in case of recommendations",
    "start": "1704760",
    "end": "1710880"
  },
  {
    "text": "you wait for user activity to be loaded completely then we would run some computation inside redshift if it's",
    "start": "1710880",
    "end": "1716760"
  },
  {
    "text": "possible so if we can like do all our transformations with sequel will basically run these transformations and",
    "start": "1716760",
    "end": "1722429"
  },
  {
    "text": "create derive tables within redshift itself and then you can optionally use",
    "start": "1722429",
    "end": "1727530"
  },
  {
    "text": "EMR or ec2 to process the data further and build like dashboards or recommendation models and then as we",
    "start": "1727530",
    "end": "1734880"
  },
  {
    "text": "talked about earlier you can load back into like my sequel or Cassandra based on latency requirements and then product",
    "start": "1734880",
    "end": "1740640"
  },
  {
    "text": "features get built on top of these data source data sources so to walk you",
    "start": "1740640",
    "end": "1746250"
  },
  {
    "text": "through an example let's talk about recommendations so recommendations so",
    "start": "1746250",
    "end": "1754440"
  },
  {
    "text": "here we want to like bring the objective is basically taking a learner and bringing them to the right content so",
    "start": "1754440",
    "end": "1761190"
  },
  {
    "text": "every time a learner comes to our website we want to recommend them relevant courses based on their past",
    "start": "1761190",
    "end": "1766410"
  },
  {
    "text": "behavior they're based on their past course courses they have taken enrolled in or courses they have completed or",
    "start": "1766410",
    "end": "1772799"
  },
  {
    "text": "course pages they're visited to show get a sense of like interest based on their previous search queries the products we",
    "start": "1772799",
    "end": "1780870"
  },
  {
    "text": "can build out of it is basically recommendations email so you can send a weekly email saying hey these are few",
    "start": "1780870",
    "end": "1787080"
  },
  {
    "text": "courses that we think you might be interested in and then you can have a course discovery experience where every",
    "start": "1787080",
    "end": "1792870"
  },
  {
    "text": "time someone searches on your platform you basically personalize that search based on their past activity and then",
    "start": "1792870",
    "end": "1798960"
  },
  {
    "text": "you can use this tuple like reactivate old users who haven't been on your platform by sending the information about your current courses",
    "start": "1798960",
    "end": "1806030"
  },
  {
    "text": "that might have updated or changed so how about pipeline looks like currently",
    "start": "1806030",
    "end": "1811040"
  },
  {
    "text": "is basically we let's assume we are building a simple recommendations model",
    "start": "1811040",
    "end": "1817550"
  },
  {
    "text": "based on enrollments so we have enrollment data of per course per user entry in our redshift database and then",
    "start": "1817550",
    "end": "1825770"
  },
  {
    "text": "we run some computation to calculate like a co enrollment matrix which is basically an in specification of",
    "start": "1825770",
    "end": "1831860"
  },
  {
    "text": "interest of people who are interested in course a how likely are they to be interested in course P we run an EMR job",
    "start": "1831860",
    "end": "1839840"
  },
  {
    "text": "which you can either specify if you were like using spark you can use ml lab we",
    "start": "1839840",
    "end": "1845300"
  },
  {
    "text": "loaded ants carding if you can there are other machine learning options that you could use to train your model then you",
    "start": "1845300",
    "end": "1852110"
  },
  {
    "text": "basically push this model file into s3 so now and then the prediction API layer",
    "start": "1852110",
    "end": "1858170"
  },
  {
    "text": "which like the backend server can basically continuously monitor this file every time you update the model it",
    "start": "1858170",
    "end": "1863360"
  },
  {
    "text": "basically starts using the newer model to create predictions so the contract",
    "start": "1863360",
    "end": "1868880"
  },
  {
    "text": "between our prediction server and the offline training job is basically specified by this model file till the",
    "start": "1868880",
    "end": "1875300"
  },
  {
    "text": "time it's in the same format so this is like similar to what the contract between the front end and the back end server is so you basically till the time",
    "start": "1875300",
    "end": "1881990"
  },
  {
    "text": "it's in a certain JSON object format the front-end and back-end can talk to each other similarly you can say your",
    "start": "1881990",
    "end": "1887870"
  },
  {
    "text": "training pipeline and your prediction service basically talk via this model layer and now instead of just doing ETL",
    "start": "1887870",
    "end": "1895490"
  },
  {
    "text": "with our job flow manager we basically extended it to do more complex things and so this is one example which we",
    "start": "1895490",
    "end": "1903050"
  },
  {
    "text": "currently use our ETL framework for the other big use case internally we have",
    "start": "1903050",
    "end": "1909070"
  },
  {
    "text": "so when use case could be internal dashboards so you have like key",
    "start": "1913310",
    "end": "1918440"
  },
  {
    "text": "performance indicators about your different teams in the company you have like different goals different metrics",
    "start": "1918440",
    "end": "1923960"
  },
  {
    "text": "that each of the teams are amazing on you have company level goals such as revenue or user growth that you want to",
    "start": "1923960",
    "end": "1931100"
  },
  {
    "text": "monitor and make them accessible to everyone in the company so instead of everybody writing sequel on your data",
    "start": "1931100",
    "end": "1937040"
  },
  {
    "text": "warehouse you can build dashboards on top of it when you can build dashboards",
    "start": "1937040",
    "end": "1942440"
  },
  {
    "text": "directly using like a third-party tool like looker but if you have like more complex transformations that cannot",
    "start": "1942440",
    "end": "1947780"
  },
  {
    "text": "directly be specified using a sequel query you want to like basically do more transformations to calculate these",
    "start": "1947780",
    "end": "1954110"
  },
  {
    "text": "metrics load them back into like my sequel and then like basically you have an API layer on this my sequel database",
    "start": "1954110",
    "end": "1960370"
  },
  {
    "text": "so our objective here for building dashboards would be we want to make data",
    "start": "1960370",
    "end": "1966170"
  },
  {
    "text": "more accessible use cases can be results from an a/b test experiments that depend",
    "start": "1966170",
    "end": "1973220"
  },
  {
    "text": "on like clickstream data other metrics that come from different data sources all of them being computed and loaded",
    "start": "1973220",
    "end": "1979520"
  },
  {
    "text": "into my sequel and then we basically build like a thin API layer which",
    "start": "1979520",
    "end": "1984860"
  },
  {
    "text": "basically fetches the data in the Ender set so this dashboard basically shows number of page views about that",
    "start": "1984860",
    "end": "1990500"
  },
  {
    "text": "dashboard itself and we basically render this these words like some of the",
    "start": "1990500",
    "end": "1997370"
  },
  {
    "text": "example cases so we talked about like two ETLs that we pull data into redshift and then some examples of how you can",
    "start": "1997370",
    "end": "2003760"
  },
  {
    "text": "use this data so now I want to leave you with some of the learnings that we have",
    "start": "2003760",
    "end": "2009070"
  },
  {
    "text": "derived in running and managing all these ETL pipelines so first thing is",
    "start": "2009070",
    "end": "2014440"
  },
  {
    "text": "you should always focus on monitoring things that are not not just failures about your pipeline or failures between",
    "start": "2014440",
    "end": "2021010"
  },
  {
    "text": "different actions there are other things that you can monitor such as runtime of your pipeline number of retries that you",
    "start": "2021010",
    "end": "2027310"
  },
  {
    "text": "run into number of times they like this pipeline has been deployed so in case you have like issues the failures and",
    "start": "2027310",
    "end": "2033610"
  },
  {
    "text": "then like query time so like how long does it take for you to like load data back into that shift and if you see like",
    "start": "2033610",
    "end": "2039850"
  },
  {
    "text": "a to exit like like if your time doubles in like a single day you want to probably notice",
    "start": "2039850",
    "end": "2046410"
  },
  {
    "text": "those cases as well originally when we started writing ETL a lot of our ETL",
    "start": "2046410",
    "end": "2052500"
  },
  {
    "text": "code used to live in scripts and moving that and then you would have like the copy of the same script running across",
    "start": "2052500",
    "end": "2058830"
  },
  {
    "text": "different pipelines which made it very hard to update because every time you have to update a single script you would",
    "start": "2058830",
    "end": "2063990"
  },
  {
    "text": "have to redeploy all of these like five pipelines so moving code back into libraries instead of script and keeping",
    "start": "2063990",
    "end": "2070830"
  },
  {
    "text": "the script as thin as possible so that you can like dynamically update your pipelines without having to redeploy each of them so only time we try to",
    "start": "2070830",
    "end": "2077940"
  },
  {
    "text": "deploy a pipeline is when you like the flow of the data changes and not just like some logic within a single step",
    "start": "2077940",
    "end": "2083700"
  },
  {
    "text": "changing then you should have your test environment be as close to prod as",
    "start": "2083700",
    "end": "2088980"
  },
  {
    "text": "possible so in our case we run like a smaller hf cluster which basically is only used for testing pipelines so we",
    "start": "2088980",
    "end": "2096210"
  },
  {
    "text": "use the same backups or each replicas when you run a pipeline and test and just change the s3 path of what like",
    "start": "2096210",
    "end": "2101730"
  },
  {
    "text": "intermediate nodes to store data in and then configs and credentials are updated",
    "start": "2101730",
    "end": "2106800"
  },
  {
    "text": "based on what can figure run the pipeline with and then it gets loaded into a smaller hf cluster and then",
    "start": "2106800",
    "end": "2114350"
  },
  {
    "text": "having like a sing library or framework to write ETL so that you can get every",
    "start": "2114350",
    "end": "2120390"
  },
  {
    "text": "developer to write their own ETL so every time a new product feature is launched we don't have to wait for the",
    "start": "2120390",
    "end": "2125520"
  },
  {
    "text": "ETL to be created like a month like I think the ETL should be as seamless as possible so that we can get data access",
    "start": "2125520",
    "end": "2131910"
  },
  {
    "text": "to this data as fast as possible and then there are things like using backups which you can do for like no sequel",
    "start": "2131910",
    "end": "2138150"
  },
  {
    "text": "stores that make writing ETL much more easily and keeping in mind that you don't want to affect production traffic",
    "start": "2138150",
    "end": "2144480"
  },
  {
    "text": "while you're writing your ETL is definitely a good important thing some of the things that I would avoid would",
    "start": "2144480",
    "end": "2151890"
  },
  {
    "text": "be having large scripts so one script specifying 10 different actions for a single pipeline so every time you have a",
    "start": "2151890",
    "end": "2158820"
  },
  {
    "text": "retry you basically go back to the whole script and do the whole processing these things should be avoided and then non",
    "start": "2158820",
    "end": "2167280"
  },
  {
    "text": "version control pipelines so even if you have a web UI layer having audit logs about every time your pipeline changed",
    "start": "2167280",
    "end": "2173840"
  },
  {
    "text": "you want to notice that it is if someone is on call they can basically look up whatever change within pipelines",
    "start": "2173840",
    "end": "2181259"
  },
  {
    "text": "as fast as possible and then having really huge pipeline so if we could if",
    "start": "2181259",
    "end": "2187049"
  },
  {
    "text": "we had build our recommendations pipeline saying user activity is 1 by half first half of the pipeline then the",
    "start": "2187049",
    "end": "2192630"
  },
  {
    "text": "recommendations model is the second half there would have been much harder to maintain than saying user activity is",
    "start": "2192630",
    "end": "2198059"
  },
  {
    "text": "one pipeline and then recommendations is another pipeline and we just have like dependencies because different pipelines",
    "start": "2198059",
    "end": "2203999"
  },
  {
    "text": "so one cheap way of the bordering dependencies even if you had known have like explicit dependency specified is",
    "start": "2203999",
    "end": "2209999"
  },
  {
    "text": "having like a time buffer between them so you want like a single pipeline to be",
    "start": "2209999",
    "end": "2215939"
  },
  {
    "text": "in charge of like only one thing rather than doing multiple things across different pipe across like different",
    "start": "2215939",
    "end": "2221429"
  },
  {
    "text": "areas of jobs and then there's some things that generally people don't catch",
    "start": "2221429",
    "end": "2226650"
  },
  {
    "text": "which is like your query times increasing or having delays in a single",
    "start": "2226650",
    "end": "2232079"
  },
  {
    "text": "pipeline based on different due to different issues so catching those is also important because that can give me",
    "start": "2232079",
    "end": "2238920"
  },
  {
    "text": "a trend setter for like if your query times are increasing continuously then you want to like do something about it",
    "start": "2238920",
    "end": "2244650"
  },
  {
    "text": "before you actually see a failure so in this presentation we talked about like",
    "start": "2244650",
    "end": "2250469"
  },
  {
    "text": "how we like create a huge reusable extensible experience but data dot B",
    "start": "2250469",
    "end": "2256289"
  },
  {
    "text": "command-line interface and having a good staging environment makes writing ETL much more easily easier and friendly to",
    "start": "2256289",
    "end": "2263249"
  },
  {
    "text": "developers and then how dynamic updates basically save us from like having to deploy all pipelines like multiple times",
    "start": "2263249",
    "end": "2269579"
  },
  {
    "text": "in the day and then it is open source so feel free to contribute using creating",
    "start": "2269579",
    "end": "2276059"
  },
  {
    "text": "issues pull request and then general guidance if you can improve the project the documentation is available and",
    "start": "2276059",
    "end": "2283099"
  },
  {
    "text": "installation is very straightforward there are around 15 examples in the documentation which you can get started",
    "start": "2283099",
    "end": "2289140"
  },
  {
    "text": "to get started with and then feel free to reach out and github for any issues",
    "start": "2289140",
    "end": "2294509"
  },
  {
    "text": "or comments that you might have and now I can take some questions we also have",
    "start": "2294509",
    "end": "2300150"
  },
  {
    "text": "Steve from the data pipeline team who can answer some questions about the product as",
    "start": "2300150",
    "end": "2305959"
  }
]