[
  {
    "text": "- Hi, I'm Nuwan from AWS.",
    "start": "630",
    "end": "2550"
  },
  {
    "text": "- And I'm Ben from AssemblyAI.",
    "start": "2550",
    "end": "4567"
  },
  {
    "text": "\"This is My Architecture.\"",
    "start": "4567",
    "end": "6010"
  },
  {
    "text": "(upbeat music)",
    "start": "6011",
    "end": "8594"
  },
  {
    "text": "- Ben, tell us a little\nbit about AssemblyAI",
    "start": "14820",
    "end": "16830"
  },
  {
    "text": "and type of challenges\nthat you are solving",
    "start": "16830",
    "end": "18750"
  },
  {
    "text": "with this architecture.",
    "start": "18750",
    "end": "19860"
  },
  {
    "text": "- Yeah. At AssemblyAI,",
    "start": "19860",
    "end": "21390"
  },
  {
    "text": "we're laser focused on\ndeveloping ML models",
    "start": "21390",
    "end": "24390"
  },
  {
    "text": "to understand human speech\nwith superhuman ability.",
    "start": "24390",
    "end": "27660"
  },
  {
    "text": "In a nutshell, we have a\ncomplete AI system for customers",
    "start": "27660",
    "end": "31320"
  },
  {
    "text": "to get the most out of their audio data.",
    "start": "31320",
    "end": "33960"
  },
  {
    "text": "The three challenges that\nwe're trying to solve",
    "start": "33960",
    "end": "36300"
  },
  {
    "text": "with this architecture, our scalability,",
    "start": "36300",
    "end": "39239"
  },
  {
    "text": "so handling millions of requests per day,",
    "start": "39240",
    "end": "42000"
  },
  {
    "text": "ease of deployment and iteration,",
    "start": "42000",
    "end": "44370"
  },
  {
    "text": "so developing new models quickly",
    "start": "44370",
    "end": "46350"
  },
  {
    "text": "and updating existing models,",
    "start": "46350",
    "end": "48600"
  },
  {
    "text": "and security and compliance,",
    "start": "48600",
    "end": "50160"
  },
  {
    "text": "making sure we work with customers",
    "start": "50160",
    "end": "52050"
  },
  {
    "text": "to meet their various compliance\nneeds around the globe.",
    "start": "52050",
    "end": "55800"
  },
  {
    "text": "- What kind of users\ninteract with the system",
    "start": "55800",
    "end": "57719"
  },
  {
    "text": "and can you explain\nthe user workflow here?",
    "start": "57720",
    "end": "60870"
  },
  {
    "text": "- Yeah, all sorts of users\ninteract with us via an API,",
    "start": "60870",
    "end": "65869"
  },
  {
    "text": "whether it's podcast companies",
    "start": "65940",
    "end": "67680"
  },
  {
    "text": "looking to understand the\ncontent on their platforms",
    "start": "67680",
    "end": "70500"
  },
  {
    "text": "or telephony companies looking to evaluate",
    "start": "70500",
    "end": "74040"
  },
  {
    "text": "and understand call center performance.",
    "start": "74040",
    "end": "76893"
  },
  {
    "text": "It all starts with audio though.",
    "start": "77760",
    "end": "79350"
  },
  {
    "text": "So customers, AssemblyAI users,",
    "start": "79350",
    "end": "82409"
  },
  {
    "text": "upload their binary audio data to our API",
    "start": "82410",
    "end": "87240"
  },
  {
    "text": "or submit a reference to that data",
    "start": "87240",
    "end": "89790"
  },
  {
    "text": "from something like their own S3 bucket.",
    "start": "89790",
    "end": "91860"
  },
  {
    "text": "- And then the flow continues\nfrom that ECS component",
    "start": "91860",
    "end": "95730"
  },
  {
    "text": "that's the API manager",
    "start": "95730",
    "end": "98760"
  },
  {
    "text": "and then moves to the Amazon ECS here.",
    "start": "98760",
    "end": "101760"
  },
  {
    "text": "Can you explain the flow a bit too?",
    "start": "101760",
    "end": "103980"
  },
  {
    "text": "- Yeah, after we've validated the request",
    "start": "103980",
    "end": "107610"
  },
  {
    "text": "and recorded what type\nof features they want,",
    "start": "107610",
    "end": "111360"
  },
  {
    "text": "we send that over to our orchestrator.",
    "start": "111360",
    "end": "113790"
  },
  {
    "text": "This thing is the brain of the operation",
    "start": "113790",
    "end": "116460"
  },
  {
    "text": "that really decides what models\nto call and in what order.",
    "start": "116460",
    "end": "120540"
  },
  {
    "text": "After we've completed the\nrequest for the customer,",
    "start": "120540",
    "end": "125310"
  },
  {
    "text": "we send a notification over SNS,",
    "start": "125310",
    "end": "128640"
  },
  {
    "text": "which is subscribed to\nby a variety of services,",
    "start": "128640",
    "end": "131760"
  },
  {
    "text": "one of which is this webhook Lambda.",
    "start": "131760",
    "end": "134610"
  },
  {
    "text": "This notifies customers\nin an asynchronous fashion",
    "start": "134610",
    "end": "138480"
  },
  {
    "text": "that their transcription is completed.",
    "start": "138480",
    "end": "140310"
  },
  {
    "text": "- I know there's this\npart of the architecture",
    "start": "140310",
    "end": "141780"
  },
  {
    "text": "that we didn't talk about,",
    "start": "141780",
    "end": "142709"
  },
  {
    "text": "and I assume the orchestrator\nhere that you talked about",
    "start": "142710",
    "end": "147150"
  },
  {
    "text": "delegates that task into\nthe inferencing here.",
    "start": "147150",
    "end": "150000"
  },
  {
    "text": "Talk us a little bit about\nwhat's happening here.",
    "start": "150000",
    "end": "152550"
  },
  {
    "text": "- Yeah, well, the\norchestrator, like I said,",
    "start": "152550",
    "end": "155130"
  },
  {
    "text": "might be the brains of the operation.",
    "start": "155130",
    "end": "156960"
  },
  {
    "text": "All the work gets done over\nhere in this inference pipeline.",
    "start": "156960",
    "end": "161100"
  },
  {
    "text": "We have dozens of models deployed.",
    "start": "161100",
    "end": "163620"
  },
  {
    "text": "We're iterating on them constantly,",
    "start": "163620",
    "end": "165569"
  },
  {
    "text": "so deploying new versions, new models,",
    "start": "165570",
    "end": "168210"
  },
  {
    "text": "really laser focused on finding ways",
    "start": "168210",
    "end": "171510"
  },
  {
    "text": "to deliver that value back to customers.",
    "start": "171510",
    "end": "174360"
  },
  {
    "text": "So this orchestrator\nsends messages into SQS,",
    "start": "174360",
    "end": "179360"
  },
  {
    "text": "which drives our actual\nmodels running on ECS",
    "start": "180060",
    "end": "184903"
  },
  {
    "text": "to scale up and down and to\nperform inference on that data.",
    "start": "185850",
    "end": "189960"
  },
  {
    "text": "- Okay. How do you use\nS3 in this architecture?",
    "start": "189960",
    "end": "192810"
  },
  {
    "text": "- Yeah, so audio data, as you\ncan imagine, is pretty big.",
    "start": "192810",
    "end": "196530"
  },
  {
    "text": "So we actually pass it by reference",
    "start": "196530",
    "end": "199500"
  },
  {
    "text": "from S3 into the ECS container.",
    "start": "199500",
    "end": "203163"
  },
  {
    "text": "After a customer has submitted\ntheir data to our API,",
    "start": "204060",
    "end": "207959"
  },
  {
    "text": "we download it, transcode it,",
    "start": "207960",
    "end": "210120"
  },
  {
    "text": "and then it actually gets\nstored over here in S3.",
    "start": "210120",
    "end": "212366"
  },
  {
    "text": "- S3. I see.",
    "start": "212367",
    "end": "213300"
  },
  {
    "text": "So you explain like you're using SQS,",
    "start": "213300",
    "end": "215670"
  },
  {
    "text": "and, you know, building like a queue",
    "start": "215670",
    "end": "217080"
  },
  {
    "text": "and the jobs are getting submitted.",
    "start": "217080",
    "end": "218550"
  },
  {
    "text": "What kind of speed are we talking about?",
    "start": "218550",
    "end": "220890"
  },
  {
    "text": "- Yeah, so we really try\nto keep the queue empty.",
    "start": "220890",
    "end": "224970"
  },
  {
    "text": "This is an asynchronous system,",
    "start": "224970",
    "end": "226860"
  },
  {
    "text": "so we've created this\ncustom scaling loop here.",
    "start": "226860",
    "end": "230880"
  },
  {
    "text": "It does a couple of things.",
    "start": "230880",
    "end": "232320"
  },
  {
    "text": "It watches the depth of the\nqueue, as well as predicting",
    "start": "232320",
    "end": "236970"
  },
  {
    "text": "when a model is going to get called,",
    "start": "236970",
    "end": "239880"
  },
  {
    "text": "and being able to scale it up in time.",
    "start": "239880",
    "end": "242070"
  },
  {
    "text": "So a great example of that",
    "start": "242070",
    "end": "243600"
  },
  {
    "text": "would be a customer\nrequesting speaker labeling,",
    "start": "243600",
    "end": "247830"
  },
  {
    "text": "so who's talking and when.",
    "start": "247830",
    "end": "250350"
  },
  {
    "text": "We know that that comes after\nconverting audio into text,",
    "start": "250350",
    "end": "254220"
  },
  {
    "text": "so we can prescale that service",
    "start": "254220",
    "end": "256510"
  },
  {
    "text": "such that the capacity is\nthere right when we need it.",
    "start": "257400",
    "end": "260759"
  },
  {
    "text": "- Tell us a little bit\nabout the ML models you use",
    "start": "260760",
    "end": "262590"
  },
  {
    "text": "in this architecture.",
    "start": "262590",
    "end": "263610"
  },
  {
    "text": "- Yeah, so most requests",
    "start": "263610",
    "end": "264990"
  },
  {
    "text": "start with our flagship\nspeech-to-text model.",
    "start": "264990",
    "end": "268110"
  },
  {
    "text": "It's called Conformer-2.",
    "start": "268110",
    "end": "269909"
  },
  {
    "text": "This takes that audio binary\ndata and converts it into text.",
    "start": "269910",
    "end": "274320"
  },
  {
    "text": "From there, we can send it to\na variety of different models",
    "start": "274320",
    "end": "278040"
  },
  {
    "text": "based on the customer's use case,",
    "start": "278040",
    "end": "280470"
  },
  {
    "text": "so speaker labeling, like I mentioned,",
    "start": "280470",
    "end": "282570"
  },
  {
    "text": "sentiment analysis, what's\nthe mood of the conversation,",
    "start": "282570",
    "end": "286170"
  },
  {
    "text": "and even into things like LLMs,",
    "start": "286170",
    "end": "288600"
  },
  {
    "text": "where customers can ask\nquestions about their audio data.",
    "start": "288600",
    "end": "292440"
  },
  {
    "text": "- You briefly touched upon\nthe scalability aspect.",
    "start": "292440",
    "end": "295260"
  },
  {
    "text": "Tell us a little bit about\nwhen those requests comes in.",
    "start": "295260",
    "end": "297960"
  },
  {
    "text": "How do you scale this pipeline over here?",
    "start": "297960",
    "end": "301770"
  },
  {
    "text": "- Yeah, so requests are coming\nin all the time, millions,",
    "start": "301770",
    "end": "305970"
  },
  {
    "text": "millions per day, and we're\nrecording that in CloudWatch.",
    "start": "305970",
    "end": "310320"
  },
  {
    "text": "And we know, based on the decision engine",
    "start": "310320",
    "end": "313230"
  },
  {
    "text": "inside our orchestrator,",
    "start": "313230",
    "end": "314430"
  },
  {
    "text": "which model and in what order\nthey're going to be called.",
    "start": "314430",
    "end": "318389"
  },
  {
    "text": "So using signals like queue\ndepth and our custom scaling,",
    "start": "318390",
    "end": "322740"
  },
  {
    "text": "we can provision more\ncontainers for more models.",
    "start": "322740",
    "end": "326669"
  },
  {
    "text": "So popular models",
    "start": "326670",
    "end": "328050"
  },
  {
    "text": "are going to be scaling\nup and down more quickly",
    "start": "328050",
    "end": "329819"
  },
  {
    "text": "than less popular ones, as an example.",
    "start": "329820",
    "end": "332610"
  },
  {
    "text": "In general, these models\nare pretty expensive,",
    "start": "332610",
    "end": "334800"
  },
  {
    "text": "running on GPUs, so we really\nlike to have good scaling",
    "start": "334800",
    "end": "339449"
  },
  {
    "text": "to control cost.",
    "start": "339450",
    "end": "340620"
  },
  {
    "text": "- Tell us a little bit about security.",
    "start": "340620",
    "end": "342449"
  },
  {
    "text": "How did you handle security\nin this architecture?",
    "start": "342450",
    "end": "344460"
  },
  {
    "text": "- So there's a lot of\nnon-functional requirements,",
    "start": "344460",
    "end": "347310"
  },
  {
    "text": "compliance, things like that,",
    "start": "347310",
    "end": "349440"
  },
  {
    "text": "that we provide for our customers.",
    "start": "349440",
    "end": "351240"
  },
  {
    "text": "We're SOC 2 Type 2\ncertified and really care",
    "start": "351240",
    "end": "354900"
  },
  {
    "text": "about following best\npractices for storing data.",
    "start": "354900",
    "end": "358710"
  },
  {
    "text": "We have strict lifecycle\npolicies over here in Amazon S3,",
    "start": "358710",
    "end": "362699"
  },
  {
    "text": "so we keep the data only\nas long as it's useful",
    "start": "362700",
    "end": "367700"
  },
  {
    "text": "for our orchestrator\nand for our ML pipeline.",
    "start": "367740",
    "end": "370710"
  },
  {
    "text": "- That's awesome. Ben.",
    "start": "370710",
    "end": "371699"
  },
  {
    "text": "Thank you so much for explaining\nthis architecture to us.",
    "start": "371700",
    "end": "374370"
  },
  {
    "text": "I really enjoy the conversation.",
    "start": "374370",
    "end": "375677"
  },
  {
    "text": "(bright music)",
    "start": "375677",
    "end": "378260"
  }
]