[
  {
    "text": "- Hi, folks, my name's Emily Webber.",
    "start": "330",
    "end": "2170"
  },
  {
    "text": "I'm a machine learning\nspecialists solutions architect",
    "start": "2170",
    "end": "4220"
  },
  {
    "text": "at Amazon Web Services",
    "start": "4220",
    "end": "5379"
  },
  {
    "text": "and today, we're gonna\nlearn about Hugging Face",
    "start": "5380",
    "end": "8240"
  },
  {
    "text": "on Amazon SageMaker and on AWS",
    "start": "8240",
    "end": "10309"
  },
  {
    "text": "and in particular, we're\ngonna dive into hosting.",
    "start": "10310",
    "end": "12910"
  },
  {
    "text": "So this is the third video.",
    "start": "12910",
    "end": "14960"
  },
  {
    "text": "Remember we've got a three-video sequence.",
    "start": "14960",
    "end": "16470"
  },
  {
    "text": "That first one was an\nintroduction to Hugging Face",
    "start": "16470",
    "end": "18890"
  },
  {
    "text": "on all of SageMaker.",
    "start": "18890",
    "end": "20800"
  },
  {
    "text": "The second one is a deep dive on training",
    "start": "20800",
    "end": "22800"
  },
  {
    "text": "and now we're gonna deep dive on hosting.",
    "start": "22800",
    "end": "24580"
  },
  {
    "text": "So this is your deep dive.",
    "start": "24580",
    "end": "26473"
  },
  {
    "text": "All right, so the good news, my friends,",
    "start": "31282",
    "end": "34450"
  },
  {
    "text": "is that you can host",
    "start": "34450",
    "end": "35520"
  },
  {
    "text": "any pretrained Hugging\nFace model on SageMaker.",
    "start": "35520",
    "end": "39000"
  },
  {
    "text": "Let me say that again.",
    "start": "39000",
    "end": "40290"
  },
  {
    "text": "You can host any pretrained\nHugging Face model",
    "start": "40290",
    "end": "42700"
  },
  {
    "text": "on SageMaker, super easy.",
    "start": "42700",
    "end": "45380"
  },
  {
    "text": "So for all of those tens of thousands",
    "start": "45380",
    "end": "48630"
  },
  {
    "text": "of very interesting, very complex,",
    "start": "48630",
    "end": "50910"
  },
  {
    "text": "very high performant NLP\ntransformer-based models",
    "start": "50910",
    "end": "55270"
  },
  {
    "text": "that are sitting out there\non the Hugging Face Hub,",
    "start": "55270",
    "end": "58270"
  },
  {
    "text": "you can host any of them on SageMaker",
    "start": "58270",
    "end": "60250"
  },
  {
    "text": "and it is desperately easy to do this.",
    "start": "60250",
    "end": "62500"
  },
  {
    "text": "So the way we're gonna do this\njust as we're seeing here.",
    "start": "62500",
    "end": "65489"
  },
  {
    "text": "Go to Hugging Face models",
    "start": "65490",
    "end": "67060"
  },
  {
    "text": "and then pick your model.",
    "start": "67060",
    "end": "68600"
  },
  {
    "text": "This one is a BERT-based uncased",
    "start": "68600",
    "end": "70513"
  },
  {
    "text": "that I was testing around with",
    "start": "70513",
    "end": "72012"
  },
  {
    "text": "and then we're gonna click on Deploy",
    "start": "72012",
    "end": "73660"
  },
  {
    "text": "and then deploy via SageMaker,",
    "start": "73660",
    "end": "75269"
  },
  {
    "text": "and then we're gonna pick the task.",
    "start": "75269",
    "end": "76520"
  },
  {
    "text": "So whichever task you are most\ninterested in we'll select",
    "start": "76520",
    "end": "81450"
  },
  {
    "text": "and then we'll just pick AWS,",
    "start": "81450",
    "end": "82899"
  },
  {
    "text": "and then just make sure\nyou copy that config code.",
    "start": "83780",
    "end": "87549"
  },
  {
    "text": "Again, it's gonna define your model,",
    "start": "87550",
    "end": "89350"
  },
  {
    "text": "the model ID, the tokenizer, the task,",
    "start": "89350",
    "end": "92500"
  },
  {
    "text": "and then it actually uses",
    "start": "92500",
    "end": "94090"
  },
  {
    "text": "the base Hugging Face inference.py script",
    "start": "94090",
    "end": "97719"
  },
  {
    "text": "so Hugging Face is fully managing",
    "start": "97720",
    "end": "100310"
  },
  {
    "text": "the inferencing in that case.",
    "start": "100310",
    "end": "101820"
  },
  {
    "text": "Again, it's running on SageMaker,",
    "start": "101820",
    "end": "103480"
  },
  {
    "text": "but it's used that inference\nscript from Hugging Face.",
    "start": "103480",
    "end": "106970"
  },
  {
    "text": "You can also deploy any\nof your fine-tuned models",
    "start": "106970",
    "end": "111970"
  },
  {
    "text": "that are sitting in S3.",
    "start": "112760",
    "end": "113780"
  },
  {
    "text": "So remember in that last video,",
    "start": "113780",
    "end": "116070"
  },
  {
    "text": "actually in both the last videos",
    "start": "116070",
    "end": "117300"
  },
  {
    "text": "we were running jobs on SageMaker.",
    "start": "117300",
    "end": "120210"
  },
  {
    "text": "We ran training jobs on SageMaker",
    "start": "120210",
    "end": "122490"
  },
  {
    "text": "where we pointed to those\nbase Hugging Face models",
    "start": "122490",
    "end": "125829"
  },
  {
    "text": "and then we fine-tuned them\nusing our own data sets.",
    "start": "125830",
    "end": "129460"
  },
  {
    "text": "In this case, we're gonna\npoint to that fine-tuned model",
    "start": "129460",
    "end": "133090"
  },
  {
    "text": "that's sitting in S3",
    "start": "133090",
    "end": "133922"
  },
  {
    "text": "and then we're gonna deploy\nthat to a SageMaker endpoint",
    "start": "133923",
    "end": "137930"
  },
  {
    "text": "using this Hugging Face model construct.",
    "start": "137930",
    "end": "139829"
  },
  {
    "text": "So how does this work you ask?",
    "start": "139830",
    "end": "141860"
  },
  {
    "text": "It's pretty easy, so\nsagemaker.huggingface.",
    "start": "141860",
    "end": "145730"
  },
  {
    "text": "Important Hugging Face model, check.",
    "start": "145730",
    "end": "147959"
  },
  {
    "text": "Got our role, check.",
    "start": "147960",
    "end": "149363"
  },
  {
    "text": "You'll see this hub config.",
    "start": "150320",
    "end": "152380"
  },
  {
    "text": "Now if you are using a\nbase pretrained model",
    "start": "152380",
    "end": "157380"
  },
  {
    "text": "straight from the hub,",
    "start": "157500",
    "end": "158720"
  },
  {
    "text": "you wanna pass in the model ID.",
    "start": "158720",
    "end": "161130"
  },
  {
    "text": "You wanna directly pass in a model ID",
    "start": "161130",
    "end": "163290"
  },
  {
    "text": "and you're also gonna pass\nin a Hugging Face task.",
    "start": "163290",
    "end": "167030"
  },
  {
    "text": "So again, that task name",
    "start": "167030",
    "end": "169160"
  },
  {
    "text": "which in this case is text-generation.",
    "start": "169160",
    "end": "171290"
  },
  {
    "text": "However if you are using a\nfine-tuned model from S3,",
    "start": "171290",
    "end": "176290"
  },
  {
    "text": "please don't pass that model ID.",
    "start": "176740",
    "end": "180070"
  },
  {
    "text": "Just don't pass that model ID at all.",
    "start": "180070",
    "end": "182270"
  },
  {
    "text": "Just don't pass it",
    "start": "182270",
    "end": "184690"
  },
  {
    "text": "and it will perform a lot better for you.",
    "start": "184690",
    "end": "187440"
  },
  {
    "text": "It'll be a lot easier.",
    "start": "187440",
    "end": "188273"
  },
  {
    "text": "So just don't pass that model ID",
    "start": "188273",
    "end": "190310"
  },
  {
    "text": "when you have an S3 model data",
    "start": "190310",
    "end": "192950"
  },
  {
    "text": "and the S3 model data comes in down here.",
    "start": "192950",
    "end": "196030"
  },
  {
    "text": "So again, my Hugging Face model",
    "start": "196030",
    "end": "198340"
  },
  {
    "text": "is right here in my\ntransformers_version 4.6.1,",
    "start": "198340",
    "end": "202000"
  },
  {
    "text": "PyTorch and Python versions defined.",
    "start": "202000",
    "end": "204850"
  },
  {
    "text": "And then I'm passing in",
    "start": "204850",
    "end": "206150"
  },
  {
    "text": "that hub configuration\nvia this environment,",
    "start": "206150",
    "end": "209079"
  },
  {
    "text": "along with my SageMaker role",
    "start": "209080",
    "end": "210970"
  },
  {
    "text": "and then I'm defining my\nmodel data right here.",
    "start": "210970",
    "end": "213950"
  },
  {
    "text": "And so that is a pointer to\nmy pretrained model artifact",
    "start": "213950",
    "end": "218950"
  },
  {
    "text": "that again is sitting in my S3 bucket.",
    "start": "220170",
    "end": "222060"
  },
  {
    "text": "So I'm just passing\nthat S3 path right here",
    "start": "222060",
    "end": "226580"
  },
  {
    "text": "and then model.deploy.",
    "start": "226580",
    "end": "228810"
  },
  {
    "text": "So in this case,",
    "start": "228810",
    "end": "229642"
  },
  {
    "text": "obviously I'm setting up this model",
    "start": "229643",
    "end": "232280"
  },
  {
    "text": "using that Hugging Face model object",
    "start": "232280",
    "end": "235310"
  },
  {
    "text": "from the SageMaker SDK right here.",
    "start": "235310",
    "end": "237430"
  },
  {
    "text": "You can also do this directly\nfrom your training job.",
    "start": "237430",
    "end": "240019"
  },
  {
    "text": "So when you run that training job",
    "start": "240020",
    "end": "241930"
  },
  {
    "text": "and you have that estimator object,",
    "start": "241930",
    "end": "244099"
  },
  {
    "text": "as long as it has a finished\ntraining job associated with it",
    "start": "244100",
    "end": "247580"
  },
  {
    "text": "you can just do\nmodel.deploy really easily.",
    "start": "247580",
    "end": "250320"
  },
  {
    "text": "But if you don't have that",
    "start": "250320",
    "end": "251420"
  },
  {
    "text": "and if you're loading from S3 directly",
    "start": "251420",
    "end": "253819"
  },
  {
    "text": "or from the hub directly,",
    "start": "253820",
    "end": "255010"
  },
  {
    "text": "then you can follow this path.",
    "start": "255010",
    "end": "256510"
  },
  {
    "text": "So that is how to fine tune\nyour model in Amazon S3.",
    "start": "257640",
    "end": "261799"
  },
  {
    "text": "You can also use asynchronous\ninference with SageMaker.",
    "start": "261800",
    "end": "265389"
  },
  {
    "text": "So asynchronous inference is a solution",
    "start": "265390",
    "end": "268380"
  },
  {
    "text": "we pushed out last year for larger models",
    "start": "268380",
    "end": "271630"
  },
  {
    "text": "and models that don't need\ninferencing immediately.",
    "start": "272610",
    "end": "276219"
  },
  {
    "text": "So if you don't need within\nmilliseconds response times,",
    "start": "276220",
    "end": "280050"
  },
  {
    "text": "you're comfortable with a couple seconds,",
    "start": "280050",
    "end": "282250"
  },
  {
    "text": "then asynchronous inference\nis a better approach.",
    "start": "282250",
    "end": "285060"
  },
  {
    "text": "Asynchronous inference\nwill support autoscaling",
    "start": "285060",
    "end": "289000"
  },
  {
    "text": "and supports Hugging Face models.",
    "start": "289000",
    "end": "290760"
  },
  {
    "text": "So if you're hosting\nHugging Face NLP models,",
    "start": "290760",
    "end": "294970"
  },
  {
    "text": "then consider asynchronous\ninference as an option.",
    "start": "294970",
    "end": "298150"
  },
  {
    "text": "For the managed queue experience,",
    "start": "298150",
    "end": "299940"
  },
  {
    "text": "basically if you need a queue",
    "start": "299940",
    "end": "301960"
  },
  {
    "text": "sitting in front of\nyour SageMaker endpoint",
    "start": "301960",
    "end": "303740"
  },
  {
    "text": "and you want all of that\nto be wrapped by AWS,",
    "start": "303740",
    "end": "306660"
  },
  {
    "text": "then asynchronous\ninference is the way to go.",
    "start": "306660",
    "end": "309453"
  },
  {
    "text": "But maybe before then,",
    "start": "311150",
    "end": "313550"
  },
  {
    "text": "I wanna think about\npicking the right instance.",
    "start": "313550",
    "end": "316050"
  },
  {
    "text": "Actually picking the right\nML instance type and size.",
    "start": "316050",
    "end": "319599"
  },
  {
    "text": "So this is relevant for every\nphase of machine learning,",
    "start": "319600",
    "end": "323220"
  },
  {
    "text": "but this comes up again\nand again for hosting.",
    "start": "323220",
    "end": "325210"
  },
  {
    "text": "So let's unpack this.",
    "start": "325210",
    "end": "326580"
  },
  {
    "text": "So first off, so this ML here",
    "start": "326580",
    "end": "329610"
  },
  {
    "text": "in that name refers to\nmachine learning, right?",
    "start": "330800",
    "end": "333840"
  },
  {
    "text": "So that means it's a\nSageMaker instance basically.",
    "start": "333840",
    "end": "337070"
  },
  {
    "text": "Then you have the type of instance.",
    "start": "337070",
    "end": "339370"
  },
  {
    "text": "So you'll notice all of\nthe types of EC2 instances",
    "start": "339370",
    "end": "343330"
  },
  {
    "text": "follow the same general format",
    "start": "343330",
    "end": "345310"
  },
  {
    "text": "where you have that type of\nmachine which is the letter.",
    "start": "345310",
    "end": "349350"
  },
  {
    "text": "Then you have the generation",
    "start": "349350",
    "end": "351470"
  },
  {
    "text": "and a higher number means\nit's a more recent generation.",
    "start": "351470",
    "end": "355240"
  },
  {
    "text": "So G4, G5, P3, P4.",
    "start": "355240",
    "end": "359009"
  },
  {
    "text": "Basically again, a higher\nnumber is a more recent machine",
    "start": "359010",
    "end": "363340"
  },
  {
    "text": "and it means you are always\ngonna get better performance",
    "start": "363340",
    "end": "366949"
  },
  {
    "text": "and yeah, better performance\nfor that higher generation.",
    "start": "366950",
    "end": "370623"
  },
  {
    "text": "And then the capabilities.",
    "start": "371580",
    "end": "372610"
  },
  {
    "text": "So if it's really large,",
    "start": "372610",
    "end": "374439"
  },
  {
    "text": "you'll see those letters here,",
    "start": "374440",
    "end": "377150"
  },
  {
    "text": "and then the second dot",
    "start": "377150",
    "end": "378919"
  },
  {
    "text": "and then the size of that machine.",
    "start": "378920",
    "end": "382140"
  },
  {
    "text": "Now different machines are\navailable in different sizes",
    "start": "382140",
    "end": "385560"
  },
  {
    "text": "but you'll see they all follow\nthe same basic constructs.",
    "start": "385560",
    "end": "388200"
  },
  {
    "text": "So .large, .2xlarge, .12, .24,",
    "start": "388200",
    "end": "391347"
  },
  {
    "text": ".9, .18, et cetera, et cetera.",
    "start": "392857",
    "end": "395949"
  },
  {
    "text": "For hosting machine learning models,",
    "start": "395950",
    "end": "398270"
  },
  {
    "text": "there are typically really about four",
    "start": "398270",
    "end": "400889"
  },
  {
    "text": "or five different types of\nmachines that I'm thinking about.",
    "start": "400890",
    "end": "404180"
  },
  {
    "text": "T2 is really only for notebooks.",
    "start": "404180",
    "end": "406630"
  },
  {
    "text": "I'm only gonna use a T2\nor a T3 if I have a model",
    "start": "406630",
    "end": "411480"
  },
  {
    "text": "and I'm maybe just\ndeveloping with my notebook.",
    "start": "411480",
    "end": "414660"
  },
  {
    "text": "It'd be pretty rare for me\nto park an endpoint on a T2.",
    "start": "414660",
    "end": "418570"
  },
  {
    "text": "More commonly, I would look at something",
    "start": "418570",
    "end": "420410"
  },
  {
    "text": "in the M5 or the C5.",
    "start": "420410",
    "end": "422760"
  },
  {
    "text": "The C5 is compute optimized,",
    "start": "422760",
    "end": "424460"
  },
  {
    "text": "so you're just gonna get\nsnappier compute response times.",
    "start": "424460",
    "end": "427569"
  },
  {
    "text": "The M series is sort of more\ngeneral purpose compute.",
    "start": "427570",
    "end": "431223"
  },
  {
    "text": "And so those are two sort\nof middle of the road paths",
    "start": "432440",
    "end": "435780"
  },
  {
    "text": "and then again, they all\ncome in different sizes.",
    "start": "435780",
    "end": "437850"
  },
  {
    "text": "So different numbers of virtual CPUs,",
    "start": "437850",
    "end": "440530"
  },
  {
    "text": "different memory sizes, et cetera.",
    "start": "440530",
    "end": "443370"
  },
  {
    "text": "And then in terms of\naccelerated computing,",
    "start": "443370",
    "end": "446340"
  },
  {
    "text": "you actually have three options.",
    "start": "446340",
    "end": "449010"
  },
  {
    "text": "So one option is the P3 series.",
    "start": "449010",
    "end": "450980"
  },
  {
    "text": "So those are Nvidia, the V100s.",
    "start": "450980",
    "end": "454530"
  },
  {
    "text": "Then you also have Nvidia\nT2s which is the G4.",
    "start": "454530",
    "end": "458690"
  },
  {
    "text": "Actually, yeah, yeah, T2s.",
    "start": "458690",
    "end": "461640"
  },
  {
    "text": "And so both of these types\nof machines are available",
    "start": "461640",
    "end": "466420"
  },
  {
    "text": "and so it's common for customers to pick",
    "start": "466420",
    "end": "468860"
  },
  {
    "text": "between the P3, the G4,",
    "start": "468860",
    "end": "471870"
  },
  {
    "text": "and the Inf1 so that AWS custom hardware",
    "start": "471870",
    "end": "476810"
  },
  {
    "text": "from Annapurna Labs Inferentia\nis available on SageMaker.",
    "start": "476810",
    "end": "480940"
  },
  {
    "text": "And actually we're gonna look at compiling",
    "start": "480940",
    "end": "483720"
  },
  {
    "text": "a Hugging Face model",
    "start": "483720",
    "end": "485250"
  },
  {
    "text": "and then deploying it onto\nInferentia in the demo.",
    "start": "485250",
    "end": "488540"
  },
  {
    "text": "So we need to think about\nour different instance types",
    "start": "488540",
    "end": "491440"
  },
  {
    "text": "and then pick the right one and size.",
    "start": "491440",
    "end": "493403"
  },
  {
    "text": "And if that's too much work for you,",
    "start": "494330",
    "end": "496970"
  },
  {
    "text": "you can also use the\nInference Recommender.",
    "start": "496970",
    "end": "500060"
  },
  {
    "text": "So we launched Inference\nRecommend in SageMaker.",
    "start": "500060",
    "end": "502700"
  },
  {
    "text": "It has two modes.",
    "start": "502700",
    "end": "504500"
  },
  {
    "text": "One mode is to find the\nright instance type for you",
    "start": "504500",
    "end": "507450"
  },
  {
    "text": "where just as it sounds,",
    "start": "507450",
    "end": "509010"
  },
  {
    "text": "we will turn on a variety\nof instances for you.",
    "start": "509010",
    "end": "512469"
  },
  {
    "text": "We'll turn on the C series.",
    "start": "512470",
    "end": "513810"
  },
  {
    "text": "We'll turn on the M series",
    "start": "513810",
    "end": "515690"
  },
  {
    "text": "and definitely let you determine\nwhich one is most optimal.",
    "start": "515690",
    "end": "520690"
  },
  {
    "text": "The second type of Inference Recommender",
    "start": "520730",
    "end": "523870"
  },
  {
    "text": "is actually for the overall endpoint.",
    "start": "523870",
    "end": "526190"
  },
  {
    "text": "So defining the number of\nmachines that you wanna have",
    "start": "526190",
    "end": "529330"
  },
  {
    "text": "so you can get a sense\nof what your throughput",
    "start": "529330",
    "end": "530980"
  },
  {
    "text": "is gonna look like for those machines.",
    "start": "530980",
    "end": "532910"
  },
  {
    "text": "On each of the Inference Recommender jobs,",
    "start": "532910",
    "end": "535660"
  },
  {
    "text": "the results come back into Studio Lab.",
    "start": "535660",
    "end": "537870"
  },
  {
    "text": "I'm sorry, into Studio",
    "start": "537870",
    "end": "539480"
  },
  {
    "text": "and then in Studio you can,",
    "start": "539480",
    "end": "542810"
  },
  {
    "text": "basically we have a little wizard.",
    "start": "542810",
    "end": "544250"
  },
  {
    "text": "You can select if you care about latency,",
    "start": "544250",
    "end": "547650"
  },
  {
    "text": "or if you care about cost, or\nif you care about throughput,",
    "start": "547650",
    "end": "551420"
  },
  {
    "text": "and then we will surface which instance",
    "start": "551420",
    "end": "554880"
  },
  {
    "text": "is best for your preferences.",
    "start": "554880",
    "end": "556690"
  },
  {
    "text": "So that's the Inference Recommender",
    "start": "556690",
    "end": "558760"
  },
  {
    "text": "and it's fully supported\nwith Hugging Face.",
    "start": "558760",
    "end": "560910"
  },
  {
    "text": "And with that, let's\ntake a look at the demo.",
    "start": "561840",
    "end": "564173"
  },
  {
    "text": "All right, so in this demo,",
    "start": "566330",
    "end": "569650"
  },
  {
    "text": "I'm gonna compile my Hugging Face model",
    "start": "569650",
    "end": "572790"
  },
  {
    "text": "for Inferentia using SageMaker Neo.",
    "start": "572790",
    "end": "577000"
  },
  {
    "text": "So remember Neo is the compilation service",
    "start": "577000",
    "end": "580340"
  },
  {
    "text": "within SageMaker that's\nspecific to hosting.",
    "start": "580340",
    "end": "582480"
  },
  {
    "text": "So for supported models in\nNeo and supported operators,",
    "start": "582480",
    "end": "585810"
  },
  {
    "text": "you can take your model",
    "start": "585810",
    "end": "588060"
  },
  {
    "text": "and then basically again, compile it",
    "start": "588060",
    "end": "589779"
  },
  {
    "text": "to just make it smaller\nto run more effectively",
    "start": "589780",
    "end": "593020"
  },
  {
    "text": "on on-premises hardware, on the edge,",
    "start": "593020",
    "end": "596200"
  },
  {
    "text": "in the cloud, and on SageMaker.",
    "start": "596200",
    "end": "598700"
  },
  {
    "text": "So let's see how we're gonna\ndo this with Hugging Face.",
    "start": "598700",
    "end": "602400"
  },
  {
    "text": "So first, we're gonna follow the same flow",
    "start": "602400",
    "end": "604850"
  },
  {
    "text": "where we're gonna install\nagain, that transformers SDK.",
    "start": "604850",
    "end": "608880"
  },
  {
    "text": "We're gonna install\nSageMaker and then PyTorch",
    "start": "608880",
    "end": "611240"
  },
  {
    "text": "and then we're gonna point to a model",
    "start": "613100",
    "end": "614699"
  },
  {
    "text": "from the Hugging Face Hub.",
    "start": "614700",
    "end": "616780"
  },
  {
    "text": "So that same distilbert model",
    "start": "616780",
    "end": "619070"
  },
  {
    "text": "we've been seeing throughout the series.",
    "start": "619070",
    "end": "621290"
  },
  {
    "text": "And so, again, this is the construct",
    "start": "621290",
    "end": "623750"
  },
  {
    "text": "that you should be pretty\nfamiliar with right now.",
    "start": "623750",
    "end": "625720"
  },
  {
    "text": "So transformers, again, that's our SDK.",
    "start": "625720",
    "end": "629116"
  },
  {
    "text": ".autotokenizer.from_pretrained,",
    "start": "629117",
    "end": "632340"
  },
  {
    "text": "and then your model name.",
    "start": "632340",
    "end": "633900"
  },
  {
    "text": "And so here, we're gonna\nload that tokenizer locally",
    "start": "633900",
    "end": "636970"
  },
  {
    "text": "and the model locally.",
    "start": "636970",
    "end": "638509"
  },
  {
    "text": "So both of those we're\ngonna download from the hub.",
    "start": "638510",
    "end": "642180"
  },
  {
    "text": "When you are loading a\nmodel onto Inferentia,",
    "start": "642180",
    "end": "646260"
  },
  {
    "text": "you need to compile it actually",
    "start": "646260",
    "end": "648300"
  },
  {
    "text": "and you have two options\nfor compiling to Inferentia.",
    "start": "648300",
    "end": "652630"
  },
  {
    "text": "Option A is Neo which we're gonna look at",
    "start": "652630",
    "end": "655070"
  },
  {
    "text": "which is that sort of\nmanaged compilation job",
    "start": "655070",
    "end": "658250"
  },
  {
    "text": "that actually gonna show up\nin the SageMaker console.",
    "start": "658250",
    "end": "660720"
  },
  {
    "text": "Option B is using the Neuron SDK.",
    "start": "660720",
    "end": "664180"
  },
  {
    "text": "So if you want to install the SDK",
    "start": "664180",
    "end": "666550"
  },
  {
    "text": "and execute it locally in order\nto get that model compiled,",
    "start": "666550",
    "end": "670480"
  },
  {
    "text": "you are welcome to do that.",
    "start": "670480",
    "end": "672130"
  },
  {
    "text": "In this case, we're gonna use Neo",
    "start": "672130",
    "end": "673620"
  },
  {
    "text": "to execute the compilation.",
    "start": "673620",
    "end": "676190"
  },
  {
    "text": "So both of those as it\nturns out require tracing.",
    "start": "676190",
    "end": "681190"
  },
  {
    "text": "So torch.jit.tracing\nwhere we need to add in",
    "start": "681960",
    "end": "686350"
  },
  {
    "text": "a few more items to run this tracing",
    "start": "686350",
    "end": "690779"
  },
  {
    "text": "and then save the traced model,",
    "start": "690780",
    "end": "693087"
  },
  {
    "text": "and we'll show that right here.",
    "start": "693087",
    "end": "695170"
  },
  {
    "text": "So we're gonna create a directory",
    "start": "695170",
    "end": "696450"
  },
  {
    "text": "that's just called trace\nmodel, nice and simple,",
    "start": "696450",
    "end": "699120"
  },
  {
    "text": "and then we need a few things.",
    "start": "699120",
    "end": "701100"
  },
  {
    "text": "We need a sample sequences.",
    "start": "701100",
    "end": "703399"
  },
  {
    "text": "Actually, JIT tracing likes to see",
    "start": "703400",
    "end": "705910"
  },
  {
    "text": "sample inputs to your model",
    "start": "705910",
    "end": "708000"
  },
  {
    "text": "and then will pass that to the tokenizer",
    "start": "708000",
    "end": "711470"
  },
  {
    "text": "to pass in again, this tokenized pair.",
    "start": "711470",
    "end": "714259"
  },
  {
    "text": "So we have a few different sequences",
    "start": "714260",
    "end": "717480"
  },
  {
    "text": "and actually they're exactly the same",
    "start": "717480",
    "end": "719910"
  },
  {
    "text": "because it's just tracing",
    "start": "719910",
    "end": "721069"
  },
  {
    "text": "and the text input doesn't\nreally impact the tracing",
    "start": "721070",
    "end": "725070"
  },
  {
    "text": "so long as it's sort of\nauthentic to your model.",
    "start": "725070",
    "end": "728290"
  },
  {
    "text": "And so through that tokenizer,",
    "start": "728290",
    "end": "730839"
  },
  {
    "text": "we'll pass in the sequences",
    "start": "730840",
    "end": "733590"
  },
  {
    "text": "and the parameters here,",
    "start": "733590",
    "end": "735720"
  },
  {
    "text": "returning the PyTorch tensors,",
    "start": "735720",
    "end": "738269"
  },
  {
    "text": "and then we'll get this example.",
    "start": "738270",
    "end": "740393"
  },
  {
    "text": "So the input IDs and\nthen the attention mask",
    "start": "741670",
    "end": "744490"
  },
  {
    "text": "and then torch.jit.trace.",
    "start": "744490",
    "end": "747440"
  },
  {
    "text": "We're gonna call model.eval\ndirectly inside of this",
    "start": "747440",
    "end": "750580"
  },
  {
    "text": "with that example text.",
    "start": "750580",
    "end": "752750"
  },
  {
    "text": "So that's loaded into this traced model",
    "start": "752750",
    "end": "755230"
  },
  {
    "text": "and then we'll save that\nlocally in that .pth format.",
    "start": "755230",
    "end": "759233"
  },
  {
    "text": "Once that is saved locally,",
    "start": "760160",
    "end": "762149"
  },
  {
    "text": "we're gonna drop that in S3.",
    "start": "762150",
    "end": "763450"
  },
  {
    "text": "So again, session.upload_data\nwill drop that into S3.",
    "start": "763450",
    "end": "768450"
  },
  {
    "text": "Now let's take a look at\nour inference in code.",
    "start": "768840",
    "end": "773840"
  },
  {
    "text": "So there's really one function",
    "start": "774140",
    "end": "777870"
  },
  {
    "text": "that matters the most when inferencing.",
    "start": "777870",
    "end": "781507"
  },
  {
    "text": "I mean, they're all important.",
    "start": "781507",
    "end": "782340"
  },
  {
    "text": "You've got four functions\nfor your inferencing script.",
    "start": "782340",
    "end": "785590"
  },
  {
    "text": "The model function\nwhich you see right here",
    "start": "785590",
    "end": "788090"
  },
  {
    "text": "picks your model up from disk",
    "start": "788090",
    "end": "789560"
  },
  {
    "text": "for health checks for SageMaker pings.",
    "start": "789560",
    "end": "793350"
  },
  {
    "text": "So when you're calling model.deploy,",
    "start": "793350",
    "end": "795540"
  },
  {
    "text": "you need to make sure",
    "start": "795540",
    "end": "796660"
  },
  {
    "text": "that this model function is operational.",
    "start": "796660",
    "end": "799230"
  },
  {
    "text": "And that's why again, using Studio",
    "start": "799230",
    "end": "800870"
  },
  {
    "text": "locally just to test that function out",
    "start": "800870",
    "end": "803010"
  },
  {
    "text": "is a great first step.",
    "start": "803010",
    "end": "804889"
  },
  {
    "text": "So again, make sure that model\nfunction is working locally",
    "start": "804889",
    "end": "808130"
  },
  {
    "text": "and then drop it into\nyour inference.py script.",
    "start": "808130",
    "end": "810460"
  },
  {
    "text": "If you want to, not strictly required.",
    "start": "810460",
    "end": "812940"
  },
  {
    "text": "But if you want to,",
    "start": "812940",
    "end": "813773"
  },
  {
    "text": "you can again load it into\nthe inference.py script.",
    "start": "813773",
    "end": "817720"
  },
  {
    "text": "So you've got your model function",
    "start": "817720",
    "end": "818930"
  },
  {
    "text": "which is used by SageMaker\nfor health checks.",
    "start": "818930",
    "end": "823300"
  },
  {
    "text": "So make sure that that model\nfunction is operational.",
    "start": "823300",
    "end": "825970"
  },
  {
    "text": "You have an input function",
    "start": "825970",
    "end": "827449"
  },
  {
    "text": "which is accepting the\ninferencing request.",
    "start": "828360",
    "end": "830980"
  },
  {
    "text": "So the input function is looking",
    "start": "830980",
    "end": "833120"
  },
  {
    "text": "at again, the data that's\nhitting your endpoint",
    "start": "833120",
    "end": "836370"
  },
  {
    "text": "and is doing any pre-processing",
    "start": "836370",
    "end": "838440"
  },
  {
    "text": "that you need to right there.",
    "start": "838440",
    "end": "840040"
  },
  {
    "text": "So inputs, model_function, predict.",
    "start": "840040",
    "end": "843500"
  },
  {
    "text": "So the predict function takes the outputs",
    "start": "843500",
    "end": "845870"
  },
  {
    "text": "of both of those two,",
    "start": "845870",
    "end": "847390"
  },
  {
    "text": "your parse data and your model,",
    "start": "847390",
    "end": "849200"
  },
  {
    "text": "and just runs your data through the model.",
    "start": "849200",
    "end": "851740"
  },
  {
    "text": "So predict and then output",
    "start": "851740",
    "end": "853760"
  },
  {
    "text": "which sends it directly\nback to the client.",
    "start": "853760",
    "end": "857840"
  },
  {
    "text": "Again, those functions are optional,",
    "start": "857840",
    "end": "859760"
  },
  {
    "text": "but if you need to\ndefine more specifically",
    "start": "859760",
    "end": "862620"
  },
  {
    "text": "how that operates, then you\ncan bring them yourself.",
    "start": "862620",
    "end": "865920"
  },
  {
    "text": "And so in this case,",
    "start": "865920",
    "end": "866769"
  },
  {
    "text": "we're obviously bringing\nthem for Inferentia.",
    "start": "866770",
    "end": "869320"
  },
  {
    "text": "And so with our\ninferencing.py script defined,",
    "start": "870510",
    "end": "874890"
  },
  {
    "text": "we're gonna deploy this.",
    "start": "874890",
    "end": "875910"
  },
  {
    "text": "Actually in this notebook,",
    "start": "875910",
    "end": "876899"
  },
  {
    "text": "we deploy two machines.",
    "start": "876900",
    "end": "879040"
  },
  {
    "text": "We deploy it onto a\nG4dn.xl which is this one",
    "start": "879040",
    "end": "884040"
  },
  {
    "text": "and then we're gonna compare it",
    "start": "884790",
    "end": "888009"
  },
  {
    "text": "with the same model\ndeployed onto Inferentia,",
    "start": "888010",
    "end": "890370"
  },
  {
    "text": "and then we'll look at the\nchanges in performance.",
    "start": "890370",
    "end": "892720"
  },
  {
    "text": "And so first off, so we've\ngot this deployed onto G4.",
    "start": "892720",
    "end": "896810"
  },
  {
    "text": "So let's check this out.",
    "start": "896810",
    "end": "898520"
  },
  {
    "text": "So again, this one is\nusing that PyTorch model.",
    "start": "898520",
    "end": "901330"
  },
  {
    "text": "You can switch that out",
    "start": "901330",
    "end": "902300"
  },
  {
    "text": "with a Hugging Face model,\ntotally appropriate.",
    "start": "902300",
    "end": "905380"
  },
  {
    "text": "So we'll use the PyTorch model.",
    "start": "905380",
    "end": "907550"
  },
  {
    "text": "We got our predictor object and et cetera.",
    "start": "907550",
    "end": "912550"
  },
  {
    "text": "Great, so that's our\ninference_normal.py scripts",
    "start": "912570",
    "end": "915930"
  },
  {
    "text": "and our source directory,\nand then the model name.",
    "start": "915930",
    "end": "920260"
  },
  {
    "text": "Actually this is just the job name.",
    "start": "920260",
    "end": "922010"
  },
  {
    "text": "All right, so then model.deploy",
    "start": "923560",
    "end": "925800"
  },
  {
    "text": "and so again, we're\ndeploying that out to a G4dn,",
    "start": "925800",
    "end": "930029"
  },
  {
    "text": "and that is one machine.",
    "start": "930030",
    "end": "932060"
  },
  {
    "text": "We'll do just some quick predicting.",
    "start": "932060",
    "end": "935760"
  },
  {
    "text": "So just again, we're gonna\ninvoke that model.predict,",
    "start": "935760",
    "end": "938400"
  },
  {
    "text": "that endpoint.predict\nright here on our payload",
    "start": "938400",
    "end": "941510"
  },
  {
    "text": "which is predicting that\nthis is a paraphrase.",
    "start": "942880",
    "end": "945110"
  },
  {
    "text": "So that's paraphrase detection.",
    "start": "945110",
    "end": "947563"
  },
  {
    "text": "And then we're gonna\ncompile this model using Neo",
    "start": "948530",
    "end": "951660"
  },
  {
    "text": "and deploy onto Inferentia.",
    "start": "951660",
    "end": "954350"
  },
  {
    "text": "So this next one is now using",
    "start": "954350",
    "end": "956149"
  },
  {
    "text": "that Neuron SDK, again via Neo.",
    "start": "956150",
    "end": "958763"
  },
  {
    "text": "bert-sequence-classification.",
    "start": "959620",
    "end": "961910"
  },
  {
    "text": "Still using this PyTorch model constructs",
    "start": "961910",
    "end": "964910"
  },
  {
    "text": "and we're gonna trace this model URL.",
    "start": "964910",
    "end": "968110"
  },
  {
    "text": "Again, that's the same model\nthat we pass for both of them.",
    "start": "968110",
    "end": "971060"
  },
  {
    "text": "So that model data in S3.",
    "start": "972910",
    "end": "974852"
  },
  {
    "text": "It should be the same for both.",
    "start": "975990",
    "end": "976990"
  },
  {
    "text": "Yeah, here we go.",
    "start": "976990",
    "end": "978440"
  },
  {
    "text": "Yep, it's the same in both.",
    "start": "978440",
    "end": "979650"
  },
  {
    "text": "So the same model data.",
    "start": "979650",
    "end": "981173"
  },
  {
    "text": "Great, inference.py.",
    "start": "984360",
    "end": "986970"
  },
  {
    "text": "Except instead of calling\n.predict or .deploy,",
    "start": "986970",
    "end": "991970"
  },
  {
    "text": "we're gonna call .compile.",
    "start": "992210",
    "end": "994260"
  },
  {
    "text": "So now we're gonna take that model",
    "start": "994260",
    "end": "996210"
  },
  {
    "text": "and we're gonna compile it for Inferentia.",
    "start": "996210",
    "end": "999630"
  },
  {
    "text": "So we're gonna compile it for Inf1.",
    "start": "999630",
    "end": "1001873"
  },
  {
    "text": "So again, we're specifying\nyour target instance family",
    "start": "1003140",
    "end": "1006050"
  },
  {
    "text": "which is Inf1, defining the shape,",
    "start": "1006050",
    "end": "1010450"
  },
  {
    "text": "the job name, framework version,",
    "start": "1010450",
    "end": "1013700"
  },
  {
    "text": "and then where we want the artifact",
    "start": "1013700",
    "end": "1015510"
  },
  {
    "text": "to go in S3 on completion.",
    "start": "1015510",
    "end": "1017960"
  },
  {
    "text": "Once this finishes, so again,\nafter that model is compiled,",
    "start": "1017960",
    "end": "1021990"
  },
  {
    "text": "we can deploy it onto Inferentia.",
    "start": "1021990",
    "end": "1024550"
  },
  {
    "text": "So then we'll do our Inf1\ndeploy which is this one.",
    "start": "1024550",
    "end": "1027293"
  },
  {
    "text": "All right, and then we add\nour two objects here again",
    "start": "1029100",
    "end": "1031709"
  },
  {
    "text": "and we'll call that same syntax.",
    "start": "1031710",
    "end": "1035140"
  },
  {
    "text": "So predict or .predict on our payload",
    "start": "1035140",
    "end": "1037970"
  },
  {
    "text": "which also predicts that\nit's a paraphrase, right?",
    "start": "1037970",
    "end": "1040049"
  },
  {
    "text": "'Cause it's the same model.",
    "start": "1040050",
    "end": "1041709"
  },
  {
    "text": "So the question is which one is faster",
    "start": "1041710",
    "end": "1044870"
  },
  {
    "text": "and which one has more\nthroughput and costs less?",
    "start": "1044870",
    "end": "1048750"
  },
  {
    "text": "Which is the important\none at the end of the day.",
    "start": "1048750",
    "end": "1052300"
  },
  {
    "text": "So this is just some benchmarking content",
    "start": "1052300",
    "end": "1055570"
  },
  {
    "text": "and I apologize, this\nis still in dark mode.",
    "start": "1055570",
    "end": "1058419"
  },
  {
    "text": "This is time, and this is the,",
    "start": "1058420",
    "end": "1062710"
  },
  {
    "text": "basically the number of requests\nthat we're hitting through.",
    "start": "1062710",
    "end": "1066980"
  },
  {
    "text": "Yeah, there we go.",
    "start": "1066980",
    "end": "1068419"
  },
  {
    "text": "And so essentially we're seeing\nthat 95% of these requests",
    "start": "1068420",
    "end": "1072650"
  },
  {
    "text": "are less than 91 milliseconds",
    "start": "1072650",
    "end": "1075420"
  },
  {
    "text": "and the rough throughput per seconds,",
    "start": "1075420",
    "end": "1077320"
  },
  {
    "text": "so the number of requests",
    "start": "1077320",
    "end": "1079740"
  },
  {
    "text": "that we can respond to in a\nsecond is 57, 57.12 seconds.",
    "start": "1079740",
    "end": "1084740"
  },
  {
    "text": "So that's the number of requests.",
    "start": "1086510",
    "end": "1087870"
  },
  {
    "text": "And then here's the same one",
    "start": "1087870",
    "end": "1088960"
  },
  {
    "text": "with a little bit of an easier read there.",
    "start": "1088960",
    "end": "1091382"
  },
  {
    "text": "All right, so the latency,",
    "start": "1092730",
    "end": "1095010"
  },
  {
    "text": "pretty concentrated around the\n85 to 90 millisecond range.",
    "start": "1095010",
    "end": "1098573"
  },
  {
    "text": "Throughput is around 60 TPS",
    "start": "1099720",
    "end": "1102690"
  },
  {
    "text": "and that was for that G4.",
    "start": "1102690",
    "end": "1105049"
  },
  {
    "text": "So remember that was on that G4 hardware",
    "start": "1105050",
    "end": "1107160"
  },
  {
    "text": "and now we're gonna look",
    "start": "1107160",
    "end": "1108070"
  },
  {
    "text": "at the same thing but with Inferentia.",
    "start": "1108070",
    "end": "1110509"
  },
  {
    "text": "And so another just local function here",
    "start": "1112220",
    "end": "1116120"
  },
  {
    "text": "to invoke the endpoint",
    "start": "1116120",
    "end": "1117960"
  },
  {
    "text": "and we'll see a big move to the left.",
    "start": "1117960",
    "end": "1120987"
  },
  {
    "text": "And so remember higher\nlatency is for the right.",
    "start": "1120987",
    "end": "1125410"
  },
  {
    "text": "Lower latency is lower left.",
    "start": "1125410",
    "end": "1127520"
  },
  {
    "text": "And so when we're\nbenchmarking for latency,",
    "start": "1127520",
    "end": "1129510"
  },
  {
    "text": "it's better to be to the left.",
    "start": "1129510",
    "end": "1131800"
  },
  {
    "text": "So we see that for Inferentia overall,",
    "start": "1131800",
    "end": "1134900"
  },
  {
    "text": "the time moved down.",
    "start": "1134900",
    "end": "1136580"
  },
  {
    "text": "And let's see, so now 95% of requests",
    "start": "1136580",
    "end": "1140659"
  },
  {
    "text": "are less than 36 milliseconds",
    "start": "1140660",
    "end": "1144170"
  },
  {
    "text": "and the throughput is through the roof.",
    "start": "1144170",
    "end": "1146900"
  },
  {
    "text": "The throughput is 165 requests per second",
    "start": "1146900",
    "end": "1151900"
  },
  {
    "text": "and then here's a bit\nof an easier read there.",
    "start": "1152000",
    "end": "1153790"
  },
  {
    "text": "So again, move to the left.",
    "start": "1153790",
    "end": "1155590"
  },
  {
    "text": "So here essentially we\nsaw that latency dropped",
    "start": "1155590",
    "end": "1158840"
  },
  {
    "text": "to 25 to 30 milliseconds per range",
    "start": "1158840",
    "end": "1161669"
  },
  {
    "text": "which was a 70% latency decrease,",
    "start": "1161670",
    "end": "1165270"
  },
  {
    "text": "while throughput increase to\n220 transactions per second",
    "start": "1165270",
    "end": "1170260"
  },
  {
    "text": "which was about a 400% increase.",
    "start": "1170260",
    "end": "1172670"
  },
  {
    "text": "And so again, all of\nthat was using Inferentia",
    "start": "1172670",
    "end": "1176720"
  },
  {
    "text": "to deploy onto SageMaker hosting.",
    "start": "1176720",
    "end": "1180210"
  },
  {
    "text": "And so I hope you enjoyed the demo.",
    "start": "1180210",
    "end": "1183510"
  },
  {
    "text": "All right, so some pro tips\nto close this out here.",
    "start": "1183510",
    "end": "1185660"
  },
  {
    "text": "So the same pattern holds true.",
    "start": "1185660",
    "end": "1188600"
  },
  {
    "text": "Test locally, then scale.",
    "start": "1188600",
    "end": "1190809"
  },
  {
    "text": "When you have a fine-tuned model,",
    "start": "1190810",
    "end": "1193470"
  },
  {
    "text": "when you have a model\nthat has been trained",
    "start": "1193470",
    "end": "1195409"
  },
  {
    "text": "on the Hugging Face Hub,",
    "start": "1195410",
    "end": "1197380"
  },
  {
    "text": "definitely make sure that\nyou're testing it locally.",
    "start": "1197380",
    "end": "1199980"
  },
  {
    "text": "Please, please, please\ntry it on your notebook.",
    "start": "1199980",
    "end": "1203690"
  },
  {
    "text": "try it on your notebook instance.",
    "start": "1203690",
    "end": "1205009"
  },
  {
    "text": "Try it on your laptop.",
    "start": "1205010",
    "end": "1206350"
  },
  {
    "text": "Make sure that that script operates",
    "start": "1206350",
    "end": "1208870"
  },
  {
    "text": "before you try and turn on\nyour SageMaker resources.",
    "start": "1208870",
    "end": "1211990"
  },
  {
    "text": "So that's pro tip number one.",
    "start": "1211990",
    "end": "1213750"
  },
  {
    "text": "Number two, I actually really prefer",
    "start": "1213750",
    "end": "1217120"
  },
  {
    "text": "just using the boiler-plate code.",
    "start": "1217120",
    "end": "1219110"
  },
  {
    "text": "It is so much more simple to just point",
    "start": "1219110",
    "end": "1221740"
  },
  {
    "text": "to the Hugging Face code\nthat's already out there.",
    "start": "1221740",
    "end": "1225120"
  },
  {
    "text": "All the solutions that they have,",
    "start": "1225120",
    "end": "1226570"
  },
  {
    "text": "all of the models that they have.",
    "start": "1226570",
    "end": "1228120"
  },
  {
    "text": "Again, they have",
    "start": "1228120",
    "end": "1229240"
  },
  {
    "text": "an incredibly active\nopen source communities,",
    "start": "1229240",
    "end": "1232770"
  },
  {
    "text": "so tap into that.",
    "start": "1232770",
    "end": "1234600"
  },
  {
    "text": "Ask questions on their GitHub.",
    "start": "1234600",
    "end": "1236740"
  },
  {
    "text": "Open issues, look at\nissues that they've closed.",
    "start": "1236740",
    "end": "1239940"
  },
  {
    "text": "They have a ton of helpful\ncomments out there,",
    "start": "1239940",
    "end": "1241659"
  },
  {
    "text": "so definitely take a look",
    "start": "1241660",
    "end": "1244320"
  },
  {
    "text": "at their code that's available.",
    "start": "1244320",
    "end": "1248130"
  },
  {
    "text": "And it's available not\njust at the model level,",
    "start": "1248130",
    "end": "1250180"
  },
  {
    "text": "but at the overall use case level.",
    "start": "1250180",
    "end": "1251870"
  },
  {
    "text": "So when you're fine-tuning the model,",
    "start": "1251870",
    "end": "1254070"
  },
  {
    "text": "or when you're solving\nfor question answering,",
    "start": "1254070",
    "end": "1255860"
  },
  {
    "text": "when you're solving for translation,",
    "start": "1255860",
    "end": "1257970"
  },
  {
    "text": "they have these larger\nscripts that you can run",
    "start": "1257970",
    "end": "1259900"
  },
  {
    "text": "and they'll import the tokenizer for you",
    "start": "1259900",
    "end": "1262550"
  },
  {
    "text": "and they'll make sure that\nit's been tokenized or not,",
    "start": "1262550",
    "end": "1264970"
  },
  {
    "text": "or they'll just run a lot\nof the content for you.",
    "start": "1264970",
    "end": "1269730"
  },
  {
    "text": "The one addendum to that I would say",
    "start": "1269730",
    "end": "1273100"
  },
  {
    "text": "that's specific to hosting",
    "start": "1273100",
    "end": "1275370"
  },
  {
    "text": "is when you need to change hyperparameters",
    "start": "1275370",
    "end": "1278330"
  },
  {
    "text": "on the invocation for hosting.",
    "start": "1278330",
    "end": "1279990"
  },
  {
    "text": "So this happens a lot in\ntext-generation especially",
    "start": "1279990",
    "end": "1283350"
  },
  {
    "text": "and text-generation when you're working",
    "start": "1283350",
    "end": "1285160"
  },
  {
    "text": "with the GPT-2 models\nof the world and beyond,",
    "start": "1285160",
    "end": "1289503"
  },
  {
    "text": "the parameters that you pass\nin can actually really define",
    "start": "1290640",
    "end": "1294000"
  },
  {
    "text": "the type of text that is produced.",
    "start": "1294000",
    "end": "1295900"
  },
  {
    "text": "You can define how creative it is almost,",
    "start": "1295900",
    "end": "1298280"
  },
  {
    "text": "how diverse the words are.",
    "start": "1298280",
    "end": "1300560"
  },
  {
    "text": "You can define how precise\nyou want them to be.",
    "start": "1300560",
    "end": "1302920"
  },
  {
    "text": "You can define what the\nlengths of them should be.",
    "start": "1302920",
    "end": "1305450"
  },
  {
    "text": "So the addendum to the\nboiler-plate code tip",
    "start": "1305450",
    "end": "1309380"
  },
  {
    "text": "is to when you're working\nwith again, generation models,",
    "start": "1309380",
    "end": "1313800"
  },
  {
    "text": "just spend some time testing out",
    "start": "1313800",
    "end": "1315740"
  },
  {
    "text": "the best hyperparameters for inferencing",
    "start": "1315740",
    "end": "1319530"
  },
  {
    "text": "and then make sure that you load those in.",
    "start": "1319530",
    "end": "1321760"
  },
  {
    "text": "And then the last pro tip\nis again, plan for bias.",
    "start": "1321760",
    "end": "1325560"
  },
  {
    "text": "Plan on bias being in your models.",
    "start": "1325560",
    "end": "1328220"
  },
  {
    "text": "Develop ways to review these.",
    "start": "1328220",
    "end": "1330390"
  },
  {
    "text": "So use SageMaker Clarify\nto set up notifications.",
    "start": "1330390",
    "end": "1334190"
  },
  {
    "text": "Use SageMaker Ground Truth.",
    "start": "1334190",
    "end": "1335903"
  },
  {
    "text": "Use your your boots-on-the-ground experts",
    "start": "1336910",
    "end": "1339250"
  },
  {
    "text": "who can help you figure out",
    "start": "1339250",
    "end": "1340990"
  },
  {
    "text": "why a model is doing a certain thing",
    "start": "1340990",
    "end": "1342679"
  },
  {
    "text": "or what the best response\nis for a customer",
    "start": "1342680",
    "end": "1344900"
  },
  {
    "text": "and ensure that you're developing",
    "start": "1344900",
    "end": "1346930"
  },
  {
    "text": "these overall system designs",
    "start": "1346930",
    "end": "1349550"
  },
  {
    "text": "that are robust to that type of bias.",
    "start": "1349550",
    "end": "1352320"
  },
  {
    "text": "And with that, thank you very much.",
    "start": "1352320",
    "end": "1355120"
  },
  {
    "text": "Let's go train some models.",
    "start": "1355120",
    "end": "1356503"
  }
]