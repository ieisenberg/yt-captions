[
  {
    "text": "In this video, you’ll learn about data producer \noptions for Amazon Kinesis Data Streams.",
    "start": "58",
    "end": "4535"
  },
  {
    "text": "Amazon Kinesis services enable any \norganization to easily collect, process,",
    "start": "5706",
    "end": "9894"
  },
  {
    "text": "and analyze data streams in real time.",
    "start": "9894",
    "end": "12092"
  },
  {
    "text": "Within the streaming flow, Kinesis Data \nStreams is used to collect and store",
    "start": "13000",
    "end": "16540"
  },
  {
    "text": "data streams from multiple sources\n-or data producers--for analytics.",
    "start": "16540",
    "end": "20499"
  },
  {
    "text": "Afterwards, data consumers such \nas Kinesis Data Firehose and Kinesis",
    "start": "21498",
    "end": "25345"
  },
  {
    "text": "Data Analytics can retrieve and \nprocess the data records as needed.",
    "start": "25346",
    "end": "28665"
  },
  {
    "text": "Amazon Kinesis Data Streams \nprocesses trillions of records per day",
    "start": "29886",
    "end": "33378"
  },
  {
    "text": "across tens of thousands of customers.",
    "start": "33379",
    "end": "35476"
  },
  {
    "text": "The largest customers ingest more than \n5 Gigabytes per second of real-time data",
    "start": "36000",
    "end": "40013"
  },
  {
    "text": "with individual Kinesis Streams.",
    "start": "40013",
    "end": "41829"
  },
  {
    "text": "To meet customers’ data streaming \nneeds, Amazon Kinesis Data Streams",
    "start": "43238",
    "end": "46670"
  },
  {
    "text": "provides three ways to build data producers.",
    "start": "46670",
    "end": "49000"
  },
  {
    "text": "Amazon service integrations enable \ncustomers to build streaming-data use",
    "start": "50049",
    "end": "53562"
  },
  {
    "text": "cases across a variety of data source types.",
    "start": "53562",
    "end": "56075"
  },
  {
    "text": "Some of these, such as Amazon \nEventBridge can act as both a data",
    "start": "56657",
    "end": "59709"
  },
  {
    "text": "producer and data consumer \nfor Kinesis Data Streams.",
    "start": "59709",
    "end": "62590"
  },
  {
    "text": "Many third-party applications have \nnative integrations for producing",
    "start": "64192",
    "end": "67332"
  },
  {
    "text": "data into Kinesis Data Streams.",
    "start": "67332",
    "end": "69099"
  },
  {
    "text": "For example, Apache Flink has a Kinesis\n connector that can provide access to",
    "start": "69810",
    "end": "73465"
  },
  {
    "text": "Kinesis Data Streams as a \ndata producer or a consumer.",
    "start": "73466",
    "end": "76437"
  },
  {
    "text": "AWS toolkits or libraries allow \ncustomers to build custom data producers.",
    "start": "77601",
    "end": "82183"
  },
  {
    "text": "More details can be found \nin the AWS documentation.",
    "start": "82780",
    "end": "85597"
  },
  {
    "text": "Let’s look at two examples with AWS \nservices acting as data producers.",
    "start": "86819",
    "end": "90682"
  },
  {
    "text": "In the first example, a customer uses \nAmazon DynamoDB to gather sports",
    "start": "91444",
    "end": "95634"
  },
  {
    "text": "data feeds and ingest them into Amazon\n Kinesis Data Streams for analysis.",
    "start": "95634",
    "end": "99501"
  },
  {
    "text": "This enables betting throughout \nthe entire duration of matches.",
    "start": "100233",
    "end": "102933"
  },
  {
    "text": "In the second example, NerdWallet \nuses AWS Database Migration Service",
    "start": "104255",
    "end": "108534"
  },
  {
    "text": "to stream change data capture (CDC) \nlogs into Amazon Kinesis Data Streams.",
    "start": "108535",
    "end": "113034"
  },
  {
    "text": "This enables them to scale to \nthousands of writes per second",
    "start": "113803",
    "end": "116194"
  },
  {
    "text": "within minutes of freshness of data.",
    "start": "116194",
    "end": "117885"
  },
  {
    "text": "These examples highlight the \nease and potential power of",
    "start": "119128",
    "end": "121348"
  },
  {
    "text": "Amazon service integrations.",
    "start": "121348",
    "end": "123028"
  },
  {
    "text": "When building custom data producers, \nhere are some key facts to keep in mind.",
    "start": "124228",
    "end": "128239"
  },
  {
    "text": "If you use the Kinesis Producer Library, \nit can be monitored with Amazon",
    "start": "129038",
    "end": "132642"
  },
  {
    "text": "CloudWatch for visibility \ninto producer performance.",
    "start": "132642",
    "end": "135371"
  },
  {
    "text": "If you use AWS SDK, the two main APIs \nit uses are PutRecord, for ingesting",
    "start": "136000",
    "end": "140990"
  },
  {
    "text": "single records, and PutRecords, for \ningesting multiple records at a time.",
    "start": "140990",
    "end": "145000"
  },
  {
    "text": "Each data record has a partition key, \nwhich can be used to separate sets",
    "start": "146000",
    "end": "149531"
  },
  {
    "text": "of data and dedicate certain \nshards to certain datasets.",
    "start": "149531",
    "end": "152326"
  },
  {
    "text": "Each data record has a unique sequence\n number that is assigned by Kinesis Data",
    "start": "153483",
    "end": "156740"
  },
  {
    "text": "Streams after ingestion.",
    "start": "156740",
    "end": "158241"
  },
  {
    "text": "This can be used afterwards either for \ndata replay or for consumption and analysis.",
    "start": "158794",
    "end": "162805"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "163689",
    "end": "166842"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "167065",
    "end": "169156"
  }
]