[
  {
    "text": "hi everyone this is Kosta vasila caucus and I'm a Global Business Development Manager from the ml services at AWS",
    "start": "149",
    "end": "8540"
  },
  {
    "text": "today I have with me Stefan NATO a senior and mouth specialist and we want",
    "start": "8540",
    "end": "14400"
  },
  {
    "text": "to help you and all other stakeholders from highly regulated industries like data financial services develop a deeper",
    "start": "14400",
    "end": "21600"
  },
  {
    "text": "understanding of how you can build end-to-end and mal workflows would say its major while complying with your",
    "start": "21600",
    "end": "27240"
  },
  {
    "text": "industries legislation now let's see what we're going to cover today today we",
    "start": "27240",
    "end": "34950"
  },
  {
    "text": "aim to address these age requirements which we have consolidated from a series of NASA services customers we have put",
    "start": "34950",
    "end": "41910"
  },
  {
    "text": "together a presentation and a Jupiter notebook so that with not only sir best practices but we'll also give you a",
    "start": "41910",
    "end": "47969"
  },
  {
    "text": "hands-on concrete an end to an example from data preparation all the way to model monitoring more specifically we",
    "start": "47969",
    "end": "55770"
  },
  {
    "text": "will showcase these eight following objectives first how to deploy sales",
    "start": "55770",
    "end": "61859"
  },
  {
    "text": "maker in a V PC and then C compute and network isolation if you come from",
    "start": "61859",
    "end": "67470"
  },
  {
    "text": "financial services that's expected and no internet connectivity should be given by default second how to interrogate",
    "start": "67470",
    "end": "75900"
  },
  {
    "text": "only users that can be identified and authorized them appropriately based on",
    "start": "75900",
    "end": "81630"
  },
  {
    "text": "their I am permissions with normal to tenants third how to integrate with your",
    "start": "81630",
    "end": "87659"
  },
  {
    "text": "private gig code commit or artifactory as a common source of truth for prep",
    "start": "87659",
    "end": "92850"
  },
  {
    "text": "transparency and versioning as well as how to standardize not provisioning with",
    "start": "92850",
    "end": "98579"
  },
  {
    "text": "lifecycle config fourth how to encrypt data in transit and entrust with",
    "start": "98579",
    "end": "104970"
  },
  {
    "text": "customer provides the corruption kids fifth how to trace a male-model lineage",
    "start": "104970",
    "end": "110310"
  },
  {
    "text": "from data preparation model development and training iterations as well as how to audit who did what at any given point",
    "start": "110310",
    "end": "118170"
  },
  {
    "text": "in time sixth how to explain and interpret the Train model by using sub",
    "start": "118170",
    "end": "123630"
  },
  {
    "text": "failures and feature importance seventh how to monitor your model in production",
    "start": "123630",
    "end": "129869"
  },
  {
    "text": "to protect against data drift and automatically react rules that you define and lastly how to",
    "start": "129869",
    "end": "137170"
  },
  {
    "text": "reproduce the ml model based on model lineage and the stored artifacts here we",
    "start": "137170",
    "end": "143739"
  },
  {
    "text": "saw the most common steps in an ml workflow from data sourcing all the way to product ization of the trend model",
    "start": "143739",
    "end": "149349"
  },
  {
    "text": "and we wanted to quit in set expectations as to who we believe should",
    "start": "149349",
    "end": "154359"
  },
  {
    "text": "help in satisfying its requirement and where we'll be diving deeper today the",
    "start": "154359",
    "end": "160419"
  },
  {
    "text": "three most common personas that we have encountered at the first the system operator or sis ops team that set the",
    "start": "160419",
    "end": "167349"
  },
  {
    "text": "necessary security guard rails and enable the systematic access to the ml environment second the data scientists",
    "start": "167349",
    "end": "175719"
  },
  {
    "text": "that work from data sourcing and feature engineering to training a model and then handing it over to the third persona the",
    "start": "175719",
    "end": "182709"
  },
  {
    "text": "DevOps engineers or else the ml ops team that either uses an automated CI CI",
    "start": "182709",
    "end": "188829"
  },
  {
    "text": "pipeline for the deployments or goes through a manual product ization process there's of course other personas",
    "start": "188829",
    "end": "195250"
  },
  {
    "text": "depending on the organization structure and for example in many organizations data engineers are responsible for",
    "start": "195250",
    "end": "201669"
  },
  {
    "text": "sourcing and performing intel on the data when data scientists only focus on the modeling and feature engineering",
    "start": "201669",
    "end": "208290"
  },
  {
    "text": "today we'll showcase how we leverage a prepackaged workshop that helps his shops teams",
    "start": "208290",
    "end": "213939"
  },
  {
    "text": "enable secure and systematic access to say its maker by the AWS service catalog",
    "start": "213939",
    "end": "219549"
  },
  {
    "text": "while complying with the force own requirements on that front we will",
    "start": "219549",
    "end": "224829"
  },
  {
    "text": "redirect you to the detailed workshop and just serve some best practices in today's demo we will dive deep on the",
    "start": "224829",
    "end": "232870"
  },
  {
    "text": "data scientist persona and what they need to do to comply with the above-mentioned requirements",
    "start": "232870",
    "end": "238329"
  },
  {
    "text": "as you can see data encryption is also mentioned here and in the SI subs",
    "start": "238329",
    "end": "243549"
  },
  {
    "text": "responsibilities and that's because both personas need to partake Stefan in a",
    "start": "243549",
    "end": "249040"
  },
  {
    "text": "little bit will walk us through the Jupiter notebook which includes an end-to-end example of the areas that you",
    "start": "249040",
    "end": "254739"
  },
  {
    "text": "see here in green from data preparation to model development all the way to triggering the deployment of the model",
    "start": "254739",
    "end": "261219"
  },
  {
    "text": "as well as the model monitoring on its requirement we will still give you best practices through slide",
    "start": "261219",
    "end": "267370"
  },
  {
    "text": "but we will dive deep on the Jupiter notebook itself the DevOps experience and requirements will actually not be",
    "start": "267370",
    "end": "274120"
  },
  {
    "text": "covered today but will be addressed through another demo that we're all currently working on so let's see the",
    "start": "274120",
    "end": "281020"
  },
  {
    "text": "architecture and the ML step that will be part of today's demo in the demo we",
    "start": "281020",
    "end": "287500"
  },
  {
    "text": "will spin up sets maker under a V PC with no internet access we will use IIM",
    "start": "287500",
    "end": "293320"
  },
  {
    "text": "to provide only the necessary access to a single user and we integrate it with a",
    "start": "293320",
    "end": "298600"
  },
  {
    "text": "code commit Ripper we will use AWS KMS or else key management service to",
    "start": "298600",
    "end": "305710"
  },
  {
    "text": "encrypt data at rest and in transit and as every action is an API call we will",
    "start": "305710",
    "end": "311950"
  },
  {
    "text": "track all of them by a cloud trail and we will use sage maker experiments and code commit which in your case could be",
    "start": "311950",
    "end": "320020"
  },
  {
    "text": "an artifact or for example for model units in this example we will pre",
    "start": "320020",
    "end": "327070"
  },
  {
    "text": "process raw data using a monitor is called sage maker processing which lets",
    "start": "327070",
    "end": "332950"
  },
  {
    "text": "you easily run pre-processing post-processing and model evaluation workloads on a fully managed",
    "start": "332950",
    "end": "339550"
  },
  {
    "text": "infrastructure we will also use a custom container with EXCI boost which we will",
    "start": "339550",
    "end": "346150"
  },
  {
    "text": "build within sage maker and we will deploy in the AWS elastic container",
    "start": "346150",
    "end": "351550"
  },
  {
    "text": "registry or else called ECR we will then use sails makers one-click training to",
    "start": "351550",
    "end": "359080"
  },
  {
    "text": "scale our training job in a man√≠s fashion over spot instances to reduce",
    "start": "359080",
    "end": "364090"
  },
  {
    "text": "cost and we will then save the trained model artifacts in AWS s3 which we will",
    "start": "364090",
    "end": "371980"
  },
  {
    "text": "be able to use to showcase interpretability and explain bility in the sales maker notebook itself we will",
    "start": "371980",
    "end": "379300"
  },
  {
    "text": "then go through a manual model deployment process which in a real scenario in the future demo will be a",
    "start": "379300",
    "end": "385120"
  },
  {
    "text": "csv pipeline that will automatically deploy the model we will then deploy a",
    "start": "385120",
    "end": "391510"
  },
  {
    "text": "monitoring job that you see here to evaluate if there are violations on the data type or significant data on the",
    "start": "391510",
    "end": "398260"
  },
  {
    "text": "live data and lastly will case how you can use sales make your experiments and code commit or",
    "start": "398260",
    "end": "405550"
  },
  {
    "text": "artifactory to reproduce the exact same pipeline and model by these third",
    "start": "405550",
    "end": "410889"
  },
  {
    "text": "artifacts as mentioned before we leverage a comprehensive workshop which",
    "start": "410889",
    "end": "417520"
  },
  {
    "text": "can be found in the link here and help us provision sales maker via the AWS",
    "start": "417520",
    "end": "422530"
  },
  {
    "text": "Service Catalog that means that once the data scientist requests a notebook from",
    "start": "422530",
    "end": "427900"
  },
  {
    "text": "the cell-cell report on a cloud formation script configure sales maker to direct all that working traffic",
    "start": "427900",
    "end": "434410"
  },
  {
    "text": "through an e a9 under the customers VP scene in which we can enable or disable",
    "start": "434410",
    "end": "439539"
  },
  {
    "text": "the intern gateway that controls access to the public internet in our scenario",
    "start": "439539",
    "end": "445870"
  },
  {
    "text": "we chose to disable the internal access for all sales major components and more specifically across notebooks training",
    "start": "445870",
    "end": "453759"
  },
  {
    "text": "and hosting we also use V PC endpoints to gain access to data in s3 as well as",
    "start": "453759",
    "end": "461380"
  },
  {
    "text": "enforce km mask is use it for all data once there is very detailed guidelines",
    "start": "461380",
    "end": "467440"
  },
  {
    "text": "on the workshop for completion purposes I'll just spend two more minutes walking you through the best practices on the",
    "start": "467440",
    "end": "474190"
  },
  {
    "text": "first four regulatory requirements and then unless the fund dive into the Hajin them as discussed also is major",
    "start": "474190",
    "end": "482949"
  },
  {
    "text": "components should be deployed and accessed over the PPC with no internet connectivity and the vp scene points",
    "start": "482949",
    "end": "490750"
  },
  {
    "text": "should be used to control data traffic across the necessary at the real services vp c employment policies can",
    "start": "490750",
    "end": "498909"
  },
  {
    "text": "help you further control touch traffic but you can also limit access to one or",
    "start": "498909",
    "end": "504099"
  },
  {
    "text": "a couple of IPS to enforce user to only go through the corporate network and",
    "start": "504099",
    "end": "510330"
  },
  {
    "text": "lastly add subnets and security group ideals when you call it training from",
    "start": "510330",
    "end": "515950"
  },
  {
    "text": "your notebook those are your VP C's subnets and security groups and can help",
    "start": "515950",
    "end": "521020"
  },
  {
    "text": "in building the in eyes for you in addition for proper authorization of",
    "start": "521020",
    "end": "528010"
  },
  {
    "text": "Education use I am and tag based access control to allow only the one that",
    "start": "528010",
    "end": "533560"
  },
  {
    "text": "assigned this notebook that way once you launch it a notebook here are the identities",
    "start": "533560",
    "end": "538779"
  },
  {
    "text": "attached as per the logging through the corporate federated ad and as you can make sure only your name and ID can",
    "start": "538779",
    "end": "546100"
  },
  {
    "text": "access it ice age make it runs on insta too it also entails the same I am roles",
    "start": "546100",
    "end": "551830"
  },
  {
    "text": "which follow the same access patterns without use I am conditioned keys to",
    "start": "551830",
    "end": "557589"
  },
  {
    "text": "make sure that for example when a user tries to insensate a training job they have indeed given the necessary subnet",
    "start": "557589",
    "end": "564459"
  },
  {
    "text": "and security group parameters that I just mentioned and lastly use detective",
    "start": "564459",
    "end": "569860"
  },
  {
    "text": "controls to detect and terminate immediately if a user is not following the best practices enlarging resources",
    "start": "569860",
    "end": "576310"
  },
  {
    "text": "under the subnet with regards to artifact management remember that apart",
    "start": "576310",
    "end": "582399"
  },
  {
    "text": "from building the models in sales maker you need to manage both the training and infrastructure containers all these need",
    "start": "582399",
    "end": "588730"
  },
  {
    "text": "to be managed using a centralized artifact management or dependency management system for version control",
    "start": "588730",
    "end": "595120"
  },
  {
    "text": "you can use the AWS code asterisk product suite and any github commands",
    "start": "595120",
    "end": "600700"
  },
  {
    "text": "directly from Sage make your notebook for the repositories management of",
    "start": "600700",
    "end": "606250"
  },
  {
    "text": "docker containers or the artifacts and Python or our libraries you can use",
    "start": "606250",
    "end": "611290"
  },
  {
    "text": "Amazon s3 Amazon ECR or artifactory in addition use lifecycle configurations to",
    "start": "611290",
    "end": "619209"
  },
  {
    "text": "distribute common libraries your team uses and they're not available by default in the sage maker instance you",
    "start": "619209",
    "end": "625720"
  },
  {
    "text": "can actually use lifecycle config for both when you create a notebook as well as when you start a notebook lastly as",
    "start": "625720",
    "end": "633760"
  },
  {
    "text": "for model artifacts which are saved in a string we recommend that you enable s3 versioning to track all versions on the",
    "start": "633760",
    "end": "643899"
  },
  {
    "text": "data encryption front it is important to note that sales maker has encryption of data at rest and in transit built into",
    "start": "643899",
    "end": "651399"
  },
  {
    "text": "the service for the data stored in a string it follows the exact same best practices that s3 comes with meaning",
    "start": "651399",
    "end": "658690"
  },
  {
    "text": "that you can use your own customer manage keys to encrypt and decrypt your data exactly the same occurs for the EBS",
    "start": "658690",
    "end": "666550"
  },
  {
    "text": "volumes that are you on the notebooks on the training jobs and on the hosting instances lastly for",
    "start": "666550",
    "end": "674140"
  },
  {
    "text": "the training containers at support cluster mode like pester flow for example the traffic between training",
    "start": "674140",
    "end": "680080"
  },
  {
    "text": "nodes can be encrypted but you have to remember that it actually might downgrade your performance now I'll hand",
    "start": "680080",
    "end": "687430"
  },
  {
    "text": "it over to Stefan that we walk us through the secure environment we instantiated through the cloud formation",
    "start": "687430",
    "end": "693250"
  },
  {
    "text": "script as well as the Jupiter in watt book with the end-to-end ml work through that covers all ADA requirements once he",
    "start": "693250",
    "end": "701350"
  },
  {
    "text": "is done I'll wrap it up and walk you through the best practices on the last four objectives Stefan here you go",
    "start": "701350",
    "end": "709950"
  },
  {
    "text": "Thank You Kosta for that high-level explanation of the best practices for the first four compliance requirements",
    "start": "710580",
    "end": "716140"
  },
  {
    "text": "on compute and network isolation authentication artifact management and data encryption now I will dive deeper",
    "start": "716140",
    "end": "722860"
  },
  {
    "text": "and walk you through a demo showing the end-to-end data scientist experience on Amazon sage maker using familiar Jupiter",
    "start": "722860",
    "end": "728740"
  },
  {
    "text": "notebooks as a best practice in most regulated industries such as financial services data scientists are typically",
    "start": "728740",
    "end": "734890"
  },
  {
    "text": "not allowed to create notebooks provisions or modify elastic network interfaces or Yen eyes or provision any",
    "start": "734890",
    "end": "740800"
  },
  {
    "text": "underlying instances this can be done using infrastructures code best practices by administrators as cost you",
    "start": "740800",
    "end": "747010"
  },
  {
    "text": "mentioned here we have used cloud formation and the AWS service catalog to already provision a secure environment",
    "start": "747010",
    "end": "752800"
  },
  {
    "text": "for data scientists to work with this script is available as part of the security workshop we leverage but will also be available in the github repo",
    "start": "752800",
    "end": "759610"
  },
  {
    "text": "associated with this notebook so let's jump right into the console to pre create notebooks we can use the service",
    "start": "759610",
    "end": "765760"
  },
  {
    "text": "catalog data scientists can simply click on launch product and the appropriate product such as a sage maker notebook",
    "start": "765760",
    "end": "772300"
  },
  {
    "text": "environment can be provisioned for you notice that we have also created an iam role for our data scientist to assume",
    "start": "772300",
    "end": "779140"
  },
  {
    "text": "which will only allow them access to the services that they can use for example while data scientists can access to sage",
    "start": "779140",
    "end": "786370"
  },
  {
    "text": "maker console and open Jupiter notebooks data scientists should not have any",
    "start": "786370",
    "end": "791560"
  },
  {
    "text": "permissions to access services such as I am or AWS kms to modify any of the",
    "start": "791560",
    "end": "798040"
  },
  {
    "text": "underlying roles or permissions or create or any delete any access keys that are granted to them data scientists",
    "start": "798040",
    "end": "805070"
  },
  {
    "text": "should also typically not be able to access any developer tools such as code commit code build code deploy or code",
    "start": "805070",
    "end": "811040"
  },
  {
    "text": "pipeline that could potentially be used to deploy models into production however",
    "start": "811040",
    "end": "816470"
  },
  {
    "text": "data scientists can have access to the code commit api's as we shall see later in order to push and pull code into the",
    "start": "816470",
    "end": "823700"
  },
  {
    "text": "appropriate repositories or branches this ensures that administrators can enforce least privileged access to AWS",
    "start": "823700",
    "end": "829700"
  },
  {
    "text": "services let's now take a look at the sage maker notebook environment that we have pre created using the service",
    "start": "829700",
    "end": "835730"
  },
  {
    "text": "catalog before jumping into the notebooks let's take a quick look at the",
    "start": "835730",
    "end": "842120"
  },
  {
    "text": "notebook environment that we have pre created we have provisioned a t3 instance which is sufficient in many",
    "start": "842120",
    "end": "847250"
  },
  {
    "text": "cases for data exploration within the notebook attached with it is an encrypted EBS volume to ensure data",
    "start": "847250",
    "end": "853160"
  },
  {
    "text": "encryption at rest we have also used lifecycle configurations to pre download any open source libraries or any data",
    "start": "853160",
    "end": "860089"
  },
  {
    "text": "sets let's take a quick look at the lifecycle configuration script attached to this notebook here we are pre",
    "start": "860089",
    "end": "866089"
  },
  {
    "text": "downloading a data set that we are going to use for data exploration and for training models in addition we're also",
    "start": "866089",
    "end": "872360"
  },
  {
    "text": "installing using pip open source libraries such as chef that we shall use for extracting feature importances in",
    "start": "872360",
    "end": "879380"
  },
  {
    "text": "order to maintain network isolation such that the VPC that is connected to the data scientist account does not have",
    "start": "879380",
    "end": "885470"
  },
  {
    "text": "internet access this can be done using a private pi PI server or a pip mirror from a centralized IT or a shared",
    "start": "885470",
    "end": "891740"
  },
  {
    "text": "services account which in turn does have access to the public Internet in",
    "start": "891740",
    "end": "896750"
  },
  {
    "text": "addition we've attached a code commit repository with appropriate secrets that has been attached to this notebook for",
    "start": "896750",
    "end": "903170"
  },
  {
    "text": "versioning any code note that while we denied data scientists access to the code commit console access to specific",
    "start": "903170",
    "end": "909980"
  },
  {
    "text": "API is forgetting pulling and pushing code from the repos has been granted enterprises can replace code code commit",
    "start": "909980",
    "end": "917630"
  },
  {
    "text": "or get with bitbucket or their own on Prem Enterprise get repo for this demo",
    "start": "917630",
    "end": "923750"
  },
  {
    "text": "however we shall be using a code commit repository to check in and check out any code",
    "start": "923750",
    "end": "928870"
  },
  {
    "text": "finally in order to maintain Network isolation we have",
    "start": "928870",
    "end": "933980"
  },
  {
    "text": "we disabled any internet access from this notebook and we have attached a security group and the proper subnets in",
    "start": "933980",
    "end": "940580"
  },
  {
    "text": "order to control the ingress and egress of network traffic to and from our notebooks system administrators can now",
    "start": "940580",
    "end": "946400"
  },
  {
    "text": "monitor the network traffic for API calls using tools such as AWS cloud rail",
    "start": "946400",
    "end": "951580"
  },
  {
    "text": "let us now take a look at the notebook that we have built for our data scientists in particular to illustrate",
    "start": "951580",
    "end": "958190"
  },
  {
    "text": "the separation between data scientists and engineers I have created two notebooks that demonstrate the typical",
    "start": "958190",
    "end": "963650"
  },
  {
    "text": "data scientist workflow and the second notebook to demonstrate the model deployment and monitoring workflow as",
    "start": "963650",
    "end": "969110"
  },
  {
    "text": "Kosti mentioned we shall not be demonstrating how to automate the DevOps workflow in this video and we leave that",
    "start": "969110",
    "end": "974570"
  },
  {
    "text": "for a separate video so here I am in a",
    "start": "974570",
    "end": "981170"
  },
  {
    "text": "sage maker notebook environment that we have pre created for you for the purposes of this demo throughout these",
    "start": "981170",
    "end": "987410"
  },
  {
    "text": "two notebooks we will demonstrate the eight key concepts required for most regulatory industries to build ml workflows and how",
    "start": "987410",
    "end": "994040"
  },
  {
    "text": "to ensure and implement them in the sage maker environment so let's get started with the data science experience the",
    "start": "994040",
    "end": "1000280"
  },
  {
    "text": "first thing we want to take a look at is network isolation to test our network",
    "start": "1000280",
    "end": "1006160"
  },
  {
    "text": "isolation as we have said earlier we have disabled any internet access either from the Amazon V PC or from the private",
    "start": "1006160",
    "end": "1013210"
  },
  {
    "text": "V PC that this notebook is attached to to test us let me go ahead and run this curl command and as we see the command",
    "start": "1013210",
    "end": "1026140"
  },
  {
    "text": "does not run indicating that we have indeed disabled internet access from both Amazon and custom V PC that's been",
    "start": "1026140",
    "end": "1032319"
  },
  {
    "text": "attached to this notebook next let's take a look at authentication and authorization in addition to attaching a",
    "start": "1032320",
    "end": "1040510"
  },
  {
    "text": "role to the data scientists say to make our notebooks also need to assume an I am role in order to be able to access",
    "start": "1040510",
    "end": "1047470"
  },
  {
    "text": "API such as sage maker API code comment' api's or access data in Amazon s3 so",
    "start": "1047470",
    "end": "1054790"
  },
  {
    "text": "let's take a look at the I am role that we have attached to this notebook",
    "start": "1054790",
    "end": "1059879"
  },
  {
    "text": "the first thing we notice about the IM policy that has been attached to this notebook is that we have granted sage",
    "start": "1066010",
    "end": "1072200"
  },
  {
    "text": "maker full access this is generally not a best practice for example if data scientists in your organization do not",
    "start": "1072200",
    "end": "1079340"
  },
  {
    "text": "put models into production directly they should not be granted access to create sage maker endpoints or en eyes however",
    "start": "1079340",
    "end": "1086270"
  },
  {
    "text": "they may need access to sage make up batch transform for validating their models against a batch of data also",
    "start": "1086270",
    "end": "1094160"
  },
  {
    "text": "notice that we have granted full access to Amazon s3 fine-grained access control",
    "start": "1094160",
    "end": "1099950"
  },
  {
    "text": "to Amazon s3 can be ensured either using i.m permissions on the sage maker notebook or using bucket policies as we",
    "start": "1099950",
    "end": "1107030"
  },
  {
    "text": "shall see later on in this video having addressed I am and networking let's now",
    "start": "1107030",
    "end": "1114170"
  },
  {
    "text": "go ahead and import the libraries and utilities that we will need throughout this notebook in particular in order to",
    "start": "1114170",
    "end": "1123620"
  },
  {
    "text": "be able to track the metadata associated with our training jobs as well as our",
    "start": "1123620",
    "end": "1128720"
  },
  {
    "text": "processing jobs we are going to load in sage maker experiments additionally",
    "start": "1128720",
    "end": "1137600"
  },
  {
    "text": "we're also importing at the KMS key as well as the network configuration settings that we will need in order to",
    "start": "1137600",
    "end": "1144350"
  },
  {
    "text": "in order to ensure that any network traffic traverses private link and does",
    "start": "1144350",
    "end": "1150050"
  },
  {
    "text": "not traverse the public Internet finally",
    "start": "1150050",
    "end": "1155540"
  },
  {
    "text": "in order to manage our artifacts we are going to import the names of the buckets that have been created along with this",
    "start": "1155540",
    "end": "1161900"
  },
  {
    "text": "demo for us to use let's jump into one of the buckets and let's take a look at the policies that are attached to it so",
    "start": "1161900",
    "end": "1170080"
  },
  {
    "text": "here I am in the data team bucket which I'm going to use for storing any",
    "start": "1170830",
    "end": "1176000"
  },
  {
    "text": "training data test data or validation data let's take a look at the properties associated with this bucket the first",
    "start": "1176000",
    "end": "1183530"
  },
  {
    "text": "thing we see is that as required by a lot of our regulatory highly regulated industries we have attached default kms",
    "start": "1183530",
    "end": "1190850"
  },
  {
    "text": "encryption with this bucket additionally in order to control the ingress and",
    "start": "1190850",
    "end": "1196400"
  },
  {
    "text": "aggressive traffic to this bucket we can controlled us using fine-grained bucket policies for example this bucket has a",
    "start": "1196400",
    "end": "1204050"
  },
  {
    "text": "policy which denies any get or put objects from any principle unless they",
    "start": "1204050",
    "end": "1209900"
  },
  {
    "text": "emanate from the source VPC endpoint that we have created this ensures that",
    "start": "1209900",
    "end": "1214910"
  },
  {
    "text": "all network traffic to and from this bucket only traverses private link and will not traverse the public Internet",
    "start": "1214910",
    "end": "1222910"
  },
  {
    "text": "further fine-grained access controls can be applied to the s3 buckets directly or",
    "start": "1222910",
    "end": "1228230"
  },
  {
    "text": "to the sage maker iam role that we demonstrated above in order to ensure that the data scientists only have",
    "start": "1228230",
    "end": "1234800"
  },
  {
    "text": "access to the buckets that they should have access to this can also be done",
    "start": "1234800",
    "end": "1240050"
  },
  {
    "text": "using tags attached to I am roles to ensure the different sub teams within",
    "start": "1240050",
    "end": "1245390"
  },
  {
    "text": "the larger data science team for example can only access certain buckets having",
    "start": "1245390",
    "end": "1251510"
  },
  {
    "text": "imported the necessary libraries that we need for our demonstration let us now move into the next stage which is on",
    "start": "1251510",
    "end": "1257720"
  },
  {
    "text": "pre-processing and feature engineering our data set the first step in any data",
    "start": "1257720",
    "end": "1266240"
  },
  {
    "text": "science lifecycle is data exploration pre-processing and feature engineering here we have used a credit card data set",
    "start": "1266240",
    "end": "1273080"
  },
  {
    "text": "to predict whether or not a user will default on their credit card payment based on historical payment data",
    "start": "1273080",
    "end": "1279410"
  },
  {
    "text": "historical bills that they have received as well as some demographic information about the user as we can see this is a",
    "start": "1279410",
    "end": "1286400"
  },
  {
    "text": "binary classification problem where we want to predict the default probability for whether the user will default on",
    "start": "1286400",
    "end": "1291920"
  },
  {
    "text": "their payments or not typically once we import the data set we want to do some simple data exploration such as we want",
    "start": "1291920",
    "end": "1298700"
  },
  {
    "text": "to look for any missing labels or we want to look at look for any missing values in our features we also want to",
    "start": "1298700",
    "end": "1306170"
  },
  {
    "text": "see if our labels themselves are actually imbalanced in other words whether there are more of one label in",
    "start": "1306170",
    "end": "1311750"
  },
  {
    "text": "our data set than the other this is the indeed the case here for example we can see that there are more users that have",
    "start": "1311750",
    "end": "1317510"
  },
  {
    "text": "not defaulted on their payment sources those that have another thing that's important to look at is to look for",
    "start": "1317510",
    "end": "1323540"
  },
  {
    "text": "correlations amongst our features or our independent variables themselves for example we see here that the payment",
    "start": "1323540",
    "end": "1329300"
  },
  {
    "text": "histories and the bill are actually correlated with each other if we were to use a simple model like a",
    "start": "1329300",
    "end": "1334320"
  },
  {
    "text": "linear regression mium we may want to consider a technique such as principal component analysis 2d correlate some of",
    "start": "1334320",
    "end": "1341040"
  },
  {
    "text": "these features or some other dimensionality reduction method in order to remove some of the correlations that",
    "start": "1341040",
    "end": "1346320"
  },
  {
    "text": "we have in our independent features having explored the data we can now",
    "start": "1346320",
    "end": "1351390"
  },
  {
    "text": "pre-process our data set this typically involves steps such as outlier removal transformations such as one hard",
    "start": "1351390",
    "end": "1357960"
  },
  {
    "text": "encoding of categorical variables at splitting of the data into train and test or validation this can be done in",
    "start": "1357960",
    "end": "1364410"
  },
  {
    "text": "small data sets for small data sets directly within the stage maker notebook environment itself however this is not",
    "start": "1364410",
    "end": "1370470"
  },
  {
    "text": "really a scalable method for larger data sets however customers can now use sage",
    "start": "1370470",
    "end": "1376830"
  },
  {
    "text": "maker processing to pre-process their data at scale sage maker handles the provisioning and",
    "start": "1376830",
    "end": "1382350"
  },
  {
    "text": "the setup of the underlying instances that you need in order to prep your data we've run the pre processing job on",
    "start": "1382350",
    "end": "1388170"
  },
  {
    "text": "those instances and automatically deep revisions the instances so you stop paying for them once your processing job",
    "start": "1388170",
    "end": "1394410"
  },
  {
    "text": "is complete sage maker currently provides managed containers for your scikit-learn code which is the most common pre-processing",
    "start": "1394410",
    "end": "1400800"
  },
  {
    "text": "library but customers can also bring their own containers for example to run pre processing jobs on spark or on desk",
    "start": "1400800",
    "end": "1408240"
  },
  {
    "text": "to see how this is done we first call the psychic learn processor API and provided an instance type that we're",
    "start": "1408240",
    "end": "1415260"
  },
  {
    "text": "going to use for our pre processing job as well as a count of the number of instances that we want to use",
    "start": "1415260",
    "end": "1420770"
  },
  {
    "text": "additionally we also provided the KMS key to ensure that any EBS volumes that",
    "start": "1420770",
    "end": "1426630"
  },
  {
    "text": "are attached to this instance are encrypted so we have data encryption at rest we also provide the network",
    "start": "1426630",
    "end": "1432960"
  },
  {
    "text": "configuration settings that we defined above in order to ensure that the traffic network traffic only traverses",
    "start": "1432960",
    "end": "1438990"
  },
  {
    "text": "private link and does not traverse the public Internet the next thing we want to do is we want to take the same code",
    "start": "1438990",
    "end": "1445500"
  },
  {
    "text": "that we had above but now we want to package it into a Python script this is done here once we package our code into",
    "start": "1445500",
    "end": "1455400"
  },
  {
    "text": "a Python script we simply upload that code to an s3 location we provide the locations for the training and the test",
    "start": "1455400",
    "end": "1461550"
  },
  {
    "text": "data and we're ready to run our job to run the processing job we simply",
    "start": "1461550",
    "end": "1466690"
  },
  {
    "text": "call the psychic learn processor run command providing it the location of our source code as well as our source data",
    "start": "1466690",
    "end": "1473559"
  },
  {
    "text": "and the destinations where we want our train validation and output data sets to be stored we also provide any other",
    "start": "1473559",
    "end": "1480250"
  },
  {
    "text": "arguments that we might need in order to run our script such as the Train test split ratio here we choose a standard",
    "start": "1480250",
    "end": "1486340"
  },
  {
    "text": "train test split ratio of 8020 once that's done stage maker processing takes",
    "start": "1486340",
    "end": "1492250"
  },
  {
    "text": "care of setting up the instances of running our code and then deep provisioning those instances are tearing",
    "start": "1492250",
    "end": "1497890"
  },
  {
    "text": "them down as soon as a processing job is complete we shall now we now have our",
    "start": "1497890",
    "end": "1503170"
  },
  {
    "text": "train and test data setting in the s3 buckets that we specified let's go take a look so here I am in the bucket that",
    "start": "1503170",
    "end": "1513520"
  },
  {
    "text": "I'm using to log my data artifacts and what you can see is that a number of different folders have been created",
    "start": "1513520",
    "end": "1519040"
  },
  {
    "text": "after running the processing job for example we have a folder for holding the training data the testing data as well",
    "start": "1519040",
    "end": "1525250"
  },
  {
    "text": "as the training data with the headers included which we will need later in order to run our model monitoring jobs",
    "start": "1525250",
    "end": "1531030"
  },
  {
    "text": "let's take a look at one of these folders so here we have a CSV file that",
    "start": "1531030",
    "end": "1537340"
  },
  {
    "text": "holds our training data that we are now going to be using for training our machine learning models before getting",
    "start": "1537340",
    "end": "1547179"
  },
  {
    "text": "into our model training let's take a quick look at traceability and auditability most regulated industries",
    "start": "1547179",
    "end": "1553330"
  },
  {
    "text": "have requirements around tracking metadata associated with the training jobs additionally if different data",
    "start": "1553330",
    "end": "1558940"
  },
  {
    "text": "scientists for example run multiple models on the same data or the same algorithm with different hyper",
    "start": "1558940",
    "end": "1564850"
  },
  {
    "text": "parameters they want to be able to compare their training runs to determine the best model to deploy stage maker",
    "start": "1564850",
    "end": "1571900"
  },
  {
    "text": "experiments helps us do this it automatically tracks the metadata associated with training jobs as well as",
    "start": "1571900",
    "end": "1577600"
  },
  {
    "text": "any other jobs that are related such as our pre-processing job although we do not demonstrate this here",
    "start": "1577600",
    "end": "1583030"
  },
  {
    "text": "with ch maker studio we can view all of our training runs in a single pane of glass view compare them as well as do",
    "start": "1583030",
    "end": "1590110"
  },
  {
    "text": "any custom plotting comparing these runs against different metrics here we will",
    "start": "1590110",
    "end": "1595780"
  },
  {
    "text": "demonstrate how to call the stage experiments api's directly in the notebook environment and visualize the",
    "start": "1595780",
    "end": "1601679"
  },
  {
    "text": "outputs in the notebook the first step is to create called sage maker experiments API and create a sage maker",
    "start": "1601679",
    "end": "1607980"
  },
  {
    "text": "experiments experiment with a suitable name and a suitable description next we",
    "start": "1607980",
    "end": "1614549"
  },
  {
    "text": "want to be able to track jobs that we have run previously for example the pre-processing job that we just ran in",
    "start": "1614549",
    "end": "1620460"
  },
  {
    "text": "particular we want to log certain parameters such as to train test split ratio potentially also any random seat",
    "start": "1620460",
    "end": "1626280"
  },
  {
    "text": "parameter that we used in order to generate our train and test splits such that we have reproducibility we also",
    "start": "1626280",
    "end": "1632370"
  },
  {
    "text": "want to be able to log for example the locations of our raw data our train data and our test data so that this can be",
    "start": "1632370",
    "end": "1638940"
  },
  {
    "text": "used later for auditability having now logged our pre processing job let's take",
    "start": "1638940",
    "end": "1644400"
  },
  {
    "text": "a look at how we can run the model training in Amazon Sage Maker thereat in fact multiple ways to run",
    "start": "1644400",
    "end": "1650640"
  },
  {
    "text": "model training in Amazon Sage Maker for example customers can choose from 17 built-in algorithms to train from such",
    "start": "1650640",
    "end": "1657450"
  },
  {
    "text": "as XG boost or linear learner additionally they can also bring their own scripts that are written in popular",
    "start": "1657450",
    "end": "1663480"
  },
  {
    "text": "frameworks such as tensorflow MX net or pi torch they can also bring their own containers and sage maker can",
    "start": "1663480",
    "end": "1670200"
  },
  {
    "text": "run the training job directly on those containers here we will demonstrate this",
    "start": "1670200",
    "end": "1675480"
  },
  {
    "text": "latter aspect as a lot of our customers already have their code packaged into containers that they are currently",
    "start": "1675480",
    "end": "1680820"
  },
  {
    "text": "running on Prem we have written a training script using XG boost which is available in our github repo and as a",
    "start": "1680820",
    "end": "1688080"
  },
  {
    "text": "first step we're going to put our code and all of its dependencies into a docker container and upload that docker",
    "start": "1688080",
    "end": "1693720"
  },
  {
    "text": "container into ECR and that is what this code snippet here is exactly doing once",
    "start": "1693720",
    "end": "1704340"
  },
  {
    "text": "our code has been successfully uploaded to ECR we are now ready to train our machine learning model the first thing",
    "start": "1704340",
    "end": "1710940"
  },
  {
    "text": "we want to do is we want to allow log our training job as a trial this way",
    "start": "1710940",
    "end": "1716250"
  },
  {
    "text": "different experiments that are different trials I eat different training runs that are conducted by different data",
    "start": "1716250",
    "end": "1722370"
  },
  {
    "text": "scientists or maybe even the same data scientist on the same data set can be logged under that experiment header that",
    "start": "1722370",
    "end": "1728549"
  },
  {
    "text": "we created earlier think of experiments as a folder that tracks all of your metadata associated with with your",
    "start": "1728549",
    "end": "1736110"
  },
  {
    "text": "different experiments and the trials are the different files that you put into that folder we are now going to add our",
    "start": "1736110",
    "end": "1743040"
  },
  {
    "text": "pre-processing track our pre-processing tracker earlier to the trial component",
    "start": "1743040",
    "end": "1748590"
  },
  {
    "text": "because that pre-processing job in fact led to the trial training job that we are about to run having now created the",
    "start": "1748590",
    "end": "1756210"
  },
  {
    "text": "trial we can now create the stage maker estimator object the first thing we need to do is we need to point the stage",
    "start": "1756210",
    "end": "1762180"
  },
  {
    "text": "maker estimator to the docker image that we just created above secondly we need to specify the instance type where we",
    "start": "1762180",
    "end": "1768780"
  },
  {
    "text": "want to run our training job the amount of time you want to wait for our training job to run and the output path",
    "start": "1768780",
    "end": "1775080"
  },
  {
    "text": "where we want to store our model artifacts a key advantage of using",
    "start": "1775080",
    "end": "1780570"
  },
  {
    "text": "Amazon stage maker for machine learning training is that we can save up to 90% and cost by using spot instances we",
    "start": "1780570",
    "end": "1788640"
  },
  {
    "text": "simply need to set the Train using spot instances flag to be true and specify a maximum amount of time we're willing to",
    "start": "1788640",
    "end": "1794850"
  },
  {
    "text": "wait for a spot instance to become available and stage maker will take care of the rest as before as we saw in our",
    "start": "1794850",
    "end": "1801180"
  },
  {
    "text": "previous assessing jobs in order to ensure that any network ingress and egress of traffic occurs through private",
    "start": "1801180",
    "end": "1807120"
  },
  {
    "text": "link we provide the subnet IDs as well as the security group IDs that we're going to be using to ensure our traffic",
    "start": "1807120",
    "end": "1813320"
  },
  {
    "text": "is secure we're also going to provide a KMS key to encrypt any EBS volumes that",
    "start": "1813320",
    "end": "1819330"
  },
  {
    "text": "are attached to our training instances note that Amazon sage maker takes security extremely seriously and has all",
    "start": "1819330",
    "end": "1826650"
  },
  {
    "text": "of the best security best practices already put into place for example different training jobs are isolated on",
    "start": "1826650",
    "end": "1833670"
  },
  {
    "text": "different instances in Amazon Sage Maker additionally by using security group",
    "start": "1833670",
    "end": "1839310"
  },
  {
    "text": "level isolation we can ensure the different jobs that are running different models do not have any way of",
    "start": "1839310",
    "end": "1845040"
  },
  {
    "text": "communicating with each other if multiple instances however are running the same job for example if you're",
    "start": "1845040",
    "end": "1851520"
  },
  {
    "text": "running distributed computing you can use the encrypt inter container traffic flag and set that to equal true to",
    "start": "1851520",
    "end": "1858930"
  },
  {
    "text": "ensure that any data in transit is also in crypted between our training containers",
    "start": "1858930",
    "end": "1864450"
  },
  {
    "text": "here since we're only using a single instance to run our training job we set",
    "start": "1864450",
    "end": "1869669"
  },
  {
    "text": "that flag to false however we do have this option to set that flag to true if we do choose to run distributed training",
    "start": "1869669",
    "end": "1875970"
  },
  {
    "text": "on a much larger data set having created our estimator we can now specify the",
    "start": "1875970",
    "end": "1881970"
  },
  {
    "text": "hyper parameters that we're going to be using to run our extra booster model we provided the training input which is the",
    "start": "1881970",
    "end": "1888299"
  },
  {
    "text": "location of the training data set that we saw earlier we create an experiment",
    "start": "1888299",
    "end": "1893610"
  },
  {
    "text": "config where as we call the fit function we provided the trial name where we want to log this particular trial run and we",
    "start": "1893610",
    "end": "1900840"
  },
  {
    "text": "give it a prominent display name such as training and now we're ready to kick off our training job and as we can see sage",
    "start": "1900840",
    "end": "1907950"
  },
  {
    "text": "maker takes care of our training run for us but a key thing to notice here is that while our entire training job in",
    "start": "1907950",
    "end": "1914970"
  },
  {
    "text": "fact took about 83 seconds we were only built for 32 of those seconds in other words we have managed to save over 60%",
    "start": "1914970",
    "end": "1922610"
  },
  {
    "text": "using spot instances as such using spot instances is always a best practice",
    "start": "1922610",
    "end": "1928200"
  },
  {
    "text": "wherever possible to run your training jobs having completed our training let's",
    "start": "1928200",
    "end": "1937470"
  },
  {
    "text": "take a look at what the experiments API has tracked we can do this by calling experiment analytics and specifying the",
    "start": "1937470",
    "end": "1945029"
  },
  {
    "text": "trial name that we used for our train job we can plot the results in a data",
    "start": "1945029",
    "end": "1950700"
  },
  {
    "text": "frame and as we can see the sage maker experiments has managed to track our pre processing job as well as our training",
    "start": "1950700",
    "end": "1957509"
  },
  {
    "text": "job let's dive a little bit deeper into what our training job was able to log this can be done by calling the",
    "start": "1957509",
    "end": "1963899"
  },
  {
    "text": "described trial component API and specifying the trial component name to be the name of our trial component",
    "start": "1963899",
    "end": "1970649"
  },
  {
    "text": "associated with our training job and the first thing we see is that our sage",
    "start": "1970649",
    "end": "1976500"
  },
  {
    "text": "maker experiments has logged all the metadata that's associated with our training run for example it has logged",
    "start": "1976500",
    "end": "1982740"
  },
  {
    "text": "whether our training job was completed or not it has logged the start time and the end time as well as the creation",
    "start": "1982740",
    "end": "1989009"
  },
  {
    "text": "time for that training job and potentially also who created that training if we chose to specify a name",
    "start": "1989009",
    "end": "1994500"
  },
  {
    "text": "here it has also tracked parameters such as the location of the docker image that",
    "start": "1994500",
    "end": "2000690"
  },
  {
    "text": "was used to run our training job as well as any metadata about associated with",
    "start": "2000690",
    "end": "2006210"
  },
  {
    "text": "the instances that was used to run our training it also tracked the hyper parameters that we use to run our",
    "start": "2006210",
    "end": "2011850"
  },
  {
    "text": "training jobs as well as other artifacts such as the location of our training data and the location of our model",
    "start": "2011850",
    "end": "2018780"
  },
  {
    "text": "outputs where our model artifacts are stored in a tar file finally if we",
    "start": "2018780",
    "end": "2024300"
  },
  {
    "text": "choose to do so we can also log any custom metrics and have those be tracked using sage maker experiments these",
    "start": "2024300",
    "end": "2030630"
  },
  {
    "text": "custom metrics can then be used to compare different training jobs against one another finally we want to take a",
    "start": "2030630",
    "end": "2040290"
  },
  {
    "text": "look at model explained ability with sage maker explain ability is a key regulatory requirement and often",
    "start": "2040290",
    "end": "2045870"
  },
  {
    "text": "dictates the model choices for models that can be put into production for tree based models such as XP boost we can",
    "start": "2045870",
    "end": "2051840"
  },
  {
    "text": "either use the built-in api's that will extract feature importances or we can use a library such as Shapp to obtain",
    "start": "2051840",
    "end": "2057510"
  },
  {
    "text": "feature importances all we need to first do is we need to pull in the model thar object from our Amazon s3 bucket into",
    "start": "2057510",
    "end": "2065370"
  },
  {
    "text": "our local notebook environment where our sage maker notebook is hosted we have written some custom utilities to do this",
    "start": "2065370",
    "end": "2070860"
  },
  {
    "text": "that are not shown but it will be available in the github repo associated with this notebook once we have pulled",
    "start": "2070860",
    "end": "2077128"
  },
  {
    "text": "in the model source files we can now unpack the model and we can see that indeed we trained an extra boost",
    "start": "2077129",
    "end": "2082648"
  },
  {
    "text": "classifier we can now use the built-in scikit-learn libraries to extract and plot the",
    "start": "2082649",
    "end": "2087990"
  },
  {
    "text": "feature importances directly from this XG boost model additionally we can also use a library",
    "start": "2087990",
    "end": "2094080"
  },
  {
    "text": "such as chef chef values can be calculated by computing two models one with and one without that feature",
    "start": "2094080",
    "end": "2099990"
  },
  {
    "text": "included and comparing the model outputs to assign an importance value to that feature this can be done by calling the",
    "start": "2099990",
    "end": "2108300"
  },
  {
    "text": "tree explainer API on the model and providing it our data set in order to extract the chef's values once the shop",
    "start": "2108300",
    "end": "2115260"
  },
  {
    "text": "value calculation is complete we can plot a summary plot either in the form of this bar graph that's shown here or",
    "start": "2115260",
    "end": "2121350"
  },
  {
    "text": "in the standard summary platform at that is provided by the shot by the shop",
    "start": "2121350",
    "end": "2127109"
  },
  {
    "text": "library to extract a relative importances of our different features this now concludes the data science",
    "start": "2127109",
    "end": "2133859"
  },
  {
    "text": "portion of our demo in particular we've covered security we have covered",
    "start": "2133859",
    "end": "2139559"
  },
  {
    "text": "notebook network isolation as well as network isolation during training and processing we have covered encryption of",
    "start": "2139559",
    "end": "2145559"
  },
  {
    "text": "data at rest and in transit we have covered data pre-processing using sage maker processing we have demonstrated",
    "start": "2145559",
    "end": "2151950"
  },
  {
    "text": "how you can use a BYO container approach to train your machine learning models as",
    "start": "2151950",
    "end": "2157140"
  },
  {
    "text": "well as achieve cost optimization and cost savings with using spot instances",
    "start": "2157140",
    "end": "2162270"
  },
  {
    "text": "and finally we have shown how you can use sage maker experiments to track the lineage associated with your model down",
    "start": "2162270",
    "end": "2169020"
  },
  {
    "text": "to any data artifacts as well as any model artifacts and model parameters and",
    "start": "2169020",
    "end": "2174210"
  },
  {
    "text": "model metrics that are used or generated during your training job and finally we have demonstrated explained ability and",
    "start": "2174210",
    "end": "2180720"
  },
  {
    "text": "interpretability using shaft values once",
    "start": "2180720",
    "end": "2187319"
  },
  {
    "text": "the training job is complete data scientists will typically upload their code to a code commit or git branch for",
    "start": "2187319",
    "end": "2193980"
  },
  {
    "text": "a review once the code has been checked in it can be tagged different tags can",
    "start": "2193980",
    "end": "2199349"
  },
  {
    "text": "be viewed for any associated metadata as shown here for example we have specified",
    "start": "2199349",
    "end": "2205770"
  },
  {
    "text": "a tagged version 0.1 to our data scientist workflow notebook that we",
    "start": "2205770",
    "end": "2211500"
  },
  {
    "text": "demonstrated earlier if we call the get show API we can see all the associated metadata associated with that particular",
    "start": "2211500",
    "end": "2218700"
  },
  {
    "text": "commit for example when that commit was made what was the message that was attached to that commit what was the IP",
    "start": "2218700",
    "end": "2225630"
  },
  {
    "text": "address that was used to make that particular commit as well as the details",
    "start": "2225630",
    "end": "2231240"
  },
  {
    "text": "associated with the code that we committed later on we shall also see how",
    "start": "2231240",
    "end": "2236430"
  },
  {
    "text": "we can track this tagged metadata using sage maker experiments as well here the",
    "start": "2236430",
    "end": "2242069"
  },
  {
    "text": "code will now undergo unit testing built testing and any other QA tests that are needed models might also undergo a",
    "start": "2242069",
    "end": "2248670"
  },
  {
    "text": "human-in-the-loop review to check the model outputs to feature importances etc once the code has passed any necessary",
    "start": "2248670",
    "end": "2255119"
  },
  {
    "text": "reviews we are now ready to move to the model deployment and monitoring portion which is typically",
    "start": "2255119",
    "end": "2260849"
  },
  {
    "text": "by a data engineer or by DevOps engineers for simplicity we will continue to demonstrate this in the",
    "start": "2260849",
    "end": "2266940"
  },
  {
    "text": "context of in notebook environment to demonstrate the model deployment portion of the workflow in this notebook we",
    "start": "2266940",
    "end": "2276479"
  },
  {
    "text": "demonstrate how to manually deploy a DevOps workflow by taking the model that we trained in the previous notebook and",
    "start": "2276479",
    "end": "2281880"
  },
  {
    "text": "deploying it into it into production and monitoring the model in point first we",
    "start": "2281880",
    "end": "2287130"
  },
  {
    "text": "will need to load in necessary libraries that we need to deploy our model as well as run any monitoring jobs that we are",
    "start": "2287130",
    "end": "2294420"
  },
  {
    "text": "going to run on our endpoint we will also import the network configurations as well as the KMS keys that are needed",
    "start": "2294420",
    "end": "2301170"
  },
  {
    "text": "to enforce security guard rails as well as the bucket name where our model artifacts have been stored the first",
    "start": "2301170",
    "end": "2309299"
  },
  {
    "text": "step then is to create a model object by providing it the location of our inference container that contains our",
    "start": "2309299",
    "end": "2316019"
  },
  {
    "text": "inference or serving script as well as the location of our trained model artifacts which contain the weights that",
    "start": "2316019",
    "end": "2322019"
  },
  {
    "text": "are associated with our model we also provided the VPC configuration to let Amazon sage maker know to create an en",
    "start": "2322019",
    "end": "2328739"
  },
  {
    "text": "i4 model hosting since we now want to",
    "start": "2328739",
    "end": "2333930"
  },
  {
    "text": "monitor our deployed models for any data drift or any concept drift that might be associated with the changing",
    "start": "2333930",
    "end": "2339779"
  },
  {
    "text": "relationships between the features and the labels in our incoming data set we are going to use a data capture",
    "start": "2339779",
    "end": "2345839"
  },
  {
    "text": "configuration the first thing we need to do is set the enable capture flag which by default is set to false we need to",
    "start": "2345839",
    "end": "2352229"
  },
  {
    "text": "change that default setting to true we also need to specify a sampling percentage for the purposes of",
    "start": "2352229",
    "end": "2357719"
  },
  {
    "text": "simplicity here we choose to sample all of the inference requests that are sent to our endpoint and we provided an s3",
    "start": "2357719",
    "end": "2364710"
  },
  {
    "text": "capture path where our sampled served requests will be stored next we create",
    "start": "2364710",
    "end": "2373979"
  },
  {
    "text": "an endpoint configuration where we specify the instance type that we use to deploy our model and the number of",
    "start": "2373979",
    "end": "2379710"
  },
  {
    "text": "instances that we want to use we can also choose an a/b testing scenario where we can route a part of our traffic",
    "start": "2379710",
    "end": "2386789"
  },
  {
    "text": "to a different model however in this case since we only have a single model",
    "start": "2386789",
    "end": "2392099"
  },
  {
    "text": "that we are deploying into production we're going to send all for network traffic to that model and we're also going to specify the data",
    "start": "2392099",
    "end": "2398740"
  },
  {
    "text": "capture path that we had specified earlier now we are ready to create our",
    "start": "2398740",
    "end": "2405550"
  },
  {
    "text": "endpoint sage maker will manage the task of creating an HTTP endpoint for us that",
    "start": "2405550",
    "end": "2411670"
  },
  {
    "text": "can serve inference requests to a client's application let's test that this endpoint has indeed been created as",
    "start": "2411670",
    "end": "2418420"
  },
  {
    "text": "this creation AR n shows the first thing we can do is we can call the real-time",
    "start": "2418420",
    "end": "2423640"
  },
  {
    "text": "predictor API and we can provided a sample from our test data so in this",
    "start": "2423640",
    "end": "2429130"
  },
  {
    "text": "case we're just pulling 10 rows from our test data in this test sample dot CSV file and we're calling this predictor",
    "start": "2429130",
    "end": "2435370"
  },
  {
    "text": "dot predict a bi by passing it the payload from our sample dataset and if this can predict successfully then we",
    "start": "2435370",
    "end": "2443320"
  },
  {
    "text": "should see a done command after all of the predictions have been run and indeed that's what we see here so let's now",
    "start": "2443320",
    "end": "2449920"
  },
  {
    "text": "take a look at the captured data outputs",
    "start": "2449920",
    "end": "2454049"
  },
  {
    "text": "we first see that the inference outputs have been stored in the form of a JSON",
    "start": "2455130",
    "end": "2460420"
  },
  {
    "text": "lines formatted file in Amazon s3 let's quickly navigate to that bucket where",
    "start": "2460420",
    "end": "2465700"
  },
  {
    "text": "those outputs are stored as before we want to manage the security of that",
    "start": "2465700",
    "end": "2472510"
  },
  {
    "text": "bucket by enforcing things like encryption addressed as well as stringent bucket policies since we're",
    "start": "2472510",
    "end": "2477790"
  },
  {
    "text": "using the same data team bucket in order to store our model monitoring outputs our data capture outputs in this case we",
    "start": "2477790",
    "end": "2484360"
  },
  {
    "text": "shall not demonstrate those encryption at rest as well as those bucket policies earlier as we have already done that we",
    "start": "2484360",
    "end": "2490390"
  },
  {
    "text": "see however that the data capture config creates a folder in our s3 bucket called monitoring if we investigate this deeper",
    "start": "2490390",
    "end": "2499859"
  },
  {
    "text": "we see that it has logged a folder under the name of the training job or the endpoint configuration or the endpoint",
    "start": "2500100",
    "end": "2506860"
  },
  {
    "text": "that we used for our particular for that particular model we have chosen to log",
    "start": "2506860",
    "end": "2513730"
  },
  {
    "text": "all the traffic and now we have specific folders associated with the actual time",
    "start": "2513730",
    "end": "2520960"
  },
  {
    "text": "stamps where those inference requests were sent if we navigate to one of these folders we indeed see that we have",
    "start": "2520960",
    "end": "2527460"
  },
  {
    "text": "inference requests in the form of this JSON lines format let's now jump back into the notebook and take a look at one",
    "start": "2527460",
    "end": "2533490"
  },
  {
    "text": "of these inference requests and see what the GUP see what it looks like here we",
    "start": "2533490",
    "end": "2540960"
  },
  {
    "text": "see that our data capture config has actually captured both the input that",
    "start": "2540960",
    "end": "2546569"
  },
  {
    "text": "was sent to our payload or to our endpoint as well as the final output in this case our model predicted that the",
    "start": "2546569",
    "end": "2553500"
  },
  {
    "text": "transaction itself that the individual would not default on their credit card payment based on these inputs that were",
    "start": "2553500",
    "end": "2560190"
  },
  {
    "text": "sent to the model endpoint this can be used for model monitoring to ensure that the endpoints or that the features that",
    "start": "2560190",
    "end": "2567270"
  },
  {
    "text": "are being sent to our model for inference are indeed the correct ones that the model was actually trained on",
    "start": "2567270",
    "end": "2575000"
  },
  {
    "text": "most client requests to endpoints will not be made via notebooks to demonstrate",
    "start": "2577579",
    "end": "2583079"
  },
  {
    "text": "the typical separation between API requests and model endpoints we infused API gateway and 11 a function that is",
    "start": "2583079",
    "end": "2590490"
  },
  {
    "text": "sitting in our private vbc in front of our endpoint to let external client applications send API requests to our",
    "start": "2590490",
    "end": "2598230"
  },
  {
    "text": "production model this could be a situation for online inferencing for example where someone swipes their",
    "start": "2598230",
    "end": "2603569"
  },
  {
    "text": "credit card at a store when that transaction is approved or denied by first calling an EMA model that ensures",
    "start": "2603569",
    "end": "2609720"
  },
  {
    "text": "that the transaction isn't fraudulent to demonstrate this we have placed our API gateway behind an Internet enabled me PC",
    "start": "2609720",
    "end": "2616740"
  },
  {
    "text": "and have used postman to send an API requests to the model for example I'm",
    "start": "2616740",
    "end": "2622410"
  },
  {
    "text": "sending this data payload to the model and I'm gonna click on the send button here and when I do that I get an API",
    "start": "2622410",
    "end": "2630750"
  },
  {
    "text": "output those are very similar to the output that we saw from a data capture configuration before for example we see both the input data",
    "start": "2630750",
    "end": "2638099"
  },
  {
    "text": "that we sent to the model as well as the output that was generated by the train model now for internal requests that are",
    "start": "2638099",
    "end": "2645960"
  },
  {
    "text": "made for internal clients this can all be done within an intimate disabled VPC",
    "start": "2645960",
    "end": "2652460"
  },
  {
    "text": "now that we have successfully deployed our model as well as captured the inference data we're ready to monitor",
    "start": "2656420",
    "end": "2663150"
  },
  {
    "text": "our model model monitoring is a service for checking our deployed end points against data or concept drift from",
    "start": "2663150",
    "end": "2669450"
  },
  {
    "text": "incoming data in particular to notify administrators or engineers of any performance degradation associated with",
    "start": "2669450",
    "end": "2675779"
  },
  {
    "text": "our models as well as when it may be time to retrain DML models in order to",
    "start": "2675779",
    "end": "2681059"
  },
  {
    "text": "do so first we need to specify the buckets where our monitoring outputs will be stored let's take a look at this",
    "start": "2681059",
    "end": "2688230"
  },
  {
    "text": "particular bucket that we have created for storing our model outputs here we",
    "start": "2688230",
    "end": "2694440"
  },
  {
    "text": "have the bucket that's storing our model monitoring outputs as before we want to",
    "start": "2694440",
    "end": "2699569"
  },
  {
    "text": "ensure that this bucket has default kms encryption to ensure encryption of data addressed as well as bucket policies",
    "start": "2699569",
    "end": "2707549"
  },
  {
    "text": "that are attached to it that enforce the enforce of EPC endpoint in order to",
    "start": "2707549",
    "end": "2714329"
  },
  {
    "text": "control the ingress and egress of data in and out of the bucket having",
    "start": "2714329",
    "end": "2721710"
  },
  {
    "text": "specified the locations where we want our monitoring jobs to store their outputs let's now start a baselining job",
    "start": "2721710",
    "end": "2728059"
  },
  {
    "text": "Sage Maker will automatically handle the provisioning of the underlying infants",
    "start": "2728059",
    "end": "2733140"
  },
  {
    "text": "instance that is required to run the baselining job but we need we need to provide it the instance type as well as",
    "start": "2733140",
    "end": "2739529"
  },
  {
    "text": "our training data with the headers that we created earlier sage maker uses the DQ library to",
    "start": "2739529",
    "end": "2746460"
  },
  {
    "text": "extract statistics from the data the DQ library is available for customers to",
    "start": "2746460",
    "end": "2751799"
  },
  {
    "text": "use and can be found on this link that customers can use to peruse the library",
    "start": "2751799",
    "end": "2756900"
  },
  {
    "text": "as well as the source code in greater detail let's now see what the baselining",
    "start": "2756900",
    "end": "2763559"
  },
  {
    "text": "job has extracted as the baseline for our data notice that the baselining outputs are",
    "start": "2763559",
    "end": "2769349"
  },
  {
    "text": "located in this particular output folder which is in the same bucket that we demonstrated earlier with the prefix",
    "start": "2769349",
    "end": "2776130"
  },
  {
    "text": "baselining / results let's go take a look so here I am back in that model",
    "start": "2776130",
    "end": "2783180"
  },
  {
    "text": "team sage maker demo bucket and I have a prefix under the sage maker",
    "start": "2783180",
    "end": "2791270"
  },
  {
    "text": "demo I can click on the training job that's associated with this particular",
    "start": "2791270",
    "end": "2796630"
  },
  {
    "text": "endpoint and you see that the folder baselining is located here if I now look at the results of folder I see that I",
    "start": "2796630",
    "end": "2803630"
  },
  {
    "text": "have a constraints a JSON and a statistics JSON file that was created for me after I ran the baseline job",
    "start": "2803630",
    "end": "2811010"
  },
  {
    "text": "let's now go back to the sage maker notebook and take a look at what these JSON files look like so here we have the",
    "start": "2811010",
    "end": "2824150"
  },
  {
    "text": "statistics JSON file and we see that for each of the features as well as the labels this file has inferred a",
    "start": "2824150",
    "end": "2831290"
  },
  {
    "text": "particular data type as well as extracted some common statistics associated with those particular",
    "start": "2831290",
    "end": "2836540"
  },
  {
    "text": "features for example the number of missing values the min and the max the",
    "start": "2836540",
    "end": "2842300"
  },
  {
    "text": "standard deviations etc all these additional statistics are visualized",
    "start": "2842300",
    "end": "2849350"
  },
  {
    "text": "here in a data frame format for customers to look at in greater detail additionally it has also extracted some",
    "start": "2849350",
    "end": "2856400"
  },
  {
    "text": "constraints that it's now going to look for in particular it's going to look to see whether certain features are allowed",
    "start": "2856400",
    "end": "2862700"
  },
  {
    "text": "to be non-negative or not and we can see that while certain features are indeed not allowed to be non-negative others",
    "start": "2862700",
    "end": "2869180"
  },
  {
    "text": "are in our training data set and any violation against these for example will",
    "start": "2869180",
    "end": "2874850"
  },
  {
    "text": "be flagged by the model monitoring service it has also inferred the data types that are associated with those",
    "start": "2874850",
    "end": "2880520"
  },
  {
    "text": "individual features once the baseline",
    "start": "2880520",
    "end": "2886369"
  },
  {
    "text": "job is complete we need to define a monitoring schedule to run the model monitoring this can be done at hourly",
    "start": "2886369",
    "end": "2892700"
  },
  {
    "text": "weekly or daily and for this demo you're going to choose an hourly cadence we are",
    "start": "2892700",
    "end": "2898550"
  },
  {
    "text": "also going to let model monitor know to publish metrics to cloud watch this can",
    "start": "2898550",
    "end": "2903740"
  },
  {
    "text": "be used in generate cloud watch alarms that notify administrators or engineers of any model performance degradation",
    "start": "2903740",
    "end": "2910000"
  },
  {
    "text": "once we create this model monitoring schedule we can generate some artificial",
    "start": "2910000",
    "end": "2915020"
  },
  {
    "text": "traffic for the purposes of this demo in particular we are going to look",
    "start": "2915020",
    "end": "2921380"
  },
  {
    "text": "the signs on two of our key demographic features and store this fake data set in",
    "start": "2921380",
    "end": "2926569"
  },
  {
    "text": "s3 and we're going to invoke our endpoint constantly with this bogus data",
    "start": "2926569",
    "end": "2933819"
  },
  {
    "text": "so here we see that indeed our model monitoring schedule has been scheduled",
    "start": "2935170",
    "end": "2941569"
  },
  {
    "text": "as expected and we are now ready to look at some of the model monitor outputs the",
    "start": "2941569",
    "end": "2947690"
  },
  {
    "text": "first thing you might see when you call the model monitor list executions API is that initially you might find that no",
    "start": "2947690",
    "end": "2954109"
  },
  {
    "text": "executions were found for that particular schedule and the reason for that is that we have kicked off an",
    "start": "2954109",
    "end": "2959960"
  },
  {
    "text": "hourly schedule and therefore the schedules the model monitoring jobs only run on the hour with a 20-minute buffer",
    "start": "2959960",
    "end": "2967819"
  },
  {
    "text": "so we may have to wait for a while before we see an execution to happen however after some amount of time indeed",
    "start": "2967819",
    "end": "2974869"
  },
  {
    "text": "we should start to see some metadata associated with the monitoring executions in particular the model",
    "start": "2974869",
    "end": "2981619"
  },
  {
    "text": "monitoring service will track things like the locations of the constraints file as well as the statistics file that",
    "start": "2981619",
    "end": "2987140"
  },
  {
    "text": "was used to generate any violations it will also track the instance types that",
    "start": "2987140",
    "end": "2993890"
  },
  {
    "text": "were used to run that particular monitoring job it'll track whether",
    "start": "2993890",
    "end": "3000249"
  },
  {
    "text": "CloudWatch metrics have been weather published CloudWatch metrics has been enabled and it'll track the status of",
    "start": "3000249",
    "end": "3008799"
  },
  {
    "text": "that monitoring job and in this case the processing job status has been completed and indeed it has been completed with a",
    "start": "3008799",
    "end": "3015219"
  },
  {
    "text": "particular violation being found now if you run this command over and over and you let the monitoring jobs happen on",
    "start": "3015219",
    "end": "3021339"
  },
  {
    "text": "the hour you might find that more and more violations have been found with your data so let's take a look at one of",
    "start": "3021339",
    "end": "3028599"
  },
  {
    "text": "these violation jobs that we found here we see that the model monitoring service",
    "start": "3028599",
    "end": "3034329"
  },
  {
    "text": "generates a report that report is located in our s3 bucket from earlier",
    "start": "3034329",
    "end": "3039489"
  },
  {
    "text": "with the prefix underscore with the prefix slash reports so let's go take a",
    "start": "3039489",
    "end": "3044769"
  },
  {
    "text": "look at that so here I am back in that s3 bucket from earlier and indeed I have",
    "start": "3044769",
    "end": "3052180"
  },
  {
    "text": "this reports folder so look at the reports folder and look at the reports associated with my training",
    "start": "3052180",
    "end": "3057470"
  },
  {
    "text": "job I have my model monitoring schedule from before and then I have folders that",
    "start": "3057470",
    "end": "3064130"
  },
  {
    "text": "are based on the time stamp that's associated with when that model monitoring job ran and here we see that",
    "start": "3064130",
    "end": "3069320"
  },
  {
    "text": "we have that same constraints file in statistics file from earlier but now we have a new file that has been populated",
    "start": "3069320",
    "end": "3074720"
  },
  {
    "text": "here which is our constraints violations JSON which captures the violations that we have found against those constraints",
    "start": "3074720",
    "end": "3082270"
  },
  {
    "text": "let's now take a look at what one of these files looks like in our sage maker notebook so back in our sage maker",
    "start": "3082270",
    "end": "3089570"
  },
  {
    "text": "notebook environment we can parse one of these JSON files and let's take a look at the outputs so here we see that our",
    "start": "3089570",
    "end": "3098750"
  },
  {
    "text": "monitoring job has detected some baseline drift from our baseline for a",
    "start": "3098750",
    "end": "3105170"
  },
  {
    "text": "number of different features but in particular the largest baseline drift was detected for the two features where",
    "start": "3105170",
    "end": "3111530"
  },
  {
    "text": "we decided to flip the signs to minus a to minus of their values from before and",
    "start": "3111530",
    "end": "3117740"
  },
  {
    "text": "the largest drift was observed for those particular two features as a final",
    "start": "3117740",
    "end": "3127220"
  },
  {
    "text": "requirement most regulated industries have is reproducibility this means that they would like to log all the metadata",
    "start": "3127220",
    "end": "3134120"
  },
  {
    "text": "that's associated with our endpoints and our trained models in particular compliance requirements may dictate that",
    "start": "3134120",
    "end": "3140930"
  },
  {
    "text": "you not only need to track the lineage of your endpoints down to the raw data level location and the datum the",
    "start": "3140930",
    "end": "3147260"
  },
  {
    "text": "metadata associated with your raw data but actually also down to the code level where the code that was used to train",
    "start": "3147260",
    "end": "3153770"
  },
  {
    "text": "and deploy your models has been tracked by the source or version control tools",
    "start": "3153770",
    "end": "3158990"
  },
  {
    "text": "that you are using here we can use the familiar tracker API from sage maker",
    "start": "3158990",
    "end": "3164720"
  },
  {
    "text": "experiments to track the metadata associated with the git commits from our source code and our deployment code that",
    "start": "3164720",
    "end": "3170960"
  },
  {
    "text": "we have just shown not shown in this demo is that I have gone ahead and uploaded the code that generated this",
    "start": "3170960",
    "end": "3178610"
  },
  {
    "text": "particular notebook to the private code commit repo that I've created I've also",
    "start": "3178610",
    "end": "3183830"
  },
  {
    "text": "written some code here that pulls from commit from the code commit api's any",
    "start": "3183830",
    "end": "3190330"
  },
  {
    "text": "metadata that's associated with the notebooks that I have committed I can",
    "start": "3190330",
    "end": "3196930"
  },
  {
    "text": "now visualize this in the sage maker experiments using that experiments analytics API from before and here we",
    "start": "3196930",
    "end": "3205210"
  },
  {
    "text": "see that we have now captured the end-to-end data scientist as well as the",
    "start": "3205210",
    "end": "3210700"
  },
  {
    "text": "DevOps workflow in our in this particular data frame indeed we have captured the source control lineage",
    "start": "3210700",
    "end": "3217330"
  },
  {
    "text": "using sage maker sage maker experiments we've captured the author name as well as the commit ID",
    "start": "3217330",
    "end": "3223510"
  },
  {
    "text": "that was associated with that particular commit a message or any date and epoch format those associated with that",
    "start": "3223510",
    "end": "3230020"
  },
  {
    "text": "particular commit we have tracked our processing job we have logged our training job as I showed earlier with",
    "start": "3230020",
    "end": "3236170"
  },
  {
    "text": "all the training metadata for example the instance types used to train the model the hyper parameters potentially",
    "start": "3236170",
    "end": "3242950"
  },
  {
    "text": "the model metrics as well if we choose to do so and finally we have also tracked the production endpoint lineage",
    "start": "3242950",
    "end": "3249280"
  },
  {
    "text": "for example we have tracked the author name as well as the commit ID associated with the code that led to the production",
    "start": "3249280",
    "end": "3257050"
  },
  {
    "text": "deployment of the production endpoint as well as any messages and timestamps for when that code was deployed into",
    "start": "3257050",
    "end": "3263590"
  },
  {
    "text": "production this now concludes the manual deployment or the DevOps workflow that",
    "start": "3263590",
    "end": "3269770"
  },
  {
    "text": "we wanted to demonstrate from Sage Maker notebook so let's recap quickly about the various things that we have",
    "start": "3269770",
    "end": "3275860"
  },
  {
    "text": "demonstrated in this notebook in particular we demonstrated how to deploy securely deploy a trained model using",
    "start": "3275860",
    "end": "3284020"
  },
  {
    "text": "the same network configuration as a data scientist portion of the workflow we have demonstrated auditability and",
    "start": "3284020",
    "end": "3289600"
  },
  {
    "text": "reproducibility by using sage maker experiments to track the lineage of our processing jobs our model artifacts as",
    "start": "3289600",
    "end": "3295480"
  },
  {
    "text": "well as the git commits that are associated with our deployed model endpoints we have demonstrated how to",
    "start": "3295480",
    "end": "3302830"
  },
  {
    "text": "deploy a model endpoint and how to query that endpoint or send inferences to that endpoint both from a sage maker notebook",
    "start": "3302830",
    "end": "3309370"
  },
  {
    "text": "as well as using an external tool such as postman we have set up model",
    "start": "3309370",
    "end": "3315010"
  },
  {
    "text": "monitoring and we have demonstrated how we can monitor our models and for any data or concept drift I will now",
    "start": "3315010",
    "end": "3321509"
  },
  {
    "text": "pass it back to caste who will give us the best practices on the last four objectives that we set at the beginning",
    "start": "3321509",
    "end": "3327479"
  },
  {
    "text": "thank you very much for watching this demo Thank You Stefan",
    "start": "3327479",
    "end": "3335039"
  },
  {
    "text": "truly fantastic demo thank you as you said I'll wrap it up today by giving you",
    "start": "3335039",
    "end": "3340589"
  },
  {
    "text": "the best practices on the last four objectives we stated at the beginning on the traceability and auditability front",
    "start": "3340589",
    "end": "3347239"
  },
  {
    "text": "all states make your API calls as well as the s3 data events are being tracked",
    "start": "3347239",
    "end": "3353369"
  },
  {
    "text": "by cloud trail and you can audit them anytime also all Jupiter logs are sent to cloud",
    "start": "3353369",
    "end": "3360059"
  },
  {
    "text": "boards where you can set triggers for automated responses to any state changes",
    "start": "3360059",
    "end": "3365099"
  },
  {
    "text": "and lastly as shown in the demo do leverage the sage make your experiments",
    "start": "3365099",
    "end": "3370229"
  },
  {
    "text": "and your version control system to achieve end-to-end traceability of the metadata of the processing job the",
    "start": "3370229",
    "end": "3376739"
  },
  {
    "text": "training job and the code behind the deployed model on the explained ability",
    "start": "3376739",
    "end": "3383130"
  },
  {
    "text": "and readability front as the farm showed us you can quickly address regulatory audits by visualizing and saving",
    "start": "3383130",
    "end": "3390269"
  },
  {
    "text": "insightful metrics like feature importance and sub values right from your notebook just by calling the",
    "start": "3390269",
    "end": "3397349"
  },
  {
    "text": "feature importance fraction or by installing the supply berry the exact same outcome though can be achieved by",
    "start": "3397349",
    "end": "3404729"
  },
  {
    "text": "using the same major the buyer API which supports Xzibit on the real-time",
    "start": "3404729",
    "end": "3413459"
  },
  {
    "text": "monitoring front remember that you can monitor at any frequency the built-in",
    "start": "3413459",
    "end": "3418920"
  },
  {
    "text": "algorithms but any other container that you bring in addition apart from the",
    "start": "3418920",
    "end": "3424469"
  },
  {
    "text": "built-in rules like completeness tag or datatype tag that model monitoring comes",
    "start": "3424469",
    "end": "3429479"
  },
  {
    "text": "with you can define your own rules that will automatically check for and lastly",
    "start": "3429479",
    "end": "3435779"
  },
  {
    "text": "do you use lambda functions to automate retraining based on the CloudWatch",
    "start": "3435779",
    "end": "3441390"
  },
  {
    "text": "metrics that model monitoring emits or even immediately stopping inference",
    "start": "3441390",
    "end": "3446400"
  },
  {
    "text": "endpoint depending on the violation type or the data drift magnitude",
    "start": "3446400",
    "end": "3452310"
  },
  {
    "text": "last but certainly not the least on the reproducibility front once you have",
    "start": "3452310",
    "end": "3457420"
  },
  {
    "text": "enabled sales make your experiments and your version control system you can track all the limits of the model",
    "start": "3457420",
    "end": "3463600"
  },
  {
    "text": "version that was deployed and last rerun the same pre-processing pipeline on the",
    "start": "3463600",
    "end": "3468730"
  },
  {
    "text": "same data and then use the same model with the same hyper parameters of course",
    "start": "3468730",
    "end": "3474550"
  },
  {
    "text": "do remember that due to the randomness entailed in the optimizer implementing",
    "start": "3474550",
    "end": "3479560"
  },
  {
    "text": "so has the gradient descent exactly same process and artifacts might have slightly different prediction outcomes",
    "start": "3479560",
    "end": "3486400"
  },
  {
    "text": "and to wrap this up we hope we managed to address some of your key questions on",
    "start": "3486400",
    "end": "3492820"
  },
  {
    "text": "how to build compliant machine learning workflows with say its maker and will be happy to stay in touch about any",
    "start": "3492820",
    "end": "3499300"
  },
  {
    "text": "additional questions feel free to review the github repo that contains a notebook safe and just walked us through and do",
    "start": "3499300",
    "end": "3505900"
  },
  {
    "text": "reach out to our emails shown here with the handles if you need more help thank you very very much for your time and",
    "start": "3505900",
    "end": "3512530"
  },
  {
    "text": "looking forward to hearing from you",
    "start": "3512530",
    "end": "3516180"
  }
]