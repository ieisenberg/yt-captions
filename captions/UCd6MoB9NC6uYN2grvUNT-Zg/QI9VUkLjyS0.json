[
  {
    "start": "0",
    "end": "115000"
  },
  {
    "text": "so thanks everyone for coming we'll get started so today we're going to talk",
    "start": "30",
    "end": "5190"
  },
  {
    "text": "about media intelligence for the cloud using Amazon AI services a lot of this",
    "start": "5190",
    "end": "10860"
  },
  {
    "text": "will be about recognition but there are additional services obviously that can be used to extend say deep learning",
    "start": "10860",
    "end": "18890"
  },
  {
    "text": "pipelines from media in the cloud so my name is Constantine films I'm a",
    "start": "18890",
    "end": "24269"
  },
  {
    "text": "principal si I work on the media and entertainment team and I'm here with Dean Perrine who's a VP of technical",
    "start": "24269",
    "end": "32550"
  },
  {
    "text": "solutions at Fox networks so we have a actual use case that I hope you'll find interesting and just for the agenda Dino",
    "start": "32550",
    "end": "42059"
  },
  {
    "text": "intro that use case and from there on we'll talk about the architecture the components that were used to build us",
    "start": "42059",
    "end": "48750"
  },
  {
    "text": "and then future looking you know what could be used to extend this going",
    "start": "48750",
    "end": "54570"
  },
  {
    "text": "forward from a deep learning perspective so with that handed over to Dean hi there I'm Dean",
    "start": "54570",
    "end": "60809"
  },
  {
    "text": "and as Khan said we'd like to talk about media intelligence specifically enriching metadata and unleashing the",
    "start": "60809",
    "end": "67770"
  },
  {
    "text": "value of content some of you may have heard of the company that I'm going to highlight today as they've been around",
    "start": "67770",
    "end": "73799"
  },
  {
    "text": "for a little while as you can imagine over the last 129 years",
    "start": "73799",
    "end": "79979"
  },
  {
    "text": "National Geographic has created a little bit of content National Geographic has",
    "start": "79979",
    "end": "85650"
  },
  {
    "text": "shown the world through wonderful imagery video and text publications",
    "start": "85650",
    "end": "91130"
  },
  {
    "text": "National Geographic also permits the user contribution of photos through applications such as your shot there's",
    "start": "91130",
    "end": "99390"
  },
  {
    "text": "one thing that we can be sure of is that there will always be more photos taken than the year prior in other words",
    "start": "99390",
    "end": "104939"
  },
  {
    "text": "there's always more photos uploaded and more photos contributed to the National Geographic archives that being said the",
    "start": "104939",
    "end": "114570"
  },
  {
    "text": "opportunity so as we migrate petabytes of images into AWS we can ask ourselves",
    "start": "114570",
    "end": "121610"
  },
  {
    "start": "115000",
    "end": "115000"
  },
  {
    "text": "how can we then enrich our metadata once in AWS how can we unleash the value of",
    "start": "121610",
    "end": "129170"
  },
  {
    "text": "129 years of content as you can imagine there's some differences in the metadata",
    "start": "129170",
    "end": "134840"
  },
  {
    "text": "captured in 1890 versus 2017 so how can",
    "start": "134840",
    "end": "140180"
  },
  {
    "text": "we create that baseline so for instance some images may not even have any metadata as they should associated with",
    "start": "140180",
    "end": "146510"
  },
  {
    "text": "them or it may be vague how can we just create that baseline across all of our",
    "start": "146510",
    "end": "151910"
  },
  {
    "text": "images in the archive National Geographic has some unique challenges",
    "start": "151910",
    "end": "157880"
  },
  {
    "text": "when it comes to automated metadata generations such as niche image categories where high confidence labels",
    "start": "157880",
    "end": "166220"
  },
  {
    "text": "that are returned by recognition may not always equal high accuracy labels how",
    "start": "166220",
    "end": "173090"
  },
  {
    "text": "can we upscale images without introducing artifacts or noise and upscaling for the purpose of recognizing",
    "start": "173090",
    "end": "180769"
  },
  {
    "text": "what's in the image how can we colorize black-and-white images with little or no",
    "start": "180769",
    "end": "187790"
  },
  {
    "text": "historical context was the sash red or was it blue we don't know or recognition",
    "start": "187790",
    "end": "194299"
  },
  {
    "text": "may not know speaking of historical context for historical images let's say of clothing or maps or structures that",
    "start": "194299",
    "end": "203390"
  },
  {
    "text": "may not exist any longer how can we how can we then recognize those photos imagine trying to run the photo shown on",
    "start": "203390",
    "end": "210950"
  },
  {
    "text": "top the the microscopic photo through recognition what do you think it would",
    "start": "210950",
    "end": "215959"
  },
  {
    "text": "say the expectation of National Geographic is that of a scientific",
    "start": "215959",
    "end": "223130"
  },
  {
    "text": "journal so you can imagine that high accuracy is required National Geographic",
    "start": "223130",
    "end": "229700"
  },
  {
    "start": "229000",
    "end": "229000"
  },
  {
    "text": "is undergoing a digital transformation so they are migrating a lot of their",
    "start": "229700",
    "end": "235069"
  },
  {
    "text": "Nazz faz san LTO tape library systems that have images and video on them into",
    "start": "235069",
    "end": "242150"
  },
  {
    "text": "AWS into systems such as s3 or services such as s3 they are transforming their",
    "start": "242150",
    "end": "247940"
  },
  {
    "text": "editing in publishing platforms let's say to run on GPU back ec2 instances and",
    "start": "247940",
    "end": "254209"
  },
  {
    "text": "workspaces alongside the content they're also transforming their video streaming",
    "start": "254209",
    "end": "261480"
  },
  {
    "text": "and mobile applications and by doing this the hope is is to take advantage of",
    "start": "261480",
    "end": "268250"
  },
  {
    "text": "you know next-generation services such as machine learning and recognition in other words some of the cool stuff that",
    "start": "268250",
    "end": "274920"
  },
  {
    "text": "Khan's going to talk about next Thanks so those of you that have used",
    "start": "274920",
    "end": "282060"
  },
  {
    "start": "278000",
    "end": "278000"
  },
  {
    "text": "recognition know that they're a variety of options for processing media content",
    "start": "282060",
    "end": "287310"
  },
  {
    "text": "right so if you're taking video clips extracting frames and then funneling those to recognition or you have like in",
    "start": "287310",
    "end": "293730"
  },
  {
    "text": "this particular use case may be the vast majority of the content is image based",
    "start": "293730",
    "end": "298880"
  },
  {
    "text": "and some of the content is then also media based it needs to get analyzed but",
    "start": "298880",
    "end": "304320"
  },
  {
    "text": "if we look at the type the content body if you will for which we're you know processing now the important things are",
    "start": "304320",
    "end": "311790"
  },
  {
    "text": "for example object and scene detection a lot of the other features that recognition provides maybe as a",
    "start": "311790",
    "end": "318960"
  },
  {
    "text": "secondary sort of processing phase in what we're doing so say for example",
    "start": "318960",
    "end": "324980"
  },
  {
    "text": "celebrity recognition or image moderation are not really key to this type of workload but text attack",
    "start": "324980",
    "end": "332550"
  },
  {
    "text": "detection that some of you may have noticed we released last week this can be used for say historical context",
    "start": "332550",
    "end": "339240"
  },
  {
    "text": "images where we can't identify landmarks but potentially we can use OCR and text",
    "start": "339240",
    "end": "344310"
  },
  {
    "text": "recognition to then do a second research savior archives as one example so if we",
    "start": "344310",
    "end": "351990"
  },
  {
    "text": "break this down to the simple components of this pipeline this is probably part",
    "start": "351990",
    "end": "357660"
  },
  {
    "text": "of a larger pipeline like a media supply chain pipeline for example and we're basically taking deep learning metadata",
    "start": "357660",
    "end": "365910"
  },
  {
    "text": "enrichment and also services like recognition and injecting them into the",
    "start": "365910",
    "end": "372720"
  },
  {
    "text": "pipeline as the content flows through it so there's obviously the notion of 129",
    "start": "372720",
    "end": "378270"
  },
  {
    "text": "years of content but there's new content arriving everyday so we need to be able",
    "start": "378270",
    "end": "383370"
  },
  {
    "text": "to ingest store then run analysis on that and then deliver that content to",
    "start": "383370",
    "end": "389370"
  },
  {
    "text": "say API endpoints maybe business users or consumer users",
    "start": "389370",
    "end": "394730"
  },
  {
    "text": "so this this section I'm going to talk mainly about recognition and what value",
    "start": "394730",
    "end": "400640"
  },
  {
    "text": "that provides here so if you look at just the object and scene detection which is the majority of the use case",
    "start": "400640",
    "end": "407150"
  },
  {
    "start": "402000",
    "end": "402000"
  },
  {
    "text": "here the important thing here is that for like a National Geographic use case",
    "start": "407150",
    "end": "413740"
  },
  {
    "text": "identifying objects like say you know skateboard or you know bench things like",
    "start": "413740",
    "end": "420350"
  },
  {
    "text": "that there's use for that in terms of tagging metadata in say asset management systems",
    "start": "420350",
    "end": "426620"
  },
  {
    "text": "but really the other things are also equally important so recognition",
    "start": "426620",
    "end": "431660"
  },
  {
    "text": "provides object detection and labeling with a confidence score some like 99%",
    "start": "431660",
    "end": "437030"
  },
  {
    "text": "confidence that this is issue but also scenes and concepts so if you think of",
    "start": "437030",
    "end": "443930"
  },
  {
    "text": "like a boat on water it may tag boat water but then it may also tag as seen",
    "start": "443930",
    "end": "449930"
  },
  {
    "text": "as being ocean and then the concept has been sailing so those like second and",
    "start": "449930",
    "end": "455690"
  },
  {
    "text": "third dimensions are really important to be able to process those it's also",
    "start": "455690",
    "end": "461450"
  },
  {
    "text": "complex to know which concepts and words are actually you know scenes and",
    "start": "461450",
    "end": "467570"
  },
  {
    "text": "concepts right and we can use natural language processing to extract that kind of data from all of the data that's",
    "start": "467570",
    "end": "474560"
  },
  {
    "text": "returned by recognition and then the other thing that's important is the service integration component so if",
    "start": "474560",
    "end": "482270"
  },
  {
    "start": "477000",
    "end": "477000"
  },
  {
    "text": "we're utilizing a managed service that's given us this deterministic consistent",
    "start": "482270",
    "end": "487400"
  },
  {
    "text": "performance as we're piping all of this content through it we want the labels to",
    "start": "487400",
    "end": "493160"
  },
  {
    "text": "be returned within a certain amount of time we potentially don't want to have to deal with downtime or blue/green if",
    "start": "493160",
    "end": "499730"
  },
  {
    "text": "we have to retrain the network so recognition abstracts all of that is away in terms of that level of maybe",
    "start": "499730",
    "end": "506690"
  },
  {
    "text": "deep learning undifferentiated heavy lifting for the business but you can see",
    "start": "506690",
    "end": "512120"
  },
  {
    "text": "here there another you know there are a number of key integration services or",
    "start": "512120",
    "end": "518740"
  },
  {
    "text": "you know services that work really well with recognition so in this case landing",
    "start": "518740",
    "end": "524240"
  },
  {
    "text": "content on s3 utilizing api gateway you you lambda so the entire workflow for",
    "start": "524240",
    "end": "531320"
  },
  {
    "text": "something like this could be built on lambda and then for the things that we can't do that are sort of outliers for",
    "start": "531320",
    "end": "539150"
  },
  {
    "text": "example maybe highly specialized deep learning pipelines maybe it's a specific",
    "start": "539150",
    "end": "544820"
  },
  {
    "text": "type of object recognition we need to do we can utilize easy to spot instances",
    "start": "544820",
    "end": "550370"
  },
  {
    "text": "AWS batch and ECS to basically compartmentalize those and then have a",
    "start": "550370",
    "end": "556940"
  },
  {
    "text": "second layer of processing after we've identified or maybe failed to identify some of the topics and then the rest of",
    "start": "556940",
    "end": "565070"
  },
  {
    "text": "these are fairly common from an architecture perspective but you can see",
    "start": "565070",
    "end": "570650"
  },
  {
    "text": "on the processing side release the elemental media services that just got",
    "start": "570650",
    "end": "575900"
  },
  {
    "text": "released so media converts and media live if we're pulling in live streams chunking them onto say for example s3 or",
    "start": "575900",
    "end": "584180"
  },
  {
    "text": "if we need to say transcode proxies of media files from an incoming mezzanine",
    "start": "584180",
    "end": "590450"
  },
  {
    "text": "asset I mean that may be high resolution high quality and we don't really want to",
    "start": "590450",
    "end": "595880"
  },
  {
    "text": "process that with lambda the full sizing it down so the other thing is your",
    "start": "595880",
    "end": "601520"
  },
  {
    "text": "recognition has this use case in this pipeline but it also has a use case across all of these other areas within",
    "start": "601520",
    "end": "608720"
  },
  {
    "text": "M&E so you can see things like if we're doing editing in the cloud",
    "start": "608720",
    "end": "613880"
  },
  {
    "text": "maybe our assets we want to be tags the exif data we could tag that data so that",
    "start": "613880",
    "end": "620210"
  },
  {
    "text": "then artists or Photoshop editors can search against that using recognition to inject metadata into those files the",
    "start": "620210",
    "end": "627920"
  },
  {
    "text": "same with things like play out distribution and obviously analytics so there's a high amount of code reuse here",
    "start": "627920",
    "end": "635300"
  },
  {
    "text": "the lighter boxes are sort of the pipeline that we're following here from acquisition to supply chain archive and",
    "start": "635300",
    "end": "641840"
  },
  {
    "text": "then finally analytics but you can see it branches out into all of these other areas too and then if we look at where",
    "start": "641840",
    "end": "649460"
  },
  {
    "text": "do we go from here right if we're using recognition we're using it with AWS services such as s3",
    "start": "649460",
    "end": "655580"
  },
  {
    "text": "lambda maybe dynamo for storing the labels for high-performance that's critical to this",
    "start": "655580",
    "end": "662660"
  },
  {
    "text": "all sake partners some of them are in the marketplace some of them have path",
    "start": "662660",
    "end": "668180"
  },
  {
    "text": "source as offerings that can couple into this workflow so essentially we'd be",
    "start": "668180",
    "end": "673250"
  },
  {
    "text": "handing off API calls to them to further enrich the data and then there's also",
    "start": "673250",
    "end": "679160"
  },
  {
    "text": "third-party software so you know you could build those enhance deep learning",
    "start": "679160",
    "end": "685029"
  },
  {
    "text": "frameworks or build out those models on top of for example the AWS AI army or we",
    "start": "685029",
    "end": "691879"
  },
  {
    "text": "could utilize things like open CV image magic ffmpeg and others with lambda as",
    "start": "691879",
    "end": "698000"
  },
  {
    "text": "wrapped and encapsulated binaries so really the thing is why do this in the",
    "start": "698000",
    "end": "704420"
  },
  {
    "text": "cloud is a managed service versus building everything yourself you can certainly do that there are lots of",
    "start": "704420",
    "end": "709970"
  },
  {
    "text": "options for doing that their partners too but part of the problem here is having full control over the entire",
    "start": "709970",
    "end": "717709"
  },
  {
    "text": "content pipeline and this may be for security purposes as well but also you",
    "start": "717709",
    "end": "723139"
  },
  {
    "text": "know there are a number of points yeah I think the most important here and some which I described is that you want a",
    "start": "723139",
    "end": "730000"
  },
  {
    "text": "consistent response rate so if we're versioning if we're having to update",
    "start": "730000",
    "end": "735790"
  },
  {
    "text": "infrastructure it takes a lot of personnel to do that so essentially we're looking at solving 80% of the",
    "start": "735790",
    "end": "742189"
  },
  {
    "text": "problem using a managed service and in taking that engineering talent and",
    "start": "742189",
    "end": "747230"
  },
  {
    "text": "applying them to 20% of the problems such as the lambda code or the additional deep learning models so with",
    "start": "747230",
    "end": "753920"
  },
  {
    "text": "that to talk about the architecture and the back to Dean sure great thank you",
    "start": "753920",
    "end": "759610"
  },
  {
    "text": "so again circling back to the to the National Geographic yeast case I'll",
    "start": "759610",
    "end": "764779"
  },
  {
    "text": "quickly go through the media intelligence pipeline that Kahn had just mentioned in some of the requirements or",
    "start": "764779",
    "end": "770660"
  },
  {
    "text": "some of the initial requirements that we came up with so the media intelligence pipeline I'm going to quickly run",
    "start": "770660",
    "end": "776360"
  },
  {
    "text": "through the ingest portion the analyzation of content and delivery programmatic delivery of that metadata",
    "start": "776360",
    "end": "784160"
  },
  {
    "text": "that's generated so some of the design requirements obviously we wanted a system that would be self-service so us",
    "start": "784160",
    "end": "791959"
  },
  {
    "start": "786000",
    "end": "786000"
  },
  {
    "text": "being Fox networks a shared services unit we wanted to make sure that anything that we're building for",
    "start": "791959",
    "end": "797060"
  },
  {
    "text": "National Geographic could potentially be used for others in a multi-tenant fashion within Fox so in other words the",
    "start": "797060",
    "end": "804830"
  },
  {
    "text": "the metadata that we go and generate needs to be available via an API programmatically available and we're",
    "start": "804830",
    "end": "811970"
  },
  {
    "text": "also creating a method for it to be available through web interface you know manually searched as well but",
    "start": "811970",
    "end": "817910"
  },
  {
    "text": "the primary function was programmatically available so that it could be used by various applications or",
    "start": "817910",
    "end": "824900"
  },
  {
    "text": "plugged into by various applications a very large component of this no pun",
    "start": "824900",
    "end": "830630"
  },
  {
    "text": "intended is the image resizing functionality the automatic image resizing functionality as recognition as",
    "start": "830630",
    "end": "837320"
  },
  {
    "text": "you know has size limitations for images that it can process in National",
    "start": "837320",
    "end": "843560"
  },
  {
    "text": "Geographic as you can imagine has some extremely large images think of maps for",
    "start": "843560",
    "end": "849500"
  },
  {
    "text": "instance you know some of these are extremely trimly large image files lastly I'll dive into this a little bit",
    "start": "849500",
    "end": "857480"
  },
  {
    "text": "more on the on the next slide but we needed a way to have a unique identification of content as it passes",
    "start": "857480",
    "end": "863120"
  },
  {
    "text": "through the system so jumping into that for a second needing to identify assets",
    "start": "863120",
    "end": "869360"
  },
  {
    "text": "so a naming convention method especially if we're going to run this in a distributed fashion for multiple",
    "start": "869360",
    "end": "875900"
  },
  {
    "text": "multiple businesses or even just within National Geographic itself naming convention just doesn't scale it doesn't",
    "start": "875900",
    "end": "882380"
  },
  {
    "text": "work and it gets tricky when you have multiple versions of an asset maybe they're named the same so we needed a",
    "start": "882380",
    "end": "889070"
  },
  {
    "text": "way to not only register globally register assets as they pass through this system so they can understand if",
    "start": "889070",
    "end": "895970"
  },
  {
    "text": "they've already been recognized or not but also you know be able to see this down to the bit level in the in the",
    "start": "895970",
    "end": "902570"
  },
  {
    "text": "actual images itself and not rely on anything else a sidecar benefit of that is that inherently handles parent-child",
    "start": "902570",
    "end": "909740"
  },
  {
    "text": "relationships of of content so as you have maybe an app master asset and you",
    "start": "909740",
    "end": "915230"
  },
  {
    "text": "have multiple file types underneath of that that you know that can all be",
    "start": "915230",
    "end": "920440"
  },
  {
    "text": "categorized and registered as a single system next the the AWS components that",
    "start": "920440",
    "end": "928160"
  },
  {
    "start": "926000",
    "end": "926000"
  },
  {
    "text": "we utilize to to build out our our system as you can",
    "start": "928160",
    "end": "933290"
  },
  {
    "text": "see here we utilize recognition itself obviously and what I'd say is the the",
    "start": "933290",
    "end": "938690"
  },
  {
    "text": "second most critical function outside of lambda that's doing a lot of the a lot of the lifting here are the are the step",
    "start": "938690",
    "end": "945020"
  },
  {
    "text": "functions so actually creating a step function workflow to to run this whole",
    "start": "945020",
    "end": "951920"
  },
  {
    "text": "process lambda being inherently stateless we needed something they can keep state and you know be the source of",
    "start": "951920",
    "end": "959540"
  },
  {
    "text": "truth and I'll dive into that more on some following slides this is the",
    "start": "959540",
    "end": "964850"
  },
  {
    "start": "964000",
    "end": "964000"
  },
  {
    "text": "solution architecture where you can see the components that we use and how they relate to each other for instance you",
    "start": "964850",
    "end": "972440"
  },
  {
    "text": "can see we're using s3 as images land and s3 is part of the ingest process",
    "start": "972440",
    "end": "978250"
  },
  {
    "text": "lambda is watching for s3 put objects and from there lambda keeps kicks off",
    "start": "978250",
    "end": "984200"
  },
  {
    "text": "the step function workflow the goal being that we get into that step",
    "start": "984200",
    "end": "990260"
  },
  {
    "text": "function workflow as quickly as possible so that we're in a safe workflow place",
    "start": "990260",
    "end": "996320"
  },
  {
    "text": "we're not we don't have a all these peripheral lambda functions with let's say sqs running and need to manage all",
    "start": "996320",
    "end": "1003100"
  },
  {
    "text": "those components we have one system keep state you have some failure recovery built in there so",
    "start": "1003100",
    "end": "1009880"
  },
  {
    "text": "and lastly pointing out obviously the the UID a new idea is something we built",
    "start": "1009880",
    "end": "1015570"
  },
  {
    "text": "elsewhere in Fox where reutilizing in this architecture so sending a",
    "start": "1015570",
    "end": "1021280"
  },
  {
    "text": "request through an API gateway to the UID to do that registration process I'll dive more into the step function",
    "start": "1021280",
    "end": "1028240"
  },
  {
    "start": "1028000",
    "end": "1028000"
  },
  {
    "text": "workflow now again as named lambda is",
    "start": "1028240",
    "end": "1033730"
  },
  {
    "text": "inherently stateless we needed something that could keep State and act as the single source of truth for this for this",
    "start": "1033730",
    "end": "1040810"
  },
  {
    "text": "process so if you can see here the step function workflow that we're utilizing where as an image comes in we're",
    "start": "1040810",
    "end": "1048100"
  },
  {
    "text": "checking to see if the image has ever had a UID generated for it if an image",
    "start": "1048100",
    "end": "1053380"
  },
  {
    "text": "has had a UID generated for it we can then just check to see if it's",
    "start": "1053380",
    "end": "1058660"
  },
  {
    "text": "been recognized and if it's been recognized we can - can can kill the process if an image",
    "start": "1058660",
    "end": "1065040"
  },
  {
    "text": "has not been recognized there or there is no UID we then pass we then skip down",
    "start": "1065040",
    "end": "1070920"
  },
  {
    "text": "to the check size function so we can be pretty sure of it it's never had a UID it's never been recognized because it",
    "start": "1070920",
    "end": "1077640"
  },
  {
    "text": "hasn't been registered into this system yet we skip straight down into check size again recognition having a size",
    "start": "1077640",
    "end": "1084000"
  },
  {
    "text": "limitation if it needs resizing we'll resize that image with lambda",
    "start": "1084000",
    "end": "1089330"
  },
  {
    "text": "pass-through recognition as as the data the metadata circles back from",
    "start": "1089330",
    "end": "1095790"
  },
  {
    "text": "recognition we then simultaneously write to both elasticsearch and dynamo DB note",
    "start": "1095790",
    "end": "1102030"
  },
  {
    "text": "that we're not writing to dynamo DB been replicating to elasticsearch we're writing to both simultaneously the goal",
    "start": "1102030",
    "end": "1108450"
  },
  {
    "text": "again to reiterate is to keep as much within this workflow as possible and",
    "start": "1108450",
    "end": "1113760"
  },
  {
    "text": "keeping everything tracked keeping state of everything the step function is your source of truth any information and",
    "start": "1113760",
    "end": "1120390"
  },
  {
    "text": "metadata that has been retrieved through the step function is stored there forever so you can go back and reference",
    "start": "1120390",
    "end": "1126510"
  },
  {
    "text": "that and I'll explain you know some of the benefits of that a little later",
    "start": "1126510",
    "end": "1132710"
  },
  {
    "text": "diving into a sample response we can see we have a Hawksbill turtle here on the",
    "start": "1132710",
    "end": "1138840"
  },
  {
    "start": "1133000",
    "end": "1133000"
  },
  {
    "text": "left and the mountain gorilla on the right the Hawksbill turtle recognition has returned",
    "start": "1138840",
    "end": "1144600"
  },
  {
    "text": "95% rating that this is a reptile we're looking at sea life there's a sea turtle",
    "start": "1144600",
    "end": "1150150"
  },
  {
    "text": "here it's a tortoise for the mountain gorillas a little higher rating 98 98",
    "start": "1150150",
    "end": "1156840"
  },
  {
    "text": "percent that it's a it's an animal gorilla a mammal a monkey so reasonably",
    "start": "1156840",
    "end": "1164570"
  },
  {
    "text": "good labels that have been returned here one thing to keep in mind one thing",
    "start": "1164570",
    "end": "1170160"
  },
  {
    "text": "we've seen maybe not so much in these images maybe a little in the in the gorilla image is that with professional",
    "start": "1170160",
    "end": "1175860"
  },
  {
    "text": "photography the the subject of interest may not always be Center in the image",
    "start": "1175860",
    "end": "1181470"
  },
  {
    "text": "there may be various aperture settings raised focus settings and so it may be a",
    "start": "1181470",
    "end": "1186510"
  },
  {
    "text": "little harder to recognize in which case you may need to pass off and actually",
    "start": "1186510",
    "end": "1192080"
  },
  {
    "text": "just dial in on one part of an image then upscale that for for recognizing",
    "start": "1192080",
    "end": "1197190"
  },
  {
    "text": "itself looking at the storage of the",
    "start": "1197190",
    "end": "1203130"
  },
  {
    "start": "1201000",
    "end": "1201000"
  },
  {
    "text": "labels that come back after we've received them from from recognition so",
    "start": "1203130",
    "end": "1208559"
  },
  {
    "text": "this is an example of what we would store in dynamodb elasticsearch you can",
    "start": "1208559",
    "end": "1214140"
  },
  {
    "text": "see that at the top of this JSON blob we have the UID listed there so we know",
    "start": "1214140",
    "end": "1221520"
  },
  {
    "text": "what the asset is that we're talking about along with the confidence labels one thing to note here is that what",
    "start": "1221520",
    "end": "1229080"
  },
  {
    "text": "we're looking to do we don't have in this version but what we're looking to implement here is actually adding another section under UID",
    "start": "1229080",
    "end": "1235340"
  },
  {
    "text": "which is the work flow ID of the step function process that had generated this",
    "start": "1235340",
    "end": "1241200"
  },
  {
    "text": "metadata and I'll explain why that's interesting in a later slide but just",
    "start": "1241200",
    "end": "1247140"
  },
  {
    "text": "think about that tying a asset ID down to the bits and the workflow ID along",
    "start": "1247140",
    "end": "1252660"
  },
  {
    "text": "with the JSON and storing that away you could also utilize recognitions minimum",
    "start": "1252660",
    "end": "1259410"
  },
  {
    "text": "confidence level for the moment we have it split off at 50% confidence to just",
    "start": "1259410",
    "end": "1264900"
  },
  {
    "text": "go ahead and eliminate those false positives no there's not a truck in the turtle image or something like that you",
    "start": "1264900",
    "end": "1270929"
  },
  {
    "text": "know you can just scrape off the last percentage there additionally the max labels function for",
    "start": "1270929",
    "end": "1276770"
  },
  {
    "text": "limiting the total number of labels you want returned if you want to have some",
    "start": "1276770",
    "end": "1282270"
  },
  {
    "text": "constraint or boundary there client-side filtering obviously probably done as well a little bit of the user experience",
    "start": "1282270",
    "end": "1291590"
  },
  {
    "start": "1290000",
    "end": "1290000"
  },
  {
    "text": "so again we focused on the programmatic",
    "start": "1291590",
    "end": "1296640"
  },
  {
    "text": "access of of this data so we're focused on the API things of that nature but we",
    "start": "1296640",
    "end": "1302940"
  },
  {
    "text": "have generated an alpha interface this is just an example of the alpha interface where we put a sample bit of",
    "start": "1302940",
    "end": "1309000"
  },
  {
    "text": "content in and search for turtle this is all done through recognition so the",
    "start": "1309000",
    "end": "1314250"
  },
  {
    "text": "recognition realizes a turtle and then we can then see the labels that were",
    "start": "1314250",
    "end": "1319260"
  },
  {
    "text": "returned there on the left of the turtle so we're really thinking about how we",
    "start": "1319260",
    "end": "1325860"
  },
  {
    "text": "can maybe not so much this interface but think about integrating this into a National Geographic mobile application",
    "start": "1325860",
    "end": "1332860"
  },
  {
    "text": "or website where then you unleash you know all consumers to all of national",
    "start": "1332860",
    "end": "1338350"
  },
  {
    "text": "Geographics content some content which may not have ever been surface before or may have been harder to surface because",
    "start": "1338350",
    "end": "1344830"
  },
  {
    "text": "there was no metadata to really search about you know how to find that image that's the goal is really unleashing",
    "start": "1344830",
    "end": "1350590"
  },
  {
    "text": "them the value of all of this content that they have because they have so much of it next steps so I'm really excited",
    "start": "1350590",
    "end": "1358600"
  },
  {
    "start": "1357000",
    "end": "1357000"
  },
  {
    "text": "about some of these next steps particularly the video capability were we're extremely interested in extending",
    "start": "1358600",
    "end": "1365800"
  },
  {
    "text": "this out not only it's a video but but text content is as well video is just a",
    "start": "1365800",
    "end": "1372640"
  },
  {
    "text": "natural next step for this process next step no pun intended for step functions",
    "start": "1372640",
    "end": "1377920"
  },
  {
    "text": "but it would be a next step in a step function process metadata transformer so on the metadata transformer you know we",
    "start": "1377920",
    "end": "1385630"
  },
  {
    "text": "have various applications various business units which which may want this data so we're looking at user utilizing",
    "start": "1385630",
    "end": "1392770"
  },
  {
    "text": "API gateway to transform that JSON blob at you know upon a request by a",
    "start": "1392770",
    "end": "1400360"
  },
  {
    "text": "particular application or business into their expected metadata schema so we can",
    "start": "1400360",
    "end": "1406210"
  },
  {
    "text": "have varying output requirements recognition so that result differential",
    "start": "1406210",
    "end": "1411370"
  },
  {
    "text": "tracking is where I had thrown in the comment about keeping track of the workflow ID so the step function",
    "start": "1411370",
    "end": "1417760"
  },
  {
    "text": "workflow ID plus the data the UID of the content itself so by keeping track of",
    "start": "1417760",
    "end": "1423190"
  },
  {
    "text": "that over time obviously we're expecting that recognition gets better and better by keeping track of that workflow ID we",
    "start": "1423190",
    "end": "1431320"
  },
  {
    "text": "can then you know if images uploaded the same image is uploaded again or for just cycling through and trying to feel out",
    "start": "1431320",
    "end": "1438460"
  },
  {
    "text": "if you know if there's any better recognizing capability we can look up",
    "start": "1438460",
    "end": "1444220"
  },
  {
    "text": "that workflow ID look at the results that were passed back by recognition and",
    "start": "1444220",
    "end": "1449350"
  },
  {
    "text": "we can do a diff on that and just see if there's any difference if there is cool tack that on to the end of onto the end",
    "start": "1449350",
    "end": "1455470"
  },
  {
    "text": "of that blob or integrated and based on you know the label confidence levels goes without saying that we would want",
    "start": "1455470",
    "end": "1462450"
  },
  {
    "text": "to integrate this within our web and mobile applications maybe as a back-end engine to to unleash that content and",
    "start": "1462450",
    "end": "1471300"
  },
  {
    "text": "let it be viewed by consumers so you",
    "start": "1471300",
    "end": "1479520"
  },
  {
    "start": "1476000",
    "end": "1476000"
  },
  {
    "text": "know if we look back at the original challenge right that you can see at least one of these has been immediately",
    "start": "1479520",
    "end": "1486270"
  },
  {
    "text": "solved right I'll try resolution or high resolution we can resize with lambda potentially we could use something else",
    "start": "1486270",
    "end": "1492510"
  },
  {
    "text": "if it was maybe for the 60 mega pixels if if you will but you know how do we",
    "start": "1492510",
    "end": "1497670"
  },
  {
    "text": "solve for these other problems as part of this workflow and this is where this",
    "start": "1497670",
    "end": "1503730"
  },
  {
    "text": "is you know at least a very interesting you know set of problems to look at from",
    "start": "1503730",
    "end": "1509700"
  },
  {
    "text": "the perspective of recognition so yeah I'm sure many of you that have built out",
    "start": "1509700",
    "end": "1514860"
  },
  {
    "text": "a pipeline with recognition are often looking at these types of additional edge cases or you know foreign use cases",
    "start": "1514860",
    "end": "1522360"
  },
  {
    "text": "how do you solve that what's the best way to do that so you know especially like these niche image categories maybe",
    "start": "1522360",
    "end": "1528900"
  },
  {
    "text": "Mechanical Turk is a good option here up sizing the resolution we could use some",
    "start": "1528900",
    "end": "1534510"
  },
  {
    "text": "of the newer papers that have been published for example at SIGGRAPH where we can take deep learning use that to do",
    "start": "1534510",
    "end": "1541620"
  },
  {
    "text": "edge detection and then basically via deep learning interpolate missing pixels",
    "start": "1541620",
    "end": "1547620"
  },
  {
    "text": "versus simply trying to scale up images and then apply smoothing or sharpening",
    "start": "1547620",
    "end": "1553530"
  },
  {
    "text": "algorithm over the top of that which doesn't work that well the other thing is artifacts and noise so we could do we",
    "start": "1553530",
    "end": "1561210"
  },
  {
    "text": "could use image magic to do noise reduction as an example but maybe we could utilize deep learning to do more",
    "start": "1561210",
    "end": "1567570"
  },
  {
    "text": "advanced noise reductions so you know one of the byproducts of doing global noise reduction is that sometimes detail",
    "start": "1567570",
    "end": "1574470"
  },
  {
    "text": "gets lost in images like in the particularly dark areas of an image if they have a high amount of noise so",
    "start": "1574470",
    "end": "1581190"
  },
  {
    "text": "maybe we can utilize some of the other advancements in the field to funnel that",
    "start": "1581190",
    "end": "1586800"
  },
  {
    "text": "through a deep learning pipeline to do intelligent denoising and any other things like black-and-white footage",
    "start": "1586800",
    "end": "1592940"
  },
  {
    "text": "there are a lot of approaches to doing this now you know some of them are",
    "start": "1592940",
    "end": "1598640"
  },
  {
    "text": "manuals some of them are automatic a lot of the manual ones that have context to",
    "start": "1598640",
    "end": "1604100"
  },
  {
    "text": "related data we could use recognition tags and then process it I'll talk a little bit about that in a later slide",
    "start": "1604100",
    "end": "1610210"
  },
  {
    "text": "and in this notion of high accuracy right the high accuracy is great but",
    "start": "1610210",
    "end": "1615650"
  },
  {
    "text": "it's not really great if the context is incorrect so we don't want users to be",
    "start": "1615650",
    "end": "1621260"
  },
  {
    "text": "searching a large archive and getting content back where potentially if",
    "start": "1621260",
    "end": "1626960"
  },
  {
    "text": "they're getting thousands of images returned at the first page of thumbnail as little thumbnails is potentially the",
    "start": "1626960",
    "end": "1633290"
  },
  {
    "text": "wrong data and they have to paginate through to find the things they really want so if we look back at this pipeline",
    "start": "1633290",
    "end": "1641060"
  },
  {
    "text": "right now we're really talking about deep learning over on the top of this",
    "start": "1641060",
    "end": "1648500"
  },
  {
    "text": "existing you know pipeline that we have going with recognition so you can look at recognition here as a feedback",
    "start": "1648500",
    "end": "1654560"
  },
  {
    "text": "mechanism for us to be able to say we got labels and it's a certain category or we've analyzed these these images and",
    "start": "1654560",
    "end": "1662480"
  },
  {
    "text": "they appear to have shop and you know sharpening or color gamut issues maybe",
    "start": "1662480",
    "end": "1668150"
  },
  {
    "text": "we need to correct those via additional pipelines that we handoff to and you're",
    "start": "1668150",
    "end": "1675500"
  },
  {
    "start": "1674000",
    "end": "1674000"
  },
  {
    "text": "not just to you know just to talk about this a little bit in terms of that last point right that this is a great problem",
    "start": "1675500",
    "end": "1683870"
  },
  {
    "text": "and that people can solve this people can solve this fast but you know interestingly enough little children",
    "start": "1683870",
    "end": "1690290"
  },
  {
    "text": "have problems with this so I have two 80 year olds and I ran this through them",
    "start": "1690290",
    "end": "1695960"
  },
  {
    "text": "and they sort of you know that they'll nail it in about a second or two or three seconds but really as a you know",
    "start": "1695960",
    "end": "1703030"
  },
  {
    "text": "with a developed brain a human will it actually be able to tag these in about 200 milliseconds that recognition of the",
    "start": "1703030",
    "end": "1710390"
  },
  {
    "text": "shapes and the outlines and everything else but you know it gets back to is it",
    "start": "1710390",
    "end": "1715610"
  },
  {
    "text": "artificial intelligence or is it assisted intelligence and in some cases like this it's a lot of assisted",
    "start": "1715610",
    "end": "1721550"
  },
  {
    "text": "intelligence so you know our algorithms will return back 99% confidence that",
    "start": "1721550",
    "end": "1727850"
  },
  {
    "text": "this is a dog but also 99% or 98% or something slightly less that potentially",
    "start": "1727850",
    "end": "1733910"
  },
  {
    "text": "this dog is also a muffin so that's a problem right and we can't really we can",
    "start": "1733910",
    "end": "1739550"
  },
  {
    "text": "spend a lot of engineering time and hours potentially months trying to solve this building models for this but if we",
    "start": "1739550",
    "end": "1747470"
  },
  {
    "text": "looked at something like say Mechanical Turk maybe that would be a better use case for these edge cases that haven't",
    "start": "1747470",
    "end": "1753350"
  },
  {
    "text": "been correctly tagged and then looking specifically at NatGeo at like some of",
    "start": "1753350",
    "end": "1759920"
  },
  {
    "text": "the photos right so the one on the left you may you know if you had a deep",
    "start": "1759920",
    "end": "1766430"
  },
  {
    "text": "learning algorithm look at that it may tag that as floorboards or it may tag it as just lines or it may tag it as",
    "start": "1766430",
    "end": "1773810"
  },
  {
    "text": "railroad tracks you know part of the problem here is that this image may not",
    "start": "1773810",
    "end": "1779600"
  },
  {
    "text": "have sufficient resolution to be able to dial down to say an 80 pixel boundary",
    "start": "1779600",
    "end": "1785030"
  },
  {
    "text": "box around the cows that are in the feed Lots so we can't extract that information and there's a whole bunch of",
    "start": "1785030",
    "end": "1791690"
  },
  {
    "text": "permutations that make it too complex the Regal shoes this one is actually",
    "start": "1791690",
    "end": "1796700"
  },
  {
    "text": "more easily solved now using recognition OCR if you will a text recognition we",
    "start": "1796700",
    "end": "1803720"
  },
  {
    "text": "were able to pass these fonts potentially if they're hand-drawn different font styles and then also you",
    "start": "1803720",
    "end": "1810260"
  },
  {
    "text": "know if they're skewed if they're slightly rotated or tilted it does a really good job on parsing that as well",
    "start": "1810260",
    "end": "1817100"
  },
  {
    "text": "so that's critical for these types of use cases where turn-of-the-century there were no fonts and typefaces well",
    "start": "1817100",
    "end": "1824660"
  },
  {
    "text": "they were but you know in many of these ads people were drawing this by hand and",
    "start": "1824660",
    "end": "1830270"
  },
  {
    "text": "then running it through printing presses the other problem here is that you can see that this is a bezel landmark you",
    "start": "1830270",
    "end": "1837200"
  },
  {
    "text": "know around say New York these landmarks have changed over the last century so even if we extract it or try to run some",
    "start": "1837200",
    "end": "1844460"
  },
  {
    "text": "deep learning algorithm to identify landmarks it wouldn't successfully identify burrs and then the last problem",
    "start": "1844460",
    "end": "1852200"
  },
  {
    "text": "is for example the Sun Bay there is this is one of like pixel density resolution as well if we don't have",
    "start": "1852200",
    "end": "1858230"
  },
  {
    "text": "sufficient resolution to dial down to say a towel and umbrella or a person laying on the beach then this",
    "start": "1858230",
    "end": "1865250"
  },
  {
    "text": "potentially looks like maybe a bunch of candy sitting on sand in terms of",
    "start": "1865250",
    "end": "1871340"
  },
  {
    "text": "tagging it so you know like Dean was talking about going through this process",
    "start": "1871340",
    "end": "1876980"
  },
  {
    "start": "1873000",
    "end": "1873000"
  },
  {
    "text": "of improving the image sourcing in order to improve the recognition confidence of",
    "start": "1876980",
    "end": "1883610"
  },
  {
    "text": "these labels so one way that this can be done or there are a number of ways to do",
    "start": "1883610",
    "end": "1888620"
  },
  {
    "text": "this which we almost consider now to be best practices because we see so many customers doing the same thing the one",
    "start": "1888620",
    "end": "1896299"
  },
  {
    "text": "is to stabilize the image right so we can use image magic filters we could use",
    "start": "1896299",
    "end": "1902419"
  },
  {
    "text": "binaries that are encapsulated with lambda we build them on you know maybe the the Linux army is 64 but static",
    "start": "1902419",
    "end": "1910760"
  },
  {
    "text": "binaries and then we wrap them with lambda and we can then pass data to them have them clean it up return",
    "start": "1910760",
    "end": "1917659"
  },
  {
    "text": "you know cleaned up images that we can then move back to s3 but you know if you",
    "start": "1917659",
    "end": "1922669"
  },
  {
    "text": "look at like utilizing Python or node or whatnot a lot of the libraries that you",
    "start": "1922669",
    "end": "1927919"
  },
  {
    "text": "may want to use like image magic already available for use right so you can then",
    "start": "1927919",
    "end": "1933919"
  },
  {
    "text": "extend that with custom algorithms so we could do things like highly specialized",
    "start": "1933919",
    "end": "1939470"
  },
  {
    "text": "done shot mark mosques so if you look at the bottom two faces",
    "start": "1939470",
    "end": "1944600"
  },
  {
    "text": "the one on the left has been improved with Richardson Lucy you know at least",
    "start": "1944600",
    "end": "1951620"
  },
  {
    "text": "correction algorithm and this is important because a lot of the default ways to do this that you may look at",
    "start": "1951620",
    "end": "1958340"
  },
  {
    "text": "with say image magic will simply emboss the edges of the image and when you're",
    "start": "1958340",
    "end": "1964640"
  },
  {
    "text": "doing embossing as like an unsharp mask where all the edges are sort of drawn out that can give you false confidence",
    "start": "1964640",
    "end": "1971630"
  },
  {
    "text": "scores so things like the location of the eyes and whatnot but may now be at",
    "start": "1971630",
    "end": "1977390"
  },
  {
    "text": "the wrong places right because you've repaired or attempted to repair the image the other thing that we can do is",
    "start": "1977390",
    "end": "1984169"
  },
  {
    "text": "we could wrap things like OpenCV so if you think about maybe it's a really",
    "start": "1984169",
    "end": "1989299"
  },
  {
    "text": "large follow that we're dealing with maybe gigapixel in size maybe we want to have",
    "start": "1989299",
    "end": "1996059"
  },
  {
    "text": "some first pass to extract the phases and there are a number of open CV algorithms it's got about 2,500 that you",
    "start": "1996059",
    "end": "2003500"
  },
  {
    "text": "can pick from but there are a number of algorithms that you can use there to look at the pixel data and then extract",
    "start": "2003500",
    "end": "2010039"
  },
  {
    "text": "the faces and then push those into recognition and then the final important",
    "start": "2010039",
    "end": "2015500"
  },
  {
    "text": "thing here and this is probably the most important one it's been able to pass",
    "start": "2015500",
    "end": "2021080"
  },
  {
    "text": "image data to recognition without any loss so if you look at the top two",
    "start": "2021080",
    "end": "2027200"
  },
  {
    "text": "examples there so this extracted piece at the top there shows the amount of",
    "start": "2027200",
    "end": "2032870"
  },
  {
    "text": "dithering in the scene and you can see between the one on the left and then the one on the right there's not much",
    "start": "2032870",
    "end": "2038179"
  },
  {
    "text": "difference and unless you really looked at the pixels but this is application of",
    "start": "2038179",
    "end": "2043490"
  },
  {
    "text": "perceptual image compression so in other words we're keeping the resolution we're keeping the data in terms of pixel data",
    "start": "2043490",
    "end": "2050868"
  },
  {
    "text": "but we're maybe dithering part of the images that the human eye doesn't really pick up easily on so there options that",
    "start": "2050869",
    "end": "2058849"
  },
  {
    "text": "are open source like jpg PNG quant Mozilla mods a peg and then there are",
    "start": "2058849",
    "end": "2065358"
  },
  {
    "text": "also commercial options such as JPEG money from one of our partners it also",
    "start": "2065359",
    "end": "2070550"
  },
  {
    "text": "has options of marketplace for that but essentially those can be used to size the images down without reducing the",
    "start": "2070550",
    "end": "2077118"
  },
  {
    "text": "resolution that we can then pass them into recognition without losing",
    "start": "2077119",
    "end": "2082868"
  },
  {
    "text": "potentially the ability to tag or identify objects so we looked at you",
    "start": "2082869",
    "end": "2090080"
  },
  {
    "start": "2088000",
    "end": "2088000"
  },
  {
    "text": "know the service stack right in terms of if we look at the entire service stack top to bottom you know we have this",
    "start": "2090080",
    "end": "2096618"
  },
  {
    "text": "notion of higher note or higher order services and we talked a lot about recognition in the vision category but",
    "start": "2096619",
    "end": "2104089"
  },
  {
    "text": "then we below that we have a number of you know options for platforms etc and really when we talk about this specific",
    "start": "2104089",
    "end": "2110780"
  },
  {
    "text": "use case here we're looking at deploying frameworks and this could be tensorflow",
    "start": "2110780",
    "end": "2116420"
  },
  {
    "text": "it could be MX net it could be Cafe on top of infrastructure",
    "start": "2116420",
    "end": "2122770"
  },
  {
    "text": "the one thing that isn't pointed out here is that you can do fairly novel things like some of the frameworks like",
    "start": "2122770",
    "end": "2129980"
  },
  {
    "text": "cafe - can be wrapped to actually be executed from within lambdas so that's a",
    "start": "2129980",
    "end": "2135440"
  },
  {
    "text": "kind of interesting use case for you know maybe tech space deep learning and",
    "start": "2135440",
    "end": "2141140"
  },
  {
    "text": "then the other problem is if we have this infrastructure where do we go where do we start with modeling this out to",
    "start": "2141140",
    "end": "2147740"
  },
  {
    "text": "solve these edge use cases right so if we have you know we have this large image library maybe we want to use that",
    "start": "2147740",
    "end": "2155300"
  },
  {
    "text": "as a training data set or maybe a validation or test data set for whichever deep learning",
    "start": "2155300",
    "end": "2162050"
  },
  {
    "text": "you know framework we're using we can actually start with some of the existing work that's been done so if you look at",
    "start": "2162050",
    "end": "2168440"
  },
  {
    "text": "the places MIT places data set is about 2.5 million images of just different",
    "start": "2168440",
    "end": "2174230"
  },
  {
    "text": "places around the world that have been labeled already so you can use that as a way to train a new data set or create a",
    "start": "2174230",
    "end": "2181609"
  },
  {
    "text": "new data set but you start not having to create it from scratch similarly if you",
    "start": "2181609",
    "end": "2188000"
  },
  {
    "text": "had some kind of niche case for identifying celebrities or faces or whatnot in a crowd is a celeb a data set",
    "start": "2188000",
    "end": "2195920"
  },
  {
    "text": "which is about two hundred thousand different images of celebrities it's",
    "start": "2195920",
    "end": "2202040"
  },
  {
    "text": "open source one and then you know what is almost the gold standard here is the",
    "start": "2202040",
    "end": "2208190"
  },
  {
    "text": "cipher 10,000 or 100,000 labelled data set so these are small pixels the actual",
    "start": "2208190",
    "end": "2215570"
  },
  {
    "text": "tiny image data set of unlabeled data is about 30 million images but that's a lot",
    "start": "2215570",
    "end": "2221180"
  },
  {
    "text": "of data that you know when you're building these types of models and evaluating these use cases you don't",
    "start": "2221180",
    "end": "2227900"
  },
  {
    "text": "have to reinvent the wheel and then the other thing here is if we look at the",
    "start": "2227900",
    "end": "2234170"
  },
  {
    "text": "infrastructure to run these models and train them on you know what would be the best match for that right so there are a",
    "start": "2234170",
    "end": "2241040"
  },
  {
    "text": "lot of you know there are a lot of instance families that you can choose from to run these deep learning",
    "start": "2241040",
    "end": "2247850"
  },
  {
    "text": "you know models on for example but as we release you know say p3 vs. p2 etc",
    "start": "2247850",
    "end": "2255890"
  },
  {
    "text": "there's a lot of improvement that comes in speeds to say do inference or speed to train but it doesn't mean that you",
    "start": "2255890",
    "end": "2263539"
  },
  {
    "text": "shouldn't be using other types of instances for example utilizing spots and diversifying across others so this",
    "start": "2263539",
    "end": "2270739"
  },
  {
    "text": "slide pretty much points to these instance families and good use cases for",
    "start": "2270739",
    "end": "2275839"
  },
  {
    "text": "them right so if we look to FPGA we have a partner there that has a marketplace",
    "start": "2275839",
    "end": "2283549"
  },
  {
    "text": "solution that does one millisecond inference on images so if we trained a",
    "start": "2283549",
    "end": "2289609"
  },
  {
    "text": "niche use case and we wanted to batch a whole bunch of images through it it's able to do about 900 to 1200 plus frames",
    "start": "2289609",
    "end": "2297349"
  },
  {
    "text": "per second so that's really interesting from that perspective potentially if",
    "start": "2297349",
    "end": "2302450"
  },
  {
    "text": "it's media files where we need to analyze every single frame really fast and then there are other options here as",
    "start": "2302450",
    "end": "2308869"
  },
  {
    "text": "well all right so obviously when you look at p3 and you're going to want potentially one petaflop of performance",
    "start": "2308869",
    "end": "2315309"
  },
  {
    "text": "you know those of you that have followed you know the NVIDIA GPU roadmap over the",
    "start": "2315309",
    "end": "2320749"
  },
  {
    "text": "last couple of years about two plus years ago the training time would be",
    "start": "2320749",
    "end": "2326299"
  },
  {
    "text": "about thirty days to train a layer of about or at least a model of about ten",
    "start": "2326299",
    "end": "2331400"
  },
  {
    "text": "layers with X amount of images in the data set and pretty much now this can be",
    "start": "2331400",
    "end": "2336950"
  },
  {
    "text": "done in a factor of eight hours so it really changes the way that you can look at building testing and evaluating these",
    "start": "2336950",
    "end": "2344630"
  },
  {
    "text": "different models and data sets and lastly the important thing is especially",
    "start": "2344630",
    "end": "2350329"
  },
  {
    "text": "with the p3s and other instances you know if you do have say ten thousand or",
    "start": "2350329",
    "end": "2355460"
  },
  {
    "text": "a hundred thousand images sitting on the s3 you need both the EBS bandwidth to be",
    "start": "2355460",
    "end": "2360589"
  },
  {
    "text": "able to hydrate that locally and then train the data set plus you need the network IO to be able to reach back and",
    "start": "2360589",
    "end": "2367220"
  },
  {
    "text": "forth to s3 so you can see the scale here is fairly linear in terms of the",
    "start": "2367220",
    "end": "2373160"
  },
  {
    "text": "capacity you get for network and EBS matching to the amount of GPUs so that's",
    "start": "2373160",
    "end": "2378589"
  },
  {
    "text": "really important for deep learning from this perspective so getting back to you",
    "start": "2378589",
    "end": "2384650"
  },
  {
    "text": "know what about those edge use cases that we potentially cannot use",
    "start": "2384650",
    "end": "2389849"
  },
  {
    "text": "or maybe the engineering time is too high to actually build and spend time modeling this out for the nice use case",
    "start": "2389849",
    "end": "2397400"
  },
  {
    "text": "so the good option here is that you know Mechanical Turk and be utilized for",
    "start": "2397400",
    "end": "2403050"
  },
  {
    "text": "these types of functions so imagenet was you know the most popular implementation",
    "start": "2403050",
    "end": "2409619"
  },
  {
    "text": "there but basically what we're talking about here is building a UI presenting",
    "start": "2409619",
    "end": "2416220"
  },
  {
    "text": "that to human as human intelligence tasks so in other words I present an",
    "start": "2416220",
    "end": "2421619"
  },
  {
    "text": "image to someone that is a Mechanical Turk er that's signed up they're potentially paid maybe a penny maybe",
    "start": "2421619",
    "end": "2429240"
  },
  {
    "text": "more depending on if they have specialized skills to click through that maybe select labels or type in a",
    "start": "2429240",
    "end": "2435720"
  },
  {
    "text": "description and the thing is you know we know that if a human can identify an",
    "start": "2435720",
    "end": "2442200"
  },
  {
    "text": "image at about say within 200 milliseconds if we can optimize this",
    "start": "2442200",
    "end": "2448200"
  },
  {
    "text": "task for this particular use case where it's just clicking through and selecting labels we can actually get a high",
    "start": "2448200",
    "end": "2454560"
  },
  {
    "text": "inference rate from utilizing workers so the next question is you know how many",
    "start": "2454560",
    "end": "2460230"
  },
  {
    "text": "how many workers can we actually get access to right so the graph on the Left",
    "start": "2460230",
    "end": "2465330"
  },
  {
    "text": "shows basically the population of workers and this is a fairly old one",
    "start": "2465330",
    "end": "2472200"
  },
  {
    "text": "it's probably a year old or so but the interesting part here is the darker or",
    "start": "2472200",
    "end": "2477240"
  },
  {
    "text": "at least the brighter green curves are actually people that have returned to do more tasks so you can see there's a",
    "start": "2477240",
    "end": "2484800"
  },
  {
    "text": "population that absent flows but you can potentially get you know 10,000 plus",
    "start": "2484800",
    "end": "2490589"
  },
  {
    "text": "people working on a task if it's interesting enough and how we'd present that is you know simply building a maybe",
    "start": "2490589",
    "end": "2498089"
  },
  {
    "text": "a web-based UI you can see on the right example here you know one of the things",
    "start": "2498089",
    "end": "2503940"
  },
  {
    "text": "we have to do is we have to train the workers to be able to correctly tag the",
    "start": "2503940",
    "end": "2509460"
  },
  {
    "text": "data because we don't want them tagging data incorrectly so we may say if you've",
    "start": "2509460",
    "end": "2514560"
  },
  {
    "text": "tagged X amount of images and we're happy with the scoring on that and we do you know",
    "start": "2514560",
    "end": "2520170"
  },
  {
    "text": "spot check QA maybe there's an additional incentive for people that are doing this so maybe they get paid more",
    "start": "2520170",
    "end": "2526740"
  },
  {
    "text": "per task because they're really accurate at doing it or maybe this other some other incentive but essentially that you",
    "start": "2526740",
    "end": "2534390"
  },
  {
    "text": "are in the right right basically allows people to type in a description they're also hot links so that they can click",
    "start": "2534390",
    "end": "2541260"
  },
  {
    "text": "it'll take the image and maybe it'll do a turn I search you know maybe it'll do",
    "start": "2541260",
    "end": "2546750"
  },
  {
    "text": "a Google search etc search the internet for keywords and so on that they can",
    "start": "2546750",
    "end": "2552240"
  },
  {
    "text": "then go and do research and use that to tag the image correctly so optimizing",
    "start": "2552240",
    "end": "2558150"
  },
  {
    "text": "for point-and-click versus having people context switch between using a mouse and",
    "start": "2558150",
    "end": "2563580"
  },
  {
    "text": "actually typing in text is really important here so back to our image",
    "start": "2563580",
    "end": "2568950"
  },
  {
    "start": "2568000",
    "end": "2568000"
  },
  {
    "text": "archive right so if we look at these things that we are enta fired right so these custom concepts on the right hand",
    "start": "2568950",
    "end": "2576660"
  },
  {
    "text": "side is actually manual colorization I'll talk about that in a bit but if we",
    "start": "2576660",
    "end": "2581910"
  },
  {
    "text": "look at these you know custom concepts right in terms of we're getting these tags back from recognition",
    "start": "2581910",
    "end": "2587880"
  },
  {
    "text": "what if we're getting plurals singulars different spelling etc maybe we're getting people person maybe the tense is",
    "start": "2587880",
    "end": "2595230"
  },
  {
    "text": "important it's not that important for recognition but if we're say processing close captions the text and that you",
    "start": "2595230",
    "end": "2602850"
  },
  {
    "text": "know tense past tense present tense etc we need to normalize that so they're",
    "start": "2602850",
    "end": "2608820"
  },
  {
    "text": "libraries like Spacey that can be wrapped into lambda which allow you to normalize text lemma ties it in other",
    "start": "2608820",
    "end": "2616110"
  },
  {
    "text": "words reduce everything that is a person people maybe it's singular plural etc",
    "start": "2616110",
    "end": "2621930"
  },
  {
    "text": "into a single word and from there you can do things like do similarity scoring",
    "start": "2621930",
    "end": "2628290"
  },
  {
    "text": "you could say compare a dog a cat is this an animal those types of",
    "start": "2628290",
    "end": "2634890"
  },
  {
    "text": "comparisons that you need to be able to drive more like intelligence into this pipeline beyond just the correct tags if",
    "start": "2634890",
    "end": "2643380"
  },
  {
    "text": "we look at the specialized categories this is highly utilized obvious in things",
    "start": "2643380",
    "end": "2649640"
  },
  {
    "text": "manufacturing processes right so we take an existing deep learning framework we",
    "start": "2649640",
    "end": "2655350"
  },
  {
    "text": "tear down the last layer and then we basically retrain it utilizing",
    "start": "2655350",
    "end": "2660810"
  },
  {
    "text": "specialized images so the interesting thing there is we don't redo all the training for things like detecting pixel",
    "start": "2660810",
    "end": "2667680"
  },
  {
    "text": "edges and so forth we're just doing transfer learning and then fine-tuning it for these highly specialized use",
    "start": "2667680",
    "end": "2673740"
  },
  {
    "text": "cases so it's one example of you know it's building on top of the shoulders of work that has been done already the",
    "start": "2673740",
    "end": "2681510"
  },
  {
    "text": "black and white footage is an interesting one there are a lot of papers around this a lot of the",
    "start": "2681510",
    "end": "2687030"
  },
  {
    "text": "automated algorithms will colorize an image but you'll see a sheen around the",
    "start": "2687030",
    "end": "2692610"
  },
  {
    "text": "edge of objects so for example a person standing on grass there may be a section around them that's not green it sort of",
    "start": "2692610",
    "end": "2700350"
  },
  {
    "text": "blends out to black and white but this particular project here on the right",
    "start": "2700350",
    "end": "2706500"
  },
  {
    "text": "hand side which is open-source it basically does allows you to tag",
    "start": "2706500",
    "end": "2712470"
  },
  {
    "text": "different parts of an image and then it color eise's it based on semantic theme",
    "start": "2712470",
    "end": "2719970"
  },
  {
    "text": "and that theme is derived from which you can see on the bottom side we could use",
    "start": "2719970",
    "end": "2725280"
  },
  {
    "text": "labels coming back from recognition for that so we may use the seen label you",
    "start": "2725280",
    "end": "2731640"
  },
  {
    "text": "know which is potentially outdoors and then pair it with another image that's been trained for outdoor colorization",
    "start": "2731640",
    "end": "2737670"
  },
  {
    "text": "all potentially we could take the tag for a bird and then train this to",
    "start": "2737670",
    "end": "2743310"
  },
  {
    "text": "recolor eyes with another image of a bird so these are two options to get",
    "start": "2743310",
    "end": "2748950"
  },
  {
    "text": "higher quality colorization of content that was potentially black and white",
    "start": "2748950",
    "end": "2754980"
  },
  {
    "text": "with no context the low resolutions we talked about so this is a classical",
    "start": "2754980",
    "end": "2762110"
  },
  {
    "text": "scaling problem where we're actually upscaling and then interpolating the difference instead of just trying to",
    "start": "2762110",
    "end": "2768000"
  },
  {
    "text": "stretch and add more pixels which just leads to a you know blended out picture",
    "start": "2768000",
    "end": "2773820"
  },
  {
    "text": "with less detail and then the nisshin historical context like I said the",
    "start": "2773820",
    "end": "2779790"
  },
  {
    "text": "important thing here to use things like potentially OCR to identify labels and then search outside",
    "start": "2779790",
    "end": "2785730"
  },
  {
    "text": "of the image context or to utilize crowd working like Mechanical Turk so getting",
    "start": "2785730",
    "end": "2793200"
  },
  {
    "start": "2792000",
    "end": "2792000"
  },
  {
    "text": "back to how this is all put together right so the the pipeline that Dean",
    "start": "2793200",
    "end": "2798329"
  },
  {
    "text": "talked about has a lot of lambda lambda step functions etc but key to that is",
    "start": "2798329",
    "end": "2803550"
  },
  {
    "text": "that the content lands on s3 gets processed by lambda and then lands on s3 again so potentially we can use for",
    "start": "2803550",
    "end": "2811380"
  },
  {
    "text": "example multiple buckets we could use auto scaling groups each order scaling group has a different deep deep learning",
    "start": "2811380",
    "end": "2818520"
  },
  {
    "text": "you know algorithm to look at different bucket locations and when then we use",
    "start": "2818520",
    "end": "2823650"
  },
  {
    "text": "that simple s3 event triggers to then funnel that into say s and s queues to go off and analyze this image using a",
    "start": "2823650",
    "end": "2831180"
  },
  {
    "text": "different type of deep learning infrastructure for example so that's an",
    "start": "2831180",
    "end": "2836220"
  },
  {
    "text": "easy way to expand that where you having to use things like say the AI army for",
    "start": "2836220",
    "end": "2843270"
  },
  {
    "text": "example and set up all those scaling and so forth so this basically is a you know",
    "start": "2843270",
    "end": "2849030"
  },
  {
    "text": "at the core here the crux is a feedback loop on training and then identifying",
    "start": "2849030",
    "end": "2857010"
  },
  {
    "text": "and then funneling that data to customers or users and then potentially",
    "start": "2857010",
    "end": "2862619"
  },
  {
    "text": "having them provide feedback into the system so you know from the perspective",
    "start": "2862619",
    "end": "2867660"
  },
  {
    "text": "of the pipeline then overlapped on top of this lambda obviously is key to this",
    "start": "2867660",
    "end": "2873480"
  },
  {
    "text": "right so we'll utilize through our data scientists all those images coming in",
    "start": "2873480",
    "end": "2879150"
  },
  {
    "text": "that we're going to Train would land on s3 if they're unlabeled potentially we",
    "start": "2879150",
    "end": "2884220"
  },
  {
    "text": "need to get them labeled and we use Mechanical Turk to generate that ground truth for the training data set which",
    "start": "2884220",
    "end": "2891660"
  },
  {
    "text": "once we've trained it can go back into s3 potentially a different bucket maybe different security because this is now",
    "start": "2891660",
    "end": "2897859"
  },
  {
    "text": "classified as you know intellectual property so we'd want to encrypt the assets have separate buckets lock it",
    "start": "2897859",
    "end": "2904410"
  },
  {
    "text": "down with I am etc and then ec2 right so this would be our trigger to process",
    "start": "2904410",
    "end": "2910230"
  },
  {
    "text": "using these other models and then finally the same API key it way because we can do multiple",
    "start": "2910230",
    "end": "2916270"
  },
  {
    "text": "targets for endpoints could look at a different location for that data or maybe a different table and say Dinamo",
    "start": "2916270",
    "end": "2923860"
  },
  {
    "text": "as an example so lambda you know lambda functions and step functions are key to",
    "start": "2923860",
    "end": "2930820"
  },
  {
    "text": "all of these right eventing of s3 processing the images maybe labeling and",
    "start": "2930820",
    "end": "2936580"
  },
  {
    "text": "putting the data into dynamo and then a feedback loop which can empower these other deep learning processes and then",
    "start": "2936580",
    "end": "2944860"
  },
  {
    "text": "the final delivery obviously to s3 with lambda for additional processing you",
    "start": "2944860",
    "end": "2950350"
  },
  {
    "text": "know as the API calls come in maybe we want to do authentication etc so to sum",
    "start": "2950350",
    "end": "2958330"
  },
  {
    "start": "2957000",
    "end": "2957000"
  },
  {
    "text": "up write the sum up there's no magic bullet as you can see that just passing",
    "start": "2958330",
    "end": "2965260"
  },
  {
    "text": "images off to recognition may not be sufficient right so you let's say as",
    "start": "2965260",
    "end": "2971770"
  },
  {
    "text": "Khan was showing the picture of the image with fog maybe maybe the first step in your step function workflow is",
    "start": "2971770",
    "end": "2978880"
  },
  {
    "text": "the passive recognition identify that there's fog then programmatically say all right now that I know there's fog",
    "start": "2978880",
    "end": "2985720"
  },
  {
    "text": "let me send it to my tool that I know can remove fog without embossing then send it back to recognition to rear",
    "start": "2985720",
    "end": "2992350"
  },
  {
    "text": "ekend eyes then use that data and move forward so you may need to do layering or have a layering approach to getting",
    "start": "2992350",
    "end": "3000960"
  },
  {
    "text": "your results back or even additionally you know if you need to if you really",
    "start": "3000960",
    "end": "3006930"
  },
  {
    "text": "can't recognize within the image think the microscopic image for instance I mean come on you may need to send that",
    "start": "3006930",
    "end": "3012420"
  },
  {
    "text": "as a step in your workflow to Mechanical Turk and have someone who specialized in",
    "start": "3012420",
    "end": "3018090"
  },
  {
    "text": "that actually identify with some that image to get those results backs really you know the focus here as I mentioned",
    "start": "3018090",
    "end": "3023760"
  },
  {
    "text": "National Geographic scientific journal we need high accuracy it goes without",
    "start": "3023760",
    "end": "3028920"
  },
  {
    "text": "saying that you'll want to get the right people involved you know image",
    "start": "3028920",
    "end": "3034380"
  },
  {
    "text": "specialist data scientists coupled with your DevOps team and developers to really make this successful you know how",
    "start": "3034380",
    "end": "3041760"
  },
  {
    "text": "do you how do you know what happens to an image when you scale it up and induce elections and make sure that",
    "start": "3041760",
    "end": "3047540"
  },
  {
    "text": "there's no artifacts and noise how do you reduce that in an image goes without",
    "start": "3047540",
    "end": "3054380"
  },
  {
    "text": "saying as well to not overcomplicate the the infrastructure in the pipeline",
    "start": "3054380",
    "end": "3059390"
  },
  {
    "text": "itself as I mentioned a couple times was you know really trying to get things within a within a safe workflow within a",
    "start": "3059390",
    "end": "3066710"
  },
  {
    "text": "step function workflow so you have something that's scalable reliable it's a single source of truth you don't have",
    "start": "3066710",
    "end": "3073460"
  },
  {
    "text": "to go hunting around in to CES or dynamo to to retrieve results and go back to",
    "start": "3073460",
    "end": "3078560"
  },
  {
    "text": "one location just really follow the kids process keep it simple yeah that and the",
    "start": "3078560",
    "end": "3084500"
  },
  {
    "text": "key thing there is the you know utilizing the managed service to solve 80% of that problem",
    "start": "3084500",
    "end": "3090410"
  },
  {
    "text": "so at least on the media side we deal a lot with customers that are utilizing",
    "start": "3090410",
    "end": "3095599"
  },
  {
    "text": "services such as recognition and in many cases the problem will get attacked at",
    "start": "3095599",
    "end": "3101840"
  },
  {
    "text": "the development level by it doesn't do function X we should just build this custom but the problem there like I said",
    "start": "3101840",
    "end": "3109640"
  },
  {
    "text": "is you know training that versioning at keeping it up doing the blue green or red black deployments etc it takes a lot",
    "start": "3109640",
    "end": "3116810"
  },
  {
    "text": "of time and man-hours especially when you're training against a datasets that you know for this sides size you may",
    "start": "3116810",
    "end": "3124040"
  },
  {
    "text": "need to utilize that entire 30 million tiny image data set to have high",
    "start": "3124040",
    "end": "3129619"
  },
  {
    "text": "accuracy which is a lot of storage and i/o against s3 hydrating the content",
    "start": "3129619",
    "end": "3135470"
  },
  {
    "text": "training at potentially pulling it back on Prem you know if you have clusters of say 10 80s or Tesla cards or whatnot",
    "start": "3135470",
    "end": "3143119"
  },
  {
    "text": "that the data scientists are using so solving at 80% of the problem using",
    "start": "3143119",
    "end": "3148790"
  },
  {
    "text": "recognition and then focusing on that those smaller things either by building them with those teams or by using some",
    "start": "3148790",
    "end": "3156650"
  },
  {
    "text": "of our partners at GRA meta etc that specialize in that you know and then",
    "start": "3156650",
    "end": "3161660"
  },
  {
    "text": "lastly the compute diversification that I talked about you know not everything",
    "start": "3161660",
    "end": "3167359"
  },
  {
    "text": "has to run on a p3 some of the other instance families work really well",
    "start": "3167359",
    "end": "3172750"
  },
  {
    "text": "especially things like the natural language processing you can do like say",
    "start": "3172750",
    "end": "3178040"
  },
  {
    "text": "inference on a bag of words in about 800 milliseconds on an i5",
    "start": "3178040",
    "end": "3184300"
  },
  {
    "text": "CPU so you don't even need a GPU for that and you can potentially look at wrapping that into lambda to even reduce",
    "start": "3184300",
    "end": "3190120"
  },
  {
    "text": "the cost there but you know utilizing FPGA is and especially object storage is",
    "start": "3190120",
    "end": "3196570"
  },
  {
    "text": "the key to all of this along with if you have to use instances do it on spots",
    "start": "3196570",
    "end": "3202180"
  },
  {
    "text": "because inference is fast and if your spot instance gets terminated you know",
    "start": "3202180",
    "end": "3207610"
  },
  {
    "text": "you'll just pick up and resume the next job you potentially won't have an interrupted job there so with that",
    "start": "3207610",
    "end": "3216630"
  },
  {
    "text": "thanks and there are a couple related sessions as well so around OTT will serve a couple others",
    "start": "3217290",
    "end": "3225730"
  },
  {
    "text": "on recognition more deep dives on that and then there are a couple workshops",
    "start": "3225730",
    "end": "3231010"
  },
  {
    "text": "around this as well so we'll take any questions",
    "start": "3231010",
    "end": "3236339"
  },
  {
    "text": "got one of them go ahead",
    "start": "3244100",
    "end": "3249820"
  },
  {
    "text": "I think you have to look at it from the perspective of depending on the",
    "start": "3261670",
    "end": "3267580"
  },
  {
    "text": "framework you select I'll just repeat the question are there particular frameworks that work better for say news",
    "start": "3267580",
    "end": "3274210"
  },
  {
    "text": "media production right and that could be things like motion graphics so on on the",
    "start": "3274210",
    "end": "3279550"
  },
  {
    "text": "one side you can look at some of the developments that are being done to say",
    "start": "3279550",
    "end": "3284590"
  },
  {
    "text": "track people and video frames in other words the optical flow you know where the players moving on the field etc",
    "start": "3284590",
    "end": "3291390"
  },
  {
    "text": "there's a lot of research in that category but as to the actual framework",
    "start": "3291390",
    "end": "3296710"
  },
  {
    "text": "that you deploy on it doesn't really matter but a lot of the frameworks out there",
    "start": "3296710",
    "end": "3302200"
  },
  {
    "text": "like cafe MX net etc we're sort of we don't have a recommended framework but",
    "start": "3302200",
    "end": "3309070"
  },
  {
    "text": "if you look at it from the perspective of how can I get going quickly the two",
    "start": "3309070",
    "end": "3314110"
  },
  {
    "text": "options there are using like cafe has a model zoo most other frameworks have a",
    "start": "3314110",
    "end": "3319300"
  },
  {
    "text": "model zoo as well that you can find a model and then fine tune it but the",
    "start": "3319300",
    "end": "3324340"
  },
  {
    "text": "other option is we also have a now a group within AWS AI labs that will actually from that perspective help",
    "start": "3324340",
    "end": "3332290"
  },
  {
    "text": "build out those models for that use case",
    "start": "3332290",
    "end": "3336060"
  },
  {
    "text": "any other question one question",
    "start": "3342090",
    "end": "3346110"
  },
  {
    "text": "yeah so the question was what about NLP basically right what's going on around",
    "start": "3362770",
    "end": "3369340"
  },
  {
    "text": "that so I mentioned one framework spacy that's actually really high-performance",
    "start": "3369340",
    "end": "3374800"
  },
  {
    "text": "it allows you to load custom models for different languages and you can do the",
    "start": "3374800",
    "end": "3380560"
  },
  {
    "text": "same kind of confidence scoring that you can do with images with Spacey there are also additional frameworks",
    "start": "3380560",
    "end": "3387760"
  },
  {
    "text": "that have been built on top of that such as text acity those are they the underpinnings is you know Sai kit and",
    "start": "3387760",
    "end": "3394840"
  },
  {
    "text": "other Python libraries right for this so they're higher order but they'll do",
    "start": "3394840",
    "end": "3400840"
  },
  {
    "text": "things like you can compare say a banana and a dog versus a dog and a cat and",
    "start": "3400840",
    "end": "3407140"
  },
  {
    "text": "they'll give you a confidence score that the one is an animal but the other ones aren't and you can do that within lambda",
    "start": "3407140",
    "end": "3414070"
  },
  {
    "text": "there is a process to package it correctly compile it reduce the binary size like one of the other sessions I'll",
    "start": "3414070",
    "end": "3420430"
  },
  {
    "text": "talk about doing that but essentially that's a really good option for doing that yeah that's one thing that we're",
    "start": "3420430",
    "end": "3426670"
  },
  {
    "text": "looking to do with the next step after you generate the labels is actually run NLP on top of that and then get those",
    "start": "3426670",
    "end": "3433359"
  },
  {
    "text": "results and then tag those back into that same block so we have you know mammal monkey sea",
    "start": "3433359",
    "end": "3439240"
  },
  {
    "text": "turtle and then you know have run NLP on top of that and have it maybe generate a string a sentiment of what this is about",
    "start": "3439240",
    "end": "3445950"
  },
  {
    "text": "yeah if you look at labels coming back from recognition that's the simplest",
    "start": "3445950",
    "end": "3451150"
  },
  {
    "text": "kind of NLP use case you can have because you're simply comparing terms right we're not having to do topic",
    "start": "3451150",
    "end": "3457510"
  },
  {
    "text": "modeling or anything like that but the interesting use case is if you do have something that potentially there is an",
    "start": "3457510",
    "end": "3464619"
  },
  {
    "text": "image and there's a lot of metadata as sidecar data that's been supplied with that or potentially it's media and",
    "start": "3464619",
    "end": "3470740"
  },
  {
    "text": "there's closed captions you could use those same libraries to normalize the words and do topic modeling against that",
    "start": "3470740",
    "end": "3477760"
  },
  {
    "text": "so that can then be further used to accurately categorize that data if you have text data to go with it any other",
    "start": "3477760",
    "end": "3488470"
  },
  {
    "text": "questions one right here",
    "start": "3488470",
    "end": "3491760"
  },
  {
    "text": "I think that that's a just kind of a core function of the set functions themselves right so you can run a lot of things at once and really handle the",
    "start": "3507670",
    "end": "3514960"
  },
  {
    "text": "parallel processes so we really yep",
    "start": "3514960",
    "end": "3520800"
  },
  {
    "text": "thanks all right thank you [Applause]",
    "start": "3524520",
    "end": "3531489"
  }
]