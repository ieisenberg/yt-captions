[
  {
    "start": "0",
    "end": "59000"
  },
  {
    "text": "all right good morning everybody I think we're gonna get started first off what",
    "start": "1340",
    "end": "8280"
  },
  {
    "text": "an amazing turnout thanks for coming here bright and early bright and early for Vegas course and",
    "start": "8280",
    "end": "14730"
  },
  {
    "text": "I'm delighted to be here to talk to you about redshift and what we've been up to",
    "start": "14730",
    "end": "20250"
  },
  {
    "text": "recently with recent launches as well as some upcoming launches I should",
    "start": "20250",
    "end": "25859"
  },
  {
    "text": "introduce myself I'm Vidya Sreenivasan I'm the general manager for redshift and I'm Co presenting this session with 21st",
    "start": "25859",
    "end": "34410"
  },
  {
    "text": "Century Fox and we have Balaji from Fox and he is going to talk to us about their journey is there data warehousing",
    "start": "34410",
    "end": "42540"
  },
  {
    "text": "journey from an on-premise solution to redshift on the cloud and he's also going to touch on some of the lessons",
    "start": "42540",
    "end": "48809"
  },
  {
    "text": "learnt along the way and some future thoughts on how they expect their architecture to evolve it's a pretty",
    "start": "48809",
    "end": "55680"
  },
  {
    "text": "full session let me get started so before I jump into redshift itself",
    "start": "55680",
    "end": "61949"
  },
  {
    "start": "59000",
    "end": "196000"
  },
  {
    "text": "I just want to orient ourselves because we have a wide set of people who have different backgrounds in terms of where",
    "start": "61949",
    "end": "67799"
  },
  {
    "text": "redshift really fits into our broader big data portfolio so if you look at any",
    "start": "67799",
    "end": "73530"
  },
  {
    "text": "big data application it typically has three phases there is a collection phase where data gets acquired and collected",
    "start": "73530",
    "end": "80460"
  },
  {
    "text": "there is a storage tier where you have to store the data and then finally analyze it using one of our many",
    "start": "80460",
    "end": "87060"
  },
  {
    "text": "solutions I'm just going to call out a couple of these that are relevant for",
    "start": "87060",
    "end": "92070"
  },
  {
    "text": "redshift so for example Kinesis firehose is a service that allows you to directly",
    "start": "92070",
    "end": "98369"
  },
  {
    "text": "stream data this is hot data that's coming in from IOT devices and such",
    "start": "98369",
    "end": "103700"
  },
  {
    "text": "directly into your h of cluster when it comes to storage the key store their",
    "start": "103700",
    "end": "110159"
  },
  {
    "text": "data store for big data applications has become s3 because it's scalable it's durable it's fairly cheap and it's",
    "start": "110159",
    "end": "118049"
  },
  {
    "text": "pretty much become the default landing zone for big data applications when it",
    "start": "118049",
    "end": "124020"
  },
  {
    "text": "comes to analytics we have a bunch of different options redshift of course is the data warehousing service but we also",
    "start": "124020",
    "end": "130259"
  },
  {
    "text": "have other offerings such as aw Amazon EMR which is essentially a",
    "start": "130259",
    "end": "135940"
  },
  {
    "text": "managed solution to do Hadoop spark hi",
    "start": "135940",
    "end": "140950"
  },
  {
    "text": "etc and Amazon Athena which is essentially clustered less query service",
    "start": "140950",
    "end": "147100"
  },
  {
    "text": "for directly querying data on s3 and quick site which is our bi solution now",
    "start": "147100",
    "end": "153459"
  },
  {
    "text": "Tyrande is all out we have a couple of solutions in the bottom here one is DMS data migration service as the",
    "start": "153459",
    "end": "160420"
  },
  {
    "text": "name suggests it's used for migrating data from various sources be it OLTP or",
    "start": "160420",
    "end": "165940"
  },
  {
    "text": "other data warehousing sources into redshift and AWS glue this is a new",
    "start": "165940",
    "end": "171250"
  },
  {
    "text": "service that we announced during the last raiment and it serves two purposes one it acts as a catalog for the data",
    "start": "171250",
    "end": "178870"
  },
  {
    "text": "that you store in s3 so you can know what what you have where you can store schemas etc and it also provides ETL",
    "start": "178870",
    "end": "187060"
  },
  {
    "text": "capabilities so essentially you can set up drops to do orchestration and data transformations so that's just a data",
    "start": "187060",
    "end": "194020"
  },
  {
    "text": "landscape let me also give you a sense for momentum in the market so we",
    "start": "194020",
    "end": "200350"
  },
  {
    "start": "196000",
    "end": "240000"
  },
  {
    "text": "launched ratchet about five years ago at a reinvent and as of second quarter 2017",
    "start": "200350",
    "end": "208150"
  },
  {
    "text": "the Forrester wave called out redshift as being a leader in the data warehousing segment and specifically why",
    "start": "208150",
    "end": "215140"
  },
  {
    "text": "they call out is redshift has some of the largest deployments of data",
    "start": "215140",
    "end": "220209"
  },
  {
    "text": "warehouses as well as the most number of deployments of any cloud solution out",
    "start": "220209",
    "end": "226269"
  },
  {
    "text": "there now this is really thanks to all of you the customers who use the service",
    "start": "226269",
    "end": "231280"
  },
  {
    "text": "who have been supporting it over the years and on all the feedback that you've given us to make this better over",
    "start": "231280",
    "end": "237250"
  },
  {
    "text": "time so very quickly what is retro",
    "start": "237250",
    "end": "244260"
  },
  {
    "start": "240000",
    "end": "354000"
  },
  {
    "text": "redshift is fully managed massively parallel columnar petabyte scale data",
    "start": "244260",
    "end": "252519"
  },
  {
    "text": "data warehouse the couple of things we set out to do when we started this service was to build something that",
    "start": "252519",
    "end": "258430"
  },
  {
    "text": "could deliver really high performance sequel analytics and we wanted to do",
    "start": "258430",
    "end": "263800"
  },
  {
    "text": "this in a way that was cost effective for as well as in a way that did not require",
    "start": "263800",
    "end": "269470"
  },
  {
    "text": "a whole lot of management of the database itself now these are still the",
    "start": "269470",
    "end": "274840"
  },
  {
    "text": "guiding tenets for us as we continue to evolve the service now in the years that we've been in operation we have seen",
    "start": "274840",
    "end": "281830"
  },
  {
    "text": "some changes in the overall Big Data landscape and we've seen some new use cases emerged for analytics that are",
    "start": "281830",
    "end": "289690"
  },
  {
    "text": "slightly different so redshift is a clustered solution where you have a",
    "start": "289690",
    "end": "295000"
  },
  {
    "text": "leader node and some number of compute nodes and you can pick what kind of how many nodes you want you can even pick",
    "start": "295000",
    "end": "301870"
  },
  {
    "text": "the instance type whether you want to have an HDD instance or an SSD instance depending on the cost performance curve",
    "start": "301870",
    "end": "308230"
  },
  {
    "text": "that you wish to hit and it requires the data be ingested into this cluster and then analyzed now in the big nailer",
    "start": "308230",
    "end": "316360"
  },
  {
    "text": "space in the meantime what we see is people have started to collect and first of all generate collect and then store",
    "start": "316360",
    "end": "322930"
  },
  {
    "text": "really you've asked what volumes of data and this data is arriving at a really",
    "start": "322930",
    "end": "328090"
  },
  {
    "text": "high rate so now there's all this data pooled and s3 and we see this with our",
    "start": "328090",
    "end": "333820"
  },
  {
    "text": "s3 customers this is all this data sitting there and people want to drive value out of that now at Ko redshift is a high-performance",
    "start": "333820",
    "end": "341500"
  },
  {
    "text": "sequel processor and we also want to support use cases that leverage the data",
    "start": "341500",
    "end": "347110"
  },
  {
    "text": "in this data Lake what's being called as a data Lake so to do that we announce an",
    "start": "347110",
    "end": "355450"
  },
  {
    "start": "354000",
    "end": "492000"
  },
  {
    "text": "extension to redshift a new feature called redshift spectrum earlier this year excuse me so spectrum essentially",
    "start": "355450",
    "end": "367630"
  },
  {
    "text": "is an extension to the redshift query engine that allows you to directly query",
    "start": "367630",
    "end": "373090"
  },
  {
    "text": "your data and s3 without requiring any ingestion or transformations meaning you",
    "start": "373090",
    "end": "378730"
  },
  {
    "text": "can directly query data in open formats this is quite powerful some of the use",
    "start": "378730",
    "end": "387370"
  },
  {
    "text": "cases that we see popular with spectrum because it has been and used for some time now is people like to use spectrum",
    "start": "387370",
    "end": "393760"
  },
  {
    "text": "when they have really hard data and they want to do analytics all that immediately",
    "start": "393760",
    "end": "399289"
  },
  {
    "text": "as data gets streamed into S into s3 there is really no time to do the ingest",
    "start": "399289",
    "end": "404300"
  },
  {
    "text": "into the cluster and people run a query across data on s3 as well as data on the cluster as the data arrives the other",
    "start": "404300",
    "end": "413180"
  },
  {
    "text": "popular use case with spectrum is when you don't access data for very often so",
    "start": "413180",
    "end": "418729"
  },
  {
    "text": "it's for the less frequently accessed data that still has to be kept around for compliance purposes or maybe you",
    "start": "418729",
    "end": "424669"
  },
  {
    "text": "want to do some analysis once a year to see how trends are over the last 10",
    "start": "424669",
    "end": "429949"
  },
  {
    "text": "years but it's not really worth it to store all of that in your warehouse and replicate the data so data can be in s3",
    "start": "429949",
    "end": "437360"
  },
  {
    "text": "so that it can be available for historical analysis except it doesn't have to be in your active data warehouse",
    "start": "437360",
    "end": "443899"
  },
  {
    "text": "and spectrum allows you to do that now as you can imagine the scheme for how",
    "start": "443899",
    "end": "450919"
  },
  {
    "text": "you pay for this is also different for redshift you pay for the number of nodes that you use in the time that you use",
    "start": "450919",
    "end": "457039"
  },
  {
    "text": "those nodes it has both the storage as well as the compute with spectrum the",
    "start": "457039",
    "end": "462289"
  },
  {
    "text": "payment model changes we move to a paper query type model where you pay for the",
    "start": "462289",
    "end": "469339"
  },
  {
    "text": "terabyte scanned for the particular spectrum query that got run and we charged $5 per compressed terabytes",
    "start": "469339",
    "end": "476629"
  },
  {
    "text": "can't so again it's a lot of flexibility that you can store your data either in thruster or an s3 and you can choose to",
    "start": "476629",
    "end": "483740"
  },
  {
    "text": "move move it between these two places but still query across the entire data set as if it were one unit let me spend",
    "start": "483740",
    "end": "494209"
  },
  {
    "start": "492000",
    "end": "610000"
  },
  {
    "text": "a minute on the architecture of spectrum so to make spectrum possible we",
    "start": "494209",
    "end": "499490"
  },
  {
    "text": "essentially had to do two things the first thing is we created this elastic",
    "start": "499490",
    "end": "504769"
  },
  {
    "text": "pool of compute nodes this is the purple layer that you see here that act as that",
    "start": "504769",
    "end": "510559"
  },
  {
    "text": "can take tasks from redshift and exq that on the data and s3 so there they",
    "start": "510559",
    "end": "516318"
  },
  {
    "text": "would ingest the data as well as execute tasks on the data from s3 and this pool",
    "start": "516319",
    "end": "522258"
  },
  {
    "text": "can is can be arbitrarily large and it's an auto scaling multi-tenant fleet that",
    "start": "522259",
    "end": "527689"
  },
  {
    "text": "just gets used with every single dredge of cluster as and when",
    "start": "527689",
    "end": "532720"
  },
  {
    "text": "the second thing we had to do was we had to build quite a bit of intelligence into the redshifts optimizer itself we",
    "start": "532720",
    "end": "540019"
  },
  {
    "text": "had to make it smarter to understand how to leverage the spectrum fleet and",
    "start": "540019",
    "end": "545079"
  },
  {
    "text": "essentially execute queries against the data and s3 but still with really good",
    "start": "545079",
    "end": "551779"
  },
  {
    "text": "performance and to do that well we had to make sure that we don't scan more",
    "start": "551779",
    "end": "557149"
  },
  {
    "text": "data than we absolutely need to and we also have to make sure that the data that gets moved across these layers so",
    "start": "557149",
    "end": "563240"
  },
  {
    "text": "between s3 and spectrum and spectrum and the cluster is minimized so a lot of these optimized mize the",
    "start": "563240",
    "end": "570559"
  },
  {
    "text": "changes went into making spectrum both very fast and very scalable one of the",
    "start": "570559",
    "end": "577399"
  },
  {
    "text": "interesting use cases that have evolved after we had spectrum in the field was customers started to use this pretty",
    "start": "577399",
    "end": "584629"
  },
  {
    "text": "much without any local data so they would even just stand up transient less rich of clusters and you can have any",
    "start": "584629",
    "end": "591170"
  },
  {
    "text": "number of them and directly operate against data sets and s3 the only",
    "start": "591170",
    "end": "596329"
  },
  {
    "text": "requirement is the s3 data sets have to be catalogued they have to be registered either with a high meta store or they",
    "start": "596329",
    "end": "602990"
  },
  {
    "text": "have to be registered with your now with the AWS Glu catalog you're now going to",
    "start": "602990",
    "end": "611929"
  },
  {
    "start": "610000",
    "end": "682000"
  },
  {
    "text": "go into all the details here at the pretty busy chart but the only point I wanted to make is Spectrum affords us",
    "start": "611929",
    "end": "617839"
  },
  {
    "text": "another order of magnitude increase in parallelism to make that point let me",
    "start": "617839",
    "end": "624350"
  },
  {
    "text": "just show it to you in this diagram so if you see here from every redshift node in fact every slice in the register of",
    "start": "624350",
    "end": "631129"
  },
  {
    "text": "node can I can't access up to ten Spectrum nodes so if you look at ds2 8xl",
    "start": "631129",
    "end": "638749"
  },
  {
    "text": "it has 16 slices and each of these slices can siphon off tasks to another 10 nodes which makes for some ridiculous",
    "start": "638749",
    "end": "646220"
  },
  {
    "text": "levels of parallelism and this allows us to scale across really large data sets and have very good query performance and",
    "start": "646220",
    "end": "653949"
  },
  {
    "text": "we actually did a test where we ran a complex query this had a couple of",
    "start": "653949",
    "end": "659569"
  },
  {
    "text": "joints as the Karim Iran and some aggregates across the exabyte",
    "start": "659569",
    "end": "664660"
  },
  {
    "text": "sighs dataset and we had the results for this query return in under three minutes",
    "start": "664660",
    "end": "671050"
  },
  {
    "text": "I mean there were multiple techniques that were in place to make this happen but essentially the architecture and the",
    "start": "671050",
    "end": "677170"
  },
  {
    "text": "optimizer changes were what made it possible and here is a customer example",
    "start": "677170",
    "end": "685029"
  },
  {
    "start": "682000",
    "end": "766000"
  },
  {
    "text": "so Nuvi ad is one of the largest attic providers in israel and they do both",
    "start": "685029",
    "end": "691870"
  },
  {
    "text": "real-time bidding as well as analytics on and the ads that get published now they had as particular requirement",
    "start": "691870",
    "end": "699370"
  },
  {
    "text": "where they had both a large data set they had petabytes of data and they were also creating data at a very high rate",
    "start": "699370",
    "end": "706060"
  },
  {
    "text": "data was they were processing 700,000 transactions per second from over 50",
    "start": "706060",
    "end": "711300"
  },
  {
    "text": "channels and so they needed to provide these analytic responses to their",
    "start": "711300",
    "end": "718180"
  },
  {
    "text": "customers as soon as the data landed ingestion just wasn't an option for them because it would just slow things down",
    "start": "718180",
    "end": "724259"
  },
  {
    "text": "so they leveraged spectrum to essentially get to data as it was",
    "start": "724259",
    "end": "729490"
  },
  {
    "text": "arriving in the format that it was arriving and they did this with multiple",
    "start": "729490",
    "end": "735490"
  },
  {
    "text": "clusters as well and once you have multiple clusters you can pretty much increase the number of users the number",
    "start": "735490",
    "end": "742540"
  },
  {
    "text": "of concurrent queries you want to run to arbitrary amounts so this essentially",
    "start": "742540",
    "end": "748240"
  },
  {
    "text": "gave them a way to scale the storage and s3 pretty much independently of their",
    "start": "748240",
    "end": "754540"
  },
  {
    "text": "compute needs because they could spin up a spectrum and retrieve classes as they needed depending on the particular",
    "start": "754540",
    "end": "761079"
  },
  {
    "text": "workload and query volume at that point in time",
    "start": "761079",
    "end": "765449"
  },
  {
    "start": "766000",
    "end": "791000"
  },
  {
    "text": "switching gears a little bit redshift is considered to be one of the core",
    "start": "767889",
    "end": "773089"
  },
  {
    "text": "services for a EWS and what that means is when AWS launches in a new region redshift is there as part of the initial",
    "start": "773089",
    "end": "780889"
  },
  {
    "text": "launch so we are there in 14 regions and adding three more so pretty it's pretty",
    "start": "780889",
    "end": "787160"
  },
  {
    "text": "widely deployed across lots of places and we are being used by a pretty wide",
    "start": "787160",
    "end": "795319"
  },
  {
    "start": "791000",
    "end": "841000"
  },
  {
    "text": "variety of customers I just want to point that out we have customers who have couple of hundred gigabytes in the service as well",
    "start": "795319",
    "end": "801470"
  },
  {
    "text": "as several petabytes in fact amazon.com stores a very large volume of data and",
    "start": "801470",
    "end": "808339"
  },
  {
    "text": "runs a lot of their analytics on redshift Black Friday is an interesting",
    "start": "808339",
    "end": "814459"
  },
  {
    "text": "day for the service we also have customers just across different verticals whether it's healthcare",
    "start": "814459",
    "end": "820850"
  },
  {
    "text": "finance etc and spending a lot of time on compliance and making sure we had",
    "start": "820850",
    "end": "826130"
  },
  {
    "text": "HIPAA compliance FedRAMP sock 1-2-3 all of these things has helped quite a bit with getting these customers the",
    "start": "826130",
    "end": "833569"
  },
  {
    "text": "necessary level of security needed so that they can use retrofit as their choice for data warehousing a word on",
    "start": "833569",
    "end": "844130"
  },
  {
    "start": "841000",
    "end": "896000"
  },
  {
    "text": "partners so we think so working with partners has always been a part of the service right from the beginning and we",
    "start": "844130",
    "end": "849860"
  },
  {
    "text": "think it's super critical that we invest a lot of energy in that because our goal is retro just slides into your existing",
    "start": "849860",
    "end": "857870"
  },
  {
    "text": "infrastructure to the degree possible and that's possible only if we work with",
    "start": "857870",
    "end": "863389"
  },
  {
    "text": "the tools you already use so be it ETL tools or BI tools we just want to work",
    "start": "863389",
    "end": "869480"
  },
  {
    "text": "with the existing infrastructure that you have in place now having a sequel interface and JDBC ODBC helps us quite a",
    "start": "869480",
    "end": "876560"
  },
  {
    "text": "bit because these are all standardized but even beyond that we spend quite a bit of time working with our partners to",
    "start": "876560",
    "end": "882470"
  },
  {
    "text": "make sure that as we evolve when they evolve they still stay in sync in fact",
    "start": "882470",
    "end": "888350"
  },
  {
    "text": "with this fox use case we work pretty closely with informatica to make sure the implementation went through without",
    "start": "888350",
    "end": "894259"
  },
  {
    "text": "a hitch all right so moving on to some exciting stuff",
    "start": "894259",
    "end": "900420"
  },
  {
    "start": "896000",
    "end": "1001000"
  },
  {
    "text": "some more recent launches so we recently launched a new node type for our SSD",
    "start": "900420",
    "end": "908279"
  },
  {
    "text": "family called DC to the prior version was called DC one no surprises there and",
    "start": "908279",
    "end": "916879"
  },
  {
    "text": "you know if I could do a Black Friday deal for retro of this would be it because this instance offers on average",
    "start": "916879",
    "end": "925439"
  },
  {
    "text": "2x better performance for the same price of DC one so if you're running on DC one",
    "start": "925439",
    "end": "931379"
  },
  {
    "text": "I strongly encourage you to try this out and these sorts of big numbers come",
    "start": "931379",
    "end": "936929"
  },
  {
    "text": "about you to both hardware changes just the platform is better this is just better instances but also software",
    "start": "936929",
    "end": "942480"
  },
  {
    "text": "changes that we've made so in terms of the hardware itself DC tools used nvme",
    "start": "942480",
    "end": "948720"
  },
  {
    "text": "drives and these offer better bandwidth but more importantly these offer much",
    "start": "948720",
    "end": "954959"
  },
  {
    "text": "higher IAP setting its 1.6 million i ops for a DC twos and that makes a significant impact to the workloads that",
    "start": "954959",
    "end": "962399"
  },
  {
    "text": "you run on top of it additionally the bandwidth from DC to to",
    "start": "962399",
    "end": "967889"
  },
  {
    "text": "s3 is also double what is available in DC one and that makes a big change to",
    "start": "967889",
    "end": "972959"
  },
  {
    "text": "how we run copies and backups and all the operations that go back to s3 these",
    "start": "972959",
    "end": "979290"
  },
  {
    "text": "are also based on a custom I'd customized Broadwell chipset versus the Ivy Bridge that DC ones had and here is",
    "start": "979290",
    "end": "987059"
  },
  {
    "text": "a customer quote on the 9x improvement in response times that they saw but",
    "start": "987059",
    "end": "993540"
  },
  {
    "text": "we've had pretty immediate adoption of this platform just based on performance",
    "start": "993540",
    "end": "998579"
  },
  {
    "text": "results the next thing that I wanted",
    "start": "998579",
    "end": "1004699"
  },
  {
    "start": "1001000",
    "end": "1101000"
  },
  {
    "text": "pre-announce this is not out yet",
    "start": "1004699",
    "end": "1008738"
  },
  {
    "text": "is results at cashing as the name suggests this is a feature where we",
    "start": "1010089",
    "end": "1015629"
  },
  {
    "text": "figure out results that are worthy of being cashed because they're being used",
    "start": "1015629",
    "end": "1021550"
  },
  {
    "text": "again and we cash them the cash itself resides in memory on the leader node and",
    "start": "1021550",
    "end": "1028140"
  },
  {
    "text": "this if you have a workload where you do run the same queries over and over again",
    "start": "1028140",
    "end": "1033220"
  },
  {
    "text": "which is pretty typical of - boring workloads this can be a very significant",
    "start": "1033220",
    "end": "1038470"
  },
  {
    "text": "performance improvement to what you see otherwise and essentially what this",
    "start": "1038470",
    "end": "1044199"
  },
  {
    "text": "allows us to do is base skip the wlm queues keep processing no need to do any kind of optimizations and",
    "start": "1044199",
    "end": "1050919"
  },
  {
    "text": "directly just gets to the results and pretty much the best part of this feature is that it just works by default",
    "start": "1050919",
    "end": "1059020"
  },
  {
    "text": "it's transparent to you as a user once this gets rolled out which is going to",
    "start": "1059020",
    "end": "1064450"
  },
  {
    "text": "happen in the next couple of weeks the clusters that are currently running will just automatically start using them and",
    "start": "1064450",
    "end": "1073630"
  },
  {
    "text": "the other point where this is apart from the queries that can be cached it also",
    "start": "1073630",
    "end": "1080860"
  },
  {
    "text": "impacts the queries that are running that cannot be cached like the copies in the vacuum and everything else because",
    "start": "1080860",
    "end": "1086679"
  },
  {
    "text": "they just have more resources available to work on them everything around what",
    "start": "1086679",
    "end": "1092409"
  },
  {
    "text": "is to be cached when it's to be invalidated all of that gets taken care of internally by the service there are",
    "start": "1092409",
    "end": "1098289"
  },
  {
    "text": "really no knobs to tune here so here is",
    "start": "1098289",
    "end": "1103720"
  },
  {
    "start": "1101000",
    "end": "1191000"
  },
  {
    "text": "an example result of course this is a workflow that we have internally this is",
    "start": "1103720",
    "end": "1109720"
  },
  {
    "text": "a workflow that mimics customer workloads so we basically have something that has a bunch of copies and vacuum",
    "start": "1109720",
    "end": "1116649"
  },
  {
    "text": "inserts all of that happening at the same time and it has a combination of queries that can be cached and that",
    "start": "1116649",
    "end": "1122350"
  },
  {
    "text": "cannot be cached and we saw some pretty significant increase in throughput when we turned on results at caching so the",
    "start": "1122350",
    "end": "1129370"
  },
  {
    "text": "red bar here is the new throughput with the cache on and of course it affects queries that benefit from it and not so",
    "start": "1129370",
    "end": "1136360"
  },
  {
    "text": "much where is it don't benefit from it now here is another result and this is",
    "start": "1136360",
    "end": "1143020"
  },
  {
    "text": "something we got from a customer that sure talks to the latency of",
    "start": "1143020",
    "end": "1148270"
  },
  {
    "text": "queries not the throughput by the latency so that the part below here",
    "start": "1148270",
    "end": "1154570"
  },
  {
    "text": "shows the latency so these are all dashboard queries on the y-axis each one is a different query and this shows the",
    "start": "1154570",
    "end": "1160600"
  },
  {
    "text": "latency that they had prior to turning on results at caching and the bar above",
    "start": "1160600",
    "end": "1166539"
  },
  {
    "text": "is what they saw after turning on caching and they actually let us know this is not a mistake it's just that it",
    "start": "1166539",
    "end": "1173200"
  },
  {
    "text": "was so fast that they couldn't actually plot it now granted this is a workload that benefited from crack caching every",
    "start": "1173200",
    "end": "1180399"
  },
  {
    "text": "workload isn't going to be so dramatic but it can still be pretty meaningful because typically we see workers have a",
    "start": "1180399",
    "end": "1186789"
  },
  {
    "text": "combination of queries that get repeated and those that don't",
    "start": "1186789",
    "end": "1191640"
  },
  {
    "start": "1191000",
    "end": "1345000"
  },
  {
    "text": "okay so the next feature I'd like to talk about is short query acceleration",
    "start": "1192059",
    "end": "1197309"
  },
  {
    "text": "this is another performance feature so the again let's spend a minute about the",
    "start": "1197309",
    "end": "1204700"
  },
  {
    "text": "workload itself so on a typical workload you have queries that are short running",
    "start": "1204700",
    "end": "1209860"
  },
  {
    "text": "meaning they take a couple of seconds to execute typically for short running queries what we see is it's usually a",
    "start": "1209860",
    "end": "1216610"
  },
  {
    "text": "user sitting on the other end writing this query and they expect an answer",
    "start": "1216610",
    "end": "1221710"
  },
  {
    "text": "within a short amount of time and they expect expect consistent performance of these queries because they're probably",
    "start": "1221710",
    "end": "1226929"
  },
  {
    "text": "just getting up in the morning and sending a bunch of queries to see what happened with sales the prior day and",
    "start": "1226929",
    "end": "1233070"
  },
  {
    "text": "then there are long-running queries which could be copies answers but also long dashboarding queries etc but this",
    "start": "1233070",
    "end": "1241750"
  },
  {
    "text": "feature our goal is to build a system where we automatically identify the",
    "start": "1241750",
    "end": "1247059"
  },
  {
    "text": "short running queries and essentially create an express lane for them and and",
    "start": "1247059",
    "end": "1252630"
  },
  {
    "text": "it comes at a cost the cost that you pay for creating the excess Lane is some of the long-running queries the things that",
    "start": "1252630",
    "end": "1259149"
  },
  {
    "text": "take 45 minutes will take a little bit longer they might take 47 minutes but in our",
    "start": "1259149",
    "end": "1264909"
  },
  {
    "text": "experience and in talking to customers that hasn't actually mattered because the response of queries need to come",
    "start": "1264909",
    "end": "1270880"
  },
  {
    "text": "back if something takes two minutes over 45 minutes it's not really meaningful from a customer experience point of view",
    "start": "1270880",
    "end": "1278460"
  },
  {
    "text": "so that so that was the goal of the feature now it is not always easy for",
    "start": "1278460",
    "end": "1284740"
  },
  {
    "text": "customers to figure out which of the queries coming in are short running queries so we take over the burden of",
    "start": "1284740",
    "end": "1291250"
  },
  {
    "text": "figuring that out and automatically moving them to the express lane and the way we do it is one we have an optimizer",
    "start": "1291250",
    "end": "1297039"
  },
  {
    "text": "that gives us a cost that the query will take but the optimizer isn't relief of",
    "start": "1297039",
    "end": "1302889"
  },
  {
    "text": "the wear of the current run times of the cluster so how loaded is the cluster how",
    "start": "1302889",
    "end": "1308590"
  },
  {
    "text": "many users are on it and things like that so we actually embedded a machine learning algorithm a classifier that can",
    "start": "1308590",
    "end": "1314799"
  },
  {
    "text": "basically learn from your system from your running cluster how to take how to",
    "start": "1314799",
    "end": "1320080"
  },
  {
    "text": "take the set of queries that you have and batch them into short running and long running queries and start with an",
    "start": "1320080",
    "end": "1326889"
  },
  {
    "text": "initial classifier and relearn's every 50 queries so that way similar to a",
    "start": "1326889",
    "end": "1333580"
  },
  {
    "text": "result set caching this is a feature that's just transparent it just works once you have this patch on your cluster",
    "start": "1333580",
    "end": "1340600"
  },
  {
    "text": "is just going to be active and speed up the short running queries now here is a",
    "start": "1340600",
    "end": "1347110"
  },
  {
    "start": "1345000",
    "end": "1382000"
  },
  {
    "text": "graph that shows some results based on this so the orange line is the latency",
    "start": "1347110",
    "end": "1355809"
  },
  {
    "text": "for queries when this feature was not turned on and as you can see the latency",
    "start": "1355809",
    "end": "1364000"
  },
  {
    "text": "for queries that don't take a very long time this is the query latency it's pretty high without this feature on and",
    "start": "1364000",
    "end": "1371110"
  },
  {
    "text": "it goes down significantly once we turn it on but the latency of the",
    "start": "1371110",
    "end": "1376330"
  },
  {
    "text": "long-running queries do increase a little bit but that's the trade-off that we've made all right the next feature",
    "start": "1376330",
    "end": "1385360"
  },
  {
    "start": "1382000",
    "end": "1517000"
  },
  {
    "text": "I'd like to announce is support for nested data so today you can use nested",
    "start": "1385360",
    "end": "1392110"
  },
  {
    "text": "data with redshift but to do so you will have to ingest the data as a string into",
    "start": "1392110",
    "end": "1397330"
  },
  {
    "text": "one of the fields into a string field and then use functions to essentially query parts of the nested data we just",
    "start": "1397330",
    "end": "1404860"
  },
  {
    "text": "wanted to make this super simple with this feature we are allowing you to query nested data using a very intuitive dot",
    "start": "1404860",
    "end": "1412650"
  },
  {
    "text": "notation using spectrum so you don't even have to ingest the data you can just directly query the data as this",
    "start": "1412650",
    "end": "1420230"
  },
  {
    "text": "using using an extension of sequel to access the nested fields within the data",
    "start": "1420230",
    "end": "1426990"
  },
  {
    "text": "type so here we have an example or I did the wrong thing so here is an example",
    "start": "1426990",
    "end": "1434929"
  },
  {
    "text": "this is a JSON file that's pretty typical of clickstream data it has a",
    "start": "1434929",
    "end": "1440850"
  },
  {
    "text": "nested field for clicks and here is a sequel to worry that basically computes",
    "start": "1440850",
    "end": "1446130"
  },
  {
    "text": "how many clicks per page for every link on the home page how many clicks have",
    "start": "1446130",
    "end": "1451770"
  },
  {
    "text": "happened for each of those things and to access the clicks field we just had to use a dot notation to get to the clicks",
    "start": "1451770",
    "end": "1459780"
  },
  {
    "text": "and then compute the aggregate so it makes it very easy to query the data in this format and we support open formats",
    "start": "1459780",
    "end": "1468750"
  },
  {
    "text": "like 4k or C JSON etc I think one of the side benefits of this feature that we've",
    "start": "1468750",
    "end": "1475740"
  },
  {
    "text": "heard from customers is you know it's great that we can query the data but sometimes I just want to ingest this",
    "start": "1475740",
    "end": "1481620"
  },
  {
    "text": "nested data or portions of it into my cluster I do that today and this feature actually allows you to write Sita's jobs",
    "start": "1481620",
    "end": "1488970"
  },
  {
    "text": "that are far simpler than what you can do today so rather than so if you want to transform and load data that is",
    "start": "1488970",
    "end": "1496410"
  },
  {
    "text": "nested into the cluster into flat to normalize tables this can still help you just run a citas command for that and",
    "start": "1496410",
    "end": "1502830"
  },
  {
    "text": "move the data in using sequel that is far simpler than running a different job",
    "start": "1502830",
    "end": "1507990"
  },
  {
    "text": "to extract those fields for use within your data warehouse so as sort of the side benefit of of this feature",
    "start": "1507990",
    "end": "1516529"
  },
  {
    "start": "1517000",
    "end": "1567000"
  },
  {
    "text": "so here is a an example where it's an orders and customer table where this I'm",
    "start": "1518410",
    "end": "1527110"
  },
  {
    "text": "just bad with this clicker we're on the top it's a it's a normalized table if",
    "start": "1527110",
    "end": "1533050"
  },
  {
    "text": "you were to put it in a data warehouse in a flat format and this is how it",
    "start": "1533050",
    "end": "1538450"
  },
  {
    "text": "would look in nested format the only point I want to make is with nested data",
    "start": "1538450",
    "end": "1543760"
  },
  {
    "text": "you do get better query performance because essentially the the fact that",
    "start": "1543760",
    "end": "1548890"
  },
  {
    "text": "it's nested is equivalent to have done a pre join of those tables so if your",
    "start": "1548890",
    "end": "1554140"
  },
  {
    "text": "query patterns such that you're going to join the data anyway it's worthy to think about hey maybe you want to leave",
    "start": "1554140",
    "end": "1560620"
  },
  {
    "text": "it in the nested format and query it directly because it'll actually perform better and the last thing that I want to",
    "start": "1560620",
    "end": "1570580"
  },
  {
    "start": "1567000",
    "end": "1649000"
  },
  {
    "text": "announce today is enhanced monitoring so we have a console for rich of general",
    "start": "1570580",
    "end": "1578140"
  },
  {
    "text": "administrative console where you you get different parameters about CPU usage i/o",
    "start": "1578140",
    "end": "1583300"
  },
  {
    "text": "rates etc we are putting a lot of energy into improving that entire console",
    "start": "1583300",
    "end": "1589330"
  },
  {
    "text": "experience to manage cluster and have better visibility into what is actually happening and the goal here is to",
    "start": "1589330",
    "end": "1596380"
  },
  {
    "text": "understand what is happening with your cluster easier and also to troubleshoot in case you end up having issues for",
    "start": "1596380",
    "end": "1604450"
  },
  {
    "text": "simple problems like if you feel like this cluster is slow we want to be able to give you enough data on the console",
    "start": "1604450",
    "end": "1610540"
  },
  {
    "text": "that lets you understand what might have happened to make that happen now first of all to even know in in absolute terms",
    "start": "1610540",
    "end": "1618340"
  },
  {
    "text": "if indeed the cluster is slow if something else is going on so a couple of fields that we will have immediately",
    "start": "1618340",
    "end": "1624310"
  },
  {
    "text": "is a metric for query throughput and query latency and these are metrics for",
    "start": "1624310",
    "end": "1631120"
  },
  {
    "text": "me these are five-minute metrics that get aggregated over five-minute periods but then you can display them for the",
    "start": "1631120",
    "end": "1637210"
  },
  {
    "text": "time frame you can pick the time frame that you want to display them and this is really just a start this is just two",
    "start": "1637210",
    "end": "1642700"
  },
  {
    "text": "things that we are going to start with but you will see a lot more along monitoring",
    "start": "1642700",
    "end": "1648780"
  },
  {
    "start": "1649000",
    "end": "1758000"
  },
  {
    "text": "some closing thoughts before I hand off so highly recommend folks who use DC",
    "start": "1649680",
    "end": "1656770"
  },
  {
    "text": "ones to look at DC - especially the eight Excel platform is significantly",
    "start": "1656770",
    "end": "1662800"
  },
  {
    "text": "better in performance and in fact it's true for the event ds2 customers if you",
    "start": "1662800",
    "end": "1668230"
  },
  {
    "text": "fear using a ds2 cluster but it's not very high in storage utilization it's",
    "start": "1668230",
    "end": "1673450"
  },
  {
    "text": "worthwhile to take a look at DC - as we",
    "start": "1673450",
    "end": "1679180"
  },
  {
    "text": "talked talked today's spectrum extends the capabilities of redshift and provides a great deal of flexibility in",
    "start": "1679180",
    "end": "1686260"
  },
  {
    "text": "how you can organize your data and query on it and there are more features coming",
    "start": "1686260",
    "end": "1691570"
  },
  {
    "text": "for it such as a nested data suppose there will be more again if you haven't tried it it's worthy to check that out",
    "start": "1691570",
    "end": "1697800"
  },
  {
    "text": "and there's there are significant improvements I mean we talked about results at caching and short query",
    "start": "1697800",
    "end": "1704470"
  },
  {
    "text": "acceleration today we announced another feature called query monitoring rules earlier this year which essentially",
    "start": "1704470",
    "end": "1711250"
  },
  {
    "text": "allows you to set up rules for your cluster for your queues in the cluster so that you can automatically either",
    "start": "1711250",
    "end": "1719380"
  },
  {
    "text": "kill or hop runaway queries so if a query is spilling a lot to disk or the",
    "start": "1719380",
    "end": "1725140"
  },
  {
    "text": "fish is going to hog up all the resources in the cluster you can just automatically kill it when these",
    "start": "1725140",
    "end": "1730450"
  },
  {
    "text": "features sort of come together and I used in tandem we see customers able to drive pretty significant throughput",
    "start": "1730450",
    "end": "1737050"
  },
  {
    "text": "improvements to their clusters so again something to think about and look at and as always if you have questions",
    "start": "1737050",
    "end": "1743590"
  },
  {
    "text": "suggestions feedback for us please use this alias to let us know thank you",
    "start": "1743590",
    "end": "1749950"
  },
  {
    "text": "[Applause] thank you Jeff good morning everyone",
    "start": "1749950",
    "end": "1761240"
  },
  {
    "text": "I am Balaji muta ramalingam executive director data and analytics at 21st Century Fox next 15 minutes I am going",
    "start": "1761240",
    "end": "1769220"
  },
  {
    "text": "to share our most recent analytics modernization project experiences that we did it for our studio side of the",
    "start": "1769220",
    "end": "1775250"
  },
  {
    "text": "business Fox film entertainment it's a part of 21st Century Fox and we are one of the",
    "start": "1775250",
    "end": "1782750"
  },
  {
    "text": "largest movie making studios in the world we produce and distribute great",
    "start": "1782750",
    "end": "1788059"
  },
  {
    "text": "content reaching global audience in theatres and at home through this",
    "start": "1788059",
    "end": "1795080"
  },
  {
    "text": "customer platforms and broadcasters in 193 countries and 63 languages",
    "start": "1795080",
    "end": "1802540"
  },
  {
    "text": "we have many analytical processes and solutions that support our business decision making process let me give you",
    "start": "1802540",
    "end": "1810799"
  },
  {
    "text": "key highlights of those processes we",
    "start": "1810799",
    "end": "1815840"
  },
  {
    "text": "process about hundred terabytes of data on a daily basis there are about 25,000",
    "start": "1815840",
    "end": "1822290"
  },
  {
    "text": "user requests run on our system having",
    "start": "1822290",
    "end": "1827900"
  },
  {
    "text": "more than 100 data providers provide data to us also about 3,500 35,000 data",
    "start": "1827900",
    "end": "1835340"
  },
  {
    "text": "pipelines that process the data crunch the data on any given day we have many",
    "start": "1835340",
    "end": "1843799"
  },
  {
    "start": "1843000",
    "end": "1915000"
  },
  {
    "text": "challenges that go beyond our technology in general there is a huge digital",
    "start": "1843799",
    "end": "1850460"
  },
  {
    "text": "transformation in our media industry our consumer is changing they have many",
    "start": "1850460",
    "end": "1856370"
  },
  {
    "text": "options now in the way they how they consume our content it called our system",
    "start": "1856370",
    "end": "1863059"
  },
  {
    "text": "to be much faster in terms of ingesting the data providing a quicker inside also",
    "start": "1863059",
    "end": "1871000"
  },
  {
    "text": "it called us to process vast amount of",
    "start": "1871000",
    "end": "1876260"
  },
  {
    "text": "data and variety of data on the other hand we were in a traditional data",
    "start": "1876260",
    "end": "1882679"
  },
  {
    "text": "warehousing platform at its end of life having multiple unplanned outages",
    "start": "1882679",
    "end": "1890510"
  },
  {
    "text": "struggling to scale we had many options",
    "start": "1890510",
    "end": "1897870"
  },
  {
    "text": "but we know that it is not going to help us to grow in terms of meeting our",
    "start": "1897870",
    "end": "1904260"
  },
  {
    "text": "business needs so we decided to modernize our analytical platform with a",
    "start": "1904260",
    "end": "1912330"
  },
  {
    "text": "few key principles here are our key principles first democracy in the data",
    "start": "1912330",
    "end": "1922590"
  },
  {
    "start": "1915000",
    "end": "2010000"
  },
  {
    "text": "again it's a wide and broader subject to discuss but we are very clear we want to",
    "start": "1922590",
    "end": "1931260"
  },
  {
    "text": "bring all our data to a common platform and bring down our data silos and",
    "start": "1931260",
    "end": "1937340"
  },
  {
    "text": "provide a cross-functional visibility to our business so that they can take a",
    "start": "1937340",
    "end": "1943130"
  },
  {
    "text": "best decision out of our own data next",
    "start": "1943130",
    "end": "1948200"
  },
  {
    "text": "cloud leverage the cloud that offers in terms of scale and elasticity through",
    "start": "1948200",
    "end": "1955470"
  },
  {
    "text": "which design our solutions much faster in nimble in nature also we want to have",
    "start": "1955470",
    "end": "1964140"
  },
  {
    "text": "a cost-efficient solution so that we can scale so considering all our",
    "start": "1964140",
    "end": "1972049"
  },
  {
    "text": "complexities enterprise scale in nature we were looking for a solution or a",
    "start": "1972110",
    "end": "1979700"
  },
  {
    "text": "partner with the white suite of analytical tools and it's our natural",
    "start": "1979700",
    "end": "1986340"
  },
  {
    "text": "choice to partner with Amazon we",
    "start": "1986340",
    "end": "1991440"
  },
  {
    "text": "partnered together we finalized our approach in architectures by conducting",
    "start": "1991440",
    "end": "1998520"
  },
  {
    "text": "multiple pilots and also flushed out an aggressive project plan to make this",
    "start": "1998520",
    "end": "2004429"
  },
  {
    "text": "happen in six months",
    "start": "2004429",
    "end": "2007480"
  },
  {
    "start": "2010000",
    "end": "2057000"
  },
  {
    "text": "it is our conceptual architecture that supports our modernization as you see",
    "start": "2010070",
    "end": "2016200"
  },
  {
    "text": "here from the left to right we collect the information store the data and analyze and do further analytics out of",
    "start": "2016200",
    "end": "2023190"
  },
  {
    "text": "it and at a collect layer we have scheduled and ingest and data transfer",
    "start": "2023190",
    "end": "2028760"
  },
  {
    "text": "the key difference what we are making it from our modernization design point we",
    "start": "2028760",
    "end": "2033960"
  },
  {
    "text": "are bringing your data lake with a vast object storage and having an a sequel",
    "start": "2033960",
    "end": "2039809"
  },
  {
    "text": "MPP platform on top of it it clearly it clearly answered our all our challenges",
    "start": "2039809",
    "end": "2048330"
  },
  {
    "text": "and also met our principles we have a now we can scale our data also with very",
    "start": "2048330",
    "end": "2054658"
  },
  {
    "text": "cost efficiently there is our software and technology stack that supports our",
    "start": "2054659",
    "end": "2061190"
  },
  {
    "start": "2057000",
    "end": "2136000"
  },
  {
    "text": "architecture traditionally we were having multiple scripts to collect and",
    "start": "2061190",
    "end": "2066510"
  },
  {
    "text": "ingest the informations what we have done it as a part of this project we standardized it in the Python framework",
    "start": "2066510",
    "end": "2073020"
  },
  {
    "text": "and use lambda for server less scale also we were using informatica for our",
    "start": "2073020",
    "end": "2079020"
  },
  {
    "text": "data transformations and data ingestion tool it collects the sources from the",
    "start": "2079020",
    "end": "2084060"
  },
  {
    "text": "source system and provides and put it in our s3 bucket on its raw format also we",
    "start": "2084060",
    "end": "2090510"
  },
  {
    "text": "have a glue ETL for EMR and red shift our data load use cases and also using",
    "start": "2090510",
    "end": "2097200"
  },
  {
    "text": "informatica push downs for our red shift data loads we are using blue gatlocke",
    "start": "2097200",
    "end": "2102900"
  },
  {
    "text": "for tagging and creating a metadata out of our data Lake so if we provide a data",
    "start": "2102900",
    "end": "2109290"
  },
  {
    "text": "dictionary for us and using MicroStrategy as our visualization tool",
    "start": "2109290",
    "end": "2116700"
  },
  {
    "text": "and provide a data product for our end-users it directly reads it from s3 and red ship as you might expect we",
    "start": "2116700",
    "end": "2127950"
  },
  {
    "text": "experienced many challenges also learned multiple lessons",
    "start": "2127950",
    "end": "2134960"
  },
  {
    "start": "2136000",
    "end": "2251000"
  },
  {
    "text": "here are our key learnings out of this entire project as we are coming from a",
    "start": "2136240",
    "end": "2143260"
  },
  {
    "text": "traditional background has to be limited with our size and scale we and we started with the large cluster and down",
    "start": "2143260",
    "end": "2152020"
  },
  {
    "text": "the line we find out splitting the cluster between the read and write it's",
    "start": "2152020",
    "end": "2158320"
  },
  {
    "text": "the most appropriate thing for our use cases now we have a smaller clusters with a multiple use cases for read and",
    "start": "2158320",
    "end": "2165310"
  },
  {
    "text": "write and also it helps us to get a great flexibility if we need to run and a heavy hit hitting reports during the",
    "start": "2165310",
    "end": "2172300"
  },
  {
    "text": "month and our quarter close we can have a read cluster bigger cluster and then we can bring it down after the period",
    "start": "2172300",
    "end": "2179640"
  },
  {
    "text": "the next thing is the vacuum and tables initially we were seeing a huge growth",
    "start": "2179640",
    "end": "2188710"
  },
  {
    "text": "in terms of our cluster sizing then we realized that already mentioned that we",
    "start": "2188710",
    "end": "2194800"
  },
  {
    "text": "are processing about hundred terabyte of data we were seeing lot of like data",
    "start": "2194800",
    "end": "2200530"
  },
  {
    "text": "delete blocks accumulated in our clusters then we changed our vacuum",
    "start": "2200530",
    "end": "2206050"
  },
  {
    "text": "strategy in a way that we can do it on a weekly and daily in fact enough in a",
    "start": "2206050",
    "end": "2212230"
  },
  {
    "text": "monthly basis now we are managing our cluster size properly similarly analyze",
    "start": "2212230",
    "end": "2220590"
  },
  {
    "text": "in a mid big point we were seeing a degradation in purgatory performance that also like same traditional",
    "start": "2220590",
    "end": "2227710"
  },
  {
    "text": "background we were having an analyze at the end of each week or a downtime when",
    "start": "2227710",
    "end": "2232720"
  },
  {
    "text": "the business was not running the reports then we changed our design in a way that we put our analyzed at the end of the",
    "start": "2232720",
    "end": "2239050"
  },
  {
    "text": "data pipeline and we started to analyze the objects most appropriately the",
    "start": "2239050",
    "end": "2245710"
  },
  {
    "text": "performance difference is a day-and-night difference we got a great performance they commit you it's an",
    "start": "2245710",
    "end": "2253330"
  },
  {
    "start": "2251000",
    "end": "2348000"
  },
  {
    "text": "another interesting one redshift is so sensitive in terms of commits it always channels all their commits through the",
    "start": "2253330",
    "end": "2261930"
  },
  {
    "text": "leader node we have been doing lot of commits I explained earlier if you are",
    "start": "2261930",
    "end": "2268480"
  },
  {
    "text": "using the informatica don't to do that then later we realized that it's completely clogged our commit",
    "start": "2268480",
    "end": "2275300"
  },
  {
    "text": "cues in the leader node then worked with the partners and changed the design in a",
    "start": "2275300",
    "end": "2280400"
  },
  {
    "text": "way that we make it as a bigger batches and provided a less commits for example",
    "start": "2280400",
    "end": "2288980"
  },
  {
    "text": "yeah it data pipeline for our products it used to run for 43 minutes after this",
    "start": "2288980",
    "end": "2295250"
  },
  {
    "text": "change it change it ran for four minutes very impressive schema design there are",
    "start": "2295250",
    "end": "2304310"
  },
  {
    "text": "also an interesting learning we were",
    "start": "2304310",
    "end": "2310250"
  },
  {
    "text": "hitting like a kind of a disk space issues often when heavy reports are",
    "start": "2310250",
    "end": "2316940"
  },
  {
    "text": "heavy data loads process in our system lately we found it there are a lot of",
    "start": "2316940",
    "end": "2323770"
  },
  {
    "text": "rebroadcasting is happening in between the clusters then we worked with AWS and",
    "start": "2323770",
    "end": "2331790"
  },
  {
    "text": "our partners we redesign our keys in a way that like redistributed across all",
    "start": "2331790",
    "end": "2338270"
  },
  {
    "text": "nodes and changed our sort keys we got a great execution plan now and we resolve",
    "start": "2338270",
    "end": "2345830"
  },
  {
    "text": "those issues another interesting one is",
    "start": "2345830",
    "end": "2350840"
  },
  {
    "start": "2348000",
    "end": "2395000"
  },
  {
    "text": "workload management we have different use cases we have continuously running",
    "start": "2350840",
    "end": "2356630"
  },
  {
    "text": "reports and some broadcasting reports as well as heavy-hitting data science",
    "start": "2356630",
    "end": "2361910"
  },
  {
    "text": "sequel hating our systems as well what we have done it we divided those by",
    "start": "2361910",
    "end": "2368330"
  },
  {
    "text": "appropriate rules setting up the queues also dynamically we changed it according",
    "start": "2368330",
    "end": "2373700"
  },
  {
    "text": "to our use case change also we closely",
    "start": "2373700",
    "end": "2379430"
  },
  {
    "text": "engaged with the AWS and the processor the rich experience is really important",
    "start": "2379430",
    "end": "2385160"
  },
  {
    "text": "and it's priceless when you are having a project like this scale and going against time this space is so important",
    "start": "2385160",
    "end": "2395380"
  },
  {
    "start": "2395000",
    "end": "2509000"
  },
  {
    "text": "so here is our benefit we absolutely got",
    "start": "2398609",
    "end": "2404289"
  },
  {
    "text": "a business agility after this project now we are completely empowered now we",
    "start": "2404289",
    "end": "2409420"
  },
  {
    "text": "are meaning our business also empowering",
    "start": "2409420",
    "end": "2414510"
  },
  {
    "text": "now we can take any data at scale and we can scale it whenever we need it as well",
    "start": "2414809",
    "end": "2423000"
  },
  {
    "text": "more than that we are seeing a very tangible benefit in addition to this",
    "start": "2423000",
    "end": "2428020"
  },
  {
    "text": "agility we reduced our overall overhead cost about 10 to 15 percentage and also",
    "start": "2428020",
    "end": "2436059"
  },
  {
    "text": "we decommission hundreds of our servers released our data center footprint now",
    "start": "2436059",
    "end": "2444250"
  },
  {
    "text": "we can probably say that our studio analytics run on cloud also we have 30",
    "start": "2444250",
    "end": "2450970"
  },
  {
    "text": "to 35 percentage of process improvement and efficiency across the board across",
    "start": "2450970",
    "end": "2457029"
  },
  {
    "text": "all applications I want to mention one",
    "start": "2457029",
    "end": "2462220"
  },
  {
    "text": "more thing it's not just an a platform alone help desk to help get this benefit",
    "start": "2462220",
    "end": "2469319"
  },
  {
    "text": "the partnership how we got together with Amazon whenever we ask for a",
    "start": "2469319",
    "end": "2474990"
  },
  {
    "text": "enhancements or a bug fixing always they listen to us they are properly they",
    "start": "2474990",
    "end": "2481990"
  },
  {
    "text": "released it on time also worked closely with hand-in-hand helped us to met all",
    "start": "2481990",
    "end": "2489130"
  },
  {
    "text": "the milestones on time it is a great partnership so from here where are we",
    "start": "2489130",
    "end": "2498700"
  },
  {
    "text": "going Amazon is innovating fast at the same",
    "start": "2498700",
    "end": "2505510"
  },
  {
    "text": "time at Fox we are embracing those innovations much faster we are just now mentioning about all",
    "start": "2505510",
    "end": "2512200"
  },
  {
    "start": "2509000",
    "end": "2577000"
  },
  {
    "text": "those new features we are ready working with them and releasing it in fact we",
    "start": "2512200",
    "end": "2517569"
  },
  {
    "text": "released few of them into our production systems we upgraded to DC - we saw a",
    "start": "2517569",
    "end": "2525670"
  },
  {
    "text": "great performance like it's about 50 - more than 50 percentage also we release this shot Korea",
    "start": "2525670",
    "end": "2531980"
  },
  {
    "text": "accelerator and we are seeing a great performance for our MicroStrategy related use cases where our list of",
    "start": "2531980",
    "end": "2538190"
  },
  {
    "text": "value are or a prompts it's comes out really in sub seconds and spectrum it",
    "start": "2538190",
    "end": "2547519"
  },
  {
    "text": "got our data Lake to the next level now we are dividing our compute need as well",
    "start": "2547519",
    "end": "2554089"
  },
  {
    "text": "as data need totally separate we no longer need to put our cold data into",
    "start": "2554089",
    "end": "2559700"
  },
  {
    "text": "redshift spectrum does it for under the hood blended nicely and provided as an",
    "start": "2559700",
    "end": "2565009"
  },
  {
    "text": "end result curry result caching that still we are evaluating it we are seeing",
    "start": "2565009",
    "end": "2571670"
  },
  {
    "text": "a promising results looking forward to implement it also we are expanding our",
    "start": "2571670",
    "end": "2579380"
  },
  {
    "start": "2577000",
    "end": "2615000"
  },
  {
    "text": "data Lake to our next other business units based on the experience we got it from our studio implementation also",
    "start": "2579380",
    "end": "2589849"
  },
  {
    "text": "working on a project to provide a real-time analytic solutions through using through Genesis so excited to",
    "start": "2589849",
    "end": "2597529"
  },
  {
    "text": "extend our data leg to provide deeper insights and predictive insights using machine Lang machine language in EAS",
    "start": "2597529",
    "end": "2606940"
  },
  {
    "text": "hope it helps for you guys thank you thank you for your time",
    "start": "2608710",
    "end": "2614390"
  },
  {
    "text": "[Applause]",
    "start": "2614390",
    "end": "2617179"
  }
]