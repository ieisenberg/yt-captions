[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "hi everyone today i'll be giving an",
    "start": "960",
    "end": "2879"
  },
  {
    "text": "overview and demoing a feature that aws",
    "start": "2879",
    "end": "4960"
  },
  {
    "text": "transfer family launch at aws storage",
    "start": "4960",
    "end": "6960"
  },
  {
    "text": "day 2021 for using transfer family with",
    "start": "6960",
    "end": "9599"
  },
  {
    "text": "low code or no code workflows for",
    "start": "9599",
    "end": "11200"
  },
  {
    "text": "managed file transfer",
    "start": "11200",
    "end": "13120"
  },
  {
    "start": "13000",
    "end": "13000"
  },
  {
    "text": "aws transfer family is a secure fully",
    "start": "13120",
    "end": "15280"
  },
  {
    "text": "managed transfer service that enables",
    "start": "15280",
    "end": "16880"
  },
  {
    "text": "you to transfer files into and out of",
    "start": "16880",
    "end": "18720"
  },
  {
    "text": "amazon storage over sftp ftps or ftp adb",
    "start": "18720",
    "end": "23119"
  },
  {
    "text": "transfer family supports landing data",
    "start": "23119",
    "end": "24880"
  },
  {
    "text": "into and retrieving data from amazon s3",
    "start": "24880",
    "end": "27840"
  },
  {
    "text": "and amazon elastic file system or efs",
    "start": "27840",
    "end": "30800"
  },
  {
    "start": "30000",
    "end": "30000"
  },
  {
    "text": "with just a few clicks in the aws",
    "start": "30800",
    "end": "32480"
  },
  {
    "text": "transfer family console customers are",
    "start": "32480",
    "end": "34399"
  },
  {
    "text": "able to select one or more protocols for",
    "start": "34399",
    "end": "36079"
  },
  {
    "text": "their users to interface with configure",
    "start": "36079",
    "end": "38079"
  },
  {
    "text": "amazon s3 buckets or efs to store the",
    "start": "38079",
    "end": "40399"
  },
  {
    "text": "transfer data and set up end user",
    "start": "40399",
    "end": "42239"
  },
  {
    "text": "authentication by either importing",
    "start": "42239",
    "end": "43840"
  },
  {
    "text": "existing end user credentials or",
    "start": "43840",
    "end": "45680"
  },
  {
    "text": "integrating with an identity provider",
    "start": "45680",
    "end": "47600"
  },
  {
    "text": "your end users can continue transferring",
    "start": "47600",
    "end": "49520"
  },
  {
    "text": "files using existing clients such as",
    "start": "49520",
    "end": "51760"
  },
  {
    "text": "filezilla winscp cyberduck and openssh",
    "start": "51760",
    "end": "55360"
  },
  {
    "text": "and the data will be stored in s3 or efs",
    "start": "55360",
    "end": "58640"
  },
  {
    "text": "after receiving files you often need to",
    "start": "58640",
    "end": "60559"
  },
  {
    "text": "process your data prior to it being",
    "start": "60559",
    "end": "62320"
  },
  {
    "text": "consumed by downstream systems you may",
    "start": "62320",
    "end": "64478"
  },
  {
    "text": "view this as post-processing in terms of",
    "start": "64479",
    "end": "66880"
  },
  {
    "text": "file ingestion into your aws transfer",
    "start": "66880",
    "end": "68880"
  },
  {
    "text": "family server or pre-processing if",
    "start": "68880",
    "end": "71200"
  },
  {
    "text": "you're thinking in terms of what will be",
    "start": "71200",
    "end": "72479"
  },
  {
    "text": "consuming the data such as your data",
    "start": "72479",
    "end": "74400"
  },
  {
    "text": "lake",
    "start": "74400",
    "end": "75280"
  },
  {
    "text": "until today for s3 customers have been",
    "start": "75280",
    "end": "77920"
  },
  {
    "text": "required to use s3 event notifications",
    "start": "77920",
    "end": "79920"
  },
  {
    "text": "to invoke and orchestrate downstream",
    "start": "79920",
    "end": "81680"
  },
  {
    "text": "processing and for efs to manage",
    "start": "81680",
    "end": "83920"
  },
  {
    "text": "workflows downstream from their aws",
    "start": "83920",
    "end": "85600"
  },
  {
    "text": "transfer family server customers have",
    "start": "85600",
    "end": "87439"
  },
  {
    "text": "needed a file system listener against",
    "start": "87439",
    "end": "89119"
  },
  {
    "text": "their efs file system",
    "start": "89119",
    "end": "92479"
  },
  {
    "text": "aws transfer family now supports managed",
    "start": "92479",
    "end": "94799"
  },
  {
    "text": "workflows for file processing which",
    "start": "94799",
    "end": "97040"
  },
  {
    "text": "enables you to create automate and",
    "start": "97040",
    "end": "98880"
  },
  {
    "text": "monitor your file transfers using low",
    "start": "98880",
    "end": "101280"
  },
  {
    "text": "code or no code automation this allows",
    "start": "101280",
    "end": "103600"
  },
  {
    "text": "you to orchestrate a linear sequence of",
    "start": "103600",
    "end": "105360"
  },
  {
    "text": "processing steps as a part of a workflow",
    "start": "105360",
    "end": "107680"
  },
  {
    "text": "which can prepare data for analytics and",
    "start": "107680",
    "end": "109520"
  },
  {
    "text": "insights or other business purposes it's",
    "start": "109520",
    "end": "112000"
  },
  {
    "text": "easy to customize you can use custom",
    "start": "112000",
    "end": "113920"
  },
  {
    "text": "steps for any variety of use cases such",
    "start": "113920",
    "end": "116399"
  },
  {
    "text": "as file validation encryption and",
    "start": "116399",
    "end": "118079"
  },
  {
    "text": "decryption compression or extraction",
    "start": "118079",
    "end": "120640"
  },
  {
    "text": "security scanning filtering content",
    "start": "120640",
    "end": "123040"
  },
  {
    "text": "notifications among other use cases",
    "start": "123040",
    "end": "125680"
  },
  {
    "text": "transfer family managed workflows also",
    "start": "125680",
    "end": "127520"
  },
  {
    "text": "supports a library of commonly used",
    "start": "127520",
    "end": "129360"
  },
  {
    "text": "operations that you can use individually",
    "start": "129360",
    "end": "131840"
  },
  {
    "text": "or chain together as of today transfer",
    "start": "131840",
    "end": "134400"
  },
  {
    "text": "family supports copy tag and delete",
    "start": "134400",
    "end": "136640"
  },
  {
    "text": "built-in workflow steps",
    "start": "136640",
    "end": "138640"
  },
  {
    "text": "manage workflows allows you to",
    "start": "138640",
    "end": "140000"
  },
  {
    "text": "coordinate all necessary steps required",
    "start": "140000",
    "end": "142080"
  },
  {
    "text": "for file processing while having",
    "start": "142080",
    "end": "143920"
  },
  {
    "text": "end-to-end visibility and auditing",
    "start": "143920",
    "end": "145840"
  },
  {
    "text": "through aws transfer families console",
    "start": "145840",
    "end": "148000"
  },
  {
    "text": "and cloudwatch logs",
    "start": "148000",
    "end": "151040"
  },
  {
    "text": "in this video i'll show you how to set",
    "start": "151360",
    "end": "153280"
  },
  {
    "start": "152000",
    "end": "152000"
  },
  {
    "text": "up two transfer family managed workflows",
    "start": "153280",
    "end": "155360"
  },
  {
    "text": "and then show them an action the first",
    "start": "155360",
    "end": "157200"
  },
  {
    "text": "demo uses the copy and tag workflow",
    "start": "157200",
    "end": "159519"
  },
  {
    "text": "steps for a record archival scenario",
    "start": "159519",
    "end": "161840"
  },
  {
    "text": "followed by a demo where we have an aws",
    "start": "161840",
    "end": "163840"
  },
  {
    "text": "lambda function which extracts and loads",
    "start": "163840",
    "end": "165440"
  },
  {
    "text": "data out of a csv and then deletes the",
    "start": "165440",
    "end": "167440"
  },
  {
    "text": "file i plan to go step by step and we'll",
    "start": "167440",
    "end": "169599"
  },
  {
    "text": "show the processing through cloudwatch",
    "start": "169599",
    "end": "171200"
  },
  {
    "text": "logs along the way",
    "start": "171200",
    "end": "174080"
  },
  {
    "start": "174000",
    "end": "174000"
  },
  {
    "text": "in the first demo i'll cover a record",
    "start": "174160",
    "end": "175920"
  },
  {
    "text": "archival scenario first we'll set up a",
    "start": "175920",
    "end": "178480"
  },
  {
    "text": "step to copy new records to a new bucket",
    "start": "178480",
    "end": "181120"
  },
  {
    "text": "under transfer prefix then we'll set a",
    "start": "181120",
    "end": "184080"
  },
  {
    "text": "step to tag the copied record with the",
    "start": "184080",
    "end": "186400"
  },
  {
    "text": "key of archive and the value of yes",
    "start": "186400",
    "end": "189040"
  },
  {
    "text": "this scenario is useful if a business",
    "start": "189040",
    "end": "190800"
  },
  {
    "text": "wants to retain any files sent to them",
    "start": "190800",
    "end": "192640"
  },
  {
    "text": "by a business partner for retention and",
    "start": "192640",
    "end": "194319"
  },
  {
    "text": "compliance so note that the s3 bucket",
    "start": "194319",
    "end": "196720"
  },
  {
    "text": "has object versioning enabled since we",
    "start": "196720",
    "end": "198560"
  },
  {
    "text": "want to archive both rights and",
    "start": "198560",
    "end": "200000"
  },
  {
    "text": "overwrites and we have an s3 lifecycle",
    "start": "200000",
    "end": "202400"
  },
  {
    "text": "policy set to archive objects that are",
    "start": "202400",
    "end": "204959"
  },
  {
    "text": "tagged",
    "start": "204959",
    "end": "206480"
  },
  {
    "text": "i'm starting the demos with the",
    "start": "206480",
    "end": "207920"
  },
  {
    "text": "assumption that you already have a",
    "start": "207920",
    "end": "209360"
  },
  {
    "text": "transfer family server and user setup if",
    "start": "209360",
    "end": "212080"
  },
  {
    "text": "you want to follow along and are setting",
    "start": "212080",
    "end": "213599"
  },
  {
    "text": "yours up for the first time there's",
    "start": "213599",
    "end": "215120"
  },
  {
    "text": "another transfer family demo in the",
    "start": "215120",
    "end": "216879"
  },
  {
    "text": "description which walks through this",
    "start": "216879",
    "end": "218560"
  },
  {
    "text": "step by step for this demo i already",
    "start": "218560",
    "end": "220799"
  },
  {
    "text": "have a user on a transfer family server",
    "start": "220799",
    "end": "222959"
  },
  {
    "text": "with an sftp interface backed by s3 so",
    "start": "222959",
    "end": "225920"
  },
  {
    "text": "first we'll navigate to the aws transfer",
    "start": "225920",
    "end": "227840"
  },
  {
    "text": "family console and select workflows",
    "start": "227840",
    "end": "230799"
  },
  {
    "text": "we'll see that there are no workflows",
    "start": "230799",
    "end": "232400"
  },
  {
    "text": "created yet and we'll select create",
    "start": "232400",
    "end": "234080"
  },
  {
    "text": "workflow and we'll describe that",
    "start": "234080",
    "end": "235680"
  },
  {
    "text": "workflow as demo 1.",
    "start": "235680",
    "end": "238959"
  },
  {
    "text": "now we can add our steps",
    "start": "239280",
    "end": "241200"
  },
  {
    "text": "you can see that we have copy tag delete",
    "start": "241200",
    "end": "243920"
  },
  {
    "text": "and custom lambda step types for this",
    "start": "243920",
    "end": "246239"
  },
  {
    "text": "demo the first one we'll be using is",
    "start": "246239",
    "end": "248080"
  },
  {
    "text": "copying the file",
    "start": "248080",
    "end": "250959"
  },
  {
    "text": "we'll name the step copy to archive",
    "start": "252159",
    "end": "256159"
  },
  {
    "text": "we'll select the bucket where we want to",
    "start": "260079",
    "end": "262000"
  },
  {
    "text": "write our copy",
    "start": "262000",
    "end": "264320"
  },
  {
    "text": "and the destination key prefix",
    "start": "264320",
    "end": "268080"
  },
  {
    "text": "don't forget the forward slash since we",
    "start": "268080",
    "end": "269840"
  },
  {
    "text": "want to retain the object name",
    "start": "269840",
    "end": "272000"
  },
  {
    "text": "and for this use case we want to",
    "start": "272000",
    "end": "273440"
  },
  {
    "text": "overwrite existing objects since we want",
    "start": "273440",
    "end": "275759"
  },
  {
    "text": "to archive all files sent to us for",
    "start": "275759",
    "end": "277520"
  },
  {
    "text": "compliance",
    "start": "277520",
    "end": "278800"
  },
  {
    "text": "we'll review the information and create",
    "start": "278800",
    "end": "280639"
  },
  {
    "text": "our step",
    "start": "280639",
    "end": "281680"
  },
  {
    "text": "and then we'll add a second step for",
    "start": "281680",
    "end": "283199"
  },
  {
    "text": "tagging",
    "start": "283199",
    "end": "284880"
  },
  {
    "text": "we'll name it tag to archive",
    "start": "284880",
    "end": "288320"
  },
  {
    "text": "and once copied we want to tag our",
    "start": "288320",
    "end": "290400"
  },
  {
    "text": "copied file with a key of archive and a",
    "start": "290400",
    "end": "293120"
  },
  {
    "text": "value of yes",
    "start": "293120",
    "end": "296600"
  },
  {
    "text": "and once we have both steps",
    "start": "299280",
    "end": "301199"
  },
  {
    "text": "we want to create our workflow",
    "start": "301199",
    "end": "304639"
  },
  {
    "text": "to associate the workflow with your",
    "start": "306000",
    "end": "307680"
  },
  {
    "text": "server you navigate to servers",
    "start": "307680",
    "end": "311120"
  },
  {
    "text": "select your aws transfer family server",
    "start": "311120",
    "end": "314000"
  },
  {
    "text": "scroll down to additional details",
    "start": "314000",
    "end": "316800"
  },
  {
    "text": "and edit your server",
    "start": "316800",
    "end": "319840"
  },
  {
    "text": "you scroll down to post upload",
    "start": "320000",
    "end": "322080"
  },
  {
    "text": "processing",
    "start": "322080",
    "end": "323199"
  },
  {
    "text": "and i'm selecting my demo1 workflow and",
    "start": "323199",
    "end": "325919"
  },
  {
    "text": "a workflow execution rule",
    "start": "325919",
    "end": "328639"
  },
  {
    "text": "note that the workflow execution iam",
    "start": "328639",
    "end": "330720"
  },
  {
    "text": "role must have a trust relationship for",
    "start": "330720",
    "end": "334440"
  },
  {
    "text": "transfer.amazonaws.com for authorization",
    "start": "334440",
    "end": "337039"
  },
  {
    "text": "an s3 backed transfer family server",
    "start": "337039",
    "end": "339280"
  },
  {
    "text": "grants permissions for copy tag and",
    "start": "339280",
    "end": "341520"
  },
  {
    "text": "delete actions through s3 iam policy",
    "start": "341520",
    "end": "344479"
  },
  {
    "text": "actions",
    "start": "344479",
    "end": "345680"
  },
  {
    "text": "for an efs backed transfer family server",
    "start": "345680",
    "end": "348160"
  },
  {
    "text": "the execution rule must have posix",
    "start": "348160",
    "end": "350080"
  },
  {
    "text": "permissions to the file or directory",
    "start": "350080",
    "end": "352720"
  },
  {
    "text": "for custom apis the execution rule",
    "start": "352720",
    "end": "355360"
  },
  {
    "text": "requires permissions to invoke the",
    "start": "355360",
    "end": "357199"
  },
  {
    "text": "lambda function",
    "start": "357199",
    "end": "358479"
  },
  {
    "text": "next we'll save the change to our server",
    "start": "358479",
    "end": "362319"
  },
  {
    "text": "the next thing we'll do is navigate to",
    "start": "363280",
    "end": "364960"
  },
  {
    "text": "our terminal log in to our sftp server",
    "start": "364960",
    "end": "368319"
  },
  {
    "text": "upload a file and then see the workflow",
    "start": "368319",
    "end": "370479"
  },
  {
    "text": "execute",
    "start": "370479",
    "end": "372000"
  },
  {
    "text": "so here from the command line we're",
    "start": "372000",
    "end": "373520"
  },
  {
    "text": "logging into it and we're putting an",
    "start": "373520",
    "end": "375520"
  },
  {
    "text": "object onto s3",
    "start": "375520",
    "end": "379479"
  },
  {
    "text": "when we navigate to cloudwatch logs i",
    "start": "380479",
    "end": "382720"
  },
  {
    "text": "can see that my workflow has already",
    "start": "382720",
    "end": "384400"
  },
  {
    "text": "been executed and completed which are",
    "start": "384400",
    "end": "386639"
  },
  {
    "text": "the first and last logs emitted between",
    "start": "386639",
    "end": "389280"
  },
  {
    "text": "them you can also see both steps",
    "start": "389280",
    "end": "391280"
  },
  {
    "text": "completed in order",
    "start": "391280",
    "end": "395080"
  },
  {
    "text": "first you can see details about the copy",
    "start": "397199",
    "end": "399600"
  },
  {
    "text": "step",
    "start": "399600",
    "end": "401919"
  },
  {
    "text": "you can see the bucket key the step type",
    "start": "403440",
    "end": "406160"
  },
  {
    "text": "was copy the step name was copy to",
    "start": "406160",
    "end": "408240"
  },
  {
    "text": "archive and the server information",
    "start": "408240",
    "end": "411039"
  },
  {
    "text": "you can also see the same details for",
    "start": "411039",
    "end": "412720"
  },
  {
    "text": "the tagging step",
    "start": "412720",
    "end": "415599"
  },
  {
    "text": "next if you navigate to the new records",
    "start": "416479",
    "end": "418960"
  },
  {
    "text": "s3 bucket we're able to see that the",
    "start": "418960",
    "end": "421199"
  },
  {
    "text": "record was uploaded to the user's home",
    "start": "421199",
    "end": "423440"
  },
  {
    "text": "directory",
    "start": "423440",
    "end": "426000"
  },
  {
    "text": "also in the archive records s3 bucket",
    "start": "426160",
    "end": "428880"
  },
  {
    "text": "we're able to see that it was put into",
    "start": "428880",
    "end": "430560"
  },
  {
    "text": "the archive records bucket",
    "start": "430560",
    "end": "432479"
  },
  {
    "text": "and we can also see that the object was",
    "start": "432479",
    "end": "434240"
  },
  {
    "text": "tagged for archive",
    "start": "434240",
    "end": "437720"
  },
  {
    "text": "workflows are immutable so to adjust",
    "start": "439280",
    "end": "441520"
  },
  {
    "text": "your workflow you need to create a new",
    "start": "441520",
    "end": "443120"
  },
  {
    "text": "workflow to associate to your server",
    "start": "443120",
    "end": "445520"
  },
  {
    "text": "you can declare your workflows as json",
    "start": "445520",
    "end": "447520"
  },
  {
    "text": "and then use the create workflow api",
    "start": "447520",
    "end": "449440"
  },
  {
    "text": "over the aws cli or sdks here's the same",
    "start": "449440",
    "end": "452479"
  },
  {
    "text": "workflow we created just to find us json",
    "start": "452479",
    "end": "456160"
  },
  {
    "start": "456000",
    "end": "456000"
  },
  {
    "text": "in demo 2 i'll cover a data extraction",
    "start": "456160",
    "end": "458479"
  },
  {
    "text": "scenario first we'll create a custom",
    "start": "458479",
    "end": "460720"
  },
  {
    "text": "step using an aws lambda function to",
    "start": "460720",
    "end": "463280"
  },
  {
    "text": "confirm that the file extension csv and",
    "start": "463280",
    "end": "465599"
  },
  {
    "text": "extract and load the csv data",
    "start": "465599",
    "end": "467840"
  },
  {
    "text": "then we'll set a step to delete the",
    "start": "467840",
    "end": "469440"
  },
  {
    "text": "object once we have the data loaded also",
    "start": "469440",
    "end": "471759"
  },
  {
    "text": "set an exception handler step that we'll",
    "start": "471759",
    "end": "473520"
  },
  {
    "text": "review",
    "start": "473520",
    "end": "474879"
  },
  {
    "text": "for demo 2 we want to update the server",
    "start": "474879",
    "end": "477039"
  },
  {
    "text": "from using a demo1 workflow to a demo2",
    "start": "477039",
    "end": "479360"
  },
  {
    "text": "workflow so first we'll need to create",
    "start": "479360",
    "end": "481199"
  },
  {
    "text": "the demo2 workflow we'll select create",
    "start": "481199",
    "end": "483120"
  },
  {
    "text": "workflow and add a description we need",
    "start": "483120",
    "end": "485360"
  },
  {
    "text": "to create a step that uses our lambda",
    "start": "485360",
    "end": "487280"
  },
  {
    "text": "function and this lambda function",
    "start": "487280",
    "end": "489360"
  },
  {
    "text": "validates that the file type of csv",
    "start": "489360",
    "end": "491360"
  },
  {
    "text": "extracts the csv content and loads it",
    "start": "491360",
    "end": "493360"
  },
  {
    "text": "into dynamodb and then sends a",
    "start": "493360",
    "end": "495120"
  },
  {
    "text": "notification whether or not it was",
    "start": "495120",
    "end": "496479"
  },
  {
    "text": "successful so we'll choose the lambda",
    "start": "496479",
    "end": "498400"
  },
  {
    "text": "function that we've already imported",
    "start": "498400",
    "end": "499840"
  },
  {
    "text": "which i'll show",
    "start": "499840",
    "end": "502560"
  },
  {
    "text": "we then create a delete step",
    "start": "503120",
    "end": "505120"
  },
  {
    "text": "to delete the file now that we've",
    "start": "505120",
    "end": "506800"
  },
  {
    "text": "extracted the data from it",
    "start": "506800",
    "end": "510000"
  },
  {
    "text": "we'll also create an exception handler",
    "start": "512959",
    "end": "514719"
  },
  {
    "text": "step which is the second lambda function",
    "start": "514719",
    "end": "516800"
  },
  {
    "text": "which will alert that the workflow",
    "start": "516800",
    "end": "518159"
  },
  {
    "text": "execution failed when you're doing this",
    "start": "518159",
    "end": "520080"
  },
  {
    "text": "as code the create for workflow api",
    "start": "520080",
    "end": "522159"
  },
  {
    "text": "would reference two json files pass as",
    "start": "522159",
    "end": "524159"
  },
  {
    "text": "arguments for steps and on exception",
    "start": "524159",
    "end": "526160"
  },
  {
    "text": "steps",
    "start": "526160",
    "end": "528399"
  },
  {
    "text": "we'll associate the new workflow with",
    "start": "528399",
    "end": "530240"
  },
  {
    "text": "our existing server so we navigate to",
    "start": "530240",
    "end": "532320"
  },
  {
    "text": "our server",
    "start": "532320",
    "end": "534399"
  },
  {
    "text": "scroll down to additional details",
    "start": "534399",
    "end": "536800"
  },
  {
    "text": "and select edit",
    "start": "536800",
    "end": "539600"
  },
  {
    "text": "when we scroll down to post upload",
    "start": "539680",
    "end": "541120"
  },
  {
    "text": "processing we can select our new",
    "start": "541120",
    "end": "543120"
  },
  {
    "text": "workflow",
    "start": "543120",
    "end": "545040"
  },
  {
    "text": "and we're going to use the transfer",
    "start": "545040",
    "end": "546399"
  },
  {
    "text": "workflow execution role",
    "start": "546399",
    "end": "549760"
  },
  {
    "text": "this role also has a trust policy with",
    "start": "549760",
    "end": "551680"
  },
  {
    "text": "aws transfer and authorization to invoke",
    "start": "551680",
    "end": "554320"
  },
  {
    "text": "our lambda functions and delete s3",
    "start": "554320",
    "end": "556240"
  },
  {
    "text": "objects in our bucket prefix",
    "start": "556240",
    "end": "559279"
  },
  {
    "text": "so next we'll hit save and the server",
    "start": "559279",
    "end": "561279"
  },
  {
    "text": "will update",
    "start": "561279",
    "end": "563839"
  },
  {
    "text": "so next we'll navigate to our dynamodb",
    "start": "566399",
    "end": "568640"
  },
  {
    "text": "table where we can scan our table and",
    "start": "568640",
    "end": "571040"
  },
  {
    "text": "see that we have no items in the table",
    "start": "571040",
    "end": "572880"
  },
  {
    "text": "yet",
    "start": "572880",
    "end": "575880"
  },
  {
    "text": "so next we'll log into our sftp server",
    "start": "579040",
    "end": "582000"
  },
  {
    "text": "and upload a simple csv file which has",
    "start": "582000",
    "end": "584480"
  },
  {
    "text": "three rows for this example",
    "start": "584480",
    "end": "587120"
  },
  {
    "text": "so we log in",
    "start": "587120",
    "end": "589360"
  },
  {
    "text": "and we'll put the object to s3",
    "start": "589360",
    "end": "593120"
  },
  {
    "text": "we can then see the workflow execution",
    "start": "600480",
    "end": "602720"
  },
  {
    "text": "and cloudwatch logs almost immediately",
    "start": "602720",
    "end": "605360"
  },
  {
    "text": "so similar to demo 1's workflow",
    "start": "605360",
    "end": "607279"
  },
  {
    "text": "execution we can see the execution",
    "start": "607279",
    "end": "609440"
  },
  {
    "text": "started and execution completed logs are",
    "start": "609440",
    "end": "611680"
  },
  {
    "text": "the first and last logs emitted",
    "start": "611680",
    "end": "614079"
  },
  {
    "text": "also the main difference is when there",
    "start": "614079",
    "end": "615519"
  },
  {
    "text": "is a custom step we can see a custom",
    "start": "615519",
    "end": "617279"
  },
  {
    "text": "step invoked log",
    "start": "617279",
    "end": "619040"
  },
  {
    "text": "all step started events are after the",
    "start": "619040",
    "end": "620880"
  },
  {
    "text": "previous step completed event since the",
    "start": "620880",
    "end": "622880"
  },
  {
    "text": "workflow executes in a linear fashion",
    "start": "622880",
    "end": "625920"
  },
  {
    "text": "we can also inspect the logs for the",
    "start": "625920",
    "end": "627920"
  },
  {
    "text": "lambda function execution",
    "start": "627920",
    "end": "629760"
  },
  {
    "text": "here we can see the responses for",
    "start": "629760",
    "end": "631440"
  },
  {
    "text": "uploading the three rows into dynamodb",
    "start": "631440",
    "end": "633839"
  },
  {
    "text": "and if we go back to dynamodb and scan",
    "start": "633839",
    "end": "636079"
  },
  {
    "text": "our table three items now return",
    "start": "636079",
    "end": "640079"
  },
  {
    "text": "within our new records amazon s3 bucket",
    "start": "641279",
    "end": "643839"
  },
  {
    "text": "we can refresh and see that our",
    "start": "643839",
    "end": "645480"
  },
  {
    "text": "demo2.csv object does not exist in this",
    "start": "645480",
    "end": "648480"
  },
  {
    "text": "case because of the delete step so we",
    "start": "648480",
    "end": "650640"
  },
  {
    "text": "still only have demo1.csv",
    "start": "650640",
    "end": "654000"
  },
  {
    "text": "i want to go through the requirements",
    "start": "654000",
    "end": "655760"
  },
  {
    "text": "for a lambda function associated with a",
    "start": "655760",
    "end": "657680"
  },
  {
    "text": "custom step the lambda functions iam",
    "start": "657680",
    "end": "660160"
  },
  {
    "text": "role must be allowed to perform",
    "start": "660160",
    "end": "661680"
  },
  {
    "text": "whichever iem actions you require based",
    "start": "661680",
    "end": "664240"
  },
  {
    "text": "on your custom logic the most important",
    "start": "664240",
    "end": "666560"
  },
  {
    "text": "thing is that the send workflow step",
    "start": "666560",
    "end": "668880"
  },
  {
    "text": "state method which we define in the",
    "start": "668880",
    "end": "670800"
  },
  {
    "text": "highlighted function gets a status of",
    "start": "670800",
    "end": "672880"
  },
  {
    "text": "success with all of the proper workflow",
    "start": "672880",
    "end": "674800"
  },
  {
    "text": "metadata this is how the workflow knows",
    "start": "674800",
    "end": "677040"
  },
  {
    "text": "that the step has been completed",
    "start": "677040",
    "end": "678320"
  },
  {
    "text": "successfully if your lambda times out",
    "start": "678320",
    "end": "680720"
  },
  {
    "text": "and getting a response back to this api",
    "start": "680720",
    "end": "682800"
  },
  {
    "text": "or if your lambda reports back a failure",
    "start": "682800",
    "end": "684880"
  },
  {
    "text": "instead of success the next step is the",
    "start": "684880",
    "end": "687120"
  },
  {
    "text": "exception handler",
    "start": "687120",
    "end": "689040"
  },
  {
    "text": "for this lambda function within the",
    "start": "689040",
    "end": "690720"
  },
  {
    "text": "lambda handler we're extracting",
    "start": "690720",
    "end": "692640"
  },
  {
    "text": "properties from the workflow event",
    "start": "692640",
    "end": "694880"
  },
  {
    "text": "implementing our custom logic",
    "start": "694880",
    "end": "696800"
  },
  {
    "text": "and then we call our send step state",
    "start": "696800",
    "end": "698399"
  },
  {
    "text": "function",
    "start": "698399",
    "end": "700800"
  },
  {
    "text": "this is an example json test event and",
    "start": "701600",
    "end": "703839"
  },
  {
    "text": "the structure that your lambda would",
    "start": "703839",
    "end": "705120"
  },
  {
    "text": "receive it from the workflow",
    "start": "705120",
    "end": "706800"
  },
  {
    "text": "orchestrator",
    "start": "706800",
    "end": "708240"
  },
  {
    "text": "you have the workflow id execution id",
    "start": "708240",
    "end": "710959"
  },
  {
    "text": "user server and file location structure",
    "start": "710959",
    "end": "713440"
  },
  {
    "text": "which depends on if it's from s3 or efs",
    "start": "713440",
    "end": "716800"
  },
  {
    "text": "here you can see the bucket key and",
    "start": "716800",
    "end": "718639"
  },
  {
    "text": "other object metadata associated with",
    "start": "718639",
    "end": "720959"
  },
  {
    "text": "the s3 object",
    "start": "720959",
    "end": "723360"
  },
  {
    "text": "similar to the other custom lambda step",
    "start": "723360",
    "end": "725600"
  },
  {
    "text": "for the exception handler step to",
    "start": "725600",
    "end": "727120"
  },
  {
    "text": "complete you need to report back success",
    "start": "727120",
    "end": "729200"
  },
  {
    "text": "to the workflow",
    "start": "729200",
    "end": "730399"
  },
  {
    "text": "with the send workflow step state method",
    "start": "730399",
    "end": "733120"
  },
  {
    "text": "for my exception handler i'm sending an",
    "start": "733120",
    "end": "735279"
  },
  {
    "text": "sns message that our workflow failed for",
    "start": "735279",
    "end": "737760"
  },
  {
    "text": "a user and file with the metadata of",
    "start": "737760",
    "end": "740000"
  },
  {
    "text": "that event",
    "start": "740000",
    "end": "741360"
  },
  {
    "text": "then we report back success to the",
    "start": "741360",
    "end": "743360"
  },
  {
    "text": "workflow",
    "start": "743360",
    "end": "744880"
  },
  {
    "text": "to recap aws transfer family managed",
    "start": "744880",
    "end": "747120"
  },
  {
    "start": "745000",
    "end": "745000"
  },
  {
    "text": "workflows is a new feature which allows",
    "start": "747120",
    "end": "749040"
  },
  {
    "text": "you to orchestrate file processing",
    "start": "749040",
    "end": "750560"
  },
  {
    "text": "workflows it's generally available today",
    "start": "750560",
    "end": "753360"
  },
  {
    "text": "and easy to get started",
    "start": "753360",
    "end": "755920"
  },
  {
    "text": "all transfer family server types are",
    "start": "755920",
    "end": "757519"
  },
  {
    "text": "supported so customers no longer need to",
    "start": "757519",
    "end": "759680"
  },
  {
    "text": "manage their own workflow orchestration",
    "start": "759680",
    "end": "761440"
  },
  {
    "text": "using event notifications or file system",
    "start": "761440",
    "end": "763440"
  },
  {
    "text": "listeners",
    "start": "763440",
    "end": "765120"
  },
  {
    "text": "transfer family provides a set of",
    "start": "765120",
    "end": "766720"
  },
  {
    "text": "commonly used operations as workflow",
    "start": "766720",
    "end": "768800"
  },
  {
    "text": "step types",
    "start": "768800",
    "end": "770399"
  },
  {
    "text": "there is also the ability to extend the",
    "start": "770399",
    "end": "772160"
  },
  {
    "text": "functionality using aws lambda",
    "start": "772160",
    "end": "774399"
  },
  {
    "text": "integration with custom steps",
    "start": "774399",
    "end": "777200"
  },
  {
    "text": "your organization can have visibility",
    "start": "777200",
    "end": "778959"
  },
  {
    "text": "into your workflow executions with",
    "start": "778959",
    "end": "780399"
  },
  {
    "text": "cloudwatch logs",
    "start": "780399",
    "end": "783200"
  },
  {
    "text": "thanks for watching i hope you've",
    "start": "783200",
    "end": "784800"
  },
  {
    "text": "learned more about aws transfer family",
    "start": "784800",
    "end": "786720"
  },
  {
    "text": "and managed workflows have a great day",
    "start": "786720",
    "end": "791800"
  }
]