[
  {
    "start": "0",
    "end": "52000"
  },
  {
    "text": "all right well good afternoon everyone Salaam Greg Corolla I lead our business",
    "start": "290",
    "end": "5670"
  },
  {
    "text": "development team for our database services at AWS and joining me today is John and Erin and only to do some sales",
    "start": "5670",
    "end": "12540"
  },
  {
    "text": "and that comes time for their section we just to kind of give you a expectation so I'll start things out talk a little",
    "start": "12540",
    "end": "18510"
  },
  {
    "text": "bit about you know what's really the advantages of migrating to open source database engines some of the motivations",
    "start": "18510",
    "end": "25140"
  },
  {
    "text": "customers have to migrate and then John will talk about actual mechanisms of",
    "start": "25140",
    "end": "30960"
  },
  {
    "text": "migrating in terms of services we offer to help with that transition and then",
    "start": "30960",
    "end": "36420"
  },
  {
    "text": "Aaron will actually talk about at FINRA how they were able to implement this and some of the lessons learned and then our",
    "start": "36420",
    "end": "42629"
  },
  {
    "text": "anticipation is that you know we'll leave about 15 minutes or so for questions because we want to make this interactive as well as much as possible",
    "start": "42629",
    "end": "49260"
  },
  {
    "text": "so we'll try to leave time at the end for Q&A so just talking about open",
    "start": "49260",
    "end": "54960"
  },
  {
    "start": "52000",
    "end": "376000"
  },
  {
    "text": "source database engine so you know myself I've been with AWS about three years majority of my career I was at",
    "start": "54960",
    "end": "61379"
  },
  {
    "text": "Oracle we're all as part of a pre sales team for Oracle database and I think you",
    "start": "61379",
    "end": "66420"
  },
  {
    "text": "know if you are coming from a commercial database engine today perhaps this kind",
    "start": "66420",
    "end": "71490"
  },
  {
    "text": "of pattern of purchasing commercial licenses makes sense right and you know",
    "start": "71490",
    "end": "76530"
  },
  {
    "text": "as you kind of increase capacity and you increase demand of course you know there's always a step function typically",
    "start": "76530",
    "end": "83430"
  },
  {
    "text": "at the end of the year or some kind do you have to true up where you can see is ever escalating commercial licensing",
    "start": "83430",
    "end": "89490"
  },
  {
    "text": "costs for your databases right and oftentimes this is one of the key motivations that customers come to us",
    "start": "89490",
    "end": "95310"
  },
  {
    "text": "with is that you know at some point this cost curve becomes unsustainable right",
    "start": "95310",
    "end": "100530"
  },
  {
    "text": "or becomes too onerous to the business and particularly if you're not using commercial applications that are",
    "start": "100530",
    "end": "107070"
  },
  {
    "text": "dependent upon the commercial database itself so you know many times you know you might be have a custom-built",
    "start": "107070",
    "end": "112770"
  },
  {
    "text": "application or you're not using all the features and functions that you know commercial databases provide and so the",
    "start": "112770",
    "end": "120090"
  },
  {
    "text": "question is you know can we essentially set the high-water mark in terms of the licensing fees and and support",
    "start": "120090",
    "end": "127530"
  },
  {
    "text": "maintenance fees and then you know think about is there way we can bring that down right so do an evaluation of our",
    "start": "127530",
    "end": "133810"
  },
  {
    "text": "understand you know which ones you know have strict dependencies either from a code perspective or from application",
    "start": "133810",
    "end": "140950"
  },
  {
    "text": "dependencies and then segregate out the ones that don't have those dependencies and choose an open source database",
    "start": "140950",
    "end": "147220"
  },
  {
    "text": "engine like my sequel there Postgres that could be alternative to commercial",
    "start": "147220",
    "end": "152470"
  },
  {
    "text": "database license and also go about how you know AWS can help that and then of course some customers say you know that",
    "start": "152470",
    "end": "158920"
  },
  {
    "text": "goal at the end is to be fully on open source engines right but in reality particularly with enterprise customers",
    "start": "158920",
    "end": "165340"
  },
  {
    "text": "you say well you know there could be a core set of applications that you know really will continue to require the",
    "start": "165340",
    "end": "171910"
  },
  {
    "text": "commercial database and that commercial database can either be on AWS either on",
    "start": "171910",
    "end": "176980"
  },
  {
    "text": "ec2 or part of RDS or even it could be on premise database right so you know depends upon where that makes the most",
    "start": "176980",
    "end": "183790"
  },
  {
    "text": "sense to run that but for the rest of the database is hopefully you know in the end or are much more agile in terms",
    "start": "183790",
    "end": "189220"
  },
  {
    "text": "of using the capacity we actually require and then hopefully you know the",
    "start": "189220",
    "end": "195100"
  },
  {
    "text": "cost perspective taking advantage of a lot of the features that open-source databases bring running on AWS platform",
    "start": "195100",
    "end": "202769"
  },
  {
    "text": "so you know there's always a choice in terms of where you run your database within AWS of course you know there's we",
    "start": "202769",
    "end": "210310"
  },
  {
    "text": "call it the the first option is self managed so you know using ec2 an elastic",
    "start": "210310",
    "end": "215680"
  },
  {
    "text": "block store EBS you can install and configure your database of choice but increasingly customers want to remove",
    "start": "215680",
    "end": "222250"
  },
  {
    "text": "some of the complexity of managing their database in terms of you might call undifferentiated effort and these are",
    "start": "222250",
    "end": "228519"
  },
  {
    "text": "these super critical tasks like backups and patching and planning for high availability that are you know critical",
    "start": "228519",
    "end": "234700"
  },
  {
    "text": "to running our database at the same time don't necessarily add uniqueness to your application so the core tenets of RDS or",
    "start": "234700",
    "end": "243010"
  },
  {
    "text": "relational database services let's remove some of that undifferentiated effort and think of the databases and",
    "start": "243010",
    "end": "248920"
  },
  {
    "text": "endpoints really a database as a service and then as you'll commonly find with AWS services we want to enable choice",
    "start": "248920",
    "end": "256019"
  },
  {
    "text": "through that so we don't think that and don't want to force you into you know one database engine in the case we've",
    "start": "256019",
    "end": "262390"
  },
  {
    "text": "already asked to meet all of your needs so we think by providing a wide spectrum of both commercial databases as well as",
    "start": "262390",
    "end": "268100"
  },
  {
    "text": "open-source databases can not only provide flexibility for your business but also choices in terms of how you",
    "start": "268100",
    "end": "274190"
  },
  {
    "text": "migrate and if you take this path of moving to open-source databases from commercial that you essentially have a",
    "start": "274190",
    "end": "280490"
  },
  {
    "text": "managed option within AWS as well if it makes sense and many customers will kind of add on",
    "start": "280490",
    "end": "286580"
  },
  {
    "text": "top of this where on the commercial databases we have two licensing models the first is bring your own license",
    "start": "286580",
    "end": "293690"
  },
  {
    "text": "where if you have licenses Oracle or sequel server you can bring them to AWS but then still take",
    "start": "293690",
    "end": "299810"
  },
  {
    "text": "advantage of the management capabilities of RDS service or depending upon you",
    "start": "299810",
    "end": "305270"
  },
  {
    "text": "know your region and the specific version of the database you're looking at we have options in terms of a license",
    "start": "305270",
    "end": "311120"
  },
  {
    "text": "included and what license included does is it provides as part of the hourly cost not only the infrastructure and the",
    "start": "311120",
    "end": "318590"
  },
  {
    "text": "services in terms of RDS monitoring and backups and configuration but actually",
    "start": "318590",
    "end": "324290"
  },
  {
    "text": "you provide the commercial software licensing through that cost as well and many details on our pricing page to go",
    "start": "324290",
    "end": "330170"
  },
  {
    "text": "through that but the reason I bring that up and in this context of this conversation is that if you are",
    "start": "330170",
    "end": "335990"
  },
  {
    "text": "embarking upon this like commercial license reduction strategy you know you might not want to renew for a full year",
    "start": "335990",
    "end": "342230"
  },
  {
    "text": "or three years of your commercial licenses and so the license included option kind of provides that runway",
    "start": "342230",
    "end": "348680"
  },
  {
    "text": "where you can continue to run your Oracle or sequel server workload but now you know pay-by-the-hour",
    "start": "348680",
    "end": "354680"
  },
  {
    "text": "right and then as you migrate to open-source potentially you know Postgres through my sequel or Moorea DB",
    "start": "354680",
    "end": "361160"
  },
  {
    "text": "that you'd be able to then you know extend the life for the current application so lots of details that that",
    "start": "361160",
    "end": "366560"
  },
  {
    "text": "are provided on our pricing page and of course options and pricing do vary by regions so it's important to understand",
    "start": "366560",
    "end": "372770"
  },
  {
    "text": "the differences in the region and what you're operating and then you know I've talked about this",
    "start": "372770",
    "end": "378410"
  },
  {
    "start": "376000",
    "end": "453000"
  },
  {
    "text": "little bit so you know what's the premise of RDS is you know thinking about where do database administrators spend their time and so a lot of time is",
    "start": "378410",
    "end": "385640"
  },
  {
    "text": "spent on you know from an infrastructure perspective you know there's the basics like you know you know configuring and",
    "start": "385640",
    "end": "392060"
  },
  {
    "text": "provisioning the hardware right rocky and sacking it there's the idea that you have to install the operating system patch the operating system",
    "start": "392060",
    "end": "399409"
  },
  {
    "text": "you need to think about installing and patching the database software think about storage and how much storage I",
    "start": "399409",
    "end": "406429"
  },
  {
    "text": "need to allocate and what's the demand gonna be like one year or two years or three years because you're essentially you know purchasing this hardware to be",
    "start": "406429",
    "end": "413719"
  },
  {
    "text": "able to run but even if you choose to run your database on ec2 you certainly you remove some of that complexity so it",
    "start": "413719",
    "end": "420739"
  },
  {
    "text": "provides a lot of flexibility in terms of right sizing your database workload to the actual computational and memory",
    "start": "420739",
    "end": "426919"
  },
  {
    "text": "requirements but still the operating system and database patching and planning for backups planning for high",
    "start": "426919",
    "end": "432589"
  },
  {
    "text": "availability are all still required and then if you look at the blue piece this",
    "start": "432589",
    "end": "437719"
  },
  {
    "text": "on this particular visualization we say this is roughly 25 to 30 percent of your time which is actually the interesting",
    "start": "437719",
    "end": "444169"
  },
  {
    "text": "thing which is making your application perform to add value to the business right to be able to add uniqueness in",
    "start": "444169",
    "end": "450559"
  },
  {
    "text": "terms of the functionality that the database is providing and so with RDS you know we want to think about you know",
    "start": "450559",
    "end": "457249"
  },
  {
    "start": "453000",
    "end": "592000"
  },
  {
    "text": "what if you could provision a new database in less than six minutes right so in terms of evaluating open-source",
    "start": "457249",
    "end": "463219"
  },
  {
    "text": "databases do they make sense a lot of customers will say well I don't have the operational experience in terms of",
    "start": "463219",
    "end": "468949"
  },
  {
    "text": "providing high availability or backups or Postgres for instance but by leveraging RDS service this is all",
    "start": "468949",
    "end": "475279"
  },
  {
    "text": "included and you can quickly test and evaluate and then if the test is successful scale to productive",
    "start": "475279",
    "end": "480829"
  },
  {
    "text": "production scale or turn off right and this you know iterative approach really becomes an important step in terms of",
    "start": "480829",
    "end": "487639"
  },
  {
    "text": "how we migrate and then you know things like multi availability zone so how do I provision a standby database with the",
    "start": "487639",
    "end": "493639"
  },
  {
    "text": "completely separate data center or availability zone if you look at doing that on your own sometimes it could take",
    "start": "493639",
    "end": "499999"
  },
  {
    "text": "you know they'd be documented about 122 steps on my sequel for high availability and multi AZ if we did it through ec2",
    "start": "499999",
    "end": "508489"
  },
  {
    "text": "but on RDS service it's just a single API call or single check box it would",
    "start": "508489",
    "end": "513589"
  },
  {
    "text": "revise a lot of flexibility in terms of having that production quality database and then patching on RDS there's a lot",
    "start": "513589",
    "end": "519439"
  },
  {
    "text": "of details in terms of how patches are applied you can opt in or opt out of patches but simplistically you know we",
    "start": "519439",
    "end": "525410"
  },
  {
    "text": "want to provide an automated way for database patches for operating system patches so of course you have control in",
    "start": "525410",
    "end": "531139"
  },
  {
    "text": "terms of when patches you and you know being able to test them beforehand but you know always staying",
    "start": "531139",
    "end": "537089"
  },
  {
    "text": "current on those becomes a increasing value proposition that customers take advantage of an RDS and then you know",
    "start": "537089",
    "end": "545130"
  },
  {
    "text": "the fact that RDS provides automatic backups is you know really important as well as point in time recovery so if",
    "start": "545130",
    "end": "551610"
  },
  {
    "text": "there is a failure you know certainly you can failover to second availability zone but also having your backup stored",
    "start": "551610",
    "end": "557100"
  },
  {
    "text": "into s3 with eleven nines of availability provides assurance and again this is something that you just",
    "start": "557100",
    "end": "562589"
  },
  {
    "text": "schedule the time and when the backups are taken your database stays online but is able to continue to run and there's",
    "start": "562589",
    "end": "568380"
  },
  {
    "text": "many other features in terms of security and encryption and particularly if you're thinking about migrating from",
    "start": "568380",
    "end": "573899"
  },
  {
    "text": "commercial database you know high availability encryption security controls oftentimes these are additional",
    "start": "573899",
    "end": "580170"
  },
  {
    "text": "expenses or features that force you into sort of the enterprise edition of commercial databases whereas from an RDS",
    "start": "580170",
    "end": "586709"
  },
  {
    "text": "perspective you can use all of these functions with an open-source framework to simply through the capabilities that",
    "start": "586709",
    "end": "591930"
  },
  {
    "text": "RDS itself provides so moving beyond just traditional RDS you know we want to",
    "start": "591930",
    "end": "598139"
  },
  {
    "text": "think about and customers asked us to consider well you know that's great from an availability perspective patching and",
    "start": "598139",
    "end": "604230"
  },
  {
    "text": "backups but still from the execution engine perspective there's the idea that",
    "start": "604230",
    "end": "609720"
  },
  {
    "text": "commercial databases provide superior durability or throughput or performance right so we often kinds if you've seen",
    "start": "609720",
    "end": "617819"
  },
  {
    "text": "the India talks around Amazon Aurora you'll hear that you know we wanted to reimagine the way the relational",
    "start": "617819",
    "end": "623399"
  },
  {
    "text": "database worked right so if you're building a relational database today you take advantage of cloud native",
    "start": "623399",
    "end": "628439"
  },
  {
    "text": "architectures and be able to deliver the performance and durability and",
    "start": "628439",
    "end": "633630"
  },
  {
    "text": "availability of commercial databases but do so at a price point a more aligned with open source ecosystem and this is",
    "start": "633630",
    "end": "640589"
  },
  {
    "text": "really what you know the design principles of Amazon Aurora and so when",
    "start": "640589",
    "end": "646740"
  },
  {
    "text": "we started with with Aurora you might be familiar it's been available for over two years and it has you know full my",
    "start": "646740",
    "end": "652529"
  },
  {
    "text": "sequel compatibility for my sequel five six compatibility and I didn't get a chance to update the slide but if you're",
    "start": "652529",
    "end": "658589"
  },
  {
    "text": "part of the you know yesterday we also extended the Aurora",
    "start": "658589",
    "end": "663750"
  },
  {
    "text": "to now have post compatibility as well so we have the choice of either my sequel or Postgres",
    "start": "663750",
    "end": "669410"
  },
  {
    "text": "compatibility so sometimes I get asked what's the on premise equivalent of Aurora and of course that that answer is",
    "start": "669410",
    "end": "675380"
  },
  {
    "text": "my sequel or Postgres right so from an application perspective from the query",
    "start": "675380",
    "end": "680900"
  },
  {
    "text": "execution perspective human perspective 100% compatible and just we walk through some things we've done at the storage",
    "start": "680900",
    "end": "686960"
  },
  {
    "text": "side to make it to make it unique so really we've talked about this idea of a storage service oriented architecture",
    "start": "686960",
    "end": "693050"
  },
  {
    "start": "689000",
    "end": "722000"
  },
  {
    "text": "applied to the database right so we thought about you know purpose-built storage for database workloads on",
    "start": "693050",
    "end": "699800"
  },
  {
    "text": "transaction processing and we wanted to spread that across multiple availability zones to provide high availability and",
    "start": "699800",
    "end": "706090"
  },
  {
    "text": "separate out the storage from the database and then think about the different layers of the database",
    "start": "706090",
    "end": "712280"
  },
  {
    "text": "architecture and can we segment that as out into services and then use native AWS services like DynamoDB and civil",
    "start": "712280",
    "end": "719630"
  },
  {
    "text": "workflow to essentially provide the coordination and so from a storage perspective again from the API",
    "start": "719630",
    "end": "726080"
  },
  {
    "start": "722000",
    "end": "780000"
  },
  {
    "text": "perspective sequel perspective compatibility perspective we have full compatibility with my sequel and",
    "start": "726080",
    "end": "731840"
  },
  {
    "text": "Postgres but from the storage perspective now we are starting to deliver on many of the performance",
    "start": "731840",
    "end": "737620"
  },
  {
    "text": "increasing performance that commercial database is office so Aurora storage is not available to systems outside of",
    "start": "737620",
    "end": "744950"
  },
  {
    "text": "Aurora it's purpose-built it requires three availability zones it actually looks at 10 megabyte",
    "start": "744950",
    "end": "751760"
  },
  {
    "text": "sections of redo logs and replays those review logs directly and storage and it",
    "start": "751760",
    "end": "756860"
  },
  {
    "text": "gets replicates them six times across three availability zones to provide high availability for your database",
    "start": "756860",
    "end": "762830"
  },
  {
    "text": "transactions and then continuously and automatically pushes those those changes down to s3 as well so of course there's",
    "start": "762830",
    "end": "770360"
  },
  {
    "text": "six copies across three availability zones of your data in the Aurora storage and then it's continuously automatically",
    "start": "770360",
    "end": "776090"
  },
  {
    "text": "replayed back into s3 which provides the eleven nines of availability there and",
    "start": "776090",
    "end": "781460"
  },
  {
    "start": "780000",
    "end": "836000"
  },
  {
    "text": "some of the benefits is of course you know instant crash recovery so if you've ever had a failure or had to look at you",
    "start": "781460",
    "end": "787130"
  },
  {
    "text": "know filling over two separate database instance the database needs to catch up and so failover might take you know 15",
    "start": "787130",
    "end": "793370"
  },
  {
    "text": "seconds 30 seconds whatever but it might take a few minutes for the database to process through the arc log files or whatever files there might",
    "start": "793370",
    "end": "800420"
  },
  {
    "text": "be to get the point in time back in database up and running of course with Aurora because we have segmented these",
    "start": "800420",
    "end": "806960"
  },
  {
    "text": "out and just 10 megabyte chunks are able to replay a lot faster and so we say it's nearly instantaneous crash recovery",
    "start": "806960",
    "end": "812720"
  },
  {
    "text": "for database so when we do a failover in however unlikely inventive a failure the",
    "start": "812720",
    "end": "817790"
  },
  {
    "text": "database can be up and running typically in less than 45 seconds which includes the time to detect the failure move the",
    "start": "817790",
    "end": "824270"
  },
  {
    "text": "DNS information over and then start the database up and get it open and some customers actually find that time to be",
    "start": "824270",
    "end": "830480"
  },
  {
    "text": "significantly less than 45 seconds but as a rule of thumb significant gain over even RDS architecture and then the other",
    "start": "830480",
    "end": "838160"
  },
  {
    "start": "836000",
    "end": "869000"
  },
  {
    "text": "point that we deliver through Aurora is the fact that the database cache remains",
    "start": "838160",
    "end": "843320"
  },
  {
    "text": "survivable in case of failover right so one of the problems of you know standby databases if it does fail over and I go",
    "start": "843320",
    "end": "850430"
  },
  {
    "text": "through my crash recovery within the database cache all the database objects need to be repopulated back into memory",
    "start": "850430",
    "end": "855830"
  },
  {
    "text": "that takes a period of time but with the Aurora storage engines again for both my sequel and Postgres provides a",
    "start": "855830",
    "end": "862250"
  },
  {
    "text": "survivable cache those provide significant application performance in the event I do need to failover to one",
    "start": "862250",
    "end": "868400"
  },
  {
    "text": "of my standbys and then with Aurora you know I move beyond the RDS model of a",
    "start": "868400",
    "end": "874310"
  },
  {
    "text": "single fail of our target that remains inaccessible except in the case of",
    "start": "874310",
    "end": "879890"
  },
  {
    "text": "failure but now I can have up to 15 Aurora replicas across three availability zones any of those 15",
    "start": "879890",
    "end": "886610"
  },
  {
    "text": "replicas can be failover targets and you can set precedents in terms of which one goes first second third fourth but then",
    "start": "886610",
    "end": "893840"
  },
  {
    "text": "the standby database is also available for reads which can be an important cost-saving consideration so you know",
    "start": "893840",
    "end": "901640"
  },
  {
    "text": "the Aurora storage one it can go up to 64 terabytes you only pay for the storage you're actually using so it",
    "start": "901640",
    "end": "906860"
  },
  {
    "text": "dynamically grows and then the fact that you know your standby database can now be read from kind of results in you know",
    "start": "906860",
    "end": "913910"
  },
  {
    "text": "customers actually find lower-cost by adopting Aurora then even if the running",
    "start": "913910",
    "end": "918950"
  },
  {
    "text": "already asked my sequel because in you know already as my sequel your standby databases is not available for reads",
    "start": "918950",
    "end": "924800"
  },
  {
    "text": "it's only there for failure and then also you have to pre allocate storage in traditional RDS model words of the",
    "start": "924800",
    "end": "931730"
  },
  {
    "text": "Aurora you can simply start expanding dynamically in 10 megabyte chunks which provides net cost savings for for most",
    "start": "931730",
    "end": "939290"
  },
  {
    "text": "customers so another enhancement to kind of you know bring bridge this gap in",
    "start": "939290",
    "end": "945260"
  },
  {
    "start": "940000",
    "end": "1065000"
  },
  {
    "text": "terms of commercial database performance on open source engines at AWS delivers is increasing insight into what's going",
    "start": "945260",
    "end": "951920"
  },
  {
    "text": "on the database itself so one of the the push backs we hear is you know in the RDS model since you're delivering",
    "start": "951920",
    "end": "958070"
  },
  {
    "text": "database as a service you know my database administrators don't have you know the super user privilege you know",
    "start": "958070",
    "end": "964070"
  },
  {
    "text": "sis or sis TVA or sa roll kind of equivalent but then also you know they",
    "start": "964070",
    "end": "969589"
  },
  {
    "text": "can't log in to the operating system you know there's good reasons from a managed service perspective that we limit that",
    "start": "969589",
    "end": "974750"
  },
  {
    "text": "because you know we have a lot of automation and we manage a pretty massive fleet of databases so you still",
    "start": "974750",
    "end": "980810"
  },
  {
    "text": "have the most relevant roles for application developers but you know sometimes it's seen as this black box",
    "start": "980810",
    "end": "986720"
  },
  {
    "text": "and you're not able to see what's actually going on the database so over time even increasingly added new metrics",
    "start": "986720",
    "end": "993140"
  },
  {
    "text": "in terms of moving beyond just CPU and i/o and stats and so there's over 40",
    "start": "993140",
    "end": "998360"
  },
  {
    "text": "metrics now that you can monitor under one-second granularity you know have",
    "start": "998360",
    "end": "1003370"
  },
  {
    "text": "them exposed to cloud watch logs and then be able to do alerts of those so if you're out of so you know going out of",
    "start": "1003370",
    "end": "1009250"
  },
  {
    "text": "memory or storage kind of problem the database these cannot be alerted upon but then we've kind of extended that now",
    "start": "1009250",
    "end": "1014890"
  },
  {
    "text": "so another announcement that you might have heard about earlier yesterday was performance insights right so moving",
    "start": "1014890",
    "end": "1021310"
  },
  {
    "text": "beyond the operating system and and basic IO characteristics but actually being able to get insights into the",
    "start": "1021310",
    "end": "1027850"
  },
  {
    "text": "database performance right so we want to continue to open up the monitoring configuration and availability of the",
    "start": "1027850",
    "end": "1034780"
  },
  {
    "text": "database within the open source world but delivering it as a managed service so this kind of combination of things",
    "start": "1034780",
    "end": "1040300"
  },
  {
    "text": "you know looking at you know reducing licensing costs by adopting you know open source databases and then making",
    "start": "1040300",
    "end": "1045970"
  },
  {
    "text": "the decision if Amazon aurora you know delivers the durability performance and throughput you're looking for while",
    "start": "1045970",
    "end": "1052630"
  },
  {
    "text": "delivering that as a managed service with appropriate insights into actual debugging or application performance",
    "start": "1052630",
    "end": "1058750"
  },
  {
    "text": "they're kind of the core tenants that customers take advantage of as they go through this process of moving off",
    "start": "1058750",
    "end": "1063880"
  },
  {
    "text": "commercial databases um so I'm a pricing perspective you know I think you know some of the tenants are",
    "start": "1063880",
    "end": "1069940"
  },
  {
    "start": "1065000",
    "end": "1120000"
  },
  {
    "text": "again you know get away from the perpetual licensing or the one year two or three year commitments and really",
    "start": "1069940",
    "end": "1076090"
  },
  {
    "text": "moving towards you know on-demand pricing right so you just pay for it you know the the capacity that you need you",
    "start": "1076090",
    "end": "1083770"
  },
  {
    "text": "can write size based on your workload and then you know of course we do provide you know discount so with a",
    "start": "1083770",
    "end": "1089140"
  },
  {
    "text": "nature of database workload such that I might have on-demand period of time for you know a few months or maybe six",
    "start": "1089140",
    "end": "1095049"
  },
  {
    "text": "months where I'm testing does it make sense does my application work but then given the nature of database workloads",
    "start": "1095049",
    "end": "1101080"
  },
  {
    "text": "that they're typically on 24 by 7 you know thinking about you know reserved instance pricing becomes you know a",
    "start": "1101080",
    "end": "1106809"
  },
  {
    "text": "really a great cost savings mechanism but you know there's no lock-in through",
    "start": "1106809",
    "end": "1111880"
  },
  {
    "text": "you know software contracts or 22% maintenance costs that customers really appreciate and and oftentimes provide a",
    "start": "1111880",
    "end": "1118630"
  },
  {
    "text": "motivation for change so having said that hopefully gave you a little bit of an overview of you know some of the",
    "start": "1118630",
    "end": "1125080"
  },
  {
    "text": "reasons why customers move and some of the platform's they develop and now John's going to talk a little bit more about how customers actually can",
    "start": "1125080",
    "end": "1130990"
  },
  {
    "text": "services we provide to actually do that migration great thanks Greg thank you",
    "start": "1130990",
    "end": "1137860"
  },
  {
    "text": "for the introduction my name is John Winfred I am the technical program manager for the database migration service and scheme the conversion tool",
    "start": "1137860",
    "end": "1144340"
  },
  {
    "text": "so we're gonna talk a little bit today about how you can migrate your databases to AWS as well as do a bit of a demo so",
    "start": "1144340",
    "end": "1151750"
  },
  {
    "text": "we're just not all looking at powerpoints all day long so when I joined AWS I thought I was doing a tech",
    "start": "1151750",
    "end": "1158740"
  },
  {
    "start": "1156000",
    "end": "1291000"
  },
  {
    "text": "company thought you know play the latest greatest technology what it's ended up is I'm actually spending all my time",
    "start": "1158740",
    "end": "1163990"
  },
  {
    "text": "moving stuff around I ended up being a bit of a moving company if you will but in all this experience helping many",
    "start": "1163990",
    "end": "1169780"
  },
  {
    "text": "thousands of customers move their databases I've come up with a lot of interesting tidbits of information and",
    "start": "1169780",
    "end": "1175120"
  },
  {
    "text": "things that people need to be aware of because as much as I like to project that migration is easy the fact is it's",
    "start": "1175120",
    "end": "1181750"
  },
  {
    "text": "not an afternoon project and a lot of analysis and thought is really good thing to put into your project before",
    "start": "1181750",
    "end": "1187600"
  },
  {
    "text": "you get going things to think about right off the bat are can you afford downtime because if you can afford",
    "start": "1187600",
    "end": "1193450"
  },
  {
    "text": "downtime your migration options are much wider than if you can't it makes it simpler but",
    "start": "1193450",
    "end": "1199450"
  },
  {
    "text": "you know has those drawbacks about being a bit more intrusive to the end-users if you're doing a migration is it a good",
    "start": "1199450",
    "end": "1206350"
  },
  {
    "text": "time to think about moving off a commercial database Greg talked a lot about some of the advantages of the",
    "start": "1206350",
    "end": "1211840"
  },
  {
    "text": "open-source platforms but of course what with RDS and ec2 you can stick with what you've got no problem at all as part of",
    "start": "1211840",
    "end": "1219279"
  },
  {
    "text": "the migration you could also think about you know what data you need to bring do you need to bring everything you've ever",
    "start": "1219279",
    "end": "1225250"
  },
  {
    "text": "recorded for all time now I know when you probably speak to your finance department they're gonna say oh no we",
    "start": "1225250",
    "end": "1230740"
  },
  {
    "text": "need that data from 1972 but do you really migrating into the cloud is a",
    "start": "1230740",
    "end": "1236080"
  },
  {
    "text": "good time to look at you know shrinking that down and just bring you what you need now as much as we'd like to talk",
    "start": "1236080",
    "end": "1244210"
  },
  {
    "text": "about moving to open-source fully aware that it's not always plausible but a good thing to do is take an inventory of",
    "start": "1244210",
    "end": "1250450"
  },
  {
    "text": "your applications and go do they support another application other-- database engine things may have changed since you",
    "start": "1250450",
    "end": "1256210"
  },
  {
    "text": "purchased it they may have added new engine support or if you have in-house applications what's the effort that it's",
    "start": "1256210",
    "end": "1262809"
  },
  {
    "text": "going to take to change it to work with a new engine and probably more importantly than anything else do you",
    "start": "1262809",
    "end": "1268210"
  },
  {
    "text": "have the technical expertise to support a new engine type although all database",
    "start": "1268210",
    "end": "1273340"
  },
  {
    "text": "engines are inherently similar there's obviously some technical differences not saying people can't learn but if you are",
    "start": "1273340",
    "end": "1278710"
  },
  {
    "text": "going to switch engines it's something to consider and the other bullet point I usually have at the anteriors are you",
    "start": "1278710",
    "end": "1284139"
  },
  {
    "text": "aware of some of the benefits and managed service can bring I think Greg's gone through a lot of those so I would answer that question yes right now so",
    "start": "1284139",
    "end": "1293190"
  },
  {
    "start": "1291000",
    "end": "1357000"
  },
  {
    "text": "this is gonna sound sort of strange coming from me but in a way we don't care what you use to migrate to the",
    "start": "1293190",
    "end": "1299649"
  },
  {
    "text": "cloud we just want you to migrate so I'm gonna appear to talk about DMS and SCT but it's actually important to note",
    "start": "1299649",
    "end": "1305559"
  },
  {
    "text": "there's a lot of other options especially like I mentioned if you're willing to take some downtime if you're",
    "start": "1305559",
    "end": "1310809"
  },
  {
    "text": "not willing to take downtime this list gets a lot shorter but if you are there's some great options out there",
    "start": "1310809",
    "end": "1315880"
  },
  {
    "text": "things like with sequel server the RDS team recently announced the support for back file import which is something you",
    "start": "1315880",
    "end": "1322330"
  },
  {
    "text": "won't find anywhere else so if you can take that outage great just take the back file it's gonna be the simplest way",
    "start": "1322330",
    "end": "1327850"
  },
  {
    "text": "to get your data into the cloud now that's not to say you can't use it in conjunction with things like DMS attached",
    "start": "1327850",
    "end": "1332890"
  },
  {
    "text": "replicate the changes across if you wanted to after the fact but you know just be aware of the some of the options",
    "start": "1332890",
    "end": "1338020"
  },
  {
    "text": "that are out there my sequel read replicas perfect example you know if you're gonna stick with my sequel platformer there's no harm at all going",
    "start": "1338020",
    "end": "1344740"
  },
  {
    "text": "that road and of course the other other engines have their own equivalents of the way you can get data in and out but",
    "start": "1344740",
    "end": "1350590"
  },
  {
    "text": "as I mentioned if you want to have a low downtime migration that list gets a lot shorter so at AWS were all about being",
    "start": "1350590",
    "end": "1359770"
  },
  {
    "start": "1357000",
    "end": "1430000"
  },
  {
    "text": "responsive to customers and one of the things we heard for many years was that customers wanted it to be easier and",
    "start": "1359770",
    "end": "1366070"
  },
  {
    "text": "less intrusive to migrate to the cloud what I mean by less intrusive is that your end-users aren't actually going to",
    "start": "1366070",
    "end": "1372340"
  },
  {
    "text": "even know that the migration is happening the downtime during the migration to switch from your original",
    "start": "1372340",
    "end": "1377380"
  },
  {
    "text": "source on-premise your new cloud will be very minimal in fact on the order of seconds to minutes as opposed to hours",
    "start": "1377380",
    "end": "1384060"
  },
  {
    "text": "and then once the migration is complete customers asked us to enable ongoing replication ongoing sync between your",
    "start": "1384060",
    "end": "1391360"
  },
  {
    "text": "on-premise system to the cloud to make sure everything's all good or perhaps synchronizing back to",
    "start": "1391360",
    "end": "1396550"
  },
  {
    "text": "on-premise if you feel you need to have a dr on-premise and the other thing they've actually asked is to enable a",
    "start": "1396550",
    "end": "1402850"
  },
  {
    "text": "sync between database engines between database engines to give you that flexibility to perhaps store bi data in",
    "start": "1402850",
    "end": "1409420"
  },
  {
    "text": "one engine that's maybe open source and free versus a commercial engine which you need to use because you have some",
    "start": "1409420",
    "end": "1414850"
  },
  {
    "text": "application that only supports a commercial engine now all of that said this sync and the ability to move",
    "start": "1414850",
    "end": "1420610"
  },
  {
    "text": "between engines addresses one of our number one requests and that is the ability to move off commercial license",
    "start": "1420610",
    "end": "1426220"
  },
  {
    "text": "intensive engines onto cloud native open-source solutions so with all that",
    "start": "1426220",
    "end": "1432190"
  },
  {
    "start": "1430000",
    "end": "1503000"
  },
  {
    "text": "in mind that we came up with DMS for those of you that were here last year and they announced at last year at reinvent we went into a beta program and",
    "start": "1432190",
    "end": "1439630"
  },
  {
    "text": "we open it up GA early 20 what years a 2016 and it's now been widely available",
    "start": "1439630",
    "end": "1446260"
  },
  {
    "text": "we've had a number of new updates and we've actually added a number of new engines the great thing about DMS is we",
    "start": "1446260",
    "end": "1453250"
  },
  {
    "text": "designed to be simple you can get going in 10 minutes or less and we designed it to enable a near zero downtime migration",
    "start": "1453250",
    "end": "1459220"
  },
  {
    "text": "the kind of catch phrase we use is we try to call it a replication Swiss Army knife we give you the ability to move",
    "start": "1459220",
    "end": "1464440"
  },
  {
    "text": "between engines between on-premise ec2 RDS or back the other way it's",
    "start": "1464440",
    "end": "1470380"
  },
  {
    "text": "always a good time to point out that AWS doesn't believe in vendor lock-in if you want to use DMS to move your data back",
    "start": "1470380",
    "end": "1475960"
  },
  {
    "text": "out of the cloud somewhere else you can do it the only restriction we have with DMS is you can't move your data from an",
    "start": "1475960",
    "end": "1481960"
  },
  {
    "text": "on-premise system back to an on-premise system and good time to just reiterate",
    "start": "1481960",
    "end": "1488080"
  },
  {
    "text": "that Amazon Aurora logo actually includes the new Postgres sequel compatible version so you can definitely",
    "start": "1488080",
    "end": "1494260"
  },
  {
    "text": "use the MS to move data from whatever data source you're using into the new Aurora Postgres sequel compatible",
    "start": "1494260",
    "end": "1501690"
  },
  {
    "text": "edition so you won't get into these",
    "start": "1501690",
    "end": "1507400"
  },
  {
    "start": "1503000",
    "end": "1567000"
  },
  {
    "text": "details in some of the big keynotes migration is a complicated thing if you were moving data you know that's only",
    "start": "1507400",
    "end": "1515020"
  },
  {
    "text": "one part of the equation there's actually this whole thing called the database schema which I'm sure everyone in this room knows about DMS is",
    "start": "1515020",
    "end": "1521170"
  },
  {
    "text": "primarily a data mover if you were to fire up a DMS task right now and move data from source to target DMS is gonna",
    "start": "1521170",
    "end": "1527440"
  },
  {
    "text": "create the tables that it needs to move data it's gonna use the same names the same data types or the equivalent data",
    "start": "1527440",
    "end": "1533080"
  },
  {
    "text": "types in a new engine type but it's not going to move all the other stuff the stored procedures the triggers the views",
    "start": "1533080",
    "end": "1539320"
  },
  {
    "text": "all of this sort of almost intelligence part of database if you will that's where the schema conversion tool comes",
    "start": "1539320",
    "end": "1545620"
  },
  {
    "text": "in it's a free download off our website and it helps you convert from one engine type to another and move all of those",
    "start": "1545620",
    "end": "1552310"
  },
  {
    "text": "objects across it's not going to get at 100% obviously there are some very engine specific things that differ from",
    "start": "1552310",
    "end": "1559000"
  },
  {
    "text": "engine to engine but it gets a huge percentage of it and will save you many many hours of work and it's available",
    "start": "1559000",
    "end": "1565570"
  },
  {
    "text": "for a lot of different engines here are all the relational engines SCT works",
    "start": "1565570",
    "end": "1571420"
  },
  {
    "start": "1567000",
    "end": "1660000"
  },
  {
    "text": "with so you can see you could convert an Oracle schema to Postgres or just as easily if you were in my sequel and you",
    "start": "1571420",
    "end": "1577930"
  },
  {
    "text": "wanted to try a Postgres it can help you convert that scheme as well just reiterating free download give it a shot",
    "start": "1577930",
    "end": "1584260"
  },
  {
    "text": "works with a variety of different platforms and it really does help automate that conversion between engines",
    "start": "1584260",
    "end": "1589360"
  },
  {
    "text": "we actually also recently added what we call schema copy so your actual first",
    "start": "1589360",
    "end": "1594580"
  },
  {
    "text": "step of a migration is to move your schema even if you're sticking with the same database engine SCT can actually copy that in that",
    "start": "1594580",
    "end": "1601250"
  },
  {
    "text": "schema saved from one oracle database to a new empty oracle database and then use DMS to move the data across it has some",
    "start": "1601250",
    "end": "1608360"
  },
  {
    "text": "neat features in there it will highlight for example if you are using features in",
    "start": "1608360",
    "end": "1613400"
  },
  {
    "text": "your source schema that would not be available in RDS sometimes already s isn't going to be the solution for you",
    "start": "1613400",
    "end": "1618440"
  },
  {
    "text": "and this will do a good job about telling you what will and will not work it will also help automate code changes",
    "start": "1618440",
    "end": "1626390"
  },
  {
    "text": "so if you have a bunch of in-house applications after you've converted your schema you can point it at your source",
    "start": "1626390",
    "end": "1631880"
  },
  {
    "text": "code repository and it will scan your source code and attempt to change any embedded sequel statements from whatever",
    "start": "1631880",
    "end": "1638510"
  },
  {
    "text": "they were written in to whatever the new target language will be as CTE is also",
    "start": "1638510",
    "end": "1644450"
  },
  {
    "text": "available for data warehouse conversions so if you're looking to move off one data warehouse engine and obviously",
    "start": "1644450",
    "end": "1650030"
  },
  {
    "text": "we're a little bit biased here to Amazon redshift it can help convert that schema and optimize it by evaluating your usage",
    "start": "1650030",
    "end": "1658040"
  },
  {
    "text": "as well in the process so to reiterate data migration is a two-step process",
    "start": "1658040",
    "end": "1664520"
  },
  {
    "start": "1660000",
    "end": "1693000"
  },
  {
    "text": "your first step is to move your schema your second step is to move your data using DMS now moving your schema as I",
    "start": "1664520",
    "end": "1671150"
  },
  {
    "text": "said you can use SCT whether you're switching database engines or sticking with the same engine or you can use your",
    "start": "1671150",
    "end": "1677330"
  },
  {
    "text": "native tools if you're more comfortable if you're staying with the same engine type so say you were working with sequel",
    "start": "1677330",
    "end": "1683000"
  },
  {
    "text": "server you could just use sequel management studio to export your schema and then import it and apply it to your",
    "start": "1683000",
    "end": "1688669"
  },
  {
    "text": "new target before using DMS you certainly don't have to use SCT so how",
    "start": "1688669",
    "end": "1694610"
  },
  {
    "start": "1693000",
    "end": "1728000"
  },
  {
    "text": "is it the DMS enables near zero downtime migration if you look at the picture up on the screen although it does say",
    "start": "1694610",
    "end": "1700280"
  },
  {
    "text": "customer premise and AWS remember like I said you can easily use this to go the other way so you could reverse the",
    "start": "1700280",
    "end": "1706580"
  },
  {
    "text": "diagram or you could be moving data between ec2 and RDS so you start by",
    "start": "1706580",
    "end": "1711740"
  },
  {
    "text": "spinning up a replication server behind the scenes this is an ec2 box that we have our software installed on but",
    "start": "1711740",
    "end": "1718370"
  },
  {
    "text": "what's different from a normal ec2 host is you never log on to it directly it's a managed service so you interact",
    "start": "1718370",
    "end": "1723890"
  },
  {
    "text": "with it through the AWS console the CLI or the SDK once it's up and running you",
    "start": "1723890",
    "end": "1729470"
  },
  {
    "start": "1728000",
    "end": "1777000"
  },
  {
    "text": "define your endpoints so your endpoints are in essence like an ODBC connection string you're basically telling DMS how do I connect to my",
    "start": "1729470",
    "end": "1736019"
  },
  {
    "text": "database now the DMS server is actually like a big CPU so you can have many",
    "start": "1736019",
    "end": "1741990"
  },
  {
    "text": "different endpoints and many different replication tasks going on at the same time you could be moving data from",
    "start": "1741990",
    "end": "1746999"
  },
  {
    "text": "on-prem to the cloud and at the same time moving data from another database that's already in the cloud to a data",
    "start": "1746999",
    "end": "1753029"
  },
  {
    "text": "warehouse for analysis if you wanted so as many endpoints as you want but what you do then is you select which tables",
    "start": "1753029",
    "end": "1759509"
  },
  {
    "text": "schemas or databases you want to move you can move the whole database or if you move bits of the database it's a",
    "start": "1759509",
    "end": "1765570"
  },
  {
    "text": "logical replication product it doesn't move things down at the block level it in essence queries the data out of the",
    "start": "1765570",
    "end": "1771899"
  },
  {
    "text": "engine and inserts it into the target and that's how we can go between different engines then you sit back",
    "start": "1771899",
    "end": "1778679"
  },
  {
    "start": "1777000",
    "end": "1819000"
  },
  {
    "text": "relax and watch DMS move the data from source to target and if you've chosen it we'll keep it in sync keep it in the",
    "start": "1778679",
    "end": "1785970"
  },
  {
    "text": "sync is a good time to mention that although it's called the database migration service it's actually a",
    "start": "1785970",
    "end": "1791519"
  },
  {
    "text": "replication tool so you can use this for an ongoing replication between systems",
    "start": "1791519",
    "end": "1796769"
  },
  {
    "text": "whether you're doing this for a dr purpose or perhaps for bi and analysis so you could be continually feeding data",
    "start": "1796769",
    "end": "1803220"
  },
  {
    "text": "from your ERP system to your BI system and it'll be up-to-date within seconds so there are a lot of uses that you can",
    "start": "1803220",
    "end": "1809669"
  },
  {
    "text": "do with DMS and it's also important to note that you can filter down tables as well so you don't need to move all the",
    "start": "1809669",
    "end": "1815820"
  },
  {
    "text": "records in a table you can just move groups of records and when you're ready",
    "start": "1815820",
    "end": "1821159"
  },
  {
    "start": "1819000",
    "end": "1843000"
  },
  {
    "text": "you take a small outage flip your applications over from source to target and away you go so that's why we call it a minimal",
    "start": "1821159",
    "end": "1827129"
  },
  {
    "text": "downtime migration not a no downtime migration because that flip it's gonna take some time whether it's a second or",
    "start": "1827129",
    "end": "1833909"
  },
  {
    "text": "two because all you're doing is updating a DNS record or you need to recompile your app to point at the new engine you",
    "start": "1833909",
    "end": "1839340"
  },
  {
    "text": "know that's why we don't give any set figures but it's pretty quick going back",
    "start": "1839340",
    "end": "1845009"
  },
  {
    "start": "1843000",
    "end": "1875000"
  },
  {
    "text": "to how DMS works logical replication product like I mentioned we load table by table by default we do eight tables",
    "start": "1845009",
    "end": "1851789"
  },
  {
    "text": "at a time although you can configure it if there's any interruption during the replication for whatever reason during",
    "start": "1851789",
    "end": "1858299"
  },
  {
    "text": "the first phase which we call the bulk load phase when you're taking the whole set of data in the table it will restart",
    "start": "1858299",
    "end": "1865049"
  },
  {
    "text": "whatever table it happens to be middle of if you're doing the replication phase where things are being kept in sync it's just gonna resume from",
    "start": "1865049",
    "end": "1871830"
  },
  {
    "text": "where it left off down to the row level so how does all of this work we have",
    "start": "1871830",
    "end": "1878160"
  },
  {
    "start": "1875000",
    "end": "1926000"
  },
  {
    "text": "something we call change data capture behind the scenes or CDC data base is 101 tells you that changes are written to a",
    "start": "1878160",
    "end": "1884490"
  },
  {
    "text": "database log all we are doing is reading those changes from the database log through the native API and that's why we",
    "start": "1884490",
    "end": "1892290"
  },
  {
    "text": "don't support really old versions of database engines because those api's were not available and it's also",
    "start": "1892290",
    "end": "1899340"
  },
  {
    "text": "important to note that because we do this there's a great benefit and that is there is no client-side install required",
    "start": "1899340",
    "end": "1905330"
  },
  {
    "text": "DMS just makes the connection out over the wire and bring us the data in it does require a little bit of",
    "start": "1905330",
    "end": "1911370"
  },
  {
    "text": "configuration on your part which you may have already done you'll have to enable that logging on the source database",
    "start": "1911370",
    "end": "1916740"
  },
  {
    "text": "engine if you want to use the replication Oracle that's supplemental logging Postgres you got to have access",
    "start": "1916740",
    "end": "1922020"
  },
  {
    "text": "to the wall log just the usual configurations there so what else can",
    "start": "1922020",
    "end": "1927840"
  },
  {
    "start": "1926000",
    "end": "1961000"
  },
  {
    "text": "you do consolidation is a great one DMS doesn't care you can go from three",
    "start": "1927840",
    "end": "1933330"
  },
  {
    "text": "different engines take bits and pieces of the data and funnel it all into one target it's a great thing to do for",
    "start": "1933330",
    "end": "1938400"
  },
  {
    "text": "analysis if you've got customer lists in a CRM system and then a Sales System would having you want to get a cohesive",
    "start": "1938400",
    "end": "1943890"
  },
  {
    "text": "list of all your customers you can use DMS to pull all that information together and put in one spot you can of",
    "start": "1943890",
    "end": "1949230"
  },
  {
    "text": "course also do the reverse if you have a monolithic ERP system that has just huge and you want to split out into micro",
    "start": "1949230",
    "end": "1955350"
  },
  {
    "text": "services you can use DMS to take that information and split it into multiple different targets now I'm kind of done",
    "start": "1955350",
    "end": "1962940"
  },
  {
    "start": "1961000",
    "end": "2423000"
  },
  {
    "text": "with the slides so I'm gonna get on to a demo I have to apparently start this from the computer usually I'd run these",
    "start": "1962940",
    "end": "1970230"
  },
  {
    "text": "live but for the sake of some time-saving we're just gonna do a video and I'm gonna try to talk to it all",
    "start": "1970230",
    "end": "1975410"
  },
  {
    "text": "right so the theory here is we are going to move some data from Oracle to",
    "start": "1975410",
    "end": "1980460"
  },
  {
    "text": "Postgres so right here on the screen I'm just showing you an Oracle demo database we have available a bunch of tables in",
    "start": "1980460",
    "end": "1987750"
  },
  {
    "text": "here procedures views what-have-you this sample is actually available on github if you want to go download it and",
    "start": "1987750",
    "end": "1993420"
  },
  {
    "text": "try it yourself you can see the sports ticket in the system we've got a bunch of information in there right now",
    "start": "1993420",
    "end": "1999179"
  },
  {
    "text": "just going through showing you a bit of the tables and conversely I'll go and connect to my Postgres sequel target and",
    "start": "1999179",
    "end": "2005160"
  },
  {
    "text": "you can see in here there's is really nothing here it's a empty database with a couple of default schemas",
    "start": "2005160",
    "end": "2011179"
  },
  {
    "text": "so the first step like I mentioned is to go and convert your schema so I'm going to launch the schema conversion tool",
    "start": "2011179",
    "end": "2017610"
  },
  {
    "text": "which as I mentioned is a free download you can go try today and what you do is",
    "start": "2017610",
    "end": "2022980"
  },
  {
    "text": "you make a connection to your database so I'll just go do that",
    "start": "2022980",
    "end": "2028340"
  },
  {
    "text": "define your connection string in this case my database is already s but remember this can be done on premise no",
    "start": "2028340",
    "end": "2033990"
  },
  {
    "text": "problem at all I'm gonna make a connection and it's gonna have a look and get a list of all the schemas that are in the database I'm just gonna work",
    "start": "2033990",
    "end": "2040620"
  },
  {
    "text": "with my sample schema for the moment so once I do that and then does a bit more analysis looks through all of the",
    "start": "2040620",
    "end": "2046770"
  },
  {
    "text": "objects in the database it figures out which ones can be converted easily and which ones are going to take a little",
    "start": "2046770",
    "end": "2052050"
  },
  {
    "text": "bit more work so once it's done doing that processing it's gonna pull up what we call the assessment report and this",
    "start": "2052050",
    "end": "2058350"
  },
  {
    "text": "assessment report gives you a snapshot view as to the work effort it's gonna take you if you choose to switch",
    "start": "2058350",
    "end": "2064020"
  },
  {
    "text": "database engines so it should come up in",
    "start": "2064020",
    "end": "2069060"
  },
  {
    "text": "a second here and one of the neat things about it is it gives you a good executive summary which executives like",
    "start": "2069060",
    "end": "2075000"
  },
  {
    "text": "but then of course we drill into some more detail you can export it to PDF or CSV and the first half the report talks",
    "start": "2075000",
    "end": "2081510"
  },
  {
    "text": "about conversions to my sequel based engines so you can see here we're gonna",
    "start": "2081510",
    "end": "2086608"
  },
  {
    "text": "have most objects convert but we've got some got some work to do actually in the raid area you move down to the second",
    "start": "2086609",
    "end": "2092010"
  },
  {
    "text": "half of the report and it shows you how things are going to convert if you went to a Postgres engine now just to be",
    "start": "2092010",
    "end": "2097590"
  },
  {
    "text": "perfectly clear this varies from database to database in this case this source database is going to convert",
    "start": "2097590",
    "end": "2103410"
  },
  {
    "text": "better to Postgres than it will to my sequel which is why I'm selecting it here but it's certainly not always the",
    "start": "2103410",
    "end": "2108900"
  },
  {
    "text": "case I should mention that SCT is going to work fine with the new arora version but this screenshot was done before that",
    "start": "2108900",
    "end": "2116280"
  },
  {
    "text": "was released so I've selected Postgres I'm a target and you can see it's appeared on the right hand side so the",
    "start": "2116280",
    "end": "2122640"
  },
  {
    "text": "left is the source the right is the target and it's just going and readying",
    "start": "2122640",
    "end": "2128369"
  },
  {
    "text": "the project it's going to flip across here - just waiting - Mattel I switch - do",
    "start": "2128369",
    "end": "2135550"
  },
  {
    "text": "you think we wanted to do just that one scheme I was talking about DMS sample and then the next thing that we're gonna",
    "start": "2135550",
    "end": "2140560"
  },
  {
    "text": "do is go through you can see we've got all the same tables before but I like to switch back to the assessment report",
    "start": "2140560",
    "end": "2145570"
  },
  {
    "text": "view because one of the neat things about it is it has this action items list this action item list shows you",
    "start": "2145570",
    "end": "2151000"
  },
  {
    "text": "here are all the things that I couldn't convert automatically and it sort of step-by-step you can go through and",
    "start": "2151000",
    "end": "2156070"
  },
  {
    "text": "click on them and see what it is you're going to have to do to get it to work so you can see it's highlighted what hasn't",
    "start": "2156070",
    "end": "2161920"
  },
  {
    "text": "worked with that conversion and what's good about it is when you highlight it over it it gives you a link tells you",
    "start": "2161920",
    "end": "2167109"
  },
  {
    "text": "what's wrong and a lot of times it will direct you to a URL saying this is where you can go to look up some more",
    "start": "2167109",
    "end": "2172210"
  },
  {
    "text": "information on it I've just selected all the tables I've converted them and you can see it's moved it over to the",
    "start": "2172210",
    "end": "2177670"
  },
  {
    "text": "right-hand side you don't have to convert every last object in your database you can obviously point and",
    "start": "2177670",
    "end": "2182710"
  },
  {
    "text": "click on the tree and choose only the objects you wish to convert so I've converted my tables my views but if I",
    "start": "2182710",
    "end": "2189880"
  },
  {
    "text": "switch back here this is again going back to that work item list what I can do is I can convert this particular",
    "start": "2189880",
    "end": "2196119"
  },
  {
    "text": "procedure that's having issues so on the left at the bottom is what it is on the right is now SC T's best effort",
    "start": "2196119",
    "end": "2204760"
  },
  {
    "text": "at converting but you can see it's highlighted in comments what hasn't worked and what you should go to go do",
    "start": "2204760",
    "end": "2210970"
  },
  {
    "text": "or investigate to try to make it work now obviously I don't want to apply that dysfunctional procedure to my database",
    "start": "2210970",
    "end": "2217240"
  },
  {
    "text": "right now so I'm just going to apply all of my tables and my views to the database you can apply bits and pieces",
    "start": "2217240",
    "end": "2223060"
  },
  {
    "text": "come back later continue working on it but for the sake of the demo of course I can't go through every last action item",
    "start": "2223060",
    "end": "2228609"
  },
  {
    "text": "here so I've now gone I've applied my tables and I've applied my views to my target database if I switch back to my",
    "start": "2228609",
    "end": "2235510"
  },
  {
    "text": "database query tool and do a refresh you can now see those objects have appeared in my Postgres sequel target so there",
    "start": "2235510",
    "end": "2243970"
  },
  {
    "text": "they all are there's the tables and the associated views of course if I go in and look at",
    "start": "2243970",
    "end": "2250480"
  },
  {
    "text": "the tables you're gonna see there's no data in there because remember step one move your schema step to move your data",
    "start": "2250480",
    "end": "2256680"
  },
  {
    "text": "empty tables but at least they're there so the next step is to go and move the",
    "start": "2256680",
    "end": "2261790"
  },
  {
    "text": "data with DMS so I'm going to switch over to the AWS console actually thought about this",
    "start": "2261790",
    "end": "2267010"
  },
  {
    "text": "they're very closely linked as SCT knows all about the schema knows what the",
    "start": "2267010",
    "end": "2272380"
  },
  {
    "text": "source is and knows what the target is so I'm actually gonna export a script that will tell DMS what it needs to do",
    "start": "2272380",
    "end": "2279040"
  },
  {
    "text": "to move the data from the source to the target that's what this is up on the screen it's just basic JSON you can see",
    "start": "2279040",
    "end": "2285069"
  },
  {
    "text": "that we are taking all the tables and we're also converting the schema and the tables to lowercase that's because by",
    "start": "2285069",
    "end": "2291760"
  },
  {
    "text": "default Postgres has lowercase objects Oracle has uppercase objects so we're handling that as part of the conversion",
    "start": "2291760",
    "end": "2298470"
  },
  {
    "text": "so when I go to DMS on the console I'm just showing you here I've got a replication server up and running",
    "start": "2298470",
    "end": "2304180"
  },
  {
    "text": "already and I've also pre created endpoints like I mentioned earlier there are a lot of",
    "start": "2304180",
    "end": "2309670"
  },
  {
    "text": "different endpoints you can have as many as you want or as few as you want I've just pre created them to save time I'm",
    "start": "2309670",
    "end": "2315670"
  },
  {
    "text": "gonna go and create a task to move the data give the task a name as I said it",
    "start": "2315670",
    "end": "2321190"
  },
  {
    "text": "was a sports demo database I'm gonna choose my Oracle source end point and my Postgres target end point I want to",
    "start": "2321190",
    "end": "2328960"
  },
  {
    "text": "migrate the existing data but I could also do changes if I wanted and then I always enable logging it's not on by",
    "start": "2328960",
    "end": "2335650"
  },
  {
    "text": "default because of course CloudWatch logs does cost some money but it's just good to know in case something is going wrong I can manually specify what it is",
    "start": "2335650",
    "end": "2342970"
  },
  {
    "text": "I want to move like I could go here and say look I want to move all tables or some tables I can do custom rules but",
    "start": "2342970",
    "end": "2349150"
  },
  {
    "text": "instead I'm gonna go to the JSON tab which is you know admittedly a little bit more technically advanced but I'm",
    "start": "2349150",
    "end": "2354790"
  },
  {
    "text": "gonna copy that JSON that SCT generated and just paste it in so the same jason",
    "start": "2354790",
    "end": "2360609"
  },
  {
    "text": "is in there with those transformations I mentioned to move things to lowercase but I'm gonna edit it just to save a bit",
    "start": "2360609",
    "end": "2366190"
  },
  {
    "text": "of time and I want to bring on only a subset of tables over so this wild card the % I'm just gonna add NFL in front of",
    "start": "2366190",
    "end": "2373720"
  },
  {
    "text": "it and bring over only tables let's start with NFL I kick it off there's the",
    "start": "2373720",
    "end": "2378790"
  },
  {
    "text": "task in the tasks tab of DMS I've condensed the time a little bit on this",
    "start": "2378790",
    "end": "2384339"
  },
  {
    "text": "too to get through things and as information starts moving across you can",
    "start": "2384339",
    "end": "2389859"
  },
  {
    "text": "monitor the progress up there in the summary as well as down in the details tab at the bottom the task runs it's",
    "start": "2389859",
    "end": "2397329"
  },
  {
    "text": "moving the data it's transforming things so it knows look the tables uppercase in Oracle it's",
    "start": "2397329",
    "end": "2403160"
  },
  {
    "text": "lowercase in Postgres it goes across and now if I run that same query you can see",
    "start": "2403160",
    "end": "2408170"
  },
  {
    "text": "the data is there in the Postgres database a couple of NFL tables we're looking at here data in both of them but",
    "start": "2408170",
    "end": "2416150"
  },
  {
    "text": "if I go and I look in a table the person table in this case you'll see there's no data present because we only wanted to",
    "start": "2416150",
    "end": "2421310"
  },
  {
    "text": "move the NFL tables and that pretty much wraps up what I was going to show for a",
    "start": "2421310",
    "end": "2426590"
  },
  {
    "text": "demo today you've heard of us from AWS talk about the service but probably the",
    "start": "2426590",
    "end": "2431630"
  },
  {
    "text": "best thing of all is to hear from a real customer that has done one of these migrations and can really tell you about",
    "start": "2431630",
    "end": "2436730"
  },
  {
    "text": "how it works so I'll hand it over to Aaron thanks John I'm Mary curse I'm a senior director at",
    "start": "2436730",
    "end": "2444860"
  },
  {
    "text": "FINRA on the enterprise data platforms team FINRA is the Financial Industry Regulatory Authority we oversee all",
    "start": "2444860",
    "end": "2453380"
  },
  {
    "text": "security firms doing business with the public in the United States we get billions of trades quotes and order data",
    "start": "2453380",
    "end": "2460880"
  },
  {
    "text": "daily upwards of 75 billion events per day as well as forms filings membership",
    "start": "2460880",
    "end": "2467030"
  },
  {
    "text": "information and financial information from our member firms in support of our mission for investor protection and",
    "start": "2467030",
    "end": "2473720"
  },
  {
    "text": "market integrity about three years ago",
    "start": "2473720",
    "end": "2480020"
  },
  {
    "start": "2477000",
    "end": "2652000"
  },
  {
    "text": "finra started it's moved to the cloud we started with our market reg portfolio which was primarily big data they lived",
    "start": "2480020",
    "end": "2487880"
  },
  {
    "text": "on big data appliances and we saw an opportunity in the cloud to improve our",
    "start": "2487880",
    "end": "2493670"
  },
  {
    "text": "ability to scale both now and in the future as well as lower our cost and improve our agility we wanted to move",
    "start": "2493670",
    "end": "2501710"
  },
  {
    "text": "off of our proprietary hardware software onto a more open source platform so we",
    "start": "2501710",
    "end": "2508520"
  },
  {
    "text": "also wanted at the same time to take that opportunity to automate and minimize the amount of effort each team",
    "start": "2508520",
    "end": "2514910"
  },
  {
    "text": "was spending on their core operational needs as a financial regulator it was important that we not sacrifice in the",
    "start": "2514910",
    "end": "2521930"
  },
  {
    "text": "areas of data security and operational awareness we needed to understand what",
    "start": "2521930",
    "end": "2527810"
  },
  {
    "text": "was happening when there are problems happening so that we could troubleshoot the issue fast forward to",
    "start": "2527810",
    "end": "2534789"
  },
  {
    "text": "about middle of this year June July we wrapped that migration up and realize many of those benefits the stack at this",
    "start": "2534789",
    "end": "2542979"
  },
  {
    "text": "point looks pretty much like this that we continue to enhance it we're using Amazon s3 for our data Lake we do the",
    "start": "2542979",
    "end": "2550900"
  },
  {
    "text": "ingestion and processing primarily in Amazon EMR that we're finding more and more use cases for lambda we're",
    "start": "2550900",
    "end": "2556959"
  },
  {
    "text": "leveraging a space for our oats processing and stock market order flow reconstruction as part of that cloud",
    "start": "2556959",
    "end": "2563529"
  },
  {
    "text": "migration we built out and the open sourced herd which is our unified data catalog which keeps track of everything",
    "start": "2563529",
    "end": "2570369"
  },
  {
    "text": "in our data Lake and we use Splunk for monitoring and processing monitoring processing and cost well before that",
    "start": "2570369",
    "end": "2580809"
  },
  {
    "text": "migration completed we started to look ahead at other the next set of applications that we're gonna have to",
    "start": "2580809",
    "end": "2586209"
  },
  {
    "text": "migrate those applications were primarily relational databases online",
    "start": "2586209",
    "end": "2591539"
  },
  {
    "text": "transactional type systems there's more than a hundred of those systems and we",
    "start": "2591539",
    "end": "2596709"
  },
  {
    "text": "expected that they would continue to need to leverage relational databases",
    "start": "2596709",
    "end": "2601920"
  },
  {
    "text": "based on the positive experience we had leveraging the Amazon EMR service we",
    "start": "2601920",
    "end": "2608229"
  },
  {
    "text": "immediately started to take a look at RDS its support for multi AZ read",
    "start": "2608229",
    "end": "2613420"
  },
  {
    "text": "replicas and the security as well as the ability to scale instance types and storage were extremely attractive to us",
    "start": "2613420",
    "end": "2620259"
  },
  {
    "text": "in terms of the engine we needed to take a look because we're a very sequel",
    "start": "2620259",
    "end": "2625660"
  },
  {
    "text": "centric shop we evaluated what areas were important to us and those areas",
    "start": "2625660",
    "end": "2630759"
  },
  {
    "text": "were more advanced sequel like common table expressions and analytic functions as well as strong procedural language",
    "start": "2630759",
    "end": "2637569"
  },
  {
    "text": "support we needed replication capabilities security at rest and in transit and they needed to perform well",
    "start": "2637569",
    "end": "2644650"
  },
  {
    "text": "even in the more complicated sequel so for us the answer was Postgres so for me",
    "start": "2644650",
    "end": "2653920"
  },
  {
    "start": "2652000",
    "end": "2723000"
  },
  {
    "text": "one of the areas I needed to focus on was actually our data sharing hub these applications rely heavily on this hub",
    "start": "2653920",
    "end": "2660640"
  },
  {
    "text": "the hub exists in our on-premise and and today and was built in response to a",
    "start": "2660640",
    "end": "2666530"
  },
  {
    "text": "growing problem we were having with point-to-point dependencies between databases those point the points were",
    "start": "2666530",
    "end": "2672470"
  },
  {
    "text": "creating challenges where the consuming application would create performance",
    "start": "2672470",
    "end": "2678200"
  },
  {
    "text": "impacts on the providing a database as well there was a potential for a coupling of SaaS we're consuming",
    "start": "2678200",
    "end": "2685460"
  },
  {
    "text": "application may have a much tighter SLA than the database that was pulling from in which case it would automatically",
    "start": "2685460",
    "end": "2691670"
  },
  {
    "text": "elevate the SLA of that source database this is primarily done using Oracle",
    "start": "2691670",
    "end": "2697970"
  },
  {
    "text": "materialized views so we moved to this hub architecture which you can kind of see I guess to your left where",
    "start": "2697970",
    "end": "2706520"
  },
  {
    "text": "applications that one expose certain data sets to other applications do so",
    "start": "2706520",
    "end": "2712970"
  },
  {
    "text": "through replicating a materialized view into the hub and consuming applications are able to leverage that data by a",
    "start": "2712970",
    "end": "2719030"
  },
  {
    "text": "materialized view replication out so the",
    "start": "2719030",
    "end": "2724700"
  },
  {
    "start": "2723000",
    "end": "2775000"
  },
  {
    "text": "next step was to figure out what this would look like an AWS obviously as I already mentioned the hub would be",
    "start": "2724700",
    "end": "2731270"
  },
  {
    "text": "sitting on an RDS post per sequel database it would leverage the multi a-z support that comes out of the box as",
    "start": "2731270",
    "end": "2738410"
  },
  {
    "text": "well as kms encryption for our data security needs at rest and enforce",
    "start": "2738410",
    "end": "2743840"
  },
  {
    "text": "enforce SSL connections we saw good potential and offloading some of the",
    "start": "2743840",
    "end": "2749960"
  },
  {
    "text": "REIT Rafic using the read replicas and we played around with some of the early",
    "start": "2749960",
    "end": "2755840"
  },
  {
    "text": "incarnations of the AWS database migration service I think it was the database migration tool and and saw some",
    "start": "2755840",
    "end": "2762950"
  },
  {
    "text": "benefits there it supported the database engines we needed to support but most importantly the database migration",
    "start": "2762950",
    "end": "2769550"
  },
  {
    "text": "service exposed an API which will allow us to automate this so the problem",
    "start": "2769550",
    "end": "2776930"
  },
  {
    "start": "2775000",
    "end": "2798000"
  },
  {
    "text": "solved not quite we can't expect all 100 of databases to move in one weekend we",
    "start": "2776930",
    "end": "2783050"
  },
  {
    "text": "can't expect that every time one database moves all of its dependent applications make either code changes or",
    "start": "2783050",
    "end": "2790130"
  },
  {
    "text": "move along with them so we had to find some way to minimize the impact for each of these application",
    "start": "2790130",
    "end": "2795710"
  },
  {
    "text": "databases to move to the cloud so it got a little bit more complicated in the end",
    "start": "2795710",
    "end": "2803599"
  },
  {
    "start": "2798000",
    "end": "2929000"
  },
  {
    "text": "we introduced an intermediate state where we actually kept the on premise help database up and available to",
    "start": "2803599",
    "end": "2810920"
  },
  {
    "text": "support the replication needs of those applications still residing in our on-premise data center and are standing",
    "start": "2810920",
    "end": "2817730"
  },
  {
    "text": "up our AWS implementation leveraging DMS so those applications that did move to",
    "start": "2817730",
    "end": "2823640"
  },
  {
    "text": "the cloud would be able to leverage the new hub in AWS but we would expand the",
    "start": "2823640",
    "end": "2829119"
  },
  {
    "text": "expectations of DMS we would actually use it to replicate the data that was",
    "start": "2829119",
    "end": "2834440"
  },
  {
    "text": "produced in the cloud back down to our on-premise hub as well as replicate the",
    "start": "2834440",
    "end": "2839780"
  },
  {
    "text": "data from our hub from the providers on inside of our data center up to the",
    "start": "2839780",
    "end": "2845089"
  },
  {
    "text": "cloud this would minimize the amount of impact each application would feel as",
    "start": "2845089",
    "end": "2850490"
  },
  {
    "text": "their perhaps upstream data source moved to the cloud they would continue to be able to replicate the data out of the",
    "start": "2850490",
    "end": "2855980"
  },
  {
    "text": "on-premise hub even though that their partners database was no longer on-premise so we decided to wrap that",
    "start": "2855980",
    "end": "2868160"
  },
  {
    "text": "DMS API because our overall service needed to support a few additional items we wanted to be able to support user",
    "start": "2868160",
    "end": "2875180"
  },
  {
    "text": "grants and administration on those hub objects that the producer applications created the producers own the entire",
    "start": "2875180",
    "end": "2882770"
  },
  {
    "text": "what we call data interface there the response were defining what they want to expose as well as who should have access",
    "start": "2882770",
    "end": "2889250"
  },
  {
    "text": "to it we also wanted to give them the ability to create secondary indexes and views on top of the tables they exposed",
    "start": "2889250",
    "end": "2896150"
  },
  {
    "text": "as well we wanted to capture metadata and register these data interfaces and",
    "start": "2896150",
    "end": "2901940"
  },
  {
    "text": "that herd unified data catalog that we mentioned before in addition automation lights-out",
    "start": "2901940",
    "end": "2908690"
  },
  {
    "text": "operation is a first-class citizen at FINRA so we wanted to incorporate the process in their overall SDLC we wanted",
    "start": "2908690",
    "end": "2915320"
  },
  {
    "text": "to have them actually register the changes that they were going to be making in their replication tasks inside",
    "start": "2915320",
    "end": "2922820"
  },
  {
    "text": "their code repositories and tested along with the code that they were doing with each change",
    "start": "2922820",
    "end": "2929500"
  },
  {
    "start": "2929000",
    "end": "3082000"
  },
  {
    "text": "so some lessons we learned along the way limited lab setting versus full lob had",
    "start": "2929829",
    "end": "2936859"
  },
  {
    "text": "no real benefit having no real benefit above 64 K we found it to not be terribly beneficial for us that that",
    "start": "2936859",
    "end": "2944240"
  },
  {
    "text": "setting is at the task level which is also the transactional boundary there applies to all lobs within that task and",
    "start": "2944240",
    "end": "2951170"
  },
  {
    "text": "you have to set a maximum and if the maximum is exceeded it would truncate the data so for us we opted not to use",
    "start": "2951170",
    "end": "2957230"
  },
  {
    "text": "that instance size matters that's not news but I think in a lot of times people look at instance type and their",
    "start": "2957230",
    "end": "2963800"
  },
  {
    "text": "first thought is look at CPU and RAM but keep in mind what DMS is doing it's",
    "start": "2963800",
    "end": "2968900"
  },
  {
    "text": "moving data for us the network was a big factor that definitely made us move up",
    "start": "2968900",
    "end": "2974300"
  },
  {
    "text": "in instance type case sensitivity in terms of table names if you have mixed",
    "start": "2974300",
    "end": "2980690"
  },
  {
    "text": "case and that case difference is the only thing that's differentiating two tables those can be challenging pay",
    "start": "2980690",
    "end": "2987109"
  },
  {
    "text": "attention to those single byte character to utf-8 for us the best approach was to",
    "start": "2987109",
    "end": "2994760"
  },
  {
    "text": "actually pre create these tables you could use sct'd do that if you leave it",
    "start": "2994760",
    "end": "2999950"
  },
  {
    "text": "to DMS it will widen the columns to account for the potential multi bytes",
    "start": "2999950",
    "end": "3005880"
  },
  {
    "text": "DMS also exposes error behavior settings which allows you to have more granular control about the state of a task when",
    "start": "3005880",
    "end": "3012880"
  },
  {
    "text": "it encounters certain conditions we found those to be very important for us to be able to control and have TAS stop",
    "start": "3012880",
    "end": "3022000"
  },
  {
    "text": "when certain conditions are encountered be aware of your cloud watch retention",
    "start": "3022000",
    "end": "3027760"
  },
  {
    "text": "by default I believe that is set to forever which can lead to filling up",
    "start": "3027760",
    "end": "3034030"
  },
  {
    "text": "your replication instant storage which leads to the next one pay attention to",
    "start": "3034030",
    "end": "3039130"
  },
  {
    "text": "your DMS replication instant storage if that fills up your tasks tend to become",
    "start": "3039130",
    "end": "3044349"
  },
  {
    "text": "unresponsive and you're probably going to need to manually intervene to try and get things right again",
    "start": "3044349",
    "end": "3050549"
  },
  {
    "text": "also for us some of the documented scenarios that DMS doesn't support that they definitely call this out can be",
    "start": "3050640",
    "end": "3057849"
  },
  {
    "text": "very hard to detect and two of them in particular are updates to",
    "start": "3057849",
    "end": "3062890"
  },
  {
    "text": "primary teas which is something that you don't expect to happen but if it does happen you want to know so that your rep",
    "start": "3062890",
    "end": "3069910"
  },
  {
    "text": "because your replication may break you'll start to diverge from the source in addition table truncation on the",
    "start": "3069910",
    "end": "3075790"
  },
  {
    "text": "source side can also lead to those same type of scenarios features we want to",
    "start": "3075790",
    "end": "3084460"
  },
  {
    "start": "3082000",
    "end": "3370000"
  },
  {
    "text": "see for us we'd like to see better support for scaling of DMS what we mean",
    "start": "3084460",
    "end": "3089830"
  },
  {
    "text": "by that is really more horizontal scaling we would like the ability if we",
    "start": "3089830",
    "end": "3094960"
  },
  {
    "text": "get to a point where we see replication instants struggling to keep up we'd like",
    "start": "3094960",
    "end": "3101530"
  },
  {
    "text": "to be able to actually move a replication task to a new DMS instance and not have to have it start from",
    "start": "3101530",
    "end": "3109300"
  },
  {
    "text": "scratch basically know where it was in terms of CDC processing we'd like to",
    "start": "3109300",
    "end": "3115690"
  },
  {
    "text": "have support for materialized views and Postgres sequel as a source we encourage applications to not necessarily directly",
    "start": "3115690",
    "end": "3122710"
  },
  {
    "text": "expose their internal schema and this would be a good way for the those applications to encapsulate what their",
    "start": "3122710",
    "end": "3129670"
  },
  {
    "text": "internal scheme is versus what they're sharing with other applications we'd",
    "start": "3129670",
    "end": "3136089"
  },
  {
    "text": "also like to see some DMS events for subscription meaning in in the cases where there's a multi AZ failover or a",
    "start": "3136089",
    "end": "3142720"
  },
  {
    "text": "task failure have some way to subscribe to those events and already s read",
    "start": "3142720",
    "end": "3151030"
  },
  {
    "text": "replica multi a-z ELB support so this one's probably a little different than",
    "start": "3151030",
    "end": "3156910"
  },
  {
    "text": "most been thinking of we would like to leverage those read replicas to support offload some of that read traffic but",
    "start": "3156910",
    "end": "3163180"
  },
  {
    "text": "what we would like to be able to do is not have to worry about one of them falling over and then having to point a",
    "start": "3163180",
    "end": "3169000"
  },
  {
    "text": "bunch of apt client applications to a different source as well as potentially",
    "start": "3169000",
    "end": "3174070"
  },
  {
    "text": "seeing a different state of the data and then I think the database performance management sounds like it's on its way",
    "start": "3174070",
    "end": "3180250"
  },
  {
    "text": "I've saw the earlier presentation on performance insights and that's along the lines of what we were hoping to see",
    "start": "3180250",
    "end": "3187089"
  },
  {
    "text": "we've historically been a Oracle shop so we're used to OEM and toe",
    "start": "3187089",
    "end": "3192100"
  },
  {
    "text": "to be able to analyze what's been happening over time trying to troubleshoot particular performance",
    "start": "3192100",
    "end": "3197830"
  },
  {
    "text": "issues what kind of plans were used what times and so on so that's exactly the kind of thing we're looking to see and",
    "start": "3197830",
    "end": "3206560"
  },
  {
    "text": "with the rollout of Amazon or a poster sequel compatible edition we're",
    "start": "3206560",
    "end": "3212170"
  },
  {
    "text": "definitely interested in looking at this for this hub architecture why we're interested is our hub's workload is read",
    "start": "3212170",
    "end": "3220600"
  },
  {
    "text": "heavy for every data table that is replicated into the hub it's probably",
    "start": "3220600",
    "end": "3225880"
  },
  {
    "text": "replicated out more than once so we see a good opportunity there to potentially",
    "start": "3225880",
    "end": "3230980"
  },
  {
    "text": "use those closer in sync read replicas though I know I don't",
    "start": "3230980",
    "end": "3237790"
  },
  {
    "text": "think out of the gate it will support it ideally we'd love to see those read replicas be supported as an endpoint or",
    "start": "3237790",
    "end": "3245110"
  },
  {
    "text": "for a source on the DMS side the faster recovery and survival cache I think they",
    "start": "3245110",
    "end": "3251890"
  },
  {
    "text": "went well into those those are definitely something that we would need",
    "start": "3251890",
    "end": "3257890"
  },
  {
    "text": "for this application it's a it's very critical since it's at the center of many of our other database applications",
    "start": "3257890",
    "end": "3263110"
  },
  {
    "text": "and finally the fault injection queries I don't know that necessarily if they'll be out day one and the Postgres",
    "start": "3263110",
    "end": "3268360"
  },
  {
    "text": "compatible Edition but the my sequel Edition has these where you're able to simulate certain failures that otherwise",
    "start": "3268360",
    "end": "3274720"
  },
  {
    "text": "since it's a managed service you really I don't know how to do it they will be supported and I think that's that's all",
    "start": "3274720",
    "end": "3286330"
  },
  {
    "text": "I had don't forget to turn in your course eval thinking about a few minutes left for",
    "start": "3286330",
    "end": "3292690"
  },
  {
    "text": "questions and just one thing I just feature lists so you know one of the",
    "start": "3292690",
    "end": "3298210"
  },
  {
    "text": "point in the survival the failure scenarios so in Aurora you know you through sequels so we",
    "start": "3298210",
    "end": "3304630"
  },
  {
    "text": "mentioned that Aurora is fully conversant compatible with my sequel and Postgres but some additions we did put",
    "start": "3304630",
    "end": "3310210"
  },
  {
    "text": "into it is you can actually through sequel simulate an availability zone failure a disk failure or storage node",
    "start": "3310210",
    "end": "3317800"
  },
  {
    "text": "failure all through sequel which kind of helps for testing the application and that's because it's actually a feature",
    "start": "3317800",
    "end": "3323110"
  },
  {
    "text": "of Aurora storage the our kites crest would support that that's good that's a kind of okay we",
    "start": "3323110",
    "end": "3328860"
  },
  {
    "text": "actually did rehearse beforehand and then you know with Aurora you can have up to 15 replicas so 15 replicas that",
    "start": "3328860",
    "end": "3335190"
  },
  {
    "text": "I'll read off the same storage and so in my sequel you know it's not it's not a",
    "start": "3335190",
    "end": "3340770"
  },
  {
    "text": "typical to see like seconds to minutes replication lag but because the replicas",
    "start": "3340770",
    "end": "3345780"
  },
  {
    "text": "are all reading off the same storage we typically see less than 10 millisecond replication lag and that'll equal would",
    "start": "3345780",
    "end": "3352500"
  },
  {
    "text": "be true for Postgres and a new feature that came out for my sequel which will",
    "start": "3352500",
    "end": "3357660"
  },
  {
    "text": "take a little bit of time for the Postgres side but we do have a single end point for all your 15 up to 15",
    "start": "3357660",
    "end": "3363980"
  },
  {
    "text": "readers and we load balance between all them on the my sequel side so",
    "start": "3363980",
    "end": "3371780"
  }
]