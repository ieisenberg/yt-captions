[
  {
    "start": "0",
    "end": "55000"
  },
  {
    "text": "uh I'm uh Roy rport and I'm Chris sandon uh we're here to talk about realtime",
    "start": "1199",
    "end": "7080"
  },
  {
    "text": "analytics and uh how you can use them to uh make better self-healing ecosystems",
    "start": "7080",
    "end": "12160"
  },
  {
    "text": "so uh let's start with maybe introductions um I've been at Netflix for a little more than six years I",
    "start": "12160",
    "end": "17800"
  },
  {
    "text": "actually joined back when we had an IT group responsible for a whole bunch of data uh Center systems um so I remember",
    "start": "17800",
    "end": "23519"
  },
  {
    "text": "those days um not particularly fondly uh these days I work in a group called Insight engineering we are a software",
    "start": "23519",
    "end": "29720"
  },
  {
    "text": "developer group uh part of Josh evans's operations Engineering Group if you attended his talk um responsible for",
    "start": "29720",
    "end": "35719"
  },
  {
    "text": "real-time operational Insight systems uh and I am Chris sandon I'm a",
    "start": "35719",
    "end": "41000"
  },
  {
    "text": "senior analytics engineer on the realtime analytics team which is part of Roy's Insight engineering team I've been",
    "start": "41000",
    "end": "48039"
  },
  {
    "text": "at Netflix for roughly two and a half years so hi okay good so um to start",
    "start": "48039",
    "end": "53359"
  },
  {
    "text": "this off to set it up I want to talk a little about uh the prerequisites for uh getting value out of this uh talk so so",
    "start": "53359",
    "end": "60160"
  },
  {
    "start": "55000",
    "end": "55000"
  },
  {
    "text": "let's start with this uh who here has a PHD actually maybe let's go as low as a master's in machine learning raise your",
    "start": "60160",
    "end": "69119"
  },
  {
    "text": "hands okay so you might be bored by this presentation and it's okay if you leave right",
    "start": "71280",
    "end": "76960"
  },
  {
    "text": "now everybody else you're in the right room so our goal here is to go over some",
    "start": "76960",
    "end": "84079"
  },
  {
    "text": "algorithms that people like well people more like me I guess than than Chris",
    "start": "84079",
    "end": "89200"
  },
  {
    "text": "people who are respons for operational systems can use to uh build uh self-healing ecosystems so we're going",
    "start": "89200",
    "end": "95119"
  },
  {
    "text": "to talk a little about data science we we'll talk a little about algorithms but the real goal here is to expose you to",
    "start": "95119",
    "end": "100280"
  },
  {
    "text": "enough to actually be able to play with this and uh start experimenting in your own space and the way experimenting with",
    "start": "100280",
    "end": "105520"
  },
  {
    "text": "this uh ends up looking like I think is I think for most of us who have what I",
    "start": "105520",
    "end": "110920"
  },
  {
    "start": "109000",
    "end": "109000"
  },
  {
    "text": "might think of as reasonable Telemetry platforms you know you have this Telemetry platform I I use a heart both",
    "start": "110920",
    "end": "117479"
  },
  {
    "text": "because I love Telemetry platforms by the way but also because this is sort of like the heartbeat of your environment um and you probably have some sort of",
    "start": "117479",
    "end": "124119"
  },
  {
    "text": "orchestration system um and I'm saying that because frankly most of the people here I'm guessing either are working in",
    "start": "124119",
    "end": "130319"
  },
  {
    "text": "the cloud or are looking to work in the cloud and at minimum you have the AWS API and so the nice thing is even if",
    "start": "130319",
    "end": "136640"
  },
  {
    "text": "your Telemetry platform doesn't have real-time analytics built into it and don't feel bad ours doesn't either um",
    "start": "136640",
    "end": "143720"
  },
  {
    "text": "given the information that we're going to give you today you can go away and start thinking really as an RTA system",
    "start": "143720",
    "end": "150200"
  },
  {
    "text": "that is Standalone and then can basically get information from your Telemetry platform assuming your Telemetry platform has uh API access to",
    "start": "150200",
    "end": "157560"
  },
  {
    "text": "the data it holds um otherwise I would argue it's not particularly reasonable and then the RTA platform can make",
    "start": "157560",
    "end": "163720"
  },
  {
    "text": "decisions based on that Data Drive those decisions into your orchestration systems which can then make your",
    "start": "163720",
    "end": "168760"
  },
  {
    "text": "environment change for the better hopefully um and that actually drives better observation and you've got this",
    "start": "168760",
    "end": "174360"
  },
  {
    "text": "virtual cycle of data decision and observation so uh that's the way we're",
    "start": "174360",
    "end": "180360"
  },
  {
    "text": "hoping you can use the information we'll talk about today so why is this a good thing well I want to talk a little about",
    "start": "180360",
    "end": "187120"
  },
  {
    "start": "185000",
    "end": "185000"
  },
  {
    "text": "bad news um when I moved over from the it side to uh engineering at Netflix",
    "start": "187120",
    "end": "193080"
  },
  {
    "text": "back in 2011 and we were starting to figure out how do we know when there's a problem with the website um it started",
    "start": "193080",
    "end": "199159"
  },
  {
    "text": "with basically you know we'll just be told about it so you know it started",
    "start": "199159",
    "end": "204560"
  },
  {
    "text": "with in 2011 our customers are the best source of information we have for uh",
    "start": "204560",
    "end": "209760"
  },
  {
    "text": "whether or not we have a problem which is a a great way to start but it's kind of embarrassing because ideally we'd like to notice that there",
    "start": "209760",
    "end": "216120"
  },
  {
    "text": "are problems before our customers do and ideally maybe even fix them before our customers do so everybody starts with",
    "start": "216120",
    "end": "222120"
  },
  {
    "text": "gosh it's embarrassing when our customers tell us that there's a problem we should figure this out of ourselves so the next step is you go with",
    "start": "222120",
    "end": "228959"
  },
  {
    "text": "dashboards and everybody loves dashboards um dashboards I'm convinced are responsible for some of the biggest",
    "start": "228959",
    "end": "235319"
  },
  {
    "text": "explosions and sales of large TVs in the industry because everybody ends up putting all over the walls um here you",
    "start": "235319",
    "end": "242680"
  },
  {
    "text": "actually see two of the most important graphs probably at Netflix uh the top one is calls into our Customer Center um",
    "start": "242680",
    "end": "250239"
  },
  {
    "text": "basically minute over minute this is a 7day cycle black is last week red is this week the reason this is important",
    "start": "250239",
    "end": "256440"
  },
  {
    "text": "is because we don't actually sell anything um over the phone and that means that chances are when people call",
    "start": "256440",
    "end": "262320"
  },
  {
    "text": "us it's not to tell us that we're doing a great job chances are when they're calling us",
    "start": "262320",
    "end": "268960"
  },
  {
    "text": "it's probably Rel related to the bottom graph which we call SPS or starts per second which is actually the number of",
    "start": "268960",
    "end": "275199"
  },
  {
    "text": "times people hit play on their remotes to start watching something on Netflix again black is last week red is this",
    "start": "275199",
    "end": "282120"
  },
  {
    "text": "week SPS by the way is the metric we use to define availability so when we uh",
    "start": "282120",
    "end": "287479"
  },
  {
    "text": "look at how good our availability was for a given week we basically look at the difference between what we expected",
    "start": "287479",
    "end": "292680"
  },
  {
    "text": "SPS would be and what SPS was and look at that is essentially the downtime one",
    "start": "292680",
    "end": "297840"
  },
  {
    "text": "way to think about that is that essentially we measure our downtime in customer disappointments which makes this I think",
    "start": "297840",
    "end": "304800"
  },
  {
    "text": "Super relevant for us and kind of painful which is the way you should be thinking about your downtime so you've got graphs and that's great and you you",
    "start": "304800",
    "end": "311000"
  },
  {
    "text": "may even hire some people whose uh only purpose in life is to watch graphs and watch when they you know go wrong so",
    "start": "311000",
    "end": "317280"
  },
  {
    "text": "they can start hitting the big red button and get people you know notified but that's a miserable existence and",
    "start": "317280",
    "end": "322680"
  },
  {
    "text": "there's just too many graphs to look at so the next obvious step is you",
    "start": "322680",
    "end": "328639"
  },
  {
    "text": "know",
    "start": "328639",
    "end": "331639"
  },
  {
    "text": "alerting I'm sure all of us have gotten this kind of alert before",
    "start": "334120",
    "end": "339600"
  },
  {
    "text": "um and uh that's where a lot of us stay for a while and we stay here because to",
    "start": "340800",
    "end": "346600"
  },
  {
    "text": "be perfectly honest this is not a bad place to be you've got some visualization you've got some alerting",
    "start": "346600",
    "end": "351800"
  },
  {
    "text": "and frankly this is a pretty necessary uh way to deal with the world um it works it works pretty reasonably well",
    "start": "351800",
    "end": "358840"
  },
  {
    "text": "but it's not particularly uh sufficient as you grow larger or more",
    "start": "358840",
    "end": "364120"
  },
  {
    "text": "complex and then what happens well as it turns out for us",
    "start": "364120",
    "end": "369880"
  },
  {
    "start": "366000",
    "end": "366000"
  },
  {
    "text": "especially as we get bigger uh we end up with a whole bunch of kind of interesting problems um and what we end",
    "start": "369880",
    "end": "375400"
  },
  {
    "text": "up with is essentially complexity but not just complexity in one dimension but",
    "start": "375400",
    "end": "380479"
  },
  {
    "text": "a whole bunch of different dimensions and I want to talk a little about those Dimensions uh and I'm just going to throw some numbers at you uh",
    "start": "380479",
    "end": "388560"
  },
  {
    "text": "421,000 10 we have a system that actually logs every change in production",
    "start": "388560",
    "end": "394319"
  },
  {
    "text": "that we make uh and I looked in preparation for this uh talk at the number of changes we made in one of our",
    "start": "394319",
    "end": "400120"
  },
  {
    "text": "production regions over a week this was the number of changes um to put it in perspective for people who haven't yet",
    "start": "400120",
    "end": "406400"
  },
  {
    "text": "done the math in their heads it's about one production change every 1.1 1.2 seconds at that rate it's hard to keep",
    "start": "406400",
    "end": "413680"
  },
  {
    "text": "track of what your environment should look like and what it looks like uh when it performs well the other interesting problem that",
    "start": "413680",
    "end": "420360"
  },
  {
    "text": "we have is that our Telemetry volume is kind of silly and you can ask every",
    "start": "420360",
    "end": "426479"
  },
  {
    "text": "engineer in my team who's working on Telemetry platform and all the conversations has anybody here by the",
    "start": "426479",
    "end": "432199"
  },
  {
    "text": "way attended Andrew Park's pres presentation about Cloud efficiency and cost yeah so I'm I'm one of the problems",
    "start": "432199",
    "end": "439199"
  },
  {
    "text": "that Andrew has uh because my uh product costs a ton of money for Netflix and",
    "start": "439199",
    "end": "444960"
  },
  {
    "text": "when I say our Telemetry volume is silly let me put this in perspective in uh us east1 which is one",
    "start": "444960",
    "end": "450879"
  },
  {
    "text": "of our production regions we've seen upwards of about two billion different metrics being reported every",
    "start": "450879",
    "end": "455960"
  },
  {
    "text": "minute two billion is a lot two billion is really hard to visualize um so I'd like to actually change from two billion",
    "start": "455960",
    "end": "462400"
  },
  {
    "text": "which is kind of a large number to something that is a little more sort of think of it as a as a unit of measure",
    "start": "462400",
    "end": "468960"
  },
  {
    "text": "that is a little more relevant and intuitive to all of",
    "start": "468960",
    "end": "473800"
  },
  {
    "text": "us this is a kitten the average kitten",
    "start": "474520",
    "end": "480400"
  },
  {
    "text": "is 14 in long I bet you didn't think we' talk about this so if every one of our",
    "start": "480400",
    "end": "487800"
  },
  {
    "text": "metrics was a kitten and we had 2 billion 14 in",
    "start": "487800",
    "end": "492960"
  },
  {
    "text": "kittens what could we do with this well one thing we could do is we could",
    "start": "492960",
    "end": "499240"
  },
  {
    "text": "actually put them back to back and form a line from the earth to the moon and",
    "start": "499240",
    "end": "505080"
  },
  {
    "text": "then back now uh before you go trying this at home because we're hoping this is",
    "start": "505080",
    "end": "511479"
  },
  {
    "text": "applicable to most people I should know that if you're going to do that I urge you to actually equip your uh kittens",
    "start": "511479",
    "end": "519680"
  },
  {
    "text": "with laser pointers that work let's see this perfect good so it's",
    "start": "519719",
    "end": "529120"
  },
  {
    "text": "uh it's called in outer space put them in space suits the good news is by the way this is the AWS conference Amazon",
    "start": "529120",
    "end": "534839"
  },
  {
    "text": "can help you with that as well [Laughter]",
    "start": "534839",
    "end": "541200"
  },
  {
    "text": "so another thing that happens at Large Scale is that you end up with really interesting failure modes and to do that",
    "start": "541200",
    "end": "549680"
  },
  {
    "text": "I want to talk about the Netflix outage that for me at least was the most memorable outage in uh the third quarter",
    "start": "549680",
    "end": "556000"
  },
  {
    "text": "of this year you've probably not heard about this before so um has anybody here",
    "start": "556000",
    "end": "562440"
  },
  {
    "start": "560000",
    "end": "560000"
  },
  {
    "text": "seen House of Cards great fantastic one of my favorite shows so uh June 4th 2015 we released a",
    "start": "562440",
    "end": "571560"
  },
  {
    "text": "small bit of software to production That was supposed to not have any adverse impact on anybody anybody ever done",
    "start": "571560",
    "end": "580320"
  },
  {
    "text": "that the good news is we didn't actually completely break anybody all the all",
    "start": "580320",
    "end": "585600"
  },
  {
    "text": "that happened was well one device out of about a thousand different device types",
    "start": "585600",
    "end": "591200"
  },
  {
    "text": "by the way the numbers here are for me estimates there're just orders of magnitude so one device out of a thousand device types in one test cell",
    "start": "591200",
    "end": "599079"
  },
  {
    "text": "out of of about 10 test cells in one test out of about 10,000",
    "start": "599079",
    "end": "605120"
  },
  {
    "text": "tests couldn't view season 3 episode one of House of",
    "start": "605120",
    "end": "611760"
  },
  {
    "text": "Cards and um this went on for a week and we didn't even catch it the only reason",
    "start": "615760",
    "end": "621760"
  },
  {
    "text": "we found this to be honest is that we released something else a week later and somebody was looking at some graphs and noticed that there was some sort of",
    "start": "621760",
    "end": "627680"
  },
  {
    "text": "weird deviation and then we you know jumped on this and found the problem and uh keep this in mind right we had",
    "start": "627680",
    "end": "634440"
  },
  {
    "text": "billions of successful streams that week the amount of disappointments that we generated out of this was vanishingly",
    "start": "634440",
    "end": "642040"
  },
  {
    "text": "small I actually did the math um in preparing for this presentation it was something like five we disappointed",
    "start": "642040",
    "end": "648440"
  },
  {
    "text": "either five customers once or One customer one really pissed off customer five",
    "start": "648440",
    "end": "654680"
  },
  {
    "text": "times and this is a really hard thing to frankly um catch in fact when I was",
    "start": "655240",
    "end": "660519"
  },
  {
    "text": "talking to some people at work I got this great quote from one of our UI engineering uh directors who said we have weird device specific problems all",
    "start": "660519",
    "end": "667519"
  },
  {
    "text": "the time and interactions with AB tests only make them more complicated so I'm not sure we have a pat moral of the",
    "start": "667519",
    "end": "673160"
  },
  {
    "text": "story other than you know we really like alerting and fast responses so what makes the cloud",
    "start": "673160",
    "end": "679000"
  },
  {
    "start": "678000",
    "end": "678000"
  },
  {
    "text": "interesting well I have some bad news about this right when I join Netflix in the data",
    "start": "679000",
    "end": "684399"
  },
  {
    "text": "center we're running our uh databases on one huge monolithic piece of Hardware",
    "start": "684399",
    "end": "689720"
  },
  {
    "text": "from our favorite Hardware vendor who should remain nameless and um every year",
    "start": "689720",
    "end": "695000"
  },
  {
    "text": "or 18 months we would start noticing slowdowns because we're exceeding the capacity of the server we would take a few days or weeks to figure out what we",
    "start": "695000",
    "end": "702120"
  },
  {
    "text": "needed to get we would place the order for the next big piece of Hardware that cost millions of dollars they would take",
    "start": "702120",
    "end": "708000"
  },
  {
    "text": "some weeks to produce this thing and days to ship it and then we would take a week or two to to build it into the data",
    "start": "708000",
    "end": "713959"
  },
  {
    "text": "center and provision and configure and then do the database migration and it would take like two or three months and",
    "start": "713959",
    "end": "719399"
  },
  {
    "text": "the good news is that um when it takes you two three months to remedy this kind of problem then um you frankly have more",
    "start": "719399",
    "end": "727000"
  },
  {
    "text": "time to notice it so let's look at the new world we use Dynamo DB um I have U",
    "start": "727000",
    "end": "733279"
  },
  {
    "text": "occasionally actually found that I need more capacity from Dynamo DB in which case all you need to do is do an API",
    "start": "733279",
    "end": "738639"
  },
  {
    "text": "call to ask for more read or write capacity and in my original version of the presentation I was going to say that in my experience it's taken seconds for",
    "start": "738639",
    "end": "745880"
  },
  {
    "text": "that capacity to be provisioned Amazon actually uh asked me to say it could take minutes so as to not overpromise",
    "start": "745880",
    "end": "753120"
  },
  {
    "text": "but still minutes is pretty darn good and the thing is if it takes minutes for Amazon to fix stuff then you can't take",
    "start": "753120",
    "end": "760199"
  },
  {
    "text": "days to notice the problem that essentially means that Amazon has the",
    "start": "760199",
    "end": "765360"
  },
  {
    "text": "potential to make us look bad because they react really really quickly now there's some good news too uh it turns",
    "start": "765360",
    "end": "772720"
  },
  {
    "text": "out that infrastructure is no longer the bottleneck and that things that took weeks before to change uh now just take",
    "start": "772720",
    "end": "778639"
  },
  {
    "text": "an API call um and by the way I love slide reuse and that also means that you can",
    "start": "778639",
    "end": "784519"
  },
  {
    "text": "do a lot of rapid recovery and you can do automated response because everything essentially is one API call away which",
    "start": "784519",
    "end": "790160"
  },
  {
    "text": "if you are a lazy person as I am and as most of the engineers that we've hired are then Amazon potentially allows you",
    "start": "790160",
    "end": "796959"
  },
  {
    "text": "to do a lot less boring work so how does this end up working for your advantage that's Chris's part but",
    "start": "796959",
    "end": "803639"
  },
  {
    "text": "before uh Chris gets into his part I want to talk a little about some of the capabilities that you probably want to",
    "start": "803639",
    "end": "808839"
  },
  {
    "start": "806000",
    "end": "806000"
  },
  {
    "text": "have in your your environment in order to be able to take advantage of the uh algorithms that we talked about the",
    "start": "808839",
    "end": "814440"
  },
  {
    "text": "first thing is time series so you know you probably should have some sort of monitoring system that allows you to create some sort of mon uh monitoring",
    "start": "814440",
    "end": "820480"
  },
  {
    "text": "time series for some sort of metric that's pretty Elementary it's also nice to have some sort of event streaming",
    "start": "820480",
    "end": "825639"
  },
  {
    "text": "system um whether it's you know based on elk or spark streaming or anything else",
    "start": "825639",
    "end": "831959"
  },
  {
    "text": "and uh if you're running in a service oriented architecture uh automatically discovering dependencies and looking at",
    "start": "831959",
    "end": "837920"
  },
  {
    "text": "their relationships is kind of a nice thing and on that note Chris would you like this non-functional pointer I would",
    "start": "837920",
    "end": "845279"
  },
  {
    "text": "love it yeah it's not functioning it kind of is does work that's a that's a tough",
    "start": "845279",
    "end": "852160"
  },
  {
    "start": "848000",
    "end": "848000"
  },
  {
    "text": "lead up to follow right um so I want to take some time and and talk to about uh three different broad categories of of",
    "start": "852160",
    "end": "858639"
  },
  {
    "text": "analytics uh that we have found useful um to address some of these challenges that Roy has laid out um namely I'm",
    "start": "858639",
    "end": "865360"
  },
  {
    "text": "going to talk about prediction talk about detection and also the concept of correlation uh so let's dive",
    "start": "865360",
    "end": "873160"
  },
  {
    "text": "in so the first category prediction prediction is something we we should all be relatively familiar with uh whether",
    "start": "873639",
    "end": "879360"
  },
  {
    "text": "you've uh read the historic works of Nostradamus or you're trying to figure out if the black blackjack table out in",
    "start": "879360",
    "end": "885279"
  },
  {
    "text": "the lobby is in your benefit um so we want to be able to leverage this power of prediction to be able to make some",
    "start": "885279",
    "end": "892160"
  },
  {
    "text": "informed decisions about our environment and so one of those cases or one use case we can think about is",
    "start": "892160",
    "end": "899800"
  },
  {
    "text": "the use of predictions for predictive scaling now who here is familiar with",
    "start": "899800",
    "end": "908399"
  },
  {
    "start": "905000",
    "end": "905000"
  },
  {
    "text": "autoscaling all right a good a good part of us um so at as core autoscaling is a",
    "start": "908399",
    "end": "913839"
  },
  {
    "text": "reactive model for example let's take a look at this example autoscaling policy so we want to scale up our service here",
    "start": "913839",
    "end": "920120"
  },
  {
    "text": "by 10% and we're going to do that when some workload in this case for example",
    "start": "920120",
    "end": "925920"
  },
  {
    "text": "request per second breach some threshold so by Nature we are reacting to",
    "start": "925920",
    "end": "931800"
  },
  {
    "text": "something in our environment so we're reacting to when our requests per second exceed some threshold this has been a big win for a",
    "start": "931800",
    "end": "938759"
  },
  {
    "text": "lot of companies big and small uh but there are a set of advanced use cases that reactive",
    "start": "938759",
    "end": "944160"
  },
  {
    "text": "autoscaling uh doesn't handle uh for example one of those use cases is where",
    "start": "944160",
    "end": "949560"
  },
  {
    "text": "a service may have rapid re uh reoccurring spikes in demand so for example let's say you have a service",
    "start": "949560",
    "end": "955720"
  },
  {
    "text": "that every Monday at 4:00 sees a big increase in in traffic and let's say for",
    "start": "955720",
    "end": "961560"
  },
  {
    "text": "example sake that your service takes 40 minutes to start up in this case every Monday you may be underprovision to",
    "start": "961560",
    "end": "968759"
  },
  {
    "text": "handle this load another case where reactive may not",
    "start": "968759",
    "end": "974079"
  },
  {
    "text": "may not handle well is where we have variable traffic patterns for example the traffic pattern on a Monday Tuesday Wednesday or Thursday are all going to",
    "start": "974079",
    "end": "980279"
  },
  {
    "text": "be slightly different and third and maybe more",
    "start": "980279",
    "end": "985519"
  },
  {
    "text": "importantly outages as Roy noted we all do push code sometimes that can cause outages so in this case if we using",
    "start": "985519",
    "end": "992399"
  },
  {
    "text": "reactive autoscaling you push a new piece of your software and there's a bug in it and your requests per second come",
    "start": "992399",
    "end": "999279"
  },
  {
    "text": "down and that's what you're using to scale reactive autoscaling may start to scale your service down and when",
    "start": "999279",
    "end": "1006639"
  },
  {
    "text": "inevitably the retry storm comes when your service becomes available you're going to be under Provisions so these",
    "start": "1006639",
    "end": "1012920"
  },
  {
    "text": "are some use cases where we can apply predictive scaling or Predictive Analytics",
    "start": "1012920",
    "end": "1019800"
  },
  {
    "text": "so at the core concept is that we want to anticipate change in our traffics and",
    "start": "1019800",
    "end": "1024918"
  },
  {
    "text": "our workload from that we want to make some predictions we want to predict the",
    "start": "1024919",
    "end": "1030280"
  },
  {
    "text": "resources we'll need ahead of time and finally we want to proactively scale up or scale down our",
    "start": "1030280",
    "end": "1037360"
  },
  {
    "text": "services but before we can actually talk about the algorithms or the techniques we need to pick the right metrics we",
    "start": "1037360",
    "end": "1042720"
  },
  {
    "text": "need need to pick the right data so we need to have a clear",
    "start": "1042720",
    "end": "1048438"
  },
  {
    "text": "relatively stable and reoccurring pattern to our data it's very hard to make a prediction on noisy random",
    "start": "1048439",
    "end": "1055559"
  },
  {
    "text": "data further more we want our metric to be independent of any type of cluster performance as we'll see shortly so for",
    "start": "1055559",
    "end": "1063400"
  },
  {
    "text": "these reasons we use a metric called requests per second or RPS so we can see that this",
    "start": "1063400",
    "end": "1070480"
  },
  {
    "text": "metric exhibits those criteria we have a reoccurring pattern for example our weekly Curiosities you know have the",
    "start": "1070480",
    "end": "1076720"
  },
  {
    "text": "same day of week so our Monday traffic going to look like Monday traffic our Tuesday traffic is going to look like Tuesday traffic furthermore request per",
    "start": "1076720",
    "end": "1084960"
  },
  {
    "text": "second for us is independent of cluster performance it is just measuring the total number of requests coming into our",
    "start": "1084960",
    "end": "1090400"
  },
  {
    "text": "service so if we decide to change the underlying instance type or make a performance change that doesn't affect our",
    "start": "1090400",
    "end": "1096919"
  },
  {
    "text": "predictive autoscaling so how can we actually use a",
    "start": "1096919",
    "end": "1102280"
  },
  {
    "text": "metric like this to predict the future well there's a breadth of techniques we can use out there from the world of uh",
    "start": "1102280",
    "end": "1109880"
  },
  {
    "text": "statistics and predictions uh but one Technique we have found to be extremely useful is by applying a concept called",
    "start": "1109880",
    "end": "1116039"
  },
  {
    "text": "the fast forer transformation or fft for short this is a technique that comes from the digital signal processing",
    "start": "1116039",
    "end": "1122400"
  },
  {
    "text": "domain and it has some nice advantages and we've demonstrated or tried to illustrate one here so imagine at the",
    "start": "1122400",
    "end": "1129080"
  },
  {
    "text": "bottom of this graph we have our input metric this is our requests per second this is a complex waveform it's it's",
    "start": "1129080",
    "end": "1135360"
  },
  {
    "text": "made up of multiple things when we pass this through our FF algorithm we can recover or decompose this input signal",
    "start": "1135360",
    "end": "1142720"
  },
  {
    "text": "into a set of simpler sine waves or components and that's what these top three represent here so the way we want",
    "start": "1142720",
    "end": "1150080"
  },
  {
    "text": "to use this to do predictions is we want to M make sure we maintain our Curiosities in our data make sure we",
    "start": "1150080",
    "end": "1156600"
  },
  {
    "text": "have the same Trends and the same Cycles but we want to remove the noise we want to remove that noise from our signal and",
    "start": "1156600",
    "end": "1163240"
  },
  {
    "text": "so to do that it's quite simple we can decompose our input metric into the set of components",
    "start": "1163240",
    "end": "1169720"
  },
  {
    "text": "and we can remove the high frequency components and these high frequency components represent the noise in our",
    "start": "1169720",
    "end": "1174840"
  },
  {
    "text": "metric so in this example it might be the top component which is high frequency noise what we can do then is",
    "start": "1174840",
    "end": "1181960"
  },
  {
    "text": "exclude that and use the middle two components so the second and the third add them back together and we get a new",
    "start": "1181960",
    "end": "1189120"
  },
  {
    "text": "signal and this new signal is going to be smoother than the input signal and we can use this for prediction so let's look at an example",
    "start": "1189120",
    "end": "1196360"
  },
  {
    "text": "so here's a here's an example where we have an input metric requests per second and it's kind of noisy we can see we",
    "start": "1196360",
    "end": "1201880"
  },
  {
    "text": "have a spike in the middle there we can also see we have some variability in the underlying data and we want to remove",
    "start": "1201880",
    "end": "1209000"
  },
  {
    "text": "that we want to be able to use this to make a prediction so what we do is we run it through the fast foror transform",
    "start": "1209000",
    "end": "1215000"
  },
  {
    "text": "and outcomes the green line and we see it has the same shape Trend and curiosity as the input but it's clean",
    "start": "1215000",
    "end": "1221240"
  },
  {
    "text": "it's smoother but to use this we need to convert it into some type of action so",
    "start": "1221240",
    "end": "1228320"
  },
  {
    "text": "it's one to predict in the future your workload or your metric was one thing to turn that into actions so in this case",
    "start": "1228320",
    "end": "1234559"
  },
  {
    "text": "we see on the left here our prediction and we're predicting our workload into the future for a certain service and",
    "start": "1234559",
    "end": "1240799"
  },
  {
    "text": "this is RPS request per second and on the right hand side we see a generated",
    "start": "1240799",
    "end": "1245919"
  },
  {
    "text": "action plan this action plan says that at any given minute the number of instances or servers we need um as",
    "start": "1245919",
    "end": "1253919"
  },
  {
    "text": "predicted by our prediction function so the important thing to take take away",
    "start": "1253919",
    "end": "1258960"
  },
  {
    "text": "from this is that our action plan matches in shape and kind of curve to",
    "start": "1258960",
    "end": "1264360"
  },
  {
    "text": "our prediction so how do we do this in in reality so this is a simplified kind of",
    "start": "1264360",
    "end": "1271039"
  },
  {
    "text": "workflow but we take our metric we're interested in observing for example request per second run it through our",
    "start": "1271039",
    "end": "1277159"
  },
  {
    "text": "fft based prediction algorithm then we compute that action plan and that action plan is what we use to talk to the AWS",
    "start": "1277159",
    "end": "1284279"
  },
  {
    "text": "API to say we need more capacity we need less capacity remember this is all",
    "start": "1284279",
    "end": "1289720"
  },
  {
    "text": "proactively however we can't rely purely on predictive scaling if you remember we",
    "start": "1289720",
    "end": "1295400"
  },
  {
    "text": "were removing the noise from our signal now that noise represents true workload",
    "start": "1295400",
    "end": "1300679"
  },
  {
    "text": "in our environment so we have to create a hybrid approach so we're going to use",
    "start": "1300679",
    "end": "1306400"
  },
  {
    "text": "both the benefits of reactive and predictive together uh so in this case we're going to predict our workload uh",
    "start": "1306400",
    "end": "1312559"
  },
  {
    "text": "for an application um and proactively scale it at the same time we're going to use the reactive autoscaling to handle",
    "start": "1312559",
    "end": "1318679"
  },
  {
    "text": "are unexpected surges uh in work so this this demonstrates a great example where",
    "start": "1318679",
    "end": "1325000"
  },
  {
    "text": "we can use prediction to help make our environment uh healthier or more robust",
    "start": "1325000",
    "end": "1330400"
  },
  {
    "text": "to change another place we can think about using prediction is in the area of",
    "start": "1330400",
    "end": "1337240"
  },
  {
    "text": "detection uh so detection is very important given any environment that we want to observe when something goes Ary",
    "start": "1337240",
    "end": "1343559"
  },
  {
    "text": "we want to be able to know when unexpected changes happen and one of those ways of doing",
    "start": "1343559",
    "end": "1349600"
  },
  {
    "text": "that is through the use of anomaly detection so anomal detection is a very very popular Topic at the moment",
    "start": "1349600",
    "end": "1357559"
  },
  {
    "text": "um and Wikipedia defines anomaly detection as the identification of observations which do not conform to an",
    "start": "1357559",
    "end": "1364520"
  },
  {
    "text": "expected pattern um this is a very general broad term so we want to be",
    "start": "1364520",
    "end": "1369640"
  },
  {
    "text": "really concrete what we mean by anomaly so in this case I like to call them blips and bloops very scientific um but",
    "start": "1369640",
    "end": "1377320"
  },
  {
    "start": "1374000",
    "end": "1374000"
  },
  {
    "text": "really they represent short shortterm anomalies and long-term anomalies so in the short-term case on the left we can",
    "start": "1377320",
    "end": "1384360"
  },
  {
    "text": "see that we have some deviation of expected behaviors for some period of time and this period of time may be a",
    "start": "1384360",
    "end": "1390600"
  },
  {
    "text": "few minutes could be up to an hour maybe and on the right hand side we see some deviation but over a longer period of",
    "start": "1390600",
    "end": "1397320"
  },
  {
    "text": "time and this deviation may be over days weeks hours as well but the fundamental",
    "start": "1397320",
    "end": "1403039"
  },
  {
    "text": "thing to recognize here is that our in our our two types here also contain",
    "start": "1403039",
    "end": "1410440"
  },
  {
    "text": "Curiosities they contain continue uh contain Trends and also",
    "start": "1410440",
    "end": "1416039"
  },
  {
    "text": "Cycles so how might we be able to detect these two different types of anomalies where we have a short-term or a",
    "start": "1416039",
    "end": "1421480"
  },
  {
    "text": "long-term anomaly well a natural starting places to try and use something like static",
    "start": "1421480",
    "end": "1427400"
  },
  {
    "text": "thresholds uh so static thresholds are a great starting place if you know your data is relatively stable in this case",
    "start": "1427400",
    "end": "1433360"
  },
  {
    "text": "we're looking at an example where we have CPU utilization and we can see it's relatively stable so we can use static",
    "start": "1433360",
    "end": "1439720"
  },
  {
    "text": "thresholds which are kind of a constant type of prediction to say have we",
    "start": "1439720",
    "end": "1446200"
  },
  {
    "text": "exceeded some workload however can we use static",
    "start": "1446200",
    "end": "1451720"
  },
  {
    "text": "thresholds to help find these these short-term and long-term anomalies in",
    "start": "1451720",
    "end": "1456760"
  },
  {
    "text": "our data that has Cycles Curiosities and Trends well becomes very difficult where",
    "start": "1456760",
    "end": "1462440"
  },
  {
    "text": "would we fit a static threshold in this case to find this this short-term anomaly so we have to use different",
    "start": "1462440",
    "end": "1469440"
  },
  {
    "text": "techniques but we can use the power of prediction like we saw in the previous section to help solve this problem and",
    "start": "1469440",
    "end": "1477080"
  },
  {
    "text": "there's a wealth of these different types of prediction algorithms as we saw so we have an fft based prediction that we use predictive scaling but there's",
    "start": "1477080",
    "end": "1483520"
  },
  {
    "text": "also other techniques such as double exponential smoothing this is a concept that we use a lot at Netflix so much so",
    "start": "1483520",
    "end": "1489440"
  },
  {
    "text": "that it's built into our primary Telemetry system but then there's other examples like H Winters which is an extension to double exponential",
    "start": "1489440",
    "end": "1495480"
  },
  {
    "text": "smoothing or Rema which is a technique that has a funny name",
    "start": "1495480",
    "end": "1500320"
  },
  {
    "text": "so how how do we actually go about doing this in our environment so one way is we want to",
    "start": "1500720",
    "end": "1506559"
  },
  {
    "text": "observe a set of metrics these metrics could be system level metrics it could be application performance level metrics",
    "start": "1506559",
    "end": "1512600"
  },
  {
    "text": "so what we're going to do is we take that metric we're going to compute our prediction maybe we use the fast forward transform again or maybe we use",
    "start": "1512600",
    "end": "1519520"
  },
  {
    "text": "something else but we're going to compute a prediction then from that prediction we're going to compute the residual and this residual is really the",
    "start": "1519520",
    "end": "1525640"
  },
  {
    "text": "difference between our prediction and our are actual so what do we see right now so you can consider it a Delta it's",
    "start": "1525640",
    "end": "1532799"
  },
  {
    "text": "the the residual then from that we can set our static threshold all a sudden so let's walk through a real quick example",
    "start": "1532799",
    "end": "1538520"
  },
  {
    "text": "of this so here's an example where we're going to use the double exponential smoothing function uh to make a",
    "start": "1538520",
    "end": "1544520"
  },
  {
    "text": "prediction um so here we see our original input line which has a blip in it or a short-term temp um anomaly",
    "start": "1544520",
    "end": "1551799"
  },
  {
    "text": "that's the blue line then we're going to compute our forecast which is the the red line that's using double exponential smoothing so at each time step we're",
    "start": "1551799",
    "end": "1557679"
  },
  {
    "text": "going to compute a forecast then from that we can compute our residual so each time step we're",
    "start": "1557679",
    "end": "1563279"
  },
  {
    "text": "going to then compute the difference between our prediction and our actual and this is what it looks like so our",
    "start": "1563279",
    "end": "1570480"
  },
  {
    "text": "our anomaly becomes very evident all of a sudden from this residual we can then",
    "start": "1570480",
    "end": "1575760"
  },
  {
    "text": "fit or set our constant prediction or our static threshold in this case and all is well in the world and",
    "start": "1575760",
    "end": "1582720"
  },
  {
    "text": "you can go home and not get woken up at 3: in the morning however you still have to pick this threshold",
    "start": "1582720",
    "end": "1589440"
  },
  {
    "text": "we're still trying to pick a static threshold for these signals so we can Leverage The Power of",
    "start": "1589440",
    "end": "1595039"
  },
  {
    "text": "Statistics to help with this for example we can use very simple statistical methods to automatically find these",
    "start": "1595039",
    "end": "1601399"
  },
  {
    "text": "anomalies in our residuals now and a very common technique might be that of three sigma or really three standard",
    "start": "1601399",
    "end": "1608240"
  },
  {
    "text": "deviations and there's a breadth of other techniques so if you recall our initial workflow there we we were observing a",
    "start": "1608240",
    "end": "1614840"
  },
  {
    "text": "metric Computing a prediction taking the difference the the residual and then putting a threshold so a simple",
    "start": "1614840",
    "end": "1621720"
  },
  {
    "text": "modification to this gets us some nice advantages so instead of a threshold there we're going to replace it with a",
    "start": "1621720",
    "end": "1627360"
  },
  {
    "text": "statistical test such as 3 Sigma so we no longer have to set a threshold and we",
    "start": "1627360",
    "end": "1633120"
  },
  {
    "text": "can go off and set this up loose on all two billion metrics I don't think Roy will be too happy about that but we can do",
    "start": "1633120",
    "end": "1640840"
  },
  {
    "text": "better why I use one algorithm when we can use multiple so in this case we could use",
    "start": "1640840",
    "end": "1646520"
  },
  {
    "text": "three different detection techniques or three different statistical tests we could use for example our three sigma we",
    "start": "1646520",
    "end": "1651799"
  },
  {
    "text": "could use something called intertile range and so on then we have each of those different techniques vote is there",
    "start": "1651799",
    "end": "1658200"
  },
  {
    "text": "anomaly or isn't there anomaly right now then we want to combine those votes in some way so a very simple way to combine",
    "start": "1658200",
    "end": "1663720"
  },
  {
    "text": "those votes is majority so do two of the three at least vote that there is",
    "start": "1663720",
    "end": "1668799"
  },
  {
    "text": "something wrong and this is the what is classically two is an ensemble approach we have gotten really good",
    "start": "1668799",
    "end": "1675960"
  },
  {
    "text": "leverage out of using this in our production environment we use a technique very similar to this to watch our kpis so as Roy mentioned we have a",
    "start": "1675960",
    "end": "1683200"
  },
  {
    "text": "kpi of stream starts per second SPS we use a technique very similar to this to watch our stream starts per second for",
    "start": "1683200",
    "end": "1689399"
  },
  {
    "text": "any large deviations now it's important to note there's also a breadth of other",
    "start": "1689399",
    "end": "1695799"
  },
  {
    "text": "techniques out there um and libraries uh that have been released by companies ranging from Netflix Twitter Yahoo Etsy",
    "start": "1695799",
    "end": "1703000"
  },
  {
    "text": "so I encourage you go check these out if you're interested in the area of anomaly detection so let's let's take a little",
    "start": "1703000",
    "end": "1709440"
  },
  {
    "text": "bit of a look at a different detection problem now let's let's turn anomaly detection upside down and let's look at something we call",
    "start": "1709440",
    "end": "1717159"
  },
  {
    "text": "outlier",
    "start": "1717159",
    "end": "1719679"
  },
  {
    "text": "detection now we classify outlier detection to be",
    "start": "1722799",
    "end": "1728519"
  },
  {
    "text": "the task of identifying unusual members from a set of generating mechanisms",
    "start": "1728519",
    "end": "1734000"
  },
  {
    "text": "again that is very broad but it shouldn't be confused with the task of anomaly detection so we have found that by breaking up the",
    "start": "1734000",
    "end": "1741919"
  },
  {
    "text": "problems into anomaly out we get some very good mileage in that we're able to talk with the domains separately and",
    "start": "1741919",
    "end": "1747840"
  },
  {
    "text": "we'll see through an example what we mean by anomaly versus outlier so in this case let's talk about anomaly real",
    "start": "1747840",
    "end": "1755000"
  },
  {
    "text": "quickly so anomaly detection is the process of defining or trying to identify unusual events from a single",
    "start": "1755000",
    "end": "1762559"
  },
  {
    "text": "member of a population over time so that's our horizontal line here so over time we're going to look at one member",
    "start": "1762559",
    "end": "1768159"
  },
  {
    "text": "of a population and try and find Oddities outlier detection is the other way given a single point in time we're",
    "start": "1768159",
    "end": "1775200"
  },
  {
    "text": "going to look for unusual members so this there there are many",
    "start": "1775200",
    "end": "1781000"
  },
  {
    "text": "different ways or different uh areas we want to apply this type of technique of outlier detection and one of those is in",
    "start": "1781000",
    "end": "1787519"
  },
  {
    "text": "the area of server outlier detection so for example Netflix runs on thousands of",
    "start": "1787519",
    "end": "1793360"
  },
  {
    "start": "1788000",
    "end": "1788000"
  },
  {
    "text": "servers and of those thousands of servers we we typically see a small percentage of them become unhealthy",
    "start": "1793360",
    "end": "1799200"
  },
  {
    "text": "um and this can relate to customer degreg furthermore if if we're have our",
    "start": "1799200",
    "end": "1804320"
  },
  {
    "text": "service owners trying to go find these time can be wasted just paging through graphs so we want a way to automatically",
    "start": "1804320",
    "end": "1810000"
  },
  {
    "text": "find these servers that aren't behaving like each other",
    "start": "1810000",
    "end": "1815159"
  },
  {
    "text": "now just want to know that you don't have to have thousands of servers for this to be of use you could have small",
    "start": "1815159",
    "end": "1821200"
  },
  {
    "text": "deployments medium or large deployments and the concepts still apply you want to make sure that your services and",
    "start": "1821200",
    "end": "1826960"
  },
  {
    "text": "applications are relative performing uh as expected so let's look at some examples of what we mean by outliers so",
    "start": "1826960",
    "end": "1834320"
  },
  {
    "text": "here's a here's an example where we see a band of servers as reported by some",
    "start": "1834320",
    "end": "1839600"
  },
  {
    "text": "Metric and we can see that we have a lone server that's trying to be unique and deviating away from the rest of the",
    "start": "1839600",
    "end": "1845440"
  },
  {
    "text": "herd in this case this is what we refer to as outlier detection or server outlier detection we want to find that",
    "start": "1845440",
    "end": "1850760"
  },
  {
    "text": "one server that is not behaving like the others so this is this is a a clean easy",
    "start": "1850760",
    "end": "1857720"
  },
  {
    "text": "examp example so what about this case are there any outliers in this case in",
    "start": "1857720",
    "end": "1862880"
  },
  {
    "text": "this example there actually are not any outliers what we see here is that we can have metrics that start to band together",
    "start": "1862880",
    "end": "1869760"
  },
  {
    "text": "this could be because we have two different performance characteristics for example the top one might be one instance type and the bottom might be a",
    "start": "1869760",
    "end": "1875720"
  },
  {
    "text": "new instance type so we want to be robust to these types of scenarios as well so one way we can solve this",
    "start": "1875720",
    "end": "1882600"
  },
  {
    "text": "problem of identifying servers that aren't really behaving like their peers is through the the use of cluster",
    "start": "1882600",
    "end": "1888960"
  },
  {
    "text": "analysis cluster analysis is a unsupervised machine learning technique and conceptually it it defines that if a",
    "start": "1888960",
    "end": "1896080"
  },
  {
    "text": "server belongs to a group it should be near lots of other points as measured by",
    "start": "1896080",
    "end": "1901279"
  },
  {
    "text": "some distance function so things should group together now we have to make one assumption here our assumption is that",
    "start": "1901279",
    "end": "1908799"
  },
  {
    "text": "servers that are running on the same hardware and software should relatively uh behave in a similar",
    "start": "1908799",
    "end": "1916360"
  },
  {
    "text": "fashion so there are a lot of different clustering algorithms out there uh in our use case we have found that the DB",
    "start": "1916360",
    "end": "1924320"
  },
  {
    "text": "scan algorithm works extremely well and DB scan stands for density based spatial clustering of applications with noise",
    "start": "1924320",
    "end": "1930880"
  },
  {
    "text": "I'm not sure what came first the name or the acronym um in this case DB scan is going",
    "start": "1930880",
    "end": "1937080"
  },
  {
    "text": "to iterate over a set of points or servers in this case and we're going to Mark those those servers in regions with",
    "start": "1937080",
    "end": "1942600"
  },
  {
    "text": "many neighbors as clusters and Mark those in lower dimensional or lower regions",
    "start": "1942600",
    "end": "1948519"
  },
  {
    "text": "as outliers and this diagram depicts that quite nicely so you can see we have three different high density",
    "start": "1948519",
    "end": "1955279"
  },
  {
    "text": "groups as identified by three different colors and those points that don't belong to any of these clusters that",
    "start": "1955279",
    "end": "1961200"
  },
  {
    "text": "fall in uh these low density regions are actually outliers so built right into",
    "start": "1961200",
    "end": "1966480"
  },
  {
    "text": "the algorithm we have the concept of an outlier and that is fantastic so to use this in in practice",
    "start": "1966480",
    "end": "1974120"
  },
  {
    "text": "it's very similar to our anomaly detection case we're going to observe some meth this could be system level application",
    "start": "1974120",
    "end": "1979960"
  },
  {
    "text": "Level run it through our dbcan algorithm dbcan is going to return to us the set of servers that are outlining we then",
    "start": "1979960",
    "end": "1987799"
  },
  {
    "text": "want to perform some of filtering and this is where we can apply domain specific Logic for example let's say we",
    "start": "1987799",
    "end": "1993120"
  },
  {
    "text": "don't want to actually do anything with outliers servers that are say not taking traffic so we are able to apply some",
    "start": "1993120",
    "end": "1998880"
  },
  {
    "text": "domain specific logic here and then finally we want to take some action and there's a broad range of actions we can",
    "start": "1998880",
    "end": "2004760"
  },
  {
    "text": "take for example we can send you an email we can leverage the fact that we're in the cloud and terminate some of",
    "start": "2004760",
    "end": "2011240"
  },
  {
    "text": "these instances or we can even say detach these instances from a load balancer uh so we can do some uh triage",
    "start": "2011240",
    "end": "2017559"
  },
  {
    "text": "and maybe uh remediation so we've seen here how we can use the power of unsupervised",
    "start": "2017559",
    "end": "2023639"
  },
  {
    "text": "machine learning to be able to identify servers part of our applications that aren't behaving as",
    "start": "2023639",
    "end": "2030559"
  },
  {
    "text": "expected so I wanted to go into another use of outlier detection uh it's a",
    "start": "2030559",
    "end": "2035639"
  },
  {
    "text": "simpler use case and a little bit more concrete and it's called automated Canary analysis so who here is familiar with a",
    "start": "2035639",
    "end": "2044080"
  },
  {
    "start": "2044000",
    "end": "2044000"
  },
  {
    "text": "canary release process all right we got some bird",
    "start": "2044080",
    "end": "2051358"
  },
  {
    "text": "enthusiasts so for the uninitiated the canary release process is is a",
    "start": "2051359",
    "end": "2056638"
  },
  {
    "text": "deployment methodology where a new change to your application is gradually rolled out into",
    "start": "2056639",
    "end": "2062358"
  },
  {
    "text": "production as we do this gradual roll out we're going to perform checks along the way and we're going to perform these",
    "start": "2062359",
    "end": "2067760"
  },
  {
    "text": "checks each time we gradually roll out our software and at each of these checkpoints we're going to make some",
    "start": "2067760",
    "end": "2074398"
  },
  {
    "text": "decision as to whether or not we want to continue moving forward or if we want to roll back so this process affords us some",
    "start": "2074399",
    "end": "2081520"
  },
  {
    "text": "nice advantages uh one of the advantages we're afforded is that we get a better",
    "start": "2081520",
    "end": "2086560"
  },
  {
    "text": "degree of trust and safety in our deployments as we're slowly rolling out our new software we can perform these",
    "start": "2086560",
    "end": "2091960"
  },
  {
    "text": "checks and as we do those checks we can get some better um trust in our systems",
    "start": "2091960",
    "end": "2098240"
  },
  {
    "text": "we can also based on that get faster deployment Cadence we can deploy multiple canaries all slowly rolling",
    "start": "2098240",
    "end": "2103320"
  },
  {
    "text": "them out and making decisions as we go and finally we don't have to invest",
    "start": "2103320",
    "end": "2108680"
  },
  {
    "text": "much into simulation uh simulation engineering now I want to point out that",
    "start": "2108680",
    "end": "2113920"
  },
  {
    "text": "Canary analysis is not meant to replace any type of uh testing whether it's functional testing integration testing",
    "start": "2113920",
    "end": "2119680"
  },
  {
    "text": "it's meant to augment your deployment strategy or your deployment pipeline so let's take an example look",
    "start": "2119680",
    "end": "2126240"
  },
  {
    "text": "at an example of what we mean by the canary release process so in in this little example we have some traffic and",
    "start": "2126240",
    "end": "2132160"
  },
  {
    "text": "that traffic is going through a load balancer and we can see that that load balancer splitting our traffic between",
    "start": "2132160",
    "end": "2137560"
  },
  {
    "text": "two versions of our software we got version one at the top there which is currently in production and version two",
    "start": "2137560",
    "end": "2142880"
  },
  {
    "text": "or version 1.1 which is our new Canary version and we can see that traffic is split proportionately based on the",
    "start": "2142880",
    "end": "2149000"
  },
  {
    "text": "number of servers we have so as we go along and we keep turning the dial up and keep sending more and more traffic",
    "start": "2149000",
    "end": "2155040"
  },
  {
    "text": "to our Canary version we're going to do these checks we're going to check is it good are things looking good ideally we",
    "start": "2155040",
    "end": "2161319"
  },
  {
    "text": "get to a place where everything's looked good and 100% of the traffic is now going to our new",
    "start": "2161319",
    "end": "2167839"
  },
  {
    "text": "version now one way you might want to do these checks is to look at",
    "start": "2167839",
    "end": "2174319"
  },
  {
    "text": "dashboards uh and as your metric volumes grow and you reach two billion metrics building dashboards to do this type of",
    "start": "2175280",
    "end": "2181800"
  },
  {
    "text": "things just no longer works so we need to come up with different ways to help automate this process for us",
    "start": "2181800",
    "end": "2188839"
  },
  {
    "text": "and one way we can do that is by once again leveraging the power of say statistics um so let's talk a little bit",
    "start": "2188839",
    "end": "2196359"
  },
  {
    "text": "how we might automate this process first we need to identify a set of metrics that we want to actually analyze or",
    "start": "2196359",
    "end": "2201760"
  },
  {
    "text": "compare again these can be system level metrics application Level metrics or performance level",
    "start": "2201760",
    "end": "2207440"
  },
  {
    "text": "metrics we're then going to use a statistical test to try and identify the differences between our version one and",
    "start": "2207440",
    "end": "2213839"
  },
  {
    "text": "our version 1.1 so our new and our old and there's different ways we can do this we could use something like a man",
    "start": "2213839",
    "end": "2220000"
  },
  {
    "text": "Whitney statistical test once we've identified the metrics that are different we can then compute",
    "start": "2220000",
    "end": "2227119"
  },
  {
    "text": "what we call a canary score now this Canary score indicates to us the overall similarity of our new software to our",
    "start": "2227119",
    "end": "2233640"
  },
  {
    "text": "old software and ideally they should match 100% so one way you can calculate this",
    "start": "2233640",
    "end": "2239319"
  },
  {
    "text": "is as a percent of metrics that actually match so in in reality our our kind of",
    "start": "2239319",
    "end": "2246319"
  },
  {
    "text": "our analysis workflow looks very similar to this we're going to look at our set of metrics again we're going to perform",
    "start": "2246319",
    "end": "2252480"
  },
  {
    "text": "that statistical test identifying the metrics that are different between our Canary and our production we're then",
    "start": "2252480",
    "end": "2259200"
  },
  {
    "text": "going to calculate that score ideally getting 100% once we have that score we're going to make some decision and",
    "start": "2259200",
    "end": "2264960"
  },
  {
    "text": "that decision is going to indicate to us should we continue going forward with our roll out or should we move",
    "start": "2264960",
    "end": "2271640"
  },
  {
    "text": "back now this technique has been extremely useful for Netflix so much so that has been built into our continuous",
    "start": "2271640",
    "end": "2277560"
  },
  {
    "text": "delivery pipeline uh Spiner but we've had to make some changes to this the way we do this",
    "start": "2277560",
    "end": "2283319"
  },
  {
    "text": "analysis so we've augmented it so in this case we're actually adding",
    "start": "2283319",
    "end": "2289000"
  },
  {
    "text": "a little bit of complexity but it's it's the complexity in a sense of we're adding a control group right in the",
    "start": "2289000",
    "end": "2294440"
  },
  {
    "text": "middle now this control group you note is the exact same version as the previous versions version one but you'll",
    "start": "2294440",
    "end": "2300520"
  },
  {
    "text": "notice it size the same size as our Canary version so it's six servers by",
    "start": "2300520",
    "end": "2305800"
  },
  {
    "text": "doing this and introducing the concept of a control group we can get an Apples to Apples comparison so we can now",
    "start": "2305800",
    "end": "2312200"
  },
  {
    "text": "compare our metrics from the canary to metrics from a control and we're going to do an actual um Apples to Apples",
    "start": "2312200",
    "end": "2318200"
  },
  {
    "text": "comparison something else this affords us is the ability to start the two groups up at the same time and monitor",
    "start": "2318200",
    "end": "2325040"
  },
  {
    "text": "their startup characteristics and then finally we've augmented our workflow by adding on our automated analysis on the end of our",
    "start": "2325040",
    "end": "2332079"
  },
  {
    "text": "Pipeline and this ties into our primary Telemetry system so it's one thing to",
    "start": "2332079",
    "end": "2338880"
  },
  {
    "text": "automatically generate the score and to decide if we want to move forward or move back with our deployments but sometimes you need manual intervention",
    "start": "2338880",
    "end": "2346079"
  },
  {
    "text": "so instead of giving our developers and service owners dashboards and dashboards to look at we're able to generate a",
    "start": "2346079",
    "end": "2352599"
  },
  {
    "text": "report this is a curated Canary report so in this report we can see we have that score we have that high level",
    "start": "2352599",
    "end": "2358160"
  },
  {
    "text": "Canary score and this one scored 92.3% so our service owners can use this",
    "start": "2358160",
    "end": "2363560"
  },
  {
    "text": "to help triage and dig into why did their new software their new deployment not actually uh match in performance to",
    "start": "2363560",
    "end": "2370640"
  },
  {
    "text": "the existing software so over the last couple of",
    "start": "2370640",
    "end": "2376520"
  },
  {
    "text": "minutes here we've looked at different ways we can go about using say machine",
    "start": "2376520",
    "end": "2382079"
  },
  {
    "text": "learning statistics to monitor our environment and help make some automated decisions for",
    "start": "2382079",
    "end": "2388440"
  },
  {
    "text": "us so moving forward and know the path ahead of us we want to be able to use",
    "start": "2389480",
    "end": "2395319"
  },
  {
    "text": "and Leverage The Power of correlation so everything we've looked at so far is",
    "start": "2395319",
    "end": "2401640"
  },
  {
    "text": "looking at an application or um service level uh itself so we're",
    "start": "2401640",
    "end": "2407920"
  },
  {
    "text": "looking in isolations we're analyzing each individual service themselves however we work in a",
    "start": "2407920",
    "end": "2413960"
  },
  {
    "text": "microservice architecture so on Netflix we have over 700 microservices uh and this diagram is",
    "start": "2413960",
    "end": "2420920"
  },
  {
    "start": "2415000",
    "end": "2415000"
  },
  {
    "text": "what we call the Death Star uh this represents the interactions between all of our services uh so this is a a real",
    "start": "2420920",
    "end": "2426960"
  },
  {
    "text": "diagram has been anonymized but we can see that there's some services that are heavily interconnected with other",
    "start": "2426960",
    "end": "2433560"
  },
  {
    "text": "services so all our current detection techniques have been looking at a service by service level but we know",
    "start": "2433560",
    "end": "2439960"
  },
  {
    "text": "there's interactions between all these services so we want to be able to Leverage The Power of correlation",
    "start": "2439960",
    "end": "2445240"
  },
  {
    "text": "analysis to be able to discover some of those interesting correlations or dependencies and help make us better",
    "start": "2445240",
    "end": "2451280"
  },
  {
    "text": "informed decisions about our environment so let's look at a little bit more of a concrete example of how",
    "start": "2451280",
    "end": "2456599"
  },
  {
    "text": "we're thinking about doing this so in this case we have a we have a a",
    "start": "2456599",
    "end": "2462160"
  },
  {
    "text": "dependency graph where we have a set of services and their dependence so for example the node a here represents some",
    "start": "2462160",
    "end": "2467640"
  },
  {
    "text": "service a and it has some dependencies B and C now let's say we're running these detection techniques that we have and we",
    "start": "2467640",
    "end": "2475000"
  },
  {
    "text": "all of a sudden notice we have a spike in alarms on three of our services say",
    "start": "2475000",
    "end": "2480560"
  },
  {
    "text": "service J and H so we may alarm our service owners at each of these tiers and say hey your application is is a",
    "start": "2480560",
    "end": "2487640"
  },
  {
    "text": "little wonky right now however we can see that there's a there's a common dependent here we got a common dependent",
    "start": "2487640",
    "end": "2493800"
  },
  {
    "text": "on F and so why didn't F fire well maybe F here doesn't have any alarms or",
    "start": "2493800",
    "end": "2500440"
  },
  {
    "text": "doesn't have any Telemetry around it so we want to be able to use the power of correlation analysis to be able to look",
    "start": "2500440",
    "end": "2506680"
  },
  {
    "text": "at our environment from a macro level and say well J and H all have a common",
    "start": "2506680",
    "end": "2512680"
  },
  {
    "text": "dependen f is it actually encountering any issues",
    "start": "2512680",
    "end": "2519240"
  },
  {
    "text": "and there's another case where we want to use correlation analysis and that is in the case where we're looking at a",
    "start": "2521760",
    "end": "2527119"
  },
  {
    "text": "single service itself and we want to build correlate metrics so what I mean by this is let's say that we have an",
    "start": "2527119",
    "end": "2534800"
  },
  {
    "text": "alarm setup on our CPU so let's say we set up a static threshold and it triggers an alarm there's an increase in",
    "start": "2534800",
    "end": "2542359"
  },
  {
    "text": "CPU we can then go and correlate that potential with other metrics in our environment so for example we can May correlate that with a spike in our HTTP",
    "start": "2542359",
    "end": "2550280"
  },
  {
    "text": "400s at the same time it may be inversely correlated with",
    "start": "2550280",
    "end": "2555520"
  },
  {
    "text": "HTTP requests or maybe the input into our service and if I recall Roy this was",
    "start": "2555520",
    "end": "2560920"
  },
  {
    "text": "actually a real case example or a real example sorry yeah actually this was a",
    "start": "2560920",
    "end": "2565960"
  },
  {
    "text": "funny one um we had a service that uh had a CPU Spike and fired uh some alarms",
    "start": "2565960",
    "end": "2571000"
  },
  {
    "text": "and then when we looked uh because our system was able to use is it Pearson uh at Pearson algorithm to tell",
    "start": "2571000",
    "end": "2578359"
  },
  {
    "text": "us what other metrics were changing in about the same sort of direction as CPU we saw an increase in 400s and a",
    "start": "2578359",
    "end": "2584280"
  },
  {
    "text": "decrease in requests which is kind of weird if you think about it it turned out that the service had a um a 400",
    "start": "2584280",
    "end": "2591280"
  },
  {
    "text": "Handler that was a CGI script written in Pearl and was remarkably non-performing",
    "start": "2591280",
    "end": "2597119"
  },
  {
    "text": "because it was doing really complex stuff so as soon as some people started hitting uh an un an unallowed endpoint",
    "start": "2597119",
    "end": "2603720"
  },
  {
    "text": "and generated a bunch of 400s performance dropped through the floor floor while CPUs spiked uh and it was",
    "start": "2603720",
    "end": "2610359"
  },
  {
    "text": "really interesting to actually look at how these three graphs were you know very tightly",
    "start": "2610359",
    "end": "2615480"
  },
  {
    "text": "correlated so this is an example again where we can potentially Leverage The Power of correlation analysis something",
    "start": "2615480",
    "end": "2621359"
  },
  {
    "text": "to note is in AA in an environment where we have powerful correlation methods we",
    "start": "2621359",
    "end": "2626839"
  },
  {
    "text": "don't have to set up alarms on all of our metrics for example let's say we just set up an alarm or an alert on our",
    "start": "2626839",
    "end": "2632599"
  },
  {
    "text": "CPU we can then use correlation analysis to go through and find other metrics that are very similar um similarly",
    "start": "2632599",
    "end": "2641160"
  },
  {
    "text": "correlated and then finally there's another use case where we want to start investigating the use of correlation",
    "start": "2641160",
    "end": "2647720"
  },
  {
    "text": "analysis so imagine that you care about this metric let's say this is a",
    "start": "2647720",
    "end": "2653920"
  },
  {
    "text": "kpi so we can see that it has some some interesting characteristics and let's say that we use correlation analysis and",
    "start": "2653920",
    "end": "2660200"
  },
  {
    "text": "we find out that it is strongly correlated with this metric",
    "start": "2660200",
    "end": "2665359"
  },
  {
    "text": "so do we actually care about these metrics are we able to use correlation analysis to say well we should more more",
    "start": "2665359",
    "end": "2672760"
  },
  {
    "text": "or less be interested in the left one or maybe we're more interested in the W one or actually maybe there's a parent metric that we should be really watching",
    "start": "2672760",
    "end": "2679760"
  },
  {
    "text": "so we thinking about how we can use correlation analysis to help uh prune the space of metrics that our users have",
    "start": "2679760",
    "end": "2686319"
  },
  {
    "text": "to explore through um and do that in an automated fashion and the whole idea",
    "start": "2686319",
    "end": "2691960"
  },
  {
    "text": "here is to reduce the cognitive workload of our service owners and Engineers to figure figure out which metrics from our",
    "start": "2691960",
    "end": "2698440"
  },
  {
    "text": "2 billion are interesting and meaningful uh for",
    "start": "2698440",
    "end": "2703799"
  },
  {
    "text": "them shall I Al righty so um what's the whole",
    "start": "2704240",
    "end": "2711359"
  },
  {
    "text": "takeaway here well I'm hoping that if we've done our",
    "start": "2711359",
    "end": "2716440"
  },
  {
    "start": "2714000",
    "end": "2714000"
  },
  {
    "text": "job right um algorithms like Dez or IQR or fft or DB scan are a little less",
    "start": "2716440",
    "end": "2722680"
  },
  {
    "text": "mysterious and magical um for what it's worth speaking not as a data scientist Wikipedia article for example on DB scan",
    "start": "2722680",
    "end": "2728920"
  },
  {
    "text": "is so readable even I could read it and understand it um and that's a very very high bar",
    "start": "2728920",
    "end": "2735800"
  },
  {
    "text": "um and we think that if you U have a reasonable Telemetry platform you can actually start uh experimenting with",
    "start": "2735800",
    "end": "2742079"
  },
  {
    "text": "these algorithms and approaches by basically not trying to build it into your Telemetry platform necessarily but",
    "start": "2742079",
    "end": "2747559"
  },
  {
    "text": "really have a standalone component that simply talks to both your Telemetry platform and to your orchestration",
    "start": "2747559",
    "end": "2754119"
  },
  {
    "text": "system uh to do some interesting things in your envirment so I started off saying that our goal",
    "start": "2754119",
    "end": "2761520"
  },
  {
    "text": "was to help uh people figure out how they might find some relevant uh use cases here and some relevant algorithms",
    "start": "2761520",
    "end": "2768119"
  },
  {
    "text": "they can start working with and I'm interested in testing the hypothesis that this was potentially going to be",
    "start": "2768119",
    "end": "2773640"
  },
  {
    "text": "useful show of hands did anybody here find something here that they're interested in actually experimenting",
    "start": "2773640",
    "end": "2779160"
  },
  {
    "text": "with when they go back to work hey awesome so we were successful um this was a brief survey we didn't go",
    "start": "2779160",
    "end": "2786960"
  },
  {
    "text": "in deeply into a lot of data science and algorithms we're happy to have those conversations um at least Chris is I'm",
    "start": "2786960",
    "end": "2793720"
  },
  {
    "text": "totally ignorant um we're also happy to have conversations about different use cases for these kind of things in um",
    "start": "2793720",
    "end": "2799920"
  },
  {
    "text": "cloud-based ecosystems we've talked about a few of our use cases um as you find other use cases frankly uh we'd",
    "start": "2799920",
    "end": "2805920"
  },
  {
    "text": "love to know about them so with that in mind um we're going to publish this presentation sometime today and tweet",
    "start": "2805920",
    "end": "2813240"
  },
  {
    "text": "about it uh it includes a bunch of links to uh more detailed information including some Netflix",
    "start": "2813240",
    "end": "2820400"
  },
  {
    "text": "presentations uh remember to complete your evaluations this is Chris these are",
    "start": "2820400",
    "end": "2825559"
  },
  {
    "text": "Chris's and mine Twitter handles feel free to tweet us uh with any interesting use cases you can come up with and I",
    "start": "2825559",
    "end": "2832760"
  },
  {
    "text": "believe we have uh 11 minutes and 56 seconds for questions what can we tell",
    "start": "2832760",
    "end": "2838839"
  },
  {
    "text": "you",
    "start": "2838839",
    "end": "2841839"
  }
]