[
  {
    "start": "0",
    "end": "33000"
  },
  {
    "text": "Welcome to another episode of 'This is My Architecture'.",
    "start": "7341",
    "end": "9590"
  },
  {
    "text": "Today I'm joined by Aveek from Zillow.",
    "start": "9590",
    "end": "12106"
  },
  {
    "text": "Hey, Aveek.\nHi, Adrian, thank you for having me.",
    "start": "12106",
    "end": "14590"
  },
  {
    "text": "Thanks for coming in.",
    "start": "14590",
    "end": "16013"
  },
  {
    "text": "So tell us a little bit about what Zillow does.",
    "start": "16013",
    "end": "18228"
  },
  {
    "text": "Of course. So Zillow is the largest \nreal estate platform in the country.",
    "start": "18228",
    "end": "21910"
  },
  {
    "text": "We get the most amount of real estate traffic on our website.",
    "start": "21910",
    "end": "24768"
  },
  {
    "text": "And we do everything from\n selling, buying, providing rentals,",
    "start": "24768",
    "end": "30503"
  },
  {
    "text": "and providing home loans.",
    "start": "30503",
    "end": "31987"
  },
  {
    "text": "So all things real estate, come to Zillow, right?\nZillow.com",
    "start": "31987",
    "end": "34767"
  },
  {
    "start": "33000",
    "end": "96000"
  },
  {
    "text": "So, tell us a little bit about the interactions \nthat you have with some of your customers.",
    "start": "34767",
    "end": "38938"
  },
  {
    "text": "Of course. So one of the things that \nwe do is connect our potential buyers",
    "start": "38938",
    "end": "42796"
  },
  {
    "text": "to real estate agents through phone conversations.",
    "start": "42796",
    "end": "45063"
  },
  {
    "text": "And that's where this architecture comes in, \nin helping our customers as well as our agents. ",
    "start": "45063",
    "end": "49636"
  },
  {
    "text": "And here, we have a near real-time pipeline",
    "start": "50181",
    "end": "52473"
  },
  {
    "text": "that allows us to run machine-learning models on top \nof these conversations, to generate insights.",
    "start": "52473",
    "end": "56944"
  },
  {
    "text": "I can't wait. Let's dive in.\nSure!",
    "start": "56944",
    "end": "58944"
  },
  {
    "text": "So it all starts\nby calling AWS Transcribe,",
    "start": "58944",
    "end": "62570"
  },
  {
    "text": "which converts\nthe phone call speech to text,",
    "start": "62570",
    "end": "65697"
  },
  {
    "text": "and one of the important things in here is \nwe use, we take data security very seriously at Zillow,",
    "start": "65697",
    "end": "70793"
  },
  {
    "text": "so we use the redaction API in AWS Transcribe,",
    "start": "70793",
    "end": "74251"
  },
  {
    "text": "and that scrubs off all PII content, for example, name,\n phone number, email address, from these transcripts.",
    "start": "74251",
    "end": "79701"
  },
  {
    "text": "So, once that is done, these \ntranscripts are sent over to Firehose,",
    "start": "80212",
    "end": "84468"
  },
  {
    "text": "and we have a limit of one minute \nof buffering on this Firehose,",
    "start": "84469",
    "end": "88452"
  },
  {
    "text": "and every one minute we are spitting out \na file containing these transcripts onto S3.",
    "start": "88452",
    "end": "93199"
  },
  {
    "text": "So, one file for one minute.",
    "start": "93199",
    "end": "95436"
  },
  {
    "text": "So, you've got an async pattern to deal with \nall that volume and all that data sitting in S3.",
    "start": "95818",
    "end": "102871"
  },
  {
    "start": "96000",
    "end": "121000"
  },
  {
    "text": "Tell us a little bit about\nwhat Lambda is doing.",
    "start": "102871",
    "end": "105317"
  },
  {
    "text": "Of course. So, this Lambda is to handle the trigger \nthat is generated off of S3 for this event,",
    "start": "105317",
    "end": "111767"
  },
  {
    "text": "and this Lambda then pulls this\n container information from ECR",
    "start": "111767",
    "end": "117152"
  },
  {
    "text": "and creates a job\nand submits it to AWS Batch.",
    "start": "117152",
    "end": "120511"
  },
  {
    "text": "Right, so you've got your container registry maintaining \nall of the different models that you have.",
    "start": "120511",
    "end": "125859"
  },
  {
    "start": "121000",
    "end": "179000"
  },
  {
    "text": "Now, tell us a little bit about \nhow you're using Batch.",
    "start": "126322",
    "end": "129056"
  },
  {
    "text": "Of course. So the Batch is where\nour machine learning inferences run.",
    "start": "129056",
    "end": "133694"
  },
  {
    "text": "We use transformer-based architectures for our machine\n learning models, so previously, of the R deep learn models,",
    "start": "133694",
    "end": "139377"
  },
  {
    "text": "and AWS Batch helps us centralize\nbetween ECR and ECS",
    "start": "139377",
    "end": "144855"
  },
  {
    "text": "and helps us scale up and down \ndepending on our job load.",
    "start": "144855",
    "end": "147823"
  },
  {
    "text": "For example, when we submit a job, \nthis job is put in a job queue over here,",
    "start": "147824",
    "end": "152930"
  },
  {
    "text": "and this job queue triggers the\n scale-up and scale-down in ECS.",
    "start": "152930",
    "end": "156810"
  },
  {
    "text": "So when there is a lot of load and we get \na load of traffic, say, for example, in the mornings,",
    "start": "156811",
    "end": "161651"
  },
  {
    "text": "our cluster scales up, and at night, \nwhen there are no conversations happening,",
    "start": "161651",
    "end": "166462"
  },
  {
    "text": "we scale down all the way to zero.",
    "start": "166462",
    "end": "168377"
  },
  {
    "text": "So that's a clever way of using Lambda and \nBatch together to scale your EC2 container service.",
    "start": "168817",
    "end": "174946"
  },
  {
    "text": "So, what sort of savings\nhave you seen there?",
    "start": "175546",
    "end": "178771"
  },
  {
    "text": "So just by choosing this architecture and not \ngoing with something, for example, Cron-based",
    "start": "178771",
    "end": "183707"
  },
  {
    "text": "on a Kubernetes, or doing\nindividual services for the models,",
    "start": "183707",
    "end": "186466"
  },
  {
    "text": "because we run multiple models\nin a single container through AWS Batch,",
    "start": "186466",
    "end": "189961"
  },
  {
    "text": "we are saving up to 75%\nof the compute cost.",
    "start": "189961",
    "end": "192487"
  },
  {
    "text": "So, once you've done that inference, \nyou've matched some agents, what happens next? ",
    "start": "192890",
    "end": "197354"
  },
  {
    "text": "So what happens next is we put these \npredictions into 2 different channels,",
    "start": "197354",
    "end": "201436"
  },
  {
    "text": "one of the channels being an SNS  topic.",
    "start": "201436",
    "end": "204829"
  },
  {
    "text": "So the downstream teams can\n then subscribe to this topic,",
    "start": "204829",
    "end": "208676"
  },
  {
    "text": "and then, in near-real time, within 5 minutes \nof the call ending, give these valuable insights.",
    "start": "208676",
    "end": "213666"
  },
  {
    "text": "And we also send it off\nto another Firehose",
    "start": "213666",
    "end": "216986"
  },
  {
    "text": "which buffers it for a longer duration \nand then puts it into a data lake,",
    "start": "217748",
    "end": "222284"
  },
  {
    "text": "secure data lake on S3, so that our data scientists can run\n analytics with Spark and Presto and what have you.",
    "start": "222284",
    "end": "228564"
  },
  {
    "text": "Great, so you're using SNS to give \nnear-real time data availability.",
    "start": "228564",
    "end": "232948"
  },
  {
    "start": "229000",
    "end": "292000"
  },
  {
    "text": "Tell me a little bit about \nwhat the developer experience is like.",
    "start": "232948",
    "end": "236773"
  },
  {
    "text": "Of course. So, we use GitLab\n pipelines and GitLab bespells.",
    "start": "236773",
    "end": "241049"
  },
  {
    "text": "Our applied scientists, they build their models \nand they push their Python code",
    "start": "241050",
    "end": "245024"
  },
  {
    "text": "over here, to GitLab.",
    "start": "245024",
    "end": "247137"
  },
  {
    "text": "And the model dates go to S3.",
    "start": "247137",
    "end": "250176"
  },
  {
    "text": "And we take both of these and run in GitLab pipelines",
    "start": "253089",
    "end": "256422"
  },
  {
    "text": "and we push the new image to ECR, \nso this image tag would plug in",
    "start": "256422",
    "end": "262624"
  },
  {
    "text": "to a particle of GitLab commit\n and a particle of model date version.",
    "start": "262624",
    "end": "265461"
  },
  {
    "text": "So you've built a pipeline for your data \nanalysts and they don't have to",
    "start": "265461",
    "end": "270314"
  },
  {
    "text": "ever deal with any of the infrastructure.",
    "start": "270314",
    "end": "272104"
  },
  {
    "text": "That's correct. So, if your model is not \ndoing well in production, you get an easy way  ",
    "start": "272104",
    "end": "276164"
  },
  {
    "text": "to roll back or roll forward.",
    "start": "276164",
    "end": "278028"
  },
  {
    "text": "This is a great architecture.",
    "start": "278028",
    "end": "279549"
  },
  {
    "text": "Thanks so much for sharing it\nwith us today, Aveek.",
    "start": "279549",
    "end": "281628"
  },
  {
    "text": "Thank you so much for having me.",
    "start": "281628",
    "end": "282865"
  },
  {
    "text": "And thank you for joining us\non 'This is My Architecture'.",
    "start": "282865",
    "end": "285561"
  }
]