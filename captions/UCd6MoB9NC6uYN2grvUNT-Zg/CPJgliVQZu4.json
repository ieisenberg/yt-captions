[
  {
    "start": "0",
    "end": "68000"
  },
  {
    "text": "- [Sohaib] Hello, everyone,\nmy name is Sohaib Katariwala",
    "start": "1020",
    "end": "3310"
  },
  {
    "text": "and I am an analytic\nspecialist solutions architect",
    "start": "3310",
    "end": "5720"
  },
  {
    "text": "with AWS.",
    "start": "5720",
    "end": "7189"
  },
  {
    "text": "I'm joined with my colleague\nSam Selvan who is also",
    "start": "7190",
    "end": "9970"
  },
  {
    "text": "an analytic specialist\nsolutions architect with AWS.",
    "start": "9970",
    "end": "13390"
  },
  {
    "text": "Today, we will be giving you a demo",
    "start": "13390",
    "end": "15230"
  },
  {
    "text": "of the news Streaming Ingestion\nfeature of Amazon Redshift.",
    "start": "15230",
    "end": "19199"
  },
  {
    "text": "There are existing solutions\nto ingest streaming data",
    "start": "19200",
    "end": "21830"
  },
  {
    "text": "into Amazon Redshift such\nas Amazon Kinesis Firehose",
    "start": "21830",
    "end": "26100"
  },
  {
    "text": "that have been available\nfor some time now.",
    "start": "26100",
    "end": "28860"
  },
  {
    "text": "Customers have been asking us",
    "start": "28860",
    "end": "30700"
  },
  {
    "text": "for a way to increase the throughput",
    "start": "30700",
    "end": "32669"
  },
  {
    "text": "and latency of streaming data\ningested into Amazon Redshift.",
    "start": "32670",
    "end": "36899"
  },
  {
    "text": "This was a key driver for us to create",
    "start": "36900",
    "end": "38760"
  },
  {
    "text": "the new native streaming\ningestion feature for on Redshift.",
    "start": "38760",
    "end": "43100"
  },
  {
    "text": "With Redshift Streaming Ingestion,",
    "start": "43100",
    "end": "45070"
  },
  {
    "text": "the latency of your streaming data",
    "start": "45070",
    "end": "47020"
  },
  {
    "text": "goes from minutes to seconds.",
    "start": "47020",
    "end": "49143"
  },
  {
    "text": "Redshift Streaming Ingestion\nis entirely configurable",
    "start": "50070",
    "end": "53400"
  },
  {
    "text": "with SQL which makes it very easy to use.",
    "start": "53400",
    "end": "57210"
  },
  {
    "text": "You can define schema\nfor your streaming data",
    "start": "57210",
    "end": "59739"
  },
  {
    "text": "or choose to ingest\nsemi-structured JSON data",
    "start": "59740",
    "end": "62760"
  },
  {
    "text": "with the Redshift SUPER data type.",
    "start": "62760",
    "end": "65100"
  },
  {
    "text": "Now Sam will show you how it works.",
    "start": "65100",
    "end": "67783"
  },
  {
    "start": "68000",
    "end": "444000"
  },
  {
    "text": "First of all, we need streaming data.",
    "start": "69960",
    "end": "72650"
  },
  {
    "text": "You can use existing Kinesis\ndata streams that you have.",
    "start": "72650",
    "end": "76120"
  },
  {
    "text": "For this demo, we'll\ncreate a new data stream.",
    "start": "76120",
    "end": "79500"
  },
  {
    "text": "We will use a publicly available tool",
    "start": "79500",
    "end": "81500"
  },
  {
    "text": "that the Kinesis team has created",
    "start": "81500",
    "end": "83400"
  },
  {
    "text": "called the Amazon Kinesis Data Generator.",
    "start": "83400",
    "end": "86640"
  },
  {
    "text": "It allows you to create a\ntemplate for your JSON payload",
    "start": "86640",
    "end": "89560"
  },
  {
    "text": "using some functions from\nthe Faker Python package",
    "start": "90420",
    "end": "93549"
  },
  {
    "text": "that generates fake data for you.",
    "start": "93550",
    "end": "95713"
  },
  {
    "text": "Once we have a template,",
    "start": "96620",
    "end": "98100"
  },
  {
    "text": "we click on Send Data to start generating",
    "start": "98100",
    "end": "100799"
  },
  {
    "text": "and sending data to a Kinesis data stream.",
    "start": "100800",
    "end": "103283"
  },
  {
    "text": "Now Sam will jump over to\nthe Redshift query editor",
    "start": "104220",
    "end": "107940"
  },
  {
    "text": "and connect to a Redshift cluster.",
    "start": "107940",
    "end": "110150"
  },
  {
    "text": "The first thing he'll do",
    "start": "110150",
    "end": "111520"
  },
  {
    "text": "is create an external schema from Kinesis.",
    "start": "111520",
    "end": "115259"
  },
  {
    "text": "To create the external\nschema from Kinesis,",
    "start": "115260",
    "end": "118250"
  },
  {
    "text": "he'll give the external schema",
    "start": "118250",
    "end": "119930"
  },
  {
    "text": "a unique and descriptive name.",
    "start": "119930",
    "end": "122160"
  },
  {
    "text": "Then he will specify",
    "start": "122160",
    "end": "123670"
  },
  {
    "text": "that this external schema is from Kinesis.",
    "start": "123670",
    "end": "126680"
  },
  {
    "text": "And finally, he will specify an IAM role",
    "start": "126680",
    "end": "130460"
  },
  {
    "text": "that Redshift will assume",
    "start": "130460",
    "end": "131810"
  },
  {
    "text": "to access the Kinesis data streams.",
    "start": "131810",
    "end": "134222"
  },
  {
    "text": "This role must have access\nto read the data stream,",
    "start": "135380",
    "end": "139050"
  },
  {
    "text": "as well as list the streams",
    "start": "139050",
    "end": "140920"
  },
  {
    "text": "that you plan on ingesting\ninto Amazon Redshift.",
    "start": "140920",
    "end": "143900"
  },
  {
    "text": "Here is an example of the IAM role policy",
    "start": "143900",
    "end": "147390"
  },
  {
    "text": "we are using for this demo.",
    "start": "147390",
    "end": "149113"
  },
  {
    "text": "You can see that this role",
    "start": "150270",
    "end": "151440"
  },
  {
    "text": "is allowed to DescribeStreamSummary,",
    "start": "151440",
    "end": "153733"
  },
  {
    "text": "GetChardIterator, GetRecords,\nand DescribeStream.",
    "start": "155120",
    "end": "157722"
  },
  {
    "text": "This role is also allowed to ListStreams",
    "start": "159030",
    "end": "161410"
  },
  {
    "text": "and ListShards for all streams",
    "start": "161410",
    "end": "164040"
  },
  {
    "text": "just for the purpose of this demo.",
    "start": "164040",
    "end": "165739"
  },
  {
    "text": "Once the external schema is created,",
    "start": "166740",
    "end": "169420"
  },
  {
    "text": "Sam will enable case sensitivity",
    "start": "169420",
    "end": "171940"
  },
  {
    "text": "for identifiers in Redshift.",
    "start": "171940",
    "end": "174630"
  },
  {
    "text": "This is because Kinesis stream\nnames are case sensitive.",
    "start": "174630",
    "end": "178383"
  },
  {
    "text": "Next, we want to specify which data stream",
    "start": "179290",
    "end": "181950"
  },
  {
    "text": "we'd like to ingest into Amazon Redshift.",
    "start": "181950",
    "end": "185129"
  },
  {
    "text": "We do this by creating a materialized view",
    "start": "185130",
    "end": "187570"
  },
  {
    "text": "in our external schema",
    "start": "187570",
    "end": "189050"
  },
  {
    "text": "from the Kinesis data stream\nthat we want to ingest.",
    "start": "189050",
    "end": "192570"
  },
  {
    "text": "Make sure to use the exact\nstream name within double quotes,",
    "start": "192570",
    "end": "196540"
  },
  {
    "text": "keeping case sensitivity in mind.",
    "start": "196540",
    "end": "198853"
  },
  {
    "text": "Now let's look at what goes\nin the select statement",
    "start": "199690",
    "end": "203620"
  },
  {
    "text": "in our materialized view definition",
    "start": "203620",
    "end": "206019"
  },
  {
    "text": "when ingesting from Kinesis data streams.",
    "start": "206020",
    "end": "209390"
  },
  {
    "text": "The first four columns are\napproximatearrivaltimestamp,",
    "start": "209390",
    "end": "213123"
  },
  {
    "text": "partitionkey, shardid, and sequencenumber.",
    "start": "214362",
    "end": "218740"
  },
  {
    "text": "These are metadata columns",
    "start": "218740",
    "end": "220620"
  },
  {
    "text": "that are available by default\nfor all Kinesis data streams.",
    "start": "220620",
    "end": "224909"
  },
  {
    "text": "They are helpful for\nauditing, troubleshooting,",
    "start": "224910",
    "end": "227740"
  },
  {
    "text": "and monitoring your streaming data.",
    "start": "227740",
    "end": "229943"
  },
  {
    "text": "The payload column\ncontains the actual data",
    "start": "230790",
    "end": "234659"
  },
  {
    "text": "in the payload of the data stream.",
    "start": "234660",
    "end": "236623"
  },
  {
    "text": "In this case, we are choosing\nto ingest the entire payload",
    "start": "237770",
    "end": "241830"
  },
  {
    "text": "into a SUPER data type column\nin our materialized view.",
    "start": "241830",
    "end": "245653"
  },
  {
    "text": "Sam will run the statement to\ncreate the materialized view.",
    "start": "246780",
    "end": "251370"
  },
  {
    "text": "It is important to note that creating",
    "start": "251370",
    "end": "253450"
  },
  {
    "text": "the materialized view doesn't\nactually populate the data",
    "start": "253450",
    "end": "256730"
  },
  {
    "text": "from the Kinesis data\nstream into Amazon Redshift.",
    "start": "256730",
    "end": "260420"
  },
  {
    "text": "Once the materialized view is created,",
    "start": "260420",
    "end": "263150"
  },
  {
    "text": "we need to refresh the\nmaterialized view to pull the data",
    "start": "263150",
    "end": "267030"
  },
  {
    "text": "from the Kinesis data\nstream into Amazon Redshift.",
    "start": "267030",
    "end": "271120"
  },
  {
    "text": "Sam will now run the refresh\nmaterialize view command",
    "start": "271120",
    "end": "274660"
  },
  {
    "text": "to pull the data from\nthe Kinesis data stream",
    "start": "274660",
    "end": "278240"
  },
  {
    "text": "and materialize it in\nthe materialized view.",
    "start": "278240",
    "end": "281522"
  },
  {
    "text": "The first time you refresh\na materialized view",
    "start": "282560",
    "end": "285210"
  },
  {
    "text": "after creating it from\na Kinesis data stream,",
    "start": "285210",
    "end": "288400"
  },
  {
    "text": "Redshift will go back to the\ntrim horizon of your data",
    "start": "288400",
    "end": "291830"
  },
  {
    "text": "and pull all the data from\nthat point in your data stream.",
    "start": "292770",
    "end": "296552"
  },
  {
    "text": "Depending on how long the\ndata has been streaming,",
    "start": "297530",
    "end": "300220"
  },
  {
    "text": "it may take longer for the first refresh",
    "start": "300220",
    "end": "302340"
  },
  {
    "text": "to pull all the data from the trim horizon",
    "start": "302340",
    "end": "305260"
  },
  {
    "text": "compared to subsequent refreshes\nof your materialized view.",
    "start": "305260",
    "end": "308763"
  },
  {
    "text": "Once the materialized view is refreshed,",
    "start": "309900",
    "end": "312400"
  },
  {
    "text": "Sam will run a query to check the max",
    "start": "312400",
    "end": "314800"
  },
  {
    "text": "and min approximate arrival\ntimestamp of the data.",
    "start": "314800",
    "end": "318653"
  },
  {
    "text": "We can see that the minimum timestamp",
    "start": "319530",
    "end": "322330"
  },
  {
    "text": "goes back to when the\nKinesis data generator",
    "start": "322330",
    "end": "324939"
  },
  {
    "text": "started streaming data\ninto this data stream",
    "start": "324940",
    "end": "328370"
  },
  {
    "text": "and the max timestamp is\njust around a few seconds",
    "start": "328370",
    "end": "332169"
  },
  {
    "text": "prior to the current\ntime on Sam's computer.",
    "start": "332170",
    "end": "335760"
  },
  {
    "text": "Next, Sam will select the payload",
    "start": "335760",
    "end": "338460"
  },
  {
    "text": "from this materialized view",
    "start": "338460",
    "end": "340460"
  },
  {
    "text": "and we can see the JSON objects",
    "start": "340460",
    "end": "343370"
  },
  {
    "text": "generated by the data generator",
    "start": "343370",
    "end": "346500"
  },
  {
    "text": "and published into the data stream",
    "start": "346500",
    "end": "348520"
  },
  {
    "text": "are now in our materialized\nview as a SUPER data type.",
    "start": "348520",
    "end": "351883"
  },
  {
    "text": "Sam will now navigate\nto the data generator",
    "start": "352830",
    "end": "356000"
  },
  {
    "text": "where he has created another data stream",
    "start": "356000",
    "end": "359080"
  },
  {
    "text": "that is also called EV_Station_Data,",
    "start": "359080",
    "end": "361653"
  },
  {
    "text": "but it has mixed case letters in the name.",
    "start": "362610",
    "end": "365759"
  },
  {
    "text": "He will now create a materialized view",
    "start": "365760",
    "end": "369240"
  },
  {
    "text": "from this different data stream.",
    "start": "369240",
    "end": "371870"
  },
  {
    "text": "For this new materialized view,",
    "start": "371870",
    "end": "373910"
  },
  {
    "text": "Sam is not storing the entire JSON payload",
    "start": "373910",
    "end": "377190"
  },
  {
    "text": "into a single SUPER data type column.",
    "start": "377190",
    "end": "380660"
  },
  {
    "text": "For this stream, the\nschema is well defined",
    "start": "380660",
    "end": "384040"
  },
  {
    "text": "and he knows the schema of the\ndata that will be streaming,",
    "start": "384040",
    "end": "387410"
  },
  {
    "text": "so he will use Redshift's\nbuilt-in JSON functions",
    "start": "387410",
    "end": "392050"
  },
  {
    "text": "to extract specific fields from the JSON",
    "start": "392050",
    "end": "395840"
  },
  {
    "text": "and store them into separate columns",
    "start": "395840",
    "end": "397760"
  },
  {
    "text": "in the materialized view.",
    "start": "397760",
    "end": "399053"
  },
  {
    "text": "Here he is also able to\ndefine specific columns",
    "start": "400040",
    "end": "404130"
  },
  {
    "text": "as distribution keys and sort key,",
    "start": "404130",
    "end": "407090"
  },
  {
    "text": "just like he could in any\nother materialized view",
    "start": "407090",
    "end": "410160"
  },
  {
    "text": "or table in Amazon Redshift.",
    "start": "410160",
    "end": "412233"
  },
  {
    "text": "He will now create this materialized view",
    "start": "413140",
    "end": "415870"
  },
  {
    "text": "and refresh it to pull the data",
    "start": "415870",
    "end": "418510"
  },
  {
    "text": "from the Kinesis data\nstream into Amazon Redshift.",
    "start": "418510",
    "end": "422150"
  },
  {
    "text": "Since the refresh\nmaterialized view command",
    "start": "422150",
    "end": "424600"
  },
  {
    "text": "will only pull the data from\nthe Kinesis data stream once,",
    "start": "424600",
    "end": "427810"
  },
  {
    "text": "you can create a schedule to\nrefresh the materialized view",
    "start": "427810",
    "end": "431340"
  },
  {
    "text": "to pull data from your\nKinesis data streams",
    "start": "431340",
    "end": "433860"
  },
  {
    "text": "into Amazon Redshift as\nfrequently as you like.",
    "start": "433860",
    "end": "437129"
  },
  {
    "text": "Sam will now show you you how to schedule",
    "start": "437130",
    "end": "439240"
  },
  {
    "text": "the refresh materialized view query",
    "start": "439240",
    "end": "441599"
  },
  {
    "text": "using the Redshift query editor.",
    "start": "441600",
    "end": "443283"
  },
  {
    "start": "444000",
    "end": "593000"
  },
  {
    "text": "He'll navigate to the\nAmazon Redshift console",
    "start": "444640",
    "end": "447210"
  },
  {
    "text": "and click on his Redshift cluster.",
    "start": "447210",
    "end": "450150"
  },
  {
    "text": "From there, he'll click the\nQuery Data dropdown button",
    "start": "450150",
    "end": "452710"
  },
  {
    "text": "on the top right of the page",
    "start": "452710",
    "end": "454360"
  },
  {
    "text": "and select Query in query editor.",
    "start": "454360",
    "end": "456389"
  },
  {
    "text": "Here he'll connect to his Redshift cluster",
    "start": "456390",
    "end": "459230"
  },
  {
    "text": "and in the query editor,",
    "start": "459230",
    "end": "460880"
  },
  {
    "text": "he'll paste the refresh\nmaterialized view command",
    "start": "460880",
    "end": "463840"
  },
  {
    "text": "for a specific materialized\nview that he created earlier.",
    "start": "463840",
    "end": "467470"
  },
  {
    "text": "He'll then click on Schedule",
    "start": "467470",
    "end": "469030"
  },
  {
    "text": "and provide all the necessary permissions,",
    "start": "469030",
    "end": "471160"
  },
  {
    "text": "as well as scheduling options.",
    "start": "471160",
    "end": "473080"
  },
  {
    "text": "In this case, he's going to\nschedule the materialized view",
    "start": "473080",
    "end": "475729"
  },
  {
    "text": "to refresh every two minutes",
    "start": "475730",
    "end": "477770"
  },
  {
    "text": "and click on Save Changes.",
    "start": "477770",
    "end": "479193"
  },
  {
    "text": "Now this materialized\nview will be refreshed",
    "start": "480330",
    "end": "482470"
  },
  {
    "text": "every two minutes by Amazon Redshift",
    "start": "482470",
    "end": "485180"
  },
  {
    "text": "which means Redshift will\npull in all the new data",
    "start": "485180",
    "end": "488320"
  },
  {
    "text": "that was published to\nthe Kinesis data streams",
    "start": "488320",
    "end": "490620"
  },
  {
    "text": "since the last time the\nmaterialized view was refreshed.",
    "start": "490620",
    "end": "494673"
  },
  {
    "text": "Now Sam will run a query against\nthe new materialized view",
    "start": "497030",
    "end": "501180"
  },
  {
    "text": "to aggregate the data.",
    "start": "501180",
    "end": "502593"
  },
  {
    "text": "In this query, he is\nconverting the connection time",
    "start": "503590",
    "end": "507310"
  },
  {
    "text": "from the materialized view\nto a timestamp data type.",
    "start": "507310",
    "end": "510490"
  },
  {
    "text": "This is because timestamp data types",
    "start": "510490",
    "end": "513610"
  },
  {
    "text": "are not currently supported\nfor the materialized view.",
    "start": "513610",
    "end": "516880"
  },
  {
    "text": "He is also adding up the\nkilowatt hours delivered",
    "start": "516880",
    "end": "520110"
  },
  {
    "text": "as the energy consumed",
    "start": "520110",
    "end": "522789"
  },
  {
    "text": "and counting the distinct users.",
    "start": "522790",
    "end": "525779"
  },
  {
    "text": "He's also filtering the data",
    "start": "525780",
    "end": "527590"
  },
  {
    "text": "to only look at the last\nfive minutes of activity.",
    "start": "527590",
    "end": "531060"
  },
  {
    "text": "He will now run the query",
    "start": "531060",
    "end": "533000"
  },
  {
    "text": "and the result will display how many users",
    "start": "533000",
    "end": "535670"
  },
  {
    "text": "have connected to the charging station",
    "start": "535670",
    "end": "538320"
  },
  {
    "text": "and how much energy they\nhave consumed per second",
    "start": "538320",
    "end": "541740"
  },
  {
    "text": "over the past five minutes.",
    "start": "541740",
    "end": "544149"
  },
  {
    "text": "We can see in the result that\neach row represents a second",
    "start": "544150",
    "end": "549150"
  },
  {
    "text": "and we can see the total number of users",
    "start": "549820",
    "end": "551790"
  },
  {
    "text": "connected to the charging station,",
    "start": "551790",
    "end": "554370"
  },
  {
    "text": "along with a total energy\nconsumed in kilowatt hours.",
    "start": "554370",
    "end": "558190"
  },
  {
    "text": "Sam will now create a new view",
    "start": "558190",
    "end": "560650"
  },
  {
    "text": "on top of the materialized view",
    "start": "560650",
    "end": "563400"
  },
  {
    "text": "to convert the connection time field",
    "start": "563400",
    "end": "566390"
  },
  {
    "text": "from a string to a timestamp data type.",
    "start": "566390",
    "end": "569580"
  },
  {
    "text": "Here he can apply other business logic",
    "start": "569580",
    "end": "571680"
  },
  {
    "text": "as well in this view if you would like to",
    "start": "571680",
    "end": "574089"
  },
  {
    "text": "and use it for his downstream analytics.",
    "start": "575030",
    "end": "578890"
  },
  {
    "text": "This was a demonstration\nof how you can use",
    "start": "578890",
    "end": "581130"
  },
  {
    "text": "the new Streaming Ingestion\nfeature of Amazon Redshift",
    "start": "581130",
    "end": "584390"
  },
  {
    "text": "to ingest streaming data\nfrom Kinesis data streams",
    "start": "584390",
    "end": "588140"
  },
  {
    "text": "into Amazon Redshift directly.",
    "start": "588140",
    "end": "591100"
  },
  {
    "text": "Thank you all for watching.",
    "start": "591100",
    "end": "592449"
  }
]