[
  {
    "text": "okay it's 4:15 so we'll go ahead and get started my name's Nate Slater I'm an AWS",
    "start": "2840",
    "end": "9330"
  },
  {
    "text": "Solutions Architect and today we're gonna be talking about Amazon DynamoDB for big data and unlike many of the",
    "start": "9330",
    "end": "16680"
  },
  {
    "text": "presentations you probably sat through today we're actually going to take a hands-on look at this so I'll be doing",
    "start": "16680",
    "end": "22230"
  },
  {
    "text": "some live demonstrations of Amazon DynamoDB and hopefully I will not regret",
    "start": "22230",
    "end": "27480"
  },
  {
    "text": "the decision to do a live demo at this session okay so today we're gonna focus",
    "start": "27480",
    "end": "35460"
  },
  {
    "text": "on the how and not the what a lot of the sessions that we present here which have",
    "start": "35460",
    "end": "40770"
  },
  {
    "text": "a lot of great content tend to focus on what you can do with AWS what customers have done with AWS today we're actually",
    "start": "40770",
    "end": "46500"
  },
  {
    "text": "gonna focus on how you do things with AWS we're also going to learn how ad bus",
    "start": "46500",
    "end": "52680"
  },
  {
    "text": "services abstract some of the complexity of processing big data without having to",
    "start": "52680",
    "end": "58079"
  },
  {
    "text": "sacrifice power or scale or even in some cases control and then we're gonna also",
    "start": "58079",
    "end": "63449"
  },
  {
    "text": "demonstrate how combinations of services from the AWS data ecosystem can be used to create feature-rich systems for",
    "start": "63449",
    "end": "69600"
  },
  {
    "text": "analyzing data it's kind of a mouthful but basically what that means is that you know systems like Amazon DynamoDB",
    "start": "69600",
    "end": "75360"
  },
  {
    "text": "rarely live in isolation right there they're our most powerful when you combine them with other AWS services and",
    "start": "75360",
    "end": "81780"
  },
  {
    "text": "we're gonna go through several scenarios today so let's first just by start by",
    "start": "81780",
    "end": "88229"
  },
  {
    "text": "asking a simple question what is Big Data and and like many technology catchphrases this is kind of a loaded",
    "start": "88229",
    "end": "94439"
  },
  {
    "text": "question right depending on who you ask you might get a different answer but here at Amazon we generally think of it",
    "start": "94439",
    "end": "102240"
  },
  {
    "text": "across two sort of dimensions or characteristics Big Data obviously has size but it also has velocity right it's",
    "start": "102240",
    "end": "109530"
  },
  {
    "text": "not so much only the size of the data you're dealing with it's also how fast that data set is growing so quantity of",
    "start": "109530",
    "end": "118979"
  },
  {
    "text": "data is increasing at a rapid rate we're starting to see more and more that raw data from all kinds of different sources",
    "start": "118979",
    "end": "125520"
  },
  {
    "text": "are being used to answer key business questions log files that's probably the most mature",
    "start": "125520",
    "end": "131380"
  },
  {
    "text": "sort of scenario for big data processing by processing all this log date of your",
    "start": "131380",
    "end": "136870"
  },
  {
    "text": "applications are generating your web log data you can get quite a bit of good information about you know who's using",
    "start": "136870",
    "end": "142840"
  },
  {
    "text": "your application how they're using them application performance monitoring another area that's become increasingly",
    "start": "142840",
    "end": "148480"
  },
  {
    "text": "important over the last several years as apps have moved from the desktop to",
    "start": "148480",
    "end": "153910"
  },
  {
    "text": "mobile devices to tablets you basically have all of this information that you",
    "start": "153910",
    "end": "159730"
  },
  {
    "text": "can look at to determine how well is my app performing you know what kind of experience is a user on an iPad on a cellular network getting versa",
    "start": "159730",
    "end": "166480"
  },
  {
    "text": "versus someone who's you know attached to a desktop at their office you also have application metrics right you can",
    "start": "166480",
    "end": "173200"
  },
  {
    "text": "do things like a be testing right you can roll out subsets of features and then compare the response that you see",
    "start": "173200",
    "end": "179650"
  },
  {
    "text": "users giving to those new features to determine you know whether or not it's something you want to move forward with and then security obviously very very",
    "start": "179650",
    "end": "187030"
  },
  {
    "text": "important right as you a gregarious data oftentimes you can look through it and",
    "start": "187030",
    "end": "192640"
  },
  {
    "text": "do some pattern detection pattern recognition to see you know hey are we seeing patterns of access that may",
    "start": "192640",
    "end": "197950"
  },
  {
    "text": "represent a threat so this growth of",
    "start": "197950",
    "end": "203500"
  },
  {
    "text": "data means that the flow of data is moving at an ever-faster rate so mega megabytes per second is pretty normal",
    "start": "203500",
    "end": "208780"
  },
  {
    "text": "with big data right if you're familiar with our Kinesis service the write throughput to a single shard on a stream",
    "start": "208780",
    "end": "215920"
  },
  {
    "text": "is is one megabyte per second so that's sort of just like where you start right and gigabytes per second are becoming",
    "start": "215920",
    "end": "221709"
  },
  {
    "text": "increasingly common right and you know part of the reason for this is that the number of connected users is growing at",
    "start": "221709",
    "end": "227800"
  },
  {
    "text": "an amazing rate there are some estimates out there today that estimate there'll be 75 billion connected devices by 2020",
    "start": "227800",
    "end": "234940"
  },
  {
    "text": "and that includes not only the devices we know about today phones computers laptops but also things like smart",
    "start": "234940",
    "end": "242890"
  },
  {
    "text": "devices right Internet of Things type devices and when you're dealing with that kind of scale you know 10 to the",
    "start": "242890",
    "end": "248830"
  },
  {
    "text": "5th 10 to the 6 transactions per second are not uncommon at all right you need to be able to support a hundred thousand a million transactions per second so you",
    "start": "248830",
    "end": "259090"
  },
  {
    "text": "know this diagram is showing here you know where I feel at least that the DynamoDB really hits the sweet spot of",
    "start": "259090",
    "end": "265010"
  },
  {
    "text": "because it can handle both size and velocity and on top of that it also has",
    "start": "265010",
    "end": "270620"
  },
  {
    "text": "a very useful structure at its core it's a key value store but you can store",
    "start": "270620",
    "end": "275630"
  },
  {
    "text": "complex types in it as well such as JSON documents and so it really is is unique",
    "start": "275630",
    "end": "280850"
  },
  {
    "text": "I think in the Big Data no sequel solution an ecosystem in that it can",
    "start": "280850",
    "end": "288290"
  },
  {
    "text": "handle both size and velocity right it's it's not like something like a memory cache which is really really good at",
    "start": "288290",
    "end": "294140"
  },
  {
    "text": "velocity but not as good at size right you're not going to put gigabytes of data into RAM at least not today",
    "start": "294140",
    "end": "301270"
  },
  {
    "text": "so transactional data processing is where a dynamo really shines it can handle high concurrency it does have",
    "start": "301840",
    "end": "308480"
  },
  {
    "text": "strong consistency you can specify the consistency level that you want when you're getting reads of data from dynamo",
    "start": "308480",
    "end": "314540"
  },
  {
    "text": "it supports atomic updates of single items so if you're changing a single item in a table you can do that",
    "start": "314540",
    "end": "319970"
  },
  {
    "text": "atomically you can use conditional updates to D dupe your data and for optimistic concurrency right you can you",
    "start": "319970",
    "end": "326150"
  },
  {
    "text": "can update only if you know a value hasn't changed since the last time you read the data as I previously mentioned",
    "start": "326150",
    "end": "332600"
  },
  {
    "text": "you can do both key value and JSON document schemas and it is capable of",
    "start": "332600",
    "end": "339170"
  },
  {
    "text": "large table size I mean I know a lot of folks probably don't think of dynamo as being capable of really large tables but",
    "start": "339170",
    "end": "344570"
  },
  {
    "text": "it can write the part the way dynamo partitions data you can go out to terabytes of size I mean that's a pretty",
    "start": "344570",
    "end": "350030"
  },
  {
    "text": "big table it may not be the most cost-effective way to store that data and we can talk about some of those",
    "start": "350030",
    "end": "355370"
  },
  {
    "text": "those use cases later but it will support it okay so let's jump in to our",
    "start": "355370",
    "end": "362660"
  },
  {
    "text": "demos now and hope they work so the first demo we're going to look at is",
    "start": "362660",
    "end": "367910"
  },
  {
    "text": "going to use s3 notifications which are feature of s3 that generate events when",
    "start": "367910",
    "end": "374120"
  },
  {
    "text": "certain things happen in your s3 bucket and we're going to process those events from s3 with a lambda function the",
    "start": "374120",
    "end": "381830"
  },
  {
    "text": "lambda function is going to store some data into DynamoDB and then we're gonna",
    "start": "381830",
    "end": "387860"
  },
  {
    "text": "have a second lambda function that's going to read off the DynamoDB stream that's attached to the table that we're",
    "start": "387860",
    "end": "393800"
  },
  {
    "text": "updating and the data from that dynamodb stream that's being processed by the lambda function",
    "start": "393800",
    "end": "398870"
  },
  {
    "text": "is going to get passed into an elasticsearch instance so in this case what we're doing is we're basically",
    "start": "398870",
    "end": "404080"
  },
  {
    "text": "storing raw data and dinamo and then making it searchable with elasticsearch",
    "start": "404080",
    "end": "410710"
  },
  {
    "text": "and so the use case here may be something we have a large number of digital audio files in s3 right maybe",
    "start": "410710",
    "end": "415970"
  },
  {
    "text": "we're building a streaming media service and we want to be able to make those files searchable by all the things that",
    "start": "415970",
    "end": "421250"
  },
  {
    "text": "people like to search for when they're searching for music right the artist the album the title the genre and we're",
    "start": "421250",
    "end": "428270"
  },
  {
    "text": "gonna use DynamoDB as the primary data store for all that metadata and then we'll be able to do queries and indexing",
    "start": "428270",
    "end": "434780"
  },
  {
    "text": "using elastic search so why don't why are we putting data in two places right that seems a little odd well elastic",
    "start": "434780",
    "end": "440780"
  },
  {
    "text": "search really isn't designed with the same availability and durability as dynamo right so we want to use dynamo as",
    "start": "440780",
    "end": "446960"
  },
  {
    "text": "the primary data store and elastic search you know is going to be used to provide a very rich query capability on",
    "start": "446960",
    "end": "453770"
  },
  {
    "text": "top of that data much richer than what you can do with dynamo DB today using global secondary indexes and local",
    "start": "453770",
    "end": "460520"
  },
  {
    "text": "secondary indexes so how do we do this well the first thing we're gonna do is",
    "start": "460520",
    "end": "466040"
  },
  {
    "text": "we're gonna create a lambda function that reads the metadata from the id3 tag and it sorts it into a dynamo table then",
    "start": "466040",
    "end": "472070"
  },
  {
    "text": "we're gonna enable s3 notifications on the bucket storing the audio files we're gonna then enable streams on the Dynamo",
    "start": "472070",
    "end": "478040"
  },
  {
    "text": "table we'll create a second lambda function that takes the metadata in dynamo dB that's being put into the",
    "start": "478040",
    "end": "483650"
  },
  {
    "text": "stream and calls the index operation on elasticsearch and then finally you know",
    "start": "483650",
    "end": "490070"
  },
  {
    "text": "make the wire this all together we just have to enable that that DynamoDB stream",
    "start": "490070",
    "end": "495080"
  },
  {
    "text": "as the event source for that second lambda function okay so let's jump right",
    "start": "495080",
    "end": "500660"
  },
  {
    "text": "in here so here I am in the lambda console and",
    "start": "500660",
    "end": "505880"
  },
  {
    "text": "the first thing we're gonna do is we're gonna go ahead and we're gonna create the lambda function I will assume in as",
    "start": "505880",
    "end": "511820"
  },
  {
    "text": "I'm as I'm doing this on the parts that are sort of the important parts we're gonna go ahead and skip this first step",
    "start": "511820",
    "end": "518240"
  },
  {
    "text": "we don't need to use a blueprint so let's call our lambda function something like process audio file and the",
    "start": "518240",
    "end": "528500"
  },
  {
    "text": "description will be read the id3 tag data and insert into d DB okay",
    "start": "528500",
    "end": "536649"
  },
  {
    "text": "now we're gonna use the node.js runtime and I'm going to go ahead and upload this from s3 I've already staged my",
    "start": "536649",
    "end": "544810"
  },
  {
    "text": "lambda function there and let's do this",
    "start": "544810",
    "end": "561519"
  },
  {
    "text": "so I'm specifying the s3 bucket and I'm specifying the key of the zip file that",
    "start": "562899",
    "end": "568069"
  },
  {
    "text": "contains not only my lambda node.js code but also all of the dependencies right when you're when you're deploying a",
    "start": "568069",
    "end": "574819"
  },
  {
    "text": "lambda function that has NPM node module dependencies you have to package it up as a zip file I'll go ahead down here",
    "start": "574819",
    "end": "583990"
  },
  {
    "text": "this is just the name of the JavaScript file that contains the method we're",
    "start": "586779",
    "end": "592190"
  },
  {
    "text": "going to execute let me go ahead and zoom out so I can see the I'm gonna go",
    "start": "592190",
    "end": "598910"
  },
  {
    "text": "ahead and select the role I've pre created the role we're gonna use here so lambda basically operates in the same",
    "start": "598910",
    "end": "605899"
  },
  {
    "text": "way as ec2 does when you assign an ec2 instance to an instance profile right you put the instance in an instance",
    "start": "605899",
    "end": "611540"
  },
  {
    "text": "profile the profile is associated with an I am role and that role defines what that instance can do right well all the",
    "start": "611540",
    "end": "617870"
  },
  {
    "text": "other AWS API calls that it can make lam does the exact same way right so we're",
    "start": "617870",
    "end": "622880"
  },
  {
    "text": "gonna basically we're going to need to do a get object from s3 and we're gonna need to do a put item into dynamodb so",
    "start": "622880",
    "end": "628370"
  },
  {
    "text": "this role allows the lambda function to do that the bottom part of the screen here I'm gonna bump up the memory a",
    "start": "628370",
    "end": "635060"
  },
  {
    "text": "little bit the id3 tag reader does use a little bit of memory I'm gonna make the timeout 30 seconds that doesn't mean",
    "start": "635060",
    "end": "641930"
  },
  {
    "text": "this is gonna take 30 seconds to run it just means I don't want lambda to timeout my function until after 30",
    "start": "641930",
    "end": "647569"
  },
  {
    "text": "seconds has elapsed we are gonna be downloading files from s3 in the lambda",
    "start": "647569",
    "end": "652670"
  },
  {
    "text": "function it's not gonna take 30 seconds but you know it we want to have enough time for it to execute so I go ahead and",
    "start": "652670",
    "end": "659000"
  },
  {
    "text": "I hit Dave this is basically what we're",
    "start": "659000",
    "end": "664070"
  },
  {
    "text": "looking at here we've got a process audiophile function it's running nodejs",
    "start": "664070",
    "end": "670000"
  },
  {
    "text": "pretty much just a summarization of what we just did let's go ahead and create it",
    "start": "670000",
    "end": "677710"
  },
  {
    "text": "okay great so our function has been created and now we want to test this",
    "start": "678250",
    "end": "685250"
  },
  {
    "text": "right before we actually start wiring in the s3 notifications it's useful to test",
    "start": "685250",
    "end": "690500"
  },
  {
    "text": "our function to see if it's working so one of the nice features of lamda is you can configure these sample events and",
    "start": "690500",
    "end": "696200"
  },
  {
    "text": "this is basically the JSON format that the s3 notification uses and you can see",
    "start": "696200",
    "end": "705290"
  },
  {
    "text": "it has the name of the bucket and the name of the object right so when an object is put to s3 it generates this",
    "start": "705290",
    "end": "712120"
  },
  {
    "text": "JSON and passes that in to the lambda function right so I can actually use this to test right here I click Submit",
    "start": "712120",
    "end": "723010"
  },
  {
    "text": "okay and it looks like you could see the",
    "start": "724150",
    "end": "729770"
  },
  {
    "text": "little green checkmark down here it says execution results succeeded and it you",
    "start": "729770",
    "end": "735170"
  },
  {
    "text": "can see we even have some logging information down here let me zoom in",
    "start": "735170",
    "end": "740050"
  },
  {
    "text": "right you can see this this is some log information that I emitted in the lambda function as I was reading the id3 tag",
    "start": "740230",
    "end": "746840"
  },
  {
    "text": "data so everything looks good there so now as a result of this the end result",
    "start": "746840",
    "end": "752060"
  },
  {
    "text": "is you know we want to have data and DynamoDB so if I go over to dynamo DB console and I click on our audio file",
    "start": "752060",
    "end": "758540"
  },
  {
    "text": "table we should see this item in there and lo and behold we do right so our",
    "start": "758540",
    "end": "764030"
  },
  {
    "text": "test worked so now it's time to actually make this go live with data that's being",
    "start": "764030",
    "end": "769490"
  },
  {
    "text": "written to s3 okay so what do we need to do that well let's first go over into s3",
    "start": "769490",
    "end": "776540"
  },
  {
    "text": "and set up the event notification and so I have my bucket here let me just",
    "start": "776540",
    "end": "781940"
  },
  {
    "text": "refresh the screen and I'm gonna go over here to the events section and again",
    "start": "781940",
    "end": "788720"
  },
  {
    "text": "I'll zoom in once I start typing all the stuff in here and I'm going to click Add notification and so first we're gonna give it a name",
    "start": "788720",
    "end": "796810"
  },
  {
    "text": "let's call it put audio file event now we're gonna specify the s3 actions",
    "start": "796810",
    "end": "804580"
  },
  {
    "text": "that we want to trigger this event and so in this case I'm gonna select object created all which will basically trigger",
    "start": "804580",
    "end": "810760"
  },
  {
    "text": "the event if we do an s3 post a put a copy or a complete multi-part upload I'm",
    "start": "810760",
    "end": "818230"
  },
  {
    "text": "gonna limit this to only fire when we have objects that are being created in",
    "start": "818230",
    "end": "824770"
  },
  {
    "text": "the audio files folder if you will write s3 doesn't really have folders this is a",
    "start": "824770",
    "end": "830200"
  },
  {
    "text": "prefix so basically any object that has this prefix will trigger this event and",
    "start": "830200",
    "end": "836470"
  },
  {
    "text": "then finally I'm going to select our lambda function that we created in the previous step and let me zoom in a bit",
    "start": "836470",
    "end": "842890"
  },
  {
    "text": "so you guys can see so right through the console here we've now set everything up that we need to wire our s3 notification",
    "start": "842890",
    "end": "851350"
  },
  {
    "text": "to our lambda function I click Save that looks good we can double check this now",
    "start": "851350",
    "end": "857860"
  },
  {
    "text": "if I go over to lambda and I go to my",
    "start": "857860",
    "end": "863800"
  },
  {
    "text": "function process audio file right here and I click event sources we should now",
    "start": "863800",
    "end": "869410"
  },
  {
    "text": "see our s3 guess it's kind of hard to see but basically you can see that our",
    "start": "869410",
    "end": "875260"
  },
  {
    "text": "our s3 bucket is now an advanced source to this so let me now go ahead and run a",
    "start": "875260",
    "end": "880300"
  },
  {
    "text": "couple of commands here first thing I'm gonna do is I'm gonna log in to an ec2",
    "start": "880300",
    "end": "886360"
  },
  {
    "text": "instance that has the audio files",
    "start": "886360",
    "end": "892860"
  },
  {
    "text": "okay and now I'm gonna go ahead and I'm just gonna use the AWS CLI here to just",
    "start": "895310",
    "end": "905220"
  },
  {
    "text": "copy a bunch of audio files that are attached to an EBS volume attached to this instance are stored in the EBS from",
    "start": "905220",
    "end": "911459"
  },
  {
    "text": "into to s3 and you can see I just ran it",
    "start": "911459",
    "end": "917279"
  },
  {
    "text": "and let's run another one for good measure here okay great and you can see",
    "start": "917279",
    "end": "931410"
  },
  {
    "text": "it did a multi-part upload by default that's why I had to you know use the the",
    "start": "931410",
    "end": "936740"
  },
  {
    "text": "object created action in my s3 notification because if you're uploading large files obviously we don't want the",
    "start": "936740",
    "end": "942810"
  },
  {
    "text": "event to fire until that multi-part upload complete action has triggered and if we go back over now let's first check",
    "start": "942810",
    "end": "950220"
  },
  {
    "text": "dynamo DB I'll scan my table and you can see we have a whole lot more data in",
    "start": "950220",
    "end": "956190"
  },
  {
    "text": "there so it looks like our lambda function worked I'll go over to lamda one nice feature of lamda is I can",
    "start": "956190",
    "end": "963660"
  },
  {
    "text": "actually take a look at some of the cloud watch metrics that shows sort of",
    "start": "963660",
    "end": "969170"
  },
  {
    "text": "useful information about lambda such as how many times has been invoked the the average duration of each invocation and",
    "start": "969170",
    "end": "975810"
  },
  {
    "text": "you can actually view the logs and if we go over here and we look at this guy",
    "start": "975810",
    "end": "981029"
  },
  {
    "text": "right here you can see you know this is the log output of the lambda function so",
    "start": "981029",
    "end": "990300"
  },
  {
    "text": "in this case we're invoking this lambda function once for every object that we just put into s3 it doesn't bundle them",
    "start": "990300",
    "end": "997050"
  },
  {
    "text": "together but we'll see with DynamoDB streams it can also process events in batches okay so we're reading the",
    "start": "997050",
    "end": "1006920"
  },
  {
    "text": "metadata in it's going into dynamo DB we now want to make that searchable via",
    "start": "1006920",
    "end": "1012740"
  },
  {
    "text": "elastic search so we're gonna do that with the second lambda function I've already created this one it's called",
    "start": "1012740",
    "end": "1018019"
  },
  {
    "text": "process audio file stream the process for creating this was ABS you know totally identical to what we just did so",
    "start": "1018019",
    "end": "1023990"
  },
  {
    "text": "I didn't want to consume time walking through it again so this this has already been created",
    "start": "1023990",
    "end": "1029000"
  },
  {
    "text": "so it's all ready to go what we do need to do though is we need to make sure we have the stream setup on the DynamoDB",
    "start": "1029000",
    "end": "1035120"
  },
  {
    "text": "table and so if I go back over here to my dynamo console and I click on my audiophile table you should see if I",
    "start": "1035120",
    "end": "1042199"
  },
  {
    "text": "click on the stream tab that we do have one this one on the top that says enabled this is my active stream on this",
    "start": "1042199",
    "end": "1051169"
  },
  {
    "text": "table the only reason I have a bunch of disabled ones is because I've been testing this and so I keep deleting the",
    "start": "1051169",
    "end": "1056419"
  },
  {
    "text": "stream in reality it dynamodb streams are like Kinesis they're there they live for 24 hours right",
    "start": "1056419",
    "end": "1061820"
  },
  {
    "text": "so I've got my stream let me now go back",
    "start": "1061820",
    "end": "1068809"
  },
  {
    "text": "into lambda and if we go over here to this tab you can see sure enough my",
    "start": "1068809",
    "end": "1074540"
  },
  {
    "text": "stream is enabled as the event source for this lambda function and if we",
    "start": "1074540",
    "end": "1084620"
  },
  {
    "text": "scroll out you'll see even over here on the side it may be a little bit difficult it actually says last result okay so I just uploaded a bunch of files",
    "start": "1084620",
    "end": "1091400"
  },
  {
    "text": "so this actually has already run so let's go ahead and just do a couple more files here let's let it run I have",
    "start": "1091400",
    "end": "1098600"
  },
  {
    "text": "another there we go I'm gonna just copy",
    "start": "1098600",
    "end": "1105320"
  },
  {
    "text": "a whole bunch of files and while that's running we can go back over here and we",
    "start": "1105320",
    "end": "1112040"
  },
  {
    "text": "can look at monitoring we can view our logs and cloud watch okay and so this is",
    "start": "1112040",
    "end": "1122179"
  },
  {
    "text": "what this event looks like let me zoom in so this is this is the JSON format",
    "start": "1122179",
    "end": "1130520"
  },
  {
    "text": "that the DynamoDB stream passes in so it passes me in the key of the item and",
    "start": "1130520",
    "end": "1137419"
  },
  {
    "text": "then I'm doing something the stream that I enabled is what's called a new image stream so I'm only seeing the new data",
    "start": "1137419",
    "end": "1144380"
  },
  {
    "text": "I'm not seeing the the the record prior to the update I'm only seeing what it looked like after the update but you can",
    "start": "1144380",
    "end": "1151640"
  },
  {
    "text": "enable a DynamoDB stream to show you both right you I would have just to JSON blocks in there one would be called old",
    "start": "1151640",
    "end": "1157760"
  },
  {
    "text": "image that was the pre update state and then is called one's called new image that's the post they'd say I don't care about the pre",
    "start": "1157760",
    "end": "1163640"
  },
  {
    "text": "update state in this case I only care about the new image and so you this is this is what's been passed in to me and",
    "start": "1163640",
    "end": "1170270"
  },
  {
    "text": "so to do the elasticsearch you can see",
    "start": "1170270",
    "end": "1175960"
  },
  {
    "text": "you can see some debugging information what I'm doing is I'm basically taking that new image JSON turning it into a",
    "start": "1175960",
    "end": "1184160"
  },
  {
    "text": "JSON document that's suitable for indexing in elasticsearch and then calling the elasticsearch API and the",
    "start": "1184160",
    "end": "1189830"
  },
  {
    "text": "last a search has a REST API so it's very simple to call it from JavaScript and we'll take a look at the code for",
    "start": "1189830",
    "end": "1195650"
  },
  {
    "text": "these functions in a second okay so our functions appear to be executing",
    "start": "1195650",
    "end": "1201980"
  },
  {
    "text": "properly and I can even show you if I go over here and very quickly let me",
    "start": "1201980",
    "end": "1207409"
  },
  {
    "text": "actually quickly go back here let me just get an ID of one of these guys this",
    "start": "1207409",
    "end": "1213799"
  },
  {
    "text": "guy about and I think if I just do something like see URL",
    "start": "1213799",
    "end": "1221799"
  },
  {
    "text": "yeah a little hard to see sort of JSON",
    "start": "1233260",
    "end": "1240200"
  },
  {
    "text": "that comes back from elasticsearch but you can see this I just fetched this index document from elasticsearch and",
    "start": "1240200",
    "end": "1247100"
  },
  {
    "text": "you can see lo and behold it has all of the fields that we have grabbed from the",
    "start": "1247100",
    "end": "1253039"
  },
  {
    "text": "metadata from the s3 file and stored in dynamo ok so let's take a quick look at",
    "start": "1253039",
    "end": "1260870"
  },
  {
    "text": "the source code for these functions first one this is the function that",
    "start": "1260870",
    "end": "1269980"
  },
  {
    "text": "processes the object from s3 the early",
    "start": "1269980",
    "end": "1276440"
  },
  {
    "text": "rather the Anoat the event from s3 and you can see the the real sort of stuff",
    "start": "1276440",
    "end": "1281510"
  },
  {
    "text": "that happens here is we first we call s3 get object I'll zoom in on this in a second",
    "start": "1281510",
    "end": "1287559"
  },
  {
    "text": "you can see a lo me it's kind of hard to see if I go back up maybe we can oops",
    "start": "1293049",
    "end": "1301210"
  },
  {
    "text": "so this first block here is s3 get object right all I'm doing is just getting the s3 object based on the key",
    "start": "1301630",
    "end": "1311150"
  },
  {
    "text": "and the bucket that was passed in to me the second step is I do the id3 read",
    "start": "1311150",
    "end": "1317870"
  },
  {
    "text": "which is not specific at all to AWS so we'll skip that and then finally in this",
    "start": "1317870",
    "end": "1323059"
  },
  {
    "text": "last function this is where I take the metadata that was extracted and I",
    "start": "1323059",
    "end": "1330020"
  },
  {
    "text": "inserted into dynamo and I'm using the dynamo DB document interface to do that so they said I construct a JSON document",
    "start": "1330020",
    "end": "1336290"
  },
  {
    "text": "and then I just do a put item into dynamo so pretty pretty straightforward",
    "start": "1336290",
    "end": "1341690"
  },
  {
    "text": "right not uh not a lot of code I think this entire this entire thing is 130 lines of code and quite a bit of this is",
    "start": "1341690",
    "end": "1348110"
  },
  {
    "text": "actually just doing the waterfall the async waterfall because node.js is asynchronous I need to make sure I serialize all these operations so",
    "start": "1348110",
    "end": "1354710"
  },
  {
    "text": "there's a lot of sort of boilerplate stuff in here so let's take a quick look now at the stream processing this one is",
    "start": "1354710",
    "end": "1364820"
  },
  {
    "text": "some ways even simpler I'm basically passed in a batch of those DynamoDB",
    "start": "1364820",
    "end": "1370220"
  },
  {
    "text": "update stream JSON documents and I just",
    "start": "1370220",
    "end": "1377870"
  },
  {
    "text": "essentially convert the that update",
    "start": "1377870",
    "end": "1383030"
  },
  {
    "text": "stream JSON which is uses something called the dynamo DB line format I",
    "start": "1383030",
    "end": "1388340"
  },
  {
    "text": "convert it to just a plain old JavaScript object and I call audio file",
    "start": "1388340",
    "end": "1393950"
  },
  {
    "text": "I set a bunch of properties on that and then down here I call the index method on the elasticsearch client last I first",
    "start": "1393950",
    "end": "1401750"
  },
  {
    "text": "has a nice node j/s SDK so I'm using that here and so that's it that's",
    "start": "1401750",
    "end": "1407240"
  },
  {
    "text": "basically you know the the code to essentially build a searchable index of",
    "start": "1407240",
    "end": "1414530"
  },
  {
    "text": "audio files and again this one is only 85 lines so you know pretty pretty straightforward ok so let's go back to",
    "start": "1414530",
    "end": "1421909"
  },
  {
    "text": "our presentation",
    "start": "1421909",
    "end": "1424360"
  },
  {
    "text": "key takeaways dynamo plus elasticsearch you get the durability scalability and",
    "start": "1433700",
    "end": "1439110"
  },
  {
    "text": "high high availability of dynamo with the rich query capable is Vlasic search kind of a nice combination of two",
    "start": "1439110",
    "end": "1444179"
  },
  {
    "text": "different data data stores and in lambda functions are sort of the glue that makes this all work right you can",
    "start": "1444179",
    "end": "1449700"
  },
  {
    "text": "basically have lambda functions that respond to various different events and then take action when those events occur",
    "start": "1449700",
    "end": "1455309"
  },
  {
    "text": "and the beautiful thing about lambda is you're not managing any computer infrastructure at all for folks that have been building on AWS for a number",
    "start": "1455309",
    "end": "1462090"
  },
  {
    "text": "of years you could do this entire thing with SQS and ec2 instances that are",
    "start": "1462090",
    "end": "1467220"
  },
  {
    "text": "sitting on the SQL queues and D queuing tasks but we don't have to run any ec2",
    "start": "1467220",
    "end": "1472320"
  },
  {
    "text": "at all for this right the fact the only easy - that I'm running on this is the elasticsearch instance and when I built",
    "start": "1472320",
    "end": "1478650"
  },
  {
    "text": "this the managed elasticsearch that was just announced last week wasn't available so I wouldn't even need to run any ec2 if I were doing this today okay",
    "start": "1478650",
    "end": "1489720"
  },
  {
    "text": "demo number two so here we're gonna look about look at how you can execute queries against multiple data sources",
    "start": "1489720",
    "end": "1495510"
  },
  {
    "text": "using dynamo DB and hive so say for example you have a bunch of items in your Dynamo table and then you have a",
    "start": "1495510",
    "end": "1501659"
  },
  {
    "text": "bunch of other data that lives in text files and you want to you want to basically do an ETL type operation that",
    "start": "1501659",
    "end": "1507059"
  },
  {
    "text": "combines the two data you can do this using Amazon Elastic MapReduce and hive",
    "start": "1507059",
    "end": "1514309"
  },
  {
    "text": "hive for those that aren't familiar is a sequel like language or framework that",
    "start": "1514370",
    "end": "1520320"
  },
  {
    "text": "sits on top of MapReduce so it has a sequel like syntax and what it does is it compiles that down to actual",
    "start": "1520320",
    "end": "1526559"
  },
  {
    "text": "MapReduce jobs that run in your cluster so we're building our streaming media",
    "start": "1526559",
    "end": "1532500"
  },
  {
    "text": "service and we've decided that you know the the the id3 data is good but you know we need a little bit more in here",
    "start": "1532500",
    "end": "1538110"
  },
  {
    "text": "we wanna we want to actually be able to do dynamic playlist generation based on sort of other characteristics of the",
    "start": "1538110",
    "end": "1545520"
  },
  {
    "text": "audio files and so there's this data set that we're hosting on s3 called the",
    "start": "1545520",
    "end": "1551010"
  },
  {
    "text": "million song data set and it's stored in text files and we've got our ID three",
    "start": "1551010",
    "end": "1556649"
  },
  {
    "text": "data stored DynamoDB and the million song data set has a bunch of other metadata about",
    "start": "1556649",
    "end": "1562260"
  },
  {
    "text": "about audio about songs right it has things like the tempo and the key you",
    "start": "1562260",
    "end": "1568260"
  },
  {
    "text": "know it's just some other things that you could use to maybe construct a playlist of songs that sound alike right and what we're gonna do is we're",
    "start": "1568260",
    "end": "1575130"
  },
  {
    "text": "gonna use Amazon EMR with hive to join these two things together in a query so",
    "start": "1575130",
    "end": "1581309"
  },
  {
    "text": "to implement this what you would do is you'd spin up an Amazon email cluster you would create an external hive table",
    "start": "1581309",
    "end": "1587370"
  },
  {
    "text": "using the dynamo DB storage handler you'd create a second external hive table using the Amazon s3 location for",
    "start": "1587370",
    "end": "1594540"
  },
  {
    "text": "the text files that contain the million song data and then you create a query that joins those two external tables",
    "start": "1594540",
    "end": "1599970"
  },
  {
    "text": "together and writes the results back out to s3 and then of course you could very easily load those combined results back",
    "start": "1599970",
    "end": "1607140"
  },
  {
    "text": "into dynamo okay so let's jump into the",
    "start": "1607140",
    "end": "1612660"
  },
  {
    "text": "demo so for this one I have already spun",
    "start": "1612660",
    "end": "1619470"
  },
  {
    "text": "up the EMR cluster just because it can take sometimes 5-10 minutes to bring up",
    "start": "1619470",
    "end": "1625770"
  },
  {
    "text": "the cluster I didn't want to consume that time during the demo just wasted waiting for to come up so I've already brought up the entire cluster it's very",
    "start": "1625770",
    "end": "1631830"
  },
  {
    "text": "very easy to bring up a cluster using the EMR console and very very easy to enable hive it's just an option that you",
    "start": "1631830",
    "end": "1637470"
  },
  {
    "text": "pick when you bring up the cluster so here's my cluster right here and the first thing I'm going to do is I'm gonna",
    "start": "1637470",
    "end": "1643110"
  },
  {
    "text": "log in to this",
    "start": "1643110",
    "end": "1646040"
  },
  {
    "text": "thank you actually I can get it from right here yeah so I'm gonna log into",
    "start": "1651310",
    "end": "1658420"
  },
  {
    "text": "the master node of my EMR cluster let me",
    "start": "1658420",
    "end": "1665170"
  },
  {
    "text": "just go ahead and get rid of the tables if they're already there so I can walk you through creating them",
    "start": "1665170",
    "end": "1671700"
  },
  {
    "text": "oops it's not what I wanted to do there",
    "start": "1672060",
    "end": "1678280"
  },
  {
    "text": "we go",
    "start": "1678280",
    "end": "1680520"
  },
  {
    "text": "okay great so now we're back at the",
    "start": "1688210",
    "end": "1693929"
  },
  {
    "text": "clean state so first thing we're gonna do is we need to define these two",
    "start": "1693929",
    "end": "1699820"
  },
  {
    "text": "external tables that are gonna contain both our dynamodb data in one table and",
    "start": "1699820",
    "end": "1705309"
  },
  {
    "text": "then the million song database table and the other so let's look at the hive syntax for doing that this guy right",
    "start": "1705309",
    "end": "1714340"
  },
  {
    "text": "here is the DynamoDB external table zoom",
    "start": "1714340",
    "end": "1719379"
  },
  {
    "text": "in so pretty pretty straightforward simple syntax if you're used to writing",
    "start": "1719379",
    "end": "1726480"
  },
  {
    "text": "you know DDL in sequel we do a create a create table statement we pass in the",
    "start": "1726480",
    "end": "1733029"
  },
  {
    "text": "the column names and the datatypes of the columns in this case the sort of",
    "start": "1733029",
    "end": "1738179"
  },
  {
    "text": "specifically we're doing here to enable this is what we have to tell hive how is this table stored and what we're using",
    "start": "1738179",
    "end": "1744190"
  },
  {
    "text": "this DynamoDB storage handler class to do this and then the table properties",
    "start": "1744190",
    "end": "1750610"
  },
  {
    "text": "down here this is telling hive how to map the attributes of the items in the",
    "start": "1750610",
    "end": "1756309"
  },
  {
    "text": "dynamo table to the columns we defined up here and it's pretty straightforward I mean you know I encourage you to read",
    "start": "1756309",
    "end": "1761440"
  },
  {
    "text": "read the documentation of this there's there's some pretty good docs on this but you can see you know I have an audio underscore file underscore ID column and",
    "start": "1761440",
    "end": "1768369"
  },
  {
    "text": "I'm mapping that to a an item attribute called audio file ID you know it's pretty pretty straightforward so that's",
    "start": "1768369",
    "end": "1774909"
  },
  {
    "text": "the audio file table then we look at the the metadata table this one's even easier right these these million song",
    "start": "1774909",
    "end": "1780850"
  },
  {
    "text": "database metadata files are CSV files that I created from the million song",
    "start": "1780850",
    "end": "1786539"
  },
  {
    "text": "database Masson database is distributed in a specific database file format and I",
    "start": "1786539",
    "end": "1793119"
  },
  {
    "text": "converted those to CSVs basically and this is you know even similar as simpler",
    "start": "1793119",
    "end": "1798399"
  },
  {
    "text": "right but here we're basically just saying this is our table these are the columns of our table we're dealing with",
    "start": "1798399",
    "end": "1804399"
  },
  {
    "text": "a delimited text file that's delimited by a comma so it's a CSV file and here's",
    "start": "1804399",
    "end": "1809470"
  },
  {
    "text": "the location in s3 where those files live and one thing that's that's worth pointing out is what's really nice about",
    "start": "1809470",
    "end": "1817539"
  },
  {
    "text": "this is if we look at these files in s3",
    "start": "1817539",
    "end": "1822690"
  },
  {
    "text": "it's actually more than one file right so it's really nice is I can build a table that is gonna link to data that's",
    "start": "1822840",
    "end": "1831280"
  },
  {
    "text": "spread across a whole bunch of files in s3 there's all kinds of fancy things you can do to do group bys when you create",
    "start": "1831280",
    "end": "1838240"
  },
  {
    "text": "the tables that you're aggregating data across these files you can filter by by name we're just doing something really simple here and saying hey every CSV file in",
    "start": "1838240",
    "end": "1845320"
  },
  {
    "text": "this prefix of metadata is going to go into the table so let's go ahead now and create these tables I do this by running",
    "start": "1845320",
    "end": "1854169"
  },
  {
    "text": "hive let's create the audiophile table first and you'll see as we go through",
    "start": "1854169",
    "end": "1864429"
  },
  {
    "text": "this demo you know hive is not the fastest query engine right it's really good for sort of batch processing ETL",
    "start": "1864429",
    "end": "1872110"
  },
  {
    "text": "operations it's not necessarily something you would want to do to do real-time looks at your data that being",
    "start": "1872110",
    "end": "1878919"
  },
  {
    "text": "said I will caveat all this by saying I basically am using the default out-of-the-box hive settings I haven't",
    "start": "1878919",
    "end": "1884440"
  },
  {
    "text": "tried to tune it at all so there probably are things you could do to make it make it faster and now I'll run the",
    "start": "1884440",
    "end": "1890890"
  },
  {
    "text": "script that creates the metadata table",
    "start": "1890890",
    "end": "1894510"
  },
  {
    "text": "great oK we've got our two tables created and remember now what we're trying to do is we want to join the id3",
    "start": "1903550",
    "end": "1912100"
  },
  {
    "text": "data that we have in our dynamo table with a million song database data that we have in our metadata table and we do",
    "start": "1912100",
    "end": "1918910"
  },
  {
    "text": "that with a query and the query looks like this a little bit different than",
    "start": "1918910",
    "end": "1928600"
  },
  {
    "text": "what you would do with a normal sequel query but not that much different so the first thing we're gonna do is we're gonna create in another external table",
    "start": "1928600",
    "end": "1935170"
  },
  {
    "text": "to store the results of our query and this is basically the inverse of what we just did with the table creation - for",
    "start": "1935170",
    "end": "1943690"
  },
  {
    "text": "the metadata table right in this case I'm saying create me a table and we're gonna be row format delimited by commas",
    "start": "1943690",
    "end": "1951510"
  },
  {
    "text": "lines are gonna be terminated by a newline character and then put the this",
    "start": "1951510",
    "end": "1956530"
  },
  {
    "text": "table is going to live in this location s3 so what's going to happen is instead of reading the data from s3 in this",
    "start": "1956530",
    "end": "1962080"
  },
  {
    "text": "table we're gonna write data into the table and then store it out to s3 and then the second piece is where we do",
    "start": "1962080",
    "end": "1968200"
  },
  {
    "text": "that right we're gonna do an insert into our joined results table from this select statement and this is exactly",
    "start": "1968200",
    "end": "1973270"
  },
  {
    "text": "like sequel right all I'm doing is I am joining the DynamoDB data on the artist",
    "start": "1973270",
    "end": "1979270"
  },
  {
    "text": "the title and the album which are the shared fields between the dynamo data and the million song database data and",
    "start": "1979270",
    "end": "1986350"
  },
  {
    "text": "then basically storing those results in s3 so let's go ahead and run this query",
    "start": "1986350",
    "end": "1993570"
  },
  {
    "text": "and let's see here",
    "start": "1998149",
    "end": "2003359"
  },
  {
    "text": "this may take a couple of seconds here so now you can see it's printing out some information it's actually running",
    "start": "2007240",
    "end": "2013540"
  },
  {
    "text": "the MapReduce job so we get a whole bunch of log information here doesn't",
    "start": "2013540",
    "end": "2019090"
  },
  {
    "text": "have to do any reducing because we're not doing any group by queries in here",
    "start": "2019090",
    "end": "2024390"
  },
  {
    "text": "and now we're in the mapping phase so",
    "start": "2024390",
    "end": "2033640"
  },
  {
    "text": "you can see if you were doing this in in practice hive is really great if you schedule these things and they run overnight or something and you have your",
    "start": "2033640",
    "end": "2040570"
  },
  {
    "text": "results in the morning okay great so finish took about 32 seconds now if we go to s3 let's go ahead and refresh",
    "start": "2040570",
    "end": "2057179"
  },
  {
    "text": "yep you can see I have a results folder I've got this sort of ugly named thing",
    "start": "2057179",
    "end": "2062830"
  },
  {
    "text": "in here but I'll go ahead and now download that oops",
    "start": "2062830",
    "end": "2070290"
  },
  {
    "text": "that'll work then what is it five 1bb okay let's just go back here that's it and you can see",
    "start": "2070320",
    "end": "2089858"
  },
  {
    "text": "now I have my let me make it bigger I have my joined results right I have the",
    "start": "2089859",
    "end": "2096460"
  },
  {
    "text": "key the audio file key that's in dynamo dB and then I have all of the million song",
    "start": "2096460",
    "end": "2102780"
  },
  {
    "text": "database extra data sort of extra data and I can easily now just reload this in",
    "start": "2102780",
    "end": "2108940"
  },
  {
    "text": "why do I only have seven rows here well remember it's a million song database and apparently there's not a whole lot",
    "start": "2108940",
    "end": "2114160"
  },
  {
    "text": "of overlap between the songs I loaded in and the million song database but you can imagine that there probably are",
    "start": "2114160",
    "end": "2120880"
  },
  {
    "text": "databases that are a lot bigger than a million songs you could get this data for basically any song that you were",
    "start": "2120880",
    "end": "2126130"
  },
  {
    "text": "offering out of your streaming media service so there we have it basically streaming media pipeline using dynamo DB",
    "start": "2126130",
    "end": "2135780"
  },
  {
    "text": "EMR and lambda and s3 okay let's go back to our presentation",
    "start": "2135780",
    "end": "2144109"
  },
  {
    "text": "okay key takeaways EMR is really really really convenient for spinning up Hadoop",
    "start": "2150380",
    "end": "2156770"
  },
  {
    "text": "clusters and running frameworks like hive on top of it and the beautiful thing about you know but even this",
    "start": "2156770",
    "end": "2162620"
  },
  {
    "text": "picture of use cases I could have torn this cluster down when I was done with it I kept it up and running just gonna make sure everything was was working for",
    "start": "2162620",
    "end": "2168620"
  },
  {
    "text": "this demo but under normal circumstances a lot of times what you'll do is you'll spin the cluster up you'll run your job you'll get your results and you tear it",
    "start": "2168620",
    "end": "2174830"
  },
  {
    "text": "down right you don't pay for using any of you know more compute than you have to and then the second point you know",
    "start": "2174830",
    "end": "2181790"
  },
  {
    "text": "hive is actually a great tool for combining data from different data sources right we were able to take data",
    "start": "2181790",
    "end": "2187130"
  },
  {
    "text": "in a DynamoDB table and join it to data that was living in text files right hive really excels at that it's a very nice",
    "start": "2187130",
    "end": "2193610"
  },
  {
    "text": "tool for that so definitely encourage you to look at it ok so let's move on to",
    "start": "2193610",
    "end": "2199040"
  },
  {
    "text": "demo number three so you know we've sold our streaming media service we got a billion dollar unicorn valuation got a",
    "start": "2199040",
    "end": "2206240"
  },
  {
    "text": "nice exit for yourself and your investors you've got a hundred million bucks or so burning a hole in your",
    "start": "2206240",
    "end": "2211730"
  },
  {
    "text": "pocket you want to move on to the next great thing so you are going to create an Internet of Things company and what",
    "start": "2211730",
    "end": "2219620"
  },
  {
    "text": "we're gonna do here is we're going to ingest sensor data using Amazon Kinesis",
    "start": "2219620",
    "end": "2225040"
  },
  {
    "text": "we're gonna read that sensor data off the Kinesis stream using a lambda",
    "start": "2225040",
    "end": "2231380"
  },
  {
    "text": "function lambda function is gonna do two things it's actually gonna be two different lambda functions diagrams not",
    "start": "2231380",
    "end": "2236390"
  },
  {
    "text": "not quite accurate here but one of them is going to write it to dynamodb the other is going to write it into Amazon",
    "start": "2236390",
    "end": "2241400"
  },
  {
    "text": "s3 as a text file and then we'll have a third lambda function that's actually",
    "start": "2241400",
    "end": "2246560"
  },
  {
    "text": "going to respond to those text files being deposited into s3 just like we did",
    "start": "2246560",
    "end": "2252110"
  },
  {
    "text": "in the first demo and it's going to actually ETL those into a redshift data warehouse so we're doing here is we're",
    "start": "2252110",
    "end": "2258710"
  },
  {
    "text": "building a building an IOT pipeline so that you can actually look at the real-time data from a dashboard that's a",
    "start": "2258710",
    "end": "2266210"
  },
  {
    "text": "query and dynam DynamoDB and then have some more deep analysis that you can do using redshift this is basically what I",
    "start": "2266210",
    "end": "2275750"
  },
  {
    "text": "just said you know we've got a number of sensors they were taking readings we're gonna use Kinesis to ingest those",
    "start": "2275750",
    "end": "2281510"
  },
  {
    "text": "dynamos great for the fast access piece s3 is great for the raw data that's kind",
    "start": "2281510",
    "end": "2287060"
  },
  {
    "text": "of your backup right you can always get back to it if you need to you can age it out to glacier or to other storage",
    "start": "2287060",
    "end": "2292790"
  },
  {
    "text": "categories and then we can use redshift to do our sort of true sort of",
    "start": "2292790",
    "end": "2298310"
  },
  {
    "text": "analytical type queries on it so how do we do this we're gonna create two lambda",
    "start": "2298310",
    "end": "2304609"
  },
  {
    "text": "functions to read data from the Dynamo or from the Kinesis stream rather one of",
    "start": "2304609",
    "end": "2309980"
  },
  {
    "text": "them is going to basically update Amazon DynamoDB and the other is going to write data",
    "start": "2309980",
    "end": "2315320"
  },
  {
    "text": "into Amazon s3 so it'll be two different lambda functions and we'll look at both of those and then for the third part",
    "start": "2315320",
    "end": "2321160"
  },
  {
    "text": "which is getting it into redshift I'm actually using an open source lambda",
    "start": "2321160",
    "end": "2327170"
  },
  {
    "text": "project called the AWS lambda redshift loader it's on the a Tobias labs github",
    "start": "2327170",
    "end": "2332359"
  },
  {
    "text": "repo and we're not gonna look at the code for that today because this is open sourced already and you can look at it",
    "start": "2332359",
    "end": "2338750"
  },
  {
    "text": "it's a really really nice tool I'll walk you through sort of how it works that's what will actually get the data from s3",
    "start": "2338750",
    "end": "2345980"
  },
  {
    "text": "into redshift",
    "start": "2345980",
    "end": "2349090"
  },
  {
    "text": "okay so let's jump in to this one so",
    "start": "2351310",
    "end": "2359240"
  },
  {
    "text": "first thing",
    "start": "2359240",
    "end": "2361840"
  },
  {
    "text": "the first thing we'll do here is let's just go quickly over to the console just",
    "start": "2365660",
    "end": "2372109"
  },
  {
    "text": "as I did with the EMR cluster I've already created the redshift cluster in",
    "start": "2372109",
    "end": "2377779"
  },
  {
    "text": "advance again I just didn't want to have you guys sit around and wait as this got provision and producing a redshift",
    "start": "2377779",
    "end": "2383329"
  },
  {
    "text": "clustered you know can take take a little bit of time but you can see I've got a three node redshift cluster it's up and running here so let's take a look",
    "start": "2383329",
    "end": "2393019"
  },
  {
    "text": "at the actual data warehouse that we're gonna be dealing with on this on this cluster I can show you the tables it's a",
    "start": "2393019",
    "end": "2400549"
  },
  {
    "text": "real simple star schema it's this one",
    "start": "2400549",
    "end": "2409250"
  },
  {
    "text": "right here so we have a fact table it's",
    "start": "2409250",
    "end": "2415309"
  },
  {
    "text": "gonna have the sensor ID the reading the date that the reading was taken from the sensor and in this case we're reading",
    "start": "2415309",
    "end": "2421759"
  },
  {
    "text": "temperature off the sensor and this piece down here is basically something",
    "start": "2421759",
    "end": "2428480"
  },
  {
    "text": "specific to redshift this this tells redshift how to distribute the data across the nodes so like many",
    "start": "2428480",
    "end": "2434269"
  },
  {
    "text": "distributed systems like most distributed systems for that matter redshift wants to have data be uniformly",
    "start": "2434269",
    "end": "2440660"
  },
  {
    "text": "distributed across all the nodes in the cluster right if the data is not uniformly distributed when you do",
    "start": "2440660",
    "end": "2445880"
  },
  {
    "text": "queries you have to essentially copy data from one node to another and that's inefficient so we're gonna say let's use",
    "start": "2445880",
    "end": "2451819"
  },
  {
    "text": "the sensor ID as the key for how we distribute data across the nodes in the",
    "start": "2451819",
    "end": "2457160"
  },
  {
    "text": "cluster very very similar to the hash key concept with DynamoDB and sensor ID",
    "start": "2457160",
    "end": "2462230"
  },
  {
    "text": "is a good one because presumably we're gonna have about the same number of readings per sensor and it's also",
    "start": "2462230",
    "end": "2468650"
  },
  {
    "text": "essentially random it's a it's a four character or hex ID then we create our",
    "start": "2468650",
    "end": "2474440"
  },
  {
    "text": "dimension table the sensor itself and you can see we have the sensor ID we have via geo coordinates of the sensor",
    "start": "2474440",
    "end": "2479839"
  },
  {
    "text": "so that's where it's actually located in this case we're gonna be simulating temperature sensors that have been",
    "start": "2479839",
    "end": "2485150"
  },
  {
    "text": "spread out across the western United States we have the city that the sensors",
    "start": "2485150",
    "end": "2490400"
  },
  {
    "text": "in we have the state that sensors in in the region so this is a very sort of classic hierarchical dimension right you",
    "start": "2490400",
    "end": "2496940"
  },
  {
    "text": "can you can see just looking at this oh yeah you'll answers and pretty nice questions to be able to roll up to city state region",
    "start": "2496940",
    "end": "2502430"
  },
  {
    "text": "right very very sort of classic OLAP type queries and then we have the the",
    "start": "2502430",
    "end": "2507680"
  },
  {
    "text": "date/time table here and this is basically again a very sort of classic",
    "start": "2507680",
    "end": "2513249"
  },
  {
    "text": "time table in OLAP where we break up break it out by minute hour day month",
    "start": "2513249",
    "end": "2519349"
  },
  {
    "text": "and year right so the sensors are basically grabbing the raw gauge down to the second but then we're also rolling",
    "start": "2519349",
    "end": "2525440"
  },
  {
    "text": "up to these other portions of the time so that's what our that's what our",
    "start": "2525440",
    "end": "2531680"
  },
  {
    "text": "tables look like I've already created these tables on our redshift cluster so let's actually go ahead and log in or",
    "start": "2531680",
    "end": "2538099"
  },
  {
    "text": "rather just run a quick query here and",
    "start": "2538099",
    "end": "2543470"
  },
  {
    "text": "I'll do it from this one so redshift",
    "start": "2543470",
    "end": "2549200"
  },
  {
    "text": "uses the Postgres client libraries so I",
    "start": "2549200",
    "end": "2554869"
  },
  {
    "text": "have P sequel installed here oops and what is it Oh wrong folder",
    "start": "2554869",
    "end": "2567880"
  },
  {
    "text": "great let me make it a little bigger so these are all of our our sensors so",
    "start": "2578220",
    "end": "2584230"
  },
  {
    "text": "basically you can see we've spread them out around the western part of the US",
    "start": "2584230",
    "end": "2589410"
  },
  {
    "text": "okay so those are already loaded now",
    "start": "2589410",
    "end": "2594579"
  },
  {
    "text": "what we're gonna do is I'm actually gonna simulate these sensors taking temperature readings and what I've done",
    "start": "2594579",
    "end": "2600519"
  },
  {
    "text": "here is I've written a Python script that uses a publicly available API from",
    "start": "2600519",
    "end": "2609760"
  },
  {
    "text": "forecast I oh that actually these are real temperature readings what I'm doing is I'm just doing a rest call and it's",
    "start": "2609760",
    "end": "2622329"
  },
  {
    "text": "gonna I think we have about 43 or 44 sensors so I'll read these okay great",
    "start": "2622329",
    "end": "2632470"
  },
  {
    "text": "so I've taken these readings and I've put them onto a Kinesis stream right so the these data's have been put to a",
    "start": "2632470",
    "end": "2639609"
  },
  {
    "text": "Kinesis stream and now if we take a look at lambda we can see lambda reading data",
    "start": "2639609",
    "end": "2647529"
  },
  {
    "text": "off that stream and so let's take a look first at the one that's reading data off the stream and putting it into DynamoDB",
    "start": "2647529",
    "end": "2656250"
  },
  {
    "text": "let's grab this",
    "start": "2659400",
    "end": "2662880"
  },
  {
    "text": "I make it a little bigger this is what the payload of the Kinesis data looks like I have my sensor ID I have my",
    "start": "2664670",
    "end": "2671150"
  },
  {
    "text": "temperature and I have my time and so what we're doing is we're basically is reading this off the stream and inserting it at the Dynamo time",
    "start": "2671150",
    "end": "2677480"
  },
  {
    "text": "permitting I'll show you the code for that it's virtually identical to what we did with the demo1 right you just do an",
    "start": "2677480",
    "end": "2682730"
  },
  {
    "text": "insert into the Dynamo table and we can",
    "start": "2682730",
    "end": "2688099"
  },
  {
    "text": "even look at the Dynamo data that's going into the sensor data table and",
    "start": "2688099",
    "end": "2694520"
  },
  {
    "text": "there it is right so the next function",
    "start": "2694520",
    "end": "2703160"
  },
  {
    "text": "is the one that writes to s3 and you can see what it does is it's putting the",
    "start": "2703160",
    "end": "2711650"
  },
  {
    "text": "data into a folder called sensor data as CSV files and so what happens is well",
    "start": "2711650",
    "end": "2719270"
  },
  {
    "text": "these CSV files get written to s3 and just as we did with demo one there's a put object notification on this bucket",
    "start": "2719270",
    "end": "2726859"
  },
  {
    "text": "and that gets consumed by the open source the AWS redshift lambda loader right you can look at the code for that",
    "start": "2726859",
    "end": "2733339"
  },
  {
    "text": "but the what what that does is it's basically using dynamo to track which files it's processed and so you can see",
    "start": "2733339",
    "end": "2740569"
  },
  {
    "text": "if I go into this processed files it's keeping track of all of the files and the batches it loads files and batches",
    "start": "2740569",
    "end": "2747260"
  },
  {
    "text": "and if we look here we should have an open - yes and sure enough we do it's",
    "start": "2747260",
    "end": "2754130"
  },
  {
    "text": "this guy right here right you can control the batch size through this loader you in this case I'm gonna I did",
    "start": "2754130",
    "end": "2760490"
  },
  {
    "text": "a batch size of two so that means every time there are there are two files that have been processed and added to the batch it'll actually then upload the",
    "start": "2760490",
    "end": "2766730"
  },
  {
    "text": "batch into redshift so let me go ahead",
    "start": "2766730",
    "end": "2772670"
  },
  {
    "text": "and run the sensor simulation again to get a second second bat or a second file",
    "start": "2772670",
    "end": "2781869"
  },
  {
    "text": "okay and all of this does happen pretty fast let's see if yep we now have no",
    "start": "2802890",
    "end": "2812580"
  },
  {
    "text": "more open batches so this has actually been loaded into redshift we can actually see this if I come over here",
    "start": "2812580",
    "end": "2819030"
  },
  {
    "text": "and go to the loads tab and i refresh this looks like it was probably this guy",
    "start": "2819030",
    "end": "2826950"
  },
  {
    "text": "right here yep 501 p.m. and you can see the actual command it executed this copy",
    "start": "2826950",
    "end": "2835350"
  },
  {
    "text": "fact sensor data from this this is a man if so when you load data into redshift you can load individual files or you can",
    "start": "2835350",
    "end": "2842160"
  },
  {
    "text": "load groups of files that are listed in a manifest and so we're using the manifest technique to load this so you",
    "start": "2842160",
    "end": "2849840"
  },
  {
    "text": "can see it loaded our data and the final piece to all of this is let's actually",
    "start": "2849840",
    "end": "2857120"
  },
  {
    "text": "run a query",
    "start": "2857120",
    "end": "2860750"
  },
  {
    "text": "and great we got some data I've been I've been running this intermittently",
    "start": "2876710",
    "end": "2882589"
  },
  {
    "text": "since September 24th but you can see you know today this is a 10-8 oh I think we",
    "start": "2882589",
    "end": "2890150"
  },
  {
    "text": "must be on a different time time zone you can see basically it's just doing a",
    "start": "2890150",
    "end": "2896839"
  },
  {
    "text": "very straightforward aggregation where I'm counting and averaging these these",
    "start": "2896839",
    "end": "2902690"
  },
  {
    "text": "values and you can see because we're used dealing with a dimensional model I'm actually rolling up to state right",
    "start": "2902690",
    "end": "2909349"
  },
  {
    "text": "so the sensors themselves are located specific geo coordinates but I can actually do a an aggregate up to the state and if we look whoops",
    "start": "2909349",
    "end": "2920030"
  },
  {
    "text": "over here the query is real straightforward I just joined my",
    "start": "2920030",
    "end": "2929210"
  },
  {
    "text": "dimensions to my facts table or a fact table rather and then do a group by and then order it so it looks good and you",
    "start": "2929210",
    "end": "2936650"
  },
  {
    "text": "know in in real life you probably wouldn't do this by by hand I'm a I'm a coder so I like writing code",
    "start": "2936650",
    "end": "2944329"
  },
  {
    "text": "but you know most people would actually use like a bi tool to actually look at this data maybe a pivot table something",
    "start": "2944329",
    "end": "2949579"
  },
  {
    "text": "like that okay so that's it for the demos let's just finish off the",
    "start": "2949579",
    "end": "2954890"
  },
  {
    "text": "presentation",
    "start": "2954890",
    "end": "2957190"
  },
  {
    "text": "so key takeaways theme that we've been riffing on throughout this whole session Kinesis Lando DynamoDB you get scale",
    "start": "2962140",
    "end": "2969549"
  },
  {
    "text": "durability and availability with really really low operational overhead right again no ec2 instances at all in this not that",
    "start": "2969549",
    "end": "2977019"
  },
  {
    "text": "ec2 is bad you know I love ec2 but it's just you know one other thing you need to worry about and run right in this",
    "start": "2977019",
    "end": "2984039"
  },
  {
    "text": "particular case we did put data both into redshift and Dynamo and I didn't really focus on the Dynamo part but that",
    "start": "2984039",
    "end": "2990249"
  },
  {
    "text": "would be if you wanted to do like a real-time dashboard or maybe a map that showed you your sensors you would want",
    "start": "2990249",
    "end": "2995319"
  },
  {
    "text": "to actually probably pull that data from dynamo right redshift doesn't like high concurrency right it has a limited",
    "start": "2995319",
    "end": "3002549"
  },
  {
    "text": "number of active connections that you can have so if you wanted to have like a mobile dashboard we're gonna have potentially thousands of mobile devices",
    "start": "3002549",
    "end": "3008789"
  },
  {
    "text": "that are looking at the the sensors on a map and seeing the last temperature reading you'd want to use dynamo for",
    "start": "3008789",
    "end": "3014190"
  },
  {
    "text": "that redshift though of course is better for the deeper analysis right you can you can build star schemas you can do",
    "start": "3014190",
    "end": "3020599"
  },
  {
    "text": "rich queries join data together do aggregates it's very very good at dealing with very large data set so you",
    "start": "3020599",
    "end": "3027839"
  },
  {
    "text": "can go way back in time on these queries and then lambda loading data in a redshift I mean you're basically doing",
    "start": "3027839",
    "end": "3033569"
  },
  {
    "text": "ETL continuously right I wouldn't necessarily recommend trying to get this down to you know sub minute ETLs",
    "start": "3033569",
    "end": "3040769"
  },
  {
    "text": "that's probably a little too frequent and if you're gonna play around with the AWS lambda redshift loader you know you",
    "start": "3040769",
    "end": "3047670"
  },
  {
    "text": "want to pick batch sizes that sort of are sane for for doing ETL to redshift",
    "start": "3047670",
    "end": "3053309"
  },
  {
    "text": "but you know certainly five to ten minute increments is not not a problem and what that essentially means is that",
    "start": "3053309",
    "end": "3058799"
  },
  {
    "text": "your data warehouse is now really only slightly eventually consistent it's not that far behind",
    "start": "3058799",
    "end": "3064170"
  },
  {
    "text": "you know the the actual live dynamo data",
    "start": "3064170",
    "end": "3069019"
  },
  {
    "text": "so a couple of closing points you know as we saw today dynamo is really",
    "start": "3070700",
    "end": "3076559"
  },
  {
    "text": "versatile you can use it in a lot of different ways and it works really really well when taken in the context of",
    "start": "3076559",
    "end": "3082470"
  },
  {
    "text": "the larger data ecosystem most Big Data solutions really are going to use",
    "start": "3082470",
    "end": "3088859"
  },
  {
    "text": "different tools right there there there aren't really that many use cases we're gonna use one too in one tool only so oftentimes you're",
    "start": "3088859",
    "end": "3096209"
  },
  {
    "text": "gonna use a combination of EMR or Hadoop Kinesis you might use Apache spark for",
    "start": "3096209",
    "end": "3103019"
  },
  {
    "text": "certain things right so you should've have to consider these the this tool set in in sort of the larger context and AWS",
    "start": "3103019",
    "end": "3110910"
  },
  {
    "text": "ecosystem has a very rich and powerful set of services to build these architectures right with lambda and",
    "start": "3110910",
    "end": "3117719"
  },
  {
    "text": "Kinesis already asked if you're dealing with relational data redshift EMR you",
    "start": "3117719",
    "end": "3123930"
  },
  {
    "text": "can run spark on top of EMR there are a whole bunch of announcements this morning at the keynote you know like fire hose for example there's a bi",
    "start": "3123930",
    "end": "3131329"
  },
  {
    "text": "service that's in preview now right so there's a lot of just sort of great services that you can combine to put",
    "start": "3131329",
    "end": "3138479"
  },
  {
    "text": "together some pretty compelling Big Data architectures so that's all I had I hope",
    "start": "3138479",
    "end": "3145140"
  },
  {
    "text": "you enjoyed it it was pretty fun building it I'm glad it worked that would have been embarrassing had it not",
    "start": "3145140",
    "end": "3151200"
  },
  {
    "text": "worked",
    "start": "3151200",
    "end": "3153410"
  }
]