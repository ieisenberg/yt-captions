[
  {
    "start": "0",
    "end": "330000"
  },
  {
    "text": "thank you guys for coming out spending your afternoon with us my name is Prakash this is Matt and",
    "start": "140",
    "end": "6990"
  },
  {
    "text": "Slava and we all are from a company called next door how many people are familiar with next door just quick oh",
    "start": "6990",
    "end": "12509"
  },
  {
    "text": "there we go man I did a I did a presentation yesterday I did the same question and like three quarters of the room was like",
    "start": "12509",
    "end": "19170"
  },
  {
    "text": "next door what so that was it that was a much better start thank you guys we're gonna talk a little bit today about how",
    "start": "19170",
    "end": "26099"
  },
  {
    "text": "we rebuilt our data pipeline we're not gonna go into a lot of detail about the",
    "start": "26099",
    "end": "31619"
  },
  {
    "text": "data pipeline itself we're going to talk about some of the component parts and some of the technological decisions that we made but the key here is we had a",
    "start": "31619",
    "end": "39030"
  },
  {
    "text": "legacy data pipeline it was really important to us and I'm gonna tell you a few ways that we use data in our organization and then these guys are",
    "start": "39030",
    "end": "45780"
  },
  {
    "text": "gonna tell you more about some of the problems with that data pipeline and how we replaced it in a server lissa way but",
    "start": "45780",
    "end": "53699"
  },
  {
    "text": "first a little bit about next door so we started next door in the summer of 2010 so it's been about seven years we are",
    "start": "53699",
    "end": "60629"
  },
  {
    "text": "now in about 80% actually more than 80% of u.s. neighborhoods we're also in",
    "start": "60629",
    "end": "66390"
  },
  {
    "text": "Germany the UK and the Netherlands and people are using next door in a wide variety of ways whether it's finding a",
    "start": "66390",
    "end": "71760"
  },
  {
    "text": "babysitter or a plumber or an auto mechanic reporting and reuniting with a lost pet reporting crime and safety",
    "start": "71760",
    "end": "78450"
  },
  {
    "text": "issues lots of different ways that people are using it and in fact it's used by more than just members today",
    "start": "78450",
    "end": "83970"
  },
  {
    "text": "started out just being about connecting residents and local communities but now it's also about connecting them to all",
    "start": "83970",
    "end": "90450"
  },
  {
    "text": "of the organizations and constituents in their local community including local law enforcement municipal agencies",
    "start": "90450",
    "end": "95909"
  },
  {
    "text": "businesses and eventually organizations and interests that they share and again",
    "start": "95909",
    "end": "102600"
  },
  {
    "text": "used by 80 percent of neighborhoods we actually have mobile applications and web applications we're trying to get",
    "start": "102600",
    "end": "109079"
  },
  {
    "text": "more people to use our mobile applications because that's a better experience for them and outside of like",
    "start": "109079",
    "end": "115310"
  },
  {
    "text": "Facebook and Twitter which really pioneered the whole concept around social networking we're not about",
    "start": "115310",
    "end": "121560"
  },
  {
    "text": "self-expression we're not about connecting on the basis of things that you're necessarily like interested in or",
    "start": "121560",
    "end": "127530"
  },
  {
    "text": "that you want to express about yourself it's really about utility and so when it comes to building a product like next",
    "start": "127530",
    "end": "133980"
  },
  {
    "text": "or it's really important that we're able to get feedback from our members and unlike the early days when we could just",
    "start": "133980",
    "end": "139830"
  },
  {
    "text": "go out and talk to our members because there are only a handful of them and that's how we started the company very very qualitative sort of interviews with",
    "start": "139830",
    "end": "147299"
  },
  {
    "text": "our users we now rely pretty heavily on data and a lot of instrumentation whether it's analyzing the results of an",
    "start": "147299",
    "end": "153930"
  },
  {
    "text": "a/b test or thinking about different signals that we want to integrate into feed personalization or email delivery",
    "start": "153930",
    "end": "160230"
  },
  {
    "text": "it's really really important and so today we're going to look at a few of the ways that we use data and then these",
    "start": "160230",
    "end": "165629"
  },
  {
    "text": "guys again are going to talk about the tools that we use to collect that data and pass it along through our pipeline at scale and so again data drives a lot",
    "start": "165629",
    "end": "173760"
  },
  {
    "text": "of our product decisions at next door and what you see here is a sampling of the different ways that we use data whether it's delivering emails",
    "start": "173760",
    "end": "180750"
  },
  {
    "text": "delivering push notifications and figuring out which of those are important and urgent for us to notify",
    "start": "180750",
    "end": "186030"
  },
  {
    "text": "our members about and today we're pretty generous about that any of you guys that use nextdoor probably get a lot of emails from us so sorry about that",
    "start": "186030",
    "end": "192329"
  },
  {
    "text": "we're working on it though we're working on it and then also things like statistics that we display all",
    "start": "192329",
    "end": "198480"
  },
  {
    "text": "throughout the site for example here you can see that in my neighborhood which is called central film art in in San",
    "start": "198480",
    "end": "205169"
  },
  {
    "text": "Francisco we show a little counter of what percentage of the neighborhood is actually on on the platform based on the",
    "start": "205169",
    "end": "211319"
  },
  {
    "text": "number of residences there of course ads are really important to us and so for anybody that's worked on an ad serving",
    "start": "211319",
    "end": "217440"
  },
  {
    "text": "system you know that there are a lot of signals that you ingest some of those are behavioral signals that you have to instrument in the clients to be able to",
    "start": "217440",
    "end": "224220"
  },
  {
    "text": "pass down through and use and so we do that as well and then of course a B tests are really really important to us",
    "start": "224220",
    "end": "229290"
  },
  {
    "text": "and over time while this is not exactly to scale this is representative of the",
    "start": "229290",
    "end": "236190"
  },
  {
    "text": "growth in data since we started the company and really started to focus on data almost four or five years ago to",
    "start": "236190",
    "end": "242549"
  },
  {
    "text": "today we're ingesting over three billion events a day and so it's it's nothing to",
    "start": "242549",
    "end": "247829"
  },
  {
    "text": "shake a stick at right there's a lot of there's a lot of stuff in here and what you're seeing here represents two-thirds",
    "start": "247829",
    "end": "255180"
  },
  {
    "text": "of our entire DevOps team what we call systems infrastructure inside a next",
    "start": "255180",
    "end": "260250"
  },
  {
    "text": "door and that's on a team of close to 100 engineers we're starting to grow closer to 100 engineers and so",
    "start": "260250",
    "end": "266070"
  },
  {
    "text": "two-thirds of the team is here right now so the one guy that's left I hope everything's going okay but but really",
    "start": "266070",
    "end": "272400"
  },
  {
    "text": "this is this is by design and the only way that you can design an organization around this is by relying heavily on",
    "start": "272400",
    "end": "279300"
  },
  {
    "text": "automation on repeatable systems that you can observe and that you have instrumentation around and really it's",
    "start": "279300",
    "end": "285780"
  },
  {
    "text": "about building robust technologies and being very selective about the things that you run internally and the things",
    "start": "285780",
    "end": "292080"
  },
  {
    "text": "that you decide to allow someone else to run like AWS or some of our partners for example we use send grid for sending",
    "start": "292080",
    "end": "298320"
  },
  {
    "text": "email because we don't want to run our own email infrastructure and so that's what's allowed us to keep our team small",
    "start": "298320",
    "end": "304260"
  },
  {
    "text": "and lean and so I think when we were thinking about our data pipeline it was",
    "start": "304260",
    "end": "309630"
  },
  {
    "text": "really about improving its reliability designing a system that could scale with very little kind of operational overhead",
    "start": "309630",
    "end": "316050"
  },
  {
    "text": "or maintenance or more of these guys although you know you probably owned a",
    "start": "316050",
    "end": "321570"
  },
  {
    "text": "couple more guys so sorry about that but it's really about minimizing the amount of intervention that we need to",
    "start": "321570",
    "end": "327240"
  },
  {
    "text": "do manually and relying on automation as much as possible so Slava is going to talk a little bit",
    "start": "327240",
    "end": "332700"
  },
  {
    "text": "more about this real-time data ingestion pipeline so I'm gonna hand this off to him all right Thank You Prakash hello my",
    "start": "332700",
    "end": "341370"
  },
  {
    "text": "name is Slava Marquis ov and I'm an engineer on the systems infrastructure team at next door my background is in",
    "start": "341370",
    "end": "347430"
  },
  {
    "text": "data engineering and I worked on the data ingestion pipeline that we'll be talking about today I have a lot of",
    "start": "347430",
    "end": "353460"
  },
  {
    "text": "content to get through and I'd like to ask everyone to hold your questions until the very end so before getting to",
    "start": "353460",
    "end": "361590"
  },
  {
    "text": "the cool new stuff I want to step back and take a moment to describe where we came from we're an Apache flume which is",
    "start": "361590",
    "end": "368100"
  },
  {
    "text": "a distributed self-hosted streaming service similar to apache kafka or ADA",
    "start": "368100",
    "end": "373919"
  },
  {
    "text": "briefs hosted Kinesis streams flume served as well for four years and scaled up to handle 1.5 billion events per day",
    "start": "373919",
    "end": "383840"
  },
  {
    "text": "our legacy flume pipeline has consisted of two parts flume agents and flume",
    "start": "384350",
    "end": "389970"
  },
  {
    "text": "collectors agents ran on hosts and collected logs off of servers and",
    "start": "389970",
    "end": "396510"
  },
  {
    "text": "forwarded them to a cluster of flume collectors",
    "start": "396510",
    "end": "401300"
  },
  {
    "text": "from there the collectors partitioned and furthered the data and bachelors two",
    "start": "401580",
    "end": "407050"
  },
  {
    "text": "different data sinks in our case agents collected syslog and apik application",
    "start": "407050",
    "end": "412090"
  },
  {
    "text": "logs for them to collectors and collectors would multiplex that data to s3 and elasticsearch the data was made",
    "start": "412090",
    "end": "420610"
  },
  {
    "text": "available for real-time consumption and elastic search by developers and product owners separately the data was dumped to",
    "start": "420610",
    "end": "428950"
  },
  {
    "text": "s3 where are offline batch ETL workflow would further process the data and act",
    "start": "428950",
    "end": "434920"
  },
  {
    "text": "on it the legacy pipeline worked well for a",
    "start": "434920",
    "end": "440950"
  },
  {
    "text": "while but was susceptible to that pressure was very rigid in its configuration and sometimes collectors",
    "start": "440950",
    "end": "448000"
  },
  {
    "text": "buffers became corrupt making it very hard to recover that data",
    "start": "448000",
    "end": "453420"
  },
  {
    "text": "additionally we found that flume agents were very CPU hungry which caused issues",
    "start": "453420",
    "end": "459160"
  },
  {
    "text": "with mission critical infrastructure like databases I finally it was",
    "start": "459160",
    "end": "466630"
  },
  {
    "text": "impossible to dynamically scale this this pipeline to handle cyclic loads let",
    "start": "466630",
    "end": "472300"
  },
  {
    "text": "alone spikes I glossed over this but this this type of pipeline is very",
    "start": "472300",
    "end": "479290"
  },
  {
    "text": "susceptible to back pressure so if you if you run spot nodes you want to get",
    "start": "479290",
    "end": "486730"
  },
  {
    "text": "the data off of those as quickly as possible before they're terminated so if you have large delays upstream in your",
    "start": "486730",
    "end": "492730"
  },
  {
    "text": "pipeline you're potentially going to lose data so as Prakash mentioned my",
    "start": "492730",
    "end": "500800"
  },
  {
    "text": "team is very small and we support lots of different product initiatives as well as general infrastructure usage at next",
    "start": "500800",
    "end": "506890"
  },
  {
    "text": "door we needed something that would provide the service stability we were looking for while decreased the",
    "start": "506890",
    "end": "512260"
  },
  {
    "text": "operational overhead of running a pipeline we wanted something that was easy to iterate on and reconfigure",
    "start": "512260",
    "end": "519360"
  },
  {
    "text": "something that would provide better SLA x' and something that was more resilient to influxes of data",
    "start": "519360",
    "end": "527430"
  },
  {
    "text": "additionally we wanted to enable data-driven product features that required tighter oscillates",
    "start": "528030",
    "end": "534610"
  },
  {
    "text": "for example up on next-door we collect clickstream and impression logs through",
    "start": "534610",
    "end": "539710"
  },
  {
    "text": "our data pipeline to understand what content is trending upwards in popularity this data ends up in s3 where",
    "start": "539710",
    "end": "548530"
  },
  {
    "text": "we can analyze it to understand which users in a neighborhood still haven't seen a particular piece of content and",
    "start": "548530",
    "end": "555240"
  },
  {
    "text": "ride to aggregate statistics that influence our push notification trailers",
    "start": "555240",
    "end": "560580"
  },
  {
    "text": "now while this type of product feature was technologically possible with our",
    "start": "560580",
    "end": "566170"
  },
  {
    "text": "old pipeline we found that it couldn't deliver a consistent of SLA SLA to",
    "start": "566170",
    "end": "572230"
  },
  {
    "text": "provide a good user experience so like",
    "start": "572230",
    "end": "577690"
  },
  {
    "text": "so many companies today nextdoor started in the cloud and we have been a heavy user of AWS services",
    "start": "577690",
    "end": "583150"
  },
  {
    "text": "since when exploring options for the new pipeline we naturally look to AWS to see",
    "start": "583150",
    "end": "590050"
  },
  {
    "text": "what services they could offer us we decided to use these eight of us men managed offerings to build our new data",
    "start": "590050",
    "end": "597670"
  },
  {
    "text": "ingestion pipeline so quickly going through them Kinesis streams allows data",
    "start": "597670",
    "end": "602890"
  },
  {
    "text": "to be streamed in batches events lambda is a service where you can upload your",
    "start": "602890",
    "end": "610150"
  },
  {
    "text": "own code and it handles X again at executing it for you it is able to be",
    "start": "610150",
    "end": "616600"
  },
  {
    "text": "linked with data sources like Nisa streams allowing your code to transform",
    "start": "616600",
    "end": "621730"
  },
  {
    "text": "incoming data Kinesis firehose is similar to Kinesis streams but",
    "start": "621730",
    "end": "627280"
  },
  {
    "text": "aggregates large batches of events and writes them to s3 hosted elasticsearch",
    "start": "627280",
    "end": "633370"
  },
  {
    "text": "or redshift and lastly s3 stores the",
    "start": "633370",
    "end": "638950"
  },
  {
    "text": "data the data making it available for offline processing",
    "start": "638950",
    "end": "644430"
  },
  {
    "text": "the new setup is similar to what we had before Kinesis agents collect logs off",
    "start": "650640",
    "end": "656320"
  },
  {
    "text": "of hosts and send them directly to Kinesis streams our code is executed by",
    "start": "656320",
    "end": "662200"
  },
  {
    "text": "lambda where does some data transformation on the incoming logs and sends them to elasticsearch and",
    "start": "662200",
    "end": "668230"
  },
  {
    "text": "separately to firehose where they are dumped to s3 we needed to write a little",
    "start": "668230",
    "end": "674080"
  },
  {
    "text": "code to transform the incoming data making it consumable by downstream processes lambda makes it easy to write",
    "start": "674080",
    "end": "683980"
  },
  {
    "text": "lightweight processing functions in node Python Java and c-sharp however its ETL",
    "start": "683980",
    "end": "690730"
  },
  {
    "text": "and there's a lot of boilerplate things are constantly being reimplemented miss implemented or left unimplemented so",
    "start": "690730",
    "end": "700330"
  },
  {
    "text": "what is ETL ETL consists of three parts extract the data by reading from a data",
    "start": "700330",
    "end": "705339"
  },
  {
    "text": "source and create a machine readable object transform the data in some way and finally loaded somewhere else in a",
    "start": "705339",
    "end": "712330"
  },
  {
    "text": "potentially different format ETL can be thought of as three steps but",
    "start": "712330",
    "end": "718750"
  },
  {
    "text": "in reality it consists of a handful of steps you need to read your data from a data source and be resilient to",
    "start": "718750",
    "end": "725410"
  },
  {
    "text": "intermittent failures deserialize the data from binary form into something you",
    "start": "725410",
    "end": "730630"
  },
  {
    "text": "can work with filter out what you don't need do some transformations batch it up",
    "start": "730630",
    "end": "736240"
  },
  {
    "text": "and transport it to a data sink this entire process requires logging Mon",
    "start": "736240",
    "end": "742120"
  },
  {
    "text": "during retrying and error handling so for the vast majority the only custom",
    "start": "742120",
    "end": "748150"
  },
  {
    "text": "logic you really need is if you require custom data transformations so we had",
    "start": "748150",
    "end": "754900"
  },
  {
    "text": "all the building blocks to make a streaming ETL pipeline with AWS but we needed to standardize or lambda code to",
    "start": "754900",
    "end": "761770"
  },
  {
    "text": "make it reconfigurable and reusable what if tomorrow we wanted to ingest logs from a elby's ELB Zoar VP sees what if",
    "start": "761770",
    "end": "770170"
  },
  {
    "text": "we wanted to pipe that data to a bi tools such as Splunk or join it with",
    "start": "770170",
    "end": "775630"
  },
  {
    "text": "another data source copy pasting tacking on features or worst case rewriting from",
    "start": "775630",
    "end": "780910"
  },
  {
    "text": "scratch isn't in line with our engineering principles at next door",
    "start": "780910",
    "end": "787270"
  },
  {
    "text": "think about how vastly different a function that processes 10,000 events",
    "start": "787570",
    "end": "793910"
  },
  {
    "text": "from a stream is from one that processes 10 million events from a file nest 3 consider the implications on compute",
    "start": "793910",
    "end": "801830"
  },
  {
    "text": "time network performance memory footprint and a whole host of other things the business logic for",
    "start": "801830",
    "end": "808940"
  },
  {
    "text": "transformation may be the same but they are hardly the same function our",
    "start": "808940",
    "end": "820550"
  },
  {
    "text": "solution was creating vendor in AWS lambda function written Java that we",
    "start": "820550",
    "end": "826220"
  },
  {
    "text": "could reconfigure and reuse across different pipelines and teams within the company bedre can be thought of as the",
    "start": "826220",
    "end": "834890"
  },
  {
    "text": "skeleton for creating streaming ETL functions on lambda the modular design makes it easy to plug in custom code but",
    "start": "834890",
    "end": "842390"
  },
  {
    "text": "leverage what has already been built it comes with a handful of different implementations for each step of the ETL",
    "start": "842390",
    "end": "849080"
  },
  {
    "text": "process the same function could be used regardless of the data source that",
    "start": "849080",
    "end": "854170"
  },
  {
    "text": "transformations or the data sink those things are automatically handled by bender simply write your business logic",
    "start": "854170",
    "end": "861080"
  },
  {
    "text": "where it makes sense and create your function confederaci√≥n bender can",
    "start": "861080",
    "end": "868550"
  },
  {
    "text": "currently read from Kinesis s3 and SNS filter data with reg axes or simple",
    "start": "868550",
    "end": "874670"
  },
  {
    "text": "string contains parse it into objects from json or reg X patterns which allows",
    "start": "874670",
    "end": "880490"
  },
  {
    "text": "for processing of unstructured data such as web server logs we have a growing set",
    "start": "880490",
    "end": "886339"
  },
  {
    "text": "of operations to transform the data and wrappers to provide some context as to where that data came from and what",
    "start": "886339",
    "end": "893390"
  },
  {
    "text": "processed it finally vendor consent to a handful of data sinks if you didn't see",
    "start": "893390",
    "end": "900200"
  },
  {
    "text": "a data source or data sink to your liking we made it easy to add additional support",
    "start": "900200",
    "end": "906910"
  },
  {
    "text": "a few non ETL functions took call out are graceful retries event count and",
    "start": "908889",
    "end": "914619"
  },
  {
    "text": "performance reporting configuration validation and documentation I considered it the standardization of",
    "start": "914619",
    "end": "922170"
  },
  {
    "text": "configuration to be the cornerstone of vendor it ensures new features get",
    "start": "922170",
    "end": "928509"
  },
  {
    "text": "standard configuration and documentation which is essential in a tool like this",
    "start": "928509",
    "end": "934199"
  },
  {
    "start": "934000",
    "end": "1103000"
  },
  {
    "text": "so having touched on what's supported let's now go over how to configure a bender lambda function",
    "start": "934679",
    "end": "942720"
  },
  {
    "text": "bender is configured via llamo or JSON I left a few things out of the slide but",
    "start": "943350",
    "end": "948759"
  },
  {
    "text": "let's go through the different parts the handler section defines the data source",
    "start": "948759",
    "end": "955209"
  },
  {
    "text": "this is the type of lambda trigger Banger supports multiple triggers with different data types but we don't allow",
    "start": "955209",
    "end": "962589"
  },
  {
    "text": "mixing different types of triggers you'll want to use multiple bender functions for that next we define how to",
    "start": "962589",
    "end": "971980"
  },
  {
    "text": "treat data coming from a source you may you can have multiple data sources and",
    "start": "971980",
    "end": "977319"
  },
  {
    "text": "have them treated differently maybe you want the same function handling staging and production but filter out trace",
    "start": "977319",
    "end": "983889"
  },
  {
    "text": "debugging and production the source reg",
    "start": "983889",
    "end": "989079"
  },
  {
    "text": "acts allows you to fill out streams and s3 paths prior to reading data in case you",
    "start": "989079",
    "end": "994569"
  },
  {
    "text": "accidentally added a trigger and while reading the data but before further",
    "start": "994569",
    "end": "999879"
  },
  {
    "text": "processing it the read acts patterns allow you to filter out records based on their contents",
    "start": "999879",
    "end": "1006980"
  },
  {
    "text": "the decider eliezer instructs bender how to read the raw data into an object it's",
    "start": "1006980",
    "end": "1013199"
  },
  {
    "text": "possible to also add schema validation in this step the list of operations is",
    "start": "1013199",
    "end": "1020519"
  },
  {
    "text": "where you define what transformations to perform on your data this is your business logic maybe you want to rename",
    "start": "1020519",
    "end": "1027779"
  },
  {
    "text": "fields join with another data source for example adding geolocation for logs",
    "start": "1027779",
    "end": "1033990"
  },
  {
    "text": "containing IP addresses or some other type of mutations I'll get into what that looks like in a moment and finally",
    "start": "1033990",
    "end": "1041010"
  },
  {
    "text": "you can also add complex filter logic here as well next rappers",
    "start": "1041010",
    "end": "1047850"
  },
  {
    "text": "can add context to your data such as where came from how long took to process and what processed it this can be",
    "start": "1047850",
    "end": "1055350"
  },
  {
    "text": "invaluable when debugging SLA issues with example on the right you can see",
    "start": "1055350",
    "end": "1061170"
  },
  {
    "text": "when the message arrived to Kinesis when it was processed and how long it took to",
    "start": "1061170",
    "end": "1066810"
  },
  {
    "text": "process",
    "start": "1066810",
    "end": "1069380"
  },
  {
    "text": "next the serializer writes the object we've been working with back to binary a",
    "start": "1071900",
    "end": "1076920"
  },
  {
    "text": "form getting it ready for transport and",
    "start": "1076920",
    "end": "1081440"
  },
  {
    "text": "the transporter transmits the data in batches downstream in this case",
    "start": "1082160",
    "end": "1088160"
  },
  {
    "text": "elasticsearch lastly reporters log",
    "start": "1088160",
    "end": "1093720"
  },
  {
    "text": "things like performance metrics metrics for each step counts of input and output events and failures so we've gone over",
    "start": "1093720",
    "end": "1104100"
  },
  {
    "start": "1103000",
    "end": "1176000"
  },
  {
    "text": "what bender supports sube how to configure it and i literature earlier that bender is designed to be easy to",
    "start": "1104100",
    "end": "1110730"
  },
  {
    "text": "extend so let's add a new operation to",
    "start": "1110730",
    "end": "1115860"
  },
  {
    "text": "transform json something very simple like if key exists replace with value we",
    "start": "1115860",
    "end": "1125640"
  },
  {
    "text": "start with defining our configuration we need a key and value configuration properties the app decorators help with",
    "start": "1125640",
    "end": "1132570"
  },
  {
    "text": "automatic documentation generation and configuration validation and this is",
    "start": "1132570",
    "end": "1143760"
  },
  {
    "text": "what our Yamla configuration will look like",
    "start": "1143760",
    "end": "1147710"
  },
  {
    "text": "and this is a screenshot of the auto-generated HTML documentation every",
    "start": "1150500",
    "end": "1156260"
  },
  {
    "text": "feature and bender it gets this type of documentation finally let's write the",
    "start": "1156260",
    "end": "1165470"
  },
  {
    "text": "basic if payload has key replaced with value it's just as easy to add",
    "start": "1165470",
    "end": "1178010"
  },
  {
    "start": "1176000",
    "end": "1207000"
  },
  {
    "text": "additional support for different types of data as well as different sources and destinations bender is available on",
    "start": "1178010",
    "end": "1184520"
  },
  {
    "text": "github and will also be part of the ADA Bria serverless application repository if you have questions we'll do a Q&A at",
    "start": "1184520",
    "end": "1192110"
  },
  {
    "text": "the end or find us after I'll be handing it off to Matt we'll discuss why we",
    "start": "1192110",
    "end": "1197180"
  },
  {
    "text": "chose AWS server lists thanks a lot",
    "start": "1197180",
    "end": "1204610"
  },
  {
    "start": "1207000",
    "end": "1561000"
  },
  {
    "text": "so hi everyone my name is Matt Weiss and I've run the systems team at next-door for the last six years so that means",
    "start": "1212340",
    "end": "1219460"
  },
  {
    "text": "that I've had kind of the distinct displeasure of having both designed and run the original data pipeline we talked",
    "start": "1219460",
    "end": "1224740"
  },
  {
    "text": "about earlier I wanted to spend a bit of time today touching on why we picked the AWS service offerings for a new pipeline",
    "start": "1224740",
    "end": "1232559"
  },
  {
    "text": "new small companies that don't have a lot of data it might be a really obvious choice the service is easy to use and",
    "start": "1232559",
    "end": "1239980"
  },
  {
    "text": "the cost is virtually nothing when you don't have a ton of data but when you start operating at scale things change",
    "start": "1239980",
    "end": "1246100"
  },
  {
    "text": "right and billions of events a day I think is considered at at scale so we hear this kind of the statement all the",
    "start": "1246100",
    "end": "1252519"
  },
  {
    "text": "time oh the cloud is great but its scale isn't it more expensive than a real data center that kind of depends on your",
    "start": "1252519",
    "end": "1259600"
  },
  {
    "text": "definition of cost so I wanted to walk through the services we use and I'll tell you about a little bit about why we",
    "start": "1259600",
    "end": "1265330"
  },
  {
    "text": "chose each one at the beginning of the",
    "start": "1265330",
    "end": "1270820"
  },
  {
    "text": "presentation Prakash and Slava or Prakash told you that we designed our new data pipeline to be easy to scale",
    "start": "1270820",
    "end": "1276159"
  },
  {
    "text": "and have the absolute minimum and operational overhead our goal is to be to build great community for our members",
    "start": "1276159",
    "end": "1282909"
  },
  {
    "text": "not be great at running complex infrastructure frequently people claim",
    "start": "1282909",
    "end": "1288279"
  },
  {
    "text": "that running your own infrastructure is cheaper than hosted services but when we compared tools like Kafka to Kinesis the",
    "start": "1288279",
    "end": "1295120"
  },
  {
    "text": "raw out-of-pocket costs were actually pretty close more importantly though if we ran it ourselves it would be another",
    "start": "1295120",
    "end": "1301330"
  },
  {
    "text": "tool that our team had to operate monitor and scale as the business grew and we knew from past experience with",
    "start": "1301330",
    "end": "1308049"
  },
  {
    "text": "flume that we wanted to have his hands off of a system it's possible we wanted our developers to feel free to",
    "start": "1308049",
    "end": "1313510"
  },
  {
    "text": "instrument as much or as little of the user experience as necessary without being hampered by concerns of breaking",
    "start": "1313510",
    "end": "1320590"
  },
  {
    "text": "the underlying infrastructure Kinesis",
    "start": "1320590",
    "end": "1326289"
  },
  {
    "text": "gives us streams that are highly flexible performant reliable and scalable and with tools at AWS is",
    "start": "1326289",
    "end": "1332350"
  },
  {
    "text": "supplied we can interact with the streams without writing almost any of our own client code we can even auto",
    "start": "1332350",
    "end": "1337899"
  },
  {
    "text": "scale with streams as our data patterns change ensuring that we don't have to manually monitor and scale things up and",
    "start": "1337899",
    "end": "1343029"
  },
  {
    "text": "down one of it's happening in the old flume pipeline",
    "start": "1343029",
    "end": "1349559"
  },
  {
    "text": "model your servers are always ingesting and forwarding data and that's the basic flume design that generally means that",
    "start": "1349559",
    "end": "1356309"
  },
  {
    "text": "each server has its own local storage where it stores the data as it comes in before the data can be pushed out to",
    "start": "1356309",
    "end": "1362610"
  },
  {
    "text": "your targets and this makes the whole system really susceptible to back logs that can take hours and hours to work",
    "start": "1362610",
    "end": "1367620"
  },
  {
    "text": "through if you pile too much data up on one host there's really nothing you can do to get the data off that host other",
    "start": "1367620",
    "end": "1373559"
  },
  {
    "text": "than wait for it to go newer systems like Apache Kafka and Kinesis both allow",
    "start": "1373559",
    "end": "1378659"
  },
  {
    "text": "you to separate your data ingestion and storage model from your execution layer",
    "start": "1378659",
    "end": "1383669"
  },
  {
    "text": "in both you pay the computer overhead of course of separating these duties but now you can scale these two layers",
    "start": "1383669",
    "end": "1389759"
  },
  {
    "text": "separately the operational benefits of allowing an AWS to handle the execution",
    "start": "1389759",
    "end": "1396840"
  },
  {
    "text": "of our function significantly outweigh any cost savings that we might see by running this on our own ec2 instances",
    "start": "1396840",
    "end": "1402419"
  },
  {
    "text": "lambda scales up quickly when we need it to it skills down or we don't it",
    "start": "1402419",
    "end": "1408659"
  },
  {
    "text": "provides simple management tools for controlling execution and releases and rollback some maintenance windows and",
    "start": "1408659",
    "end": "1414149"
  },
  {
    "text": "all the things you'd really want in a solution like this and we don't have to build or design any of that so the big",
    "start": "1414149",
    "end": "1422999"
  },
  {
    "text": "performance trade-off though that you make when you're using lambda and Kinesis streams together is that you end up executing your function millions and",
    "start": "1422999",
    "end": "1429240"
  },
  {
    "text": "millions of times and only operating on a very small number of events at a time and that's sort of the design of Kinesis",
    "start": "1429240",
    "end": "1436619"
  },
  {
    "text": "and lambdas to do lots and lots of tiny operations over and over and over again when your data target can handle this",
    "start": "1436619",
    "end": "1442320"
  },
  {
    "text": "that model works really well so if you are pushing to elastic search which we do we hit nearly 70,000 events per",
    "start": "1442320",
    "end": "1449820"
  },
  {
    "text": "second going into elastic search without any real problems however if your downstream endpoint is s3 for example",
    "start": "1449820",
    "end": "1456269"
  },
  {
    "text": "and you plan on analyzing your data using Hadoop or Athena or something else having millions of files that have only",
    "start": "1456269",
    "end": "1464190"
  },
  {
    "text": "a few thousand records in them is really inefficient and your developers are gonna be pretty pissed when you tell them that they have to read a hundred",
    "start": "1464190",
    "end": "1470700"
  },
  {
    "text": "and fifty thousand files to get an hour worth of data firehose actually as it turns out is a really great tool for",
    "start": "1470700",
    "end": "1476490"
  },
  {
    "text": "push lots of small data chunks batching them up and dumping them into s3 and we're easy able to easily control the match",
    "start": "1476490",
    "end": "1483130"
  },
  {
    "text": "backs batch size delay etc and we end up being it will continue to operate a really simple streaming architecture",
    "start": "1483130",
    "end": "1489730"
  },
  {
    "text": "rather than adding in any kind of storage or buffering mechanisms into bender which would make it more complicated and more more buggy so I",
    "start": "1489730",
    "end": "1499059"
  },
  {
    "text": "won't spend a ton of time on s3 but it's the gold standard in cloud storage and you know it's fast it's easy to run it",
    "start": "1499059",
    "end": "1505090"
  },
  {
    "text": "works with all of Amazon services newer services like Athena make it really easy to do direct analysis on your data in s3",
    "start": "1505090",
    "end": "1512559"
  },
  {
    "text": "and they and there's such a natural fit that when we were developing vendor late",
    "start": "1512559",
    "end": "1517809"
  },
  {
    "text": "last year the Athena team even reached out to us to have us trial their product with bender because those two fit really",
    "start": "1517809",
    "end": "1523330"
  },
  {
    "text": "well together one thing I'll mention those s3 is not only good for your long-term storage but",
    "start": "1523330",
    "end": "1529360"
  },
  {
    "text": "it's really great as a short-term storage location as I mentioned before we use firehose to dump a near Ross",
    "start": "1529360",
    "end": "1537160"
  },
  {
    "text": "stream of our Kinesis data into a temporary s3 bucket and then from there SNS notifications",
    "start": "1537160",
    "end": "1543400"
  },
  {
    "text": "trigger bender to pick up these files process them pick up the files and",
    "start": "1543400",
    "end": "1549280"
  },
  {
    "text": "process them bender is able to do all of the complex key manipulation and partition the data out the way we want",
    "start": "1549280",
    "end": "1555630"
  },
  {
    "text": "and that keeps our downstream data teams pretty happy and keeps things working well",
    "start": "1555630",
    "end": "1562260"
  },
  {
    "start": "1561000",
    "end": "1743000"
  },
  {
    "text": "this is fun so I wanted to put this into perspective a little bit we've talked about the kinds of problems that could",
    "start": "1565270",
    "end": "1572660"
  },
  {
    "text": "happen with our old pipeline but we didn't really give you anything concrete what times what kinds of issues were we really seeing so this is how bad it was",
    "start": "1572660",
    "end": "1580100"
  },
  {
    "text": "at the end this is a graph of the old pipeline in its final few months of service the y-axis here is a percentage",
    "start": "1580100",
    "end": "1586670"
  },
  {
    "text": "of fill and any time it's filled up at all that's bad if it's filled up a",
    "start": "1586670",
    "end": "1592100"
  },
  {
    "text": "couple of percent you know things can recover on their own generally if it's filled up beyond ten or fifteen percent you start running into real problems so",
    "start": "1592100",
    "end": "1599290"
  },
  {
    "text": "we had weeks upon weeks of regular outages and delays and most of these delays lasted hours to to recover the",
    "start": "1599290",
    "end": "1607970"
  },
  {
    "text": "one really big spike actually represented data loss for us because we actually filled the pipeline up and couldn't stuff anything else into it so",
    "start": "1607970",
    "end": "1614840"
  },
  {
    "text": "during these times our product and engineering teams were partially or completely blind",
    "start": "1614840",
    "end": "1620210"
  },
  {
    "text": "they couldn't develop the product and actually in some cases user-facing fortunes to the website were showing",
    "start": "1620210",
    "end": "1625730"
  },
  {
    "text": "stale data so what did it look like on a new pipeline for the same time frame so",
    "start": "1625730",
    "end": "1634700"
  },
  {
    "text": "to be clear we're looking at a different metric Kinesis doesn't have a concept of fill percentage but rather there's a",
    "start": "1634700",
    "end": "1641090"
  },
  {
    "text": "metric that indicates how far behind now your function is operating so effectively it's a direct measure of how",
    "start": "1641090",
    "end": "1647660"
  },
  {
    "text": "quickly your function is able to keep up with the incoming data flow just to remind you this is the same timeframe as",
    "start": "1647660",
    "end": "1654230"
  },
  {
    "text": "the other graph and the data volumes were exactly the same we were duplicating the data into the old pipeline in the new pipeline the largest",
    "start": "1654230",
    "end": "1660890"
  },
  {
    "text": "spike we have here is less than a minute and I that was probably even self-inflicted somehow so I can't say",
    "start": "1660890",
    "end": "1669410"
  },
  {
    "text": "it's been perfect the whole time but it's been significantly better than the",
    "start": "1669410",
    "end": "1674420"
  },
  {
    "text": "previous graph",
    "start": "1674420",
    "end": "1677110"
  },
  {
    "text": "so by the time we had fully switched over to the Kinesis pipeline our flume",
    "start": "1680430",
    "end": "1686070"
  },
  {
    "text": "pipeline was failing and dropping events and that's it nearly half the data load we currently support so to that and I",
    "start": "1686070",
    "end": "1692760"
  },
  {
    "text": "can't tell you how much it would have cost to actually run the new pipe the old pipeline at our current scale significant architectural changes would",
    "start": "1692760",
    "end": "1699660"
  },
  {
    "text": "have had to happen that said on paper the old pipeline was cheaper on a per billion event cent basis but that",
    "start": "1699660",
    "end": "1707220"
  },
  {
    "text": "doesn't take into account the business impacts of the outages we're dealing with or the engineering time that was required to sort of maintain and run the",
    "start": "1707220",
    "end": "1714690"
  },
  {
    "text": "old system and deal with these outages the old pipeline was delaying product decisions distracting the systems team",
    "start": "1714690",
    "end": "1720650"
  },
  {
    "text": "frustrating dozens of engineers and the new pipeline works so smoothly that I think most of our engineers don't know",
    "start": "1720650",
    "end": "1726390"
  },
  {
    "text": "it exists anymore so on paper at scale",
    "start": "1726390",
    "end": "1731400"
  },
  {
    "text": "the cloud may look a little more expensive but when you start looking at the business impacts and the",
    "start": "1731400",
    "end": "1737220"
  },
  {
    "text": "distractions that it can that running it yourself can create it's it's a different game so want to do I'll take",
    "start": "1737220",
    "end": "1749700"
  },
  {
    "start": "1743000",
    "end": "1798000"
  },
  {
    "text": "you guys through a quick tour of a real pipeline that we created using vendor this is a really simple pipeline we have",
    "start": "1749700",
    "end": "1756990"
  },
  {
    "text": "a basic flow of cloud trail events that go into an AWS hosted elasticsearch",
    "start": "1756990",
    "end": "1762360"
  },
  {
    "text": "service in this example we've built the entire pipeline from start to finish in a single cloud formation stack and the",
    "start": "1762360",
    "end": "1768930"
  },
  {
    "text": "stacks actually going to be available for you to download at the end of the presentation the way this works is that",
    "start": "1768930",
    "end": "1774420"
  },
  {
    "text": "we have cloud trail that writes files into an s3 bucket from there the s3",
    "start": "1774420",
    "end": "1779970"
  },
  {
    "text": "bucket publishes notifications to a topic the topic in turn triggers the execution of our vendor function and",
    "start": "1779970",
    "end": "1786360"
  },
  {
    "text": "then bender uses an iam role to read the original data files from s3 process the",
    "start": "1786360",
    "end": "1791790"
  },
  {
    "text": "data and finally ship them into elasticsearch so I'll walk through some of the the individual components here if",
    "start": "1791790",
    "end": "1799170"
  },
  {
    "start": "1798000",
    "end": "1925000"
  },
  {
    "text": "you're not already familiar with it AWS cloud trail allows you to log every action every API action taken in your",
    "start": "1799170",
    "end": "1804990"
  },
  {
    "text": "account either by you or on your behalf you can configure unique trails per region or you can configure one trail",
    "start": "1804990",
    "end": "1811260"
  },
  {
    "text": "for all your regions here you can see that we've got one trail for kind of all region is going into an s3 bucket and if we look at the",
    "start": "1811260",
    "end": "1820230"
  },
  {
    "text": "s3 bucket we can see all the files of cloud trails uploading and these files are dumped every couple of minutes and then s3 fires off a notification to our",
    "start": "1820230",
    "end": "1827159"
  },
  {
    "text": "function the trick here is that each of these files while being you know 60 K",
    "start": "1827159",
    "end": "1832320"
  },
  {
    "text": "and 38 K and you know fair you know methi each file is one line long it's a",
    "start": "1832320",
    "end": "1838649"
  },
  {
    "text": "single JSON object with an array in it and in the array is all of the actual cloud trail records so that makes",
    "start": "1838649",
    "end": "1844830"
  },
  {
    "text": "processing this data with most tools just pretty obnoxious you certainly can't pipe it directly into",
    "start": "1844830",
    "end": "1849960"
  },
  {
    "text": "elasticsearch when we go when we look at the lambda console we can see how",
    "start": "1849960",
    "end": "1856619"
  },
  {
    "text": "frequently our function is running how long it takes on average min max it you know durations all that you can modify",
    "start": "1856619",
    "end": "1863519"
  },
  {
    "text": "your triggers add more triggers etc and I'll note that lambda doesn't really",
    "start": "1863519",
    "end": "1868799"
  },
  {
    "text": "force you to design your triggers in any particular way the slava alluded to this earlier and that you can have one",
    "start": "1868799",
    "end": "1875159"
  },
  {
    "text": "function that is triggered by dozens of different types of sources or just dozens of different for the same source",
    "start": "1875159",
    "end": "1881039"
  },
  {
    "text": "so Kinesis streams and you know SMS notifications etc we've mirrored most of",
    "start": "1881039",
    "end": "1886440"
  },
  {
    "text": "this into the vendor configuration we do limit you to one type of source but not one source and way you can treat",
    "start": "1886440",
    "end": "1892470"
  },
  {
    "text": "different sources differently so one of the goals of this particular demo right",
    "start": "1892470",
    "end": "1897720"
  },
  {
    "text": "was to have a system that was completely managed by AWS so here we're using the hosted elastic search service as our",
    "start": "1897720",
    "end": "1904169"
  },
  {
    "text": "target CloudFormation makes these things really easy to launch and run and if you've ever run elastic search you know",
    "start": "1904169",
    "end": "1910139"
  },
  {
    "text": "that even the smallest clusters or generally require a little bit of hand-holding and monitoring so using a",
    "start": "1910139",
    "end": "1915659"
  },
  {
    "text": "hosted solution here it's great for this LS bender can push of course to any elastic search cluster but this this",
    "start": "1915659",
    "end": "1923940"
  },
  {
    "text": "made it really easy so let's see here's the end product we",
    "start": "1923940",
    "end": "1932230"
  },
  {
    "start": "1925000",
    "end": "2046000"
  },
  {
    "text": "have a Cabana dashboard served on an AWS managed elasticsearch cluster with data",
    "start": "1932230",
    "end": "1938019"
  },
  {
    "text": "populated by a completely serverless ETL framework while technically there are servers here from our perspective this",
    "start": "1938019",
    "end": "1944110"
  },
  {
    "text": "is a serverless pipeline because we aren't managing any ec2 instances for it so what why did I use this example right",
    "start": "1944110",
    "end": "1951309"
  },
  {
    "text": "why did I do this instead of something more product focused cloud trail logs",
    "start": "1951309",
    "end": "1956740"
  },
  {
    "text": "are one of those great tools that AWS provides but they expect you to do some some work to get value out of them and",
    "start": "1956740",
    "end": "1962700"
  },
  {
    "text": "there's a number of different reasons why you might want to go through them perhaps you're trying to figure out why you're being throttled by route 53 or",
    "start": "1962700",
    "end": "1969600"
  },
  {
    "text": "maybe you have concerns about a security breach and you want to find out which engineer flipped the public bit on an s3",
    "start": "1969600",
    "end": "1974980"
  },
  {
    "text": "bucket and leaked a few million social security numbers whatever the reason never done we",
    "start": "1974980",
    "end": "1980379"
  },
  {
    "text": "haven't done that as far but whatever",
    "start": "1980379",
    "end": "1985899"
  },
  {
    "text": "the reason right parsing these logs is hard and especially since as I mentioned you know they're one-line format makes",
    "start": "1985899",
    "end": "1992110"
  },
  {
    "text": "things a little more difficult so just by logging into the main interface we",
    "start": "1992110",
    "end": "1997179"
  },
  {
    "text": "can quickly see a histogram of API calls and a few of the most recently recorded calls the data is a little dense here",
    "start": "1997179",
    "end": "2004110"
  },
  {
    "text": "but if you look carefully you can see that bender already did some work all the key names are in bold and we've",
    "start": "2004110",
    "end": "2011039"
  },
  {
    "text": "bender as appended the type of data to the key name ensuring that in elasticsearch we don't have any schema",
    "start": "2011039",
    "end": "2016919"
  },
  {
    "text": "conflicts if we dig into a single event",
    "start": "2016919",
    "end": "2022759"
  },
  {
    "text": "we can see the service region event type details about the client you can see",
    "start": "2022759",
    "end": "2027779"
  },
  {
    "text": "who's making the calls and you can even get the you know individual request parameters and so from here you could",
    "start": "2027779",
    "end": "2034200"
  },
  {
    "text": "see how if you knew what you were looking for it would be pretty easy to start searching through and and trace",
    "start": "2034200",
    "end": "2039210"
  },
  {
    "text": "down the actions of a particular particular user or role or account in your in your system but what if you",
    "start": "2039210",
    "end": "2044399"
  },
  {
    "text": "don't know what you're looking for so this is where a dashboard could come",
    "start": "2044399",
    "end": "2050820"
  },
  {
    "start": "2046000",
    "end": "2197000"
  },
  {
    "text": "in really handy here's a this is a quick simple dashboard that gives you a few views and kind of the most 8 the the",
    "start": "2050820",
    "end": "2056669"
  },
  {
    "text": "different types of API call service by type by user over a period of time and",
    "start": "2056669",
    "end": "2062550"
  },
  {
    "text": "sort of from this board you could start filtering for what you care about and as you can see here the vast majority of",
    "start": "2062550",
    "end": "2068908"
  },
  {
    "text": "our calls are from cloud help and Amazon and data dog and so maybe we want to",
    "start": "2068909",
    "end": "2074550"
  },
  {
    "text": "filter those out so now as we filtered that out we start to see kind of a more",
    "start": "2074550",
    "end": "2079710"
  },
  {
    "text": "expected pattern among the users in our account and in our environment we as",
    "start": "2079710",
    "end": "2085080"
  },
  {
    "text": "octa to delegate short-term access to our Amazon resources through sam'l assertions and iam roles and so maybe in",
    "start": "2085080",
    "end": "2091408"
  },
  {
    "text": "this account we don't expect to see very many API calls from that particular role",
    "start": "2091409",
    "end": "2096530"
  },
  {
    "text": "so we zoom in on this particular role and we can start to see what they're up to we see some describe calls probably",
    "start": "2096530",
    "end": "2103470"
  },
  {
    "text": "not that worrisome but you also see a stop task and a change resource record",
    "start": "2103470",
    "end": "2108720"
  },
  {
    "text": "sets call and if this is your production account maybe that was maybe that's worrisome maybe someone's messing with your dns now if we care about those",
    "start": "2108720",
    "end": "2117810"
  },
  {
    "text": "calls maybe we want to find out where they came from so Amazon embeds the source IP into all of the all of the",
    "start": "2117810",
    "end": "2125910"
  },
  {
    "text": "cloud trail records so what can we do here so using a bender operator we're",
    "start": "2125910",
    "end": "2134910"
  },
  {
    "text": "able to enrich the cloud trail data with the Geo IP database as it flows through the pipeline this makes it really simple",
    "start": "2134910",
    "end": "2141540"
  },
  {
    "text": "to plot the location of all the API calls on a map and look for outliers now our office is in San Francisco so",
    "start": "2141540",
    "end": "2147510"
  },
  {
    "text": "the big red dot makes a ton of sense but maybe the other blue dots especially down in Santa Clara don't make that much",
    "start": "2147510",
    "end": "2153060"
  },
  {
    "text": "sense for us so we're able to select the area using some mapping tools and that",
    "start": "2153060",
    "end": "2159240"
  },
  {
    "text": "automatically creates some latitude longitude filters for us and then we can use that filter to go in back into the",
    "start": "2159240",
    "end": "2165270"
  },
  {
    "text": "search and by the search view and look at the individual calls that are being made apparently I like to work remotely",
    "start": "2165270",
    "end": "2172560"
  },
  {
    "text": "so those API calls for actually me probably working out of a Starbucks somewhere but using vendor to enrich the",
    "start": "2172560",
    "end": "2179580"
  },
  {
    "text": "data as it goes through the pipeline is a really efficient way about sort of relevant metadata as without",
    "start": "2179580",
    "end": "2185860"
  },
  {
    "text": "adding a whole processing job down the road you could see how this might be useful with other data sources perhaps",
    "start": "2185860",
    "end": "2192250"
  },
  {
    "text": "looking for where ad purchases are coming from and spotting concerning trends and so the pipeline we've demoed",
    "start": "2192250",
    "end": "2199900"
  },
  {
    "start": "2197000",
    "end": "2255000"
  },
  {
    "text": "here is actually a real pipeline we run as Prakash and Slava mentioned right we run a much larger pipeline for all of",
    "start": "2199900",
    "end": "2207130"
  },
  {
    "text": "our logs and events that we transform into business dashboards and product decisions and even to customize the next",
    "start": "2207130",
    "end": "2212980"
  },
  {
    "text": "dorks experience for each of our users but it doesn't mean it's our only pipeline as you grow you may find you",
    "start": "2212980",
    "end": "2219130"
  },
  {
    "text": "have needs for many different pipelines each with their own data sources transformations and outputs that are",
    "start": "2219130",
    "end": "2224410"
  },
  {
    "text": "required to make the data usable your IT team might want to run a pipeline like this to keep a close eye on security",
    "start": "2224410",
    "end": "2230470"
  },
  {
    "text": "events or your engineers might want to run a pipeline to track alb and alb request logs or your product might need",
    "start": "2230470",
    "end": "2237070"
  },
  {
    "text": "to be able to trance take Kinesis stream events and pipe them into API endpoints",
    "start": "2237070",
    "end": "2242880"
  },
  {
    "text": "rather than rebuilding the wheel each time using a single tool that has all the boilerplate behaviors built in",
    "start": "2242880",
    "end": "2249340"
  },
  {
    "text": "provide serious leverage and helps your team's focus on the real work that they need to get done so just a reminder the",
    "start": "2249340",
    "end": "2260020"
  },
  {
    "start": "2255000",
    "end": "2302000"
  },
  {
    "text": "system seems very small we've talked about that a few times and we managed hundreds of servers and billions of",
    "start": "2260020",
    "end": "2265390"
  },
  {
    "text": "events daily and in order to do that we have to automate everything and we operate only the things we really really",
    "start": "2265390",
    "end": "2271720"
  },
  {
    "text": "have to operate so lambda Kinesis firehose and s3 have allowed us to take this sort of traditionally finicky piece",
    "start": "2271720",
    "end": "2277840"
  },
  {
    "text": "of infrastructure to a service that scales and handles failures automatically the building vendor has",
    "start": "2277840",
    "end": "2284080"
  },
  {
    "text": "given us a single ETL tool that we leverage across multiple teams and products rather than building custom",
    "start": "2284080",
    "end": "2289960"
  },
  {
    "text": "tools each time anita comes up our reliability has gone up dramatically and the effort we spend of running this",
    "start": "2289960",
    "end": "2295690"
  },
  {
    "text": "stuff now is near zero so we appreciate everyone coming out today we're open to Q&A now and of course you can always",
    "start": "2295690",
    "end": "2303910"
  },
  {
    "start": "2302000",
    "end": "2437000"
  },
  {
    "text": "talk to us after and you know we have jobs",
    "start": "2303910",
    "end": "2309570"
  },
  {
    "text": "thank you guys have any questions",
    "start": "2309690",
    "end": "2319800"
  },
  {
    "text": "yep in the blue",
    "start": "2321360",
    "end": "2325410"
  },
  {
    "text": "so the question was correct me if I get this right or wrong the question was are",
    "start": "2332760",
    "end": "2337930"
  },
  {
    "text": "we having problems with file sizes on s3 downstream after Oh after fire us no we",
    "start": "2337930",
    "end": "2347049"
  },
  {
    "text": "don't generally have that problem firehose limits the file size is appropriately to certain maxes that they",
    "start": "2347049",
    "end": "2352900"
  },
  {
    "text": "have and we just generally use their max and then when bender picks those files",
    "start": "2352900",
    "end": "2358329"
  },
  {
    "text": "up it it's partitioning just that one file at a time and so we just end up with",
    "start": "2358329",
    "end": "2363520"
  },
  {
    "text": "smaller files after that",
    "start": "2363520",
    "end": "2366750"
  },
  {
    "text": "sure so I think the question was you know what kinds of problems have we had",
    "start": "2376730",
    "end": "2382170"
  },
  {
    "text": "with the new pipeline",
    "start": "2382170",
    "end": "2384710"
  },
  {
    "text": "the most I'll give you an example the most recent one we we end up now that we",
    "start": "2398610",
    "end": "2404380"
  },
  {
    "text": "have this pipe restate that question alright sorry so that the question was what kinds of problems have we had",
    "start": "2404380",
    "end": "2409570"
  },
  {
    "text": "moving to this new pipeline and a recent example is we had a 20 minute sort of mysterious blockage of data going into",
    "start": "2409570",
    "end": "2415810"
  },
  {
    "text": "Kinesis and that backed up on our servers and as we mentioned we run spot machines and that meant that there was",
    "start": "2415810",
    "end": "2422500"
  },
  {
    "text": "this window of some some amount of panic as we tried to figure out what was happening it resolved itself and",
    "start": "2422500",
    "end": "2428350"
  },
  {
    "text": "everything flowed through quickly after that and it's dramatically better than the types of problems we'd have before",
    "start": "2428350",
    "end": "2434170"
  },
  {
    "text": "but it still can't happen absolutely right I think to to add the",
    "start": "2434170",
    "end": "2440680"
  },
  {
    "start": "2437000",
    "end": "2687000"
  },
  {
    "text": "biggest limitation I've had with kind of dealing with lambda is for instance our",
    "start": "2440680",
    "end": "2447160"
  },
  {
    "text": "functions get executed like 25 to 30 million times a day so if there's a",
    "start": "2447160",
    "end": "2452680"
  },
  {
    "text": "failure cloud watch logs are very very hard to look through and I think that's",
    "start": "2452680",
    "end": "2460210"
  },
  {
    "text": "as we have more teams start to use bender or other lambda functions because",
    "start": "2460210",
    "end": "2467890"
  },
  {
    "text": "we we really pioneered that at the company is we need to think about how we",
    "start": "2467890",
    "end": "2473380"
  },
  {
    "text": "collect that log data but that's not too dissimilar from like if you run flume or",
    "start": "2473380",
    "end": "2480670"
  },
  {
    "text": "cough or whatever it generates logs I don't know if you wanted like kind of send it back into itself you first",
    "start": "2480670",
    "end": "2499200"
  },
  {
    "text": "repeat the question right so the question was what kind of error handling that did we need to put in to our lambda",
    "start": "2511010",
    "end": "2517940"
  },
  {
    "text": "function so for example elasticsearch",
    "start": "2517940",
    "end": "2523180"
  },
  {
    "text": "can sometimes be backed up a little bit and if you're making multiple API calls",
    "start": "2523180",
    "end": "2530330"
  },
  {
    "text": "to elasticsearch you're not guaranteed that you had the same nodes so even if",
    "start": "2530330",
    "end": "2536960"
  },
  {
    "text": "we retry like three times within the function that that is much better if we",
    "start": "2536960",
    "end": "2542360"
  },
  {
    "text": "can retry in batches in small batches rather than retrying in like 10,000",
    "start": "2542360",
    "end": "2547370"
  },
  {
    "text": "events or at one point we decided to like use s3 files directly to",
    "start": "2547370",
    "end": "2554530"
  },
  {
    "text": "elasticsearch in a kind of keeled over and died ya know so this is this is",
    "start": "2554530",
    "end": "2567080"
  },
  {
    "text": "important and this is one of the things we like about this infrastructure but took us a little while to sort of understand the impacts of you just fail",
    "start": "2567080",
    "end": "2575180"
  },
  {
    "text": "the function and you'll end any amazon will rerun it for you on their own and so there's actually because you're being",
    "start": "2575180",
    "end": "2582860"
  },
  {
    "text": "billed per you know per millisecond and in lambda if you sit there and you say well retry and then back off five",
    "start": "2582860",
    "end": "2589010"
  },
  {
    "text": "seconds and retry again and back off for ten seconds and retry again now you're wasting time where you're being billed",
    "start": "2589010",
    "end": "2594650"
  },
  {
    "text": "for it your functions running for that whole time it's sitting idle in most cases we've actually tuned most of our",
    "start": "2594650",
    "end": "2601130"
  },
  {
    "text": "our retries down to one or two and then we let the function fail now that's not",
    "start": "2601130",
    "end": "2606170"
  },
  {
    "text": "entire that that's true when you're operating off Kinesis streams if you're operating off of an SNS notification",
    "start": "2606170",
    "end": "2612080"
  },
  {
    "text": "from an s3 bucket then your sort of rules change a little bit",
    "start": "2612080",
    "end": "2617560"
  },
  {
    "text": "oh sorry with disability",
    "start": "2620100",
    "end": "2627370"
  },
  {
    "text": "so the question was do we keep everything separated in individual regions or do we group it all in one",
    "start": "2631829",
    "end": "2638670"
  },
  {
    "text": "right and so we pipe all of our data into one region today so while we run",
    "start": "2638670",
    "end": "2643710"
  },
  {
    "text": "the website out of many regions we we have one area where we do our RBI and analysis and so we pipe it all there yes",
    "start": "2643710",
    "end": "2679740"
  },
  {
    "text": "I spent a lot of time on the phone with engineers I I think it's a you know",
    "start": "2679740",
    "end": "2688099"
  },
  {
    "start": "2687000",
    "end": "2887000"
  },
  {
    "text": "double-edged sword if you will it depends I would say it depends on your",
    "start": "2688099",
    "end": "2693920"
  },
  {
    "text": "final destination right if you're hitting something that's like an API or",
    "start": "2693920",
    "end": "2700890"
  },
  {
    "text": "a service like elastic search it can only handle so many requests at a time",
    "start": "2700890",
    "end": "2706380"
  },
  {
    "text": "right alternatively it can only handle like so much data per request",
    "start": "2706380",
    "end": "2712859"
  },
  {
    "text": "so there is a balancing act to perform so we found that if we tuned down the",
    "start": "2712859",
    "end": "2720660"
  },
  {
    "text": "number of shards at for our case for elastic search it was much better for it",
    "start": "2720660",
    "end": "2727140"
  },
  {
    "text": "and then what is going to say we're going going into firehose for example it",
    "start": "2727140",
    "end": "2733500"
  },
  {
    "text": "doesn't seem to matter fire host just gobbles data up as fast as it can but elasticsearch we we",
    "start": "2733500",
    "end": "2740789"
  },
  {
    "text": "absolutely ran into an issue or we it said why is the cluster performing so badly and we don't understand it worked",
    "start": "2740789",
    "end": "2748319"
  },
  {
    "text": "fine and the date of volume was the same but it was the number we had almost doubled the number of executions of the",
    "start": "2748319",
    "end": "2754680"
  },
  {
    "text": "lambda function which doubled the number of sort of simultaneous puts into the cluster so that's it's a",
    "start": "2754680",
    "end": "2761080"
  },
  {
    "text": "trade-off right I mean I think just to add to that is if you have too much",
    "start": "2761080",
    "end": "2767980"
  },
  {
    "text": "capacity with your shards that could also potentially be an issue because",
    "start": "2767980",
    "end": "2773890"
  },
  {
    "text": "then you're kind of wasting execution time because your function may just get",
    "start": "2773890",
    "end": "2778900"
  },
  {
    "text": "one single record that it's handling and that it's just expensive so yeah it's a",
    "start": "2778900",
    "end": "2786400"
  },
  {
    "text": "balancing act right",
    "start": "2786400",
    "end": "2789450"
  },
  {
    "text": "I'm not writing she's sorry well so I think the question was how do we tune the RS 3 puts at the end of the day so",
    "start": "2814490",
    "end": "2820670"
  },
  {
    "text": "that when we are putting files down we're putting reasonably sized files down in s3 and as I mentioned earlier we",
    "start": "2820670",
    "end": "2826970"
  },
  {
    "text": "use firehose to do some batching and so that helps and so during we use the Mac's batch sizes for fire house so it's",
    "start": "2826970",
    "end": "2832430"
  },
  {
    "text": "like you know 300 seconds or well 120 Meg's of compressed e or 128 Meg's of",
    "start": "2832430",
    "end": "2839000"
  },
  {
    "text": "compressed data and when when our function picks that 128 Meg file up it",
    "start": "2839000",
    "end": "2845180"
  },
  {
    "text": "it may split that into three files 30 M xzh or 40 Meg's each or it might split",
    "start": "2845180",
    "end": "2852020"
  },
  {
    "text": "it into 50 and we don't really optimize for that that's the that's the long winded version of the saying we don't we",
    "start": "2852020",
    "end": "2858320"
  },
  {
    "text": "don't really pay that much attention to it we in the in the events in the cases where we're creating a very small file",
    "start": "2858320",
    "end": "2865160"
  },
  {
    "text": "with very few records we generally just don't have that many of those records in the first place so the downstream",
    "start": "2865160",
    "end": "2871820"
  },
  {
    "text": "impacts are not that significant",
    "start": "2871820",
    "end": "2875440"
  },
  {
    "start": "2887000",
    "end": "3202000"
  },
  {
    "text": "well I think the sorry to rephrase the question why not use EMR for this well",
    "start": "2887260",
    "end": "2896240"
  },
  {
    "text": "that's just another piece of infrastructure that you need to manage and operate and have alarms on and it's",
    "start": "2896240",
    "end": "2902810"
  },
  {
    "text": "I think relatively complicated right now that being said we do use Hadoop and",
    "start": "2902810",
    "end": "2911000"
  },
  {
    "text": "hive and tez on our data team separately but that's a dedicated team that manages",
    "start": "2911000",
    "end": "2919099"
  },
  {
    "text": "those resources yeah I think maybe another way to think about it is that there's there's clearly a business use",
    "start": "2919099",
    "end": "2924890"
  },
  {
    "text": "case for something like EMR in our case we use a third party product called cubile but if you think about this as",
    "start": "2924890",
    "end": "2933010"
  },
  {
    "text": "transporting data around really so it's a transportation mechanism as opposed to",
    "start": "2933010",
    "end": "2938030"
  },
  {
    "text": "a true analysis sort of engine that's kind of how we created some differentiation for the roles in the",
    "start": "2938030",
    "end": "2944150"
  },
  {
    "text": "infrastructure which is we need an efficient platform to move data into an eventual data store that we can then",
    "start": "2944150",
    "end": "2950180"
  },
  {
    "text": "analyze and it may be that we use extensions of this to do things that are later more kind of real-time II or",
    "start": "2950180",
    "end": "2957290"
  },
  {
    "text": "things like that but we haven't yet found any use cases that sort of serve that so and also it's you know I'll",
    "start": "2957290",
    "end": "2965569"
  },
  {
    "text": "point out that by creating a small function that does sort of the same",
    "start": "2965569",
    "end": "2970579"
  },
  {
    "text": "thing over and over again it's just an easier system to build and test and and extend and we mentioned this briefly in",
    "start": "2970579",
    "end": "2977839"
  },
  {
    "text": "there where we have support for sumo logic and Splunk and scaler as outputs and as well as just sort of generic HTTP",
    "start": "2977839",
    "end": "2984349"
  },
  {
    "text": "endpoints literally like being a bolt those on was a matter of like an hour of",
    "start": "2984349",
    "end": "2990230"
  },
  {
    "text": "work in each case and and being able to do that on a very small tightly controlled platform where you can even",
    "start": "2990230",
    "end": "2996170"
  },
  {
    "text": "run it all and test it all locally on your laptop is pretty valuable",
    "start": "2996170",
    "end": "3000930"
  },
  {
    "text": "so the question was do we have any use cases where we need to preserve ordering no we don't that's not well okay",
    "start": "3008599",
    "end": "3016729"
  },
  {
    "text": "elasticsearch preserves ordering in the sense that we tell at which time what",
    "start": "3016729",
    "end": "3021809"
  },
  {
    "text": "the time field is when we're inserting the data and so that's some semblance of ordering we yeah we actually a bender",
    "start": "3021809",
    "end": "3029670"
  },
  {
    "text": "can actually process and find the timestamp field and do some parsing and then oh and then it can then when you",
    "start": "3029670",
    "end": "3036299"
  },
  {
    "text": "set your schema up an elastic search you can sort of tell it where that timestamp field is gonna be but generally speaking",
    "start": "3036299",
    "end": "3042359"
  },
  {
    "text": "Kinesis is not an ordered system right if once you go beyond one shard it's sort of unordered so to that sense no",
    "start": "3042359",
    "end": "3051890"
  },
  {
    "text": "sorry yes",
    "start": "3051890",
    "end": "3055068"
  },
  {
    "text": "so the question was you know for for sort of ree tribal transient failures",
    "start": "3068100",
    "end": "3073240"
  },
  {
    "text": "you let Amazon rerun the function but for fatal exceptions you know what do we do a good example is if you have say a",
    "start": "3073240",
    "end": "3081640"
  },
  {
    "text": "bad schema in elasticsearch and you so now you can't pop you can't push data at all there have been times where we've",
    "start": "3081640",
    "end": "3090040"
  },
  {
    "text": "swallowed those and and figured it out later and then there I think now we take",
    "start": "3090040",
    "end": "3096730"
  },
  {
    "text": "more of approach of you don't let it break the pipeline and we get alerted very quickly when things back up and we",
    "start": "3096730",
    "end": "3102280"
  },
  {
    "text": "can start going so by break I mean just fail and if all the functions start",
    "start": "3102280",
    "end": "3108160"
  },
  {
    "text": "failing then the metric I showed you guys earlier the the sort of iterator",
    "start": "3108160",
    "end": "3113470"
  },
  {
    "text": "age or the function you know how far behind now the function is very quickly ramps up and it's usually really easy to",
    "start": "3113470",
    "end": "3119440"
  },
  {
    "text": "see in that pattern like oh it's perfectly up into the right it's not you know it's not waving anyway so that's",
    "start": "3119440",
    "end": "3125590"
  },
  {
    "text": "usually a good indication that you've completely broken some downstream end point and I would just add one thing",
    "start": "3125590",
    "end": "3131770"
  },
  {
    "text": "which is I think it is a little bit use case dependent because in some cases we can respond to failures like that by",
    "start": "3131770",
    "end": "3138280"
  },
  {
    "text": "republishing data and in other cases it's not so easy to go back to you know",
    "start": "3138280",
    "end": "3143380"
  },
  {
    "text": "the point of failure and figure out exactly what was coming through and in those cases we we have to be careful",
    "start": "3143380",
    "end": "3149980"
  },
  {
    "text": "right and and so far we haven't had anything fatal happen I wouldn't say that we have the the best sort of most",
    "start": "3149980",
    "end": "3155740"
  },
  {
    "text": "robust robust answer to this some of it goes back someone had mentioned like do you do like cyclical stuff like we",
    "start": "3155740",
    "end": "3161920"
  },
  {
    "text": "published something back through and you know even to log something like that this is our log pipe pipe line yeah it's",
    "start": "3161920",
    "end": "3169810"
  },
  {
    "text": "sort of a weird kind of like recursive thing that happens right another thing",
    "start": "3169810",
    "end": "3175120"
  },
  {
    "text": "that I think could be useful in this situation is if you have distinct platforms for how you for example we use",
    "start": "3175120",
    "end": "3182290"
  },
  {
    "text": "data dog right and data dog is sort of our metrics platform so we can say failure failure failure failure over",
    "start": "3182290",
    "end": "3189220"
  },
  {
    "text": "there and that's informative and that has kind of an alerting system and over here it's really our delivery pipeline",
    "start": "3189220",
    "end": "3195130"
  },
  {
    "text": "into our eventual data store so I think there's some room for us to use both of those systems to kind of",
    "start": "3195130",
    "end": "3201040"
  },
  {
    "text": "keep track of one another a little bit so right just add I think it really depends on what your source trigger is",
    "start": "3201040",
    "end": "3207550"
  },
  {
    "start": "3202000",
    "end": "3600000"
  },
  {
    "text": "right if it's Kinesis and we built it in",
    "start": "3207550",
    "end": "3212920"
  },
  {
    "text": "where if there is a failure you can just swallow it and just the functional will",
    "start": "3212920",
    "end": "3220660"
  },
  {
    "text": "keep going but in our case we just want to stop what that shard is on and just",
    "start": "3220660",
    "end": "3227080"
  },
  {
    "text": "keep retrying now the caveat is it's gonna be back up but we have 24 hours to",
    "start": "3227080",
    "end": "3234280"
  },
  {
    "text": "recover that or configurable up to seven days right so we're not going to lose",
    "start": "3234280",
    "end": "3240790"
  },
  {
    "text": "data slightly better than the old pipeline where we usually had about 40 minutes before if things were still",
    "start": "3240790",
    "end": "3247810"
  },
  {
    "text": "backed up then then it would go south and it would be backed up for like eight or ten hours so there was another",
    "start": "3247810",
    "end": "3253720"
  },
  {
    "text": "question over here yes",
    "start": "3253720",
    "end": "3258510"
  },
  {
    "text": "so the question was you know if there's a long-term outage sort of what's the plan and it does depend a little bit on",
    "start": "3273770",
    "end": "3279320"
  },
  {
    "text": "where that outage is so if there's a long-term lamda outage which that that's definitely happened a few times in the",
    "start": "3279320",
    "end": "3285770"
  },
  {
    "text": "last six months I think generally our plan is right now is to just let it be",
    "start": "3285770",
    "end": "3292790"
  },
  {
    "text": "it and Amazon's gonna work on recovering it faster than we could make any major changes and as we said you have you know",
    "start": "3292790",
    "end": "3300290"
  },
  {
    "text": "you have at least 24 hours of sort of data hold in Kinesis and so we don't have to worry about it sort of filling",
    "start": "3300290",
    "end": "3306110"
  },
  {
    "text": "up in that sense if your problem is it the Kinesis level it depends on how",
    "start": "3306110",
    "end": "3311780"
  },
  {
    "text": "quickly your machines come and go on the machine wait wait you know and some of our machines like datastore machines we",
    "start": "3311780",
    "end": "3319250"
  },
  {
    "text": "could let it backup for days and it wouldn't be that big a deal but we run ECS and docker and some of our machines",
    "start": "3319250",
    "end": "3326570"
  },
  {
    "text": "live for an hour or less so in that case like our sort of a",
    "start": "3326570",
    "end": "3331700"
  },
  {
    "text": "standard procedure is to stop any scaling activities that we can control and then work as quickly as we can to",
    "start": "3331700",
    "end": "3338240"
  },
  {
    "text": "figure out what Amazon what's going on if we had to it wouldn't it would not be difficult to roll out a change to just",
    "start": "3338240",
    "end": "3345320"
  },
  {
    "text": "change the stream target but there are some consequences of doing that so we",
    "start": "3345320",
    "end": "3351020"
  },
  {
    "text": "haven't had to do anything like that yet yes",
    "start": "3351020",
    "end": "3356770"
  },
  {
    "text": "mm-hmm I so the question was a little more",
    "start": "3369870",
    "end": "3378190"
  },
  {
    "text": "direct around what is the cost impact of doing this you know all in Amazon service environment rather than running",
    "start": "3378190",
    "end": "3383260"
  },
  {
    "text": "it yourself and I had a slide actually and I removed it where I had I actually",
    "start": "3383260",
    "end": "3388750"
  },
  {
    "text": "worked out the actual per billion event cost and it was they I didn't think",
    "start": "3388750",
    "end": "3394359"
  },
  {
    "text": "Amazon want me to put that up here it was reasonably more expensive so 30 40 %",
    "start": "3394359",
    "end": "3402460"
  },
  {
    "text": "more expensive now again it was really really hard for us to look at the data",
    "start": "3402460",
    "end": "3407470"
  },
  {
    "text": "though and decide if that was accurate because the old pipeline was losing events and to some extent we that that",
    "start": "3407470",
    "end": "3413920"
  },
  {
    "text": "might have been us sort of trying to operate the old pipeline to efficiently because when you are managing your",
    "start": "3413920",
    "end": "3419650"
  },
  {
    "text": "individual resources your individual servers I think you have a tendency to look at them and go why is this machine",
    "start": "3419650",
    "end": "3426310"
  },
  {
    "text": "running at 5% why can't I run it at 70% right and the you know and the answer",
    "start": "3426310",
    "end": "3432430"
  },
  {
    "text": "there is that on these traditional pipelines you can't really scale them up and down very easily so you sort of have",
    "start": "3432430",
    "end": "3438880"
  },
  {
    "text": "to always build for what scale limit you want to be able to hit so yeah it's",
    "start": "3438880",
    "end": "3444369"
  },
  {
    "text": "definitely more expensive no question the but the trade-off for us is that",
    "start": "3444369",
    "end": "3449410"
  },
  {
    "text": "when we have big spikes we don't notice them when our data patterns just change",
    "start": "3449410",
    "end": "3455170"
  },
  {
    "text": "over time we don't really notice them the only place we actually notice when when our developers have suddenly turned",
    "start": "3455170",
    "end": "3461190"
  },
  {
    "text": "the trace debugging you know the trace bit on on some application is in elastic",
    "start": "3461190",
    "end": "3466900"
  },
  {
    "text": "search where we still have to sort of manage that to to our max capacity and that that's kind of our early canary to",
    "start": "3466900",
    "end": "3472990"
  },
  {
    "text": "tell when something's happened but no it's it's a trade-off the other thing I would say at kind of the highest level",
    "start": "3472990",
    "end": "3479740"
  },
  {
    "text": "when we think about our infrastructure costs and we sort of itemized those we're all we're always stack ranking",
    "start": "3479740",
    "end": "3485380"
  },
  {
    "text": "where are we spending the most money are we doing that responsibly and efficiently and what are we trading off",
    "start": "3485380",
    "end": "3490690"
  },
  {
    "text": "in terms of what we get for that money that we spent right so as an example we mentioned we use some third-party",
    "start": "3490690",
    "end": "3496119"
  },
  {
    "text": "providers for things like email infrastructure and other other sorts of stuff and I think it's a very similar",
    "start": "3496119",
    "end": "3502830"
  },
  {
    "text": "it's a similar process right I don't know that we eventually get to the same outcomes but as things start to bubble",
    "start": "3502830",
    "end": "3509380"
  },
  {
    "text": "up as larger expenses then clearly we take a much much closer look at it in",
    "start": "3509380",
    "end": "3515410"
  },
  {
    "text": "this particular case there was an old framework that we use at Google for",
    "start": "3515410",
    "end": "3520840"
  },
  {
    "text": "thinking you know this is back like 10 years ago for thinking about the opportunity cost of an engineer and the",
    "start": "3520840",
    "end": "3526150"
  },
  {
    "text": "opportunity cost for one engineer was something like a million dollars or something like that and and that's",
    "start": "3526150",
    "end": "3531190"
  },
  {
    "text": "basically how much value could one engineer deliver for the company and so what's the threshold at which you trade",
    "start": "3531190",
    "end": "3537790"
  },
  {
    "text": "them off to do something that you get otherwise sort of you know throw throw",
    "start": "3537790",
    "end": "3543010"
  },
  {
    "text": "something else at right throw throw a hosted solution at so for us we kind of keep that in our mind as well yeah",
    "start": "3543010",
    "end": "3555810"
  },
  {
    "text": "yeah we think about it all the time we think about it all the time and it's kind of a long-term versus short-term decision as well so they we last one",
    "start": "3557300",
    "end": "3567080"
  },
  {
    "text": "probably here yeah",
    "start": "3567080",
    "end": "3570070"
  },
  {
    "text": "these guys here they are responsible for everything that represents kind of",
    "start": "3577019",
    "end": "3582689"
  },
  {
    "text": "infrastructure and the company so anything involving the codification",
    "start": "3582689",
    "end": "3588079"
  },
  {
    "text": "deployment and provisioning of infrastructure that's used by all the teams resides in these two plus one",
    "start": "3588079",
    "end": "3594329"
  },
  {
    "text": "other persons capable hands",
    "start": "3594329",
    "end": "3598699"
  },
  {
    "text": "we've been AWS customers now since 2008 so that includes the company that we",
    "start": "3605260",
    "end": "3611990"
  },
  {
    "text": "started before next door we've been pretty happy with them and I think as",
    "start": "3611990",
    "end": "3618349"
  },
  {
    "text": "you start to continue your relationship with AWS they afford you you know different benefits that that help you",
    "start": "3618349",
    "end": "3625220"
  },
  {
    "text": "kind of stay in and as you start to use more and more of the services then it's really hard to switch so we we have not",
    "start": "3625220",
    "end": "3631490"
  },
  {
    "text": "really given serious thought to switching yeah so I think our times up",
    "start": "3631490",
    "end": "3637760"
  },
  {
    "text": "but yep but thank you everyone world will hang around chatting outside [Applause]",
    "start": "3637760",
    "end": "3647359"
  }
]