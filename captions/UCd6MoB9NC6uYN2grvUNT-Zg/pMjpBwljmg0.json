[
  {
    "start": "0",
    "end": "15000"
  },
  {
    "text": "In this video, you'll see how you can use modern data\narchitecture for purpose-built data stores on AWS. ",
    "start": "160",
    "end": "5720"
  },
  {
    "text": "With this approach, you can seamlessly\nmove data between data stores,",
    "start": "6301",
    "end": "9600"
  },
  {
    "text": "deploy machine learning (ML) models, and derive \nanalytical data insights using federated queries.",
    "start": "9600",
    "end": "15264"
  },
  {
    "text": "Let's begin with an example of moving data from \nAmazon Relational Database Service (Amazon RDS)",
    "start": "16282",
    "end": "21642"
  },
  {
    "text": "to Amazon Simple Storage Service (Amazon S3).",
    "start": "21642",
    "end": "24582"
  },
  {
    "text": "Here, you can see a PostgreSQL \ndatabase in Amazon RDS.",
    "start": "25441",
    "end": "28628"
  },
  {
    "text": "To perform the migration, we defined our source and target endpoints\nin AWS Database Migration Service (AWS DMS).",
    "start": "30880",
    "end": "37975"
  },
  {
    "text": "We also have a change data capture (CDC) endpoint for collecting\nchanges to the source data store during the migration.",
    "start": "38722",
    "end": "44934"
  },
  {
    "text": "A replication instance was set \nup to perform the data migration.",
    "start": "47322",
    "end": "50281"
  },
  {
    "text": "We also have a database migration \ntask defined to perform a full dump.",
    "start": "52742",
    "end": "56395"
  },
  {
    "text": "The full dump task connects to the source endpoint, reads it,\nand formats it into a schema for the target endpoint.",
    "start": "58812",
    "end": "64533"
  },
  {
    "text": "We also applied mapping rules for the schemas \nand table names included in the source database.",
    "start": "66924",
    "end": "71234"
  },
  {
    "text": "The database migration task dumps data in \nour S3 bucket, which acts as our data lake.",
    "start": "72384",
    "end": "76864"
  },
  {
    "text": "Here we can see the data that was dumped\nby the database migration task.",
    "start": "77802",
    "end": "81068"
  },
  {
    "text": "It’s composed of player, stadium, and ticket data\nfor professional sporting events.",
    "start": "81231",
    "end": "85076"
  },
  {
    "text": "Our second example will show data \nmovement between AWS Glue and S3.",
    "start": "87212",
    "end": "91051"
  },
  {
    "text": "This two-step process involves running a data \ncrawler and then translating the data format.",
    "start": "93542",
    "end": "97779"
  },
  {
    "text": "We defined our Glue crawler to crawl all \nfolders of the destination data store,",
    "start": "100202",
    "end": "104122"
  },
  {
    "text": "which is the \"tickets\" folder in our S3 data bucket.",
    "start": "104122",
    "end": "106803"
  },
  {
    "text": "We established an AWS Identity and Access Management\n(AWS IAM) role with the necessary permissions,",
    "start": "110052",
    "end": "116274"
  },
  {
    "text": "created a schedule for the crawler,\nand configured a database for the crawler’s output.",
    "start": "116274",
    "end": "120364"
  },
  {
    "text": "Here, the crawler returned all\nthe tables in the database.",
    "start": "122402",
    "end": "125020"
  },
  {
    "text": "We can validate the schema before starting an extract,\ntransform, and load (ETL) job.",
    "start": "125335",
    "end": "130026"
  },
  {
    "text": "Let's view the \"person\" table.",
    "start": "130567",
    "end": "131923"
  },
  {
    "text": "The data files are stored in CSV format.",
    "start": "134303",
    "end": "136623"
  },
  {
    "text": "When the Glue crawler cannot define a header type from\nthe source, as in the first column here,",
    "start": "138641",
    "end": "143029"
  },
  {
    "text": "the name defaults to a numerical placeholder.",
    "start": "143029",
    "end": "145156"
  },
  {
    "text": "We can edit the schema so that \nthe data is properly transformed.",
    "start": "145564",
    "end": "148506"
  },
  {
    "start": "155000",
    "end": "216000"
  },
  {
    "text": "Once all the data is validated, we’ll go to AWS Glue Studio and\nrun an ETL job that transforms the CSV files into Parquet format.",
    "start": "156212",
    "end": "163663"
  },
  {
    "text": "For this example, we defined the job to capture \ndata from the S3 bucket,",
    "start": "165993",
    "end": "169856"
  },
  {
    "text": "perform an \"apply mapping\" action that transforms the data format,\nand then deliver the transformed data back to S3.",
    "start": "169856",
    "end": "176054"
  },
  {
    "text": "In the job details, we supplied an appropriate \nIAM role and defined the type as a Spark job.",
    "start": "178463",
    "end": "183474"
  },
  {
    "text": "We also specified the worker types, number \nof workers, job bookmarking capability,",
    "start": "184913",
    "end": "189321"
  },
  {
    "text": "and job timeout along with the number of retries.",
    "start": "189322",
    "end": "191885"
  },
  {
    "text": "Now we’ll run the job and monitor its progress.",
    "start": "192982",
    "end": "195346"
  },
  {
    "text": "Back in Glue Studio, we can review the job script for converting\nthe job from CSV to Parquet and edit it for future job runs, if necessary.",
    "start": "202422",
    "end": "209461"
  },
  {
    "text": "Now let's go into Amazon Athena and run \nsome queries on the transformed data.",
    "start": "210633",
    "end": "214312"
  },
  {
    "start": "216000",
    "end": "266000"
  },
  {
    "text": "Athena makes it easy to access data in S3.",
    "start": "216692",
    "end": "219066"
  },
  {
    "text": "All we have to do is point to the database in the AWS Glue Data Catalog,\ndefine the schema, and run the query.",
    "start": "219484",
    "end": "225000"
  },
  {
    "text": "Let's start by running a simple query.",
    "start": "225654",
    "end": "227283"
  },
  {
    "text": "Here are the results.",
    "start": "230054",
    "end": "231014"
  },
  {
    "text": "To simplify further, we can create views \nfrom existing queries like this one.",
    "start": "232073",
    "end": "235673"
  },
  {
    "text": "Views allow us to query subsets of data \nor combine subsets in a single query.",
    "start": "239661",
    "end": "243543"
  },
  {
    "text": "For example, we have created \nthis view from our table.",
    "start": "244042",
    "end": "246362"
  },
  {
    "text": "When we analyze against the view, the \namount of data scanned is 50.7 MB,",
    "start": "250553",
    "end": "255353"
  },
  {
    "text": "compared to 1.59 GB had the data been \nin CSV format and no view existed.",
    "start": "255353",
    "end": "260182"
  },
  {
    "text": "We can also build Amazon QuickSight \nvisualizations with this data.",
    "start": "261283",
    "end": "264563"
  },
  {
    "start": "266000",
    "end": "303000"
  },
  {
    "text": "To do this we'll import the queries \nwe just performed in Amazon Athena.",
    "start": "266842",
    "end": "270042"
  },
  {
    "text": "In this case, we've already imported the queries \nand have them as an existing data source.",
    "start": "271312",
    "end": "274985"
  },
  {
    "text": "We can edit the database, if needed.",
    "start": "277172",
    "end": "278772"
  },
  {
    "text": "We can also create a dataset \nfor the visualizations.",
    "start": "279863",
    "end": "282424"
  },
  {
    "text": "Here is a sample visualization, that shows the aggregate\nof seat levels and ticket prices by location.",
    "start": "286813",
    "end": "291373"
  },
  {
    "text": "QuickSight also has an AutoGraph option that \nuses machine learning capabilities to choose",
    "start": "293622",
    "end": "297964"
  },
  {
    "text": "the most appropriate visual type for the \nnumber and type of the fields selected.",
    "start": "297964",
    "end": "301443"
  },
  {
    "start": "303000",
    "end": "349000"
  },
  {
    "text": "Next, let's run an Athena Federated Query that targets data in\nthe Amazon RDS PostgreSQL database we moved to S3 earlier.",
    "start": "303681",
    "end": "310433"
  },
  {
    "text": "From Athena, we'll connect to the new data source.",
    "start": "311253",
    "end": "313345"
  },
  {
    "text": "You can use an existing Lambda function to \nmanage the connection or create a new one.",
    "start": "315994",
    "end": "319632"
  },
  {
    "text": "We already created a function tailored \nto our needs. Let’s quickly look it over.",
    "start": "322044",
    "end": "325484"
  },
  {
    "text": "Back in the Athena query editor,",
    "start": "329094",
    "end": "330811"
  },
  {
    "text": "let's run a query to join data from S3 with the PostgreSQL database\nvia the Athena Federated Query connection.",
    "start": "330812",
    "end": "336445"
  },
  {
    "text": "The results represent the joined data.",
    "start": "339724",
    "end": "341576"
  },
  {
    "text": "Now we can use Amazon SageMaker to build, train,\nand deploy ML models from this joined data.",
    "start": "342482",
    "end": "347520"
  },
  {
    "start": "349000",
    "end": "412000"
  },
  {
    "text": "To prepare the data for the ML algorithms,",
    "start": "349773",
    "end": "352078"
  },
  {
    "text": "we’ve used a Jupyter notebook to integrate with S3\nusing Athena and populate the frames for data manipulation.",
    "start": "352078",
    "end": "357484"
  },
  {
    "text": "We assigned the notebook with the necessary IAM roles\nto allow it to access the S3 bucket, Athena, and Glue.",
    "start": "359785",
    "end": "365212"
  },
  {
    "text": "Let's review the commands the notebook executed.",
    "start": "365953",
    "end": "368033"
  },
  {
    "text": "The first command applied a pip install for \nPyAthena to install an Athena JDBC drive.",
    "start": "370284",
    "end": "375164"
  },
  {
    "text": "With the driver installed, the notebook used the JDBC connection\nto connect to Athena and populate the Pandas DataFrame.",
    "start": "375935",
    "end": "381884"
  },
  {
    "text": "Another command loaded the Athena data from S3 \ninto the Pandas DataFrame to join the query.",
    "start": "384323",
    "end": "388963"
  },
  {
    "text": "As you can see, we applied a query to \ndeliver 10 data fields on stadiums.",
    "start": "390233",
    "end": "393993"
  },
  {
    "text": "With another query, we applied ML algorithms \nto interpret the data in the Pandas DataFrame.",
    "start": "397183",
    "end": "401743"
  },
  {
    "text": "After capturing ticket data \nfor different sporting events,",
    "start": "404403",
    "end": "407043"
  },
  {
    "text": "we applied a mat plot library \nfor an analytical visualization.",
    "start": "407043",
    "end": "410444"
  },
  {
    "text": "In our final example, we’ll join data between Amazon Redshift\nand Aurora PostgreSQL using federated queries in Redshift.",
    "start": "412985",
    "end": "419224"
  },
  {
    "text": "We have an Aurora PostgreSQL \ninstance stored in Amazon RDS.",
    "start": "420105",
    "end": "423385"
  },
  {
    "text": "We used the query editor to load \ndata from the \"customer\" table.",
    "start": "425654",
    "end": "428694"
  },
  {
    "text": "We also have a Redshift cluster \nloaded with customer data.",
    "start": "430994",
    "end": "433784"
  },
  {
    "text": "We then created an external table by running commands to apply\nappropriate permissions while joining with the Aurora PostgreSQL database.",
    "start": "436054",
    "end": "442425"
  },
  {
    "text": "We are now able to create the schema associated \nwith the Aurora PostgreSQL RDS database.",
    "start": "444794",
    "end": "449229"
  },
  {
    "text": "Here we can see the columns \nthat are joined in the schema.",
    "start": "450745",
    "end": "452914"
  },
  {
    "text": "At this point, we could run any number of \nqueries from the federated customer table.",
    "start": "456000",
    "end": "459539"
  },
  {
    "text": "For our purposes, we'll end here.",
    "start": "459814",
    "end": "461414"
  },
  {
    "text": "You've just seen how you can use modern data \narchitecture for purpose-built data stores on AWS.",
    "start": "462583",
    "end": "467496"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "468495",
    "end": "471520"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "471764",
    "end": "473710"
  }
]