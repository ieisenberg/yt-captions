[
  {
    "start": "0",
    "end": "24000"
  },
  {
    "text": "hi everyone this is Blaire Layton I'm the business development manager for word database services across Asia",
    "start": "1580",
    "end": "8670"
  },
  {
    "text": "Pacific at AWS welcome to the answer webinar and today's topic is on",
    "start": "8670",
    "end": "14179"
  },
  {
    "text": "migrating business critical applications to AWS with minimal downtime the first",
    "start": "14179",
    "end": "21539"
  },
  {
    "text": "things that I want to talk about really about what customers are looking for",
    "start": "21539",
    "end": "28170"
  },
  {
    "start": "24000",
    "end": "131000"
  },
  {
    "text": "when they're wanting to migrate to AWS and some of the problems are they're having the first thing that often comes",
    "start": "28170",
    "end": "34440"
  },
  {
    "text": "up is that especially for business critical applications they can't afford long periods of application downtime and",
    "start": "34440",
    "end": "41270"
  },
  {
    "text": "when they look at how they can mitigate that the tools that enable it are pretty much expensive and sometimes quite",
    "start": "41270",
    "end": "48780"
  },
  {
    "text": "complex to use and then when they look at it then they say ok well this is a bit too complex and a bit expensive to",
    "start": "48780",
    "end": "54539"
  },
  {
    "text": "migrate and then they say ok well I don't think we can move some of these applications to the cloud we're going to",
    "start": "54539",
    "end": "59670"
  },
  {
    "text": "have to keep keep them on premise also even if they do move some of the applications to the cloud they would",
    "start": "59670",
    "end": "65610"
  },
  {
    "text": "still need a copy of the data on-premise so keeping things in sync becomes a problem and sometimes they decide that",
    "start": "65610",
    "end": "72960"
  },
  {
    "text": "if they're going to move to the cloud there might be an opportunity of moving away from commercial engines such as",
    "start": "72960",
    "end": "79830"
  },
  {
    "text": "Oracle and sequel server to an open source database as part of the migration",
    "start": "79830",
    "end": "86030"
  },
  {
    "text": "another one is that when you're looking at the size of particular databases especially if you're looking at doing",
    "start": "86030",
    "end": "92759"
  },
  {
    "text": "that warehousing then sending large volumes of data to ALUs requires an expensive international network link for",
    "start": "92759",
    "end": "99540"
  },
  {
    "text": "some of our customers who are sending data from different subsidiaries and different countries luckily in Australia",
    "start": "99540",
    "end": "106110"
  },
  {
    "text": "you do have reasonable networks to to get into the Sydney data centers the",
    "start": "106110",
    "end": "111210"
  },
  {
    "text": "other one is they just don't have the skills inside their organization so when you combine all of this it does mean",
    "start": "111210",
    "end": "117060"
  },
  {
    "text": "that even though some customers see the benefits of moving to the cloud and specifically the AWS platform that they",
    "start": "117060",
    "end": "124170"
  },
  {
    "text": "do have some concerns and you know pretty much on the fence or just decide",
    "start": "124170",
    "end": "129750"
  },
  {
    "text": "that they can't do that so if you were to look at this traditional approach what would you",
    "start": "129750",
    "end": "135930"
  },
  {
    "start": "131000",
    "end": "185000"
  },
  {
    "text": "actually do to move an application system to the crowd well the first thing obviously you need to create your AWS",
    "start": "135930",
    "end": "141900"
  },
  {
    "text": "account and the next thing is to set up your virtual private cloud and make sure that you get all the networking is set",
    "start": "141900",
    "end": "148590"
  },
  {
    "text": "up and compliant with your on-premise standards and then connect to our WS with a VPN or direct connect to make",
    "start": "148590",
    "end": "155130"
  },
  {
    "text": "sure the connections secure and once all of that set up then you need to start planning the actual migration of your",
    "start": "155130",
    "end": "161180"
  },
  {
    "text": "database and application so you've got to shut down and backup your database transmit that backup to our simple",
    "start": "161180",
    "end": "168480"
  },
  {
    "text": "storage service s3 configure an ec2 instance with the database software restore that backup and then configure",
    "start": "168480",
    "end": "176550"
  },
  {
    "text": "all the ec2 instances for the application and then switch the users to use AWS now this is high level obviously",
    "start": "176550",
    "end": "183570"
  },
  {
    "text": "some more steps in there but if you look at four to nine alone that could take a",
    "start": "183570",
    "end": "189720"
  },
  {
    "text": "week of more or more depending on how much data you actually have to transfer and what your familiarity is with AWS",
    "start": "189720",
    "end": "196500"
  },
  {
    "text": "and so forth and you know most customers when you say this is a business critical applications simply can't take that form",
    "start": "196500",
    "end": "203550"
  },
  {
    "text": "of downtime so now there is a better way and what we've introduced is a new",
    "start": "203550",
    "end": "210690"
  },
  {
    "start": "204000",
    "end": "447000"
  },
  {
    "text": "service and this was introduced into the Sydney region earlier this year and it's",
    "start": "210690",
    "end": "217019"
  },
  {
    "text": "called the AWS database migration service the idea is that you can actually start your first migration in",
    "start": "217019",
    "end": "223470"
  },
  {
    "text": "ten minutes or less and you can keep your applications running during the migration we also support the capability",
    "start": "223470",
    "end": "231750"
  },
  {
    "text": "to replicate data from on-premise to Amazon either easy to or RDS and now",
    "start": "231750",
    "end": "239850"
  },
  {
    "text": "redshift as well you can also replicate data from AWS back to one premise and",
    "start": "239850",
    "end": "246230"
  },
  {
    "text": "you can also do data replication inside the AWS environment not only does it",
    "start": "246230",
    "end": "252510"
  },
  {
    "text": "support homogeneous it also supports heterogeneous migration of data as well",
    "start": "252510",
    "end": "258410"
  },
  {
    "text": "so the way that it actually works is that we set up you can see here on the",
    "start": "258410",
    "end": "263940"
  },
  {
    "text": "screen your application users connecting to your existing application on pram and then through the internet a",
    "start": "263940",
    "end": "270340"
  },
  {
    "text": "VPN connection to AWS and then we've got the database being set up on the AWS",
    "start": "270340",
    "end": "276490"
  },
  {
    "text": "side and once you start a replication instance inside AWS then you connect up",
    "start": "276490",
    "end": "284200"
  },
  {
    "text": "and say this is the source and this is the target after that you can select the",
    "start": "284200",
    "end": "289870"
  },
  {
    "text": "tables the schemas or the databases that you would like the database migration",
    "start": "289870",
    "end": "295630"
  },
  {
    "text": "service to migrate and once that happens then we start populating the data across",
    "start": "295630",
    "end": "301960"
  },
  {
    "text": "and then allow you once it's complete to switch that application from pointing to",
    "start": "301960",
    "end": "307930"
  },
  {
    "text": "your existing on-premise to the AWS server and once that happens we're",
    "start": "307930",
    "end": "314530"
  },
  {
    "text": "talking you know minutes of downtime basically refreshing your connections",
    "start": "314530",
    "end": "320110"
  },
  {
    "text": "making sure that your customers are able to restart their applications and the",
    "start": "320110",
    "end": "327160"
  },
  {
    "text": "DNS change is propagated through it's basically as simple as that so a new",
    "start": "327160",
    "end": "333100"
  },
  {
    "text": "capability that was just introduced recently was a multi AZ AZ option for",
    "start": "333100",
    "end": "338830"
  },
  {
    "text": "high availability so the the whole process is basically the same but this time instead of having one instance",
    "start": "338830",
    "end": "345910"
  },
  {
    "text": "we're now going to be running two instances so those of you who might be familiar with AWS this is very similar",
    "start": "345910",
    "end": "352030"
  },
  {
    "text": "to what we do in our managed database service RDS where we have two different instances running in different",
    "start": "352030",
    "end": "357820"
  },
  {
    "text": "availability zones so that if one has a problem then the other one will be able",
    "start": "357820",
    "end": "362890"
  },
  {
    "text": "to continue where that left off now the idea for this is more for continuous",
    "start": "362890",
    "end": "368050"
  },
  {
    "text": "propagation of data where we are taking constantly taking data from on-premise",
    "start": "368050",
    "end": "373690"
  },
  {
    "text": "and bringing it in to AWS so same",
    "start": "373690",
    "end": "379900"
  },
  {
    "text": "procedure filling that across and then moving everything into a WMS looking at",
    "start": "379900",
    "end": "387190"
  },
  {
    "text": "how we do this we do support change data capture and apply so if you can look on",
    "start": "387190",
    "end": "393970"
  },
  {
    "text": "the source system again on the left you've got transactions coming in updates happening and transactions being",
    "start": "393970",
    "end": "400630"
  },
  {
    "text": "recorded on source the replication instance receives those changes and then applies those to",
    "start": "400630",
    "end": "408220"
  },
  {
    "text": "the target now the way it works first is that we take essentially a select star",
    "start": "408220",
    "end": "414280"
  },
  {
    "text": "if you like in the basic use case on the table and then we record the",
    "start": "414280",
    "end": "419620"
  },
  {
    "text": "transactions that occur since that select star on the original table and",
    "start": "419620",
    "end": "425260"
  },
  {
    "text": "once the bulk apply has been loaded into the target then the back duck",
    "start": "425260",
    "end": "431050"
  },
  {
    "text": "transactions are applied on top of that and once the backed up transactions are complete then we continuously apply",
    "start": "431050",
    "end": "436990"
  },
  {
    "text": "other transactions that come through so this is what allows you to actually keep and in your applications on Prem while",
    "start": "436990",
    "end": "444699"
  },
  {
    "text": "you're doing the whole migration task looking at DMS this is what it looks",
    "start": "444699",
    "end": "453280"
  },
  {
    "start": "447000",
    "end": "738000"
  },
  {
    "text": "like from a console perspective you've got some very basic interface here",
    "start": "453280",
    "end": "458680"
  },
  {
    "text": "showing you about creating tasks to do migrations and then some progress",
    "start": "458680",
    "end": "464050"
  },
  {
    "text": "information about how that's actually going there are some information in terms of errors and logs that you can",
    "start": "464050",
    "end": "470349"
  },
  {
    "text": "get access to here as well but you don't have to just use the console you can use",
    "start": "470349",
    "end": "476139"
  },
  {
    "text": "the API as well so that means that you can programmatically build migrations or",
    "start": "476139",
    "end": "481599"
  },
  {
    "text": "perhaps do tasks on a regular basis now these tasks you can use as many or as",
    "start": "481599",
    "end": "490120"
  },
  {
    "text": "few as you want even against the same database instance so you could have different tasks doing different",
    "start": "490120",
    "end": "496630"
  },
  {
    "text": "migration jobs you can choose also the power the speed and the cost of your",
    "start": "496630",
    "end": "502840"
  },
  {
    "text": "migration by using a tea to micro for testing say all the way up to a C for 4x",
    "start": "502840",
    "end": "509199"
  },
  {
    "text": "large to do a major job with a lot of transaction processing you can also",
    "start": "509199",
    "end": "516760"
  },
  {
    "text": "choose the tables that you want and you can as I mentioned do a whole lot of",
    "start": "516760",
    "end": "522580"
  },
  {
    "text": "different scenarios from on-prem to RDS or EC to EC to EC - IDs - RDS and IDs",
    "start": "522580",
    "end": "530740"
  },
  {
    "text": "and ec2 - on Prem now I forgot to mention at the start if you do have questions please do ask them in the question box",
    "start": "530740",
    "end": "538360"
  },
  {
    "text": "and we do have to solution architects with on board to help Sebastian and crash so they'll be able to take",
    "start": "538360",
    "end": "545709"
  },
  {
    "text": "questions and answer them as we go through looking at how the load actually",
    "start": "545709",
    "end": "552850"
  },
  {
    "text": "works it is done table by table so you specify what tables you would like to",
    "start": "552850",
    "end": "559360"
  },
  {
    "text": "migrate and you can set up the parallelism as well so did fault is 8",
    "start": "559360",
    "end": "565390"
  },
  {
    "text": "and what will happen then is that it will initiate select stars on those eight tables and start bringing all of",
    "start": "565390",
    "end": "571899"
  },
  {
    "text": "that data through the replication instance and applying it to the target if you believe that that's going to have",
    "start": "571899",
    "end": "578080"
  },
  {
    "text": "too much of a big impact on production you could reduce that rippln that replication all or parallel capability",
    "start": "578080",
    "end": "584740"
  },
  {
    "text": "say down to two tables or even one of you like if you have a larger capability",
    "start": "584740",
    "end": "590080"
  },
  {
    "text": "you could increase that as well so that's configurable but it is a table by table process you can actually take a",
    "start": "590080",
    "end": "599110"
  },
  {
    "text": "source and split that up into multiple targets so this is where perhaps you might have",
    "start": "599110",
    "end": "605440"
  },
  {
    "text": "a large operational system on-premise and you would like to separate that into",
    "start": "605440",
    "end": "610540"
  },
  {
    "text": "separate targets perhaps for different reporting purposes into the cloud and again you set this up from the",
    "start": "610540",
    "end": "617740"
  },
  {
    "text": "replication instance and define where you want certain tables and data sets to",
    "start": "617740",
    "end": "623110"
  },
  {
    "text": "go as a target and you can set these up as different tasks as well so you can control each of the tasks individually",
    "start": "623110",
    "end": "630959"
  },
  {
    "text": "we can also also take multiple sources of data so from multiple databases and",
    "start": "630959",
    "end": "637779"
  },
  {
    "text": "then produce that into a single target so this is a very good example for",
    "start": "637779",
    "end": "644020"
  },
  {
    "text": "analytics where you've got multiple operational databases on-premise and",
    "start": "644020",
    "end": "650010"
  },
  {
    "text": "then you want to consolidate those in the cloud to a single target such as",
    "start": "650010",
    "end": "656170"
  },
  {
    "text": "Amazon redshift you don't actually have",
    "start": "656170",
    "end": "661510"
  },
  {
    "text": "to take everything as well so you can specify predicates and say that you only",
    "start": "661510",
    "end": "666640"
  },
  {
    "text": "want to take partial data from some tables or of course exclude some tables completely as well so that gives",
    "start": "666640",
    "end": "674410"
  },
  {
    "text": "you a lot of flexibility if you're again doing something like analytics where you only want to take certain aspects of an",
    "start": "674410",
    "end": "680769"
  },
  {
    "text": "operational system and subsets of data from particular tables to bring those",
    "start": "680769",
    "end": "685959"
  },
  {
    "text": "across into a target system and this is",
    "start": "685959",
    "end": "691540"
  },
  {
    "text": "really a key capability it's all about the the choice of where you can move data around so we have the capability to",
    "start": "691540",
    "end": "699310"
  },
  {
    "text": "say take Oracle data from an Oracle database and ship that to another Oracle database and as you'll see there can be",
    "start": "699310",
    "end": "705550"
  },
  {
    "text": "different versions and editions of Oracle at each end we can also take data from one database such as Microsoft",
    "start": "705550",
    "end": "712240"
  },
  {
    "text": "sequel server and change that to be hitting a my sequel database and then",
    "start": "712240",
    "end": "718029"
  },
  {
    "text": "another common one and that we're seeing with customer call to database Amazon",
    "start": "718029",
    "end": "724600"
  },
  {
    "text": "Arora which is our my sequel Enterprise version of the database that we've",
    "start": "724600",
    "end": "731170"
  },
  {
    "text": "created inside Amazon and that is now available in the Sydney region as well",
    "start": "731170",
    "end": "738420"
  },
  {
    "start": "738000",
    "end": "858000"
  },
  {
    "text": "so looking at the sources for the database of migration service there are",
    "start": "738509",
    "end": "745120"
  },
  {
    "text": "a couple of things that you need to be aware of when looking at this and you",
    "start": "745120",
    "end": "750279"
  },
  {
    "text": "can see there it's all about version restrictions now this is to do mostly",
    "start": "750279",
    "end": "755439"
  },
  {
    "text": "with the capability to support the CDC or the change data capture feature for",
    "start": "755439",
    "end": "761139"
  },
  {
    "text": "example Postgres 9.3 does not support us having",
    "start": "761139",
    "end": "766630"
  },
  {
    "text": "the capability to use CDC so we only support nine point four and nine point",
    "start": "766630",
    "end": "772809"
  },
  {
    "text": "five at the moment and obviously nine point six when that comes out also if you look down at the RDS capability",
    "start": "772809",
    "end": "779470"
  },
  {
    "text": "there are some restrictions on Microsoft sequel server and Postgres so if you",
    "start": "779470",
    "end": "786550"
  },
  {
    "text": "have an RDS source which is sequel server or Postgres then we don't support",
    "start": "786550",
    "end": "792180"
  },
  {
    "text": "CDC on this particular database at the",
    "start": "792180",
    "end": "797259"
  },
  {
    "text": "time so if you've got a sequel server database that's running on pram or running an ec2 then",
    "start": "797259",
    "end": "802540"
  },
  {
    "text": "it's fine same with Postgres but explicitly in the RDS environment as a source we don't support those operations",
    "start": "802540",
    "end": "809230"
  },
  {
    "text": "yet we we will introduce those capabilities at a later date on we are working on that but as it stands at the",
    "start": "809230",
    "end": "815680"
  },
  {
    "text": "moment that's not something we can support in terms of the targets as you",
    "start": "815680",
    "end": "822580"
  },
  {
    "text": "can see the the turf targets are slightly different and again the Postgres comes out where we can support",
    "start": "822580",
    "end": "828790"
  },
  {
    "text": "nine point three as a target not that I think you'll be going from nine point four to nine point three but it is",
    "start": "828790",
    "end": "834310"
  },
  {
    "text": "available and on the RDS side you can see that we support all of the database",
    "start": "834310",
    "end": "839560"
  },
  {
    "text": "versions that we have an RDS and then there's a new database at the bottom",
    "start": "839560",
    "end": "845560"
  },
  {
    "text": "which is Amazon redshift so this is where we can take the more operational",
    "start": "845560",
    "end": "850600"
  },
  {
    "text": "and relational data and put that into Amazon's data warehouse Amazon redshift",
    "start": "850600",
    "end": "857399"
  },
  {
    "start": "858000",
    "end": "995000"
  },
  {
    "text": "from a pricing perspective and this is what gets a lot of people you know",
    "start": "858180",
    "end": "864490"
  },
  {
    "text": "typically as we pointed out at the front at the start of the presentation there are tools out there such as Oracle",
    "start": "864490",
    "end": "871870"
  },
  {
    "text": "GoldenGate informatica and other tools that do support CDC but the cost of",
    "start": "871870",
    "end": "878710"
  },
  {
    "text": "those tools is quite prohibitive so if we look at the comparison here your you",
    "start": "878710",
    "end": "884980"
  },
  {
    "text": "can use something like a t2 micro which is this is US based pricing but it's so",
    "start": "884980",
    "end": "890140"
  },
  {
    "text": "1.8 cents per hour and if you want to use multi a-z basically that doubles the",
    "start": "890140",
    "end": "895240"
  },
  {
    "text": "price so this is very cost effective to do you know testing on a t2 micro or t2",
    "start": "895240",
    "end": "901600"
  },
  {
    "text": "small and even for small jobs you may be able to use that for production migration tasks and especially if you're",
    "start": "901600",
    "end": "909340"
  },
  {
    "text": "looking for something where you want to do constant migration from on-prem into the cloud but the transaction rate is",
    "start": "909340",
    "end": "914680"
  },
  {
    "text": "quite small then again the T twos even the small instances might be sufficient",
    "start": "914680",
    "end": "919890"
  },
  {
    "text": "where you want to look at the the C fours is where you have more CPU grunt",
    "start": "919890",
    "end": "925360"
  },
  {
    "text": "from the C force to process a larger number of transactions that are occurring on the source while you're",
    "start": "925360",
    "end": "931000"
  },
  {
    "text": "doing the bulk transfer and then of course those transactions need to be applied also there",
    "start": "931000",
    "end": "936280"
  },
  {
    "text": "the database migration service does support some very small transformations",
    "start": "936280",
    "end": "942310"
  },
  {
    "text": "so it's definitely not an ETL tool it's kind of more of an extract and Lowe tool but you can do some forms of trend",
    "start": "942310",
    "end": "949080"
  },
  {
    "text": "transformations so based on that you may want to use a larger instance for that",
    "start": "949080",
    "end": "955300"
  },
  {
    "text": "processing you can see that we do provide some storage so with the t2 instances you get 50 gigabytes of SSD",
    "start": "955300",
    "end": "962290"
  },
  {
    "text": "back storage and for the c4 instances a hundred gigabytes you can add more if you believe that you need more to store",
    "start": "962290",
    "end": "970120"
  },
  {
    "text": "your transaction load while the bulk transfer is occurring but that's what's given this part of the package so from",
    "start": "970120",
    "end": "977830"
  },
  {
    "text": "that we believe that that's sufficient for most use cases but again if you find it's not during testing you can definitely add more to cater for extra",
    "start": "977830",
    "end": "984970"
  },
  {
    "text": "transactions and remember the data transfer into Amazon Web Services is",
    "start": "984970",
    "end": "990130"
  },
  {
    "text": "free and if you're transferring across a ZZZ and it's 1 cent per gigabyte so",
    "start": "990130",
    "end": "996610"
  },
  {
    "start": "995000",
    "end": "1087000"
  },
  {
    "text": "looking at what does this mean on an actual migration well from the the documentation the development team",
    "start": "996610",
    "end": "1002730"
  },
  {
    "text": "actually did some tests and they were able to migrate 5 terabytes of data in 33 hours so this was in the best",
    "start": "1002730",
    "end": "1011100"
  },
  {
    "text": "scenario everything was running on AWS from ec2 to and RDS",
    "start": "1011100",
    "end": "1016440"
  },
  {
    "text": "database so it's not you know dependent on the network connection that's coming from you perhaps your on-premise site",
    "start": "1016440",
    "end": "1023790"
  },
  {
    "text": "into AWS which could be a major bottleneck the database here that they",
    "start": "1023790",
    "end": "1029130"
  },
  {
    "text": "were looking at included four large tables of 250 gigabytes a huge one terabyte table a thousand of small to",
    "start": "1029130",
    "end": "1036150"
  },
  {
    "text": "moderately sized tables three tables which included lobs with different sizes of lobs inside that and then 10,000 very",
    "start": "1036150",
    "end": "1043110"
  },
  {
    "text": "small tables so this is a fairly large and complex beast of a database that we were able to migrate in 33 hours in the",
    "start": "1043110",
    "end": "1051300"
  },
  {
    "text": "best conditions as I mentioned the key point here is how much does it cost and it would be for less than ten dollars",
    "start": "1051300",
    "end": "1059040"
  },
  {
    "text": "per terabyte to do through that particular migration and this is you know not what the low end this is at the",
    "start": "1059040",
    "end": "1067429"
  },
  {
    "text": "sorry this is that the that the high end of what you could expect to pay so it probably can be done",
    "start": "1067429",
    "end": "1074430"
  },
  {
    "text": "much cheaper than this and when you put that in the context of other tools then",
    "start": "1074430",
    "end": "1079890"
  },
  {
    "text": "I think you'll start to see just how cost-effective it is to use the database migration service compared to the",
    "start": "1079890",
    "end": "1085950"
  },
  {
    "text": "traditional providers so the use cases what can you actually do with DMS now",
    "start": "1085950",
    "end": "1092520"
  },
  {
    "start": "1087000",
    "end": "1331000"
  },
  {
    "text": "obviously the number one is that the title of this particular talk which is",
    "start": "1092520",
    "end": "1097650"
  },
  {
    "text": "the migration of business critical applications and then the next one is",
    "start": "1097650",
    "end": "1104820"
  },
  {
    "text": "migration from classic to VPC so have you been using AWS for a while",
    "start": "1104820",
    "end": "1110160"
  },
  {
    "text": "our original network stack was actually called classic and then we introduced the virtual private cloud which is a",
    "start": "1110160",
    "end": "1116610"
  },
  {
    "text": "more secure networking concept now there are still customers out there that do have applications and databases running",
    "start": "1116610",
    "end": "1123600"
  },
  {
    "text": "in our classic network and to propagate that across to the VPC is essentially",
    "start": "1123600",
    "end": "1129540"
  },
  {
    "text": "like a migration as well so what we can do is use DMS to do that classic to VPC",
    "start": "1129540",
    "end": "1136230"
  },
  {
    "text": "migration if you're running Oracle and you would like to offload some reporting",
    "start": "1136230",
    "end": "1143700"
  },
  {
    "text": "or perhaps read operations to help scale an application traditionally you would need to use Oracle Enterprise Edition",
    "start": "1143700",
    "end": "1150330"
  },
  {
    "text": "and you would have to purchase the active Data Guard option for Oracle",
    "start": "1150330",
    "end": "1156510"
  },
  {
    "text": "database so you haven't used the top-of-the-line Oracle database and pay for an additional option to get a read",
    "start": "1156510",
    "end": "1163560"
  },
  {
    "text": "replica what we can do with DMS is actually set up the replication from any edition of Oracle whether that's",
    "start": "1163560",
    "end": "1169980"
  },
  {
    "text": "Enterprise Edition standard edition standard edition one or Stan Edition 2 and then set that as a target again for",
    "start": "1169980",
    "end": "1176340"
  },
  {
    "text": "one of the cheaper versions of Oracle or if you wish you can use Enterprise Edition of course but you don't need to",
    "start": "1176340",
    "end": "1182310"
  },
  {
    "text": "pay for the active Data Guard option for this and that can really save quite a",
    "start": "1182310",
    "end": "1187560"
  },
  {
    "text": "lot of money when you're looking at doing things like analytics and offloading read operations read replicas",
    "start": "1187560",
    "end": "1193710"
  },
  {
    "text": "on other engines as well same applies for sequel server if you'd like a read replica for that and then for you know",
    "start": "1193710",
    "end": "1201090"
  },
  {
    "text": "my sequel Postgres if you want to do you know subselection of certain",
    "start": "1201090",
    "end": "1207870"
  },
  {
    "text": "tables and you don't need the whole database thing you may prefer to use DMS over their native replication and read",
    "start": "1207870",
    "end": "1213060"
  },
  {
    "text": "replicas support that we have inside yes looking at cross region read replicas",
    "start": "1213060",
    "end": "1219630"
  },
  {
    "text": "for Oracle and sequel server this is something that we do not support inside RDS at the moment where we do support",
    "start": "1219630",
    "end": "1226170"
  },
  {
    "text": "that for Postgres my sequel and Aurora so if you're looking at that then what",
    "start": "1226170",
    "end": "1232170"
  },
  {
    "text": "we can say is instead of relying on RDS for that now let's use DMS to take an",
    "start": "1232170",
    "end": "1237840"
  },
  {
    "text": "audio source and then propagate that across to another region to accelerate",
    "start": "1237840",
    "end": "1243390"
  },
  {
    "text": "career operations in another region analytics in the cloud as I mentioned this was you know something that we",
    "start": "1243390",
    "end": "1250230"
  },
  {
    "text": "can't a consolidated systems from on-premise and push them into a single",
    "start": "1250230",
    "end": "1255380"
  },
  {
    "text": "system in AWS to allow you to do analytics and to keep that data constantly updated with DMS performing",
    "start": "1255380",
    "end": "1263670"
  },
  {
    "text": "that and you can use the multi a-z option with DMS to make sure that that propagation is always going to occur",
    "start": "1263670",
    "end": "1270180"
  },
  {
    "text": "even if there's an outage in a particular area of AWS another one which",
    "start": "1270180",
    "end": "1276240"
  },
  {
    "text": "is interesting is dev test and production environments Inc now many of",
    "start": "1276240",
    "end": "1281460"
  },
  {
    "text": "you many of you I'm sure have been involved in projects where you've tested everything in dev tests that you never",
    "start": "1281460",
    "end": "1287520"
  },
  {
    "text": "really had the production transaction load and you never really have the production data set it was always",
    "start": "1287520",
    "end": "1292680"
  },
  {
    "text": "contrived data and contrived load tests and once you put that into production of",
    "start": "1292680",
    "end": "1299520"
  },
  {
    "text": "course it doesn't quite work as intended and I'm sure many of you have heard about the Australian census problems",
    "start": "1299520",
    "end": "1304950"
  },
  {
    "text": "over the last weekend so if you were to",
    "start": "1304950",
    "end": "1309960"
  },
  {
    "text": "have a production data environment that you could keep in sync with your dev",
    "start": "1309960",
    "end": "1315870"
  },
  {
    "text": "test account then you would actually be able to run your transaction load and",
    "start": "1315870",
    "end": "1321030"
  },
  {
    "text": "use production data and keep that completely in sync by using DMS so",
    "start": "1321030",
    "end": "1326730"
  },
  {
    "text": "that's an interesting one to think about as one of the use cases for this new so this and what else could you do with DMS",
    "start": "1326730",
    "end": "1335100"
  },
  {
    "start": "1331000",
    "end": "1361000"
  },
  {
    "text": "you know we talked to a number of customers and they've with different ideas that they would for their specific scenarios and use cases",
    "start": "1335100",
    "end": "1341520"
  },
  {
    "text": "use DMS for and I'm sure that you have some other use cases that you would like",
    "start": "1341520",
    "end": "1347130"
  },
  {
    "text": "to use DMS for as well and if you do have something different then you know tell us what are some of the things",
    "start": "1347130",
    "end": "1352260"
  },
  {
    "text": "you'd like to do or there's something that it can't do at the moment do tell us what you would like to do with it as",
    "start": "1352260",
    "end": "1357660"
  },
  {
    "text": "well and we can feed that back to the development team in Seattle so what",
    "start": "1357660",
    "end": "1364320"
  },
  {
    "start": "1361000",
    "end": "1484000"
  },
  {
    "text": "happens if your database is too big this",
    "start": "1364320",
    "end": "1370410"
  },
  {
    "text": "is a problem especially if you're looking at things inside a country",
    "start": "1370410",
    "end": "1377520"
  },
  {
    "text": "outside of the Sydney region so if you're looking at say in New Zealand and",
    "start": "1377520",
    "end": "1382860"
  },
  {
    "text": "moving data across to Sydney then there are issues about trying to say move five",
    "start": "1382860",
    "end": "1388980"
  },
  {
    "text": "terabytes or more data given the network bandwidth so we do have a couple of",
    "start": "1388980",
    "end": "1394230"
  },
  {
    "text": "different ways to help with that one of them is to look at using AWS",
    "start": "1394230",
    "end": "1400190"
  },
  {
    "text": "import/export discs and to ship your hard disk to AWS this is supported in the Singapore region it's not available",
    "start": "1400190",
    "end": "1407100"
  },
  {
    "text": "in the Sydney region at the moment but we do have AWS import/export for",
    "start": "1407100",
    "end": "1413250"
  },
  {
    "text": "snowball which is a secure storage appliance which you can see on the right with up to 80 terabytes that AWS ships",
    "start": "1413250",
    "end": "1421050"
  },
  {
    "text": "to you and then once you've filled it up it can be shipped back to AWS and loaded",
    "start": "1421050",
    "end": "1427230"
  },
  {
    "text": "into s3 for you to then use that data so if you do have something like a data",
    "start": "1427230",
    "end": "1433170"
  },
  {
    "text": "warehouse and you would like to ship a lot of data there then this is an idea or mechanism to do that there's another",
    "start": "1433170",
    "end": "1440880"
  },
  {
    "text": "way if let's say you're sitting in Melbourne and your network connection up",
    "start": "1440880",
    "end": "1446280"
  },
  {
    "text": "to Sydney is you know pretty pretty much maximized and you don't have much",
    "start": "1446280",
    "end": "1451290"
  },
  {
    "text": "capacity on it then we do have a point of presence in Melbourne as part of our",
    "start": "1451290",
    "end": "1456330"
  },
  {
    "text": "content delivery network and you can use our s3 transfer acceleration capability",
    "start": "1456330",
    "end": "1462210"
  },
  {
    "text": "what this will do is basically give you the capability to upload data to that point in Melbourne and from there we use",
    "start": "1462210",
    "end": "1470100"
  },
  {
    "text": "the AWS backbone back to Sydney to them populate that data into the Sydney data centers so",
    "start": "1470100",
    "end": "1476340"
  },
  {
    "text": "that so you know kind of gives you a more economical way to get data into AWS",
    "start": "1476340",
    "end": "1484190"
  },
  {
    "start": "1484000",
    "end": "1790000"
  },
  {
    "text": "now what about if you're going to look at doing a migration not just of the",
    "start": "1484190",
    "end": "1490710"
  },
  {
    "text": "data from say Oracle to Oracle or sequel see what the circle server the sequel",
    "start": "1490710",
    "end": "1495720"
  },
  {
    "text": "server but you're looking as part of a migration to AWS to actually change the underlying database well that's where",
    "start": "1495720",
    "end": "1502980"
  },
  {
    "text": "the AWS schema conversion tool comes in so this tool is all about helping you",
    "start": "1502980",
    "end": "1509190"
  },
  {
    "text": "take the schema which is the database definitions in terms of the tables the",
    "start": "1509190",
    "end": "1515179"
  },
  {
    "text": "store procedures the triggers and other related database related objects and",
    "start": "1515179",
    "end": "1521460"
  },
  {
    "text": "moving them from one particular database engine to another so what we support at",
    "start": "1521460",
    "end": "1527820"
  },
  {
    "text": "the moment is the capability to convert relational databases Oracle sequel server",
    "start": "1527820",
    "end": "1532890"
  },
  {
    "text": "mariya DB Postgres and my sequel to the open source engines so Postgres again",
    "start": "1532890",
    "end": "1539670"
  },
  {
    "text": "Murray DB my sequel and Amazon Arora which remember is a my sequel compatible",
    "start": "1539670",
    "end": "1544679"
  },
  {
    "text": "database so we're taking the commercial",
    "start": "1544679",
    "end": "1549720"
  },
  {
    "text": "engines here and helping you move them to the open source engines and also interchange between the open source engines we are not giving you the",
    "start": "1549720",
    "end": "1557100"
  },
  {
    "text": "capability to go from open source engines to the commercial engines which want to save you money plus the providers Oracle and Microsoft do have",
    "start": "1557100",
    "end": "1565200"
  },
  {
    "text": "their own conversion tools to help you with that we've also recently introduced the capabilities to take Oracle's",
    "start": "1565200",
    "end": "1571620"
  },
  {
    "text": "schemas for Oracle data warehouses and Tara data and convert those to Amazon",
    "start": "1571620",
    "end": "1576750"
  },
  {
    "text": "redshift as well so this also is is a tool that's going to help you during the",
    "start": "1576750",
    "end": "1583350"
  },
  {
    "text": "process it's not the tool that is going to be a one-button magic push so as you",
    "start": "1583350",
    "end": "1589890"
  },
  {
    "text": "can see from the screen here on the left hand side you've got the individual",
    "start": "1589890",
    "end": "1595350"
  },
  {
    "text": "components from the source on the right hand side you've got the target and then",
    "start": "1595350",
    "end": "1600419"
  },
  {
    "text": "in the middle it's got information about what are some of the area errors and you see there's an Oracle stored procedure",
    "start": "1600419",
    "end": "1606840"
  },
  {
    "text": "at the bottom and what that translates to into a my sequel store procedure so",
    "start": "1606840",
    "end": "1612180"
  },
  {
    "text": "there are going to be some automatic conversions done but there are going to be a number of issues that you'll have",
    "start": "1612180",
    "end": "1617190"
  },
  {
    "text": "to make decisions on because there's going to be functions procedures internal database capabilities that are",
    "start": "1617190",
    "end": "1624060"
  },
  {
    "text": "perhaps not there in the target in terms of the objects you can see what is supported on the right hand side there",
    "start": "1624060",
    "end": "1630360"
  },
  {
    "text": "from sequences user-defined types synonyms packages etc and all of those",
    "start": "1630360",
    "end": "1635730"
  },
  {
    "text": "can be converted across to the target schema now what is particularly useful",
    "start": "1635730",
    "end": "1642930"
  },
  {
    "text": "about this tool is that it supports a",
    "start": "1642930",
    "end": "1648260"
  },
  {
    "text": "report and this report is a PDF document that you can run and you don't actually",
    "start": "1648260",
    "end": "1654540"
  },
  {
    "text": "have to do any migration whatsoever it's basically pointing it at the source",
    "start": "1654540",
    "end": "1659820"
  },
  {
    "text": "database pointing it as the tag it is what you would like it to have let's say it's an Oracle database and you want to",
    "start": "1659820",
    "end": "1665670"
  },
  {
    "text": "migrate it to my sequel or the Postgres you choose the target and then it will run through and analyze what are the",
    "start": "1665670",
    "end": "1672090"
  },
  {
    "text": "particular things it can migrate automatically and what are some of the issues and as you can see there's a",
    "start": "1672090",
    "end": "1677130"
  },
  {
    "text": "summary on the left-hand side there of what some of the the the find side",
    "start": "1677130",
    "end": "1683550"
  },
  {
    "text": "you've got some of the things about things like sequence changes and index changes constraint changes and what",
    "start": "1683550",
    "end": "1689520"
  },
  {
    "text": "those recommendations are and how many of them are so this becomes a really",
    "start": "1689520",
    "end": "1694830"
  },
  {
    "text": "useful tool to actually assess what the migration involves and how much effort",
    "start": "1694830",
    "end": "1700770"
  },
  {
    "text": "would be involved in a particular database migration because quite often if you're looking at an application for",
    "start": "1700770",
    "end": "1707550"
  },
  {
    "text": "example that's meeting the business requirements it's running say a Microsoft sequel server today you're",
    "start": "1707550",
    "end": "1713730"
  },
  {
    "text": "concerned about the licensing costs of sequel server and would like to make save some money and move that to say my",
    "start": "1713730",
    "end": "1719760"
  },
  {
    "text": "sequel well if the actual migration cost of moving that because of the complexity",
    "start": "1719760",
    "end": "1726300"
  },
  {
    "text": "of the original database and the store procedures and the triggers that it has is not really going to be worth it",
    "start": "1726300",
    "end": "1732120"
  },
  {
    "text": "because that you know as I said it's meeting the business requirements then it may not make sense to actually do",
    "start": "1732120",
    "end": "1738090"
  },
  {
    "text": "that micro just to save on license and support costs however on the you know contrary",
    "start": "1738090",
    "end": "1744540"
  },
  {
    "text": "of that is that if you do have an application which is say five years old it's no longer meeting the business",
    "start": "1744540",
    "end": "1750510"
  },
  {
    "text": "requirements it needs to be really developed and modernized and as part of that strategy you thinking about doing a",
    "start": "1750510",
    "end": "1756300"
  },
  {
    "text": "migration to an open source database then that can make business sense given",
    "start": "1756300",
    "end": "1761370"
  },
  {
    "text": "the amount of effort that it takes to do a migration now in my experience in many",
    "start": "1761370",
    "end": "1766920"
  },
  {
    "text": "customer environments there are plenty of Oracle databases and sequel server databases that are sitting there just",
    "start": "1766920",
    "end": "1773130"
  },
  {
    "text": "doing basic crud no they don't actually use a lot of stored procedures or triggers and they could be converted",
    "start": "1773130",
    "end": "1779490"
  },
  {
    "text": "very easily to Postgres or my sequel so you have to actually understand what the",
    "start": "1779490",
    "end": "1785130"
  },
  {
    "text": "environment is and what the database is like before doing these types of migrations now the pricing this is very",
    "start": "1785130",
    "end": "1793200"
  },
  {
    "start": "1790000",
    "end": "1831000"
  },
  {
    "text": "important it does not cost any money to use the schema conversion tool it is a",
    "start": "1793200",
    "end": "1798720"
  },
  {
    "text": "tool that you can download as long as you're in good standing with your AWS account down to you or a laptop or your",
    "start": "1798720",
    "end": "1805970"
  },
  {
    "text": "machine server with a workstation and run that if you do want to use it to do",
    "start": "1805970",
    "end": "1813570"
  },
  {
    "text": "migrations that are not going to be to the cloud we do have the capability to",
    "start": "1813570",
    "end": "1819000"
  },
  {
    "text": "offer some pricing for that but as long as you're using it to migrate databases to Amazon RDS Amazon redshift or the ec2",
    "start": "1819000",
    "end": "1826770"
  },
  {
    "text": "based databases then it's free to use and this is how it actually looks like",
    "start": "1826770",
    "end": "1835740"
  },
  {
    "start": "1831000",
    "end": "1942000"
  },
  {
    "text": "from a database migration perspective so the first step is that we're going to",
    "start": "1835740",
    "end": "1840960"
  },
  {
    "text": "look at taking an Oracle database on-premise in this case which can be",
    "start": "1840960",
    "end": "1846360"
  },
  {
    "text": "either in ec2 or IDs as well then doing a schema conversion tool of that",
    "start": "1846360",
    "end": "1852570"
  },
  {
    "text": "database to target Amazon Arora now the reason we need to do this first is that",
    "start": "1852570",
    "end": "1858240"
  },
  {
    "text": "before the database migration service can take the data we need to actually look at all of the tables and the data",
    "start": "1858240",
    "end": "1864840"
  },
  {
    "text": "types and map those to Amazon Arora because if we try to you know do the",
    "start": "1864840",
    "end": "1871080"
  },
  {
    "text": "conversion the tables will not be there if we use the database migration service and some of",
    "start": "1871080",
    "end": "1877230"
  },
  {
    "text": "the other things that you need like store procedures and triggers etc won't be there for the application to run as well so you definitely need to use the",
    "start": "1877230",
    "end": "1883890"
  },
  {
    "text": "schema conversion tool first and get make sure that the target schema is accurate and sufficient for that",
    "start": "1883890",
    "end": "1891150"
  },
  {
    "text": "particular target database before you start bringing the database across but once you've done that then the next step",
    "start": "1891150",
    "end": "1897000"
  },
  {
    "text": "is to point the database migration service to the source database and to",
    "start": "1897000",
    "end": "1902790"
  },
  {
    "text": "the target database just as we had explained previously and once that's",
    "start": "1902790",
    "end": "1908550"
  },
  {
    "text": "done then it can start using the CDC",
    "start": "1908550",
    "end": "1913620"
  },
  {
    "text": "capabilities to keep that application running on the source side as you're moving the data across to Aurora and",
    "start": "1913620",
    "end": "1920220"
  },
  {
    "text": "then hopefully you've got your new application which is being changed to use sequel code that's suitable for",
    "start": "1920220",
    "end": "1927960"
  },
  {
    "text": "Amazon Aurora versus Oracle and once you've got all the data across then you're going to point your users at that",
    "start": "1927960",
    "end": "1934050"
  },
  {
    "text": "new application environment running in AWS and the downtime we've very very minimal when you make that switch over",
    "start": "1934050",
    "end": "1942410"
  },
  {
    "start": "1942000",
    "end": "1989000"
  },
  {
    "text": "so looking at some customers who've actually used the database migration",
    "start": "1942410",
    "end": "1947850"
  },
  {
    "text": "tool Expedia which you've probably heard of as a world leading online travel company and they have used it to take",
    "start": "1947850",
    "end": "1955530"
  },
  {
    "text": "some databases to Amazon Aurora and you can see here from the quote the ease by",
    "start": "1955530",
    "end": "1961380"
  },
  {
    "text": "which we can do this using AWS database migration service has simplified this process for us and enabled us to",
    "start": "1961380",
    "end": "1967560"
  },
  {
    "text": "accelerate our migration efforts and a key point they're mentioning here is the ability to monitor the process and see",
    "start": "1967560",
    "end": "1974460"
  },
  {
    "text": "how it's going as well as the logging feature so they can see what actually happened during the process if there",
    "start": "1974460",
    "end": "1979920"
  },
  {
    "text": "were any problems especially during testing to make sure that the whole thing is going to go smoothly when you",
    "start": "1979920",
    "end": "1985560"
  },
  {
    "text": "do it from a production perspective another one Tomasz publishing they",
    "start": "1985560",
    "end": "1992040"
  },
  {
    "start": "1989000",
    "end": "2034000"
  },
  {
    "text": "connect buyers and suppliers across all different industrial sectors and they're going from a traditional you know print",
    "start": "1992040",
    "end": "1999450"
  },
  {
    "text": "publisher to a digital friendly business and as part of that they're going to",
    "start": "1999450",
    "end": "2005090"
  },
  {
    "text": "have to increase their Oracle database footprint and because of that that would increase the licensing",
    "start": "2005090",
    "end": "2010460"
  },
  {
    "text": "and ongoing support costs associated with that so they wanted to try and migrate to Amazon Arora to save cost and",
    "start": "2010460",
    "end": "2017270"
  },
  {
    "text": "they found that the database migration service automated most of the work and they also found that that is going to be",
    "start": "2017270",
    "end": "2025520"
  },
  {
    "text": "something that they will continue to use as they move more of their footprint from Oracle into Amazon Arora in 2016",
    "start": "2025520",
    "end": "2034090"
  },
  {
    "start": "2034000",
    "end": "2087000"
  },
  {
    "text": "another one pegasus systems they are looking at doing a different track where",
    "start": "2034090",
    "end": "2040310"
  },
  {
    "text": "they're taking an Oracle database to Postgres using our relational database service and once they've done that they",
    "start": "2040310",
    "end": "2047450"
  },
  {
    "text": "experience better availability and performance in AWS environment and they were using things such as the elastic",
    "start": "2047450",
    "end": "2054919"
  },
  {
    "text": "load balancer auto scaling in the middle tier and of course the RDS multi a-z feature which allows them to remove any",
    "start": "2054920",
    "end": "2062300"
  },
  {
    "text": "single points of failure in the architecture so of course they experience cost savings by moving from",
    "start": "2062300",
    "end": "2069139"
  },
  {
    "text": "Oracle to RDS Postgres as well so that's three customers to give you an idea of",
    "start": "2069140",
    "end": "2075080"
  },
  {
    "text": "what their capabilities they've been able to use on DMS to help them move",
    "start": "2075080",
    "end": "2080540"
  },
  {
    "text": "from you know Oracle database to Aurora or in this case Oracle - Postgres now as",
    "start": "2080540",
    "end": "2088790"
  },
  {
    "start": "2087000",
    "end": "2156000"
  },
  {
    "text": "part of the whole migration process there are concerns with some customers",
    "start": "2088790",
    "end": "2095690"
  },
  {
    "text": "about whether they have the capabilities inside their companies to be able to do",
    "start": "2095690",
    "end": "2101510"
  },
  {
    "text": "the migrations so we are working on developing a number of different database migration partners and these",
    "start": "2101510",
    "end": "2108170"
  },
  {
    "text": "are some of the international companies that we have at the moment I'm also working across a pack to develop some of",
    "start": "2108170",
    "end": "2115520"
  },
  {
    "text": "the local partners and I'll be meeting with some partners in Australia next week to get some capabilities around the",
    "start": "2115520",
    "end": "2121730"
  },
  {
    "text": "database migration service and to help customers do assessments yeah -",
    "start": "2121730",
    "end": "2127220"
  },
  {
    "text": "migrations and even look at some of the application migrations as well in terms of the code changes that are needed",
    "start": "2127220",
    "end": "2133400"
  },
  {
    "text": "there but if you've also got capabilities in your own company where you feel comfortable using a tool like this and you've got expertise especially",
    "start": "2133400",
    "end": "2139910"
  },
  {
    "text": "you're doing a migration from Oracle or sequel server to an open-source engine you go",
    "start": "2139910",
    "end": "2145309"
  },
  {
    "text": "expertise on both sides of that then yes you can definitely go ahead and do it yourself but if you prefer to get a",
    "start": "2145309",
    "end": "2151309"
  },
  {
    "text": "partner in then you can contact one of these partners to help you with the migration so if we look at the issues",
    "start": "2151309",
    "end": "2161150"
  },
  {
    "text": "that we raised at the top of the hour and what we have talked about as we've",
    "start": "2161150",
    "end": "2167059"
  },
  {
    "text": "gone through the presentation the biggest one that we mentioned for business critical applications is that",
    "start": "2167059",
    "end": "2174130"
  },
  {
    "text": "customers can't afford any application downtime for those types of applications",
    "start": "2174130",
    "end": "2179530"
  },
  {
    "text": "so we talked about how we actually have got the database migration service that",
    "start": "2179530",
    "end": "2184819"
  },
  {
    "text": "supports change data capture or CDC to minimize the amount of downtime by",
    "start": "2184819",
    "end": "2190250"
  },
  {
    "text": "allowing the application to stay up and the transactions to be propagated as the",
    "start": "2190250",
    "end": "2195349"
  },
  {
    "text": "migration is happening from the source to the target system so once that propagation is up-to-date then it's",
    "start": "2195349",
    "end": "2202670"
  },
  {
    "text": "minimal downtime to actually switch the application users from that source system to the target so that's something",
    "start": "2202670",
    "end": "2209480"
  },
  {
    "text": "that we have now addressed the next one is the tools that enable minimal",
    "start": "2209480",
    "end": "2215029"
  },
  {
    "text": "downtime are expensive now imagine that we are talking about moving a terabyte of data for less than ten dollars and",
    "start": "2215029",
    "end": "2222589"
  },
  {
    "text": "that's a huge reduction compared with the likes of Oracle GoldenGate where you",
    "start": "2222589",
    "end": "2228319"
  },
  {
    "text": "have to license the source processes and the target processes and even if you",
    "start": "2228319",
    "end": "2233630"
  },
  {
    "text": "were to run ongoing replication kind of constantly you would still be saving a",
    "start": "2233630",
    "end": "2239690"
  },
  {
    "text": "huge amount of money based on the cost of DMS and it can combining that with",
    "start": "2239690",
    "end": "2245569"
  },
  {
    "text": "the schema conversion tool if you want to do a migration from commercial engine",
    "start": "2245569",
    "end": "2250910"
  },
  {
    "text": "to an open source which is also free then you have a much lower cost to",
    "start": "2250910",
    "end": "2256819"
  },
  {
    "text": "experience for this and from the complexity angle as well as you can see",
    "start": "2256819",
    "end": "2263660"
  },
  {
    "text": "the complexity of the console is very simple where the way you actually go through if you go and look at the tool",
    "start": "2263660",
    "end": "2269599"
  },
  {
    "text": "to do setting up a source and then setting up a target it is very very simple as well it's all",
    "start": "2269599",
    "end": "2275840"
  },
  {
    "text": "about just defining the database connections and then selecting what you want to migrate from that so the",
    "start": "2275840",
    "end": "2282110"
  },
  {
    "text": "complexity of the tools are much much simpler as well from the ability to need",
    "start": "2282110",
    "end": "2289430"
  },
  {
    "text": "to have a copy of the data on-premise whether you want to keep that master copy on-premise and propagate that data",
    "start": "2289430",
    "end": "2295940"
  },
  {
    "text": "into the cloud or whether you want to have production in the cloud but keep a copy of that on premise then DMS",
    "start": "2295940",
    "end": "2302720"
  },
  {
    "text": "supports that and gives you that capability to either perhaps even have",
    "start": "2302720",
    "end": "2307910"
  },
  {
    "text": "dr scenarios where you have production in the cloud or production on-premise",
    "start": "2307910",
    "end": "2313610"
  },
  {
    "text": "and being able to swap back and forth before that if you want to use your existing infrastructure from a dr",
    "start": "2313610",
    "end": "2320030"
  },
  {
    "text": "perspective so that's also being dealt with and then i mentioned about the",
    "start": "2320030",
    "end": "2326180"
  },
  {
    "text": "schema conversion tool if you want to migrate to an open-source database and not only you know from commercial",
    "start": "2326180",
    "end": "2333470"
  },
  {
    "text": "engines but as i said between my sequel and Postgres as well then looking at",
    "start": "2333470",
    "end": "2340640"
  },
  {
    "text": "sending the large volumes of data to AWS that traditionally requires an expensive",
    "start": "2340640",
    "end": "2346010"
  },
  {
    "text": "international network link or even in country it can be helped with different things like the s3 acceleration that we",
    "start": "2346010",
    "end": "2354350"
  },
  {
    "text": "have and then snowball for you for bringing large amounts of data into AWS",
    "start": "2354350",
    "end": "2360530"
  },
  {
    "text": "so that's also being addressed and then if you don't have the skills inside your",
    "start": "2360530",
    "end": "2367340"
  },
  {
    "text": "organization we do have partners who are able to help you with the migrations so",
    "start": "2367340",
    "end": "2374450"
  },
  {
    "text": "what that means now is once we've addressed these common concerns is that now you can migrate your business",
    "start": "2374450",
    "end": "2381290"
  },
  {
    "text": "critical applications to AWS obviously there's still work involved but the risk",
    "start": "2381290",
    "end": "2387350"
  },
  {
    "text": "the cost and the complexity have been dramatically reduced to enable you to do",
    "start": "2387350",
    "end": "2394280"
  },
  {
    "text": "these migrations by the introduction of the database migration service specifically and of course for the",
    "start": "2394280",
    "end": "2401270"
  },
  {
    "text": "schema conversion tool to help you with heterogeneous migrations",
    "start": "2401270",
    "end": "2406970"
  },
  {
    "text": "so I just like to cover off a couple of things before we end up on this the AWS",
    "start": "2406970",
    "end": "2417589"
  },
  {
    "start": "2408000",
    "end": "2440000"
  },
  {
    "text": "team in E&Z has actually revived the podcast and this at the moment is occurring on a monthly basis so this is",
    "start": "2417589",
    "end": "2426849"
  },
  {
    "text": "something where we're going to talk about the latest releases of features and services and what's happening with",
    "start": "2426849",
    "end": "2433369"
  },
  {
    "text": "AWS so if you want to get involved in that this is where you can sign up for",
    "start": "2433369",
    "end": "2438740"
  },
  {
    "text": "it and I think we're going to be able to take some questions and I'll see if I",
    "start": "2438740",
    "end": "2443930"
  },
  {
    "start": "2440000",
    "end": "2490000"
  },
  {
    "text": "can if there's any questions that I need to have a look at an answer and also if",
    "start": "2443930",
    "end": "2451010"
  },
  {
    "text": "you do have more complex questions that you would like to write something that's quite you know long and you need to get",
    "start": "2451010",
    "end": "2458210"
  },
  {
    "text": "that answered you can send an email to the AWS - a and Z - marketing at",
    "start": "2458210",
    "end": "2465290"
  },
  {
    "text": "amazon.com address and we will make sure that somebody is going to be able to",
    "start": "2465290",
    "end": "2470480"
  },
  {
    "text": "attend to that and get you an answer so",
    "start": "2470480",
    "end": "2475700"
  },
  {
    "text": "I'd like to ask the the guys if there's any questions for me could they send",
    "start": "2475700",
    "end": "2481760"
  },
  {
    "text": "them to me and while I'm waiting to see",
    "start": "2481760",
    "end": "2491900"
  },
  {
    "start": "2490000",
    "end": "2651000"
  },
  {
    "text": "if there's any questions I will address some common ones that I tend to to get",
    "start": "2491900",
    "end": "2499520"
  },
  {
    "text": "asked as well so with the migration of",
    "start": "2499520",
    "end": "2505180"
  },
  {
    "text": "objects using DMS are there restrictions of certain things that we can't move and",
    "start": "2505180",
    "end": "2513160"
  },
  {
    "text": "one of the particular ones that comes up again again is large objects or lobs",
    "start": "2513160",
    "end": "2518420"
  },
  {
    "text": "binary objects or text objects as well there are some restrictions that you",
    "start": "2518420",
    "end": "2524150"
  },
  {
    "text": "might find if you launch the tool in a service by default and there are tuning",
    "start": "2524150",
    "end": "2530530"
  },
  {
    "text": "optimizations that you can do to make sure that you have the right size buffers for large objects so you can for",
    "start": "2530530",
    "end": "2538730"
  },
  {
    "text": "each of the different databases we read the documentation and see what some of the restrictions are for the types of objects that we can",
    "start": "2538730",
    "end": "2544619"
  },
  {
    "text": "move and you will have to you know make some adjustments for your particular database source so there's one day a",
    "start": "2544619",
    "end": "2552480"
  },
  {
    "text": "database question that's come through can data be obsolete when replicating to their test with the",
    "start": "2552480",
    "end": "2559859"
  },
  {
    "text": "built-in transformation capabilities no at the moment we do not support any data",
    "start": "2559859",
    "end": "2565890"
  },
  {
    "text": "masking or application using the service that is definitely something that we've heard from customers and we are actually",
    "start": "2565890",
    "end": "2572700"
  },
  {
    "text": "working with partners so two partners that I have talked to and are actively",
    "start": "2572700",
    "end": "2578910"
  },
  {
    "text": "talking to one is data guys and another one is del phix both of those companies",
    "start": "2578910",
    "end": "2585720"
  },
  {
    "text": "do have data masking capabilities and we're talking to them to see how we can",
    "start": "2585720",
    "end": "2593220"
  },
  {
    "text": "recommend those partners for particular solutions with customers and this is very relevant for customers who have you",
    "start": "2593220",
    "end": "2600839"
  },
  {
    "text": "know oversight such as financial companies or telcos and they need to make sure that data is obfuscated when",
    "start": "2600839",
    "end": "2608579"
  },
  {
    "text": "it's going to in their own dev test accounts even if it's on-premise as well as when they're moving data from the",
    "start": "2608579",
    "end": "2614279"
  },
  {
    "text": "clouds in production even on Prem and from on Prem into the cloud just to make",
    "start": "2614279",
    "end": "2620039"
  },
  {
    "text": "sure the data is obsoleted before it gets into the cloud environment so there's no unfortunately straightforward",
    "start": "2620039",
    "end": "2626549"
  },
  {
    "text": "answer to say yep we recommend this particular solution at this point in time but the traditional players are",
    "start": "2626549",
    "end": "2632130"
  },
  {
    "text": "also there so Oracle does have data masking capabilities IBM - and then also",
    "start": "2632130",
    "end": "2637700"
  },
  {
    "text": "informatica does support capabilities so the traditional players are there and",
    "start": "2637700",
    "end": "2642779"
  },
  {
    "text": "then there's other niche partners that I mentioned such as data guys and del phix who are able to help with some of these",
    "start": "2642779",
    "end": "2649230"
  },
  {
    "text": "aspects another one that's come through",
    "start": "2649230",
    "end": "2654359"
  },
  {
    "start": "2651000",
    "end": "2738000"
  },
  {
    "text": "is what is the load on the source and",
    "start": "2654359",
    "end": "2660200"
  },
  {
    "text": "target databases so what we're actually doing is we're using the mining the logs",
    "start": "2660200",
    "end": "2667170"
  },
  {
    "text": "and taking the transactions from that and applying those again so that's what this is for CDC the initial",
    "start": "2667170",
    "end": "2674760"
  },
  {
    "text": "we're we're doing the bulk load and select star on a table that will have an impact on production systems so you want",
    "start": "2674760",
    "end": "2682500"
  },
  {
    "text": "to do that bulk load at an off peak time because we will be doing select stars on the tables as I mentioned you can",
    "start": "2682500",
    "end": "2689070"
  },
  {
    "text": "actually refine that by specifying the parallelism of the of the number of",
    "start": "2689070",
    "end": "2696570"
  },
  {
    "text": "tables that you're going to do at a time so if you want to do it an ongoing migration and it's you know it's going",
    "start": "2696570",
    "end": "2703440"
  },
  {
    "text": "to take say two days to migrate the data because the volume of transactions and the volume of data and your network then",
    "start": "2703440",
    "end": "2709200"
  },
  {
    "text": "what you might want to do is bring the parallelism down to make sure the load on the server is not that high now for",
    "start": "2709200",
    "end": "2716880"
  },
  {
    "text": "the ongoing replication then the load is quite low because we're actually mining",
    "start": "2716880",
    "end": "2721920"
  },
  {
    "text": "the logs and then applying those changes onto the target system as they're",
    "start": "2721920",
    "end": "2726990"
  },
  {
    "text": "happening on the source so that yes there'll be some overhead and network traffic and IO and those types of things",
    "start": "2726990",
    "end": "2732869"
  },
  {
    "text": "but it's not going to be as much as when we're doing the bulk transfer at the start next one that's come through is",
    "start": "2732869",
    "end": "2742340"
  },
  {
    "start": "2738000",
    "end": "2855000"
  },
  {
    "text": "look of using the service to keep a read replica in sync and perform a database",
    "start": "2742340",
    "end": "2747450"
  },
  {
    "text": "upgrade on the source schema how do we keep the read replicas and sync the service updated yeah if you were going",
    "start": "2747450",
    "end": "2754320"
  },
  {
    "text": "to do something like a database upgrade I can take that to be a couple of what",
    "start": "2754320",
    "end": "2759480"
  },
  {
    "text": "things so let's say you're going to make massive schema changes where you're going to add a number of tables and",
    "start": "2759480",
    "end": "2766859"
  },
  {
    "text": "different columns we actually allow you to then create another task where you could define the migration of those",
    "start": "2766859",
    "end": "2773040"
  },
  {
    "text": "tables and columns on existing tables we do support in most of the databases picking up another column the things",
    "start": "2773040",
    "end": "2780510"
  },
  {
    "text": "that we won't support our changes to store procedures and triggers those do",
    "start": "2780510",
    "end": "2786420"
  },
  {
    "text": "not get propagated across with DMS it literally is about moving the data not",
    "start": "2786420",
    "end": "2791760"
  },
  {
    "text": "about moving the database objects that involve codes such as triggers store",
    "start": "2791760",
    "end": "2798150"
  },
  {
    "text": "procedures and so on that's where the schema conversion tool comes into play for those things so if you're looking",
    "start": "2798150",
    "end": "2804540"
  },
  {
    "text": "for something to put a read replica or in place which is for an actively developed",
    "start": "2804540",
    "end": "2811180"
  },
  {
    "text": "system then you would have to put mechanisms in place to make sure that if you're making changes on a stored procedure in in production that you",
    "start": "2811180",
    "end": "2818950"
  },
  {
    "text": "would also have to make those changes to the rig replica so if you do have those types of types of requirements that's we",
    "start": "2818950",
    "end": "2825220"
  },
  {
    "text": "might be better off using the native replication technology of say oracle sequel server my sequel Postgres etc to",
    "start": "2825220",
    "end": "2832480"
  },
  {
    "text": "do these types of read replicates of course you will on the commercial",
    "start": "2832480",
    "end": "2837910"
  },
  {
    "text": "engines have to pay more for that but given the capabilities of DMS at this time that is a restriction any more",
    "start": "2837910",
    "end": "2849130"
  },
  {
    "text": "questions we're looking at a migration",
    "start": "2849130",
    "end": "2860740"
  },
  {
    "start": "2855000",
    "end": "3059000"
  },
  {
    "text": "of ec2 Microsoft sequel server 2 RDS",
    "start": "2860740",
    "end": "2866470"
  },
  {
    "text": "Aurora database what are the issues and limitations ok so let's look at the big picture here of where you're going to go",
    "start": "2866470",
    "end": "2873900"
  },
  {
    "text": "for different database types if you think about the maturity of different",
    "start": "2873900",
    "end": "2879940"
  },
  {
    "text": "database engines Oracle has the most features it still is the most scalable",
    "start": "2879940",
    "end": "2885040"
  },
  {
    "text": "and highest performance database out there sequel server is behind that in",
    "start": "2885040",
    "end": "2890410"
  },
  {
    "text": "terms of features capabilities and scale abilities but it's still very good we typically see that Postgres performs",
    "start": "2890410",
    "end": "2898390"
  },
  {
    "text": "very very well and each revision from nine point four to nine point five and",
    "start": "2898390",
    "end": "2903580"
  },
  {
    "text": "indeed what's happening with nine point six is becoming extremely scalable and very high performance however it doesn't",
    "start": "2903580",
    "end": "2910960"
  },
  {
    "text": "have as many features and capabilities as the commercial engines dropping down another level you're looking at Marea DB",
    "start": "2910960",
    "end": "2917650"
  },
  {
    "text": "my sequel and a call was Aurora which is",
    "start": "2917650",
    "end": "2923080"
  },
  {
    "text": "all of the my sequel family the features and functions of that family is again less than Postgres and far less than the",
    "start": "2923080",
    "end": "2929710"
  },
  {
    "text": "commercial vendors whilst we have better performance of with Maree DB compared to my sequel as well as with Aurora having",
    "start": "2929710",
    "end": "2936580"
  },
  {
    "text": "superior performance availability and scalability above my sequel the core",
    "start": "2936580",
    "end": "2941620"
  },
  {
    "text": "features are roughly the same on the my sequel platform so if you're taking a database",
    "start": "2941620",
    "end": "2946960"
  },
  {
    "text": "that is from the commercial engines which of course can have many many features used and over years have a lot",
    "start": "2946960",
    "end": "2954250"
  },
  {
    "text": "of store procedures a lot of triggers and then you're going to try and say let's put that into something like my",
    "start": "2954250",
    "end": "2959830"
  },
  {
    "text": "sequel well unfortunately that's going to be a pretty big ask what we typically",
    "start": "2959830",
    "end": "2965350"
  },
  {
    "text": "see is from the commercial engines where the customers would for simple use cases favor my sequel because there's",
    "start": "2965350",
    "end": "2972160"
  },
  {
    "text": "familiarity around that for more complex use cases Postgres is a better target especially",
    "start": "2972160",
    "end": "2978880"
  },
  {
    "text": "because the features around things like spatial json support and in nine point",
    "start": "2978880",
    "end": "2985900"
  },
  {
    "text": "six they're going to be introducing parallel query so for larger queries and analytics Postgres is going to get a big",
    "start": "2985900",
    "end": "2992560"
  },
  {
    "text": "boost as well so we tend to see especially for Oracle that Postgres is a",
    "start": "2992560",
    "end": "2997900"
  },
  {
    "text": "great target and this is because the architecture of Postgres and the language of Postgres is also much closer",
    "start": "2997900",
    "end": "3003990"
  },
  {
    "text": "to Oracle so you have to kind of look at the different databases and what the targets are what the complexities are",
    "start": "3003990",
    "end": "3010260"
  },
  {
    "text": "and of course what your skill sets are to be able to decide what's the right target so there's no easy answer but",
    "start": "3010260",
    "end": "3017220"
  },
  {
    "text": "that's hopefully that's giving you some idea of what are the concerns that you have to look at when deciding to move",
    "start": "3017220",
    "end": "3022620"
  },
  {
    "text": "from a you know commercial engine like sequel server oracle to the open source",
    "start": "3022620",
    "end": "3028200"
  },
  {
    "text": "engines another question on roadmap do we have a db2 migration in the pipeline",
    "start": "3028200",
    "end": "3034320"
  },
  {
    "text": "that is support from for DMS or for schema conversion tool at this stage we",
    "start": "3034320",
    "end": "3040620"
  },
  {
    "text": "don't we actually released one which I haven't put in SI base capability for",
    "start": "3040620",
    "end": "3047340"
  },
  {
    "text": "DMS so you do you can actually move data from Sybase to any of the targets as",
    "start": "3047340",
    "end": "3053010"
  },
  {
    "text": "well but at this time we do not have a db2 coming up in another question coming",
    "start": "3053010",
    "end": "3063360"
  },
  {
    "start": "3059000",
    "end": "3180000"
  },
  {
    "text": "through is what about the ongoing copying of sequence values that need to align with values within the tables well",
    "start": "3063360",
    "end": "3070980"
  },
  {
    "text": "if you're applying a transaction on the master database",
    "start": "3070980",
    "end": "3076380"
  },
  {
    "text": "is going to use a sequence value then that value that's been assigned is going to then be propagated across to the",
    "start": "3076380",
    "end": "3083369"
  },
  {
    "text": "other database now if you're talking about whether the sequences themselves are going to be kept up-to-date",
    "start": "3083369",
    "end": "3089819"
  },
  {
    "text": "I would suspect and this is just a guess at this time that that would not be the case because unless you're relying on",
    "start": "3089819",
    "end": "3097710"
  },
  {
    "text": "that sequence being used at the other end then that sequence is not going to",
    "start": "3097710",
    "end": "3103079"
  },
  {
    "text": "be updated with the latest values I can't say that accurately because I haven't tested it but I'm at the you",
    "start": "3103079",
    "end": "3110880"
  },
  {
    "text": "know just asking off-the-cuff that's my response to that probably is not the case if you would like to collect that",
    "start": "3110880",
    "end": "3118079"
  },
  {
    "text": "particular question we can get back with an official answer after the webinar",
    "start": "3118079",
    "end": "3123920"
  },
  {
    "text": "okay so I understand that that was the the last question at the moment so thank",
    "start": "3124910",
    "end": "3132119"
  },
  {
    "text": "you very much everyone for attending today's webinar again if you have further questions please feel free to",
    "start": "3132119",
    "end": "3139019"
  },
  {
    "text": "send an email to the email address on the screen I hope you have learnt a lot",
    "start": "3139019",
    "end": "3144869"
  },
  {
    "text": "today about what how you can now migrate your business critical applications to AWS and thank you very much for your",
    "start": "3144869",
    "end": "3151619"
  },
  {
    "text": "time",
    "start": "3151619",
    "end": "3153828"
  }
]