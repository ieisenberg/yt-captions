[
  {
    "start": "0",
    "end": "112000"
  },
  {
    "text": "MATT YANCHYSHYN: My name is Matt\nYanchyshyn, and I'm a partner of Solutions Architect. I lead the ISV Partner Solutions\nArchitecture Team,",
    "start": "400",
    "end": "6700"
  },
  {
    "text": "so the people who work with ISV partners. This is always a really fun event\nfor me, it's also a crazy event because the partner ecosystem has\ngrown considerably as well.",
    "start": "6700",
    "end": "15200"
  },
  {
    "text": "I've been with AWS about three and\na half years, and what's interesting about that\nrelative to this talk",
    "start": "15200",
    "end": "21000"
  },
  {
    "text": "is that three and a half years ago I was like a lot of you guys in the audience today, is that I was building my first big\ndata applications.",
    "start": "21000",
    "end": "28500"
  },
  {
    "text": "I came from a broadcast video and\nmedia background, so, you know, I could code and I could\nwork with broadcast video workflows",
    "start": "28500",
    "end": "35000"
  },
  {
    "text": "and I could deploy clusters to do\nthings like transcoding, but if you said, hey, Matt, go set\nup a, you know, streaming cluster - streaming data\nprocessing cluster -",
    "start": "35000",
    "end": "42100"
  },
  {
    "text": "you know, my eyes would kind of go like this. And been doing a lot of that in the\nlast three and a half years and working with a lot of customers,\nboth startups and enterprises,",
    "start": "42100",
    "end": "49500"
  },
  {
    "text": "and it's amazing how our portfolio has grown and how our big data ecosystem has\ngrown in this regard,",
    "start": "49500",
    "end": "56399"
  },
  {
    "text": "and it's also amazing how easy it is. We're going to do something fun,\nit' s called a life demo, today,",
    "start": "56400",
    "end": "62300"
  },
  {
    "text": "so we'll see how we go. I didn't even include screenshots,\nso if I screw up you can laugh. But I think it's more fun",
    "start": "62300",
    "end": "68700"
  },
  {
    "text": "and really what we're going to do today is to show you how easy it is to spin\nup clusters to do data processing,",
    "start": "68700",
    "end": "75000"
  },
  {
    "text": "to get into Hadoop, to start using\na data warehouse, Amazon Redshift, and even to do complex things",
    "start": "75000",
    "end": "80200"
  },
  {
    "text": "like streaming data processing. How many of you were in the Keynote\nthis morning?",
    "start": "80200",
    "end": "86300"
  },
  {
    "text": "Okay, pretty cool. So, Kinesis Firehose came out, which is great because that means I can talk about\nit, and same with QuickSight.",
    "start": "86300",
    "end": "93900"
  },
  {
    "text": "So, I'll try to integrate the new services that came out this morning that are\ndirectly applicable to what we're talking about today. But things like streaming data processing,",
    "start": "93900",
    "end": "101100"
  },
  {
    "text": "things like BI of scale, running a Hadoop cluster, this is entirely possible and very\neasy for you to do today.",
    "start": "101100",
    "end": "107700"
  },
  {
    "text": "And that's what we're going to do together, we're going to build an end to end\nbig data application. But before we do, I think a lot of people",
    "start": "107700",
    "end": "114299"
  },
  {
    "start": "112000",
    "end": "287000"
  },
  {
    "text": "focus too much on the tools. Now, I give the Big Data Bootcamp at AWS, and we just gave it on Monday and Tuesday,",
    "start": "114300",
    "end": "119700"
  },
  {
    "text": "and I noticed that a lot of the customers that I meet in these camps, they\nagonize over should I use Hive,",
    "start": "119700",
    "end": "126400"
  },
  {
    "text": "should I use Spark, should I use\na data warehouse, what database should I use. Really, it's more about the business problem.",
    "start": "126400",
    "end": "133000"
  },
  {
    "text": "And I always tell people to work backwards from the actual business problem\nyou're trying to solve. End of the day, big data is not about\nwhat tool you use,",
    "start": "133000",
    "end": "138900"
  },
  {
    "text": "it's about the value that you can\nderive from your data, and it's about design patterns that\nyou can represent in any number of ways with third\nparty solutions",
    "start": "138900",
    "end": "146700"
  },
  {
    "text": "in the AWS ecosystem, or with AWS\nnative services. The way I like to think about it\nis data in, data out,",
    "start": "146700",
    "end": "152600"
  },
  {
    "text": "and everything in between is just\nthe details. We have a number of services in this space, we have storage services like the\nobject storage service, Amazon S3,",
    "start": "152600",
    "end": "160500"
  },
  {
    "text": "we have Kinesis, they just launched\nKinesis Firehose this morning - think of Kinesis like a buffer or a queue,",
    "start": "160500",
    "end": "166000"
  },
  {
    "text": "a great place to catch inbound streaming data, we have DynamoDB, a managed NoSQL offering",
    "start": "166000",
    "end": "171700"
  },
  {
    "text": "that makes running NoSQL databases\nas scale really easy, and we have this sort of new paradigm\nwith event-based processing",
    "start": "171700",
    "end": "178600"
  },
  {
    "text": "notably with AWS Lambda, responding\nto changes as they occur.",
    "start": "178600",
    "end": "184000"
  },
  {
    "text": "You drop a JPEG into an S3 bucket, you file off a Lambda function automatically and it creates a thumbnail.",
    "start": "184000",
    "end": "190800"
  },
  {
    "text": "In the big data world, you drop a\nlog file into an S3 bucket, you fire off a Lambda function and\nyou transform that log",
    "start": "190800",
    "end": "196100"
  },
  {
    "text": "into something structured that you\ncan log into a database. Rather than pulling, we're starting to move",
    "start": "196100",
    "end": "201400"
  },
  {
    "text": "to this event-driven paradigm. But what we're here to talk about\nmost today is the data processing,",
    "start": "201400",
    "end": "207100"
  },
  {
    "text": "on the right hand side of this. We're going to learn how to spin\nup a Hadoop cluster, we're going to learn how to spin\nup a Redshift cluster,",
    "start": "207100",
    "end": "213200"
  },
  {
    "text": "a data warehouse cluster. Unfortunately we don't have time\nin 56 minutes and 16 seconds to cover machine learning, but there's\nsome great sessions",
    "start": "213200",
    "end": "219800"
  },
  {
    "text": "on that today as well. But we are going to touch on something\npretty cool. And I'm going to put some code on\nthe screen, so warning. I'm going to put some Scala on the screen,",
    "start": "219800",
    "end": "226700"
  },
  {
    "text": "I'm going to run Scala. And if you are not a programmer it\ndoesn't matter, I'm going to show you that even the\nprogramming element of this",
    "start": "226700",
    "end": "232400"
  },
  {
    "text": "is not difficult, especially with\ntools like Kinesis Firehose, it makes that getting data into a system",
    "start": "232400",
    "end": "239200"
  },
  {
    "text": "that you can then use to analyze\nthe data really, really easy. But I want to show you that next step. If you want to not only get the data in",
    "start": "239200",
    "end": "245100"
  },
  {
    "text": "but you also want to transform the\ndata as it flows in, and I want to show you that it's easy. Because just like me three and a\nhalf years ago",
    "start": "245100",
    "end": "251600"
  },
  {
    "text": "that first step is kind of tough,\nit's intimidating. You're dealing with clusters, and\nmost of us in the back of our minds,",
    "start": "251600",
    "end": "256900"
  },
  {
    "text": "we have these painful memories of that guy who set up the data warehousing cluster\nwho lost his mind",
    "start": "256900",
    "end": "262699"
  },
  {
    "text": "because it was really hard a few years ago. That the guy who set up the Hadoop\ncluster was never seen again.",
    "start": "262700",
    "end": "268800"
  },
  {
    "text": "And I'm going to show you how in\njust one command - and by the way, we're going to use\nthe command line too - and I can do pretty console stuff",
    "start": "268800",
    "end": "274699"
  },
  {
    "text": "but let's get nasty, let's go in\nthe terminal and let's use the command line to do this. It's more fun, and it shows you how\nyou can create a cluster",
    "start": "274700",
    "end": "280600"
  },
  {
    "text": "with one single command. Forget clicks, it's easy. And so let's do it together.",
    "start": "280600",
    "end": "287699"
  },
  {
    "start": "287000",
    "end": "377000"
  },
  {
    "text": "This is what we're going to build together. Remember I said that, don't focus\non the tools focus on the design pattern?",
    "start": "287700",
    "end": "293500"
  },
  {
    "text": "And this is a really common design pattern that many of us in the room have. You have logs, or maybe censored\ndata, or whatever,",
    "start": "293500",
    "end": "301500"
  },
  {
    "text": "it's coming at you full force. You know, your business is growing,\nyou're doing great, but secretly you're stressing out",
    "start": "301500",
    "end": "307300"
  },
  {
    "text": "because there's a lot of data coming at you and you're like, man, how am I going\nto process this, this Python script that the intern\nwrote three years ago",
    "start": "307300",
    "end": "312500"
  },
  {
    "text": "just is not scaling. So, we're going to use Kinesis to\ncatch that inbound data, we're going to persist that data\ninto S3 using Amazon EMR",
    "start": "312500",
    "end": "322400"
  },
  {
    "text": "and Spark Streaming, and then we're\ngoing to load it into Redshift, and then we're going\nto visualize it.",
    "start": "322400",
    "end": "328500"
  },
  {
    "text": "QuickSight came out today, that's my favorite announcement personally. It's an amazing data visualization\nof BI engine",
    "start": "328500",
    "end": "335000"
  },
  {
    "text": "that you can use with third party tools or you can use with its own graphical\nengine on top.",
    "start": "335000",
    "end": "340600"
  },
  {
    "text": "But it just came out today and I don't have access to it yet for this demo, so we're going to use Matt's D3 web\nvisualization,",
    "start": "340600",
    "end": "346500"
  },
  {
    "text": "not the fanciest tool. But I think it's relevant because\nit also shows you that you can build very cost effective\nvisualization dashboards",
    "start": "346500",
    "end": "353200"
  },
  {
    "text": "that help you visualize business problems after you process the data very easily. These slides will be available, and\nin fact they're complete,",
    "start": "353200",
    "end": "360400"
  },
  {
    "text": "so apologies in advance for a lot\nof Scala on the screen, but they're designed so that you\ncan take these slides",
    "start": "360400",
    "end": "366300"
  },
  {
    "text": "and do it yourself. At the end, I show you how much it\nwould cost you in your personal AWS accounts to\ndo what we do today,",
    "start": "366300",
    "end": "372900"
  },
  {
    "text": "and hopefully you'll be pleasantly surprise. Let's just say it's a single digit.",
    "start": "372900",
    "end": "378000"
  },
  {
    "start": "377000",
    "end": "461000"
  },
  {
    "text": "So, the first part is collecting. So, we have logs on the left, we need to get them into Amazon Kinesis -",
    "start": "378000",
    "end": "384900"
  },
  {
    "text": "which again, think of Kinesis like\na buffer or a queue, a place to temporarily store your\nstreaming data",
    "start": "384900",
    "end": "390099"
  },
  {
    "text": "while you figure out what to do with it - and then we're going to use something\ncalled Spark Streaming.",
    "start": "390100",
    "end": "395200"
  },
  {
    "text": "And Spark is a wonderful framework\nthat's become very popular, I'd say, in the last two years.",
    "start": "395200",
    "end": "401000"
  },
  {
    "text": "And if you have ever heard of MapReduce or Hadoop, you can run Spark in a variety of ways.",
    "start": "401000",
    "end": "406100"
  },
  {
    "text": "Today, we're going to run Spark on\nAmazon Elastic MapReduce, and that means that we're going to\nbe running Spark on YARN.",
    "start": "406100",
    "end": "411800"
  },
  {
    "text": "YARN is really just the resource manager, the tool that takes care of distributing\nwork across a cluster",
    "start": "411800",
    "end": "417400"
  },
  {
    "text": "so you don't have to figure out how to run that intern Python script on a bunch\nof servers, it does that part for you.",
    "start": "417400",
    "end": "423199"
  },
  {
    "text": "And Spark Streaming is one of the\ncomponents or modules of Spark that is specifically for handling\ninbound streaming data.",
    "start": "423200",
    "end": "429000"
  },
  {
    "text": "So, we're going to use Spark Streaming\nto reach into Kinesis, to pull data out of the Kinesis stream, to transform it just a little bit\nto show you",
    "start": "429000",
    "end": "435400"
  },
  {
    "text": "how you would do real time data transformation, and then finally to persist it to\nS3.",
    "start": "435400",
    "end": "440500"
  },
  {
    "text": "I chose Spark Streaming, and I could have written a Kinesis Client Library, or I could have used another framework, I could have done it manually myself,",
    "start": "440500",
    "end": "446300"
  },
  {
    "text": "but Spark Streaming is powerful and simple and it gives you sort of a unified platform that you can use with Python, or\nyou can use even Spark SQL",
    "start": "446300",
    "end": "453700"
  },
  {
    "text": "if you prefer SQL and you don't want to code. It's a great framework that makes\nbig data accessible and yet powerful and scalable.",
    "start": "453700",
    "end": "462600"
  },
  {
    "start": "461000",
    "end": "475000"
  },
  {
    "text": "Then we're going to process the data. We're going to do some processing\nwith Spark, you're going to see Spark in action.",
    "start": "462600",
    "end": "467699"
  },
  {
    "text": "There's a difference between Spark Streaming, which is taking the streaming data\nout and manipulating it, and then Spark, which actually does\nthe in-place data processing.",
    "start": "467700",
    "end": "477000"
  },
  {
    "start": "475000",
    "end": "511000"
  },
  {
    "text": "Then once we process the data and\nre-persist the transformed data, we do some basically ETL -",
    "start": "477000",
    "end": "482300"
  },
  {
    "text": "we persist it back on S3 and then\nwe load from S3 into Redshift. And we use Redshift as a data warehouse",
    "start": "482300",
    "end": "488400"
  },
  {
    "text": "that we can point Tableau to or TIBCO\nJaspersoft or MicroStrategy, or whatever your data visualization\ntool of choice is to,",
    "start": "488400",
    "end": "495800"
  },
  {
    "text": "and show that pretty picture. Because at the end of the day, what\nare we here for? Not to tell people about Hadoop, it's to show that pie chart to the\nCFO, that's what really matters.",
    "start": "495800",
    "end": "504200"
  },
  {
    "text": "It's to solve a business problem, it's to present the data in a way\nthat makes the data valuable.",
    "start": "504200",
    "end": "512300"
  },
  {
    "start": "511000",
    "end": "558000"
  },
  {
    "text": "So, let's do it. So, first step is to set up the AWS\ncommand line tool.",
    "start": "512300",
    "end": "520500"
  },
  {
    "text": "And we're going to switch to console here. So, AWS command line tool - that's\nkind of hard to see,",
    "start": "520500",
    "end": "526200"
  },
  {
    "text": "that's okay - AWS command line tool is a very powerful tool. It's simple but it can be used with\na large number of services",
    "start": "526200",
    "end": "534000"
  },
  {
    "text": "and we're going to be using it today\nto launch clusters, to create S3 buckets, to create Kinesis streams.",
    "start": "534000",
    "end": "539500"
  },
  {
    "text": "And the reason why I use the command\nline tool is because if you get used to the command line tool then it's one step further to be\nable to use the SKD in programming,",
    "start": "539500",
    "end": "546400"
  },
  {
    "text": "and it's also something that you\ncan script. So, what we do today, you can potentially automate by scripting a lot of these commands.",
    "start": "546400",
    "end": "552000"
  },
  {
    "text": "It's harder to script clicks on the\nmanagement console, right?",
    "start": "552000",
    "end": "559700"
  },
  {
    "start": "558000",
    "end": "629000"
  },
  {
    "text": "So, the first thing we're going to\ndo with the command line tool is to create a Kinesis stream. We have to create a location where we can -",
    "start": "559700",
    "end": "565200"
  },
  {
    "text": "and push our streaming data. So, let's use the command line tool\nto create a stream. And a stream is really just, again,",
    "start": "565200",
    "end": "570900"
  },
  {
    "text": "think of it like a queue or a buffer, and you can add or remove capacity,\nthe ability to ingest at the scale that you need to ingest,",
    "start": "570900",
    "end": "577500"
  },
  {
    "text": "and also read and write from it by the number of shards in your stream. We're going to create a single shard stream",
    "start": "577500",
    "end": "585700"
  },
  {
    "text": "which is more than enough for this demo. So, let's take this one line command\n- there we go.",
    "start": "585700",
    "end": "595200"
  },
  {
    "text": "So, I created a stream. And this is pretty amazing.",
    "start": "595200",
    "end": "600500"
  },
  {
    "text": "If you've ever set up, for example,\nlike, a Kafka cluster or any kind of, you know, stream\nto capture data",
    "start": "600500",
    "end": "606500"
  },
  {
    "text": "you know that it doesn't take one\ncommand in two seconds. And so what's interesting about this\nexample, too,",
    "start": "606500",
    "end": "612300"
  },
  {
    "text": "is that I did a single shard stream but I could have done a 100 shard stream to handle an enormous amount of big data.",
    "start": "612300",
    "end": "618699"
  },
  {
    "text": "I was joking yesterday that if, you\nknow, I wrote a game and suddenly Taylor Swift started\nplaying with it and Tweeted about it, I better have\na system ready",
    "start": "618700",
    "end": "624800"
  },
  {
    "text": "to capture all of the in-game events\nat scale when that happens.",
    "start": "624800",
    "end": "630800"
  },
  {
    "start": "629000",
    "end": "718000"
  },
  {
    "text": "Next thing we're going to do is create\nan Amazon S3 bucket. Now, S3, you can think of it like a just super scalable file system -",
    "start": "630800",
    "end": "636600"
  },
  {
    "text": "in reality it's an object file storage system. You can put anything in S3. It's a great place to store all your data",
    "start": "636600",
    "end": "643200"
  },
  {
    "text": "because it's highly, highly durable\nand it's very cost effective. It's extremely inexpensive to store\na lot of data in S3.",
    "start": "643200",
    "end": "650300"
  },
  {
    "text": "So, we're going to use Kinesis as\nthat temporary - think of it like a scalable staging area where we push the streaming data -",
    "start": "650300",
    "end": "655600"
  },
  {
    "text": "and then we're going to pull the data out, but we're going to persist it for\nthe long term in S3. One great advantage of S3 is that it works",
    "start": "655600",
    "end": "661699"
  },
  {
    "text": "with a lot of different products,\na lot of different frameworks. So, if I have S3 as kind of the center\nof my data universe",
    "start": "661700",
    "end": "668300"
  },
  {
    "text": "then I can use Spark Steaming, I\ncan use Presto, I can load into Redshift.",
    "start": "668300",
    "end": "673399"
  },
  {
    "text": "It's a great place to put your data because multiple tools can read and\nwrite to S3, and importantly, multiple tools in parallel.",
    "start": "673400",
    "end": "680700"
  },
  {
    "text": "Many of use work in big companies, and big companies, you'll know, may\nhave different BI units or different affiliates and, you know,",
    "start": "680700",
    "end": "687900"
  },
  {
    "text": "the big data people in the white lab coats actually write the algorithms. They\nmay have their tool of choice. And it's important to give people\nflexibility.",
    "start": "687900",
    "end": "694800"
  },
  {
    "text": "So, if you have your data in S3 then they can use a variety of tools\nto access that data without having to create multiple\ncopies of the data.",
    "start": "694800",
    "end": "702500"
  },
  {
    "text": "So, if you think of a database - if they want to use different databases, you'd have to have different databases\nand copies of the data wasting storage space and money,",
    "start": "702500",
    "end": "708399"
  },
  {
    "text": "whereas if you use S3 in a framework\nlike Spark, or Hive, or anything that can talk to S3,",
    "start": "708400",
    "end": "713900"
  },
  {
    "text": "then you can have one copy of the data and multiple tools talking to it. So, let's make a bucket.",
    "start": "713900",
    "end": "720300"
  },
  {
    "start": "718000",
    "end": "749000"
  },
  {
    "text": "A bucket is just a place in S3 -\nthink of it like a file folder - where we're going to stick data.",
    "start": "720300",
    "end": "728200"
  },
  {
    "text": "And hopefully the name isn't taken. Oh, someone already took that. We'll\ncall it 2. Great.",
    "start": "728200",
    "end": "747100"
  },
  {
    "text": "Next what we're going to do is create a cluster. Now, what I did so far, you know,",
    "start": "747100",
    "end": "752300"
  },
  {
    "start": "749000",
    "end": "882000"
  },
  {
    "text": "some people are like, wow, slow clap, but now we get into the cool stuff. We're going to create a Hadoop cluster.",
    "start": "752300",
    "end": "758000"
  },
  {
    "text": "And for a lot of people this is the\nintimidating part when you're starting with big data. I'm going to create a - and this\nis - what am I doing? -",
    "start": "758000",
    "end": "763400"
  },
  {
    "text": "a two node EMR cluster - an Elastic\nMapReduce cluster - that's a managed Hadoop cluster.",
    "start": "763400",
    "end": "768700"
  },
  {
    "text": "And I like to think about Hadoop a bit like an operating system for big data. It used to be that Hadoop was primarily\na distributed file system",
    "start": "768700",
    "end": "776199"
  },
  {
    "text": "called HDSF, and a data processing\nframework called MapReduce - which is really a way to distribute\nwork around a cluster",
    "start": "776200",
    "end": "783300"
  },
  {
    "text": "and process it - but Hadoop has evolved a lot and EMR has evolved with it. So, Hadoop 2 has something called YARN.",
    "start": "783300",
    "end": "790200"
  },
  {
    "text": "And YARN, think of it like the coach, it's a resource manager that distributes\nwork around a cluster. But on top of YARN you can have a number",
    "start": "790200",
    "end": "796400"
  },
  {
    "text": "of different applications running\nin parallel. So, you can have a SQL application\ncalled Hive",
    "start": "796400",
    "end": "802000"
  },
  {
    "text": "that uses HiveQL to do, like, select * from my data in S3, for example. You can have Pig as another popular framework,",
    "start": "802000",
    "end": "808300"
  },
  {
    "text": "Spark is very popular. Think of these like applications\non your operating system. So, many of you may use Linux, or\nhave used Linux,",
    "start": "808300",
    "end": "814900"
  },
  {
    "text": "and Linux has something called distributions. So, there's Debian, there's Red Hat,\nthere's CentOS, there's all these distributions.",
    "start": "814900",
    "end": "820300"
  },
  {
    "text": "All these distributions are somewhat similar but they have different packaging formats. Hadoop is really kind of the same thing.",
    "start": "820300",
    "end": "825399"
  },
  {
    "text": "There's different Hadoop distributions and EMR supports both the MapR commercial\ndistribution and our own distribution which is",
    "start": "825400",
    "end": "832000"
  },
  {
    "text": "pretty much the same as Apache Hadoop. And then on top of these distributions you can run Hadoop applications that run on YARN",
    "start": "832000",
    "end": "839200"
  },
  {
    "text": "that takes care of distributing the\nwork that they do out to the cluster. And one of these applications is Spark.",
    "start": "839200",
    "end": "845100"
  },
  {
    "text": "And we're going to use Spark and\nHive today. And what Hive gives you is a convenient way",
    "start": "845100",
    "end": "850300"
  },
  {
    "text": "to express SQL commands - and so\nyou can write the SQL that you've always written, pretty much - and it takes that SQL and translates it",
    "start": "850300",
    "end": "857000"
  },
  {
    "text": "into MapReduce jobs, or Tez jobs, or whatever execution engine Hive\nis sitting on top of.",
    "start": "857000",
    "end": "863800"
  },
  {
    "text": "Today, we're going to be using Spark SQL. And Spark SQL similarly, you write SQL and then it goes off and bends off\na Spark job.",
    "start": "863800",
    "end": "871100"
  },
  {
    "text": "You don't have to know Java, you\ndon't have to know Scala, you don't have to know Python, you\ncan write as you always have and it takes care of the heavy lifting\nunder the covers.",
    "start": "871100",
    "end": "878200"
  },
  {
    "text": "So, there's a lot of frameworks and you can go as deep as you want\nwith Hadoop. So, again, we're going to use the\ncommand line to spin up a cluster.",
    "start": "878200",
    "end": "888300"
  },
  {
    "start": "882000",
    "end": "1027000"
  },
  {
    "text": "And again, this is designed to be taken home and tired on your own time. There.",
    "start": "888300",
    "end": "896100"
  },
  {
    "text": "And let's just think about this for a second - and I'll show you the console so\nyou don't think I'm faking it - I just spun up a Hadoop cluster and\nI did a 2 node simple cluster",
    "start": "896100",
    "end": "904300"
  },
  {
    "text": "because I want it to actually spin\nup in time for this demo because I have 44 minutes and 47\nseconds left. It's going to spin up in about two,\nthree minutes.",
    "start": "904300",
    "end": "910700"
  },
  {
    "text": "This could have been a 100 node cluster. If you had expanded your limits ahead\nof time with AWS",
    "start": "910700",
    "end": "916100"
  },
  {
    "text": "this could have been a thousand node cluster, this could have been a ten thousand\nnode cluster.",
    "start": "916100",
    "end": "921400"
  },
  {
    "text": "We have customers regularly running\nmulti-thousand node clusters to do their jobs, and this is incredible.",
    "start": "921400",
    "end": "927100"
  },
  {
    "text": "Again, think back to that guy who\nwas never seen again four years ago who built your Hadoop\ncluster on premises,",
    "start": "927100",
    "end": "933000"
  },
  {
    "text": "it was not one command in a minute. Not only is it provisioning the cluster,",
    "start": "933000",
    "end": "938899"
  },
  {
    "text": "it's going off and it's installing Spark, it's installing those applications\nthat you need to do your big data processing as well.",
    "start": "938900",
    "end": "944399"
  },
  {
    "text": "So, it's taking care of all of that\nheavy lifting because in this room very few of\nus are getting paid for spinning up cluster, we're getting\npaid for analyzing data",
    "start": "944400",
    "end": "951300"
  },
  {
    "text": "and the insights that we derive from them. So, again, we like to say at Amazon, you know,",
    "start": "951300",
    "end": "956500"
  },
  {
    "text": "it gets rid of undifferentiated heavy lifting. It's a bit of a muffle. It gets rid of the muck that is really\nnot core to most of our jobs.",
    "start": "956500",
    "end": "963300"
  },
  {
    "text": "So, let's look at the console and\nmake sure that Matt's not lying.",
    "start": "963300",
    "end": "968899"
  },
  {
    "text": "Hopefully that's big enough for you\nguys to see. And we'll go to EMR.",
    "start": "968900",
    "end": "975300"
  },
  {
    "text": "I do have a cluster that is spun\nup for Strata conference last week - oops, I guess I spun it up in the\nother environment -",
    "start": "975300",
    "end": "984800"
  },
  {
    "text": "one second, let me get to my other\nenvironment.",
    "start": "984800",
    "end": "992300"
  },
  {
    "text": "We'll wait for that. There we go.",
    "start": "992300",
    "end": "1007100"
  },
  {
    "text": "While we wait for the Wi-Fi - big data is fast, but sometimes conference\nWi-Fi is a little slow. There it is.",
    "start": "1007100",
    "end": "1013600"
  },
  {
    "text": "So, demo reInvent, if you were paying attention, is the one we just spun up, and I have one that I spun up for\nStrata last week just in case,",
    "start": "1013600",
    "end": "1020100"
  },
  {
    "text": "but we'll come back to this in a\ncouple of minutes so you can see how that cluster will\nbe fully provisioned and running.",
    "start": "1020100",
    "end": "1028000"
  },
  {
    "start": "1027000",
    "end": "1144000"
  },
  {
    "text": "Okay, so, two minutes, we have a\nHadoop cluster spun up. Now, let's start up a Redshift cluster,\nso, data warehouses.",
    "start": "1028000",
    "end": "1034900"
  },
  {
    "text": "There was another guy who lost his\nmind, remember, who set up a data warehouse on-premises\na few years ago. Data warehouses are notoriously complex,\nslow, and hard to manage.",
    "start": "1034900",
    "end": "1045000"
  },
  {
    "text": "They solve a big data business problem but there's a really good reason\nwhy we came out with Amazon Redshift,",
    "start": "1045000",
    "end": "1051100"
  },
  {
    "text": "is because they're particularly hard to tune, they're hard to deploy, and just\nlike Hadoop, they're typically a large cluster",
    "start": "1051100",
    "end": "1058000"
  },
  {
    "text": "that is an operation burden for many companies. So, Redshift is a fully managed data warehouse. And again, to spin up a Redshift\ncluster, one command.",
    "start": "1058000",
    "end": "1064800"
  },
  {
    "text": "We're going to do a single node cluster today that this could easily be a multi-node cluster. And this isn't a full session on\nRedshift, but long story short,",
    "start": "1064800",
    "end": "1072700"
  },
  {
    "text": "Redshift is a columnar data warehouse, so it's - think of it like a database that's\noptimized for analytics.",
    "start": "1072700",
    "end": "1080300"
  },
  {
    "text": "A lot of people get confused by the difference between columnar and a traditional\nrelational database, and what it really boils down to\nis if you think of, like,",
    "start": "1080300",
    "end": "1086700"
  },
  {
    "text": "a user profile when you log into\na website or to a mobile application, usually if you pull the user profile table,",
    "start": "1086700",
    "end": "1092799"
  },
  {
    "text": "or a role from the user profile, you need most if not all of the columns\nto populate the UI,",
    "start": "1092800",
    "end": "1098200"
  },
  {
    "text": "you know, the name, the profile picture,\nall this other stuff. Whereas if it's a columnar data warehouse and a typical analytics query, it's\nsomething like,",
    "start": "1098200",
    "end": "1106100"
  },
  {
    "text": "select * from AWS employees who like\nthe Maple Leafs, and there's only a few, one of whom is Matt -",
    "start": "1106100",
    "end": "1112500"
  },
  {
    "text": "but the point is that I didn't need\nall of the columns in the employee database to answer\nthat question.",
    "start": "1112500",
    "end": "1117600"
  },
  {
    "text": "So, Redshift arranges its data on\ndisk in columnar format. So, I only need the names of the employee",
    "start": "1117600",
    "end": "1123700"
  },
  {
    "text": "and their favorite hokey team, and then I can just ask those two\ncolumns for the information",
    "start": "1123700",
    "end": "1129300"
  },
  {
    "text": "that I need to answer my question, and Redshift's going to very quickly\ndescend into those columns and pull out the information it needs",
    "start": "1129300",
    "end": "1134399"
  },
  {
    "text": "instead of having to scan every single row. So, it's column based instead of row bases. It's a columnar data warehouse.",
    "start": "1134400",
    "end": "1139600"
  },
  {
    "text": "It does a lot of other stuff under\nthe covers like encryption, and it does great compression, and\nmany other things",
    "start": "1139600",
    "end": "1146500"
  },
  {
    "start": "1144000",
    "end": "1221000"
  },
  {
    "text": "that makes it run quickly for billions\nand billions of rows. But it's a great tool for analytic\nstyle queries",
    "start": "1146500",
    "end": "1152000"
  },
  {
    "text": "if you're using SQL and if you have tools that need to connect to it with JDBC and ODBC.",
    "start": "1152000",
    "end": "1158000"
  },
  {
    "text": "It works with all of your existing tools. So, let's spin up that cluster.",
    "start": "1158000",
    "end": "1168700"
  },
  {
    "text": "Woops, I already spun up that one,\nlet's do another one.",
    "start": "1168700",
    "end": "1174100"
  },
  {
    "text": "What should we call it? Demo Awesome. Sorry.",
    "start": "1174100",
    "end": "1195700"
  },
  {
    "text": "I get pumped up by the keynote. That\nshould be fine. There.",
    "start": "1195700",
    "end": "1205500"
  },
  {
    "text": "So, sorry that took, you know, 23\nseconds instead of 10. Here we have the output - and again,",
    "start": "1205500",
    "end": "1210800"
  },
  {
    "text": "I'll show you on the console in a second - that we have a data warehouse that\nwill spin up in just a couple of minutes and will\nbe available for us to use",
    "start": "1210800",
    "end": "1216700"
  },
  {
    "text": "before the end of the session. Okay, so I'm doing pretty good, I\nspun up a Hadoop cluster,",
    "start": "1216700",
    "end": "1223600"
  },
  {
    "text": "I spun up a data warehouse, I spun\nup a Kinesis stream, I created and S3 bucket all in about, you know,",
    "start": "1223600",
    "end": "1229100"
  },
  {
    "text": "less than 20 minutes. So, let's go back to what we're actually\ngoing to do. So, we're going to collect the data. What we're going to do is we're going to fake it",
    "start": "1229100",
    "end": "1235000"
  },
  {
    "text": "in terms of getting data into our stream. We're going to use Log4j, or rather\na jar file, to push Apache HTTP server logs into\nour Kinesis stream.",
    "start": "1235000",
    "end": "1243300"
  },
  {
    "text": "Then we're going to process those\nlogs with Amazon EMR, and then we're going to analyze them\nfinally with Redshift.",
    "start": "1243300",
    "end": "1253500"
  },
  {
    "text": "So, first step is to download a jar, and I've already done this, so I'm\nnot going to do it. But it's just a simple demo program",
    "start": "1253500",
    "end": "1259799"
  },
  {
    "text": "that has Apache log files - or rather\nyou log entries - and it's going to push those automatically\ninto our Kinesis stream",
    "start": "1259800",
    "end": "1266399"
  },
  {
    "text": "just to have some data, some real\ndata to play with. These are just instructions that\nI included in the slide",
    "start": "1266400",
    "end": "1272600"
  },
  {
    "text": "so you have them when you take them home on how to set up the program so you\ncan push some data",
    "start": "1272600",
    "end": "1277900"
  },
  {
    "text": "into your Kinesis stream. And importantly, this is what the\nlog file looks like. And look at this, because this is\nthe data that we're going to grab.",
    "start": "1277900",
    "end": "1286300"
  },
  {
    "start": "1279000",
    "end": "1324000"
  },
  {
    "text": "And some important characteristics\nabout this data is that we don't necessarily need\nall of the data. This is a very simple example, very\nsimple log file.",
    "start": "1286300",
    "end": "1293200"
  },
  {
    "text": "If any of you have ever worked in\nadvertising or marketing you'll know that Omniture logs, for example, have a lot more columns than this.",
    "start": "1293200",
    "end": "1299400"
  },
  {
    "text": "And typically the questions that\nwe need to answer to solve our business problems don't\nneed all of those columns. So, one thing we're going to do today",
    "start": "1299400",
    "end": "1305100"
  },
  {
    "text": "in our streaming data program is\ndo field extraction. When we go and grab these log lines\nfrom the stream,",
    "start": "1305100",
    "end": "1311100"
  },
  {
    "text": "we're only going to pull out and\nwe're going to extract the data that we need to actually\nload into the data warehouse.",
    "start": "1311100",
    "end": "1317200"
  },
  {
    "text": "That's going to be part of our ETL process. So, if you're familiar with Apache\nHTTP server then this will be no news to you,\njust a standard log.",
    "start": "1317200",
    "end": "1324700"
  },
  {
    "start": "1324000",
    "end": "1360000"
  },
  {
    "text": "Again, we're going to use Spark. So, Spark is a fast, general purpose\ndata processing engine,",
    "start": "1324700",
    "end": "1329899"
  },
  {
    "text": "and you can think of it kind of as\na new and improved MapReduce. So, MapReduce is one framework, one\nway of distributing",
    "start": "1329900",
    "end": "1336400"
  },
  {
    "text": "data processing work around a cluster. Spark has a lot of improvements over\ntraditional MapReduce,",
    "start": "1336400",
    "end": "1342100"
  },
  {
    "text": "mainly it uses memory and other techniques to more quickly and efficiently use\nyour cluster to process data.",
    "start": "1342100",
    "end": "1349100"
  },
  {
    "text": "Nice thing about Spark, too, is it\nhas a machine learning framework, it has a graph framework, it has\na streaming framework.",
    "start": "1349100",
    "end": "1354800"
  },
  {
    "text": "So, it kind of gives you a unified platform to do various different types of\ndata processing.",
    "start": "1354800",
    "end": "1361300"
  },
  {
    "text": "And this is a little bit more of\na detailed view of how we're going to do the data\nprocessing. So, we're going to push that data\ninto Kinesis, and then we're going to use Spark\nStreaming to pull it out.",
    "start": "1361300",
    "end": "1368000"
  },
  {
    "text": "Now, interestingly, Spark Streaming,\nbehind the scenes, actually has a Kinesis library, and it uses our Kinesis Client Library\nunder the covers",
    "start": "1368000",
    "end": "1374900"
  },
  {
    "text": "to interact with Kinesis. So, just if you were to write your\nown custom Kinesis application,",
    "start": "1374900",
    "end": "1381000"
  },
  {
    "text": "you can use Spark Streaming as a\nhigher level framework, and under the cover it uses the Kinesis\nClient Library. Importantly, it also uses DynamoDB via",
    "start": "1381000",
    "end": "1388100"
  },
  {
    "text": "the Kinesis Client Library to keep\ntrack of things. Because what happens if you're processing\nthat stream and then, you know, you fat finger it",
    "start": "1388100",
    "end": "1393600"
  },
  {
    "text": "and you shut down your cluster by accident, or you pause the streaming processing, how do you know where to pick up again?",
    "start": "1393600",
    "end": "1399800"
  },
  {
    "text": "Or if you have multiple applications\nreading from the stream, you need some form of check pointing, you need to be able to keep track\nof what has been processed.",
    "start": "1399800",
    "end": "1406500"
  },
  {
    "text": "And that's why DynamoDB is used on\nthe backend, it's kind of your source of truth. What messages have I touched, what\nlogs have I processed,",
    "start": "1406500",
    "end": "1413300"
  },
  {
    "text": "where do I need to start again if\nthings get interrupted. And then Spark Streaming will then persist it",
    "start": "1413300",
    "end": "1419100"
  },
  {
    "text": "and push it back to S3. So, let's log into our cluster.",
    "start": "1419100",
    "end": "1426500"
  },
  {
    "start": "1421000",
    "end": "1491000"
  },
  {
    "text": "First of all we're going to start\npushing to the stream. One second.",
    "start": "1426500",
    "end": "1439200"
  },
  {
    "text": "Okay, great. So, that was - I just\nfired off a Java application that's going to start pushing records\ninto the stream. So, this is in real time we're now\nwriting to our Kinesis stream.",
    "start": "1439200",
    "end": "1446000"
  },
  {
    "text": "So, we're simulating streaming data, we're pretending that we had a fleet of servers running Apache HTTP server,",
    "start": "1446000",
    "end": "1451800"
  },
  {
    "text": "and that we're pushing the logs in\nreal time to the Kinesis stream. That's what that's doing.",
    "start": "1451800",
    "end": "1460100"
  },
  {
    "text": "Now, we're going to log into our\nEMR server.",
    "start": "1460100",
    "end": "1472299"
  },
  {
    "text": "That's a little messy, let's do another one.",
    "start": "1472300",
    "end": "1488600"
  },
  {
    "text": "Okay, so we're going to log into\nour EMR server, there we are, so we have SSHed into\nour EMR server.",
    "start": "1488600",
    "end": "1495500"
  },
  {
    "start": "1491000",
    "end": "1569000"
  },
  {
    "text": "The first thing we need to do is\ndownload the Kinesis client because remember, the Spark Streaming\napplication uses the Kinesis client to go do\ncheck pointing",
    "start": "1495500",
    "end": "1504000"
  },
  {
    "text": "and also to reach into the Kinesis stream. And I think I've already done this,\nbut we'll double check.",
    "start": "1504000",
    "end": "1519700"
  },
  {
    "text": "There we go. Okay, yeah, I did already do it. And then we're going to launch the\nSpark shell.",
    "start": "1519700",
    "end": "1525500"
  },
  {
    "text": "Now, there are some graphical interfaces for Spark, and there's some partners we have\nin the ecosystem that provide a nice kind of easy\nway to interact with Spark,",
    "start": "1525500",
    "end": "1531900"
  },
  {
    "text": "but we're going to go deep and we're\ngoing to do it using the shell and actually in the console to show you, you know, how the pros do it.",
    "start": "1531900",
    "end": "1538600"
  },
  {
    "text": "And there should be no difference\nfrom how you do it and the pros do it, it's a very easy\nline to cross.",
    "start": "1538600",
    "end": "1544799"
  },
  {
    "text": "So, the Spark shell, think of it\nlike a terminal, like a shell, and it allows you to in real time\nto program Spark applications.",
    "start": "1544800",
    "end": "1550400"
  },
  {
    "text": "And we're going to use Scala, and\nthere's also a Python Spark shell. So, you can - if you've ever used\nPython, there's a Python shell",
    "start": "1550400",
    "end": "1556500"
  },
  {
    "text": "that you can launch and you can sort\nof in real time code Python and figure out, you know,\nhow you're messing up commands when it gives you errors in real time",
    "start": "1556500",
    "end": "1562200"
  },
  {
    "text": "instead of having to write the whole\nprogram and then run it. Same kind of concept.",
    "start": "1562200",
    "end": "1569000"
  },
  {
    "start": "1569000",
    "end": "1627000"
  },
  {
    "text": "Now, you might be asking yourself\nif you paid attention today in the keynote, why not just use\nKinesis Firehose?",
    "start": "1569000",
    "end": "1575200"
  },
  {
    "text": "That's a good question. If your sole aim is push the data\ninto Redshift for analysis",
    "start": "1575200",
    "end": "1580799"
  },
  {
    "text": "then Firehose is actually probably\nthe best choice. It's a really easy way to push data\nvia Kinesis into S3",
    "start": "1580800",
    "end": "1587400"
  },
  {
    "text": "or into Redshift for analysis. The reason why I'm using Spark Streaming today and why I didn't change it for Firehose\nthis morning",
    "start": "1587400",
    "end": "1592799"
  },
  {
    "text": "after the announcement, is because\nwe're doing one extra step, we're manipulating the data on the way.",
    "start": "1592800",
    "end": "1598300"
  },
  {
    "text": "So, as we pull it out of the stream\nwe're doing some field extraction and some transformation, and we're\nstructuring or ordering the data.",
    "start": "1598300",
    "end": "1603700"
  },
  {
    "text": "So, imagine you have a bunch of different logs from different HTTP servers that\ndon't have the same format. Image some of these HTTP servers\noccasionally kind of mangle their logs.",
    "start": "1603700",
    "end": "1611400"
  },
  {
    "text": "We would need some kind of a streaming framework that could take all of these different\nlog formats and account for some of the errors\nthat it creates",
    "start": "1611400",
    "end": "1617300"
  },
  {
    "text": "and introduced into the log files\nand normalize them into a structured format that can\nthen go into your data warehouse.",
    "start": "1617300",
    "end": "1623200"
  },
  {
    "text": "And that's what streaming frameworks\nare really good for. So, now we get to the Scala. You ready?",
    "start": "1623200",
    "end": "1629600"
  },
  {
    "start": "1627000",
    "end": "1669000"
  },
  {
    "text": "And again, all of the code that you\nneed is in these slides. And it's not much, it's, like, two\nor three slides, tops.",
    "start": "1629600",
    "end": "1634799"
  },
  {
    "text": "And even if you don't code, don't\ngot to sleep now, you can follow this. So, let's look at it.",
    "start": "1634800",
    "end": "1640100"
  },
  {
    "text": "So, first of all it's an easy part, we're just importing some libraries\nthat we need our program",
    "start": "1640100",
    "end": "1645200"
  },
  {
    "text": "to be aware of or to have loaded such that it can understand how to\nread from the Kinesis stream",
    "start": "1645200",
    "end": "1650500"
  },
  {
    "text": "and persist stuff to S3 and do some\nprocessing.",
    "start": "1650500",
    "end": "1665400"
  },
  {
    "text": "So, let that import, nothing fancy here. The next thing we do is some pretty\nsimple stuff.",
    "start": "1665400",
    "end": "1670900"
  },
  {
    "start": "1669000",
    "end": "1701000"
  },
  {
    "text": "We say, hey, I'm going to point to a stream called AccessLogStream. That was that Kinesis stream that\ncreated in that first step.",
    "start": "1670900",
    "end": "1677200"
  },
  {
    "text": "That's where our data has been pushed. Remember, there was all the log files\nbeing pushed into the stream?\n615\n00:28:00,800 --> 00:28:02,600 That's it.",
    "start": "1677200",
    "end": "1680800"
  },
  {
    "text": "The second thing is what region are you in and where do you want to persist\nthis stuff, what S3 bucket do you want to be\nthe destination",
    "start": "1682600",
    "end": "1689700"
  },
  {
    "text": "after you do that pulling it out\nof the stream and transforming it. And then there's just a couple of\nlines to configure",
    "start": "1689700",
    "end": "1694900"
  },
  {
    "text": "what's called the SparkContext, and that is what actually runs the\napplication.",
    "start": "1694900",
    "end": "1701700"
  },
  {
    "text": "And then the last thing you need\nto do, and this is it, is actually read the stuff from the stream",
    "start": "1701700",
    "end": "1708299"
  },
  {
    "text": "and then persist it to S3. So, these few lines of code - the first one determines how many\nshards do you have in the stream,",
    "start": "1708300",
    "end": "1715500"
  },
  {
    "text": "how much capacity have you added\nto your Kinesis stream. So, if there's multiple shards, that\nmeans you can ingest",
    "start": "1715500",
    "end": "1720600"
  },
  {
    "text": "and also read more data, but the program needs to know to\ngo read from those multiple shards and then create a worker per stream.",
    "start": "1720600",
    "end": "1727400"
  },
  {
    "text": "So, this is a fully scalable application. I did one stream today, but if I\ndid 50, the code as is would auto-scale.",
    "start": "1727400",
    "end": "1735800"
  },
  {
    "text": "We may need a bigger cluster if we\nhad a lot more data coming in, but this code as written will scale\nto your big data needs.",
    "start": "1735800",
    "end": "1741899"
  },
  {
    "text": "So, I encourage you to take the code\nand take this code and to try and expand it, try and\npush more data at it and to watch it scale.",
    "start": "1741900",
    "end": "1747900"
  },
  {
    "text": "It'll add more workers depending\non the size of the stream that is capturing your streaming\ndata automatically.",
    "start": "1747900",
    "end": "1761900"
  },
  {
    "text": "So, we can see the program is working\nbecause it just said 1, so yes, that's correct, I have a\n1 shard in my stream.",
    "start": "1761900",
    "end": "1768500"
  },
  {
    "text": "And now, let's actually fire off\nthe program.",
    "start": "1768500",
    "end": "1780900"
  },
  {
    "start": "1780000",
    "end": "1834000"
  },
  {
    "text": "And this last part that I just did is this, and all this does is kicks it off, says - it's a little bit of logic.",
    "start": "1780900",
    "end": "1786400"
  },
  {
    "text": "And this is my crazy transformation\nthat I did. So, the ETL, the data transformation,\nis just in the middle there, it's just going to output using a\nsimple date format to S3.",
    "start": "1786400",
    "end": "1795100"
  },
  {
    "text": "But in that center block there where it says, write each RDD, that's just Sparks\nrepresentation of data to S3.",
    "start": "1795100",
    "end": "1802500"
  },
  {
    "text": "That's where you would add your data\nprocessing logic. So, a lot of the other lines you\ncan just copy, forget about it, someone else wrote them, it works,\nbut that middle part",
    "start": "1802500",
    "end": "1809200"
  },
  {
    "text": "in Python or Spark, or I'll show\nyou in a SQL in a bit, that's where you add your data processing logic. So, you need to transform some text\nvalue inside the logs,",
    "start": "1809200",
    "end": "1817300"
  },
  {
    "text": "that's where it goes. So, if you're expanding this program,\nyou drop it into that block,",
    "start": "1817300",
    "end": "1822800"
  },
  {
    "text": "you know what your data looks like coming in and you know what your data has to\nlook like coming out, so you write a little bit of code\nto true that transformation.",
    "start": "1822800",
    "end": "1828600"
  },
  {
    "text": "And that's it. So, it's going to\npull out of the Kinesis stream, transform it, and persist it to S3.",
    "start": "1828600",
    "end": "1834900"
  },
  {
    "text": "And it should be working. Great. Now, this actually works on sort\nof a window,",
    "start": "1834900",
    "end": "1840900"
  },
  {
    "text": "so what it does is I think I had\nit set to 60 seconds by default, but it pulls some data from the stream and every 10 seconds persists to\nS3,",
    "start": "1840900",
    "end": "1848399"
  },
  {
    "text": "maybe every 60 seconds, whatever\nit is in the code there. And now it batches it a little bit,",
    "start": "1848400",
    "end": "1853600"
  },
  {
    "text": "because rather than write a million\nlittle log files to S3, it bundles up little chunks in a\nwindow that you define and persist it to S3.",
    "start": "1853600",
    "end": "1861500"
  },
  {
    "text": "Let's look at that data.",
    "start": "1861500",
    "end": "1870800"
  },
  {
    "text": "So, by the way, the Hadoop cluster is up. See, it says, waiting? That means\nit's waiting for you.",
    "start": "1870800",
    "end": "1877200"
  },
  {
    "text": "It's ready to go. And let's look at that Redshift cluster.",
    "start": "1877200",
    "end": "1889600"
  },
  {
    "text": "It's also up. So, remember demo Awesome - so, we have a Redshift cluster, a\ndata warehouse,",
    "start": "1889600",
    "end": "1895700"
  },
  {
    "text": "and a Hadoop cluster fully provisioned\nand ready for jobs. Let's look at the S3 bucket.",
    "start": "1895700",
    "end": "1911600"
  },
  {
    "text": "Like I was writing to reInvent, too, right? There we go. So, it's not empty.",
    "start": "1911600",
    "end": "1917100"
  },
  {
    "text": "Did you see me do a command to push\nstuff into S3 and try to fake it? No, because I didn't, it's working. We have a streaming application using Hadoop,",
    "start": "1917100",
    "end": "1923700"
  },
  {
    "text": "pulling data from a Kinesis queue\nthat we pushed during this class doing some kind of basis transformation - And in this case, I'm not doing much,\nto be fair -",
    "start": "1923700",
    "end": "1930100"
  },
  {
    "text": "and pushing it into S3. So, I know that seems like sort of\na blur of Scala",
    "start": "1930100",
    "end": "1935400"
  },
  {
    "text": "but it wasnt that complicated, was it? And we've set up a fully, end to\nend streaming data processing application now.",
    "start": "1935400",
    "end": "1943299"
  },
  {
    "text": "Now, what's the next step? Because this isn't going to do us\nmuch, is it? You show this to your CFO and they're\ngoing to be like,",
    "start": "1943300",
    "end": "1949500"
  },
  {
    "text": "what is that. We need to get to that\nbar chart, right? We need to get to that final step, how do we actually take this processed\ndata and make sense of it.",
    "start": "1949500",
    "end": "1956400"
  },
  {
    "text": "Well, the answer is we want to query\nagainst it. Right? We want to be able to execute analytics queries, and then as an extension of that we want",
    "start": "1956400",
    "end": "1961800"
  },
  {
    "text": "to visualize the data in some way. So, that's the fun part. Well, maybe some of you think the\nterminal part is the fun part,",
    "start": "1961800",
    "end": "1967400"
  },
  {
    "text": "I know I do, but other people want to see some fun stuff on the screen, so\nlet's do that.",
    "start": "1967400",
    "end": "1973900"
  },
  {
    "text": "So, we just saw the files.",
    "start": "1973900",
    "end": "1979900"
  },
  {
    "text": "Okay, so we're going to do two approaches, we're going to query the data that\nwe have persisted in S3 using two different frameworks.",
    "start": "1979900",
    "end": "1986500"
  },
  {
    "text": "We're going to use Spark SQL to query\nthe data directly in S3, and then we're going to load the\ndata into Redshift and use Redshift to query it.",
    "start": "1986500",
    "end": "1992900"
  },
  {
    "text": "Now, a lot of people ask me in the\nBig Data Bootcamps and elsewhere, if I can use Spark SQL, or Hive,\nor Impala, or Presto",
    "start": "1992900",
    "end": "2000400"
  },
  {
    "text": "to query my data using SQL with my\nHadoop cluster what do I need Redshift for. Good question.",
    "start": "2000400",
    "end": "2008799"
  },
  {
    "text": "The answer is really Redshift is designed to be a well-oiled machine for one\npurpose only and that's for data warehousing,\nfor executing SQL statements",
    "start": "2008800",
    "end": "2015799"
  },
  {
    "text": "again billions and billions of rows. It's a highly tuned, highly optimized\nenterprise-grade data warehouse.",
    "start": "2015800",
    "end": "2021300"
  },
  {
    "text": "EMR is an enterprise-grade Hadoop framework, but it does a lot of things. And a lot of these frameworks like\nSpark SQL, they're pretty new,",
    "start": "2021300",
    "end": "2028200"
  },
  {
    "text": "they're pretty new. You know, they've\nonly been around - if anyone was here last year, people\nwere still talking about Shark. Shark is gone and now there's Spark\nSQL. It's that new.",
    "start": "2028200",
    "end": "2035700"
  },
  {
    "text": "And in terms of query latency, what\nI usually tell people is that if you're doing exploratory analytics,",
    "start": "2035700",
    "end": "2041000"
  },
  {
    "text": "if you don't know what your data\nlooks like, or you don't know what your big analytics\nqueries are going to look like, things like Spark SQL, or Presto, or Impala",
    "start": "2041000",
    "end": "2048700"
  },
  {
    "text": "running on Hadoop are great because they can query data on S3 directly and you can kind of explore your data.",
    "start": "2048700",
    "end": "2055500"
  },
  {
    "text": "And with Spark SQL, it's gotten pretty fast. Hive used to take, like, 20 minutes\nto run one SQL query on a relatively small amount of data.",
    "start": "2055500",
    "end": "2061700"
  },
  {
    "text": "Some Hive queries of customers that\nI've talk to take multiple days to run. Things have gotten a lot faster with Spark and other frameworks like Tez and elsewhere.",
    "start": "2061700",
    "end": "2068600"
  },
  {
    "text": "So, now you can run Spark SQL, like we'll see, and have responses back in a few seconds. And typically Redshift will give\nyou faster performance",
    "start": "2068600",
    "end": "2075000"
  },
  {
    "text": "on large data sets because it's structured,\nit's optimized, it's just generally a better choice for a lot",
    "start": "2075000",
    "end": "2081399"
  },
  {
    "text": "of these enterprise data warehouses. That said, your mileage may vary,\nso give it a shot.",
    "start": "2081400",
    "end": "2087100"
  },
  {
    "text": "I like to use both. I like to use EMR for exploratory\nanalytics for data processing and I like to use Redshift for sort\nof once the data is structured",
    "start": "2087100",
    "end": "2094399"
  },
  {
    "text": "and for doing the white lab coat\nstyle hardcore data analytics. So, we're going to use both today. So, a reminder, Spark SQL is just a module,",
    "start": "2094400",
    "end": "2101300"
  },
  {
    "text": "part of sort of the Spark family, and it allows you to write SQL statements that get translated into Spark jobs.",
    "start": "2101300",
    "end": "2107000"
  },
  {
    "text": "So, you don't need to learn Scala\nlike I just showed you, you don't need to learn Python, you can do this select * from Matt\nlike you always have",
    "start": "2107000",
    "end": "2113500"
  },
  {
    "text": "and it gets auto translated into\na Spark job and then it gets distributed and\nrun on the cluster. So, I'll show you what that looks like.",
    "start": "2113500",
    "end": "2121700"
  },
  {
    "text": "Again, on the EMR cluster - and this\nis worked, so we'll cancel it.",
    "start": "2121700",
    "end": "2130700"
  },
  {
    "text": "So, just like the Spark shell where you can run Python or Scala in a shell -\nwhich by the way,",
    "start": "2130700",
    "end": "2137500"
  },
  {
    "text": "you wouldn't do - I mean, this is a demo, you would actually prewrite the Spark\njob and then submit it.",
    "start": "2137500",
    "end": "2143000"
  },
  {
    "text": "There's a Spark submit or APIs you can use, but for demo purposes or for testing,\nthe shell is great. But like the Spark shell, there's\na Spark SQL shell,",
    "start": "2143000",
    "end": "2150500"
  },
  {
    "text": "and instead of writing Scala we're\ngoing to write SQL. Right? You guessed it.",
    "start": "2150500",
    "end": "2159200"
  },
  {
    "text": "So, first of all, we have to tell\nSpark, where is this data. Now, I said something actually mind blowing,",
    "start": "2159200",
    "end": "2165500"
  },
  {
    "text": "but I kind of said it pretty quickly, is that you can use Spark SQL and\nother frameworks like Hive to access data and process data directly\non S3.",
    "start": "2165500",
    "end": "2175900"
  },
  {
    "text": "This is super significant. You don't need to copy the data into\nthe cluster, the data resides on S3, it uses something\ncalled EMRFS.",
    "start": "2175900",
    "end": "2182500"
  },
  {
    "text": "So, again, you can have arbitrary\ndata sitting on S3 - and think of Hive or Spark SQL as pointers.",
    "start": "2182500",
    "end": "2188800"
  },
  {
    "text": "So, when I create a table in Spark SQL or create a table in Hive, I'm not\ncreating a table",
    "start": "2188800",
    "end": "2194300"
  },
  {
    "text": "like in a database or in a data warehouse\nthat I load data into, I'm creating a reference of what\nthe data looks like somewhere else.",
    "start": "2194300",
    "end": "2202500"
  },
  {
    "text": "So, by creating a table, I'm saying\nthat these Apache logs that we just persisted into S3 have\nthese values in them",
    "start": "2202500",
    "end": "2209000"
  },
  {
    "text": "that correspond to these columns. So, when you go to query this is\nwhat you should expect. When you go to reach out to S3 to\ngo grab that data,",
    "start": "2209000",
    "end": "2216100"
  },
  {
    "text": "this is what it's going to look like. So, think about, like, a pointer\nor a representation of data that is stored elsewhere.",
    "start": "2216100",
    "end": "2221700"
  },
  {
    "text": "And there's no need to copy the data in, it's called schema on read. It means that you effectively define",
    "start": "2221700",
    "end": "2226900"
  },
  {
    "text": "what the data's going to look at,\nat query time. And this is incredibly powerful because in businesses we have all\nsorts of different type of data,",
    "start": "2226900",
    "end": "2233500"
  },
  {
    "text": "all sorts of different types of log formats, and imagine the power if you had a database that can morph on demand.",
    "start": "2233500",
    "end": "2238900"
  },
  {
    "text": "You know, oh, the data changed, we\nadded a new column. How many times has that happened and they need you to copy the data\ninto another table",
    "start": "2238900",
    "end": "2244400"
  },
  {
    "text": "in another database? You don't have to do that with this, you can just create a new table definition, the data stays in the same place,",
    "start": "2244400",
    "end": "2249600"
  },
  {
    "text": "and if the data has changed shape\nyou just change the schema and then issue your queries and it works.",
    "start": "2249600",
    "end": "2256000"
  },
  {
    "text": "So, in practice, what does that look like? Well, let's create a table. I'll make this a little bigger.",
    "start": "2256000",
    "end": "2264500"
  },
  {
    "text": "So, I've created an external table, and that means that the data is located,\nyou guessed it, externally.",
    "start": "2264500",
    "end": "2270200"
  },
  {
    "text": "And if you look at the last line\nthere, it says, location, that's where my data is.",
    "start": "2270200",
    "end": "2278599"
  },
  {
    "text": "So, the columns there host, user, request, and that regular expression, that\nis just representing",
    "start": "2278600",
    "end": "2285400"
  },
  {
    "text": "what the data looks like on S3, what\nthose files are, what those logs are that we persisted\nto S3 look like.",
    "start": "2285400",
    "end": "2295700"
  },
  {
    "text": "This is what we just did. So, again, an external table with\nthe location pointing to your S3 bucket.",
    "start": "2295700",
    "end": "2305900"
  },
  {
    "text": "And then, let's query. And this SQL, for those of you who\nhave ever used a database, is going to be, well, really familiar.",
    "start": "2305900",
    "end": "2311800"
  },
  {
    "text": "You can run regular SQL, except what\nthis is doing when you run the SQL, it's going\nout to S3 and querying the data directly.",
    "start": "2311800",
    "end": "2318800"
  },
  {
    "text": "So, this isn't some database where\nwe've copied data, very powerful framework. Let's do that.",
    "start": "2318800",
    "end": "2338800"
  },
  {
    "text": "There we go. So, what did I fetch there? Well, that looks like an Apache log, right,",
    "start": "2338800",
    "end": "2344700"
  },
  {
    "text": "except it's sitting on a table. So, I've actually reached into S3\nand pulled out one line, I just did a count of one or a limit of one,",
    "start": "2344700",
    "end": "2350400"
  },
  {
    "text": "from access logs. But this looks - and this could have\nbeen a transformed or manipulated version of the Apache log",
    "start": "2350400",
    "end": "2356300"
  },
  {
    "text": "that I transformed on the way in\nwith Spark Streaming, as it happens with my application, I didn't do much transformation,\nso it looks pretty much the same.",
    "start": "2356300",
    "end": "2362100"
  },
  {
    "text": "But the point is, is it works. I'm using Spark SQL as an engine\nto read data on S3.",
    "start": "2362100",
    "end": "2368099"
  },
  {
    "text": "And Spark SQL's fast. If I had billions and billions and\nbillions or records on S3, it would still only take few seconds if I have",
    "start": "2368100",
    "end": "2373500"
  },
  {
    "text": "an appropriately sized cluster to execute SQL statements against them. So, I'm doing analytics, we're doing analytics",
    "start": "2373500",
    "end": "2380500"
  },
  {
    "text": "against data in S3 from streaming data that we have processed and pulled\nfrom a Kinesis stream.",
    "start": "2380500",
    "end": "2388900"
  },
  {
    "text": "Okay, so that's Spark SQL in about\ntwo minutes, but what - you know, I do some analytics,",
    "start": "2388900",
    "end": "2395300"
  },
  {
    "text": "I run some queries, I figure out\nwhat my data looks like, and then I decide, okay, now I need\nto move it into Redshift",
    "start": "2395300",
    "end": "2400500"
  },
  {
    "text": "so I can hook Tableau or whatever onto it and start to do the hardcore stuff. So, you maybe do some additional\ndata processing -",
    "start": "2400500",
    "end": "2406300"
  },
  {
    "text": "Redshift is a structured database, so unlike Spark SQL or Hive, it needs\nto have structure.",
    "start": "2406300",
    "end": "2411799"
  },
  {
    "text": "In other words, the data needs to\ncorrespond to the columns as they are defined in the table\nin your database.",
    "start": "2411800",
    "end": "2417200"
  },
  {
    "text": "So, once we've sort of cleaned up\nour data and structured it and chucked it into files in S3,",
    "start": "2417200",
    "end": "2423100"
  },
  {
    "text": "then we can load those files into Redshift, we can create a table in Redshift\nand then load the files in. So, let's do that.",
    "start": "2423100",
    "end": "2430700"
  },
  {
    "start": "2430000",
    "end": "2533000"
  },
  {
    "text": "Now, Redshift, as opposed to EMR\nwith Spark SQL, has the storage - is coupled with the nodes.",
    "start": "2430700",
    "end": "2437600"
  },
  {
    "text": "So, like you just saw with EMR, the\nstorage is over here on S3 and the compute processing was on EMR.",
    "start": "2437600",
    "end": "2442700"
  },
  {
    "text": "This is very powerful because historically we have sized our Hadoop cluster\nfor storage.",
    "start": "2442700",
    "end": "2447900"
  },
  {
    "text": "Okay, I meet a ton of people who\nthey are very worried about, you know, I have 200 terabytes of big data,",
    "start": "2447900",
    "end": "2454100"
  },
  {
    "text": "and so that means I have to buy X\nnumber of servers to have a Hadoop cluster that has\nan HDFS file system",
    "start": "2454100",
    "end": "2459900"
  },
  {
    "text": "that is 200 terabytes or more big. But when you think about it, you're not spinning up a Hadoop cluster\nto store stuff,",
    "start": "2459900",
    "end": "2467000"
  },
  {
    "text": "that's sort of a secondary concern, you're spinning up a Hadoop cluster\nto do data processing. That's what it's for, right? So, why are you sizing your cluster\nfor storage?",
    "start": "2467000",
    "end": "2473799"
  },
  {
    "text": "So, by decoupling storage from compute, it gives you the freedom to tune\nand size your cluster",
    "start": "2473800",
    "end": "2479400"
  },
  {
    "text": "for your compute needs, not for your\nstorage needs. S3 has effectively unlimited storage, so I can put as much data as I want\nin S3",
    "start": "2479400",
    "end": "2485800"
  },
  {
    "text": "and all I have to worry about is how fast I want my queries to run by sizing my cluster accordingly  because the storage is decoupled\nfrom the compute.",
    "start": "2485800",
    "end": "2494400"
  },
  {
    "text": "Now again, in a lot of cases, having this storage coupled with\nthe compute, like in Redshift, can result in some better performance -",
    "start": "2494400",
    "end": "2500300"
  },
  {
    "text": "and I was talking about this earlier - for certain types of analytics workloads you'll see better performance for\nhardcore analytics queries",
    "start": "2500300",
    "end": "2506200"
  },
  {
    "text": "when using Redshift as opposed to,\nsay, Spark SQL with the EMRFS EMR. So, for SQL based analytics queries,",
    "start": "2506200",
    "end": "2512700"
  },
  {
    "text": "the storage comes back to the compute. So, we need to move that data into\nthe cluster because it can't live on S3,",
    "start": "2512700",
    "end": "2518200"
  },
  {
    "text": "you can't use Redshift to query data\non S3, we have to move the data into Redshift. So, let's do that.",
    "start": "2518200",
    "end": "2528500"
  },
  {
    "text": "Are we - sorry, we already did this.",
    "start": "2528500",
    "end": "2535300"
  },
  {
    "start": "2533000",
    "end": "2588000"
  },
  {
    "text": "So, first of all, let's connect to\nthe Redshift cluster, and I'm going to be nerdy and use\nthe Postgres command line,",
    "start": "2535300",
    "end": "2540700"
  },
  {
    "text": "but you could use SQL Workbench/J. If you're a Window's user, there's\na wonderful tool by our partner of ours called Aginity\nWorkbench for Redshift.",
    "start": "2540700",
    "end": "2549000"
  },
  {
    "text": "If you're a Mac user there's a nice\ntool called Postico. Any SQL client that can either use\nthe Amazon Redshift driver",
    "start": "2549000",
    "end": "2557200"
  },
  {
    "text": "and speak JDBC or ODBC, or use the\n- in some cases - certain versions of the Postgres driver will work with Redshift.",
    "start": "2557200",
    "end": "2563600"
  },
  {
    "text": "And this is an important characteristic\nabout Redshift, is that your existing tools, for\nthe most part, just work out of the box with Redshift.",
    "start": "2563600",
    "end": "2568800"
  },
  {
    "text": "In this case, again to be nerdy,\nI'm going to use the P SQL, just the Postgres command line client, to connect to my Redshift cluster.",
    "start": "2568800",
    "end": "2588599"
  },
  {
    "start": "2588000",
    "end": "2728000"
  },
  {
    "text": "Oops, I better get out of Hadoop cluster.",
    "start": "2588600",
    "end": "2598300"
  },
  {
    "text": "There, I'm connected to my Redshift cluster. Now, we need to create a table to\nhouse the data.",
    "start": "2598300",
    "end": "2605500"
  },
  {
    "text": "This is going to look very familiar\nif you've ever done any SQL.",
    "start": "2605500",
    "end": "2610700"
  },
  {
    "text": "Oh, I'll drop it.",
    "start": "2610700",
    "end": "2620200"
  },
  {
    "text": "There, I just created a table.",
    "start": "2620200",
    "end": "2627300"
  },
  {
    "text": "I'll just remind myself what the\nbucket is called. I'll do the bucket - I have a bucket\nfilled with a lot more stuff.",
    "start": "2627300",
    "end": "2634099"
  },
  {
    "text": "There we go.",
    "start": "2634100",
    "end": "2640200"
  },
  {
    "text": "So, now I'm going to copy from a bucket that was running with Kinesis for\na lot longer so it has some more logs so we can\ndo some more interesting queries.",
    "start": "2640200",
    "end": "2646700"
  },
  {
    "text": "But this could be the bucket that\nyou just saw with the access logs. And I'm going to load those process\naccess logs into Redshift.",
    "start": "2646700",
    "end": "2654700"
  },
  {
    "text": "So, I'm going to do a copy command. And what the copy command does on Redshift is it loads data in parallel from\nS3.",
    "start": "2654700",
    "end": "2661700"
  },
  {
    "text": "And the key word here is parallel. I was joking that I wanted to get\na t-shirt made that said ABP, always be parallelizing.",
    "start": "2661700",
    "end": "2668200"
  },
  {
    "text": "It's so important for everything at Amazon and especially things like Redshift. The nice thing about Redshift and\nwhat makes it so fast",
    "start": "2668200",
    "end": "2673700"
  },
  {
    "text": "for both data load and data unload is that it leverages all of the compute\nnodes of the cluster,",
    "start": "2673700",
    "end": "2679299"
  },
  {
    "text": "and inside each compute node you\nhave this notion called slices, and each slice can load independently.",
    "start": "2679300",
    "end": "2684900"
  },
  {
    "text": "So, you can have multiple slices\nin multiple servers all loading data in parallel from\nRedshift or from S3.",
    "start": "2684900",
    "end": "2690099"
  },
  {
    "text": "And what this means in simpler terms is that it's very fast to load large\namounts of data into and out of Redshift.",
    "start": "2690100",
    "end": "2696000"
  },
  {
    "text": "So, the fact that the compute is coupled with the storage in Redshift is no big deal, because if your data is on S3 you can get it into Redshift very fast.",
    "start": "2696000",
    "end": "2702200"
  },
  {
    "text": "If you need to get it into Redshift\nfaster, just add more nodes, because the more nodes you have, the faster your ingest is going to be,",
    "start": "2702200",
    "end": "2707900"
  },
  {
    "text": "because the more slices and nodes you have to load in parallel from S3.",
    "start": "2707900",
    "end": "2722300"
  },
  {
    "text": "So, we wait until we copy from S3. Let's get back to the slides for a second.",
    "start": "2722300",
    "end": "2730400"
  },
  {
    "start": "2728000",
    "end": "2820000"
  },
  {
    "text": "So, again, we created a table to\nhold the data in Redshift, and we use the copy commands to load\nthose files in parallels from S3.",
    "start": "2730400",
    "end": "2743200"
  },
  {
    "text": "And then we're going to run some queries. Great, so we just loaded 81,000 rows,",
    "start": "2743200",
    "end": "2748400"
  },
  {
    "text": "not much for a data warehouse, but\nstill significant for, you know, a single, teeny, tiny data warehouse.",
    "start": "2748400",
    "end": "2754200"
  },
  {
    "text": "Now, if I had a very large warehouse, that could have been 81 million or\n8 billion. And again, if you had sized your\ncluster accordingly,",
    "start": "2754200",
    "end": "2760200"
  },
  {
    "text": "you could have loaded it just as fast if you had enough compute notice\nto do that kind of parallel loading.",
    "start": "2760200",
    "end": "2765300"
  },
  {
    "text": "So, with those 81,000 rows, let's\nfinish today by doing some queries.",
    "start": "2765300",
    "end": "2777700"
  },
  {
    "text": "There we go. What is this? This is value. I have now taken streaming data,\nI have taken logs",
    "start": "2777700",
    "end": "2784800"
  },
  {
    "text": "and now I'm executing SQL statements to get them to solve real business problems. And I'll give you one, a very simple one. I have a really big website and I'm\nserving up a lot of 404 errors.",
    "start": "2784800",
    "end": "2793500"
  },
  {
    "text": "And you know what, that costs me money because my 404 error has this really\nawesome picture on it and that's data transfer charges",
    "start": "2793500",
    "end": "2799900"
  },
  {
    "text": "and over the month when Taylor Swift\ntweets about my new mobile application, suddenly I have 4 billion users,\nand that 404 page",
    "start": "2799900",
    "end": "2805700"
  },
  {
    "text": "is actually costing me a lot of money. So, why am I getting a 404 page? What is missing on my web server\nor my website?",
    "start": "2805700",
    "end": "2813400"
  },
  {
    "text": "So, let's use an analytics query\nto figure that out.",
    "start": "2813400",
    "end": "2822000"
  },
  {
    "start": "2820000",
    "end": "2873000"
  },
  {
    "text": "How many 404s do I have in this data\nset? Let's just check. A lot, 2,000. For 81,000 logs -",
    "start": "2822000",
    "end": "2827200"
  },
  {
    "text": "so, 2,000 out of 81,000 are 404s,\nthat's not good, I got to find a new web designer.",
    "start": "2827200",
    "end": "2833800"
  },
  {
    "text": "Let's find the top ones. Who's the offender? Oh, I missed a favicon,",
    "start": "2833800",
    "end": "2838900"
  },
  {
    "text": "I always do that. Now, this actually wouldn't cause\nmuch data transfer problems, but there, we solved the problem.",
    "start": "2838900",
    "end": "2844900"
  },
  {
    "text": "In real time here, using steaming\nanalytics, we solved the problem. Why am I getting so many 404 errors\nclogging up my logs?",
    "start": "2844900",
    "end": "2851099"
  },
  {
    "text": "There we go. Now, again, you show this to your\nCFO they'll be like, what is that black screen, why are\nyou showing me this.",
    "start": "2851100",
    "end": "2858900"
  },
  {
    "text": "So, let's do something pretty. Now, you guys have QuickSight now - or will, I think they had said it's going\nto come out in preview in a month -",
    "start": "2858900",
    "end": "2864700"
  },
  {
    "text": "that's amazing. So, you'll be able\nto do some really pretty graphs. Today, let's use Matt's not quite\nas pretty graph to show this.",
    "start": "2864700",
    "end": "2874300"
  },
  {
    "text": "Now - let's make this bigger so you\ncould actually see it.",
    "start": "2874300",
    "end": "2879600"
  },
  {
    "text": "What's interesting about this is that this is a static website hosted\non S3.",
    "start": "2879600",
    "end": "2887500"
  },
  {
    "text": "And a lot of you may have dashboards\nthat you've written, and these dashboards typically are\nserved up by application servers,",
    "start": "2887500",
    "end": "2893300"
  },
  {
    "text": "and you have web front ends, and\napplication servers, and database in the back - what I'll show you now is admittedly\na very simple example,",
    "start": "2893300",
    "end": "2900300"
  },
  {
    "text": "but this website would probably cost\nme, I think, $.04 to $.06 a month to host.",
    "start": "2900300",
    "end": "2905400"
  },
  {
    "text": "Why? Because it's hosted on S3, it's\njust all client side, it's just HTML and Java script.",
    "start": "2905400",
    "end": "2911400"
  },
  {
    "text": "And what this little website does\nis when I hit query there's no application server, it goes off and fires a Lambda function,",
    "start": "2911400",
    "end": "2918200"
  },
  {
    "text": "Lambda function goes and talks to\nRedshift, gets the data it needs, returns the data to the web browser\nof the user,",
    "start": "2918200",
    "end": "2924000"
  },
  {
    "text": "and then uses Plattable, which is\na library that sits on top of D3, to render the data.",
    "start": "2924000",
    "end": "2930700"
  },
  {
    "text": "So, by using very powerful Java script\nframeworks like D3, and the fact that our web browsers\nand our mobile devices",
    "start": "2930700",
    "end": "2936200"
  },
  {
    "text": "are now way more powerful than my\ncomputer from three years ago, it's something that you can leverage. So, ask yourself, do I need that\napplication server.",
    "start": "2936200",
    "end": "2943799"
  },
  {
    "text": "This is a preview for some sessions\nlater in reInvent about serverless architectures or\nZeroTier Architectures, An Even Driven Computing.",
    "start": "2943800",
    "end": "2950300"
  },
  {
    "text": "Maybe I can use a Lambda function\ninstead of an application server, maybe I can use a client side data\nvisualization library",
    "start": "2950300",
    "end": "2956200"
  },
  {
    "text": "to render my pretty graphs. I don't want to get too off topic,\nI just want to plant a seed because it's a theme you'll see at\nreInvent this time.",
    "start": "2956200",
    "end": "2963600"
  },
  {
    "text": "And what I'm going to do, I'm going\nto hit query here, it's query that same Redshift table\nand answer that same problem,",
    "start": "2963600",
    "end": "2968700"
  },
  {
    "text": "you know, what are my top 404 errors\nin my web logs,",
    "start": "2968700",
    "end": "2975200"
  },
  {
    "start": "2974000",
    "end": "3072000"
  },
  {
    "text": "and bang, no application server,\nhit Redshift,",
    "start": "2975200",
    "end": "2980500"
  },
  {
    "text": "and very clearly and easily, an analyst\nwho knows nothing about Hadoop and who doesn't want to see a black screen",
    "start": "2980500",
    "end": "2986300"
  },
  {
    "text": "can very easily understand the data. So, we have succeeded. We've taken streaming data coming\nin for our web logs,",
    "start": "2986300",
    "end": "2992400"
  },
  {
    "text": "we've processed it, we've done some\nexploratory analytics to figure it out, we've loaded into Redshift for the actual analytics queries,",
    "start": "2992400",
    "end": "2998800"
  },
  {
    "text": "and we bolted on a visualization\nlayer so it can make sense to mortals. And we've done it.",
    "start": "2998800",
    "end": "3003900"
  },
  {
    "text": "So, thank you, with together, for\nbuilding this application with me. These slides will be available by\nthe end of the week,",
    "start": "3003900",
    "end": "3009400"
  },
  {
    "text": "and I strongly encourage you to try\nit yourself. And why? Because it's not going to\ncost you much.",
    "start": "3009400",
    "end": "3023000"
  },
  {
    "text": "This whole end to end architecture,\neverything we did today, would cost you about $2.44.",
    "start": "3023000",
    "end": "3030700"
  },
  {
    "text": "That's probably the least expensive\nbig data processing pipeline I've ever seen in my life.",
    "start": "3030700",
    "end": "3036000"
  },
  {
    "text": "I would have got a big promotion\nat my last job if I had gotten a Hadoop cluster\nfor that much. Remember, everything we did today\nis pay as you go.",
    "start": "3036000",
    "end": "3041400"
  },
  {
    "text": "So, that data processing I did to\nsolve that business problem, when that problem is solved you shut it off and you don't pay for it anymore.",
    "start": "3041400",
    "end": "3048200"
  },
  {
    "text": "So, I encourage you to take this, if you can afford the $2.44, try\nit yourself, extend it, write a Sparks Streaming\napplication.",
    "start": "3048200",
    "end": "3055500"
  },
  {
    "text": "Even if you've never touched Scala before and never touched Python before,\nit doesn't matter, you can figure this out, it's super easy. Take that first step just like I\ndid three and half years ago",
    "start": "3055500",
    "end": "3062900"
  },
  {
    "text": "and you'll be a big data pro in no time. So, thanks for your time today, hope\nyou enjoyed the ride, have a good show.",
    "start": "3062900",
    "end": "3072600"
  }
]