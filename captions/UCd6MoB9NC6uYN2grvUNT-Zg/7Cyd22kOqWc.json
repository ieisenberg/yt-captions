[
  {
    "start": "0",
    "end": "129000"
  },
  {
    "text": "good day and welcome uh my name is Brennan Greg and this talk is on Performance Tuning ec2",
    "start": "2520",
    "end": "8240"
  },
  {
    "text": "instances you may know me from some of my prior work which includes the systems performance book uh I've done a lot with",
    "start": "8240",
    "end": "14480"
  },
  {
    "text": "performance over the years and now I'm working at Netflix it's a great company to work for as a I'm working as a senior",
    "start": "14480",
    "end": "21039"
  },
  {
    "text": "performance architect my manager coin Watson is is in the room and we're also hiring so you can come and speak to us",
    "start": "21039",
    "end": "27000"
  },
  {
    "text": "at the Netflix Booth afterwards if you'd like to talk about that Netflix is we have a massive Amazon ec2",
    "start": "27000",
    "end": "34239"
  },
  {
    "text": "Cloud uh it's quite well known especially here at reinvent tens of thousands of server instances and we",
    "start": "34239",
    "end": "40239"
  },
  {
    "text": "Auto scale by around 3,000 each day and it's running Centos in auntu performance",
    "start": "40239",
    "end": "45440"
  },
  {
    "text": "is critical so this is for customer satisfaction we have over 50 million subscribers now and also for ourselves",
    "start": "45440",
    "end": "53079"
  },
  {
    "text": "we care about customers being happy and we also care about price performance so tuning Our ec2 instances to make sure",
    "start": "53079",
    "end": "59559"
  },
  {
    "text": "they Del the the best price performance we can get the Netflix performance engineering team which I'm on we do many",
    "start": "59559",
    "end": "66680"
  },
  {
    "text": "different activities so we evaluate technology instance types Amazon ec2 options we do recommendations and best",
    "start": "66680",
    "end": "73799"
  },
  {
    "text": "practices and so that's figuring out the best kernel tuning for different workloads and also helping out with application tuning we develop",
    "start": "73799",
    "end": "80320"
  },
  {
    "text": "Performance Tools uh that's just the nature of the business with performance analysis is from time to time you you'll need to develop custom tools we have a",
    "start": "80320",
    "end": "88000"
  },
  {
    "text": "good reputation of open sourcing them as well uh the Netflix OSS group project support",
    "start": "88000",
    "end": "94320"
  },
  {
    "text": "so if teams want to try out a new language runtime or database we can help out there as well analyze the",
    "start": "94320",
    "end": "99840"
  },
  {
    "text": "performance and we also help with instant response so I'm going to talk about",
    "start": "99840",
    "end": "105159"
  },
  {
    "text": "instant selection and the different procedures we use for that Amazon ec2 features for improving performance",
    "start": "105159",
    "end": "111240"
  },
  {
    "text": "kernel tuning uh which I love it's about making their instances themselves go faster and also observability which is a",
    "start": "111240",
    "end": "117719"
  },
  {
    "text": "really important topic for performance tuning that's how we discover the certain areas that need to be tuned and",
    "start": "117719",
    "end": "125520"
  },
  {
    "text": "often comes up with the biggest wins of all performance tuning on Amazon ec2 is",
    "start": "125520",
    "end": "130840"
  },
  {
    "start": "129000",
    "end": "129000"
  },
  {
    "text": "really interesting environment I in the past I've done Enterprise performance tuning and that's where you would spend",
    "start": "130840",
    "end": "136959"
  },
  {
    "text": "a while analyzing the workload understanding the the attributes and then picking a a Mainframe server to run",
    "start": "136959",
    "end": "142879"
  },
  {
    "text": "it on and you may be signing a support contract for multiple years on that server and it was pretty stressful if",
    "start": "142879",
    "end": "148080"
  },
  {
    "text": "you got it wrong and now you've paid PA a few million dollars for a very large iron and you have to have to performance",
    "start": "148080",
    "end": "154080"
  },
  {
    "text": "tune to make it work otherwise people lose their jobs uh with Amazon ec2 we we",
    "start": "154080",
    "end": "159120"
  },
  {
    "text": "can just pick an instance type and if that doesn't work it's very easy to pick another instance type it's really amazing the instance type itself becomes",
    "start": "159120",
    "end": "166640"
  },
  {
    "text": "almost just like another tunable parameter just like setting a TCP window size or buffer size we just set the",
    "start": "166640",
    "end": "172360"
  },
  {
    "text": "instance type to meet the different workload so it's really exciting for me as a performance engineer as WIS that we",
    "start": "172360",
    "end": "178360"
  },
  {
    "text": "discover can have immediate benefits even if that means changing the platform what I'm about to go through is",
    "start": "178360",
    "end": "185920"
  },
  {
    "text": "what's really in our medicine cabinet so consider these best per 2005 Performance",
    "start": "185920",
    "end": "192040"
  },
  {
    "text": "Tuning is really a process it's not a product and so it's something that you you need to revisit the tunables you",
    "start": "192040",
    "end": "197560"
  },
  {
    "text": "need to revisit the areas that need tuning uh I think this is a very helpful presentation because it will expose you",
    "start": "197560",
    "end": "203400"
  },
  {
    "text": "to all the different areas that may need attention but this isn't necessarily things you should copy and paste to make",
    "start": "203400",
    "end": "209319"
  },
  {
    "text": "things go faster because they will go out of date uh it is more about things that do need attention that you'll need",
    "start": "209319",
    "end": "214439"
  },
  {
    "text": "to evaluate and try out yourself so the first section is instance selection how do we pick what",
    "start": "214439",
    "end": "221239"
  },
  {
    "text": "instance type to run on and just to give you the big picture of the Netflix Cloud",
    "start": "221239",
    "end": "226280"
  },
  {
    "start": "225000",
    "end": "225000"
  },
  {
    "text": "we have a a massive ec2 uh massive ec2 deployment that runs all of our applic",
    "start": "226280",
    "end": "232959"
  },
  {
    "text": "applications we have many service teams we have a lot of Cassandra elastic search and EV cache we're using e B for",
    "start": "232959",
    "end": "240560"
  },
  {
    "text": "managing load uh and also S3 for an object store the current generation of instance",
    "start": "240560",
    "end": "247799"
  },
  {
    "text": "families these are the ones that we care about the most at Netflix I2 storage optimized with ssds i2s are really fast",
    "start": "247799",
    "end": "255239"
  },
  {
    "text": "and so uh sometimes you'll find people are deploying on I 2.8x larges to give them the best performance if if they're",
    "start": "255239",
    "end": "261359"
  },
  {
    "text": "trying things out it's uh the Temptation is there but of course you need to uh balance it with all the other options",
    "start": "261359",
    "end": "267840"
  },
  {
    "text": "because we have a lot of flexibility on the cloud we we can pick r3s we can pick c3s M3s and",
    "start": "267840",
    "end": "273880"
  },
  {
    "text": "others the instance sizes range from medium to uh 8 extra large and currently",
    "start": "273880",
    "end": "280360"
  },
  {
    "start": "275000",
    "end": "275000"
  },
  {
    "text": "we have about 30 different instance types in use so that's including both the class and the",
    "start": "280360",
    "end": "285560"
  },
  {
    "text": "size and like I just mentioned traditionally you tune the workload to match the server that you just bought",
    "start": "285560",
    "end": "291120"
  },
  {
    "text": "and you're going to have that server for three years so you you wanted to get the best Roi of it in the cloud it's",
    "start": "291120",
    "end": "296440"
  },
  {
    "text": "different you tune you can tune the workload to match instance type but you",
    "start": "296440",
    "end": "302000"
  },
  {
    "text": "you also have more flexibility so I may come up with different application tunings that make sense for different",
    "start": "302000",
    "end": "307280"
  },
  {
    "text": "instance types and then I can pick the best price performance out of all of them and so this is why being on Amazon",
    "start": "307280",
    "end": "313199"
  },
  {
    "text": "is is quite exciting and and beneficial for us because there are so many different instance types we can really",
    "start": "313199",
    "end": "318560"
  },
  {
    "text": "fine-tune price performance so instead of just given a platform go and optimize",
    "start": "318560",
    "end": "323880"
  },
  {
    "text": "the product it's optimizing both together so the three different ways I'm",
    "start": "323880",
    "end": "330360"
  },
  {
    "start": "329000",
    "end": "329000"
  },
  {
    "text": "going to summarize an instance type selection uh three different procedures",
    "start": "330360",
    "end": "335639"
  },
  {
    "text": "this is how things can be done generally you may find a particular service team at Netflix has their own custom way of",
    "start": "335639",
    "end": "341639"
  },
  {
    "text": "doing it and that's fine the first one is describing the general procedures a flowchart so on the left I have",
    "start": "341639",
    "end": "348280"
  },
  {
    "text": "exceptions and on the right I have the trade-off and so we'll start with do you need large disc capacity and if so we'll",
    "start": "348280",
    "end": "355560"
  },
  {
    "text": "probably steer you towards the I2 class if not is the workload disio bound is it",
    "start": "355560",
    "end": "360919"
  },
  {
    "text": "constrained by disio performance if so if the workload also can't cash then",
    "start": "360919",
    "end": "367880"
  },
  {
    "text": "you do want to be on the best storage devices you can and so we'll get get that workload onto a SSD instance if the",
    "start": "367880",
    "end": "373800"
  },
  {
    "text": "workload can cash if that's a possibility then we'll start to focus on memory on the right is the trade-off and",
    "start": "373800",
    "end": "381000"
  },
  {
    "text": "so I've got two different accesses axes we've got memory and CPU and so if you",
    "start": "381000",
    "end": "386319"
  },
  {
    "text": "care a lot about memory uh you may be looking at the instance types in R3 uh",
    "start": "386319",
    "end": "391919"
  },
  {
    "text": "as as you need more CPU you move over into M3s or c3s uh and the distance I've got on",
    "start": "391919",
    "end": "398199"
  },
  {
    "text": "those axes is the size of the instance type so this is just one way to visualize the the sort of thought",
    "start": "398199",
    "end": "403479"
  },
  {
    "text": "process we go for a a new service we're not sure what it's going to run best on uh are there are there some exceptions",
    "start": "403479",
    "end": "410479"
  },
  {
    "text": "where it absolutely needs a capability and if not let's get into the tradeoffs uh which is the flexibility that Amazon",
    "start": "410479",
    "end": "417800"
  },
  {
    "text": "provides our environment is actually uh really easy to deploy new instance types",
    "start": "417800",
    "end": "424520"
  },
  {
    "text": "and to try things out we make really good use of e elastic load balancing and",
    "start": "424520",
    "end": "430639"
  },
  {
    "text": "this is much better than micro benchmarking micro benchmarking is is seriously error prone I've done a lot of this over the years and it's a it's a",
    "start": "430639",
    "end": "438440"
  },
  {
    "text": "useful tool but you have to take it with a grain of salt and it's really at Netflix it's great that we can test",
    "start": "438440",
    "end": "443960"
  },
  {
    "text": "things out with production workloads now for elbs we can spin up a single Canary and try a work Lo on that so let's say",
    "start": "443960",
    "end": "451160"
  },
  {
    "text": "we've we've gone through a process and we've picked an instance type that looks suitable we can try that out with a",
    "start": "451160",
    "end": "456479"
  },
  {
    "text": "small amount of traffic uh or we can give it to a a an entire order scaling",
    "start": "456479",
    "end": "462599"
  },
  {
    "text": "group and so this is also How We Do code deploys and so maybe we're changing instance types for an application and",
    "start": "462599",
    "end": "469039"
  },
  {
    "text": "you can create a new order scaling group on that instance type and then the elb can move the load over gradually so that",
    "start": "469039",
    "end": "475479"
  },
  {
    "text": "you can test it out and then give it more and more load and see how it performs and if things go back just move it back to the old ASG and things are",
    "start": "475479",
    "end": "482360"
  },
  {
    "text": "fine so it's a great way to test out uh the instance types A different procedure for instance",
    "start": "482360",
    "end": "490080"
  },
  {
    "text": "selection is by a by resource approach and that's where you will determine what the bounding resource is for that",
    "start": "490080",
    "end": "496199"
  },
  {
    "text": "workload there may be CPUs you may be constrained by disio capacity network capacity there's three different ways",
    "start": "496199",
    "end": "502440"
  },
  {
    "text": "you may do this estimation you may have developed the product and you already know that it's going to be CPU bound I",
    "start": "502440",
    "end": "507560"
  },
  {
    "text": "already know that this is this is I'm doing processing a video when I'm re doing encoding and all sorts of",
    "start": "507560",
    "end": "513360"
  },
  {
    "text": "different types I already have a good idea that this is going to be CPU heavy or I may do resource observability with",
    "start": "513360",
    "end": "519560"
  },
  {
    "text": "an existing real workload or the third one which is where it's it's an",
    "start": "519560",
    "end": "524680"
  },
  {
    "text": "experimental approach where I want to Benchmark factors that I think are relevant and then do resource",
    "start": "524680",
    "end": "530000"
  },
  {
    "text": "observability of those factors where I'll pick some micro benchmarks and then pick the resource type B based on that",
    "start": "530000",
    "end": "537000"
  },
  {
    "text": "if it's dis iio consider caching and a memory optimized type so this is I'm just introducing this this is just a",
    "start": "537000",
    "end": "543000"
  },
  {
    "text": "different way to think about it and this is where you're focusing the the philosophy is you focus on the bounding resource because if you optimize that it",
    "start": "543000",
    "end": "550720"
  },
  {
    "text": "elevates the performance in all other areas and you get closer to having a balanced system where there's no single",
    "start": "550720",
    "end": "557440"
  },
  {
    "start": "557000",
    "end": "557000"
  },
  {
    "text": "point that's a bottleneck and Netflix to Aid this we've developed a tool the nomogram visualization Tool uh I've got",
    "start": "557440",
    "end": "564640"
  },
  {
    "text": "a very simple picture here but this is where you can select the different classes you're interested in M3s c3s and",
    "start": "564640",
    "end": "570839"
  },
  {
    "text": "whatever different attributes are interested in and the memory range",
    "start": "570839",
    "end": "577440"
  },
  {
    "text": "and let's say I I knew that my workload ran well on eight",
    "start": "578000",
    "end": "583160"
  },
  {
    "text": "vcpus I can then follow the colored lines to the different instance types that satisfy that constraint so by the",
    "start": "583160",
    "end": "590760"
  },
  {
    "text": "time you're using this you've used the resource approach and you've determined what your bounding resource is so you have an idea of what I want I want",
    "start": "590760",
    "end": "596680"
  },
  {
    "text": "memory or I want CPUs or I want networks or I want discs now I can put my finger on the thing that I actually want and",
    "start": "596680",
    "end": "603360"
  },
  {
    "text": "then I can follow the colored lines to the different offerings that Amazon makes available so just to just to",
    "start": "603360",
    "end": "609399"
  },
  {
    "text": "introduce this is something we use to help aid instance selection this is a very simple picture if I include all the",
    "start": "609399",
    "end": "615079"
  },
  {
    "text": "other instance types and attributes it becomes really really complicated and you can't read it on the projection screen but uh simple picture gives you",
    "start": "615079",
    "end": "623040"
  },
  {
    "text": "the uh the idea of what we're doing and then the third way that we",
    "start": "623040",
    "end": "629160"
  },
  {
    "text": "have have done in the past is a Brute Force choice this is really interesting and this is where you come up with a way",
    "start": "629160",
    "end": "635399"
  },
  {
    "text": "to measure price performance for a given workload and then you load test that workload on all of the instance",
    "start": "635399",
    "end": "641760"
  },
  {
    "text": "offerings that might be 30 or might be 60 different instance types and you try it on the mall and then you can sort it",
    "start": "641760",
    "end": "647519"
  },
  {
    "text": "based on the best price performance and then pick that so uh this this is the Brute Force Brute Force performance",
    "start": "647519",
    "end": "653160"
  },
  {
    "text": "engineer approach it's uh calculates a price performance of all types it's one",
    "start": "653160",
    "end": "658519"
  },
  {
    "text": "thing you do have to be careful about it uh is about coming having such a",
    "start": "658519",
    "end": "664360"
  },
  {
    "start": "660000",
    "end": "660000"
  },
  {
    "text": "synthetic approach you may drive load to an an impossible uh situation in",
    "start": "664360",
    "end": "671200"
  },
  {
    "text": "some instance types where let's say I was I was calculating my price Performance Based on average latency and",
    "start": "671200",
    "end": "677240"
  },
  {
    "text": "throughput well based on throughput and the price the the average latency itself may",
    "start": "677240",
    "end": "682440"
  },
  {
    "text": "be driven high if I am really pushing that throughput up so if I'm not using that as a constraint and I'm doing some",
    "start": "682440",
    "end": "688160"
  },
  {
    "text": "Brute Force tests I may pick oh the R3 is fantastic if you're doing 10,000 iops",
    "start": "688160",
    "end": "693920"
  },
  {
    "text": "but of course when you look at the lcy distribution those iops are all way too slow to be used in production and so I",
    "start": "693920",
    "end": "699800"
  },
  {
    "text": "just wanted to mention this in that uh if you if you want to try out the Brute Force approach and and it can be fun uh",
    "start": "699800",
    "end": "705040"
  },
  {
    "text": "you need to apply other restrictions there as well so it's like encoding an SLA it's make sure that the your 99th",
    "start": "705040",
    "end": "711440"
  },
  {
    "text": "percentile is within a range and so on and of course it's not just about um pushing the uh dialing it up so so that",
    "start": "711440",
    "end": "719519"
  },
  {
    "text": "your price performance is just off unacceptable it's about leaving some Headroom as well so however your",
    "start": "719519",
    "end": "725320"
  },
  {
    "text": "business requirements are for latency needs to be considered for that so",
    "start": "725320",
    "end": "730399"
  },
  {
    "text": "that's Netflix uh instance type selection that's three General procedures now for reelection and that's",
    "start": "730399",
    "end": "737240"
  },
  {
    "text": "where given now that we're running on a particular instance type under what what situations and tools do we use to change",
    "start": "737240",
    "end": "744120"
  },
  {
    "text": "that instance type over time and variance is one of them that's where",
    "start": "744120",
    "end": "750440"
  },
  {
    "start": "748000",
    "end": "748000"
  },
  {
    "text": "over time uh your your resource constraints may change so you may find I've deployed a database it's CPU",
    "start": "750440",
    "end": "757680"
  },
  {
    "text": "resource constraint I've optimized for CPUs but as that database populates and fills up it actually becomes disk",
    "start": "757680",
    "end": "764279"
  },
  {
    "text": "resource constraint and so that wasn't a factor when I did my earlier instance selection and so you do need to revisit",
    "start": "764279",
    "end": "770000"
  },
  {
    "text": "these from time to time things can change as things warm up you may push a code change that completely changes it",
    "start": "770000",
    "end": "776519"
  },
  {
    "text": "uh or you may have a a resources constraint only occasionally so most of",
    "start": "776519",
    "end": "781639"
  },
  {
    "text": "the time your CPU constrained but sometimes your network constrained and that's causing so many outliers and",
    "start": "781639",
    "end": "786800"
  },
  {
    "text": "unhappy customers that you need to pay attention to it you need to optimize based on that so we continually monitor",
    "start": "786800",
    "end": "792839"
  },
  {
    "text": "performance and we analyze variance and outliers and of course the the whole Amazon model makes it very easy to",
    "start": "792839",
    "end": "798440"
  },
  {
    "text": "switch instance types if we need to instance usage is a is another Vector",
    "start": "798440",
    "end": "805480"
  },
  {
    "text": "into reselection and that's where we have a tool so that we know exactly the breakdown of all the instance Types on",
    "start": "805480",
    "end": "811720"
  },
  {
    "text": "the Amazon Cloud and uh so I can see we're mostly on M3s or whatever it is or",
    "start": "811720",
    "end": "818040"
  },
  {
    "text": "c3s or whatever it is this helps because o over time uh new Amazon provides new",
    "start": "818040",
    "end": "824279"
  },
  {
    "text": "instance offerings and it's important to re-evaluate the performance of applications to see if they provide",
    "start": "824279",
    "end": "830240"
  },
  {
    "text": "Better Price performance points and we can use the instance usage charts to identify teams that are still on older",
    "start": "830240",
    "end": "836000"
  },
  {
    "text": "instance types then we can work with them and get them on something newer that has better performance and then the last tool we",
    "start": "836000",
    "end": "843079"
  },
  {
    "text": "have and I have obscured the numbers here the last tool is about instance cost and this is about tuning the price",
    "start": "843079",
    "end": "848680"
  },
  {
    "text": "part of price performance and so we're able to track over time how much we're paying for all of the different Amazon",
    "start": "848680",
    "end": "854680"
  },
  {
    "text": "services and also break it down by the service teams and applications and so if there's some subtle creeping uh growth",
    "start": "854680",
    "end": "861600"
  },
  {
    "text": "cost with a a service or an application team we can identify it we can tune that",
    "start": "861600",
    "end": "866639"
  },
  {
    "text": "and it helps us get the best price performance and so that's the different",
    "start": "866639",
    "end": "873440"
  },
  {
    "text": "ways we end up selecting instances uh the the three different General",
    "start": "873440",
    "end": "878680"
  },
  {
    "text": "procedures and also different ways we might go in and different vectors that would trigger us to reselect an instance",
    "start": "878680",
    "end": "884880"
  },
  {
    "text": "type the next section I want to do briefly is Amazon ec2 features for performance and these are outside the",
    "start": "884880",
    "end": "890199"
  },
  {
    "start": "889000",
    "end": "889000"
  },
  {
    "text": "instance uh but they're used by the instance Zen modes and srov and of",
    "start": "890199",
    "end": "895560"
  },
  {
    "text": "course over time there may be more so so Performance Tuning gets out of",
    "start": "895560",
    "end": "901040"
  },
  {
    "text": "with Zen modes the best performance is the latest pvm PRI hybrid mode that Zen",
    "start": "901120",
    "end": "907279"
  },
  {
    "text": "provides so on Amazon ec2 right now if you pick hvm it's really picking",
    "start": "907279",
    "end": "912519"
  },
  {
    "text": "something called pvm if your Linux kernel supports it and if you pick PV for paravert then you're getting PV this",
    "start": "912519",
    "end": "920399"
  },
  {
    "text": "is actually really confusing uh because over time Zen has evolved and the usage",
    "start": "920399",
    "end": "925519"
  },
  {
    "text": "of the terminology has evolved and if you if you try to do an internet search and say so tell me what's faster hvm or",
    "start": "925519",
    "end": "932120"
  },
  {
    "text": "PV because those are the Amazon options you'll find a lot of pages that say hvm is really slow you should use",
    "start": "932120",
    "end": "938160"
  },
  {
    "text": "PV uh but what they they're talking about the original hvm not the new hvm",
    "start": "938160",
    "end": "943639"
  },
  {
    "text": "the new hvm is actually PV hbm it's actually fairly confusing and so the Zen",
    "start": "943639",
    "end": "949120"
  },
  {
    "text": "Community came up with a chart to explain it I've refined it but mostly based on their chart where I've shown",
    "start": "949120",
    "end": "956120"
  },
  {
    "text": "the reason we have so many different options is that Zen has been evolving and so you have the old so so you've got",
    "start": "956120",
    "end": "964040"
  },
  {
    "text": "fully virtualized which is the hardware virtual machine and that's where you emulate everything and things can be very slow at the other end of the",
    "start": "964040",
    "end": "971040"
  },
  {
    "text": "spectrum we've got fully par paravirtualized where you're using a PV drivers for a lot of operations that's",
    "start": "971040",
    "end": "977199"
  },
  {
    "text": "great except for when you get into privileged instructions and Page tables so CPU operations like accessing memory",
    "start": "977199",
    "end": "982959"
  },
  {
    "text": "can be much slower than you hope and the best mode you want is the best balance",
    "start": "982959",
    "end": "988199"
  },
  {
    "text": "of HV M and PV at the moment that's going to be pvm and that's what we're",
    "start": "988199",
    "end": "993440"
  },
  {
    "text": "deployed mostly on PVH VM in the future there there's a new mode that Zen is has",
    "start": "993440",
    "end": "999480"
  },
  {
    "text": "been developing called PVH and PVH will also use paravert for the boot process",
    "start": "999480",
    "end": "1005519"
  },
  {
    "text": "to emulate motherboard and Legacy Boot and so in the future you'll get PVH which would be slightly better I'm not",
    "start": "1005519",
    "end": "1010839"
  },
  {
    "text": "super excited about PVH because I care more about the runtime performance on once you're up and running and once",
    "start": "1010839",
    "end": "1015920"
  },
  {
    "text": "you're up and running on pvm you're getting the best of both worlds already so PVH will come sometime in the future",
    "start": "1015920",
    "end": "1022079"
  },
  {
    "text": "and things will get a little bit faster at boot which would be nice but it's not at the moment we're already getting a",
    "start": "1022079",
    "end": "1028199"
  },
  {
    "text": "very good option and of course Zen is evolving there there's lots of performance work",
    "start": "1028199",
    "end": "1033480"
  },
  {
    "text": "is done on Zen all the time this diagram may get even more complicated and even more confusing in the future so uh but",
    "start": "1033480",
    "end": "1040798"
  },
  {
    "text": "it's it's all an effort to give us the best price performance which we enjoy srov is another really important",
    "start": "1040799",
    "end": "1048360"
  },
  {
    "text": "performance tuning option to to pay attention to and I've done a lot of uh",
    "start": "1048360",
    "end": "1054080"
  },
  {
    "text": "micro benchmarking of different hypervisors so so Zen and KVM and containers and one of the biggest",
    "start": "1054080",
    "end": "1060280"
  },
  {
    "text": "problems with Hardware virtualized hypervisors like Zen is that when you're doing Network iio you're paying a tax",
    "start": "1060280",
    "end": "1066640"
  },
  {
    "text": "for every Network packet and it depends on maybe you've got a PV driver that can coess and group those packets together",
    "start": "1066640",
    "end": "1073400"
  },
  {
    "text": "but there's still attack if you're doing 10 GB networking you can be doing millions of packets a second so even",
    "start": "1073400",
    "end": "1078520"
  },
  {
    "text": "though the tax is very small it does add up now there is a solution to this for",
    "start": "1078520",
    "end": "1084559"
  },
  {
    "text": "hardware virtualized systems and that's sov hardware manufacturers of network",
    "start": "1084559",
    "end": "1090120"
  },
  {
    "text": "cards have come up with a way that they can come up with secure virtualized",
    "start": "1090120",
    "end": "1095159"
  },
  {
    "text": "Hardware instances of a network card and Zen can then hand them down to guests so",
    "start": "1095159",
    "end": "1100679"
  },
  {
    "text": "your guest can now have bare metal Network performance by accessing this this virtual neck that the card has",
    "start": "1100679",
    "end": "1106880"
  },
  {
    "text": "provided so we don't have to do software UL in the Zen hypervisor and that's really great you can only use this on",
    "start": "1106880",
    "end": "1113159"
  },
  {
    "text": "VPC and it's only available for some instance types but from my experience this really",
    "start": "1113159",
    "end": "1120039"
  },
  {
    "text": "solves the the biggest performance problem that Zen has had which is once",
    "start": "1120039",
    "end": "1126159"
  },
  {
    "text": "you once you start doing a lot of packet rates the hypervisor cost can start to add up and so sov is great it's it's it",
    "start": "1126159",
    "end": "1133039"
  },
  {
    "text": "solves that particular problem so now with with Zen in most of the time when you're running workloads you are get",
    "start": "1133039",
    "end": "1138960"
  },
  {
    "text": "bare metal performance because I can get bare metal networking most of the time it's bare metal CPU performance so uh",
    "start": "1138960",
    "end": "1145280"
  },
  {
    "text": "and this is after many years of Zen engineering effort to get us to this point so s you should get better Network",
    "start": "1145280",
    "end": "1151280"
  },
  {
    "text": "throughput reduced uh round trip times and reduced",
    "start": "1151280",
    "end": "1156120"
  },
  {
    "text": "Jitter so those are two key features that Amazon ec2 and Zen provide now for",
    "start": "1156799",
    "end": "1163679"
  },
  {
    "text": "kernel tuning itself once you're on the instance what can we do to make it go faster with kernel tuning these are",
    "start": "1163679",
    "end": "1169360"
  },
  {
    "text": "typically 5 to 25% wins for average performance for someone the size of the",
    "start": "1169360",
    "end": "1175760"
  },
  {
    "text": "Netflix Cloud that that adds up so so these are worthwhile to explore they are much bigger wins when you're reducing",
    "start": "1175760",
    "end": "1182120"
  },
  {
    "text": "latency outliers so for latency outlier you may have say uh sometimes my",
    "start": "1182120",
    "end": "1187799"
  },
  {
    "text": "application request latency is 500 milliseconds because I'm blocked on disio when I do kernel tuning I may make",
    "start": "1187799",
    "end": "1194440"
  },
  {
    "text": "that 500 milliseconds down to 50 milliseconds so it's actually a 10x win even though it's kernel tuning even",
    "start": "1194440",
    "end": "1200039"
  },
  {
    "text": "though it's system tuning because we're uh attacking the latency outliers so just as a way to to mentally understand",
    "start": "1200039",
    "end": "1207000"
  },
  {
    "text": "the the benefits and the goals from kernel tuning when it's the runtime performance on average the winds can",
    "start": "1207000",
    "end": "1213640"
  },
  {
    "text": "appear modest but when it's latency outliers they can actually be quite great how we deploy the tuning we do do",
    "start": "1213640",
    "end": "1220240"
  },
  {
    "text": "generic performance tuning into the base Ami and experiment experimental tuning is a package add-on so you can just add",
    "start": "1220240",
    "end": "1226080"
  },
  {
    "text": "a package to an instance to try it out the workload specific tuning that we configure in the application Amis and of",
    "start": "1226080",
    "end": "1233240"
  },
  {
    "text": "course we remember to tune the workload with the tunables and so given a variety of tunables it's not a it's not it's not",
    "start": "1233240",
    "end": "1240679"
  },
  {
    "text": "a a the process of given my application and its config file what's the best that suits it I may modify the application",
    "start": "1240679",
    "end": "1247679"
  },
  {
    "text": "and config file because they're are tunables and because I can change instance types we use both sentos and auntu and",
    "start": "1247679",
    "end": "1256320"
  },
  {
    "start": "1254000",
    "end": "1254000"
  },
  {
    "text": "bear in mind operating system distributions can override tuning defaults and different kernel versions",
    "start": "1256320",
    "end": "1261760"
  },
  {
    "text": "can provide different tunings as well and so we have to have to pay close attention to all of the different",
    "start": "1261760",
    "end": "1267280"
  },
  {
    "text": "tunables that the kernel provides these are the sort of things you tune in cctl and/ CIS and by the time we automate",
    "start": "1267280",
    "end": "1274360"
  },
  {
    "text": "them like I said previously they're baked into our base Ami and so people just deploy and they're getting the best tuning for their workload so I'll go",
    "start": "1274360",
    "end": "1281679"
  },
  {
    "text": "through these and the point of these is not to give you things to copy and paste uh because your mileage may vary and and",
    "start": "1281679",
    "end": "1288039"
  },
  {
    "text": "tuning is a process it's something that needs to be Revisited but it's very I think it's",
    "start": "1288039",
    "end": "1293840"
  },
  {
    "text": "very useful to go through the different things that we can currently do just for exposure so it may may prompt you to",
    "start": "1293840",
    "end": "1300039"
  },
  {
    "start": "1294000",
    "end": "1294000"
  },
  {
    "text": "look them up later when you're when you're trying to get more performance out of different applications so the",
    "start": "1300039",
    "end": "1305600"
  },
  {
    "start": "1305000",
    "end": "1305000"
  },
  {
    "text": "first thing is the CPU Schuler so nowadays systems are scaling we have",
    "start": "1305600",
    "end": "1312360"
  },
  {
    "text": "much larger memory systems we have many more CPUs multi-socket systems and the",
    "start": "1312360",
    "end": "1319520"
  },
  {
    "text": "performance of workloads often where we have so much memory we can cash workloads and they're operating mostly",
    "start": "1319520",
    "end": "1325000"
  },
  {
    "text": "out of cash and it BEC CPU bound when you're on CPU you do actually spend a lot of time not making forward",
    "start": "1325000",
    "end": "1332919"
  },
  {
    "text": "progress which which we call retiring instructions you spend a lot of time stalling waiting for memory IO and",
    "start": "1332919",
    "end": "1339600"
  },
  {
    "text": "resource iio to complete stall cycles and the stall Cycles can be rectified by",
    "start": "1339600",
    "end": "1347120"
  },
  {
    "text": "doing layers of hardware cach and this is why we have the CPU Hardware level one cache and level two cach and level",
    "start": "1347120",
    "end": "1352240"
  },
  {
    "text": "three cache this is why the mmu for memory translation it also has a cache and so they're really important but",
    "start": "1352240",
    "end": "1358640"
  },
  {
    "text": "these get broken if your threads are migrating around quite a lot so if my application threads move around all the",
    "start": "1358640",
    "end": "1364919"
  },
  {
    "text": "different CPUs then the the hardware caches aren't so warm I I will now have",
    "start": "1364919",
    "end": "1370120"
  },
  {
    "text": "to access main memory much more often I get more stall Cycles your workload just just eats more CPU and you can't really",
    "start": "1370120",
    "end": "1376400"
  },
  {
    "text": "tell why and so tuning this CP Schuler can really benefit especially as these",
    "start": "1376400",
    "end": "1382600"
  },
  {
    "text": "systems scale up in the in in the numbers of sockets and the amount of memory so we reduce",
    "start": "1382600",
    "end": "1389000"
  },
  {
    "text": "migrations uh the the the really simple really dumb way to do it although it's a heavy Hammer is to use task set and",
    "start": "1389000",
    "end": "1395200"
  },
  {
    "text": "that's where people will glue threads to CPUs so that they don't wander around and they have the best cache Affinity",
    "start": "1395200",
    "end": "1402000"
  },
  {
    "text": "but there are other things you can tune as well so uh numactl cgroups so I can group applications to",
    "start": "1402000",
    "end": "1408120"
  },
  {
    "text": "CP use and there's also specific tuning like um the kernel I can set I can",
    "start": "1408120",
    "end": "1413840"
  },
  {
    "text": "inform the kernel of what I think it costs to do a migration of a thread from",
    "start": "1413840",
    "end": "1419320"
  },
  {
    "text": "one CPU to another which of course is dependent on Hardware uh we found that some Java apps",
    "start": "1419320",
    "end": "1425640"
  },
  {
    "text": "benefit from shed batch to reduce contact switching and so we can set that using shed tool and so reducing contact",
    "start": "1425640",
    "end": "1431559"
  },
  {
    "text": "switching what it means is threads are more likely to stay put which means they have better cach warmth uh they access",
    "start": "1431559",
    "end": "1438559"
  },
  {
    "text": "ing main memory much less Le less stall cycles and so you uh your C usage will",
    "start": "1438559",
    "end": "1444760"
  },
  {
    "text": "go down you can get more price performance out of the",
    "start": "1444760",
    "end": "1449360"
  },
  {
    "start": "1449000",
    "end": "1449000"
  },
  {
    "text": "instance virtual memory uh if you've been around L for a long time you're actually quite familiar with these sort",
    "start": "1450279",
    "end": "1455440"
  },
  {
    "text": "of tunables swappiness overcommit and O Behavior we actually just set swappiness to zero because we don't configure swap",
    "start": "1455440",
    "end": "1462520"
  },
  {
    "text": "devices and so swappiness is where Linux has the ability to uh favor ditching the",
    "start": "1462520",
    "end": "1468399"
  },
  {
    "text": "F system catch versus paging application memory and that's really cool for things like embedded devices and mobile phones",
    "start": "1468399",
    "end": "1475440"
  },
  {
    "text": "but for Server operating systems is often disabled and uh we disable we don't have swap anyway huge pages is",
    "start": "1475440",
    "end": "1482120"
  },
  {
    "start": "1480000",
    "end": "1480000"
  },
  {
    "text": "another error of tuning and this is this is another way to improve uh what the",
    "start": "1482120",
    "end": "1488880"
  },
  {
    "text": "CPUs are doing and to reduce stall cycles and also to reduce the just the overhead of of accessing main memory so",
    "start": "1488880",
    "end": "1496120"
  },
  {
    "text": "the kernel accesses main memory in groups and and and as as does the the hardware the the CPU itself it accesses",
    "start": "1496120",
    "end": "1503399"
  },
  {
    "text": "main memory in groups called Pages they're usually 4 kilobytes and if I'm deploying a",
    "start": "1503399",
    "end": "1508600"
  },
  {
    "text": "database that's that's 100 gabt of main memory you can imagine how many 4 kilobyte Pages the kernel has to go and",
    "start": "1508600",
    "end": "1515320"
  },
  {
    "text": "initialize and create and then have pointers to and there's a lot of overhead and cost involved and so",
    "start": "1515320",
    "end": "1521039"
  },
  {
    "text": "switching to using huge pages so I can refer to the same amount of main memory but using doing it in 2 or 4 megabyte",
    "start": "1521039",
    "end": "1527799"
  },
  {
    "text": "size ch chunks reduces the overhead or maintenance of doing that the CPU plays",
    "start": "1527799",
    "end": "1534080"
  },
  {
    "text": "along as well processor manufacturers have done this for many years so they're able to cach large",
    "start": "1534080",
    "end": "1539600"
  },
  {
    "text": "Pages uh in Linux you used to do this explicitly but now there's transparent huge pages and so you don't even have to",
    "start": "1539600",
    "end": "1545960"
  },
  {
    "text": "think about it your applications will automatically be using huge pages and the two benefits you get from it is the",
    "start": "1545960",
    "end": "1551880"
  },
  {
    "text": "the less CPU cost to maintain all of those pages when you're doing uh when you're doing page faults and allocation",
    "start": "1551880",
    "end": "1558080"
  },
  {
    "text": "and also the mmu the memory management unit that's a hardware on the CPU it has a cache its job is to do virtual to",
    "start": "1558080",
    "end": "1565039"
  },
  {
    "text": "physical address translation and it has a cache of translations the reason of this is we're",
    "start": "1565039",
    "end": "1570760"
  },
  {
    "text": "running in virtual memory systems when your application runs and it's using 100 gigabyt of memory that's virtual memory",
    "start": "1570760",
    "end": "1576880"
  },
  {
    "text": "and Hardware has to translate that into where it's mapped on physical memory and that translation happens all the time",
    "start": "1576880",
    "end": "1582080"
  },
  {
    "text": "because you're doing loads on stores all the time and so this is why we cach it all to improve performance now the cach",
    "start": "1582080",
    "end": "1587679"
  },
  {
    "text": "on the CPU you uh it's the translation lookaside buffer it has a fixed number of slots and so if it may only be 64 it",
    "start": "1587679",
    "end": "1595120"
  },
  {
    "text": "may only be 16 if those 16 slots can only refer to 4 kilobyte Pages you don't",
    "start": "1595120",
    "end": "1600159"
  },
  {
    "text": "have very much of a span that you can cash if instead they can they can they can cash four megabyte Pages you have a",
    "start": "1600159",
    "end": "1606480"
  },
  {
    "text": "much bigger span and so you get a better cash hit rate out of that Hardware cache",
    "start": "1606480",
    "end": "1611919"
  },
  {
    "text": "so that's all interesting it all sounds like it should work in theory really well and we shouldn't even have to think about this stuff but as it turns out it",
    "start": "1611919",
    "end": "1618480"
  },
  {
    "text": "doesn't work that well in in in practice for some processor types we actually found transparent huge Pages made",
    "start": "1618480",
    "end": "1624880"
  },
  {
    "text": "performance goore 25% worse for some workloads and the reason I believe is it",
    "start": "1624880",
    "end": "1630880"
  },
  {
    "text": "it was a particular processor type on IV bridge and it happened to have a much",
    "start": "1630880",
    "end": "1636399"
  },
  {
    "text": "the the cache for Pages there's not one tlb there's actually multiple tlbs for",
    "start": "1636399",
    "end": "1641840"
  },
  {
    "text": "the different page size and so while there were many 4K slots there was only a small number of 4 me slots and so it",
    "start": "1641840",
    "end": "1649559"
  },
  {
    "text": "just didn't cash so well in the in the mmu once you went to large pages so we actually found that disabling huge Pages",
    "start": "1649559",
    "end": "1656399"
  },
  {
    "text": "made performance go 25% better which is completely unexpected based on how all this works uh and that's more of an",
    "start": "1656399",
    "end": "1662159"
  },
  {
    "text": "artifact of a particular process or type I believe the next generation of processors fix this problem by giving",
    "start": "1662159",
    "end": "1667919"
  },
  {
    "text": "you a larger tlb so interesting but it's something that we had to pay attention to uh and",
    "start": "1667919",
    "end": "1673679"
  },
  {
    "text": "it was something that's that's also difficult to debug or to observe because CPU is just taking longer to",
    "start": "1673679",
    "end": "1680240"
  },
  {
    "text": "run uh most of the time T transparent huge pages should help you so I'm just giving you some exposure to some of the",
    "start": "1680240",
    "end": "1686960"
  },
  {
    "text": "some of the times where it doesn't and you do need to pay attention FAL system tuning so we tune the page cache",
    "start": "1686960",
    "end": "1693880"
  },
  {
    "start": "1688000",
    "end": "1688000"
  },
  {
    "text": "Behavior what Linux will do by default what operating systems like to do by default is when you're writing to a file",
    "start": "1693880",
    "end": "1700159"
  },
  {
    "text": "system it will buffer that in main memory and then return that IO to your application and say I've done i' I've",
    "start": "1700159",
    "end": "1706399"
  },
  {
    "text": "done that IO really fast only took like one one microsc whereas the kernel sometime later will flush it out to disk",
    "start": "1706399",
    "end": "1713480"
  },
  {
    "text": "what happens is in Linux unless you say synchronous I am in Linux there are two",
    "start": "1713480",
    "end": "1718720"
  },
  {
    "text": "thresholds there is a threshold where Linux will begin to gently flush dirty",
    "start": "1718720",
    "end": "1724080"
  },
  {
    "text": "data to disk and then when you when you stu writing to main memory with dirty",
    "start": "1724080",
    "end": "1729360"
  },
  {
    "text": "pages so much you cross another threshold where Linux will aggressively page out dirty pages to disk and will",
    "start": "1729360",
    "end": "1735600"
  },
  {
    "text": "slow down the application because it needs to because your saturating main memory with dirty pages so what we like",
    "start": "1735600",
    "end": "1740880"
  },
  {
    "text": "to do is we like to tune the threshold so that you hit the gentle background",
    "start": "1740880",
    "end": "1746080"
  },
  {
    "text": "flushing earlier and then you hit the more aggressive application hering flushing much later and so we we'll go",
    "start": "1746080",
    "end": "1753080"
  },
  {
    "text": "and tune those things storage IO storage iio is uh we do tune this as",
    "start": "1753080",
    "end": "1761480"
  },
  {
    "start": "1760000",
    "end": "1760000"
  },
  {
    "text": "well so this is once once I've gotten past the file system and I'm going down to desks there are things like the",
    "start": "1761480",
    "end": "1766799"
  },
  {
    "text": "readhead size number of inflight requests the I scheduler volume stripe width we actually found Cassandra can be",
    "start": "1766799",
    "end": "1773840"
  },
  {
    "text": "sensitive to the readhead size just changing from one operating system uh distribution version to another the",
    "start": "1773840",
    "end": "1780039"
  },
  {
    "text": "default reader head size went from 128 Koby to 2 megabytes and performance was",
    "start": "1780039",
    "end": "1785399"
  },
  {
    "text": "much worse I think it was like 40% worse and it's just because of a a tunable of",
    "start": "1785399",
    "end": "1791200"
  },
  {
    "text": "course you can you can improve performance by fine-tuning what the readhead size is to match your workload so the reader head is how much extra",
    "start": "1791200",
    "end": "1798039"
  },
  {
    "text": "space once you're doing storage IO will you pull into main memory to pre-warm the cache uh and so we will tune various",
    "start": "1798039",
    "end": "1804399"
  },
  {
    "text": "things you can also do things like if it's ssds the konel should try and pick the the no up scheduler so that it try",
    "start": "1804399",
    "end": "1811000"
  },
  {
    "text": "doesn't try and waste CPU Cycles doing elevator seeking which is what Linux would does to optimize uh rotational",
    "start": "1811000",
    "end": "1817919"
  },
  {
    "text": "hard diss so for ssds there should really be a different scheduler that's more efficient if Linux hasn't picked",
    "start": "1817919",
    "end": "1823480"
  },
  {
    "text": "that for you you may need to manually pick it uh and also how many request in Flight things like that can be tuned",
    "start": "1823480",
    "end": "1831360"
  },
  {
    "start": "1831000",
    "end": "1831000"
  },
  {
    "text": "networking it's always a big area of tuning in in the lenux kernel and so things like the TCP buffer sizes the",
    "start": "1831360",
    "end": "1836720"
  },
  {
    "text": "backlog the device backlog and so long and I've got a large long list of the the tunables we're",
    "start": "1836720",
    "end": "1842360"
  },
  {
    "text": "deploying uh apart from obvious things like setting increasing the buffer size to improve uh TCP throughput we also do",
    "start": "1842360",
    "end": "1849000"
  },
  {
    "text": "things like set TCP tww reuse for the time weight reuse algorithm so when you're doing lots of connections to a",
    "start": "1849000",
    "end": "1854559"
  },
  {
    "text": "server it can re Can it can intelligently reuse a Time weight",
    "start": "1854559",
    "end": "1859760"
  },
  {
    "text": "session uh because otherwise you'll run out of slots and then uh packets can get",
    "start": "1859760",
    "end": "1865440"
  },
  {
    "text": "dropped and there's also a really interesting one uh ipv4 TCP aboard on overflow which I I leared from Facebook",
    "start": "1865440",
    "end": "1873039"
  },
  {
    "text": "U their C Engineers found this to be quite a valuable tuner ball normally when you open a sock and you're",
    "start": "1873039",
    "end": "1878559"
  },
  {
    "text": "listening to network connections you have a backlog in the kernel and once you fill up the backlog the colel will just drop the packet and let the client",
    "start": "1878559",
    "end": "1885679"
  },
  {
    "text": "retransmit uh that's that's actually not so fun for for creating latency outliers",
    "start": "1885679",
    "end": "1890799"
  },
  {
    "text": "because the retransmit interval is usually 1 second and so it's kind of a fingerprint if you see these one set",
    "start": "1890799",
    "end": "1895960"
  },
  {
    "text": "slightly over 1 second request latency outliers you start looking for retransmits especially when you're",
    "start": "1895960",
    "end": "1901559"
  },
  {
    "text": "connecting to a socket and so a TP board on overflow means I can return a reset",
    "start": "1901559",
    "end": "1906799"
  },
  {
    "text": "if my backlog has has become full so that I'm not just dropping a packet and forcing a client to retransmit I'm",
    "start": "1906799",
    "end": "1912760"
  },
  {
    "text": "actually letting the client know that there's a problem uh you would have to test that because it is really changing",
    "start": "1912760",
    "end": "1918840"
  },
  {
    "text": "the relationship between the server and the client so there may be some Services where that's a bad tuning but it's quite",
    "start": "1918840",
    "end": "1925639"
  },
  {
    "text": "interesting and then for the hypervisor or Zen tunables include PVH VM which is",
    "start": "1925639",
    "end": "1931600"
  },
  {
    "start": "1926000",
    "end": "1926000"
  },
  {
    "text": "baked into the uh a am it's baked into the Ami your choice of whether it's",
    "start": "1931600",
    "end": "1937120"
  },
  {
    "text": "going to be pvm or just PV uh colel CLX Source has been another one that's",
    "start": "1937120",
    "end": "1942200"
  },
  {
    "text": "really interesting uh we debugged one recently where changing the clock Source",
    "start": "1942200",
    "end": "1948679"
  },
  {
    "text": "reduced the average application latency by 43% and also reduced CPU usage by 30%",
    "start": "1948679",
    "end": "1955320"
  },
  {
    "text": "this is crazy it's the the kernel itself can make use of different clock sources clock is actually if you haven't done",
    "start": "1955320",
    "end": "1961720"
  },
  {
    "text": "clock before it's actually really complicated in an interesting part of the kernel about giving you an accurate",
    "start": "1961720",
    "end": "1968000"
  },
  {
    "text": "high resolution timestamp that's monotonically increasing across all of the cpuse in a multiprocessor",
    "start": "1968000",
    "end": "1973559"
  },
  {
    "text": "environment it's actually hard and there's different clock sources that the Kel provides that you can pick between",
    "start": "1973559",
    "end": "1979480"
  },
  {
    "text": "and uh we found the Zen clock Source on ontu trusty in particular was very slow",
    "start": "1979480",
    "end": "1984960"
  },
  {
    "text": "we have applications that to check time stamps very frequently and so when I I I",
    "start": "1984960",
    "end": "1990559"
  },
  {
    "text": "take that back when I say the Zen clock Source was very slow I mean relatively it's actually still pretty fast but when",
    "start": "1990559",
    "end": "1996480"
  },
  {
    "text": "you when you make the call so frequently the small overhead does add up and so we",
    "start": "1996480",
    "end": "2001840"
  },
  {
    "text": "switch to TS TSC is the time stamp clock do be aware of clock drift because there were problems many years ago but the",
    "start": "2001840",
    "end": "2007760"
  },
  {
    "text": "processors have been getting better with TSC and so we've been experimenting with going to TSC to improve that and the",
    "start": "2007760",
    "end": "2014039"
  },
  {
    "text": "performance improvements have been huge 30% reduction in CP usage and of course that it depends on your",
    "start": "2014039",
    "end": "2021279"
  },
  {
    "text": "application so that gives you an idea of all the different kernel tunings we do to improve performance the last section",
    "start": "2021279",
    "end": "2026760"
  },
  {
    "text": "I've got is observability and this gives you an idea of how we find these tunbles as",
    "start": "2026760",
    "end": "2032720"
  },
  {
    "text": "well so finding quantifying and confirming T confirming confirming tunables",
    "start": "2032720",
    "end": "2038360"
  },
  {
    "text": "the system wins 5 to 25% and also application wins which are often much larger and this observability is really",
    "start": "2038360",
    "end": "2045799"
  },
  {
    "text": "a core part of tuning just to introduce observability before I start showing you the tools how you approach it uh what",
    "start": "2045799",
    "end": "2053440"
  },
  {
    "text": "methodology do you use to uh do observability uh observability based",
    "start": "2053440",
    "end": "2059240"
  },
  {
    "text": "tuning and to explain it I've come up with two uh anti- methodologies just to",
    "start": "2059240",
    "end": "2064599"
  },
  {
    "text": "put a name on something that that people may do uh and really to emphasize how inefficient it is one of them is the",
    "start": "2064599",
    "end": "2070760"
  },
  {
    "text": "wacka anti method where you just tune things at random until the problem goes away has anyone done this wacka anti",
    "start": "2070760",
    "end": "2076760"
  },
  {
    "text": "method there we go we have like almost the whole room confesses it's it's convenient to have a name on name on it",
    "start": "2076760",
    "end": "2082878"
  },
  {
    "text": "I I came to work one day and it's like wow why did we deploy these tun tunings",
    "start": "2082879",
    "end": "2087919"
  },
  {
    "text": "and a fellow engineer said I I used the the wacka all anti method to find it actually at the time I was calling it",
    "start": "2087919",
    "end": "2093398"
  },
  {
    "text": "the drunk men anti method you get the picture same thing um but it's really useful to say yeah I used the drunk man",
    "start": "2093399",
    "end": "2099760"
  },
  {
    "text": "anti method that's how I found those tunables because this is a performance engineer I now know okay got it you completely guessed and you got lucky uh",
    "start": "2099760",
    "end": "2106520"
  },
  {
    "text": "I I may now go and revisit those tunings and see if there's a see what You' found because I mean it may be an an efficient",
    "start": "2106520",
    "end": "2113320"
  },
  {
    "text": "way to discover things that are worth investigating uh the street light anti method this is a well the first one is",
    "start": "2113320",
    "end": "2119839"
  },
  {
    "text": "an experimental method this is an observability method and that's where I'm trying to find uh an issue by",
    "start": "2119839",
    "end": "2125960"
  },
  {
    "text": "picking observability that are familiar found on the internet or at random then I run them and look for obvious",
    "start": "2125960",
    "end": "2132440"
  },
  {
    "text": "issues uh and and I often see this done when people are running top and they're",
    "start": "2132440",
    "end": "2137480"
  },
  {
    "text": "not using any other tools and you say why why are you still using top like there's many other tools you should be",
    "start": "2137480",
    "end": "2142640"
  },
  {
    "text": "using to diagnose this and it's because they're familiar with top that's that's what they want to use Top's great top",
    "start": "2142640",
    "end": "2147839"
  },
  {
    "text": "does solve a lot of issues but there are also many other tools you can use uh for it a key problem with a street light",
    "start": "2147839",
    "end": "2153960"
  },
  {
    "text": "anti method is if you're only using things that are familiar or that you find you may be overlooking areas that",
    "start": "2153960",
    "end": "2159520"
  },
  {
    "text": "you should be investigating because they're not familiar efficient tuning",
    "start": "2159520",
    "end": "2166720"
  },
  {
    "text": "so to contrast inefficient tuning with doing something efficiently based on",
    "start": "2166720",
    "end": "2172720"
  },
  {
    "text": "observability so being data driven about this this is where you want to observe the workload and resource usage and then",
    "start": "2172720",
    "end": "2178280"
  },
  {
    "text": "analyze the results so what software and Hardware components are in use okay got it for those software and Hardware",
    "start": "2178280",
    "end": "2186599"
  },
  {
    "text": "components that are involved in the hot path that are doing all my data what tunables exist for them uh and so now",
    "start": "2186599",
    "end": "2193960"
  },
  {
    "text": "you've narrowed down your investigation you're not doing wacka mol antim method you've narrowed it down to there's only",
    "start": "2193960",
    "end": "2199200"
  },
  {
    "text": "these five chines that are related to our current workload so these are the ones that are interesting now can study",
    "start": "2199200",
    "end": "2204280"
  },
  {
    "text": "them and quantify expected speed up um tune using experimental the",
    "start": "2204280",
    "end": "2209480"
  },
  {
    "text": "highest value targets first to give you a more concrete example of efficient tuning the use method is something I",
    "start": "2209480",
    "end": "2215400"
  },
  {
    "text": "I've documented a while ago and that's where you come with a functional diagram of the system that's where you have all your Hardware components and software",
    "start": "2215400",
    "end": "2221440"
  },
  {
    "text": "components and then for every component or resource you check three metrics only utilization saturation and errors this",
    "start": "2221440",
    "end": "2229000"
  },
  {
    "text": "is a great quick way to take the uh simple performance issues off the table to identify bottlenecks and errors early",
    "start": "2229000",
    "end": "2236200"
  },
  {
    "text": "on in an investigation this is not the only methodology you use there's many other methodologies you use as well but",
    "start": "2236200",
    "end": "2241880"
  },
  {
    "text": "this is just an effective one that uh it Narrows down how many things you're checking because we run going to check",
    "start": "2241880",
    "end": "2247760"
  },
  {
    "text": "three three important metrics per thing whether it's a software component or Hardware it also starts by posing the",
    "start": "2247760",
    "end": "2254480"
  },
  {
    "text": "questions so this isn't a methodology where it's where you run the tools or you load the monitoring guey based on",
    "start": "2254480",
    "end": "2260760"
  },
  {
    "text": "the metrics what can we interpret now you're coming up to the questions and then now you're trying to find metrics",
    "start": "2260760",
    "end": "2266520"
  },
  {
    "text": "that satisfy them and what can be really interesting is then then if you haven't done this sort of thing before you can",
    "start": "2266520",
    "end": "2272119"
  },
  {
    "text": "you can enter my life where you realize there's actually a bunch of things that are really hard to observe and a lot of",
    "start": "2272119",
    "end": "2277280"
  },
  {
    "text": "the common observability tools don't provide uh visibility into them like things like the the CPU interconnect or",
    "start": "2277280",
    "end": "2283240"
  },
  {
    "text": "going into the hardware buses or looking at how your PCI devices the PCI control is saturated uh things that are actually",
    "start": "2283240",
    "end": "2290000"
  },
  {
    "text": "really important because they're part of your data path especially if you're doing like high degrees of networking and yet aren't commonly examined things",
    "start": "2290000",
    "end": "2297599"
  },
  {
    "text": "like the use method will help you realize that because it's posing all the questions you want answered first then",
    "start": "2297599",
    "end": "2302720"
  },
  {
    "text": "you go and try and find the metrics as another example of more",
    "start": "2302720",
    "end": "2307880"
  },
  {
    "start": "2306000",
    "end": "2306000"
  },
  {
    "text": "inefficient tuning the blame someone else anti method does anyone know of anyone doing",
    "start": "2307880",
    "end": "2314599"
  },
  {
    "text": "this you don't have to confess that you did this but if have you seen this done in practice all right so no now I have",
    "start": "2314599",
    "end": "2320240"
  },
  {
    "text": "like half the room this way you find a system or environment component you aren't responsible for and you say oh",
    "start": "2320240",
    "end": "2326280"
  },
  {
    "text": "the problem must be with that go talk to that team and uh when Pro proven wrong just find something else so a great way",
    "start": "2326280",
    "end": "2332480"
  },
  {
    "text": "to uh to offload the work by the time they've exhausted all the other teams then and you definitely know it's a",
    "start": "2332480",
    "end": "2338640"
  },
  {
    "text": "problem with your system so it's it is kind of annoying if you feel that someone's doing this to",
    "start": "2338640",
    "end": "2344560"
  },
  {
    "text": "you where where they're blaming someone else ask them for some observability tool output for how they made that",
    "start": "2344560",
    "end": "2350800"
  },
  {
    "text": "conclusion one thing that often gets blamed is the instance especially if the blame someone else Ting method it's great if you can",
    "start": "2350800",
    "end": "2357280"
  },
  {
    "text": "pick a a a resource that's really hard to diagnose because they won't come back for months like go go check out like uh",
    "start": "2357280",
    "end": "2364880"
  },
  {
    "text": "the network latency across the continental US like somewhere in there that's the problem uh so blaming",
    "start": "2364880",
    "end": "2370839"
  },
  {
    "text": "instance is one example where um some people have the nature of blaming the thing that you can't see um so you have",
    "start": "2370839",
    "end": "2377359"
  },
  {
    "text": "a complex performance issue of no obvious cause maybe that's an instance issue you know after analysis sometimes",
    "start": "2377359",
    "end": "2383040"
  },
  {
    "text": "it is a problem with the in the ec2 instance often it is the application and something that comes to mind is coin's",
    "start": "2383040",
    "end": "2389280"
  },
  {
    "text": "8020 rule which is 80% of the Improvement is gained through application refactoring and tuning and",
    "start": "2389280",
    "end": "2395560"
  },
  {
    "text": "only 20% is everything else like OS tuning and infrastructure Improvement and ec2 instance types and and that's",
    "start": "2395560",
    "end": "2402720"
  },
  {
    "text": "that's that's something I agree with in reality is blaming the instance you've got to just to sanity check it it's like",
    "start": "2402720",
    "end": "2409000"
  },
  {
    "text": "well may maybe 20% of the time it might have something to do with the instance usually it's going to be the application",
    "start": "2409000",
    "end": "2414200"
  },
  {
    "text": "usually comes down to how good you are at observing the application I would also throw in the 2080 latency outlier rule uh where if",
    "start": "2414200",
    "end": "2422319"
  },
  {
    "text": "you're talking about latency outliers I often find they're caused by the everything else and the application may",
    "start": "2422319",
    "end": "2428079"
  },
  {
    "text": "only be 20% of the outliers or finding the latency monkey",
    "start": "2428079",
    "end": "2433800"
  },
  {
    "text": "so uh when when you're doing this uh performance analysis in the Netflix environment we can reduce latency to",
    "start": "2433800",
    "end": "2439880"
  },
  {
    "text": "make in odd places to make sure people uh are familiar with using their tools to find it so if you're thinking about",
    "start": "2439880",
    "end": "2446560"
  },
  {
    "text": "wow it's going to be hard to investigate all of the our environment all of the different components uh leny monkey is a",
    "start": "2446560",
    "end": "2453680"
  },
  {
    "text": "great way to uh force staff to get good at doing this because we can simulate",
    "start": "2453680",
    "end": "2459000"
  },
  {
    "text": "latency in odd places so that we can practice finding it uh just one more background before",
    "start": "2459000",
    "end": "2465400"
  },
  {
    "text": "the observability tools analysis perspectives so top down or bottom up are different ways you can approach a",
    "start": "2465400",
    "end": "2470520"
  },
  {
    "text": "system and so working from the application down or from the devices to the top and so both of them are valid",
    "start": "2470520",
    "end": "2476240"
  },
  {
    "text": "and they both work there are two groups of instance observability tools and these lead to",
    "start": "2476240",
    "end": "2481359"
  },
  {
    "text": "wins on Linux the statistical tools like vmstat pitat SAR and so on these work as",
    "start": "2481359",
    "end": "2488000"
  },
  {
    "start": "2484000",
    "end": "2484000"
  },
  {
    "text": "normal you in most cases uh because we're in a hardware virtualized environment and devices are virtualized",
    "start": "2488000",
    "end": "2494200"
  },
  {
    "text": "so use those tools as normal there's profiling tools this is where you're",
    "start": "2494200",
    "end": "2499480"
  },
  {
    "start": "2497000",
    "end": "2497000"
  },
  {
    "text": "characterizing usage so I might be sampling on CPU stack traces to explain CP usage or identifying where the code",
    "start": "2499480",
    "end": "2506520"
  },
  {
    "text": "paths what Cod paths are hot to look for related tuning now profiling types you can do",
    "start": "2506520",
    "end": "2512960"
  },
  {
    "start": "2512000",
    "end": "2512000"
  },
  {
    "text": "application profiling where I'm looking inside the application for Java it might be things like Java flight record or system profiling where I'm looking at",
    "start": "2512960",
    "end": "2519599"
  },
  {
    "text": "the operating system and what it's doing and so we'll use things like Linux per Advance F trace and system",
    "start": "2519599",
    "end": "2525839"
  },
  {
    "text": "tap application profiling just to give you a a taste of of what we have been",
    "start": "2525839",
    "end": "2530920"
  },
  {
    "text": "doing the lightweight Java profiler uh is just a simple open source asynchronous profiler for Java and we",
    "start": "2530920",
    "end": "2538119"
  },
  {
    "text": "can visualize the output as flame graphs which are really cool where at the top",
    "start": "2538119",
    "end": "2544119"
  },
  {
    "text": "that's where the CPU is running stack frames are Vis visualizes A rectangle",
    "start": "2544119",
    "end": "2549160"
  },
  {
    "text": "everything beneath it is a is ancestry and the width of the frame is relative to the uh degree that that that was on",
    "start": "2549160",
    "end": "2557400"
  },
  {
    "text": "CPU being sampled so the width is relative to its presence in the population so when you interpret these",
    "start": "2557400",
    "end": "2563640"
  },
  {
    "text": "you you look around the flame graph and find the widest parts and then you performance tune",
    "start": "2563640",
    "end": "2569960"
  },
  {
    "text": "those we can do the same thing for the system level and so there is perents based flame graphs uh and I've got the",
    "start": "2569960",
    "end": "2576800"
  },
  {
    "text": "all my slides I will put on SlideShare so you can I'll tweet the link so the system flame graphs they're really cool",
    "start": "2576800",
    "end": "2583200"
  },
  {
    "text": "and they show everything else like this is my Colonel tcpi Stacks why they're",
    "start": "2583200",
    "end": "2589240"
  },
  {
    "text": "eating CPU I've got lock time GC time and so on on the left I actually have some broken Java St Stacks that missing",
    "start": "2589240",
    "end": "2596160"
  },
  {
    "text": "the frame pointer uh because Java when jit runs and compiles things that throws",
    "start": "2596160",
    "end": "2601559"
  },
  {
    "text": "at the frame po actually came up with a fix for open jdk last week that we haven't open sourced yet I'm really excited because I I can now do uh the",
    "start": "2601559",
    "end": "2608559"
  },
  {
    "text": "Java Stacks as well and then there's tracing tools there another group from Linux uh there",
    "start": "2608559",
    "end": "2615480"
  },
  {
    "text": "are many and I'll just mention F trce and per events f trce is really cool it's part of the links kernel it was",
    "start": "2615480",
    "end": "2621960"
  },
  {
    "text": "first added in 2627 and I've been creating front-end tools to Aid usage like iOS Snoop so you",
    "start": "2621960",
    "end": "2628319"
  },
  {
    "text": "just run iOS Snoop and you can see my dis latency that also has like a a usage message just like a good posix or or a",
    "start": "2628319",
    "end": "2635800"
  },
  {
    "text": "Unix tool and then per events I showed an example earlier with flame graphs uh",
    "start": "2635800",
    "end": "2641160"
  },
  {
    "text": "but this can also do tracing and profiling it's really uh quite powerful and I can do things like let's trace a",
    "start": "2641160",
    "end": "2647880"
  },
  {
    "text": "kernel event and look at the c-path to see how we got there and then the last group of Linux",
    "start": "2647880",
    "end": "2653720"
  },
  {
    "start": "2652000",
    "end": "2652000"
  },
  {
    "text": "instance tools are Hardware counters uh msrs and PS pmc's msrs can be used to",
    "start": "2653720",
    "end": "2659119"
  },
  {
    "text": "verify the real clock rate and so I've got tools on GitHub like show boost which show what your CPUs are really",
    "start": "2659119",
    "end": "2665599"
  },
  {
    "text": "doing which is great for performance comparisons and then the last group is",
    "start": "2665599",
    "end": "2670839"
  },
  {
    "text": "Netflix online tools apart from all of the Linux tools for observability we have some we use ourselves we've got",
    "start": "2670839",
    "end": "2676760"
  },
  {
    "text": "Atlas and Atlas lets us do the cloud wide and instance monitoring and so we",
    "start": "2676760",
    "end": "2682240"
  },
  {
    "text": "can look at Trends we can break it down by regions and metrics and quickly narrow down uh where",
    "start": "2682240",
    "end": "2689800"
  },
  {
    "text": "latency is is coming from which instance type and then Vector is the one we're building right now it is a realtime per",
    "start": "2689800",
    "end": "2696800"
  },
  {
    "text": "second uh performance monitoring tool for on demand profiling and it's great because I get to do methodologies like the used",
    "start": "2696800",
    "end": "2703040"
  },
  {
    "text": "method with it and so in summary the performance",
    "start": "2703040",
    "end": "2708640"
  },
  {
    "text": "tuning at Netflix we go through instance selection to make sure we're on the best instance types Amazon ec2 features there",
    "start": "2708640",
    "end": "2715079"
  },
  {
    "text": "is uh uh PVH VM and Zen the sov are great ones to help improve performance",
    "start": "2715079",
    "end": "2722319"
  },
  {
    "text": "kernel tuning we we there are lots of different things you can do and observability is is what helps identify",
    "start": "2722319",
    "end": "2728920"
  },
  {
    "text": "areas that need analysis and tuning my slides are online will be",
    "start": "2728920",
    "end": "2735400"
  },
  {
    "text": "online I'll I'll upload them there are some of the other talks at Netflix we have at reinvent uh at the end of this session",
    "start": "2735400",
    "end": "2742760"
  },
  {
    "text": "we I will also be outside in the hall so for Q&A feel free to ask me questions",
    "start": "2742760",
    "end": "2748319"
  },
  {
    "text": "and we also have some other Netflix people in the room as well uh about the session so thank you",
    "start": "2748319",
    "end": "2755160"
  },
  {
    "text": "very much",
    "start": "2755160",
    "end": "2758920"
  }
]