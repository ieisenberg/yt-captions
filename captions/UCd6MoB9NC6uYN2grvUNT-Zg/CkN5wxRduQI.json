[
  {
    "text": "hello everyone I'm D I'm the generative AI specialist sa and today I am going to",
    "start": "160",
    "end": "7439"
  },
  {
    "text": "show how to deploy multiple fine-tune Foundation models on sagemaker at scale",
    "start": "7439",
    "end": "13960"
  },
  {
    "text": "with high performance cost effectively let's dive in so there are various",
    "start": "13960",
    "end": "19160"
  },
  {
    "text": "reasons why you might want to fine-tune or customize your foundation models now",
    "start": "19160",
    "end": "26279"
  },
  {
    "text": "you want to increase the accuracy and adapt your models based on your business domain or customer data or a specific",
    "start": "26279",
    "end": "35120"
  },
  {
    "text": "Downstream task and make them highly personalized now there are several",
    "start": "35120",
    "end": "41000"
  },
  {
    "text": "methods to customizing your foundation models um first is the prompt",
    "start": "41000",
    "end": "48000"
  },
  {
    "text": "engineering which is carefully crafting and iterating on the proms provided to",
    "start": "48000",
    "end": "53719"
  },
  {
    "text": "the foundation models to steer their outputs uh in their Des desired",
    "start": "53719",
    "end": "58960"
  },
  {
    "text": "Direction or you have retrieval augmented generation which basically",
    "start": "58960",
    "end": "65600"
  },
  {
    "text": "relies on retrieving the relevant text from the large",
    "start": "65600",
    "end": "70840"
  },
  {
    "text": "Corpus uh and Aid the text generation now the retrieval happens first and then",
    "start": "70840",
    "end": "76840"
  },
  {
    "text": "the retrieved texts are used to condition the models uh text generation",
    "start": "76840",
    "end": "82159"
  },
  {
    "text": "allows generating very high quality factually considered uh text by grounding the generations in retrieved",
    "start": "82159",
    "end": "89479"
  },
  {
    "text": "uh evid evidence now the other type is uh the fine-tuning or or or adopting a",
    "start": "89479",
    "end": "96320"
  },
  {
    "text": "pre-trained foundation model on a specific Downstream task uh for specific",
    "start": "96320",
    "end": "103479"
  },
  {
    "text": "uh data set and this allows tailoring the model to a particular domain by",
    "start": "103479",
    "end": "109840"
  },
  {
    "text": "updating the model weights on the new data set makes the model more",
    "start": "109840",
    "end": "116079"
  },
  {
    "text": "specialized for the Target task while retaining the the the General capability",
    "start": "116079",
    "end": "121159"
  },
  {
    "text": "so that's the advantage of this approach uh or you can rely on training uh from",
    "start": "121159",
    "end": "127920"
  },
  {
    "text": "scratch the entire pre-ra uh entire Foundation model uh call it as pre-training and so that basically is",
    "start": "127920",
    "end": "134879"
  },
  {
    "text": "training on the large uh training on a large language model on a on a data set from random initialized weights uh",
    "start": "134879",
    "end": "141480"
  },
  {
    "text": "rather than free training um and so allows fully optimizing the model uh for",
    "start": "141480",
    "end": "148160"
  },
  {
    "text": "a specific domain without inherited biases from other data sets uh but requires large amount of task specific",
    "start": "148160",
    "end": "156080"
  },
  {
    "text": "data and compute power and so sometimes it becomes cost prohibitive so in this session we are going to uh focus on how",
    "start": "156080",
    "end": "163400"
  },
  {
    "text": "to deploy the customized fine-tune Foundation models at scale with high performance cost",
    "start": "163400",
    "end": "169800"
  },
  {
    "text": "effectively um so many applications in natural language processing or generative AI uh specifically related to",
    "start": "169800",
    "end": "176920"
  },
  {
    "text": "the large language models rely on adapting one large scale pre-train",
    "start": "176920",
    "end": "183120"
  },
  {
    "text": "language model to multiple Downstream applications such adaptation is usually",
    "start": "183120",
    "end": "190080"
  },
  {
    "text": "done via fine-tuning which updates all the parameters of the pre-train model",
    "start": "190080",
    "end": "197000"
  },
  {
    "text": "the major downside of fine-tuning uh all the parameters is the new model contains many parameters as in the original model",
    "start": "197000",
    "end": "204560"
  },
  {
    "text": "uh for example fine-tuning of 175 billion as an example and multiple",
    "start": "204560",
    "end": "211519"
  },
  {
    "text": "independent uh instances of this huge model for fine-tuned model models each",
    "start": "211519",
    "end": "218120"
  },
  {
    "text": "with 175 billion parameters is prohibitively expensive from training",
    "start": "218120",
    "end": "225480"
  },
  {
    "text": "and inference perspective um and so parameter efficient fine-tuning technique which we",
    "start": "225480",
    "end": "232319"
  },
  {
    "text": "also call it as path offers an effective Solution by reducing the number of",
    "start": "232319",
    "end": "239959"
  },
  {
    "text": "fine-tuning parameters and memory usage while achieving almost comparable",
    "start": "239959",
    "end": "246200"
  },
  {
    "text": "performance to full fine tuning uh so low rank adaptation or we",
    "start": "246200",
    "end": "253840"
  },
  {
    "text": "call it as Laura in short is a type of parameter efficient fine-tuning",
    "start": "253840",
    "end": "260600"
  },
  {
    "text": "approaches for fine-tuning your models Laura freezes the pre-trained model",
    "start": "260600",
    "end": "269160"
  },
  {
    "text": "weights and injects the trainable rank decomposition matrices in each layer of",
    "start": "269160",
    "end": "276759"
  },
  {
    "text": "the Transformer architecture greatly reducing the number of trainable parameters for the",
    "start": "276759",
    "end": "284199"
  },
  {
    "text": "downstream tasks compared to 175 billion fine tune",
    "start": "284199",
    "end": "290560"
  },
  {
    "text": "model Laura can reduce the number of trainable parameters by",
    "start": "290560",
    "end": "297199"
  },
  {
    "text": "10,000 times and the CPU memory requirement by three times without",
    "start": "297199",
    "end": "304680"
  },
  {
    "text": "additional inference uh overhead latency now this is great approach to adopt uh",
    "start": "304680",
    "end": "311080"
  },
  {
    "text": "when you are looking to deploy tens to hundreds of Highly customized uh models",
    "start": "311080",
    "end": "317360"
  },
  {
    "text": "at scale cost effectively and I'll show you how to deply multiple Laura adapter",
    "start": "317360",
    "end": "324639"
  },
  {
    "text": "fine tune models at scale cost effectively on Sage maker all right so",
    "start": "324639",
    "end": "331919"
  },
  {
    "text": "let's dive into the Cod so in this screen what I'm showing is serving Laura based llama to 7 billion adopters with",
    "start": "331919",
    "end": "340520"
  },
  {
    "text": "high performance on Sage maker the use case is let's you have uh you want to use llama 2 7 billion chat model as a",
    "start": "340520",
    "end": "348880"
  },
  {
    "text": "base model um and then you want to have fine tuned your llama 27 billion models using",
    "start": "348880",
    "end": "357440"
  },
  {
    "text": "Laura based adapter technique and you ended up having three Laura based",
    "start": "357440",
    "end": "364160"
  },
  {
    "text": "adapter uh specifically for specific language so you want to serve uh you",
    "start": "364160",
    "end": "371199"
  },
  {
    "text": "want to use only one copy of the base model while you have language specific",
    "start": "371199",
    "end": "376880"
  },
  {
    "text": "lurer adapter fine-tuned and then you want to use uh that deployment to power",
    "start": "376880",
    "end": "384000"
  },
  {
    "text": "your business applications for different language in this case I'm going to show you three language adapters that you",
    "start": "384000",
    "end": "389800"
  },
  {
    "text": "have fine tune which is Russian language Spanish and French and let's see how you",
    "start": "389800",
    "end": "395280"
  },
  {
    "text": "can deploy these three adapter models attached to the base model and run the",
    "start": "395280",
    "end": "400800"
  },
  {
    "text": "INF at scale on stage maker all right so",
    "start": "400800",
    "end": "407479"
  },
  {
    "text": "um what we're going to use is the large model inference container which is a pre-built highly optimized compatible uh",
    "start": "407479",
    "end": "415680"
  },
  {
    "text": "container with Amazon Sage maker for inference uh for for running Foundation",
    "start": "415680",
    "end": "421080"
  },
  {
    "text": "models at scale uh and large model inference container U basically has uh a",
    "start": "421080",
    "end": "427720"
  },
  {
    "text": "full integration with opsource um Frameworks libraries that",
    "start": "427720",
    "end": "433840"
  },
  {
    "text": "offers high performance for running Foundation models uh and one of these open-source Frameworks uh that we",
    "start": "433840",
    "end": "440879"
  },
  {
    "text": "integrate with is called as VM VM is open-source fast serving uh serving uh",
    "start": "440879",
    "end": "448199"
  },
  {
    "text": "framework uh including the the the the fast model server um for running the",
    "start": "448199",
    "end": "453319"
  },
  {
    "text": "large language models and running the Laura based adapters um at scale so um",
    "start": "453319",
    "end": "460560"
  },
  {
    "text": "here is the stack serving stack or runtime serving stack of of large model",
    "start": "460560",
    "end": "467199"
  },
  {
    "text": "inance container on Sage maker so on top we have the the front end server which is based on djl serving which is based",
    "start": "467199",
    "end": "473919"
  },
  {
    "text": "on a Java and then we have different uh backends um including the the VM backend",
    "start": "473919",
    "end": "482400"
  },
  {
    "text": "and the and the the the large model influence distributor backend um and the",
    "start": "482400",
    "end": "488639"
  },
  {
    "text": "VM backend and the LMI just in short is um it uses underlying WM open source",
    "start": "488639",
    "end": "497560"
  },
  {
    "text": "backend for uh for doing additional optimizations um in terms of serving the",
    "start": "497560",
    "end": "504560"
  },
  {
    "text": "foundation models and serving these Lara based adapter at scale we LM under the",
    "start": "504560",
    "end": "510319"
  },
  {
    "text": "hood uses open-source libraries for",
    "start": "510319",
    "end": "516000"
  },
  {
    "text": "optimizing for optimal Laura based ruptor serving uh including s Laura",
    "start": "516000",
    "end": "521959"
  },
  {
    "text": "which is another open-source framework uh for for specifically Laura based",
    "start": "521959",
    "end": "527279"
  },
  {
    "text": "serving and Pica which is uh another open- source uh framework that offers",
    "start": "527279",
    "end": "534920"
  },
  {
    "text": "Cuda adapter optimizations or Cuda kernel optimizations for adapter models so combined together VM offers a full",
    "start": "534920",
    "end": "544240"
  },
  {
    "text": "integrated solution which is again integrated with sagemaker large modal inference container which we're going to",
    "start": "544240",
    "end": "550320"
  },
  {
    "text": "use in this in this uh in this session okay um the there are several advantages",
    "start": "550320",
    "end": "557560"
  },
  {
    "text": "of using um the VM and the esur approach uh the number one",
    "start": "557560",
    "end": "563600"
  },
  {
    "text": "is when you have multiple uh fine-tuned Laura adapter",
    "start": "563600",
    "end": "570120"
  },
  {
    "text": "um and running against the base model um what you end up generally as a",
    "start": "570120",
    "end": "575240"
  },
  {
    "text": "traditional approach is to merge these adapter models with the base model and",
    "start": "575240",
    "end": "581800"
  },
  {
    "text": "then run the inference but when you have more adapter at and you want to serve that at scale you can't possibly rely on",
    "start": "581800",
    "end": "589519"
  },
  {
    "text": "the approach of merging all of these adapters with the base model because just going to increase the number of",
    "start": "589519",
    "end": "595959"
  },
  {
    "text": "copies of the base model and and it's eventually going to increase the number of uh the amount of memory required to",
    "start": "595959",
    "end": "602920"
  },
  {
    "text": "load these multiple instances of the base models and it becomes cost prohibitive and it also becomes uh a",
    "start": "602920",
    "end": "610279"
  },
  {
    "text": "very non-performing solution from the inference standpoint so the approach",
    "start": "610279",
    "end": "615519"
  },
  {
    "text": "that esora takes and eventually it was also offered as as a feature in the the",
    "start": "615519",
    "end": "620760"
  },
  {
    "text": "large model inference container of sage maker is unmerged Laura inference basically what that means is these",
    "start": "620760",
    "end": "627880"
  },
  {
    "text": "adoptors are not merg at all and are kept decoupled from the base model the",
    "start": "627880",
    "end": "634760"
  },
  {
    "text": "first place and the inference or the forward pass is run um independently uh",
    "start": "634760",
    "end": "641240"
  },
  {
    "text": "for both the adapter models and the base models and the final output is sent back",
    "start": "641240",
    "end": "646480"
  },
  {
    "text": "to the the caller that we call it as unmerged uh",
    "start": "646480",
    "end": "652160"
  },
  {
    "text": "Laura based adapter inference and then if you have multiple different types of",
    "start": "652160",
    "end": "657360"
  },
  {
    "text": "adapters to use just like in this case you have three adapters um and multiple different types of requests for",
    "start": "657360",
    "end": "663440"
  },
  {
    "text": "different types of adapters may come concurrently at the same time um and you",
    "start": "663440",
    "end": "668519"
  },
  {
    "text": "don't want to have multiple copies of the base model uh because it it will increase the memory uh consumption you",
    "start": "668519",
    "end": "675560"
  },
  {
    "text": "in so you can use this unmerged inference along with the dynamic heterogeneous batching so you might want",
    "start": "675560",
    "end": "681200"
  },
  {
    "text": "to batch these uh different inference request coming for different adapters uh",
    "start": "681200",
    "end": "686360"
  },
  {
    "text": "which you call it heterogeneous and then you want to patch them together and to increase the throughput uh and to get",
    "start": "686360",
    "end": "692680"
  },
  {
    "text": "the best price performance um and there are added added additional optimizations that esora brings on the table including",
    "start": "692680",
    "end": "700240"
  },
  {
    "text": "offloading these adapter models if it exceeds the gpus um high bandwidth memory um it",
    "start": "700240",
    "end": "707720"
  },
  {
    "text": "offers an ability to um to offload some of the adapters that are not used are",
    "start": "707720",
    "end": "714639"
  },
  {
    "text": "based on the least recently used algorithm um offload them into the CPU host memory and then whenever they are",
    "start": "714639",
    "end": "721120"
  },
  {
    "text": "required it it brings them back into the gpu's working memory here is the quick a",
    "start": "721120",
    "end": "727680"
  },
  {
    "text": "diagram medic representation of how the dynamic adapter loading happens or the offloading to the CPU happens um so uh",
    "start": "727680",
    "end": "735800"
  },
  {
    "text": "the the adapters could be could be loaded actively on the GPU uh high",
    "start": "735800",
    "end": "742120"
  },
  {
    "text": "bandwidth memory uh and then they might get offloaded to the the CPU memory if",
    "start": "742120",
    "end": "747600"
  },
  {
    "text": "they are not being used or there's there a memory pressure of loading more active adapters into the memory and you have to",
    "start": "747600",
    "end": "754320"
  },
  {
    "text": "make some space to accommodate them then the least recently used adaptors may get",
    "start": "754320",
    "end": "760000"
  },
  {
    "text": "offloaded to the CPU and eventually um to the and and eventually the external",
    "start": "760000",
    "end": "765560"
  },
  {
    "text": "storage if required but uh uh the the Sora offers that feature of CPU off",
    "start": "765560",
    "end": "773240"
  },
  {
    "text": "loading and then reloading from CPU to the to the GPU high band birth memory",
    "start": "773240",
    "end": "778680"
  },
  {
    "text": "based on least recently used algorithm out of the box you don't have to write a single line of code it just it just it's",
    "start": "778680",
    "end": "785000"
  },
  {
    "text": "just there out of the box so let's go back to our code another open source uh",
    "start": "785000",
    "end": "791440"
  },
  {
    "text": "framework called is Puna uh it it offers Cuda adapter kernel optimizations so",
    "start": "791440",
    "end": "798040"
  },
  {
    "text": "that you can reduce the response times reduce the the GPU I memory iio operations um and it also optimizes the",
    "start": "798040",
    "end": "806480"
  },
  {
    "text": "the matrix multiplication operations uh for different different Laura based request in a batch um and uh combined",
    "start": "806480",
    "end": "814079"
  },
  {
    "text": "together all these features combined together um offers high performance in",
    "start": "814079",
    "end": "819199"
  },
  {
    "text": "terms of reduced response times and high through but um on stage maker okay so",
    "start": "819199",
    "end": "824680"
  },
  {
    "text": "we're going to use uh large model insance container called LMI container and we going to use VM backin in this",
    "start": "824680",
    "end": "830680"
  },
  {
    "text": "case as I'm showing in this diagram and then we are going to uh serve multiple",
    "start": "830680",
    "end": "836480"
  },
  {
    "text": "adapter models uh for inance okay so here are some of the code to to typical",
    "start": "836480",
    "end": "842920"
  },
  {
    "text": "cord to create the session for sagemaker we're going to skip it uh border of three client very easy all right here is",
    "start": "842920",
    "end": "850279"
  },
  {
    "text": "very very important step that I want to highlight here um so what we're going to do is you",
    "start": "850279",
    "end": "857920"
  },
  {
    "text": "can um upload the adapter models uh model weights um and the model artifacts",
    "start": "857920",
    "end": "866320"
  },
  {
    "text": "um to Amazon S3 and uh the way have to package or bin pack these uh adapter",
    "start": "866320",
    "end": "873320"
  },
  {
    "text": "models there's a specific structure that you have to follow in terms of the folder structure uh so what we're going",
    "start": "873320",
    "end": "879759"
  },
  {
    "text": "to do is first uh we going to uh we are going to use the base model as Lama 27",
    "start": "879759",
    "end": "885920"
  },
  {
    "text": "billion chat f with the data type of fp16 which requires two two bytes per",
    "start": "885920",
    "end": "891680"
  },
  {
    "text": "per parameter to store to store the single parameter and then we're going to use uh three fine tune adapter models",
    "start": "891680",
    "end": "899959"
  },
  {
    "text": "uh in this case uh they we have an adopter model um fine-tuned for Russian",
    "start": "899959",
    "end": "906240"
  },
  {
    "text": "language Spanish and uh and French and so if uh if you go to the hugging face",
    "start": "906240",
    "end": "914800"
  },
  {
    "text": "uh uh website and check out their uh adapter configuration uh there is a file called",
    "start": "914800",
    "end": "924040"
  },
  {
    "text": "as adapter config.js I'll give",
    "start": "924040",
    "end": "929759"
  },
  {
    "text": "lot of great information metadata information about the adapter and how was it fine-tuned and specifically the",
    "start": "929759",
    "end": "935480"
  },
  {
    "text": "rank of the adapter which is extremely important parameter to configure when we",
    "start": "935480",
    "end": "942199"
  },
  {
    "text": "configure our our stage maker endpoint all right um so we'll come back to this",
    "start": "942199",
    "end": "947880"
  },
  {
    "text": "file um so we'll go back to our cord so we have three Laura fine tune adapters",
    "start": "947880",
    "end": "955440"
  },
  {
    "text": "and one base model all right the first",
    "start": "955440",
    "end": "960560"
  },
  {
    "text": "step is to download these adapters Laura",
    "start": "960560",
    "end": "966120"
  },
  {
    "text": "based adapters um and convert that into or bin",
    "start": "966120",
    "end": "971319"
  },
  {
    "text": "pack into uh model. tar.gz with a specific folder structure and the folder",
    "start": "971319",
    "end": "977079"
  },
  {
    "text": "structure is you have a model directory and then you have a specific adapter uh folder and within the adapter",
    "start": "977079",
    "end": "984959"
  },
  {
    "text": "folder uh you you have multiple uh different ad uh",
    "start": "984959",
    "end": "990720"
  },
  {
    "text": "artifacts downloaded from hugging phase and then you have a common serving properties which is configuration file",
    "start": "990720",
    "end": "997360"
  },
  {
    "text": "for configuring the large model inference container my container which you can configure for uh different uh uh",
    "start": "997360",
    "end": "1005199"
  },
  {
    "text": "different for for the for the performance reasons and then you can also bring your own inference script",
    "start": "1005199",
    "end": "1011199"
  },
  {
    "text": "called as model.py and you can write pre-processing or postprocessing code that you want to write uh and then get",
    "start": "1011199",
    "end": "1017880"
  },
  {
    "text": "going that that the serving. properties and model.py they're they're optional uh",
    "start": "1017880",
    "end": "1023199"
  },
  {
    "text": "you can also specify the parameters that you that you specify in serving the",
    "start": "1023199",
    "end": "1028640"
  },
  {
    "text": "properties in the environment variable as well um and and and go from there so",
    "start": "1028640",
    "end": "1034240"
  },
  {
    "text": "first step is to download the three Laura fine tune",
    "start": "1034240",
    "end": "1039600"
  },
  {
    "text": "adapters uh and create a directory called as uh create a directory uh",
    "start": "1039600",
    "end": "1046000"
  },
  {
    "text": "called as Laura multi-adapter and within that we create a folder called as adapter and then we download one by one",
    "start": "1046000",
    "end": "1053559"
  },
  {
    "text": "all the three adapters from hugging phase once we do that we then",
    "start": "1053559",
    "end": "1062559"
  },
  {
    "text": "tar.gz the folder and then upload it on S3 and if",
    "start": "1062559",
    "end": "1068640"
  },
  {
    "text": "you want to show you want to see the how the folder structure looks like here I I'm showing it here my local in my",
    "start": "1068640",
    "end": "1076600"
  },
  {
    "text": "studio notebook um and you have you can see here the adapter folder is created within the",
    "start": "1076600",
    "end": "1084400"
  },
  {
    "text": "adapter you have the Spanish adapter e with es F FR for French Ru for Russian",
    "start": "1084400",
    "end": "1091679"
  },
  {
    "text": "and then individual adapter directory has all the",
    "start": "1091679",
    "end": "1097559"
  },
  {
    "text": "artifacts needed including adapter config.js which is a metadata file for",
    "start": "1097559",
    "end": "1103960"
  },
  {
    "text": "the adapter and then the safe tensor file B is basically the actual model",
    "start": "1103960",
    "end": "1109200"
  },
  {
    "text": "weights of the adapter and then there are a lot of other configuration files like including tokenizer Json and so and",
    "start": "1109200",
    "end": "1116080"
  },
  {
    "text": "so forth which is definitely essential for serving these models um um for",
    "start": "1116080",
    "end": "1122440"
  },
  {
    "text": "inference and so remember very important uh within the adapter directory the name",
    "start": "1122440",
    "end": "1128600"
  },
  {
    "text": "of the folder like here in this case es FR and rudu will become your adapter",
    "start": "1128600",
    "end": "1134080"
  },
  {
    "text": "names and that is required when you invol the endo so keep that in mind the",
    "start": "1134080",
    "end": "1140799"
  },
  {
    "text": "name of the adopter is required to invoke the sagemaker inference endpoint",
    "start": "1140799",
    "end": "1146360"
  },
  {
    "text": "basically that would uh that is needed to invoke the respective adapter for",
    "start": "1146360",
    "end": "1153799"
  },
  {
    "text": "your inference request letc all right so now that we have created the model. tar.gz we have upload",
    "start": "1153799",
    "end": "1163000"
  },
  {
    "text": "let's upload that on in S3 and so here is the code for uploading",
    "start": "1163000",
    "end": "1168400"
  },
  {
    "text": "the model t. gz with this predefined structure where we have multiple adapters uh been packed now uh you can",
    "start": "1168400",
    "end": "1177440"
  },
  {
    "text": "also choose to not t or gz and just upload the raw artifacts on S in that S3",
    "start": "1177440",
    "end": "1183400"
  },
  {
    "text": "location uh in an uncompressed format too that is also supported all right so",
    "start": "1183400",
    "end": "1190240"
  },
  {
    "text": "let's go over to the next step the next step is to con to set the configuration parameters using environment variables",
    "start": "1190240",
    "end": "1198000"
  },
  {
    "text": "uh remember I I said that there are two ways to configure LMI container uh",
    "start": "1198000",
    "end": "1203880"
  },
  {
    "text": "either use serving the properties and uh make it as a part of the model. tar.gz",
    "start": "1203880",
    "end": "1209799"
  },
  {
    "text": "or you can configure as part of the environment variables when you create",
    "start": "1209799",
    "end": "1215240"
  },
  {
    "text": "the sagemaker model object so we are going to do we are going to follow the second approach",
    "start": "1215240",
    "end": "1220640"
  },
  {
    "text": "here there are specific lur specific fine tunables or configuration",
    "start": "1220640",
    "end": "1226480"
  },
  {
    "text": "parameters that you can set the first very important parameter is the um",
    "start": "1226480",
    "end": "1234559"
  },
  {
    "text": "enable uncore LA or option _ enor laara which you have to set it to True by",
    "start": "1234559",
    "end": "1240600"
  },
  {
    "text": "default is false when you want the Laura adopters survey in sagemaker okay and",
    "start": "1240600",
    "end": "1247600"
  },
  {
    "text": "then the next one is Max lauras this determines the maximum",
    "start": "1247600",
    "end": "1252960"
  },
  {
    "text": "number of Lor adapters that you can run at any at uh",
    "start": "1252960",
    "end": "1259360"
  },
  {
    "text": "run at once uh and so it all it serves as allocating more GPU memory for for",
    "start": "1259360",
    "end": "1266280"
  },
  {
    "text": "for those adapters um default is four you can increase it if you are loading",
    "start": "1266280",
    "end": "1271480"
  },
  {
    "text": "more than four adapters uh and you want them to be served concurrently all right",
    "start": "1271480",
    "end": "1276559"
  },
  {
    "text": "and then you have um you have Max laa rank which basically is the rank of",
    "start": "1276559",
    "end": "1282880"
  },
  {
    "text": "these lur adapter the maximum rank of these lur adapters and remember I showed you ad _ config.js file let's go there",
    "start": "1282880",
    "end": "1291799"
  },
  {
    "text": "let's see the the rank of one of these adapter models which in this case is",
    "start": "1291799",
    "end": "1297159"
  },
  {
    "text": "Russian uh fine tune adapter model the rank is specified by the attribute called as R in this",
    "start": "1297159",
    "end": "1303760"
  },
  {
    "text": "file which is four or which is 64 sorry and so if you go",
    "start": "1303760",
    "end": "1309000"
  },
  {
    "text": "back and and check the rank for all other uh lower adapters you will see the",
    "start": "1309000",
    "end": "1315120"
  },
  {
    "text": "rank is same which is 64 so we're going to set that to 64 by the way in our",
    "start": "1315120",
    "end": "1320880"
  },
  {
    "text": "configuration um and then you have uh additional Laura uh",
    "start": "1320880",
    "end": "1326720"
  },
  {
    "text": "configuration um that that basically is going to drive the the allocation of the",
    "start": "1326720",
    "end": "1332360"
  },
  {
    "text": "the the the memory we're not going to go into those details the the more details are available here if you're curious um",
    "start": "1332360",
    "end": "1340440"
  },
  {
    "text": "if you're curious to to learn about them then I have attached the link to excuse",
    "start": "1340440",
    "end": "1348480"
  },
  {
    "text": "excuse me I have attached the link to uh the extended uh fine tunables or",
    "start": "1348480",
    "end": "1355200"
  },
  {
    "text": "configuration parameters and it just walks through uh these uh parameters",
    "start": "1355200",
    "end": "1360840"
  },
  {
    "text": "default values what does that mean and when should you set them gives you best practices so feel free to refer to them",
    "start": "1360840",
    "end": "1367240"
  },
  {
    "text": "let's come back all right so what we're going to do is we're going to create a sagemaker model object for that you need",
    "start": "1367240",
    "end": "1373640"
  },
  {
    "text": "to retrieve the image URI container image URI of large model INF container",
    "start": "1373640",
    "end": "1378799"
  },
  {
    "text": "for sage maker in this case is the V2 uh v27 version of the large moral inference",
    "start": "1378799",
    "end": "1384840"
  },
  {
    "text": "container for deep speed so we retrieve the URI and then we set these configuration parameters as part of the",
    "start": "1384840",
    "end": "1391440"
  },
  {
    "text": "environment variable of the model model Creation in this case we specify the",
    "start": "1391440",
    "end": "1397000"
  },
  {
    "text": "base model the base model which is llama 27 billion in this case uh with",
    "start": "1397000",
    "end": "1403520"
  },
  {
    "text": "fp16 uh you can specify the hugging face model ID here and the option _ model _",
    "start": "1403520",
    "end": "1409640"
  },
  {
    "text": "ID or you can upload your own base model in the S3 location and provide the S3 location",
    "start": "1409640",
    "end": "1415279"
  },
  {
    "text": "here um and then you can specify how many shards you want for that for that",
    "start": "1415279",
    "end": "1422919"
  },
  {
    "text": "base model in this case uh we uh have set it to Max that means uh use the",
    "start": "1422919",
    "end": "1428520"
  },
  {
    "text": "maximum CPU device G sorry GPU devices available in the host which um which is",
    "start": "1428520",
    "end": "1434279"
  },
  {
    "text": "um uh which is what we are going to default to right now and so basic basically uh whatever instance type that",
    "start": "1434279",
    "end": "1440400"
  },
  {
    "text": "you that we select it's going to fully utilize all of the devices GP devices in that host and Shard it across uh we are",
    "start": "1440400",
    "end": "1448000"
  },
  {
    "text": "going to use uh the the back end as VM as we as as I showed you earlier um so",
    "start": "1448000",
    "end": "1454640"
  },
  {
    "text": "the rolling badge is set to VM which basically which basically which basically means that you you want to use",
    "start": "1454640",
    "end": "1461279"
  },
  {
    "text": "uh uh VM and then there are additional parameters like bat size or continuous",
    "start": "1461279",
    "end": "1466399"
  },
  {
    "text": "bat size uh the dat I for the uh the model the base model which is",
    "start": "1466399",
    "end": "1472600"
  },
  {
    "text": "fp16 um and as I referred to you earlier you need to enable the Lura base serving",
    "start": "1472600",
    "end": "1477679"
  },
  {
    "text": "by setting this uh Encore Laura to true and we set the max Laura rank to 64",
    "start": "1477679",
    "end": "1485480"
  },
  {
    "text": "remember we saw the config adapter Json file and we saw uh the rank of these",
    "start": "1485480",
    "end": "1490760"
  },
  {
    "text": "adapter models was 64 so we're going to set that same here um and then we go",
    "start": "1490760",
    "end": "1496240"
  },
  {
    "text": "from there a Max CPU Laura is very important parameter now Mac CPU Lura is",
    "start": "1496240",
    "end": "1502640"
  },
  {
    "text": "a configuration defines maximum number of Laura adapters uh to cache in the CPU",
    "start": "1502640",
    "end": "1509559"
  },
  {
    "text": "memory as well um and so all others will be then evicted to this the external dis",
    "start": "1509559",
    "end": "1515679"
  },
  {
    "text": "um so you might want to set this to a specific number um so that you don't uh",
    "start": "1515679",
    "end": "1523120"
  },
  {
    "text": "you don't over you don't have to swap out to the to the dis and that all these adapter",
    "start": "1523120",
    "end": "1529080"
  },
  {
    "text": "and that the instance that you select has the the the the required amount of the CPU memory to hold these adapters um",
    "start": "1529080",
    "end": "1537360"
  },
  {
    "text": "in in the host memory so in this case we're going to set it to four because the number of adapters that we are going to take it up here in this uh example is",
    "start": "1537360",
    "end": "1544399"
  },
  {
    "text": "just four just three so four is more than enough uh and just and then we got",
    "start": "1544399",
    "end": "1549600"
  },
  {
    "text": "get going um with the creation of the model so we create the model object of sage maker with the image with the",
    "start": "1549600",
    "end": "1556559"
  },
  {
    "text": "environment variable and the S3 location where our adaptor uh artifacts are",
    "start": "1556559",
    "end": "1562559"
  },
  {
    "text": "stored or uploaded okay and then we create a stagemaker and point config and",
    "start": "1562559",
    "end": "1570120"
  },
  {
    "text": "basically uh you can you you select the instance type is uh g5x large which has",
    "start": "1570120",
    "end": "1578000"
  },
  {
    "text": "one uh GPU A10 G GPU with um 24 gigabyt of high band bird",
    "start": "1578000",
    "end": "1585679"
  },
  {
    "text": "memory for G for for um per per device and there's only one device by the way",
    "start": "1585679",
    "end": "1591000"
  },
  {
    "text": "so the overall we have only 24 uh gab of the high bandwidth memory",
    "start": "1591000",
    "end": "1597200"
  },
  {
    "text": "um and we can easily fit 7 billion with fp16 uh base model um single copy in uh",
    "start": "1597200",
    "end": "1605960"
  },
  {
    "text": "single device uh of a1g so shouldn't you shouldn't have any code out of memory",
    "start": "1605960",
    "end": "1611600"
  },
  {
    "text": "errors or suchar um um so we choose to select only x large we can also select",
    "start": "1611600",
    "end": "1618559"
  },
  {
    "text": "4X large uh excuse me the the 12x large which has four gpus um or you feel free",
    "start": "1618559",
    "end": "1624960"
  },
  {
    "text": "to use other instance types um um in that case you would have to increase you",
    "start": "1624960",
    "end": "1630240"
  },
  {
    "text": "can also increase the number of modal copies and and increase the performance that way we going to use G5 XL uh in",
    "start": "1630240",
    "end": "1636799"
  },
  {
    "text": "this case Okay and then you create the end point with the model and the",
    "start": "1636799",
    "end": "1642960"
  },
  {
    "text": "endpoint config we created with create endpoint API of bordo 3 by the way",
    "start": "1642960",
    "end": "1648760"
  },
  {
    "text": "entire the whole code that I'm showing you can also do it with salemaker python SDK and there are different uh",
    "start": "1648760",
    "end": "1654600"
  },
  {
    "text": "constructs for that too um um and you can also um you can also use bord of",
    "start": "1654600",
    "end": "1661799"
  },
  {
    "text": "three as I'm showing here so um all right and then we wait for the endpoint",
    "start": "1661799",
    "end": "1668120"
  },
  {
    "text": "creation once the endpoint is created you call um invoke endpoint API or invol",
    "start": "1668120",
    "end": "1675919"
  },
  {
    "text": "endpoint API of stagemaker using B of three and then um you uh have two attributes",
    "start": "1675919",
    "end": "1683200"
  },
  {
    "text": "within the Json file or the request Json one is the inputs which basically Bears the prompts um for the inference and the",
    "start": "1683200",
    "end": "1691919"
  },
  {
    "text": "adapters uh which basically you can specify the name of the adopters that you want to invoke for that specific",
    "start": "1691919",
    "end": "1697880"
  },
  {
    "text": "prompt now you can um uh you can in you can batch these prompts and invoke in",
    "start": "1697880",
    "end": "1703880"
  },
  {
    "text": "one go or you can have multiple different requests coming into to your container at a different time interval",
    "start": "1703880",
    "end": "1711919"
  },
  {
    "text": "and remember there are L there there's heterogeneous Dynamic batching also implemented on the server side we take",
    "start": "1711919",
    "end": "1718919"
  },
  {
    "text": "care of um batching them together and um running the inference in",
    "start": "1718919",
    "end": "1724240"
  },
  {
    "text": "batch to maximize the throughput and give you the best price performance so that that still it will that will still",
    "start": "1724240",
    "end": "1730720"
  },
  {
    "text": "happen um on the server side and you get all these uh goodies on the in the inference optimization from the",
    "start": "1730720",
    "end": "1737320"
  },
  {
    "text": "inference optimiz ation perspective so in this case we are going to invoke uh",
    "start": "1737320",
    "end": "1742440"
  },
  {
    "text": "the the Spanish adapter uh so you can see here the prompt is in Spanish and we",
    "start": "1742440",
    "end": "1748279"
  },
  {
    "text": "invoke that you can also change the adapter to uh let's say Ru uh which is",
    "start": "1748279",
    "end": "1754279"
  },
  {
    "text": "Russian or you can also uh use uh French uh and you have to adjust the prompt",
    "start": "1754279",
    "end": "1760640"
  },
  {
    "text": "accordingly and then invoke the the the Endo um and so you can see here the very",
    "start": "1760640",
    "end": "1767320"
  },
  {
    "text": "uh it's a three-step process upload your adapter models on S3 in a specific um in",
    "start": "1767320",
    "end": "1773039"
  },
  {
    "text": "a specific folder structure uh you can upload the compressed or uncompressed uh",
    "start": "1773039",
    "end": "1778320"
  },
  {
    "text": "artifacts on S3 you create a stagemaker model object with the right uh with the LMI container Ur and set the the right",
    "start": "1778320",
    "end": "1785840"
  },
  {
    "text": "environment variables or configuration variables um and then uh create a create",
    "start": "1785840",
    "end": "1792320"
  },
  {
    "text": "a model object create a an end endpoint config object and select your instance type um you can also configure the",
    "start": "1792320",
    "end": "1798960"
  },
  {
    "text": "autoscaling policy and then you um create the endpoint and invoke the endpoint it's a four-step process um and",
    "start": "1798960",
    "end": "1806399"
  },
  {
    "text": "you're good to go and uh we'll take care of loading the the the adopters uh",
    "start": "1806399",
    "end": "1812200"
  },
  {
    "text": "dynamically and implementing uh the dynamic batching and make sure that they are not merged and then you can get the",
    "start": "1812200",
    "end": "1818200"
  },
  {
    "text": "best out of uh best out of uh the price performance um and um and serve these uh",
    "start": "1818200",
    "end": "1826360"
  },
  {
    "text": "adapter based fine tune mark mod at scale you can also Autos scale them uh",
    "start": "1826360",
    "end": "1832240"
  },
  {
    "text": "with to uh using multiple instances behind the behind the inference endpoint and that way you can serve hundreds to",
    "start": "1832240",
    "end": "1840320"
  },
  {
    "text": "thousands of uh Lura based adapters at scale um for your production use cases",
    "start": "1840320",
    "end": "1846960"
  },
  {
    "text": "okay so here's the example notebook that I was going uh going through um uh you can uh you can scan this and you can",
    "start": "1846960",
    "end": "1853440"
  },
  {
    "text": "directly go to my example notebook and give it a try uh and don't forget you can also scale this Solution by",
    "start": "1853440",
    "end": "1860120"
  },
  {
    "text": "attaching a r scaling policy um and you can scale out to hundreds of uh Laura",
    "start": "1860120",
    "end": "1865760"
  },
  {
    "text": "based adapter serving at scale on Sage maker I hope this session was useful I'll see you in the next session thank",
    "start": "1865760",
    "end": "1871000"
  },
  {
    "text": "you for watching",
    "start": "1871000",
    "end": "1874399"
  }
]