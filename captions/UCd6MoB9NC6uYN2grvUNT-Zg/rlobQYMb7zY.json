[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "thank you everybody good afternoon we are really happy that you could join us",
    "start": "30",
    "end": "5400"
  },
  {
    "text": "today my name is Arvind Shah I am the engineering manager on the data team at Robin Hood and with me I have Sam Satya",
    "start": "5400",
    "end": "12690"
  },
  {
    "text": "who's the data engineer and grace Lu who is also a data engineer on the rabbinate team so for those of you who don't know",
    "start": "12690",
    "end": "19230"
  },
  {
    "text": "Robin Hood is a commission free investment platform that allows you to very easily buy and trade",
    "start": "19230",
    "end": "26609"
  },
  {
    "text": "cryptocurrencies ETFs stocks and options our mission is to democratize the",
    "start": "26609",
    "end": "32369"
  },
  {
    "text": "American financial system and making data-driven decisions is a big part of that so that brings us to today's talk",
    "start": "32369",
    "end": "38640"
  },
  {
    "text": "and why we are here to talk to you about data leaks a daedalic is a central",
    "start": "38640",
    "end": "44460"
  },
  {
    "text": "repository that a company maintains for structured and unstructured information that allows the company to make",
    "start": "44460",
    "end": "51329"
  },
  {
    "text": "decisions and acts as a platform for other tools and technologies that need the company's data to be built on top of",
    "start": "51329",
    "end": "59420"
  },
  {
    "text": "so we have a packed agenda for you today we'll start talking about the journey",
    "start": "59420",
    "end": "65250"
  },
  {
    "start": "60000",
    "end": "95000"
  },
  {
    "text": "that Robin took why we felt we needed a data Lake what are some of the decisions that we",
    "start": "65250",
    "end": "70619"
  },
  {
    "text": "had to make the object architecture that we finally came up with and finally we will talk about a lot of the learnings",
    "start": "70619",
    "end": "76590"
  },
  {
    "text": "that we've had that we hope will help you guys build your own data Lakes as well and in the end we'd love to take",
    "start": "76590",
    "end": "82740"
  },
  {
    "text": "any of the questions you have and help you guys think through any of our tech stack or any of the decisions that we've",
    "start": "82740",
    "end": "88320"
  },
  {
    "text": "made with that I'd love to hand it off to Sam and grace two of the key engineers who helped build this data",
    "start": "88320",
    "end": "93689"
  },
  {
    "text": "like thank you thanks Ron so as arpan",
    "start": "93689",
    "end": "100200"
  },
  {
    "start": "95000",
    "end": "111000"
  },
  {
    "text": "said let's start by diving into the problems we saw before we set out on this journey to build the data lake the",
    "start": "100200",
    "end": "106049"
  },
  {
    "text": "patterns we saw emerging at Robin Hood and how we actually ended up dealing with these so one of the basic problems",
    "start": "106049",
    "end": "113280"
  },
  {
    "start": "111000",
    "end": "193000"
  },
  {
    "text": "that we saw that doesn't look up like a problem at first is that we had several data stores at Robin Hood and all of",
    "start": "113280",
    "end": "119219"
  },
  {
    "text": "these are optimized for their needs we have a redshift instance that is basically storing a lot of events and",
    "start": "119219",
    "end": "124590"
  },
  {
    "text": "analytical data and it's really good at doing that really good at calling a compressing all of this data and answering queries we have elastic search",
    "start": "124590",
    "end": "131459"
  },
  {
    "text": "clusters that are that is working as our documents it's really good because it's based on Apache you seen and acts as a search",
    "start": "131459",
    "end": "137280"
  },
  {
    "text": "engine so our in-app search is something that is powered by this and it's really efficient at doing that we have in flux",
    "start": "137280",
    "end": "143610"
  },
  {
    "text": "CB which is our time series database it's really good at answering time series based question ingesting large",
    "start": "143610",
    "end": "148920"
  },
  {
    "text": "amounts of data in a small amount of time and answering questions on top of that we have s3 as our object store we",
    "start": "148920",
    "end": "154260"
  },
  {
    "text": "have post squares as our relational database several of these used by different services for their relational needs we have Dropbox for sharing files",
    "start": "154260",
    "end": "161550"
  },
  {
    "text": "within the company there's several third-party integrations we have for getting to certain points we",
    "start": "161550",
    "end": "167100"
  },
  {
    "text": "want within the company the problem that comes into this is not their primary work is the secondary workflow that",
    "start": "167100",
    "end": "173100"
  },
  {
    "text": "comes with these systems which is usually the analytical purposes whenever a data scientist wants to do something",
    "start": "173100",
    "end": "178500"
  },
  {
    "text": "interesting build models on top of this data or do some in-depth analysis the problem that comes in is that they usually need to go into multiple systems",
    "start": "178500",
    "end": "184950"
  },
  {
    "text": "pull the data that they need aggregate into a central place with loss of memory lots of compute and do something over",
    "start": "184950",
    "end": "191190"
  },
  {
    "text": "there let's take an example of what this would look like so let's say a data",
    "start": "191190",
    "end": "196260"
  },
  {
    "text": "scientist wants to aggregate our events data with some relational data and have some documents that we are showing in",
    "start": "196260",
    "end": "201660"
  },
  {
    "text": "elasticsearch and work with all of them at the same time now this becomes really really problematic because the person",
    "start": "201660",
    "end": "207269"
  },
  {
    "text": "has to ramp up on the different dialects each of these data stores are speaking while redshift and Postgres you can just",
    "start": "207269",
    "end": "212700"
  },
  {
    "text": "use sequel for example elasticsearch uses Lucene query syntax which is not trivial to ramp up on in your first go",
    "start": "212700",
    "end": "219630"
  },
  {
    "text": "so normally do they need to learn about all of these different things they need to pull it down to a central location",
    "start": "219630",
    "end": "224760"
  },
  {
    "text": "for example their local disk and then they have to upload it to a box with ungodly amounts of CPU on Galli amounts",
    "start": "224760",
    "end": "230459"
  },
  {
    "text": "of memory so that they can do some transformations join these datasets together using something like a panda's",
    "start": "230459",
    "end": "236040"
  },
  {
    "text": "data frame another problem that we start started seeing as we started growing as",
    "start": "236040",
    "end": "242130"
  },
  {
    "start": "237000",
    "end": "298000"
  },
  {
    "text": "a company internally with our internal data users and even externally as we started rolling out robin out to more",
    "start": "242130",
    "end": "248250"
  },
  {
    "text": "and more people started launching more and more features is that we saw like things turned from gigabyte scale to a",
    "start": "248250",
    "end": "253680"
  },
  {
    "text": "terabyte scale we saw our events data set go from few gigabytes per day to several terabytes per day we saw our lb",
    "start": "253680",
    "end": "260700"
  },
  {
    "text": "logs which is our amazon load balancer logs for different services as we started scaling up from like few handful",
    "start": "260700",
    "end": "266100"
  },
  {
    "text": "services to like hundreds of services we saw this spinning out of hand as well into",
    "start": "266100",
    "end": "271110"
  },
  {
    "text": "doTERRA buyers into the petabytes and this becomes really problematic because the system we had previously was",
    "start": "271110",
    "end": "277139"
  },
  {
    "text": "basically that a single system would go and read the data that it needs to do some transformations and so the only",
    "start": "277139",
    "end": "283379"
  },
  {
    "text": "tool or the only lever we had in our hand was to vertically scale up these systems as we got more and more data and",
    "start": "283379",
    "end": "289129"
  },
  {
    "text": "as the data uses within the company grows well there were different applications that they want to do and",
    "start": "289129",
    "end": "294330"
  },
  {
    "text": "some of these are not really trivial to be doing with the bottleneck of a single box let's take a look at what the",
    "start": "294330",
    "end": "300270"
  },
  {
    "text": "problem at its core looks like the problem at its core as we analyze further is that we were still using systems like redshift and elasticsearch",
    "start": "300270",
    "end": "306509"
  },
  {
    "text": "which are great at front but as the problem with these systems is that the compute and storage you have in these",
    "start": "306509",
    "end": "313590"
  },
  {
    "text": "systems it's kind of very closely coupled together so compute would increase when your applications your",
    "start": "313590",
    "end": "318990"
  },
  {
    "text": "queries are increasing which is your internal data users increasing storage would increase as you start offering",
    "start": "318990",
    "end": "324090"
  },
  {
    "text": "more and more services you start getting more and more users the problem is that like even though only one of these",
    "start": "324090",
    "end": "329669"
  },
  {
    "text": "things would I would usually increase at one point in time we would have to scale the system up to handle large amounts of",
    "start": "329669",
    "end": "335819"
  },
  {
    "text": "load on both of these axes which becomes really per bliley expensive and we saw really big slowdowns among our redshift",
    "start": "335819",
    "end": "341819"
  },
  {
    "text": "and elasticsearch clusters because they were having trouble operating at that scale as well Robin Hood is a company",
    "start": "341819",
    "end": "348270"
  },
  {
    "start": "346000",
    "end": "419000"
  },
  {
    "text": "that is largely driven by the raid team it's a focal point within the company and a lot of different teams go through with us an example you can see on this",
    "start": "348270",
    "end": "354870"
  },
  {
    "text": "particular slide we work a lot with the compliance team which works in turn with a lot of regulatory agencies within the",
    "start": "354870",
    "end": "361199"
  },
  {
    "text": "finance industry since we work in a real highly regulated space they need to find",
    "start": "361199",
    "end": "366539"
  },
  {
    "text": "out a lot of information whenever such regulatory requests come in and access our data stores to get this information",
    "start": "366539",
    "end": "372180"
  },
  {
    "text": "to regulatory authorities as soon as possible we also work with the brokerage team which needs access to a lot of trade",
    "start": "372180",
    "end": "378810"
  },
  {
    "text": "data they also need access to a lot of codes data to see what the price was at certain point in time and we have",
    "start": "378810",
    "end": "385319"
  },
  {
    "text": "accounting and business teams basically that would be using a lot of our financial data that we are storing",
    "start": "385319",
    "end": "390569"
  },
  {
    "text": "in-house and need to work on top of this currently the system is that all of these requests go through data",
    "start": "390569",
    "end": "396449"
  },
  {
    "text": "scientists which end up becoming a bottleneck for all of these people doing their to day jobs which is not ideal and it's",
    "start": "396449",
    "end": "403020"
  },
  {
    "text": "really not trivial to ramp up these non-technical users onto our systems since there is actually a lot of",
    "start": "403020",
    "end": "408150"
  },
  {
    "text": "technical knowledge involved into going on a lot of these systems pulling the information down understanding the",
    "start": "408150",
    "end": "414300"
  },
  {
    "text": "dialects and using complex mechanisms to achieve what they want at the end of the day another thing that we always want to",
    "start": "414300",
    "end": "421350"
  },
  {
    "start": "419000",
    "end": "490000"
  },
  {
    "text": "keep we want to strive to keep getting better at is better privacy controls better security in-house and making sure",
    "start": "421350",
    "end": "427260"
  },
  {
    "text": "that the data is accessed by exactly who needs the data previously we had a system where we had a single research",
    "start": "427260",
    "end": "432900"
  },
  {
    "text": "environment and this research environment basically had access to the systems we wanted to access the data",
    "start": "432900",
    "end": "438690"
  },
  {
    "text": "from for example if there's a Postgres instance that we would want access to this system would be able to talk to",
    "start": "438690",
    "end": "444000"
  },
  {
    "text": "that the problem is with that with this particular instance is that there would be very coarse-grained access controls",
    "start": "444000",
    "end": "450030"
  },
  {
    "text": "so you can decide whether or not to give access to a particular system but as soon as you start diving into table",
    "start": "450030",
    "end": "455220"
  },
  {
    "text": "level column level access that gets harder and harder for the security and the development operational teams to",
    "start": "455220",
    "end": "460980"
  },
  {
    "text": "keep track of and maintain as we move forward another issue that we saw over there is that as people log on to these",
    "start": "460980",
    "end": "467010"
  },
  {
    "text": "boxes and make queries the queries essentially get masked by the research system itself so when we see on the data",
    "start": "467010",
    "end": "473130"
  },
  {
    "text": "stored we saw that these queries were run by the research system and not by the individual user who ran these while",
    "start": "473130",
    "end": "478830"
  },
  {
    "text": "it's still possible to tie these two together by looking on the logs on the box this is something that's not trivial",
    "start": "478830",
    "end": "484440"
  },
  {
    "text": "to do and it's really cumbersome to do on an ad-hoc basis for our security and devops teams",
    "start": "484440",
    "end": "490040"
  },
  {
    "start": "490000",
    "end": "534000"
  },
  {
    "text": "so to summarize there's a set of problems that we want to do that kind of set us out on this path to look for our",
    "start": "490040",
    "end": "496200"
  },
  {
    "text": "data like solution one of these was that data was really fragmented across multiple systems",
    "start": "496200",
    "end": "501540"
  },
  {
    "text": "there were computer limitations whenever we were working with large amounts of data and as these data set scaled these",
    "start": "501540",
    "end": "506550"
  },
  {
    "text": "provinces kept on getting bigger and bigger and at the root of that was the issue that scaling this infrastructure",
    "start": "506550",
    "end": "511800"
  },
  {
    "text": "the way it was set up initially was becoming prohibitively expensive and was also becoming becoming inefficient so we",
    "start": "511800",
    "end": "517680"
  },
  {
    "text": "were paying exponentially more for the service that we were getting in a linear space we also wanted to get finer",
    "start": "517680",
    "end": "523500"
  },
  {
    "text": "grained ACS that were not only easy to set up but easy to manage as we moved forward as we created new divisions",
    "start": "523500",
    "end": "529260"
  },
  {
    "text": "within the company and created new access levels that we wanted for our different users so the deadly catalogue robber nerd",
    "start": "529260",
    "end": "536430"
  },
  {
    "start": "534000",
    "end": "606000"
  },
  {
    "text": "solves a lot of this a lot of these problems to varying degrees it solves a lot a lot of these problems in the sense",
    "start": "536430",
    "end": "542309"
  },
  {
    "text": "that we can now process large volumes of data in a distributed fashion by using distributed computation frameworks",
    "start": "542309",
    "end": "548360"
  },
  {
    "text": "allowing these different distributed computation frameworks to work on our data gets rid of the vertical scaling",
    "start": "548360",
    "end": "554759"
  },
  {
    "text": "bottlenecks that we were facing before now if we want to process more data we basically just add more boxes to a",
    "start": "554759",
    "end": "560430"
  },
  {
    "text": "cluster of community hardware that we're just using which makes this problem a lot more tractable we're also now",
    "start": "560430",
    "end": "566339"
  },
  {
    "text": "providing a unified query layer to our users what this means that the users can now just straightaway access the data",
    "start": "566339",
    "end": "572129"
  },
  {
    "text": "that we're providing in the d-league they don't need to worry about whether this data is coming from postcards they don't need to date worry about whether",
    "start": "572129",
    "end": "577740"
  },
  {
    "text": "this data is coming from elasticsearch or some other instance store all of that is abstracted away and for users that even knew this there's a lot of",
    "start": "577740",
    "end": "584220"
  },
  {
    "text": "repeating work that we just took away from them we said rely on us will provide this for you you only worry",
    "start": "584220",
    "end": "589379"
  },
  {
    "text": "about the analysis part since that's what primarily their job is this also allows our security and development",
    "start": "589379",
    "end": "595949"
  },
  {
    "text": "operational teams to manage our ACLs at a very finer grained level allows us to isolate on a query level to see who",
    "start": "595949",
    "end": "602699"
  },
  {
    "text": "exactly ran this query and gives us better tracking in that sense so let's start looking into how we went about",
    "start": "602699",
    "end": "609179"
  },
  {
    "start": "606000",
    "end": "728000"
  },
  {
    "text": "solving these problems so before we actually started writing any code before we started just ramping up towards like",
    "start": "609179",
    "end": "616290"
  },
  {
    "text": "how we want to build this we actually started out doing some research we were quite sure that we're not the first company to face problems like these and",
    "start": "616290",
    "end": "622889"
  },
  {
    "text": "so we wanted to take some inspiration from bigger and more respected companies in the tech companies to see how they",
    "start": "622889",
    "end": "628769"
  },
  {
    "text": "had tackled these particular problems and if we could take some cues from over there and come to a good solution for",
    "start": "628769",
    "end": "633809"
  },
  {
    "text": "our company as well so you can see a lot of big names in the tech space over here Netflix for example use press to do",
    "start": "633809",
    "end": "640230"
  },
  {
    "text": "query over and store terabytes and petabytes of data which was really inspiring to see uber for example use",
    "start": "640230",
    "end": "646290"
  },
  {
    "text": "Parque at a really grandiose scale they contributed a lot back to the open source community as part of their",
    "start": "646290",
    "end": "652050"
  },
  {
    "text": "exploration on this project FINRA is a company we closely work with and has similar workloads to ours since they're",
    "start": "652050",
    "end": "658170"
  },
  {
    "text": "also in the finance phase so it was really interesting to see how they were able to use a day late to solve some of",
    "start": "658170",
    "end": "663629"
  },
  {
    "text": "their problems over here as well then we went and looked at a lot of AWS talk commentation on how you we could build",
    "start": "663629",
    "end": "669970"
  },
  {
    "text": "solutions like add a lake using managed and serverless solutions that AWS provided this was really important for",
    "start": "669970",
    "end": "676250"
  },
  {
    "text": "us at this point in time because when we were starting out on this journey we were really unsure whether this was the right path there were several solutions",
    "start": "676250",
    "end": "682520"
  },
  {
    "text": "we were comparing and contrasting within data Lake was one of them we really wanted to be sure that this was one of",
    "start": "682520",
    "end": "688550"
  },
  {
    "text": "the things that we wanted to do and in order to do that we didn't want to spend a lot of time doing this since since",
    "start": "688550",
    "end": "694190"
  },
  {
    "text": "most of us are working at startups and all of us can empathize or resources to work on a single project are quite",
    "start": "694190",
    "end": "699710"
  },
  {
    "text": "limited we didn't have a lot of Hadoop expertise in-house so we wanted to get off the bat quickly with like minimal",
    "start": "699710",
    "end": "706160"
  },
  {
    "text": "overhead as much as possible and AWS management several solutions provided this for us we didn't have to set up our",
    "start": "706160",
    "end": "712100"
  },
  {
    "text": "own clusters we didn't have to deal with version conflicts we didn't have to deal with a lot of overhead that comes with",
    "start": "712100",
    "end": "718520"
  },
  {
    "text": "maintaining and moving forward these with these technologies as time goes on so that was a good starting point for us",
    "start": "718520",
    "end": "723530"
  },
  {
    "text": "to try and test how things would work and how things would look in an ideal scenario so this is what our",
    "start": "723530",
    "end": "729740"
  },
  {
    "start": "728000",
    "end": "793000"
  },
  {
    "text": "architecture looks at this point in time on a very high level it's very important to note at this point in time that there",
    "start": "729740",
    "end": "735890"
  },
  {
    "text": "is no one data lake fits all solution over here trust me we looked and if there was a simple solution we would",
    "start": "735890",
    "end": "741740"
  },
  {
    "text": "have probably gone with that instead of like building full fledged architecture but the simple truth of the matter as",
    "start": "741740",
    "end": "747740"
  },
  {
    "text": "most of you already know there is no one way to solve such a complex problem the",
    "start": "747740",
    "end": "752750"
  },
  {
    "text": "truth of the matter is that like each of us at our companies work with the different set of systems there's a different set of problems we want to",
    "start": "752750",
    "end": "758750"
  },
  {
    "text": "solve and there's a different set of clients we want to serve and so there will be different solutions that people will come up for different companies so",
    "start": "758750",
    "end": "766250"
  },
  {
    "text": "we have six layers in our architecture over here we start with the ingestion layer we take in a lot of data we store",
    "start": "766250",
    "end": "771440"
  },
  {
    "text": "it in the storage layer we then do some processing ETL jobs on top of that in the processing layer we allow this data",
    "start": "771440",
    "end": "777620"
  },
  {
    "text": "then to be queried using the query layer a couple of tools in there we then do some validation on top of this framework",
    "start": "777620",
    "end": "783410"
  },
  {
    "text": "and then we allow the user-facing layer which is the layer or most of our users interact with to actually go into and",
    "start": "783410",
    "end": "789800"
  },
  {
    "text": "look and look at this data and interact with it let's start with the ingestion",
    "start": "789800",
    "end": "795050"
  },
  {
    "text": "layer so before we build the ingestion layer we set on on this journey to understand what the ingestion patterns are draw up in order the data",
    "start": "795050",
    "end": "801230"
  },
  {
    "text": "patterns are dropping hard work we saw that they were categorized into two major categories one of these systems",
    "start": "801230",
    "end": "807380"
  },
  {
    "text": "was a streaming framework essentially we leveraged Kafka very highly of the",
    "start": "807380",
    "end": "813800"
  },
  {
    "text": "company and it's a system that we use as a buffer between our different services every service that is computing",
    "start": "813800",
    "end": "819860"
  },
  {
    "text": "something will basically output a Kafka log and other services that are downstream of that service will consume",
    "start": "819860",
    "end": "825530"
  },
  {
    "text": "that and operate on top of that since we saw several such use cases we even built an in-house computation framework on top",
    "start": "825530",
    "end": "831950"
  },
  {
    "text": "of streams which we call Faust and is open source and if you're leveraging Python and Kafka a highly at your",
    "start": "831950",
    "end": "837380"
  },
  {
    "text": "company I do recommend taking a look at that something we had set up long ago because we highly use Kafka our company is a",
    "start": "837380",
    "end": "844160"
  },
  {
    "text": "sequel pipeline seeker is an open source library offered by Pinterest that allows us to archive or kafka topics into s3 in",
    "start": "844160",
    "end": "851840"
  },
  {
    "text": "a reliable manner it gives us some really good guarantees that we were really happy with and this is the pipeline we had set up long ago",
    "start": "851840",
    "end": "858220"
  },
  {
    "text": "another major way of getting the data into the lake is through bad jobs we have several bad jobs running that fetch",
    "start": "858220",
    "end": "864740"
  },
  {
    "text": "data from external integrations like adjust which we work with for our as datasets Zendesk which we work with for our",
    "start": "864740",
    "end": "870890"
  },
  {
    "text": "customer support data sets and then there like some use cases that come up with the daily leg itself like our",
    "start": "870890",
    "end": "876190"
  },
  {
    "text": "already a snapshots that we take and put into the link and all of these at the",
    "start": "876190",
    "end": "881360"
  },
  {
    "text": "end of the day are stored in our storage layer so let's take a closer look at what that looks like at this point in",
    "start": "881360",
    "end": "887720"
  },
  {
    "start": "884000",
    "end": "953000"
  },
  {
    "text": "time we chose Amazon s3 to be our storage layer simply because it's simple it's in the name and another issue of",
    "start": "887720",
    "end": "893900"
  },
  {
    "text": "our another issue that we wanted to resolve by using s3 is that other alternatives we saw in this space which",
    "start": "893900",
    "end": "899840"
  },
  {
    "text": "were like HDFS again I really hard to get off the ground with when you have a really small team working on such a big",
    "start": "899840",
    "end": "905450"
  },
  {
    "text": "problem s3 also builds really seamlessly on top of all the management server solutions",
    "start": "905450",
    "end": "910460"
  },
  {
    "text": "that we saw that AWS was offering and that we could leverage to build our data like the way it works right now is that",
    "start": "910460",
    "end": "917450"
  },
  {
    "text": "we have an ingestion layer that basically puts data into this s3 bucket we have into a raw path once data is put",
    "start": "917450",
    "end": "924050"
  },
  {
    "text": "into this raw path with it then interacts with the processing layer and puts data back into a colorized format",
    "start": "924050",
    "end": "930110"
  },
  {
    "text": "for large and immutable data sets there are certain use cases where we ask want to work with mutable and hard data",
    "start": "930110",
    "end": "937120"
  },
  {
    "text": "what I mean by hard data over here is data that is accessed very frequently by a lot of different services by a lot of",
    "start": "937120",
    "end": "943720"
  },
  {
    "text": "different dashboarding tools that we want to build on top of the lake so that data we prefer storing into redshift to",
    "start": "943720",
    "end": "949810"
  },
  {
    "text": "get sub-second latencies on top of that data before we set out on doing this we",
    "start": "949810",
    "end": "956200"
  },
  {
    "start": "953000",
    "end": "1062000"
  },
  {
    "text": "actually want to do some back of the envelope calculations to see how much better this would be than our current",
    "start": "956200",
    "end": "961510"
  },
  {
    "text": "solution if at all so the one test case we kind of played around with is that if we were to assume that our events data",
    "start": "961510",
    "end": "967930"
  },
  {
    "text": "was seventy-five terabyte compressed using park' and snappy we found out that if we were to store this data into s3 it",
    "start": "967930",
    "end": "974829"
  },
  {
    "text": "would cost us about 21 grand per year and if we were destroyed the same data into redshift and just scale up the",
    "start": "974829",
    "end": "981459"
  },
  {
    "text": "cluster to whatever size we needed it would cost about 1.2 million dollars even though this looks like a really big",
    "start": "981459",
    "end": "987730"
  },
  {
    "text": "number let's not discount redshift right away because redshift allows you to cry over this data as much as you want while",
    "start": "987730",
    "end": "993220"
  },
  {
    "text": "s3 you cannot just create the data directly you need some kind of tooling on top of that we provide that tooling",
    "start": "993220",
    "end": "998589"
  },
  {
    "text": "to our customers using Amazon Athena Athena has a cost in itself we saw that",
    "start": "998589",
    "end": "1003810"
  },
  {
    "text": "like it were it costed $5 per terabyte to scan over data that was that it used in order to answer the questions you",
    "start": "1003810",
    "end": "1010470"
  },
  {
    "text": "wanted so if we were to I will get these two together and assume that like every person were to query someone ungodly mad",
    "start": "1010470",
    "end": "1016860"
  },
  {
    "text": "like 1% of the entire mass dataset which they usually don't because it's a really huge data set of spanning over several",
    "start": "1016860",
    "end": "1022290"
  },
  {
    "text": "years but even if we were to assume something like this we saw that like people had to make close to a quarter",
    "start": "1022290",
    "end": "1027808"
  },
  {
    "text": "million queries per year in order to get close to this amount we were paying at redshift since we are a very small data",
    "start": "1027809",
    "end": "1036030"
  },
  {
    "text": "team we saw that we were at a very small scale compared to this and this would be a really good place to start off at and",
    "start": "1036030",
    "end": "1041910"
  },
  {
    "text": "if we found that we were getting closer to this level we could move to more complex on more streamlined solutions",
    "start": "1041910",
    "end": "1047880"
  },
  {
    "text": "exactly for this need because then we would have a demonstrated use case for this to show to our engineering leaders we could move to solutions like presto",
    "start": "1047880",
    "end": "1055559"
  },
  {
    "text": "on EMR which would reduce our cause in allow us more flexibility once we were sure that this was the right way to go",
    "start": "1055559",
    "end": "1061050"
  },
  {
    "text": "about it let's talk about the processing layer that interacts with the storage layer and the query layer this is where",
    "start": "1061050",
    "end": "1067290"
  },
  {
    "start": "1062000",
    "end": "1379000"
  },
  {
    "text": "a lot of the magic happens we are really lucky since we already had airflow as our work flow composer and",
    "start": "1067290",
    "end": "1073350"
  },
  {
    "text": "scheduler at Robin Hood several several years ago before we started on this journey airflow gives you a lot of neat",
    "start": "1073350",
    "end": "1080010"
  },
  {
    "text": "features that we really appreciated if you're just a eunuch show up running cron jobs I understand that eunuchs is",
    "start": "1080010",
    "end": "1085620"
  },
  {
    "text": "really streamlined it's really easy to use but there are some things that it uh that airflow doesn't offer in its manual",
    "start": "1085620",
    "end": "1091800"
  },
  {
    "text": "that you kind of learn later on down the road things like when your jobs fail finding out about these job failures we running",
    "start": "1091800",
    "end": "1098670"
  },
  {
    "text": "these jobs is a very manual and cumbersome process you SSH on to the box and then do something over there this",
    "start": "1098670",
    "end": "1104760"
  },
  {
    "text": "increases operational overhead and since we are a very small team we want to go on and build as many things as possible as many cool features as possible for",
    "start": "1104760",
    "end": "1111870"
  },
  {
    "text": "users and this kind of held us back so having air flow kind of solved a lot of these problems because airflow has",
    "start": "1111870",
    "end": "1116910"
  },
  {
    "text": "built-in retry logic it also allows for things like exponential back-off so that you're not overwhelming your upstream",
    "start": "1116910",
    "end": "1123330"
  },
  {
    "text": "services and are making sure that you're giving them in our first time between clearing again and again it also offers",
    "start": "1123330",
    "end": "1129630"
  },
  {
    "text": "a way to define dependency between different directed acyclic graph jobs that you might have it also offers a lot",
    "start": "1129630",
    "end": "1135990"
  },
  {
    "text": "of flexibility in this sense because basically you define your workflow using something like Python and it's really",
    "start": "1135990",
    "end": "1142230"
  },
  {
    "text": "extensible at that point in time because what you're limited by is basically what Python can do it's also really cool that",
    "start": "1142230",
    "end": "1149160"
  },
  {
    "text": "there's a huge community building and developing on top of these tools and releasing these every month and so that",
    "start": "1149160",
    "end": "1155310"
  },
  {
    "text": "allows us to build on top of the operators and connectors that are already available out there and just",
    "start": "1155310",
    "end": "1161760"
  },
  {
    "text": "build this project on the shoulder of giants instead of reinventing the wheel from scratch every time we need to do something new the usual way this",
    "start": "1161760",
    "end": "1169710"
  },
  {
    "text": "workflow would work for us at Robin Hood is that first we would go on to the schema discovery step this is one of the",
    "start": "1169710",
    "end": "1174900"
  },
  {
    "text": "first steps that would run as part of a new job for schema discovery we use a tool tool like AWS clue crawlers what a",
    "start": "1174900",
    "end": "1182340"
  },
  {
    "text": "glue collar is is basically a system that scans over your data and tries to find out what the schema of your data",
    "start": "1182340",
    "end": "1188160"
  },
  {
    "text": "set is what this would involve is metadata related to your data store it would see what columns you have where",
    "start": "1188160",
    "end": "1193680"
  },
  {
    "text": "these are stored what partitions you have and basically just find out some neat little details and put them into",
    "start": "1193680",
    "end": "1199860"
  },
  {
    "text": "the glue data catalog and the will later be exploited by the Presto or the Athena layer in order to make more",
    "start": "1199860",
    "end": "1206760"
  },
  {
    "text": "efficient queries on top of your on top of your data then we would go on to the",
    "start": "1206760",
    "end": "1213390"
  },
  {
    "text": "spark layer to do some of the competition over here for our distributed competition we chose spark for several reasons one",
    "start": "1213390",
    "end": "1219840"
  },
  {
    "text": "it's really flexible it's really extensible to build on top of there's two development experiences it provides",
    "start": "1219840",
    "end": "1225900"
  },
  {
    "text": "one is Python and one is Scala and since we run a primarily Python workshop this was right in the wheelhouse of our",
    "start": "1225900",
    "end": "1231930"
  },
  {
    "text": "engineers and there would be minimal overhead in learning this new technology another thing that there's really good",
    "start": "1231930",
    "end": "1238080"
  },
  {
    "text": "over here with SPARC is that it allows distributed computation and is in memory so that means that it's really fast as",
    "start": "1238080",
    "end": "1244020"
  },
  {
    "text": "compared to some of the other solutions we were looking at which would spill to disk every time between different stages of the process and it has really good",
    "start": "1244020",
    "end": "1250710"
  },
  {
    "text": "retry logic if some stage fails in the in the scheduled tasks it had produced",
    "start": "1250710",
    "end": "1256010"
  },
  {
    "text": "there's two ways to leverage spark within AWS ecosystem one of them is AWS",
    "start": "1256010",
    "end": "1261240"
  },
  {
    "text": "klum it's a managed and serverless solution provided by AWS and this is really good because it lets you get off",
    "start": "1261240",
    "end": "1267360"
  },
  {
    "text": "the ground without customizing a lot of things it gives you common-sense defaults and all you need to do is just",
    "start": "1267360",
    "end": "1272580"
  },
  {
    "text": "feed it your spark script and it will do wonders with your data however there's not all use cases that can follow fit",
    "start": "1272580",
    "end": "1279060"
  },
  {
    "text": "into the glue pipeline even though most of our use cases are good with this there's some key use cases where we think that EMR is better suited for our",
    "start": "1279060",
    "end": "1285540"
  },
  {
    "text": "needs use cases that where we want fine grained control where we want executed logs where we want to know how much",
    "start": "1285540",
    "end": "1291570"
  },
  {
    "text": "memory is occupied for for each partition we want to vertically scale-up our instances we want a horizontally",
    "start": "1291570",
    "end": "1297630"
  },
  {
    "text": "scale I want to change the yarn configurations these instances don't come up very often but whenever we're",
    "start": "1297630",
    "end": "1302910"
  },
  {
    "text": "working with really large data cells for example when we were back filling the events data set at Robinhood which was close to a petabyte we thought that EMR",
    "start": "1302910",
    "end": "1310080"
  },
  {
    "text": "was the right choice and we basically used it to work over huge amounts of workload and backfill close to a",
    "start": "1310080",
    "end": "1315450"
  },
  {
    "text": "petabyte of alb log and events data set and robin hood and that kind of feeds",
    "start": "1315450",
    "end": "1320670"
  },
  {
    "text": "back once it has done the processing it feeds back into the storage layer into the same bucket and we run a crawler",
    "start": "1320670",
    "end": "1326280"
  },
  {
    "text": "again over the same data at the end of the day to find out what the final schema for your data set will be this",
    "start": "1326280",
    "end": "1333960"
  },
  {
    "text": "will primarily be same as the raw scheme mind cases but your pulsing job could do some transformations that make it appear",
    "start": "1333960",
    "end": "1339719"
  },
  {
    "text": "different since I also said there's some neat features that the blue-collar will look for your data when it stores it",
    "start": "1339719",
    "end": "1345869"
  },
  {
    "text": "into metadata catalog this includes compression that you're applying on your data and columnar encoding formats like",
    "start": "1345869",
    "end": "1351299"
  },
  {
    "text": "the one we're using like parquet these will all be different and will help the query layer in the future to make better",
    "start": "1351299",
    "end": "1357179"
  },
  {
    "text": "quiz make faster queries on top of your data to enlighten us more on top of that and to discuss our architecture and the",
    "start": "1357179",
    "end": "1362999"
  },
  {
    "text": "next steps are robbing it further I'll pass on to grace now let's come to our",
    "start": "1362999",
    "end": "1380519"
  },
  {
    "start": "1379000",
    "end": "1449000"
  },
  {
    "text": "current layer so our core layer is the layer allows you to query the data store",
    "start": "1380519",
    "end": "1385709"
  },
  {
    "text": "in our storage layer as we mentioned before we have two major datastore in our storage layer so for data in rush if",
    "start": "1385709",
    "end": "1392909"
  },
  {
    "text": "you can terrific queries to rush FCC has building for engine with it and for",
    "start": "1392909",
    "end": "1397950"
  },
  {
    "text": "Delhi s3 we you'd use a double Athena which is a tabs menu serverless",
    "start": "1397950",
    "end": "1403649"
  },
  {
    "text": "presto presto is distributing SQL engine allow you to query the data in where is",
    "start": "1403649",
    "end": "1410399"
  },
  {
    "text": "Thal without moving they'd had to another adult expand her store so like as we mentioned before we use blue color",
    "start": "1410399",
    "end": "1417719"
  },
  {
    "text": "to detect the data schema for the data store s3 and write this method data to",
    "start": "1417719",
    "end": "1423509"
  },
  {
    "text": "look at a log which was hi mekka store on AWS so land I think I can useless metadata",
    "start": "1423509",
    "end": "1432059"
  },
  {
    "text": "information in catalog to query the underlying l3 data directly so it says I've seen a like you try to us by the",
    "start": "1432059",
    "end": "1438779"
  },
  {
    "text": "how much need how you scamper query we also use a colander formal package to",
    "start": "1438779",
    "end": "1444059"
  },
  {
    "text": "optimize the current performance also reduce the data size we need to scan so this is the query layer let's come to",
    "start": "1444059",
    "end": "1451049"
  },
  {
    "start": "1449000",
    "end": "1550000"
  },
  {
    "text": "our validation layer so during the time we are building our data Lake we realize they are lots of silencing their might",
    "start": "1451049",
    "end": "1457619"
  },
  {
    "text": "happening like for example like you might see all your jobs are seated here flow but you",
    "start": "1457619",
    "end": "1462970"
  },
  {
    "text": "ended still ended up with incorrect or incomplete data in your data Lake in might because of some enola upstream",
    "start": "1462970",
    "end": "1470110"
  },
  {
    "text": "failure or just some hidden gems in arrow in your pipeline and this is actually very bad because the only way",
    "start": "1470110",
    "end": "1476680"
  },
  {
    "text": "you can discover it it's at the time you actually try to use it and it's like we also like also during the time we want",
    "start": "1476680",
    "end": "1483250"
  },
  {
    "text": "to build our date on a project within lots of research not only in the successful example industry we also",
    "start": "1483250",
    "end": "1489430"
  },
  {
    "text": "research on how they collect project fill in at the company where you see led the major reason of the datalink project",
    "start": "1489430",
    "end": "1496870"
  },
  {
    "text": "fail is because people lose trust in the data link like for example like people just randomly dumping date hi today",
    "start": "1496870",
    "end": "1503050"
  },
  {
    "text": "today without actually understanding and check the data and the downstream users so realize the audit data in Petaling is",
    "start": "1503050",
    "end": "1510160"
  },
  {
    "text": "not very reliable and the internal users stop using data like so this make this project just become manila's so we are",
    "start": "1510160",
    "end": "1517420"
  },
  {
    "text": "aware that it's very important for us to ensure the data quality before we ship",
    "start": "1517420",
    "end": "1522460"
  },
  {
    "text": "to an announcement service or downstream user so that's why we fill this validation layer to check the data",
    "start": "1522460",
    "end": "1529000"
  },
  {
    "text": "quality so this validation layer is a kind of extensible framework for you to define different kinds of check for each",
    "start": "1529000",
    "end": "1535690"
  },
  {
    "text": "data set so let's connect to our query layer so you it allows you to run a",
    "start": "1535690",
    "end": "1540850"
  },
  {
    "text": "validation query in our underlying a three-day pass l to check is that the if",
    "start": "1540850",
    "end": "1545890"
  },
  {
    "text": "date has to correct all the integrator is good so let's take a look for self",
    "start": "1545890",
    "end": "1551740"
  },
  {
    "start": "1550000",
    "end": "1612000"
  },
  {
    "text": "come on basically check we have for our dataset like for example we have reco",
    "start": "1551740",
    "end": "1556840"
  },
  {
    "text": "count and now come for example lag is if you have like 1 million events per day",
    "start": "1556840",
    "end": "1562600"
  },
  {
    "text": "usually but today you do see 20k events coming it's definitely the worst you to explore if little players and is enroll",
    "start": "1562600",
    "end": "1569559"
  },
  {
    "text": "in the pipeline and also the partition if the underlying data is partitioned data or the partition table you expect",
    "start": "1569559",
    "end": "1576640"
  },
  {
    "text": "to see new partition landing after today's pipeline finish so you may also want to check that and also the",
    "start": "1576640",
    "end": "1582490"
  },
  {
    "text": "similarity existing value and unique sometimes sound : in some day that said",
    "start": "1582490",
    "end": "1587880"
  },
  {
    "text": "should be distinct and unique for example like the user your ID so your mouth so want to verify it a paddleboat",
    "start": "1587880",
    "end": "1593970"
  },
  {
    "text": "of less common checks we also allow you to do some user-defined checks for you your own dataset and mrs. mark might be",
    "start": "1593970",
    "end": "1601680"
  },
  {
    "text": "better fitting to your own use cases so since we have this validation layer we only ship date her to our downstream use",
    "start": "1601680",
    "end": "1608610"
  },
  {
    "text": "like when although all this validation check pass so let's come to our final layer like",
    "start": "1608610",
    "end": "1615150"
  },
  {
    "start": "1612000",
    "end": "1703000"
  },
  {
    "text": "user-facing layer this is the layer we provide to our internal user for them to access the data and use that they hide",
    "start": "1615150",
    "end": "1621450"
  },
  {
    "text": "the lake one major component of Alice user facing layer is our research environment this research environment is",
    "start": "1621450",
    "end": "1627929"
  },
  {
    "text": "filled with Drupal hub is like bliss as well route to the habala or the user you",
    "start": "1627929",
    "end": "1633210"
  },
  {
    "text": "see like herpes kind of like um server in container and its own like workspace",
    "start": "1633210",
    "end": "1638610"
  },
  {
    "text": "to create and run Lea Python notebook or our notebooks so a list research",
    "start": "1638610",
    "end": "1644400"
  },
  {
    "text": "environment also contact with our query layer so you can like such data from different places just in one notebook",
    "start": "1644400",
    "end": "1650490"
  },
  {
    "text": "and they prefer to do further analysis by your code and this research remember also like connect with github so all the",
    "start": "1650490",
    "end": "1657690"
  },
  {
    "text": "notebooks act like work we are keep track of all the notebooks on all the novel's out kind of version control so",
    "start": "1657690",
    "end": "1664440"
  },
  {
    "text": "this is the use this is the research environment and another major component of user facing layer is our PA analyst",
    "start": "1664440",
    "end": "1672000"
  },
  {
    "text": "wall Wilker so luke race also connect to our query layer it also allows you to",
    "start": "1672000",
    "end": "1677250"
  },
  {
    "text": "run back SQL queries but also lets you build like your own visualization or dashboard in looker and is much more",
    "start": "1677250",
    "end": "1684600"
  },
  {
    "text": "user friendly to our non-technical internal users so let's use a base layer",
    "start": "1684600",
    "end": "1690809"
  },
  {
    "text": "is kind of the final layer on top of all our data Lake systems this is the end of",
    "start": "1690809",
    "end": "1696030"
  },
  {
    "text": "our architecture review let's summarize and what we have achieved validating project and what is our next step so",
    "start": "1696030",
    "end": "1704309"
  },
  {
    "start": "1703000",
    "end": "1744000"
  },
  {
    "text": "let's start with one we have achieve one say like I will highlight like previously as I mentioned you need to",
    "start": "1704309",
    "end": "1710050"
  },
  {
    "text": "jump up in different different servers or they passed all together fetching data but right now you can easily join across",
    "start": "1710050",
    "end": "1716620"
  },
  {
    "text": "multiple data stores just in pictures by one single quarry in a notebook you can see that example here like previously",
    "start": "1716620",
    "end": "1723490"
  },
  {
    "text": "lost on the line theta are in different DPS so it's very hard to join of course the database but right now you can just join",
    "start": "1723490",
    "end": "1730210"
  },
  {
    "text": "them by one single quarry in notebooks like through theta like with is they",
    "start": "1730210",
    "end": "1735460"
  },
  {
    "text": "have misses the RDS snapshot we have and so this is where highly boosts up the",
    "start": "1735460",
    "end": "1740530"
  },
  {
    "text": "productivity of our engineers and data centers and the second thing I wanna",
    "start": "1740530",
    "end": "1745840"
  },
  {
    "start": "1744000",
    "end": "1792000"
  },
  {
    "text": "mention is the computer and storage are now decoupled not right now our very large data set are lying or s3 and",
    "start": "1745840",
    "end": "1753400"
  },
  {
    "text": "apatow we are running our porridge to Hasina so we pay for the computer storage separately unlike before where",
    "start": "1753400",
    "end": "1760780"
  },
  {
    "text": "we just use restive and ill Assessors we have to keep scale up our instant type like why our data quotes regardless like",
    "start": "1760780",
    "end": "1768400"
  },
  {
    "text": "we need to use the computing power of this box or not so this is quite beneficial for our futures going you can",
    "start": "1768400",
    "end": "1775480"
  },
  {
    "text": "see the graph here so we when we want to scale scale after our computes like we usually see the see the race of like the",
    "start": "1775480",
    "end": "1783280"
  },
  {
    "text": "application increase or the poor increase and also we can just scale up our storage we're with our use of base",
    "start": "1783280",
    "end": "1789160"
  },
  {
    "text": "growth and the data volume grows analysis like we now have a future rich",
    "start": "1789160",
    "end": "1795610"
  },
  {
    "text": "research environment and also very good - moaning - oh look err so this kind of",
    "start": "1795610",
    "end": "1800770"
  },
  {
    "text": "like make all the data in our lake like become very useful for our internal users now they'd have scientists are",
    "start": "1800770",
    "end": "1807580"
  },
  {
    "text": "happy because we make layer work easier and also our marketing and business and PM's also happy because they can easily",
    "start": "1807580",
    "end": "1814510"
  },
  {
    "text": "visualize the matrix and and layer like kind of the performance of related to",
    "start": "1814510",
    "end": "1820330"
  },
  {
    "text": "layer job Duty very easy so to summarize like what problem we",
    "start": "1820330",
    "end": "1826340"
  },
  {
    "start": "1823000",
    "end": "1934000"
  },
  {
    "text": "have solved by outdated a project there are several things I want to highlight here the first way as we just mentioned",
    "start": "1826340",
    "end": "1832730"
  },
  {
    "text": "like previous date has gotten everywhere but right now we have a unified interface for various type of data and",
    "start": "1832730",
    "end": "1838760"
  },
  {
    "text": "the second thing is like the pair has previously required lots of technical knowledge to get the data and walk",
    "start": "1838760",
    "end": "1844490"
  },
  {
    "text": "around so all the data requests need to go through our data centers to get the result but right now like a group no",
    "start": "1844490",
    "end": "1850220"
  },
  {
    "text": "matter you're come fro like a me go team accounting team of financing like it's very easy for your tutors access the",
    "start": "1850220",
    "end": "1856460"
  },
  {
    "text": "data and find also your own and the third problem is previously we have computed limitation for very big dataset",
    "start": "1856460",
    "end": "1863450"
  },
  {
    "text": "for example like law we have lots of like computing worker boxes but still like one single jobs just running one",
    "start": "1863450",
    "end": "1870680"
  },
  {
    "text": "particular box and also like where we want to fetch our data back and data order for around through a single ray",
    "start": "1870680",
    "end": "1876800"
  },
  {
    "text": "replica but now we have leveraging distributed computing for all data processing process like we have the like",
    "start": "1876800",
    "end": "1884690"
  },
  {
    "text": "batch job runs through spark cluster we have Korematsu presto so this is will like benefit our future scaling and also",
    "start": "1884690",
    "end": "1892040"
  },
  {
    "text": "better for our cost control and the first thing like as they also as I just mentioned the storage and computing",
    "start": "1892040",
    "end": "1897980"
  },
  {
    "text": "couple is also for our future scalability and the last thing is like right now we have like much more",
    "start": "1897980",
    "end": "1903440"
  },
  {
    "text": "fine-grained access control so I am Rose because of all your user identity you",
    "start": "1903440",
    "end": "1909170"
  },
  {
    "text": "use in research environment is attached with your AWS I am rope so what did how",
    "start": "1909170",
    "end": "1914480"
  },
  {
    "text": "you can access them how can you access this data can all be controlled through the I am royal permission configuration",
    "start": "1914480",
    "end": "1921410"
  },
  {
    "text": "and also we eliminate or like hide all the sensitive user VI information from",
    "start": "1921410",
    "end": "1927020"
  },
  {
    "text": "our data Lake so this kind of will remove like most of the security concern from our company Osberg",
    "start": "1927020",
    "end": "1934290"
  },
  {
    "text": "so let's talk about where we are headed so few things we ask you're working always they had like usage checking a",
    "start": "1934290",
    "end": "1941370"
  },
  {
    "text": "monetary yeah something like we want to be aware and like kind of prevent overuse of data like resources for",
    "start": "1941370",
    "end": "1948180"
  },
  {
    "text": "example you might have like overuse of the blue prowler sometimes you do have new schema to detect you don't actually",
    "start": "1948180",
    "end": "1953850"
  },
  {
    "text": "need to run the crawlers and through all the data you can just rather simple command to add partition to your type",
    "start": "1953850",
    "end": "1959910"
  },
  {
    "text": "method in half and this this is one example another is if we might have overuse of DP you maybe use is the",
    "start": "1959910",
    "end": "1966540"
  },
  {
    "text": "computing unit we pay for to run the Spock job in pool so like if you for",
    "start": "1966540",
    "end": "1971730"
  },
  {
    "text": "example if you have a job you can finish in two minutes with just two DP you but you still specified to run this job with",
    "start": "1971730",
    "end": "1978060"
  },
  {
    "text": "30 TPU so it's kind of a big waste and also sometimes people around the oak quarry with that with like doesn't help",
    "start": "1978060",
    "end": "1985140"
  },
  {
    "text": "like Mayo query doesn't have like for I follow the best practice so something has and scoring my skills to like Motyka",
    "start": "1985140",
    "end": "1991410"
  },
  {
    "text": "partitions learn Lane need to scan so this way all like also like increase our cost and increase the query time so",
    "start": "1991410",
    "end": "1999210"
  },
  {
    "text": "these are all the things we want to be aware of and prevent so in order to do this we are still working on building a",
    "start": "1999210",
    "end": "2005360"
  },
  {
    "text": "batter like checking system to do the monitoring and alerting and the second thing says they want to have computing",
    "start": "2005360",
    "end": "2011660"
  },
  {
    "text": "at scale for research environment so basically we want to leverage the distributed computing also in your",
    "start": "2011660",
    "end": "2018200"
  },
  {
    "text": "research environment we want to hook up a computing cluster with research so in the future like Ellis and his or",
    "start": "2018200",
    "end": "2024560"
  },
  {
    "text": "engineers here on complex analysis of machine learning model who is very huge data set like through spark or other",
    "start": "2024560",
    "end": "2031310"
  },
  {
    "text": "distributed computing framework and the last census we all we will always continue working on the performance",
    "start": "2031310",
    "end": "2037880"
  },
  {
    "text": "optimization like one says like we for example will use tableau LM the workload",
    "start": "2037880",
    "end": "2043550"
  },
  {
    "text": "management management to manage the rest of resource allocation to a different group of users for using aggressive for",
    "start": "2043550",
    "end": "2050898"
  },
  {
    "text": "example we have data Santa's group we have bi who group of lists have different",
    "start": "2050899",
    "end": "2056120"
  },
  {
    "text": "configuration after their concurrency level and how much memory they can use what's the best setup here is what we",
    "start": "2056120",
    "end": "2062690"
  },
  {
    "text": "are always learning and keep like updating and also like how to reformat in our data and attritional data to",
    "start": "2062690",
    "end": "2069710"
  },
  {
    "text": "achieve a better career performers with presto is also something we are always learning and keep tuning so luckily I",
    "start": "2069710",
    "end": "2078020"
  },
  {
    "start": "2076000",
    "end": "2163000"
  },
  {
    "text": "want to like introduce some of the project we are currently working away or we plan to work on in the future",
    "start": "2078020",
    "end": "2084470"
  },
  {
    "text": "on top of the data leg for example in experiment Alice's lots of this but",
    "start": "2084470",
    "end": "2089658"
  },
  {
    "text": "experiment Alice's can be automated like through by hooking up the with our of",
    "start": "2089659",
    "end": "2094780"
  },
  {
    "text": "processing layer and the pure layer in data link and also the risk compliance reports right now we are able to like",
    "start": "2094780",
    "end": "2101690"
  },
  {
    "text": "visualize lots of this reporting our pH rose and definitely a lots of ozone layer loss of business and marketing",
    "start": "2101690",
    "end": "2107510"
  },
  {
    "text": "people round layer analysis or billion on - boy and visualization in looker and a platform layer we plan to build our",
    "start": "2107510",
    "end": "2114170"
  },
  {
    "text": "mushroom unify machine learning platform on top of the data Lexus right now is much easier to gather and collect the",
    "start": "2114170",
    "end": "2120830"
  },
  {
    "text": "large volume of different kind of data and similar idea here we want to use the information in theta Lake to build a",
    "start": "2120830",
    "end": "2126950"
  },
  {
    "text": "better fraud detection model in the future so all this like kind of indicator data Lake project become the",
    "start": "2126950",
    "end": "2133040"
  },
  {
    "text": "foundation of all our new like future projects which back pipe they have so I",
    "start": "2133040",
    "end": "2138260"
  },
  {
    "text": "think this is the time the head of the order story about Robin Hood in her lake",
    "start": "2138260",
    "end": "2143540"
  },
  {
    "text": "for now and this is just a starting point for us to tackling the data problem in Robin Hood we have lots of",
    "start": "2143540",
    "end": "2149840"
  },
  {
    "text": "like interesting and exciting opportunity and probably our own map if you feel excited working on this kind of",
    "start": "2149840",
    "end": "2155570"
  },
  {
    "text": "problem feel free to reach out our treatment in your team like always have opening for asthma engineers thank you",
    "start": "2155570",
    "end": "2161780"
  },
  {
    "text": "for listening [Applause]",
    "start": "2161780",
    "end": "2169569"
  },
  {
    "start": "2163000",
    "end": "2182000"
  },
  {
    "text": "we are happy to take any questions you have all right we are gonna take some questions and boy do you have a lot of",
    "start": "2169569",
    "end": "2175910"
  },
  {
    "text": "questions so Michaels first question is how do you automate testing of all of your ETL workflows one of you guys wanna",
    "start": "2175910",
    "end": "2186140"
  },
  {
    "text": "take this sure go for it so I think there's lots of different components for",
    "start": "2186140",
    "end": "2191390"
  },
  {
    "text": "this one part is that we have a central repository where all of our ETL scripts",
    "start": "2191390",
    "end": "2197239"
  },
  {
    "text": "live and it's heavily unit has said we do some integration tests on top of this and once this pipeline if somebody wants",
    "start": "2197239",
    "end": "2203539"
  },
  {
    "text": "to productionize this pipeline we basically have something like develop an endpoint setup for our developers to",
    "start": "2203539",
    "end": "2209239"
  },
  {
    "text": "start looking at the data what this data looks like make sure that like things they want to do or actually tractable we",
    "start": "2209239",
    "end": "2214789"
  },
  {
    "text": "have a staging environment where these scripts are first deployed and we try these out with staging data which is not exactly like real data but basically it",
    "start": "2214789",
    "end": "2222199"
  },
  {
    "text": "gives you an idea of the scale that these scripts would be working in and once we are sure that this works and",
    "start": "2222199",
    "end": "2227449"
  },
  {
    "text": "there's version control on top of these scripts and that if things don't actually end up working we can roll back",
    "start": "2227449",
    "end": "2233329"
  },
  {
    "text": "to an older version then we can actually bump up the version of the script we're using actually in production to make",
    "start": "2233329",
    "end": "2239209"
  },
  {
    "text": "sure it's using the latest script once we are sure that all of these steps are completed and some of these steps are",
    "start": "2239209",
    "end": "2244459"
  },
  {
    "text": "automated by an internal tool we use called Jenkins it's it's an open-source",
    "start": "2244459",
    "end": "2249709"
  },
  {
    "text": "tool that we have set up internally to automate some of our testing pipeline for us how do you deal with complex",
    "start": "2249709",
    "end": "2256789"
  },
  {
    "text": "multi-level JSON or XML data with keys or array lights which vary for example those lovely names after marketing",
    "start": "2256789",
    "end": "2263239"
  },
  {
    "text": "campaigns your intake disgrace we use",
    "start": "2263239",
    "end": "2272539"
  },
  {
    "start": "2264000",
    "end": "2330000"
  },
  {
    "text": "our like storage format it's kind of half schema is also some other nasty structure so it's kind of a handle this",
    "start": "2272539",
    "end": "2279289"
  },
  {
    "text": "case and also we use group crawler it also like can create the schema who switch the polite complex negative",
    "start": "2279289",
    "end": "2286080"
  },
  {
    "text": "so I think this is kind of handle way or allocate having structure what about schema changes the schema is",
    "start": "2286080",
    "end": "2292650"
  },
  {
    "text": "evolved over time how do you deal with them yeah like as I mentioned we use Fowler so what well qualit the past is",
    "start": "2292650",
    "end": "2300630"
  },
  {
    "text": "usually Mexican so the data kind of the data path you specify and we'll create a",
    "start": "2300630",
    "end": "2306150"
  },
  {
    "text": "superset of the schema on top of all your data partitions for example yesterday you have a column called IP",
    "start": "2306150",
    "end": "2312000"
  },
  {
    "text": "today state huh doesn't have a colon call ID but like we get through all the data you see in the past you have the ID",
    "start": "2312000",
    "end": "2318630"
  },
  {
    "text": "field it will still keep the ID column for you but other like nude if you correlate to this data and all the ID",
    "start": "2318630",
    "end": "2324810"
  },
  {
    "text": "like like value in this column will be now that it is kind of like how you handle the scheme evolution I mean your",
    "start": "2324810",
    "end": "2332250"
  },
  {
    "text": "financial company personally identifiable information is a big deal how do you scrub that this whole process right so I can take up that so I think",
    "start": "2332250",
    "end": "2339060"
  },
  {
    "text": "one of the things that we've done really well upstream is to make sure that all of our PII data is actually stored in a",
    "start": "2339060",
    "end": "2344190"
  },
  {
    "text": "single database and does not pollute and it's not funny you know it's not scattered all over the place so we",
    "start": "2344190",
    "end": "2349470"
  },
  {
    "text": "basically just do not take already a snapshot of that particular database so that makes sure that like our downstream",
    "start": "2349470",
    "end": "2355080"
  },
  {
    "text": "pipelines are completely clean another thing that we do in our RDA snapshot face since it's basically an air flow",
    "start": "2355080",
    "end": "2360540"
  },
  {
    "text": "operator that we wrote ourselves you can specify a configuration of what tables or columns you want to ship and don't",
    "start": "2360540",
    "end": "2365820"
  },
  {
    "text": "want to ship so it allows you to easily whitelist and blacklist sins that you want to do keep moving on and I think",
    "start": "2365820",
    "end": "2371070"
  },
  {
    "text": "another thing that can really help with this is just having a really strong review process for whenever like new",
    "start": "2371070",
    "end": "2376320"
  },
  {
    "text": "models are created in your databases to make sure that like we are not putting PII in two places where it should not be",
    "start": "2376320",
    "end": "2382110"
  },
  {
    "text": "going so a strong review process upstream helps us deal with these issues a lot easier downstream why Athena",
    "start": "2382110",
    "end": "2389490"
  },
  {
    "start": "2387000",
    "end": "2446000"
  },
  {
    "text": "over spectrum I can't take this one yeah so one of the big things that we found",
    "start": "2389490",
    "end": "2396270"
  },
  {
    "text": "after doing a lot of research was that presto is a really powerful tool that doesn't just limit you to you know",
    "start": "2396270",
    "end": "2402390"
  },
  {
    "text": "joined data sets across obviously as 3 and redshift and other things in the",
    "start": "2402390",
    "end": "2407400"
  },
  {
    "text": "future but you can also scale to other new connectors because you know Facebook which you know supports presto keeps",
    "start": "2407400",
    "end": "2413340"
  },
  {
    "text": "adding new things so while we do use spectrum occasionally when we really need to join only within redshift NS",
    "start": "2413340",
    "end": "2419460"
  },
  {
    "text": "we do envision a world in the future where we mean we may need to also join across other datasets that we might have",
    "start": "2419460",
    "end": "2425280"
  },
  {
    "text": "in other data stores and presto is is a very robust open-source project we felt",
    "start": "2425280",
    "end": "2431160"
  },
  {
    "text": "like in the future if we were to do our own presto on top of EMR instead of using Athena that might be a more",
    "start": "2431160",
    "end": "2437099"
  },
  {
    "text": "flexible option for us that was everything how do you decide to choose between elastic MapReduce and glue one",
    "start": "2437099",
    "end": "2444180"
  },
  {
    "text": "of the one of the cases for each well",
    "start": "2444180",
    "end": "2450680"
  },
  {
    "start": "2446000",
    "end": "2504000"
  },
  {
    "text": "that's the most yeah yeah I mean stuck a votive yeah so yeah so I think this is",
    "start": "2450680",
    "end": "2457680"
  },
  {
    "text": "one of the things that I kind of like we're done it's really it's hard to kind of get a track of where to use which and",
    "start": "2457680",
    "end": "2463170"
  },
  {
    "text": "this is something that we learned over time so I think glue the way we see it is kind of like the AWS lambda for the",
    "start": "2463170",
    "end": "2468599"
  },
  {
    "text": "spark environment if there's something that you just need to do has minimal overhead this is a simple script that",
    "start": "2468599",
    "end": "2473609"
  },
  {
    "text": "you want to like transform gigabytes of data from one place to another I think glue is the perfect option since",
    "start": "2473609",
    "end": "2478680"
  },
  {
    "text": "you need minimal configuration to do that if there's a really large workload that you're working on top of where you need to fine-tune a lot of things you",
    "start": "2478680",
    "end": "2485099"
  },
  {
    "text": "need to find clean the resources you're using you need to fine-tune how much memory you're using or like a particular instance type that you really like and",
    "start": "2485099",
    "end": "2491760"
  },
  {
    "text": "you want executed logs you want driver logs to see how things are going I think EMI would be a better solution",
    "start": "2491760",
    "end": "2497010"
  },
  {
    "text": "as you start moving into terabytes of data and want some of these like fine-tuning configurations that's that's",
    "start": "2497010",
    "end": "2503220"
  },
  {
    "text": "when do you go for anymore you guys collect a lot of data how much of it you never touch as accurate as you want to",
    "start": "2503220",
    "end": "2509369"
  },
  {
    "start": "2504000",
    "end": "2584000"
  },
  {
    "text": "be you're just roughly yeah so I think we've been pretty good about only building downstream data sets for those",
    "start": "2509369",
    "end": "2517589"
  },
  {
    "text": "use cases that actually come up so the big the cool thing about the data lake as opposed to traditional data warehouse",
    "start": "2517589",
    "end": "2523980"
  },
  {
    "text": "is that even though you may dump a bunch of raw data into your data lake you actually transform and utilize the data",
    "start": "2523980",
    "end": "2530670"
  },
  {
    "text": "only when applications come up so when a new application comes up you still have the raw data you can transform it for that specific use case and make that",
    "start": "2530670",
    "end": "2536790"
  },
  {
    "text": "available so there's obviously datasets that we ship into our s3 buckets like our server logs or ELB logs that are",
    "start": "2536790",
    "end": "2544109"
  },
  {
    "text": "used rather rarely but there are certain use cases for example in the case of ELB logs we do have a use case that we want",
    "start": "2544109",
    "end": "2550829"
  },
  {
    "text": "to monitor the latency is on certain points for that use case we actually looked at that very frequently so while",
    "start": "2550829",
    "end": "2556589"
  },
  {
    "text": "it's true that there's certain datasets where certain aspects of the data are not looked at very frequently for all the downstream views on our data that",
    "start": "2556589",
    "end": "2563849"
  },
  {
    "text": "we've exposed either by Athena or by or looker we only build them when we have a user in place and we can do that because we",
    "start": "2563849",
    "end": "2570390"
  },
  {
    "text": "take the data lake approach which is you know you have this raw data layer and you transform it as you need it as opposed to always thinking about dumping",
    "start": "2570390",
    "end": "2577980"
  },
  {
    "text": "the data in the format that you think it'll be used because you're assuming things down the road which may not actually come to fruition as this",
    "start": "2577980",
    "end": "2583410"
  },
  {
    "text": "question points out why did you choose looker over things like meta base or a periscope data what were the features I",
    "start": "2583410",
    "end": "2589619"
  },
  {
    "start": "2584000",
    "end": "2625000"
  },
  {
    "text": "really chew you to looker I it's a good question I mean there's a lot of it's hard to point out exactly you know",
    "start": "2589619",
    "end": "2596069"
  },
  {
    "text": "there's this one tipping point feature that you know makes you decide one versus the other for us it was you know just talking to a",
    "start": "2596069",
    "end": "2601289"
  },
  {
    "text": "lot of people internally a lot of the people who have worked at Robinhood have some experience with looker in the past",
    "start": "2601289",
    "end": "2606510"
  },
  {
    "text": "they felt pretty good about it it integrated with all the data sources it plugged into everything that we're",
    "start": "2606510",
    "end": "2611940"
  },
  {
    "text": "trying to build with the data Lake so given its flexibility given that you know there were a lot of internal",
    "start": "2611940",
    "end": "2617099"
  },
  {
    "text": "customers were on board with that idea already it seemed like a natural fit for us and basically experience using Airbnb",
    "start": "2617099",
    "end": "2623069"
  },
  {
    "text": "is airflow what are some of the pros and cons right so I can make that up so I",
    "start": "2623069",
    "end": "2628380"
  },
  {
    "text": "think airflow certainly has a lot of pros as I spoke it certainly automates a lot of different things that are really",
    "start": "2628380",
    "end": "2634529"
  },
  {
    "text": "difficult or cumbersome to do manually yourself but I think there definitely some cons with any project like for",
    "start": "2634529",
    "end": "2640109"
  },
  {
    "text": "example some of the things since we're running on a slightly older version of air flow a fourth version we saw some",
    "start": "2640109",
    "end": "2646170"
  },
  {
    "text": "things like that were not intuitive like how the scheduler schedule certain things like it's done in a data science way that's really hard to understand",
    "start": "2646170",
    "end": "2652890"
  },
  {
    "text": "that like it's schedules for 24 hours before whatever your actual workload is if your workload is for a day time",
    "start": "2652890",
    "end": "2658859"
  },
  {
    "text": "there's some logic in our version which actually takes it for granted and is non",
    "start": "2658859",
    "end": "2664680"
  },
  {
    "text": "configurable that any time you create a dag with like an earlier step earlier starter point it'll try to backfill everything in the meantime these are",
    "start": "2664680",
    "end": "2671190"
  },
  {
    "text": "things that are fixed over time and I think the best part about air flow has committed like some other solutions is that all the code is out there the",
    "start": "2671190",
    "end": "2677400"
  },
  {
    "text": "community is really actively developing on top of this so if I do see a problem in air flow at the end of the day it's something I can just write some code to",
    "start": "2677400",
    "end": "2683819"
  },
  {
    "text": "fix so I think that's really configurable that's the thing that sticks out to me at the end of the day every software will have pulse but the extensibility",
    "start": "2683819",
    "end": "2690660"
  },
  {
    "text": "that I can just go and fix it like really quickly I think is the best part about it what happens if someone comes",
    "start": "2690660",
    "end": "2696060"
  },
  {
    "text": "to you and they want access to data which hasn't made it into the data like how do you and it's not available looker what do you do I think I think there's a",
    "start": "2696060",
    "end": "2703500"
  },
  {
    "text": "whole process that goes about that we have to go through we want to understand the exact use case they want to be using do they want to make adult queries",
    "start": "2703500",
    "end": "2709680"
  },
  {
    "text": "through the research environment or is this something that they just want to visualize through look and based on that we will basically",
    "start": "2709680",
    "end": "2714750"
  },
  {
    "text": "design a workflow in that first we will see where this data is actually coming from is it a patch or streaming use case",
    "start": "2714750",
    "end": "2719940"
  },
  {
    "text": "we'll start dumping that through air flow or seek or whatever the use case be respectively",
    "start": "2719940",
    "end": "2724980"
  },
  {
    "text": "once that is in the data Lake we will basically just create a dag in air flow that orchestrates all of this workflow",
    "start": "2724980",
    "end": "2730830"
  },
  {
    "text": "it runs the crawlers or runs the ad partition commands and basically then goes on and runs the spark job so",
    "start": "2730830",
    "end": "2736920"
  },
  {
    "text": "somebody has to go in and write a custom spark job for this particular dataset based on how the users wanted and the",
    "start": "2736920",
    "end": "2742860"
  },
  {
    "text": "end of the day right now a lot of the brunt of this work is borne by the data team but we have a hope that we can make",
    "start": "2742860",
    "end": "2749610"
  },
  {
    "text": "this development environment really flexible and really easy to use such that engineers across the company",
    "start": "2749610",
    "end": "2754740"
  },
  {
    "text": "anytime they want access to some data if they're technical users they can do it themselves so we've had to abstract away a lot of",
    "start": "2754740",
    "end": "2760200"
  },
  {
    "text": "like these common-sense things like adding partitions or discovering new data using crawlers you just turn these",
    "start": "2760200",
    "end": "2766200"
  },
  {
    "text": "into air flow operators that people can easily use in their job you want your QA",
    "start": "2766200",
    "end": "2771930"
  },
  {
    "start": "2770000",
    "end": "2835000"
  },
  {
    "text": "and staging environments to match production so how do you replicate this data the volume is data down to lower environments so what's your strategy",
    "start": "2771930",
    "end": "2777900"
  },
  {
    "text": "yeah I mean I think I think it's very difficult you don't want to be duplicating all the data downstream",
    "start": "2777900",
    "end": "2783090"
  },
  {
    "text": "because then you're paying twice the amount because you're storing to data again and again right so what we try to do is you know we've you've leveraged",
    "start": "2783090",
    "end": "2789270"
  },
  {
    "text": "things like the glue development endpoint that allows developers to kind of talk a little bit to sanitize",
    "start": "2789270",
    "end": "2795570"
  },
  {
    "text": "production data so they can estimate the scale of the operation going on then they can test at a much lower volume of",
    "start": "2795570",
    "end": "2801150"
  },
  {
    "text": "data in the queuing staging environment so we don't replicate the exact problem we have a much lower volume but then",
    "start": "2801150",
    "end": "2806220"
  },
  {
    "text": "they know that the logic is going to be correct right there's some things that you only know when it hits prod for example you know if you're looking at",
    "start": "2806220",
    "end": "2812520"
  },
  {
    "text": "the events data set and you're you want to operate over terabytes and terabytes of data the exact runtime of your job and the exact",
    "start": "2812520",
    "end": "2818460"
  },
  {
    "text": "implication of the data use that result as a result of a spark job will only really be visible at that",
    "start": "2818460",
    "end": "2824010"
  },
  {
    "text": "scale so yes there's definitely cases where you can't replicate everything you know at the lower scales but most of the",
    "start": "2824010",
    "end": "2830010"
  },
  {
    "text": "logic issues most of the issues related to bad structure of your code etc can be found out at the QN staging later can",
    "start": "2830010",
    "end": "2836819"
  },
  {
    "start": "2835000",
    "end": "2911000"
  },
  {
    "text": "you share some of the complexities of transformations in spark and do you have any runtime benchmark comparison numbers",
    "start": "2836819",
    "end": "2841890"
  },
  {
    "text": "when you're comparing it to redshift so we use spark and redshift for different",
    "start": "2841890",
    "end": "2847289"
  },
  {
    "text": "things here so I'm gonna try to answer the question as I understand it even though this is not exactly how we look",
    "start": "2847289",
    "end": "2853920"
  },
  {
    "text": "at it so spark is used to handle data transformations in ETL while redshift is",
    "start": "2853920",
    "end": "2859740"
  },
  {
    "text": "once we have done these transformation that's there for analytics or reporting workloads so we look at it more from the",
    "start": "2859740",
    "end": "2865500"
  },
  {
    "text": "querying perspective so an equivalent there for us would be something like Athena and what we've noticed is that",
    "start": "2865500",
    "end": "2871260"
  },
  {
    "text": "while you know redshift is great it's really fast you obviously paying for the scanning",
    "start": "2871260",
    "end": "2876420"
  },
  {
    "text": "compute with the storage so you kind of have to keep both you know moving up in lockstep with each other so for a lot of",
    "start": "2876420",
    "end": "2882569"
  },
  {
    "text": "use cases where people really need to ask a lot of questions really frequently or reporting workflow thing to need to have a very tight SLA that ship is great",
    "start": "2882569",
    "end": "2888960"
  },
  {
    "text": "you're getting value for that that cost but cases where you know it's more ad hoc we're willing to slow down our",
    "start": "2888960",
    "end": "2894690"
  },
  {
    "text": "queries a little bit to kind of separate that computer and storage that we talked about and they're attina even though the",
    "start": "2894690",
    "end": "2900779"
  },
  {
    "text": "query performance itself is slightly slower if you look at both cost and query performance as benchmarks that you care about",
    "start": "2900779",
    "end": "2906299"
  },
  {
    "text": "it still works out to be better you know we're near the answer question so here",
    "start": "2906299",
    "end": "2911819"
  },
  {
    "start": "2911000",
    "end": "2947000"
  },
  {
    "text": "we go how do you manage your EMR clusters or the transient or persistent right so I think I've worked on this",
    "start": "2911819",
    "end": "2917460"
  },
  {
    "text": "part so I can answer that so I think since our use cases for EMR right now are not consistent this is usually used",
    "start": "2917460",
    "end": "2923460"
  },
  {
    "text": "for backfill purposes so these are just transient clusters that we bring up with the exact configuration that we think we",
    "start": "2923460",
    "end": "2929190"
  },
  {
    "text": "need for a particular use case so right now these clusters transient but I think like we're getting to a point where even",
    "start": "2929190",
    "end": "2934289"
  },
  {
    "text": "our daily workloads are getting to a point where they need this these kind of configurability didn't need these kind",
    "start": "2934289",
    "end": "2939809"
  },
  {
    "text": "of computation power so we're getting to a point where we are going to look into having a persistent cluster but right",
    "start": "2939809",
    "end": "2946079"
  },
  {
    "text": "now these clusters are just transient how do you handle incremental updates of mutable snapshot updates",
    "start": "2946079",
    "end": "2952660"
  },
  {
    "start": "2947000",
    "end": "2997000"
  },
  {
    "text": "right so I think the way we do that is we take a complete snapshot of the entire database every time we do a",
    "start": "2952660",
    "end": "2958840"
  },
  {
    "text": "snapshot we don't do any incremental updates we don't do a lot Delta update on top of this I think this gives us",
    "start": "2958840",
    "end": "2964470"
  },
  {
    "text": "while this is kind of we're over redoing a lot of competition in that like we're",
    "start": "2964470",
    "end": "2969670"
  },
  {
    "text": "just taking most of it is redundant between different snapshots I think this gives us a really cool ability for",
    "start": "2969670",
    "end": "2975220"
  },
  {
    "text": "tables that are mutable that don't retain their original form this allows us to do some things like for each",
    "start": "2975220",
    "end": "2981550"
  },
  {
    "text": "snapshot we can take a look at what the table look like at a certain point in time so if we're taking a snapshot of a",
    "start": "2981550",
    "end": "2987160"
  },
  {
    "text": "mutable table each day we can see over time like how this table changed over time and do some queries that would not",
    "start": "2987160",
    "end": "2993700"
  },
  {
    "text": "be able that we won't be able to do if we just maintain the latest snapshot what technologies are using for the",
    "start": "2993700",
    "end": "3000570"
  },
  {
    "start": "2997000",
    "end": "3042000"
  },
  {
    "text": "validation layer and the end users like analysts and scientists also get to write their own rules for their ADL's",
    "start": "3000570",
    "end": "3006060"
  },
  {
    "text": "right so I think this is a very good question the reason like we're trying to do we made this framework as",
    "start": "3006060",
    "end": "3011550"
  },
  {
    "text": "customizable as possible since s3 is like the Wild West of the storage layer lets you store anything we're trying to",
    "start": "3011550",
    "end": "3016740"
  },
  {
    "text": "bring a lot of the guarantees that come with a relational database like like null checks and unique constraints a lot",
    "start": "3016740",
    "end": "3025590"
  },
  {
    "text": "of primary key constraints has come out of the box with systems like Postgres are not available in s3 and for the data",
    "start": "3025590",
    "end": "3030780"
  },
  {
    "text": "you're storing in s3 so what we try to do is like we try to implement most of these checks out of the box so if",
    "start": "3030780",
    "end": "3035820"
  },
  {
    "text": "there's a data set where you want to make sure that there's an enum that all the Pauli's values file fall between",
    "start": "3035820",
    "end": "3041190"
  },
  {
    "text": "these said you can just make sure just use that out of the box with your custom configuration but we understand at the",
    "start": "3041190",
    "end": "3047100"
  },
  {
    "start": "3042000",
    "end": "3085000"
  },
  {
    "text": "end of the day that each data set is different and the use cases would be different and common-sense things you want to check for will be different",
    "start": "3047100",
    "end": "3053310"
  },
  {
    "text": "there might be different things you want to check that like oh these two tables should be the same size well this should be strictly greater than the other",
    "start": "3053310",
    "end": "3058770"
  },
  {
    "text": "so for these use cases we define a custom use case or custom query where you can just specify whatever query you",
    "start": "3058770",
    "end": "3065220"
  },
  {
    "text": "want to make and it will just run that and make sure that like the result is in line with whatever your configuration is",
    "start": "3065220",
    "end": "3070890"
  },
  {
    "text": "to come up with these queries or like what a common sense is for some of the data sets we work with our daily",
    "start": "3070890",
    "end": "3076260"
  },
  {
    "text": "scientists and our end users to see what actually makes sense for certain data set before we actually set up these",
    "start": "3076260",
    "end": "3081270"
  },
  {
    "text": "checks why did you have to build some of your ingestion layer yourself so we actually chose to do this for two",
    "start": "3081270",
    "end": "3090350"
  },
  {
    "start": "3085000",
    "end": "3144000"
  },
  {
    "text": "reasons one we wanted control over how the data gets into s3 from a data like",
    "start": "3090350",
    "end": "3096080"
  },
  {
    "text": "perspective the way we understand it is that once the raw data is in s3 since it's the raw data any",
    "start": "3096080",
    "end": "3103160"
  },
  {
    "text": "transformation that we need to do on top of that can be done or changed and you know we still have the raw data so nothing is lossy in that sense so having",
    "start": "3103160",
    "end": "3111590"
  },
  {
    "text": "control to make sure that everything ends up in the data lake in the correct format and its raw format is something",
    "start": "3111590",
    "end": "3117320"
  },
  {
    "text": "that we wanted complete fine granularity over if any bugs are introduced there we want to know exactly what's going on we",
    "start": "3117320",
    "end": "3123050"
  },
  {
    "text": "wanted to make sure that we were very very deeply understanding every logical pieces so we didn't want to go and outsource this with some other company",
    "start": "3123050",
    "end": "3129170"
  },
  {
    "text": "that would take data from some place and push it into our lake because if something went wrong there and the data",
    "start": "3129170",
    "end": "3134960"
  },
  {
    "text": "didn't end up there one if it's not in your data link in the raw format you don't have that data anymore right it's gone forever",
    "start": "3134960",
    "end": "3140240"
  },
  {
    "text": "so yeah we wanted complete control of that so it was primarily from that perspective where do a couple more and",
    "start": "3140240",
    "end": "3145910"
  },
  {
    "start": "3144000",
    "end": "3194000"
  },
  {
    "text": "then wrap it up but you'll be around afterwards and the rest of them so did you do any performance comparisons",
    "start": "3145910",
    "end": "3151850"
  },
  {
    "text": "between Athena and redshift spectrum for user facing queries so I think I can take up this part since I looked into",
    "start": "3151850",
    "end": "3157490"
  },
  {
    "text": "that I think the way we looked at it is that there are different use cases that these would be used for let's say that",
    "start": "3157490",
    "end": "3162560"
  },
  {
    "text": "we already have a redshift class you're standing up there is no point in using redshift spectrum for queries that are only in s3 lecture spectrum instead like",
    "start": "3162560",
    "end": "3170120"
  },
  {
    "text": "since you already have the compute for the nodes over there and it would just spin up other nodes that it needs to do the computation it's best to use it or",
    "start": "3170120",
    "end": "3176240"
  },
  {
    "text": "reserve it for the use cases where you want to do join operations between data that is stored in redshift and Dana that",
    "start": "3176240",
    "end": "3182750"
  },
  {
    "text": "is stored in s3 because there's no way you can just use the thena for that and it's instead better to use Athena just",
    "start": "3182750",
    "end": "3188120"
  },
  {
    "text": "for the use cases where you actually just want to query the data that is already present in s3 and don't need any",
    "start": "3188120",
    "end": "3193760"
  },
  {
    "text": "interaction with red shell so there's no way or there's no reason to actually get redshift involved if the data is not",
    "start": "3193760",
    "end": "3199370"
  },
  {
    "start": "3194000",
    "end": "3213000"
  },
  {
    "text": "related to that and if we were to only use redshift over here then we are kind of defeating the same purpose of like",
    "start": "3199370",
    "end": "3205730"
  },
  {
    "text": "storing or decoupling our storage and compute so we kind of look at these for different use cases so we don't want",
    "start": "3205730",
    "end": "3211700"
  },
  {
    "text": "these to collide together we might be able make all three let's see how quickly we can do this how easy is it",
    "start": "3211700",
    "end": "3217380"
  },
  {
    "start": "3213000",
    "end": "3267000"
  },
  {
    "text": "debug ETL process using this architecture okay I can take this up so basically the problem that we saw over",
    "start": "3217380",
    "end": "3223469"
  },
  {
    "text": "here at times is that initially it's kind of hard and there were two factors to this I think a lot of the engineers",
    "start": "3223469",
    "end": "3228989"
  },
  {
    "text": "were using spark for the first time a lot of the engineers were working with like these big data technologies for the",
    "start": "3228989",
    "end": "3234059"
  },
  {
    "text": "first time so there was an initial ramp up process as well and for some of the tooling we have found that like debugging is kind of difficult like for",
    "start": "3234059",
    "end": "3241170"
  },
  {
    "text": "example with clue since we don't have detailed executor logs or at one point in time we didn't even have visualizations into how different",
    "start": "3241170",
    "end": "3247920"
  },
  {
    "text": "executors were utilizing their memory and if one was filling to disk too much so that kind of made debugging a little",
    "start": "3247920",
    "end": "3254279"
  },
  {
    "text": "difficult at that point time but there are certain alternatives the clue has come up with such that like they came up with development endpoints that were",
    "start": "3254279",
    "end": "3260609"
  },
  {
    "text": "like that allowed us to at least test the code and they have added much better visualization tools on top of that but I",
    "start": "3260609",
    "end": "3267719"
  },
  {
    "text": "agree that this is a this is a problem that we did face concerns with and this is something that we thought that what",
    "start": "3267719",
    "end": "3273029"
  },
  {
    "text": "better solved with the EMR if we were writing something completely from scratch it's something that would have these kind of failures where like we",
    "start": "3273029",
    "end": "3279390"
  },
  {
    "text": "were spilling today's cruising too much memory these kind of issues we think we try to like outsource them to our EMR",
    "start": "3279390",
    "end": "3285269"
  },
  {
    "text": "cluster instead of using this one glue because we have a lot more debugging ability over there but I think these are things that as we've talked with the",
    "start": "3285269",
    "end": "3291809"
  },
  {
    "text": "glue team these are something that things that are on the roadmap and we'd be excited to test them once they're out and then what's total processing time",
    "start": "3291809",
    "end": "3298680"
  },
  {
    "start": "3296000",
    "end": "3379000"
  },
  {
    "text": "from when you adjust an event to visualization I think I think both of these questions are related so I'll just",
    "start": "3298680",
    "end": "3303809"
  },
  {
    "text": "kind of answer both of them in one go so most of our data ends up nightly in our",
    "start": "3303809",
    "end": "3310019"
  },
  {
    "text": "in our data Lake there's some that ends up much more faster based on the use cases so we work with the either the",
    "start": "3310019",
    "end": "3317309"
  },
  {
    "text": "provider of the data or the users of that data eventually to figure out what are the SLA is that acceptable mostly data is available every day so",
    "start": "3317309",
    "end": "3325019"
  },
  {
    "text": "the previous day's data becomes queryable you know and analyzable the next morning how long does it take for that data to",
    "start": "3325019",
    "end": "3332069"
  },
  {
    "text": "end up to local users looker you know obviously have to you have to add a model layer on top of it the awesome thing about look at is it",
    "start": "3332069",
    "end": "3338789"
  },
  {
    "text": "gives you a lot of flexibility so we'll work with the end use cases at that point so let's say you want to run a",
    "start": "3338789",
    "end": "3344849"
  },
  {
    "text": "cohort analysis over this data for some reason will actually understand what is the cohorting analysis you want to do",
    "start": "3344849",
    "end": "3350240"
  },
  {
    "text": "and based on that we'll model out looker in that pattern and so you know it's just a question of getting the person",
    "start": "3350240",
    "end": "3355880"
  },
  {
    "text": "who wants that specific answer and the kind of analysis they need into the same room and once they've given it to us it",
    "start": "3355880",
    "end": "3361280"
  },
  {
    "text": "takes very little time to build it out for them so it varies you know it varies on the use case it varies on what they",
    "start": "3361280",
    "end": "3366350"
  },
  {
    "text": "actually want to do if they wanted something really complicated might take a little longer but usually you know as soon as the data is available we can",
    "start": "3366350",
    "end": "3371990"
  },
  {
    "text": "make it variable viola Kerr you survived the 37 questions nice gun please menopause Robin Hood",
    "start": "3371990",
    "end": "3378230"
  },
  {
    "text": "[Applause]",
    "start": "3378230",
    "end": "3381289"
  }
]