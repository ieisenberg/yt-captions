[
  {
    "start": "0",
    "end": "13000"
  },
  {
    "text": "- [Instructor] Amazon Bedrock's\nModel Evaluation Tool,",
    "start": "2610",
    "end": "5520"
  },
  {
    "text": "makes it easier to\nidentify the optimal model",
    "start": "5520",
    "end": "8430"
  },
  {
    "text": "for your use case.",
    "start": "8430",
    "end": "9990"
  },
  {
    "text": "We simply select our candidate\nmodel for evaluation.",
    "start": "9990",
    "end": "13830"
  },
  {
    "start": "13000",
    "end": "26000"
  },
  {
    "text": "In this case, we'll\nevaluate Titan Text Express.",
    "start": "13830",
    "end": "17163"
  },
  {
    "text": "Then we set our task type,",
    "start": "18000",
    "end": "20430"
  },
  {
    "text": "general text generation performance.",
    "start": "20430",
    "end": "22950"
  },
  {
    "text": "Next, we'll specify\nour evaluation metrics.",
    "start": "22950",
    "end": "26010"
  },
  {
    "start": "26000",
    "end": "40000"
  },
  {
    "text": "For this project, we're most interested",
    "start": "26010",
    "end": "28200"
  },
  {
    "text": "in accuracy and robustness.",
    "start": "28200",
    "end": "30780"
  },
  {
    "text": "For each metric, we can choose\nfrom the built-in data sets,",
    "start": "30780",
    "end": "34020"
  },
  {
    "text": "or we can provide our\nown dataset stored in S3.",
    "start": "34020",
    "end": "38100"
  },
  {
    "text": "Let's go with the built-in data sets.",
    "start": "38100",
    "end": "40769"
  },
  {
    "start": "40000",
    "end": "92000"
  },
  {
    "text": "Next, we need to define the S3 bucket",
    "start": "40770",
    "end": "43590"
  },
  {
    "text": "where the evaluation\nresults will be stored.",
    "start": "43590",
    "end": "46590"
  },
  {
    "text": "Finally, we need to set the\nservice role to allow Bedrock",
    "start": "46590",
    "end": "50070"
  },
  {
    "text": "to upload data to the S3 bucket.",
    "start": "50070",
    "end": "52563"
  },
  {
    "text": "Let's run the evaluation\nand check out our results.",
    "start": "54000",
    "end": "57900"
  },
  {
    "text": "For accuracy, the model scored a 0.21,",
    "start": "57900",
    "end": "61650"
  },
  {
    "text": "which means the model outputs\nscored low on accuracy.",
    "start": "61650",
    "end": "65223"
  },
  {
    "text": "For robustness, the\nmodel was scored against",
    "start": "66660",
    "end": "69360"
  },
  {
    "text": "three different data sets",
    "start": "69360",
    "end": "71010"
  },
  {
    "text": "to compare how consistent it performed",
    "start": "71010",
    "end": "73170"
  },
  {
    "text": "on slight changes to input prompts.",
    "start": "73170",
    "end": "75423"
  },
  {
    "text": "The model scored very well in Wiki Text 2,",
    "start": "76807",
    "end": "80430"
  },
  {
    "text": "with a score of 0.968,\nand lower in the others.",
    "start": "80430",
    "end": "85170"
  },
  {
    "text": "It's never been easier\nto choose the right model",
    "start": "85170",
    "end": "87720"
  },
  {
    "text": "for your use case.",
    "start": "87720",
    "end": "89103"
  }
]