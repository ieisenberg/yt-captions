[
  {
    "start": "0",
    "end": "60000"
  },
  {
    "text": "again welcome to today's webinar introduction to Amazon Kinesis fire hose our presenter today is Ray Zoo Ray is a",
    "start": "520",
    "end": "8320"
  },
  {
    "text": "senior product manager for Amazon Kinesis prior to joining AWS Ry had five",
    "start": "8320",
    "end": "14280"
  },
  {
    "text": "years of experiences in the business intelligence software and telecommunication Industries Ry holds a",
    "start": "14280",
    "end": "20439"
  },
  {
    "text": "bachelor degree in electronic and information engineering and a master degree in Business",
    "start": "20439",
    "end": "27480"
  },
  {
    "text": "Administration we also have with us today Alan mckinnes and Roy benala they",
    "start": "27599",
    "end": "33800"
  },
  {
    "text": "will be our webinar moderators today who'll be engaging you with your questions through the Q&A",
    "start": "33800",
    "end": "39399"
  },
  {
    "text": "panel Ray welcome to the session and the floor is now yours thank you Jerry and good morning",
    "start": "39399",
    "end": "46120"
  },
  {
    "text": "everyone and um welcome to today's webinar um in this session I'm going to",
    "start": "46120",
    "end": "52199"
  },
  {
    "text": "give you an introduction to the Amazon Kinesis fire host service and um we'll first start talking",
    "start": "52199",
    "end": "60039"
  },
  {
    "start": "60000",
    "end": "60000"
  },
  {
    "text": "a bit about streaming data um in general and then give you an overview on the",
    "start": "60039",
    "end": "65760"
  },
  {
    "text": "Kinesis fire host service then of course we're going to go deeper and on the fost",
    "start": "65760",
    "end": "71439"
  },
  {
    "text": "service I'll walk you through the console experience for using Kinesis fost to ingest data into the three",
    "start": "71439",
    "end": "79360"
  },
  {
    "text": "destinations we support today as I walk you through the console setup experience",
    "start": "79360",
    "end": "85119"
  },
  {
    "text": "I'll also talk a bit about the configurations and how they work behind the scene so you'll get a good sense of",
    "start": "85119",
    "end": "91240"
  },
  {
    "text": "how the service Works behind the scene then lastly I'll talk a bit about the Kinesis agent the F host monitoring",
    "start": "91240",
    "end": "98119"
  },
  {
    "text": "experience and close with a bit about the fire hose pricing so to get started um what is",
    "start": "98119",
    "end": "105280"
  },
  {
    "start": "103000",
    "end": "103000"
  },
  {
    "text": "streaming data um for those of you who are new to this space um streaming data",
    "start": "105280",
    "end": "110840"
  },
  {
    "text": "is a mechanism where you collect store and analyze your data in a continuous",
    "start": "110840",
    "end": "116960"
  },
  {
    "text": "manner in real time this is as as opposed to the traditional batch",
    "start": "116960",
    "end": "122119"
  },
  {
    "text": "oriented way where you do the same thing over a big chunk of batch and that often",
    "start": "122119",
    "end": "127640"
  },
  {
    "text": "involves days of hours of latency so we do see customers adopting",
    "start": "127640",
    "end": "134040"
  },
  {
    "text": "streaming data from different IND industry segments for different use cases and use different of data types",
    "start": "134040",
    "end": "141160"
  },
  {
    "text": "for example we see customers using streaming data for their it logs application logs click streams Market",
    "start": "141160",
    "end": "149000"
  },
  {
    "text": "data and Etc now over time we do see customers",
    "start": "149000",
    "end": "154720"
  },
  {
    "text": "adoption and usage of streaming data evolve over three different phases and",
    "start": "154720",
    "end": "160480"
  },
  {
    "text": "the first phase is what we call Accelerated data inest transform and",
    "start": "160480",
    "end": "165599"
  },
  {
    "text": "load in this set of use cases where customers still store and analyze the",
    "start": "165599",
    "end": "171920"
  },
  {
    "text": "data in the data stores and using the tools that they are familiar with for",
    "start": "171920",
    "end": "176959"
  },
  {
    "text": "example data warehouse business and intelligence tools Hadoop job elastic",
    "start": "176959",
    "end": "183080"
  },
  {
    "text": "search but what streaming data can bring to the table is now instead of having",
    "start": "183080",
    "end": "188640"
  },
  {
    "text": "your data loaded into these stores and tools on a daily or hourly basis now you",
    "start": "188640",
    "end": "194840"
  },
  {
    "text": "get that in minutes now as customers move on to their streaming data use cases um they",
    "start": "194840",
    "end": "202000"
  },
  {
    "text": "get to the second phase which is what we call continual metric generation so in",
    "start": "202000",
    "end": "207720"
  },
  {
    "text": "this set of use cases customers compute and an analyze the data and metrix",
    "start": "207720",
    "end": "214879"
  },
  {
    "text": "directly against the data stream so that they can gain business insights from the data stream directly and in these set of",
    "start": "214879",
    "end": "222599"
  },
  {
    "text": "use cases latencies often seconds or even milliseconds depending on your use",
    "start": "222599",
    "end": "228080"
  },
  {
    "text": "case and setup then the third phase is really for more sophisticated customers in use",
    "start": "228080",
    "end": "235000"
  },
  {
    "text": "cases we call it responsive data analysis in this set of use cases",
    "start": "235000",
    "end": "241680"
  },
  {
    "text": "customers build their streaming data pipeline into their core application and",
    "start": "241680",
    "end": "247000"
  },
  {
    "text": "business and using streaming data to feedback to their core logic in real",
    "start": "247000",
    "end": "252360"
  },
  {
    "text": "time for example in the mobile gaming industry suppose you have a mobile game",
    "start": "252360",
    "end": "258560"
  },
  {
    "text": "app and you constantly CLEP your user interaction Gamers interaction data in",
    "start": "258560",
    "end": "264199"
  },
  {
    "text": "real time then behind the scene based on the sequence of the actions the gamer",
    "start": "264199",
    "end": "269800"
  },
  {
    "text": "take it'll feed back into the game itself in real time and Trigger certain events such as level up or drop a weapon",
    "start": "269800",
    "end": "278160"
  },
  {
    "text": "in the game so this way you'll provide a much more engaging experience to your",
    "start": "278160",
    "end": "284680"
  },
  {
    "text": "customers so this kind of an overview of um how customers are using streaming",
    "start": "284680",
    "end": "290240"
  },
  {
    "text": "data and how they go through their streaming data Journey at Amazon Kinesis it's a",
    "start": "290240",
    "end": "297320"
  },
  {
    "start": "295000",
    "end": "295000"
  },
  {
    "text": "platform of streaming data service we currently have three different services at Amazon Kinesis the first one",
    "start": "297320",
    "end": "305600"
  },
  {
    "text": "is kinesis streams it stores your data in a continuous manner for your",
    "start": "305600",
    "end": "310960"
  },
  {
    "text": "downstreaming applications to replay process and it's fully flexible you",
    "start": "310960",
    "end": "316880"
  },
  {
    "text": "build your own application with your own custom logic and you have full control of the underlying data stream then we",
    "start": "316880",
    "end": "324319"
  },
  {
    "text": "have a second service which is the main topic for today's webinar session and",
    "start": "324319",
    "end": "329720"
  },
  {
    "text": "this is kinesis F host the Kinesis F host service automatically loads the",
    "start": "329720",
    "end": "335720"
  },
  {
    "text": "streaming data into destination such as Amazon S3 R shift and elas search you do",
    "start": "335720",
    "end": "342919"
  },
  {
    "text": "not need to manage any application behind the scene and the service does that for you then we have the third",
    "start": "342919",
    "end": "349960"
  },
  {
    "text": "service called Kinesis analytics this service allows you to use standard SQL",
    "start": "349960",
    "end": "356479"
  },
  {
    "text": "to run queries directly against the data stream and it can run cures again a k",
    "start": "356479",
    "end": "361759"
  },
  {
    "text": "against the kesa stream or a fire host delivery Stream So for those of you whove been paying attention to the",
    "start": "361759",
    "end": "368639"
  },
  {
    "text": "previous slide about the three phases of use cases that customers go through you",
    "start": "368639",
    "end": "373680"
  },
  {
    "text": "kind of figured it's kind of match to the services we offer at Amazon",
    "start": "373680",
    "end": "380199"
  },
  {
    "text": "Kinesis and for the first off streaming data use case The Accelerated inj",
    "start": "380880",
    "end": "386479"
  },
  {
    "start": "381000",
    "end": "381000"
  },
  {
    "text": "transform and load you can probably tell the Kinesis file host is exactly for",
    "start": "386479",
    "end": "392360"
  },
  {
    "text": "that particular use case it streams your data in near real time into the data",
    "start": "392360",
    "end": "397639"
  },
  {
    "text": "stores that we've been using for years you can still use the favorite analytical tool that we've been using to",
    "start": "397639",
    "end": "404280"
  },
  {
    "text": "analyze your data but now the latency gets reduced down to minutes or even",
    "start": "404280",
    "end": "411319"
  },
  {
    "text": "seconds and kesis analytics allows you to run standard SQL to analyze your data",
    "start": "411639",
    "end": "418039"
  },
  {
    "text": "directly against the Stream and extract metric from the data stream and that's targeted for the second set of use case",
    "start": "418039",
    "end": "425160"
  },
  {
    "text": "Contin metric generation and of course we have kesa streams a fully flexible infrastructural",
    "start": "425160",
    "end": "433360"
  },
  {
    "text": "service for you to build your own streaming data applications and it's targeted for more sophisticated use",
    "start": "433360",
    "end": "439319"
  },
  {
    "text": "cases where we call it responsive data analysis so this is kind of like a five",
    "start": "439319",
    "end": "445400"
  },
  {
    "text": "minutes overview for streaming data and what we offer here I'm at Amazon Kinesis",
    "start": "445400",
    "end": "450520"
  },
  {
    "text": "you can see that really streaming data can be applied across a very um diverse",
    "start": "450520",
    "end": "456639"
  },
  {
    "text": "set of use cases and at Amazon Kinesis we have a different set of services",
    "start": "456639",
    "end": "461759"
  },
  {
    "text": "catering to different use cases and now let's get into the the",
    "start": "461759",
    "end": "467240"
  },
  {
    "text": "topic uh the main topic we're going to cover today about canesas fire",
    "start": "467240",
    "end": "472680"
  },
  {
    "text": "hose so first we'll go through the three key concepts of the service the first",
    "start": "472680",
    "end": "479080"
  },
  {
    "start": "473000",
    "end": "473000"
  },
  {
    "text": "one is delivery stream it's the underlying entity of the file host service you use a service by first",
    "start": "479080",
    "end": "486599"
  },
  {
    "text": "creating a delivery stream and you'll configure your destination options in the delivery stream once it's set up",
    "start": "486599",
    "end": "494199"
  },
  {
    "text": "you'll just configure your data producers to send data to your delivery stream then behind the scene the service",
    "start": "494199",
    "end": "501440"
  },
  {
    "text": "will automatically deliver the data to the destination that you",
    "start": "501440",
    "end": "506759"
  },
  {
    "text": "specified the second concept is called a record a data record is the data of",
    "start": "506759",
    "end": "512440"
  },
  {
    "text": "interest that your data producer sent to your data stream for example a data",
    "start": "512440",
    "end": "517839"
  },
  {
    "text": "record could be a Json object triggered by a user's click on the website or it",
    "start": "517839",
    "end": "523440"
  },
  {
    "text": "could be a log line from your web application server and each of the record can be as large as 1,000",
    "start": "523440",
    "end": "531279"
  },
  {
    "text": "kilobytes then the third concept is a data producer and producers send data",
    "start": "531279",
    "end": "536560"
  },
  {
    "text": "records into your delivery stream uh um for example the data Y data producer",
    "start": "536560",
    "end": "541800"
  },
  {
    "text": "could be a web application server could be a mobile device or could be iot",
    "start": "541800",
    "end": "548360"
  },
  {
    "text": "device this is the high level data flow of the kesa F host service on the very",
    "start": "548959",
    "end": "556279"
  },
  {
    "start": "549000",
    "end": "549000"
  },
  {
    "text": "left hand side you'll configure your data producers to continuously push data",
    "start": "556279",
    "end": "561800"
  },
  {
    "text": "into your file host deliver stream through file host per record API and you",
    "start": "561800",
    "end": "567360"
  },
  {
    "text": "can configure hundreds or even thousands of data producers to push into the delivery stream and it'll scale really",
    "start": "567360",
    "end": "575040"
  },
  {
    "text": "well and on the right hand side the destination is already configured in",
    "start": "575040",
    "end": "580200"
  },
  {
    "text": "your fost delivery stream and F host will continuously push the data into",
    "start": "580200",
    "end": "585399"
  },
  {
    "text": "these destinations and once the data lands into these destinations you can",
    "start": "585399",
    "end": "591399"
  },
  {
    "text": "use your favorite analytical tools to analyze and query the data from these",
    "start": "591399",
    "end": "597360"
  },
  {
    "text": "data destinations okay so now you kind of have a good",
    "start": "597360",
    "end": "604440"
  },
  {
    "text": "sense of how file holes work um at a high level next I'm going to walk you",
    "start": "604440",
    "end": "610760"
  },
  {
    "text": "through how you configure a f host deliv stream through the fire host console",
    "start": "610760",
    "end": "617000"
  },
  {
    "text": "first I'm going to talk about the S3 destination where fire host will stream your data into your S3",
    "start": "617000",
    "end": "625160"
  },
  {
    "start": "624000",
    "end": "624000"
  },
  {
    "text": "bucket step one you go to the F host console choose S3 as the destination and",
    "start": "625160",
    "end": "632240"
  },
  {
    "text": "then specify your delivery stream name then you'll choose an S3 bucket that you",
    "start": "632240",
    "end": "637880"
  },
  {
    "text": "own for the F host service to deliver the data intake you can also create a",
    "start": "637880",
    "end": "643839"
  },
  {
    "text": "new bucket on the fly through the F host console now optionally you can specify",
    "start": "643839",
    "end": "650079"
  },
  {
    "text": "an S3 prefix so F host will add this prefix that is specify into each of the",
    "start": "650079",
    "end": "657800"
  },
  {
    "text": "object it deliver to our destination so for use cases where you'll use the same",
    "start": "657800",
    "end": "664120"
  },
  {
    "text": "bucket for different kind of stream or different kind of objects you can use this particular prefix to distinguish",
    "start": "664120",
    "end": "671720"
  },
  {
    "text": "and group the data logically in your S3 bucket other than this specific S3",
    "start": "671720",
    "end": "677560"
  },
  {
    "text": "prefix that you specify firehost will also automatically append a timestamp",
    "start": "677560",
    "end": "683839"
  },
  {
    "text": "free prefix into your object so that way your data is partitioned by time in your",
    "start": "683839",
    "end": "689720"
  },
  {
    "text": "S3 bucket step two we'll go through some of",
    "start": "689720",
    "end": "695560"
  },
  {
    "text": "the configurations uh first is S3 buffering fire host buffers your incoming",
    "start": "695560",
    "end": "701880"
  },
  {
    "text": "streaming data into small batches before we send it to the S3 bucket that you own",
    "start": "701880",
    "end": "708279"
  },
  {
    "text": "this way it's efficient data load into S3 bucket and also cost effective um as",
    "start": "708279",
    "end": "714600"
  },
  {
    "text": "you'll pay very um a lot less put cost on the S3 bucket now buffering option",
    "start": "714600",
    "end": "721360"
  },
  {
    "text": "can be configured around two different dimensions you can either configure",
    "start": "721360",
    "end": "726639"
  },
  {
    "text": "buffering minutes or a buffering interval buffering minutes can be",
    "start": "726639",
    "end": "732040"
  },
  {
    "text": "anywhere uh from 5 minutes to 60 minutes and buffer interval can be anywhere from",
    "start": "732040",
    "end": "739560"
  },
  {
    "text": "60 seconds to 12 minutes now whichever condition is",
    "start": "739560",
    "end": "746079"
  },
  {
    "text": "satisfied first will trigger the data delivery to your S3",
    "start": "746079",
    "end": "751600"
  },
  {
    "text": "bucket then optionally you can configure your F host delivery stream to compress",
    "start": "751600",
    "end": "757480"
  },
  {
    "text": "your data before it delivers data to your destination you can also choose in KMS",
    "start": "757480",
    "end": "764480"
  },
  {
    "text": "key that you own for file host to trigger the S3 server side encryption so",
    "start": "764480",
    "end": "770480"
  },
  {
    "text": "that as soon as the objects is delivered to the S3 bucket the S3 service will",
    "start": "770480",
    "end": "776000"
  },
  {
    "text": "encrypt it on the server side with the encryption key key that we",
    "start": "776000",
    "end": "781320"
  },
  {
    "text": "provide then there's also this error login option so in case where fire host",
    "start": "781320",
    "end": "787320"
  },
  {
    "text": "is not able to deliver data to your destination bucket it'll log in an error",
    "start": "787320",
    "end": "793839"
  },
  {
    "text": "message to your cloudwatch LW group you can also view that error message from",
    "start": "793839",
    "end": "798880"
  },
  {
    "text": "the fir host console it'll become very handy for troubleshooting",
    "start": "798880",
    "end": "804360"
  },
  {
    "text": "purposes lastly we have this IR row configuration where you explicitly allow",
    "start": "804360",
    "end": "810079"
  },
  {
    "text": "fir host to access the resource is specifying this delivery stream configuration for example the particular",
    "start": "810079",
    "end": "817000"
  },
  {
    "text": "S3 bucket the particular particular km key you use in this deliver stream",
    "start": "817000",
    "end": "822399"
  },
  {
    "text": "configuration this way is to ensure that all your resources are securely guarded by AWS and only fire host service has",
    "start": "822399",
    "end": "830759"
  },
  {
    "text": "access and only to the specific resources is specified in this particular delivery Stream So once the",
    "start": "830759",
    "end": "837959"
  },
  {
    "text": "configuration is configured you click on next review all the options and then click on create the deriv stream so you",
    "start": "837959",
    "end": "845399"
  },
  {
    "text": "can see it probably takes about 60 seconds for you to go through and set up",
    "start": "845399",
    "end": "850680"
  },
  {
    "text": "this streaming data Pipeline and once it's set up you just configure data",
    "start": "850680",
    "end": "855720"
  },
  {
    "text": "producers to send data to the delivery stream then you'll just wait and see the data continuously show up in your S3",
    "start": "855720",
    "end": "863759"
  },
  {
    "text": "bucket so this is S3 destination now let's switch and gear a little bit and",
    "start": "863759",
    "end": "869199"
  },
  {
    "text": "getting to the rift destination similar console um interface",
    "start": "869199",
    "end": "875320"
  },
  {
    "start": "873000",
    "end": "873000"
  },
  {
    "text": "um in this case you'll choose Amazon rare shift as your",
    "start": "875320",
    "end": "880480"
  },
  {
    "text": "destination against specified delivery stream now on this page you'll notice a",
    "start": "880480",
    "end": "885959"
  },
  {
    "text": "configuration for an intermediate S3 bucket so the way file host to RI works",
    "start": "885959",
    "end": "892000"
  },
  {
    "text": "is fire host will first deliver the streaming data into your S3 bucket and",
    "start": "892000",
    "end": "898120"
  },
  {
    "text": "then from the S3 bucket will continuously issue a copy command to load the data into a red shift cluster",
    "start": "898120",
    "end": "906040"
  },
  {
    "text": "there are two reasons to have this intermediate S3 bucket number one",
    "start": "906040",
    "end": "911519"
  },
  {
    "text": "loading data from S3 bucket to R shift is still the most efficient way for",
    "start": "911519",
    "end": "916680"
  },
  {
    "text": "loading data into rare shift this way is to ensure we preserve enough resource on",
    "start": "916680",
    "end": "922639"
  },
  {
    "text": "the ri cluster side so that the resource can be used for you to run queries",
    "start": "922639",
    "end": "927759"
  },
  {
    "text": "against now the second reason is for whatever reason where the ri cluster is",
    "start": "927759",
    "end": "933920"
  },
  {
    "text": "not accessible for example there might be nwork issue or your R of clusters",
    "start": "933920",
    "end": "938959"
  },
  {
    "text": "under maintenance or resizing this way the S3 bucket ensures",
    "start": "938959",
    "end": "944959"
  },
  {
    "text": "that you always have a copy of your data in the S3 bucket so that this way you'll",
    "start": "944959",
    "end": "950759"
  },
  {
    "text": "never lose any data and once your RI cluster is back up you can use the data",
    "start": "950759",
    "end": "956199"
  },
  {
    "text": "in your S3 bucket for backfield purpose the bottom half of the page are the",
    "start": "956199",
    "end": "962800"
  },
  {
    "text": "configurations specific to Red shift you'll first choose a cluster that you own specify the database name and table",
    "start": "962800",
    "end": "970839"
  },
  {
    "text": "name and in case where the number of data field values within your record",
    "start": "970839",
    "end": "976720"
  },
  {
    "text": "doesn't exactly match the number of columns in your table you can also specify the table columns here for the F",
    "start": "976720",
    "end": "984279"
  },
  {
    "text": "host service to copy the data in you'll then specify r username and a password",
    "start": "984279",
    "end": "990319"
  },
  {
    "text": "and the file host service will use the credential of this user to copy data from S3 to your R cluster you need to",
    "start": "990319",
    "end": "998199"
  },
  {
    "text": "make sure this particular user has insert permission to the table you",
    "start": "998199",
    "end": "1003240"
  },
  {
    "text": "specify above then you can also specify Rift copy options for custom data load into",
    "start": "1003240",
    "end": "1010680"
  },
  {
    "text": "your Rift cluster there's also a retry duration",
    "start": "1010680",
    "end": "1016199"
  },
  {
    "text": "configuration it can be anywhere from 0 second to two hours so in case where",
    "start": "1016199",
    "end": "1022160"
  },
  {
    "text": "fire host is not able to deliver data to your RI cluster it'll keep retrying for",
    "start": "1022160",
    "end": "1028400"
  },
  {
    "text": "a certain period of time until it gives up and move on to the next batch of data",
    "start": "1028400",
    "end": "1034678"
  },
  {
    "text": "so for use cases where you care about data recency for example a realtime dashboard you can configure relatively",
    "start": "1034679",
    "end": "1042000"
  },
  {
    "text": "low retry duration so that you can make sure the most fresh data is always get",
    "start": "1042000",
    "end": "1047600"
  },
  {
    "text": "delivered first into a r shift cluster and for other set of use cases where you",
    "start": "1047600",
    "end": "1052880"
  },
  {
    "text": "care about completeness of the data you can configure relatively larger retry",
    "start": "1052880",
    "end": "1058559"
  },
  {
    "text": "duration for example if we configure retry duration to two hours it's",
    "start": "1058559",
    "end": "1063840"
  },
  {
    "text": "normally enough to cover the ri maintenance time window so that fost will keep retrying until your RI cluster",
    "start": "1063840",
    "end": "1071880"
  },
  {
    "text": "finishes the maintenance but one thing to note that whenever fire host gives up a reach try",
    "start": "1071880",
    "end": "1078880"
  },
  {
    "text": "and move on to the next batch of data records it always sends an error manage",
    "start": "1078880",
    "end": "1084039"
  },
  {
    "text": "Fest file into your S3 bucket so that later on you know what are the set of",
    "start": "1084039",
    "end": "1089200"
  },
  {
    "text": "objects the service skip for copying and you can do the manual backfill",
    "start": "1089200",
    "end": "1095039"
  },
  {
    "text": "afterwards then the bottom of the page is a copy command so based on the configuration you putting into the",
    "start": "1095039",
    "end": "1101320"
  },
  {
    "text": "delivery stream the F host will generate a copy command and the command you see",
    "start": "1101320",
    "end": "1106440"
  },
  {
    "text": "here on this page will be the exact copy command the service will use to copy the",
    "start": "1106440",
    "end": "1112400"
  },
  {
    "text": "data from S3 bucket to your Rift cluster it will be very handy for you to run a",
    "start": "1112400",
    "end": "1118320"
  },
  {
    "text": "Tas command through your jdbc or odbc connector tools before you set up the",
    "start": "1118320",
    "end": "1124159"
  },
  {
    "text": "DAT stream so that we can ensure all the configurations are set up",
    "start": "1124159",
    "end": "1131039"
  },
  {
    "text": "properly next page the configuration page it's identical to what we've seen",
    "start": "1132039",
    "end": "1138480"
  },
  {
    "text": "under the S3 destination um you'll first configure S3 buffering compression and encryption",
    "start": "1138480",
    "end": "1145039"
  },
  {
    "text": "options for data being delivered into your intermediate S3 bucket then we also",
    "start": "1145039",
    "end": "1150480"
  },
  {
    "text": "have the error logging feature to log the errors when fost is not able to deliver data to your Rift cluster and",
    "start": "1150480",
    "end": "1158520"
  },
  {
    "text": "similarly we have an IM row option to control strict access to the resources you configure in this delivery",
    "start": "1158520",
    "end": "1166760"
  },
  {
    "text": "stream after these two steps you review and uh finish creating the delivery",
    "start": "1166760",
    "end": "1172799"
  },
  {
    "text": "stream again it's fairly easy and simple just to set up a streaming data pipeline",
    "start": "1172799",
    "end": "1178559"
  },
  {
    "text": "from your data producer to your Rift cluster now lastly we'll go through the",
    "start": "1178559",
    "end": "1184520"
  },
  {
    "text": "third destination that we support at kesa fire hose which is elastic",
    "start": "1184520",
    "end": "1191919"
  },
  {
    "text": "search on the first page you'll configure theas search service to be",
    "start": "1193919",
    "end": "1199600"
  },
  {
    "text": "your destination and at the same time specify a delivery stream",
    "start": "1199600",
    "end": "1204720"
  },
  {
    "text": "name and subsequently you'll configure the elas search domain information",
    "start": "1204720",
    "end": "1210159"
  },
  {
    "text": "you'll pass a domain name into the configuration specify an index you can",
    "start": "1210159",
    "end": "1216320"
  },
  {
    "text": "also configure index rotation so that file host will rotate your index on a",
    "start": "1216320",
    "end": "1222760"
  },
  {
    "text": "hourly daily weekly or monthly basis because most of the time customer do not",
    "start": "1222760",
    "end": "1229159"
  },
  {
    "text": "use elas search as a long-term storage it often rotate the index over time and",
    "start": "1229159",
    "end": "1235799"
  },
  {
    "text": "retire the older ones over time so depending on your rotation configuration",
    "start": "1235799",
    "end": "1241400"
  },
  {
    "text": "fire host will append a timestamp behind the index name while it index documents",
    "start": "1241400",
    "end": "1247440"
  },
  {
    "text": "to a last search domain so this way you can keep the most fresh index and retire",
    "start": "1247440",
    "end": "1253159"
  },
  {
    "text": "the older ones over time you'll also specify a type Tye for",
    "start": "1253159",
    "end": "1258760"
  },
  {
    "text": "the documents to be grouped in logical manner on your index and similarly",
    "start": "1258760",
    "end": "1264520"
  },
  {
    "text": "there's this retry duration anywhere from 0 second which is no retry to two",
    "start": "1264520",
    "end": "1270360"
  },
  {
    "text": "hours so in similar cases where your elas search cluster is not accessible by",
    "start": "1270360",
    "end": "1276360"
  },
  {
    "text": "the fir host service the service will keep retrying for a certain period of time as you configured until it gives up",
    "start": "1276360",
    "end": "1284400"
  },
  {
    "text": "and moves on to the next batch set of data documents then we also have a backup S3 bucket",
    "start": "1284400",
    "end": "1291520"
  },
  {
    "text": "option it's slightly different from the intermediate S3 bucket option you see under the r of destination in this case",
    "start": "1291520",
    "end": "1299440"
  },
  {
    "start": "1297000",
    "end": "1297000"
  },
  {
    "text": "the backup S3 bucket it's a concurrent process as the data delivery to your",
    "start": "1299440",
    "end": "1304720"
  },
  {
    "text": "lastas search cluster and it's not on the path to the elas search cluster you",
    "start": "1304720",
    "end": "1310640"
  },
  {
    "text": "can configure backup mode to be two different options number one is to back up fail documents only so in case where",
    "start": "1310640",
    "end": "1319400"
  },
  {
    "text": "fir host is not able to deliver your documents to a last search cluster it'll send all the fill",
    "start": "1319400",
    "end": "1326520"
  },
  {
    "text": "documents into your S3 bucket so that you never lose any data and you can do manual backfill later",
    "start": "1326520",
    "end": "1333320"
  },
  {
    "text": "on or you can configure your file host deliver stream to back up all the raw",
    "start": "1333320",
    "end": "1339000"
  },
  {
    "text": "data into an S3 bucket concurrently as when it is delivering the same documents",
    "start": "1339000",
    "end": "1345279"
  },
  {
    "text": "to the elas search cluster this is very useful for a set of use cases where",
    "start": "1345279",
    "end": "1351279"
  },
  {
    "text": "customers want to use elas search for Real Time data query and analytics and",
    "start": "1351279",
    "end": "1357600"
  },
  {
    "text": "at the same time they want to keep a set of data in the S3 bucket for more",
    "start": "1357600",
    "end": "1363559"
  },
  {
    "text": "complicated analysis through EMR or Hadoop jobs and similarly you can either",
    "start": "1363559",
    "end": "1370000"
  },
  {
    "text": "choose or create a new S3 bucket through the fire host console and you can also",
    "start": "1370000",
    "end": "1375559"
  },
  {
    "text": "specify custom prefix to distinguish the objects in your S3",
    "start": "1375559",
    "end": "1382440"
  },
  {
    "text": "bucket okay then step two the configuration page um compared to the S3",
    "start": "1383320",
    "end": "1389840"
  },
  {
    "text": "and a rift configuration page now we also have a buffering option for elas",
    "start": "1389840",
    "end": "1395360"
  },
  {
    "text": "search and that'll be different from S3 buffering you can use a different set of",
    "start": "1395360",
    "end": "1400520"
  },
  {
    "text": "settings again as I can mentioned customers often use elas search for more",
    "start": "1400520",
    "end": "1405559"
  },
  {
    "text": "real time analytical use cases so often they choose to use a relatively small",
    "start": "1405559",
    "end": "1411600"
  },
  {
    "text": "buffer size and shorter time interval so that the data can be delivered to the",
    "start": "1411600",
    "end": "1417000"
  },
  {
    "text": "last search cluster more frequently and in a more real time Manner and on the other side for the S3 buffering option",
    "start": "1417000",
    "end": "1425080"
  },
  {
    "text": "we often see customers using a relatively large buffering size and",
    "start": "1425080",
    "end": "1430240"
  },
  {
    "text": "longer buffering interval because that way it's more efficient for S3 to store",
    "start": "1430240",
    "end": "1435600"
  },
  {
    "text": "and for the downstream applications to process these objects similarly you'll configure the",
    "start": "1435600",
    "end": "1442960"
  },
  {
    "text": "compression encryption and error loging option uh for this particular delivery",
    "start": "1442960",
    "end": "1448320"
  },
  {
    "text": "stream and lastly specify an IM role for resource control and a security",
    "start": "1448320",
    "end": "1456559"
  },
  {
    "text": "control so these are the three destinations the fire host service support today as you can see it only",
    "start": "1457039",
    "end": "1465120"
  },
  {
    "text": "takes about 60 seconds for you to create this data pipeline um through the F host",
    "start": "1465120",
    "end": "1471159"
  },
  {
    "text": "console and behind the scene you do not need to manage any infrastructure or",
    "start": "1471159",
    "end": "1476840"
  },
  {
    "text": "servers and it's entirely serverless and also as you scale up your",
    "start": "1476840",
    "end": "1482399"
  },
  {
    "text": "data traffic volume the service will scale up the delivery stream on your behalf you don't need to worry about",
    "start": "1482399",
    "end": "1489000"
  },
  {
    "text": "under underlying infrastructure or storage",
    "start": "1489000",
    "end": "1494159"
  },
  {
    "text": "space now I'll talk a bit about the Kinesis agent as I mentioned the main mechanism for",
    "start": "1494159",
    "end": "1501279"
  },
  {
    "text": "you to send data to your file host delivery stream is through the F host",
    "start": "1501279",
    "end": "1506640"
  },
  {
    "text": "API cost and we offer two put API cost the first one is called put record what",
    "start": "1506640",
    "end": "1513320"
  },
  {
    "text": "allows you to pass a single data record through an API call and the second API",
    "start": "1513320",
    "end": "1519600"
  },
  {
    "text": "call is called put record badge as indicated in the name you can pass",
    "start": "1519600",
    "end": "1525720"
  },
  {
    "text": "multiple data records within the same sing API call and you can often achieve more efficiency and higher throughput",
    "start": "1525720",
    "end": "1532840"
  },
  {
    "text": "through that API call now other than the two API calls we provide here atesus",
    "start": "1532840",
    "end": "1539960"
  },
  {
    "text": "firehost we also have aesis agent it is a Java software",
    "start": "1539960",
    "end": "1546320"
  },
  {
    "text": "library that helps you to collect and monitor and send data records to your",
    "start": "1546320",
    "end": "1553360"
  },
  {
    "start": "1547000",
    "end": "1547000"
  },
  {
    "text": "fold to rest stream more easily what it does is you install the software agent",
    "start": "1553360",
    "end": "1560480"
  },
  {
    "text": "on your servers and configure it to monitor a set of files and as new data",
    "start": "1560480",
    "end": "1567880"
  },
  {
    "text": "gets written into these files the Kinesis agent will",
    "start": "1567880",
    "end": "1573600"
  },
  {
    "text": "automatically pick up these new data records and send it to your delivery",
    "start": "1573600",
    "end": "1579240"
  },
  {
    "text": "stream the agent also handles file rotation checkpointing and retry upon",
    "start": "1579240",
    "end": "1586600"
  },
  {
    "text": "failures and uh early this year we also added a pre-processing capability to",
    "start": "1586600",
    "end": "1592559"
  },
  {
    "text": "this agent so for certain uh processing logic such as par and convert Apache log",
    "start": "1592559",
    "end": "1600720"
  },
  {
    "text": "and CIS log into Json format these are building to the agent you can just",
    "start": "1600720",
    "end": "1606000"
  },
  {
    "text": "configure the agent to do that at as it transmits the data into your file host",
    "start": "1606000",
    "end": "1611320"
  },
  {
    "text": "dely stream so it's particularly useful for set of use cases where customers",
    "start": "1611320",
    "end": "1617000"
  },
  {
    "text": "want to analyze the log files the Kinesis agent will be installed on the",
    "start": "1617000",
    "end": "1622919"
  },
  {
    "text": "server hosts and monitor a set of log files as the new log entry is being",
    "start": "1622919",
    "end": "1628600"
  },
  {
    "text": "written into the log files the agent will pick up the log line and convert",
    "start": "1628600",
    "end": "1633760"
  },
  {
    "text": "that into a Json object and stream that directly into your file host D stream",
    "start": "1633760",
    "end": "1639880"
  },
  {
    "text": "and behind the scene you can configure a fold D stream to stream the data into",
    "start": "1639880",
    "end": "1644919"
  },
  {
    "text": "the lastas search service for direct anal is so it'll probably take less than an hour for you to set up a streaming",
    "start": "1644919",
    "end": "1651760"
  },
  {
    "text": "data pipeline to analyze your Law data in your last search service or rare",
    "start": "1651760",
    "end": "1658440"
  },
  {
    "text": "shift and for this Kinesis agent it'll also emit AWS cloudwatch metrics for you",
    "start": "1658440",
    "end": "1665039"
  },
  {
    "text": "to monitor the performance of the agent and again it's particularly useful when",
    "start": "1665039",
    "end": "1670240"
  },
  {
    "text": "you want to troubleshoot your agent and the data producer behaviors and this Kinesis agent not",
    "start": "1670240",
    "end": "1676679"
  },
  {
    "text": "only works with with the Kinesis fire host service it also works with the Kinesis stream service so it'll also",
    "start": "1676679",
    "end": "1683360"
  },
  {
    "text": "send data to kesa stream so this is about agent the next",
    "start": "1683360",
    "end": "1691120"
  },
  {
    "text": "I'll talk a little bit about the um fire host monitoring",
    "start": "1691120",
    "end": "1696720"
  },
  {
    "text": "experience so as you can see um the goal of the fire host service is to make it",
    "start": "1696720",
    "end": "1704720"
  },
  {
    "text": "really simple and easy for you to set it up in user service the service is trying",
    "start": "1704720",
    "end": "1711080"
  },
  {
    "text": "to manage as many things as possible behind the scen so that you don't need",
    "start": "1711080",
    "end": "1716559"
  },
  {
    "text": "to worry about that and the tradeoff for simpl Simplicity is often transparency",
    "start": "1716559",
    "end": "1723120"
  },
  {
    "text": "and visibility into the service so at aesis file host we do provide um a",
    "start": "1723120",
    "end": "1730320"
  },
  {
    "text": "pretty transparent monitoring experience so you get the benefit of a fully managed service but at the same time",
    "start": "1730320",
    "end": "1737519"
  },
  {
    "text": "time you clearly know what's going on behind the",
    "start": "1737519",
    "end": "1743240"
  },
  {
    "start": "1743000",
    "end": "1743000"
  },
  {
    "text": "scene we first offer Matrix monitoring through",
    "start": "1743240",
    "end": "1748440"
  },
  {
    "text": "cloudwatch Matrix so you can see the data records",
    "start": "1748440",
    "end": "1754039"
  },
  {
    "text": "traffic volume being ingested into your data dat stream you can see the data",
    "start": "1754039",
    "end": "1761159"
  },
  {
    "text": "flow through your S3 bucket to your R shift cluster or to your last search",
    "start": "1761159",
    "end": "1766799"
  },
  {
    "text": "cluster so this way you can clearly see the data flow through the whole Pipeline and see",
    "start": "1766799",
    "end": "1774240"
  },
  {
    "text": "where issue has happened if anything has gone wrong you can also monitor the data",
    "start": "1774240",
    "end": "1780640"
  },
  {
    "text": "freshness of the data being delivered into your destination so that you can",
    "start": "1780640",
    "end": "1785760"
  },
  {
    "text": "tell once the data is being delivered to your file a final",
    "start": "1785760",
    "end": "1791039"
  },
  {
    "text": "destination how long it has passed since the data is been ingested into the file",
    "start": "1791039",
    "end": "1797080"
  },
  {
    "text": "host service you can also monitor your data delivery success metric where you can",
    "start": "1797080",
    "end": "1804679"
  },
  {
    "text": "set alert arm so in cases where the data delivery to your destination fails",
    "start": "1804679",
    "end": "1810559"
  },
  {
    "text": "you'll receive an email or a text message or trigger Downstream workflow",
    "start": "1810559",
    "end": "1815880"
  },
  {
    "text": "on your side so this is cloudwatch",
    "start": "1815880",
    "end": "1821600"
  },
  {
    "text": "metrix as you see from the error login configuration option on the cre ation",
    "start": "1821640",
    "end": "1827679"
  },
  {
    "start": "1822000",
    "end": "1822000"
  },
  {
    "text": "flow of the delivery stream we also have this error logging feature through",
    "start": "1827679",
    "end": "1832919"
  },
  {
    "text": "cloudwatch logs you can view these set of error messages either through the",
    "start": "1832919",
    "end": "1838440"
  },
  {
    "text": "cloud watch console or through the fire host console so in this case I've set up",
    "start": "1838440",
    "end": "1845880"
  },
  {
    "text": "a f host Del stream using RF as a destination and as you can see two extra",
    "start": "1845880",
    "end": "1853039"
  },
  {
    "text": "tabs appears on this stream detail page one is S3 lock it locks the error",
    "start": "1853039",
    "end": "1860799"
  },
  {
    "text": "message where we deliver the data to the intermediate S3 bucket in the r shift",
    "start": "1860799",
    "end": "1867000"
  },
  {
    "text": "logs tab it logs the error message as fir host copies the data from the S3",
    "start": "1867000",
    "end": "1873760"
  },
  {
    "text": "bucket to the r shift cluster so in this case you can see from the error message after I configure the",
    "start": "1873760",
    "end": "1881480"
  },
  {
    "text": "fold delivery stream I actually deleted the S3 bucket so that it's like a fail",
    "start": "1881480",
    "end": "1887320"
  },
  {
    "text": "by intention you can see that fire host logs the error message here saying the",
    "start": "1887320",
    "end": "1893360"
  },
  {
    "text": "specified S3 bucket does not exist create a new bucket or use a different",
    "start": "1893360",
    "end": "1898399"
  },
  {
    "text": "bucket name that does exist so it'll become very handy when you set up an",
    "start": "1898399",
    "end": "1903799"
  },
  {
    "text": "alert when the data delivery fail and then you can come to this error login page to see the exact error message that",
    "start": "1903799",
    "end": "1912240"
  },
  {
    "text": "is triggered the delivery issue so in this case I'll just go ahead and create",
    "start": "1912240",
    "end": "1917760"
  },
  {
    "text": "a new bucket and update my f host delivery stream to deliver the data into",
    "start": "1917760",
    "end": "1923639"
  },
  {
    "text": "this new bucket so this is the um monitoring",
    "start": "1923639",
    "end": "1930120"
  },
  {
    "text": "experience for the fire host service so as you can see we're trying to expose as",
    "start": "1930120",
    "end": "1936120"
  },
  {
    "text": "many monitoring mechanisms as possible behind the scene for you to have a clear",
    "start": "1936120",
    "end": "1941279"
  },
  {
    "text": "view of what's going on as the data is being delivered through your F host delivery stream and we also offer a set",
    "start": "1941279",
    "end": "1949440"
  },
  {
    "text": "of troubleshooting features so in case when the data delivery fails you can",
    "start": "1949440",
    "end": "1954600"
  },
  {
    "text": "figure out the issue very quickly and resolve that",
    "start": "1954600",
    "end": "1959600"
  },
  {
    "text": "accordingly lastly I'll talk a bit about the Kinesis fos",
    "start": "1960039",
    "end": "1966919"
  },
  {
    "text": "pricing pricing is fairly simple as most of the adabas services it uses pay as",
    "start": "1966919",
    "end": "1973639"
  },
  {
    "start": "1967000",
    "end": "1967000"
  },
  {
    "text": "you go pricing model you don't pay for the underlying delivery stream and you",
    "start": "1973639",
    "end": "1979120"
  },
  {
    "text": "only pay for the amount of data that is being ingested through the F host",
    "start": "1979120",
    "end": "1985000"
  },
  {
    "text": "delivery Stream So This is a pricing sample in the US East one and US West 2",
    "start": "1985000",
    "end": "1992480"
  },
  {
    "text": "region for every gigabyte of data ingested through the fire host delivery",
    "start": "1992480",
    "end": "1998039"
  },
  {
    "text": "stream will charge 3.5 cents regardless of which destination you choose you'll",
    "start": "1998039",
    "end": "2005559"
  },
  {
    "text": "build by the same pricing Dimension and pricing points one small caveat for the F",
    "start": "2005559",
    "end": "2013519"
  },
  {
    "text": "pricing is for any set of data records that are smaller than 5 kilobytes they",
    "start": "2013519",
    "end": "2020440"
  },
  {
    "text": "will be runed up to 5 KB for billing purpose so for cost conscientious",
    "start": "2020440",
    "end": "2028080"
  },
  {
    "text": "customers we often see our customers to pre-at the data",
    "start": "2028080",
    "end": "2033679"
  },
  {
    "text": "records into close to five kilobytes so this way um you'll save cost on the F",
    "start": "2033679",
    "end": "2041320"
  },
  {
    "text": "host service so this is kind of the brief",
    "start": "2041320",
    "end": "2048878"
  },
  {
    "text": "overview and introduction to the Kinesis fire host service and hopefully after",
    "start": "2048879",
    "end": "2055560"
  },
  {
    "text": "this session you'll have a basic understanding of the streaming data scenario and the three services we offer",
    "start": "2055560",
    "end": "2063638"
  },
  {
    "text": "here at Amazon Kinesis and added the the same time have a good understanding of",
    "start": "2063639",
    "end": "2070040"
  },
  {
    "text": "the fire host service how it works behind the scene what are the set of",
    "start": "2070040",
    "end": "2075118"
  },
  {
    "text": "destinations we support at a fire host service and also have an understanding",
    "start": "2075119",
    "end": "2080398"
  },
  {
    "text": "of the monitoring experience and also the pricing dimension for the F host",
    "start": "2080399",
    "end": "2085480"
  },
  {
    "text": "service and before we get into the Q&A section I do want to mention that",
    "start": "2085480",
    "end": "2090679"
  },
  {
    "text": "tomorrow at the same time we'll have an webinar session particularly about a",
    "start": "2090679",
    "end": "2096358"
  },
  {
    "text": "streaming data analysis with the F host service and Rift service in tomorrow's",
    "start": "2096359",
    "end": "2102960"
  },
  {
    "text": "webinar I'm going to walk you through a stepbystep um process of using Kinesis",
    "start": "2102960",
    "end": "2110280"
  },
  {
    "text": "fire hose and rash shift to analyze Maryland's traffic violation data in a",
    "start": "2110280",
    "end": "2116640"
  },
  {
    "text": "streaming and realtime manner so for those of you who wants to go deeper on",
    "start": "2116640",
    "end": "2122160"
  },
  {
    "text": "the service and have some material and content to get started with a service I encourage you to attend tomorrow's",
    "start": "2122160",
    "end": "2128880"
  },
  {
    "text": "webinar session as well I think now we can uh open up to",
    "start": "2128880",
    "end": "2134359"
  },
  {
    "text": "questions all right well thank you Ray thanks for the great presentation thanks to Alan Roy for engaging the audience",
    "start": "2134359",
    "end": "2139760"
  },
  {
    "text": "throughout the presentation with our Q&A answers so again there some of you have noticed on our screen on the left hand",
    "start": "2139760",
    "end": "2145760"
  },
  {
    "text": "side we do have polls open we do value your feedback so please go ahead and take time and complete those you can",
    "start": "2145760",
    "end": "2151440"
  },
  {
    "text": "also add questions in the Q&A panel you'll see that right above the slide at the top of the screen so feel free fre",
    "start": "2151440",
    "end": "2156960"
  },
  {
    "text": "to input those in and we can continue on with our",
    "start": "2156960",
    "end": "2162039"
  },
  {
    "text": "Q&A so the first question is what are the recommendations if the",
    "start": "2165480",
    "end": "2172440"
  },
  {
    "text": "data record size is larger than 1,000 kilobytes now this is the current",
    "start": "2172440",
    "end": "2179599"
  },
  {
    "text": "limitation um on the record size for the file host service we often see customers",
    "start": "2179599",
    "end": "2185960"
  },
  {
    "text": "work around this limitation through two different cases number",
    "start": "2185960",
    "end": "2193040"
  },
  {
    "text": "one is to Chunk Up the data record itself and then stream the relatively",
    "start": "2193960",
    "end": "2199880"
  },
  {
    "text": "small record into the F host service or customers often use a",
    "start": "2199880",
    "end": "2205240"
  },
  {
    "text": "different data route for record that is relatively large for example if you are",
    "start": "2205240",
    "end": "2211040"
  },
  {
    "text": "streaming data through F host into your S3 bucket for the data record that is",
    "start": "2211040",
    "end": "2216839"
  },
  {
    "text": "larger than 1,000 kilobytes customers can have a separate flow to directly put",
    "start": "2216839",
    "end": "2222359"
  },
  {
    "text": "that data record into the S3 bucket and uh from the service",
    "start": "2222359",
    "end": "2227680"
  },
  {
    "text": "perspective we're constantly getting feedback from our customers um to raise up these limits um for example under the",
    "start": "2227680",
    "end": "2235960"
  },
  {
    "text": "kinesia stream service actually we started with a record siiz limit of 50 kilobytes and later on we raised that",
    "start": "2235960",
    "end": "2242760"
  },
  {
    "text": "into 1,000 kilobytes now as we continous getting feedback from our customers if",
    "start": "2242760",
    "end": "2249280"
  },
  {
    "text": "we see high demand of larger record size we'll certainly go ahead and raise the service limit down the",
    "start": "2249280",
    "end": "2257000"
  },
  {
    "text": "road then the next question is what is the difference between kesa stream and",
    "start": "2260000",
    "end": "2267160"
  },
  {
    "text": "Kinesis F host I kind of touch a little bit at the beginning of the the session and I'll go a bit deeper um now with",
    "start": "2267160",
    "end": "2275000"
  },
  {
    "text": "this question so so kesa stream it's open-ended service on both end on the",
    "start": "2275000",
    "end": "2282800"
  },
  {
    "text": "data producer side you'll configure data producer to write the data into the kesa",
    "start": "2282800",
    "end": "2288160"
  },
  {
    "text": "stream and the service will store your data in a continuous Manner and a",
    "start": "2288160",
    "end": "2293680"
  },
  {
    "text": "replayable manner with order retained on the other side you'll configure your",
    "start": "2293680",
    "end": "2299319"
  },
  {
    "text": "data consumer to read the data out of the stream and then process it with your",
    "start": "2299319",
    "end": "2305359"
  },
  {
    "text": "custom application so you can think of kesa stream as a stream data storage system it collects",
    "start": "2305359",
    "end": "2313240"
  },
  {
    "text": "the data and stores the data as a stream and then you'll use your custom application to read and process the data",
    "start": "2313240",
    "end": "2320839"
  },
  {
    "text": "it is fully flexible you can build whatever customer application use whatever logic that fits your business",
    "start": "2320839",
    "end": "2327520"
  },
  {
    "text": "use case and you have full control of the underlying stream how you partition your data how many shots you'll have for",
    "start": "2327520",
    "end": "2334240"
  },
  {
    "text": "your particular stream and you have full control and you'll build your own custom",
    "start": "2334240",
    "end": "2339319"
  },
  {
    "text": "application for that on the other side kesa fire host it's open-ended on one",
    "start": "2339319",
    "end": "2346160"
  },
  {
    "text": "side you'll configure your data producer to continuously push data into the fost",
    "start": "2346160",
    "end": "2352359"
  },
  {
    "text": "Z stream however on the other side you don't read data from the F host deliver",
    "start": "2352359",
    "end": "2357880"
  },
  {
    "text": "stream and you don't write in any application for that F host automatically delivers the data to your",
    "start": "2357880",
    "end": "2365000"
  },
  {
    "text": "destinations such as an S3 bucket a RI cluster or an El last search service",
    "start": "2365000",
    "end": "2371359"
  },
  {
    "text": "cluster so I would say for the set of use cases where you want to use your",
    "start": "2371359",
    "end": "2377359"
  },
  {
    "text": "data in data stores such as S3 R shift or elastic search but you want to get",
    "start": "2377359",
    "end": "2383319"
  },
  {
    "text": "that in real time manner then you should use the fost service it's easier to use",
    "start": "2383319",
    "end": "2389520"
  },
  {
    "text": "you don't need to manage the online infrastructure stream or write any application for that now for for more",
    "start": "2389520",
    "end": "2396920"
  },
  {
    "text": "custom use cases that goes beyond this set of the delivery scenarios then you",
    "start": "2396920",
    "end": "2402040"
  },
  {
    "text": "can go explore the kesa stream [Music]",
    "start": "2402040",
    "end": "2408440"
  },
  {
    "text": "service so the next question is I am familiar with kafa and Elk can you draw",
    "start": "2408440",
    "end": "2416640"
  },
  {
    "text": "a comparison between those and Kinesis would you compare Kinesis agent to be",
    "start": "2416640",
    "end": "2422160"
  },
  {
    "text": "like Kafka or lock stash so for those of you are not familiar with okay it stands",
    "start": "2422160",
    "end": "2428440"
  },
  {
    "text": "for elas search Lodge stash and Cabana it's a set of um tools offered by elas",
    "start": "2428440",
    "end": "2434200"
  },
  {
    "text": "search uh for log Antics purpose so CFA is often use for as a",
    "start": "2434200",
    "end": "2441560"
  },
  {
    "text": "comparison for Kinesis streams where you can think of it as a again as a",
    "start": "2441560",
    "end": "2447079"
  },
  {
    "text": "streaming data storage system or a messaging C so as you can tell the file",
    "start": "2447079",
    "end": "2453280"
  },
  {
    "text": "host to elas search service is the the targeted use case for E kind of",
    "start": "2453280",
    "end": "2460240"
  },
  {
    "text": "scenario so as you know for large scale use cases for the Elk",
    "start": "2460240",
    "end": "2466960"
  },
  {
    "text": "cluster other than the components of e lnk people often will have a messaging",
    "start": "2466960",
    "end": "2473839"
  },
  {
    "text": "queue sitting in front of the large stash to handle the large scale and that",
    "start": "2473839",
    "end": "2479040"
  },
  {
    "text": "messaging queue will often be cka on Prem so the objective of f host to",
    "start": "2479040",
    "end": "2484839"
  },
  {
    "text": "elastic search is to replace the kka cluster replace the log stash so that",
    "start": "2484839",
    "end": "2491880"
  },
  {
    "text": "you don't need to manage these infrastructures in between all you need to do is as I showed you spend 60",
    "start": "2491880",
    "end": "2498119"
  },
  {
    "text": "seconds to create a f host delivery stream and pointing that to the delivery",
    "start": "2498119",
    "end": "2503520"
  },
  {
    "text": "to your elas search cluster then fire hostes will automatically scale handle very large scale and stream the data",
    "start": "2503520",
    "end": "2511079"
  },
  {
    "text": "into your lastas search index at the same time optionally back up your data in into",
    "start": "2511079",
    "end": "2517160"
  },
  {
    "text": "S3 now the pre-processing agent on the Kus the pre-processing feature on the",
    "start": "2517160",
    "end": "2524000"
  },
  {
    "text": "kis agent is kind of equivalent feature for lock stash because last search",
    "start": "2524000",
    "end": "2530800"
  },
  {
    "text": "cluster only takes Json format object and log lines are often not generated in",
    "start": "2530800",
    "end": "2536400"
  },
  {
    "text": "Json format so the Kinesis agent basically handle what you will actually",
    "start": "2536400",
    "end": "2542680"
  },
  {
    "text": "use the Glock function in log stash it'll pass log line convert that into",
    "start": "2542680",
    "end": "2547839"
  },
  {
    "text": "the Json format and stream that directly through the F host service and the elas",
    "start": "2547839",
    "end": "2554079"
  },
  {
    "text": "cluster so again to restate and in summarize uh what I just said the",
    "start": "2554079",
    "end": "2559280"
  },
  {
    "text": "objective is to make fire hose to be the ingestion pipeline into your elas",
    "start": "2559280",
    "end": "2565359"
  },
  {
    "text": "cluster so that it's much easier for you to use and you don't need to manage any infrastructure in between your data",
    "start": "2565359",
    "end": "2571800"
  },
  {
    "text": "producer and El last search cluster",
    "start": "2571800",
    "end": "2577720"
  },
  {
    "text": "the next question is can you please repeat what Kinesis agent is used for",
    "start": "2577720",
    "end": "2583480"
  },
  {
    "text": "sure so Kinesis agent is a Java software package where you install it on your",
    "start": "2583480",
    "end": "2591280"
  },
  {
    "text": "servers the agent will monitor a set of files on the local disk of your server",
    "start": "2591280",
    "end": "2598079"
  },
  {
    "text": "and as new data gets written into these files the agent will automatically pick",
    "start": "2598079",
    "end": "2603760"
  },
  {
    "text": "up these new data entries and stream the data into the kesa services so as",
    "start": "2603760",
    "end": "2611280"
  },
  {
    "text": "similarly to the answer to the previous question the agent is particularly useful for the log analytics use cases",
    "start": "2611280",
    "end": "2619079"
  },
  {
    "text": "you can configure your Kinesis agent to monitor a set of local log files on your",
    "start": "2619079",
    "end": "2625359"
  },
  {
    "text": "for example web server or application servers as new data gets written into",
    "start": "2625359",
    "end": "2631559"
  },
  {
    "text": "these log files the agent will pick up these new log lines conver convert that into a Json format for example and",
    "start": "2631559",
    "end": "2639480"
  },
  {
    "text": "stream that into the file host stream and from the file host stream the",
    "start": "2639480",
    "end": "2644559"
  },
  {
    "text": "service will stream the data directly into your last search cluster so depending on your buffering",
    "start": "2644559",
    "end": "2650520"
  },
  {
    "text": "configuration after seconds or minutes these log lines will show up in your",
    "start": "2650520",
    "end": "2656480"
  },
  {
    "text": "last search index that you can search for so it's particularly powerful for these set of use",
    "start": "2656480",
    "end": "2663280"
  },
  {
    "text": "cases so I think uh these are the questions that I get and I think all the other questions U Roy and Adam have",
    "start": "2663280",
    "end": "2671400"
  },
  {
    "text": "answered offline so again thanks everybody for um coming to today's webinar session and this concludes our",
    "start": "2671400",
    "end": "2678079"
  },
  {
    "text": "session today and thank you Ray and thanks for the uh presentation today and thanks",
    "start": "2678079",
    "end": "2683280"
  },
  {
    "text": "Alan Roy for running through our Q&A today I want to thank our audience for attending today thanks for taking your",
    "start": "2683280",
    "end": "2689240"
  },
  {
    "text": "time and completing our polls on the left side if you haven't completed that poll please take time to do that now",
    "start": "2689240",
    "end": "2695319"
  },
  {
    "text": "thanks to everybody for joining have a great day if you have any comments on our previous and upcoming webinars or",
    "start": "2695319",
    "end": "2700839"
  },
  {
    "text": "suggestions on the topics that you wish AWS to cover in future webinars please",
    "start": "2700839",
    "end": "2706240"
  },
  {
    "text": "feel free to email to AWS webcast amazon.com your feedback will be very",
    "start": "2706240",
    "end": "2713720"
  },
  {
    "text": "important to us to help improve our webinar going forward so thanks again have a great day and at this time you",
    "start": "2713720",
    "end": "2719200"
  },
  {
    "text": "can disconnect",
    "start": "2719200",
    "end": "2722838"
  }
]