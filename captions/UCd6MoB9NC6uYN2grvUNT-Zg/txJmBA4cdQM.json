[
  {
    "start": "0",
    "end": "21000"
  },
  {
    "text": "welcome everyone to a game 402 turbine a micro service approach to 3 billion game",
    "start": "199",
    "end": "5700"
  },
  {
    "text": "requests per day I'm Romesh McCullough staff software engineer at turbine i'm",
    "start": "5700",
    "end": "11460"
  },
  {
    "text": "william de a senior software engineer on the WT play team and I'm Evan peep out the engineering lead for the mobile game",
    "start": "11460",
    "end": "17430"
  },
  {
    "text": "platform server Tim so what to expect from this session we're going to talk a",
    "start": "17430",
    "end": "23730"
  },
  {
    "start": "21000",
    "end": "21000"
  },
  {
    "text": "bit about handling gaming workloads on Amazon we're going to share a bunch of personal experiences from a triple a",
    "start": "23730",
    "end": "29699"
  },
  {
    "text": "multi-platform game launch unfortunately due to studio policy i'm not allowed to name the games and then also just to",
    "start": "29699",
    "end": "35640"
  },
  {
    "text": "unpack this a little bit or anybody that's not in the game industry triple a's sort of just a way to denote like a larger budget or production I'm kind of",
    "start": "35640",
    "end": "42420"
  },
  {
    "text": "like calling a movie a blockbuster and the multi-platform in this case we're talking about console so Xbox and",
    "start": "42420",
    "end": "47809"
  },
  {
    "text": "Playstation we're also doing pc mobile iOS android we're going to overview our",
    "start": "47809",
    "end": "53760"
  },
  {
    "text": "new containerized service platform running on core OS and AWS and they're also going to talk a little bit about",
    "start": "53760",
    "end": "59340"
  },
  {
    "text": "scaling MongoDB and some of the challenges and strategies we found for some specific issues that we hit that were a little more specific I think to",
    "start": "59340",
    "end": "65970"
  },
  {
    "text": "handling gaming workloads so this talk is gonna be arranged into a handful of",
    "start": "65970",
    "end": "72540"
  },
  {
    "text": "specific challenges no I just want to overview them quickly for everyone to so it makes sense when we're going through so we're gonna talk about managing",
    "start": "72540",
    "end": "79380"
  },
  {
    "start": "77000",
    "end": "77000"
  },
  {
    "text": "multiple games on AWS and part of what we're talking about is it necessarily provisioning but like how would you actually structure things for the good",
    "start": "79380",
    "end": "85259"
  },
  {
    "text": "multi studio multi-tenant type setup or talk about scale as a challenge like what you don't know what to expect and",
    "start": "85259",
    "end": "92549"
  },
  {
    "text": "how you can sort of use some of the features at amazon to make that a something a little singer that you",
    "start": "92549",
    "end": "98280"
  },
  {
    "text": "handle also picking your battles there's a number of things going into this launch that i mentioned that we could",
    "start": "98280",
    "end": "104880"
  },
  {
    "text": "have done and we were very specific about what we chose to solvin what we did not and we're going to go through a",
    "start": "104880",
    "end": "109920"
  },
  {
    "text": "little bit about what we did choose to solve and that's where we'll start talking about the containerized service platform as well like I said MongoDB yet",
    "start": "109920",
    "end": "117509"
  },
  {
    "text": "scale and the last one staying up through launch I think everyone that's",
    "start": "117509",
    "end": "122579"
  },
  {
    "text": "familiar with games or the gaming industry is well aware of launch time outages and how common they're becoming",
    "start": "122579",
    "end": "128009"
  },
  {
    "text": "and so we'll share some of the strategies we found to help control what's happening",
    "start": "128009",
    "end": "133800"
  },
  {
    "text": "little bit so quickly a little bit about us we are turbine we've been around for a while founded in nineteen ninety-four",
    "start": "133800",
    "end": "139410"
  },
  {
    "start": "136000",
    "end": "136000"
  },
  {
    "text": "we are purchased by Warner Brothers in 2010 we've been doing this for a while I don't know people remember Asheron's",
    "start": "139410",
    "end": "145440"
  },
  {
    "text": "Call was very early mm oh so we're very experienced with those large properties as well as lots of online real-time",
    "start": "145440",
    "end": "150690"
  },
  {
    "text": "games at scale we do work with large properties Lord of the Rings is a big one dungeon dragons DC Comics and we're",
    "start": "150690",
    "end": "157530"
  },
  {
    "text": "about to launch our first mobile game batman arkham underworld and we'll be talking a little bit about that as well",
    "start": "157530",
    "end": "163070"
  },
  {
    "text": "but across all of door warner brothers turbine actually does quite well a lot of stuff we work with a number of",
    "start": "163070",
    "end": "169290"
  },
  {
    "text": "different games with a number of different platforms eyes when I walk you through quickly what those are we have",
    "start": "169290",
    "end": "176070"
  },
  {
    "start": "173000",
    "end": "173000"
  },
  {
    "text": "the WB play game stack so a customized cross-platform cross studio game back end route and analytics and that's",
    "start": "176070",
    "end": "181440"
  },
  {
    "text": "something that will is going to talk about we also have the mobile game platform so the next-gen game back end",
    "start": "181440",
    "end": "186840"
  },
  {
    "text": "is specifically geared for the turbine mobile games and that's what Evans going to go over but additionally at turbine",
    "start": "186840",
    "end": "192660"
  },
  {
    "text": "we also have a pretty significant analytics platform for crunching game data across all of the Warner Brothers and directed organization we also do the",
    "start": "192660",
    "end": "200430"
  },
  {
    "text": "W play identity sex so that is a lot of identity account management and e-commerce but we're really gonna be",
    "start": "200430",
    "end": "206790"
  },
  {
    "text": "focusing on the first two that's going to be a lot of what we're talking about so just to overview those a little bit",
    "start": "206790",
    "end": "211980"
  },
  {
    "start": "210000",
    "end": "210000"
  },
  {
    "text": "right now they're going to be powering batman arkham underworld and this unnamed triple a multi-platform game",
    "start": "211980",
    "end": "217020"
  },
  {
    "text": "that game is going to be kind of the context of or that launch will be the context of everything we're talking",
    "start": "217020",
    "end": "222060"
  },
  {
    "text": "about how we got ready for it how we handle things that happen how we handle triage and all that stuff and a little",
    "start": "222060",
    "end": "227820"
  },
  {
    "text": "bit of an overview of what they are providing for those games does stuff like anything you need to online enable your your game so authentication",
    "start": "227820",
    "end": "235320"
  },
  {
    "text": "profiles matchmaking match history Guild cross-platform unlocks feeds near the hole yeah everything you need and so",
    "start": "235320",
    "end": "243750"
  },
  {
    "text": "since we are turbine we've been doing this for a long time we do have data centers we do have existing",
    "start": "243750",
    "end": "248940"
  },
  {
    "text": "infrastructure so why do we want to go to AWS I know if it's not something a super focused on I'm sure everyone here",
    "start": "248940",
    "end": "255330"
  },
  {
    "text": "is pretty familiar with it but some of the reasons we had for taking this specific launch out there had some",
    "start": "255330",
    "end": "261419"
  },
  {
    "text": "concerns about datacenter capacity we do have multiple data centers but we're not used to",
    "start": "261419",
    "end": "266580"
  },
  {
    "text": "console pc and mobile all of us so there's a sort of traffic profile that we weren't super familiar with related",
    "start": "266580",
    "end": "273900"
  },
  {
    "text": "to that it was an unknown info structure of a print we weren't sure if we would actually be able to scale within our own",
    "start": "273900",
    "end": "279780"
  },
  {
    "text": "existing infrastructure so we wanted to make sure that we had the best chance of actually being go do that additionally",
    "start": "279780",
    "end": "285539"
  },
  {
    "text": "since we've had a platform running for a little while we don't have a lot of great automation tools for our physical infrastructure today and we didn't have",
    "start": "285539",
    "end": "291780"
  },
  {
    "text": "the time to get there before we had to launch so you know that was really helpful gaming we're closer very bursty",
    "start": "291780",
    "end": "297509"
  },
  {
    "text": "you know you're going to see we see more traffic on day one than you see basically ever after that so going from zero to everything is a non-trivial",
    "start": "297509",
    "end": "304139"
  },
  {
    "text": "challenge and then of course just multiple availability those you can build in multi AZ and you have to real",
    "start": "304139",
    "end": "310289"
  },
  {
    "text": "work so when we run multiple data centers we still have even more presents that we can leverage so I'm gonna take",
    "start": "310289",
    "end": "317280"
  },
  {
    "text": "this right into our first challenge how do we actually set up AWS it's something more for like I said more fundamental",
    "start": "317280",
    "end": "323310"
  },
  {
    "start": "318000",
    "end": "318000"
  },
  {
    "text": "than using an API to provision servers like we need to support multiple games",
    "start": "323310",
    "end": "328650"
  },
  {
    "text": "multiple studios with multiple environments and multiple teams that's a lot of stuff and we weren't really sure how to structure our accounts up front",
    "start": "328650",
    "end": "335340"
  },
  {
    "text": "to make sure that we don't have anything clash one game because we work across Studios can't affect another there's a",
    "start": "335340",
    "end": "342270"
  },
  {
    "text": "lot of way that it could specifically like service limits and accounts or just the things that can happen when you're",
    "start": "342270",
    "end": "347879"
  },
  {
    "text": "running a lot of environments with a lot of people that have to access it and of course access control we need to figure",
    "start": "347879",
    "end": "353520"
  },
  {
    "text": "out how we wanted to solve that and then part of working for Warner Brothers as",
    "start": "353520",
    "end": "358650"
  },
  {
    "text": "they have some rules about how and where we're allowed to store our source code so we need to be able to access some",
    "start": "358650",
    "end": "364020"
  },
  {
    "text": "centralized services we weren't able to use some of the public cloud services out there so working with our solutions",
    "start": "364020",
    "end": "370949"
  },
  {
    "text": "architect we actually settled on multiple us accounts we have one account per game so every title get just own",
    "start": "370949",
    "end": "376889"
  },
  {
    "start": "371000",
    "end": "371000"
  },
  {
    "text": "account with its own service limits we said a VPC for environment we tend to be you know five dev QA prod proof um we",
    "start": "376889",
    "end": "385349"
  },
  {
    "text": "have a separate account just for core services some good sides service limits",
    "start": "385349",
    "end": "390690"
  },
  {
    "text": "can't clash so if we have a major scaling event in one game because you never know it can't prevent a major",
    "start": "390690",
    "end": "396899"
  },
  {
    "text": "scaling event in another the same time and that's extremely important when you have to work with multiple large projects projects at the",
    "start": "396899",
    "end": "402610"
  },
  {
    "text": "same time there are some cons though it's a lot of Management there's a lot of stuff you really need to automate it",
    "start": "402610",
    "end": "410160"
  },
  {
    "text": "but in the end we felt that if quantity was the thing that we were really concerned about that that's an",
    "start": "410160",
    "end": "415600"
  },
  {
    "text": "orchestration problem and we can solve orchestration problems and unfortunately is not even actually the exact topic of",
    "start": "415600",
    "end": "420640"
  },
  {
    "text": "this top and I'm going to share a few things about it and it's just kind of visualize what this looks like so we",
    "start": "420640",
    "end": "426100"
  },
  {
    "text": "have our one ops VPC use VPC pairing set",
    "start": "426100",
    "end": "431500"
  },
  {
    "text": "up a second account 11 note each of those eee pc they're actually all separate bbc's we just sort of condensed the slides to make it fun were clear but",
    "start": "431500",
    "end": "438790"
  },
  {
    "text": "you know they can talk over bpc peering is all private IP s it you know makes it really easy to discover stuff you can use route 53 private zones to direct",
    "start": "438790",
    "end": "444940"
  },
  {
    "text": "traffic as you need very specifically also we're leveraging the kind of inherent blast radius of a VPC so by",
    "start": "444940",
    "end": "453040"
  },
  {
    "text": "choice dev can't talk too broad can't talk to her we could allow it we elect",
    "start": "453040",
    "end": "458410"
  },
  {
    "text": "not to for a number of reasons you know blast radius you can't miss configure something to talk across account of a",
    "start": "458410",
    "end": "463660"
  },
  {
    "text": "prod app can talk to a dead database that's important and we get that for free which is really nice set up another",
    "start": "463660",
    "end": "470320"
  },
  {
    "text": "account it sprays basically identical PPC peering very very specifically they",
    "start": "470320",
    "end": "476410"
  },
  {
    "text": "can't talk across again could but we don't want them to and that that gives",
    "start": "476410",
    "end": "481900"
  },
  {
    "text": "us a lot of the isolation that we need as a company to run a number of you know major titles simultaneously so one issue",
    "start": "481900",
    "end": "491220"
  },
  {
    "start": "489000",
    "end": "489000"
  },
  {
    "text": "probably not new to a lot of anybody that's set up an actual network before but running that centralized bpc a rep",
    "start": "491220",
    "end": "498520"
  },
  {
    "text": "table router tables need to have unique entries to route to every other VPC so you know we had to pre provision that at",
    "start": "498520",
    "end": "505540"
  },
  {
    "text": "first we had some organic growth early on that actually caused a problem there's a little bit of an overview but we did we had 5 / so 1 / 20 /",
    "start": "505540",
    "end": "512680"
  },
  {
    "text": "environment we proved it in five environments up front we divided it into three public in three private subnets we",
    "start": "512680",
    "end": "518140"
  },
  {
    "text": "can put a public a whole bees and publix on that and we can use you know private resources most of our resources live in private sub that's so they're just nicely isolated from the I ahead in",
    "start": "518140",
    "end": "524110"
  },
  {
    "text": "general and we also came up with a bit mask early on to this pre-compute",
    "start": "524110",
    "end": "529150"
  },
  {
    "text": "probably the next five years of our growth and we can look at ni p we know what environments is in what region it's in",
    "start": "529150",
    "end": "534370"
  },
  {
    "text": "and that that was actually really helpful we'd have it mapped and we're going so let's do a little bit of an",
    "start": "534370",
    "end": "544330"
  },
  {
    "text": "architecture overview of what we're about what we're going to talk about as far as the core systems and platforms that I mentioned earlier 10,000 square",
    "start": "544330",
    "end": "552490"
  },
  {
    "text": "foot view like to very very high level you'll see we have the wvg stack on the left MGP stack on the right a lot of our",
    "start": "552490",
    "end": "560230"
  },
  {
    "text": "talk is going to be going through a reasoning for why we actually needed to add that fork on the right and how that",
    "start": "560230",
    "end": "565750"
  },
  {
    "text": "was actually extremely valuable for us and some of the issues that it let us not even have to worry about so I hand",
    "start": "565750",
    "end": "574269"
  },
  {
    "text": "over to will he's going to talk about the wbg stack thanks for me so I'm going",
    "start": "574269",
    "end": "581230"
  },
  {
    "text": "to talk about this left fork here the wbg stack and how we dealt with scaling this up and prod this is a pretty old",
    "start": "581230",
    "end": "589149"
  },
  {
    "start": "588000",
    "end": "588000"
  },
  {
    "text": "code base going back while it's primarily the game back end and analytics data it's a monolithic code",
    "start": "589149",
    "end": "595000"
  },
  {
    "text": "base that goes back to 2006 it supported a lot of games it's evolved a lot over time and seeing a lot of dev teams a lot",
    "start": "595000",
    "end": "601630"
  },
  {
    "text": "of different games it's Python on Linux pretty standard it's using MongoDB and",
    "start": "601630",
    "end": "607060"
  },
  {
    "text": "Redis for storage and because it's evolved over time it has a lot of code inside to handle different binary",
    "start": "607060",
    "end": "613029"
  },
  {
    "text": "message formats for different games and different environments it's got SDKs out in the wild for different consoles",
    "start": "613029",
    "end": "619029"
  },
  {
    "text": "different mobile games over time and the configuration of it was a lot of Chef",
    "start": "619029",
    "end": "624339"
  },
  {
    "text": "solo so we had a lot of things to think about with this stack conceptually it's",
    "start": "624339",
    "end": "630880"
  },
  {
    "text": "pretty straightforward we have a proxy layer that's handling traffic it hands into RabbitMQ sign up for anyone doesn't",
    "start": "630880",
    "end": "636940"
  },
  {
    "text": "know RabbitMQ is a message bus that supports the amqp protocol it can route messages from the proxy layer to the",
    "start": "636940",
    "end": "642820"
  },
  {
    "text": "services registered with the bus and then all of the services that are attached to RabbitMQ are handing that",
    "start": "642820",
    "end": "647970"
  },
  {
    "text": "doing that work in a shared data layer which in this case is MongoDB all of the services are sharing the same data layer",
    "start": "647970",
    "end": "655350"
  },
  {
    "text": "well how do you take something conceptual like that and run it at scale for a game well we need high",
    "start": "655649",
    "end": "662079"
  },
  {
    "text": "availability we want to isolate it from the public internet we're going to have to balance load across the proxies and the services",
    "start": "662079",
    "end": "669000"
  },
  {
    "text": "we're going to need to scale all the application components we're going to scale MongoDB we're going to need to",
    "start": "669000",
    "end": "675370"
  },
  {
    "text": "automate our configuration management and we're going to need Redis to be highly available this is a lot of things",
    "start": "675370",
    "end": "680470"
  },
  {
    "text": "to care about for this stack but we used a lot of the Amazon features to sort of cross things off the list that we had to",
    "start": "680470",
    "end": "685960"
  },
  {
    "text": "care about high availability we can use a Z's we can use a lot of the Amazon features with API is just and make that",
    "start": "685960",
    "end": "691960"
  },
  {
    "text": "not a problem for us to care about we put it in be pcs to get isolated from the public internet also something we don't have to care about uz lbs for load",
    "start": "691960",
    "end": "699370"
  },
  {
    "text": "balancing also not a problem that we have to think about we use baked am eyes so we had to configure things once and",
    "start": "699370",
    "end": "705580"
  },
  {
    "text": "then not automate configuration and prod and we use the last of cash for highly available Redis we're down to two",
    "start": "705580",
    "end": "711790"
  },
  {
    "text": "problems that we have to solve at this point so this is what it looks like when we implement it and you're not meant to",
    "start": "711790",
    "end": "718630"
  },
  {
    "text": "actually take this all in but what you're meant to take in is this is a lot of stuff to care about and we were able to use a lot of the Amazon ap is to",
    "start": "718630",
    "end": "725020"
  },
  {
    "text": "focus down on the things we actually want to care about which is the application stack we still have a lot going on here we have you know groups of",
    "start": "725020",
    "end": "732520"
  },
  {
    "text": "proxies talking to RabbitMQ in different az's the services registered in different az's and subnets we still have",
    "start": "732520",
    "end": "738250"
  },
  {
    "text": "a lot of to care about we still have a lot to scale so what's the big",
    "start": "738250",
    "end": "744970"
  },
  {
    "text": "challenge with scale for gaming in this case where we're doing a multi-platform launch well day one patches are a thing",
    "start": "744970",
    "end": "752680"
  },
  {
    "text": "we have game clients being finalized close to launch we have a lot of",
    "start": "752680",
    "end": "758080"
  },
  {
    "text": "different builds of clients that are coming in from different teams all over the place different platforms are handling the game in different ways you",
    "start": "758080",
    "end": "764320"
  },
  {
    "text": "can using the API is differently in the different clients the different ports for the different clients are being",
    "start": "764320",
    "end": "770170"
  },
  {
    "text": "handled by different studios so we have a lot of variation going on and how the different the one game is going to use",
    "start": "770170",
    "end": "776650"
  },
  {
    "text": "the same stack so it's really hard to establish metric baselines to understand how to scale everything when you have",
    "start": "776650",
    "end": "781870"
  },
  {
    "text": "this much going on and coming in this hot at launch we have to put bounds on",
    "start": "781870",
    "end": "788230"
  },
  {
    "start": "787000",
    "end": "787000"
  },
  {
    "text": "stuff so we you know first thing we did for scaling you said what are the what are the absolute bounds of things that will need one thing you'll be pre",
    "start": "788230",
    "end": "794620"
  },
  {
    "text": "warming saying do it you need it we also said what we thought were crazy high ec2 instance",
    "start": "794620",
    "end": "800759"
  },
  {
    "text": "limits for note at peak we had but 88c for ATX larges and 72 hour 38 x largest",
    "start": "800759",
    "end": "808470"
  },
  {
    "text": "so these limits we really needed to bump them up not real easy to do and we had about almost 200 terabytes of GP to",
    "start": "808470",
    "end": "814079"
  },
  {
    "text": "allocated for the game and just followed the standard practice of randomizing your s3 buckets to scale out the",
    "start": "814079",
    "end": "820529"
  },
  {
    "text": "performance of those well how are we",
    "start": "820529",
    "end": "825779"
  },
  {
    "start": "823000",
    "end": "823000"
  },
  {
    "text": "going to understand how the game is working once we launch it pre-production we had a lot of metrics from load tests",
    "start": "825779",
    "end": "830850"
  },
  {
    "text": "and from clients but the metrics we were getting were external metrics they were coming from the load test clients we",
    "start": "830850",
    "end": "836489"
  },
  {
    "text": "were looking at them in Amazon Cloud watch when we launched we weren't going to have that stuff we weren't going to be able to get the metrics from the",
    "start": "836489",
    "end": "842339"
  },
  {
    "text": "actual game clients so pretty late in the game we went through and added instrumentation right into the actual back end we did this sort of what",
    "start": "842339",
    "end": "848999"
  },
  {
    "text": "standard practice this point you stats d to send its librado stats d is just a protocol for sending metrics barato is a",
    "start": "848999",
    "end": "855509"
  },
  {
    "text": "hosted metric collection and graphing solution basically hosted graphite but",
    "start": "855509",
    "end": "863339"
  },
  {
    "text": "as I said really hard to get Mitch baselines and figure out what are the actual things want to measure so we just",
    "start": "863339",
    "end": "869279"
  },
  {
    "text": "measured everything what's our level of",
    "start": "869279",
    "end": "874920"
  },
  {
    "start": "871000",
    "end": "871000"
  },
  {
    "text": "operational complexity for this we've already eliminated a lot of the things we have to care about and limit our scope down to the application stack but",
    "start": "874920",
    "end": "881699"
  },
  {
    "text": "we still have nine scalable components just for the a player we still have to figure out how to scale a make RabbitMQ",
    "start": "881699",
    "end": "887160"
  },
  {
    "text": "highly available across multiple az's do we want to use federation do we want to do shovel how are we going to do this",
    "start": "887160",
    "end": "892319"
  },
  {
    "text": "what are the failure mode is going to be we need to be able to chardon scale MongoDB and we need to be able to",
    "start": "892319",
    "end": "900179"
  },
  {
    "text": "distribute Redis load so used to em proxy across the last Akash to get rid of that that's we're not going to talk",
    "start": "900179",
    "end": "905699"
  },
  {
    "text": "about that again but it worked out well we've got this diagram here a lot of",
    "start": "905699",
    "end": "912029"
  },
  {
    "text": "stuff to scale a lot of unknowns which I can step back and try to think about how can we take this and focus on the real",
    "start": "912029",
    "end": "918179"
  },
  {
    "text": "the key bottleneck so we knew that rabbitmq was everything has to go through RabbitMQ everything eventually",
    "start": "918179",
    "end": "923699"
  },
  {
    "text": "ends up with so we have to be able to scale those when we need to everything else the proxy the services",
    "start": "923699",
    "end": "929459"
  },
  {
    "text": "the all just in auto scaling groups we can scale them up and down as we need but there's more mechanics to scaling",
    "start": "929459",
    "end": "935400"
  },
  {
    "text": "RabbitMQ and more mechanics to scaling mongodb so we took the application stack",
    "start": "935400",
    "end": "942390"
  },
  {
    "text": "and figured out a way to make it so we only had to think about MongoDB we took all the application stack except for",
    "start": "942390",
    "end": "948420"
  },
  {
    "text": "mongodb and baked it onto an ami so we have the proxy talking to rabbit talking to all the services and that's then",
    "start": "948420",
    "end": "954660"
  },
  {
    "text": "talking to and we can just scale this whole thing through one auto scaling group there's some upsides to",
    "start": "954660",
    "end": "960870"
  },
  {
    "text": "this you know it makes much easier to make decisions about scaling and troubleshooting it makes it much easier",
    "start": "960870",
    "end": "966300"
  },
  {
    "text": "to build and deploy the application we don't have to care about rabbitmq as a bottleneck anymore it gives us course",
    "start": "966300",
    "end": "972120"
  },
  {
    "text": "control on the whole stack basically we have one big knob on the application we can turn it up when we need to of course",
    "start": "972120",
    "end": "977730"
  },
  {
    "text": "the downside here is we have one big knob on the whole stack we can only turn it up or turn it down so it's pretty",
    "start": "977730",
    "end": "982890"
  },
  {
    "text": "could be really easy to be resource and efficient if we don't have the services balance turns out that wasn't too much of a problem but it was definitely a",
    "start": "982890",
    "end": "989190"
  },
  {
    "text": "fear that we had that we'd be really resource and efficient with just one knob on everything but it made it easier",
    "start": "989190",
    "end": "994440"
  },
  {
    "text": "just to focus a MongoDB so we've taken this here we've already eliminated a lot",
    "start": "994440",
    "end": "999810"
  },
  {
    "text": "of stuff we've gone to the sliced model to be able to just change this into this we have an auto scaling group and we",
    "start": "999810",
    "end": "1005330"
  },
  {
    "text": "have MongoDB so we're working on this we're also thinking about what are the other parts of the stack that are likely",
    "start": "1005330",
    "end": "1012560"
  },
  {
    "text": "to cause problems so we had it we had a service this part of one of the services called event log so it's we knew from",
    "start": "1012560",
    "end": "1019580"
  },
  {
    "start": "1015000",
    "end": "1015000"
  },
  {
    "text": "the past this was probably two-thirds of our traffic where it's just what it sounds like events are getting fired they're being stored in MongoDB and",
    "start": "1019580",
    "end": "1026120"
  },
  {
    "text": "there's a batch process to go through and scrape and dump these daily and make reports there are pros here it's already",
    "start": "1026120",
    "end": "1031970"
  },
  {
    "text": "integrated with clients they're sending this information we already have the service we already have the data it's already working but the cons that we saw",
    "start": "1031970",
    "end": "1040130"
  },
  {
    "text": "was this is all going to we have one shared layer this could be a huge performance it it's going to",
    "start": "1040130",
    "end": "1046760"
  },
  {
    "text": "be really expensive because we're using replication it's all going through rabbit making rep scaling rabbit even more of a concern and it takes a long",
    "start": "1046760",
    "end": "1053210"
  },
  {
    "text": "time to scrape a lot of data and because it's all part of that stack all going through RabbitMQ it has a large halo if",
    "start": "1053210",
    "end": "1059780"
  },
  {
    "text": "event log is doing bad things to rabbit or it's going to affect all services and we don't want to vent log",
    "start": "1059780",
    "end": "1065210"
  },
  {
    "text": "to bring down the game back-end services so this is where i'm going to hand off",
    "start": "1065210",
    "end": "1071540"
  },
  {
    "text": "to evan to talk about how we work together to solve this problem so now",
    "start": "1071540",
    "end": "1080810"
  },
  {
    "text": "we're going to talk about the right hand side so after at this point in time the",
    "start": "1080810",
    "end": "1087320"
  },
  {
    "start": "1084000",
    "end": "1084000"
  },
  {
    "text": "WB play games team has a lot of problems that they're dealing with they have a lot of work they have to do and at this",
    "start": "1087320",
    "end": "1094730"
  },
  {
    "text": "point mobile gaming platform is ian were involved we ended up getting involved in order to help them solve this particular",
    "start": "1094730",
    "end": "1100370"
  },
  {
    "text": "problem mobile gaming platform we were working with in turbine on a brand new",
    "start": "1100370",
    "end": "1105410"
  },
  {
    "text": "platform that was meant to power our next generation mobile games this is",
    "start": "1105410",
    "end": "1111200"
  },
  {
    "text": "almost entirely microservices architecture it's radically different",
    "start": "1111200",
    "end": "1116240"
  },
  {
    "text": "than what we have today it's all written in go it's all containerized it's containerized with docker it runs on",
    "start": "1116240",
    "end": "1122960"
  },
  {
    "text": "core OS which is specialized specialized OS designed just for running containers",
    "start": "1122960",
    "end": "1128060"
  },
  {
    "text": "and we run it's built from the ground up to run in AWS like I said was being",
    "start": "1128060",
    "end": "1136010"
  },
  {
    "text": "written deployed it's meant to be parallel to the WB game stack is not meant to be a full replacement for it",
    "start": "1136010",
    "end": "1142780"
  },
  {
    "text": "it's all HT plus rest there's no no special binary formats here no",
    "start": "1142780",
    "end": "1148790"
  },
  {
    "text": "proprietary formats and we were all greenfield we didn't have the problem",
    "start": "1148790",
    "end": "1153800"
  },
  {
    "text": "the WB play games had where they had a whole bunch of clients already in the wild they had a lot of API is that",
    "start": "1153800",
    "end": "1159590"
  },
  {
    "text": "people are already integrating with their very very close to release and so we had a lot more a lot more leeway to",
    "start": "1159590",
    "end": "1166820"
  },
  {
    "text": "do more interesting new things using new new practices in New Tech so what our",
    "start": "1166820",
    "end": "1173990"
  },
  {
    "start": "1173000",
    "end": "1173000"
  },
  {
    "text": "market services look like they're all HT plus rest we spend a lot of time up front developing a pis and sending the",
    "start": "1173990",
    "end": "1182180"
  },
  {
    "text": "API Doc's off to the game team so they can start to integrate even before we have the service it also allows us to",
    "start": "1182180",
    "end": "1188870"
  },
  {
    "text": "build prototype services very very quickly and since we have an API",
    "start": "1188870",
    "end": "1194960"
  },
  {
    "text": "contract we know that we can go back later and do some of the scalability concerns and some of the",
    "start": "1194960",
    "end": "1200850"
  },
  {
    "text": "rewriting that we need to do once the game teams fully integrated they do one thing they're focused on the code being",
    "start": "1200850",
    "end": "1207540"
  },
  {
    "text": "sane they're focused on doing exactly what they need we split up our data",
    "start": "1207540",
    "end": "1214530"
  },
  {
    "text": "store rather than having one single MongoDB layer we each microservice if it",
    "start": "1214530",
    "end": "1220410"
  },
  {
    "text": "stores data stores its own data and that data is only accessed through the services API we split out all of our",
    "start": "1220410",
    "end": "1228990"
  },
  {
    "text": "configuration there's no configuration within our service there's no there's no",
    "start": "1228990",
    "end": "1234299"
  },
  {
    "text": "chef there's no convergence at startup each service will load its config from",
    "start": "1234299",
    "end": "1239580"
  },
  {
    "text": "console console is a key value store by",
    "start": "1239580",
    "end": "1246090"
  },
  {
    "text": "a company called heshy corp that runs in maintains quorum for us on a separate set of machines we cache all for",
    "start": "1246090",
    "end": "1253410"
  },
  {
    "text": "configuration at startup we never change a service once it's running we developed a deployment pipeline that allows us to",
    "start": "1253410",
    "end": "1260309"
  },
  {
    "text": "take services down and replace them much quicker allowing us to much easier",
    "start": "1260309",
    "end": "1265950"
  },
  {
    "text": "reason about what's in production right now as well as make sure that we don't",
    "start": "1265950",
    "end": "1272040"
  },
  {
    "text": "have configuration mess ups that take down the whole stack all of our configuration is encrypted it's",
    "start": "1272040",
    "end": "1280020"
  },
  {
    "text": "encrypted per service with a brand new key every time we deploy this makes us much more secure and storing things like",
    "start": "1280020",
    "end": "1287250"
  },
  {
    "text": "SSL keys or database passwords inside a",
    "start": "1287250",
    "end": "1292290"
  },
  {
    "text": "configuration like I said it's all containers we don't deploy anything",
    "start": "1292290",
    "end": "1299370"
  },
  {
    "start": "1295000",
    "end": "1295000"
  },
  {
    "text": "that's on the container even third-party applications so for our own stuff because it is written in go it allows us",
    "start": "1299370",
    "end": "1306179"
  },
  {
    "text": "to statically compile a vast majority of our services which means to run from scratch containers there's no OS in the",
    "start": "1306179",
    "end": "1312600"
  },
  {
    "text": "container there's no shell there's no extra libraries that makes them very",
    "start": "1312600",
    "end": "1318780"
  },
  {
    "text": "very small they're about there about one Meg on the wire once you compress the container that means we can employ very",
    "start": "1318780",
    "end": "1325860"
  },
  {
    "text": "very quickly because we're not shoving a lot of data around we use the same containers we are built",
    "start": "1325860",
    "end": "1332730"
  },
  {
    "text": "for every single commit for every environment from the developers desktops",
    "start": "1332730",
    "end": "1337860"
  },
  {
    "text": "all the way up to production so we know what we are testing is actually what goes live we also deploy both our",
    "start": "1337860",
    "end": "1345539"
  },
  {
    "text": "proprietary services and our third-party services in the same way and I'll talk",
    "start": "1345539",
    "end": "1351029"
  },
  {
    "text": "about that in a couple of minutes we know exactly what's out there we deploy",
    "start": "1351029",
    "end": "1356909"
  },
  {
    "text": "by get Shaw which means that we know exactly what commits are in live and",
    "start": "1356909",
    "end": "1363450"
  },
  {
    "text": "exactly which ones are not this also helps us improve our security because we're using from scratch containers and",
    "start": "1363450",
    "end": "1369600"
  },
  {
    "text": "because we are using services specifically written to run in them we can drop almost all of our kernel",
    "start": "1369600",
    "end": "1376620"
  },
  {
    "text": "capabilities meaning we do we can't change the permissions of files in more",
    "start": "1376620",
    "end": "1381690"
  },
  {
    "text": "we can't call shown we can't do any of the superuser things that people worry",
    "start": "1381690",
    "end": "1386970"
  },
  {
    "text": "about when they're running inside containers so what does this actually",
    "start": "1386970",
    "end": "1392399"
  },
  {
    "start": "1390000",
    "end": "1390000"
  },
  {
    "text": "look like on AWS so we run a central that's what we call a central services",
    "start": "1392399",
    "end": "1398279"
  },
  {
    "text": "cluster though the central services cluster only job is to maintain quorum",
    "start": "1398279",
    "end": "1403799"
  },
  {
    "text": "and maintain cluster state this runs console as I mentioned earlier which we use for all of our own configuration as",
    "start": "1403799",
    "end": "1409440"
  },
  {
    "text": "well as our own DNS lookups it also runs at CD which is a part of my core OS that",
    "start": "1409440",
    "end": "1415669"
  },
  {
    "text": "overlaps a lot with console and maintains their own cluster state and",
    "start": "1415669",
    "end": "1421590"
  },
  {
    "text": "their own in its state we ended up going with console because it has a lot more features it has a better API and it",
    "start": "1421590",
    "end": "1428490"
  },
  {
    "text": "provides a DNS access as well as health checks from there we have a set of auto",
    "start": "1428490",
    "end": "1435809"
  },
  {
    "text": "scaling groups most things go on to a single auto scaling group called in this case MGP worker our services are",
    "start": "1435809",
    "end": "1442740"
  },
  {
    "text": "multi-tenant and they they coexist with each other they're all isolated from each other due to the containers we also",
    "start": "1442740",
    "end": "1451559"
  },
  {
    "text": "have separate auto scaling groups for really really big things are really hard to scale things such as databases where",
    "start": "1451559",
    "end": "1457620"
  },
  {
    "text": "we want to make sure that we don't end up putting 10 databases on one machine but even our database is all run inside",
    "start": "1457620",
    "end": "1463860"
  },
  {
    "text": "these containers in order to do that we",
    "start": "1463860",
    "end": "1469290"
  },
  {
    "start": "1467000",
    "end": "1467000"
  },
  {
    "text": "use the product we use a system called sidekicks each service isn't aware of",
    "start": "1469290",
    "end": "1474750"
  },
  {
    "text": "its environment it doesn't understand that it's on it doesn't know stance on AWS it doesn't understand that it's",
    "start": "1474750",
    "end": "1481740"
  },
  {
    "text": "playing with in a multi-tenant environment doesn't understand how storageworks does understand anything it's only job is to serve requests so we",
    "start": "1481740",
    "end": "1491910"
  },
  {
    "text": "created additional containers that we deploy at the same time that we that",
    "start": "1491910",
    "end": "1497760"
  },
  {
    "text": "allow these service to get other pieces of resources that it needs for instance in this case we first apply an EBS",
    "start": "1497760",
    "end": "1503670"
  },
  {
    "text": "sidekick whose job is to go and apply our EBS storage mounted on whatever host we've been deployed on mount the drive",
    "start": "1503670",
    "end": "1510870"
  },
  {
    "text": "so we can see it and the service will be able to see the sea its storage then we",
    "start": "1510870",
    "end": "1516210"
  },
  {
    "text": "deploy the service the service comes up you can see its starch because you provision it with DBS sidekick and then",
    "start": "1516210",
    "end": "1522990"
  },
  {
    "text": "we attach an EOB sidekick which allows us to register out with an ALB rather than pre-registering every single",
    "start": "1522990",
    "end": "1530010"
  },
  {
    "text": "instance in the in the auto scaling group and then we in this case we deployed a DNS sidekick to register with",
    "start": "1530010",
    "end": "1536130"
  },
  {
    "text": "DNS that we can find it we don't we use a party called fleet which is also by",
    "start": "1536130",
    "end": "1543210"
  },
  {
    "text": "car OS to treat our entire cluster as essentially one giant machine which",
    "start": "1543210",
    "end": "1548550"
  },
  {
    "text": "means that we don't actually know ahead of time where any of our services are going to be deployed so we use serve",
    "start": "1548550",
    "end": "1553830"
  },
  {
    "text": "records in our DNS sidekick to register every serves as it comes up with",
    "start": "1553830",
    "end": "1559559"
  },
  {
    "text": "consoles so that we can find it other services can find it as well this also means that we don't reserve ports for",
    "start": "1559559",
    "end": "1566970"
  },
  {
    "text": "anything so any service can come up on any port and the DNS sidekick deals with that as well so psychics are very very",
    "start": "1566970",
    "end": "1576300"
  },
  {
    "text": "powerful some of the other things that we do with sidekicks are we use them to reg with other load balancers things",
    "start": "1576300",
    "end": "1583920"
  },
  {
    "text": "like Vulcan drh a proxy engine X we can",
    "start": "1583920",
    "end": "1589050"
  },
  {
    "text": "stack sidekicks on top of each other so we have a single service but it's",
    "start": "1589050",
    "end": "1594210"
  },
  {
    "text": "registered with two 40lbs so we talked about so we have a internally lb for internal use maybe",
    "start": "1594210",
    "end": "1602400"
  },
  {
    "text": "even another account via VPC peering or we have an EOB sidekick that for the",
    "start": "1602400",
    "end": "1608520"
  },
  {
    "text": "public so that clients can hit it directly we also have sidekicks at",
    "start": "1608520",
    "end": "1614790"
  },
  {
    "text": "register with Public DNS via route 53 in this out this also works for things we",
    "start": "1614790",
    "end": "1620160"
  },
  {
    "text": "didn't write for instance we run Kafka this way Kafka's inside the container and it has sidekicks that come up and",
    "start": "1620160",
    "end": "1627140"
  },
  {
    "text": "acquire its storage before starting Kafka then we register with a knee lb we",
    "start": "1627140",
    "end": "1634740"
  },
  {
    "text": "even run our databases like this we can acquire multiple EBS volumes this way maybe for your journal and for your data",
    "start": "1634740",
    "end": "1643340"
  },
  {
    "text": "start our database we use a lot of post grass we also use we mentioned earlier when we kick off sequel",
    "start": "1643340",
    "end": "1650370"
  },
  {
    "text": "migration in the container attached to the database and then we ret when all that water all of that son and all that",
    "start": "1650370",
    "end": "1656130"
  },
  {
    "text": "succeeds we then register with DNS so that other services that want to talk to database can actually find it because of",
    "start": "1656130",
    "end": "1663390"
  },
  {
    "text": "this we actually end up treating most of our databases like a micro service unto themselves our deployment our deployment",
    "start": "1663390",
    "end": "1670350"
  },
  {
    "text": "system sees a database as a separate service and knows how to deploy it separately so now that we've talked",
    "start": "1670350",
    "end": "1680220"
  },
  {
    "text": "about what we actually do let's talk about how we were able to help with this particular launch so we ended up",
    "start": "1680220",
    "end": "1687720"
  },
  {
    "text": "building a service called event ingestion to replace event log it's the first service that the team ever",
    "start": "1687720",
    "end": "1693300"
  },
  {
    "text": "released we were at like I said we were doing mostly research and green field work for upcoming games so this was",
    "start": "1693300",
    "end": "1702510"
  },
  {
    "text": "going to be the first thing that we were going to release to the public clients send events directly to this service",
    "start": "1702510",
    "end": "1707640"
  },
  {
    "text": "that's why there's the the split that you saw earlier the service sends all that data into Kafka and the analytics",
    "start": "1707640",
    "end": "1715680"
  },
  {
    "text": "team can consume directly from Kafka as fast as they want or as fast as they can so why is this good it lets us scale",
    "start": "1715680",
    "end": "1725460"
  },
  {
    "text": "completely independently we didn't have to worry about what the WB game stack was doing if it was having",
    "start": "1725460",
    "end": "1732039"
  },
  {
    "text": "problems or what our scaling would do would do to their stack it supports",
    "start": "1732039",
    "end": "1737590"
  },
  {
    "text": "real-time analytics this was kind of an extra bonus but because we're sending everything into Kafka in real time the",
    "start": "1737590",
    "end": "1744099"
  },
  {
    "text": "analytics team can use things like spark to put a pull the data off Kafka at their leisure and it's completely it's",
    "start": "1744099",
    "end": "1751179"
  },
  {
    "text": "completely isolated so the cons is I had to go out to all these game teams and",
    "start": "1751179",
    "end": "1757359"
  },
  {
    "text": "convince them that two months before launch they should rip out an integration they had and do it my way instead this this is pretty hard when",
    "start": "1757359",
    "end": "1767649"
  },
  {
    "text": "you have game teams struggling for launch because they don't they don't care about my problems and I had to go",
    "start": "1767649",
    "end": "1773919"
  },
  {
    "text": "to multiple Studios doing multiple ports multiple teams but ultimately we were",
    "start": "1773919",
    "end": "1780879"
  },
  {
    "start": "1780000",
    "end": "1780000"
  },
  {
    "text": "able to convince them so what does the traffic actually look like we come in",
    "start": "1780879",
    "end": "1786970"
  },
  {
    "text": "through the elb we talked to vent justin and then we go over the VPC VPC peering",
    "start": "1786970",
    "end": "1793419"
  },
  {
    "text": "link we showed you earlier and to talk to Kafka we also have a what we call",
    "start": "1793419",
    "end": "1799629"
  },
  {
    "text": "schema service the schema service holds in our case avro schemas which is a way",
    "start": "1799629",
    "end": "1804909"
  },
  {
    "text": "that we can have the game teams publish what their events look like ahead of time so that once it gets to analytics",
    "start": "1804909",
    "end": "1811929"
  },
  {
    "text": "they don't have to they don't have to infer what all the events mean what all the data types are we've already told",
    "start": "1811929",
    "end": "1817599"
  },
  {
    "text": "them and then one one thing to note here too is that the actual event traffic from event ingestion ND kafka doesn't",
    "start": "1817599",
    "end": "1823989"
  },
  {
    "text": "actually go through the eld one of the common problems we see a lot on the copy a user nailing list is running on Amazon",
    "start": "1823989",
    "end": "1829419"
  },
  {
    "text": "when you have private and public networks and it can be a little it doesn't handle the multi addresses very well so one thing we were able to do is",
    "start": "1829419",
    "end": "1836739"
  },
  {
    "text": "it registers with the nail be the cup of clients by default will reach out talk to you whatever connection string you give it to disk where you for the broker",
    "start": "1836739",
    "end": "1842769"
  },
  {
    "text": "list so you can set up a route 53 DNS alias just to talk to the ALB to get your list of brokers and as we register",
    "start": "1842769",
    "end": "1849489"
  },
  {
    "text": "everything then on the private eye peas everything can talk over the bpc peering links I'm sort of ease that service",
    "start": "1849489",
    "end": "1855009"
  },
  {
    "text": "discovery a lot so we have one connection string we can change out copco host under knee hood and neither any of the event",
    "start": "1855009",
    "end": "1860850"
  },
  {
    "text": "ingestion environments or any of the analytics team environment actually need to know about it which is something I was originally a little harder with",
    "start": "1860850",
    "end": "1866580"
  },
  {
    "text": "Kafka and was helpful having the MGP sidekick system to sort of make that simple so we were able to scale this up",
    "start": "1866580",
    "end": "1876419"
  },
  {
    "start": "1873000",
    "end": "1873000"
  },
  {
    "text": "pretty rapidly we tested it up to about 200 thousand requests a second at 250",
    "start": "1876419",
    "end": "1882450"
  },
  {
    "text": "bite messages each request or each request had about 126 messages in it",
    "start": "1882450",
    "end": "1887879"
  },
  {
    "text": "most of them were on the small side we can do that on about 15 c4 larges at",
    "start": "1887879",
    "end": "1893519"
  },
  {
    "text": "seventy-five percent CPU and this includes verifying that the data is coming in is for a valid schema that",
    "start": "1893519",
    "end": "1900350"
  },
  {
    "text": "allows us to reject a bunch of data that malformed clients are sending or maybe",
    "start": "1900350",
    "end": "1905580"
  },
  {
    "text": "bad actors are sending right away on the kafka side that translates to about",
    "start": "1905580",
    "end": "1911399"
  },
  {
    "text": "250,000 messages a second we set up as 20 partitions per topic with the replication factor too that means every",
    "start": "1911399",
    "end": "1918600"
  },
  {
    "text": "piece of data that comes in is on at least two Kopke brokers in this set in",
    "start": "1918600",
    "end": "1925710"
  },
  {
    "text": "this set where you only use about 6 brokers and they're only are 3x largest and they only have a single 10 terabyte",
    "start": "1925710",
    "end": "1932220"
  },
  {
    "text": "gp2 volume thankfully AWS released the large volumes the literally the day we",
    "start": "1932220",
    "end": "1938519"
  },
  {
    "text": "had to set this up because we were not looking forward to managing 10 1 terabyte gp2 volumes on every single",
    "start": "1938519",
    "end": "1944429"
  },
  {
    "text": "broker this is a pretty small footprint",
    "start": "1944429",
    "end": "1949100"
  },
  {
    "text": "Kafka is our Kafka is used by all of the games it's used by analytics as well",
    "start": "1949669",
    "end": "1958139"
  },
  {
    "text": "it's used in some cases to self from the analytic stack to publish stuff for the other piece of the analytic stack and",
    "start": "1958139",
    "end": "1965190"
  },
  {
    "text": "with this method we believe we can just add more brokers even even at the small size and get at least a million messages",
    "start": "1965190",
    "end": "1972179"
  },
  {
    "text": "per second before we have to look at either scaling up the brokers or",
    "start": "1972179",
    "end": "1977429"
  },
  {
    "text": "potentially scaling up any of the storage so side by side we have both of",
    "start": "1977429",
    "end": "1983940"
  },
  {
    "start": "1981000",
    "end": "1981000"
  },
  {
    "text": "our platforms up at this point we've got most of our data going in through a vent",
    "start": "1983940",
    "end": "1990450"
  },
  {
    "text": "ingestion now we've got about two-thirds of our data going through there which for us is about two billion",
    "start": "1990450",
    "end": "1995580"
  },
  {
    "text": "requests a day so this it's not a trivial amount of data it's not a trivial number of requests and we've",
    "start": "1995580",
    "end": "2002330"
  },
  {
    "text": "saved the other stack which is doing a lot more logic and has a lot more a lot",
    "start": "2002330",
    "end": "2007370"
  },
  {
    "text": "more to deal with due to the storage and all the other things that's going to do we've taken all that load off it and",
    "start": "2007370",
    "end": "2013250"
  },
  {
    "text": "we've moved it all over to a vet ingestion without ever without ever",
    "start": "2013250",
    "end": "2021429"
  },
  {
    "text": "affecting the WB play games team they were barely involved in this particular transition because they were dealing",
    "start": "2021429",
    "end": "2027740"
  },
  {
    "text": "with a lot of other things such as launch time so we've got this stuff in",
    "start": "2027740",
    "end": "2038990"
  },
  {
    "text": "place and we're ready to launch and so we're doing a midnight release and we're",
    "start": "2038990",
    "end": "2044270"
  },
  {
    "text": "figured you know well midnight release can't be that many people who are going to be sitting there midnight just waiting for the game to come out turns",
    "start": "2044270",
    "end": "2053929"
  },
  {
    "start": "2051000",
    "end": "2051000"
  },
  {
    "text": "out it's we actually couldn't find the graphs but it's pretty cool because you can see the unlocks coming across Europe",
    "start": "2053929",
    "end": "2059628"
  },
  {
    "text": "and then us East hit and all the traffic hit exactly at midnight our new players",
    "start": "2059629",
    "end": "2065030"
  },
  {
    "text": "per second right here jumped up to its highest number ever and just continued along we went from like 100 requests a",
    "start": "2065030",
    "end": "2071388"
  },
  {
    "text": "second to 60,000 request a second in a matter of minutes and then it just never stopped so for context this is new",
    "start": "2071389",
    "end": "2077810"
  },
  {
    "text": "players per second right at midnight and then now a couple weeks after launch it's settled down to like a much order",
    "start": "2077810",
    "end": "2084108"
  },
  {
    "text": "of magnitude lower number pretty interesting so this was pretty",
    "start": "2084109",
    "end": "2090138"
  },
  {
    "text": "interesting we you know we load tested we were ready to handle you know a certain requests rate in and we",
    "start": "2090139",
    "end": "2096050"
  },
  {
    "text": "immediately had to start scaling up so we i'm going to talk through three challenges we had right at launch with MongoDB scaling that up one of the first",
    "start": "2096050",
    "end": "2103550"
  },
  {
    "text": "things was our up log data was through the roof so the up blog stores all the operations that modify the database",
    "start": "2103550",
    "end": "2109010"
  },
  {
    "text": "if you're modifying a database a lot and get a lot about blog data we're also getting read cues that we're backing up",
    "start": "2109010",
    "end": "2115670"
  },
  {
    "text": "the services is writers block readers so it's an in-memory latch when",
    "start": "2115670",
    "end": "2121310"
  },
  {
    "text": "you take the right lock readers are blocked until the lock is done so we were getting a lot of block reads from right lots talk about that and then",
    "start": "2121310",
    "end": "2129320"
  },
  {
    "text": "eventually we started getting these frequent mongodb fail overs that we had to deal with so first the OP log data",
    "start": "2129320",
    "end": "2136370"
  },
  {
    "start": "2136000",
    "end": "2136000"
  },
  {
    "text": "well actually first architecture so I standard sharp replicated charted set up we're doing a three node sharded",
    "start": "2136370",
    "end": "2142580"
  },
  {
    "text": "replicated set we have a primary replica and two secondary replicas that can fail over and primary can move around the",
    "start": "2142580",
    "end": "2148850"
  },
  {
    "text": "Jeep the data volumes were using your GP too we had pretty large volumes so the ups we got from just the size of the",
    "start": "2148850",
    "end": "2155660"
  },
  {
    "text": "long-term needed for GP to was enough for i/o throughput we're a little behind we're using 269 but work we",
    "start": "2155660",
    "end": "2162800"
  },
  {
    "text": "started out with our 32 XL's using two terabyte GP two volumes the two terabyte GP two volumes will give you six",
    "start": "2162800",
    "end": "2168530"
  },
  {
    "text": "thousand I ops no burst just baseline and we were in three az's for high a little high availability and we started",
    "start": "2168530",
    "end": "2178610"
  },
  {
    "start": "2177000",
    "end": "2177000"
  },
  {
    "text": "with six yards so the OP log data weird 6 charge at launch we get all this traffic and we're seeing up to 100",
    "start": "2178610",
    "end": "2184940"
  },
  {
    "text": "gigabyte per hour and out blog / shard it's a lot of data every day you know to be able to take that and use that for",
    "start": "2184940",
    "end": "2191030"
  },
  {
    "text": "backup and it basically meant we were unable to use off-the-shelf backup solutions we need to solve this problem",
    "start": "2191030",
    "end": "2196400"
  },
  {
    "text": "right away so we did the deep dive into what was going on why the op log data was so large so high and when we found",
    "start": "2196400",
    "end": "2202910"
  },
  {
    "text": "as we sort of had we had some server-side bugs and some client-side patches that we needed to get out we're on the server side we hit we were",
    "start": "2202910",
    "end": "2208580"
  },
  {
    "text": "writing and reading way too much data you know we were writing full documents every time and we just went in and",
    "start": "2208580",
    "end": "2213710"
  },
  {
    "text": "worked on a patch for that and also on different platforms different clients were sending updates much more",
    "start": "2213710",
    "end": "2219260"
  },
  {
    "text": "frequently so we had some really chatty clients that we're sending updates really often and we're actually able to",
    "start": "2219260",
    "end": "2224600"
  },
  {
    "text": "get that with a patch out to one of the clients that turn down the update rate by about a factor of 20 and we also were",
    "start": "2224600",
    "end": "2231110"
  },
  {
    "text": "expecting diffs and some of the clients were sending full documents instead of diffs so we're writing all the keys every time so we fix the bug on server",
    "start": "2231110",
    "end": "2238460"
  },
  {
    "text": "side got patches out on the client side and reduce the upload rate from 100 gigabytes per hour down to about a gigabyte per hour which is much more",
    "start": "2238460",
    "end": "2245000"
  },
  {
    "text": "manageable and we were able to use off-the-shelf backup solutions at that point so we got that out of the way",
    "start": "2245000",
    "end": "2250100"
  },
  {
    "text": "feeling pretty good and I'm going to go through the timeline of the other two challenges",
    "start": "2250100",
    "end": "2257080"
  },
  {
    "start": "2257000",
    "end": "2257000"
  },
  {
    "text": "so this diagram here is sort of showing our scaling timeline over time you can",
    "start": "2258030",
    "end": "2264340"
  },
  {
    "text": "see at the bottom is sort of a representative measure you know subjective measure of stability over time or over time we're doing things",
    "start": "2264340",
    "end": "2270670"
  },
  {
    "text": "we're gaining more and more stability but we're not fully stable until we get to the end at the top it's showing the",
    "start": "2270670",
    "end": "2277180"
  },
  {
    "text": "footprint of the environment and telling you how many shards and instance types we had for that footprint so for",
    "start": "2277180",
    "end": "2283420"
  },
  {
    "text": "example 6 shards at our 38 XL towards the left is the same footprint is 24 shards at our three two XL in terms of",
    "start": "2283420",
    "end": "2290650"
  },
  {
    "text": "like the amount of resources using Amazon so over time we're going we're scaling up we're going from two XL for",
    "start": "2290650",
    "end": "2298120"
  },
  {
    "text": "our cells to a tech cells then we're adding shards we're adding more shards and then we start scaling down over time",
    "start": "2298120",
    "end": "2304210"
  },
  {
    "text": "to go from a text i was 24 excels 22 excels i'm going to focus on the right",
    "start": "2304210",
    "end": "2309250"
  },
  {
    "text": "lock problems first that was the the second problem we had here and while",
    "start": "2309250",
    "end": "2315040"
  },
  {
    "text": "we're working through this problem this is where we initially started scaling up and then scaling out where we went from two excels 24 excels to a tech cells and",
    "start": "2315040",
    "end": "2321730"
  },
  {
    "text": "then out to 12 shards and then out eventually out to 24 shards so what were",
    "start": "2321730",
    "end": "2330610"
  },
  {
    "start": "2330000",
    "end": "2330000"
  },
  {
    "text": "we seeing we were seeing huge spikes in cute operations you know ten twenty thousand reads and we weren't doing",
    "start": "2330610",
    "end": "2337300"
  },
  {
    "text": "anything really that unusual with manga we're primarily read operations you know order of magnitude more reads for every",
    "start": "2337300",
    "end": "2342310"
  },
  {
    "text": "right and we had a relatively little lock percentage under normal over twenty thirty percent this should be fine there",
    "start": "2342310",
    "end": "2347530"
  },
  {
    "text": "shouldn't be a problem for like give us we didn't understand why we're getting these r eq backlogs I mean if",
    "start": "2347530",
    "end": "2353260"
  },
  {
    "text": "you look at this we're doing you know about 80,000 reads per second at the database we're only doing three thousand writes per second on the database but",
    "start": "2353260",
    "end": "2360490"
  },
  {
    "text": "the rights are huge they're 50 50 to 100 k documents that we're putting into the database so we're seeing twenty percent",
    "start": "2360490",
    "end": "2366760"
  },
  {
    "text": "lock on average io is fine we're getting these massive spikes up to 20,000 cube",
    "start": "2366760",
    "end": "2371860"
  },
  {
    "text": "reads which were just tanking an application so we spent a lot of time",
    "start": "2371860",
    "end": "2377230"
  },
  {
    "text": "with my new favorite tool m plot queries",
    "start": "2377230",
    "end": "2382480"
  },
  {
    "start": "2380000",
    "end": "2380000"
  },
  {
    "text": "and log analysis what I'm showing you here is this is the cumulative distribution",
    "start": "2382480",
    "end": "2388049"
  },
  {
    "text": "for the right locks that we were looking at so what we did here is we took the  logs we set it to log level 1 so",
    "start": "2388049",
    "end": "2394920"
  },
  {
    "text": "we hit a write lock time for all of the locks over a five minute period and then threw in some analysis tools and took a",
    "start": "2394920",
    "end": "2401069"
  },
  {
    "text": "look at this the x-axis is the cumulative frequency so you go from zero probability to 1 probability of seeing",
    "start": "2401069",
    "end": "2407729"
  },
  {
    "text": "this right lock time that y-axis is the right lock time in microseconds for people familiar with this scale is",
    "start": "2407729",
    "end": "2414299"
  },
  {
    "text": "kind of insane because it goes up to 10 milliseconds what you can see here is at",
    "start": "2414299",
    "end": "2419880"
  },
  {
    "text": "the very lower left corner we have a sort of standard Whitelock of like tens to hundreds of microseconds it",
    "start": "2419880",
    "end": "2425699"
  },
  {
    "text": "immediately jumps up to hundreds of microseconds and then by the time you get to the 95th percentile we're having",
    "start": "2425699",
    "end": "2431910"
  },
  {
    "text": "one in 20 requests one in 20 updates taking more than two milliseconds to write to the database so the we have a",
    "start": "2431910",
    "end": "2438179"
  },
  {
    "text": "lot of requests are taking a long time to write and that's meaning that our updates per shard we're way lower than",
    "start": "2438179",
    "end": "2443729"
  },
  {
    "text": "you would expect for the update way we were getting so that's the one we started shorting out horizontally when we saw this we doubled the number of",
    "start": "2443729",
    "end": "2449160"
  },
  {
    "text": "shards once then double the number of shards again to get the updates per shard down to something that can be handled with right locks this long so to",
    "start": "2449160",
    "end": "2458130"
  },
  {
    "text": "summarize you know what was going on we had very long right locks which were increasing the impact at lough lough",
    "start": "2458130",
    "end": "2463349"
  },
  {
    "text": "percentage we also found another thing where we're using a relatively old linux kernel and it turns out it had a problem",
    "start": "2463349",
    "end": "2470039"
  },
  {
    "text": "where it did a lot of context switches and they were also slow context switches so we're actually just able to change",
    "start": "2470039",
    "end": "2475229"
  },
  {
    "text": "the Linux kernel to a newer linux kernel 314 and that actually have the number of context switches and reduce the right",
    "start": "2475229",
    "end": "2481079"
  },
  {
    "text": "lock time by about forty percent and then we also doubled the number of shards one of the things we ran into",
    "start": "2481079",
    "end": "2487559"
  },
  {
    "text": "here is because the documents we were seeing were much larger than what we expected and thought would ever go in",
    "start": "2487559",
    "end": "2493170"
  },
  {
    "text": "were like documents removing uses a power of two padding to allocate space but if you exceed that padding it has to",
    "start": "2493170",
    "end": "2499349"
  },
  {
    "text": "find a new location to write the document into the database it has to move it takes a long time to do that so",
    "start": "2499349",
    "end": "2506969"
  },
  {
    "text": "we've shorted out to handle the long right locks that",
    "start": "2506969",
    "end": "2512789"
  },
  {
    "text": "we're seeing so we're focusing on this second part of the stability over time timeline that we have where now we've",
    "start": "2512789",
    "end": "2519210"
  },
  {
    "text": "got 24 shards there eight excels for context those are three node replica sets that's 72 hour 38 XL's if you do",
    "start": "2519210",
    "end": "2527190"
  },
  {
    "text": "the math that's about 18 tera bytes of RAM it's only six tera bytes of RAM for the active working set but that actually",
    "start": "2527190",
    "end": "2533130"
  },
  {
    "text": "wasn't enough to keep all the working set in memory so we're looking at this",
    "start": "2533130",
    "end": "2538470"
  },
  {
    "text": "we're trying to we think we're going to be stable now but we're in this zone here where we have ton of notes ton of",
    "start": "2538470",
    "end": "2544319"
  },
  {
    "text": "memory and we start seeing the primaries failover every now and then no good reason to explain it we spent a lot of",
    "start": "2544319",
    "end": "2550859"
  },
  {
    "text": "time investigating it and we're looking at this mystery where we have these",
    "start": "2550859",
    "end": "2556589"
  },
  {
    "start": "2553000",
    "end": "2553000"
  },
  {
    "text": "symptoms we're seeing spikes in system CPU we're seeing the manga d processed",
    "start": "2556589",
    "end": "2563099"
  },
  {
    "text": "all it's never good when you see a process at 99 99 percent cpu for two minutes if you have a process that's at",
    "start": "2563099",
    "end": "2570960"
  },
  {
    "text": "9999 cpu for a couple minutes and it's in a quorum system it triggers an election because it wasn't able to",
    "start": "2570960",
    "end": "2576690"
  },
  {
    "text": "respond two heartbeats and it basically showed up as a network partition to the replica set members while this is",
    "start": "2576690",
    "end": "2583710"
  },
  {
    "text": "happening we have two minutes where the where the process is stalled it's failing over this is causing the",
    "start": "2583710",
    "end": "2588960"
  },
  {
    "text": "application to backlog it's got a queue work waiting to come in so as soon as a failover happens we immediately swamp",
    "start": "2588960",
    "end": "2594839"
  },
  {
    "text": "the system by trying to flush out the backlog of work that's just waiting for this to happen not particularly fun what",
    "start": "2594839",
    "end": "2602549"
  },
  {
    "text": "was going on our hypothesis was too much RAM basically we spent a lot of time",
    "start": "2602549",
    "end": "2608130"
  },
  {
    "text": "looking at this and looking at the kernel source calling everyone reading",
    "start": "2608130",
    "end": "2614219"
  },
  {
    "text": "through everything looking at the compact stall statistics in the in the kernel and basically with the amount of",
    "start": "2614219",
    "end": "2619769"
  },
  {
    "text": "RAM we had the memory compaction to satisfy allocations was just taking a really long time and causing the process to stall while that's happening so what",
    "start": "2619769",
    "end": "2627119"
  },
  {
    "text": "do we do we went to smaller instances we gave ourselves less Ram is really hard to convince yourself when you're having",
    "start": "2627119",
    "end": "2633569"
  },
  {
    "text": "problems with database less memory it's going to help but you know this did it we went to 4xl",
    "start": "2633569",
    "end": "2640130"
  },
  {
    "text": "started getting better we went to two excels got rid of the process dolls we can still see the spikes in system CPU",
    "start": "2640130",
    "end": "2645470"
  },
  {
    "text": "and we still have the same symptoms but they're smaller and faster and not causing the problems so we just scaled",
    "start": "2645470",
    "end": "2651200"
  },
  {
    "text": "out widely with smaller instances so we've gone through this long process",
    "start": "2651200",
    "end": "2656300"
  },
  {
    "text": "we're working through all of these things we're adding stability over time eventually we ended up at 24 shards are",
    "start": "2656300",
    "end": "2661700"
  },
  {
    "text": "32 excels and it's pretty good now side",
    "start": "2661700",
    "end": "2667310"
  },
  {
    "start": "2667000",
    "end": "2667000"
  },
  {
    "text": "note for Python and sci-fi fans really easy to make pdfs super useful to try",
    "start": "2667310",
    "end": "2672980"
  },
  {
    "text": "and understand bottlenecks in your system this is just an example of the code we were using to make that type of graph where it's really easy grab numpy",
    "start": "2672980",
    "end": "2680060"
  },
  {
    "text": "pull out the right locks with some Bach grep whatever you want make the histogram do a cumulative sum over the",
    "start": "2680060",
    "end": "2686630"
  },
  {
    "text": "bins plot it and you'll get a really good idea of what your what the distribution of what you're looking at",
    "start": "2686630",
    "end": "2691970"
  },
  {
    "text": "is really easy working out but",
    "start": "2691970",
    "end": "2697130"
  },
  {
    "text": "we're also trying to solve other problems so i'm going to end up to row me to talk about some of the other things we're doing so during this entire",
    "start": "2697130",
    "end": "2708640"
  },
  {
    "start": "2706000",
    "end": "2706000"
  },
  {
    "text": "troubleshooting process with the game was up I mean you couldn't take it down so we need to figure out something",
    "start": "2708640",
    "end": "2714800"
  },
  {
    "text": "that would help us kind of control what was coming in we need to be able to buy some time to solve some of the backend issues that we saw we need time to look",
    "start": "2714800",
    "end": "2721310"
  },
  {
    "text": "at our up log data and when we need to take some of the load off the system as well you know as I said the game was live we couldn't pull it so we started",
    "start": "2721310",
    "end": "2729470"
  },
  {
    "text": "looking through and trying to find which specific cause we're at kohls were actually causing problems turned out we",
    "start": "2729470",
    "end": "2735680"
  },
  {
    "text": "actually managed to find a handful of call that were actually pretty low value to the client but surprising impact on the back end so we needed some way that",
    "start": "2735680",
    "end": "2743060"
  },
  {
    "text": "we could kind of configure this on the fly to say maybe this one call left may be populating this one little widget can",
    "start": "2743060",
    "end": "2749600"
  },
  {
    "text": "go down to little or no traffic while this platform maybe is really important right now we really want to let everything through so and the next its",
    "start": "2749600",
    "end": "2757910"
  },
  {
    "start": "2756000",
    "end": "2756000"
  },
  {
    "text": "seem silly but it's you know it would let us do everything we needed to do it let us cash calls that were kind of",
    "start": "2757910",
    "end": "2763700"
  },
  {
    "text": "uncatchable so if you'll earlier using cdns or varnish kind of",
    "start": "2763700",
    "end": "2768839"
  },
  {
    "text": "and cash posts that are getting data turns out we had some it was really important for us to be able to return",
    "start": "2768839",
    "end": "2773849"
  },
  {
    "text": "them from the very front of our application stack before ever cut anywhere else it also has a really good built-in rate-limiting engine so we can",
    "start": "2773849",
    "end": "2780299"
  },
  {
    "text": "actually pick individual endpoints platform keys whatever we need to do and apply a rate limit across the farm",
    "start": "2780299",
    "end": "2786349"
  },
  {
    "text": "specifically to those calls to allow a little bit of pressure to get relieved on some other ones additionally since we",
    "start": "2786349",
    "end": "2794039"
  },
  {
    "text": "had the MGP stack running we had console so I know if people are familiar with come fidy it's a little tool that a Kelsey Hightower from core OS route",
    "start": "2794039",
    "end": "2800690"
  },
  {
    "text": "basically it's a templating language that will pull keys from console either on a schedule or just a key watch and as",
    "start": "2800690",
    "end": "2806880"
  },
  {
    "text": "you change them in the key value store it will just automatically either restart your application to run a series of commands that you specify so that let",
    "start": "2806880",
    "end": "2813359"
  },
  {
    "text": "us men at all of our we could horizontally scale our engine next farm and then manage arc config centrally",
    "start": "2813359",
    "end": "2819180"
  },
  {
    "text": "which would then allow us to quickly iterate on these rules and test hypotheses and check our metrics so the",
    "start": "2819180",
    "end": "2825390"
  },
  {
    "text": "first thing we really focused on cashing this is something specific I think in the kind of the gaming industry to us is",
    "start": "2825390",
    "end": "2831269"
  },
  {
    "text": "we're talking to a number of game teams and we had we discovered very different definitions of what a real-time meant we're an MMO company so we real-time use",
    "start": "2831269",
    "end": "2839849"
  },
  {
    "text": "like you do it you see it right away after launch we realized that maybe real-time meant more like somebody in",
    "start": "2839849",
    "end": "2846210"
  },
  {
    "text": "customer service needed to update a post and real time is really like five minutes which is a very different",
    "start": "2846210",
    "end": "2851759"
  },
  {
    "text": "problems that we had sold as we actually initially identified just two endpoints that saved fifty percent of our traffic back to the the actual application stack",
    "start": "2851759",
    "end": "2859829"
  },
  {
    "text": "and that actually made everything way better like so on the left you can see",
    "start": "2859829",
    "end": "2864839"
  },
  {
    "text": "the mom go left percentage already relatively low but still really spiky and kind of weird even that out re Lee",
    "start": "2864839",
    "end": "2870749"
  },
  {
    "text": "latency that's insane it should never go up to four seconds though um and we were to seeing these odd spikes that once we",
    "start": "2870749",
    "end": "2876210"
  },
  {
    "text": "actually just buffered everything up front it allowed us to actually smooth everything out and then as well as just",
    "start": "2876210",
    "end": "2881329"
  },
  {
    "text": "establish TCP connections we were holding a million connections hoping for curve sorry hundreds of thousands of",
    "start": "2881329",
    "end": "2886650"
  },
  {
    "text": "connections open a time for no reason so just a couple of high-impact calls really really gave us the time to",
    "start": "2886650",
    "end": "2892559"
  },
  {
    "text": "actually you know saw what we need on the back end also a couple of problems I'm personally",
    "start": "2892559",
    "end": "2900450"
  },
  {
    "start": "2897000",
    "end": "2897000"
  },
  {
    "text": "very used to cashing with varnish using a CDN you know very much in memory part of what makes engine X awesome is that",
    "start": "2900450",
    "end": "2906420"
  },
  {
    "text": "it has a great way to handle set static files and it turns out the cash engine leverages that same technology",
    "start": "2906420",
    "end": "2913500"
  },
  {
    "text": "underneath plus we also had huge request you know hundred fifty to a hundred k",
    "start": "2913500",
    "end": "2918569"
  },
  {
    "text": "rights none of the defaults are really particularly good for that and will actually write extra temporary files and",
    "start": "2918569",
    "end": "2925440"
  },
  {
    "text": "additionally we're cashing too much a number of those calls maybe only got access a couple of times but we had enough players that maybe mattered so",
    "start": "2925440",
    "end": "2933710"
  },
  {
    "text": "the the caching engine has the proc statement uses a config value so we I think we settled around five so once",
    "start": "2933710",
    "end": "2940200"
  },
  {
    "text": "something I've been cashed five times then we hadn't been requested five times then we'd actually hold on to it problem",
    "start": "2940200",
    "end": "2947339"
  },
  {
    "text": "before we actually configured all this is that we're using relatively small EBS three volumes I wasn't expecting our",
    "start": "2947339",
    "end": "2953069"
  },
  {
    "text": "disk to be a bottleneck for caching turned out that we were improperly",
    "start": "2953069",
    "end": "2958530"
  },
  {
    "text": "configured so it was actually a problem when we maybe went down at maybe three o'clock in the morning may be called a lot of people um and also we you know it",
    "start": "2958530",
    "end": "2966089"
  },
  {
    "text": "unexpected just muted it wrote out a million temporary five millions of temporary files and the default ext4 I know settings are pretty small so",
    "start": "2966089",
    "end": "2972920"
  },
  {
    "text": "between those two things we used up all the burst credits on our GP volumes and then we run out of inodes once we got",
    "start": "2972920",
    "end": "2978150"
  },
  {
    "text": "through that but a couple of config tweaks and then you know being able to scale out and change the infrastructure",
    "start": "2978150",
    "end": "2984750"
  },
  {
    "text": "that were running on then we're actually pretty good and we're running pretty steady state and that's really where a lot of those smooth doubt graphs came",
    "start": "2984750",
    "end": "2990299"
  },
  {
    "text": "from so head back over to will to talk about actually throttling the traffic so",
    "start": "2990299",
    "end": "2996869"
  },
  {
    "text": "part of what we're doing with the selected degredation was throttling some of the endpoints for different platforms or the low impact the low value but high",
    "start": "2996869",
    "end": "3003559"
  },
  {
    "text": "impact queries so we went through our logs and focused on you know the highest impact calls first again we went through",
    "start": "3003559",
    "end": "3009290"
  },
  {
    "text": "used my queries to find some we had some poorly indexed queries we also have the really chatty rights and with the right",
    "start": "3009290",
    "end": "3015890"
  },
  {
    "text": "lock problems we're having these chatty rights for blocking reads so we use throttling to be able to like to noun",
    "start": "3015890",
    "end": "3022190"
  },
  {
    "text": "the number of rights who are accepting to let through the reeds and let the backlights come through and then we just blocked some calls we had some calls",
    "start": "3022190",
    "end": "3028880"
  },
  {
    "text": "that we could just block in the game what be not totally complete but it would work we used a lot of instrumentation",
    "start": "3028880",
    "end": "3036650"
  },
  {
    "text": "for this because we had we had instrumented all the things we're able to look through and see what like the call rates where for all them points see",
    "start": "3036650",
    "end": "3042589"
  },
  {
    "text": "how frequent they were used in by platform and so with those detailed metrics we were able to go through and say like who's using what how often are",
    "start": "3042589",
    "end": "3048920"
  },
  {
    "text": "they using it what are the rights were getting from that and use that to build some like how we would throttle the",
    "start": "3048920",
    "end": "3054260"
  },
  {
    "text": "things that we needed to and let the reeds through that we want to get through this is no panacea you know we",
    "start": "3054260",
    "end": "3061400"
  },
  {
    "start": "3058000",
    "end": "3058000"
  },
  {
    "text": "have different clients behave in different ways and we're trying to balance all this so we can buy time to scale up how we need to scale up one of",
    "start": "3061400",
    "end": "3067099"
  },
  {
    "text": "the problems we ran into is you know you're essentially a prying a probability of success to the end point that you're throttling well if you have",
    "start": "3067099",
    "end": "3073070"
  },
  {
    "text": "multi-step flows so a client is doing multi step off than you or throttling off that probability gets multiplied out",
    "start": "3073070",
    "end": "3078500"
  },
  {
    "text": "so multi step off was failing more often than you would expect from what you were throttling at all so retries retries or",
    "start": "3078500",
    "end": "3085670"
  },
  {
    "text": "a thing we throttle to mend points and then clients immediately tried as fast as they could to get back into the",
    "start": "3085670",
    "end": "3091339"
  },
  {
    "text": "endpoint which was not particular great and some clients actually would see one",
    "start": "3091339",
    "end": "3096500"
  },
  {
    "text": "failure at an end point and assume the whole game was offline so we really had to balance like what we were throttling with how the different clients handle",
    "start": "3096500",
    "end": "3102770"
  },
  {
    "text": "those endpoints being throttled so we learned a lot of stuff you know this is",
    "start": "3102770",
    "end": "3110270"
  },
  {
    "text": "a lot of experience really fast one of the key sort of takeaways that I hear",
    "start": "3110270",
    "end": "3115670"
  },
  {
    "start": "3115000",
    "end": "3115000"
  },
  {
    "text": "from this is it's very easy to have metric overload so you know we measured everything we were graphing everything but don't display graphs unless there's",
    "start": "3115670",
    "end": "3122480"
  },
  {
    "text": "a problem or someone will find one because you look at them we eventually summarized this as measure everything",
    "start": "3122480",
    "end": "3128180"
  },
  {
    "text": "mysteries everywhere because you're going to have outliers because the internet",
    "start": "3128180",
    "end": "3134619"
  },
  {
    "text": "and just don't stare graphs all day it's very easy to do that you know just build",
    "start": "3134799",
    "end": "3140709"
  },
  {
    "text": "a process have your baselines be able to measure everything and then show important things and use alerts to drive when you actually want to look at your",
    "start": "3140709",
    "end": "3146799"
  },
  {
    "text": "graphs and not just stare at them all night going it's just the internet or as this us so I my hand back off to Evan to",
    "start": "3146799",
    "end": "3153189"
  },
  {
    "text": "talk about some of the other things so",
    "start": "3153189",
    "end": "3158739"
  },
  {
    "start": "3158000",
    "end": "3158000"
  },
  {
    "text": "this wasn't a game that we got to soft launch we went literally in minutes to",
    "start": "3158739",
    "end": "3164349"
  },
  {
    "text": "our peak request that we would ever see when we talked about going to 3 billion requests we handled three billion",
    "start": "3164349",
    "end": "3169959"
  },
  {
    "text": "requests the first day we were out right and we we never got as big or a spiky as",
    "start": "3169959",
    "end": "3176739"
  },
  {
    "text": "we did that first day unfortunately that meant that some of the problems that we",
    "start": "3176739",
    "end": "3182319"
  },
  {
    "text": "had could we couldn't pivot as quickly or know as much as we wanted to ahead of",
    "start": "3182319",
    "end": "3188949"
  },
  {
    "text": "time so if you can soft launch figure out what's out there build your",
    "start": "3188949",
    "end": "3194289"
  },
  {
    "text": "baselines figure out what your metrics actually mean we definitely had some metrics that we didn't know how they",
    "start": "3194289",
    "end": "3200439"
  },
  {
    "text": "related to the actual health of the stack the actual health that the clients were actually seeing test and validate",
    "start": "3200439",
    "end": "3207309"
  },
  {
    "text": "your metrics and confirm your grafts mean what you think they mean you're",
    "start": "3207309",
    "end": "3212619"
  },
  {
    "text": "real players will create real data we did a lot of load testing and we did a",
    "start": "3212619",
    "end": "3217959"
  },
  {
    "text": "lot of theoretical testing but it turns out there was some fairly big holes we had no idea how big some of the requests",
    "start": "3217959",
    "end": "3223900"
  },
  {
    "text": "were going to end up being for instance we had no idea what certain call patterns were going to emerge flexible",
    "start": "3223900",
    "end": "3230140"
  },
  {
    "text": "api's they let your clients do really flexible things we had clients that like",
    "start": "3230140",
    "end": "3237640"
  },
  {
    "text": "we said earlier we're sending really big rights clients that we're doing things really fast or not backing off when they",
    "start": "3237640",
    "end": "3243939"
  },
  {
    "text": "saw errors so make sure you keep on top of what's actually happening and look at",
    "start": "3243939",
    "end": "3249339"
  },
  {
    "text": "your self launch data usage patterns and some of the stuff that we had to learn during our launch window we could",
    "start": "3249339",
    "end": "3255670"
  },
  {
    "text": "learned earlier don't forget your blast rated scales too if you're using other",
    "start": "3255670",
    "end": "3262150"
  },
  {
    "start": "3257000",
    "end": "3257000"
  },
  {
    "text": "vendors and you almost definitely are if you're going to launch a large-scale game make sure you let them know what",
    "start": "3262150",
    "end": "3267579"
  },
  {
    "text": "you're doing we had we had occasion the broader we",
    "start": "3267579",
    "end": "3273530"
  },
  {
    "text": "were using a beta some of their beta stuff and we actually managed to create",
    "start": "3273530",
    "end": "3279560"
  },
  {
    "text": "a graph that broke them there they were able to pivot really fast and it didn't",
    "start": "3279560",
    "end": "3284690"
  },
  {
    "text": "ever they never stopped stopped accepting metrics or anything like that but the only reason this was really bad",
    "start": "3284690",
    "end": "3289700"
  },
  {
    "text": "as we had forgotten to tell them who to contact when we did this so make sure",
    "start": "3289700",
    "end": "3294980"
  },
  {
    "text": "you have your other internal team contacts to you might think you're not affecting them but there's a good chance you are let everybody know before",
    "start": "3294980",
    "end": "3302570"
  },
  {
    "text": "something big happens remember your blast radius is more than is more than you you can easily take down other third",
    "start": "3302570",
    "end": "3309140"
  },
  {
    "text": "parties other teams other games other services in your own platform pick your",
    "start": "3309140",
    "end": "3315650"
  },
  {
    "start": "3314000",
    "end": "3314000"
  },
  {
    "text": "battles solve your biggest problem first even if there's other things that look",
    "start": "3315650",
    "end": "3320900"
  },
  {
    "text": "like they're low hanging fruit you will waste time trying to pick up things that aren't actually helping you use your use",
    "start": "3320900",
    "end": "3327710"
  },
  {
    "text": "AWS scalability to buy time you can trade you can trade money for stability for a short period of time for a lot of",
    "start": "3327710",
    "end": "3334310"
  },
  {
    "text": "things make sure you have someone dedicated to triage with so many people",
    "start": "3334310",
    "end": "3340940"
  },
  {
    "text": "in so many teams there were none of the teams where I actually talk to each",
    "start": "3340940",
    "end": "3346160"
  },
  {
    "text": "other because they're not in the same building so one team would see a problem and they would talk to us on HipChat and",
    "start": "3346160",
    "end": "3351650"
  },
  {
    "text": "people would jump in and try to help because people are helpful but it took people away from the important problems",
    "start": "3351650",
    "end": "3357760"
  },
  {
    "text": "you don't need to rewrite everything one of the big things we found here is the",
    "start": "3357760",
    "end": "3363440"
  },
  {
    "text": "mobile game platform team was able to peel off and do a huge amount of work that saved a lot of a lot of trouble",
    "start": "3363440",
    "end": "3369230"
  },
  {
    "text": "without having to fully understand everything the other team was doing and without having to rewrite everything",
    "start": "3369230",
    "end": "3375500"
  },
  {
    "text": "that they had to do literally weeks before launch share your API is not your",
    "start": "3375500",
    "end": "3383060"
  },
  {
    "text": "data this helped this helped us a lot because we're able to isolate our data over to the side and allow other",
    "start": "3383060",
    "end": "3391520"
  },
  {
    "text": "services and other teams to get at our data in a sane way so what's the future",
    "start": "3391520",
    "end": "3397330"
  },
  {
    "start": "3395000",
    "end": "3395000"
  },
  {
    "text": "or do more of this as we as new features are require as new sources are required we're going",
    "start": "3397330",
    "end": "3404360"
  },
  {
    "text": "to break them off from the more micro service type platform we're going to continue to embrace the mutability we",
    "start": "3404360",
    "end": "3409850"
  },
  {
    "text": "talked earlier about we don't reconfigure services on the fly we don't reconfigure instances on the fly we're",
    "start": "3409850",
    "end": "3416180"
  },
  {
    "text": "going to keep going down that path to make sure that we can destroy instances or we could just try and recreate",
    "start": "3416180",
    "end": "3423260"
  },
  {
    "text": "instances environments without having any any knowledge of what they're currently doing more crossed in",
    "start": "3423260",
    "end": "3430670"
  },
  {
    "text": "collaboration this is really hard sometimes in a big company you get you tend to get siloed don't there's a lot",
    "start": "3430670",
    "end": "3435950"
  },
  {
    "text": "of people out there that even if they don't look like they're doing what you're doing they probably have a lot of really good knowledge for it more Amazon",
    "start": "3435950",
    "end": "3442430"
  },
  {
    "text": "support our solutions architect is in here somewhere he talked just before this hour amazon support team actually",
    "start": "3442430",
    "end": "3450800"
  },
  {
    "text": "came in for this launch and SAT with us on site in our war room for 48 straight hours through this launch and where I",
    "start": "3450800",
    "end": "3458840"
  },
  {
    "text": "was hiring not only for this but also for the game teams everything come work for us so this takes a ton of people",
    "start": "3458840",
    "end": "3466910"
  },
  {
    "text": "certainly not just us but as to everybody inside to her about all of our vendors amazon everyone who lets us do",
    "start": "3466910",
    "end": "3472760"
  },
  {
    "text": "this and yeah that's about it yes thanks",
    "start": "3472760",
    "end": "3479470"
  }
]