[
  {
    "start": "0",
    "end": "37000"
  },
  {
    "text": "hi welcome back to AWS innovate you are",
    "start": "0",
    "end": "3270"
  },
  {
    "text": "currently logged into the big data and",
    "start": "3270",
    "end": "4920"
  },
  {
    "text": "analytics track in this track we are",
    "start": "4920",
    "end": "7589"
  },
  {
    "text": "covering topics around big data",
    "start": "7589",
    "end": "9660"
  },
  {
    "text": "analytics data leaks machine learning",
    "start": "9660",
    "end": "11849"
  },
  {
    "text": "and artificial intelligence I hope you",
    "start": "11849",
    "end": "14519"
  },
  {
    "text": "are finding this track useful and are",
    "start": "14519",
    "end": "16379"
  },
  {
    "text": "able to relate to the use cases and",
    "start": "16379",
    "end": "18000"
  },
  {
    "text": "requirements in your organization's in",
    "start": "18000",
    "end": "20100"
  },
  {
    "text": "this session we will look into how to",
    "start": "20100",
    "end": "22470"
  },
  {
    "text": "build data pipelines with AWS glue my",
    "start": "22470",
    "end": "25380"
  },
  {
    "text": "name is una pillai and I'm a Solutions",
    "start": "25380",
    "end": "27060"
  },
  {
    "text": "Architect with Amazon Web Services on a",
    "start": "27060",
    "end": "29849"
  },
  {
    "text": "day to day basis I work with customers",
    "start": "29849",
    "end": "31949"
  },
  {
    "text": "across Asia to help them build their",
    "start": "31949",
    "end": "33989"
  },
  {
    "text": "analytics platforms and data leaks on",
    "start": "33989",
    "end": "35820"
  },
  {
    "text": "AWS and today I have the honor of having",
    "start": "35820",
    "end": "38940"
  },
  {
    "start": "37000",
    "end": "88000"
  },
  {
    "text": "one of my customers join us to share how",
    "start": "38940",
    "end": "41610"
  },
  {
    "text": "they build their data pipelines using",
    "start": "41610",
    "end": "43320"
  },
  {
    "text": "AWS glue in this session I will give you",
    "start": "43320",
    "end": "47190"
  },
  {
    "text": "a quick introduction to AWS glue and",
    "start": "47190",
    "end": "49649"
  },
  {
    "text": "share why did we create the service we",
    "start": "49649",
    "end": "52770"
  },
  {
    "text": "will take a look at the different",
    "start": "52770",
    "end": "54180"
  },
  {
    "text": "components of AWS glue and understand",
    "start": "54180",
    "end": "56690"
  },
  {
    "text": "some other key features that glue",
    "start": "56690",
    "end": "58800"
  },
  {
    "text": "provides we will dive into how do you",
    "start": "58800",
    "end": "62100"
  },
  {
    "text": "construct an ETL workflow in blue all",
    "start": "62100",
    "end": "64320"
  },
  {
    "text": "the way from taking raw unfiltered data",
    "start": "64320",
    "end": "66240"
  },
  {
    "text": "to building a data pipeline in four easy",
    "start": "66240",
    "end": "69360"
  },
  {
    "text": "steps I'm going to dive into the AWS",
    "start": "69360",
    "end": "72540"
  },
  {
    "text": "console and actually build this out for",
    "start": "72540",
    "end": "74700"
  },
  {
    "text": "you and show you how it is done in the",
    "start": "74700",
    "end": "77400"
  },
  {
    "text": "latter part of this talk I will have mr.",
    "start": "77400",
    "end": "79560"
  },
  {
    "text": "Donham stock from next she Electronics",
    "start": "79560",
    "end": "81600"
  },
  {
    "text": "Island office which is part of the",
    "start": "81600",
    "end": "83430"
  },
  {
    "text": "Toyota auto show group to join us today",
    "start": "83430",
    "end": "85530"
  },
  {
    "text": "and share the story with us as well so",
    "start": "85530",
    "end": "89070"
  },
  {
    "start": "88000",
    "end": "136000"
  },
  {
    "text": "what is Glu Glu is a fully managed",
    "start": "89070",
    "end": "91650"
  },
  {
    "text": "service extract transform and load",
    "start": "91650",
    "end": "93390"
  },
  {
    "text": "service also called ETL for short now",
    "start": "93390",
    "end": "97409"
  },
  {
    "text": "there are plenty of ETL tools out there",
    "start": "97409",
    "end": "98939"
  },
  {
    "text": "but glue is one of those that are",
    "start": "98939",
    "end": "101490"
  },
  {
    "text": "designed for developers and designed by",
    "start": "101490",
    "end": "103320"
  },
  {
    "text": "developers we have thousands of",
    "start": "103320",
    "end": "106649"
  },
  {
    "text": "developers on a platform who are running",
    "start": "106649",
    "end": "108540"
  },
  {
    "text": "thousands and thousands of jobs on a day",
    "start": "108540",
    "end": "110430"
  },
  {
    "text": "to day basis in our ecosystem in AWS we",
    "start": "110430",
    "end": "114479"
  },
  {
    "text": "actually have a number of partners who",
    "start": "114479",
    "end": "116520"
  },
  {
    "text": "do ETL there are many tools already",
    "start": "116520",
    "end": "118920"
  },
  {
    "text": "available to customers here are some of",
    "start": "118920",
    "end": "122189"
  },
  {
    "text": "the ETL tools that are listed on the AWS",
    "start": "122189",
    "end": "124610"
  },
  {
    "text": "redshift Partner page but it turns out",
    "start": "124610",
    "end": "128160"
  },
  {
    "text": "that there are a lot of AWS customers",
    "start": "128160",
    "end": "130080"
  },
  {
    "text": "who are developing their ETL scripts by",
    "start": "130080",
    "end": "131970"
  },
  {
    "text": "hand that is actually",
    "start": "131970",
    "end": "133819"
  },
  {
    "text": "according their ETL jobs this is a very",
    "start": "133819",
    "end": "137299"
  },
  {
    "start": "136000",
    "end": "220000"
  },
  {
    "text": "reason why we built glue in AWS because",
    "start": "137299",
    "end": "142189"
  },
  {
    "text": "most of the tools that I showed you in",
    "start": "142189",
    "end": "143989"
  },
  {
    "text": "the previous slide does one thing and",
    "start": "143989",
    "end": "146569"
  },
  {
    "text": "very and what that one thing extremely",
    "start": "146569",
    "end": "148670"
  },
  {
    "text": "well but your ETL job in your company",
    "start": "148670",
    "end": "151400"
  },
  {
    "text": "isn't just about that one thing and when",
    "start": "151400",
    "end": "154639"
  },
  {
    "text": "you want to go beyond that one thing you",
    "start": "154639",
    "end": "156530"
  },
  {
    "text": "will have to end up having to build your",
    "start": "156530",
    "end": "158959"
  },
  {
    "text": "own ETL jobs and that is when you start",
    "start": "158959",
    "end": "162019"
  },
  {
    "text": "coding your ETL jobs by hand but code is",
    "start": "162019",
    "end": "165319"
  },
  {
    "text": "powerful and flexible there are a lot of",
    "start": "165319",
    "end": "167510"
  },
  {
    "text": "things that you can do with code where",
    "start": "167510",
    "end": "170060"
  },
  {
    "text": "with very very less limitations but code",
    "start": "170060",
    "end": "174379"
  },
  {
    "text": "is modular and you can actually build it",
    "start": "174379",
    "end": "176959"
  },
  {
    "text": "in the way you want it you can share it",
    "start": "176959",
    "end": "179239"
  },
  {
    "text": "with your team members using it or other",
    "start": "179239",
    "end": "181790"
  },
  {
    "text": "source control management tools that you",
    "start": "181790",
    "end": "183829"
  },
  {
    "text": "and your company uses and more",
    "start": "183829",
    "end": "186590"
  },
  {
    "text": "importantly your organization already",
    "start": "186590",
    "end": "188959"
  },
  {
    "text": "has a bunch of developers who know how",
    "start": "188959",
    "end": "191299"
  },
  {
    "text": "to deal with code they are building our",
    "start": "191299",
    "end": "193129"
  },
  {
    "text": "applications for you they have the",
    "start": "193129",
    "end": "195290"
  },
  {
    "text": "existing dev by plan in place used for",
    "start": "195290",
    "end": "198799"
  },
  {
    "text": "developing and managing your application",
    "start": "198799",
    "end": "200329"
  },
  {
    "text": "code so why not use the same to develop",
    "start": "200329",
    "end": "203659"
  },
  {
    "text": "and manage your ETL scripts and your ETL",
    "start": "203659",
    "end": "205489"
  },
  {
    "text": "jobs there are familiar tools of their",
    "start": "205489",
    "end": "209090"
  },
  {
    "text": "IDs for developing and debugging you",
    "start": "209090",
    "end": "211819"
  },
  {
    "text": "have got version control for tracking",
    "start": "211819",
    "end": "213500"
  },
  {
    "text": "changes you have got the testing",
    "start": "213500",
    "end": "215509"
  },
  {
    "text": "pipeline and continuous integration and",
    "start": "215509",
    "end": "217159"
  },
  {
    "text": "deployment pipelines and so on even",
    "start": "217159",
    "end": "221209"
  },
  {
    "text": "though everybody is coding doesn't meet",
    "start": "221209",
    "end": "223129"
  },
  {
    "text": "coding is easy in particularly writing",
    "start": "223129",
    "end": "225889"
  },
  {
    "text": "ETL job is especially hard the reason",
    "start": "225889",
    "end": "229489"
  },
  {
    "text": "it's hard is because the underlying data",
    "start": "229489",
    "end": "231319"
  },
  {
    "text": "is changing the underlying data schemas",
    "start": "231319",
    "end": "233599"
  },
  {
    "text": "are changing and the data formats keep",
    "start": "233599",
    "end": "235759"
  },
  {
    "text": "changing as well in an agile environment",
    "start": "235759",
    "end": "238099"
  },
  {
    "text": "you need to add or remove data sources",
    "start": "238099",
    "end": "240469"
  },
  {
    "text": "that have and most importantly the data",
    "start": "240469",
    "end": "243709"
  },
  {
    "text": "keeps growing and it grows pretty fast",
    "start": "243709",
    "end": "246280"
  },
  {
    "text": "what this means for developers is you",
    "start": "246280",
    "end": "249229"
  },
  {
    "text": "have to constantly change and to adapt",
    "start": "249229",
    "end": "251870"
  },
  {
    "text": "the scripts which makes hand coding ETL",
    "start": "251870",
    "end": "254060"
  },
  {
    "text": "jobs especially error-prone and brittle",
    "start": "254060",
    "end": "256750"
  },
  {
    "text": "that's why we created AWS glue it",
    "start": "256750",
    "end": "260239"
  },
  {
    "text": "actually does a lot of the",
    "start": "260239",
    "end": "261530"
  },
  {
    "text": "undifferentiated heavy lifting that you",
    "start": "261530",
    "end": "263539"
  },
  {
    "text": "need to do to manage these changes so",
    "start": "263539",
    "end": "265880"
  },
  {
    "text": "that developers can for",
    "start": "265880",
    "end": "267020"
  },
  {
    "text": "on the high level things that they need",
    "start": "267020",
    "end": "269060"
  },
  {
    "text": "to get done for their ETL code we do the",
    "start": "269060",
    "end": "273470"
  },
  {
    "text": "rest underneath let me walk you through",
    "start": "273470",
    "end": "276800"
  },
  {
    "text": "the three main components of AWS glue",
    "start": "276800",
    "end": "279430"
  },
  {
    "text": "first we have the data catalog this is",
    "start": "279430",
    "end": "282530"
  },
  {
    "text": "where all the metadata about your data",
    "start": "282530",
    "end": "284120"
  },
  {
    "text": "sets it's second component is the job",
    "start": "284120",
    "end": "287810"
  },
  {
    "text": "authoring and ETL system which you use",
    "start": "287810",
    "end": "290300"
  },
  {
    "text": "to write your ETL jobs the third",
    "start": "290300",
    "end": "292520"
  },
  {
    "text": "component is the one which provides you",
    "start": "292520",
    "end": "296060"
  },
  {
    "text": "the job execution framework in a server",
    "start": "296060",
    "end": "299300"
  },
  {
    "text": "list manner you write your ETL code and",
    "start": "299300",
    "end": "301849"
  },
  {
    "text": "give us the code we execute it for you",
    "start": "301849",
    "end": "304159"
  },
  {
    "text": "as a developer you need not worry about",
    "start": "304159",
    "end": "306050"
  },
  {
    "text": "the underlying infrastructure and worry",
    "start": "306050",
    "end": "308659"
  },
  {
    "text": "about provisioning those servers glue",
    "start": "308659",
    "end": "311720"
  },
  {
    "start": "310000",
    "end": "358000"
  },
  {
    "text": "catalog is a hive compatible meta store",
    "start": "311720",
    "end": "314710"
  },
  {
    "text": "which means that you can integrate with",
    "start": "314710",
    "end": "317810"
  },
  {
    "text": "a lot of other tools including other AWS",
    "start": "317810",
    "end": "320419"
  },
  {
    "text": "analytic services like EMR Athena and",
    "start": "320419",
    "end": "323360"
  },
  {
    "text": "redshift spectrum glue crawlers can",
    "start": "323360",
    "end": "326750"
  },
  {
    "text": "automatically discover your data in",
    "start": "326750",
    "end": "328520"
  },
  {
    "text": "further schema and store the details in",
    "start": "328520",
    "end": "331280"
  },
  {
    "text": "blue catalog blue catalog makes it easy",
    "start": "331280",
    "end": "334280"
  },
  {
    "text": "for data engineers and developers to",
    "start": "334280",
    "end": "336620"
  },
  {
    "text": "quickly search and locate your data and",
    "start": "336620",
    "end": "338750"
  },
  {
    "text": "use it for rewriting your ETL scripts",
    "start": "338750",
    "end": "341180"
  },
  {
    "text": "without having to manually keep track of",
    "start": "341180",
    "end": "343610"
  },
  {
    "text": "the schema and data formats it also has",
    "start": "343610",
    "end": "346729"
  },
  {
    "text": "information on statistics of your data",
    "start": "346729",
    "end": "349070"
  },
  {
    "text": "like how big the data is and how many",
    "start": "349070",
    "end": "352159"
  },
  {
    "text": "records do you have what are the other",
    "start": "352159",
    "end": "354500"
  },
  {
    "text": "partition information and so forth the",
    "start": "354500",
    "end": "359479"
  },
  {
    "text": "first thing that you do is it lets you",
    "start": "359479",
    "end": "361719"
  },
  {
    "text": "get started quickly with the ETL flow it",
    "start": "361719",
    "end": "365090"
  },
  {
    "text": "generates code for you if you point it",
    "start": "365090",
    "end": "367159"
  },
  {
    "text": "to a table inside the data catalog it",
    "start": "367159",
    "end": "370430"
  },
  {
    "text": "generates code in Scala or Python it",
    "start": "370430",
    "end": "374419"
  },
  {
    "text": "works with Apache spark in the backend",
    "start": "374419",
    "end": "376099"
  },
  {
    "text": "so it's an environment that many of the",
    "start": "376099",
    "end": "378710"
  },
  {
    "text": "data engineers and developers are",
    "start": "378710",
    "end": "380210"
  },
  {
    "text": "already familiar with then we offer",
    "start": "380210",
    "end": "383120"
  },
  {
    "text": "tools that allow you to edit debug your",
    "start": "383120",
    "end": "385819"
  },
  {
    "text": "code and explore your data sets that you",
    "start": "385819",
    "end": "388069"
  },
  {
    "text": "want to analyze we also have new and",
    "start": "388069",
    "end": "391069"
  },
  {
    "text": "interesting primitives that make it easy",
    "start": "391069",
    "end": "393139"
  },
  {
    "text": "for you to rewrite your ETL jobs clue",
    "start": "393139",
    "end": "396229"
  },
  {
    "text": "ETL jobs can be executed on demand or",
    "start": "396229",
    "end": "398919"
  },
  {
    "text": "run on a friction",
    "start": "398919",
    "end": "400760"
  },
  {
    "text": "like a cron job it can also be triggered",
    "start": "400760",
    "end": "403300"
  },
  {
    "text": "based on when a particular trigger",
    "start": "403300",
    "end": "406250"
  },
  {
    "text": "condition is met throughout this talk",
    "start": "406250",
    "end": "409820"
  },
  {
    "start": "408000",
    "end": "610000"
  },
  {
    "text": "I'll be using an example an use case",
    "start": "409820",
    "end": "412370"
  },
  {
    "text": "where we'll be converting some JSON data",
    "start": "412370",
    "end": "414500"
  },
  {
    "text": "to park' format",
    "start": "414500",
    "end": "416150"
  },
  {
    "text": "it's a typical process that you follow",
    "start": "416150",
    "end": "418190"
  },
  {
    "text": "when building a data Lake and working",
    "start": "418190",
    "end": "420680"
  },
  {
    "text": "with data warehouses here is what we are",
    "start": "420680",
    "end": "423740"
  },
  {
    "text": "going to do we are going to work with",
    "start": "423740",
    "end": "426200"
  },
  {
    "text": "github archive data to put it simply",
    "start": "426200",
    "end": "429170"
  },
  {
    "text": "it's an archive of all the events that",
    "start": "429170",
    "end": "431030"
  },
  {
    "text": "happen in the public github repos which",
    "start": "431030",
    "end": "433490"
  },
  {
    "text": "getup makes it available to the public",
    "start": "433490",
    "end": "435290"
  },
  {
    "text": "on an hourly basis you can go to their",
    "start": "435290",
    "end": "439040"
  },
  {
    "text": "website and download these files first",
    "start": "439040",
    "end": "442070"
  },
  {
    "text": "we will find a way to ingest the data",
    "start": "442070",
    "end": "443930"
  },
  {
    "text": "into the pipeline in this case we'll be",
    "start": "443930",
    "end": "446600"
  },
  {
    "text": "manually downloading the files from",
    "start": "446600",
    "end": "448430"
  },
  {
    "text": "github archive and putting it into an s3",
    "start": "448430",
    "end": "450290"
  },
  {
    "text": "bucket second we will run the grooc",
    "start": "450290",
    "end": "453530"
  },
  {
    "text": "crawler on the data sitting in s3 the",
    "start": "453530",
    "end": "456380"
  },
  {
    "text": "crawler will infer the schema generate",
    "start": "456380",
    "end": "458690"
  },
  {
    "text": "the statistics and create the tables in",
    "start": "458690",
    "end": "461090"
  },
  {
    "text": "the glue catalog third we will author a",
    "start": "461090",
    "end": "465590"
  },
  {
    "text": "new glue job using glues in build code",
    "start": "465590",
    "end": "468410"
  },
  {
    "text": "editor to convert your JSON files to",
    "start": "468410",
    "end": "470870"
  },
  {
    "text": "parque format and store it back in s3",
    "start": "470870",
    "end": "473770"
  },
  {
    "text": "once the process and the transform files",
    "start": "473770",
    "end": "476990"
  },
  {
    "text": "are available in s3 they can be consumed",
    "start": "476990",
    "end": "479210"
  },
  {
    "text": "by a variety of analytic services to run",
    "start": "479210",
    "end": "481760"
  },
  {
    "text": "queries over in this example we will be",
    "start": "481760",
    "end": "485930"
  },
  {
    "text": "using Athena to query the data and Crick",
    "start": "485930",
    "end": "488600"
  },
  {
    "text": "site to visualize it as I said earlier",
    "start": "488600",
    "end": "492020"
  },
  {
    "text": "AWS glue is backed by Apache spark which",
    "start": "492020",
    "end": "495560"
  },
  {
    "text": "is a distributed parallel and scalable",
    "start": "495560",
    "end": "497870"
  },
  {
    "text": "data data processing engine that has got",
    "start": "497870",
    "end": "500900"
  },
  {
    "text": "fault tolerance inbuilt so if things",
    "start": "500900",
    "end": "503930"
  },
  {
    "text": "fail it can take care of it and you",
    "start": "503930",
    "end": "506420"
  },
  {
    "text": "don't have to worry about it as a",
    "start": "506420",
    "end": "508160"
  },
  {
    "text": "developer with spark you can not only",
    "start": "508160",
    "end": "511550"
  },
  {
    "text": "write scripts but you can also write",
    "start": "511550",
    "end": "513050"
  },
  {
    "text": "sequel with spark you can intermingle",
    "start": "513050",
    "end": "516050"
  },
  {
    "text": "these things freely so it's quite",
    "start": "516050",
    "end": "518000"
  },
  {
    "text": "flexible to use Apache spark has a very",
    "start": "518000",
    "end": "521599"
  },
  {
    "text": "rich ecosystem and very strong community",
    "start": "521599",
    "end": "524210"
  },
  {
    "text": "support many companies are running it",
    "start": "524210",
    "end": "527030"
  },
  {
    "text": "for all the way from processing the data",
    "start": "527030",
    "end": "529580"
  },
  {
    "text": "streams to doing graph analytics to",
    "start": "529580",
    "end": "532190"
  },
  {
    "text": "building machine learning models using",
    "start": "532190",
    "end": "534140"
  },
  {
    "text": "spark amendable",
    "start": "534140",
    "end": "536170"
  },
  {
    "text": "core of spark what they have is that",
    "start": "536170",
    "end": "539930"
  },
  {
    "text": "it's a structure called rdd's they are",
    "start": "539930",
    "end": "542959"
  },
  {
    "text": "basically resilient distributed data",
    "start": "542959",
    "end": "545060"
  },
  {
    "text": "sets on top of rdd's you have a data",
    "start": "545060",
    "end": "548870"
  },
  {
    "text": "structure called data frames it provides",
    "start": "548870",
    "end": "551329"
  },
  {
    "text": "you the structure that lets you run your",
    "start": "551329",
    "end": "554089"
  },
  {
    "text": "transformations when using glue in",
    "start": "554089",
    "end": "557569"
  },
  {
    "text": "addition to the spark data frames you",
    "start": "557569",
    "end": "560089"
  },
  {
    "text": "have access to glues ETL libraries these",
    "start": "560089",
    "end": "563629"
  },
  {
    "text": "libraries allow you to integrate with",
    "start": "563629",
    "end": "566060"
  },
  {
    "text": "all the other glue components like data",
    "start": "566060",
    "end": "568759"
  },
  {
    "text": "catalog job orchestration and bookmarks",
    "start": "568759",
    "end": "571399"
  },
  {
    "text": "for checkpointing and maintenance of the",
    "start": "571399",
    "end": "574339"
  },
  {
    "text": "state of your job execution glue",
    "start": "574339",
    "end": "577310"
  },
  {
    "text": "libraries also allow you to integrate",
    "start": "577310",
    "end": "579350"
  },
  {
    "text": "with other AWS services like s3 and RDS",
    "start": "579350",
    "end": "583329"
  },
  {
    "text": "as a part of the glue ETL libraries we",
    "start": "583329",
    "end": "586819"
  },
  {
    "text": "also have shared some really cool ETL",
    "start": "586819",
    "end": "589730"
  },
  {
    "text": "transforms connectors and format",
    "start": "589730",
    "end": "591889"
  },
  {
    "text": "supports which would require a lot of",
    "start": "591889",
    "end": "594259"
  },
  {
    "text": "manual effort from you if you had to do",
    "start": "594259",
    "end": "595970"
  },
  {
    "text": "it on your own next I want to introduce",
    "start": "595970",
    "end": "599750"
  },
  {
    "text": "the concept of a dynamic data frame but",
    "start": "599750",
    "end": "602720"
  },
  {
    "text": "before let's take a look at the github",
    "start": "602720",
    "end": "604459"
  },
  {
    "text": "data archive data sets and what are we",
    "start": "604459",
    "end": "608089"
  },
  {
    "text": "dealing with so this is the github",
    "start": "608089",
    "end": "611209"
  },
  {
    "start": "610000",
    "end": "640000"
  },
  {
    "text": "archive data set the data sets in JSON",
    "start": "611209",
    "end": "613910"
  },
  {
    "text": "format there are more than 35 different",
    "start": "613910",
    "end": "617149"
  },
  {
    "text": "types of events that are present in this",
    "start": "617149",
    "end": "619279"
  },
  {
    "text": "data set if you look at the image on the",
    "start": "619279",
    "end": "621949"
  },
  {
    "text": "screen you can see that there is a",
    "start": "621949",
    "end": "623899"
  },
  {
    "text": "different type of event and each",
    "start": "623899",
    "end": "625939"
  },
  {
    "text": "different type of event has its own",
    "start": "625939",
    "end": "627649"
  },
  {
    "text": "payload and each payload has its own",
    "start": "627649",
    "end": "629810"
  },
  {
    "text": "structure and size this particularly",
    "start": "629810",
    "end": "632810"
  },
  {
    "text": "makes it difficult to process and use",
    "start": "632810",
    "end": "635389"
  },
  {
    "text": "this data now let's take a look at how",
    "start": "635389",
    "end": "638059"
  },
  {
    "text": "glue helps us in this situation data",
    "start": "638059",
    "end": "641240"
  },
  {
    "start": "640000",
    "end": "716000"
  },
  {
    "text": "frames are the co structures for Spock",
    "start": "641240",
    "end": "643040"
  },
  {
    "text": "sequel they are like structure tables in",
    "start": "643040",
    "end": "645380"
  },
  {
    "text": "the database system you kind of need",
    "start": "645380",
    "end": "647300"
  },
  {
    "text": "them to have the schema defined up front",
    "start": "647300",
    "end": "650589"
  },
  {
    "text": "each show in the data frame has the same",
    "start": "650589",
    "end": "653269"
  },
  {
    "text": "structure so what this means is if you",
    "start": "653269",
    "end": "655610"
  },
  {
    "text": "are trying to load the data and you",
    "start": "655610",
    "end": "657769"
  },
  {
    "text": "don't know the schema of your data up",
    "start": "657769",
    "end": "659300"
  },
  {
    "text": "front Spock has to first read all the",
    "start": "659300",
    "end": "661790"
  },
  {
    "text": "data and compute the schema if you are",
    "start": "661790",
    "end": "665029"
  },
  {
    "text": "working with structured data that looks",
    "start": "665029",
    "end": "667009"
  },
  {
    "text": "tabular this may",
    "start": "667009",
    "end": "668570"
  },
  {
    "text": "cents if you want to do sql-like",
    "start": "668570",
    "end": "670550"
  },
  {
    "text": "analytics but for doing that and working",
    "start": "670550",
    "end": "675890"
  },
  {
    "text": "on ETL for unstructured data it's not",
    "start": "675890",
    "end": "677900"
  },
  {
    "text": "quite ideal this is a reason why we",
    "start": "677900",
    "end": "681320"
  },
  {
    "text": "designed dynamic data frames in AWS glue",
    "start": "681320",
    "end": "683620"
  },
  {
    "text": "it has relaxed some of the assumptions",
    "start": "683620",
    "end": "686050"
  },
  {
    "text": "about having the schema upfront and",
    "start": "686050",
    "end": "688430"
  },
  {
    "text": "having the same structure per row they",
    "start": "688430",
    "end": "692840"
  },
  {
    "text": "are like data frames but designed for",
    "start": "692840",
    "end": "694430"
  },
  {
    "text": "ETL",
    "start": "694430",
    "end": "695120"
  },
  {
    "text": "they are designed for processing semi",
    "start": "695120",
    "end": "697250"
  },
  {
    "text": "structured data what that means is if",
    "start": "697250",
    "end": "700160"
  },
  {
    "text": "you take an example that I showed you",
    "start": "700160",
    "end": "701660"
  },
  {
    "text": "earlier for public github archive events",
    "start": "701660",
    "end": "704060"
  },
  {
    "text": "you have a lot of different event types",
    "start": "704060",
    "end": "706580"
  },
  {
    "text": "and payload structures varies for each",
    "start": "706580",
    "end": "708740"
  },
  {
    "text": "event every dynamic frame is a",
    "start": "708740",
    "end": "711710"
  },
  {
    "text": "collection of individual records called",
    "start": "711710",
    "end": "713930"
  },
  {
    "text": "dynamic records dynamic records are self",
    "start": "713930",
    "end": "717680"
  },
  {
    "text": "describing so you don't actually need to",
    "start": "717680",
    "end": "719780"
  },
  {
    "text": "have schema stored up front at the",
    "start": "719780",
    "end": "722570"
  },
  {
    "text": "dynamic frame level each record",
    "start": "722570",
    "end": "724730"
  },
  {
    "text": "individually stores its own schema so it",
    "start": "724730",
    "end": "727190"
  },
  {
    "text": "can vary from record to record what this",
    "start": "727190",
    "end": "729950"
  },
  {
    "text": "means is it's really easy to run",
    "start": "729950",
    "end": "731740"
  },
  {
    "text": "transformations on semi structured data",
    "start": "731740",
    "end": "734470"
  },
  {
    "text": "if you notice here in our create event",
    "start": "734470",
    "end": "738290"
  },
  {
    "text": "pull event that is the first and the",
    "start": "738290",
    "end": "740600"
  },
  {
    "text": "third dynamic record on the screen the",
    "start": "740600",
    "end": "742940"
  },
  {
    "text": "data type for our ID fields are",
    "start": "742940",
    "end": "744650"
  },
  {
    "text": "different the first one is strength",
    "start": "744650",
    "end": "746600"
  },
  {
    "text": "while the other one is an integer if you",
    "start": "746600",
    "end": "750110"
  },
  {
    "text": "use it with spark data frames it will",
    "start": "750110",
    "end": "752510"
  },
  {
    "text": "read whatever it will read whatever it",
    "start": "752510",
    "end": "754520"
  },
  {
    "text": "is there and it converted into whatever",
    "start": "754520",
    "end": "756770"
  },
  {
    "text": "it thinks it should convert it into but",
    "start": "756770",
    "end": "759830"
  },
  {
    "text": "blue dynamic data frames doesn't do that",
    "start": "759830",
    "end": "762370"
  },
  {
    "text": "they are self describing we keep that",
    "start": "762370",
    "end": "765260"
  },
  {
    "text": "intact and when you at the later stage",
    "start": "765260",
    "end": "767420"
  },
  {
    "text": "can't decide or what you want to do with",
    "start": "767420",
    "end": "769850"
  },
  {
    "text": "it so in the dynamic frame here are the",
    "start": "769850",
    "end": "773120"
  },
  {
    "text": "bottom left side of the screen you can",
    "start": "773120",
    "end": "775580"
  },
  {
    "text": "see that there are two different",
    "start": "775580",
    "end": "776810"
  },
  {
    "text": "versions of the ID one that is integer",
    "start": "776810",
    "end": "779390"
  },
  {
    "text": "and the other one that is string these",
    "start": "779390",
    "end": "783290"
  },
  {
    "text": "features provide dynamic frames make it",
    "start": "783290",
    "end": "785810"
  },
  {
    "text": "easier for us to transform and run",
    "start": "785810",
    "end": "787790"
  },
  {
    "text": "transformations on it let's take a look",
    "start": "787790",
    "end": "791210"
  },
  {
    "text": "at some of the inbuilt transforms that",
    "start": "791210",
    "end": "792860"
  },
  {
    "text": "blue comes with with dynamic data frames",
    "start": "792860",
    "end": "795680"
  },
  {
    "start": "794000",
    "end": "824000"
  },
  {
    "text": "you can perform over 15 transforms one",
    "start": "795680",
    "end": "798710"
  },
  {
    "text": "of the simplest thing that you can do is",
    "start": "798710",
    "end": "800210"
  },
  {
    "text": "to decide how",
    "start": "800210",
    "end": "801889"
  },
  {
    "text": "you want to resolve your choices between",
    "start": "801889",
    "end": "803540"
  },
  {
    "text": "integers or strength another transform",
    "start": "803540",
    "end": "806449"
  },
  {
    "text": "that Glu supports is called apply",
    "start": "806449",
    "end": "808009"
  },
  {
    "text": "mappings this will allow you to take",
    "start": "808009",
    "end": "810259"
  },
  {
    "text": "structured semi-structured or nested",
    "start": "810259",
    "end": "812509"
  },
  {
    "text": "json data and flatten it in any way that",
    "start": "812509",
    "end": "814970"
  },
  {
    "text": "you want and pick up the columns that",
    "start": "814970",
    "end": "816949"
  },
  {
    "text": "you want and rename them you can also go",
    "start": "816949",
    "end": "820100"
  },
  {
    "text": "from a flat structure to a nested",
    "start": "820100",
    "end": "821629"
  },
  {
    "text": "structure one of the more sophisticated",
    "start": "821629",
    "end": "825439"
  },
  {
    "start": "824000",
    "end": "903000"
  },
  {
    "text": "and interesting transforms available in",
    "start": "825439",
    "end": "827660"
  },
  {
    "text": "blue is called relational eyes it takes",
    "start": "827660",
    "end": "830749"
  },
  {
    "text": "in any arbitrary schema that you have in",
    "start": "830749",
    "end": "833179"
  },
  {
    "text": "a semi structured form and turns it into",
    "start": "833179",
    "end": "835579"
  },
  {
    "text": "a collection of tables and columns the",
    "start": "835579",
    "end": "838669"
  },
  {
    "text": "resulting set of the dynamic frames can",
    "start": "838669",
    "end": "840919"
  },
  {
    "text": "then be loaded into a serving layer of",
    "start": "840919",
    "end": "842809"
  },
  {
    "text": "the data Lake either by dumping them",
    "start": "842809",
    "end": "845720"
  },
  {
    "text": "into s3 in park' format or loaded into a",
    "start": "845720",
    "end": "848749"
  },
  {
    "text": "data warehouse like Amazon redshift",
    "start": "848749",
    "end": "850239"
  },
  {
    "text": "without having to worry about the",
    "start": "850239",
    "end": "852799"
  },
  {
    "text": "semi-structured nature of the data it",
    "start": "852799",
    "end": "856129"
  },
  {
    "text": "basically creates a column for every",
    "start": "856129",
    "end": "857869"
  },
  {
    "text": "path from the root to the leaf and for",
    "start": "857869",
    "end": "861559"
  },
  {
    "text": "every array it separates it out into an",
    "start": "861559",
    "end": "863660"
  },
  {
    "text": "auxilary table this auxiliary table will",
    "start": "863660",
    "end": "866899"
  },
  {
    "text": "have multiple elements for that array",
    "start": "866899",
    "end": "868519"
  },
  {
    "text": "and it will join back up with the",
    "start": "868519",
    "end": "870949"
  },
  {
    "text": "original table using the primary foreign",
    "start": "870949",
    "end": "873019"
  },
  {
    "text": "key relationship the real power of the",
    "start": "873019",
    "end": "875749"
  },
  {
    "text": "relational eyes transform is that it",
    "start": "875749",
    "end": "877759"
  },
  {
    "text": "allows you to process some I structured",
    "start": "877759",
    "end": "879350"
  },
  {
    "text": "data in glue and then normalize the data",
    "start": "879350",
    "end": "881809"
  },
  {
    "text": "set into individual tables in redshift",
    "start": "881809",
    "end": "884329"
  },
  {
    "text": "this is far more efficient than storing",
    "start": "884329",
    "end": "886910"
  },
  {
    "text": "raw JSON data in redshift as it is",
    "start": "886910",
    "end": "889069"
  },
  {
    "text": "optimized for faster reads from a",
    "start": "889069",
    "end": "891049"
  },
  {
    "text": "columnar data format this makes your",
    "start": "891049",
    "end": "893839"
  },
  {
    "text": "sequel queries can go orders of",
    "start": "893839",
    "end": "897199"
  },
  {
    "text": "magnitude faster if if you were just to",
    "start": "897199",
    "end": "900079"
  },
  {
    "text": "natively process JSON data these are",
    "start": "900079",
    "end": "904040"
  },
  {
    "start": "903000",
    "end": "948000"
  },
  {
    "text": "some of the other transforms available",
    "start": "904040",
    "end": "905600"
  },
  {
    "text": "in glue 2df this transform converts the",
    "start": "905600",
    "end": "909949"
  },
  {
    "text": "glue dynamic data frame to a spark data",
    "start": "909949",
    "end": "911839"
  },
  {
    "text": "frame it is useful in situations where",
    "start": "911839",
    "end": "914329"
  },
  {
    "text": "glue doesn't have a transform that you",
    "start": "914329",
    "end": "915829"
  },
  {
    "text": "want to run and you would like to run",
    "start": "915829",
    "end": "918230"
  },
  {
    "text": "your own transforms on it",
    "start": "918230",
    "end": "919429"
  },
  {
    "text": "spagett this transform will jump samples",
    "start": "919429",
    "end": "923419"
  },
  {
    "text": "of your data sets running in your",
    "start": "923419",
    "end": "925579"
  },
  {
    "text": "pipeline into s3 so that you can debug",
    "start": "925579",
    "end": "927799"
  },
  {
    "text": "your code and see the pipeline is",
    "start": "927799",
    "end": "930169"
  },
  {
    "text": "spitting out data that isn't the desired",
    "start": "930169",
    "end": "932449"
  },
  {
    "text": "format join",
    "start": "932449",
    "end": "935310"
  },
  {
    "text": "helps you join two dynamic data frames",
    "start": "935310",
    "end": "937620"
  },
  {
    "text": "and gives you the combined dynamic data",
    "start": "937620",
    "end": "940320"
  },
  {
    "text": "frame there are a bunch of other",
    "start": "940320",
    "end": "942990"
  },
  {
    "text": "transforms as well and they are well",
    "start": "942990",
    "end": "945060"
  },
  {
    "text": "documented on our website all right it's",
    "start": "945060",
    "end": "948029"
  },
  {
    "start": "948000",
    "end": "1342000"
  },
  {
    "text": "time for the demo let's quickly take a",
    "start": "948029",
    "end": "950190"
  },
  {
    "text": "look at what are we going to do during",
    "start": "950190",
    "end": "951900"
  },
  {
    "text": "the demo I'm going to build out a",
    "start": "951900",
    "end": "954330"
  },
  {
    "text": "pipeline using glue in four easy steps",
    "start": "954330",
    "end": "956850"
  },
  {
    "text": "I will ingest the data into s3 in this",
    "start": "956850",
    "end": "961080"
  },
  {
    "text": "step I will do download from and",
    "start": "961080",
    "end": "963420"
  },
  {
    "text": "download the data from github archive",
    "start": "963420",
    "end": "965640"
  },
  {
    "text": "comm and put it in the s3 door folder",
    "start": "965640",
    "end": "969320"
  },
  {
    "text": "then I will create a glue crawler to",
    "start": "969320",
    "end": "972390"
  },
  {
    "text": "discover and catalog my data once the",
    "start": "972390",
    "end": "975750"
  },
  {
    "text": "crawler has done its job",
    "start": "975750",
    "end": "976980"
  },
  {
    "text": "I will create a glue job to transform my",
    "start": "976980",
    "end": "979920"
  },
  {
    "text": "JSON data into Parker format once the",
    "start": "979920",
    "end": "983460"
  },
  {
    "text": "glue job is finished transforming files",
    "start": "983460",
    "end": "986100"
  },
  {
    "text": "it will be available in s3 buckets",
    "start": "986100",
    "end": "987800"
  },
  {
    "text": "processed data folder from there on I'll",
    "start": "987800",
    "end": "991110"
  },
  {
    "text": "be able to access the process data using",
    "start": "991110",
    "end": "993540"
  },
  {
    "text": "Athena and we'll be able to visualize it",
    "start": "993540",
    "end": "995850"
  },
  {
    "text": "using tools like quick side now let's",
    "start": "995850",
    "end": "1000589"
  },
  {
    "text": "switch to the demo screen I will log",
    "start": "1000589",
    "end": "1002990"
  },
  {
    "text": "into my AWS console and show you how to",
    "start": "1002990",
    "end": "1005510"
  },
  {
    "text": "build a pipeline in AWS glue so here we",
    "start": "1005510",
    "end": "1008780"
  },
  {
    "text": "are in my AWS console and in my s3",
    "start": "1008780",
    "end": "1011240"
  },
  {
    "text": "bucket so this is the s3 bucket that",
    "start": "1011240",
    "end": "1013130"
  },
  {
    "text": "we'll be using and working with this",
    "start": "1013130",
    "end": "1014540"
  },
  {
    "text": "demo so here I am creating a new folder",
    "start": "1014540",
    "end": "1017780"
  },
  {
    "text": "to store our source data then I will",
    "start": "1017780",
    "end": "1020839"
  },
  {
    "text": "download the files from the github",
    "start": "1020839",
    "end": "1022670"
  },
  {
    "text": "archive and upload it into my history",
    "start": "1022670",
    "end": "1024500"
  },
  {
    "text": "bucket so let's see how that is done",
    "start": "1024500",
    "end": "1028780"
  },
  {
    "text": "all right so now I will upload the file",
    "start": "1038439",
    "end": "1041038"
  },
  {
    "text": "it's gonna take a couple of seconds for",
    "start": "1041039",
    "end": "1043298"
  },
  {
    "text": "the file to get uploaded",
    "start": "1043299",
    "end": "1046258"
  },
  {
    "text": "you",
    "start": "1051670",
    "end": "1053730"
  },
  {
    "text": "you know that the file is uploaded will",
    "start": "1053840",
    "end": "1056150"
  },
  {
    "text": "go to the AWS glue console and create a",
    "start": "1056150",
    "end": "1058789"
  },
  {
    "text": "crawler there so let's open the AWS",
    "start": "1058789",
    "end": "1063320"
  },
  {
    "text": "console so this is how the AWS console",
    "start": "1063320",
    "end": "1067340"
  },
  {
    "text": "looks on the left hand side I'll click",
    "start": "1067340",
    "end": "1068990"
  },
  {
    "text": "on the crawlers and I will create a new",
    "start": "1068990",
    "end": "1071690"
  },
  {
    "text": "chloro I'll specify a name for the",
    "start": "1071690",
    "end": "1074299"
  },
  {
    "text": "crawler and click Next here I'm",
    "start": "1074299",
    "end": "1079610"
  },
  {
    "text": "specifying whether the source data is",
    "start": "1079610",
    "end": "1081470"
  },
  {
    "text": "kept so in this case it is in my",
    "start": "1081470",
    "end": "1083090"
  },
  {
    "text": "innovate blue demo bucket",
    "start": "1083090",
    "end": "1086529"
  },
  {
    "text": "so here I've specified the source bucket",
    "start": "1086529",
    "end": "1089809"
  },
  {
    "text": "and here I'm specifying the I am rule",
    "start": "1089809",
    "end": "1091940"
  },
  {
    "text": "that the crawler will be using to crawl",
    "start": "1091940",
    "end": "1095179"
  },
  {
    "text": "my data in s3 here I'm adding a new",
    "start": "1095179",
    "end": "1098570"
  },
  {
    "text": "database in the glue catalog by saying",
    "start": "1098570",
    "end": "1101179"
  },
  {
    "text": "we're calling it innovate DB I'm",
    "start": "1101179",
    "end": "1104570"
  },
  {
    "text": "reviewing the changes and click on",
    "start": "1104570",
    "end": "1106070"
  },
  {
    "text": "finish now we will run the crawler and",
    "start": "1106070",
    "end": "1108350"
  },
  {
    "text": "wait for a couple of seconds for the",
    "start": "1108350",
    "end": "1109640"
  },
  {
    "text": "crawler to finish you can see that there",
    "start": "1109640",
    "end": "1112700"
  },
  {
    "text": "will be one table that will be added to",
    "start": "1112700",
    "end": "1114080"
  },
  {
    "text": "the crawler and now if you go to the",
    "start": "1114080",
    "end": "1118760"
  },
  {
    "text": "catalog click on the database and look",
    "start": "1118760",
    "end": "1120620"
  },
  {
    "text": "at the tables there is a new table",
    "start": "1120620",
    "end": "1122570"
  },
  {
    "text": "created called data and it has this data",
    "start": "1122570",
    "end": "1125000"
  },
  {
    "text": "structure that we empowered for the data",
    "start": "1125000",
    "end": "1127250"
  },
  {
    "text": "that we imported from github archive",
    "start": "1127250",
    "end": "1131169"
  },
  {
    "text": "next we will create a new job in Glu I'm",
    "start": "1133299",
    "end": "1138020"
  },
  {
    "text": "specifying a new job a new a new name",
    "start": "1138020",
    "end": "1140960"
  },
  {
    "text": "from a glue job specifying an iamb role",
    "start": "1140960",
    "end": "1143840"
  },
  {
    "text": "and choosing python as a language here",
    "start": "1143840",
    "end": "1146299"
  },
  {
    "text": "I'm specifying the source data set",
    "start": "1146299",
    "end": "1148570"
  },
  {
    "text": "specifying the target data set so in",
    "start": "1148570",
    "end": "1150620"
  },
  {
    "text": "this case it will be an s3 or folder",
    "start": "1150620",
    "end": "1152690"
  },
  {
    "text": "called processed data",
    "start": "1152690",
    "end": "1154010"
  },
  {
    "text": "I'm also specifying the format of the",
    "start": "1154010",
    "end": "1156289"
  },
  {
    "text": "file to be Parker",
    "start": "1156289",
    "end": "1159250"
  },
  {
    "text": "here in the screen I'm specifying the",
    "start": "1169970",
    "end": "1172040"
  },
  {
    "text": "mappings from my source source data set",
    "start": "1172040",
    "end": "1174380"
  },
  {
    "text": "to my target data set and I'll be adding",
    "start": "1174380",
    "end": "1176480"
  },
  {
    "text": "and removing some of the fields that I'm",
    "start": "1176480",
    "end": "1178850"
  },
  {
    "text": "interested in so in this case you can",
    "start": "1178850",
    "end": "1181640"
  },
  {
    "text": "see that I'm interested only in the repo",
    "start": "1181640",
    "end": "1183200"
  },
  {
    "text": "name so I'll be mapping the repo name",
    "start": "1183200",
    "end": "1185030"
  },
  {
    "text": "and the second field that I'm interested",
    "start": "1185030",
    "end": "1187550"
  },
  {
    "text": "in is the actor field so I'll be mapping",
    "start": "1187550",
    "end": "1189410"
  },
  {
    "text": "the repo name to repo and the actor name",
    "start": "1189410",
    "end": "1193970"
  },
  {
    "text": "that is the login ID to actor on the",
    "start": "1193970",
    "end": "1197270"
  },
  {
    "text": "right hand side once this is done I'll",
    "start": "1197270",
    "end": "1201590"
  },
  {
    "text": "be saving the job and this is if it",
    "start": "1201590",
    "end": "1204170"
  },
  {
    "text": "gives me the editor for Glu jobs here",
    "start": "1204170",
    "end": "1207530"
  },
  {
    "text": "that here you see here you can see that",
    "start": "1207530",
    "end": "1209660"
  },
  {
    "text": "there is a source system I'm applying",
    "start": "1209660",
    "end": "1211280"
  },
  {
    "text": "the mappings I'm resolving some of the",
    "start": "1211280",
    "end": "1212810"
  },
  {
    "text": "choices and dropping the fields dropping",
    "start": "1212810",
    "end": "1215840"
  },
  {
    "text": "the fields in the last sentence I'm",
    "start": "1215840",
    "end": "1217400"
  },
  {
    "text": "actually taking the data frame and",
    "start": "1217400",
    "end": "1219530"
  },
  {
    "text": "dumping it into s3 creating a sink on",
    "start": "1219530",
    "end": "1222020"
  },
  {
    "text": "the left hand side you can see that blue",
    "start": "1222020",
    "end": "1223910"
  },
  {
    "text": "has generated a diagram of what my code",
    "start": "1223910",
    "end": "1226250"
  },
  {
    "text": "is doing now once the job is run",
    "start": "1226250",
    "end": "1228620"
  },
  {
    "text": "successfully done I can go back to my s3",
    "start": "1228620",
    "end": "1231340"
  },
  {
    "text": "console and look at look for the new",
    "start": "1231340",
    "end": "1234320"
  },
  {
    "text": "folder that has been created so here",
    "start": "1234320",
    "end": "1239840"
  },
  {
    "text": "under data I have my processed directory",
    "start": "1239840",
    "end": "1243440"
  },
  {
    "text": "that has been created next we will go to",
    "start": "1243440",
    "end": "1246280"
  },
  {
    "text": "AWS blue run the crawler again once the",
    "start": "1246280",
    "end": "1252260"
  },
  {
    "text": "crawler runs you will see that on the",
    "start": "1252260",
    "end": "1253670"
  },
  {
    "text": "right hand side",
    "start": "1253670",
    "end": "1254240"
  },
  {
    "text": "it adds two tables the two tables added",
    "start": "1254240",
    "end": "1257240"
  },
  {
    "text": "here are processed and raw so if you",
    "start": "1257240",
    "end": "1260390"
  },
  {
    "text": "look at the schema it is the schema that",
    "start": "1260390",
    "end": "1262580"
  },
  {
    "text": "we specified during the mappings now we",
    "start": "1262580",
    "end": "1265100"
  },
  {
    "text": "are in a Tina and in Athena I am I'm",
    "start": "1265100",
    "end": "1268940"
  },
  {
    "text": "specifying the query that I need to run",
    "start": "1268940",
    "end": "1270590"
  },
  {
    "text": "so in this case it is looking for actors",
    "start": "1270590",
    "end": "1272780"
  },
  {
    "text": "repos in the count of how many times a",
    "start": "1272780",
    "end": "1275000"
  },
  {
    "text": "particular actor has acted on the repo",
    "start": "1275000",
    "end": "1277870"
  },
  {
    "text": "yeah now let's go to quick site and",
    "start": "1277870",
    "end": "1281420"
  },
  {
    "text": "visualize this data that we have just",
    "start": "1281420",
    "end": "1284720"
  },
  {
    "text": "processed now we go to quick site and",
    "start": "1284720",
    "end": "1287270"
  },
  {
    "text": "create a new data set we will choose",
    "start": "1287270",
    "end": "1290060"
  },
  {
    "text": "Athena as as the source and put in the",
    "start": "1290060",
    "end": "1293480"
  },
  {
    "text": "required details here for simplicity we",
    "start": "1293480",
    "end": "1297260"
  },
  {
    "text": "will just choose one table here",
    "start": "1297260",
    "end": "1301360"
  },
  {
    "text": "so I'm choosing the process table and",
    "start": "1304110",
    "end": "1307169"
  },
  {
    "text": "I'm going to tell quick site to access",
    "start": "1307169",
    "end": "1309059"
  },
  {
    "text": "the data directly from Athena here for",
    "start": "1309059",
    "end": "1313710"
  },
  {
    "text": "simplicity I'm going to use only a tree",
    "start": "1313710",
    "end": "1315630"
  },
  {
    "text": "map and look at the repo so as you can",
    "start": "1315630",
    "end": "1318960"
  },
  {
    "text": "see 50% of the events in our data set up",
    "start": "1318960",
    "end": "1321750"
  },
  {
    "text": "push events followed by creating events",
    "start": "1321750",
    "end": "1324360"
  },
  {
    "text": "and so forth",
    "start": "1324360",
    "end": "1325640"
  },
  {
    "text": "so this was a quick demo of how you can",
    "start": "1325640",
    "end": "1328500"
  },
  {
    "text": "take raw data from github archive",
    "start": "1328500",
    "end": "1330419"
  },
  {
    "text": "website putting it in s3",
    "start": "1330419",
    "end": "1332159"
  },
  {
    "text": "discovering the data and cataloging it",
    "start": "1332159",
    "end": "1334529"
  },
  {
    "text": "using glue and then converting the data",
    "start": "1334529",
    "end": "1336809"
  },
  {
    "text": "to parque format so that you can",
    "start": "1336809",
    "end": "1338760"
  },
  {
    "text": "visualize the data using quick site and",
    "start": "1338760",
    "end": "1340889"
  },
  {
    "text": "Athena if you wish to recreate the steps",
    "start": "1340889",
    "end": "1343679"
  },
  {
    "start": "1342000",
    "end": "1364000"
  },
  {
    "text": "that we did during the demo you can",
    "start": "1343679",
    "end": "1345570"
  },
  {
    "text": "follow the URL or scan the QR code on",
    "start": "1345570",
    "end": "1347639"
  },
  {
    "text": "the screen it should give you the step",
    "start": "1347639",
    "end": "1350279"
  },
  {
    "text": "by step instructions on how to build out",
    "start": "1350279",
    "end": "1352320"
  },
  {
    "text": "the whole environment in your own AWS",
    "start": "1352320",
    "end": "1354690"
  },
  {
    "text": "account now I would like to invite",
    "start": "1354690",
    "end": "1357600"
  },
  {
    "text": "thumbs up from next electronics to share",
    "start": "1357600",
    "end": "1360269"
  },
  {
    "text": "that story and how did they build the",
    "start": "1360269",
    "end": "1362100"
  },
  {
    "text": "data like on AWS hello my name is Tom",
    "start": "1362100",
    "end": "1366000"
  },
  {
    "start": "1364000",
    "end": "1384000"
  },
  {
    "text": "Santana Ponyo I'm a group manager",
    "start": "1366000",
    "end": "1368309"
  },
  {
    "text": "content department of the data to show",
    "start": "1368309",
    "end": "1370620"
  },
  {
    "text": "next year after a trial and today I",
    "start": "1370620",
    "end": "1373350"
  },
  {
    "text": "would like to tell you about our journey",
    "start": "1373350",
    "end": "1375539"
  },
  {
    "text": "on building areas data Lake and how we",
    "start": "1375539",
    "end": "1378360"
  },
  {
    "text": "build our processing pipeline to process",
    "start": "1378360",
    "end": "1381059"
  },
  {
    "text": "our traffic information statistic data",
    "start": "1381059",
    "end": "1384169"
  },
  {
    "start": "1384000",
    "end": "1416000"
  },
  {
    "text": "before going to detail let me share a",
    "start": "1384169",
    "end": "1387149"
  },
  {
    "text": "bit more about our company and what we",
    "start": "1387149",
    "end": "1389340"
  },
  {
    "text": "do we have started content business in",
    "start": "1389340",
    "end": "1392960"
  },
  {
    "text": "2011 we have been collecting GPS data",
    "start": "1392960",
    "end": "1396769"
  },
  {
    "text": "incident data and well as sensor data",
    "start": "1396769",
    "end": "1399690"
  },
  {
    "text": "and providing the traffic information",
    "start": "1399690",
    "end": "1402330"
  },
  {
    "text": "service for automotive industry in",
    "start": "1402330",
    "end": "1404340"
  },
  {
    "text": "Thailand now we are expanding into data",
    "start": "1404340",
    "end": "1408210"
  },
  {
    "text": "analysis business and providing the",
    "start": "1408210",
    "end": "1410490"
  },
  {
    "text": "platform solution followed istic parking",
    "start": "1410490",
    "end": "1413789"
  },
  {
    "text": "and health care we are using GPS data as",
    "start": "1413789",
    "end": "1418019"
  },
  {
    "start": "1416000",
    "end": "1490000"
  },
  {
    "text": "the main data source which is collected",
    "start": "1418019",
    "end": "1420690"
  },
  {
    "text": "from about 140,000",
    "start": "1420690",
    "end": "1423269"
  },
  {
    "text": "WeHo using onboard electronics every day",
    "start": "1423269",
    "end": "1427110"
  },
  {
    "text": "on an average we are getting about 100",
    "start": "1427110",
    "end": "1430500"
  },
  {
    "text": "million data points which about 2-3",
    "start": "1430500",
    "end": "1434010"
  },
  {
    "text": "billion data per month",
    "start": "1434010",
    "end": "1435559"
  },
  {
    "text": "collected from",
    "start": "1435559",
    "end": "1437080"
  },
  {
    "text": "more than 100,000 lotlinx across",
    "start": "1437080",
    "end": "1439930"
  },
  {
    "text": "Thailand our challenge is to build the",
    "start": "1439930",
    "end": "1443770"
  },
  {
    "text": "model to predict the speed and traveling",
    "start": "1443770",
    "end": "1446620"
  },
  {
    "text": "time for our load link in Thailand in",
    "start": "1446620",
    "end": "1450340"
  },
  {
    "text": "the past we start out our GPS data on",
    "start": "1450340",
    "end": "1453790"
  },
  {
    "text": "fat fry storage system and went to do a",
    "start": "1453790",
    "end": "1457180"
  },
  {
    "text": "machine learning task we had work on",
    "start": "1457180",
    "end": "1459760"
  },
  {
    "text": "preparing the data for presentation Lord",
    "start": "1459760",
    "end": "1463240"
  },
  {
    "text": "our data and transformed to the format",
    "start": "1463240",
    "end": "1465970"
  },
  {
    "text": "decide by our client in the past",
    "start": "1465970",
    "end": "1469530"
  },
  {
    "text": "processing one man of data goodness had",
    "start": "1469530",
    "end": "1472990"
  },
  {
    "text": "to spend more than six hours doing",
    "start": "1472990",
    "end": "1475810"
  },
  {
    "text": "et al which was worst time consuming and",
    "start": "1475810",
    "end": "1479230"
  },
  {
    "text": "would be massive waste time for us this",
    "start": "1479230",
    "end": "1482950"
  },
  {
    "text": "is the listen why we decided to move to",
    "start": "1482950",
    "end": "1485500"
  },
  {
    "text": "a data Lake best modern data",
    "start": "1485500",
    "end": "1487630"
  },
  {
    "text": "architecture so here is how a new data",
    "start": "1487630",
    "end": "1492130"
  },
  {
    "start": "1490000",
    "end": "1607000"
  },
  {
    "text": "processing and data Lake architecture",
    "start": "1492130",
    "end": "1494110"
  },
  {
    "text": "look like GPS data is pushed from the",
    "start": "1494110",
    "end": "1498760"
  },
  {
    "text": "source system into the cloud using",
    "start": "1498760",
    "end": "1501220"
  },
  {
    "text": "Genesis stream using can assist here",
    "start": "1501220",
    "end": "1504430"
  },
  {
    "text": "make sure that we don't have to manage",
    "start": "1504430",
    "end": "1506830"
  },
  {
    "text": "our own streaming in France ruptures in",
    "start": "1506830",
    "end": "1510040"
  },
  {
    "text": "case of amount of car on the Lord glow",
    "start": "1510040",
    "end": "1512370"
  },
  {
    "text": "we will be able to scale easily with",
    "start": "1512370",
    "end": "1515590"
  },
  {
    "text": "kinases once the data land in Genesis we",
    "start": "1515590",
    "end": "1521080"
  },
  {
    "text": "have a set of the lambda function that",
    "start": "1521080",
    "end": "1523750"
  },
  {
    "text": "dump the data into s3 in our law data",
    "start": "1523750",
    "end": "1526900"
  },
  {
    "text": "less polished here we land grew color to",
    "start": "1526900",
    "end": "1531430"
  },
  {
    "text": "disco out data schema and then catalog",
    "start": "1531430",
    "end": "1535060"
  },
  {
    "text": "it include catalog we use through ETL",
    "start": "1535060",
    "end": "1538750"
  },
  {
    "text": "scrip to then convert the data in raw",
    "start": "1538750",
    "end": "1541840"
  },
  {
    "text": "data bucket in parquet format now we",
    "start": "1541840",
    "end": "1547360"
  },
  {
    "text": "have the data available in lead",
    "start": "1547360",
    "end": "1549520"
  },
  {
    "text": "optimized columnar data format for",
    "start": "1549520",
    "end": "1552120"
  },
  {
    "text": "consumption and further analysis further",
    "start": "1552120",
    "end": "1557500"
  },
  {
    "text": "down the processing pipeline we use LED",
    "start": "1557500",
    "end": "1560620"
  },
  {
    "text": "chip attina and quick size for data",
    "start": "1560620",
    "end": "1563800"
  },
  {
    "text": "exploration and analytics for the",
    "start": "1563800",
    "end": "1567550"
  },
  {
    "text": "machine learning job",
    "start": "1567550",
    "end": "1569560"
  },
  {
    "text": "consume the data directly from history",
    "start": "1569560",
    "end": "1572110"
  },
  {
    "text": "and let's ship spectrum into our spark",
    "start": "1572110",
    "end": "1576070"
  },
  {
    "text": "cluster here we use Mac machine learning",
    "start": "1576070",
    "end": "1579760"
  },
  {
    "text": "libraries to create traffic statistic",
    "start": "1579760",
    "end": "1582580"
  },
  {
    "text": "and prediction for Yotam pipeline we",
    "start": "1582580",
    "end": "1587200"
  },
  {
    "text": "have spark streaming application landing",
    "start": "1587200",
    "end": "1590110"
  },
  {
    "text": "that consume the data from canary stream",
    "start": "1590110",
    "end": "1592330"
  },
  {
    "text": "and system that engaged with kasama in",
    "start": "1592330",
    "end": "1595870"
  },
  {
    "text": "real time making data wearable for",
    "start": "1595870",
    "end": "1600340"
  },
  {
    "text": "analytics has become much easier since",
    "start": "1600340",
    "end": "1603910"
  },
  {
    "text": "we started implementing the better legs",
    "start": "1603910",
    "end": "1606340"
  },
  {
    "text": "architecture the architecture that I",
    "start": "1606340",
    "end": "1611740"
  },
  {
    "text": "showed you in the last life now in about",
    "start": "1611740",
    "end": "1614650"
  },
  {
    "text": "as analyte traffic incident predicting",
    "start": "1614650",
    "end": "1617410"
  },
  {
    "text": "parking requirement and predicting",
    "start": "1617410",
    "end": "1620080"
  },
  {
    "text": "traffic congestion across Thailand this",
    "start": "1620080",
    "end": "1623980"
  },
  {
    "text": "have had a really big impact and has",
    "start": "1623980",
    "end": "1626950"
  },
  {
    "text": "strengthened our audit analysis business",
    "start": "1626950",
    "end": "1629500"
  },
  {
    "text": "and how we serve our customer that's all",
    "start": "1629500",
    "end": "1634870"
  },
  {
    "text": "I want to share with you today hopefully",
    "start": "1634870",
    "end": "1637180"
  },
  {
    "text": "you found it useful thanks for watching",
    "start": "1637180",
    "end": "1640390"
  },
  {
    "text": "you have a nice day ahead I will hand",
    "start": "1640390",
    "end": "1643540"
  },
  {
    "text": "you over to uni again here is the link",
    "start": "1643540",
    "end": "1646660"
  },
  {
    "start": "1645000",
    "end": "1651000"
  },
  {
    "text": "for the demo you can recreate the demo",
    "start": "1646660",
    "end": "1648910"
  },
  {
    "text": "by following the instructions provided",
    "start": "1648910",
    "end": "1650350"
  },
  {
    "text": "in this guide finally I'd like to thank",
    "start": "1650350",
    "end": "1652990"
  },
  {
    "start": "1651000",
    "end": "1689000"
  },
  {
    "text": "you for attending this session today and",
    "start": "1652990",
    "end": "1654580"
  },
  {
    "text": "sticking it out to the very end of the",
    "start": "1654580",
    "end": "1656590"
  },
  {
    "text": "presentation to gain more confidence and",
    "start": "1656590",
    "end": "1659590"
  },
  {
    "text": "hands-on experience with AWS you can",
    "start": "1659590",
    "end": "1661840"
  },
  {
    "text": "access the digital and classroom",
    "start": "1661840",
    "end": "1663190"
  },
  {
    "text": "trainings built and delivered by a",
    "start": "1663190",
    "end": "1664870"
  },
  {
    "text": "double US exports AWS also has a wide",
    "start": "1664870",
    "end": "1668380"
  },
  {
    "text": "partner ecosystem to help you focus on",
    "start": "1668380",
    "end": "1670750"
  },
  {
    "text": "your success and take advantage of all",
    "start": "1670750",
    "end": "1673120"
  },
  {
    "text": "the business benefits that AWS has to",
    "start": "1673120",
    "end": "1675280"
  },
  {
    "text": "offer",
    "start": "1675280",
    "end": "1675610"
  },
  {
    "text": "to learn more about the AWS partner",
    "start": "1675610",
    "end": "1678070"
  },
  {
    "text": "ecosystem and find the right one that",
    "start": "1678070",
    "end": "1680200"
  },
  {
    "text": "fits your needs please visit the AWS",
    "start": "1680200",
    "end": "1682540"
  },
  {
    "text": "partner booth at our showcase thank you",
    "start": "1682540",
    "end": "1685930"
  },
  {
    "text": "very much",
    "start": "1685930",
    "end": "1688260"
  }
]