[
  {
    "text": "hi i'm zoe ma and today i'm here to talk about how to create a chrome prediction model with amazon sagemaker amazon stage maker",
    "start": "4560",
    "end": "12639"
  },
  {
    "text": "helps data scientists and developers to prepare build train and deploy machine learning",
    "start": "12639",
    "end": "17760"
  },
  {
    "text": "models quickly by bringing together a broad set of capabilities purpose built for machine",
    "start": "17760",
    "end": "23119"
  },
  {
    "text": "learning in this video we're going to show you how to use various stage maker functionalities",
    "start": "23119",
    "end": "28800"
  },
  {
    "text": "to build train and deploy a turn prediction model from end to end including data",
    "start": "28800",
    "end": "34399"
  },
  {
    "text": "pre-processing steps like ingestion cleaning and processing feature engineering training hyper",
    "start": "34399",
    "end": "41600"
  },
  {
    "text": "parameter toning evaluating your model and eventually deploy it to make predictions",
    "start": "41600",
    "end": "47760"
  },
  {
    "text": "we will perform these steps from stage maker studio which is a web-based visual interface",
    "start": "47760",
    "end": "53120"
  },
  {
    "text": "that gives you complete access control and visibility into each machine learning development step all in one place",
    "start": "53120",
    "end": "61520"
  },
  {
    "text": "for how to set up sagemaker studio notable environment please check the onboarding video linked",
    "start": "61520",
    "end": "66799"
  },
  {
    "text": "in the notebook so for this use case there are two part of the demo in part one prepared data you will",
    "start": "66799",
    "end": "74240"
  },
  {
    "text": "process the data with the help of data renderer then create features from the linked data",
    "start": "74240",
    "end": "80240"
  },
  {
    "text": "by the end of part one you will have a complete feature data set that contains all the attributes built",
    "start": "80240",
    "end": "85759"
  },
  {
    "text": "for each user and it is ready for modeling then in part 2",
    "start": "85759",
    "end": "90880"
  },
  {
    "text": "modeling and reference you will use the data set viewed from part 1 to find an optimal model from the use case",
    "start": "90880",
    "end": "97360"
  },
  {
    "text": "then check the model prediction results with the test set to start with part 2 you can either read",
    "start": "97360",
    "end": "104079"
  },
  {
    "text": "in data from the output of your part 1 results or use the provided full featureddataset.csv under data",
    "start": "104079",
    "end": "111759"
  },
  {
    "text": "folder as the input for the next steps in order to follow along this demo",
    "start": "111759",
    "end": "117439"
  },
  {
    "text": "you will need to clone the example repo and download the data sets",
    "start": "117439",
    "end": "122640"
  },
  {
    "text": "once you are in the stage maker studio console you can go ahead click on the git tab and click on the",
    "start": "122640",
    "end": "129920"
  },
  {
    "text": "clone repository i already have the repository clone here the repository contains the example",
    "start": "129920",
    "end": "137440"
  },
  {
    "text": "notebooks data sets a sample data flow for the data runner",
    "start": "137440",
    "end": "142720"
  },
  {
    "text": "and also a python file that contains the code for the search meter processing",
    "start": "142720",
    "end": "148160"
  },
  {
    "text": "in the data folder you will find the full raw data set in zip files",
    "start": "148160",
    "end": "153200"
  },
  {
    "text": "simulation 1 through 4 and a smaller size simple data for you to explore",
    "start": "153200",
    "end": "160160"
  },
  {
    "text": "two process data sets are also provided including a full feature data set which includes all the user level",
    "start": "160160",
    "end": "166400"
  },
  {
    "text": "features we created from the raw data and it is also the output from the part one prepared data",
    "start": "166400",
    "end": "173200"
  },
  {
    "text": "and also the splitted train validation and test data that are ready for modeling you can look",
    "start": "173200",
    "end": "179280"
  },
  {
    "text": "at the first data set to see the results of the part one data processing in feature engineering",
    "start": "179280",
    "end": "185040"
  },
  {
    "text": "and use the certain data set to experiment with modeling and habit writer tuning without running",
    "start": "185040",
    "end": "190720"
  },
  {
    "text": "the whole notebook now let's go back and open this notebook i already have it open here",
    "start": "190720",
    "end": "198800"
  },
  {
    "text": "in this example notebook we have a mixed up documentation and code in python",
    "start": "198800",
    "end": "204080"
  },
  {
    "text": "we will work through a pre-run version of this notebook so that we can see all the outputs at",
    "start": "204080",
    "end": "209440"
  },
  {
    "text": "once for this use case demo you will utilize the following stage meter capabilities",
    "start": "209440",
    "end": "214720"
  },
  {
    "text": "to simplify and speed your machine learning development",
    "start": "214720",
    "end": "219519"
  },
  {
    "text": "for data cleaning and pre-processing you will use sage meter data wrangler a feature that allows you to import",
    "start": "220720",
    "end": "227680"
  },
  {
    "text": "normalize transform combine data sources and also create features",
    "start": "227680",
    "end": "232720"
  },
  {
    "text": "without having to write any code while managing all of the processing infrastructure under the hood",
    "start": "232720",
    "end": "239760"
  },
  {
    "text": "for feature engineering you will use sagemaker processing a capability that lets you easily run",
    "start": "239760",
    "end": "245680"
  },
  {
    "text": "your pre-processing post-processing and even model evaluation workloads on fully managed",
    "start": "245680",
    "end": "251280"
  },
  {
    "text": "infrastructure you can also use siege maker data wrangler for feature engineering steps",
    "start": "251280",
    "end": "257199"
  },
  {
    "text": "but for this use case we decided to go with stage meter processing because our data is very raw and some intensive",
    "start": "257199",
    "end": "264400"
  },
  {
    "text": "feature creation heavy lifting is needed including aggregation from event level to user level",
    "start": "264400",
    "end": "270320"
  },
  {
    "text": "calculation of user listening behavior metrics and also a few functions used to add",
    "start": "270320",
    "end": "275680"
  },
  {
    "text": "time factor to the user behavior metrics such as advertisement listened by the user in 30 days 60",
    "start": "275680",
    "end": "282240"
  },
  {
    "text": "days and also 180 days next for model training you will use a",
    "start": "282240",
    "end": "288479"
  },
  {
    "text": "siege maker building algorithm called fgboost to predict if a customer is likely to charge",
    "start": "288479",
    "end": "294720"
  },
  {
    "text": "stitch maker building algorithms provide highly optimized implementation of popular machine learning algorithms",
    "start": "294720",
    "end": "300880"
  },
  {
    "text": "simplifying the machine learning development and also accelerating your training and development",
    "start": "300880",
    "end": "306639"
  },
  {
    "text": "for training and header parameter toning you will use the siege meter hyper parameter toning job",
    "start": "306639",
    "end": "312080"
  },
  {
    "text": "it is a sage metro feature that automates the hyper priority toning process and also eliminates the undifferentiated",
    "start": "312080",
    "end": "319600"
  },
  {
    "text": "heavy lifting of searching through numerous combinations of hyper parameters",
    "start": "319600",
    "end": "324880"
  },
  {
    "text": "it launches multiple training jobs with different type of parameter combinations then trains a meta machine learning",
    "start": "324880",
    "end": "331600"
  },
  {
    "text": "model based on bayesian optimization to find the best model",
    "start": "331600",
    "end": "336720"
  },
  {
    "text": "you can create a tournament job use the siege maker console the low level stage mentor api or the",
    "start": "336720",
    "end": "341919"
  },
  {
    "text": "stage maker python sdk for deploying the model you will use the",
    "start": "341919",
    "end": "347039"
  },
  {
    "text": "siege maker batch transform a stage motor feature to transform data and also generates inferences",
    "start": "347039",
    "end": "353840"
  },
  {
    "text": "it is ideal for scenarios where you're dealing with large batches of data donate subsetting latency or just need",
    "start": "353840",
    "end": "361840"
  },
  {
    "text": "for both pre-process and also transform the training data you can create a batch transform job",
    "start": "361840",
    "end": "368000"
  },
  {
    "text": "from the sage meter console using the sagemaker high level python library or the stage meter sdk",
    "start": "368000",
    "end": "374720"
  },
  {
    "text": "lastly for model explainability you will use the siege motor clarify a new usage maker feature that provides",
    "start": "374720",
    "end": "381600"
  },
  {
    "text": "bias detection across the machine learning workflow and also feature importance for your model",
    "start": "381600",
    "end": "387039"
  },
  {
    "text": "in this use case you will use it to tell how each different feature inputted into the model",
    "start": "387039",
    "end": "392240"
  },
  {
    "text": "is affecting predictions so let's dive into this touring space",
    "start": "392240",
    "end": "397680"
  },
  {
    "text": "what is customer chore and why is it important for business customer churn or customer attrition",
    "start": "397680",
    "end": "404240"
  },
  {
    "text": "means that a customer has the tendency to leave and also stop paying for a business",
    "start": "404240",
    "end": "409759"
  },
  {
    "text": "it is one of the primary metrics companies uses as a health indicator for the business",
    "start": "409759",
    "end": "415360"
  },
  {
    "text": "and also get a sense of their customer satisfaction some use case for customer choice",
    "start": "415360",
    "end": "420479"
  },
  {
    "text": "includes music video streaming services software as service providers",
    "start": "420479",
    "end": "425599"
  },
  {
    "text": "and also telecom companies then how do we formulate a machine learning problem",
    "start": "425599",
    "end": "430800"
  },
  {
    "text": "for trump prediction you can start with the most",
    "start": "430800",
    "end": "436080"
  },
  {
    "text": "straightforward way and predict whether a user will cancel or if you want to detect early signs of",
    "start": "436080",
    "end": "442080"
  },
  {
    "text": "living you can predict whether a user will downgrade the pricing plan or renew a subscription you can either",
    "start": "442080",
    "end": "448800"
  },
  {
    "text": "formulate it as a classification problem as above or time series to predict when we will use a chord",
    "start": "448800",
    "end": "457280"
  },
  {
    "text": "to collect data for trim prediction some most common data sources are customer relationship management",
    "start": "459599",
    "end": "465680"
  },
  {
    "text": "platform like crm engagement and usage data this is usually event level log data",
    "start": "465680",
    "end": "472400"
  },
  {
    "text": "so you might want to aggregate data to a student level to use it and this is also the same case with the",
    "start": "472400",
    "end": "478000"
  },
  {
    "text": "data set we're using in this demo feedback data and that includes customer support",
    "start": "478000",
    "end": "484720"
  },
  {
    "text": "requests feedback on social media or in app store these are sometimes",
    "start": "484720",
    "end": "490080"
  },
  {
    "text": "tested data so some level of natural language processing is needed",
    "start": "490080",
    "end": "496160"
  },
  {
    "text": "some challenges with customer chores include imbalanced class where fewer torn data is available which",
    "start": "498479",
    "end": "504479"
  },
  {
    "text": "we will discuss for this use case example another one is user identification",
    "start": "504479",
    "end": "509680"
  },
  {
    "text": "mapping problem which means that if you're joining data from different platforms you would want to make sure that user a",
    "start": "509680",
    "end": "516479"
  },
  {
    "text": "is always identified as the same user across multiple platforms now let's",
    "start": "516479",
    "end": "521760"
  },
  {
    "text": "start our use case work through for this use case we are going to use the simulated music streaming event",
    "start": "521760",
    "end": "528399"
  },
  {
    "text": "level usage data and transform the data set into a feature data set that we can use to predict if a customer",
    "start": "528399",
    "end": "535600"
  },
  {
    "text": "will cancel the subscription this data set contains about 1100 users",
    "start": "535600",
    "end": "541360"
  },
  {
    "text": "in their user behavior for about one year each row in the data set contains one event and has information",
    "start": "541360",
    "end": "548560"
  },
  {
    "text": "about the timestamp auto event user page name some demographic information song",
    "start": "548560",
    "end": "555120"
  },
  {
    "text": "attributes and also session information",
    "start": "555120",
    "end": "562080"
  },
  {
    "text": "architecture diagram is also included here to give you a sense of the flow for this demo we're going to",
    "start": "562080",
    "end": "568720"
  },
  {
    "text": "ingest the data from s3 budget and to notebook for exploration then",
    "start": "568720",
    "end": "575120"
  },
  {
    "text": "using data writer to do process pre-processing the data from state regular will be",
    "start": "575120",
    "end": "580560"
  },
  {
    "text": "ingested into processing for future engineering and then we're going to ingest",
    "start": "580560",
    "end": "586560"
  },
  {
    "text": "the feature set to the extreme boost model for modeling and we will use habit parameter toning",
    "start": "586560",
    "end": "592959"
  },
  {
    "text": "job to find the optimal model and eventually we will use the batch transform to inference the output",
    "start": "592959",
    "end": "600640"
  },
  {
    "text": "and find the prediction results all the results will be saved in an amazon s3 bucket we'll also use",
    "start": "600640",
    "end": "607279"
  },
  {
    "text": "amazon sagemaker clarify to find out which features are most influential to the model prediction",
    "start": "607279",
    "end": "613040"
  },
  {
    "text": "results now let's start our part one prepared",
    "start": "613040",
    "end": "618079"
  },
  {
    "text": "data we'll first need to set up the notebook pretty standard process for a lot of",
    "start": "618079",
    "end": "624480"
  },
  {
    "text": "sphere notebook and set up parameters which we're going to use",
    "start": "624480",
    "end": "630160"
  },
  {
    "text": "in this notebook and that including a bucket name prefix region row used",
    "start": "630160",
    "end": "637519"
  },
  {
    "text": "for execute a lot of commands in this notebook and also as a statement client",
    "start": "637519",
    "end": "647120"
  },
  {
    "text": "we are going to download the data from github and also upload them to amazon s3 then",
    "start": "647120",
    "end": "654000"
  },
  {
    "text": "we will start the data cleaning in this exploration by the first look at the data",
    "start": "654000",
    "end": "662399"
  },
  {
    "text": "we are going to remove some irrelevant columns first and that includes methods and also",
    "start": "662800",
    "end": "672320"
  },
  {
    "text": "status next we will look at null values for each column",
    "start": "672839",
    "end": "679120"
  },
  {
    "text": "and we're going to remove all events without an user id assigned since that's how we will identify a user",
    "start": "679120",
    "end": "685760"
  },
  {
    "text": "for our use case in this case about three percent of users do not have a registration time",
    "start": "685760",
    "end": "693200"
  },
  {
    "text": "so we will remove all those anonymous users from the record as well",
    "start": "693200",
    "end": "702000"
  },
  {
    "text": "now let's take a closer look at the data",
    "start": "702000",
    "end": "706399"
  },
  {
    "text": "here we're listing unit values or number of unique values for each categorical column and some",
    "start": "709279",
    "end": "716160"
  },
  {
    "text": "take observations from the data are first the location and user agent",
    "start": "716160",
    "end": "722560"
  },
  {
    "text": "are not great categorical features as they contain too many unique values and when you use",
    "start": "722560",
    "end": "728320"
  },
  {
    "text": "onecall encoding to convert all those two features they're going to become too many columns and bring little",
    "start": "728320",
    "end": "734320"
  },
  {
    "text": "value to your model so we're going to remove them artists and songs",
    "start": "734320",
    "end": "739600"
  },
  {
    "text": "those can be quantified to a user level that means features like how many songs that the user has",
    "start": "739600",
    "end": "745040"
  },
  {
    "text": "listened to or how many artists the user is following for the column page cancellation",
    "start": "745040",
    "end": "751839"
  },
  {
    "text": "confirmation can be used to label user chord and other information is pretty valuable as",
    "start": "751839",
    "end": "757519"
  },
  {
    "text": "well like downgrade upgrade this can all be quantified and aggregated to the user",
    "start": "757519",
    "end": "763200"
  },
  {
    "text": "level as well we can also extract date features from the timestamp so that we can build some timely",
    "start": "763200",
    "end": "770839"
  },
  {
    "text": "features",
    "start": "770839",
    "end": "773839"
  },
  {
    "text": "one thing that is kind of worth noting is that the imbalance class",
    "start": "777120",
    "end": "782320"
  },
  {
    "text": "it can be misleading for some machine learning models as the accuracy will be biased",
    "start": "782320",
    "end": "787360"
  },
  {
    "text": "towards the majority class and in this case you have have about 12",
    "start": "787360",
    "end": "792639"
  },
  {
    "text": "to 13 percent torn users so you can either sample your data or choose an algorithm that is less",
    "start": "792639",
    "end": "799040"
  },
  {
    "text": "sensitive to imbalanced class and choose appropriate evaluation metrics which is what we will do in this",
    "start": "799040",
    "end": "804639"
  },
  {
    "text": "use case to summarize every pre-processing steps",
    "start": "804639",
    "end": "810399"
  },
  {
    "text": "you have covered we have done no removals drop irrelevant columns",
    "start": "810399",
    "end": "815440"
  },
  {
    "text": "convert event timestamps to features and we also create labels now that you've",
    "start": "815440",
    "end": "821279"
  },
  {
    "text": "finished cleaning and processing the data let's start exploring the data a little bit further more so that we can decide",
    "start": "821279",
    "end": "828000"
  },
  {
    "text": "which features to create in future engineering some possible directions to explore are",
    "start": "828000",
    "end": "834399"
  },
  {
    "text": "listed here and we're going to explore three of them we will start with",
    "start": "834399",
    "end": "839680"
  },
  {
    "text": "activity time the following two plots are generated",
    "start": "839680",
    "end": "846240"
  },
  {
    "text": "with the code and under python library matplotlib and sigborn",
    "start": "846240",
    "end": "853680"
  },
  {
    "text": "the first plot shows that the distribution of the average number of events per user",
    "start": "854800",
    "end": "861120"
  },
  {
    "text": "per day in breaking down by charm users and active users and also by weekends and weekdays",
    "start": "861120",
    "end": "869120"
  },
  {
    "text": "you can see that torn users are more active on weekdays than weekends whereas active users do",
    "start": "869120",
    "end": "875600"
  },
  {
    "text": "not show a strong difference between the two that means average events per day on",
    "start": "875600",
    "end": "881680"
  },
  {
    "text": "weekends and also average events per day on weekdays can be two useful features here",
    "start": "881680",
    "end": "888000"
  },
  {
    "text": "the second plot however does not really show significant differences between day and night for both sets of",
    "start": "888240",
    "end": "894399"
  },
  {
    "text": "users so we'll skip it as a feature",
    "start": "894399",
    "end": "900560"
  },
  {
    "text": "you can also look at some basic statistics for users listening habits",
    "start": "900880",
    "end": "907040"
  },
  {
    "text": "we are looking at average total that includes number of sessions app usage length number of songs",
    "start": "907839",
    "end": "914480"
  },
  {
    "text": "listened number of artist listen per user number of advertisement is active and we're also going to look at the same",
    "start": "914480",
    "end": "921199"
  },
  {
    "text": "metrics on a daily level",
    "start": "921199",
    "end": "927279"
  },
  {
    "text": "in this case torn users are actually heavier users from the two sets of data we can look at",
    "start": "927279",
    "end": "934399"
  },
  {
    "text": "they listen to a wider variety of songs and also artists and spending more time",
    "start": "934399",
    "end": "939680"
  },
  {
    "text": "on app you can further explore how the users",
    "start": "939680",
    "end": "945600"
  },
  {
    "text": "are using the app besides just listening by looking at the number of likes and dislikes",
    "start": "945600",
    "end": "951279"
  },
  {
    "text": "number of songs added to a playlist advertisement adding friends if a user has downloaded",
    "start": "951279",
    "end": "957600"
  },
  {
    "text": "or upgraded their plan and how many errors the user has encountered trump users are slightly more active",
    "start": "957600",
    "end": "965040"
  },
  {
    "text": "than all the other users from our data and that means that there are 800 more",
    "start": "965040",
    "end": "971440"
  },
  {
    "text": "errors listening to more advertisement and had more downgrades and upgrades",
    "start": "971440",
    "end": "977279"
  },
  {
    "text": "this statistics can be built as numerical features such as number of total events per type",
    "start": "977279",
    "end": "984000"
  },
  {
    "text": "per user or more advanced time series numerical features such as errors in the last seven days",
    "start": "984000",
    "end": "992160"
  },
  {
    "text": "now that you have a good understanding of your data and decide which steps are needed for process your data you can utilize",
    "start": "992160",
    "end": "999199"
  },
  {
    "text": "the new amazon stage meter data regular feature to prepare your data for machine learning",
    "start": "999199",
    "end": "1005279"
  },
  {
    "text": "you can open a new launcher and from the launcher you can click on",
    "start": "1005279",
    "end": "1011360"
  },
  {
    "text": "new data flow to start to create your data renderer job",
    "start": "1011360",
    "end": "1018000"
  },
  {
    "text": "you can choose to select data from amazon s3 amazon athena red shift lake formation and amazon city",
    "start": "1018480",
    "end": "1024959"
  },
  {
    "text": "maker feature store i have already uploaded the csv version of the full data set into s3",
    "start": "1024959",
    "end": "1030880"
  },
  {
    "text": "so we're going to import a full data set from the s3 location choose amazon s3 choose our default",
    "start": "1030880",
    "end": "1038079"
  },
  {
    "text": "stitch meter budget and under music streaming",
    "start": "1038079",
    "end": "1043678"
  },
  {
    "text": "and data csv we have the full data set",
    "start": "1043679",
    "end": "1048799"
  },
  {
    "text": "click on import data set now that you have completed the first step in data flow you can proceed to",
    "start": "1048799",
    "end": "1055520"
  },
  {
    "text": "transform and add transformations by clicking on the plus sign and add",
    "start": "1055520",
    "end": "1061120"
  },
  {
    "text": "transformation with over 300 building data transformations you can transform data",
    "start": "1061120",
    "end": "1066320"
  },
  {
    "text": "including handling missing values outliers managing columns and even adding custom",
    "start": "1066320",
    "end": "1071679"
  },
  {
    "text": "data transformations i already created a list of transformations here",
    "start": "1071679",
    "end": "1077280"
  },
  {
    "text": "in my flow file and that including removes a few columns",
    "start": "1077280",
    "end": "1084720"
  },
  {
    "text": "managing columns handling music values and change the format of the data",
    "start": "1084720",
    "end": "1090000"
  },
  {
    "text": "when we're done with the data transformations we would like to save the output data somewhere",
    "start": "1090000",
    "end": "1095039"
  },
  {
    "text": "for easy access so by choosing export and",
    "start": "1095039",
    "end": "1101280"
  },
  {
    "text": "by choosing all these steps you want to export by click on export steps and by choosing",
    "start": "1101280",
    "end": "1107919"
  },
  {
    "text": "that a regular job sage maker data wrangler will create a notebook for you",
    "start": "1107919",
    "end": "1114160"
  },
  {
    "text": "and by running this notebook",
    "start": "1114880",
    "end": "1123840"
  },
  {
    "text": "it creates a job and also creates the output in an s3 bucket",
    "start": "1124160",
    "end": "1130240"
  },
  {
    "text": "between the renderer integrated nicely with all the other siege meter services",
    "start": "1130720",
    "end": "1136080"
  },
  {
    "text": "in the export you can also export it as a pipeline or to feature store",
    "start": "1136080",
    "end": "1142480"
  },
  {
    "text": "which is another new switch meter feature that allows you better manage your mobile features",
    "start": "1142480",
    "end": "1150160"
  },
  {
    "text": "remember to save this output path so that you can read the output in the chrome prediction use this",
    "start": "1150160",
    "end": "1155679"
  },
  {
    "text": "notebook so we will run through this notebook",
    "start": "1155679",
    "end": "1161440"
  },
  {
    "text": "and go back to our use case now that your data is prepared using",
    "start": "1162880",
    "end": "1168240"
  },
  {
    "text": "siege meter data regular it is ready for feature engineering and we're going to use sage maker processing",
    "start": "1168240",
    "end": "1174720"
  },
  {
    "text": "to speed up the process since the data set is time series you can enrich your features by",
    "start": "1174720",
    "end": "1181280"
  },
  {
    "text": "adding a time factor to it for example for the total number of songs listened youtube created features",
    "start": "1181280",
    "end": "1187760"
  },
  {
    "text": "like total songs listened by the user in the last 7 days 30 days 90 days 180 days and so on",
    "start": "1187760",
    "end": "1194960"
  },
  {
    "text": "to see how active this user has been in a timeline the feature speed for this",
    "start": "1194960",
    "end": "1201600"
  },
  {
    "text": "use case will be at user level where each row represents one user",
    "start": "1201600",
    "end": "1206960"
  },
  {
    "text": "and will include the following features",
    "start": "1206960",
    "end": "1210799"
  },
  {
    "text": "the average number of events per day during the weekend and during the weekday",
    "start": "1216400",
    "end": "1221840"
  },
  {
    "text": "number of advertisement and total errors encountered in the last 7 days total songs played in the last 7 days 30",
    "start": "1221840",
    "end": "1229360"
  },
  {
    "text": "days 90 days number of artists and songs played number of advertisement played",
    "start": "1229360",
    "end": "1234400"
  },
  {
    "text": "encountered error number of times the user likes and dislikes the sound add the song to a playlist and as a",
    "start": "1234400",
    "end": "1240799"
  },
  {
    "text": "friend user has downgraded or upgraded plan percentage of users action is next song",
    "start": "1240799",
    "end": "1249760"
  },
  {
    "text": "or raw advert which means advertisement percentage of total songs that are",
    "start": "1249760",
    "end": "1256320"
  },
  {
    "text": "repeats meaning that if the user is a repeat listener days since the user being active number",
    "start": "1256320",
    "end": "1263760"
  },
  {
    "text": "of total sessions and also average time spent an average time number of events per session",
    "start": "1263760",
    "end": "1270480"
  },
  {
    "text": "and also average time between each session so between two sessions how long it",
    "start": "1270480",
    "end": "1276320"
  },
  {
    "text": "takes for the user to return to the app so the following function will create",
    "start": "1276320",
    "end": "1283360"
  },
  {
    "text": "and also run your sklearn job using the sagemaker processing sdk",
    "start": "1283360",
    "end": "1289200"
  },
  {
    "text": "your pre-processing script will be wrong like the following steps the data set is",
    "start": "1289200",
    "end": "1295120"
  },
  {
    "text": "automatically copied inside of the container under the destination directory",
    "start": "1295120",
    "end": "1301280"
  },
  {
    "text": "which in this case is under the slash input this is where the python",
    "start": "1301280",
    "end": "1307360"
  },
  {
    "text": "script reads it which is the preprocessing.py",
    "start": "1307360",
    "end": "1313679"
  },
  {
    "text": "optionally we could also pass command line arguments to the script here since the job will take the output from",
    "start": "1313679",
    "end": "1320559"
  },
  {
    "text": "the siege meter directly job above as the input you will see the data monitor output",
    "start": "1320559",
    "end": "1325840"
  },
  {
    "text": "name and pass it as an argument to the stage meter processing job",
    "start": "1325840",
    "end": "1332000"
  },
  {
    "text": "a process it and save the file inside the container under the opt ml processing slash",
    "start": "1332000",
    "end": "1339280"
  },
  {
    "text": "output once the job completes all the outputs are automatically copied",
    "start": "1339280",
    "end": "1345120"
  },
  {
    "text": "to your default stage meter budget in s3 this is the processing.py file",
    "start": "1345120",
    "end": "1352559"
  },
  {
    "text": "and it includes a lot of feature creation steps we just mentioned in the exploration steps",
    "start": "1352559",
    "end": "1360080"
  },
  {
    "text": "we're creating a bunch of features and finally we're combining all the",
    "start": "1360080",
    "end": "1366080"
  },
  {
    "text": "features together we're renaming some of the columns and eventually we're only keeping the",
    "start": "1366080",
    "end": "1372240"
  },
  {
    "text": "created features columns",
    "start": "1372240",
    "end": "1375840"
  },
  {
    "text": "then after this function starts to run it will takes about five minutes once it finishes you can find the output",
    "start": "1377280",
    "end": "1384799"
  },
  {
    "text": "processing under stored job on the scroll output csv in the siege major processing job folder",
    "start": "1384799",
    "end": "1390400"
  },
  {
    "text": "in your default s3 budget great congratulations you have completed",
    "start": "1390400",
    "end": "1396480"
  },
  {
    "text": "the part one prepare the data and now you should have created the complete feature set that is ready",
    "start": "1396480",
    "end": "1403440"
  },
  {
    "text": "for modeling you can now proceed to part 2 modeling and reference",
    "start": "1403440",
    "end": "1409360"
  },
  {
    "text": "now that you have created the complete feature set you can start to explore and find the",
    "start": "1409360",
    "end": "1415280"
  },
  {
    "text": "best working model for your training use case by the end of part 2 you should have selected an algorithm",
    "start": "1415280",
    "end": "1422159"
  },
  {
    "text": "find the best sets of hyper parameters for the model examine how well the model performs and",
    "start": "1422159",
    "end": "1428240"
  },
  {
    "text": "finally find top influential features as an action item to start with part two you can either",
    "start": "1428240",
    "end": "1436400"
  },
  {
    "text": "reading data from the output of your part one results which is the processing on the store job",
    "start": "1436400",
    "end": "1442640"
  },
  {
    "text": "unless your output.csv or use the provided data slash",
    "start": "1442640",
    "end": "1448159"
  },
  {
    "text": "full underscore feature underscore data.csv as the input which is the variable data",
    "start": "1448159",
    "end": "1454159"
  },
  {
    "text": "frame processed data for the next step which is also the last step",
    "start": "1454159",
    "end": "1459360"
  },
  {
    "text": "before fitting data into the model and that is splitting data into train validation and test",
    "start": "1459360",
    "end": "1467360"
  },
  {
    "text": "here we are splitting data in a way that data of both classes exist in all three data sets and making",
    "start": "1467679",
    "end": "1475440"
  },
  {
    "text": "sure that both classes are represented in all data sets audit split data sets are also provided",
    "start": "1475440",
    "end": "1482640"
  },
  {
    "text": "in the data ripple which we just mentioned the trained updated csv validation",
    "start": "1482640",
    "end": "1488320"
  },
  {
    "text": "updated csv and also test updated csv so you can also use them as input to",
    "start": "1488320",
    "end": "1494159"
  },
  {
    "text": "experiment with your own models now here comes the exciting part our",
    "start": "1494159",
    "end": "1499279"
  },
  {
    "text": "data is finally ready to be trained before choosing an algorithm you should always start with a simple one",
    "start": "1499279",
    "end": "1506000"
  },
  {
    "text": "think about data structure and interpretability you would also want to utilize the siege",
    "start": "1506000",
    "end": "1511440"
  },
  {
    "text": "major building algorithms to simplify and accelerate your training process",
    "start": "1511440",
    "end": "1516799"
  },
  {
    "text": "for complete list of available sage maker building algorithm you can always access examples notebooks",
    "start": "1516799",
    "end": "1522720"
  },
  {
    "text": "and documentations from the sagemaker jumpstart page by open a new launcher clicking on the",
    "start": "1522720",
    "end": "1529520"
  },
  {
    "text": "plus sign click on jumpstart it will bring you to",
    "start": "1529520",
    "end": "1535360"
  },
  {
    "text": "the job start page where you can find some solutions for different use cases",
    "start": "1535360",
    "end": "1540880"
  },
  {
    "text": "and you can find text models communication models and finalize stage material algorithms",
    "start": "1540880",
    "end": "1548960"
  },
  {
    "text": "you can click on any algorithms that you're interested in then it will bring you to this example",
    "start": "1549039",
    "end": "1555039"
  },
  {
    "text": "notebook showcasing how the algorithm works now let's go back to our use case",
    "start": "1555039",
    "end": "1562960"
  },
  {
    "text": "in this use case we will go within tray-based model f3 boost due to the consideration of",
    "start": "1563840",
    "end": "1569440"
  },
  {
    "text": "imbalanced class and in the family of three based models achievables usually provides the best results as it",
    "start": "1569440",
    "end": "1576480"
  },
  {
    "text": "is built and optimized for performances and speed it is also one of the sage maker",
    "start": "1576480",
    "end": "1581520"
  },
  {
    "text": "building algorithms so we're just going to utilize it in this use case",
    "start": "1581520",
    "end": "1586720"
  },
  {
    "text": "to configure a training job you will need to choose evaluation metrics and also hyper parameters here you can",
    "start": "1586720",
    "end": "1593760"
  },
  {
    "text": "initialize the training job from the notebook using the statement python sdk and you",
    "start": "1593760",
    "end": "1598880"
  },
  {
    "text": "can do the same thing from the console from an imbalanced classification problem you can choose f1 score as your",
    "start": "1598880",
    "end": "1605600"
  },
  {
    "text": "evaluation especially for comparing different models area under curve which is euc",
    "start": "1605600",
    "end": "1611760"
  },
  {
    "text": "is also a good choice when your model output is probability you can also define a job as a trial",
    "start": "1611760",
    "end": "1618159"
  },
  {
    "text": "with stage matrix experiments and you can easily check for analytics same as training job",
    "start": "1618159",
    "end": "1624320"
  },
  {
    "text": "you can create a trial and experiment from the studio notebook using python sdk like what we just did",
    "start": "1624320",
    "end": "1630640"
  },
  {
    "text": "or you can just create an experiment from the sagemaker's video jewelry tool by choosing the components",
    "start": "1630640",
    "end": "1638480"
  },
  {
    "text": "then choose experiment and trials",
    "start": "1639279",
    "end": "1644480"
  },
  {
    "text": "then click on experiment by clicking on this create experiment it",
    "start": "1644640",
    "end": "1650320"
  },
  {
    "text": "will bring you to this page where you just need to configure a few things",
    "start": "1650320",
    "end": "1655840"
  },
  {
    "text": "and you can create an experiment from here without writing a code in this use case we will drop this",
    "start": "1656320",
    "end": "1664080"
  },
  {
    "text": "python sdk and we'll show you later how to do it",
    "start": "1664080",
    "end": "1669840"
  },
  {
    "text": "here to create a training job we will first need to get the ecr image for previewed stage maker doctor",
    "start": "1670799",
    "end": "1677440"
  },
  {
    "text": "images once you have that we will initialize the model hyperparameters",
    "start": "1677440",
    "end": "1684159"
  },
  {
    "text": "with a bunch of default values we choose the euc as the evaluation",
    "start": "1684159",
    "end": "1689760"
  },
  {
    "text": "matrix here",
    "start": "1689760",
    "end": "1692559"
  },
  {
    "text": "and we will define the stage maker estimator with all the hepa parameters which is defined",
    "start": "1694799",
    "end": "1700000"
  },
  {
    "text": "and also the input path after we import all the training input",
    "start": "1700000",
    "end": "1707279"
  },
  {
    "text": "into our estimator it will start to fit in",
    "start": "1707279",
    "end": "1712158"
  },
  {
    "text": "and eventually you can define a stitch meter experiment and trial",
    "start": "1713279",
    "end": "1719519"
  },
  {
    "text": "by using the stage maker python sdk",
    "start": "1721200",
    "end": "1725840"
  },
  {
    "text": "this is what the analytics table will look like if you have multiple models you can",
    "start": "1728480",
    "end": "1735279"
  },
  {
    "text": "easily compare different evaluation metrics value here and simply choose the best one",
    "start": "1735279",
    "end": "1742720"
  },
  {
    "text": "great now that you understand how training one model works and how to create a sage maker",
    "start": "1747200",
    "end": "1752240"
  },
  {
    "text": "experiment and also you select the actual boost model as the final model",
    "start": "1752240",
    "end": "1757279"
  },
  {
    "text": "you will need to fine tune the hyper parameters for the best model performances",
    "start": "1757279",
    "end": "1763279"
  },
  {
    "text": "for the actual boost model when we specifying the half and right turning",
    "start": "1763360",
    "end": "1768480"
  },
  {
    "text": "ranges in the config file we can start with the",
    "start": "1768480",
    "end": "1773760"
  },
  {
    "text": "eta alpha mean chart weight and max depth there are a bunch of more other",
    "start": "1773760",
    "end": "1779919"
  },
  {
    "text": "parameters you can tell and you can look at the hyperproprietor tuning job documentations for more",
    "start": "1779919",
    "end": "1785840"
  },
  {
    "text": "information here for this strategy you can either choose a random search",
    "start": "1785840",
    "end": "1791679"
  },
  {
    "text": "where have a pointer turning job choose a random combination of values from within the ranges that you specify",
    "start": "1791679",
    "end": "1798559"
  },
  {
    "text": "or you can choose bayesian search where it kind of treats the hyper-provider toning like a regression problem where it",
    "start": "1798559",
    "end": "1805360"
  },
  {
    "text": "optimizes a model for the metric that you choose to solve for the regression problem",
    "start": "1805360",
    "end": "1811120"
  },
  {
    "text": "hyperparameter toning makes guesses about which hyperparameter combinations are likely to get the best results and",
    "start": "1811120",
    "end": "1818559"
  },
  {
    "text": "ron's training job to test those values here we can go with bayesian search",
    "start": "1818559",
    "end": "1824240"
  },
  {
    "text": "you will also need to configure the training job that the toning job launches and by defining this json object",
    "start": "1824240",
    "end": "1833120"
  },
  {
    "text": "you're essentially defining input output path format",
    "start": "1833120",
    "end": "1839440"
  },
  {
    "text": "and finally we're launching the job we're launching this power pointer toning job by using the python sdk",
    "start": "1840080",
    "end": "1847279"
  },
  {
    "text": "but you can also create a hyperline toning job from the siege meter console",
    "start": "1847279",
    "end": "1852960"
  },
  {
    "text": "once the job launches you can always monitor the progress of the hyper provider telling job from the console as",
    "start": "1852960",
    "end": "1858320"
  },
  {
    "text": "well by going to the console and already choose my hepa prime attorney job",
    "start": "1858320",
    "end": "1865519"
  },
  {
    "text": "on their training jobs you can always look for how many jobs are completed how many are",
    "start": "1865519",
    "end": "1872399"
  },
  {
    "text": "in progress and stopped and failed and you can check for the matrix value for each job and",
    "start": "1872399",
    "end": "1877600"
  },
  {
    "text": "comparing them once the job is finished under the best training job",
    "start": "1877600",
    "end": "1883279"
  },
  {
    "text": "you can find your best job and even directly create a model from here",
    "start": "1883279",
    "end": "1889440"
  },
  {
    "text": "now let's go back to our notebook the training job itself will take about",
    "start": "1890080",
    "end": "1895600"
  },
  {
    "text": "20 minutes after it's done you can check the best model and deploy it with batch transform you can deploy the",
    "start": "1895600",
    "end": "1902799"
  },
  {
    "text": "model directly from your hyperparameter toning job and once deployed you will take off the",
    "start": "1902799",
    "end": "1907919"
  },
  {
    "text": "batch transform job here with the sigmatry python sdk",
    "start": "1907919",
    "end": "1913760"
  },
  {
    "text": "once the job is finished we will deploy the best model",
    "start": "1914240",
    "end": "1920960"
  },
  {
    "text": "from our training job",
    "start": "1920960",
    "end": "1924000"
  },
  {
    "text": "and also tell the batch transform job where our test data is",
    "start": "1927200",
    "end": "1932559"
  },
  {
    "text": "which in this case is test.csv and then we will call the transformer",
    "start": "1932559",
    "end": "1938000"
  },
  {
    "text": "which is the batch transform sagemaker python sdk",
    "start": "1938000",
    "end": "1948399"
  },
  {
    "text": "and it's running once done you can download the model",
    "start": "1948399",
    "end": "1953679"
  },
  {
    "text": "results to your instance for further analysis let's compare our model results to actual with the test set we left",
    "start": "1953679",
    "end": "1960080"
  },
  {
    "text": "out earlier we're going to look at the f1 score precision and recall in true analysis",
    "start": "1960080",
    "end": "1968240"
  },
  {
    "text": "precision means that of the user predicted to be drawn how many actually in return means that",
    "start": "1968240",
    "end": "1976000"
  },
  {
    "text": "what percentage of torn users the model is successfully fine",
    "start": "1976000",
    "end": "1981760"
  },
  {
    "text": "from the results we can see that the best model has a good iphone score and it can also detect about 77 percent",
    "start": "1982320",
    "end": "1989600"
  },
  {
    "text": "of the tron users before they leave lastly you can visualize feature importance",
    "start": "1989600",
    "end": "1995440"
  },
  {
    "text": "with the new switch maker capability siege maker clarify which provides greater visibility into",
    "start": "1995440",
    "end": "2001519"
  },
  {
    "text": "your training data and models so that you can identify and limit bias and explain predictions it will provide",
    "start": "2001519",
    "end": "2009039"
  },
  {
    "text": "sharp values which will help you visualize future importance for all the other clarified capabilities",
    "start": "2009039",
    "end": "2014880"
  },
  {
    "text": "you can check the documentation linked in the notebook here we're utilizing the stagemaker",
    "start": "2014880",
    "end": "2021360"
  },
  {
    "text": "python sdk for clarify at the end to initialize a clarify processor job",
    "start": "2021360",
    "end": "2026880"
  },
  {
    "text": "we're defining the training input our model endpoint defining the shop",
    "start": "2026880",
    "end": "2033840"
  },
  {
    "text": "config data config model config so all the",
    "start": "2033840",
    "end": "2040240"
  },
  {
    "text": "configuration files we needed for the clarify job and eventually we're kicking off the job",
    "start": "2040240",
    "end": "2047840"
  },
  {
    "text": "by wrong explainability from calling the clarify processor",
    "start": "2047840",
    "end": "2054320"
  },
  {
    "text": "the job will give you a bunch of values after it's done including all the sharp",
    "start": "2054320",
    "end": "2060480"
  },
  {
    "text": "values for each feature you can check the values here or you can",
    "start": "2060480",
    "end": "2065919"
  },
  {
    "text": "visualize all these features by going back to the experiment",
    "start": "2065919",
    "end": "2074240"
  },
  {
    "text": "under experiment and trials",
    "start": "2074240",
    "end": "2077679"
  },
  {
    "text": "so this is where we you will see when you open experiment trials and usually it's under the unassigned",
    "start": "2079599",
    "end": "2085760"
  },
  {
    "text": "trial components double click and you can always search",
    "start": "2085760",
    "end": "2093200"
  },
  {
    "text": "for the trial component name and it will start with clarify",
    "start": "2096000",
    "end": "2102160"
  },
  {
    "text": "so we can choose the most recent statement to clarify explainability job here",
    "start": "2102160",
    "end": "2108000"
  },
  {
    "text": "and just by looking at the feature importance plot we can see that days since the user has",
    "start": "2108560",
    "end": "2114880"
  },
  {
    "text": "been active is a strong indicator which makes sense since from the data exploration we know that short users",
    "start": "2114880",
    "end": "2121680"
  },
  {
    "text": "are actually more active than those who did not churn these slides numbers of errors in",
    "start": "2121680",
    "end": "2127359"
  },
  {
    "text": "advertisement also place a factor in the model prediction this future important results",
    "start": "2127359",
    "end": "2133040"
  },
  {
    "text": "can be insightful when you think about the action items or what do i do with this model",
    "start": "2133040",
    "end": "2138720"
  },
  {
    "text": "after you deploy it for example the next time a user downgrades a pricing plan or has not been on the app for a given",
    "start": "2138720",
    "end": "2145359"
  },
  {
    "text": "period of time perhaps the logical step will be providing promotions to retain the user great now let's go",
    "start": "2145359",
    "end": "2152960"
  },
  {
    "text": "back to our use case demo you have successfully built train",
    "start": "2152960",
    "end": "2159839"
  },
  {
    "text": "fine-tuned and deployed the model to predict whether a user is likely to choreon for a music streaming company",
    "start": "2159839",
    "end": "2166079"
  },
  {
    "text": "you can further explore other options siege maker provided such as sagemaker autopilot which can",
    "start": "2166079",
    "end": "2172079"
  },
  {
    "text": "help automatically build train and tune the best machine learning models based on your data",
    "start": "2172079",
    "end": "2177200"
  },
  {
    "text": "while allowing you to maintain full control and visibility this is the end of the demo",
    "start": "2177200",
    "end": "2184160"
  },
  {
    "text": "here are some example code to show you where to find auto ml and how to deploy the model",
    "start": "2184640",
    "end": "2191520"
  },
  {
    "text": "thanks for checking out this video i hope this is helpful if you're interested in other use cases",
    "start": "2191520",
    "end": "2196640"
  },
  {
    "text": "as well you can check other demos provided by amazon sagemaker",
    "start": "2196640",
    "end": "2206000"
  }
]