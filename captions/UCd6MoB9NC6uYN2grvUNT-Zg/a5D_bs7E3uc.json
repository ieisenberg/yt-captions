[
  {
    "text": "well thanks for joining guys my name is parvis deim I'm a solution architect with Amazon web",
    "start": "2399",
    "end": "7559"
  },
  {
    "text": "services uh I work with ton of customers from startups to Enterprises to architect their infrastructure I focus",
    "start": "7559",
    "end": "14000"
  },
  {
    "text": "on uh I have a background in cloudfront or CDN business so I I do have a little bit of background and work with CDN",
    "start": "14000",
    "end": "21279"
  },
  {
    "text": "related uh customers and infrastructures I also have a my hobby is Big Data I work a lot with big data",
    "start": "21279",
    "end": "28519"
  },
  {
    "text": "stuff especially I do and especially elastic map ruce uh just as a reminder to everyone",
    "start": "28519",
    "end": "36680"
  },
  {
    "text": "this is level 400 session so I'm not going through",
    "start": "36680",
    "end": "41879"
  },
  {
    "text": "hadoop's introduction I'm going to assume you know everything about mappers reducers I'm going to assume you know",
    "start": "41879",
    "end": "48879"
  },
  {
    "text": "what what map reduces I'm going to assume what you know what hdfs is so",
    "start": "48879",
    "end": "54320"
  },
  {
    "text": "unfortunately if you're not familiar with those topics it's going to be a little bit hard to follow what I'm",
    "start": "54320",
    "end": "59399"
  },
  {
    "text": "saying I'll try my best to go slow but uh it may be a little bit hard if you don't know those",
    "start": "59399",
    "end": "66920"
  },
  {
    "text": "Concepts so I do want to give you a little bit of introduction on elastic map produce and what it",
    "start": "72240",
    "end": "78200"
  },
  {
    "text": "is so elastic map produce is a map ruce engine it's a massiv rep parallel Hadoop",
    "start": "78200",
    "end": "85400"
  },
  {
    "text": "infrastructure it's managed for you meaning that you don't have to deal with deployment and conf configuration and",
    "start": "85400",
    "end": "90720"
  },
  {
    "text": "everything else that has to come with Ado I remember I used to play with Ado when it actually got released long time",
    "start": "90720",
    "end": "95960"
  },
  {
    "text": "ago and you have to configure every tweak have to tweak a lot of knobs and",
    "start": "95960",
    "end": "101040"
  },
  {
    "text": "you have to do a lot of deployments and it wasn't fun so now I do everything on elastic map produce because it's easy it",
    "start": "101040",
    "end": "106960"
  },
  {
    "text": "gets everything deployed for you you can get everything deployed in matter of seconds from one Noe to thousand noes",
    "start": "106960",
    "end": "113399"
  },
  {
    "text": "and also has a lot of integration with a lot of tools that we have tools such as uh services such as elastic map uh the",
    "start": "113399",
    "end": "120200"
  },
  {
    "text": "Dynamo DB S3 red shift and everything",
    "start": "120200",
    "end": "125479"
  },
  {
    "text": "else so at at the very core at the very cor of core of elastic map reduce it's just map ruce with hdfs with a storage",
    "start": "126000",
    "end": "134120"
  },
  {
    "text": "attached but you also can store data in Amazon S3 uh which is our cloud storage",
    "start": "134120",
    "end": "139440"
  },
  {
    "text": "offering also also Amazon Dynamo DB which is no SQL offering as a managed",
    "start": "139440",
    "end": "145000"
  },
  {
    "text": "managed offering we also have customers using a ton of other Frameworks on elastic map",
    "start": "145000",
    "end": "151040"
  },
  {
    "text": "reduce things like uh Ed Bas map R is supported as a framework as as a platform on elastic map reduce Flume",
    "start": "151040",
    "end": "158959"
  },
  {
    "text": "they bring Flume apy Flume to collect ton of data really quickly really in distributed Manner and put that either",
    "start": "158959",
    "end": "165640"
  },
  {
    "text": "on htfs or S3 they use Hive uh Pig they use Mahood",
    "start": "165640",
    "end": "171879"
  },
  {
    "text": "to do actual processing of data so when they and they put all that data on uh",
    "start": "171879",
    "end": "176920"
  },
  {
    "text": "EMR now I do have customers who do have sort of a DAT relational data in",
    "start": "176920",
    "end": "182120"
  },
  {
    "text": "something like RDS MySQL or something else and they either want to move that to elastic map reduce uh to do",
    "start": "182120",
    "end": "189120"
  },
  {
    "text": "processing or they want to marry the data that sits on uh relational Rel relational databases such as my SQL",
    "start": "189120",
    "end": "195879"
  },
  {
    "text": "Oracle and put and and sort of marry those two uh Frameworks so they use scoop to do that and all the stuff that",
    "start": "195879",
    "end": "202840"
  },
  {
    "text": "I'm mentioning here you're not limited to these uh Frameworks we have a concept called bootstrap actions on EMR but",
    "start": "202840",
    "end": "209000"
  },
  {
    "text": "bootstrap action and if you can write anything in shell script shell scripts you can install that on elastic map",
    "start": "209000",
    "end": "215159"
  },
  {
    "text": "reduce I've I've written an article on uh how to install spark shark people",
    "start": "215159",
    "end": "220760"
  },
  {
    "text": "have done math Labs uh you can do everything on elastic ma as long as you",
    "start": "220760",
    "end": "226040"
  },
  {
    "text": "can write that in a shell script uh and this is a little bit of secret I'm I'm going to give you so uh",
    "start": "226040",
    "end": "233120"
  },
  {
    "text": "you can use it if if if it's helpful to you guys but you you don't even have to use elastic map rce for Hado processing",
    "start": "233120",
    "end": "238599"
  },
  {
    "text": "you can use it as HTF storage offering you can use it as a cluster management and just instill anything that requires",
    "start": "238599",
    "end": "244599"
  },
  {
    "text": "cluster Management on elastic map ruce so if you like to just bring anything else you want and just you can ignore Hadoop if you want",
    "start": "244599",
    "end": "252239"
  },
  {
    "text": "to and last but not least we came up with AWS dat pipeline because if you look at this diagram we have so many",
    "start": "252239",
    "end": "258639"
  },
  {
    "text": "different moving components you have data in S3 you have Dynamo DB uh and",
    "start": "258639",
    "end": "263759"
  },
  {
    "text": "processing Frameworks ton of processing Frameworks on top of it and we also announced red shift last year and",
    "start": "263759",
    "end": "268919"
  },
  {
    "text": "customers have been ask asking us to find a easier way to move data from one place to another usually you know if you",
    "start": "268919",
    "end": "275320"
  },
  {
    "text": "if you're familiar with something like Uzi or other uh workflow managements uh they give you a way to manage data and",
    "start": "275320",
    "end": "282280"
  },
  {
    "text": "moving data from one place to another we offer that as a managed service called AWS data pipeline where you have data on",
    "start": "282280",
    "end": "288919"
  },
  {
    "text": "RDS if you have data in MySQL if you have data in red shift uh Dynamo DB S3",
    "start": "288919",
    "end": "294600"
  },
  {
    "text": "all you have to do is uh through guy or even Json definition on data pipeline",
    "start": "294600",
    "end": "299880"
  },
  {
    "text": "Define multiple places where your data is and we automatically move that around for you process the data uh it gets",
    "start": "299880",
    "end": "306080"
  },
  {
    "text": "processed in EMR and gets stored in S3 Dynamo DB and other places but we help you move that around which is a really",
    "start": "306080",
    "end": "312199"
  },
  {
    "text": "cool",
    "start": "312199",
    "end": "314400"
  },
  {
    "text": "service all",
    "start": "318440",
    "end": "321639"
  },
  {
    "text": "right as I mentioned it's super easy to get elastic map reduce clusters up and running from one note to thousands notes",
    "start": "324560",
    "end": "331280"
  },
  {
    "text": "it takes matter of minute or 5 minutes 10 minutes but it's super",
    "start": "331280",
    "end": "336479"
  },
  {
    "text": "quick and you're not stuck with Hardware I've I've I've I've seen customers deploying hoo infrastructur Hardwares",
    "start": "340080",
    "end": "346319"
  },
  {
    "text": "and the the problem with on premise Hardware with using Hado is that you're stuck with that Hardware spec if your if",
    "start": "346319",
    "end": "351800"
  },
  {
    "text": "your workflow changes which is often with with big data processing you're stuck with that Hardware you can't go",
    "start": "351800",
    "end": "357479"
  },
  {
    "text": "anywhere else and you have to deal with Capac capacity management it's big data problem guys you have data coming in",
    "start": "357479",
    "end": "362800"
  },
  {
    "text": "quick and it Aggregates really quick so you have to do capacity planning you don't have to do that with EMR with the",
    "start": "362800",
    "end": "368560"
  },
  {
    "text": "man with a matter of API call you can add modes and I'll talk about this further down in the deck that you can",
    "start": "368560",
    "end": "374919"
  },
  {
    "text": "actually remove notes when you don't need to you don't need that processing anymore can also run multiple clusters",
    "start": "374919",
    "end": "381280"
  },
  {
    "text": "if youve if you have un premise Hadoop infrastructures you have single cluster if you have if you have to get",
    "start": "381280",
    "end": "387639"
  },
  {
    "text": "additional cluster because someone else wants to run processing workloads you have to go purchase Hardware again what",
    "start": "387639",
    "end": "392840"
  },
  {
    "text": "elastic map produce again what's one API col you get get one cluster two clusters variety of different uh sizes different",
    "start": "392840",
    "end": "399319"
  },
  {
    "text": "instance types if you in memory heavy CPU heavy you can get different instance",
    "start": "399319",
    "end": "405080"
  },
  {
    "text": "sizes this is by far the best integration I've seen which is integration with spot Market I don't",
    "start": "405080",
    "end": "410599"
  },
  {
    "text": "know if you guys are familiar with spot Market but spot Market is a way for you to come in bid on unused capacity of AWS",
    "start": "410599",
    "end": "417560"
  },
  {
    "text": "so I think as Andy Jesse was mentioned in his keynote we have unused capacity and if uh at let's just say 2: in the",
    "start": "417560",
    "end": "424319"
  },
  {
    "text": "morning or any anytime in and during the day if you need resources you can come in and bid on those instances what that",
    "start": "424319",
    "end": "430720"
  },
  {
    "text": "means is that if if an instance typically costs let's just say a buck an hour you can bid on that instance and",
    "start": "430720",
    "end": "436520"
  },
  {
    "text": "get that instance for 10 cents 15 cents 30 cents an",
    "start": "436520",
    "end": "442199"
  },
  {
    "text": "hour let me get into some design patterns on EMR what I've seen these are the list list of design",
    "start": "443520",
    "end": "450319"
  },
  {
    "text": "patterns that I'm going to go one by one so patterns such as trans transient clusters versus live clusters core nodes",
    "start": "450319",
    "end": "457199"
  },
  {
    "text": "and task noes this is a concept in elastic map reduce using Amazon S3 as",
    "start": "457199",
    "end": "462400"
  },
  {
    "text": "your persistent or maybe hdfs but in this case replacing s hdfs with S3 and",
    "start": "462400",
    "end": "468520"
  },
  {
    "text": "also using both in conjunction to each other so hdfs and S3 and I'll also talk",
    "start": "468520",
    "end": "474080"
  },
  {
    "text": "about elastic clusters clusters that you can either manually or automatically add and remove nodes",
    "start": "474080",
    "end": "481440"
  },
  {
    "text": "so let's talk about transient versus live clusters transient clusters are the type",
    "start": "482879",
    "end": "489319"
  },
  {
    "text": "of clusters then when you do you do processing you do you submit your job the job gets done when when the job is",
    "start": "489319",
    "end": "495000"
  },
  {
    "text": "done cluster goes down meaning that the nodes shut down uh obviously for",
    "start": "495000",
    "end": "500360"
  },
  {
    "text": "persistent you need to host your data somewhere such as S3 so S3 would be the great use case for this workload but the",
    "start": "500360",
    "end": "507280"
  },
  {
    "text": "the point is that the cluster doesn't stay around cluster goes away as soon as the processing is",
    "start": "507280",
    "end": "513240"
  },
  {
    "text": "done well the biggest benefit of this is controling your cost right if you're if",
    "start": "513240",
    "end": "518719"
  },
  {
    "text": "you're not using the processing cluster if you're not using those noes anymore after the job is done shut them down and",
    "start": "518719",
    "end": "524399"
  },
  {
    "text": "this is one pattern just don't pay for what you're not using and obviously it's a minimum maintenance if the cluster is",
    "start": "524399",
    "end": "530760"
  },
  {
    "text": "not staying around you don't have to deal with failures with nodes going down with hdfs corruption or anything else",
    "start": "530760",
    "end": "536440"
  },
  {
    "text": "that may happen on those nodes again not to repeat but again pay for what you use but also gets you in in",
    "start": "536440",
    "end": "542279"
  },
  {
    "text": "a discipline of using EMR any processing workload as just a workflow something",
    "start": "542279",
    "end": "547959"
  },
  {
    "text": "that comes up does its jobs and goes away it's very different from the other like from a traditional way of doing",
    "start": "547959",
    "end": "554120"
  },
  {
    "text": "things which is cluster runs 2477 and regardless of if you have a job if you don't have a job that those notes just",
    "start": "554120",
    "end": "560440"
  },
  {
    "text": "keeps running so it's great to get into the discipline of just bringing stuff up when you need to do something and",
    "start": "560440",
    "end": "566600"
  },
  {
    "text": "shutting them down this maybe not for cost but just as a dis",
    "start": "566600",
    "end": "571360"
  },
  {
    "text": "so when would you want to use transient clusters versus a cluster that stays around it's a simple math if if the data",
    "start": "572720",
    "end": "579600"
  },
  {
    "text": "load time when it takes for the time that it takes you load your infrastructure or your EMR cluster with",
    "start": "579600",
    "end": "585519"
  },
  {
    "text": "data and do your data processing if you aggregate that and multiply that by the",
    "start": "585519",
    "end": "591120"
  },
  {
    "text": "number of jobs you're running on a given day if that less than 24h hour you can",
    "start": "591120",
    "end": "596160"
  },
  {
    "text": "save cost and be more agile by using transient clusters that's that's an easy",
    "start": "596160",
    "end": "601760"
  },
  {
    "text": "math and I'll you know give you an example 20 minutes data load time if you have this is a recur uh reoccurring",
    "start": "601760",
    "end": "608200"
  },
  {
    "text": "workload for you 20 minutes daily load time on a given day and each workload takes an hour to process and you have 10",
    "start": "608200",
    "end": "615120"
  },
  {
    "text": "of these jobs every day that's about 13 hours on a given day so less than 24",
    "start": "615120",
    "end": "620640"
  },
  {
    "text": "hours you can uh reduce your cost by shutting down the cluster",
    "start": "620640",
    "end": "626720"
  },
  {
    "text": "in contrast we can have live clusters you I call it the live cluster you can call it a long running clusters and the",
    "start": "631720",
    "end": "638399"
  },
  {
    "text": "reason I love call it alive clusters is that when you call EMR API you have to",
    "start": "638399",
    "end": "643440"
  },
  {
    "text": "say alive and that's that's that's the way EMR knows that your job is the Clusters is be running",
    "start": "643440",
    "end": "649240"
  },
  {
    "text": "247 so very similar to traditional Hadoop infrastructures cluster stays around after the job is done cluster",
    "start": "649240",
    "end": "656320"
  },
  {
    "text": "runs 24/7 and you still use the data persistent models such as you know maybe",
    "start": "656320",
    "end": "661519"
  },
  {
    "text": "using Amazon S3 to to store data you can use S3 in conjunction with hdfs and I'll",
    "start": "661519",
    "end": "667639"
  },
  {
    "text": "talk about this pattern later on where data goes to S3 and you copy that to hdfs uh or you can just use hdfs and use",
    "start": "667639",
    "end": "675480"
  },
  {
    "text": "S3 as a backup and we'll talk about that later",
    "start": "675480",
    "end": "679880"
  },
  {
    "text": "on so it's a long running cluster you get hdfs as part of EMR cluster now you",
    "start": "681720",
    "end": "688320"
  },
  {
    "text": "could store your data on htfs only as your primary storage but even if you do",
    "start": "688320",
    "end": "693639"
  },
  {
    "text": "if you decide to go that way I always recommend keeping data on S as a backup uh design for failure is",
    "start": "693639",
    "end": "700560"
  },
  {
    "text": "something we always talk about always design for failure always assume that cluster is going to go away even though you ask for it to stay around so keep",
    "start": "700560",
    "end": "707440"
  },
  {
    "text": "your data backed up and safe on SRE now this is this is a strange",
    "start": "707440",
    "end": "712680"
  },
  {
    "text": "recommendation so I I'll explain a little bit get in the habit of shutting down the cluster once a week once a",
    "start": "712680",
    "end": "718399"
  },
  {
    "text": "month at some frequency but shut down the cluster once in a while the reason I say that is I I have a lot of customers",
    "start": "718399",
    "end": "724920"
  },
  {
    "text": "who say hey I've I've had this cluster and I've asked to be around 247 uh but I suffered a lot because that",
    "start": "724920",
    "end": "732120"
  },
  {
    "text": "cluster went got shut down for some reason or maybe and you guys may notice but the master knows name knows that",
    "start": "732120",
    "end": "737920"
  },
  {
    "text": "does everything for Hado if that goes down your cluster can go down completely",
    "start": "737920",
    "end": "743000"
  },
  {
    "text": "uh and they they get into the situation where the cluster supposed to stay up and running the master node goes down",
    "start": "743000",
    "end": "748440"
  },
  {
    "text": "and they're completely and the workload just suffers the time to process go goes high",
    "start": "748440",
    "end": "754040"
  },
  {
    "text": "and they complain about that the reason I ask you guys to shut down the cluster once in a while is to get into a habit",
    "start": "754040",
    "end": "759519"
  },
  {
    "text": "of Designing for failures and recovering from failures if even if the cluster is supposed to stay around for 247 for long",
    "start": "759519",
    "end": "766480"
  },
  {
    "text": "run long running clusters be in the habit of accounting for that failure and you can do that with data pipeline you",
    "start": "766480",
    "end": "772120"
  },
  {
    "text": "can do that with Uzi you can do that with any other workflow management but the point is that your workflow should detect these failures and run a new",
    "start": "772120",
    "end": "779240"
  },
  {
    "text": "cluster if that cluster completely If the previous cluster fails versus uh",
    "start": "779240",
    "end": "784480"
  },
  {
    "text": "relying on the previous cluster to run 24/7 forever so again get in the habit of shutting down the cluster and I don't",
    "start": "784480",
    "end": "790360"
  },
  {
    "text": "know if you guys seen the stuff that Netflix does they have something like chaos monkey that goes around and takes stuff down like random instances random",
    "start": "790360",
    "end": "797720"
  },
  {
    "text": "elbs autoscaling mostly because of that so they can design for failure so that if anything goes R wrong if anything",
    "start": "797720",
    "end": "804279"
  },
  {
    "text": "gets shut down the workflow doesn't suffer",
    "start": "804279",
    "end": "809440"
  },
  {
    "text": "so benefits of live clusters is ability to uh share data between multiple jobs",
    "start": "810920",
    "end": "816079"
  },
  {
    "text": "so with uh contrast with transient clusters transient clusters if you have multiple jobs let's just say three or",
    "start": "816079",
    "end": "821839"
  },
  {
    "text": "four jobs independent jobs but the output of the last job needs to be consumed by the next job uh one way you",
    "start": "821839",
    "end": "828880"
  },
  {
    "text": "can do with transing cluster is you can put that data on S3 as a share storage the next job will pick up after the",
    "start": "828880",
    "end": "834600"
  },
  {
    "text": "previous job now that this could be inefficient meaning that if you have a lot of data going back and forth and you",
    "start": "834600",
    "end": "839959"
  },
  {
    "text": "have lot of clusters coming around coming up and doing the processing this could be inefficient what you could do",
    "start": "839959",
    "end": "846079"
  },
  {
    "text": "is you could run a long running cluster that stays around for a long time and then have the jobs output to hdfs and",
    "start": "846079",
    "end": "853399"
  },
  {
    "text": "the next job picks up from hdfs and as goes and goes on throughout the chain of the workflow",
    "start": "853399",
    "end": "861279"
  },
  {
    "text": "processing in a strange way it's actually maybe more costeffective for you to run long running clusters and uh",
    "start": "861480",
    "end": "868040"
  },
  {
    "text": "one way uh one use case would have would be that if you have a multiple jobs Runing every hour let's just say you",
    "start": "868040",
    "end": "874600"
  },
  {
    "text": "have two jobs at 1 p.m. another two jobs at 2 p.m. and on so on it would be more coste effective to keep the cluster",
    "start": "874600",
    "end": "881079"
  },
  {
    "text": "around and not shut it down just keep the cluster around submit the jobs to the same cluster as time goes on so at",
    "start": "881079",
    "end": "886720"
  },
  {
    "text": "1: p.m. 2 p.m. 3 p.m. you have the same cluster running you can later on shut it down let's just say 9:00 or 10 o'clock",
    "start": "886720",
    "end": "893079"
  },
  {
    "text": "at night when there's no jobs running running on that cluster but uh it's better than to just keep the cluster",
    "start": "893079",
    "end": "899240"
  },
  {
    "text": "around for at least three or four hours in a given",
    "start": "899240",
    "end": "903040"
  },
  {
    "text": "day very similar math nothing nothing complex about this but if your data load",
    "start": "904279",
    "end": "909480"
  },
  {
    "text": "time plus processing time of that data times the number of jobs you have it's more than 24 uh 24 hours then it makes",
    "start": "909480",
    "end": "917000"
  },
  {
    "text": "sense for you to keep the cluster around it just shutting down the cluster up and down that just doesn't make sense",
    "start": "917000",
    "end": "924399"
  },
  {
    "text": "anymore so an example 20 minutes data load time an hour processing time you",
    "start": "925360",
    "end": "930480"
  },
  {
    "text": "have 20 jobs in a given day that's about 26 hours of processing time more than 24",
    "start": "930480",
    "end": "936240"
  },
  {
    "text": "hour you may just keep the cluster around and use alive clusters versus transient",
    "start": "936240",
    "end": "942240"
  },
  {
    "text": "clusters core and task nodes uh so very uh similar to traditional",
    "start": "943279",
    "end": "950199"
  },
  {
    "text": "Hadoop infrastructure we have Master note that was that will do coordination and sending mappers and reducers to",
    "start": "950199",
    "end": "955279"
  },
  {
    "text": "multiple nodes to for processing uh but we also have concept of core nodes core nodes are the nodes",
    "start": "955279",
    "end": "962040"
  },
  {
    "text": "that run task tracker which does computation of the job Computing running the mappers running the reducers uh we",
    "start": "962040",
    "end": "968440"
  },
  {
    "text": "also have uh which it also runs data nodes which is hdfs stores your data on local",
    "start": "968440",
    "end": "974720"
  },
  {
    "text": "disk very similar to traditional Hadoop infrastructures the good thing about",
    "start": "974720",
    "end": "980680"
  },
  {
    "text": "core nodes is that you can add nodes you can if you run a cluster for four nodes you need more capacity you need more",
    "start": "980680",
    "end": "987199"
  },
  {
    "text": "hdfs space you need more CPU you can add another node to your uh to your cluster with an API call",
    "start": "987199",
    "end": "995160"
  },
  {
    "text": "easy but the caveat is you can't remove core nodes once you add them you're not",
    "start": "995519",
    "end": "1000680"
  },
  {
    "text": "allowed to remove them the reason for that is because of hdfs hdfs doesn't really like in situations where you",
    "start": "1000680",
    "end": "1007959"
  },
  {
    "text": "randomly take nodes out out of rotation it become it creates imbalance it creates corruption it creates a lot of",
    "start": "1007959",
    "end": "1014279"
  },
  {
    "text": "Strange Behaviors so you don't want to remove hdfs noes as as when you add them",
    "start": "1014279",
    "end": "1020839"
  },
  {
    "text": "all right I'm not sure if the all right so the other type of nodes for EMR is Task nodes task nodes run",
    "start": "1027799",
    "end": "1036360"
  },
  {
    "text": "task tracker as opposed to core nodes which runs task tracker and data node task trackers task nodes only run task",
    "start": "1036360",
    "end": "1043600"
  },
  {
    "text": "trackers uh the task trackers only do computation and task noes don't have any",
    "start": "1043600",
    "end": "1049480"
  },
  {
    "text": "hdfs meaning that if they have to read any data uh and have to persist any data",
    "start": "1049480",
    "end": "1054919"
  },
  {
    "text": "you have they communicate back to core noes they go over the network to core nodes to do any read or write of",
    "start": "1054919",
    "end": "1062400"
  },
  {
    "text": "data with task nodes you can add nodes similar to core",
    "start": "1062400",
    "end": "1067679"
  },
  {
    "text": "nodes so more CPU more memory if you need",
    "start": "1067760",
    "end": "1072960"
  },
  {
    "text": "one but you can also remove them and reason for that again is because they're not running any hdfs uh assistant",
    "start": "1073000",
    "end": "1079640"
  },
  {
    "text": "storage uh when you add them if you don't need them you can easily remove",
    "start": "1079640",
    "end": "1085240"
  },
  {
    "text": "them a great use case for task noes are if you need to speed up your job",
    "start": "1085960",
    "end": "1091400"
  },
  {
    "text": "processing using spot market so let me again give you a little bit of overview of spot Market you go into spot Market",
    "start": "1091400",
    "end": "1097400"
  },
  {
    "text": "you you bid for unused capacity let's just say a cc2 instance is a great instance we have for beefy instance a",
    "start": "1097400",
    "end": "1103880"
  },
  {
    "text": "dollar an hour you go into spot Market we give you the market price let's just say 20 cents or 30 cents you bid on that",
    "start": "1103880",
    "end": "1111000"
  },
  {
    "text": "instance now you just like uh eBay pricing you give it the highest price you're willing to pay and let's just say",
    "start": "1111000",
    "end": "1117640"
  },
  {
    "text": "you say 70 cents we give you the lowest market price possible so let's just say it was 20 cents we give you 25 cents",
    "start": "1117640",
    "end": "1124400"
  },
  {
    "text": "that was low as possible if the market price goes up up to 70 cents you keep",
    "start": "1124400",
    "end": "1129440"
  },
  {
    "text": "the instance if it goes higher than 70 cents which was the maximum price you wanted to pay we take the instance away",
    "start": "1129440",
    "end": "1135720"
  },
  {
    "text": "from you spot instances are great to get a discount but it's not so great if you",
    "start": "1135720",
    "end": "1140840"
  },
  {
    "text": "can't uh deal with uh failures if if that instance goes away for example if the market price goes away we'll take",
    "start": "1140840",
    "end": "1146400"
  },
  {
    "text": "that away from you if your workload is let's just say it's a web server or a database it's not a great use case for that but it's a great use case for",
    "start": "1146400",
    "end": "1153080"
  },
  {
    "text": "Hadoop because Hadoop is really good at Distributing workload Distributing workloads such as mappers and reducers",
    "start": "1153080",
    "end": "1159919"
  },
  {
    "text": "to healthy instances so great use case for spot is to put task nodes on spot",
    "start": "1159919",
    "end": "1165159"
  },
  {
    "text": "Market to do processing if you need to speed up your processing if if the spot Market goes High the price goes high and",
    "start": "1165159",
    "end": "1171120"
  },
  {
    "text": "we need to take that instance away from you that's okay EMR or Hado can distribute that workload that failed on",
    "start": "1171120",
    "end": "1177039"
  },
  {
    "text": "that specific mapper reducer that failed on that instance to some other instance that is up and running so you can get",
    "start": "1177039",
    "end": "1183880"
  },
  {
    "text": "the discount you want you can speed up the processing job and not deal not not have having to deal with interruptions",
    "start": "1183880",
    "end": "1190320"
  },
  {
    "text": "or anything else because the inance is going",
    "start": "1190320",
    "end": "1194320"
  },
  {
    "text": "away another use case would be if you need horsepower for the short amount of time let's just say for 5 or 10 minutes",
    "start": "1196400",
    "end": "1203039"
  },
  {
    "text": "you need more CPU or more RAM and uh you only need it for a specific amount of",
    "start": "1203039",
    "end": "1208200"
  },
  {
    "text": "time example of that would be that if you're hosting ton of data on S3 you want to bring bring that to hdfs you",
    "start": "1208200",
    "end": "1214760"
  },
  {
    "text": "want to do some sort of processing and uh all you need is just horsepower to pull as much as possible from S3 and do",
    "start": "1214760",
    "end": "1221720"
  },
  {
    "text": "the later on processing and let me give you an example let's just say I have hs1 instances hs1 instances are AWS",
    "start": "1221720",
    "end": "1228320"
  },
  {
    "text": "instances that hold 48 terabyt of spindles attached to it that's a lot of",
    "start": "1228320",
    "end": "1233360"
  },
  {
    "text": "data for one node let's just say you have two of them and you have a certain",
    "start": "1233360",
    "end": "1239000"
  },
  {
    "text": "amount of data on S3 more than let's just say 48 or 50 terabytes on S3 you want to bring some of that to those to",
    "start": "1239000",
    "end": "1245400"
  },
  {
    "text": "these instances and do processing well you have two instances and they're limited as far as as far as how much",
    "start": "1245400",
    "end": "1251720"
  },
  {
    "text": "Network Nick throughput they can push to S3 you have two only two notes here so",
    "start": "1251720",
    "end": "1257039"
  },
  {
    "text": "you putting for taking 48 terabytes from S3 putting it on these instances may",
    "start": "1257039",
    "end": "1262520"
  },
  {
    "text": "take some time one way to uh reduce that time the",
    "start": "1262520",
    "end": "1268000"
  },
  {
    "text": "load time from S3 would be to get ton of M1 extra larges on task nodes that would go to S3",
    "start": "1268000",
    "end": "1276000"
  },
  {
    "text": "pull data from S3 and process data on hdfs nodes so your hs1 hdfs node hs1",
    "start": "1276000",
    "end": "1282880"
  },
  {
    "text": "instances would be core nodes M1 extra large would be on spot instances on task nodes it would read data from S3 as fast",
    "start": "1282880",
    "end": "1290000"
  },
  {
    "text": "as possible put it on hdfs core nodes hs1 core nodes and then when you're done",
    "start": "1290000",
    "end": "1296159"
  },
  {
    "text": "shut them down now you can go back to processing data on hdfs and you can use this an example of",
    "start": "1296159",
    "end": "1302559"
  },
  {
    "text": "for anything else so as if you need processing time CP processing uh horsepower so CPU memory for a short",
    "start": "1302559",
    "end": "1309760"
  },
  {
    "text": "amount of time not for the duration of the job for just short amount of time just add that on task nodes and when",
    "start": "1309760",
    "end": "1315400"
  },
  {
    "text": "you're done shut them down all right let's get into the patterns of",
    "start": "1315400",
    "end": "1321320"
  },
  {
    "text": "of data persistence so Amazon S3 has hdfs so traditionally what you would do",
    "start": "1321320",
    "end": "1326360"
  },
  {
    "text": "is you would uh put data on hdfs and process data on hdfs with this model",
    "start": "1326360",
    "end": "1332279"
  },
  {
    "text": "what you would do is you put data on S3 so you know collect logs using flumes or Flume Kafka or any other data collection",
    "start": "1332279",
    "end": "1339960"
  },
  {
    "text": "framework put that on S3 so permanently store data on S3 use htfs as a temporary",
    "start": "1339960",
    "end": "1346840"
  },
  {
    "text": "storage for jobs that need to output anything and then do the processing on uh as you read data from S3 one thing",
    "start": "1346840",
    "end": "1354000"
  },
  {
    "text": "that I get a lot from customers when I talk about this pattern is well it it adds latency to my job processing",
    "start": "1354000",
    "end": "1360080"
  },
  {
    "text": "because I have to copy data to hdfs that's that's that's that's not correct you're not copying data to hdfs",
    "start": "1360080",
    "end": "1366159"
  },
  {
    "text": "everything gets streamed directly from S3 to mappers processing gets done as",
    "start": "1366159",
    "end": "1371760"
  },
  {
    "text": "mappers reading data from S3 and output of that using reducers goes to either to",
    "start": "1371760",
    "end": "1376960"
  },
  {
    "text": "htfs or back to S3 there's no additional step in this in this pattern for you to",
    "start": "1376960",
    "end": "1382200"
  },
  {
    "text": "copy data so you're not incurring any",
    "start": "1382200",
    "end": "1385919"
  },
  {
    "text": "latency the huge benefit I'm I definitely highlighted that just to make sure it doesn't get missed the huge",
    "start": "1387720",
    "end": "1393720"
  },
  {
    "text": "benefit for that is the ability for you to shut down the cluster so we talked about transient clusters and shutting",
    "start": "1393720",
    "end": "1399120"
  },
  {
    "text": "down the stuff you don't need you know for task trackers and shutting down the even in the entire cluster when you",
    "start": "1399120",
    "end": "1404440"
  },
  {
    "text": "don't need that cluster the only way you can do that is if you data on S3 if you",
    "start": "1404440",
    "end": "1410640"
  },
  {
    "text": "don't persist data on S3 and use hdfs you have to keep the cluster around you have you don't have a way to shut down",
    "start": "1410640",
    "end": "1416559"
  },
  {
    "text": "your cluster another great benefit is 11 nines of durability that S3 provides 11",
    "start": "1416559",
    "end": "1422400"
  },
  {
    "text": "nines of durability I forgot the statistic it's some it's some crazy statistic that says if you host your uh",
    "start": "1422400",
    "end": "1427720"
  },
  {
    "text": "your one object or multiple objects on S3 you won't lose that object for million years that that's statistically",
    "start": "1427720",
    "end": "1433320"
  },
  {
    "text": "that's how what it means 119s of durability",
    "start": "1433320",
    "end": "1437919"
  },
  {
    "text": "the other great benefit of this model is you don't have to deal with capacity management for hdfs again as I mentioned",
    "start": "1439000",
    "end": "1444520"
  },
  {
    "text": "you're dealing with big data here data is coming in as fast as you can produce them and uh if you're hosting an hdfs",
    "start": "1444520",
    "end": "1450720"
  },
  {
    "text": "you have to keep adding nodes core nodes keep adding core nodes to increase your htfs cluster uh with S3 you don't have",
    "start": "1450720",
    "end": "1457400"
  },
  {
    "text": "to do that S3 scales for you to to Infinity just just put as much as you can on S3 and don't deal with hdfs",
    "start": "1457400",
    "end": "1463720"
  },
  {
    "text": "capacity management and anything the other thing that adds to capacity management for hdfs is replication for durability you",
    "start": "1463720",
    "end": "1470840"
  },
  {
    "text": "have to increase your replication Factor on hdfs three or four and then that means that if you're getting like let's",
    "start": "1470840",
    "end": "1476720"
  },
  {
    "text": "just say 48 terabytes of storage that's not actually 48 terabytes that's you know onethird of that if you're doing",
    "start": "1476720",
    "end": "1482399"
  },
  {
    "text": "three-week replication with S3 you don't have to deal with any of that just put a put data on S3 S3 behind the scenes it's",
    "start": "1482399",
    "end": "1487919"
  },
  {
    "text": "already replicated to multiple data centers and gives you the durability you",
    "start": "1487919",
    "end": "1493120"
  },
  {
    "text": "need uh this is another great use case for uh using SRE as your persistent data",
    "start": "1494640",
    "end": "1500799"
  },
  {
    "text": "store let's just say that uh you have you know 100 terabytes of data let's just say paby of data uh you have you're",
    "start": "1500799",
    "end": "1508640"
  },
  {
    "text": "in one you're part of your organization but then you have multiple organiz multiple departments in your",
    "start": "1508640",
    "end": "1515399"
  },
  {
    "text": "organization let's just say the the different department comes to you and says uh I need to use your data this is",
    "start": "1515399",
    "end": "1521320"
  },
  {
    "text": "you know web server logs generated I need to use I need to use your data I need to process the data you have what",
    "start": "1521320",
    "end": "1527000"
  },
  {
    "text": "would you do if you have a cluster there there are two ways about that I mean what you can do is you can copy the data",
    "start": "1527000",
    "end": "1532840"
  },
  {
    "text": "from your hdfs or from uh from your hdfs cluster to their hdfs cluster so they",
    "start": "1532840",
    "end": "1538600"
  },
  {
    "text": "can do the processing of of that data well that's 100 terabytes of data almost maybe even even maybe pedabytes of data",
    "start": "1538600",
    "end": "1545320"
  },
  {
    "text": "moving that around it's it's not an easy task I can tell you so that's one way the other way is would be for for you to",
    "start": "1545320",
    "end": "1551880"
  },
  {
    "text": "give them access to your cluster so you're running a cluster with 100 terabytes of data and they can log in",
    "start": "1551880",
    "end": "1557360"
  },
  {
    "text": "and do processing but then what if they just completely you know take advantage of that cluster and completely uh",
    "start": "1557360",
    "end": "1564440"
  },
  {
    "text": "exhaust that resource of that EMR of that hudo infrastructure then you're left alone you don't have any resources",
    "start": "1564440",
    "end": "1570640"
  },
  {
    "text": "left for your workload great thing with uh hosting data on S3 is that you can have multiple",
    "start": "1570640",
    "end": "1576600"
  },
  {
    "text": "clusters sharing the same data so 100 terabytes put it on S3 you can have one",
    "start": "1576600",
    "end": "1581919"
  },
  {
    "text": "to 100 clusters read from S3 same data set and do their own processing you're",
    "start": "1581919",
    "end": "1587360"
  },
  {
    "text": "not limited S2 uh copying the data around you don't have to copy the data around you don't have to give them access to your cluster",
    "start": "1587360",
    "end": "1593320"
  },
  {
    "text": "they can run a multiple clusters totally different clusters sharing the same",
    "start": "1593320",
    "end": "1599120"
  },
  {
    "text": "data everything else comes for free right uh S3 server side encryption if",
    "start": "1600399",
    "end": "1605480"
  },
  {
    "text": "you need that data to be encrypted at rest we provide that on S3 life cycle",
    "start": "1605480",
    "end": "1610799"
  },
  {
    "text": "policies it's a great thing if you need to delete data if this is let's just say you have ton of logs you don't need last",
    "start": "1610799",
    "end": "1616640"
  },
  {
    "text": "year's log you can autom automatically have S automatically delete that for you you can't do that with",
    "start": "1616640",
    "end": "1622760"
  },
  {
    "text": "hdfs uh you can use versioning to protection protect against corruption if if something gets corrupted if if you",
    "start": "1622760",
    "end": "1628919"
  },
  {
    "text": "need to recover from that you can have S3 versioning enabled and go back to the last or previous version of that data",
    "start": "1628919",
    "end": "1634919"
  },
  {
    "text": "and recover from that that all comes for free and the other thing is you can bu",
    "start": "1634919",
    "end": "1640279"
  },
  {
    "text": "build elastic cluster that notes join the cluster and go away knowing that again data is safe and safe on S you",
    "start": "1640279",
    "end": "1650158"
  },
  {
    "text": "so I get this question a lot I'm sure ton of you going to have this question uh what happens with data locality one",
    "start": "1650919",
    "end": "1656440"
  },
  {
    "text": "of the problems of Hado was Data locality data is is on hdfs on the same node and you have direct access to it uh",
    "start": "1656440",
    "end": "1664240"
  },
  {
    "text": "one thing that people and my customers forget is that if you're running your EMR nodes uh in the same region that",
    "start": "1664240",
    "end": "1671600"
  },
  {
    "text": "your S3 bucket is you have highspeed almost fiber connectivity to S3 nodes so",
    "start": "1671600",
    "end": "1679000"
  },
  {
    "text": "the speed that you get between EMR nodes and S3 nodes are super fast and most of",
    "start": "1679000",
    "end": "1685760"
  },
  {
    "text": "and I I see this a lot some jobs are CPU and memory bounded so even though you",
    "start": "1685760",
    "end": "1690880"
  },
  {
    "text": "have data local to you if your data if your workload is CPU or memory bounded data locality does not come into play so",
    "start": "1690880",
    "end": "1698240"
  },
  {
    "text": "keep that in mind what would it be not good for if if",
    "start": "1698240",
    "end": "1704360"
  },
  {
    "text": "you're uh if you're holding data on S3 what would it not be what what would it be not efficient for iterative workloads",
    "start": "1704360",
    "end": "1711840"
  },
  {
    "text": "it's something that I I won't I won't recommend S3 as your data uh persistence store for example machine learning would",
    "start": "1711840",
    "end": "1717320"
  },
  {
    "text": "be a really good example but machine learning what you would do is you would get a data set you would pass through data set multiple times and processing",
    "start": "1717320",
    "end": "1723880"
  },
  {
    "text": "the same data set multiple times if you put that on S3 your worklow may be a bit",
    "start": "1723880",
    "end": "1729159"
  },
  {
    "text": "slower on that so you have if you have dis IU intensive workloads or if you have any workload that reads the same",
    "start": "1729159",
    "end": "1735320"
  },
  {
    "text": "data multiple times I do recommend the the second pattern that we're going to talk about which is copying data to hdfs",
    "start": "1735320",
    "end": "1742039"
  },
  {
    "text": "first and then doing the processing so let's get into that Amazon",
    "start": "1742039",
    "end": "1747679"
  },
  {
    "text": "S3 and hdfs with this pattern well last pattern was was just replacing hdfs with",
    "start": "1747679",
    "end": "1753679"
  },
  {
    "text": "S3 this is having both in the picture you persist data on S3 you're",
    "start": "1753679",
    "end": "1759519"
  },
  {
    "text": "still you're still putting data on S3 you you're using anything data distributed lock collectors you put",
    "start": "1759519",
    "end": "1765120"
  },
  {
    "text": "stuff on S3 then when you do we need to do something we need to do workflow",
    "start": "1765120",
    "end": "1770480"
  },
  {
    "text": "processing or you need to process that data on S3 you launch a cluster using S3 to CP which is a tool we provide on EMR",
    "start": "1770480",
    "end": "1778640"
  },
  {
    "text": "you would copy data to hdfs and then you would run your data",
    "start": "1778640",
    "end": "1784240"
  },
  {
    "text": "processing on htfs directly so as I mentioned the pattern",
    "start": "1784240",
    "end": "1789960"
  },
  {
    "text": "for that the the one place you would use this pattern for is IO intensive workloads if you have IO intensive",
    "start": "1789960",
    "end": "1796480"
  },
  {
    "text": "workloads that needs to do do a lot of iio to disk then what you would do is you would uh copy data to to hdfs and",
    "start": "1796480",
    "end": "1803480"
  },
  {
    "text": "then run that workflow and you get this exact same benefits so durability scalability of S3 the cost and all the",
    "start": "1803480",
    "end": "1810600"
  },
  {
    "text": "features that comes with S3 only that you have to add another additional step to do to data for workflow which is",
    "start": "1810600",
    "end": "1816279"
  },
  {
    "text": "copying data to hdfs elastic cluster so they call it",
    "start": "1816279",
    "end": "1822679"
  },
  {
    "text": "elastic map R use for no reason and for that is what what I mean is that you can",
    "start": "1822679",
    "end": "1827720"
  },
  {
    "text": "add nose and remove nose and how would you do that I one way to do that is just API right and we can call the API but",
    "start": "1827720",
    "end": "1833919"
  },
  {
    "text": "there are two patterns here that you can use one is uh one pattern would be manual uh Intervention which is uh you",
    "start": "1833919",
    "end": "1841240"
  },
  {
    "text": "have a cluster of let's just say certain amount of nodes let's just say four nodes or five nodes you monitor that cluster with",
    "start": "1841240",
    "end": "1848760"
  },
  {
    "text": "cloudwatch and we provide that integration on elastic map ruce with cloudwatch you get metrics such as map",
    "start": "1848760",
    "end": "1855200"
  },
  {
    "text": "task running map task remaining mathas remaining is uh the amount of mappers",
    "start": "1855200",
    "end": "1861039"
  },
  {
    "text": "that are sitting in queue waiting for for it to be processed is cluster idle",
    "start": "1861039",
    "end": "1866440"
  },
  {
    "text": "or how many jobs are failing you monitor that and with cloudwatch the great thing is you can get emails you can get SMS to",
    "start": "1866440",
    "end": "1873480"
  },
  {
    "text": "let you know that one of these metrics is either going above the threshold you configured or maybe even going below the",
    "start": "1873480",
    "end": "1879559"
  },
  {
    "text": "threshold you configured once you get that notification what you can do is you can add uh take action so either adding",
    "start": "1879559",
    "end": "1886639"
  },
  {
    "text": "a node or maybe it's just the other the other way around maybe you can just remove the node if the cluster is",
    "start": "1886639",
    "end": "1892279"
  },
  {
    "text": "sitting idle but I put the M there because this is manual intervention you have to do something you have to go in",
    "start": "1892279",
    "end": "1898039"
  },
  {
    "text": "and and take an action which is calling EMR",
    "start": "1898039",
    "end": "1902518"
  },
  {
    "text": "API the second way to do this would be automating this again similar to last uh",
    "start": "1903360",
    "end": "1909600"
  },
  {
    "text": "pattern you start the cluster with certain amount of notes let's just say four",
    "start": "1909600",
    "end": "1915000"
  },
  {
    "text": "noes you monitor the cluster similar way map task running map task remaining if",
    "start": "1915000",
    "end": "1920760"
  },
  {
    "text": "if the cluster is Idle if any job is failing you monitor that now the other good aspect of",
    "start": "1920760",
    "end": "1928519"
  },
  {
    "text": "cloudwatch or cloudwatch integration with SNS Amazon SNS is the ability for SNS to ping you give you an HTTP ping",
    "start": "1928519",
    "end": "1937080"
  },
  {
    "text": "this is HTTP API call that SNS can do on your behalf I'm not sure if you guys",
    "start": "1937080",
    "end": "1942519"
  },
  {
    "text": "have done this before but instead of email or SMS you can give it endpoint some IP or maybe host name to say that",
    "start": "1942519",
    "end": "1949159"
  },
  {
    "text": "if you meet any of these thresholds call this endpoint for me and what you could do is you can run a very simple",
    "start": "1949159",
    "end": "1955880"
  },
  {
    "text": "lightweight application on elastic beant stock which is you know just manage application hosting run that on elastic",
    "start": "1955880",
    "end": "1962399"
  },
  {
    "text": "beant stock and uh SNS will ping that elastic beant stock application and that",
    "start": "1962399",
    "end": "1968679"
  },
  {
    "text": "elastic beanock application can make that EMR API on your behalf so in this",
    "start": "1968679",
    "end": "1974519"
  },
  {
    "text": "case hands off you you're not you're not doing anything uh you just as long as you write a very simple application that",
    "start": "1974519",
    "end": "1980760"
  },
  {
    "text": "sits on beanock that can run uh API calls for you everything's automated your treshold is met SS goes to beanock",
    "start": "1980760",
    "end": "1988399"
  },
  {
    "text": "beanock will call EMR API either to add nodes or remove nodes completely we call",
    "start": "1988399",
    "end": "1994519"
  },
  {
    "text": "it auto scaling for EMR for those guys who like Auto",
    "start": "1994519",
    "end": "1999638"
  },
  {
    "text": "scaling all right so best practices use M1 small M1 large C one",
    "start": "2000240",
    "end": "2008120"
  },
  {
    "text": "Med medium for functional testing I do not recommend running any production workload on these",
    "start": "2008120",
    "end": "2014120"
  },
  {
    "text": "instances but if you do have production workload And1 extra large or any large",
    "start": "2014120",
    "end": "2019200"
  },
  {
    "text": "large larger node would be a suitable work uh suitable uh choice so M1 extra large C1 extra large",
    "start": "2019200",
    "end": "2027559"
  },
  {
    "text": "uh anything in cluster compute families cc2 instances high1 instances hs1",
    "start": "2027559",
    "end": "2033720"
  },
  {
    "text": "instances all that would be suitable uh spot uh suitable or instances for",
    "start": "2033720",
    "end": "2039240"
  },
  {
    "text": "production workload if you have if you have a memory intensive and CPU intensive both",
    "start": "2039240",
    "end": "2046760"
  },
  {
    "text": "uh sort of constraints if you have both memory and CPU intensive workloads use cc2 instances they're great for",
    "start": "2046760",
    "end": "2054079"
  },
  {
    "text": "that uh cc2 and C1 extra large are great instances for CPU intensive",
    "start": "2054079",
    "end": "2061358"
  },
  {
    "text": "workloads hs1 instances these are the instances that I talked about which has 48 terabytes of spindles attach are",
    "start": "2061359",
    "end": "2069240"
  },
  {
    "text": "great for hdfs workload especially the pattern that we talked about where you copy data to hdfs First and do the",
    "start": "2069240",
    "end": "2077000"
  },
  {
    "text": "processing High one instances which comes with SSD and also hs1 instances",
    "start": "2077919",
    "end": "2083320"
  },
  {
    "text": "again are great for disk IO intensive this is not just to hold lot of data but also do a lot of IO uh to disk so if you",
    "start": "2083320",
    "end": "2091480"
  },
  {
    "text": "have that kind of workload hs1 and high1 instances would be great if you're using",
    "start": "2091480",
    "end": "2097040"
  },
  {
    "text": "M2 quadruple extra large instances today and you have a cluster of that you find",
    "start": "2097040",
    "end": "2102680"
  },
  {
    "text": "it more coste effective to use cc2 instances the interesting fact about that is that cc2 instances at M2",
    "start": "2102680",
    "end": "2109280"
  },
  {
    "text": "quadruple extra large are almost the same spec but the price per core for cc2",
    "start": "2109280",
    "end": "2114839"
  },
  {
    "text": "instances it's much better or it's less that M2 quadruple extra large so if",
    "start": "2114839",
    "end": "2120920"
  },
  {
    "text": "you're running M2 quadruple for large clusters you could be reducing cost on on your Hadoop or on your EMR",
    "start": "2120920",
    "end": "2126440"
  },
  {
    "text": "infrastructure if you use cc2 instances and this is sort of an",
    "start": "2126440",
    "end": "2132000"
  },
  {
    "text": "argument sometimes we get into would you get a larger cluster cluster of multiple",
    "start": "2132000",
    "end": "2137280"
  },
  {
    "text": "nodes like large number of nodes or would you shrink that to smaller amount",
    "start": "2137280",
    "end": "2142720"
  },
  {
    "text": "of smaller amount of notes but much bigger notes to basically scale up uh or",
    "start": "2142720",
    "end": "2148320"
  },
  {
    "text": "scale horizontally I tend to prefer scale up uh design pattern mostly",
    "start": "2148320",
    "end": "2154800"
  },
  {
    "text": "because let's just say instead of getting thousand M1 Smalls get let's just say 500 ccts then you're",
    "start": "2154800",
    "end": "2160839"
  },
  {
    "text": "maintaining less amount of nose you have to deal with less amount of failures and pretty much the maintenance is much",
    "start": "2160839",
    "end": "2166319"
  },
  {
    "text": "better on a smaller cluster but much bigger nose but again there's no bad and",
    "start": "2166319",
    "end": "2171520"
  },
  {
    "text": "good or bad pattern this is just what I prefer and what what I can suggest to you",
    "start": "2171520",
    "end": "2176599"
  },
  {
    "text": "guys all right this gets really interesting because once in a while I could ask well how many notes do I",
    "start": "2176599",
    "end": "2181960"
  },
  {
    "text": "need I typically say it depends on how much data you have and how fast data has",
    "start": "2181960",
    "end": "2188079"
  },
  {
    "text": "to be processed and some people don't like that answer but uh so let me give you a little bit",
    "start": "2188079",
    "end": "2195400"
  },
  {
    "text": "of insight into you know I'm Contra Contracting uh contradicting myself I I",
    "start": "2195400",
    "end": "2200520"
  },
  {
    "text": "told you I'm not going to give you how Hadoop works but I'll touch on that on this piece a little bit because it's",
    "start": "2200520",
    "end": "2205560"
  },
  {
    "text": "important because I later on I want to tell you how many notes you need so bear with me uh so let's get into some good stuff",
    "start": "2205560",
    "end": "2213839"
  },
  {
    "text": "uh so before we understand how many notes you need for a given job you have to understand what Hadoop does with your",
    "start": "2213839",
    "end": "2219720"
  },
  {
    "text": "with your data Hadoop does this thing which is splitting your data uh files to",
    "start": "2219720",
    "end": "2224960"
  },
  {
    "text": "multiple chunks so let's just say you have 128 228 megabyte files what Hado",
    "start": "2224960",
    "end": "2231960"
  },
  {
    "text": "will do before even processing your job is splitting that to multiple chunks and now the chunk size depends on you you",
    "start": "2231960",
    "end": "2238200"
  },
  {
    "text": "can configure that sometimes it's 64 megabytes uh it can be 128 megabytes it can be higher or lower depending on what",
    "start": "2238200",
    "end": "2244480"
  },
  {
    "text": "you configure K to do but let's just go with in this case I believe I'm going with 64 yeah 64 megabyte chunks chunk",
    "start": "2244480",
    "end": "2252079"
  },
  {
    "text": "size so if you have 2 128 megabyte files you get four",
    "start": "2252079",
    "end": "2257760"
  },
  {
    "text": "chunks those chunks becomes mappers and mappers are just a very simple",
    "start": "2257760",
    "end": "2263520"
  },
  {
    "text": "constructive uh processing work units so don't have to worry about exactly what mappers are but uh splits becomes",
    "start": "2263520",
    "end": "2270599"
  },
  {
    "text": "mappers and mappers will get assigned to instances or or your uh infrastructure",
    "start": "2270599",
    "end": "2277680"
  },
  {
    "text": "on premise infrastructure to servers to for for later on processing each instance on EMR has uh certain amount of",
    "start": "2277680",
    "end": "2287079"
  },
  {
    "text": "mapper capacity so for example M1 extra large can process eight uh I believe six",
    "start": "2287079",
    "end": "2292480"
  },
  {
    "text": "mappers or maybe eight mappers I forgot uh mappers in parallel at the same time",
    "start": "2292480",
    "end": "2298359"
  },
  {
    "text": "uh so in this case we have two instances that will run two mappers have a two mapper capacity each we have four",
    "start": "2298359",
    "end": "2303839"
  },
  {
    "text": "mappers who do will assign those four mappers uh two mappers to each instance",
    "start": "2303839",
    "end": "2309240"
  },
  {
    "text": "and run four of them all in parallel so more data you have that",
    "start": "2309240",
    "end": "2315720"
  },
  {
    "text": "means you have more splits means that you're generating more mappers now you",
    "start": "2315720",
    "end": "2321520"
  },
  {
    "text": "know this is a simple case but most use cases more data workloads you have ton of mappers it's not uncommon to have",
    "start": "2321520",
    "end": "2328800"
  },
  {
    "text": "hundred thousands of mappers for a given workflow work processing workflow so one",
    "start": "2328800",
    "end": "2335440"
  },
  {
    "text": "way to process that data is to get as many noes as possible to run all your mappers in parallel at the same time but",
    "start": "2335440",
    "end": "2342720"
  },
  {
    "text": "we're talking about 100,000 sometimes 200,000 mappers that's a large amount of nodes that you have to do all in",
    "start": "2342720",
    "end": "2348480"
  },
  {
    "text": "parallel but you don't have to do that what you could do is you could get certain amount of nodes just let's just",
    "start": "2348480",
    "end": "2353880"
  },
  {
    "text": "say two three four whatever number of nodes and have Hadoop uh cue the the",
    "start": "2353880",
    "end": "2359839"
  },
  {
    "text": "remaining uh mappers in Q and then send them to uh instances as capacity becomes",
    "start": "2359839",
    "end": "2365599"
  },
  {
    "text": "available so if you have four mappers uh for two nodes two mappers per each node for mapper capacity you have the",
    "start": "2365599",
    "end": "2372280"
  },
  {
    "text": "remaining uh mappers sitting in Q as the processing gets done in each uh instance",
    "start": "2372280",
    "end": "2377359"
  },
  {
    "text": "uh and it's and it's done map Hado will assign the mapper to the free resource",
    "start": "2377359",
    "end": "2382599"
  },
  {
    "text": "will do the job and keeps stuff getting stuff out of a queue for",
    "start": "2382599",
    "end": "2387760"
  },
  {
    "text": "processing but that also means that if you have more data more splits uh and you have don't have enough",
    "start": "2389920",
    "end": "2396920"
  },
  {
    "text": "instance is behind the scene to process the data for you what that means is that you have to wait for mappers to get",
    "start": "2396920",
    "end": "2402880"
  },
  {
    "text": "processed which means that adds latency to your data",
    "start": "2402880",
    "end": "2407200"
  },
  {
    "text": "workflow so if you want to speed that up you can add more instances so this is it's not a crazy MTH so if you need to",
    "start": "2408359",
    "end": "2414119"
  },
  {
    "text": "speed that up add more instances that means that less amount amount of mappers sit in Q that means that your job gets",
    "start": "2414119",
    "end": "2420079"
  },
  {
    "text": "processed much faster so before I can get into the rest",
    "start": "2420079",
    "end": "2425359"
  },
  {
    "text": "of the talk I we have to talk about splits as it happens to compress and uncompressed files uncompressed files",
    "start": "2425359",
    "end": "2432640"
  },
  {
    "text": "are easy to split for Hadoop so let's just say text files has a clear boundary Hadoop will uh split that file let's",
    "start": "2432640",
    "end": "2439640"
  },
  {
    "text": "just say 64 megabyte boundary or 128 megabyte boundary we'll split that up to multiple chunks so that's easy for",
    "start": "2439640",
    "end": "2445880"
  },
  {
    "text": "Hadoop to do if you have compressed files compressed files are not easy to uh to",
    "start": "2445880",
    "end": "2452520"
  },
  {
    "text": "split for that we have two different type of compressions splitable compressions and unsplittable",
    "start": "2452520",
    "end": "2458079"
  },
  {
    "text": "compressions for splitable compressions hadum knows how to split your file just",
    "start": "2458079",
    "end": "2464040"
  },
  {
    "text": "like to uncompressed file like text files it knows the boundaries and will split the file for",
    "start": "2464040",
    "end": "2469760"
  },
  {
    "text": "you but something like gzip compressed gzip gzs are not splitable meaning that",
    "start": "2469760",
    "end": "2475640"
  },
  {
    "text": "you have you can have two gab 2 gigabyte file 4 gigabyte file a petabyte file",
    "start": "2475640",
    "end": "2481280"
  },
  {
    "text": "Hadoop will treat that as a single mapper does not split that for you because it doesn't know the boundaries of data to split for you",
    "start": "2481280",
    "end": "2489240"
  },
  {
    "text": "so how many mappers you need I'm I'm I'm going through the workflow or or the thought process of how many mappers you",
    "start": "2491440",
    "end": "2497640"
  },
  {
    "text": "need for a given data uh data set so the number of mappers uh that job flow needs",
    "start": "2497640",
    "end": "2504920"
  },
  {
    "text": "depends how many files you have if your files are gzip so if for example if you have 10 gzip files that you need to",
    "start": "2504920",
    "end": "2510520"
  },
  {
    "text": "process that means 10 mappers and that becomes important in The Next Step which is by this one everyone says",
    "start": "2510520",
    "end": "2516720"
  },
  {
    "text": "just tell me tell me how many notes I need so so I'll give you",
    "start": "2516720",
    "end": "2522680"
  },
  {
    "text": "that all right so I came up with this way to do to estimate how many nodes you need uh it's not crazy math but uh based",
    "start": "2522680",
    "end": "2529640"
  },
  {
    "text": "on experience I found this to be pretty accurate uh again disclaimer you don't",
    "start": "2529640",
    "end": "2535000"
  },
  {
    "text": "have to come you can't come and chase me after this if it doesn't work for you but in most cases it should and and one",
    "start": "2535000",
    "end": "2540160"
  },
  {
    "text": "dependency it has is that uh it's going to assume that all your workflow uh workload files are uh even size meaning",
    "start": "2540160",
    "end": "2547920"
  },
  {
    "text": "that you have 100 megabyte files you know 200 megabyte files but not one 10 megab file and the other one you know",
    "start": "2547920",
    "end": "2554119"
  },
  {
    "text": "300 megab files so it's it's evenly distributed as far as file size goes so one what you have to do is estimate the",
    "start": "2554119",
    "end": "2559920"
  },
  {
    "text": "number of mappers your job requires and we want we went through that so if you have 10 gzip files that's",
    "start": "2559920",
    "end": "2566079"
  },
  {
    "text": "10 mappers so that's 10 mapper you need for for your job and I'll give you example after this pick an instance that",
    "start": "2566079",
    "end": "2573000"
  },
  {
    "text": "you want to run this workflow with uh let's just say M1 extra large and note down how many mappers it can process at",
    "start": "2573000",
    "end": "2579319"
  },
  {
    "text": "any given time so in this case M1 extra large can run eight mappers in parallel",
    "start": "2579319",
    "end": "2584680"
  },
  {
    "text": "note that down all right we need to run uh we need to pick sample data files from our from",
    "start": "2584680",
    "end": "2591720"
  },
  {
    "text": "from the large data set that we have and the number of files we want to pick is the number is the number that we note",
    "start": "2591720",
    "end": "2598800"
  },
  {
    "text": "down from the last step so last step was eight so we pick eight files just randomly pick any eight files for for",
    "start": "2598800",
    "end": "2605280"
  },
  {
    "text": "processing run an Amazon EMR cluster with a single core node of the instance that you pick",
    "start": "2605280",
    "end": "2612359"
  },
  {
    "text": "so EMR node uh M1 extra large pick that and run that cluster and run the data uh",
    "start": "2612359",
    "end": "2619000"
  },
  {
    "text": "run the processing of that data uh processing of the sample files that you picked from a previous step so one AMR",
    "start": "2619000",
    "end": "2626480"
  },
  {
    "text": "cluster with single node of M1 extra large run that on eight files and process your data and note down how long",
    "start": "2626480",
    "end": "2632839"
  },
  {
    "text": "it took to process that data how many needs you need is this",
    "start": "2632839",
    "end": "2638359"
  },
  {
    "text": "math total number of mappers times time to process the sample",
    "start": "2638359",
    "end": "2643680"
  },
  {
    "text": "files the mapper capacity of M1 extra large in this case 8 for example times",
    "start": "2643680",
    "end": "2649240"
  },
  {
    "text": "the desired processing time this is not the processing time of the sample files this is the processing desired",
    "start": "2649240",
    "end": "2655040"
  },
  {
    "text": "processing time for the entire data set let's go through an example to make it",
    "start": "2655040",
    "end": "2661000"
  },
  {
    "text": "clear all right let's just say that I have looked at my data set I have 150 gzip that means that that's 150 mappers",
    "start": "2661839",
    "end": "2669040"
  },
  {
    "text": "that I have to process let's just say that I picked in1 extra large which has eight mapper capacity I note that",
    "start": "2669040",
    "end": "2677440"
  },
  {
    "text": "down because of a mapper capacity the number was eight so I pick eight files",
    "start": "2677440",
    "end": "2683240"
  },
  {
    "text": "for for sample test I process my data it takes three",
    "start": "2683240",
    "end": "2688960"
  },
  {
    "text": "minutes to process this data on eight files it takes three",
    "start": "2688960",
    "end": "2694359"
  },
  {
    "text": "minutes so the trick here here is that I need this job 150 files to be processed",
    "start": "2694359",
    "end": "2700839"
  },
  {
    "text": "in 5 minutes so the math looks something like this 150 mappers eight of them",
    "start": "2700839",
    "end": "2706480"
  },
  {
    "text": "takes 3 minutes so 150 times 3 minutes I have eight mapper capacity for given",
    "start": "2706480",
    "end": "2711920"
  },
  {
    "text": "instance and need that to be processed in five minutes if I do the math I need",
    "start": "2711920",
    "end": "2717119"
  },
  {
    "text": "11 M1 extra large to process data in five minutes and I've done this test for the",
    "start": "2717119",
    "end": "2723559"
  },
  {
    "text": "most part is accurate",
    "start": "2723559",
    "end": "2727200"
  },
  {
    "text": "all right so best practices on uh file sizes avoid small fils at all costs and",
    "start": "2728599",
    "end": "2736200"
  },
  {
    "text": "I consider anything less than 100 megabyte to be small I actually call this I don't know",
    "start": "2736200",
    "end": "2742760"
  },
  {
    "text": "lack of better term cancer of of of Big Data it it's really bad for Hadoop or",
    "start": "2742760",
    "end": "2747960"
  },
  {
    "text": "EMR processing to have small files and the reason for that is each mapper that comes up is a single jvm and now that's",
    "start": "2747960",
    "end": "2755280"
  },
  {
    "text": "not completely technically correct jvms can be reused between multiple mappers but for this sake of this example let's",
    "start": "2755280",
    "end": "2761599"
  },
  {
    "text": "just say that single mapper becomes single jvm when jvm comes up it needs the resources from memory perspective",
    "start": "2761599",
    "end": "2768319"
  },
  {
    "text": "from CPU perspective so when a jvm comes up and does a job it it it spawns up and",
    "start": "2768319",
    "end": "2774520"
  },
  {
    "text": "does a takes CPU and memory resources the smaller the files are these jvms",
    "start": "2774520",
    "end": "2780480"
  },
  {
    "text": "come up and process small amount of files and they go down and they they vanish you don't want that you want jvm",
    "start": "2780480",
    "end": "2786800"
  },
  {
    "text": "to to come up stay as long as possible process the data so you don't just waste CPU and memory resources for something",
    "start": "2786800",
    "end": "2793440"
  },
  {
    "text": "really small let me give you an example based on experience mappers or",
    "start": "2793440",
    "end": "2799960"
  },
  {
    "text": "jvms or even sometimes reducers take between one second or two second or three seconds to come up well let's just",
    "start": "2799960",
    "end": "2805839"
  },
  {
    "text": "go with two seconds that's that's a time that takes for it to spawn up uh and get",
    "start": "2805839",
    "end": "2811240"
  },
  {
    "text": "ready for data processing so if you have hund uh 10 terabytes of 100 megab",
    "start": "2811240",
    "end": "2818000"
  },
  {
    "text": "per each file so again you have total data set of 100 ter 10 terabytes each",
    "start": "2818000",
    "end": "2823079"
  },
  {
    "text": "file is about 100 megabytes that's about 100,000 mappers if each takes two",
    "start": "2823079",
    "end": "2828960"
  },
  {
    "text": "seconds to come up that's about 55 hours of CPU time now 55 hours of CPU time is",
    "start": "2828960",
    "end": "2835280"
  },
  {
    "text": "not much uh to for for humans that's really fast uh but compare that with 10",
    "start": "2835280",
    "end": "2842480"
  },
  {
    "text": "terabytes of a gig per each file size so you have 10 terabyt of total size size",
    "start": "2842480",
    "end": "2847640"
  },
  {
    "text": "of for to data set size and each file is about 1 GB that's 10,000 mappers time 2",
    "start": "2847640",
    "end": "2853960"
  },
  {
    "text": "seconds each that needs to come up and and do its job or actually just get spawn up that's 5 hours so 55 hours",
    "start": "2853960",
    "end": "2861720"
  },
  {
    "text": "versus 5 hours uh to put it in perspective 5 hours is probably 5 milliseconds 55 uh",
    "start": "2861720",
    "end": "2870119"
  },
  {
    "text": "hours is probably 50 seconds that's how you can compare these two so as as",
    "start": "2870119",
    "end": "2875520"
  },
  {
    "text": "bigger the files gets the the processing becomes more efficient as far as CPU and memory goes so definitely keep that in",
    "start": "2875520",
    "end": "2881520"
  },
  {
    "text": "keep that in mind so what is the best file size for uh for storing data on S3 would be about",
    "start": "2881520",
    "end": "2888400"
  },
  {
    "text": "one or two gigabytes and I'll tell you",
    "start": "2888400",
    "end": "2893078"
  },
  {
    "text": "why uh typically when when when in in hio community when you go around and talk about mappers and how long a mapper",
    "start": "2894520",
    "end": "2901040"
  },
  {
    "text": "should stay up they talk about 60 seconds at the minimum so when it when the uh when mapper comes up should stay",
    "start": "2901040",
    "end": "2907440"
  },
  {
    "text": "up for 60 seconds or more anything less would be waste of resources CPU and memory each mapper can download from S3",
    "start": "2907440",
    "end": "2915280"
  },
  {
    "text": "at about the rate of 10 megabyte to 15 megabyte per second and the reason for that is a single mapper is a single",
    "start": "2915280",
    "end": "2921200"
  },
  {
    "text": "thread to S3 and single thread to S3 can only do between 10 megabytes and 50 megabyte per second There's No Limit",
    "start": "2921200",
    "end": "2927920"
  },
  {
    "text": "limitation on aggregated number of threads so you can get thousands of threads and each can do about 10 megabytes per second there's no",
    "start": "2927920",
    "end": "2934280"
  },
  {
    "text": "limitation on that but single thread is limited to 10 me 10 megab per second to 15 megab per second so if your mapper",
    "start": "2934280",
    "end": "2940880"
  },
  {
    "text": "comes up does processing for 60 uh pulls data from s for 60 seconds and can do",
    "start": "2940880",
    "end": "2946640"
  },
  {
    "text": "between 10 to 15 megabytes per second that's about a so 160 seconds done 15 megabytes that's about a gigabytes per",
    "start": "2946640",
    "end": "2953720"
  },
  {
    "text": "file size that you like to have on a",
    "start": "2953720",
    "end": "2957240"
  },
  {
    "text": "stream well what if you have small files issues what if today you have uh files ranging from you know 5 megabytes to 10",
    "start": "2961720",
    "end": "2968599"
  },
  {
    "text": "megabytes smaller or even a little bit larger but it's it's it's kind of small for you what do you do what you can do",
    "start": "2968599",
    "end": "2975200"
  },
  {
    "text": "is you can use Sr CP and combine smaller files and aggregate them together Sr CP",
    "start": "2975200",
    "end": "2982000"
  },
  {
    "text": "I'm not if you're not if you're not familiar with that uh Hado comes with this CP this CP is distributed copy so",
    "start": "2982000",
    "end": "2987440"
  },
  {
    "text": "if you want to copy one data from one cluster to another cluster use the CP and the SCP will do map mappers and",
    "start": "2987440",
    "end": "2993559"
  },
  {
    "text": "reducers and we'll copy data in in a distributed fashion estr DCP does almost the same thing but",
    "start": "2993559",
    "end": "2999920"
  },
  {
    "text": "it provides you a lot of other functionalities one of them one functionality for estr CP is to provide",
    "start": "2999920",
    "end": "3007720"
  },
  {
    "text": "source and destination and also provide a group by pattern for estr CP to",
    "start": "3007720",
    "end": "3013520"
  },
  {
    "text": "combine everything that matches the regular expression together and give it a Target file size so what it will do is",
    "start": "3013520",
    "end": "3020280"
  },
  {
    "text": "it will'll get as much as file possible from S3 or hdfs or anywhere else combine them together until you get to the",
    "start": "3020280",
    "end": "3027079"
  },
  {
    "text": "Target size that you like to so in this case you have file sizes let's just say 5 Meg or 10 Meg or 20 meg sitting on S3",
    "start": "3027079",
    "end": "3033839"
  },
  {
    "text": "you want to combine that and make a bigger file out of it for processing on EMR just run a CP give it an argument",
    "start": "3033839",
    "end": "3040640"
  },
  {
    "text": "which would be Source uh Source S3 as a bucket and also destination maybe local hdfs if you're doing that pattern of",
    "start": "3040640",
    "end": "3047319"
  },
  {
    "text": "copying data to hdfs and then give it a group buy and a Target size that will do the job for you",
    "start": "3047319",
    "end": "3055520"
  },
  {
    "text": "compression compress as much as you can especially data setting setting in S3 if you're not com compressing data on S3 uh",
    "start": "3059040",
    "end": "3066440"
  },
  {
    "text": "you're just paying for uh paying additional S3 cost and that's that's not good so reduce your cost by uh",
    "start": "3066440",
    "end": "3073520"
  },
  {
    "text": "compressing data on S3 and also gives you uh the great benefit of reducing the amount of uh bandwidth that you incur",
    "start": "3073520",
    "end": "3080960"
  },
  {
    "text": "between S3 and mappers on EMR and also reduce the bandwidth which means that",
    "start": "3080960",
    "end": "3086079"
  },
  {
    "text": "your your speeding up your processing",
    "start": "3086079",
    "end": "3089520"
  },
  {
    "text": "time oh all right sorry duplicate slide let me go to the next one compress",
    "start": "3091440",
    "end": "3097160"
  },
  {
    "text": "mapper and reducer output this is the mapper this is the output where mapper does its job it's done and has to send",
    "start": "3097160",
    "end": "3103640"
  },
  {
    "text": "data to reducer so it has to write it to S to to local disk first and then",
    "start": "3103640",
    "end": "3108960"
  },
  {
    "text": "reducer will pick that up uh for you to reduce this iio definitely compress your",
    "start": "3108960",
    "end": "3114680"
  },
  {
    "text": "map R output uh uh uh mapper and reducer output we I think it's already by default compressed an EMR but if not or",
    "start": "3114680",
    "end": "3121480"
  },
  {
    "text": "if you've changed it make sure you compress that there are different compression",
    "start": "3121480",
    "end": "3127520"
  },
  {
    "text": "types that you can use they're not this is just not limited to this but these are you know very popular ones so gzip",
    "start": "3127520",
    "end": "3133559"
  },
  {
    "text": "LZ and snappy some of them are much faster to encode and decode some of them",
    "start": "3133559",
    "end": "3139599"
  },
  {
    "text": "are slower but give you much more space efficiency so but what that means is that in this case gzip can compress the",
    "start": "3139599",
    "end": "3146599"
  },
  {
    "text": "best but it's the slowest Snappy can uh doesn't compress as as as",
    "start": "3146599",
    "end": "3153000"
  },
  {
    "text": "good as gzip but it's much faster compressed and",
    "start": "3153000",
    "end": "3157920"
  },
  {
    "text": "decompressed so if you're sensitive as far as how fast you have to compress uh then you use something like",
    "start": "3160240",
    "end": "3167240"
  },
  {
    "text": "Snappy or LZ if you have large amount of uh data on S3 and you want to reduce",
    "start": "3167240",
    "end": "3172839"
  },
  {
    "text": "cost use something like gzip gzip gives you the best uh compression possible between all",
    "start": "3172839",
    "end": "3178839"
  },
  {
    "text": "compressions and if you don't care I mean if you really don't care at all pick gzip again because mostly gzip is",
    "start": "3178839",
    "end": "3185160"
  },
  {
    "text": "supported by all other platforms so if you have to move data between S3 to Red shift to Red shift to something else and",
    "start": "3185160",
    "end": "3191319"
  },
  {
    "text": "uh you don't want to get into situations where you're using some uh something like Snappy that is not supported by other platforms so picking gzip is the",
    "start": "3191319",
    "end": "3198599"
  },
  {
    "text": "safest way to go another thing srcp can do for you is",
    "start": "3198599",
    "end": "3205599"
  },
  {
    "text": "can change compress question for you uh so similar to a small file issue instead",
    "start": "3205599",
    "end": "3211839"
  },
  {
    "text": "of combining files you can say you can ask S3 CP to change the compression of the file so give it a workflow let's",
    "start": "3211839",
    "end": "3218960"
  },
  {
    "text": "just say you're running a cluster you want to use that cluster for S3 CP give it a source let's just say bucket and",
    "start": "3218960",
    "end": "3225680"
  },
  {
    "text": "give it a destination let's just say htfs and say that output codec of the my",
    "start": "3225680",
    "end": "3230920"
  },
  {
    "text": "file meaning the compression of that file has to be the lzo so if we read gzip from S3 we'll convert that to lzo",
    "start": "3230920",
    "end": "3238079"
  },
  {
    "text": "if we read anything else we'll convert that to LZ for",
    "start": "3238079",
    "end": "3242759"
  },
  {
    "text": "you all right controlling cost with EMR I'll go through this fast because it's really easy concept uh on we have",
    "start": "3243640",
    "end": "3249760"
  },
  {
    "text": "multiple pricing models you guys probably aware by now on demand pricing pay as you go spot Market I talked about",
    "start": "3249760",
    "end": "3255400"
  },
  {
    "text": "that a few slides back and Reserve instances Reserve instances is a way for you to up do an up upfront payment and",
    "start": "3255400",
    "end": "3262680"
  },
  {
    "text": "get a lower hourly cost on your instances and we have three models of the light utilization medium utilization",
    "start": "3262680",
    "end": "3268799"
  },
  {
    "text": "and heavy utilization uh heavy utilization being really good for stuff that you run 24/7 that's where you get",
    "start": "3268799",
    "end": "3275480"
  },
  {
    "text": "the best cost efficiency out of your uh Reserve instances so to give you example if you",
    "start": "3275480",
    "end": "3281680"
  },
  {
    "text": "have long running live clusters that runs 247 purchase heavy utilization uh",
    "start": "3281680",
    "end": "3287119"
  },
  {
    "text": "RIS uh for your ec2 instances that gives you a best uh reduction in",
    "start": "3287119",
    "end": "3292880"
  },
  {
    "text": "price now if you have sort of predictable workload but not but not",
    "start": "3292880",
    "end": "3299000"
  },
  {
    "text": "type of workload that runs 24/7 let's just say comes up at 9 let's just say 3 or 5:00 does his job and goes away just",
    "start": "3299000",
    "end": "3305640"
  },
  {
    "text": "let's just say transan clusters but you run that almost predictable meaning that you know you run at every n at nine at",
    "start": "3305640",
    "end": "3311240"
  },
  {
    "text": "three at five whatever use medialization in uh Reserve instances that gives you",
    "start": "3311240",
    "end": "3316359"
  },
  {
    "text": "the best cost efficiency for this workload and everything else everything else that is ad hoc someone just",
    "start": "3316359",
    "end": "3322160"
  },
  {
    "text": "randomly submits jobs and you're doing testing functional testing or anything else use either on demand pricing or",
    "start": "3322160",
    "end": "3328039"
  },
  {
    "text": "spot market pricing all right we're getting to some advanced stuff so let's let's go through",
    "start": "3328039",
    "end": "3335240"
  },
  {
    "text": "this uh this is Advanced optimization stage one is realizing if you even need to do",
    "start": "3335240",
    "end": "3341799"
  },
  {
    "text": "optimization uh one thing I always mention is the best optimization you could you can do is partition your data",
    "start": "3341799",
    "end": "3348880"
  },
  {
    "text": "uh in contrast if you were to put everything in S3 in one folder and we don't have a concept of folder on S3 but",
    "start": "3348880",
    "end": "3355000"
  },
  {
    "text": "let's just say you put it on in a folder and uh you process everything in that folder but let's just say you're only uh",
    "start": "3355000",
    "end": "3361680"
  },
  {
    "text": "interested in last 10 minutes 20 minutes last couple of days of that data versus everything in that folder is is for last",
    "start": "3361680",
    "end": "3368559"
  },
  {
    "text": "two or three weeks uh what Hado will do is we'll read all data for that location for that this Source location on S3 and",
    "start": "3368559",
    "end": "3376680"
  },
  {
    "text": "then will only spit out the result that you want for last two days that's not efficient you're reading data that you don't need and then you're just spitting",
    "start": "3376680",
    "end": "3383240"
  },
  {
    "text": "out the data that you need for last two days so partition your data based on date so if you're putting on St stuff on",
    "start": "3383240",
    "end": "3388920"
  },
  {
    "text": "on uh S3 partition that by date so you're only pulling data that you need for that given",
    "start": "3388920",
    "end": "3394680"
  },
  {
    "text": "date if you're processing data and you need to process that data in matter of minutes or seconds Hadoop is not good",
    "start": "3394680",
    "end": "3401319"
  },
  {
    "text": "for you use something like storm spark or red",
    "start": "3401319",
    "end": "3406318"
  },
  {
    "text": "shift we've already done ton of configuration optimization for you uh so if you're not sure if you haven't done",
    "start": "3406760",
    "end": "3413720"
  },
  {
    "text": "work with Hadoop and you're not comfortable with configuring Hadoop just let the configuration be and use the stock configuration that comes with Hado",
    "start": "3413720",
    "end": "3420720"
  },
  {
    "text": "with EMR and best optimization as I always tell my customer is just add a note",
    "start": "3420720",
    "end": "3428200"
  },
  {
    "text": "don't worry about getting five or 10% of tweaking or or configuring uh Hadoop M mappers or reducers to get five or 10%",
    "start": "3428200",
    "end": "3435480"
  },
  {
    "text": "that just a lot of waste of time for you just add one more node and that gives you the op the resources that you",
    "start": "3435480",
    "end": "3442280"
  },
  {
    "text": "need monitor your cluster of ganglia that comes as a boost strap action that gets automatically installed on your EMR",
    "start": "3442280",
    "end": "3448760"
  },
  {
    "text": "nodes and you can monitor anything from an aspect of CPU memory dis iio and all",
    "start": "3448760",
    "end": "3453799"
  },
  {
    "text": "that kind of stuff so definitely get that installed on your cluster so again you can monitor",
    "start": "3453799",
    "end": "3459319"
  },
  {
    "text": "everything from any any bottl like from memory CPU desk iio Network iio and these are the steps you want to",
    "start": "3459319",
    "end": "3466119"
  },
  {
    "text": "follow run the job and monitor your cluster with ganglia and look for bottlenecks such as CPU dis or",
    "start": "3466119",
    "end": "3473880"
  },
  {
    "text": "memory address that bottleneck with either fine-tuning the cluster or or changing your algorithms or changing the",
    "start": "3473880",
    "end": "3480400"
  },
  {
    "text": "way you do processing but what you would do is you would repeat this pattern after again and again and again until",
    "start": "3480400",
    "end": "3486640"
  },
  {
    "text": "you address that bottleneck so again go run the job monitor that find a bottleneck fine tune it do it again you",
    "start": "3486640",
    "end": "3493200"
  },
  {
    "text": "keep doing that until that bottleneck is gone so for example Network iio you",
    "start": "3493200",
    "end": "3499160"
  },
  {
    "text": "definitely want to watch Network this is the most important part of the EMR cluster U if you're hosting data on S3",
    "start": "3499160",
    "end": "3507119"
  },
  {
    "text": "each instance AWS instance especially the large one can do up to 600 megabytes",
    "start": "3507119",
    "end": "3512359"
  },
  {
    "text": "per second for example and one extra large cluster computes can do up to a gigabits or two gigabits per second to",
    "start": "3512359",
    "end": "3519039"
  },
  {
    "text": "S3 if you're not doing that much from a given instance now you're not going to get two gigabits or two gigabits that's",
    "start": "3519039",
    "end": "3524319"
  },
  {
    "text": "the max you can do usually you're bounded by CPU or memory or something else but if you're doing much lower than",
    "start": "3524319",
    "end": "3530119"
  },
  {
    "text": "that for example if in a cluster computer instance and you're doing 200 megabytes per second uh there's",
    "start": "3530119",
    "end": "3535799"
  },
  {
    "text": "something wrong you you got to fix that you you're you're wasting",
    "start": "3535799",
    "end": "3540960"
  },
  {
    "text": "resources so again if if you're using S3 as your data store watch ganglia watch your neck throughput make sure that",
    "start": "3543079",
    "end": "3549160"
  },
  {
    "text": "based on instance size you're saturating your neck and the way you usually saturate your neck is just add more",
    "start": "3549160",
    "end": "3554280"
  },
  {
    "text": "mappers to that instance and uh unfortunately I don't have time to go through all that but yes just to make sure you watch your neck through it and",
    "start": "3554280",
    "end": "3561039"
  },
  {
    "text": "if you're not saturating the neck uh change and configure Hadoop so you can have more mappers to pull data from S3",
    "start": "3561039",
    "end": "3567599"
  },
  {
    "text": "much faster again an example of that if this",
    "start": "3567599",
    "end": "3572720"
  },
  {
    "text": "is my job and I'm running on cluster compute uh the arrow points out to the",
    "start": "3572720",
    "end": "3577880"
  },
  {
    "text": "point of my point in my job processing workflow that I'm not running the cluster in efficient way uh you know at",
    "start": "3577880",
    "end": "3584440"
  },
  {
    "text": "200 megabyte that's great that's 200 megabyte per second times 8 that's almost 2 gigabytes per second but at the",
    "start": "3584440",
    "end": "3590319"
  },
  {
    "text": "lower Peak I'm doing almost uh what is that that's almost 200 megabytes per second uh",
    "start": "3590319",
    "end": "3596799"
  },
  {
    "text": "actually that's 50 30 or 40 megabytes per second that's that's slow I can definitely increase this and get get",
    "start": "3596799",
    "end": "3603039"
  },
  {
    "text": "this high up to 200 megabytes per second so that's something that you want to watch out for if you're using ganglio",
    "start": "3603039",
    "end": "3609440"
  },
  {
    "text": "CPU loation this is very very common that if you want monitoring ganglia you",
    "start": "3609440",
    "end": "3614680"
  },
  {
    "text": "see your cluster sitting idle I give you an example something like this you're running if this is very maybe not the",
    "start": "3614680",
    "end": "3621039"
  },
  {
    "text": "best example but you definitely see it in your cluster where uh you're running a job and clusters is running at 40% 50%",
    "start": "3621039",
    "end": "3628640"
  },
  {
    "text": "utilization uh this is cloud you want to get as much as utilization as you can from your instances so what you could do",
    "start": "3628640",
    "end": "3634480"
  },
  {
    "text": "is maybe you could you could increase the number of mappers for that for that instance or increase the number of reducers for that instance so and and",
    "start": "3634480",
    "end": "3641599"
  },
  {
    "text": "then shut down the additional uh instances in that cluster so you can Shuffle more data within that node to",
    "start": "3641599",
    "end": "3647480"
  },
  {
    "text": "bring up your utilization",
    "start": "3647480",
    "end": "3650838"
  },
  {
    "text": "higher and the last piece of disc iio limit the amount of dis iio you do you can was that we're using ganglia again",
    "start": "3654079",
    "end": "3660119"
  },
  {
    "text": "compression helps a lot and uh the other thing you can do is pay attention to the metrics that mappers and reducers spill",
    "start": "3660119",
    "end": "3668119"
  },
  {
    "text": "out uh so mappers have limited amount of memory to buffer data so when they read",
    "start": "3668119",
    "end": "3673760"
  },
  {
    "text": "they put in a buffer when they go out of buffer they go out of memory they write to disk what you want to do is you want",
    "start": "3673760",
    "end": "3679760"
  },
  {
    "text": "to avoid that so let me go back to the last step you want to avoid writing this to dis a lot and this these are the",
    "start": "3679760",
    "end": "3685680"
  },
  {
    "text": "method you get I'm sorry if it's too too small for you to read uh but it gives you a number of spilled records what you",
    "start": "3685680",
    "end": "3692000"
  },
  {
    "text": "want to do is you want to make sure you spilled records out of mappers so the ratio between mappers",
    "start": "3692000",
    "end": "3698720"
  },
  {
    "text": "spilled records and mappers output records the ratio is not much shouldn't be much higher than one so 1.2 1.3 is",
    "start": "3698720",
    "end": "3706160"
  },
  {
    "text": "still okay but if you're getting the ratio of two or three that means that you're writing excessively to disk and",
    "start": "3706160",
    "end": "3711359"
  },
  {
    "text": "you want to avoid that",
    "start": "3711359",
    "end": "3715078"
  },
  {
    "text": "so again this is one example I'm reading uh spilled records about 200 or 22 I'm",
    "start": "3716880",
    "end": "3722599"
  },
  {
    "text": "sorry I can't read it right now but let's just say 22 million records uh and then output records of from the mapper",
    "start": "3722599",
    "end": "3729279"
  },
  {
    "text": "is about 10 million so that that the ratio between those two is much higher than one so you definitely know you're",
    "start": "3729279",
    "end": "3735279"
  },
  {
    "text": "writing this to dis a lot what you want to do in this case is increase your memory for that mapper just go in and",
    "start": "3735279",
    "end": "3740880"
  },
  {
    "text": "the way you would do is this way uh iio sword megabyte is a configuration parameter that you want to work with and",
    "start": "3740880",
    "end": "3746720"
  },
  {
    "text": "increase that to something larger you definitely want to make sure you have enough jvm Heap size to do this uh but",
    "start": "3746720",
    "end": "3753400"
  },
  {
    "text": "if you do just increase this parameter to something a little bit larger so your mapper has much more buffer and you're",
    "start": "3753400",
    "end": "3759160"
  },
  {
    "text": "not writing to disc",
    "start": "3759160",
    "end": "3762119"
  },
  {
    "text": "on another thing to look for if you get a pattern like this a metric like this",
    "start": "3766880",
    "end": "3772039"
  },
  {
    "text": "where you have so much CPU dis IO this is an indication that you writing to hdfs a lot either you don't have enough",
    "start": "3772039",
    "end": "3778799"
  },
  {
    "text": "memory for mapper or even for reducer so CPU is waiting for dis a to happen so if",
    "start": "3778799",
    "end": "3784079"
  },
  {
    "text": "you see something like this in your in your cluster make sure you go back to the uh previous slides that I talked",
    "start": "3784079",
    "end": "3789520"
  },
  {
    "text": "about which is increasing the memory for mapper or even reducer in that",
    "start": "3789520",
    "end": "3795119"
  },
  {
    "text": "case and this is last slide I believe but just want to remind you keep doing",
    "start": "3795720",
    "end": "3800880"
  },
  {
    "text": "this go run the job monitor with ganglia CPU idle add more mappers if doing dis",
    "start": "3800880",
    "end": "3806279"
  },
  {
    "text": "iio compress and also make sure you increase your map or memory and uh keep doing that and fine-tune your uh",
    "start": "3806279",
    "end": "3812880"
  },
  {
    "text": "algorithm or cluster to get around all the bottlenecks thanks for joining I know I've went a little longer but",
    "start": "3812880",
    "end": "3818240"
  },
  {
    "text": "thanks for staying and have fun I hope you see you guys later tonight thank you",
    "start": "3818240",
    "end": "3825359"
  }
]