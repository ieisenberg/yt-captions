[
  {
    "start": "0",
    "end": "92000"
  },
  {
    "text": "alright let's go ahead and get started hey my name is Andrew I'm from Netflix Joe Shea ews Technical",
    "start": "30",
    "end": "7200"
  },
  {
    "text": "Account Manager alright so today we're gonna talk about running containers at",
    "start": "7200",
    "end": "12630"
  },
  {
    "text": "scale on at Netflix on top of Amazon ec2 the first question you may be asking is",
    "start": "12630",
    "end": "19320"
  },
  {
    "text": "why do we need containers when we already have this really great architecture you've probably heard of",
    "start": "19320",
    "end": "24750"
  },
  {
    "text": "that Netflix has that's based on virtual machines I mean it's very resilient it has worldwide failover it's certainly",
    "start": "24750",
    "end": "32340"
  },
  {
    "text": "cloud native we've been running on AWS for a very long time it's elastically scalable all the good",
    "start": "32340",
    "end": "37890"
  },
  {
    "text": "things right so did we really need containers well we found a few years ago",
    "start": "37890",
    "end": "43649"
  },
  {
    "text": "is yes we could use another tool in our tool but when it came to compute other than virtual machines and what we found",
    "start": "43649",
    "end": "50039"
  },
  {
    "text": "was containers gave us a better way to package applications specific artifacts",
    "start": "50039",
    "end": "55829"
  },
  {
    "text": "so while an OS level ami was a great tool for a pretty standard service in",
    "start": "55829",
    "end": "62550"
  },
  {
    "text": "our infrastructure that was Java services it really didn't fit the bill for people that want to customize it especially in polygons different",
    "start": "62550",
    "end": "69750"
  },
  {
    "text": "language environments we also wanted to be ability to do local developments so use the same container you're running",
    "start": "69750",
    "end": "75750"
  },
  {
    "text": "our laptop in production and be able to kind of move easily between the two the",
    "start": "75750",
    "end": "80759"
  },
  {
    "text": "other was really came from our batch side of the house not just our services was they just wanted a simpler way to",
    "start": "80759",
    "end": "86520"
  },
  {
    "text": "manage compute they wanted to say I need this much CPU I want this much memory and I want to get the job done so this",
    "start": "86520",
    "end": "93450"
  },
  {
    "start": "92000",
    "end": "92000"
  },
  {
    "text": "is where we introduced Titus Titus is the little logo you see on my shirt it's also the name of our container",
    "start": "93450",
    "end": "100140"
  },
  {
    "text": "management platform that we've built on top of AWS it does both service and",
    "start": "100140",
    "end": "105240"
  },
  {
    "text": "batch what I mean by service is typically micro services things that are run until they're redeployed for a newer",
    "start": "105240",
    "end": "111990"
  },
  {
    "text": "version batch or things that run to completion and we just make sure get to their end state correctly we do fleet",
    "start": "111990",
    "end": "118680"
  },
  {
    "text": "wide resource management off of fairly large fleet that you'll see and we also have integrator our container runtime",
    "start": "118680",
    "end": "125130"
  },
  {
    "text": "with not only our Netflix tools and infrastructure but also very deep AWS integration which is what we're going to",
    "start": "125130",
    "end": "131250"
  },
  {
    "text": "talk about today a little longer intro to myself I manage",
    "start": "131250",
    "end": "136740"
  },
  {
    "start": "133000",
    "end": "133000"
  },
  {
    "text": "the team of ten I was an engineer when the team was of size three became a manager a few years ago and the ten",
    "start": "136740",
    "end": "143220"
  },
  {
    "text": "people up here are the engineers that actually have designed Titus have",
    "start": "143220",
    "end": "148830"
  },
  {
    "text": "developed it and also operate and support it so we carry the pager for the service that runs these containers from",
    "start": "148830",
    "end": "156750"
  },
  {
    "text": "a product management perspective when we set out to build this thing we didn't focus on containers as I said earlier it",
    "start": "156750",
    "end": "163530"
  },
  {
    "text": "was really about making things easier for developers it was about velocity we",
    "start": "163530",
    "end": "168660"
  },
  {
    "text": "really didn't focus on cost efficiency we weren't trying to get a better bin packing than say virtual machines so our",
    "start": "168660",
    "end": "176190"
  },
  {
    "text": "most our highest party was developer velocity of course this is a key part of infrastructure at Netflix so reliability",
    "start": "176190",
    "end": "182760"
  },
  {
    "text": "was our second focus and then cost efficiency is something we getting into more recently as the years have gone on",
    "start": "182760",
    "end": "188480"
  },
  {
    "text": "we also wanted to continue to use virtual machines so it had to be a",
    "start": "188480",
    "end": "193680"
  },
  {
    "text": "platform that you could move seamlessly from a virtual machine into a container the virtual machines and containers",
    "start": "193680",
    "end": "199140"
  },
  {
    "text": "needed to continue to connect with each other it just had to work for those things and another thing is we wanted to",
    "start": "199140",
    "end": "205320"
  },
  {
    "text": "make it work for just what Netflix needed and have some special logic that works for Netflix you can read the",
    "start": "205320",
    "end": "213390"
  },
  {
    "text": "abstract but this is this is kind of why we see this container management platform is different than others other",
    "start": "213390",
    "end": "219390"
  },
  {
    "text": "than the fact it's special built for Netflix we really deeply leveraged ec2 we believe strongly in our partnership",
    "start": "219390",
    "end": "225690"
  },
  {
    "text": "with AWS and we see this as a differentiator of our container management platform versus others that",
    "start": "225690",
    "end": "231750"
  },
  {
    "text": "may try to abstract files we really try to leverage the underlying cloud at a",
    "start": "231750",
    "end": "238170"
  },
  {
    "start": "237000",
    "end": "237000"
  },
  {
    "text": "really high level this is our architecture so you'll well use these",
    "start": "238170",
    "end": "243180"
  },
  {
    "text": "terms as we go through the presentation so it's worth noting the control plane on the left that's where our API is",
    "start": "243180",
    "end": "249690"
  },
  {
    "text": "that's where a scheduler that does resource management is at and on the right are thousands of virtual machines",
    "start": "249690",
    "end": "255930"
  },
  {
    "text": "that are sliced up in containers we call those agents or hosts that's where the users containers are running we've built",
    "start": "255930",
    "end": "262260"
  },
  {
    "text": "this around May so's we've built this around docker and we is a WS to provide us the underlying",
    "start": "262260",
    "end": "268990"
  },
  {
    "text": "compute fabric on the left hand side are users of Titus or our deployment",
    "start": "268990",
    "end": "274120"
  },
  {
    "text": "platform for services something that's open source called spinnaker and many many batch and workflow systems that run",
    "start": "274120",
    "end": "280930"
  },
  {
    "text": "jobs in those containers as well so how do we leverage ec2 in this container",
    "start": "280930",
    "end": "287170"
  },
  {
    "text": "environment as I said earlier we really wanted to make it work very easy with",
    "start": "287170",
    "end": "293290"
  },
  {
    "start": "288000",
    "end": "288000"
  },
  {
    "text": "virtual machines and we wanted portability that meant three really key things it meant that we had to work with",
    "start": "293290",
    "end": "298750"
  },
  {
    "text": "a networking fabric VPC that's in Amazon we had to make sure that security groups",
    "start": "298750",
    "end": "304450"
  },
  {
    "text": "continue to work so we could do ingress control from containers to VMs and from the empty containers and that AWS",
    "start": "304450",
    "end": "311800"
  },
  {
    "text": "services that are based on I am continue to work natively and you didn't have to do anything custom in your application",
    "start": "311800",
    "end": "318570"
  },
  {
    "text": "so our key leverage points then are ec2 DPC and I am from an ec2 perspective we",
    "start": "318570",
    "end": "326860"
  },
  {
    "start": "319000",
    "end": "319000"
  },
  {
    "text": "use tens of p2 a tech cells pretty beefy for some of our GPU jobs then it gets to",
    "start": "326860",
    "end": "333970"
  },
  {
    "text": "our batch environment where we're using memory optimized instances and we use",
    "start": "333970",
    "end": "339160"
  },
  {
    "text": "hundreds of our 460 neck cells and then if you get to our general services fabric we do thousands of m416 excels",
    "start": "339160",
    "end": "350039"
  },
  {
    "text": "they're real from a V PC and security group perspective we've written a network driver that integrates the whole",
    "start": "352170",
    "end": "360220"
  },
  {
    "text": "big box is an ec2 VM the sliced up into three containers here and we wrote a networking driver that would provision",
    "start": "360220",
    "end": "367150"
  },
  {
    "text": "secondary eni selects elastic networking interfaces that have security group",
    "start": "367150",
    "end": "372340"
  },
  {
    "text": "configured and then we do Linux kernel work and and and traffic control work to",
    "start": "372340",
    "end": "377680"
  },
  {
    "text": "get the IP traffic into these hosts that are the container instances that are",
    "start": "377680",
    "end": "382900"
  },
  {
    "text": "inside of this host and do quoting and quality of service bandwidth rate",
    "start": "382900",
    "end": "388600"
  },
  {
    "text": "limiting those kind of things that you would expect from a networking perspective but by the fact that we leveraged VPC directly we just get",
    "start": "388600",
    "end": "395200"
  },
  {
    "text": "security groups sort of for free and we continue the security model from a networking perspective from an IM perspective we",
    "start": "395200",
    "end": "403210"
  },
  {
    "text": "have what on a ec2 instance you have an instance metadata service that's on 169",
    "start": "403210",
    "end": "409390"
  },
  {
    "text": "to 54 169 to 54 we impersonate that inside of the container so when a",
    "start": "409390",
    "end": "416110"
  },
  {
    "text": "container starts up we'll inject a metadata proxy into its networking path and when it asked hey what host of my",
    "start": "416110",
    "end": "421660"
  },
  {
    "text": "own it'll get a containerized view as opposed to the host view when it asks things that are specific to VMs we'll",
    "start": "421660",
    "end": "428470"
  },
  {
    "text": "say yeah that doesn't apply in this case and really importantly when it asks for what are my IM credentials what are my",
    "start": "428470",
    "end": "434380"
  },
  {
    "text": "instance level credentials we will on behalf of that container assume role into that role get the credentials and",
    "start": "434380",
    "end": "441100"
  },
  {
    "text": "pass them back to the container so now that container can continue to work with the iam services that I had before",
    "start": "441100",
    "end": "448050"
  },
  {
    "start": "448000",
    "end": "448000"
  },
  {
    "text": "another place of security you'll see security security security through this and how we're leveraging ec2 for that is",
    "start": "448050",
    "end": "454630"
  },
  {
    "text": "cryptographic identity so if you want to put secrets into your containers we actually leverage the instance metadata",
    "start": "454630",
    "end": "462360"
  },
  {
    "text": "and kms in the future services that already exist in Amazon to prove two",
    "start": "462360",
    "end": "467950"
  },
  {
    "text": "things one the tightest host that's launching this container can prove to our Netflix infrastructure that is it is",
    "start": "467950",
    "end": "474580"
  },
  {
    "text": "who it says it is based on the fact that was launched from ec2 and spinnaker which is our deployment system as its",
    "start": "474580",
    "end": "481450"
  },
  {
    "text": "launching jobs and titus can sign those jobs and say this job was submitted by someone that is trusted and is who they",
    "start": "481450",
    "end": "487780"
  },
  {
    "text": "say it is that lets us put one and two together and call our internal security",
    "start": "487780",
    "end": "493150"
  },
  {
    "text": "services and get the identity for this container and prove it based on ec2 s",
    "start": "493150",
    "end": "499330"
  },
  {
    "text": "security model so with that that's the basics I want to turn it over to Joe to",
    "start": "499330",
    "end": "505360"
  },
  {
    "text": "talk about how we started to leverage Amazon more deeply so I'm part of the",
    "start": "505360",
    "end": "512200"
  },
  {
    "text": "enterprise support team has a Technical Account Manager Technical Account",
    "start": "512200",
    "end": "517539"
  },
  {
    "text": "managers are the designated technical point of contact for most customers and",
    "start": "517540",
    "end": "525150"
  },
  {
    "text": "we're expected to be a virtual part of the customers team",
    "start": "525540",
    "end": "531770"
  },
  {
    "text": "and we need to be as proactive as possible for customers operational needs",
    "start": "531770",
    "end": "537160"
  },
  {
    "text": "so as a team we have many roles and we",
    "start": "537160",
    "end": "542480"
  },
  {
    "text": "have many jobs to do however our number one job is customer success and our",
    "start": "542480",
    "end": "549140"
  },
  {
    "text": "primary role is to be the customer advocate within AWS so Tam's like to",
    "start": "549140",
    "end": "558850"
  },
  {
    "text": "emphasize the technical part of their title we're builders we loved we loved",
    "start": "558850",
    "end": "565520"
  },
  {
    "text": "technical things and we loved solving problems we come when we come in many",
    "start": "565520",
    "end": "571339"
  },
  {
    "text": "many technical flavours unfortunately I do not come from a container flavor my",
    "start": "571339",
    "end": "578029"
  },
  {
    "text": "builder experience is data centers so",
    "start": "578029",
    "end": "583510"
  },
  {
    "text": "but Netflix had reached a point in the tightest project to where they need a",
    "start": "584770",
    "end": "590690"
  },
  {
    "text": "deeper collaboration with AWS and so for me it was an interesting journey just to",
    "start": "590690",
    "end": "597290"
  },
  {
    "text": "see how this seemingly misplaced - Infrastructure guy was able to help Netflix build a be able to bring",
    "start": "597290",
    "end": "605029"
  },
  {
    "text": "containers in as a first-class citizen into their environment so talking about",
    "start": "605029",
    "end": "615140"
  },
  {
    "text": "places where we could really leverage and think through things with Amazon the first is auto scaling so we always had",
    "start": "615140",
    "end": "623600"
  },
  {
    "start": "622000",
    "end": "622000"
  },
  {
    "text": "the ability to auto scale the entire tightest fleet all the resources that were in the fleet through native a",
    "start": "623600",
    "end": "629480"
  },
  {
    "text": "Tobias auto scaling but what we lacked was the ability to auto scale clusters of containers that were inside of our",
    "start": "629480",
    "end": "636589"
  },
  {
    "text": "users applications and we had a couple choices in front of us one we had other infrastructure teams at Netflix that had",
    "start": "636589",
    "end": "642950"
  },
  {
    "text": "written their own auto scaling engines the fact was they were great but they",
    "start": "642950",
    "end": "648410"
  },
  {
    "text": "were purpose-built they didn't have all the feature set that exists in the virtual machine Auto scale or the ASG",
    "start": "648410",
    "end": "654829"
  },
  {
    "text": "service and if we pick them up we're gonna have to operate them we're gonna have to own them the next obvious one",
    "start": "654829",
    "end": "661490"
  },
  {
    "text": "for most people at Netflix is oh we can write a another one right now instead of two we've got three",
    "start": "661490",
    "end": "667180"
  },
  {
    "text": "the reality is we would ended up at the same place we would have built something fairly special-purpose it wouldn't have the feature set",
    "start": "667180",
    "end": "673210"
  },
  {
    "text": "especially as Amazon continues to add features and we'd be chasing that feature set and again we'd have to",
    "start": "673210",
    "end": "679600"
  },
  {
    "text": "operate it the third choice was we could look for one from Amazon Web Services it",
    "start": "679600",
    "end": "686560"
  },
  {
    "text": "was you know there was already a great auto scaling engine that was continuing to advance its feature set in virtual",
    "start": "686560",
    "end": "692650"
  },
  {
    "text": "machines it was very feature-rich but it only worked with virtual machines that didn't work with containers so a",
    "start": "692650",
    "end": "699670"
  },
  {
    "text": "true story is I went down and talked to Joe I think it was actually a first introduction and I said you know I was",
    "start": "699670",
    "end": "706450"
  },
  {
    "start": "700000",
    "end": "700000"
  },
  {
    "text": "crazy idea like what if we could use the AWS auto scaling for tightest container",
    "start": "706450",
    "end": "712300"
  },
  {
    "text": "based applications and so I remember sitting on the other side of the table",
    "start": "712300",
    "end": "718330"
  },
  {
    "text": "in the break room I was really minding my own business and someone pings me and one of our liaisons pings me and says hey you need to meet",
    "start": "718330",
    "end": "724720"
  },
  {
    "text": "the container manager and so I sat down with Andrew and so we started discussing",
    "start": "724720",
    "end": "729940"
  },
  {
    "text": "his idea I started probing a little bit tried to understand what he was trying",
    "start": "729940",
    "end": "735070"
  },
  {
    "text": "to do because for me as a customer advocate I really need to understand not only the requirements but what they're",
    "start": "735070",
    "end": "742270"
  },
  {
    "text": "trying to achieve otherwise I cannot make a good case inside of Amazon without that without that knowledge so",
    "start": "742270",
    "end": "750030"
  },
  {
    "text": "toward the end after everything was forming in my head I thought this is a",
    "start": "750030",
    "end": "755320"
  },
  {
    "text": "really crazy idea but however I have a customer need in hand and I have a job",
    "start": "755320",
    "end": "761620"
  },
  {
    "text": "to do so I reached out to the product management team to to start sharing some",
    "start": "761620",
    "end": "768460"
  },
  {
    "text": "ideas with them and start collaborating with them and I came to find that the",
    "start": "768460",
    "end": "775330"
  },
  {
    "text": "auto scaling team already had something like this in design they are already thinking about this they already had a",
    "start": "775330",
    "end": "781390"
  },
  {
    "text": "customer use case for this but it wasn't fully fleshed out yet so they wanted to",
    "start": "781390",
    "end": "787990"
  },
  {
    "text": "partner deeply with that Netflix to make this a reality they really wanted to try to get they really wanted to work",
    "start": "787990",
    "end": "795740"
  },
  {
    "text": "this use case to see what a general-purpose auto-scaling API would look like to a customer as well as use",
    "start": "795740",
    "end": "803420"
  },
  {
    "text": "this as an opportunity to make a country to make a to put forward a contribution",
    "start": "803420",
    "end": "808610"
  },
  {
    "text": "to the open source world with the release of titus and after many after",
    "start": "808610",
    "end": "817280"
  },
  {
    "text": "many meetings midian durations multiple issues to work through we were able to",
    "start": "817280",
    "end": "824620"
  },
  {
    "text": "we were able to release application auto-scaling in july of this past year",
    "start": "824620",
    "end": "830030"
  },
  {
    "start": "826000",
    "end": "826000"
  },
  {
    "text": "and now anyone it can use this this new service to scale virtually any",
    "start": "830030",
    "end": "836540"
  },
  {
    "text": "application that they want and now andrew is gonna go over how netflix was",
    "start": "836540",
    "end": "842060"
  },
  {
    "text": "able to integrate with this service well thanks Joe so I said before most of",
    "start": "842060",
    "end": "849830"
  },
  {
    "start": "848000",
    "end": "848000"
  },
  {
    "text": "our users that are doing services that would be auto-scaling use spinnaker all of our users that were",
    "start": "849830",
    "end": "855890"
  },
  {
    "text": "using spinnaker already understood how to configure scaling policies with inside of our deployment system spinnaker they could do things like step",
    "start": "855890",
    "end": "863030"
  },
  {
    "text": "scaling they could do things like target tracking they could see how their metrics were working in cloud watch they",
    "start": "863030",
    "end": "869690"
  },
  {
    "text": "were seeing when alarms were being triggered and it could see when the scaling actions this is actually a",
    "start": "869690",
    "end": "876050"
  },
  {
    "text": "screenshot of it in the container environment it became exactly the same so now by picking Amazon's auto scaling",
    "start": "876050",
    "end": "882440"
  },
  {
    "text": "a service that was used for VMs and using this custom resources with",
    "start": "882440",
    "end": "887720"
  },
  {
    "text": "application auto scaling we could deliver the same exact experience to our container users when they wanted to auto",
    "start": "887720",
    "end": "892850"
  },
  {
    "text": "scale and how does this work under the covers so on the left hand side is spinnaker in the middle is the tightest",
    "start": "892850",
    "end": "899810"
  },
  {
    "start": "895000",
    "end": "895000"
  },
  {
    "text": "control plane and containers and on the right hand side are the Amazon services so when a user wants to do auto scaling",
    "start": "899810",
    "end": "906860"
  },
  {
    "text": "they configure an auto scaling policy and spinnaker that comes into the tightest control plane the tightest",
    "start": "906860",
    "end": "912380"
  },
  {
    "text": "control plane reaches out to Amazon auto scaling and puts a policy in place for auto scaling and then as user containers",
    "start": "912380",
    "end": "921440"
  },
  {
    "text": "are running along and emitting their metrics they're emitting those metrics out the cloud watch cloud watch alarms",
    "start": "921440",
    "end": "926960"
  },
  {
    "text": "are going off and the the key feature that application auto-scaling added here was the ability",
    "start": "926960",
    "end": "932720"
  },
  {
    "text": "for it to call back in to us as a customer to say hey this policy that is",
    "start": "932720",
    "end": "937760"
  },
  {
    "text": "you have an alarm configured on is now alarming adjust your instance count so it doesn't adjust the number of",
    "start": "937760",
    "end": "943730"
  },
  {
    "text": "containers are running in in tightest it tells us what we should do we pay attention to that and we start",
    "start": "943730",
    "end": "949579"
  },
  {
    "text": "containers when they're needed so the",
    "start": "949579",
    "end": "956120"
  },
  {
    "text": "next one I want to talk about was networking why don't you show us what you saw from a more proactive in key so",
    "start": "956120",
    "end": "966649"
  },
  {
    "text": "I started noticing these what I would consider unusual API calls from coming",
    "start": "966649",
    "end": "972410"
  },
  {
    "text": "from the Netflix account and just by looking at this I strongly suspected",
    "start": "972410",
    "end": "979940"
  },
  {
    "text": "that this was Titus but I knew that it wasn't scalable so in order to really",
    "start": "979940",
    "end": "988459"
  },
  {
    "start": "987000",
    "end": "987000"
  },
  {
    "text": "dig into why this is is we need to understand from an infrastructure point of view how we look at applications so",
    "start": "988459",
    "end": "996910"
  },
  {
    "text": "formally with spinnaker you would have an application landing on an instance",
    "start": "996910",
    "end": "1002920"
  },
  {
    "text": "so it was tightly coupled one-to-one and then they would use auto scaling to basically be the control plane of those",
    "start": "1002920",
    "end": "1009399"
  },
  {
    "text": "instances so in essence an auto scaling group would become the logical container",
    "start": "1009399",
    "end": "1014620"
  },
  {
    "text": "of an application so you could point to this auto scaling group and say this is",
    "start": "1014620",
    "end": "1019839"
  },
  {
    "text": "an application however in the tightest container world now you have a shared",
    "start": "1019839",
    "end": "1025720"
  },
  {
    "text": "fleet of instances and then within this fleet you now have multiple network",
    "start": "1025720",
    "end": "1033100"
  },
  {
    "text": "interfaces attached to this fleet and associated with this are dozens and",
    "start": "1033100",
    "end": "1039938"
  },
  {
    "text": "dozens of IP addresses and now from an infrastructure point of view your",
    "start": "1039939",
    "end": "1045069"
  },
  {
    "text": "application could be any group of these IP of these IP addresses",
    "start": "1045069",
    "end": "1053160"
  },
  {
    "text": "so in essence what happens is is that we",
    "start": "1059539",
    "end": "1065309"
  },
  {
    "start": "1065000",
    "end": "1065000"
  },
  {
    "text": "go from a single API call to now five",
    "start": "1065309",
    "end": "1071789"
  },
  {
    "text": "API calls so effectively of multiplying the number of mutating actions happening within the",
    "start": "1071789",
    "end": "1078149"
  },
  {
    "text": "account by five X just to scale your application and so we had to work pretty",
    "start": "1078149",
    "end": "1084539"
  },
  {
    "text": "closely with the ec2 control playing team to understand the impact of these types of actions and there and this type",
    "start": "1084539",
    "end": "1091889"
  },
  {
    "text": "of behavior and then Andrews gonna talk about why this became so important yes",
    "start": "1091889",
    "end": "1100049"
  },
  {
    "text": "so previously I talked about how we run out of we have a worldwide highly",
    "start": "1100049",
    "end": "1105389"
  },
  {
    "start": "1102000",
    "end": "1102000"
  },
  {
    "text": "available Netflix service we do this by running out of three regions of AWS so",
    "start": "1105389",
    "end": "1110730"
  },
  {
    "text": "we run out of us east 1u west one and us west - what that allows us to do is if",
    "start": "1110730",
    "end": "1117539"
  },
  {
    "text": "we deploy bad code in a region or we're having a problem that can't be remediated fairly quickly and the",
    "start": "1117539",
    "end": "1122940"
  },
  {
    "text": "Netflix customer is going to notice what we do is a Kong failover between regions",
    "start": "1122940",
    "end": "1128220"
  },
  {
    "text": "so what you'll see in this in this graphic is something happened bad in u.s. East one at the beginning at the at",
    "start": "1128220",
    "end": "1134100"
  },
  {
    "text": "the red arrow and what we did was in about five to seven minutes we can globally reallocate all of our traffic",
    "start": "1134100",
    "end": "1140820"
  },
  {
    "text": "to send half of that traffic at the US one and half of that traffic to us west",
    "start": "1140820",
    "end": "1146039"
  },
  {
    "text": "- it means an interesting thing is we have to start a lot of infrastructure",
    "start": "1146039",
    "end": "1151139"
  },
  {
    "text": "really quickly when that occurs in the U West one region it's a it's a challenge",
    "start": "1151139",
    "end": "1157559"
  },
  {
    "text": "to us as Joe was saying he was seeing these call behaviors but it's a challenge to us of we really have to get",
    "start": "1157559",
    "end": "1162929"
  },
  {
    "text": "that Savior region up and running really quickly this is launching thousands of containers in somewhere between the five",
    "start": "1162929",
    "end": "1169470"
  },
  {
    "text": "to seven minute range this is easy right containers are fast right no no offense",
    "start": "1169470",
    "end": "1176940"
  },
  {
    "text": "to these works they're they're great amount of work but they usually when you read the works about containers and how",
    "start": "1176940",
    "end": "1181950"
  },
  {
    "text": "fast they are they're usually on a single node or they're talking about a synthetic scheduling sort of bench",
    "start": "1181950",
    "end": "1187500"
  },
  {
    "text": "mark or they're certainly not working with cloud networking because as I as",
    "start": "1187500",
    "end": "1194430"
  },
  {
    "text": "Joe was showing not only do I have to start the container I have to pull the docker image so I have to do a whole",
    "start": "1194430",
    "end": "1200670"
  },
  {
    "text": "bunch of s3 access I also have to integrate with cloud networking so I have to start a whole bunch of IP",
    "start": "1200670",
    "end": "1205830"
  },
  {
    "text": "addresses that's something that's hard really hard to do really really really fast but with Amazon and Titus we can",
    "start": "1205830",
    "end": "1214350"
  },
  {
    "text": "now do this and I'll show you how we've changed Titus as well as how we've worked with Amazon to make this happen",
    "start": "1214350",
    "end": "1220460"
  },
  {
    "text": "the first I'm going to cover is our scheduling behavior the second I'm going to cover is what we do on each of the",
    "start": "1220460",
    "end": "1226260"
  },
  {
    "text": "agents from a networking perspective so Titus has a scheduling system that lays",
    "start": "1226260",
    "end": "1231450"
  },
  {
    "start": "1228000",
    "end": "1228000"
  },
  {
    "text": "out the resources within the cluster and during normal scheduling we do spread",
    "start": "1231450",
    "end": "1236480"
  },
  {
    "text": "scheduling as our algorithm so what that means is as an application starts we",
    "start": "1236480",
    "end": "1243180"
  },
  {
    "text": "will put in as many hosts as possible so here you can see application one came up it landed on three hosts ended up",
    "start": "1243180",
    "end": "1249210"
  },
  {
    "text": "starting up three in eyes and it started up three IP addresses to host that application also if application two",
    "start": "1249210",
    "end": "1256620"
  },
  {
    "text": "comes on we'll spread it out as wide as we possibly can as well and this really gives us the trade off the best trade",
    "start": "1256620",
    "end": "1262560"
  },
  {
    "text": "off for reliability when you're writing a scheduling system now when we get into",
    "start": "1262560",
    "end": "1267780"
  },
  {
    "start": "1267000",
    "end": "1267000"
  },
  {
    "text": "failover that case of being able to start all these containers really really fast we actually switch our scheduling",
    "start": "1267780",
    "end": "1273270"
  },
  {
    "text": "behavior to a more packing based scheduling behavior what this means is as we start up more of application one",
    "start": "1273270",
    "end": "1280710"
  },
  {
    "text": "we're gonna start up as many of them as possible on as few as instances if possible the docker image is already",
    "start": "1280710",
    "end": "1286680"
  },
  {
    "text": "there the networking interfaces are already there all we need to do is get IP addresses similarly with application",
    "start": "1286680",
    "end": "1293820"
  },
  {
    "text": "to will put it on as few as post as possible we rebalance this later but this is the way that we can get the most",
    "start": "1293820",
    "end": "1300270"
  },
  {
    "text": "quick work done within the overall system and this of course trades off for",
    "start": "1300270",
    "end": "1305490"
  },
  {
    "text": "speed so now getting back to Joe and what he was seeing well was that we do",
    "start": "1305490",
    "end": "1313170"
  },
  {
    "text": "some interesting things from the API perspective so first of all I said we had these networking",
    "start": "1313170",
    "end": "1318570"
  },
  {
    "text": "interfaces already existing I kind of lied a little bit what we do is we actually allocate them at boot time so",
    "start": "1318570",
    "end": "1325559"
  },
  {
    "text": "we will when we launch an instance we'll launch and attach as many en eyes as that host can support and we get that",
    "start": "1325559",
    "end": "1332009"
  },
  {
    "text": "out of the way so no longer do we make that API call both the create and attach the other",
    "start": "1332009",
    "end": "1337049"
  },
  {
    "text": "thing we do is what we call burstable allocation of IPs and we worked with",
    "start": "1337049",
    "end": "1342600"
  },
  {
    "text": "Amazon on this where we figure as I said I was gonna bin pack a whole bunch of application one onto one of the nodes",
    "start": "1342600",
    "end": "1348570"
  },
  {
    "text": "instead of asking for one IP address I asked for Ni P addresses with the theory is if I'm in failover if one containers",
    "start": "1348570",
    "end": "1355649"
  },
  {
    "text": "coming there's probably a whole bunch coming after me so we'll get all the IP addresses together if we overshoot no",
    "start": "1355649",
    "end": "1361950"
  },
  {
    "text": "big problem we'll just go ahead and garbage collect them later so Jo can",
    "start": "1361950",
    "end": "1367710"
  },
  {
    "text": "tell us what that saw what he saw after that on his side and we can see here that there was a huge improvement in",
    "start": "1367710",
    "end": "1373049"
  },
  {
    "text": "their call pattern here so by doing by really working with us and understanding",
    "start": "1373049",
    "end": "1378690"
  },
  {
    "text": "what the impact of these API calls are doing and basically designating what",
    "start": "1378690",
    "end": "1384750"
  },
  {
    "text": "they can do up front by pre allocating they've essentially freed up more of their bucket space to be used when they",
    "start": "1384750",
    "end": "1391559"
  },
  {
    "text": "really need it also the bow they they batched whenever they could any time they could use a batch API call",
    "start": "1391559",
    "end": "1399450"
  },
  {
    "text": "they did because if you can get one you can get two for the same price why not",
    "start": "1399450",
    "end": "1405509"
  },
  {
    "text": "right and also being very intelligent about their about when they can do their",
    "start": "1405509",
    "end": "1412740"
  },
  {
    "text": "API calls and when they're doing their kong's they need to be fast but otherwise go for availability and it was",
    "start": "1412740",
    "end": "1419820"
  },
  {
    "text": "super important for them to understand how there are API buckets worked to really optimize to get the speed that",
    "start": "1419820",
    "end": "1426269"
  },
  {
    "text": "they really needed and now that they're now with all their optimizations instead of worrying about five separate calls",
    "start": "1426269",
    "end": "1432600"
  },
  {
    "text": "trying to stomp on each other trying to compete with each other for resources now they only have to worry about one",
    "start": "1432600",
    "end": "1438179"
  },
  {
    "text": "and this is obviously far more scalable as well as far easier to predict and",
    "start": "1438179",
    "end": "1444690"
  },
  {
    "text": "project where you need to be in the future Thanks so taking this to the bottom line",
    "start": "1444690",
    "end": "1453470"
  },
  {
    "text": "how this was most important to us is results so this is one of our recent",
    "start": "1453470",
    "end": "1460159"
  },
  {
    "text": "fail overs into EU East one in this case in our production environment and you",
    "start": "1460159",
    "end": "1466759"
  },
  {
    "text": "can see that we started 7500 containers in about five minutes and this led us",
    "start": "1466759",
    "end": "1472009"
  },
  {
    "text": "satisfy those failover requirements that we had so this is real data is not a benchmark this is real-world production",
    "start": "1472009",
    "end": "1478490"
  },
  {
    "text": "usage so next I wanted to cover load",
    "start": "1478490",
    "end": "1484490"
  },
  {
    "text": "balancing this was the next place we started to look at so originally we were running containers and we were mostly",
    "start": "1484490",
    "end": "1490549"
  },
  {
    "start": "1487000",
    "end": "1487000"
  },
  {
    "text": "focused on what you guys know as our Netflix service environment what what is",
    "start": "1490549",
    "end": "1496340"
  },
  {
    "text": "Netflix comm and we've had a pretty advanced load balancing set of technologies for some time",
    "start": "1496340",
    "end": "1501889"
  },
  {
    "text": "both Zul and eureka which are open source and the way they work is at the",
    "start": "1501889",
    "end": "1507049"
  },
  {
    "text": "highest level we use route 53 geo-location dns to split between the",
    "start": "1507049",
    "end": "1512750"
  },
  {
    "text": "three regions of the world once we know that we're in a certain region we're going to go through the ELB ELB in this",
    "start": "1512750",
    "end": "1518509"
  },
  {
    "text": "case and then we're going to come down into Netflix infrastructure and at that point it will spider out using Eureka",
    "start": "1518509",
    "end": "1525590"
  },
  {
    "text": "based service discovery and RPC this was a really rock-solid service discovery",
    "start": "1525590",
    "end": "1532039"
  },
  {
    "text": "and RPC text set of technologies but it was really complex like if you were",
    "start": "1532039",
    "end": "1537740"
  },
  {
    "text": "participating in this environment you were a fairly beefy application that was part of our streaming control plane what",
    "start": "1537740",
    "end": "1544039"
  },
  {
    "text": "we found as we started to go out to other users inside of Netflix in our partner ecosystem in our content space",
    "start": "1544039",
    "end": "1550669"
  },
  {
    "text": "in our growing studio space they wanted a simple simpler solution for hosting",
    "start": "1550669",
    "end": "1556039"
  },
  {
    "text": "applications and doing load balancing they just wanted alb support and some and will be support as well this was a",
    "start": "1556039",
    "end": "1563570"
  },
  {
    "text": "problem for us initially with Amazon well we worked again together with Amazon on this",
    "start": "1563570",
    "end": "1569710"
  },
  {
    "start": "1565000",
    "end": "1565000"
  },
  {
    "text": "instead that the problem really was if you looked at NL B's and al B's previous",
    "start": "1569710",
    "end": "1575179"
  },
  {
    "text": "to the release of IP based application load balancing you could only tie in a",
    "start": "1575179",
    "end": "1580580"
  },
  {
    "text": "lb r and n lb to an instance as you know by watching the rest of this presentation we have many IP addresses on the same instance",
    "start": "1580580",
    "end": "1587600"
  },
  {
    "text": "and we wanted to actually attach the IP addresses not the instances and in fact",
    "start": "1587600",
    "end": "1592879"
  },
  {
    "text": "at reinvent I think it was two years ago we had a hallway conversation that was like what if we could do this and this",
    "start": "1592879",
    "end": "1600350"
  },
  {
    "text": "is what resulted so again showing you how it works again in spinnaker the same",
    "start": "1600350",
    "end": "1605809"
  },
  {
    "start": "1603000",
    "end": "1603000"
  },
  {
    "text": "sort of controls that you would be used to with a virtual machine you can define an alb or you can define an NLB and now",
    "start": "1605809",
    "end": "1613070"
  },
  {
    "text": "when you're setting up the target group that you connect it to instead of a cluster you connect it to an IP group",
    "start": "1613070",
    "end": "1619580"
  },
  {
    "text": "and that IP group gets attached to a tightest cluster how does this work under the covers",
    "start": "1619580",
    "end": "1624679"
  },
  {
    "start": "1624000",
    "end": "1624000"
  },
  {
    "text": "again the spinnaker Tytus AWS services across from left to right when somebody",
    "start": "1624679",
    "end": "1631849"
  },
  {
    "text": "configures this in spinnaker it creates an IP target group directly in the alb NLB control plane then the user",
    "start": "1631849",
    "end": "1640789"
  },
  {
    "text": "associates that target group with a container cluster that's running inside of titus and as ingress traffic is",
    "start": "1640789",
    "end": "1648320"
  },
  {
    "text": "coming in to these containers and containers are starting and stopping titus updates the IP target list that's",
    "start": "1648320",
    "end": "1655999"
  },
  {
    "text": "in that NLB and alb so now we can have elastic number of containers under that",
    "start": "1655999",
    "end": "1661789"
  },
  {
    "text": "IP target group and again only made possible with the additional addition of the IP target technology in it a LBNL",
    "start": "1661789",
    "end": "1669830"
  },
  {
    "text": "Nov so wrapping up talking about all",
    "start": "1669830",
    "end": "1675200"
  },
  {
    "text": "these features they've led us to some you know fairly significant usage of",
    "start": "1675200",
    "end": "1680389"
  },
  {
    "text": "containers at Netflix it was not a short journey I wonder if anybody's starting",
    "start": "1680389",
    "end": "1686690"
  },
  {
    "start": "1683000",
    "end": "1683000"
  },
  {
    "text": "their journey into containers don't believe that you're gonna snap your fingers and then overnight you're gonna have everything running in containers in",
    "start": "1686690",
    "end": "1692749"
  },
  {
    "text": "fact Netflix is nowhere near running everything in containers at this point it started at the end of 2015 where we",
    "start": "1692749",
    "end": "1699919"
  },
  {
    "text": "were mostly running the benefit of containers for those batch jobs that wanted simpler compute we waited until",
    "start": "1699919",
    "end": "1707320"
  },
  {
    "text": "around the early parts of 2016 even to get started with services they were",
    "start": "1707320",
    "end": "1712970"
  },
  {
    "text": "basic services they were internally facing they weren't production impacting and we learned in",
    "start": "1712970",
    "end": "1718890"
  },
  {
    "text": "that environment that's where we got our networking up and going or security models up and going then we moved on to",
    "start": "1718890",
    "end": "1724680"
  },
  {
    "text": "production services and what I mean by this is they were business impacting but they weren't Netflix customer impacting",
    "start": "1724680",
    "end": "1730770"
  },
  {
    "text": "where we worked there quite a bit was with our stream processing folks that were listening to events that if they",
    "start": "1730770",
    "end": "1738090"
  },
  {
    "text": "went down they could catch back up with about a four-hour delay we'd never wanted to go down because it definitely",
    "start": "1738090",
    "end": "1743280"
  },
  {
    "text": "had a business impact but if it went down the Netflix experience continued for our customers and towards the middle",
    "start": "1743280",
    "end": "1750600"
  },
  {
    "text": "of 2017 is where we first turned on our customer facing services and at this",
    "start": "1750600",
    "end": "1755790"
  },
  {
    "text": "point if you're using Netflix on pretty much any device you're going through some sort of container in tightest so",
    "start": "1755790",
    "end": "1762060"
  },
  {
    "text": "really hard core production uses but it took us the greater part of what three years two to two-and-a-half years we're",
    "start": "1762060",
    "end": "1770130"
  },
  {
    "text": "not just using it for services we're not just using it for batch we're doing GPU based workload as I talked about with",
    "start": "1770130",
    "end": "1776370"
  },
  {
    "text": "the p2s we're also running Netflix and coding so turning raw streams into the streams that you watch we're using it in",
    "start": "1776370",
    "end": "1784020"
  },
  {
    "text": "our studio technologies our CDN is something called open connect we do planning and monitoring of our global",
    "start": "1784020",
    "end": "1791060"
  },
  {
    "text": "CDN through Titus and we also do a lot of big data workloads so we do big data",
    "start": "1791060",
    "end": "1798030"
  },
  {
    "text": "in notebooks we do presto we do a whole bunch of other things within our container environment that come out of",
    "start": "1798030",
    "end": "1803160"
  },
  {
    "text": "our Big Data side and then finally you know things in our own backyard I'm part of productivity engineering another part",
    "start": "1803160",
    "end": "1809940"
  },
  {
    "text": "of that is our CI and our build systems we do massively parallel CI systems in",
    "start": "1809940",
    "end": "1815760"
  },
  {
    "text": "containers as well what this has led to is we were at tens of containers when we",
    "start": "1815760",
    "end": "1823920"
  },
  {
    "text": "started this I actually remember our monitoring of how we were doing in a container environment was an email we",
    "start": "1823920",
    "end": "1829770"
  },
  {
    "text": "would get daily that would tell us all the containers that launched within that day that didn't scale",
    "start": "1829770",
    "end": "1836610"
  },
  {
    "text": "and now we are hitting at peak three million containers within a week and this has been the growth as you can see",
    "start": "1836610",
    "end": "1843750"
  },
  {
    "text": "since the chart starts around beginning of 2016 you can see the growth pattern",
    "start": "1843750",
    "end": "1849990"
  },
  {
    "text": "as people have adopted this technology the other thing is it's some other data",
    "start": "1849990",
    "end": "1855810"
  },
  {
    "text": "points in terms of numbers we're running thousands of applications if you just measure it by container",
    "start": "1855810",
    "end": "1860880"
  },
  {
    "text": "images that are being launched in Titus everyday I talked about the the hundreds",
    "start": "1860880",
    "end": "1866910"
  },
  {
    "text": "and thousands of different are for s and M for s if you throw that all together",
    "start": "1866910",
    "end": "1872490"
  },
  {
    "text": "we're running about four hundred thirty five thousand V CPUs managed in this tightest system running these user",
    "start": "1872490",
    "end": "1878850"
  },
  {
    "text": "containers and I said we've hit three million peak containers we've actually launched",
    "start": "1878850",
    "end": "1885060"
  },
  {
    "text": "close to three quarters of a million within a day that's been our highest peak so far all working on Amazon the",
    "start": "1885060",
    "end": "1893730"
  },
  {
    "start": "1891000",
    "end": "1891000"
  },
  {
    "text": "other thing is this is open source we'd like to open source a lot of stuff at Netflix we open sources in April 2000 18",
    "start": "1893730",
    "end": "1900390"
  },
  {
    "text": "so if you want to go and read more you want to join a conversation about it understand it more if you got to Netflix",
    "start": "1900390",
    "end": "1906120"
  },
  {
    "text": "github io / Titus you can learn more about it we didn't do this to get a whole bunch of other people using our",
    "start": "1906120",
    "end": "1913140"
  },
  {
    "text": "open source technology we did this because we saw things like ECS we think saw things like Fargate we saw things",
    "start": "1913140",
    "end": "1918990"
  },
  {
    "text": "like kubernetes and eks growing and we wanted to make sure that our AWS integration and our ideas about how to",
    "start": "1918990",
    "end": "1925170"
  },
  {
    "text": "do things that the scale that we're doing made it into these other technologies and open source projects so",
    "start": "1925170",
    "end": "1932580"
  },
  {
    "start": "1932000",
    "end": "1932000"
  },
  {
    "text": "far I talked about what we're what we have done let me talk about where we're currently at and where we're going one",
    "start": "1932580",
    "end": "1940320"
  },
  {
    "text": "of the cool projects that's going on right now that I think we're waiting for after the holidays to deploy is we've",
    "start": "1940320",
    "end": "1946590"
  },
  {
    "text": "had a fairly sophisticated set of CP o quoting and CPU and memory isolation and",
    "start": "1946590",
    "end": "1952680"
  },
  {
    "text": "networking isolation but what we found was that basic CFS basic quoting in",
    "start": "1952680",
    "end": "1958050"
  },
  {
    "text": "container runtimes wasn't sufficient we found if we let the processes that were in containers float about across",
    "start": "1958050",
    "end": "1964050"
  },
  {
    "text": "packages within a CPU or across V CPUs or Piper threads versus main course that we",
    "start": "1964050",
    "end": "1970650"
  },
  {
    "text": "would get outliers in terms of what you saw from a compute perspective we also",
    "start": "1970650",
    "end": "1976980"
  },
  {
    "text": "didn't have the ability to let workloads sort of burst across the underutilized",
    "start": "1976980",
    "end": "1982200"
  },
  {
    "text": "resources within a single node and we're introducing a scheduler that's actually",
    "start": "1982200",
    "end": "1987990"
  },
  {
    "text": "down on each one of the agents that we'll watch as containers come in and for latency-sensitive workloads it's going to repack them onto the cores and",
    "start": "1987990",
    "end": "1994830"
  },
  {
    "text": "CPUs that give them the most benefit from Numa and processor caches and for",
    "start": "1994830",
    "end": "2000260"
  },
  {
    "text": "things that are opportunistic will let them burst across the remaining resources that aren't allocated to those latency-sensitive",
    "start": "2000260",
    "end": "2005900"
  },
  {
    "text": "so watch the open source for that as well taking this to the next higher",
    "start": "2005900",
    "end": "2011240"
  },
  {
    "text": "level I was talking about that for using compute on every individual node we're also looking at what our compute story",
    "start": "2011240",
    "end": "2017420"
  },
  {
    "text": "is across nodes we buy reserved instances to run this fleet with inside",
    "start": "2017420",
    "end": "2023240"
  },
  {
    "text": "of those reserved instances our users tell us how much capacity they need inside of the capacity they told us they",
    "start": "2023240",
    "end": "2029300"
  },
  {
    "text": "need they actually allocate that some of that capacity and then they actually use some of that capacity it means that",
    "start": "2029300",
    "end": "2036080"
  },
  {
    "text": "there's a whole bunch of capacity that we've paid for that no one's using because either people have overestimated",
    "start": "2036080",
    "end": "2041660"
  },
  {
    "text": "or we have failover key capacity that's not being used if we're not in a",
    "start": "2041660",
    "end": "2047030"
  },
  {
    "text": "failover and we're looking to introduce opportunistic workloads where we can co-locate our batch jobs that are bursty",
    "start": "2047030",
    "end": "2053929"
  },
  {
    "text": "into that trough of underutilized resources that our service jobs are opening up so a definite efficiency play",
    "start": "2053929",
    "end": "2060620"
  },
  {
    "text": "we're starting to move into as well another one the the final one I'll talk about is nitro and bare metal based",
    "start": "2060620",
    "end": "2068120"
  },
  {
    "text": "instances you heard that we're on m4s and our fours and we're on those because we've you know proved them to be very",
    "start": "2068120",
    "end": "2074210"
  },
  {
    "text": "reliable we've managed them very well we're starting to look at moving on to the m5s the our 5s and then bare metal",
    "start": "2074210",
    "end": "2080750"
  },
  {
    "text": "and we're doing this because we don't really need a hypervisor there we do a",
    "start": "2080750",
    "end": "2086360"
  },
  {
    "text": "lot of this low level performance instrumentation that you need for this CPU isolation and we think we can get",
    "start": "2086360",
    "end": "2092330"
  },
  {
    "text": "more out of bare metal that we can get out of the virtualized servers finally maybe we'll come back at the next",
    "start": "2092330",
    "end": "2097580"
  },
  {
    "text": "reinvent and talk about our next partnership what do you think I'll be Joe that's a good question but I mean I'll",
    "start": "2097580",
    "end": "2104390"
  },
  {
    "text": "tell you one thing it's been an amazing journey you know helping you guys build this thing and we really couldn't have",
    "start": "2104390",
    "end": "2110869"
  },
  {
    "text": "done it without you're just willing partnership and also the amazing support we got from the AWS service teams and",
    "start": "2110869",
    "end": "2117700"
  },
  {
    "text": "whatever it is I mean I really hope the first thing I think is that's a crazy idea it's really hmm all right thank you",
    "start": "2117700",
    "end": "2127609"
  },
  {
    "text": "I I should remind everyone to fill out",
    "start": "2127609",
    "end": "2135829"
  },
  {
    "text": "the session survey but other than that we can definitely take some questions",
    "start": "2135829",
    "end": "2141789"
  },
  {
    "text": "okay so the question was when we developed this how do we test it I'd",
    "start": "2153340",
    "end": "2159109"
  },
  {
    "text": "like to say that there's a really easy answer to that question but it's it's a very very involved answer so I'll start",
    "start": "2159109",
    "end": "2166520"
  },
  {
    "text": "at you know all the code bases involved of course have a certain level of testing from a unit system level testing",
    "start": "2166520",
    "end": "2173720"
  },
  {
    "text": "perspective as titus is developed and",
    "start": "2173720",
    "end": "2178880"
  },
  {
    "text": "features are on their way to production we actually have a staging environment that gets torn down constantly and then",
    "start": "2178880",
    "end": "2185119"
  },
  {
    "text": "stood up with the latest code runs it through system level tests and smoke tests and then as we roll out Titus",
    "start": "2185119",
    "end": "2191990"
  },
  {
    "text": "since there's thousands of hosts we no longer roll out to every host simultaneously we'll roll out at",
    "start": "2191990",
    "end": "2197630"
  },
  {
    "text": "partition at a time I kind of canary it and watch for signals that tell us how we're doing and we put all those things",
    "start": "2197630",
    "end": "2204170"
  },
  {
    "text": "together to have the maximum testability across the board",
    "start": "2204170",
    "end": "2209530"
  },
  {
    "text": "so the cross the crosses of the the fall question was what about the cross impact",
    "start": "2218680",
    "end": "2224320"
  },
  {
    "text": "between say Amazon and Netflix what we do is we spend a fairly long amount of",
    "start": "2224320",
    "end": "2231220"
  },
  {
    "text": "time in sort of the design process we try to get through most of the issues there but then we work together and we",
    "start": "2231220",
    "end": "2238600"
  },
  {
    "text": "will run it in one of our sort of test stacks or staging stacks for a certain amount of time before we'll roll it out",
    "start": "2238600",
    "end": "2243940"
  },
  {
    "text": "to production and we'll give feedback to Amazon as to what we're seeing and then",
    "start": "2243940",
    "end": "2249130"
  },
  {
    "text": "they'll make iterative changes on their side and we go back and forth good questions other questions",
    "start": "2249130",
    "end": "2257020"
  },
  {
    "text": "Andrew so so the question was how do we",
    "start": "2257020",
    "end": "2268240"
  },
  {
    "text": "monitor titus with the millions of containers so Netflix for some time has",
    "start": "2268240",
    "end": "2274690"
  },
  {
    "text": "had a internal system for telemetry called Atlas and we leverage that fairly heavily so for containers that want to",
    "start": "2274690",
    "end": "2283240"
  },
  {
    "text": "have their own telemetry and manage their own monitoring they just use the existing monitoring solutions that the",
    "start": "2283240",
    "end": "2289660"
  },
  {
    "text": "VMS would which was basically based on cloud watch Plus Atlas for Titus itself",
    "start": "2289660",
    "end": "2295950"
  },
  {
    "text": "again it's a fairly long answer just in that we have I think it's it's thousands",
    "start": "2295950",
    "end": "2305290"
  },
  {
    "text": "of metrics that come out of our control planes and agents it's hundreds of",
    "start": "2305290",
    "end": "2310720"
  },
  {
    "text": "dashboards it's probably tens of alerts and 3sl O's so each of those levels it",
    "start": "2310720",
    "end": "2319840"
  },
  {
    "text": "gets more and more coarse-grained so we usually are looking at our SL O's which are launched latency API availability",
    "start": "2319840",
    "end": "2325900"
  },
  {
    "text": "and crashes that are outside of the users control and that's what we're looking at at the highest level and then",
    "start": "2325900",
    "end": "2332470"
  },
  {
    "text": "if we have an issue we'll dive down to the alerts that are probably already firing that are telling us what's going",
    "start": "2332470",
    "end": "2337540"
  },
  {
    "text": "on and then we'll look down at the dashboards and metrics that are making up the overall system another question",
    "start": "2337540",
    "end": "2346920"
  },
  {
    "text": "so the question is why ECS did not work out for us so ECS was started a good bit after",
    "start": "2348290",
    "end": "2356760"
  },
  {
    "text": "Titus got started so that's the easy answer to the question the more complex",
    "start": "2356760",
    "end": "2362520"
  },
  {
    "text": "answer the question is we've got some fairly advanced scheduling technology",
    "start": "2362520",
    "end": "2368820"
  },
  {
    "text": "that is needed for someone at our level of scale and complexity if you're someone that's if you're someone that's",
    "start": "2368820",
    "end": "2375960"
  },
  {
    "text": "failing over you know hundreds of sorry hundreds of thousands of V CPUs you end",
    "start": "2375960",
    "end": "2383100"
  },
  {
    "text": "up with a slightly different scheduling than what you may with ecs that's probably a bit more user-friendly than",
    "start": "2383100",
    "end": "2388920"
  },
  {
    "text": "say the titus environment so I'd say it's a different business focus as well",
    "start": "2388920",
    "end": "2394560"
  },
  {
    "text": "as we just got started earlier",
    "start": "2394560",
    "end": "2398030"
  },
  {
    "text": "so the question may be to make sure I got the right question the question was for some of the big data workloads",
    "start": "2418869",
    "end": "2424640"
  },
  {
    "text": "specifically flink and sparked how do we manage sort of those clusters within our",
    "start": "2424640",
    "end": "2429710"
  },
  {
    "text": "clusters yes good question so titus is staying true probably to the product",
    "start": "2429710",
    "end": "2436610"
  },
  {
    "text": "vision another thing I would have mentioned in there is we don't want to be too much we just want to be a cluster",
    "start": "2436610",
    "end": "2442820"
  },
  {
    "text": "manager an instance manager for container based applications we're perfectly fine if someone then builds a",
    "start": "2442820",
    "end": "2448640"
  },
  {
    "text": "higher-level framework that manages those clusters on top of us and in fact that's what flink and spark has done so",
    "start": "2448640",
    "end": "2455030"
  },
  {
    "text": "flink I know the best has rewritten sort of the resource manager layer off link to call in to us and instead of working",
    "start": "2455030",
    "end": "2462650"
  },
  {
    "text": "within its clusters it just defines clusters against Titus and then it slices those up to run the flink jobs",
    "start": "2462650",
    "end": "2468110"
  },
  {
    "text": "however it sees fit we don't know that flink is running as a framework on top of us we just know that",
    "start": "2468110",
    "end": "2473180"
  },
  {
    "text": "someone's starting up clusters that flink is slicing up into its compute workload that is actively running on us",
    "start": "2473180",
    "end": "2479930"
  },
  {
    "text": "that's actually one of our largest set of services that is running on top of Titus the SPARC they've done the same",
    "start": "2479930",
    "end": "2485150"
  },
  {
    "text": "thing have recoded the underlying layers of spark that hasn't quite and got there yet because if you look at the pull size",
    "start": "2485150",
    "end": "2491450"
  },
  {
    "text": "of Titus versus the full size of our spark fleet it's much much bigger so they're waiting for us to get a little",
    "start": "2491450",
    "end": "2497450"
  },
  {
    "text": "bit bigger to kind of dip their toes in our bigger pool what's that yes yeah",
    "start": "2497450",
    "end": "2507650"
  },
  {
    "text": "yeah we're definitely we run flink all day long any quick those are one in was there any",
    "start": "2507650",
    "end": "2515930"
  },
  {
    "text": "on that side the lights are blinding",
    "start": "2515930",
    "end": "2520540"
  },
  {
    "text": "yeah let me let me throw that up it's probably easier to have the picture of",
    "start": "2521530",
    "end": "2529300"
  },
  {
    "text": "and and I should mention the network driver is in the open source if you want",
    "start": "2531010",
    "end": "2536360"
  },
  {
    "text": "to get even more details than I'm about ready to tell you you're more than welcome to look there right so what the",
    "start": "2536360",
    "end": "2545960"
  },
  {
    "start": "2544000",
    "end": "2544000"
  },
  {
    "text": "networking driver does is there's an en I that comes off of the host that is for the control plane of Titus in general so",
    "start": "2545960",
    "end": "2552470"
  },
  {
    "text": "that's en I zero we the networking driver will then say as containers launch on this host they're going to be",
    "start": "2552470",
    "end": "2559700"
  },
  {
    "text": "told they're gonna tell the host what security group they need to be in what we'll do is we'll say is there a en I",
    "start": "2559700",
    "end": "2566630"
  },
  {
    "text": "already with that security group configured if it's not will actually reconfigure the en I for that security",
    "start": "2566630",
    "end": "2572570"
  },
  {
    "text": "group and allocate an IP address a secondary IP address so what you can see",
    "start": "2572570",
    "end": "2577640"
  },
  {
    "text": "hanging off of these en is is in this case there's two containers that are with security group W they're both would",
    "start": "2577640",
    "end": "2585020"
  },
  {
    "text": "be on en i1 and it would set up ei I P 1 and IP 2 off of en i1 and does all that",
    "start": "2585020",
    "end": "2591170"
  },
  {
    "text": "configuration and then routes that traffic into the container the other thing it does that I think is really",
    "start": "2591170",
    "end": "2597440"
  },
  {
    "text": "important that a lot of networking drivers aren't quite doing yet is it classifies the traffic with BPF and then",
    "start": "2597440",
    "end": "2604970"
  },
  {
    "text": "it pushes it through and what's on getting deep at this point about ready to go under my level of expertise but it",
    "start": "2604970",
    "end": "2611480"
  },
  {
    "text": "goes into a set of intermediate function block devices that are then configured through a hierarchical token bucket that",
    "start": "2611480",
    "end": "2617270"
  },
  {
    "text": "we can apply quota and fair share across the bandwidth so we can actually do bandwidth isolation as part of our",
    "start": "2617270",
    "end": "2623150"
  },
  {
    "text": "networking driver as well was there more to the question",
    "start": "2623150",
    "end": "2629599"
  },
  {
    "text": "yeah yes yep",
    "start": "2629599",
    "end": "2644219"
  },
  {
    "text": "what would cause that to happen so the question was what when will we fail over what causes us to",
    "start": "2644219",
    "end": "2651119"
  },
  {
    "text": "fail over from region to region so there's essentially two cases one happens a lot more frequent than the",
    "start": "2651119",
    "end": "2656940"
  },
  {
    "text": "other I'll start with the one that happens less frequently but it's still worth calling out sometimes there's elevated API error rates in an AWS",
    "start": "2656940",
    "end": "2663509"
  },
  {
    "text": "region and when we see that and we don't have a clue from Joe as to when they're running to recover we'd rather get out",
    "start": "2663509",
    "end": "2670109"
  },
  {
    "text": "of the region let them heal themselves and then get back into the region later the second case that happens far more",
    "start": "2670109",
    "end": "2675960"
  },
  {
    "text": "often I think it's like it's probably 20 to 1 far more often is someone deploys",
    "start": "2675960",
    "end": "2681539"
  },
  {
    "text": "bad code in part of the Netflix ecosystem and we can't quickly enough",
    "start": "2681539",
    "end": "2686849"
  },
  {
    "text": "roll back we can't diagnose the problem or we can't remediate the problem we",
    "start": "2686849",
    "end": "2692400"
  },
  {
    "text": "know that we can get all the traffic redistributed with full service within 5 to 7 minutes so the rule of thumb is if",
    "start": "2692400",
    "end": "2699329"
  },
  {
    "text": "you're a service team and you own your service much like Titus if we if we mess up Titus and we've done it twice",
    "start": "2699329",
    "end": "2705440"
  },
  {
    "text": "if you can't confirm to the core team that within 5 to 7 minutes you're gonna",
    "start": "2705440",
    "end": "2710489"
  },
  {
    "text": "be able to remediate that problem they'll start failing the traffic over to another region and then once it's",
    "start": "2710489",
    "end": "2715680"
  },
  {
    "text": "recovered we'll fail it back in it's a it's all automated but it takes a human",
    "start": "2715680",
    "end": "2721890"
  },
  {
    "text": "to kick it off because we don't want to failing that much traffic around the",
    "start": "2721890",
    "end": "2726989"
  },
  {
    "text": "world is a risky thing as well so you don't want to be going into that all the",
    "start": "2726989",
    "end": "2732029"
  },
  {
    "text": "time without having a human involved but once once a human makes the decision a whole bunch of the processes are",
    "start": "2732029",
    "end": "2737940"
  },
  {
    "text": "automated of how we scale up the capacity including Titus",
    "start": "2737940",
    "end": "2742818"
  },
  {
    "text": "yes so the question was with the orchestration technologies that have been growing what would we be doing",
    "start": "2754160",
    "end": "2761130"
  },
  {
    "text": "differently I think the biggest one that I would call out is right now titus was",
    "start": "2761130",
    "end": "2768180"
  },
  {
    "text": "written that our job management and our instance management were tightly coupled",
    "start": "2768180",
    "end": "2773490"
  },
  {
    "text": "so when you're a user of titus you say I want to start a service job with min/max desired equals XYZ or I want to run a",
    "start": "2773490",
    "end": "2780930"
  },
  {
    "text": "batch job with partitions 10 what that happens under the cover is we will start",
    "start": "2780930",
    "end": "2787080"
  },
  {
    "text": "that job well hydrate that job will start your all your containers and we manage that as a as a deep sort of",
    "start": "2787080",
    "end": "2794670"
  },
  {
    "text": "highly coupled environment I think what we've seen through kubernetes and actually ec2 before it separating the",
    "start": "2794670",
    "end": "2802020"
  },
  {
    "text": "concept of instance management versus cluster management is one of the things I think in Titus not only would we do",
    "start": "2802020",
    "end": "2809400"
  },
  {
    "text": "differently but we'll probably do differently in 2019 there's probably much more of a longer",
    "start": "2809400",
    "end": "2815280"
  },
  {
    "text": "list there I think the other thing is really and and we've been doing this by the fact that we open sourced we wish we",
    "start": "2815280",
    "end": "2821849"
  },
  {
    "text": "had more time to open source sooner but being able to get our technologies out there so others could you know absorb",
    "start": "2821849",
    "end": "2829500"
  },
  {
    "text": "this technology is in fact like the IP targets and alb support I think last",
    "start": "2829500",
    "end": "2835080"
  },
  {
    "text": "week the eks and open source team from AWS implemented a very similar version for ingress to eks so I think I wouldn't",
    "start": "2835080",
    "end": "2843089"
  },
  {
    "text": "say it's different but continue to be very open about our work so the container communities go in the direction that's beneficial to us",
    "start": "2843089",
    "end": "2850849"
  },
  {
    "text": "exhausted questions okay well you can",
    "start": "2855020",
    "end": "2861150"
  },
  {
    "text": "find me online a Spiker or check our github stuff out and let us know if you",
    "start": "2861150",
    "end": "2866490"
  },
  {
    "text": "have any other questions Thanks [Applause] [Music]",
    "start": "2866490",
    "end": "2871449"
  }
]