[
  {
    "text": "thanks for joining so I'm Joe Spisak I lead AI partnerships AI ml partnerships",
    "start": "0",
    "end": "6270"
  },
  {
    "text": "for AWS I hope everyone caught all of our announcements this morning Annie's keynote was a pretty exciting",
    "start": "6270",
    "end": "12840"
  },
  {
    "text": "and there's a lot there that we've been working out for a very long time so I talked probably not so much about that",
    "start": "12840",
    "end": "18690"
  },
  {
    "text": "but more on the open source side in this talk so I'm very excited and humbled to be honest too to have my my friend and",
    "start": "18690",
    "end": "24930"
  },
  {
    "text": "colleague Peter Nordhaus from Facebook joining me here and we're going to talk about scaling up vision models using",
    "start": "24930",
    "end": "31830"
  },
  {
    "text": "cafe 2 on AWS so we launched some really cool hardware this year in our GPU instances we'll talk about as well as a",
    "start": "31830",
    "end": "38850"
  },
  {
    "text": "lot of work we're doing around open source so this is really at that level we're gonna be talking about in this session so agenda wise I've talked a",
    "start": "38850",
    "end": "47250"
  },
  {
    "text": "little bit about what we've announced what we're actually get into details cuz I think everyone saw Andy's keynote hopefully assignees keynote I know we",
    "start": "47250",
    "end": "54390"
  },
  {
    "text": "our team was was all watching it a little bit about deep learning and why",
    "start": "54390",
    "end": "59850"
  },
  {
    "text": "GPUs matter and why we're excited about them and then I'm gonna hand it off to Peter who's going to jump into cafe to",
    "start": "59850",
    "end": "66200"
  },
  {
    "text": "how it's used at Facebook and then really the work we've been doing together on optimizing cafe to not only",
    "start": "66200",
    "end": "74850"
  },
  {
    "text": "for just distributed deep learning but also on Davis infrastructure for all of you developers to use and then actually",
    "start": "74850",
    "end": "83189"
  },
  {
    "text": "Peter has maybe a surprise for us he has a demo of some work he's been working on that just actually came to light a few",
    "start": "83189",
    "end": "89189"
  },
  {
    "text": "hours ago so that would be exciting so we'll finish up with a demo and a little bit of call to action for everyone to to",
    "start": "89189",
    "end": "95400"
  },
  {
    "text": "take away here so a little bit around the mission for AWS our goal is really",
    "start": "95400",
    "end": "101970"
  },
  {
    "text": "to enable developers and data scientists and all practitioners application developers essentially anyone at any",
    "start": "101970",
    "end": "108780"
  },
  {
    "text": "level of expertise to be efficient and be able to use machine learning and really very intelligence to their",
    "start": "108780",
    "end": "114060"
  },
  {
    "text": "applications that's and it's a it's a big it's a big meaty goal we want to",
    "start": "114060",
    "end": "120000"
  },
  {
    "text": "really drive this to every developer every customer we have one two million customers now and we want all of them to",
    "start": "120000",
    "end": "125189"
  },
  {
    "text": "be using machine learning and AI and that of course is challenging",
    "start": "125189",
    "end": "130310"
  },
  {
    "text": "because not every customer has a team of data scientists or research scientists or ml practitioners that they can throw",
    "start": "130310",
    "end": "136100"
  },
  {
    "text": "these really hairy problems to and and say go solve them they maybe have application developers they may have",
    "start": "136100",
    "end": "142790"
  },
  {
    "text": "folks there doing business intelligence they may rely on partners we don't know",
    "start": "142790",
    "end": "148640"
  },
  {
    "text": "but we're focused on really bringing it to everyone and we do that in a number",
    "start": "148640",
    "end": "153950"
  },
  {
    "text": "of ways we focus on services we launched a number of those this morning around",
    "start": "153950",
    "end": "159050"
  },
  {
    "text": "translation and around NLP video speech",
    "start": "159050",
    "end": "164900"
  },
  {
    "text": "recognition we also launched the platform it's called sage maker everyone thought that that was a big deal we also focus",
    "start": "164900",
    "end": "172100"
  },
  {
    "text": "on frameworks and infrastructure and that's actually where we're gonna focus more of our energy in the talk today but",
    "start": "172100",
    "end": "177770"
  },
  {
    "text": "you can see here the number of services this is pretty useless but you can see at the top there at the application",
    "start": "177770",
    "end": "183860"
  },
  {
    "text": "services and platform services everything that's new and those are all really things we've been working on for",
    "start": "183860",
    "end": "189200"
  },
  {
    "text": "less you're here including updates to Mechanical Turk for deep learning and machine learning adding video",
    "start": "189200",
    "end": "195670"
  },
  {
    "text": "transcription comprehend which is NLP and sage maker which is kind of a",
    "start": "195670",
    "end": "201830"
  },
  {
    "text": "crowning jewel of our AI launches or machine learning launches today and that's really a hosted machine learning",
    "start": "201830",
    "end": "207410"
  },
  {
    "text": "and data science platform that was a really big deal but really what we want",
    "start": "207410",
    "end": "212450"
  },
  {
    "text": "to focus on today is again at this level the lowest level frameworks and hardware",
    "start": "212450",
    "end": "218690"
  },
  {
    "text": "I think Peter and I both been in this space for a while so this is actually where we get really excited we're both",
    "start": "218690",
    "end": "225320"
  },
  {
    "text": "in open source and we'd love to see big parallel compute training large Nets",
    "start": "225320",
    "end": "230510"
  },
  {
    "text": "it's what gets us excited so so where we talk about that and I'm gonna cover just",
    "start": "230510",
    "end": "235850"
  },
  {
    "text": "a few things we've been working on in hardware as well as an open source and some some reason to not with",
    "start": "235850",
    "end": "241340"
  },
  {
    "text": "announcements so I think everyone saw the p3 announcement that came in October the reason I bring this up is actually",
    "start": "241340",
    "end": "247459"
  },
  {
    "text": "as the foundation of a lot of the work that we've been doing with Facebook at this point it's kind of ridiculous we're",
    "start": "247459",
    "end": "253940"
  },
  {
    "text": "at a petaflop of performance and a single instance so you know I think our",
    "start": "253940",
    "end": "259970"
  },
  {
    "text": "ptoo if you look at the biggest biggest one it was under a hundred teraflops so it's you know 14x better than a p2 so",
    "start": "259970",
    "end": "267230"
  },
  {
    "text": "incredible we have now envy link between GPUs so for those that don't know",
    "start": "267230",
    "end": "274970"
  },
  {
    "text": "certainly communication between GPUs or compute devices is really key for deep learning and Peter's gonna talk more",
    "start": "274970",
    "end": "281030"
  },
  {
    "text": "about that but that really had a big jump from the p2 so 9x better than p2",
    "start": "281030",
    "end": "286280"
  },
  {
    "text": "which is just using pretty much standard PCI Express and of course that has hbm",
    "start": "286280",
    "end": "291530"
  },
  {
    "text": "memory as well and you can see some of our launch partners like Airbnb Toyota research and open a I second the way we",
    "start": "291530",
    "end": "301820"
  },
  {
    "text": "really harness formants and really get it out to developers is through our deep learning ami this is something that back",
    "start": "301820",
    "end": "309170"
  },
  {
    "text": "when I was previously leading a product team we developed this and really focused on the research scientists the",
    "start": "309170",
    "end": "315260"
  },
  {
    "text": "developer frankly anyone who wants to spin up an instance and have a deep learning and data science environment",
    "start": "315260",
    "end": "321500"
  },
  {
    "text": "within a few minutes really so it's actually very cool you can bring up this this ami it has a number of different",
    "start": "321500",
    "end": "327980"
  },
  {
    "text": "frameworks pre installed along with nvidia drivers libraries Intel MKL everything's pre-built ready to go",
    "start": "327980",
    "end": "335419"
  },
  {
    "text": "it has cafe - it has tensorflow SMX net caris and you see some of the customers",
    "start": "335419",
    "end": "341419"
  },
  {
    "text": "like Zendesk and SC DM who use it today so this is really a foundation for",
    "start": "341419",
    "end": "346580"
  },
  {
    "text": "anyone who really wants to to just get started and there's also a number of jupiter notebooks that are actually",
    "start": "346580",
    "end": "352880"
  },
  {
    "text": "present within the Ami's so you can actually just do an SSH tunnel into a jupiter notebook and you can start",
    "start": "352880",
    "end": "357950"
  },
  {
    "text": "training Nats basically within a matter of a few minutes and i also wanted to mention onyx so if folks haven't heard",
    "start": "357950",
    "end": "367400"
  },
  {
    "text": "of onyx you should definitely check it out so it stands for the open neural network exchange and what we've seen",
    "start": "367400",
    "end": "374050"
  },
  {
    "text": "between really you know Microsoft Facebook and Amazon is this desire this",
    "start": "374050",
    "end": "381340"
  },
  {
    "text": "is really this yearning to bring research into production as quickly as possible and from an Amazon perspective",
    "start": "381340",
    "end": "388370"
  },
  {
    "text": "we have thousands of researchers focused around alexa prime air Amazon go just the list kind of keeps",
    "start": "388370",
    "end": "395390"
  },
  {
    "text": "going on and on and on and what we're doing personalization recommendations and we we do that research remember",
    "start": "395390",
    "end": "401740"
  },
  {
    "text": "parts of the world they all use different frameworks or they all use different different tools and so we see",
    "start": "401740",
    "end": "407480"
  },
  {
    "text": "the same challenges that I think Microsoft and Facebook saw in that there",
    "start": "407480",
    "end": "413210"
  },
  {
    "text": "really needs to be some type of standardization with models and so the promise of onyx which is ambitious and",
    "start": "413210",
    "end": "420560"
  },
  {
    "text": "we're very excited about it is really to have a common format between these these frameworks so you can really interoperate between you know say a PI",
    "start": "420560",
    "end": "427700"
  },
  {
    "text": "torch which is a very nice research centric framework to a cafe to framework which is more focused on production",
    "start": "427700",
    "end": "434710"
  },
  {
    "text": "which again we'll talk about and how Facebook uses that to MX net which for",
    "start": "434710",
    "end": "440450"
  },
  {
    "text": "example Amazon uses at a production scale today and really being able to take you know we've research that was",
    "start": "440450",
    "end": "446870"
  },
  {
    "text": "originally done in PI torch which is coming out very quickly and get that into production basically like that like",
    "start": "446870",
    "end": "453800"
  },
  {
    "text": "really this even as it's being published looking at trying to get right into production and running out other",
    "start": "453800",
    "end": "459350"
  },
  {
    "text": "frameworks which are built really for production scale so we're excited about that so definitely check that out with that I",
    "start": "459350",
    "end": "467000"
  },
  {
    "text": "want to pass it off to Peter he's going to talk about cafe 2 and actually dive way deep into how they handle",
    "start": "467000",
    "end": "472430"
  },
  {
    "text": "distributed deep learning and actually demo it at the end so enjoy thank you thank you Joe appreciate it",
    "start": "472430",
    "end": "479390"
  },
  {
    "text": "hi everybody I'm Peter Nordhaus I work at Facebook on cafe 2 and more importantly making coffee too fast and",
    "start": "479390",
    "end": "485540"
  },
  {
    "text": "I'll take you on a little journey to to walk through the steps that you have to",
    "start": "485540",
    "end": "492800"
  },
  {
    "text": "take to create a trainer to distribute it all the caveats the the the potential",
    "start": "492800",
    "end": "498170"
  },
  {
    "text": "bottlenecks and that sort of thing so with that I'd first like to give you a",
    "start": "498170",
    "end": "503210"
  },
  {
    "text": "little overview of coffee too so some of you may have heard of Cafe cafe the",
    "start": "503210",
    "end": "509600"
  },
  {
    "text": "first cafe this came out and I think like five or six or seven years ago or something like that it was a grad grad",
    "start": "509600",
    "end": "515960"
  },
  {
    "text": "student project by young Shin GI who unfortunately wasn't able to make it make it here today I know he was on the",
    "start": "515960",
    "end": "522050"
  },
  {
    "text": "on the speaker list he was the original author of of cafe 2 it focused purely on computer vision applications and it's",
    "start": "522050",
    "end": "530399"
  },
  {
    "text": "it's it was really written to do one task really fast and like that that was",
    "start": "530399",
    "end": "535500"
  },
  {
    "text": "it so it was adopted very quickly since it came out at kind of the start of the",
    "start": "535500",
    "end": "542670"
  },
  {
    "text": "of the deep learning boom different companies to the original cafe source",
    "start": "542670",
    "end": "547949"
  },
  {
    "text": "code and made four Forks out of it there were like Intel made Intel cafe and",
    "start": "547949",
    "end": "553050"
  },
  {
    "text": "video made and V cafe with all of their specific Hardware optimizations in there",
    "start": "553050",
    "end": "558300"
  },
  {
    "text": "but it also caused like some fragmentation all right so you had different flavors of cafe that you would",
    "start": "558300",
    "end": "563579"
  },
  {
    "text": "use on different hardware platforms and if you features came up in any one of",
    "start": "563579",
    "end": "570720"
  },
  {
    "text": "those you wouldn't have those in the other ones so that that fragmentation wasn't wasn't wasn't wasn't that great",
    "start": "570720",
    "end": "577769"
  },
  {
    "text": "it is still a very popular framework even though I wouldn't use it anymore",
    "start": "577769",
    "end": "585180"
  },
  {
    "text": "because I think it's kind of like in maintenance mode so moving on to cafe -",
    "start": "585180",
    "end": "590699"
  },
  {
    "text": "so what is different about cafe - we have a full computational graph in cafe",
    "start": "590699",
    "end": "597480"
  },
  {
    "text": "- so instead of dealing with like the tensors the input types at a level that",
    "start": "597480",
    "end": "603630"
  },
  {
    "text": "is very much suited for a cafe - for a computer vision applications I'm sorry in cafe - we don't really care what the",
    "start": "603630",
    "end": "610769"
  },
  {
    "text": "inputs and outputs are we create operators that take inputs and produce",
    "start": "610769",
    "end": "615779"
  },
  {
    "text": "outputs and off of that we can create one big graph that we then execute so it's no longer specific to computer",
    "start": "615779",
    "end": "622680"
  },
  {
    "text": "vision applications but you can do NOP tasks you can do speech tasks or ranking tasks or whatever you want to do you",
    "start": "622680",
    "end": "629670"
  },
  {
    "text": "could even do linear regression with it if you really wanted to another big difference between cafe and cafe - is",
    "start": "629670",
    "end": "635970"
  },
  {
    "text": "that it has first-class distributed support which is a big focus of today's",
    "start": "635970",
    "end": "641610"
  },
  {
    "text": "talk so I'll talk a lot about how the distributed support in cafe 2 works and",
    "start": "641610",
    "end": "647699"
  },
  {
    "text": "why we we built it the way we did another big feature that we're very",
    "start": "647699",
    "end": "655560"
  },
  {
    "text": "excited about of cafe 2 is that it's inherited platform so instead of creating an",
    "start": "655560",
    "end": "663230"
  },
  {
    "text": "environment where we would have these different Forks come out by different vendors cafe to is is very modular where",
    "start": "663230",
    "end": "671120"
  },
  {
    "text": "you can add let's say a convolution operator but you'll have one that works for Intel MPL you'll have one that works",
    "start": "671120",
    "end": "676670"
  },
  {
    "text": "for kudi and then you'll have one that works for eigen and so forth and so on and so forth so as new hardware and your",
    "start": "676670",
    "end": "684050"
  },
  {
    "text": "techniques to perform this computation come out we can easily add them to to",
    "start": "684050",
    "end": "689600"
  },
  {
    "text": "cafe to the cross-platform nature is very important for Facebook specifically because we use cafe to both on our",
    "start": "689600",
    "end": "696800"
  },
  {
    "text": "servers but also in the in the big blue app so what you saw in the in a movie let me actually go back let's see if I",
    "start": "696800",
    "end": "704720"
  },
  {
    "text": "can go back and forth yeah so that video is an example of style transfer which is",
    "start": "704720",
    "end": "712880"
  },
  {
    "text": "something we we execute on a mobile phone if you have an iPhone like 6:00 or",
    "start": "712880",
    "end": "718640"
  },
  {
    "text": "later you can and you you haven't Facebook installed you go to the camera you can have all these effects that is",
    "start": "718640",
    "end": "724550"
  },
  {
    "text": "actually running cafe to so I would with that I would say that cafe 2 is possibly",
    "start": "724550",
    "end": "730460"
  },
  {
    "text": "the largest mobile deployments of deep learning frameworks out there this is possible because it has this tiny core",
    "start": "730460",
    "end": "737780"
  },
  {
    "text": "right this tiny core that if you compile it down to native code it's maybe only like 100 200 kilobytes so it's very easy",
    "start": "737780",
    "end": "744680"
  },
  {
    "text": "to ship this on phones and not have to worry about having having to bring like gigantic interpreters or or that that",
    "start": "744680",
    "end": "752780"
  },
  {
    "text": "type of stuff that modularity is I'll",
    "start": "752780",
    "end": "759410"
  },
  {
    "text": "talk about that a little bit later so if you want to actually build cafe to all these different platforms we we are very",
    "start": "759410",
    "end": "766100"
  },
  {
    "text": "open with that it's not like Oh Facebook has its own build infrastructure for for their phones and we can't get to use it",
    "start": "766100",
    "end": "772220"
  },
  {
    "text": "all of this stuff is open source so if you go into the coffee 2 repository you'll see our CMake build system which",
    "start": "772220",
    "end": "778100"
  },
  {
    "text": "supports a variety of platforms so if you want to compile for iOS or Android even for the nvidia tegra as for the",
    "start": "778100",
    "end": "785150"
  },
  {
    "text": "raspberries then you'll be able to do so and here's a couple",
    "start": "785150",
    "end": "793010"
  },
  {
    "text": "examples of the of the of the tasks in this case computer vision tasks that we",
    "start": "793010",
    "end": "798860"
  },
  {
    "text": "where we use cafe to on the on the left hand side you see computer vision task",
    "start": "798860",
    "end": "804139"
  },
  {
    "text": "called image segmentation and identification instead of taking a",
    "start": "804139",
    "end": "810740"
  },
  {
    "text": "single image and saying hey this is a beach that that was the classical computer vision classification task",
    "start": "810740",
    "end": "817430"
  },
  {
    "text": "instead research has advanced so much that we can now say well okay what are",
    "start": "817430",
    "end": "822529"
  },
  {
    "text": "the different objects that we recognize in an image and how can we classify those individual objects the next step",
    "start": "822529",
    "end": "829100"
  },
  {
    "text": "yet you see on the right hand side which is where we take that that segmented",
    "start": "829100",
    "end": "834440"
  },
  {
    "text": "information and can do something called pose estimation all of these all of these tasks are powered by by cafe to",
    "start": "834440",
    "end": "843040"
  },
  {
    "text": "the modularity I hinted towards it earlier a little bit is is very very",
    "start": "843040",
    "end": "849230"
  },
  {
    "text": "powerful so let's going back to that conclusion operation I was talking about qu DNN and Intel MKL",
    "start": "849230",
    "end": "855470"
  },
  {
    "text": "there's there's more of them of course right there's an impact which contains a lot of like low-level assembly",
    "start": "855470",
    "end": "861490"
  },
  {
    "text": "optimizations for x86 64 and arm there's and of course you can plug-in custom",
    "start": "861490",
    "end": "868670"
  },
  {
    "text": "code metal is the iOS back-end for performing neural network primitives if",
    "start": "868670",
    "end": "874970"
  },
  {
    "text": "I say that correctly I think I do it's it's an API that gives you the same level of granularity as for example like",
    "start": "874970",
    "end": "881480"
  },
  {
    "text": "a COO DNN so that's that's where you tap in if you're if you're running on iOS now the input and output doesn't change",
    "start": "881480",
    "end": "888500"
  },
  {
    "text": "the merit the numerix of the operation don't change it's just that on a particular hardware platform you'll be",
    "start": "888500",
    "end": "894649"
  },
  {
    "text": "able to execute it faster than then on others okay so I kind of covered this",
    "start": "894649",
    "end": "903230"
  },
  {
    "text": "already but I would not get another one Snapdragon yeah on the Qualcomm phones we also had the Snapdragon library so I",
    "start": "903230",
    "end": "910389"
  },
  {
    "text": "already forgot about about two of them here but we already covered like seven different backends that you could use",
    "start": "910389",
    "end": "917569"
  },
  {
    "text": "for a single single operation and all of this is embedded in the in the same cafe like small coffee too",
    "start": "917569",
    "end": "924610"
  },
  {
    "text": "or so there's no no longer a need to have a different fork for a different different backends extending cafe to",
    "start": "924610",
    "end": "932140"
  },
  {
    "text": "yourself if you want to is also fairly fairly easy we have a directory and a",
    "start": "932140",
    "end": "937329"
  },
  {
    "text": "feature repository called contra prayer we hold a number of extensions that can be compiled optionally with cafe to",
    "start": "937329",
    "end": "943660"
  },
  {
    "text": "target one example of that is actually the operators that we use for the for",
    "start": "943660",
    "end": "949480"
  },
  {
    "text": "distributed training but another example is for example the that teensy there was",
    "start": "949480",
    "end": "955630"
  },
  {
    "text": "a little animation in there but I'm okay without it the teensy operation for example allows you to visualize a higher",
    "start": "955630",
    "end": "962170"
  },
  {
    "text": "dimensional space in a lower dimensional space which can be very helpful to visually detect minima and stuff like",
    "start": "962170",
    "end": "967600"
  },
  {
    "text": "that adding something like that is as simple as creating a new C++ class that inherits from the base operator class",
    "start": "967600",
    "end": "973600"
  },
  {
    "text": "and registering it if you compile that down to a shared library and you'll load that at runtime you have access access",
    "start": "973600",
    "end": "979750"
  },
  {
    "text": "to it and it can be part of that computation graph just as any other operator that you're already already",
    "start": "979750",
    "end": "985060"
  },
  {
    "text": "using so with kind of the introduction and the the skeleton or the framework",
    "start": "985060",
    "end": "990760"
  },
  {
    "text": "out of the way I want to I want to take you on a on a on a tour here defining",
    "start": "990760",
    "end": "996579"
  },
  {
    "text": "like how you actually go about defining a modeling cafe to and and run it and subsequently distribute it so in cafe a",
    "start": "996579",
    "end": "1005670"
  },
  {
    "text": "a model is is the container for all operations so this is this kind of like",
    "start": "1005670",
    "end": "1012750"
  },
  {
    "text": "you can think of it as a as a list and in this list you're gonna say hey I'm",
    "start": "1012750",
    "end": "1017790"
  },
  {
    "text": "gonna I'm gonna have an operator that for example does image input and it produces it's gonna produce an output to",
    "start": "1017790",
    "end": "1026280"
  },
  {
    "text": "outputs which is gonna be in the case of a computer vision task an image or a set of images and a label or several labels",
    "start": "1026280",
    "end": "1033418"
  },
  {
    "text": "saying oh this is a beach this is a car this is a truck that type of stuff this",
    "start": "1033419",
    "end": "1040410"
  },
  {
    "text": "series of labels or this series of operators can be defined sequentially and because we use the same symbols for",
    "start": "1040410",
    "end": "1049130"
  },
  {
    "text": "outputs and subsequently inputs we can distill that list of operators into a",
    "start": "1049130",
    "end": "1055020"
  },
  {
    "text": "graph where we can then paralyze and we'll without sacrificing but with without",
    "start": "1055020",
    "end": "1060470"
  },
  {
    "text": "sacrificing correctness so in this example we're going to build a a",
    "start": "1060470",
    "end": "1066899"
  },
  {
    "text": "resonant 50 model the model helper is there is going to be the container for",
    "start": "1066899",
    "end": "1072690"
  },
  {
    "text": "all the operators and it will also confer to the protobuf so the protobuf the proto buffers are our google",
    "start": "1072690",
    "end": "1079019"
  },
  {
    "text": "standard for defining structure like there's a there's an IDL you define this is these are structs with types and",
    "start": "1079019",
    "end": "1087240"
  },
  {
    "text": "fields and that type of stuff that that's what a cafe to model can be serialized into and you can take that",
    "start": "1087240",
    "end": "1094710"
  },
  {
    "text": "protobuf and then ship it someplace else to do to run it so this is how you serialize a model both the definition of",
    "start": "1094710",
    "end": "1101399"
  },
  {
    "text": "a model and the weights of a model the art scope in this case is is the order",
    "start": "1101399",
    "end": "1108360"
  },
  {
    "text": "for the tensors that we're going to be using so there's there's two different",
    "start": "1108360",
    "end": "1113789"
  },
  {
    "text": "mode actually you can you can have any permutation of this order the two commonly used waters are either NC HW or",
    "start": "1113789",
    "end": "1121980"
  },
  {
    "text": "an HW c the n stands for the number of examples in a batch so whenever your",
    "start": "1121980",
    "end": "1127710"
  },
  {
    "text": "your training you're gonna train on like a whole batch of data at once because that that worked well with your cindy",
    "start": "1127710",
    "end": "1134820"
  },
  {
    "text": "units and your massively parallel GPUs and that type of thing the C stands for the number of or for the channels in an",
    "start": "1134820",
    "end": "1141330"
  },
  {
    "text": "RGB image so in this case it's 10 or it can stand for RGB BGR some some kind of",
    "start": "1141330",
    "end": "1148638"
  },
  {
    "text": "channeling you could also consider a model where you also train on like an alpha alpha channel although that's not",
    "start": "1148669",
    "end": "1154679"
  },
  {
    "text": "really not really common then we also add H and the W channels which then for hiding with we defined our scope at the",
    "start": "1154679",
    "end": "1163080"
  },
  {
    "text": "mobile level helper so that further down we don't have to repeat ourselves all the time so the arc scope is going to be",
    "start": "1163080",
    "end": "1170130"
  },
  {
    "text": "propagated to all the operators that we define later on on this on this model helper for the next one like next",
    "start": "1170130",
    "end": "1179210"
  },
  {
    "text": "example that we see here is defining the image input operation which we defined through the what's called the Bru API in",
    "start": "1179210",
    "end": "1186269"
  },
  {
    "text": "cafe - this is all defined under a helper subdirectory which contains a large number of of files with help",
    "start": "1186269",
    "end": "1191879"
  },
  {
    "text": "to define our operators you can imagine that there's a large number of configurable arguments to different",
    "start": "1191879",
    "end": "1198809"
  },
  {
    "text": "operators and you don't want to define all those operators all the time because",
    "start": "1198809",
    "end": "1204119"
  },
  {
    "text": "you'd be repeating yourself and it becomes very inconvenient and for that reason we added this this brew API where",
    "start": "1204119",
    "end": "1210559"
  },
  {
    "text": "a lot of that is abstracted for you when you only get to define the things that",
    "start": "1210559",
    "end": "1215609"
  },
  {
    "text": "you care about and it uses sensible defaults for everything else in this case the brew API is also always",
    "start": "1215609",
    "end": "1222659"
  },
  {
    "text": "stateless so it just a static function it takes the mobile that you operate on it takes inputs one or more inputs in",
    "start": "1222659",
    "end": "1230459"
  },
  {
    "text": "this case we give it a reader input which is a an object that can read data",
    "start": "1230459",
    "end": "1236339"
  },
  {
    "text": "from from disk and seek in in that in that larger data set conceptually you",
    "start": "1236339",
    "end": "1242519"
  },
  {
    "text": "can you can think of this as the place where the images come from and are either like decoded JPEG decoded or",
    "start": "1242519",
    "end": "1248940"
  },
  {
    "text": "something like that then we have a series of outputs in this case data and the label data will hold a tensor of the",
    "start": "1248940",
    "end": "1255839"
  },
  {
    "text": "type and CHW right so we'll have a end being equal to the batch size in this",
    "start": "1255839",
    "end": "1261509"
  },
  {
    "text": "case that's say 32 so we'll have 32 images with three channels of 224 by 224",
    "start": "1261509",
    "end": "1269699"
  },
  {
    "text": "because that's the crop that we tell the image input operator to take out of that out of that larger larger image there's",
    "start": "1269699",
    "end": "1278069"
  },
  {
    "text": "a larger much larger set of image augmentation operations that the image input up I can perform but I leave those",
    "start": "1278069",
    "end": "1286139"
  },
  {
    "text": "out for brevity err so when we then go in to find a model we don't have to think about like hey what kind of",
    "start": "1286139",
    "end": "1293879"
  },
  {
    "text": "convolutions and what kind of fully connected layers shall i include every time that's where what research is for",
    "start": "1293879",
    "end": "1300359"
  },
  {
    "text": "either so research produces every now and then a model that is kind of the",
    "start": "1300359",
    "end": "1307129"
  },
  {
    "text": "current state of the art and we get to reuse them as we as we see fit and",
    "start": "1307129",
    "end": "1312239"
  },
  {
    "text": "typically use either the the final output stages or some of the later stages in the model as inputs to our own",
    "start": "1312239",
    "end": "1318329"
  },
  {
    "text": "models so in this case we can say hey we want to create a resonant model and conveniently enough Caffe provides a",
    "start": "1318329",
    "end": "1323759"
  },
  {
    "text": "bunch of proper functions again to define these canonical models on on a model model",
    "start": "1323759",
    "end": "1331760"
  },
  {
    "text": "object that you define so in this case the the resonant helper takes again like",
    "start": "1331760",
    "end": "1337250"
  },
  {
    "text": "the brew API the model as input it takes the another argument here in this case",
    "start": "1337250",
    "end": "1343520"
  },
  {
    "text": "the data like what is the data that's coming in well it was produced by the image input of earlier and label as well",
    "start": "1343520",
    "end": "1351110"
  },
  {
    "text": "as a number of labels so in the case of the image net 1k classification challenge we're gonna have 1000 labels",
    "start": "1351110",
    "end": "1358100"
  },
  {
    "text": "and this is an like an output vector of size 1,000 right if an if an image is a",
    "start": "1358100",
    "end": "1363740"
  },
  {
    "text": "beach then at N equals 0 you'll have an",
    "start": "1363740",
    "end": "1368990"
  },
  {
    "text": "image of a beach and then at N equals 0 of the labels you'll have a 1 at the",
    "start": "1368990",
    "end": "1374750"
  },
  {
    "text": "position that represents a beach with all everything else being 0 the mobile",
    "start": "1374750",
    "end": "1381860"
  },
  {
    "text": "definition helper also returns a loss function that compares the actual output of the network with the expected output",
    "start": "1381860",
    "end": "1389240"
  },
  {
    "text": "like you you know what what class of image this is so we can compare the two and then compute a gradient with respect",
    "start": "1389240",
    "end": "1397310"
  },
  {
    "text": "to that loss which we then use to update update model in the next step so the",
    "start": "1397310",
    "end": "1405710"
  },
  {
    "text": "subsequent step here is to take those gradients and take a fraction of those",
    "start": "1405710",
    "end": "1412130"
  },
  {
    "text": "compute like some moving average that type of thing and update the weights of the model to iteratively get to the best",
    "start": "1412130",
    "end": "1418760"
  },
  {
    "text": "minimum and get to the best accuracy accuracy model all right so with that",
    "start": "1418760",
    "end": "1426920"
  },
  {
    "text": "model built we haven't trained anything yet of course so let's take a quick look at at the main training loop what that",
    "start": "1426920",
    "end": "1435260"
  },
  {
    "text": "typically would look like you can imagine that it's more complex than this but this is the this is the basic skeleton of it and it and it would work",
    "start": "1435260",
    "end": "1441260"
  },
  {
    "text": "so we can't just train once over our",
    "start": "1441260",
    "end": "1448130"
  },
  {
    "text": "entire input set because we only get to see our data once and that's not enough to actually learn the classes of that",
    "start": "1448130",
    "end": "1454580"
  },
  {
    "text": "problem we have to go over it typically like 90 100 110 120 times and",
    "start": "1454580",
    "end": "1460429"
  },
  {
    "text": "that's what none a box in this case is so we create one big loop where we train let's say 90 times for every app Bach",
    "start": "1460429",
    "end": "1468890"
  },
  {
    "text": "what we call it we have to run the network enough to see the entire data",
    "start": "1468890",
    "end": "1474290"
  },
  {
    "text": "set so in this case we train that we take the size the a box size that we use",
    "start": "1474290",
    "end": "1479840"
  },
  {
    "text": "for training which in the in the case of imagenet one of imagenet 1k is 1.2",
    "start": "1479840",
    "end": "1486620"
  },
  {
    "text": "million one put two million images that we that we train on we divide it over the batch size every call that's the N",
    "start": "1486620",
    "end": "1493670"
  },
  {
    "text": "that we specify before the batch size the number of images that we train on in a single iteration and get the number of",
    "start": "1493670",
    "end": "1499670"
  },
  {
    "text": "iterations we have to run in order to see the entire data set in the single app book now if we would leave it at",
    "start": "1499670",
    "end": "1506780"
  },
  {
    "text": "that we would be good we could stop right there and say well we'll just run 90 times and it produces a model and",
    "start": "1506780",
    "end": "1514640"
  },
  {
    "text": "then we evaluate its accuracy and then choose whether we like it or not however in practice it is it is it is",
    "start": "1514640",
    "end": "1523360"
  },
  {
    "text": "wise or okay yeah it's probably wise to also after each epilogue test what the",
    "start": "1523360",
    "end": "1532910"
  },
  {
    "text": "accuracy of the of the of the network is this is how you get your like your training curves right if you've read",
    "start": "1532910",
    "end": "1540200"
  },
  {
    "text": "some lit some some literature about model architecture then you've seen",
    "start": "1540200",
    "end": "1545780"
  },
  {
    "text": "those those training curves that that start start high and gradually go to a",
    "start": "1545780",
    "end": "1552410"
  },
  {
    "text": "lower spot all right ending up at like the the top one accuracy of let's say twenty five percent twenty percent",
    "start": "1552410",
    "end": "1558380"
  },
  {
    "text": "something like that in order to do that we run the model again but on a separate",
    "start": "1558380",
    "end": "1564050"
  },
  {
    "text": "model in this test model we don't care about computing a gradient because we only use it in a forward pass we use it",
    "start": "1564050",
    "end": "1570710"
  },
  {
    "text": "in the four we compute the forward pass and after that we check hey what's the mobile right or not if it was then we",
    "start": "1570710",
    "end": "1577280"
  },
  {
    "text": "say is accurate if it wasn't then it's an inaccurate we average those accurate",
    "start": "1577280",
    "end": "1583670"
  },
  {
    "text": "and inaccurate classifications over the entire test Epoque size which",
    "start": "1583670",
    "end": "1589049"
  },
  {
    "text": "I mean it's up to you how much you want to do this this this also cost some time some computation so you can either",
    "start": "1589049",
    "end": "1596010"
  },
  {
    "text": "choose it to be like ten thousand or fifty thousand or a hundred thousand it doesn't doesn't really matter but you",
    "start": "1596010",
    "end": "1602220"
  },
  {
    "text": "can use this to compute your accuracy now so far what I've talked about is",
    "start": "1602220",
    "end": "1607380"
  },
  {
    "text": "just like how do you define a model and how do you define a trainer but that",
    "start": "1607380",
    "end": "1612750"
  },
  {
    "text": "doesn't talk anything yet about scaling actually scaling it up this is a hundred percent iterative and shirk off a two",
    "start": "1612750",
    "end": "1619409"
  },
  {
    "text": "will exploit parallelism where it can write if you if you visualize a graph",
    "start": "1619409",
    "end": "1625250"
  },
  {
    "text": "it's not going to be a linear linear sequence of operations that you execute",
    "start": "1625250",
    "end": "1630659"
  },
  {
    "text": "there's going to be branching in there for example in the backwards pass if you compute the gradient of the next level",
    "start": "1630659",
    "end": "1637409"
  },
  {
    "text": "up and the next level up in the next level up you can separately perform your your your weight updates all right so",
    "start": "1637409",
    "end": "1643470"
  },
  {
    "text": "all of that branching is exploited by coffee to in the form of parallelism and multiple threads running the operators",
    "start": "1643470",
    "end": "1650909"
  },
  {
    "text": "that can be run on that on that graph so let's talk a little bit more about scaling up and what well some caveats",
    "start": "1650909",
    "end": "1659070"
  },
  {
    "text": "that that that come into play there so the first step is to actually talk about",
    "start": "1659070",
    "end": "1664260"
  },
  {
    "text": "the type of scaling what we're gonna be doing there's generally two classes weak",
    "start": "1664260",
    "end": "1670980"
  },
  {
    "text": "scaling a strong scaling the batch size that I talked about before in a single",
    "start": "1670980",
    "end": "1677399"
  },
  {
    "text": "thread of execution like let's say that's 32 if you talk about strong",
    "start": "1677399",
    "end": "1683490"
  },
  {
    "text": "scaling you can use like a GPUs but you would also divide that batch size by",
    "start": "1683490",
    "end": "1688620"
  },
  {
    "text": "eight so you would end up only using a batch of four per GPU right and if you",
    "start": "1688620",
    "end": "1693870"
  },
  {
    "text": "go to 32 GPUs you use a batch of one and after that you can't do strong scaling anymore because that's your bad choice",
    "start": "1693870",
    "end": "1701130"
  },
  {
    "text": "that's your global batch size instead if we talk about weak scaling we just like",
    "start": "1701130",
    "end": "1708899"
  },
  {
    "text": "throw more compute at a problem we say every worker will use the same batch size so the more your workers we use the",
    "start": "1708899",
    "end": "1715890"
  },
  {
    "text": "larger the batch size becomes right there's no numerical equivalency anymore between using one worker",
    "start": "1715890",
    "end": "1721530"
  },
  {
    "text": "or two or eight or whatever the bachelors grows and that's how we get get our throughput on a related note",
    "start": "1721530",
    "end": "1728730"
  },
  {
    "text": "there's also data versus mobile parallelism and they when we talk about data parallelism we say well we'll just",
    "start": "1728730",
    "end": "1736040"
  },
  {
    "text": "throw more data run through data more through the through the model and at the",
    "start": "1736040",
    "end": "1744570"
  },
  {
    "text": "same time versus individual operators being run by multiple nodes that wouldn't use way too much communication",
    "start": "1744570",
    "end": "1751110"
  },
  {
    "text": "so out of these modes we focus at Facebook mostly about the the fourth one",
    "start": "1751110",
    "end": "1756930"
  },
  {
    "text": "where we use multiple machines and each machine use multiple GPUs to illustrate",
    "start": "1756930",
    "end": "1762150"
  },
  {
    "text": "that a little bit more here's a hitter curve describing what the GPU throughput how the GPU throughput degrades with a",
    "start": "1762150",
    "end": "1769710"
  },
  {
    "text": "shrinking batch size so if you look at at the optimal batch size here at the very tail end of the curve 64 you get a",
    "start": "1769710",
    "end": "1776940"
  },
  {
    "text": "throughput of roughly 390 images per second that's that's great so you say well we can we can apply strong scaling",
    "start": "1776940",
    "end": "1783480"
  },
  {
    "text": "here if we like double the number of workers sure but that means that you go to a batch size of 32 per node and we",
    "start": "1783480",
    "end": "1790470"
  },
  {
    "text": "only see performance of 350 per seconds now for your application that might be",
    "start": "1790470",
    "end": "1795990"
  },
  {
    "text": "fine you might say well okay I can take that hit that little hit of inefficiency as long as the numeric state is same but",
    "start": "1795990",
    "end": "1802290"
  },
  {
    "text": "if you apply that step again and you end up at 300 and again and you end up at 250 it starts to degrade and you stop",
    "start": "1802290",
    "end": "1809310"
  },
  {
    "text": "being able to scale further so this is why we like weak scaling much better",
    "start": "1809310",
    "end": "1817770"
  },
  {
    "text": "than then then strong scaling just it's it's easier to do when you can scale it much further you don't hit that hit that",
    "start": "1817770",
    "end": "1824070"
  },
  {
    "text": "wall as quickly so in order to paralyze a model we have a helper in cafe - it",
    "start": "1824070",
    "end": "1832200"
  },
  {
    "text": "doesn't require you to define your model eight times and do all of the synchronization yourself we abstract",
    "start": "1832200",
    "end": "1838260"
  },
  {
    "text": "that in cafe 2 in a helper function which we call the data parallel model it takes as input a trained model which",
    "start": "1838260",
    "end": "1845010"
  },
  {
    "text": "again is the output of the model helper that I showed before and instead of defining the model wants we extract the",
    "start": "1845010",
    "end": "1854220"
  },
  {
    "text": "process of the into a into a lambda that we end up calling from this paralyzed function so",
    "start": "1854220",
    "end": "1860730"
  },
  {
    "text": "what will happen is the parallelized function knows hey I want to create eight instances of this model because I",
    "start": "1860730",
    "end": "1865830"
  },
  {
    "text": "have a GPUs it will call that a create model operation eight times but with",
    "start": "1865830",
    "end": "1871650"
  },
  {
    "text": "different scopes so I will say well I'll create one model where all the inputs live on GPU GPUs zero I'll create",
    "start": "1871650",
    "end": "1877800"
  },
  {
    "text": "another one on GPU one and so and so on and so forth that's not the end of it",
    "start": "1877800",
    "end": "1883920"
  },
  {
    "text": "though like if you would just know instantiate model eight times and run",
    "start": "1883920",
    "end": "1889920"
  },
  {
    "text": "with that and like you're okay with that you end up with eight different models like there needs to be communication between them in order to train train",
    "start": "1889920",
    "end": "1897300"
  },
  {
    "text": "jointly so this is inserted by the paralyze function in the data that data",
    "start": "1897300",
    "end": "1903390"
  },
  {
    "text": "parallel model automatically it's not something you have to take care of yourself and what happens is that before",
    "start": "1903390",
    "end": "1910410"
  },
  {
    "text": "applying the weight updates to the models in the backwards pass it will average them across all the",
    "start": "1910410",
    "end": "1916440"
  },
  {
    "text": "participating devices and these devices can be either divided like single GPUs",
    "start": "1916440",
    "end": "1921540"
  },
  {
    "text": "or GPUs in a machine but they can also be GPUs in a machine and across machines which which which is what we end up",
    "start": "1921540",
    "end": "1929400"
  },
  {
    "text": "doing for multi multi machine training so to illustrate the problem with this",
    "start": "1929400",
    "end": "1935220"
  },
  {
    "text": "communication step let's look at a a little diagram so if we run through a",
    "start": "1935220",
    "end": "1942030"
  },
  {
    "text": "model you can imagine that we have layer 1 2 3 we run the forward pass in reverse",
    "start": "1942030",
    "end": "1947820"
  },
  {
    "text": "order we run the backward pass we have layer 3 backwards - backwards 1 backwards and then we apply the updates",
    "start": "1947820",
    "end": "1953730"
  },
  {
    "text": "right we get the gradients from the backwards pass and we apply to apply to updates according to those gradients now",
    "start": "1953730",
    "end": "1961970"
  },
  {
    "text": "if we paralyze we insert an extra step we have to synchronize those gradients",
    "start": "1961970",
    "end": "1969450"
  },
  {
    "text": "across all the participants before doing the updates otherwise we wouldn't end up training jointly and this adds execution",
    "start": "1969450",
    "end": "1977250"
  },
  {
    "text": "time however when you think back of the of the had the graph that I talked about",
    "start": "1977250",
    "end": "1984720"
  },
  {
    "text": "the compute graph there's no data dependency between some of these so what we can actually do is as soon as we computed the",
    "start": "1984720",
    "end": "1991000"
  },
  {
    "text": "gradients of the third layer we can reduce an update in parallel with",
    "start": "1991000",
    "end": "1996760"
  },
  {
    "text": "computing the the gradients of the of the second layer and this this is how we",
    "start": "1996760",
    "end": "2003240"
  },
  {
    "text": "exploit scaling up the process of training a model without taking a big",
    "start": "2003240",
    "end": "2010170"
  },
  {
    "text": "hit on the on the scaling linearity so depending on the the performance of",
    "start": "2010170",
    "end": "2015600"
  },
  {
    "text": "these synchronization operators we can we can scale with either near linear",
    "start": "2015600",
    "end": "2023550"
  },
  {
    "text": "scaling efficiency or worse than that let's say if your synchronization is very slow this is also where at the the",
    "start": "2023550",
    "end": "2030330"
  },
  {
    "text": "point that Joe made earlier about envy link on the p3 instance consensus comes in with and feeling he'll be able to",
    "start": "2030330",
    "end": "2036960"
  },
  {
    "text": "synchronize faster so this synchronization step is going to insert less of a delay in your overall training",
    "start": "2036960",
    "end": "2043860"
  },
  {
    "text": "time another problem that we see with",
    "start": "2043860",
    "end": "2050960"
  },
  {
    "text": "achieving this scalability is that if",
    "start": "2050960",
    "end": "2056100"
  },
  {
    "text": "you have a box with a GPUs in it you'll also have to saturate all those a GPUs",
    "start": "2056100",
    "end": "2061440"
  },
  {
    "text": "and if the GPUs become faster and faster it puts more and more strain on the on",
    "start": "2061440",
    "end": "2066629"
  },
  {
    "text": "the CPU to actually produce all those inputs and think about it if we have a p3 16 large and we do like six twenty",
    "start": "2066630",
    "end": "2074940"
  },
  {
    "text": "six hundred images per second or like let's say even even to thousands and you have to do JPEG decode crops and image",
    "start": "2074940",
    "end": "2082560"
  },
  {
    "text": "augmentation for all of those that puts a lot of stress on the on the processor and that and this is a this is a kind of",
    "start": "2082560",
    "end": "2089010"
  },
  {
    "text": "a well-understood challenge and there comes there they're probably good will be a time when we have to offload some",
    "start": "2089010",
    "end": "2096060"
  },
  {
    "text": "of these tasks elsewhere just because the the GPU silicon will become too fast",
    "start": "2096060",
    "end": "2102590"
  },
  {
    "text": "if we want to paralyze for multi machine not just focus on a single node what do",
    "start": "2102590",
    "end": "2108210"
  },
  {
    "text": "we do in cafe to well it's actually not that hard we can add I can define a",
    "start": "2108210",
    "end": "2114240"
  },
  {
    "text": "dictionary where we say well there's going to be four participants I am",
    "start": "2114240",
    "end": "2120060"
  },
  {
    "text": "number 0 we'll use a DCP transport and we have to",
    "start": "2120060",
    "end": "2125860"
  },
  {
    "text": "have some key value store on the side to perform a service discovery we pass that to the paralyzed function and that's it",
    "start": "2125860",
    "end": "2132670"
  },
  {
    "text": "so the paralyzed function will take care of all of the steps needed to actually have the machines find each other and",
    "start": "2132670",
    "end": "2138790"
  },
  {
    "text": "then start start communicating the operations that it will insert in the",
    "start": "2138790",
    "end": "2144220"
  },
  {
    "text": "operator graph instead of only performing reduction locally they'll perform reduction across across machines",
    "start": "2144220",
    "end": "2153299"
  },
  {
    "text": "so the scheduling aspect there in cafe - we don't take a take a take a big bet on",
    "start": "2153480",
    "end": "2160720"
  },
  {
    "text": "like saying hey we want to use this scheduler that's scheduler we don't really care as long as you can feed cafe",
    "start": "2160720",
    "end": "2166930"
  },
  {
    "text": "- with the right information and provide it the right means for initial",
    "start": "2166930",
    "end": "2172840"
  },
  {
    "text": "communication then we're fine with it so it can be either like I guess Joe the",
    "start": "2172840",
    "end": "2179830"
  },
  {
    "text": "container service is now updated we now have eks and we have all these other fancy things so who knows maybe if they",
    "start": "2179830",
    "end": "2186850"
  },
  {
    "text": "at some point support CPU instances we'll be able to use that if you have an HPC cluster like a traditional",
    "start": "2186850",
    "end": "2193150"
  },
  {
    "text": "supercomputer we you might end up using slurm and MPI or as I'll demo in a",
    "start": "2193150",
    "end": "2199030"
  },
  {
    "text": "little bit you can also just be a person typing SSH commands and that's all that's fine as well the rendezvous step",
    "start": "2199030",
    "end": "2205420"
  },
  {
    "text": "is where where different machines will",
    "start": "2205420",
    "end": "2210670"
  },
  {
    "text": "have to find each other in order to communicate so there's this is not a standard like master worker type",
    "start": "2210670",
    "end": "2218320"
  },
  {
    "text": "architecture where you have one node and it takes all the updates and then broadcasting back out to everybody",
    "start": "2218320",
    "end": "2224080"
  },
  {
    "text": "because that would put too much of a bottleneck on that single machine instead this is an all tall community or",
    "start": "2224080",
    "end": "2230260"
  },
  {
    "text": "it can be an all - all communication pattern but we don't take any any",
    "start": "2230260",
    "end": "2235390"
  },
  {
    "text": "particular node that participates it's not going to be it's not going to be special so we separate finding all the",
    "start": "2235390",
    "end": "2241690"
  },
  {
    "text": "nodes from active doing the actual synchronization this step we we call",
    "start": "2241690",
    "end": "2247030"
  },
  {
    "text": "rendezvous and you can use any key value store or shared file system to to apply this and what ends up happening is is",
    "start": "2247030",
    "end": "2253870"
  },
  {
    "text": "roughly the following if we start a trainer let assume like left is one machine right the other",
    "start": "2253870",
    "end": "2259720"
  },
  {
    "text": "machine we started machine on on the left side we say ID is zero on the right",
    "start": "2259720",
    "end": "2264970"
  },
  {
    "text": "side ID is 1 the left side will set an address and address like a TCP address",
    "start": "2264970",
    "end": "2271300"
  },
  {
    "text": "of a socket it opened itself and is listening on at a predefined key the",
    "start": "2271300",
    "end": "2276910"
  },
  {
    "text": "other side does the same thing and because they know I am 0 and I'm 1 and there's going to be 2 total it can",
    "start": "2276910",
    "end": "2284200"
  },
  {
    "text": "compute the the address that the other nodes will set their keys at and it can wait for them after everybody has set",
    "start": "2284200",
    "end": "2290440"
  },
  {
    "text": "their their addresses that are that they listen on they can massively connect to one another and since these are all",
    "start": "2290440",
    "end": "2297790"
  },
  {
    "text": "going to be listening sockets we we do a simple bite compared on the on the socket address and if we're smaller we",
    "start": "2297790",
    "end": "2304420"
  },
  {
    "text": "close and connect to the other one and if you're a larger we we call an accept and wait for the other person person to",
    "start": "2304420",
    "end": "2310030"
  },
  {
    "text": "close after that we have a connected TCP socket and we can communicate whatever whatever we want over there so this",
    "start": "2310030",
    "end": "2318700"
  },
  {
    "text": "reduction step the synchronization step that where we have to take the gradients of these of these layers and synchronize",
    "start": "2318700",
    "end": "2325750"
  },
  {
    "text": "them is also called an El reduce and it happens in three stages first step is",
    "start": "2325750",
    "end": "2333070"
  },
  {
    "text": "that we have to reduce from all the GPU buffers every call they we if we have a",
    "start": "2333070",
    "end": "2339100"
  },
  {
    "text": "box with a GPUs will have 8 different gradients we have to reduce all of them into a single buffer that represents the",
    "start": "2339100",
    "end": "2345580"
  },
  {
    "text": "buffer for that machine subsequently we run an allergist across the machines to",
    "start": "2345580",
    "end": "2352390"
  },
  {
    "text": "Brett to get the final reduced gradient and after that since we still have to apply the updates locally on those GPUs",
    "start": "2352390",
    "end": "2359650"
  },
  {
    "text": "we do a broadcast within the machine from system memory back to back to the GPUs we end up doing a single one of",
    "start": "2359650",
    "end": "2368890"
  },
  {
    "text": "these operations per layer so per every or actually per buffer per layer and the",
    "start": "2368890",
    "end": "2376480"
  },
  {
    "text": "the runtime will depend on the size of this buffer but also the networking speed you can imagine having let's say a",
    "start": "2376480",
    "end": "2383890"
  },
  {
    "text": "mobile that's a gigabyte in size 250 million parameters float 32",
    "start": "2383890",
    "end": "2389140"
  },
  {
    "text": "then if you only have a gigabit of networking bandwidth it's gonna take you at least like theoretical minimum one or",
    "start": "2389140",
    "end": "2397510"
  },
  {
    "text": "10 seconds to communicate all of the data if your GPU spends only half a second to to compute the forward and",
    "start": "2397510",
    "end": "2406480"
  },
  {
    "text": "backwards pass you're gonna be spending xx 20 times as much time waiting on the",
    "start": "2406480",
    "end": "2412990"
  },
  {
    "text": "network for the synchronization to complete and you you're you won't be doing sufficient scaling so in that case",
    "start": "2412990",
    "end": "2419230"
  },
  {
    "text": "I would encourage the operator of that system to use like 25 gigabits or maybe",
    "start": "2419230",
    "end": "2425710"
  },
  {
    "text": "40 gigabits of networking and not have that bottleneck looking at the parameter",
    "start": "2425710",
    "end": "2431109"
  },
  {
    "text": "size distribution so every one of these layers will have its of its inputs and and weights we see on the left side",
    "start": "2431109",
    "end": "2439450"
  },
  {
    "text": "everything that's smaller than four four kilobytes or a lot of lot of them actually the majority is will around 4k",
    "start": "2439450",
    "end": "2445839"
  },
  {
    "text": "and these reduction operations will be very much bound by the by the latency we",
    "start": "2445839",
    "end": "2452680"
  },
  {
    "text": "have between machines right it doesn't take a whole lot of time to send a 4 kilobyte packet but it's if you a great",
    "start": "2452680",
    "end": "2459039"
  },
  {
    "text": "the the total amount of time that you send stuff around you're gonna be waiting on the latency is gonna be the",
    "start": "2459039",
    "end": "2465250"
  },
  {
    "text": "dominant factor whereas if we move to the to the right hand side we have a couple very large weights where the the",
    "start": "2465250",
    "end": "2474490"
  },
  {
    "text": "bandwidth is going to be the dominating factor so to illustrate this a little bit more we can express the scaling",
    "start": "2474490",
    "end": "2481630"
  },
  {
    "text": "efficiency that we get as the time it takes to do a forward pass and a backward pass and divide it over the",
    "start": "2481630",
    "end": "2489369"
  },
  {
    "text": "time it takes for forward pass plus the maximum of a backwards pass and the time",
    "start": "2489369",
    "end": "2495789"
  },
  {
    "text": "it takes to reduce all your parameters give me your network bandwidth and if you look at this curve it can become a",
    "start": "2495789",
    "end": "2504819"
  },
  {
    "text": "problem fairly quickly if you're a model as large so taking taking the network speed into account this is very",
    "start": "2504819",
    "end": "2511630"
  },
  {
    "text": "important there some observations for multi machine we've been doing this for",
    "start": "2511630",
    "end": "2517599"
  },
  {
    "text": "a while now at Facebook and we've seen therefore we've seen like everything that can happen if you",
    "start": "2517599",
    "end": "2524040"
  },
  {
    "text": "have a single slow machine whatever it's doing it can be doing like some management tasks on the side if it's",
    "start": "2524040",
    "end": "2529260"
  },
  {
    "text": "slowing down that trainer a little bit it will slow down the entire collective given that all of these operations are",
    "start": "2529260",
    "end": "2535020"
  },
  {
    "text": "synchronous right they have to wait for each other so if you have a single slow machine it slows everybody down it's",
    "start": "2535020",
    "end": "2541680"
  },
  {
    "text": "only skilled well if we're on the left-hand side of that efficiency C curve if the time it takes to reduce",
    "start": "2541680",
    "end": "2547440"
  },
  {
    "text": "these gradients is lower than the time it takes to do the backward pass otherwise we start waiting on the",
    "start": "2547440",
    "end": "2553920"
  },
  {
    "text": "network and and our efficiency goes goes goes way way down applying weak scaling",
    "start": "2553920",
    "end": "2562350"
  },
  {
    "text": "can be done to extremes and it has been done to extremes the current state of the art was a paper from preferred",
    "start": "2562350",
    "end": "2570180"
  },
  {
    "text": "networks in Japan and they scaled it to 30 mm on a set of 1,000 GPUs at Facebook",
    "start": "2570180",
    "end": "2578250"
  },
  {
    "text": "we did it up to 8,000 on a set of 200 656 GPUs earlier this year which which",
    "start": "2578250",
    "end": "2585300"
  },
  {
    "text": "inspired inspired this work and it's not a free lunch like if you if you keep",
    "start": "2585300",
    "end": "2590730"
  },
  {
    "text": "scaling this up let's assume you have 2,000 GPUs that your disposal you said well we'll use a batch size of 64 K well",
    "start": "2590730",
    "end": "2597390"
  },
  {
    "text": "that doesn't mean that the accuracy is going to be equivalent after after training that model so it's it's not",
    "start": "2597390",
    "end": "2604170"
  },
  {
    "text": "free it does still require tuning to to get to this way higher skill right",
    "start": "2604170",
    "end": "2610850"
  },
  {
    "text": "talking about AWS a little bit so so far this has been generic coffee too and you can run this on like machines under your",
    "start": "2610850",
    "end": "2618330"
  },
  {
    "text": "desk a supercomputer right it doesn't matter like as long as you have a network cable between em you can you can",
    "start": "2618330",
    "end": "2624510"
  },
  {
    "text": "run this and you're good now luckily AWS gives it gives us the virtual equivalent",
    "start": "2624510",
    "end": "2630090"
  },
  {
    "text": "of that right you can just launch it with a bunch of machines and and as long as they're in the same V PC they can",
    "start": "2630090",
    "end": "2636150"
  },
  {
    "text": "they can talk to each other so that's that's great what Joe talked about before a little bit amazon publishes the",
    "start": "2636150",
    "end": "2643620"
  },
  {
    "text": "deep learning ami which contains all of these different frameworks including coffee to and including the latest CUDA",
    "start": "2643620",
    "end": "2650220"
  },
  {
    "text": "and Quyen libraries from from Nvidia the rendezvous step that we need to",
    "start": "2650220",
    "end": "2657340"
  },
  {
    "text": "perform to do distributed training in cafe 2 can be performed that task can be",
    "start": "2657340",
    "end": "2662470"
  },
  {
    "text": "performed by Amazon ElastiCache so let's the cache can provision read as instance for you that we can then pass to all of",
    "start": "2662470",
    "end": "2670600"
  },
  {
    "text": "the trainer's that are participating and they can perform service discovery there and VPC for private network this is kind",
    "start": "2670600",
    "end": "2680590"
  },
  {
    "text": "of implied I guess we need a total",
    "start": "2680590",
    "end": "2686230"
  },
  {
    "text": "access between all these machines so having firewalls in between is just a",
    "start": "2686230",
    "end": "2691630"
  },
  {
    "text": "big burden effectively because we'd have to go and open a ton of ports from",
    "start": "2691630",
    "end": "2697240"
  },
  {
    "text": "everybody to everybody so ideally just use PPC and have a open access open",
    "start": "2697240",
    "end": "2704530"
  },
  {
    "text": "access to everybody optionally I've played with this a little bit is using",
    "start": "2704530",
    "end": "2710050"
  },
  {
    "text": "EFS for storing checkpoints within fast you get a single mount point an NFS",
    "start": "2710050",
    "end": "2715120"
  },
  {
    "text": "mount point that can be shared between different machines and that's perfect for storing let's say like checkpoints",
    "start": "2715120",
    "end": "2721930"
  },
  {
    "text": "optionally also input like training data and that type of stuff so you can use",
    "start": "2721930",
    "end": "2729520"
  },
  {
    "text": "coffee to directly from the ami this is going to be the stable version of cafe 2 which is a time of at like as of now is",
    "start": "2729520",
    "end": "2737050"
  },
  {
    "text": "0.8 point one we also publish docker images for for cafe to both the stable",
    "start": "2737050",
    "end": "2745270"
  },
  {
    "text": "version but also denied lis version so if you want to have the latest in graces then you can go on docker hub and pull",
    "start": "2745270",
    "end": "2750970"
  },
  {
    "text": "down the latest copy to image and use it",
    "start": "2750970",
    "end": "2756040"
  },
  {
    "text": "Nvidia docker for execution on on these these GPU instances one last thing on",
    "start": "2756040",
    "end": "2764500"
  },
  {
    "text": "running coffee to an AWS if you have a large data set that you're working with",
    "start": "2764500",
    "end": "2769630"
  },
  {
    "text": "you still have to bring that on to the machines and that can be non-trivial I I",
    "start": "2769630",
    "end": "2776020"
  },
  {
    "text": "downloaded a large data set off of s3 just today and I got a maximum",
    "start": "2776020",
    "end": "2782380"
  },
  {
    "text": "throughput of roughly 150 megabytes which is decent I guess I mean that's that's that's pretty great but if you're",
    "start": "2782380",
    "end": "2787880"
  },
  {
    "text": "walking about 140 gigabit 140 gigabytes it's gonna take 20 minutes and if you",
    "start": "2787880",
    "end": "2793039"
  },
  {
    "text": "plan on training for 20 minutes you're gonna have a machine that sits idle for 20 minutes while it's downloading all",
    "start": "2793039",
    "end": "2798769"
  },
  {
    "text": "the data and then 20 minutes for trading so there's more work to do there there's",
    "start": "2798769",
    "end": "2804410"
  },
  {
    "text": "more opportunity there as well to to improve improve that situation all right",
    "start": "2804410",
    "end": "2809589"
  },
  {
    "text": "so with that I want to show you a little",
    "start": "2809589",
    "end": "2815450"
  },
  {
    "text": "demo of what is what this looks like let's see if it still works so what I",
    "start": "2815450",
    "end": "2820819"
  },
  {
    "text": "did was provision some some p3 instances",
    "start": "2820819",
    "end": "2825970"
  },
  {
    "text": "this morning okay to zero so I",
    "start": "2825970",
    "end": "2834430"
  },
  {
    "text": "provisioned some p3 instances the large ones you have the single GPU ones and then you have the a GPU ones these are",
    "start": "2834430",
    "end": "2840890"
  },
  {
    "text": "the a GPU ones and I can show you by running Nvidia SMI and that's that's",
    "start": "2840890",
    "end": "2846619"
  },
  {
    "text": "great we see we see eight Tests live v1 hundreds each of them have 16 Giga bits",
    "start": "2846619",
    "end": "2852170"
  },
  {
    "text": "of gigabytes of memory and to to run a",
    "start": "2852170",
    "end": "2857809"
  },
  {
    "text": "trainer what I did was used a nightly build of cafe 2 and create a little",
    "start": "2857809",
    "end": "2863150"
  },
  {
    "text": "little script so that I can kind of walk you through all the stuff that that we",
    "start": "2863150",
    "end": "2868400"
  },
  {
    "text": "have to specify and kind of fill it fill in the blanks that I didn't cover yet so",
    "start": "2868400",
    "end": "2873440"
  },
  {
    "text": "in this case we're gonna use the Nvidia runtime this is this is a docker wrapper",
    "start": "2873440",
    "end": "2879680"
  },
  {
    "text": "that is built and maintained by Nvidia whose task it is to plug through the",
    "start": "2879680",
    "end": "2886369"
  },
  {
    "text": "Nvidia driver libraries into a container otherwise you don't have access to to",
    "start": "2886369",
    "end": "2892400"
  },
  {
    "text": "cooed an Nvidia driver and continues GPU we're closed we have to pass through the network as well by the full docker runs",
    "start": "2892400",
    "end": "2899180"
  },
  {
    "text": "with network isolation and in this case we want these machines to talk to each other so we bypass the the network",
    "start": "2899180",
    "end": "2907309"
  },
  {
    "text": "isolation and just inherit the networking namespace of the host so we can use the hosts networking interface",
    "start": "2907309",
    "end": "2913630"
  },
  {
    "text": "we use the coffee to docker image image off of off of master and we run an",
    "start": "2913630",
    "end": "2920210"
  },
  {
    "text": "example here and this is the resident 50 trainer or a ResNet 50 trainer I think if you would",
    "start": "2920210",
    "end": "2926880"
  },
  {
    "text": "if you would take all the code that I talked about today put that in a file you'll you'd have roughly the equivalent",
    "start": "2926880",
    "end": "2931980"
  },
  {
    "text": "of what's what's in this one but it also takes some arguments it have some argument parsing and some logging and",
    "start": "2931980",
    "end": "2937650"
  },
  {
    "text": "that type of stuff we specify to use a GPUs total batch size of this run of 512",
    "start": "2937650",
    "end": "2944970"
  },
  {
    "text": "so we'll end up if you recall that efficiency graph of throughput over",
    "start": "2944970",
    "end": "2950880"
  },
  {
    "text": "batch size we'll end up all the way on the right end of that of that graph with 64 images per second or 64 images per",
    "start": "2950880",
    "end": "2958680"
  },
  {
    "text": "batch per GPU we specify the image size we specify a workspace limit in this",
    "start": "2958680",
    "end": "2965430"
  },
  {
    "text": "case because I wasn't able to get the training data on these machines in time the training that does know so we don't",
    "start": "2965430",
    "end": "2973380"
  },
  {
    "text": "we're not gonna see a bottleneck based off of ready reading and decoding training data and the last four are",
    "start": "2973380",
    "end": "2979980"
  },
  {
    "text": "unused for for a single run but we'll we'll see it later for a for a multi machine run so I if I just run this then",
    "start": "2979980",
    "end": "2987990"
  },
  {
    "text": "it will start a docker container it will find the a GPUs it will set an initial",
    "start": "2987990",
    "end": "2993960"
  },
  {
    "text": "parameter sink like if you start training and you initialize that with random values you want to make sure all",
    "start": "2993960",
    "end": "3000260"
  },
  {
    "text": "the GPUs have the same random values otherwise you start with with a",
    "start": "3000260",
    "end": "3005330"
  },
  {
    "text": "disadvantage the member in this case is a a graph transformation technique that",
    "start": "3005330",
    "end": "3010490"
  },
  {
    "text": "cafe 2 uses to decrease the memory memory usage and then whenever we see",
    "start": "3010490",
    "end": "3017360"
  },
  {
    "text": "this message where where it says starting at bug it will start all the operators all the memory is allocated lazily however so the first step after",
    "start": "3017360",
    "end": "3024920"
  },
  {
    "text": "starting a book in this case takes longer because the process ends up",
    "start": "3024920",
    "end": "3029960"
  },
  {
    "text": "allocating roughly 80 gigabytes of of GPU memory and that's not free I mean it",
    "start": "3029960",
    "end": "3036590"
  },
  {
    "text": "takes take some time so after warming up a little bit and everything's done all",
    "start": "3036590",
    "end": "3044060"
  },
  {
    "text": "we're processing roughly 2100 images per second and you can compare this to the",
    "start": "3044060",
    "end": "3050810"
  },
  {
    "text": "previous generation with which was NVIDIA Pascal's which do roughly 40% less of this and",
    "start": "3050810",
    "end": "3058610"
  },
  {
    "text": "then the generation before that which would do another 40% of that so the the",
    "start": "3058610",
    "end": "3063770"
  },
  {
    "text": "improvements that have been made on the performance side over the past couple of years are actually are pretty pretty",
    "start": "3063770",
    "end": "3070600"
  },
  {
    "text": "pretty intense right so let me just quit this because we're training on bogus",
    "start": "3070600",
    "end": "3076070"
  },
  {
    "text": "data this is not going to be well giving us a nice model to work with so instead",
    "start": "3076070",
    "end": "3082010"
  },
  {
    "text": "let's take a take a quick look at what what happens if we if we run run this on",
    "start": "3082010",
    "end": "3087530"
  },
  {
    "text": "two machines so I have the same instance zero here but I also have an in also an",
    "start": "3087530",
    "end": "3094820"
  },
  {
    "text": "instance one I guess it kills connections after a while and let's see",
    "start": "3094820",
    "end": "3103310"
  },
  {
    "text": "you so will cool run do I have another command in here so no run and we'll",
    "start": "3103310",
    "end": "3109520"
  },
  {
    "text": "specify that we gonna run with two two instances and I'm gonna be number zero",
    "start": "3109520",
    "end": "3117020"
  },
  {
    "text": "and we're going to specify a run ID so that they can find each other run ideas is kind of the I get token you can think",
    "start": "3117020",
    "end": "3123590"
  },
  {
    "text": "of it as a token so one two three four five six seven eight here we're gonna",
    "start": "3123590",
    "end": "3128869"
  },
  {
    "text": "run the exact same commands and notice this is a different different machine from the from the first one I'm charged",
    "start": "3128869",
    "end": "3134540"
  },
  {
    "text": "to 1 specify the same run ID 1 2 3 8 and",
    "start": "3134540",
    "end": "3142480"
  },
  {
    "text": "let's see if I made any typos here but what happens now is at the",
    "start": "3142480",
    "end": "3149510"
  },
  {
    "text": "initialization step we'll do that same that that rendezvous step right so both",
    "start": "3149510",
    "end": "3156440"
  },
  {
    "text": "machines will set a bunch of keys which hold TCP addresses of sockets that",
    "start": "3156440",
    "end": "3162350"
  },
  {
    "text": "they're listening on and after they find the addresses of their peers they'll go",
    "start": "3162350",
    "end": "3167869"
  },
  {
    "text": "on and connect connect to each other all right there's some docker related thing",
    "start": "3167869",
    "end": "3174380"
  },
  {
    "text": "there but we can ignore it right now both machines are doing all of the",
    "start": "3174380",
    "end": "3179540"
  },
  {
    "text": "initialization again so they're doing all of that memory allocation that's like roughly 80 gigabytes I see that if",
    "start": "3179540",
    "end": "3187700"
  },
  {
    "text": "we run this we we roughly end up allocating 10 gigabytes per of memory per GPU with",
    "start": "3187700",
    "end": "3193770"
  },
  {
    "text": "a batch size of 64 all right so what's",
    "start": "3193770",
    "end": "3199710"
  },
  {
    "text": "happening well we see a little bit higher throughput and that's good because that's what we want right we want if we use 2 machines and we spent",
    "start": "3199710",
    "end": "3207450"
  },
  {
    "text": "twice as many twice as much money we want to see like ideally see twice the",
    "start": "3207450",
    "end": "3212790"
  },
  {
    "text": "throughput and in this case we do right we saw 2100 before we actually get a little super linear improvement here",
    "start": "3212790",
    "end": "3219630"
  },
  {
    "text": "like it's not 20 4200 but actually 4400 this slight difference has to do with",
    "start": "3219630",
    "end": "3227450"
  },
  {
    "text": "the interleaving of operations that ends up being different if you're only",
    "start": "3227450",
    "end": "3232890"
  },
  {
    "text": "running on a single machine versus on multiple machines or you can imagine that the updates will be staggered a",
    "start": "3232890",
    "end": "3238770"
  },
  {
    "text": "little bit wider which causes less contention on the GPU and ends up with a little sub linear bonus bonus here so",
    "start": "3238770",
    "end": "3247080"
  },
  {
    "text": "with that I've shown you that I'm not this is are not selling snake oil this",
    "start": "3247080",
    "end": "3253320"
  },
  {
    "text": "is real real stuff it works all right we get awesome performance we got awesome performance on AWS out-of-the-box and I",
    "start": "3253320",
    "end": "3261620"
  },
  {
    "text": "think with that I'd like to hand it back over to Joe for a little bit we're gonna",
    "start": "3261620",
    "end": "3267420"
  },
  {
    "text": "wrap up quick so we're running out of time we can take a few questions if folks want to start lining up at the",
    "start": "3267420",
    "end": "3273330"
  },
  {
    "text": "microphones there we can switch back to the slides cool so I think we're again",
    "start": "3273330",
    "end": "3284430"
  },
  {
    "text": "out of time but I think that the key takeaways here you can you can see anyway support cafe to these part number",
    "start": "3284430",
    "end": "3291690"
  },
  {
    "text": "of different frameworks so we really support cafes too though working closely with Facebook you can also get started",
    "start": "3291690",
    "end": "3297690"
  },
  {
    "text": "quickly at Amazon ai slash Ami's so check that out Sports Cafe to PI torch a",
    "start": "3297690",
    "end": "3304020"
  },
  {
    "text": "number of frameworks as well as you can go to cafe to AI and also see a lot of",
    "start": "3304020",
    "end": "3309330"
  },
  {
    "text": "the work that Peters been working on there and get started there with a lot of code examples and like so why don't",
    "start": "3309330",
    "end": "3315690"
  },
  {
    "text": "we almost out of time I want to take a couple of questions while we start right here thanks Peter for the awesome",
    "start": "3315690",
    "end": "3322670"
  },
  {
    "text": "presentation the question I have is do you actually need a parameter server or",
    "start": "3322670",
    "end": "3329029"
  },
  {
    "text": "something equivalent in order to do the gradient calculation from all the",
    "start": "3329029",
    "end": "3334519"
  },
  {
    "text": "different machines yes so this is kind of a philosophical thing almost I guess",
    "start": "3334519",
    "end": "3339799"
  },
  {
    "text": "in in in in this case where we have all machines doing the same reduction you",
    "start": "3339799",
    "end": "3344930"
  },
  {
    "text": "you don't I think that what I've seen so far parameter shares are great if the",
    "start": "3344930",
    "end": "3350990"
  },
  {
    "text": "size of those parameters becomes too big right and you don't have to update the",
    "start": "3350990",
    "end": "3357049"
  },
  {
    "text": "whole thing at once then it's great to have a parameter server in this case you",
    "start": "3357049",
    "end": "3362059"
  },
  {
    "text": "could also choose to use it but it would have vastly different performance characteristics which is why for the the",
    "start": "3362059",
    "end": "3369049"
  },
  {
    "text": "distributed support from the coffee to parallel I like data data parallel model",
    "start": "3369049",
    "end": "3374660"
  },
  {
    "text": "helpers is is doing doing all of the synchronization without a parameter",
    "start": "3374660",
    "end": "3381710"
  },
  {
    "text": "server so it's like all tall or like tree based or ring based type",
    "start": "3381710",
    "end": "3387170"
  },
  {
    "text": "communication so still there needs to be some synchronization happening across all the different machines yes of making",
    "start": "3387170",
    "end": "3395059"
  },
  {
    "text": "sure that every one of those machines has actually communicated back so so",
    "start": "3395059",
    "end": "3401690"
  },
  {
    "text": "that that's going to be a kind of implied by the operator start and end so",
    "start": "3401690",
    "end": "3407779"
  },
  {
    "text": "we capture this synchronization step in a single operator so from the cafe graphs perspective we start with an",
    "start": "3407779",
    "end": "3414049"
  },
  {
    "text": "input that is unsynchronized and once it produces the output everything is synchronized okay thank you let's say",
    "start": "3414049",
    "end": "3419809"
  },
  {
    "text": "one more question I think we're out of time and we can hang out here for a few minutes and chat as well so why we take",
    "start": "3419809",
    "end": "3425079"
  },
  {
    "text": "so I'm an experienced software engineer and I want to get started with machine learning is cafe to a good choice to",
    "start": "3425079",
    "end": "3433400"
  },
  {
    "text": "start playing around with and if so besides sort of the tools Amazon's been",
    "start": "3433400",
    "end": "3438920"
  },
  {
    "text": "provided like what about like tutorials and references initial sort of tasks to",
    "start": "3438920",
    "end": "3445039"
  },
  {
    "text": "tackle like how do i how do I get started yeah I think there's there's",
    "start": "3445039",
    "end": "3450410"
  },
  {
    "text": "there's four so for coffee - we have a couple of tutorial out there which are like ipython notebooks that you can I'd load up on",
    "start": "3450410",
    "end": "3457970"
  },
  {
    "text": "one of these instances are you have to allocate a GPU instance on Amazon start",
    "start": "3457970",
    "end": "3464930"
  },
  {
    "text": "a notebook and then kind of walk through the tutorials besides coffee to a kind",
    "start": "3464930",
    "end": "3471200"
  },
  {
    "text": "of part of the larger Facebook family of machine learning tooling I also recommend giving part what you",
    "start": "3471200",
    "end": "3478220"
  },
  {
    "text": "look that's that's really great and kind of the whole the whole idea behind onyx",
    "start": "3478220",
    "end": "3485480"
  },
  {
    "text": "is that you can take either framework and productionize it in like just the",
    "start": "3485480",
    "end": "3492050"
  },
  {
    "text": "fasten that's available so it doesn't really matter which tool you use for getting started for like kicking the",
    "start": "3492050",
    "end": "3498140"
  },
  {
    "text": "tires for like figuring out how all of this stuff works once you want to productionize it you can use the fastest",
    "start": "3498140",
    "end": "3503660"
  },
  {
    "text": "runtime out there so I think like take a look at at the cafe two tutorials also",
    "start": "3503660",
    "end": "3509420"
  },
  {
    "text": "take a look at my torch tutorials and yeah I think one one last thing I think",
    "start": "3509420",
    "end": "3515060"
  },
  {
    "text": "you know check out sage maker you can actually use plugging Amazon sorcerer but you could actually take the docker",
    "start": "3515060",
    "end": "3521030"
  },
  {
    "text": "container that is provided from Caffe to actually bring that to Sage maker so you",
    "start": "3521030",
    "end": "3526100"
  },
  {
    "text": "really choose your framework that way and that actually I mean the brilliance of the distributed deep learning work",
    "start": "3526100",
    "end": "3531380"
  },
  {
    "text": "that Peters been doing with with the other Facebook folks is amazing but if you don't want to go through everything",
    "start": "3531380",
    "end": "3537080"
  },
  {
    "text": "that your goes through to distribute that's actually taken care of for you with sage maker and you can actually focus and actually try out Jupiter",
    "start": "3537080",
    "end": "3543140"
  },
  {
    "text": "notebooks that way start actually playing with real problems started reporting data sets and it really gets",
    "start": "3543140",
    "end": "3548390"
  },
  {
    "text": "started quickly it distributed in a distributed manner so that might be a good place to start as well awesome",
    "start": "3548390",
    "end": "3553820"
  },
  {
    "text": "thanks",
    "start": "3553820",
    "end": "3556240"
  }
]