[
  {
    "start": "0",
    "end": "93000"
  },
  {
    "text": "hello everyone my name is Anthony virtuoso and I'm a principal engineer with AWS today I'm pleased to announce",
    "start": "30",
    "end": "7170"
  },
  {
    "text": "and introduce you to a new feature of Amazon Athena that enables you to query new sources of data with the same",
    "start": "7170",
    "end": "13290"
  },
  {
    "text": "easy-to-use fully managed SQL interface that you've come to expect when using Athena with s3 available today in",
    "start": "13290",
    "end": "22289"
  },
  {
    "text": "preview Amazon Athena's new federated query capabilities allow you to connect Athena to 10 new data sources or use our",
    "start": "22289",
    "end": "30630"
  },
  {
    "text": "open source sdk to connect Athena to your own custom sources and storage formats beyond adding new sources of",
    "start": "30630",
    "end": "37050"
  },
  {
    "text": "data Athena federated queries can also take advantage of your own custom UDF's and native support for stage maker inference",
    "start": "37050",
    "end": "45110"
  },
  {
    "text": "to participate in this preview simply create an Athena workgroup called Amazon",
    "start": "45110",
    "end": "50280"
  },
  {
    "text": "Athena preview functionality any queries you run from this workgroup in the US East one North Virginia region",
    "start": "50280",
    "end": "57180"
  },
  {
    "text": "will be able to use this new federated query capability later in this presentation I'll show you how to run",
    "start": "57180",
    "end": "63149"
  },
  {
    "text": "such queries and even write your own UDF's and connector to a custom data source we built Amazon Athena federated",
    "start": "63149",
    "end": "71130"
  },
  {
    "text": "queries to help you generate insights and break the data silos that stop you from getting the most out of your data",
    "start": "71130",
    "end": "76729"
  },
  {
    "text": "let's take a look at a scenario that we see frequently when helping customers with their data Lake strategies as your",
    "start": "76729",
    "end": "83070"
  },
  {
    "text": "company's technology estate grows selecting the best fit for purpose datastore for each application you can",
    "start": "83070",
    "end": "89009"
  },
  {
    "text": "end up with useful data siloed into various disjoint systems we can use this",
    "start": "89009",
    "end": "95189"
  },
  {
    "start": "93000",
    "end": "264000"
  },
  {
    "text": "generic ecommerce architecture to illustrate what we mean by data silos imagine you were an Operations Manager",
    "start": "95189",
    "end": "101630"
  },
  {
    "text": "responsible for keeping this application running smoothly by many measures this estate seems well designed it separated",
    "start": "101630",
    "end": "108720"
  },
  {
    "text": "key responsibilities into different micro services could use application specific bbc's to improve its security",
    "start": "108720",
    "end": "114720"
  },
  {
    "text": "posture and each datastore has been carefully selected to align with the needs of the specific business function",
    "start": "114720",
    "end": "121350"
  },
  {
    "text": "it enables let's take a closer look at the database selections the payment",
    "start": "121350",
    "end": "126600"
  },
  {
    "text": "service needs to support a large number of payment transactions the use case required a write optimized datastore",
    "start": "126600",
    "end": "132090"
  },
  {
    "text": "capable of scaling horizontally and offering different storage tiers to enable long-term retention the team",
    "start": "132090",
    "end": "137700"
  },
  {
    "text": "selected HBase running on EMR and backed by s3 for this use case the order",
    "start": "137700",
    "end": "143370"
  },
  {
    "text": "processing service needs to keep track of active orders providing a low latency interface for downstream systems to get",
    "start": "143370",
    "end": "149100"
  },
  {
    "text": "basic details about the status of an order the team elected to use Redis here because of its low latency read and",
    "start": "149100",
    "end": "155190"
  },
  {
    "text": "write capabilities and because all the data could fit comfortably in memory the",
    "start": "155190",
    "end": "160440"
  },
  {
    "text": "personalization service had to enable the company's global strategy by supporting varying definitions of what a",
    "start": "160440",
    "end": "165750"
  },
  {
    "text": "customer or their shipping address might contain they opted to use document DB to fit their no sequel use case the",
    "start": "165750",
    "end": "172920"
  },
  {
    "text": "customer facing ecommerce website itself opted to use Amazon Arora to meet the read heavy demands of their product",
    "start": "172920",
    "end": "178950"
  },
  {
    "text": "catalog they valued the scalable read replicas and automated management tools that came with RDS the operations team",
    "start": "178950",
    "end": "186630"
  },
  {
    "text": "selected cloud watch for their log and metrics needs since an integrated nicely with the AWS services they were already",
    "start": "186630",
    "end": "192570"
  },
  {
    "text": "using elsewhere and offered convenient api's for submitting custom application data the business also established an",
    "start": "192570",
    "end": "200489"
  },
  {
    "text": "enterprise data warehouse and associated set of ETLs to offer long term retention and reporting on the businesses KPIs",
    "start": "200489",
    "end": "207380"
  },
  {
    "text": "redshift was their preferred solution because of its performance in these write once read many scenarios and the",
    "start": "207380",
    "end": "213329"
  },
  {
    "text": "ability to separate storage and compute via redshift spectrum lastly the shipping service needed to",
    "start": "213329",
    "end": "220230"
  },
  {
    "text": "normalize shipment processing and tracking for the company's delivery fleet they opted for dynamodb because of",
    "start": "220230",
    "end": "225600"
  },
  {
    "text": "its ability to meet their tight performance requirements and need for varied schema now that we've stepped through this",
    "start": "225600",
    "end": "231959"
  },
  {
    "text": "architecture remember we're the Operations Manager we're in charge of keeping this running smoothly and it",
    "start": "231959",
    "end": "238829"
  },
  {
    "text": "just so happens that our support staff has noticed an increase in the number of customers reporting their orders in a",
    "start": "238829",
    "end": "244290"
  },
  {
    "text": "stuck State with key operational metrics and logs residing in nine different places how do you assess the scope of",
    "start": "244290",
    "end": "250890"
  },
  {
    "text": "this problem or even begin to do root cause analysis with Athena we can run a federated query to quickly find relevant",
    "start": "250890",
    "end": "258359"
  },
  {
    "text": "details even if we aren't experienced with each of the individual systems that make up this architecture I've switched",
    "start": "258359",
    "end": "265620"
  },
  {
    "start": "264000",
    "end": "442000"
  },
  {
    "text": "to my Athena console now because I want show you an example query we could run to literally answer that question how",
    "start": "265620",
    "end": "272790"
  },
  {
    "text": "many customer orders are stuck and is there anything they have in common that might lead us towards a root cause or",
    "start": "272790",
    "end": "278640"
  },
  {
    "text": "something that we should investigate further so I've recreated that architecture that we walked through a moment ago in my AWS",
    "start": "278640",
    "end": "285420"
  },
  {
    "text": "account and I'll walk you through the query I've created using Athena federated query support to do that so I",
    "start": "285420",
    "end": "294600"
  },
  {
    "text": "start my query by querying our cloud watch logs connector specifically I'm",
    "start": "294600",
    "end": "300600"
  },
  {
    "text": "going against the order processors log group in cloud watch logs and I'm querying a special table offered by this",
    "start": "300600",
    "end": "306540"
  },
  {
    "text": "connector called the all log streams table this table is essentially a view over all the log streams in that log",
    "start": "306540",
    "end": "312660"
  },
  {
    "text": "group I'm using some regular expression matching an extraction here to say that I only care about the log lines that are",
    "start": "312660",
    "end": "318960"
  },
  {
    "text": "not informational so presumably this is going to give me all the warnings and errors I'm doing a little bit more",
    "start": "318960",
    "end": "325320"
  },
  {
    "text": "pattern matching here and I'm saying that I want to extract the order ID from any log lines that match as well as the",
    "start": "325320",
    "end": "331620"
  },
  {
    "text": "log level then I'm going to be joining",
    "start": "331620",
    "end": "337140"
  },
  {
    "text": "that data against information from our Redis connector where here I'm going to",
    "start": "337140",
    "end": "342480"
  },
  {
    "text": "Redis and getting all the active orders so that I can get just a little bit more information alongside the the data that",
    "start": "342480",
    "end": "349290"
  },
  {
    "text": "I get from cloud watch logs and then joining that data against our CMDB",
    "start": "349290",
    "end": "355920"
  },
  {
    "text": "connector and this is a connector that allows you to query your AWS resources as if it were you know a typical",
    "start": "355920",
    "end": "361680"
  },
  {
    "text": "enterprise CMDB so I can do things like in this case I'm saying give me the",
    "start": "361680",
    "end": "367350"
  },
  {
    "text": "instance ID IP address and stat and state of any of the ec2 instances in",
    "start": "367350",
    "end": "372960"
  },
  {
    "text": "this account so that I can see the connectivity information and state of the ec2 instance that logged each of the",
    "start": "372960",
    "end": "379770"
  },
  {
    "text": "log lines that we get from cloud watch logs I'm then joining this information",
    "start": "379770",
    "end": "385440"
  },
  {
    "text": "with two different tables from document dB one that's going to give me the customers contact information and one",
    "start": "385440",
    "end": "391650"
  },
  {
    "text": "that's going to give me their shipping address for the order then I'm joining",
    "start": "391650",
    "end": "398070"
  },
  {
    "text": "against our shipment details that come from our dynamo DB connector and I'm",
    "start": "398070",
    "end": "403290"
  },
  {
    "text": "grabbing again the order number the shipment number the time this shipment was created and which carrier it used",
    "start": "403290",
    "end": "409010"
  },
  {
    "text": "before finally joining against our payments data using our HBase connector",
    "start": "409010",
    "end": "414510"
  },
  {
    "text": "and in this particular case I'm interested in what was the status of the payment what was the last four of the",
    "start": "414510",
    "end": "420479"
  },
  {
    "text": "credit card used for the payment and what clearing network did it use okay so",
    "start": "420479",
    "end": "426419"
  },
  {
    "text": "I'm going to go ahead and run this query and then we'll take a look at what we get back",
    "start": "426419",
    "end": "430970"
  },
  {
    "text": "okay and now we have our data back so we can see we grabbed our order ID from Redis and then join that against some of",
    "start": "442580",
    "end": "450080"
  },
  {
    "text": "the information from document DB about the customers contact information and HBase for some of the payment details",
    "start": "450080",
    "end": "456349"
  },
  {
    "text": "and status the shipping details from dynamo DB and then further over to the",
    "start": "456349",
    "end": "463009"
  },
  {
    "text": "side we can see we got back some information from ec2 about the instance that generated the log lines and in this",
    "start": "463009",
    "end": "469669"
  },
  {
    "text": "particular case we can actually see that we have two orders that are in a bit of a weird state and both of those orders",
    "start": "469669",
    "end": "475520"
  },
  {
    "text": "were processed by the same ec2 host so now we have a general idea of what the scope of impact is right seems like",
    "start": "475520",
    "end": "481729"
  },
  {
    "text": "there's only two orders impacted and we have this interesting commonality",
    "start": "481729",
    "end": "486800"
  },
  {
    "text": "between those stuck orders that they both went through the same order processor so one of our likely next",
    "start": "486800",
    "end": "492050"
  },
  {
    "text": "actions is going to be to go and take a deeper look at the logs for that particular host around that time and see if there's something going on there or",
    "start": "492050",
    "end": "498440"
  },
  {
    "text": "we might even choose to just immediately take that one host out of service while we continue to investigate so there you",
    "start": "498440",
    "end": "504740"
  },
  {
    "text": "have it we've run a query across six or seven different sources but what I'd like to do next is actually show you how",
    "start": "504740",
    "end": "510770"
  },
  {
    "text": "easy it is to deploy and connect athina to a new source okay so as you might",
    "start": "510770",
    "end": "517940"
  },
  {
    "text": "remember earlier I mentioned that Athena uses lambda functions in order to",
    "start": "517940",
    "end": "523070"
  },
  {
    "text": "connect to new data sources or your custom UDF one of the easiest ways for you to deploy one of these connectors",
    "start": "523070",
    "end": "529700"
  },
  {
    "text": "and connect the thena to your new source is to go to server list application repo and use one of the ready-made bundles",
    "start": "529700",
    "end": "535279"
  },
  {
    "text": "that the Athena team has authored and published there so I'm gonna go and I'm going to navigate to serve the SAP repo",
    "start": "535279",
    "end": "542800"
  },
  {
    "text": "I'm gonna go to available applications and I'm gonna search for Amazon the fina",
    "start": "546100",
    "end": "554270"
  },
  {
    "text": "Federation and make sure you check this box to show apps that create custom I am",
    "start": "554270",
    "end": "559459"
  },
  {
    "text": "roles and I'll get more into that in a moment but this Amazon Athena Federation author name is what you can look for to",
    "start": "559459",
    "end": "566240"
  },
  {
    "text": "have confidence that the connector you're about to deploy was authored by the Athena team alternatively you can go",
    "start": "566240",
    "end": "572839"
  },
  {
    "text": "to our github repository and build and deploy from source yourself but this tends to be a bit more convenient so for this example I want to",
    "start": "572839",
    "end": "580310"
  },
  {
    "text": "deploy our cloud watch logs connector so I'm going to go down to Athena cloud watch connector and it'll take me to a",
    "start": "580310",
    "end": "587750"
  },
  {
    "text": "page where I can see a bit more information about what this connector is going to do gives me links to the source code in a description so I can expand",
    "start": "587750",
    "end": "594770"
  },
  {
    "text": "the permissions here I can see that it's going to need access to read and write data to s3 this is how the connector",
    "start": "594770",
    "end": "600200"
  },
  {
    "text": "will get the data to Athena if the response size is too big for lambda it's",
    "start": "600200",
    "end": "605780"
  },
  {
    "text": "also going to need access to cloud watch logs and if I scroll down further I can see the readme with some of the detailed",
    "start": "605780",
    "end": "612260"
  },
  {
    "text": "configuration information and on the right-hand side some prompts for me to provide that configuration and thumb so",
    "start": "612260",
    "end": "618530"
  },
  {
    "text": "the first thing I need to fill out is the location of a s3 bucket that this connector can use for spilling large",
    "start": "618530",
    "end": "624110"
  },
  {
    "text": "responses to Athena all the data that gets written to that bucket is encrypted using one-time-use keys you can",
    "start": "624110",
    "end": "630710"
  },
  {
    "text": "optionally turn the encryption on or off but we recommend leaving it on and so it's on by default and in my case I",
    "start": "630710",
    "end": "636410"
  },
  {
    "text": "already have a bucket called Athena Federation spill and now we need to",
    "start": "636410",
    "end": "643730"
  },
  {
    "text": "specify a catalog name so catalog name ends up being the name of the lambda function that will be created and also",
    "start": "643730",
    "end": "649040"
  },
  {
    "text": "the name that I can use to reference this source from inside my Athena queries so I'm just gonna call it cloud",
    "start": "649040",
    "end": "655190"
  },
  {
    "text": "watch I'm gonna leave the encryption turned on I also get the choice of the",
    "start": "655190",
    "end": "661600"
  },
  {
    "text": "lambda memory limits and lambda timeout I'm gonna leave those as a default and lastly you can specify a spill prefix so",
    "start": "661600",
    "end": "668630"
  },
  {
    "text": "this is going to be a prefix within that spill bucket where all where any spills data will be written to make it a bit",
    "start": "668630",
    "end": "674270"
  },
  {
    "text": "easier for you to configure lifecycle policies to delete that data after the query is done I'm going to acknowledge",
    "start": "674270",
    "end": "680210"
  },
  {
    "text": "that that by deploying this it's going to grant that lambda function permission to read from pod watch logs I'm going to",
    "start": "680210",
    "end": "687500"
  },
  {
    "text": "scroll to the bottom and hit deploy okay",
    "start": "687500",
    "end": "693350"
  },
  {
    "text": "we can see that our connector has been deployed so now I'm going to switch back to the Athena console and we're going to",
    "start": "693350",
    "end": "698510"
  },
  {
    "text": "try using our connector in a query okay so let's start with the simple query this is actually a subset of the query",
    "start": "698510",
    "end": "705140"
  },
  {
    "text": "that we ran against our example architecture a couple minutes ago but I want to dissect this a little bit so you can get an understanding for you",
    "start": "705140",
    "end": "712440"
  },
  {
    "text": "know what's involved with querying one of these custom sources so in this case I'm just doing a select star on all the",
    "start": "712440",
    "end": "718170"
  },
  {
    "text": "columns from our cloud watch connector and this is the way I'm telling Athena that the table I'm about to query is",
    "start": "718170",
    "end": "724470"
  },
  {
    "text": "coming from or is a federated table is with this lambda : cloud watch there's",
    "start": "724470",
    "end": "731490"
  },
  {
    "text": "also an option that allows you to go and pre-register these federated sources with Athena so that you don't need to",
    "start": "731490",
    "end": "737010"
  },
  {
    "text": "use the lambda : prefix for the catalog but to keep things simple in this demo I'm using our registration less option",
    "start": "737010",
    "end": "743640"
  },
  {
    "text": "which allows me to do lambda colon and then the name of our lambda function to convey to Athena that it should Feder 8",
    "start": "743640",
    "end": "750020"
  },
  {
    "text": "2004 this query I'm querying our order processor log group and the all log",
    "start": "750020",
    "end": "756030"
  },
  {
    "text": "streams table and let's run our query",
    "start": "756030",
    "end": "760520"
  },
  {
    "text": "and so there you have it in just a couple of minutes we went from a couple",
    "start": "762140",
    "end": "768480"
  },
  {
    "text": "clicks to deploy to using our connector in the real query next I'd like to talk",
    "start": "768480",
    "end": "774090"
  },
  {
    "text": "to you a little bit more about how Athena federated queries work under the hood before going into a tutorial where",
    "start": "774090",
    "end": "781080"
  },
  {
    "text": "we'll step by step write a connector from scratch deploy it and use it in an",
    "start": "781080",
    "end": "786180"
  },
  {
    "text": "actual federated query during a federated query Amazon Athena delegates",
    "start": "786180",
    "end": "791940"
  },
  {
    "start": "788000",
    "end": "941000"
  },
  {
    "text": "portions of its query planning and distributed execution to lambda functions that you own and operate since",
    "start": "791940",
    "end": "798000"
  },
  {
    "text": "these lambda functions run in your account they offer you unprecedented levels of control over security connectivity and most importantly the",
    "start": "798000",
    "end": "805680"
  },
  {
    "text": "unique ability to customize Amazon Athena with your own code our team is written integrations with 10 different",
    "start": "805680",
    "end": "811920"
  },
  {
    "text": "SQL and no SQL databases including dynamodb document DB mysql Postgres",
    "start": "811920",
    "end": "818370"
  },
  {
    "text": "redshift Redis HBase and cloud watch all these connectors as well as the SDK that",
    "start": "818370",
    "end": "823950"
  },
  {
    "text": "you can use to integrate your own custom sources are open source and available today in our github repository in a",
    "start": "823950",
    "end": "830670"
  },
  {
    "text": "moment I'll walk you through a tutorial that goes from empty file through writing the code to connect Athena to",
    "start": "830670",
    "end": "835920"
  },
  {
    "text": "your custom source in UDF before ending with running a query that uses our custom source in UTS but first I'd like",
    "start": "835920",
    "end": "842700"
  },
  {
    "text": "to touch on one the most common questions I get asked by customers and fellow engineers that I've introduced to these new features people",
    "start": "842700",
    "end": "849570"
  },
  {
    "text": "are always curious how we use lambda for big data and while it's true that lambda functions are limited to a 15-minute",
    "start": "849570",
    "end": "856110"
  },
  {
    "text": "runtime and three gigabytes of working memory Athena's execution engine is able to make use of multiple often concurrent",
    "start": "856110",
    "end": "863250"
  },
  {
    "text": "lambda invocations to pipeline the execution of federated queries so that your query can run for longer than 15",
    "start": "863250",
    "end": "869580"
  },
  {
    "text": "minutes and make use of more than three gigabytes of memory as part of the",
    "start": "869580",
    "end": "874620"
  },
  {
    "text": "validation process that all our technical designs go through we rewrote Athena's current s3 integration to run",
    "start": "874620",
    "end": "880830"
  },
  {
    "text": "on lambda using the Amazon Athena query Federation SDK and then ran several load",
    "start": "880830",
    "end": "886380"
  },
  {
    "text": "tests to understand the throughput we could expect when using lambda in this way these are not exhaustive figures but",
    "start": "886380",
    "end": "892800"
  },
  {
    "text": "we were able to achieve greater than a hundred gigabit per second or 1.5 billion rows per second via lambda",
    "start": "892800",
    "end": "899100"
  },
  {
    "text": "runtime we expect these figures to improve when this feature reaches general availability our usage of apache",
    "start": "899100",
    "end": "905640"
  },
  {
    "text": "arrow as a standard data interchange format helps reduce the serialization overhead making it more practical to",
    "start": "905640",
    "end": "911340"
  },
  {
    "text": "query data where it lives without the need to manage ETLs the one caveat to this comes into play for sources that do",
    "start": "911340",
    "end": "917460"
  },
  {
    "text": "not support parallel scans in such cases your query may be subject to the 15-minute lambda runtime limit for the",
    "start": "917460",
    "end": "924330"
  },
  {
    "text": "table scan portion of your query we found that most data sources offer at least limited support for parallel scans",
    "start": "924330",
    "end": "930590"
  },
  {
    "text": "concurrency and other key characteristics of how your federated query executes are all within your",
    "start": "930590",
    "end": "935790"
  },
  {
    "text": "control and Athena provides reasonable default behaviors to get you started now I'd",
    "start": "935790",
    "end": "942450"
  },
  {
    "start": "941000",
    "end": "1260000"
  },
  {
    "text": "like to walk you through a tutorial that will show you how to write your own custom connector for enabling Athena to",
    "start": "942450",
    "end": "948570"
  },
  {
    "text": "talk to your data source in this tutorial we'll also write a couple user-defined functions all the material",
    "start": "948570",
    "end": "955680"
  },
  {
    "text": "that we're going to go through can be found on our github repository and in fact we're going to get started by",
    "start": "955680",
    "end": "960900"
  },
  {
    "text": "navigating to that tutorial in our github repo so I'm going to go to Athena - example this folder is essentially a",
    "start": "960900",
    "end": "968220"
  },
  {
    "text": "premade project that'll allow you to just fill in a few blanks and be up and running with your own connector in your",
    "start": "968220",
    "end": "974070"
  },
  {
    "text": "own UDF's when you click on that module you'll be",
    "start": "974070",
    "end": "979329"
  },
  {
    "text": "met with fairly extensive readme file that I'll talk to you about all the important pieces of a connector how a",
    "start": "979329",
    "end": "985209"
  },
  {
    "text": "thena talks to your connector but if we scroll down a bit we'll get to the piece",
    "start": "985209",
    "end": "990279"
  },
  {
    "text": "that we're going to really care about which is how do we build and deploy a connector the first step at least in",
    "start": "990279",
    "end": "997509"
  },
  {
    "text": "this example is to create and deploy a cloud 9 instance to use as your development environment however any",
    "start": "997509",
    "end": "1003209"
  },
  {
    "text": "development environment that supports Apache maven the AWS CLI and the AWS Sam",
    "start": "1003209",
    "end": "1009629"
  },
  {
    "text": "build tool should be sufficient so that'll work on Windows that'll work on Mac that'll work on most flavors of",
    "start": "1009629",
    "end": "1015509"
  },
  {
    "text": "Linux I've already gone ahead and set up a cloud 9 instance to show you how we're gonna walk through the tutorial I've",
    "start": "1015509",
    "end": "1021779"
  },
  {
    "text": "also already done step 2 which is to grab a copy of this github repo and put it on to my cloud 9 instance so we're",
    "start": "1021779",
    "end": "1030058"
  },
  {
    "text": "gonna pick up with step three which is installing the prerequisites for the development environment in this",
    "start": "1030059",
    "end": "1035579"
  },
  {
    "text": "particular case in our github repository we provide an environment prep tool",
    "start": "1035579",
    "end": "1040819"
  },
  {
    "text": "that'll work for most flavors of Amazon Linux if you look at the file you'll probably be able to adapt it for Mac and",
    "start": "1040819",
    "end": "1046770"
  },
  {
    "text": "other flavors of Linux as well so I'm gonna switch my window back over to my cloud 9 instance and we're gonna go",
    "start": "1046770",
    "end": "1052289"
  },
  {
    "text": "ahead and run this environment prep tool",
    "start": "1052289",
    "end": "1056179"
  },
  {
    "text": "so the I just ran the prep tool and it's asking me if I'm ok with installing Apache maven homebrew which is how it's",
    "start": "1059600",
    "end": "1066870"
  },
  {
    "text": "going to get an updated copy of the AWS CLI and the AWS and build tool I'm gonna go ahead and tell it yes it's probably",
    "start": "1066870",
    "end": "1073169"
  },
  {
    "text": "also going to prompt me at least one more time when it updates homebrew ok so",
    "start": "1073169",
    "end": "1083010"
  },
  {
    "text": "now it's grabbing updated copies of the AWS CLI and the in real Sam build tool",
    "start": "1083010",
    "end": "1090350"
  },
  {
    "text": "cool and it's it updated my environment so now if I flip back to the tutorial",
    "start": "1090350",
    "end": "1096600"
  },
  {
    "text": "let's see what our next step is i've already sourced our profile so we should",
    "start": "1096600",
    "end": "1103169"
  },
  {
    "text": "be good our next step is to actually build the Federation SDK so we're using a little",
    "start": "1103169",
    "end": "1109410"
  },
  {
    "text": "bit of a modified command here we're asking Nathan to skip running tasks tests and that's mostly just to speed up",
    "start": "1109410",
    "end": "1114929"
  },
  {
    "text": "this tutorial for the most part you should always leave tests enabled and I'm also gonna be redirecting my",
    "start": "1114929",
    "end": "1120450"
  },
  {
    "text": "standardout to a log file I found that that just makes the cloud 9 ID you run",
    "start": "1120450",
    "end": "1125490"
  },
  {
    "text": "this command a bit faster but in in normal practice you don't necessarily have to do that so I'm gonna make sure I",
    "start": "1125490",
    "end": "1132900"
  },
  {
    "text": "switch directories to the SDK itself I'm",
    "start": "1132900",
    "end": "1138000"
  },
  {
    "text": "gonna run it and this command will probably take a few minutes to build for the first time while it grabs all its dependencies from maven central",
    "start": "1138000",
    "end": "1145410"
  },
  {
    "text": "repository after the first time it'll end up having all that in its local cache and it'll be much faster okay and",
    "start": "1145410",
    "end": "1153240"
  },
  {
    "text": "our SDK has built we got no error messages to screen so that's a good sign let's go to the tutorial and see what",
    "start": "1153240",
    "end": "1160020"
  },
  {
    "text": "our next at this okay it looks like our",
    "start": "1160020",
    "end": "1167040"
  },
  {
    "text": "next step is actually to sit down and start writing the code for our connector but the very beginning of that actually",
    "start": "1167040",
    "end": "1174090"
  },
  {
    "text": "includes making sure that we have an s3 bucket ready so that we can so that we",
    "start": "1174090",
    "end": "1180120"
  },
  {
    "text": "can upload the sample data and then have a place to publish our connector to for service app repo which comes a bit later",
    "start": "1180120",
    "end": "1186600"
  },
  {
    "text": "so I'm gonna grab the command that it gives me here for creating a new bucket and I'm gonna go ahead and run it on my",
    "start": "1186600",
    "end": "1192450"
  },
  {
    "text": "cloud 9 instance I'm just gonna call it",
    "start": "1192450",
    "end": "1199470"
  },
  {
    "text": "Federation demo 1000 ok so now we have",
    "start": "1199470",
    "end": "1205320"
  },
  {
    "text": "our bucket alright now the next thing",
    "start": "1205320",
    "end": "1210900"
  },
  {
    "text": "that's asking us to do is to fill in the code and three three different files the",
    "start": "1210900",
    "end": "1216210"
  },
  {
    "text": "first is example metadata handler this is going to be the class that will define the functions that Athena will be",
    "start": "1216210",
    "end": "1223290"
  },
  {
    "text": "able to use to get metadata during query planning example record handler which will provide the functions Athena will",
    "start": "1223290",
    "end": "1230160"
  },
  {
    "text": "call to actually read the data for our table and then lastly example",
    "start": "1230160",
    "end": "1235260"
  },
  {
    "text": "user-defined function handler which is where we're going to put in our you to find functions all three of these",
    "start": "1235260",
    "end": "1241120"
  },
  {
    "text": "classes are going to end up getting built and packaged into a single lambda that we're going to deploy so we're",
    "start": "1241120",
    "end": "1246580"
  },
  {
    "text": "going to switch back to my cloud 9 environment and I'm gonna walk you through where those files are you know",
    "start": "1246580",
    "end": "1252160"
  },
  {
    "text": "what do they really mean and what are they comprised of all the information that I'm about to walk you through is available on this tutorial higher in the",
    "start": "1252160",
    "end": "1259210"
  },
  {
    "text": "page ok so on my left hand nav here in cloud 9 I'm going to expand the Athena",
    "start": "1259210",
    "end": "1264309"
  },
  {
    "text": "example folder I'm going to go into source main and then it should",
    "start": "1264309",
    "end": "1269770"
  },
  {
    "text": "autoexpand and show me all the files we're talking about the first one we want to modify is example metadata",
    "start": "1269770",
    "end": "1275679"
  },
  {
    "text": "handler so as I mentioned before example",
    "start": "1275679",
    "end": "1282850"
  },
  {
    "text": "metadata handler ends up providing the pieces of code that Athena is going to delegate certain parts of query planning",
    "start": "1282850",
    "end": "1289059"
  },
  {
    "text": "I think there are about 4 different four or five different functions in here we're going to implement the way the tutorial is set up is that these classes",
    "start": "1289059",
    "end": "1296890"
  },
  {
    "text": "essentially have working examples commented out so I'm gonna walk you through each of the functions what the",
    "start": "1296890",
    "end": "1302620"
  },
  {
    "text": "function is used for and show you how to implement it so the first metadata",
    "start": "1302620",
    "end": "1309400"
  },
  {
    "text": "operation that I've seen is going to expect to be able to delegate to our connector is listing schemas or listing",
    "start": "1309400",
    "end": "1314800"
  },
  {
    "text": "databases and that's handled by this function called do list schema names and what we need to do is essentially return",
    "start": "1314800",
    "end": "1321160"
  },
  {
    "text": "a list of schema names a list of strings from this function and you can see that in the tutorial we have a working",
    "start": "1321160",
    "end": "1326350"
  },
  {
    "text": "example commented out so I'm going to go ahead and uncomment this example where we're adding schema 1 to our response",
    "start": "1326350",
    "end": "1333790"
  },
  {
    "text": "now the names in general the names that you put here are not going to be important right they should be specific",
    "start": "1333790",
    "end": "1339490"
  },
  {
    "text": "to your data source this tutorial will only work with the names that are there so if you change it to say schema 10 or you know something",
    "start": "1339490",
    "end": "1346090"
  },
  {
    "text": "else you'll run into trouble later so leave that the names of the schemas tables and columns as you find them the",
    "start": "1346090",
    "end": "1353530"
  },
  {
    "text": "next function that we need to implement is do list tables similar to lists schemas we need to return a list of",
    "start": "1353530",
    "end": "1358690"
  },
  {
    "text": "tables for the given schema that Athena called us for so again I'm going to uncomment the example or I should say",
    "start": "1358690",
    "end": "1366460"
  },
  {
    "text": "the example solution and here we're returning a table called table one in this case for any schema",
    "start": "1366460",
    "end": "1372980"
  },
  {
    "text": "that Athena calls us for our next functions a little bit more involved",
    "start": "1372980",
    "end": "1379340"
  },
  {
    "text": "Athena's going to call this function get table when it wants to get the definition of a table and by definition",
    "start": "1379340",
    "end": "1385820"
  },
  {
    "text": "that means what are the columns in the table where there types are there any partition columns that this table has so",
    "start": "1385820",
    "end": "1392990"
  },
  {
    "text": "let's go ahead and walk through that and take care of the two Jews in this function as well but first if you here",
    "start": "1392990",
    "end": "1399200"
  },
  {
    "text": "is for us to define our partition columns in this particular example we're saying that our table is defined on",
    "start": "1399200",
    "end": "1404600"
  },
  {
    "text": "year-month-day I scroll down a little bit more I can see that our second to do",
    "start": "1404600",
    "end": "1410450"
  },
  {
    "text": "is to produce a schema for the table that we've been requested so I'm gonna uncomment this and then I'm gonna walk",
    "start": "1410450",
    "end": "1415940"
  },
  {
    "text": "you through what it means if you recall earlier I mentioned that much of our SDK is based to the pot on top of Apache",
    "start": "1415940",
    "end": "1422120"
  },
  {
    "text": "arrow so Apache arrow in addition to offering a bike compatible serialization",
    "start": "1422120",
    "end": "1428300"
  },
  {
    "text": "protocol also offers some other some other utilities for managing things like schema so here we're using a schema",
    "start": "1428300",
    "end": "1435050"
  },
  {
    "text": "builder to produce an Apache arrow schema that will tell afina what are the columns their types and",
    "start": "1435050",
    "end": "1441470"
  },
  {
    "text": "even the comments for each column that are part of this table so you can see that we start off by defining three",
    "start": "1441470",
    "end": "1447890"
  },
  {
    "text": "integer columns year-month-day this corresponds to the partition columns we mentioned a few lines above then we're",
    "start": "1447890",
    "end": "1455240"
  },
  {
    "text": "adding an account ID column which is a string we're adding an encrypted payload column which is also a string and I'll",
    "start": "1455240",
    "end": "1461720"
  },
  {
    "text": "talk a little bit more about that column later and then we're adding a complex column this column is of type struct and",
    "start": "1461720",
    "end": "1468380"
  },
  {
    "text": "has two child fields the first child field is an ID that's of type integer and the second one is column completed",
    "start": "1468380",
    "end": "1476060"
  },
  {
    "text": "which is a bit or a boolean and that presumably tells us whether or not this transaction is completed now we get to",
    "start": "1476060",
    "end": "1483260"
  },
  {
    "text": "the metadata portion the metadata or table properties portion of this schema so the important thing to remember about",
    "start": "1483260",
    "end": "1490190"
  },
  {
    "text": "the metadata that we're about to talk about is that Athena essentially treats it as opaque information with one key",
    "start": "1490190",
    "end": "1497090"
  },
  {
    "text": "exception if Athena finds a piece of metadata on your table whose name matches the name of a column it will",
    "start": "1497090",
    "end": "1504020"
  },
  {
    "text": "assume that that metadata is the documentation for that so that when you run a show table or",
    "start": "1504020",
    "end": "1509060"
  },
  {
    "text": "when you run a describe table command you'll be able to see documentation for each column any other metadata that you",
    "start": "1509060",
    "end": "1515450"
  },
  {
    "text": "apply to your table Athena will ignore but we'll pass along when it calls other parts of your lambda function for",
    "start": "1515450",
    "end": "1521900"
  },
  {
    "text": "example here we're defining a piece of metadata called partition columns and we're setting it equal to a year comma",
    "start": "1521900",
    "end": "1527870"
  },
  {
    "text": "month comma day Athena will ignore this and happily pass it along but that gives us an interesting piece of plumbing for",
    "start": "1527870",
    "end": "1535880"
  },
  {
    "text": "building our connector upon so that we don't have to keep making round trips to our actual meta story and I'll show you",
    "start": "1535880",
    "end": "1543590"
  },
  {
    "text": "how we're going to use that later but at this point we've completed our get table function the next function that we have",
    "start": "1543590",
    "end": "1550460"
  },
  {
    "start": "1548000",
    "end": "1888000"
  },
  {
    "text": "to implement is called get partitions and so Athena will call this function for our table and it will supply us the",
    "start": "1550460",
    "end": "1557510"
  },
  {
    "text": "predicate for the table or sorry I should say the predicate for the query so that we have the opportunity to do",
    "start": "1557510",
    "end": "1562670"
  },
  {
    "text": "some partition pruning if your table doesn't support partitions that's not a",
    "start": "1562670",
    "end": "1567890"
  },
  {
    "text": "problem you don't you simply don't need to implement this method and the SDK will take care of communicating",
    "start": "1567890",
    "end": "1573140"
  },
  {
    "text": "appropriately to Athena however in this example we're saying that our table is partitioned on year-month-day so we're",
    "start": "1573140",
    "end": "1580010"
  },
  {
    "text": "gonna go ahead and address the single to do in this function which is for us to generate our partitions for Athena so",
    "start": "1580010",
    "end": "1587150"
  },
  {
    "text": "I'm going to uncomment and uncomment it and I'll walk you through what's happening okay",
    "start": "1587150",
    "end": "1599600"
  },
  {
    "text": "so in this particular function we're looping over some number of years some number of months and some member of days",
    "start": "1599600",
    "end": "1605960"
  },
  {
    "text": "in order to produce the list of possible partitions for our table now obviously",
    "start": "1605960",
    "end": "1611630"
  },
  {
    "text": "this is a little bit of a contrived example and in the real world you should be querying your data source to find out what partitions are available and then",
    "start": "1611630",
    "end": "1619730"
  },
  {
    "text": "for each of the partitions that we've generated we're using this block writer which is a utility provided by the",
    "start": "1619730",
    "end": "1626560"
  },
  {
    "text": "Athena query Federation SDK to write our partition response to Athena this",
    "start": "1626560",
    "end": "1633320"
  },
  {
    "text": "utility essentially encapsulate many of the Apache aero functionalities that we would need to use to write this response",
    "start": "1633320",
    "end": "1640730"
  },
  {
    "text": "and gives us a little bit more of a streamlined interface for writing that response you can at any time choose to",
    "start": "1640730",
    "end": "1646070"
  },
  {
    "text": "to drop down into a more expert mode and just interact with Apache arrow directly",
    "start": "1646070",
    "end": "1652060"
  },
  {
    "text": "so essentially what we're saying here is that every time we're gonna call right rows for each of the partitions we",
    "start": "1652060",
    "end": "1657830"
  },
  {
    "text": "generate in our loops and the SDK will provide us a Apache arrow block and a",
    "start": "1657830",
    "end": "1664100"
  },
  {
    "text": "row number that we should write into and then we call block set value and we're",
    "start": "1664100",
    "end": "1669770"
  },
  {
    "text": "giving it the year the month of the day and we're providing it the values that correspond to the partition we're",
    "start": "1669770",
    "end": "1675320"
  },
  {
    "text": "generating set value will actually automatically apply some of the query",
    "start": "1675320",
    "end": "1680600"
  },
  {
    "text": "constraints for us so that if we keep track of the response values like we're doing here in this match to boolean at",
    "start": "1680600",
    "end": "1687020"
  },
  {
    "text": "the end of the call we know whether or not this partition satisfies any of the queries constraints if it does we return",
    "start": "1687020",
    "end": "1695150"
  },
  {
    "text": "one indicating that we've wrote one valid row into our response otherwise we return zero telling the SDK that we",
    "start": "1695150",
    "end": "1702170"
  },
  {
    "text": "haven't written anything and there you have it we've implemented our get partition function with partition",
    "start": "1702170",
    "end": "1708860"
  },
  {
    "text": "pruning okay our next function is going to be the last metadata function we need",
    "start": "1708860",
    "end": "1714170"
  },
  {
    "text": "to implement and this is going to be our second opportunity to control how Athena paralyzes the federated query",
    "start": "1714170",
    "end": "1722260"
  },
  {
    "text": "essentially Athena will call get splits for every partition that we returned in",
    "start": "1722260",
    "end": "1727640"
  },
  {
    "text": "the previous function call and that gives us the opportunity to tell Athena how it can split up the read of that",
    "start": "1727640",
    "end": "1733310"
  },
  {
    "text": "partition in this particular example we're saying that the partitions can't be further subdivided and so we're going",
    "start": "1733310",
    "end": "1740210"
  },
  {
    "text": "to be returning a single split for each of the partitions that Athena calls us for so I'm gonna scroll down and we'll",
    "start": "1740210",
    "end": "1746930"
  },
  {
    "text": "go ahead and address the troduce uncomment it and then I'll explain it to you okay so Athena will call this",
    "start": "1746930",
    "end": "1756770"
  },
  {
    "text": "function with a batch of partitions and so we're looping over the partitions that were in the batch call that was",
    "start": "1756770",
    "end": "1762290"
  },
  {
    "text": "made to us we're extracting the value of year-month-day from the apache arrow",
    "start": "1762290",
    "end": "1768110"
  },
  {
    "text": "request object and then for each partition we're generating a single split now a split is mostly",
    "start": "1768110",
    "end": "1774530"
  },
  {
    "text": "take to Athena very similar to you know what I mentioned before about table properties but there are two pieces of",
    "start": "1774530",
    "end": "1779990"
  },
  {
    "text": "information on each split that Athena does understand the first one is the spill location the spill location is",
    "start": "1779990",
    "end": "1786200"
  },
  {
    "text": "essentially the place where the lambda invocation will write data if the response that it wants to send to Athena",
    "start": "1786200",
    "end": "1792380"
  },
  {
    "text": "is larger than lambdas maximum allowed response size this spill location should",
    "start": "1792380",
    "end": "1797810"
  },
  {
    "text": "be something that should be a bucket or a three location that you own it should",
    "start": "1797810",
    "end": "1802850"
  },
  {
    "text": "also be unique person so this function makes split location is provided by the",
    "start": "1802850",
    "end": "1808040"
  },
  {
    "text": "SDK it takes in the the requests that you got and it will produce a unique spill location in that pre-configured s3",
    "start": "1808040",
    "end": "1815960"
  },
  {
    "text": "bucket that you saw a set when we deployed say like the cloud watch connector earlier the second piece of",
    "start": "1815960",
    "end": "1821450"
  },
  {
    "text": "information on a split that Athena does understand is the encryption key and so",
    "start": "1821450",
    "end": "1826460"
  },
  {
    "text": "when that data spills if you've set an encryption key for the split Athena knows and it should use that encryption",
    "start": "1826460",
    "end": "1831650"
  },
  {
    "text": "key to decrypt any data that's stored at that spill location for the split again",
    "start": "1831650",
    "end": "1837740"
  },
  {
    "text": "this function make encryption key is provided by the SDK depending on how you configure your lambda function this will",
    "start": "1837740",
    "end": "1843620"
  },
  {
    "text": "be their leverage kms to generate a one-time-use key for you or it'll use a local source of randomness to generate a",
    "start": "1843620",
    "end": "1850070"
  },
  {
    "text": "one-time-use key and then we're applying to three different properties on the",
    "start": "1850070",
    "end": "1856670"
  },
  {
    "text": "split these are not fields that Athena will understand they're there just for us so that when Athena calls us to read",
    "start": "1856670",
    "end": "1862820"
  },
  {
    "text": "this split we'll be able to access these properties and know what we're being asked to do the first property is here",
    "start": "1862820",
    "end": "1869000"
  },
  {
    "text": "and we're again pulling the year from the request that requested partition Sam",
    "start": "1869000",
    "end": "1875180"
  },
  {
    "text": "goes for a month and day and then we're returning that information and in this point we've completed our metadata",
    "start": "1875180",
    "end": "1881330"
  },
  {
    "text": "handler the next thing that we need to do is implement our record handler so I'm going to go ahead and open that and",
    "start": "1881330",
    "end": "1888580"
  },
  {
    "start": "1888000",
    "end": "2593000"
  },
  {
    "text": "very similar to the metadata handler there's already going to be a working solution here but some of the important",
    "start": "1888580",
    "end": "1894440"
  },
  {
    "text": "bits are commented out in the case of the record handler we only need to implement one function that function is",
    "start": "1894440",
    "end": "1899810"
  },
  {
    "text": "read with constraint and essentially Athena will call this function when it wants to read a split the SDK will take",
    "start": "1899810",
    "end": "1906530"
  },
  {
    "text": "care of all the serialization some of the other boilerplate for us and gives us a couple useful pieces of information the first one is a blocks",
    "start": "1906530",
    "end": "1914270"
  },
  {
    "text": "pillar this is what we're going to use to write our responses and it'll automatically spill any responses that",
    "start": "1914270",
    "end": "1919640"
  },
  {
    "text": "are too large to s3 it gives us the request so that we can get access to the split and any other useful information",
    "start": "1919640",
    "end": "1926030"
  },
  {
    "text": "and it also gives us this last this last thing here called a query status checker",
    "start": "1926030",
    "end": "1931190"
  },
  {
    "text": "this is Athena's way of pushing down limit operations into our connector we're not going to use it in this",
    "start": "1931190",
    "end": "1936740"
  },
  {
    "text": "particular example but I wanted to call it out since it is a pretty handy optimization okay the first to do here",
    "start": "1936740",
    "end": "1944240"
  },
  {
    "text": "is for us to grab the information we need off the split so I'm gonna comment that uncomment that and so for this on",
    "start": "1944240",
    "end": "1951830"
  },
  {
    "text": "the split we're asking it to give us three different integer properties year month and day next we're gonna handle",
    "start": "1951830",
    "end": "1958400"
  },
  {
    "text": "this other to do which tells our connector where where the the example data that we're going to upload later",
    "start": "1958400",
    "end": "1964580"
  },
  {
    "text": "lives in this case it's going to be getting that setting from a lambda",
    "start": "1964580",
    "end": "1969650"
  },
  {
    "text": "environment variable that we will populate when we deploy the connector using service app repo we can see that",
    "start": "1969650",
    "end": "1976010"
  },
  {
    "text": "it's using the year-month-day that we took off of the split to produce an s3 path that it will then go and grab our",
    "start": "1976010",
    "end": "1983030"
  },
  {
    "text": "sample data from the next thing that we need to do is set up the the row writer",
    "start": "1983030",
    "end": "1989780"
  },
  {
    "text": "that we will use to translate from our source in this case a CSV and s3 to Apache arrow which is the data",
    "start": "1989780",
    "end": "1997010"
  },
  {
    "text": "interchange format that Athena is expecting from the function call so I'll",
    "start": "1997010",
    "end": "2002560"
  },
  {
    "text": "uncomment and I will explain each piece of it okay so essentially the the Athena",
    "start": "2002560",
    "end": "2010900"
  },
  {
    "text": "query Federation SDK provides this generated row writer and associated",
    "start": "2010900",
    "end": "2016240"
  },
  {
    "text": "builder that allows us to create a very performant way of doing this translation",
    "start": "2016240",
    "end": "2023830"
  },
  {
    "text": "from source to Apache arrow and the way we do that is by defining an extractor",
    "start": "2023830",
    "end": "2028960"
  },
  {
    "text": "for each of the columns that we want to extract from the source and right back to Athena so the for this first example",
    "start": "2028960",
    "end": "2036310"
  },
  {
    "text": "here were using the builder and telling it you know for the Year column here's the piece of code you can use to",
    "start": "2036310",
    "end": "2041770"
  },
  {
    "text": "extract a year for each row we we use this it set value to convey whether or",
    "start": "2041770",
    "end": "2047830"
  },
  {
    "text": "not the value is null it's not particularly interesting for simple columns but it gets more interesting for",
    "start": "2047830",
    "end": "2053050"
  },
  {
    "text": "a complex column and then we do the same thing for the month the day and the",
    "start": "2053050",
    "end": "2058240"
  },
  {
    "text": "encrypted payload column then we get to an example that's a little bit more",
    "start": "2058240",
    "end": "2064000"
  },
  {
    "text": "interesting the account ID column is a little bit of a special column in this example and that it's a sensitive field",
    "start": "2064000",
    "end": "2070360"
  },
  {
    "text": "so inside this extractor we're actually only returning the last four digits of",
    "start": "2070360",
    "end": "2075398"
  },
  {
    "text": "the account ID and you can see here the code where we're doing that we're checking the length and then we're",
    "start": "2075399",
    "end": "2080500"
  },
  {
    "text": "taking a substring of it so that we're essentially masking this field and now",
    "start": "2080500",
    "end": "2086440"
  },
  {
    "text": "we're going to take care of our last field which if you recall was a struct field called transaction and this field",
    "start": "2086440",
    "end": "2092020"
  },
  {
    "text": "was comprised of two different child fields so here inside of our extractor",
    "start": "2092020",
    "end": "2098170"
  },
  {
    "text": "what we're doing is we're building a map that represents our struct we're setting a value for ID as well as for completed",
    "start": "2098170",
    "end": "2104170"
  },
  {
    "text": "and then we're using another utility from inside the sdk called block utils to set a complex value in our result we",
    "start": "2104170",
    "end": "2113260"
  },
  {
    "text": "provided that map that we just created as well as a field resolver that knows how to translate from a map into a",
    "start": "2113260",
    "end": "2119530"
  },
  {
    "text": "struct and then importantly here we return true since we don't yet support",
    "start": "2119530",
    "end": "2126010"
  },
  {
    "text": "filtering on predicate we don't yet support predicate pushdown on complex types in this example okay",
    "start": "2126010",
    "end": "2133510"
  },
  {
    "text": "so at this point we've essentially handled all the traduz but i'm gonna walk you through a little bit of the remainder of this anyway so now that",
    "start": "2133510",
    "end": "2140590"
  },
  {
    "text": "we've finished kind of building all of our extractors we actually build our generated row writer then we loop over",
    "start": "2140590",
    "end": "2147970"
  },
  {
    "text": "each line in the CSV that represents this particular split in s3 we parse the",
    "start": "2147970",
    "end": "2154120"
  },
  {
    "text": "CSV and then for each of the lines in that CSV we call our row writer and we",
    "start": "2154120",
    "end": "2160060"
  },
  {
    "text": "supply it the parsed line and that's essentially",
    "start": "2160060",
    "end": "2165100"
  },
  {
    "text": "how we end up writing our response so that takes that takes care of our connector I'm now going to transition to",
    "start": "2165100",
    "end": "2172060"
  },
  {
    "text": "the user-defined function handler that we to implement and we're gonna end up implementing to UDF's that will then use",
    "start": "2172060",
    "end": "2178759"
  },
  {
    "text": "an Athena with with this data source so again same as the other two examples for",
    "start": "2178759",
    "end": "2185690"
  },
  {
    "text": "metadata handler and record handler we're going to look for two Do's in this file and we're going to address them so",
    "start": "2185690",
    "end": "2192380"
  },
  {
    "text": "the first UDF that we're gonna implement is called extract transaction ID and essentially what this UDF does is it",
    "start": "2192380",
    "end": "2198979"
  },
  {
    "text": "will extract the transaction ID from the transaction struct so I'm going to",
    "start": "2198979",
    "end": "2204769"
  },
  {
    "text": "uncomment the solution and then we will walk through it okay so the first thing",
    "start": "2204769",
    "end": "2211279"
  },
  {
    "text": "we do is some error and educates checking if the transaction is null or if the transaction doesn't have an ID",
    "start": "2211279",
    "end": "2216769"
  },
  {
    "text": "field we're going to return negative one to say that it's an unknown ID and then",
    "start": "2216769",
    "end": "2222829"
  },
  {
    "text": "if it is present were simply going to return it otherwise if any exception",
    "start": "2222829",
    "end": "2228859"
  },
  {
    "text": "occurs we're gonna return negative 1 for invalid transaction and that's it for",
    "start": "2228859",
    "end": "2234680"
  },
  {
    "text": "our first UDF our second UDF is called decrypt and we're gonna actually use this to decrypt that encrypted payload",
    "start": "2234680",
    "end": "2241670"
  },
  {
    "text": "column that you saw in the other parts of the example so in this particular",
    "start": "2241670",
    "end": "2246829"
  },
  {
    "text": "case what we're gonna do is we're gonna call get encryption key which in this",
    "start": "2246829",
    "end": "2253549"
  },
  {
    "text": "particular example is kind of mocked out to return a static key but normally would probably call something like KMS",
    "start": "2253549",
    "end": "2259489"
  },
  {
    "text": "or whatever else you're using for key management then it calls this symmetric decrypt function which in this",
    "start": "2259489",
    "end": "2266509"
  },
  {
    "text": "particular case just applies a very basic AES decrypt using the payload which is the the column that was",
    "start": "2266509",
    "end": "2272569"
  },
  {
    "text": "inputted to the UDF and the key that we just retrieved them the prior line if there's any errors we say the result is",
    "start": "2272569",
    "end": "2279559"
  },
  {
    "text": "no just to avoid failing the query otherwise we return the decrypted string",
    "start": "2279559",
    "end": "2284799"
  },
  {
    "text": "and that's it we just wrote two UDF's okay so at this point we've done I think",
    "start": "2284799",
    "end": "2290059"
  },
  {
    "text": "everything that the tutorial asked us to do for that step so I'm going to switch back to the tutorial and see what's next",
    "start": "2290059",
    "end": "2295329"
  },
  {
    "text": "okay so the last thing we need to do is part of preparing our connectors code is upload the sample data to s3 so the",
    "start": "2295329",
    "end": "2303229"
  },
  {
    "text": "sample data again comes with the the copy of the tutorial that I checked out earlier I'm just going to grab this",
    "start": "2303229",
    "end": "2308329"
  },
  {
    "text": "command and upload our data remember to replace my s3 bucket in the command",
    "start": "2308329",
    "end": "2316210"
  },
  {
    "text": "federacin demo 1000 ok our data is",
    "start": "2316540",
    "end": "2322220"
  },
  {
    "text": "uploaded now let's switch to the tutorial and see what's next",
    "start": "2322220",
    "end": "2327849"
  },
  {
    "text": "alright looks like we're ready to package and deploy our connector and the SDK actually comes with a pretty handy",
    "start": "2327849",
    "end": "2334280"
  },
  {
    "text": "publish tool which will take care of packaging everything up running some tests and then installing it in",
    "start": "2334280",
    "end": "2340520"
  },
  {
    "text": "serverless app repo for us I'm going to go ahead and copy this command it looks like we need to substitute in two pieces",
    "start": "2340520",
    "end": "2346250"
  },
  {
    "text": "of information the first being the s3 bucket that serverless app repo can go get a copy of the code from and the",
    "start": "2346250",
    "end": "2352609"
  },
  {
    "text": "publish tool will handle putting a copy there and the second thing being the AWS region that we want to publish into when",
    "start": "2352609",
    "end": "2359240"
  },
  {
    "text": "we publish this it's going to be a private application so nobody else will be able to see it unless we choose to make it public but the publish tool",
    "start": "2359240",
    "end": "2366140"
  },
  {
    "text": "should prompt you for that okay I'm going to make sure to put in my ABS",
    "start": "2366140",
    "end": "2373339"
  },
  {
    "text": "region in my case I'm working in US East to I'm also going to substitute it in my bucket name that I created earlier which",
    "start": "2373339",
    "end": "2381319"
  },
  {
    "text": "was Federation demo 1000 ok the tool",
    "start": "2381319",
    "end": "2388849"
  },
  {
    "text": "tells me what it's going to do it says it's going to build the maven project it's going to create a surrealist application package using the yamo file",
    "start": "2388849",
    "end": "2394790"
  },
  {
    "text": "provided in the sdk it's going to upload the connector to the s3 bucket we",
    "start": "2394790",
    "end": "2399799"
  },
  {
    "text": "provided it's going to check if service app repo can access that bucket if it can't it'll prompt us to add a bucket",
    "start": "2399799",
    "end": "2405859"
  },
  {
    "text": "policy to enable service apropos to read from it and then lastly it will publish",
    "start": "2405859",
    "end": "2411290"
  },
  {
    "text": "the connector to service app repo let me go ahead and hit yes it checked my bucket and my bucket does it does indeed",
    "start": "2411290",
    "end": "2417799"
  },
  {
    "text": "there is indeed lacking the bucket policy so I hit yes it shows me the policy and it applies it and then it",
    "start": "2417799",
    "end": "2424099"
  },
  {
    "text": "starts building our connector this takes a little while the first time while it",
    "start": "2424099",
    "end": "2429950"
  },
  {
    "text": "downloads dependencies ok pass testing and was just uploaded to",
    "start": "2429950",
    "end": "2437700"
  },
  {
    "text": "service a pre-bout so next thing we're gonna do is we're gonna switch over to server let's app repo and we're gonna deploy that connector and then we're",
    "start": "2437700",
    "end": "2445680"
  },
  {
    "text": "gonna see what what else the tutorial tells us to do I'm on Cerberus app repo and I'm viewing",
    "start": "2445680",
    "end": "2451800"
  },
  {
    "text": "my private applications and I can see the one I just built and deployed gonna go ahead and click on it and similar to",
    "start": "2451800",
    "end": "2459270"
  },
  {
    "text": "the other example we did earlier with cloud watch logs it's gonna ask me a few questions the first one is you know what",
    "start": "2459270",
    "end": "2465000"
  },
  {
    "text": "do I want to call this catalog in Athena and what do I want to name that lambda function so I'm just gonna call it example second question is what's the",
    "start": "2465000",
    "end": "2472800"
  },
  {
    "text": "data bucket meaning where can I go get the sample data so my bucket was called Federation demo 1000 and I'm gonna use",
    "start": "2472800",
    "end": "2482190"
  },
  {
    "text": "the same thing from my spill bucket and everything else I think I'm gonna leave",
    "start": "2482190",
    "end": "2487290"
  },
  {
    "text": "at default I'm gonna leave encryption enabled and I'm gonna leave the lambda settings the same I mean to acknowledge",
    "start": "2487290",
    "end": "2492570"
  },
  {
    "text": "that this is going to create an IM role to read s3 and if I scroll to the bottom",
    "start": "2492570",
    "end": "2499619"
  },
  {
    "text": "I should be able to deploy okay and now",
    "start": "2499619",
    "end": "2504750"
  },
  {
    "text": "we can see our connectors deployed let's go check the tutorial and see what we should do next okay so the next thing",
    "start": "2504750",
    "end": "2513869"
  },
  {
    "text": "that you toriel would like us to do is to validate our connector and the reason for this is is because one of the most challenging parts of integrating systems",
    "start": "2513869",
    "end": "2520200"
  },
  {
    "text": "is integration testing so the SDK comes with a validate connector tool that will",
    "start": "2520200",
    "end": "2526200"
  },
  {
    "text": "call your lambda function and simulate the traffic patterns and calls that Athena would make as part of a query so",
    "start": "2526200",
    "end": "2532800"
  },
  {
    "text": "I'm going to go ahead and copy this command and then we're going to have to substitute in the name of our lambda function so in my case I called the",
    "start": "2532800",
    "end": "2546180"
  },
  {
    "text": "lambda function example",
    "start": "2546180",
    "end": "2549920"
  },
  {
    "text": "okay and it's telling me what it's going to do it's going to build the maven project for the valid the validate",
    "start": "2555490",
    "end": "2562460"
  },
  {
    "text": "connector tool if it's not already built and then it's gonna call our lambda function I mean here yes okay and our",
    "start": "2562460",
    "end": "2570620"
  },
  {
    "text": "connector passed validation we can see at the bottom here are a little successfully passed validation message I think now we're ready to run a query but",
    "start": "2570620",
    "end": "2578090"
  },
  {
    "text": "let's check what the tutorial would like us to do okay it looks like we are indeed ready to run a query so I'm gonna",
    "start": "2578090",
    "end": "2584630"
  },
  {
    "text": "copy this first example query which looks like it uses both our UDF's as well and I'm going to switch to the my",
    "start": "2584630",
    "end": "2591530"
  },
  {
    "text": "Athena console okay so I've pasted the the sample query in and now we need to",
    "start": "2591530",
    "end": "2597140"
  },
  {
    "text": "substitute in the name of our lambda function and then I'll walk you through what this queries going to do so my",
    "start": "2597140",
    "end": "2602480"
  },
  {
    "text": "lambda function was called example so I'm gonna just replace it in the two user-defined function definitions and",
    "start": "2602480",
    "end": "2608300"
  },
  {
    "text": "then also in my datasource definition there okay so what this is doing is on",
    "start": "2608300",
    "end": "2615200"
  },
  {
    "text": "lines one and two its defining our extract transaction ID user defined function and it's saying that it takes a",
    "start": "2615200",
    "end": "2622310"
  },
  {
    "text": "row or struct as input and it expects that row or struct to have two fields ID and completed and it returns an integer",
    "start": "2622310",
    "end": "2629900"
  },
  {
    "text": "it's also saying that this particular user-defined function is a lambda invocation style user-defined function",
    "start": "2629900",
    "end": "2637160"
  },
  {
    "text": "and the name of the lambda to use for that invocation is example it then",
    "start": "2637160",
    "end": "2642380"
  },
  {
    "text": "defines a second function called decrypt which takes as input of varchar' or string and returns of our chart and",
    "start": "2642380",
    "end": "2649550"
  },
  {
    "text": "again is a lambda invoke type UDF that's hosted on a lambda called example and",
    "start": "2649550",
    "end": "2656230"
  },
  {
    "text": "then we have a select query in this case we're selecting the year-month-day account ID and then we're selecting the",
    "start": "2656230",
    "end": "2664730"
  },
  {
    "text": "encrypted payload column and passing it to our decrypt UDF before taking the",
    "start": "2664730",
    "end": "2670070"
  },
  {
    "text": "transaction column and passing it to our extract transaction ID UDF and we're saying that this is coming from the",
    "start": "2670070",
    "end": "2676310"
  },
  {
    "text": "lambda colon example data source which",
    "start": "2676310",
    "end": "2681530"
  },
  {
    "text": "maps to our example connector that we just wrote and then we're filtering on",
    "start": "2681530",
    "end": "2687020"
  },
  {
    "text": "the Year month and day so we're essentially creating just a single partition of this custom table so",
    "start": "2687020",
    "end": "2693650"
  },
  {
    "text": "I'm going to go ahead and run our query",
    "start": "2693650",
    "end": "2696818"
  },
  {
    "text": "and then you can see that we got some data back probably the most interesting parts are that account ID column right",
    "start": "2703450",
    "end": "2709819"
  },
  {
    "text": "which we can see is now masked down to just the last four digits our decrypted payload column which we can actually",
    "start": "2709819",
    "end": "2715880"
  },
  {
    "text": "read now because we've decrypted it and then lastly we've successfully extracted the transaction ID from the struct okay",
    "start": "2715880",
    "end": "2724250"
  },
  {
    "text": "let's go back to the tutorial and I think there's one more interesting query we can try running so the tutorial also",
    "start": "2724250",
    "end": "2730700"
  },
  {
    "text": "includes a couple example didi else we can try so let's get the query that'll let us describe the schema of our",
    "start": "2730700",
    "end": "2737089"
  },
  {
    "text": "example table go ahead and paste that in here and run it well actually first I'm",
    "start": "2737089",
    "end": "2742220"
  },
  {
    "text": "going to insert the name of my source which was example and then I'll run it",
    "start": "2742220",
    "end": "2747380"
  },
  {
    "text": "and you can see that we get back the schema information I can see that it's got so an account ID and encrypted",
    "start": "2747380",
    "end": "2753799"
  },
  {
    "text": "payload transaction which is a struct column and then three partition tables",
    "start": "2753799",
    "end": "2759279"
  },
  {
    "text": "and we're done so we've we've written a custom connector we've written some custom UDF's",
    "start": "2759279",
    "end": "2764569"
  },
  {
    "text": "we've used the connector in UDF and a query i've shown you how to deploy one of our ready-made connectors from",
    "start": "2764569",
    "end": "2770299"
  },
  {
    "text": "serverless app repo and we've talked about how Athena federated queries can help you get at your data even when it",
    "start": "2770299",
    "end": "2776809"
  },
  {
    "text": "might have been siloed in various application stacks so thanks for spending the time with me to go through these new features",
    "start": "2776809",
    "end": "2784359"
  }
]