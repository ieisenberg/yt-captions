[
  {
    "start": "0",
    "end": "98000"
  },
  {
    "text": "might as well go ahead and get started and make good use of the time we have here before I begin I hope those of you",
    "start": "0",
    "end": "6779"
  },
  {
    "text": "here in the room are able to attend the previous talk I actually thought it was an excellent talk about a journey of one",
    "start": "6779",
    "end": "12120"
  },
  {
    "text": "company going from running their own infrastructure their own software moving over to managed services to moving over",
    "start": "12120",
    "end": "18600"
  },
  {
    "text": "to server list computing and it's also a wonderful product that was actually elated we have Comcast x1 throughout our",
    "start": "18600",
    "end": "24869"
  },
  {
    "text": "house I was actually elated to find out that that was actually using Kinesis technology cuz it's a great experience I",
    "start": "24869",
    "end": "30269"
  },
  {
    "text": "live in a house with two daughters and a wife now go home and even there's no chance I'd ever get to watch the TV it",
    "start": "30269",
    "end": "35340"
  },
  {
    "text": "would all be blocked up and that that device has changed my life and it's actually great to see how it's taking advantage of cloud services so good",
    "start": "35340",
    "end": "42300"
  },
  {
    "text": "afternoon everyone I'm Roger barge a I'm general manager of AWS Kinesis I'm here with my colleague today rainy nights",
    "start": "42300",
    "end": "48690"
  },
  {
    "text": "product manager for Kinesis analytics on behalf of the entire Kinesis analytics team it's our pleasure to introduce you",
    "start": "48690",
    "end": "55140"
  },
  {
    "text": "to Kinesis analytics today I'm gonna be saying a few opening remarks just trying to frame why we've built the service how",
    "start": "55140",
    "end": "62219"
  },
  {
    "text": "it fits in with the services we have today and directionally where we're headed with this one observation that",
    "start": "62219",
    "end": "69390"
  },
  {
    "text": "we've made is that most data today is actually generated continuously and with",
    "start": "69390",
    "end": "75540"
  },
  {
    "text": "the the rise of devices this data is now coming at us at lightning speeds and",
    "start": "75540",
    "end": "81840"
  },
  {
    "text": "massive quantities and in fact we believe that this is just the tipping point this is just the beginning again",
    "start": "81840",
    "end": "87840"
  },
  {
    "text": "most data is generated continuously and this data actually has valuable",
    "start": "87840",
    "end": "92939"
  },
  {
    "text": "information in it if you can convert it from raw data to information in a timely",
    "start": "92939",
    "end": "98100"
  },
  {
    "start": "98000",
    "end": "98000"
  },
  {
    "text": "manner and regardless of whether it's something from a mobile app web clickstream smart building IOT sensors",
    "start": "98100",
    "end": "104490"
  },
  {
    "text": "media records metering records consumer interactions with your services monitoring your devices there is",
    "start": "104490",
    "end": "111540"
  },
  {
    "text": "valuable information and if for those of you who have been watching talks about Kinesis and real-time analytics and if",
    "start": "111540",
    "end": "117990"
  },
  {
    "text": "you look at the companies that are actually using it today these companies lean in very far forward and bracing new",
    "start": "117990",
    "end": "124740"
  },
  {
    "text": "technologies they roll up their sleeves and they build very complex applications that's the narrow few what happens when",
    "start": "124740",
    "end": "131610"
  },
  {
    "text": "this technology becomes democra what happens when it becomes easier to use what's gonna happen in the world we",
    "start": "131610",
    "end": "137170"
  },
  {
    "text": "live in and how fast were able to get insights out of our data another observation is what is your data worth",
    "start": "137170",
    "end": "144670"
  },
  {
    "text": "well there's really a diminishing value of data if you look at the value we extract out of data today that data is",
    "start": "144670",
    "end": "152080"
  },
  {
    "text": "days weeks old and we still get incredible value at it but as you move",
    "start": "152080",
    "end": "157360"
  },
  {
    "text": "closer to the real time you can get these valuable insights out of data Mike",
    "start": "157360",
    "end": "162790"
  },
  {
    "text": "Gillett area Forrester has a beautiful phrase for this and a wonderful series of talks I'd encourage you to look at he",
    "start": "162790",
    "end": "168069"
  },
  {
    "text": "calls these perishable insights because that value diminishes rapidly if you're",
    "start": "168069",
    "end": "173200"
  },
  {
    "start": "172000",
    "end": "172000"
  },
  {
    "text": "looking at a transaction is it fraudulent yes or no do we have systems to do that today absolutely because the",
    "start": "173200",
    "end": "178959"
  },
  {
    "text": "money that's on the line but again it's the few that can actually implement these think of all the other cases when you're looking at a customer should I",
    "start": "178959",
    "end": "185470"
  },
  {
    "text": "sell them an ad should I give them this offer are they are they near my store if you begin to think about the myriad of",
    "start": "185470",
    "end": "191950"
  },
  {
    "text": "applications that are untapped today where there's perishable insights on the line another observation is that data",
    "start": "191950",
    "end": "199989"
  },
  {
    "text": "becomes even more valuable when you can combine the recent data with the old data how are my sales trending relative",
    "start": "199989",
    "end": "207340"
  },
  {
    "text": "to last week is my are my customers or and the cohorts so that are my customers are they changing relative to last week",
    "start": "207340",
    "end": "213819"
  },
  {
    "text": "and if so is this in response to basically the ad that I just ran once again an example of not only as fresh",
    "start": "213819",
    "end": "220180"
  },
  {
    "text": "data highly valuable but if I have the means to combine it with my old data and near real-time and make decisions in",
    "start": "220180",
    "end": "225940"
  },
  {
    "text": "real-time and do it in a way that anybody can implement the application there's incredible value B to deliver to",
    "start": "225940",
    "end": "231459"
  },
  {
    "text": "customers and this has been the the journey that we have been on and again",
    "start": "231459",
    "end": "237790"
  },
  {
    "text": "an observation that you can make very few companies have been able to take advantage of this why it requires",
    "start": "237790",
    "end": "242889"
  },
  {
    "text": "programmers and developers with very special skills the ability to write programs that can process this data",
    "start": "242889",
    "end": "248970"
  },
  {
    "text": "extract information out of it and near-real-time pump it to a system where a decision maker can then make a decision on this",
    "start": "248970",
    "end": "255160"
  },
  {
    "text": "and oh by the way you don't have to actually manage a fleet of machines to run these applications you have to scale",
    "start": "255160",
    "end": "260440"
  },
  {
    "text": "it as your business runs this has been the problem that we have been looking at as a team now for several years and trying to increment",
    "start": "260440",
    "end": "266060"
  },
  {
    "text": "to leech ink away at it and again I want to make sure you leave today understanding how this new service fits into this repertoire of services that we",
    "start": "266060",
    "end": "273110"
  },
  {
    "text": "have been building so what are some of the key requirements and this is a",
    "start": "273110",
    "end": "278300"
  },
  {
    "text": "deliberately crufty slide we didn't want to go too much into it we'll talk more about it and later but it really is a",
    "start": "278300",
    "end": "283790"
  },
  {
    "text": "pipeline of steps that you have to be able to support all the way from data ingest and that data ingest has to be",
    "start": "283790",
    "end": "290900"
  },
  {
    "text": "durable you may have applications that start up a few minutes after others you may have multiple applications analyzing",
    "start": "290900",
    "end": "296840"
  },
  {
    "text": "all the streaming data coming from these myriad of devices from mobile consumer online cash registers beacons and more",
    "start": "296840",
    "end": "304250"
  },
  {
    "text": "in the future you need to be able to ingest that data and capture it in a durable and scalable manner scaleable is",
    "start": "304250",
    "end": "310040"
  },
  {
    "start": "305000",
    "end": "305000"
  },
  {
    "text": "key here because the workloads will vary throughout the day you have to have the means to transform it and not just a",
    "start": "310040",
    "end": "315680"
  },
  {
    "text": "single transform it could be multiple transforms depend on who's actually going to be using that data so that's a second important step that you have to",
    "start": "315680",
    "end": "322639"
  },
  {
    "text": "support in the pipeline you have to analyze it and again if that analysis means writing some very crufty program a",
    "start": "322639",
    "end": "329330"
  },
  {
    "text": "very special program that's again gonna actually winnow down the people that can actually use it you'd like to expose",
    "start": "329330",
    "end": "334580"
  },
  {
    "text": "something like sequel that most people understand most developers can get their head around and immediately write code",
    "start": "334580",
    "end": "340610"
  },
  {
    "text": "it's actually then to analyze it and then once you've analyzed it you need to react send it to some system that can",
    "start": "340610",
    "end": "346580"
  },
  {
    "text": "tell somebody here's a decision that has to be made here's an opportunity that you can actually capitalize on and then",
    "start": "346580",
    "end": "352100"
  },
  {
    "text": "persist it again maybe for historical analysis and that in to end pipeline has to run in less than a second or two",
    "start": "352100",
    "end": "358729"
  },
  {
    "text": "that's what we're trying to go after that's how you move companies into the real time and Ally to really get that",
    "start": "358729",
    "end": "364280"
  },
  {
    "text": "perishable insight out of the data only other thing I'd say about that hoops",
    "start": "364280",
    "end": "369590"
  },
  {
    "text": "switching on that is that it also has to basically be correct this pipeline has to ensure that each and every problem",
    "start": "369590",
    "end": "375590"
  },
  {
    "text": "record gets processed nothing gets skipped reactive reliable there's a number of Eddie's that you have to tack",
    "start": "375590",
    "end": "381320"
  },
  {
    "text": "on to this to make this whole thing work end to end but this pipeline in this transition a little bit into basically",
    "start": "381320",
    "end": "386930"
  },
  {
    "text": "looking at what we've been looking at in terms of Kinesis and you if you're in the previous session you saw",
    "start": "386930",
    "end": "392590"
  },
  {
    "text": "descriptions these I'll talk about it briefly but I'd like for you to be thinking about that pipeline that I showed you and how each one of our",
    "start": "392590",
    "end": "398900"
  },
  {
    "text": "services compose and again I'll make a statement here we've been building the Kinesis services which today consists of Kinesis",
    "start": "398900",
    "end": "406580"
  },
  {
    "text": "dreams fire hose and today Kinesis analytics each one is independent provides independent valuable but they",
    "start": "406580",
    "end": "412970"
  },
  {
    "text": "are designed and intended to be composed to actually build these end-to-end pipelines and should you want to swap",
    "start": "412970",
    "end": "418069"
  },
  {
    "start": "415000",
    "end": "415000"
  },
  {
    "text": "out another service such as Kafka for the ingest fire hose and Kinesis analytics will allow you to do that over",
    "start": "418069",
    "end": "424370"
  },
  {
    "text": "time these are meant to be composable but streams it is our durable data and",
    "start": "424370",
    "end": "430520"
  },
  {
    "text": "scalable data ingest you think about what your capacity is what your in your data rates are gonna be coming in you",
    "start": "430520",
    "end": "436789"
  },
  {
    "text": "create the stream with the necessary number of shards if you get throttled you'll be able to scale up scale down",
    "start": "436789",
    "end": "442880"
  },
  {
    "text": "your screen dynamically that is your durable record of your of your streaming data coming in for 24 hours it can be",
    "start": "442880",
    "end": "449419"
  },
  {
    "text": "extended out to seven days and again a lot of our customers highly technical customers that lean in we're using the",
    "start": "449419",
    "end": "456020"
  },
  {
    "text": "KCl to write real time processing applications with 20 millisecond response time for real-time bidding",
    "start": "456020",
    "end": "461289"
  },
  {
    "text": "we're trying to actually say how do we bring that kind of capability to a broader audience in the second one we",
    "start": "461289",
    "end": "467870"
  },
  {
    "text": "talked about in the pipeline you have to be able to deliver this data to make a decision to load a dashboard think about",
    "start": "467870",
    "end": "475940"
  },
  {
    "text": "now the role firehose plays Kinesis firehose does delivery of streaming data from sources we have the Kinesis agent",
    "start": "475940",
    "end": "483469"
  },
  {
    "text": "has an API you put your data into a Kinesis pyros you don't worry about the scale we scale it for you you can tell",
    "start": "483469",
    "end": "489770"
  },
  {
    "text": "us to encrypt it to batch it and what period in which to to load it and you choose the destinations which today is",
    "start": "489770",
    "end": "495620"
  },
  {
    "text": "s3 redshift elasticsearch and yes you can imagine that list is going to be growing every time we meet we'll be",
    "start": "495620",
    "end": "501409"
  },
  {
    "start": "497000",
    "end": "497000"
  },
  {
    "text": "talking about a new destination we can share with you but not only does this give us an easy way of delivering",
    "start": "501409",
    "end": "507949"
  },
  {
    "text": "streaming data into AWS services where a lot of our customers were already running their business have their",
    "start": "507949",
    "end": "513349"
  },
  {
    "text": "tableau dashboard hooked up to redshift but think about what in the context of you are doing extreme processing and",
    "start": "513349",
    "end": "518839"
  },
  {
    "text": "cooking data looking at outliers doing aggregates and doing it looking for insights feed it to fire us BAM",
    "start": "518839",
    "end": "525649"
  },
  {
    "text": "immediately your tableau dashboard lights up with the insight and here's our audition today Kinesis analytics",
    "start": "525649",
    "end": "533570"
  },
  {
    "text": "Ryan will be taking you on a tour of Kinesis analytics as you heard before under the covers it is a real-time",
    "start": "533570",
    "end": "539509"
  },
  {
    "text": "streaming sequel engine that runs basically in sub-second processing time",
    "start": "539509",
    "end": "544730"
  },
  {
    "text": "you have full ANSI standard sequel which has been extended with this notion of",
    "start": "544730",
    "end": "550130"
  },
  {
    "text": "windowing why because you don't have tables you have streams which are infinitely unbounded we and some of the",
    "start": "550130",
    "end": "557389"
  },
  {
    "text": "nice things about the service and you'll see it in the demonstration or later today Kinesis analytics will look at",
    "start": "557389",
    "end": "563420"
  },
  {
    "start": "560000",
    "end": "560000"
  },
  {
    "text": "your stream it will actually propose a schema for you allow you to drop columns allow you to name columns change the",
    "start": "563420",
    "end": "570350"
  },
  {
    "text": "type if we got the type inference wrong but we've schematize your streams your records in your stream and we have",
    "start": "570350",
    "end": "575810"
  },
  {
    "text": "effectively then turned it into a table which is unbounded because data can continue to come in once we have done",
    "start": "575810",
    "end": "581930"
  },
  {
    "text": "that we allow you to write sequel and change your sequel I have seen very complex analytics applications written",
    "start": "581930",
    "end": "589160"
  },
  {
    "text": "and standard sequel but we want to go beyond that when you look at the service you'll see that we have been putting in",
    "start": "589160",
    "end": "595220"
  },
  {
    "text": "some streaming analytics algorithms including anomaly detection which which has a novel and published machine",
    "start": "595220",
    "end": "601490"
  },
  {
    "text": "learning algorithm and you'll see more appearing as well because not everything fits in sequel would be kind of nice to",
    "start": "601490",
    "end": "606560"
  },
  {
    "text": "have some very advanced analytics specific to streaming inside the service and so we'll be doing more there and it",
    "start": "606560",
    "end": "612800"
  },
  {
    "text": "scales it will look at the incoming rate of your stream it will look at the complexities of the query the amount of",
    "start": "612800",
    "end": "618740"
  },
  {
    "text": "CPU you need to keep up the amount of memory you need for your windowed queries and it will scale the amount of",
    "start": "618740",
    "end": "623990"
  },
  {
    "text": "resources dedicated your service and as your stream dies down as the queries shut down it will collapse and your cost",
    "start": "623990",
    "end": "630500"
  },
  {
    "text": "will reduce so it dynamically scales with exactly what you use now if you just pause for a second and think about",
    "start": "630500",
    "end": "637010"
  },
  {
    "text": "that pipeline that I actually started with and think about how our service is now composed I can take a Kinesis stream",
    "start": "637010",
    "end": "642920"
  },
  {
    "text": "if I actually want to use that as my ingest firehose is also an ingest Kinesis analytics can connect to both of",
    "start": "642920",
    "end": "650000"
  },
  {
    "text": "these services as its input source the analytics can do the necessary processing to extract the insights from",
    "start": "650000",
    "end": "656569"
  },
  {
    "text": "the data create an alert pull out some some computed aggregate maybe compare",
    "start": "656569",
    "end": "661939"
  },
  {
    "text": "with historical analysis like our sales today are 20% lower than they were for the lot month you might want to let something",
    "start": "661939",
    "end": "667580"
  },
  {
    "start": "666000",
    "end": "666000"
  },
  {
    "text": "know we monitor that at eight and Amazon if the market basket or the amount of saw goods sold drop by some percentage",
    "start": "667580",
    "end": "673700"
  },
  {
    "text": "we think there might be something of interest here now remember you've got a Kinesis stream which has 24 hours of",
    "start": "673700",
    "end": "678800"
  },
  {
    "text": "data how cool would that be to pose a query over what what happened in the last 24 hours again that's the value of having that",
    "start": "678800",
    "end": "685340"
  },
  {
    "text": "durable scalable ingest stream coming in to the analytics and then once the analytics has been computed you can",
    "start": "685340",
    "end": "692210"
  },
  {
    "text": "deliver it straight to firehose which would then to load redshift right tableau or some other expert user",
    "start": "692210",
    "end": "697700"
  },
  {
    "text": "experience it can load s3 it can load elastic search or you can put your your",
    "start": "697700",
    "end": "703580"
  },
  {
    "text": "resulting data in a Kinesis stream and have a lambda function and yes will we be looking for more integration points",
    "start": "703580",
    "end": "709250"
  },
  {
    "text": "for Kinesis analytics to send for notifications for other dashboards so this is how we've been on thinking about",
    "start": "709250",
    "end": "715910"
  },
  {
    "text": "this broader problem the challenges what we're trying to enable to allow you to get perishable insights in real time and",
    "start": "715910",
    "end": "722570"
  },
  {
    "text": "again I'll argue a lot of batch processing jobs today if you look and say where did this data come from it was continuously generated data the users",
    "start": "722570",
    "end": "730130"
  },
  {
    "text": "would have loved to have had instant insights out of that data but the only means they had was actually MapReduce",
    "start": "730130",
    "end": "735170"
  },
  {
    "text": "and Hadoop jobs we're trying to bring a set of services together which compose and actually allow you our customers to",
    "start": "735170",
    "end": "741440"
  },
  {
    "text": "get real-time insights and then compose the services the way you want to actually build the actual application",
    "start": "741440",
    "end": "747470"
  },
  {
    "text": "that you want over your real time data so with that I'm going to turn it over to Ryan Dean hice it's going to take you",
    "start": "747470",
    "end": "753800"
  },
  {
    "text": "on a deeper dive through Kinesis analytics",
    "start": "753800",
    "end": "758230"
  },
  {
    "text": "hi everyone my name is Ryan Dean hi I'm the product manager for Amazon case analytics I'm very excited to talk to",
    "start": "763060",
    "end": "769880"
  },
  {
    "text": "you today about the service that we just launched and how it fits into you building your real-time streaming",
    "start": "769880",
    "end": "775370"
  },
  {
    "text": "processing applications so what are the",
    "start": "775370",
    "end": "781370"
  },
  {
    "text": "most important aspects of what we've built and offer to you today so one of the biggest things you'll notice",
    "start": "781370",
    "end": "786980"
  },
  {
    "text": "especially is I go through a sort of a video demo at the end of the presentation is that the service is very",
    "start": "786980",
    "end": "792350"
  },
  {
    "text": "very easy to use it allows you to connect to your streaming data sources schema ties and structure that data",
    "start": "792350",
    "end": "799160"
  },
  {
    "text": "expose it to your sequel code and allow you to run that sequel code in minutes",
    "start": "799160",
    "end": "804459"
  },
  {
    "text": "it gives you instant visibility into what's going on in the stream with real-time data processing it's very",
    "start": "804459",
    "end": "811730"
  },
  {
    "text": "elastic it will scale to handle both your query complexity and your streaming data throughput we offer sub-second",
    "start": "811730",
    "end": "819829"
  },
  {
    "text": "processing latencies for many app for many queries and its standard sequel so",
    "start": "819829",
    "end": "826310"
  },
  {
    "text": "as Roger mentioned we've implemented an ansi sequel so we're presenting you with a very familiar data manipulation",
    "start": "826310",
    "end": "832760"
  },
  {
    "text": "language in order to process your data so what do you do with Kinesis analytics",
    "start": "832760",
    "end": "839240"
  },
  {
    "text": "with casus analytics you build streaming applications that will continuously read",
    "start": "839240",
    "end": "844250"
  },
  {
    "text": "from a streaming data source process that data you're using your configured application code and emit that data to a",
    "start": "844250",
    "end": "851839"
  },
  {
    "text": "destined figured destination of your choice so we support for each one of",
    "start": "851839",
    "end": "860240"
  },
  {
    "text": "your applications we support connecting to a single streaming data source either an Amazon Kinesis firehose delivery",
    "start": "860240",
    "end": "866209"
  },
  {
    "start": "866000",
    "end": "866000"
  },
  {
    "text": "stream or an Amazon Kinesis stream and then we'll make a decision about which ingestion stream to use is if you want",
    "start": "866209",
    "end": "874370"
  },
  {
    "text": "the raw data sent to s3 or redshift or elasticsearch you'll use firehose if you",
    "start": "874370",
    "end": "880220"
  },
  {
    "text": "want to build custom applications and want some more flexibility with what you're doing with your stream you'll use Kinesis streams we support JSON CSV",
    "start": "880220",
    "end": "889699"
  },
  {
    "start": "887000",
    "end": "887000"
  },
  {
    "text": "variable column and unstructured text so you can even bring in things like Apache logs into your Canisius analytics",
    "start": "889699",
    "end": "897069"
  },
  {
    "text": "application structured them and run standard sequel each input has a schema and as I'll show",
    "start": "897069",
    "end": "905199"
  },
  {
    "text": "this in the video but basically we'll sample records off your stream and we go through a process called discovery in",
    "start": "905199",
    "end": "910929"
  },
  {
    "text": "which we do schema inference there's a it's backed by an API and we present that recommended schema back to you now",
    "start": "910929",
    "end": "917709"
  },
  {
    "text": "we don't think we're going to get this perfectly right for everyone so we view it sort of as like we'll get it if you get there 90% of the way or 95% of the",
    "start": "917709",
    "end": "924160"
  },
  {
    "text": "way and then we give you full flexibility to edit that schema so if you may want to loosen the restrictions",
    "start": "924160",
    "end": "930069"
  },
  {
    "text": "on some columns maybe you have a upcoming schema changing you need to add a couple more columns that are not there",
    "start": "930069",
    "end": "936040"
  },
  {
    "text": "yet that type of thing we give you that flexibility to do so and you'll notice in the experience that theme is present",
    "start": "936040",
    "end": "944139"
  },
  {
    "text": "throughout where we're going to help you get started very quickly and then provide you the flexibility to run with",
    "start": "944139",
    "end": "950649"
  },
  {
    "text": "it to build the application to meet your use case we also support another type of",
    "start": "950649",
    "end": "956259"
  },
  {
    "text": "data source called the reference data source reference data source is different than a streaming data source and the fact that we all continuously",
    "start": "956259",
    "end": "963189"
  },
  {
    "text": "reading data from that streaming data source so we're always reading from the",
    "start": "963189",
    "end": "968290"
  },
  {
    "text": "stream to grab new data to feed to your sequel queries a reference data source we are going to read at periodic",
    "start": "968290",
    "end": "974829"
  },
  {
    "text": "intervals and pull it entirely into memory and represent it as a sequel table the primary use case for this is",
    "start": "974829",
    "end": "980949"
  },
  {
    "text": "to do data enrichment so if you have a unique identifier coming in on your stream you've got some metadata that you",
    "start": "980949",
    "end": "986649"
  },
  {
    "text": "want to attach that stream say before you remit the data to redshift reference data sources allow you to do that so",
    "start": "986649",
    "end": "995819"
  },
  {
    "text": "what does your application code look like so we present your streaming data source",
    "start": "995819",
    "end": "1001199"
  },
  {
    "text": "after the schema is applied and what's called an in application stream this in application stream looks very similar to",
    "start": "1001199",
    "end": "1007499"
  },
  {
    "text": "a table and what you select from in your from clause of your sequel statements from there we have very robust sequel",
    "start": "1007499",
    "end": "1014910"
  },
  {
    "text": "support everything from simple mathematical operators to powerful string manipulation functions like regex",
    "start": "1014910",
    "end": "1021569"
  },
  {
    "text": "matching log parsing that type of thing and also advanced analytics so Roger",
    "start": "1021569",
    "end": "1028650"
  },
  {
    "start": "1024000",
    "end": "1024000"
  },
  {
    "text": "mentioned we have an anomaly detection algorithm we shipped with that's exposed to you in your sequel code we also have",
    "start": "1028650",
    "end": "1034550"
  },
  {
    "text": "approximate distinct items which we've implemented hyper log-log and several other algorithms that are available to",
    "start": "1034550",
    "end": "1040050"
  },
  {
    "text": "you and I think we've just gotten started there and we're continue to add them over time what we hope to do is",
    "start": "1040050",
    "end": "1045240"
  },
  {
    "text": "that for you all to try the service and give us feedback about these are the functions that we want even further we",
    "start": "1045240",
    "end": "1050700"
  },
  {
    "text": "looked at extend the functionality beyond that over time so there's an extensions that I'll cover in a couple",
    "start": "1050700",
    "end": "1057000"
  },
  {
    "text": "slides later that we've made it easier to write standard sequel over streaming data windowing is one of those concepts",
    "start": "1057000",
    "end": "1062700"
  },
  {
    "text": "that Roger mentioned in application streams is another and we have support",
    "start": "1062700",
    "end": "1069150"
  },
  {
    "text": "for at least once processing so this means that we guarantee that we will always read and process each record and",
    "start": "1069150",
    "end": "1076650"
  },
  {
    "text": "row in your stream at least once and we will deliver that process data to your one to one of your configured",
    "start": "1076650",
    "end": "1083880"
  },
  {
    "text": "destinations at least once the final part is I have my sequel results and I'm",
    "start": "1083880",
    "end": "1090480"
  },
  {
    "text": "happy with my sequel queries where does where do I send those results so we",
    "start": "1090480",
    "end": "1095850"
  },
  {
    "text": "support up to four unique destinations that can be any combination of s3 Amazon",
    "start": "1095850",
    "end": "1102390"
  },
  {
    "text": "or an Amazon redshift table an elastic search cluster another Kinesis stream",
    "start": "1102390",
    "end": "1109310"
  },
  {
    "text": "this gives you the flexibility to do things like I'm gonna parse validate",
    "start": "1109310",
    "end": "1115740"
  },
  {
    "text": "transform filter my data and then I'm gonna send it to my x3 bucket but there's only there's a small piece of my",
    "start": "1115740",
    "end": "1121380"
  },
  {
    "text": "data that I want to do analytics on in my redshift cluster and I'm gonna go ahead and send it there or alternatively",
    "start": "1121380",
    "end": "1126960"
  },
  {
    "text": "it allows you to fan out for different s3 buckets so you can do simple things like for these record types I would like",
    "start": "1126960",
    "end": "1133110"
  },
  {
    "text": "them to go to this Pacific s3 bucket for these record types I'd like to go here so it allows you that flexibility that",
    "start": "1133110",
    "end": "1142230"
  },
  {
    "text": "we thought was important to include with the initial release so you can start organizing and analyzing your data and",
    "start": "1142230",
    "end": "1150240"
  },
  {
    "text": "then processing speeds are as low as sub second so the example being with what I just gave the streaming filter you'll",
    "start": "1150240",
    "end": "1156750"
  },
  {
    "text": "find that will read process and omit that to a downstream destination under one second other queries like if you're",
    "start": "1156750",
    "end": "1163650"
  },
  {
    "text": "doing windowed analytics they'll be your processing speeds will be largely dependent on how large your windows are",
    "start": "1163650",
    "end": "1168660"
  },
  {
    "text": "so an example of a window is I'm going to compute an average over a particular column in my data over a 5-minute window",
    "start": "1168660",
    "end": "1175250"
  },
  {
    "text": "so in that particular case you're aggregating for five minutes will emit the result at the end of the window so",
    "start": "1175250",
    "end": "1181050"
  },
  {
    "text": "processing speeds will be five minutes in that case one of the things of how we've implemented that we found",
    "start": "1181050",
    "end": "1187470"
  },
  {
    "text": "customers have responded very well to is this separation of the processing the storage and the delivery so the Roger",
    "start": "1187470",
    "end": "1196350"
  },
  {
    "text": "mentioned that mentioned the aspect of composability so the fact that you can write to a firehose that is going to",
    "start": "1196350",
    "end": "1202800"
  },
  {
    "text": "take care of that delivery to your destination for you and if you have a",
    "start": "1202800",
    "end": "1207930"
  },
  {
    "text": "downstream destination that has an issue such as your redshift clusters running into problems or your last two search",
    "start": "1207930",
    "end": "1214320"
  },
  {
    "text": "cluster is having issues we don't stop processing we're continuing the processing the real-time data if one of",
    "start": "1214320",
    "end": "1220980"
  },
  {
    "text": "your destination goes down the others are not going to be impacted because the firehose provides that nice buffer between you and that final destination",
    "start": "1220980",
    "end": "1230030"
  },
  {
    "text": "so what are the top use cases that you'll implement using Amazon Kinesis analytics so first and foremost are you",
    "start": "1230810",
    "end": "1237930"
  },
  {
    "text": "building applications to perform time series analysis on your data the one of",
    "start": "1237930",
    "end": "1243090"
  },
  {
    "text": "the some of the simplest examples of this are the one that I just gave computing a five minute average over a",
    "start": "1243090",
    "end": "1249270"
  },
  {
    "text": "particular column so imagine you are ingesting application logs that have API",
    "start": "1249270",
    "end": "1255960"
  },
  {
    "text": "errors in them and you want to over that sequence of events calculate the percentage of errors in all API calls",
    "start": "1255960",
    "end": "1262530"
  },
  {
    "start": "1261000",
    "end": "1261000"
  },
  {
    "text": "that's something you can absolutely do using Kinesis analytics you can actually even do something things more powerful",
    "start": "1262530",
    "end": "1269160"
  },
  {
    "text": "things like session ization this is event correlation across the sequence of events where you imagine our users this",
    "start": "1269160",
    "end": "1275610"
  },
  {
    "text": "in your website interacting with your mobile app and you're correlating events by a user ID over a time sequence so",
    "start": "1275610",
    "end": "1282660"
  },
  {
    "text": "user clicks visits your website they click on a particular product they click on product details they add to cart",
    "start": "1282660",
    "end": "1290010"
  },
  {
    "text": "these are these windowing functions are all very easy to express in standard sequel the",
    "start": "1290010",
    "end": "1296550"
  },
  {
    "text": "other powerful thing about the service is we allow you to use independent windows on independent sequel queries so",
    "start": "1296550",
    "end": "1303450"
  },
  {
    "text": "for example in the same application you could be doing one time series analytic that's generating those five minute or one-minute statistics in the same exact",
    "start": "1303450",
    "end": "1310620"
  },
  {
    "text": "application you can be reading the same data and doing the specialization or event correlation example that I just",
    "start": "1310620",
    "end": "1315810"
  },
  {
    "text": "stated visualization is an important",
    "start": "1315810",
    "end": "1323040"
  },
  {
    "text": "aspect of analytics so we have several options that we're launching with some of them are near real time and some of",
    "start": "1323040",
    "end": "1328500"
  },
  {
    "text": "them are much closer to real time but the reason why it's so important is because as you guys know you're",
    "start": "1328500",
    "end": "1334980"
  },
  {
    "text": "performing sequel a lot of algorithms and output of your sequel results is better interpreted through a visual",
    "start": "1334980",
    "end": "1341010"
  },
  {
    "text": "representation a key example of this is our the anomaly detection function that",
    "start": "1341010",
    "end": "1347160"
  },
  {
    "text": "we're shipping with it generates an anomaly score and it's bet much easier to tell where the anomalies are if",
    "start": "1347160",
    "end": "1352740"
  },
  {
    "text": "you're visualizing the data so we have several options for this one of the",
    "start": "1352740",
    "end": "1358020"
  },
  {
    "text": "options is what we the customers that we've spoken to so far implementing sort of operational use",
    "start": "1358020",
    "end": "1363120"
  },
  {
    "text": "cases excuse me implementing operational",
    "start": "1363120",
    "end": "1371190"
  },
  {
    "text": "use cases this their right to an elastic search cluster visualize that data using",
    "start": "1371190",
    "end": "1376710"
  },
  {
    "text": "Cabana using Amazon Kinesis firehose data will be delivered in approximately one to two minutes total end-to-end time painted on",
    "start": "1376710",
    "end": "1383610"
  },
  {
    "text": "the screen in Cabana red ship is another great example hooking up with one of my favorite things to do as I interact with",
    "start": "1383610",
    "end": "1389910"
  },
  {
    "text": "the service is to hook up tableau or Amazon quick site to a redshift table and to see those analytics come in on my",
    "start": "1389910",
    "end": "1396720"
  },
  {
    "text": "stream in a minute and a half to minute latency so again near real-time latency for real-time use cases the pattern",
    "start": "1396720",
    "end": "1403650"
  },
  {
    "start": "1397000",
    "end": "1397000"
  },
  {
    "text": "which will do is you'll admit to another Kinesis stream with your analyzed data and they will write to the specific",
    "start": "1403650",
    "end": "1409350"
  },
  {
    "text": "visualization or destination of your choice this is something that we will definitely improve in the future but do",
    "start": "1409350",
    "end": "1414990"
  },
  {
    "text": "you have a lot of options available to you right at launch the third or final",
    "start": "1414990",
    "end": "1422040"
  },
  {
    "text": "use case that usually comes up when we speak to customers is creating real-time alarms and notifications kayson's so on your event stream and",
    "start": "1422040",
    "end": "1429710"
  },
  {
    "text": "this sequence of events that you're processing there are typically events of particular interest to you and these",
    "start": "1429710",
    "end": "1435049"
  },
  {
    "text": "could be computed analytics or they could be individual events an example of an individual event is a particular",
    "start": "1435049",
    "end": "1443440"
  },
  {
    "text": "customer running into an issue purchasing a product or our customer running into a blocking issue using your",
    "start": "1443440",
    "end": "1449210"
  },
  {
    "text": "mobile app that is an individual event where you can search for using a where clause in which you want to either",
    "start": "1449210",
    "end": "1454250"
  },
  {
    "start": "1450000",
    "end": "1450000"
  },
  {
    "text": "notify your ops team or you want to notify the customer hey we just found this event we're looking into this",
    "start": "1454250",
    "end": "1461409"
  },
  {
    "text": "alternatively you can do real-time alarms and notifications using analytics so this is I'm identifying I'm computing",
    "start": "1461409",
    "end": "1469789"
  },
  {
    "text": "the 1-minute average of the number of customers of my website and if I have a very large drop based off an absolute",
    "start": "1469789",
    "end": "1476659"
  },
  {
    "text": "threshold I'd like to send that to cloud watch metrics or send that to SNS to put",
    "start": "1476659",
    "end": "1482299"
  },
  {
    "text": "it into my monitoring solution so the pattern for this is we'll read data off of a kg stream or fire hose we'll",
    "start": "1482299",
    "end": "1487820"
  },
  {
    "text": "analyze the data using your configured sequel queries admit to a Kinesis stream and from there you can trigger a lambda",
    "start": "1487820",
    "end": "1496220"
  },
  {
    "text": "function right to cloud watch metrics and really that you have the flexibility",
    "start": "1496220",
    "end": "1501620"
  },
  {
    "text": "to an enable this use case how you want to the composability that we keep mentioning so sequel on streaming data a",
    "start": "1501620",
    "end": "1513970"
  },
  {
    "text": "lot of people are very familiar it's writing sequel on a static data set and they're introduced to it they're",
    "start": "1513970",
    "end": "1521720"
  },
  {
    "text": "interacting with my sequel databases or their data warehouse and when you write",
    "start": "1521720",
    "end": "1526940"
  },
  {
    "text": "sequel over a static data set you get a deterministic we know result naturally the data is not changing very very very",
    "start": "1526940",
    "end": "1532850"
  },
  {
    "text": "very quickly it looks at a snapshot of the data execute the query and gives you a result set what if your data is",
    "start": "1532850",
    "end": "1539330"
  },
  {
    "start": "1538000",
    "end": "1538000"
  },
  {
    "text": "updating continuously and needs to be processed very very quickly how do you write your sequel so some sequel queries",
    "start": "1539330",
    "end": "1547520"
  },
  {
    "text": "will operate naturally row by a row you're if you're selecting a particular set of columns you have a where clause",
    "start": "1547520",
    "end": "1554000"
  },
  {
    "text": "in that it's going to look on those rows for with your where clause",
    "start": "1554000",
    "end": "1559440"
  },
  {
    "text": "you make judgments if for your analytics but when you're doing aggregations",
    "start": "1559440",
    "end": "1564690"
  },
  {
    "text": "things like averages standard deviations mins max you're looking at things across",
    "start": "1564690",
    "end": "1570000"
  },
  {
    "text": "many rows so what is the mechanism used to bound your query so these are",
    "start": "1570000",
    "end": "1575960"
  },
  {
    "text": "important differences between sequel on streaming data and sequel on a static",
    "start": "1575960",
    "end": "1581519"
  },
  {
    "text": "data set so what does this query look like so here I have a very very simple",
    "start": "1581519",
    "end": "1587240"
  },
  {
    "text": "streaming query and you'll notice some things right away with about it that",
    "start": "1587240",
    "end": "1593820"
  },
  {
    "text": "look a little tiny bit different so the first unit we need we support sequel",
    "start": "1593820",
    "end": "1598950"
  },
  {
    "text": "tables through the reference data that I mentioned so how do we know that this is a streaming query versus in our stream Corey the first is the keyword select",
    "start": "1598950",
    "end": "1605970"
  },
  {
    "text": "stream so we tell this is a streaming based query so operate our engine whatever that to operate it as such",
    "start": "1605970",
    "end": "1613460"
  },
  {
    "start": "1613000",
    "end": "1613000"
  },
  {
    "text": "we're selecting in that from clause a stream of tweets these are the in",
    "start": "1613460",
    "end": "1620700"
  },
  {
    "text": "application streams that I'd mentioned which operate very very similar to a standard relational table you select",
    "start": "1620700",
    "end": "1628740"
  },
  {
    "text": "from these in application streams you insert into them they appear in your",
    "start": "1628740",
    "end": "1633990"
  },
  {
    "text": "from clause but it looks very much like standard sequel with some very minor modifications each row has a",
    "start": "1633990",
    "end": "1642210"
  },
  {
    "text": "corresponding special column called row time this represents the processing time",
    "start": "1642210",
    "end": "1647820"
  },
  {
    "text": "of basically the processing time of your application the every time we insert a",
    "start": "1647820",
    "end": "1654000"
  },
  {
    "text": "row into that first ending application stream we append a timestamp this is one aspect of time that we support with our",
    "start": "1654000",
    "end": "1660539"
  },
  {
    "text": "streaming analytics we also support the ingest time of where that when the data was stored in the Canisius stream or can",
    "start": "1660539",
    "end": "1666509"
  },
  {
    "text": "uses firehose and your own row time so a",
    "start": "1666509",
    "end": "1673139"
  },
  {
    "text": "streaming table is a stream the way you build applications is that",
    "start": "1673139",
    "end": "1678480"
  },
  {
    "text": "you'll create these in application streams to store intermediate sequel results the simplest application has to",
    "start": "1678480",
    "end": "1685559"
  },
  {
    "text": "in application streams the first is correlated with your configured source the second is configured with your",
    "start": "1685559",
    "end": "1692700"
  },
  {
    "text": "correlated destination and your selecting from one in application stream inserting into another an application",
    "start": "1692700",
    "end": "1698640"
  },
  {
    "text": "stream we're taking everything that was inserted into that in app that output in application stream and sending it to the",
    "start": "1698640",
    "end": "1704970"
  },
  {
    "text": "best your configured destination however you can build much richer more sophisticated applications almost",
    "start": "1704970",
    "end": "1711210"
  },
  {
    "text": "complex event like processing use cases where you're doing things like five six steps in a serial pipeline or",
    "start": "1711210",
    "end": "1718890"
  },
  {
    "text": "alternatively doing filters on your data and running different parallel pipelines where you're creating in an application",
    "start": "1718890",
    "end": "1724890"
  },
  {
    "text": "stream filtering off a portion of your data and running a specific set of business logic for that subset of data",
    "start": "1724890",
    "end": "1732590"
  },
  {
    "text": "so I mention I was I spoke briefly about writing queries on unbounded data sets",
    "start": "1735860",
    "end": "1741179"
  },
  {
    "start": "1739000",
    "end": "1739000"
  },
  {
    "text": "and what how you do that is using Windows there are several different types of windows tumbling windows",
    "start": "1741179",
    "end": "1748760"
  },
  {
    "text": "sliding windows and custom windows that we support so what we're showing here is a sliding",
    "start": "1748760",
    "end": "1754500"
  },
  {
    "text": "window and you notice it does an aggregation so I'm doing a count so this this query will operate across rows so",
    "start": "1754500",
    "end": "1763410"
  },
  {
    "text": "how do we define how to bound the quarry set so the first thing I'm going to specify as a partition by statement so",
    "start": "1763410",
    "end": "1769470"
  },
  {
    "text": "if you've used Windows and Postgres this looks really really familiar where you're just saying count author",
    "start": "1769470",
    "end": "1774720"
  },
  {
    "text": "partition by count the number of authors partition by author where you're just",
    "start": "1774720",
    "end": "1780000"
  },
  {
    "text": "going to group by the specific tweet author so I'm tweeting it'll group by",
    "start": "1780000",
    "end": "1785130"
  },
  {
    "text": "myself or the group by our Evangelist chef bar so on and so forth the next is",
    "start": "1785130",
    "end": "1790679"
  },
  {
    "text": "the the time interval in which you would like to bound the start and end of your query so here I have a one minute",
    "start": "1790679",
    "end": "1799190"
  },
  {
    "text": "sliding window the court the window starts as soon as you execute the query",
    "start": "1799190",
    "end": "1805770"
  },
  {
    "text": "and update the application and it will start emitting you know and MIT results anytime that window",
    "start": "1805770",
    "end": "1811529"
  },
  {
    "text": "changes so the in the sliding windows that it's almost like looking at like a unit of sensitivity how long the window",
    "start": "1811529",
    "end": "1817440"
  },
  {
    "text": "is you can also have row based windows so we mentioned I have a one minute",
    "start": "1817440",
    "end": "1823379"
  },
  {
    "text": "window you could also specify the last thousand rows last ten thousand rows that type of thing so what are different",
    "start": "1823379",
    "end": "1833399"
  },
  {
    "text": "types of windows so the first type of window that is using streaming analytics",
    "start": "1833399",
    "end": "1838649"
  },
  {
    "text": "is and perhaps the most easy to understand is a tumbling window this is",
    "start": "1838649",
    "end": "1845100"
  },
  {
    "text": "useful for periodic reports in which over your specified window size you want",
    "start": "1845100",
    "end": "1850620"
  },
  {
    "text": "to MIT a result at the end of that window so when you're doing aggregations before you to combine with historical",
    "start": "1850620",
    "end": "1857580"
  },
  {
    "text": "data like in redshift or s3 this is a excuse me this is a a very useful window",
    "start": "1857580",
    "end": "1867590"
  },
  {
    "text": "the window is always the same size so it's a fixed size and the windows don't",
    "start": "1867799",
    "end": "1873750"
  },
  {
    "text": "overlap so the start of the one window is the end of another so when you're",
    "start": "1873750",
    "end": "1879179"
  },
  {
    "text": "computing things like those five minute averages our talents and mins and maxes they this is very very useful functionality but what happens if you",
    "start": "1879179",
    "end": "1886470"
  },
  {
    "text": "want to know what the result of a particular window is as soon as it changes the example that I had in the",
    "start": "1886470",
    "end": "1893370"
  },
  {
    "start": "1892000",
    "end": "1892000"
  },
  {
    "text": "last slide the sliding window is great for these use cases and it's useful for real-time monitoring for there's",
    "start": "1893370",
    "end": "1898559"
  },
  {
    "text": "real-time alerting and notification like use cases so the way sliding windows work is the same fixed size as a",
    "start": "1898559",
    "end": "1905129"
  },
  {
    "text": "tumbling window but they're continuously overlapping meaning that so how do since",
    "start": "1905129",
    "end": "1912240"
  },
  {
    "text": "they're continuously overlapping how do we determine when to emit a result we emit a result any time a new row enters",
    "start": "1912240",
    "end": "1917700"
  },
  {
    "text": "the window and changes the result of the computation so a simple example of what a sliding window looks like imagine a",
    "start": "1917700",
    "end": "1924840"
  },
  {
    "text": "stream of events you're writing ten records to that she remembers events per",
    "start": "1924840",
    "end": "1930240"
  },
  {
    "text": "second and your sliding window is one second long and you're doing a count over that one second interval so the way",
    "start": "1930240",
    "end": "1938580"
  },
  {
    "text": "the sliding window will emit a results it'll it'll you'll execute it it will start to build up State so",
    "start": "1938580",
    "end": "1943860"
  },
  {
    "text": "be 1 2 3 4 up to 10 at that point sort",
    "start": "1943860",
    "end": "1948929"
  },
  {
    "text": "of reaches a steady state because you're writing through the stream of this constant interval of 10 records per second and you start to see the sliding",
    "start": "1948929",
    "end": "1955470"
  },
  {
    "text": "window and results anytime a new row enters and exits the window so it'll be 10 11 10 9 10 11 so on and so forth so",
    "start": "1955470",
    "end": "1965730"
  },
  {
    "text": "not every one of those data points is useful but when you combine with like a very simple where Clause on an absolute",
    "start": "1965730",
    "end": "1971670"
  },
  {
    "text": "threshold you just set up a super powerful application monitoring query so",
    "start": "1971670",
    "end": "1977400"
  },
  {
    "text": "sliding windows are very very useful in that regard and one of the great thing about how we've implemented the sequel",
    "start": "1977400",
    "end": "1983820"
  },
  {
    "text": "here is that you can have independent windows in the same sequel query over different metrics so for example if you",
    "start": "1983820",
    "end": "1990600"
  },
  {
    "text": "want to do an average over a particular metric over a one minute window but you want to do the max over a 10 minute",
    "start": "1990600",
    "end": "1996750"
  },
  {
    "text": "window you can do so in the same exact query the final window is what I think",
    "start": "1996750",
    "end": "2004820"
  },
  {
    "text": "is one of the most valuable windows but it is definitely one of the most difficult to understand and these are",
    "start": "2004820",
    "end": "2015440"
  },
  {
    "text": "called I'm offering a fairly ambiguous term to describe them",
    "start": "2015440",
    "end": "2020480"
  },
  {
    "text": "they're called custom windows the reason why I call them custom windows is because the start and end of the window",
    "start": "2020480",
    "end": "2027110"
  },
  {
    "text": "are not defined when you execute the sequel query you're looking for particular conditions to start the",
    "start": "2027110",
    "end": "2032750"
  },
  {
    "text": "window and end the window so an example of a custom window is the session ization example that I brought up so I",
    "start": "2032750",
    "end": "2039410"
  },
  {
    "text": "want to start a window anytime a new user logged into my app any time a new",
    "start": "2039410",
    "end": "2045910"
  },
  {
    "text": "ec2 instance starts it starts up and I want to track that over a particular",
    "start": "2045910",
    "end": "2051888"
  },
  {
    "text": "time interval looking for particular events so I'm starting up an ec2",
    "start": "2051889",
    "end": "2057408"
  },
  {
    "text": "instance I want to make sure that it successfully started up and my application was bootstrapped and is",
    "start": "2057409",
    "end": "2063230"
  },
  {
    "text": "running effectively and I want to do that to make sure that it occurs over 10 minutes so I'll end the window if I see",
    "start": "2063230",
    "end": "2070100"
  },
  {
    "text": "that I have a status notifying me that my application is rep up and running correctly and if 10 minutes passes and",
    "start": "2070100",
    "end": "2076669"
  },
  {
    "text": "that occurs all made a result these are custom windows and you can see when you have the ability to provide such flexible",
    "start": "2076669",
    "end": "2084080"
  },
  {
    "text": "windowing your gonna enable fairly powerful use cases so what are the",
    "start": "2084080",
    "end": "2094908"
  },
  {
    "text": "patterns with how you'll implement your applications what have we seen customers do from our conversations with them",
    "start": "2094909",
    "end": "2101200"
  },
  {
    "text": "there's usually a couple steps so the first step is the pre-processing step",
    "start": "2101200",
    "end": "2107270"
  },
  {
    "text": "this is I've initially received the data into my application and I want I need to transform it manipulate it sort of it's",
    "start": "2107270",
    "end": "2114050"
  },
  {
    "text": "sort of like that initial ETL step to get it to be the nice clean data that I",
    "start": "2114050",
    "end": "2119270"
  },
  {
    "text": "want to work with in my application so we take care of a part of this for you with that initial schema inference and",
    "start": "2119270",
    "end": "2126170"
  },
  {
    "text": "applying your initial schema that you specify but a lot of times people want to do more they'll filter it we've seen",
    "start": "2126170",
    "end": "2133700"
  },
  {
    "text": "some interesting cases where customers are combining different data formats like they've combined with they've",
    "start": "2133700",
    "end": "2143120"
  },
  {
    "text": "combined with JSON and CSV so for example they have a column in there or a",
    "start": "2143120",
    "end": "2150980"
  },
  {
    "text": "key value pair in there JSON document which is a csv record and they need to do additional splits or they have a",
    "start": "2150980",
    "end": "2155990"
  },
  {
    "text": "column in their data structure that is very very unstructured and they need to rip out specific values there have an",
    "start": "2155990",
    "end": "2162680"
  },
  {
    "text": "application log that they're processing and they almost want to like annotate it and tag it so they there they've put in",
    "start": "2162680",
    "end": "2168140"
  },
  {
    "text": "a rock a rock column that is your like Apache log and it's a column of type var",
    "start": "2168140",
    "end": "2174350"
  },
  {
    "start": "2172000",
    "end": "2172000"
  },
  {
    "text": "char 5,000 or something like that and they want to search for particular patterns and their Apache log and start",
    "start": "2174350",
    "end": "2180590"
  },
  {
    "text": "adding columns saying like this this was an error this was an area of type blah blah blah this was a specific",
    "start": "2180590",
    "end": "2186350"
  },
  {
    "text": "application these are these pre-processing steps sort of to get more information to understand the data a little bit more these are usually the",
    "start": "2186350",
    "end": "2192620"
  },
  {
    "text": "first quarries that you write and you write that query you select from your source in up in an application stream",
    "start": "2192620",
    "end": "2197930"
  },
  {
    "text": "you insert it into your first intermediate in application stream the",
    "start": "2197930",
    "end": "2203570"
  },
  {
    "text": "next queries that people typically men are the basic analytics so simple counts simple aggregations over the windows",
    "start": "2203570",
    "end": "2210140"
  },
  {
    "text": "that we've discussed where they're taking that cleaned up data and they're just performing that",
    "start": "2210140",
    "end": "2215510"
  },
  {
    "text": "basic analytics and the those two use case is just getting from the pre-processing step in the basic",
    "start": "2215510",
    "end": "2220610"
  },
  {
    "text": "analytics when I go through the video in a couple minutes here you'll see it's very quick getting to that so I think",
    "start": "2220610",
    "end": "2226430"
  },
  {
    "text": "that's why we find the service customers have tried to service out of try it found it so easy to use and then finally",
    "start": "2226430",
    "end": "2233510"
  },
  {
    "text": "it's the advanced analytics where you start getting that value from your data and reacting to that data in real time",
    "start": "2233510",
    "end": "2240430"
  },
  {
    "text": "to really sort of take your data from just being data to transform into",
    "start": "2240430",
    "end": "2245600"
  },
  {
    "text": "information to finally getting to insight we support things like the event",
    "start": "2245600",
    "end": "2253040"
  },
  {
    "text": "correlation use case session ization that I brought up we also have our anomaly detection algorithm which will",
    "start": "2253040",
    "end": "2258920"
  },
  {
    "text": "basically you'll pass it a set of numerical data points and we'll send you back an anomaly score it's almost like",
    "start": "2258920",
    "end": "2266270"
  },
  {
    "text": "the likelihood it's an anomaly so you can define outliers on your data stream",
    "start": "2266270",
    "end": "2271930"
  },
  {
    "text": "you can also do advanced analytics that things like filtering our particular set",
    "start": "2271930",
    "end": "2277670"
  },
  {
    "text": "of data or a particular event type off of your stream inserting it into an application stream taking a different",
    "start": "2277670",
    "end": "2284630"
  },
  {
    "text": "event type inserting into a parallel in application stream and then merging those two streams together so this is",
    "start": "2284630",
    "end": "2290990"
  },
  {
    "text": "that complex event processing use case that I like to bring you up where you're almost looking at these images in application streams as nodes in your",
    "start": "2290990",
    "end": "2297890"
  },
  {
    "text": "processing pipeline and it's processing steps finally the post-processing steps",
    "start": "2297890",
    "end": "2304390"
  },
  {
    "text": "sometimes this is needed it's not always the case you usually need it on like alerting or triggering like use cases so",
    "start": "2304390",
    "end": "2311510"
  },
  {
    "text": "you might have a bunch of analytics computed in your stream but you may only want to send to your real-time",
    "start": "2311510",
    "end": "2317720"
  },
  {
    "text": "notification service when particular events occur these are the last steps",
    "start": "2317720",
    "end": "2322880"
  },
  {
    "text": "they're typically simple where clauses they can be more complicated though you could use for example you could be",
    "start": "2322880",
    "end": "2328880"
  },
  {
    "text": "computing tumbling windows in combination with sliding windows to",
    "start": "2328880",
    "end": "2333890"
  },
  {
    "text": "figure out what to emit to that downstream destination but it's usually your last query your last sequel",
    "start": "2333890",
    "end": "2340070"
  },
  {
    "text": "statement before you insert into that output in an application stream and the data flows downstream so how do we how",
    "start": "2340070",
    "end": "2352290"
  },
  {
    "text": "do we charge you for this service what is the pricing model we have a our unit",
    "start": "2352290",
    "end": "2358920"
  },
  {
    "text": "of scalability and our unit of processing is called the Kinesis processing unit and it's fairly simple",
    "start": "2358920",
    "end": "2365160"
  },
  {
    "text": "to understand its represents a single VB Cu and 4 gigabytes of memory we will",
    "start": "2365160",
    "end": "2371490"
  },
  {
    "text": "scale the number of KP use for you based off of two things that primarily drive",
    "start": "2371490",
    "end": "2376859"
  },
  {
    "text": "what your compute and your memory usage the first is fairly obvious it's the",
    "start": "2376859",
    "end": "2382440"
  },
  {
    "text": "data throughput on your stream so just reading from that stream processing and",
    "start": "2382440",
    "end": "2387660"
  },
  {
    "text": "getting into the sequel engine is very different KP use of if you're processing one record a second versus a hundred",
    "start": "2387660",
    "end": "2394050"
  },
  {
    "text": "thousand records per second or rows per second the next is your query complexity",
    "start": "2394050",
    "end": "2399599"
  },
  {
    "text": "so example being different types of queries we'll use different types of",
    "start": "2399599",
    "end": "2404760"
  },
  {
    "text": "resources so if you have a lot of windows in your application that hold a",
    "start": "2404760",
    "end": "2409890"
  },
  {
    "text": "lot of state that will be a more memory intensive application and will scale the number of KP use for you such as the",
    "start": "2409890",
    "end": "2416609"
  },
  {
    "text": "math your memory requirements alternatively if you're not holding a lot of state but you're doing a lot of",
    "start": "2416609",
    "end": "2421830"
  },
  {
    "text": "streaming filters and you're processing a high throughput of small records these are typically CPU bound workloads will",
    "start": "2421830",
    "end": "2427770"
  },
  {
    "text": "again will scale the number of KP use for you so you don't have to choose an instance size to try to match the",
    "start": "2427770",
    "end": "2432780"
  },
  {
    "text": "requirements of your application I'm not sure if I'm actually hitting this to",
    "start": "2432780",
    "end": "2439020"
  },
  {
    "text": "skip the slide over whether it's just triggering itself but I apologize so the",
    "start": "2439020",
    "end": "2444690"
  },
  {
    "text": "KP you is 11 cents per hour in u.s. East one we're launching in three regions",
    "start": "2444690",
    "end": "2451380"
  },
  {
    "text": "today Northern Virginia Oregon Northern",
    "start": "2451380",
    "end": "2456630"
  },
  {
    "text": "Virginia Oregon and Dublin I so there is",
    "start": "2456630",
    "end": "2464940"
  },
  {
    "text": "though it's simple to understand a kpu we recognize that it can be",
    "start": "2464940",
    "end": "2471290"
  },
  {
    "text": "sometimes challenging to figure out how much am I going to spend how much is the throughput so typically recommend will",
    "start": "2471990",
    "end": "2479010"
  },
  {
    "text": "give you a visibility to in the form of CloudWatch metrics and overtime will give you more and more transparency like",
    "start": "2479010",
    "end": "2486470"
  },
  {
    "text": "definitely from talking with the customers who realize that we got to get more transparency over time on this but just to give you some examples of what",
    "start": "2486470",
    "end": "2492330"
  },
  {
    "start": "2492000",
    "end": "2492000"
  },
  {
    "text": "this looks like so a single kpu you have your processing eighty processing about",
    "start": "2492330",
    "end": "2499080"
  },
  {
    "text": "one megabyte at 1,000 records for per second on your stream so a small to",
    "start": "2499080",
    "end": "2504390"
  },
  {
    "text": "medium sized workload it's cost $80 a month so a single KP you to do a bunch",
    "start": "2504390",
    "end": "2510450"
  },
  {
    "text": "of these streaming filters use cases that I mentioned another use case is I'm doing aggregations there for one minute",
    "start": "2510450",
    "end": "2516150"
  },
  {
    "text": "windows on stream five times the size so let's just say I'm a five megabyte per stream per second streaming data source",
    "start": "2516150",
    "end": "2523859"
  },
  {
    "text": "and I have a bunch of children by trekkers flowing through it so a bunch of very small records so I'm having to",
    "start": "2523859",
    "end": "2529410"
  },
  {
    "text": "hold a lot of state in my window I need to KP use it'll cost $150 a month we",
    "start": "2529410",
    "end": "2535410"
  },
  {
    "text": "found from our initial conversations with customers that the elasticity of",
    "start": "2535410",
    "end": "2540420"
  },
  {
    "text": "this model is what they truly appreciate because it's very hard to anticipate peak loads and how how our customers",
    "start": "2540420",
    "end": "2548730"
  },
  {
    "text": "query ER is going to respond when the data changes it with a different variety over time so the final slide that I have",
    "start": "2548730",
    "end": "2558210"
  },
  {
    "text": "is we're going to go through a walkthrough of creating your first Kinesis analytics application and I really encourage everyone in the room to",
    "start": "2558210",
    "end": "2564180"
  },
  {
    "text": "try it because it takes five minutes it's very very easy so I provide a name",
    "start": "2564180",
    "end": "2569609"
  },
  {
    "text": "to my application so this will be my first Amazon Kinesis analytics application and then from there I'm",
    "start": "2569609",
    "end": "2575190"
  },
  {
    "text": "taken to the application hub page the",
    "start": "2575190",
    "end": "2582030"
  },
  {
    "text": "three components of the service that we keep mentioned are presented on that page the source the application code and the destination I'm presenting it with a",
    "start": "2582030",
    "end": "2588990"
  },
  {
    "text": "list of my Kinesis streams and fire hoses and I have a demo stream which is",
    "start": "2588990",
    "end": "2594480"
  },
  {
    "text": "available to all of you to try the service out which has simulated stock data inserted into it directly from your",
    "start": "2594480",
    "end": "2600510"
  },
  {
    "text": "console we go through a process called discovery and the schema and present it back to you",
    "start": "2600510",
    "end": "2606970"
  },
  {
    "text": "and showed you in a very familiar tabular format from there you can view",
    "start": "2606970",
    "end": "2612640"
  },
  {
    "text": "the raw data records that we sampled off the stream as well as edit the schema to",
    "start": "2612640",
    "end": "2619000"
  },
  {
    "text": "your liking to either restrict it farther or unrestricted so on the top",
    "start": "2619000",
    "end": "2629590"
  },
  {
    "text": "part of the screen you see your columns laid out the JSON path which points the",
    "start": "2629590",
    "end": "2634960"
  },
  {
    "text": "specific element in the JSON document the data type and the parameters around",
    "start": "2634960",
    "end": "2640870"
  },
  {
    "text": "the data type on the bottom half of the screen you see your data stream I'm gonna start the application right now",
    "start": "2640870",
    "end": "2646060"
  },
  {
    "text": "and I'm gonna start schema you can at at that point you can start schema Tizen a live data stream this is very helpful",
    "start": "2646060",
    "end": "2651580"
  },
  {
    "text": "for test and development where you can see different data flowing in I'm adjusting my schema to accommodate that",
    "start": "2651580",
    "end": "2657820"
  },
  {
    "text": "different data we recommend you define schemas that are fairly flexible if data",
    "start": "2657820",
    "end": "2663520"
  },
  {
    "text": "comes in that doesn't conform to your schema we sent to what's called an in application error stream where a week",
    "start": "2663520",
    "end": "2669760"
  },
  {
    "text": "you can select from that straight that in application stream but in most cases customers will just select everything",
    "start": "2669760",
    "end": "2674950"
  },
  {
    "text": "from it and insert into an STD bucket this was a malformed record this didn't apply to my schema set it here this is",
    "start": "2674950",
    "end": "2681880"
  },
  {
    "text": "our interactive sequel letter on the top half of the screen you have a text editor with intelligent autocomplete",
    "start": "2681880",
    "end": "2686890"
  },
  {
    "text": "Find and Replace syntax highlighting on the bottom half of the screen is where",
    "start": "2686890",
    "end": "2692050"
  },
  {
    "text": "your sequel results will be shown as well as there's some data exploration features in it and then we have a nice",
    "start": "2692050",
    "end": "2698230"
  },
  {
    "text": "feature that allows people to get started easily called sequel templates here's a couple that we've launched with",
    "start": "2698230",
    "end": "2704950"
  },
  {
    "text": "we are excited to work with customers and add them over time you find something useful send it to us we'll add",
    "start": "2704950",
    "end": "2710140"
  },
  {
    "text": "it to the templates list you can see we have continuous fit filters aggregations",
    "start": "2710140",
    "end": "2715180"
  },
  {
    "text": "over different types of windows simple or alerting queries these all these",
    "start": "2715180",
    "end": "2721150"
  },
  {
    "text": "templates work with the demo stream that I already selected as my streaming data source I'm going to click save and run",
    "start": "2721150",
    "end": "2727210"
  },
  {
    "text": "and you're going to see the sequel queries right in your browser in real time we item potently item they update",
    "start": "2727210",
    "end": "2732760"
  },
  {
    "text": "the application Check Point update the sequel code and then present the sequel results back to you",
    "start": "2732760",
    "end": "2738750"
  },
  {
    "text": "on the left-hand side of the screen any in application stream that you create",
    "start": "2738750",
    "end": "2744660"
  },
  {
    "text": "you have the ability to give visibility into so you have a six step pipeline you",
    "start": "2744660",
    "end": "2749799"
  },
  {
    "text": "can see the results of every single intermediate steps it gives you great visibility to what's",
    "start": "2749799",
    "end": "2755230"
  },
  {
    "text": "happening to your stream in real time on the console I'm going to select another",
    "start": "2755230",
    "end": "2760509"
  },
  {
    "text": "query here the query I'm going to select is an aggregation over a tumbling window it's going to do count over a 10 seconds",
    "start": "2760509",
    "end": "2767910"
  },
  {
    "text": "so you'll see that special column that I mentioned wrote I'm iterating every 10 seconds computing those counts it's",
    "start": "2767910",
    "end": "2776559"
  },
  {
    "text": "going to group by this ticker symbol so you look you'll see the number of transactions on the simulated stock data",
    "start": "2776559",
    "end": "2781630"
  },
  {
    "text": "stream per stickleton per tick tick or symbol come out every 10 seconds so this",
    "start": "2781630",
    "end": "2789670"
  },
  {
    "text": "quarry takes 10 seconds so I want to pause we mentioned the processing speeds",
    "start": "2789670",
    "end": "2796509"
  },
  {
    "text": "this is a reflective of that like continuous filter operates sub-second the tumbling window that's 10 seconds",
    "start": "2796509",
    "end": "2802359"
  },
  {
    "text": "long you're waiting 10 seconds for every result because it's emitting results at the end of every window will",
    "start": "2802359",
    "end": "2808630"
  },
  {
    "text": "continuously stream sample data to your browser so you can give visibility to whether your quarry was correct when",
    "start": "2808630",
    "end": "2814539"
  },
  {
    "text": "you're happy with your quarry result what you'll do is you'll create a destination and you'll basically point",
    "start": "2814539",
    "end": "2820480"
  },
  {
    "text": "to the an application stream that you created that you want to emit downstream so in this case I have a destination",
    "start": "2820480",
    "end": "2826900"
  },
  {
    "text": "sequel stream that has those aggregated results I'm going to choose my output format CSV and I'm going to choose a",
    "start": "2826900",
    "end": "2833440"
  },
  {
    "text": "firehose delivery stream which is going to emit my data to an automatically loaded into redshift from there that",
    "start": "2833440",
    "end": "2839259"
  },
  {
    "text": "from there you've got an end-to-end running application hopefully this is not the end of the video yeah and and",
    "start": "2839259",
    "end": "2847359"
  },
  {
    "text": "then running application that is continuously reading data from your streaming data source processing that",
    "start": "2847359",
    "end": "2853990"
  },
  {
    "text": "data using your sequel code and admitting it downstream so you basically have a man fully managed application that's elastic that's performing your",
    "start": "2853990",
    "end": "2861220"
  },
  {
    "text": "analytics and look there we go looks like it's a little bit too long to save",
    "start": "2861220",
    "end": "2866960"
  },
  {
    "text": "so there's your completed application so",
    "start": "2866960",
    "end": "2872660"
  },
  {
    "text": "that is the end of my talk I'm happy thank you very much for attending we're",
    "start": "2872660",
    "end": "2878809"
  },
  {
    "text": "really excited to see what customers were build with this service we think that we've just started here and I were",
    "start": "2878809",
    "end": "2885099"
  },
  {
    "text": "very encouraged with what you guys will do with the service listening to your",
    "start": "2885099",
    "end": "2890839"
  },
  {
    "text": "feedback and continue to build out the service to meet your guys's needs the",
    "start": "2890839",
    "end": "2896329"
  },
  {
    "text": "other thing that we find very valuable is that we now have an end and streaming",
    "start": "2896329",
    "end": "2903500"
  },
  {
    "text": "data services that consist of streaming capture and allowing you to build custom applications through Kinesis streams",
    "start": "2903500",
    "end": "2910240"
  },
  {
    "text": "ingestion and loading of streaming data and a high velocity that's scalable with",
    "start": "2910240",
    "end": "2915530"
  },
  {
    "text": "zero administration with Kinesis firehose and a powerful sequel based streaming data application service that",
    "start": "2915530",
    "end": "2921980"
  },
  {
    "text": "allows you to attach to both Canisius streams and firehose continuously process that data and emit it to",
    "start": "2921980",
    "end": "2927829"
  },
  {
    "text": "downstream destinations of your choice so I'm going to stay happy answer some questions thank you guys all very much",
    "start": "2927829",
    "end": "2933319"
  },
  {
    "text": "for attending",
    "start": "2933319",
    "end": "2935799"
  },
  {
    "text": "i if you could just come up to the microphone or if you don't mind speaking",
    "start": "2938610",
    "end": "2944040"
  },
  {
    "text": "up what is the reliability of these streams so if I have a window for say",
    "start": "2944040",
    "end": "2950310"
  },
  {
    "text": "three hours and my machine goes off so the question is what is the reliability of the service if I have a three hour",
    "start": "2950310",
    "end": "2962220"
  },
  {
    "text": "window what happens is the stream goes up yes so the Canisius streams Kinesis firehose",
    "start": "2962220",
    "end": "2967710"
  },
  {
    "text": "are durable storage services all right so they they guarantee no data loss all",
    "start": "2967710",
    "end": "2974790"
  },
  {
    "text": "right Kinesis analytics we do all of our processing in memory and we take checkpoints throughout the application",
    "start": "2974790",
    "end": "2980700"
  },
  {
    "text": "which are basically like watermarks of where in the stream have we read and processed data successfully we then it",
    "start": "2980700",
    "end": "2987510"
  },
  {
    "text": "have at least ones processing semantics so when we've successfully delivered your data to your destination we'll",
    "start": "2987510",
    "end": "2992970"
  },
  {
    "text": "check point and say we've successfully processed all records for this destination from the streaming data",
    "start": "2992970",
    "end": "2999570"
  },
  {
    "text": "source regarding the size of Windows this is a real-time streaming application service we typically won't",
    "start": "2999570",
    "end": "3005690"
  },
  {
    "text": "recommend you having more than say an hour-long window there are certain very specific use cases will you'll want to",
    "start": "3005690",
    "end": "3012350"
  },
  {
    "text": "compute longer windows on I'd love to have a conversation about those specific",
    "start": "3012350",
    "end": "3017990"
  },
  {
    "text": "use cases but if you're doing a three-hour long window typically it's easier just to compute what you need to",
    "start": "3017990",
    "end": "3023360"
  },
  {
    "text": "compute and get it into a Down so you have a few second window yeah I've been using stream sequel yeah but obviously",
    "start": "3023360",
    "end": "3029480"
  },
  {
    "text": "we don't want to lose data in between just in case yes in the fifth second that data is not",
    "start": "3029480",
    "end": "3036080"
  },
  {
    "text": "there because I understand so that we read data from this very durable source",
    "start": "3036080",
    "end": "3041240"
  },
  {
    "text": "and we take checkpoints as we process it does that answer your question yes all right great thank you so much this is very very",
    "start": "3041240",
    "end": "3048170"
  },
  {
    "text": "helpful and I'm very excited thank you so the question is really you've got a",
    "start": "3048170",
    "end": "3054140"
  },
  {
    "text": "fully compliant and the SQL interface as far as the roadmap is concerned do you",
    "start": "3054140",
    "end": "3060530"
  },
  {
    "text": "see the ability for other applications to write SQL against this or will it be",
    "start": "3060530",
    "end": "3066380"
  },
  {
    "text": "really just you write the SQL there and land it within you know ESB I got you it's a excellent",
    "start": "3066380",
    "end": "3075290"
  },
  {
    "text": "question so the question is you can write ansi standard sequel over streaming gated with Kinesis analytics",
    "start": "3075290",
    "end": "3082160"
  },
  {
    "text": "do you see the ability of other applications to basically tap into the application and write their own sequence",
    "start": "3082160",
    "end": "3089300"
  },
  {
    "text": "structures we're creating we're learning from our customers every day I can say",
    "start": "3089300",
    "end": "3094790"
  },
  {
    "text": "that what you just described is something that has come up quite a bit since we've talked to our customers we",
    "start": "3094790",
    "end": "3099920"
  },
  {
    "text": "do have AP is the entire cross flow experience is backed by AP is in which you can update the sequel to get the",
    "start": "3099920",
    "end": "3106700"
  },
  {
    "text": "sequel results you really need to send it to like a Kinesis streamer or firehose though but that being said like",
    "start": "3106700",
    "end": "3111890"
  },
  {
    "text": "future state I would just the only thing I can say about that is that we've heard that request like can you guys offer a",
    "start": "3111890",
    "end": "3118220"
  },
  {
    "text": "JDBC connection or something like this not say JDBC I wrote ABC because quite frankly most of the data you know deal",
    "start": "3118220",
    "end": "3125480"
  },
  {
    "text": "with the machine it's five to ten seconds and most of what you're going to throw away great question we're gonna listen to the feedback we",
    "start": "3125480",
    "end": "3131690"
  },
  {
    "text": "get over the next couple months but I realize that that's already at the top of mind for us yep thank you hi great",
    "start": "3131690",
    "end": "3140480"
  },
  {
    "text": "stuff so the format of course in my formats support JSON delimiter text yes",
    "start": "3140480",
    "end": "3147740"
  },
  {
    "text": "it's this extendable so we have a lot of data exploring through Canisius in a",
    "start": "3147740",
    "end": "3152870"
  },
  {
    "text": "burrow format this is extendable to so",
    "start": "3152870",
    "end": "3158740"
  },
  {
    "text": "unfortunately not right now when we've talked to customers Avro comes up quite a bit I mean the speaker before us is",
    "start": "3158740",
    "end": "3165650"
  },
  {
    "text": "Comcast we've been working with them quite a bit and they also use Avro this",
    "start": "3165650",
    "end": "3171500"
  },
  {
    "text": "is absolutely on our radar Avro specifically is on our radar because we've heard it a lot from customers I think what you just",
    "start": "3171500",
    "end": "3177590"
  },
  {
    "text": "described though is where we'd like to think about the service well we make a couple things super easy to use like",
    "start": "3177590",
    "end": "3184820"
  },
  {
    "text": "JSON or CSV possibly Avro and then make it extensible after that because when",
    "start": "3184820",
    "end": "3191120"
  },
  {
    "text": "we've spoken to customers they it's not just a blur they come up they bring up they bring up Avro protobuf Amazon ion",
    "start": "3191120",
    "end": "3197180"
  },
  {
    "text": "all sorts of different serializable format and we were curious to hear from customers",
    "start": "3197180",
    "end": "3202400"
  },
  {
    "text": "what they need if it's just support Avro we'll just do that if it's run a piece of Python code or something like that",
    "start": "3202400",
    "end": "3208460"
  },
  {
    "text": "before to transform the record we'll do that well I think the important thing is that it is extendable more so than just",
    "start": "3208460",
    "end": "3215720"
  },
  {
    "text": "say support Avro yes this who knows two years we might not have ever anyway so the extend ability is the most important",
    "start": "3215720",
    "end": "3222770"
  },
  {
    "text": "piece so then you meet your plants about that or I what's the Doug in the roadmap",
    "start": "3222770",
    "end": "3227800"
  },
  {
    "text": "all I can say is that we're really excited to release this to the public right now we've gotten some initial",
    "start": "3227800",
    "end": "3233960"
  },
  {
    "text": "feedback from customers that we've been working with over the last couple of months I think I've shared that a",
    "start": "3233960",
    "end": "3239810"
  },
  {
    "text": "Burroughs important part of that feedback but I can't really say when we're going to get to implement a",
    "start": "3239810",
    "end": "3245210"
  },
  {
    "text": "specific format or an extensibility I will say that I agree with your conclusion about it's more important to",
    "start": "3245210",
    "end": "3252260"
  },
  {
    "text": "be extensible than it is to support and fifty million formats thank you very",
    "start": "3252260",
    "end": "3257300"
  },
  {
    "text": "much for the question very informative session I've had a question about the",
    "start": "3257300",
    "end": "3264290"
  },
  {
    "text": "anomaly anomaly detection yeah what kind of machine learning algorithm are using and there needs a roadmap for in your",
    "start": "3264290",
    "end": "3271490"
  },
  {
    "text": "evolution of that so the question is what time of algorithm does the machine",
    "start": "3271490",
    "end": "3277910"
  },
  {
    "text": "learning online anomaly detection algorithm implement thank you for asking",
    "start": "3277910",
    "end": "3283250"
  },
  {
    "text": "that question I really like the geek out on some of the stuff so I appreciate it we have a principal machine learning",
    "start": "3283250",
    "end": "3288650"
  },
  {
    "text": "scientist names mashina nina Mishra she just attended the ICML conference and",
    "start": "3288650",
    "end": "3295040"
  },
  {
    "text": "there's a published paper that we have on our web sites you can read all about the algorithm we're really excited about",
    "start": "3295040",
    "end": "3300740"
  },
  {
    "text": "it because some of the internal teams have been playing around with it and they find it works really well I think",
    "start": "3300740",
    "end": "3306410"
  },
  {
    "text": "our approach is to make a couple of these algorithms super simple and then get feedback and see like do you guys",
    "start": "3306410",
    "end": "3313430"
  },
  {
    "text": "want extensions do you want another color a couple of subproblems but I think in order to read more the papers",
    "start": "3313430",
    "end": "3318770"
  },
  {
    "text": "on our website it's also the Omni ICML website thank you ICL I actually forget",
    "start": "3318770",
    "end": "3325250"
  },
  {
    "text": "what the acronym stands for they follow guys yeah it looks like a super interesting",
    "start": "3325250",
    "end": "3330960"
  },
  {
    "text": "product a very basic question it was hard to tell whether each stream could output into just one destination or if",
    "start": "3330960",
    "end": "3337260"
  },
  {
    "text": "you could have multiple destinations and you could dynamically sort things out into different destinations for example",
    "start": "3337260",
    "end": "3342690"
  },
  {
    "text": "if I had an incoming stream and I wanted to like dumps up into different red shift tables like dynamically is that",
    "start": "3342690",
    "end": "3348390"
  },
  {
    "text": "doable or absolutely so the question is it's hard to tell whether I can like",
    "start": "3348390",
    "end": "3354300"
  },
  {
    "text": "read from a single stream and then just fan out to different specific destinations absolutely supported for",
    "start": "3354300",
    "end": "3359970"
  },
  {
    "text": "example you could write to what we think one of the most common applications is going to be is I've got five record",
    "start": "3359970",
    "end": "3366840"
  },
  {
    "text": "types in my source stream or ten records type in my source stream I filter off into separate and application streams",
    "start": "3366840",
    "end": "3372630"
  },
  {
    "text": "for each record type I do a specific analytic like aggregation on that and then I write to a specific redshift",
    "start": "3372630",
    "end": "3377760"
  },
  {
    "text": "table that is absolutely supported we support up to four destinations at",
    "start": "3377760",
    "end": "3383310"
  },
  {
    "text": "launch tables so the way we write to redshift and the right right now we write to most destinations is to a",
    "start": "3383310",
    "end": "3389760"
  },
  {
    "text": "Kinesis firehose so each firehose is tied to a single redshift table so it'll",
    "start": "3389760",
    "end": "3394950"
  },
  {
    "text": "load up to four different tables and then red ship firehose itself is looking",
    "start": "3394950",
    "end": "3400230"
  },
  {
    "text": "at interesting functionality down the pipeline to also do more of that so I look forward to the end of towards the",
    "start": "3400230",
    "end": "3407280"
  },
  {
    "text": "end of the year okay right yeah my wish is it yeah I'm gonna looking forward to that part because we have a unbounded",
    "start": "3407280",
    "end": "3412470"
  },
  {
    "text": "number of dynamic tables that we're doing so for launch for like right now what I'd recommend is to I mean the",
    "start": "3412470",
    "end": "3419130"
  },
  {
    "text": "processing latency when you're doing filters is really really fast so what I'd recommend is just doing app and this",
    "start": "3419130",
    "end": "3425820"
  },
  {
    "text": "is a bit of a workaround so I'm gonna admit it before good what I recommend is just to do like app stream or firehose",
    "start": "3425820",
    "end": "3431910"
  },
  {
    "text": "app and then just continue to fan-out to get to the number of destinations that you need to each one of these solutions",
    "start": "3431910",
    "end": "3437400"
  },
  {
    "text": "is fully managed so but I do recognize that keeping track of it is a little bit of a pain so we can always improve in",
    "start": "3437400",
    "end": "3444240"
  },
  {
    "text": "the future there but that's how I would approach that problem for now awesome thank you it looks awesome thank you I appreciate it hi I think you can answer this but when",
    "start": "3444240",
    "end": "3453450"
  },
  {
    "text": "you define the schema you do that wasn't stream specific that was query specific no it is",
    "start": "3453450",
    "end": "3459690"
  },
  {
    "text": "stream specific so we have one schema / streaming data source that's added we",
    "start": "3459690",
    "end": "3466740"
  },
  {
    "text": "support a single streaming data source and therefore we support a single schema we have gotten some feedback that customers want us to like support",
    "start": "3466740",
    "end": "3473580"
  },
  {
    "text": "multiple schema for different record types but the sorry go ahead yeah yeah",
    "start": "3473580",
    "end": "3478740"
  },
  {
    "text": "you said that you could you could have different events going into the same team events have their own schema so",
    "start": "3478740",
    "end": "3485730"
  },
  {
    "text": "discover the discovery the schema inference does really good job like if you have 30 different record types or in",
    "start": "3485730",
    "end": "3492600"
  },
  {
    "text": "that 10 columns each so you have 300 columns it does a really good job if it reads 30 different record types",
    "start": "3492600",
    "end": "3497700"
  },
  {
    "text": "it'll just recommend a normalized schema each column is not nullable and you just",
    "start": "3497700",
    "end": "3503280"
  },
  {
    "text": "basically use a determinate er to filter off after that point so it's basically flatten everything into a normalized",
    "start": "3503280",
    "end": "3508680"
  },
  {
    "text": "schema and then filter off and I do recognize that certain situations makes it a little bit difficult is there any",
    "start": "3508680",
    "end": "3514830"
  },
  {
    "text": "way that you think we can improve that like how would you like to run it kind of back to what he was saying actually",
    "start": "3514830",
    "end": "3519990"
  },
  {
    "text": "work with him was like we have a bunch of different type of events going on to the onto the stream and then we want to",
    "start": "3519990",
    "end": "3525660"
  },
  {
    "text": "be able to filter on them say this is a dissident I've good of that redshift table so if each of those event I could have is this fine schema and then so the",
    "start": "3525660",
    "end": "3535950"
  },
  {
    "text": "approach that that I mentioned is how we're gonna take it for now on Danny is is how we're going to approach this for",
    "start": "3535950",
    "end": "3541470"
  },
  {
    "text": "now to just define a normalized schema you'll have a bunch of sparse columns for certain records god when I'm happy",
    "start": "3541470",
    "end": "3547740"
  },
  {
    "text": "where we are really curious is for customers just try this out with the different data you're in our streams now",
    "start": "3547740",
    "end": "3553110"
  },
  {
    "text": "that it's out publicly and they give us feedback on it I mean if you have I've approached that I've just recommended if",
    "start": "3553110",
    "end": "3558930"
  },
  {
    "text": "you have like 300 different record types and a hundred columns on each record types not that would work really well so",
    "start": "3558930",
    "end": "3564720"
  },
  {
    "text": "but I'm curious we think that we've solved the majority of the problem for most customers and we're looking forward",
    "start": "3564720",
    "end": "3570090"
  },
  {
    "text": "to just iterating on what we currently have to meet like Beus cases like you just specify thank you you're welcome",
    "start": "3570090",
    "end": "3577040"
  },
  {
    "text": "can we use this with XML data no unfortunately do not support XML is that",
    "start": "3577040",
    "end": "3583770"
  },
  {
    "text": "an important use case for you yes it is do you have XML data that's currently flowing through your Canisius creams or",
    "start": "3583770",
    "end": "3590370"
  },
  {
    "text": "fire hoses right now yes okay it comes to kinases tree and one more thing what we do is like some",
    "start": "3590370",
    "end": "3596250"
  },
  {
    "text": "part of data and Krypton okay so we don't the XML I if you'd say just after",
    "start": "3596250",
    "end": "3603630"
  },
  {
    "text": "the session at the run I'll give you my business card I'd love to follow up with you specifically on it because it's something that we've already spent some",
    "start": "3603630",
    "end": "3608760"
  },
  {
    "text": "time thinking about it around the encrypt the decription or encryption or",
    "start": "3608760",
    "end": "3615210"
  },
  {
    "text": "unboxing or on batching right now your data it's got to be utf-8 encoded so if you've worked with a specific well we",
    "start": "3615210",
    "end": "3622830"
  },
  {
    "text": "recommend now to customers and we'll listen and we'll improve upon the experience when we launch is to",
    "start": "3622830",
    "end": "3629310"
  },
  {
    "text": "basically have some lambda function or your a simple KCl app just reading off",
    "start": "3629310",
    "end": "3634710"
  },
  {
    "text": "of your source stream and making simplifying the data basically and inserting into another case of stream",
    "start": "3634710",
    "end": "3641130"
  },
  {
    "text": "and then running Kinesis analytics on that there the we got the question about Avro earlier one of the gentlemen is s1",
    "start": "3641130",
    "end": "3648750"
  },
  {
    "text": "whenever we have a customer that has never dated that's basically what they're doing they're an avocation stream they're already using lambda with",
    "start": "3648750",
    "end": "3654810"
  },
  {
    "text": "it but they want to do more powerful analytics in real time so they're processing that they're reading from the",
    "start": "3654810",
    "end": "3661250"
  },
  {
    "text": "source stream changing the data to standard JSON and then writing to akinesia stream our team fully",
    "start": "3661250",
    "end": "3667710"
  },
  {
    "text": "recognizes that's of workaround and will improve it in the future you could do something similar for XML but if you",
    "start": "3667710",
    "end": "3673140"
  },
  {
    "text": "stay around I'd love to talk to you a little bit longer about it thank you all",
    "start": "3673140",
    "end": "3678960"
  },
  {
    "text": "very much for attending I really appreciate it",
    "start": "3678960",
    "end": "3682910"
  }
]