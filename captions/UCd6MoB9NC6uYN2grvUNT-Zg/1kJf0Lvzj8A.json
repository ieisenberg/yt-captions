[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "because what we saw is a lot of teams you know it took many months even years you know to start a project and",
    "start": "30",
    "end": "6450"
  },
  {
    "text": "eventually get these models into production so we're sort of in the typical 80s fashion trying to take away",
    "start": "6450",
    "end": "12780"
  },
  {
    "text": "the undifferentiated heavy lifting that's involved in managing these",
    "start": "12780",
    "end": "18330"
  },
  {
    "text": "projects so if we look at the different phases you know in a typical machine",
    "start": "18330",
    "end": "24240"
  },
  {
    "start": "20000",
    "end": "96000"
  },
  {
    "text": "learning project we typically have the build train and deploy phase it will kind of look it to the specific features",
    "start": "24240",
    "end": "30539"
  },
  {
    "text": "that Amazon Sage maker can provide that can benefit developers and data scientists send the build phase",
    "start": "30539",
    "end": "37920"
  },
  {
    "text": "we have pre-built notebooks so we give you managed notebook instances with the popular jupiter open source software",
    "start": "37920",
    "end": "45020"
  },
  {
    "text": "plus a bunch of pre-built notebooks they can solve you know many different common",
    "start": "45020",
    "end": "50399"
  },
  {
    "text": "problems that customers have comes with built-in algorithms so especially for",
    "start": "50399",
    "end": "55739"
  },
  {
    "text": "developers who may be just starting off with machine learning not confident to sort of build their own models we have a",
    "start": "55739",
    "end": "62010"
  },
  {
    "text": "lot of our bunch of sort of the most popular algorithms use the machine learning that makes it very quickly to",
    "start": "62010",
    "end": "67619"
  },
  {
    "text": "get started we also if you are you know using tensorflow or MX net you can",
    "start": "67619",
    "end": "74400"
  },
  {
    "text": "essentially just bring your code and it makes sa to make it makes it very easy to train and deploy those popular open",
    "start": "74400",
    "end": "81000"
  },
  {
    "text": "source frameworks or even if you know you have your own custom framework and this is the focus of this talk you know",
    "start": "81000",
    "end": "87570"
  },
  {
    "text": "things like pi torch or cafe - you can essentially by building dock your containers to build and train your",
    "start": "87570",
    "end": "94409"
  },
  {
    "text": "models and then deploy them some training we make it very easy to essentially have data science have",
    "start": "94409",
    "end": "102450"
  },
  {
    "start": "96000",
    "end": "287000"
  },
  {
    "text": "access to infrastructure needed to run your training jobs essentially one-click training is the objective so sage make",
    "start": "102450",
    "end": "109740"
  },
  {
    "text": "essentially manages configuring and deploying all of the infrastructure",
    "start": "109740",
    "end": "114810"
  },
  {
    "text": "needed to run your training jobs so this could be a single instance or even a cluster you know sort of depending on",
    "start": "114810",
    "end": "120750"
  },
  {
    "text": "the size of the the task at hand another feature is hyper parameter optimization",
    "start": "120750",
    "end": "125899"
  },
  {
    "text": "so often you know when you are training your models there's a lot of parameters to configure and it's hard to figure out",
    "start": "125899",
    "end": "132270"
  },
  {
    "text": "you know the right nation of those parameters so there's a service to essentially find the optimum",
    "start": "132270",
    "end": "138570"
  },
  {
    "text": "parameter increase the accuracy of the of your models so then once you've",
    "start": "138570",
    "end": "145350"
  },
  {
    "text": "trained your model we make it easy to then deploy into production so again through one-click so Amazon Sage maker",
    "start": "145350",
    "end": "152040"
  },
  {
    "text": "will manage the infrastructure and follows you know a lot of the ADA based best practices using things like auto",
    "start": "152040",
    "end": "158040"
  },
  {
    "text": "scaling to ensure that your endpoint that your customers are calling for your",
    "start": "158040",
    "end": "164100"
  },
  {
    "text": "machine model inference is highly available scalable and also you can do",
    "start": "164100",
    "end": "170430"
  },
  {
    "text": "things such as include a be testing so sort of following DevOps best practices in your project so if you look at Amazon",
    "start": "170430",
    "end": "180360"
  },
  {
    "text": "Sage maker it's actually not one service actually a combination of four independent components now these",
    "start": "180360",
    "end": "187770"
  },
  {
    "text": "components can be used together or separately you know so we provide data",
    "start": "187770",
    "end": "192870"
  },
  {
    "text": "scientists and developers a lot of flexibility in how they use the service so we have the notebook instances so",
    "start": "192870",
    "end": "199890"
  },
  {
    "text": "that's where the a lot of data Sciences will start so to sort of do the exploratory data analysis we have a lot",
    "start": "199890",
    "end": "207720"
  },
  {
    "text": "of the built-in algorithms to really help you get kick-started with machine learning the machine learning service",
    "start": "207720",
    "end": "214170"
  },
  {
    "text": "for training your models and then the hosting service where you can deploy and provide an API say to your customers or",
    "start": "214170",
    "end": "221250"
  },
  {
    "text": "sort of downstream services so if we go into each one of these the say Jamaican",
    "start": "221250",
    "end": "228330"
  },
  {
    "text": "notebook instances effectively just think of it like an ec2 instance there we provision on your behalf this ec2",
    "start": "228330",
    "end": "235110"
  },
  {
    "text": "instance can be have a GPU or not depending on you know the sort of the",
    "start": "235110",
    "end": "240150"
  },
  {
    "text": "compute power that you need this East to instance has an EBS volume attached to",
    "start": "240150",
    "end": "246150"
  },
  {
    "text": "it five gigabytes and science and as such we put the jupiter open source",
    "start": "246150",
    "end": "252120"
  },
  {
    "text": "software and stored on that and give you an endpoint that you can provide to data scientists so they can immediately start",
    "start": "252120",
    "end": "258090"
  },
  {
    "text": "doing their data exploratory analysis what's on the notebook instances there's",
    "start": "258090",
    "end": "264450"
  },
  {
    "text": "a bunch of pre-built example notebooks so it's essentially showing you how you can for example create a",
    "start": "264450",
    "end": "271150"
  },
  {
    "text": "recommendations engine you know many of these the use cases you can see on the right-hand side using the pre-built",
    "start": "271150",
    "end": "277120"
  },
  {
    "text": "algorithms using tensorflow MX net or even giving examples of how you can bring your own algorithm to the platform",
    "start": "277120",
    "end": "286199"
  },
  {
    "start": "287000",
    "end": "381000"
  },
  {
    "text": "so say to make it has a bunch of built-in algorithms and essentially there's kind of three ways you can bring",
    "start": "287160",
    "end": "292630"
  },
  {
    "text": "algorithms to the platform as I mentioned you know we have the built-in algorithms these are the Amazon provided",
    "start": "292630",
    "end": "298300"
  },
  {
    "text": "algorithms where we essentially manage them and Misfits effectively we've sort",
    "start": "298300",
    "end": "304990"
  },
  {
    "text": "of looked and asked their customers what are the most popular algorithms they're using there we've made them very",
    "start": "304990",
    "end": "311200"
  },
  {
    "text": "performant and able to work on very large data sets so things such as matrix factorization regression k-means",
    "start": "311200",
    "end": "317980"
  },
  {
    "text": "clustering and many more and recently we actually launched a new one for anomaly detection so and will continue you know",
    "start": "317980",
    "end": "326620"
  },
  {
    "text": "based on customer feedback increase the number of algorithms we provide so as I",
    "start": "326620",
    "end": "332950"
  },
  {
    "text": "mentioned earlier if you have MX net or tensorflow scripts that you are using you can bring them we manage the actual",
    "start": "332950",
    "end": "340560"
  },
  {
    "text": "containers for running those algorithms so essentially you can just bring your pre-built scripts and they'll work fine",
    "start": "340560",
    "end": "348330"
  },
  {
    "text": "you can bring your own algorithm services where you know there's a little bit more work to do",
    "start": "348330",
    "end": "353620"
  },
  {
    "text": "you have to essentially build a docker image for your training job and for",
    "start": "353620",
    "end": "358810"
  },
  {
    "text": "inference but it provides the flexibility right maximum flexibility and control there are many you know",
    "start": "358810",
    "end": "364120"
  },
  {
    "text": "experience they decide to serve there's also into working with SPARC so",
    "start": "364120",
    "end": "369370"
  },
  {
    "text": "essentially what we're seeing customers do is they doing a lot of the data pre-processing and their spark cluster",
    "start": "369370",
    "end": "374950"
  },
  {
    "text": "and then offloading this sort of the model training to Sage maker so if we",
    "start": "374950",
    "end": "382660"
  },
  {
    "start": "381000",
    "end": "571000"
  },
  {
    "text": "look at the machine learning training service the family role building block",
    "start": "382660",
    "end": "387910"
  },
  {
    "text": "essentially is docker so kind of loaded to it earlier if you're using the",
    "start": "387910",
    "end": "393010"
  },
  {
    "text": "built-in algorithms or if you're using MX net or tensorflow essentially the sage maker team sort of",
    "start": "393010",
    "end": "398410"
  },
  {
    "text": "manages and maintains the docker images for you so you don't need to do it yourself so",
    "start": "398410",
    "end": "406560"
  },
  {
    "text": "and then if you bring your own framework then you knit there something that you'll need to do can essentially upload",
    "start": "406560",
    "end": "412449"
  },
  {
    "text": "those docker images into Amazon ECL so the way the sort of the training flow",
    "start": "412449",
    "end": "418150"
  },
  {
    "text": "works it all starts with the training data so your training data needs to be",
    "start": "418150",
    "end": "423370"
  },
  {
    "text": "hosted into s3 but the way the training flow works is that Amazon sage maker the",
    "start": "423370",
    "end": "428740"
  },
  {
    "text": "training service will pull the training data into the training component",
    "start": "428740",
    "end": "434729"
  },
  {
    "text": "depending on you know what algorithm you're using if it's a built-in algorithm or your own algorithm that",
    "start": "434729",
    "end": "440380"
  },
  {
    "text": "will determine the docker image that it will pull into the service it'll spin up",
    "start": "440380",
    "end": "445810"
  },
  {
    "text": "the infrastructure needed to train your particular model and at the end of it",
    "start": "445810",
    "end": "451500"
  },
  {
    "text": "it'll generate a model artifact so this is typically you know surface and neural network all of the weights of the of the",
    "start": "451500",
    "end": "459610"
  },
  {
    "text": "neurons and save them in some sort of format typically a zip file so then",
    "start": "459610",
    "end": "467259"
  },
  {
    "text": "stage maker once it has the model artifacts that saved locally it'll then upload those model model artifacts up",
    "start": "467259",
    "end": "473770"
  },
  {
    "text": "into s3 so that's sort of in a nutshell how the training service works if for",
    "start": "473770",
    "end": "480849"
  },
  {
    "text": "hosting again the is building on the foundations of using docker now it could",
    "start": "480849",
    "end": "486729"
  },
  {
    "text": "be the same docker image or you can even have a separate inference algorithm you know because the mirror code the compute",
    "start": "486729",
    "end": "495310"
  },
  {
    "text": "requirements could be different the infrastructure could be different for running your inference algorithm so again we start with the model",
    "start": "495310",
    "end": "502310"
  },
  {
    "text": "artifacts so this could be the model artifact generated from the sage maker training service or even you could even",
    "start": "502310",
    "end": "509690"
  },
  {
    "text": "train on-premise or use some other you know just a standalone ec2 instance to train your model so as long as you have",
    "start": "509690",
    "end": "516110"
  },
  {
    "text": "a model artifact that zipped it's put up into s3 that's the sort of the only requirement so the model artifact is put",
    "start": "516110",
    "end": "524060"
  },
  {
    "text": "into the hosting service from sage maker sage maker will then spin up the hosting",
    "start": "524060",
    "end": "530390"
  },
  {
    "text": "infrastructure using the inference algorithm that you decide so again this could be the manage one if you're using",
    "start": "530390",
    "end": "537080"
  },
  {
    "text": "a built in algorithm or your own custom inference docker image to then create",
    "start": "537080",
    "end": "543470"
  },
  {
    "text": "essentially the endpoint so this is the endpoint as an API that you give to your end-users so one important part of the",
    "start": "543470",
    "end": "551180"
  },
  {
    "text": "inference algorithm is that may needs to support a restful interface so this is essentially where the predictions the",
    "start": "551180",
    "end": "559190"
  },
  {
    "text": "data for predictions will be submitted to and we'll go into a bit more detail in terms of so the requirements you need",
    "start": "559190",
    "end": "565130"
  },
  {
    "text": "to build into the inference docker image",
    "start": "565130",
    "end": "570490"
  },
  {
    "start": "571000",
    "end": "636000"
  },
  {
    "text": "so on the hosting service as a alluded to earlier the one of the cool features",
    "start": "571390",
    "end": "576440"
  },
  {
    "text": "allows you to sort a be testing so allows you to have multiple versions of your model for a single endpoint and",
    "start": "576440",
    "end": "582529"
  },
  {
    "text": "then gradually you know as you see the performance of new versions of your model you can shift traffic more and",
    "start": "582529",
    "end": "589279"
  },
  {
    "text": "more to those newer newer versions so just an example here so say we have you",
    "start": "589279",
    "end": "595160"
  },
  {
    "text": "know this is version 1 of our model deployed to an endpoint we can decide that obviously a hundred percent of the",
    "start": "595160",
    "end": "600950"
  },
  {
    "text": "traffic is going to it and then say we come up with a new idea and you sort of algorithm for our model so we can deploy",
    "start": "600950",
    "end": "608990"
  },
  {
    "text": "that and then we can give it a weight so we can say initially say twenty percent of our traffic is going to go to the new",
    "start": "608990",
    "end": "615260"
  },
  {
    "text": "model but the majority 80 percent is going to our original original model then you can assess that you know the",
    "start": "615260",
    "end": "622339"
  },
  {
    "text": "performance of both of the models say against some ground truth and then gradually decide you know how you want",
    "start": "622339",
    "end": "627670"
  },
  {
    "text": "to shift your traffic if the new models is performing as it's expected to all",
    "start": "627670",
    "end": "637300"
  },
  {
    "start": "636000",
    "end": "660000"
  },
  {
    "text": "right so that's kind of just the overview of of the sage maker service now we're kind of going to jump into how",
    "start": "637300",
    "end": "644110"
  },
  {
    "text": "you can actually bring custom models you know that's not part of the some of the built-in algorithms that we haven't sage",
    "start": "644110",
    "end": "650950"
  },
  {
    "text": "maker that you might have seen in some of the other sessions or the MX net or tensorflow and I'm going to do this",
    "start": "650950",
    "end": "658030"
  },
  {
    "text": "through an example so the example we're going to do is we're going to actually only use two of the components of sage",
    "start": "658030",
    "end": "664360"
  },
  {
    "start": "660000",
    "end": "704000"
  },
  {
    "text": "maker we're going to actually spin up a notebook instance so the notebook instance is going to actually be where",
    "start": "664360",
    "end": "670780"
  },
  {
    "text": "we're going to build and train our model because often you know many companies",
    "start": "670780",
    "end": "676120"
  },
  {
    "text": "don't have massive massive data sets right could be megabytes or gigabytes it's often you know notebook instances",
    "start": "676120",
    "end": "683230"
  },
  {
    "text": "and a phone allows you to a lot of iterative development and experiment with training and models",
    "start": "683230",
    "end": "689400"
  },
  {
    "text": "so once we've built and trained tomorrow and we're kind of satisfied with this performance we're going to upload that",
    "start": "689400",
    "end": "695740"
  },
  {
    "text": "into s3 and then we can use the email hosting service to then deploy this model into production so the example",
    "start": "695740",
    "end": "705820"
  },
  {
    "start": "704000",
    "end": "789000"
  },
  {
    "text": "that library I'm going to go through is fast AI who's familiar with fast AI okay",
    "start": "705820",
    "end": "712870"
  },
  {
    "text": "it's a few hands not majority or not so this is a fairly new library it's based",
    "start": "712870",
    "end": "718510"
  },
  {
    "text": "off a very popular online course if you are interested in getting into deep learning and have some development",
    "start": "718510",
    "end": "724240"
  },
  {
    "text": "skills I highly highly recommend checking out the first day I online course it can really get you up to speed",
    "start": "724240",
    "end": "730690"
  },
  {
    "text": "and just there a matter of weeks you know you can run some of the latest state-of-the-art algorithms and around",
    "start": "730690",
    "end": "737020"
  },
  {
    "text": "deep learning so this is a library built on PI torch ever heard of PI torch this is an open source library developed by",
    "start": "737020",
    "end": "743890"
  },
  {
    "text": "Facebook and it's really focusing on so pi torch is kind of designed for",
    "start": "743890",
    "end": "749260"
  },
  {
    "text": "researchers and fast they are as more designed for practical use on real world usage and they have that motto making",
    "start": "749260",
    "end": "756170"
  },
  {
    "text": "neural nets uncool again so what they mean by that it's you know they want it",
    "start": "756170",
    "end": "761450"
  },
  {
    "text": "to be used for they're not so fanciest and that's not trying to solve go you",
    "start": "761450",
    "end": "768410"
  },
  {
    "text": "know beating go players or automated driving you know real practical use cases for machine learning and deep",
    "start": "768410",
    "end": "775010"
  },
  {
    "text": "learning which often for a lot of companies is actually the most value if you think companies have a lot of data",
    "start": "775010",
    "end": "780740"
  },
  {
    "text": "sitting in data warehouses what should we call structured data so it's really ideal for those sort of data types and",
    "start": "780740",
    "end": "787190"
  },
  {
    "text": "practical applications so first there I worked really well with a DMS this is",
    "start": "787190",
    "end": "793459"
  },
  {
    "start": "789000",
    "end": "834000"
  },
  {
    "text": "just a screenshot of a recent sort of benchmarking study that they did where",
    "start": "793459",
    "end": "798500"
  },
  {
    "text": "it was essentially a bake-off and assessing different teams seeing who can",
    "start": "798500",
    "end": "805160"
  },
  {
    "text": "train essentially the models the fastest and also at the lowest cost so team took",
    "start": "805160",
    "end": "811339"
  },
  {
    "text": "the fast AI library using AWS in a sort of standard infrastructure and managed to",
    "start": "811339",
    "end": "818120"
  },
  {
    "text": "you know beat a lot of other teams that had you know custom hardware you know really optimized you know deep research",
    "start": "818120",
    "end": "825649"
  },
  {
    "text": "teams so it's just kind of showing that sort of the power of this library you know combined with AWS is a really good",
    "start": "825649",
    "end": "831529"
  },
  {
    "text": "nice solution so the example that we're going to go through sure if you're",
    "start": "831529",
    "end": "837529"
  },
  {
    "text": "familiar with Kegel this is a fairly sort of rudimentary set example of computer vision so image classification",
    "start": "837529",
    "end": "844540"
  },
  {
    "text": "so the example we're going to go through is that by giving an image it's going to detect if it's a dog or a cat so not the",
    "start": "844540",
    "end": "852440"
  },
  {
    "text": "most you know fancy but you know just to kind of prove the example of how these things can work together so this is in",
    "start": "852440",
    "end": "860089"
  },
  {
    "text": "sort of an open data set that that we're going to be using in the example so how",
    "start": "860089",
    "end": "867860"
  },
  {
    "start": "866000",
    "end": "900000"
  },
  {
    "text": "the examples going to work is we're going to spin up a sage make a notebook we're going to be using the GPU based",
    "start": "867860",
    "end": "874310"
  },
  {
    "text": "notebook because we're actually going to be training our deep learning model so the model is a convolutional network",
    "start": "874310",
    "end": "879769"
  },
  {
    "text": "which is essentially the state of the art for computer vision image classification",
    "start": "879769",
    "end": "885380"
  },
  {
    "text": "so we're going to run through the code there we're going to upload so once we've trained our model on the notebook",
    "start": "885380",
    "end": "891440"
  },
  {
    "text": "instance the modal artifacts are going to be uploaded zipped and uploaded up into s3 so that's kind of our build and",
    "start": "891440",
    "end": "899000"
  },
  {
    "text": "train phase so now because we are using a our own algorithm you know none of the",
    "start": "899000",
    "end": "905930"
  },
  {
    "start": "900000",
    "end": "938000"
  },
  {
    "text": "sort of off-the-shelf or built-in algorithms as part of the stage maker service we're going to have to actually",
    "start": "905930",
    "end": "911270"
  },
  {
    "text": "build our own inference docker image and we're actually going to use cloud 9 so",
    "start": "911270",
    "end": "916700"
  },
  {
    "text": "cloud nine is the hosted IDE from Amazon that we launched in dove last year and",
    "start": "916700",
    "end": "922760"
  },
  {
    "text": "the nice thing about it is it surprised a nice coding environment and includes docker so we can basically build our images on cloud nine and then push them",
    "start": "922760",
    "end": "929780"
  },
  {
    "text": "up into Amazon private docker repository so once we've built and pushed our image",
    "start": "929780",
    "end": "938630"
  },
  {
    "start": "938000",
    "end": "986000"
  },
  {
    "text": "then we're ready to host the model on the sage maker hosting service so hey",
    "start": "938630",
    "end": "945080"
  },
  {
    "text": "host sage maker hosting service centrally just needs two things it just needs those model artifacts that we built earlier and pushed to s 3 and then",
    "start": "945080",
    "end": "953180"
  },
  {
    "text": "the inference image and it's ready to deploy the other thing we're doing is that I mentioned earlier on the notebook",
    "start": "953180",
    "end": "959420"
  },
  {
    "text": "instance we're using a GPU based but often for even for deep learning you don't actually need a GPU when you're",
    "start": "959420",
    "end": "966170"
  },
  {
    "text": "actually deploying your code to production for inference often the you",
    "start": "966170",
    "end": "971330"
  },
  {
    "text": "know computer optimized Intel C 5 or C 4 instances work really well enough to",
    "start": "971330",
    "end": "977390"
  },
  {
    "text": "have a low running cost and the the GPU based instances so that's them all",
    "start": "977390",
    "end": "985520"
  },
  {
    "text": "together so just a couple of words on you know building your inference docker",
    "start": "985520",
    "end": "991670"
  },
  {
    "start": "986000",
    "end": "1062000"
  },
  {
    "text": "images so there's a few things that you need to be aware of you know if you're building your own inference image first",
    "start": "991670",
    "end": "997850"
  },
  {
    "text": "of all you have to understand how Amazon sage maker calls the docker image when it's actually running them in production",
    "start": "997850",
    "end": "1004350"
  },
  {
    "text": "so it's basically running this docker run command you give it the the name so",
    "start": "1004350",
    "end": "1011050"
  },
  {
    "text": "the reference of your docker image in the private repository and Amazon ECR",
    "start": "1011050",
    "end": "1016390"
  },
  {
    "text": "and Amazon say Jamaica will also pass in the parameter surf so you have to sort",
    "start": "1016390",
    "end": "1021700"
  },
  {
    "text": "of be aware of that and sort of configure your your entry point to your",
    "start": "1021700",
    "end": "1026709"
  },
  {
    "text": "docker image to handle that command the other part is understanding how you're",
    "start": "1026710",
    "end": "1031870"
  },
  {
    "text": "going to access the model artifacts so these are the safe parameters they of generic neural network so the good thing",
    "start": "1031870",
    "end": "1038860"
  },
  {
    "text": "about stage makers it handles the copying of your artifacts from s3 into your local running docker image so where",
    "start": "1038860",
    "end": "1047170"
  },
  {
    "text": "you have to look for the model artifacts is not in s3 but you just need to look in a mounted volume in your dock your",
    "start": "1047170",
    "end": "1054280"
  },
  {
    "text": "container which is opt ml model that's where you basically pick up the zip model so you can then load your model",
    "start": "1054280",
    "end": "1060100"
  },
  {
    "text": "into memory also how the inference",
    "start": "1060100",
    "end": "1065830"
  },
  {
    "start": "1062000",
    "end": "1172000"
  },
  {
    "text": "images is accessed from the endpoint it's also important to understand this",
    "start": "1065830",
    "end": "1071470"
  },
  {
    "text": "so you have to have essentially a rest compliant web server running in your",
    "start": "1071470",
    "end": "1077350"
  },
  {
    "text": "inference image and there's two API is that you need to implant in that web server so the first is a slash and",
    "start": "1077350",
    "end": "1084940"
  },
  {
    "text": "vacations path so this is where essentially the data that you want to do",
    "start": "1084940",
    "end": "1090190"
  },
  {
    "text": "a prediction on gets posted to so in our example we are you know doing an image classification so the actual data will",
    "start": "1090190",
    "end": "1096670"
  },
  {
    "text": "be the raw image so the JPEG image itself will get posted to to that",
    "start": "1096670",
    "end": "1102100"
  },
  {
    "text": "endpoint in a post HTTP POST request so if it's you know if you're doing things",
    "start": "1102100",
    "end": "1107350"
  },
  {
    "text": "like structured data it could be a you know a CSV file or a JSON file and what",
    "start": "1107350",
    "end": "1113200"
  },
  {
    "text": "we're going to get back is there JSON file you know giving the class of what it's predicting the image to be in a",
    "start": "1113200",
    "end": "1119290"
  },
  {
    "text": "confidence score so the other the other path we have to implement is the ping",
    "start": "1119290",
    "end": "1125770"
  },
  {
    "text": "API so say to maker will frequently ping your endpoint to check the health of it make sure it's healthy so you need to",
    "start": "1125770",
    "end": "1132240"
  },
  {
    "text": "implement a slash ping reference path with the gates and then return 200",
    "start": "1132240",
    "end": "1138250"
  },
  {
    "text": "success if your inference server as is healthy",
    "start": "1138250",
    "end": "1143540"
  },
  {
    "text": "so now getting into how you actually bundle the code together you know good",
    "start": "1143540",
    "end": "1149190"
  },
  {
    "text": "practice because a lot of the machine learning libraries are implemented in Python we generally recommend to bundle",
    "start": "1149190",
    "end": "1156420"
  },
  {
    "text": "in nginx as a reverse proxy with goona corn and in flasks a flask is a very",
    "start": "1156420",
    "end": "1162930"
  },
  {
    "text": "popular restful framework for Python applications so this is a good",
    "start": "1162930",
    "end": "1168540"
  },
  {
    "text": "combination to use already so now we're going to jump into the demo so let me",
    "start": "1168540",
    "end": "1177390"
  },
  {
    "start": "1172000",
    "end": "1256000"
  },
  {
    "text": "just switch here",
    "start": "1177390",
    "end": "1180140"
  },
  {
    "text": "you can see there right so this is just showing the domain sage make a console",
    "start": "1188000",
    "end": "1196650"
  },
  {
    "text": "web console so showing the four separate components so showing the instance the",
    "start": "1196650",
    "end": "1203190"
  },
  {
    "text": "jobs models and endpoints so what we're going to do is we're going to essentially fire up our notebook",
    "start": "1203190",
    "end": "1210210"
  },
  {
    "text": "instance now when you fire up your notebook instances it has a bunch of",
    "start": "1210210",
    "end": "1216690"
  },
  {
    "text": "what we call condor environments or Condor environments as essentially defines specific version of python as",
    "start": "1216690",
    "end": "1223740"
  },
  {
    "text": "well as all of the libraries and dependencies for the particular libraries so you can have multiple Condor environments running on the same",
    "start": "1223740",
    "end": "1230460"
  },
  {
    "text": "instance with different versions of Python and different versions of the dependent libraries and packages so one",
    "start": "1230460",
    "end": "1238560"
  },
  {
    "text": "of the things to note about the sowait since we're using fast AI thankfully and",
    "start": "1238560",
    "end": "1246990"
  },
  {
    "text": "we have a look here and the here so this",
    "start": "1246990",
    "end": "1256650"
  },
  {
    "start": "1256000",
    "end": "1274000"
  },
  {
    "text": "is the case hopefully you can see that okay this is the fast a I library on",
    "start": "1256650",
    "end": "1263790"
  },
  {
    "text": "github so fast there's an open source library and the good thing about it is that it actually has a condor",
    "start": "1263790",
    "end": "1270450"
  },
  {
    "text": "environment defined here so this is the environment gamma file so this defines essentially all of the Python packages",
    "start": "1270450",
    "end": "1277470"
  },
  {
    "start": "1274000",
    "end": "1325000"
  },
  {
    "text": "and other dependencies that the faster a library needs to essentially brown sort",
    "start": "1277470",
    "end": "1283770"
  },
  {
    "text": "of because it's built on PI torch you can see up the top there it has the dependency on PI torch and other",
    "start": "1283770",
    "end": "1289950"
  },
  {
    "text": "packages so essentially what we need to do first of all is set up the Condor",
    "start": "1289950",
    "end": "1296790"
  },
  {
    "text": "environment for fast AI on our notebook instance now one of the things I'm say",
    "start": "1296790",
    "end": "1302010"
  },
  {
    "text": "to make a notebook instances is that when you stop and start your notebook instance those custom libraries are not",
    "start": "1302010",
    "end": "1308670"
  },
  {
    "text": "saved so you know we got the feedback from customers that this was not",
    "start": "1308670",
    "end": "1314790"
  },
  {
    "text": "pleasant you know experience they would have to manually run commands on the notebook instance so we introduced a new",
    "start": "1314790",
    "end": "1320790"
  },
  {
    "text": "feature few weeks ago called lifestyle configurations so we just go back to the",
    "start": "1320790",
    "end": "1327060"
  },
  {
    "text": "console here and go back into our notebook you can see this new if you",
    "start": "1327060",
    "end": "1332640"
  },
  {
    "text": "haven't you know been in the console for a while you will see this as a new new option your lifestyle configurations so",
    "start": "1332640",
    "end": "1339690"
  },
  {
    "text": "what a lifestyle contraction configuration is it allows you to define some scripts that will be run both when",
    "start": "1339690",
    "end": "1346410"
  },
  {
    "text": "your notebook is created the first time it's created and then every time your notebook has started so if you want to",
    "start": "1346410",
    "end": "1352050"
  },
  {
    "text": "install custom libraries you can essentially just call Conda creates",
    "start": "1352050",
    "end": "1357300"
  },
  {
    "text": "environment in that lifestyle configuration script and it will automatically set up and have your",
    "start": "1357300",
    "end": "1363990"
  },
  {
    "text": "notebook instance ready to go so we have one pre-configured here so here we have",
    "start": "1363990",
    "end": "1375810"
  },
  {
    "text": "two scripts so we have one script when the notebook instance is created so this",
    "start": "1375810",
    "end": "1382680"
  },
  {
    "text": "is doing things such as we're downloading the training data onto the EBS volume we're also installing the",
    "start": "1382680",
    "end": "1389190"
  },
  {
    "text": "fast AI library and was also installing the demo code for this yeah code for",
    "start": "1389190",
    "end": "1395400"
  },
  {
    "text": "this application so that's all it started so the EBS volume itself is saved but not the",
    "start": "1395400",
    "end": "1402030"
  },
  {
    "text": "custom libraries and the enecon or libraries so that's what we need to do is we need to and the start notebook",
    "start": "1402030",
    "end": "1408600"
  },
  {
    "start": "1408000",
    "end": "1451000"
  },
  {
    "text": "script this is where we need to set up our environment because especially because we're using a GPU instance for",
    "start": "1408600",
    "end": "1414690"
  },
  {
    "text": "our notebook we don't want to have this running and necessarily so we want to stop it for example you know when the",
    "start": "1414690",
    "end": "1420570"
  },
  {
    "text": "data scientist goes home so in this particular script when the data scientist comes back in the next day to",
    "start": "1420570",
    "end": "1425850"
  },
  {
    "text": "continue their work it'll install all those libraries for them so then here",
    "start": "1425850",
    "end": "1432600"
  },
  {
    "text": "we've just got the the Condor in create",
    "start": "1432600",
    "end": "1437820"
  },
  {
    "text": "update commands which one store essentially all of those packages for",
    "start": "1437820",
    "end": "1443340"
  },
  {
    "text": "the first day our library",
    "start": "1443340",
    "end": "1446779"
  },
  {
    "text": "cool so we've gone into lifestyle configurations so you know creating a",
    "start": "1450860",
    "end": "1456559"
  },
  {
    "start": "1451000",
    "end": "1459000"
  },
  {
    "text": "notebook is fairly simple we just go into the console here we'll give it a",
    "start": "1456559",
    "end": "1462470"
  },
  {
    "start": "1459000",
    "end": "1519000"
  },
  {
    "text": "name here is where we choose the",
    "start": "1462470",
    "end": "1468380"
  },
  {
    "text": "instance type so because we're training our model on the notebook instance were",
    "start": "1468380",
    "end": "1473870"
  },
  {
    "text": "actually you want to choose a GPU based so this is a fairly new option as well before they're only you know a couple of",
    "start": "1473870",
    "end": "1480200"
  },
  {
    "text": "different instance options now we're supporting most of the ec2 instance types here so we can choose the p3 which",
    "start": "1480200",
    "end": "1488210"
  },
  {
    "text": "has the very latest Nvidia volts of V 100 GPU there so it really accelerates",
    "start": "1488210",
    "end": "1495020"
  },
  {
    "text": "our training of their models you know we can choose the V PC and here is where we",
    "start": "1495020",
    "end": "1500299"
  },
  {
    "text": "define the lifestyle configuration so this is the scripts that I just showed you earlier so will install you know",
    "start": "1500299",
    "end": "1508040"
  },
  {
    "text": "download the training data and then store the fast they are libraries so this is going to take you no more than",
    "start": "1508040",
    "end": "1514190"
  },
  {
    "text": "10 minutes to do because there's quite a few libraries in here so I'm not going to actually do this and like every good cooking show I have one pre-prepared so",
    "start": "1514190",
    "end": "1521570"
  },
  {
    "start": "1519000",
    "end": "1536000"
  },
  {
    "text": "this is the summit training notebook here you can see it's in service so once that's in service it's really easy you",
    "start": "1521570",
    "end": "1528290"
  },
  {
    "text": "just need to basically cook here open and this is going to open up the web",
    "start": "1528290",
    "end": "1533750"
  },
  {
    "text": "console for the Jupiter application so here's my application so this is",
    "start": "1533750",
    "end": "1538970"
  },
  {
    "text": "something that was set up through those scripts so we can see here we have a",
    "start": "1538970",
    "end": "1544130"
  },
  {
    "text": "data directory so our lifestyle configuration on create script has set",
    "start": "1544130",
    "end": "1549559"
  },
  {
    "text": "up our data here so we can have a look here here's our dogs and cats data",
    "start": "1549559",
    "end": "1555490"
  },
  {
    "text": "it's gonna be quite slow because there's quite a few files in there",
    "start": "1561560",
    "end": "1567159"
  },
  {
    "text": "I just create a new one because it's probably blocked up a bit",
    "start": "1572440",
    "end": "1578400"
  },
  {
    "text": "joys of day my son",
    "start": "1589630",
    "end": "1592980"
  },
  {
    "text": "and I might switch to the other I'm so sick",
    "start": "1613920",
    "end": "1621020"
  },
  {
    "text": "so let's try this one okay",
    "start": "1624340",
    "end": "1630330"
  },
  {
    "start": "1634000",
    "end": "1670000"
  },
  {
    "text": "okay so we've got our data directory we also have this of a sample notebooks so",
    "start": "1634620",
    "end": "1640030"
  },
  {
    "text": "you know if you want to get familiar with the built-in algorithms nothing had to run tensorflow or MX net there's a",
    "start": "1640030",
    "end": "1647530"
  },
  {
    "text": "lot of examples here that you can go through",
    "start": "1647530",
    "end": "1651750"
  },
  {
    "text": "so here we have the specific directory so this is a source code for this particular demo so as I mentioned we've",
    "start": "1659500",
    "end": "1665860"
  },
  {
    "text": "got a example application which is a looking image classification example so",
    "start": "1665860",
    "end": "1672429"
  },
  {
    "start": "1670000",
    "end": "1736000"
  },
  {
    "text": "given a picture of a dog or a cat it's figuring out which one it is so this is actually based off the lesson one of the",
    "start": "1672429",
    "end": "1680140"
  },
  {
    "text": "first day I course and here is a the example notebook interview and here we",
    "start": "1680140",
    "end": "1687039"
  },
  {
    "text": "essentially are not doing anything sage maker specific is basically I've copied and pasted most of the same source code",
    "start": "1687039",
    "end": "1693789"
  },
  {
    "text": "from the lesson one from the first day I course except for defining a few things",
    "start": "1693789",
    "end": "1701140"
  },
  {
    "text": "here such as where we're going to save the model artifacts but essentially all",
    "start": "1701140",
    "end": "1706179"
  },
  {
    "text": "of the code here is exactly the same as you're doing any other building any",
    "start": "1706179",
    "end": "1711190"
  },
  {
    "text": "other fast AI algorithm with Jupiter notebooks so nothing stage maker",
    "start": "1711190",
    "end": "1716590"
  },
  {
    "text": "specific so here we've just got some code it does image augmentation we're",
    "start": "1716590",
    "end": "1724840"
  },
  {
    "text": "finding the the right learning rate we're doing some learning rate annealing and freezing some of the layers so these",
    "start": "1724840",
    "end": "1730150"
  },
  {
    "text": "are all the kind of standard steps if you go through the fast di course that you would do so the only thing extra",
    "start": "1730150",
    "end": "1736990"
  },
  {
    "start": "1736000",
    "end": "1756000"
  },
  {
    "text": "that we're doing Sushil is here so basically from this point on is sort of something say to make a specific so here",
    "start": "1736990",
    "end": "1743860"
  },
  {
    "text": "we need to save the model so this will save the the weights of our convolutional Network local to the local",
    "start": "1743860",
    "end": "1750880"
  },
  {
    "text": "disk and then what we're going to do is we're going to bundle that save model up",
    "start": "1750880",
    "end": "1757090"
  },
  {
    "start": "1756000",
    "end": "1805000"
  },
  {
    "text": "up to s3 so this is where we can then you know use those model artifacts from",
    "start": "1757090",
    "end": "1763919"
  },
  {
    "text": "machine learning model hosting cool so",
    "start": "1763919",
    "end": "1769390"
  },
  {
    "text": "that's the sort of the build really that's all it is for the building train phase so now we're going to talk about",
    "start": "1769390",
    "end": "1777309"
  },
  {
    "text": "how we can build the docker image for our machine learning model inference",
    "start": "1777309",
    "end": "1783080"
  },
  {
    "text": "and then actually deploy that model as well as the image to the sage maker",
    "start": "1783080",
    "end": "1789740"
  },
  {
    "text": "hosting service",
    "start": "1789740",
    "end": "1792940"
  },
  {
    "start": "1805000",
    "end": "1870000"
  },
  {
    "text": "so as I mentioned earlier the way so typically once you've you know saved",
    "start": "1805160",
    "end": "1810290"
  },
  {
    "text": "your model locally on sage maker you probably want to shut down your notebook",
    "start": "1810290",
    "end": "1816530"
  },
  {
    "text": "instance because especially if you're running for example at p3 or a p2 you know that's going to be in a you know",
    "start": "1816530",
    "end": "1822200"
  },
  {
    "text": "fairly the costs are going to add up so the rest of the exercise recs you're gonna work with cloud 9 which is the",
    "start": "1822200",
    "end": "1830590"
  },
  {
    "text": "cloud hosted IDE so we're going to actually build the docker image used for",
    "start": "1830590",
    "end": "1837800"
  },
  {
    "text": "an inference code here and we're actually going to use the same instance to actually call our endpoint so don't",
    "start": "1837800",
    "end": "1846170"
  },
  {
    "text": "know if you can see it's a little bit murky there how can you guys see the anything there is a pretty you can I'll",
    "start": "1846170",
    "end": "1854060"
  },
  {
    "text": "try and blow it up a little bit let's see if",
    "start": "1854060",
    "end": "1857890"
  },
  {
    "text": "it's a bit hard to see here is that better yeah okay cool so I just wanted",
    "start": "1861130",
    "end": "1871730"
  },
  {
    "text": "to take you camera through the source code initially of our for our inference image so here we have a folder called",
    "start": "1871730",
    "end": "1878720"
  },
  {
    "text": "continent so this is where we're going to be essentially loading the saved",
    "start": "1878720",
    "end": "1884960"
  },
  {
    "text": "convolutional Network and then calling it for predictions so we've got a number",
    "start": "1884960",
    "end": "1891650"
  },
  {
    "text": "of different files and in most cases you can keep them so for example we have an engine X configuration file we have a",
    "start": "1891650",
    "end": "1898180"
  },
  {
    "text": "WSGI file that's word goona corn we have a serve file so the only real one that",
    "start": "1898180",
    "end": "1904160"
  },
  {
    "text": "you would need to customize is this predictor dot py file so this",
    "start": "1904160",
    "end": "1911870"
  },
  {
    "text": "essentially a this one here",
    "start": "1911870",
    "end": "1920110"
  },
  {
    "text": "so this is a just a pretty simple flask application file so what we're doing is",
    "start": "1920420",
    "end": "1927080"
  },
  {
    "text": "we're loading up you know some of the fast they are libraries we're referencing opt ml and there will be",
    "start": "1927080",
    "end": "1933230"
  },
  {
    "text": "model so that's where we're going to read our model artifacts from our docker container we have a singleton object so",
    "start": "1933230",
    "end": "1944060"
  },
  {
    "text": "this is actually the singleton object where we're going to actually load the model into memory from the model",
    "start": "1944060",
    "end": "1949670"
  },
  {
    "text": "artifact I mean here we have a predict API call on that singleton object so",
    "start": "1949670",
    "end": "1956780"
  },
  {
    "text": "here's where we're going to actually call the model to a basically forward pass through the convolutional net work",
    "start": "1956780",
    "end": "1963080"
  },
  {
    "text": "to get the predicted class and here's",
    "start": "1963080",
    "end": "1968810"
  },
  {
    "text": "where we need to implement those two restful api calls",
    "start": "1968810",
    "end": "1974810"
  },
  {
    "text": "so you remember / ping we need to implement so that's going to be what",
    "start": "1974810",
    "end": "1980330"
  },
  {
    "text": "sage maker is going to call to verify that our that our inference image is working fine so here we're just",
    "start": "1980330",
    "end": "1986570"
  },
  {
    "text": "essentially loading up the model and if it loads successfully without any errors we're just going to return a success",
    "start": "1986570",
    "end": "1992060"
  },
  {
    "text": "code and then we have the invitations so this is when it going to be where the",
    "start": "1992060",
    "end": "1997970"
  },
  {
    "text": "clients are going to cause that they're going to upload our image to the service",
    "start": "1997970",
    "end": "2003670"
  },
  {
    "text": "via a post request so what this application is going to do is it's going to take that raw sort of JPEG image data",
    "start": "2003670",
    "end": "2011670"
  },
  {
    "text": "it's going to save it locally as a JPEG file and then it's going to call the predict method on that singleton class",
    "start": "2011670",
    "end": "2020010"
  },
  {
    "text": "and then it's going to return two values essentially in a JPEG JSON object sorry",
    "start": "2020010",
    "end": "2025960"
  },
  {
    "text": "it's going to return the predicted class so whether it's a dog or a cat and also",
    "start": "2025960",
    "end": "2031270"
  },
  {
    "text": "a confidence score in an era turned code so that's kind of",
    "start": "2031270",
    "end": "2036730"
  },
  {
    "text": "really all others to the actual source code for for your inference image so now",
    "start": "2036730",
    "end": "2047350"
  },
  {
    "text": "I'm gonna quickly show you as well the the docker file so this is the docker file for our inference image so what we",
    "start": "2047350",
    "end": "2055540"
  },
  {
    "text": "have here is just a standard Ubuntu docker image we're going to install mini",
    "start": "2055540",
    "end": "2062800"
  },
  {
    "text": "Khanda so this to set up a condor environment we're going to clone the",
    "start": "2062800",
    "end": "2067960"
  },
  {
    "text": "fast a I library here and here we're going to create environment for face day",
    "start": "2067960",
    "end": "2074740"
  },
  {
    "text": "I now this one changed here because if you may if you remember what I mentioned earlier is we're actually going to run",
    "start": "2074740",
    "end": "2080879"
  },
  {
    "text": "the inference image not within GPU but actually on CPU architecture using the",
    "start": "2080880",
    "end": "2086110"
  },
  {
    "text": "computer optimized instances so that means we don't need a lot of the libraries like the CUDA libraries and",
    "start": "2086110",
    "end": "2092530"
  },
  {
    "text": "GPU specific libraries so that means so thankfully in fast AI this a separate",
    "start": "2092530",
    "end": "2097990"
  },
  {
    "text": "environment file which is just specific for CPU that we can use so we install",
    "start": "2097990",
    "end": "2107920"
  },
  {
    "text": "some yeah the flask gonna corn packages from Python and then we copy basically",
    "start": "2107920",
    "end": "2115270"
  },
  {
    "text": "all that source code quote covered over into our inference image so that's basically us ready to go so the cool",
    "start": "2115270",
    "end": "2123190"
  },
  {
    "text": "thing is with cloud nine is we can run all our docker build commands so I actually have a",
    "start": "2123190",
    "end": "2128850"
  },
  {
    "text": "actually have a script here so the little script all it does is runs the docker build and there's going to",
    "start": "2132650",
    "end": "2137970"
  },
  {
    "text": "actually push their docker image that's built locally on a cloud 9 instance up",
    "start": "2137970",
    "end": "2143880"
  },
  {
    "text": "into my private docker repository in ECR and obviously done it faster cause",
    "start": "2143880",
    "end": "2149700"
  },
  {
    "text": "otherwise you know it's multiple gigabytes would take minutes to do so it's already cached locally and it's",
    "start": "2149700",
    "end": "2154740"
  },
  {
    "text": "already being pushed up but just to kind of show you the sort of the flow so here is the now the docker image that we're",
    "start": "2154740",
    "end": "2163410"
  },
  {
    "text": "going to use for inference so what we can do is we can copy this and now we're",
    "start": "2163410",
    "end": "2171090"
  },
  {
    "text": "ready to essentially create a model and deploy it using the stage maker hosting service",
    "start": "2171090",
    "end": "2177589"
  },
  {
    "start": "2180000",
    "end": "2196000"
  },
  {
    "text": "so what we can do is we can come back to our web console now a lot of the following commands you can all do",
    "start": "2180010",
    "end": "2187099"
  },
  {
    "text": "through for example the SDK rather means but just for demonstration purposes I'm",
    "start": "2187099",
    "end": "2192590"
  },
  {
    "text": "going to show it in the web console so the first thing we need to do so now",
    "start": "2192590",
    "end": "2197630"
  },
  {
    "start": "2196000",
    "end": "2219000"
  },
  {
    "text": "that we have you know built our model artifacts and we've built our docker image for inference now we can create a",
    "start": "2197630",
    "end": "2204080"
  },
  {
    "text": "model in the stage maker console so that's simple is coming in here creating",
    "start": "2204080",
    "end": "2209900"
  },
  {
    "text": "a model looking at a name so here we",
    "start": "2209900",
    "end": "2219920"
  },
  {
    "start": "2219000",
    "end": "2267000"
  },
  {
    "text": "specify a role so this will be the role given to the infrastructure that's going to use for hosting so if you need to",
    "start": "2219920",
    "end": "2225410"
  },
  {
    "text": "access s3 so the minimum sort of accessing those model artifacts in s3 and also being able to pull down your",
    "start": "2225410",
    "end": "2232790"
  },
  {
    "text": "image your inference image from ECR you",
    "start": "2232790",
    "end": "2237830"
  },
  {
    "text": "can run your hosting infrastructure in a particular VPC and here's the key ones",
    "start": "2237830",
    "end": "2244430"
  },
  {
    "text": "that we're going to fill in so we're going to put in whoops we're going to",
    "start": "2244430",
    "end": "2250460"
  },
  {
    "text": "put in the name of our docker image and upload its ECR and we're also going to",
    "start": "2250460",
    "end": "2257810"
  },
  {
    "text": "put in the model artifacts here to create our model so here we can go back",
    "start": "2257810",
    "end": "2263060"
  },
  {
    "text": "to yeah this is their notebook so we can",
    "start": "2263060",
    "end": "2268970"
  },
  {
    "start": "2267000",
    "end": "2282000"
  },
  {
    "text": "copy essentially the model artifact that was uploaded when we've finished our",
    "start": "2268970",
    "end": "2274849"
  },
  {
    "text": "training on the notebook instance we can",
    "start": "2274849",
    "end": "2282050"
  },
  {
    "start": "2282000",
    "end": "2298000"
  },
  {
    "text": "just copy it in here we can leave some of these others blank and we can just go",
    "start": "2282050",
    "end": "2288380"
  },
  {
    "text": "create model so here we have our model created and now we're ready to deploy",
    "start": "2288380",
    "end": "2295810"
  },
  {
    "text": "the model to the hosting to the hosting",
    "start": "2295810",
    "end": "2301490"
  },
  {
    "start": "2298000",
    "end": "2320000"
  },
  {
    "text": "service so we'll give it a name",
    "start": "2301490",
    "end": "2306430"
  },
  {
    "text": "can create a new endpoint configuration so this is where we're actually going to tell the endpoint what is the model",
    "start": "2312080",
    "end": "2319640"
  },
  {
    "text": "we're going to use and also define the infrastructure that we're going to use so here we can define that we're going to use the computer optimized instances",
    "start": "2319640",
    "end": "2326840"
  },
  {
    "start": "2320000",
    "end": "2348000"
  },
  {
    "text": "that we can define how many we want so that's kind of dependent on sort of the throughput of the number of requests",
    "start": "2326840",
    "end": "2333020"
  },
  {
    "text": "that are going to be called to your particular endpoint so we can go London",
    "start": "2333020",
    "end": "2339640"
  },
  {
    "text": "summit",
    "start": "2339640",
    "end": "2342640"
  },
  {
    "text": "[Music] and here we is where we add the model so",
    "start": "2344970",
    "end": "2350570"
  },
  {
    "start": "2348000",
    "end": "2368000"
  },
  {
    "text": "we've just created this model and here by default it's giving us the m4x lodge",
    "start": "2350570",
    "end": "2356060"
  },
  {
    "text": "and one so you know we can accept these defaults and create the same point",
    "start": "2356060",
    "end": "2362960"
  },
  {
    "text": "config and then here if we click here it'll basically go away and create the end point now this it'll take a few",
    "start": "2362960",
    "end": "2368930"
  },
  {
    "start": "2368000",
    "end": "2412000"
  },
  {
    "text": "minutes because it actually needs to say Jamaica is actually now going and provisioning those underlying instances",
    "start": "2368930",
    "end": "2375080"
  },
  {
    "text": "that those instances will then pull down the docker image and then if the health",
    "start": "2375080",
    "end": "2381440"
  },
  {
    "text": "checks are successful then it'll put it in service so I've created a end point",
    "start": "2381440",
    "end": "2387380"
  },
  {
    "text": "before this demo so we don't have to wait for for that to be set up and run",
    "start": "2387380",
    "end": "2394630"
  },
  {
    "text": "which is the one here so what we can do now I can show you how we can actually now call that end point see if it's",
    "start": "2394630",
    "end": "2404810"
  },
  {
    "text": "working",
    "start": "2404810",
    "end": "2407260"
  },
  {
    "start": "2412000",
    "end": "2554000"
  },
  {
    "text": "so I've got a little script here so it's a Python script that looks in the data",
    "start": "2412000",
    "end": "2417790"
  },
  {
    "text": "directory so if you see here there's the data directory and there's a test folder",
    "start": "2417790",
    "end": "2423310"
  },
  {
    "text": "so these are all test images of sort of a randomization of you know either dogs",
    "start": "2423310",
    "end": "2428349"
  },
  {
    "text": "or cats that we want to classify so it would take a random moment from that directory and in order to use the SDK",
    "start": "2428349",
    "end": "2436030"
  },
  {
    "text": "the Python SDK to call my endpoint that I've configured and I'm passing in",
    "start": "2436030",
    "end": "2441609"
  },
  {
    "text": "essentially as a parameter their endpoint name so this is the existing endpoint that's up and running and then",
    "start": "2441609",
    "end": "2449410"
  },
  {
    "text": "hopefully it'll tell us give us an accurate prediction so let's see if it working so let's just see",
    "start": "2449410",
    "end": "2458380"
  },
  {
    "text": "so it's predicted to be a cat so let's just see okay so saket so that's working okay",
    "start": "2458380",
    "end": "2465599"
  },
  {
    "text": "so get another one it's just so it's just getting another random photo from",
    "start": "2467070",
    "end": "2472830"
  },
  {
    "text": "that folder so it's cat again looking alright see how we do here what's the",
    "start": "2472830",
    "end": "2483930"
  },
  {
    "text": "swim cats or or cats let's see if we can try and get a dog here we go it's a dog",
    "start": "2483930",
    "end": "2492540"
  },
  {
    "text": "the surface is a dog yeah so as you can see it's working which is great so it's",
    "start": "2492540",
    "end": "2499050"
  },
  {
    "text": "basically calling that end point so you can basically call that end point from wherever you wherever you like great so",
    "start": "2499050",
    "end": "2506670"
  },
  {
    "text": "that's the end of the demo you know it's been obviously pretty quick but",
    "start": "2506670",
    "end": "2511710"
  },
  {
    "text": "hopefully give you a flavor of you know what's possible and how you can bring essentially your own custom algorithms",
    "start": "2511710",
    "end": "2516840"
  },
  {
    "text": "to Amazon sage maker what I'm going to do in the slides as well put a link to",
    "start": "2516840",
    "end": "2522180"
  },
  {
    "text": "I've got a github repo with the source code so it includes all of the that continent folder and with the prediction",
    "start": "2522180",
    "end": "2529380"
  },
  {
    "text": "source code the docker files and also includes the Jupiter notebook showing",
    "start": "2529380",
    "end": "2534810"
  },
  {
    "text": "you how you can train your model and then push it up into s3 so we'll make that available so you can kind of look",
    "start": "2534810",
    "end": "2540450"
  },
  {
    "text": "in detail and play around with yourself so thank you very much for attending and",
    "start": "2540450",
    "end": "2545550"
  },
  {
    "text": "enjoy the rest of the summit the Bears and see you again soon thanks",
    "start": "2545550",
    "end": "2553160"
  }
]