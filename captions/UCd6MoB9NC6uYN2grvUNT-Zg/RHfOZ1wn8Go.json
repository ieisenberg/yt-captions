[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "hi everybody um thank you so much for coming out I know it's been a long day um we're the last thing probably",
    "start": "399",
    "end": "5480"
  },
  {
    "text": "standing between you guys and a beer or a crabs table or blackjack table so we really appreciate appreciate you guys",
    "start": "5480",
    "end": "10880"
  },
  {
    "text": "coming out and spending an hour with us uh this title's uh talk is called data science and best practices for Apache",
    "start": "10880",
    "end": "16680"
  },
  {
    "text": "spark on Amazon EMR uh my name is Jonathan Fritz I'm a senior product manager on the Amazon EMR team and",
    "start": "16680",
    "end": "23119"
  },
  {
    "text": "presenting with me is m chel a specialist Solutions architect um at Amazon the web services so real quick",
    "start": "23119",
    "end": "30279"
  },
  {
    "text": "running through uh just generally what we're going to cover in this talk um a quick overview on why Apache spark is",
    "start": "30279",
    "end": "37040"
  },
  {
    "text": "being adopted really fast for data science interactive workloads in those sorts of projects um and then a few uh",
    "start": "37040",
    "end": "43879"
  },
  {
    "text": "interesting features about running Apache spark on Amazon EMR um we'll go through uh quickly a few sample",
    "start": "43879",
    "end": "50800"
  },
  {
    "text": "architectures that we see customers uh using spark on emr4 then M will go",
    "start": "50800",
    "end": "56120"
  },
  {
    "text": "through best practices for running spark and then we'll show you a quick demo with Apache Zeppelin the new notebook UI",
    "start": "56120",
    "end": "62239"
  },
  {
    "text": "that we just launched on EMR uh to do a few sample queries so uh real quick how many people",
    "start": "62239",
    "end": "68840"
  },
  {
    "text": "here have used Apache spark before okay great how many of you guys have used spark on",
    "start": "68840",
    "end": "75720"
  },
  {
    "text": "AWS so decent amount and how many of you have used spark on EMR before great well",
    "start": "75720",
    "end": "82479"
  },
  {
    "start": "82000",
    "end": "82000"
  },
  {
    "text": "hopefully this will be a quick refresher then so we see spark being adopted a lot um you know we we've seen a lot of Hado",
    "start": "82479",
    "end": "88840"
  },
  {
    "text": "map produce jobs spark is one of the new engines um and there's really a few reasons why we uh See Spark being uh",
    "start": "88840",
    "end": "95479"
  },
  {
    "text": "really popular for interactive workloads and iterative workloads I mean besides the fact it's massively parallel system",
    "start": "95479",
    "end": "100799"
  },
  {
    "text": "you add on another node you can scale out um there's a few things does that as well there a few things spark does",
    "start": "100799",
    "end": "106799"
  },
  {
    "text": "particularly that make it um that make it fast first is it uses uh directed to",
    "start": "106799",
    "end": "112799"
  },
  {
    "text": "cyclic graphs instead of say just standard map reduce map reduce to plot out uh query execution um this allows",
    "start": "112799",
    "end": "119960"
  },
  {
    "text": "spark to uh be more optimized in running queries versus Hado map reduce where you",
    "start": "119960",
    "end": "125320"
  },
  {
    "text": "uh have to have that same uh framework for each step of the job the second thing is spark minimizes IO by using",
    "start": "125320",
    "end": "134160"
  },
  {
    "text": "rdds uh resilient distributed data sets um that are like basically inmemory data sets to reduce the uh IO from going up",
    "start": "134160",
    "end": "142080"
  },
  {
    "text": "uh up and back to disk versus Hadoop say for a hive job in between every map reduce job Hive will write things out to",
    "start": "142080",
    "end": "148680"
  },
  {
    "text": "hdfs or in the shuffle phase uh mail will write things back out to local dis",
    "start": "148680",
    "end": "154519"
  },
  {
    "text": "uh spark however keeps everything as if it can in memory so you can have quick access to data sets but also if you're",
    "start": "154519",
    "end": "161080"
  },
  {
    "text": "running iterative queries when you're doing your analysis you don't have that IO overhead um and finally in M we'll",
    "start": "161080",
    "end": "167800"
  },
  {
    "text": "we'll go a little bit more into this in the best practices um spark is partitioning aware so it does everything",
    "start": "167800",
    "end": "173159"
  },
  {
    "text": "it can to reduce Shuffle um which can have Network bottleneck Hadoop on the other hand has",
    "start": "173159",
    "end": "179840"
  },
  {
    "text": "Shuffle phase in between map and reduce so it forces uh that that uh stage to",
    "start": "179840",
    "end": "187080"
  },
  {
    "start": "187000",
    "end": "187000"
  },
  {
    "text": "happen uh next we see spark being very popular and as many you've have been using it you probably use some of these",
    "start": "187080",
    "end": "192360"
  },
  {
    "text": "components too um it has a lot of tightly integrated components allowing you to address a variety of different",
    "start": "192360",
    "end": "198080"
  },
  {
    "text": "use cases with spark um you know at the core it's the spark core um execution",
    "start": "198080",
    "end": "204080"
  },
  {
    "text": "engine um that's you know generalizable execution engine but on top of that um",
    "start": "204080",
    "end": "209159"
  },
  {
    "text": "there are a bunch of libraries that you can use for uh things from streaming use cases if you say have a use case where",
    "start": "209159",
    "end": "215840"
  },
  {
    "text": "you need to be able to interact with um data coming in very fast versus batch processing uh distributed machine",
    "start": "215840",
    "end": "221799"
  },
  {
    "text": "learning libraries in spark ml available for you to use in uh spark SQL as well",
    "start": "221799",
    "end": "227439"
  },
  {
    "text": "if you want to use a SQL interface but all these come tightly uh packaged in spark and you can interoperate between",
    "start": "227439",
    "end": "233560"
  },
  {
    "text": "them we'll show an example of that um a little bit later um spark speaks your",
    "start": "233560",
    "end": "238799"
  },
  {
    "start": "236000",
    "end": "236000"
  },
  {
    "text": "language so you might feel more comfortable running analysis in a variety of different languages and Spark",
    "start": "238799",
    "end": "245439"
  },
  {
    "text": "unlike Hadoop I mean Hadoop has Hive and pig and some other uh higher level languages but they're not as tightly",
    "start": "245439",
    "end": "250560"
  },
  {
    "text": "coupled in spark there's native support for Scola python um and Java um but also",
    "start": "250560",
    "end": "256880"
  },
  {
    "text": "spark sqls tightly integrated with a SQL interface and Spark R which has just come out allows you to play with your",
    "start": "256880",
    "end": "263280"
  },
  {
    "text": "data in spark using r as well and that's available in 15 and",
    "start": "263280",
    "end": "268360"
  },
  {
    "start": "268000",
    "end": "268000"
  },
  {
    "text": "14 um spark you can interact with the shell but also um there's a a new",
    "start": "268360",
    "end": "273639"
  },
  {
    "text": "notebook that's come out over the last year called Apache zeppel and we'll show it to you in a demo but it makes interacting with spark even easier um",
    "start": "273639",
    "end": "280680"
  },
  {
    "text": "you can use the language of your choice although spark R isn't supported yet um but to interact with the data and then",
    "start": "280680",
    "end": "286600"
  },
  {
    "text": "quickly visualize it save your queries and have multiple people uh collaborating on one notebook um as",
    "start": "286600",
    "end": "293280"
  },
  {
    "text": "well so I mean so you have data in S3 or in hdfs and you want to get started and",
    "start": "293280",
    "end": "298880"
  },
  {
    "text": "use sparked a proc process it um so you know what's the next step you might do so at the core is uh the spark data",
    "start": "298880",
    "end": "305520"
  },
  {
    "text": "frame which is a distributed collection of data organized in column and it's an extension of the rdd API um and it has",
    "start": "305520",
    "end": "313160"
  },
  {
    "text": "some optimizations as well that make it uh very performant to interact with in this case here's an example you uh in",
    "start": "313160",
    "end": "319639"
  },
  {
    "text": "Zeppelin there's a a default spark context but you just it's very easy to create you have your uh your files you",
    "start": "319639",
    "end": "325840"
  },
  {
    "text": "just create the data frame and then it's available for you um in kind of a relation format and you can interact",
    "start": "325840",
    "end": "331479"
  },
  {
    "text": "with it um in any of those languages that I've showed you on the previous slide so it's really that simple to get",
    "start": "331479",
    "end": "336680"
  },
  {
    "text": "up and get started you can cache the data uh data frame in memory for even better performance or you can have it",
    "start": "336680",
    "end": "342240"
  },
  {
    "text": "read out uh read it in from disk and still get pretty good performance on your queries um data frames easy to",
    "start": "342240",
    "end": "348680"
  },
  {
    "text": "create you can create them um natively from uh Json parket if you have Hive",
    "start": "348680",
    "end": "355520"
  },
  {
    "text": "tables you can easily translate that into a data frame um or you can have spark rdds already cached in memory and",
    "start": "355520",
    "end": "362360"
  },
  {
    "text": "you can create data frames from from those as well but if you have data in other formats spark supports that too um",
    "start": "362360",
    "end": "369080"
  },
  {
    "start": "365000",
    "end": "365000"
  },
  {
    "text": "the spark SQL data sources API allows you to connect uh and load in data from",
    "start": "369080",
    "end": "374160"
  },
  {
    "text": "say an elastic search cluster or Cassandra um and there are many more libraries that are being developed um as",
    "start": "374160",
    "end": "380400"
  },
  {
    "text": "well but it really shows the uh extensibility of the data frame being able to bring in data from really",
    "start": "380400",
    "end": "385960"
  },
  {
    "text": "whatever Source you need to be able to play with it uh and Spark um here's just a few sample um",
    "start": "385960",
    "end": "393919"
  },
  {
    "start": "390000",
    "end": "390000"
  },
  {
    "text": "manipulations I mean nothing too complicated here but you know once you get the data in the data frame literally",
    "start": "393919",
    "end": "399039"
  },
  {
    "text": "it's just exposed and you can start running queries on it you can um describe Fields you can you can you know",
    "start": "399039",
    "end": "404759"
  },
  {
    "text": "do whatever you need to do once it's in uh in that format and it's registered with spark um I brought up before spark",
    "start": "404759",
    "end": "413560"
  },
  {
    "start": "410000",
    "end": "410000"
  },
  {
    "text": "variety of different um libraries that are tightly integrated one of them was ml lib which is a library of distributed",
    "start": "413560",
    "end": "419560"
  },
  {
    "text": "machine learning algorithms available to use and you know once you get data in a data frame it plays very nicely with",
    "start": "419560",
    "end": "425919"
  },
  {
    "text": "those libraries and you can use the data from that to train machine learning models generate machine learning models",
    "start": "425919",
    "end": "431160"
  },
  {
    "text": "and test uh and those sorts of things so um you know spark the spark languages",
    "start": "431160",
    "end": "437039"
  },
  {
    "text": "play very well with those libraries um same thing with spark streaming you can create data frames in",
    "start": "437039",
    "end": "443280"
  },
  {
    "start": "439000",
    "end": "439000"
  },
  {
    "text": "your streaming application or you could create another spark context and run ad hoc queries on a stream as long as you",
    "start": "443280",
    "end": "450280"
  },
  {
    "text": "have uh the the data cached long enough for you to be able to query it um so if",
    "start": "450280",
    "end": "455840"
  },
  {
    "text": "your your workload requires you to maybe run bash processes on data that's uh",
    "start": "455840",
    "end": "461319"
  },
  {
    "text": "that's pretty static but you need to say do a join with something that just came in um that's available for you as well",
    "start": "461319",
    "end": "467240"
  },
  {
    "text": "depending on your what what you need to use uh in your analytics workload uh quick example of R uh spark",
    "start": "467240",
    "end": "474520"
  },
  {
    "text": "R um is available in the latest few releases of spark um you can access it in spark shell you can create a spark",
    "start": "474520",
    "end": "481120"
  },
  {
    "text": "application that's executing uh queries with with r um and it's got comparable",
    "start": "481120",
    "end": "486680"
  },
  {
    "text": "performance we found to Python and Scola data frames um as well and hopefully they'll be supporting Zeppelin soon for",
    "start": "486680",
    "end": "493360"
  },
  {
    "text": "it and then finally um spark SQL as well once you get your data frame up you can",
    "start": "493360",
    "end": "498879"
  },
  {
    "text": "uh run you know more bi style queries using SQL uh if you have a lot of hiveql",
    "start": "498879",
    "end": "503919"
  },
  {
    "text": "you can create a hive context and use your hiveql instead of spark SQL because there's some differences",
    "start": "503919",
    "end": "510319"
  },
  {
    "text": "um in those languages and it's not one toone comparable yet but we found a lot",
    "start": "510319",
    "end": "516080"
  },
  {
    "text": "of success of people porting those over um and then you can connect uh via jdbc or obbc to another uh bi tool you might",
    "start": "516080",
    "end": "523399"
  },
  {
    "text": "have so you know spark variety different language support built-in support for",
    "start": "523399",
    "end": "529000"
  },
  {
    "text": "machine learning it's very easy to get things into a data frame um and that's and it's and it's fast and that's why",
    "start": "529000",
    "end": "535320"
  },
  {
    "text": "we've seen spark becoming adopted really quick for Interactive workloads where you need queries to return fast or",
    "start": "535320",
    "end": "541760"
  },
  {
    "text": "things that you've been iterative workloads as well um so Amazon EMR we",
    "start": "541760",
    "end": "548079"
  },
  {
    "text": "launched full support for spark in June which we were very excited about we had customers running spark long before that",
    "start": "548079",
    "end": "553880"
  },
  {
    "text": "installing Thea bootstrap action um and there's a few interesting features that we found uh customers really like when",
    "start": "553880",
    "end": "559959"
  },
  {
    "text": "running spark in EMR um it really boils down to you know a few high level points",
    "start": "559959",
    "end": "565720"
  },
  {
    "text": "uh it's super easy to install and configure spark on EMR it's it's really easy also to add and remove capacity",
    "start": "565720",
    "end": "571880"
  },
  {
    "text": "it's just an API call um variety of ways to interact and submit work um different",
    "start": "571880",
    "end": "578000"
  },
  {
    "text": "cost structures depending on the nature of your jobs if they're batch workloads if you want to use ec2 spot which is",
    "start": "578000",
    "end": "583600"
  },
  {
    "text": "where you bid on your capacity and you get up to 90% off your instances um there's a variety of security features",
    "start": "583600",
    "end": "589519"
  },
  {
    "text": "and you can use S3 instead of hdfs as your data layer to dissociate uh your compute and your storage",
    "start": "589519",
    "end": "596440"
  },
  {
    "text": "resources um so spark easy to install and configure on EMR um and we're very",
    "start": "596440",
    "end": "602360"
  },
  {
    "start": "597000",
    "end": "597000"
  },
  {
    "text": "committed to being close to releasing uh spark latest version of spark in EMR",
    "start": "602360",
    "end": "607959"
  },
  {
    "text": "very soon after it's available in the open source and here's a few examples we're very quick to launch spark 141",
    "start": "607959",
    "end": "614079"
  },
  {
    "text": "after it was released and spark5 we actually released on EMR release 41 last",
    "start": "614079",
    "end": "620000"
  },
  {
    "text": "week and we're going to continue being very close um in supporting whatever the latest version is just because there's",
    "start": "620000",
    "end": "625720"
  },
  {
    "text": "fast moving code base a lot of new features a lot of very important bug fixes as well so um it's easy to to",
    "start": "625720",
    "end": "632760"
  },
  {
    "text": "install we install spark on yarn um we choose yarn for a variety of reasons one",
    "start": "632760",
    "end": "637959"
  },
  {
    "text": "is you can install a lot of other apps on your EMR cluster as well and yarn is a good uh resource manager for also",
    "start": "637959",
    "end": "644240"
  },
  {
    "text": "Hadoop map ruce and and some of the other applications in that staff so if you want you can install map produce and",
    "start": "644240",
    "end": "651160"
  },
  {
    "text": "Spark so you can use hi and map produce and Spark on the same cluster um it's really up to you when you uh choose the",
    "start": "651160",
    "end": "656839"
  },
  {
    "text": "applications that you want to install um but there's also a few other features in yarn that are are pretty useful one is",
    "start": "656839",
    "end": "662720"
  },
  {
    "text": "just a variety of good schedulers in yarn um if you want to run a priority schedule or some other things depending",
    "start": "662720",
    "end": "668120"
  },
  {
    "text": "on your use case you can toggle those on and off in the yarn settings um yarn",
    "start": "668120",
    "end": "673320"
  },
  {
    "text": "also allows you to dynamically allocate executors and M will go into a little bit more detail on what that means but",
    "start": "673320",
    "end": "680279"
  },
  {
    "text": "supplies some automation around figuring out the actual resources needed by your spark job versus you telling you aren't",
    "start": "680279",
    "end": "685839"
  },
  {
    "text": "hey I need 10 executors um also yarn supports Kerberos",
    "start": "685839",
    "end": "690959"
  },
  {
    "text": "authentication if that's required uh for your use case um so easy to create you can create",
    "start": "690959",
    "end": "696519"
  },
  {
    "text": "a spark cluster with um you know as many notes as you need from the AWS",
    "start": "696519",
    "end": "701600"
  },
  {
    "text": "Management console that's a quick snapshot of our quick create but we have a more advanced create if you want to uh",
    "start": "701600",
    "end": "707000"
  },
  {
    "text": "twist more knobs in creating your cluster um we have the support on the AWS command line interface it's just to",
    "start": "707000",
    "end": "712920"
  },
  {
    "text": "create cluster command there's a variety of commands you can use and script against if you want to easily launch clusters or you can just use the EMR API",
    "start": "712920",
    "end": "719720"
  },
  {
    "text": "directly and it's really that simple we configure your spark cluster with um default settings uh when you say",
    "start": "719720",
    "end": "727959"
  },
  {
    "text": "want an R3 instance we've done performance testing to know how much uh default settings for you know memory",
    "start": "727959",
    "end": "733760"
  },
  {
    "text": "allocation we need to give yarn and things like that with the goal of just you create a cluster and you can use it immediately without having to tweak um",
    "start": "733760",
    "end": "740519"
  },
  {
    "text": "all the different settings and by default though we use uh Sparks uh default settings for executors so you",
    "start": "740519",
    "end": "746839"
  },
  {
    "text": "will want to change um those settings like spark executor cores and that'll that'll come up in the best practices on",
    "start": "746839",
    "end": "752760"
  },
  {
    "text": "a few ways to think about that um but it really is that easy we have a configuration uh parameter you uh",
    "start": "752760",
    "end": "758320"
  },
  {
    "text": "specify a Json with all the settings that you want to change and when you create the cluster we'll override those settings uh for you if you'd",
    "start": "758320",
    "end": "765519"
  },
  {
    "text": "like uh one great thing about running spark and AWS is a variety of different data stores you can use um depending on",
    "start": "765519",
    "end": "772360"
  },
  {
    "text": "your use case and uh with EMR we see our customers uh quering data from a variety",
    "start": "772360",
    "end": "777560"
  },
  {
    "text": "of different sources um that might be uh siloed Amazon Dynamo DB nosql database",
    "start": "777560",
    "end": "784760"
  },
  {
    "text": "in Amazon you can use the EMR Hive connector to pull data out of uh Dynamo and then run queries using spark with",
    "start": "784760",
    "end": "791720"
  },
  {
    "text": "that um you can connect via jdbc and Spark SQL to Amazon RDS the relational",
    "start": "791720",
    "end": "797279"
  },
  {
    "text": "database service um if you have an elastic search cluster running um you",
    "start": "797279",
    "end": "802519"
  },
  {
    "text": "can uh allow spark to query that um one really popular use case is using uh",
    "start": "802519",
    "end": "808160"
  },
  {
    "text": "kinesis or Kafka to stream data and to spark streaming and all of that is is easily available and configurable in",
    "start": "808160",
    "end": "814480"
  },
  {
    "text": "your environment um and then finally um S3 so Amazon EMR has uh emrfs the EMR",
    "start": "814480",
    "end": "822320"
  },
  {
    "text": "file system which is RS3 connector which is uh fully usable by sparc and that",
    "start": "822320",
    "end": "828079"
  },
  {
    "text": "allows you to performant and securely process data um in S3 with sparc you're",
    "start": "828079",
    "end": "833360"
  },
  {
    "text": "not copying it into hdfs and then in querying it you're reading it directly and and creating you know an rdd directly from your data layer in S3 um",
    "start": "833360",
    "end": "841000"
  },
  {
    "text": "but more interestingly it allows you to decouple your compute in storage so instead of sizing your spark cluster say",
    "start": "841000",
    "end": "846279"
  },
  {
    "text": "for your data that needs to be replicated three times in hdfs um you can size your cluster to the amount of",
    "start": "846279",
    "end": "852240"
  },
  {
    "text": "memory and compute you need for your job so in some cases you can save a lot of money by doing that or if you don't need",
    "start": "852240",
    "end": "858519"
  },
  {
    "text": "your cluster up let's say you want to run um some investigations on a data set in S3 today you could go into the office",
    "start": "858519",
    "end": "865320"
  },
  {
    "text": "spin up a cluster do whatever processing you need to do and shut the cluster down and stop paying for it and a cluster",
    "start": "865320",
    "end": "871000"
  },
  {
    "text": "comes up within a couple of minutes so it's uh very tailored to however you really need to use it n S3 is designed",
    "start": "871000",
    "end": "877480"
  },
  {
    "text": "for 119 of durability um low cost and very very scalable so we see a lot of",
    "start": "877480",
    "end": "883519"
  },
  {
    "text": "customers using that instead of hdfs um as their data",
    "start": "883519",
    "end": "888839"
  },
  {
    "text": "store um running spark and EMR uh there's a variety of ways you can run your spark workloads you can use you can",
    "start": "888839",
    "end": "895519"
  },
  {
    "start": "889000",
    "end": "889000"
  },
  {
    "text": "create a spark application and submit it to the EMR step API and uh you you upload your application",
    "start": "895519",
    "end": "901680"
  },
  {
    "text": "to S3 submit some information about it and the EMR API will in um",
    "start": "901680",
    "end": "906759"
  },
  {
    "text": "serial uh send those applications up to spark running on your cluster run the job and then typically the output of",
    "start": "906759",
    "end": "912959"
  },
  {
    "text": "that job will be pushed back to S3 so that's an easy way if you have say a batch job that you want to run after you",
    "start": "912959",
    "end": "919000"
  },
  {
    "text": "know working around figuring out the business logic you want putting it into production it's a good way to just run",
    "start": "919000",
    "end": "924519"
  },
  {
    "text": "jobs uh on a regular basis but if you want to do more ad hoc workloads you can",
    "start": "924519",
    "end": "929639"
  },
  {
    "text": "uh SSH to the master note of your cluster and and access spark via the spark shell in an interactive way um or",
    "start": "929639",
    "end": "936399"
  },
  {
    "text": "use a patchy Zeppelin now and and pull that up in your browser and you spark for more um ad hoc interactive",
    "start": "936399",
    "end": "943959"
  },
  {
    "text": "analytics um and finally uh there's a a few security features which we've seen customers use depending on the",
    "start": "943959",
    "end": "950440"
  },
  {
    "text": "requirements around their workloads um for encryption at rest um you know many",
    "start": "950440",
    "end": "957079"
  },
  {
    "text": "customers are using S3 as their data layer um emrfs supports both Amazon S3",
    "start": "957079",
    "end": "962440"
  },
  {
    "text": "server side encryption or Amazon S3 client side encryption either bringing your own key from an HSM or whatever key",
    "start": "962440",
    "end": "968880"
  },
  {
    "text": "vendor you have or using AWS KMS uh Key Management Service to store your keys um",
    "start": "968880",
    "end": "974720"
  },
  {
    "text": "on the cluster there's uh two uh two file systems local and hdfs on EMR",
    "start": "974720",
    "end": "980240"
  },
  {
    "text": "release uh 41 which was last week we released um the ability to install the component Hadoop KMS which can supply",
    "start": "980240",
    "end": "987480"
  },
  {
    "text": "keys to uh encrypt hdfs at rest if you need to encrypt local disc for say",
    "start": "987480",
    "end": "993399"
  },
  {
    "text": "spills from an Executor you can uh use just general Lux encryption using a",
    "start": "993399",
    "end": "999399"
  },
  {
    "text": "bootstrap action which is a script that runs on each node when a node starts up um to encrypt those volumes as",
    "start": "999399",
    "end": "1006319"
  },
  {
    "text": "well uh for encryption and flight uh traffic from S3 to ec2 instances that",
    "start": "1006319",
    "end": "1012079"
  },
  {
    "start": "1007000",
    "end": "1007000"
  },
  {
    "text": "make up your Amazon e cluster um use SSL um but if you use client side encryption",
    "start": "1012079",
    "end": "1017639"
  },
  {
    "text": "they're also encrypted over the as well for double protection uh on the cluster for inter node communication hdfs",
    "start": "1017639",
    "end": "1024079"
  },
  {
    "text": "encryption is done in the client so uh the communication between data nodes",
    "start": "1024079",
    "end": "1029280"
  },
  {
    "text": "doesn't need to be encrypted Because the actual blocks are already encrypted um and then for certain spark operations",
    "start": "1029280",
    "end": "1035760"
  },
  {
    "text": "broadcast and file server you can enable SSL for those processes but for the block transfer service which is used in",
    "start": "1035760",
    "end": "1042079"
  },
  {
    "text": "the spark Shuffle um does not use SSL yet and there's a Jura open for that and",
    "start": "1042079",
    "end": "1047880"
  },
  {
    "text": "hopefully that'll be a new feature in one of the subsequent releases and then finally there's uh",
    "start": "1047880",
    "end": "1054600"
  },
  {
    "start": "1052000",
    "end": "1052000"
  },
  {
    "text": "just some other General security features that Amazon EMR uh integrates with if you want to run your spark",
    "start": "1054600",
    "end": "1059960"
  },
  {
    "text": "clusters very securely um Amazon EMR you're required to use IM roles with your cluster which determine the",
    "start": "1059960",
    "end": "1066039"
  },
  {
    "text": "permissions of both the Amazon EMR service and the ec2 nodes that um make up your cluster there's two different",
    "start": "1066039",
    "end": "1071760"
  },
  {
    "text": "roles that you use um you could enable Kos if you'd like um and you can use IM users to call the Amazon EMR API from a",
    "start": "1071760",
    "end": "1079640"
  },
  {
    "text": "networking perspective you can launch in a VPC um and you can also tailor security groups on your cluster we have",
    "start": "1079640",
    "end": "1085000"
  },
  {
    "text": "default ones that we use but you can layer whichever settings you want over that and then finally if you want to",
    "start": "1085000",
    "end": "1090440"
  },
  {
    "text": "audit who's accessing your spark cluster via the EMR API you can use uh cloud",
    "start": "1090440",
    "end": "1096000"
  },
  {
    "text": "trail as well um so so we have we have many customers um who are running spark on",
    "start": "1096000",
    "end": "1102120"
  },
  {
    "text": "Amazon EMR here's a few examples of uh of some some folks who are running it",
    "start": "1102120",
    "end": "1107240"
  },
  {
    "text": "and really the use cases uh are in very large range um people running more uh",
    "start": "1107240",
    "end": "1113640"
  },
  {
    "text": "you know ad targeting or recommendations engines to uh security event streaming",
    "start": "1113640",
    "end": "1119919"
  },
  {
    "text": "uh workloads where they're they're looking at um and predicting uh interesting things coming out of logs",
    "start": "1119919",
    "end": "1125200"
  },
  {
    "text": "from a security system um these are a few slides that we showed uh at spark",
    "start": "1125200",
    "end": "1130400"
  },
  {
    "text": "Summit when we launched spark in June one of them is the Washington posts um",
    "start": "1130400",
    "end": "1135919"
  },
  {
    "text": "architecture um and they use uh a few different Spar clusters to power some of their forecasting and",
    "start": "1135919",
    "end": "1142679"
  },
  {
    "text": "recommendations engines um and then heavily utilize ml lib to run some of",
    "start": "1142679",
    "end": "1148080"
  },
  {
    "text": "these machine learning uh models and the thing though is that you know these are productionize workloads they're they're",
    "start": "1148080",
    "end": "1153480"
  },
  {
    "text": "running they're always on but there was a lot of um just ad hoc work to figure out what the business logic was to be",
    "start": "1153480",
    "end": "1160280"
  },
  {
    "text": "able to uh to build it into these production systems and it was very easy using EMR to spin up new clusters to",
    "start": "1160280",
    "end": "1166000"
  },
  {
    "text": "test um you know and then moving it over to to a production stack gumgum also",
    "start": "1166000",
    "end": "1172280"
  },
  {
    "text": "uses Spark on EMR to do a variety of workflows here they have a always on spark cluster that's working as an",
    "start": "1172280",
    "end": "1177799"
  },
  {
    "text": "interactive dashboard and a forecasting system allowing bi Engineers to go in and play with data in a really fast",
    "start": "1177799",
    "end": "1184559"
  },
  {
    "text": "response time with with cached rdds um but they also use batch Spar clusters to spin up do some processing shut down and",
    "start": "1184559",
    "end": "1191520"
  },
  {
    "text": "then load into red shift as well for other for other workloads and use other systems for just uh development and test",
    "start": "1191520",
    "end": "1197919"
  },
  {
    "text": "so there really vers platform and there's a lot of uh different things you can do with it um and with that I'll",
    "start": "1197919",
    "end": "1203400"
  },
  {
    "text": "turn it over to M to talk a little bit more about best practice sister spark thank you um so let's go into the best",
    "start": "1203400",
    "end": "1210640"
  },
  {
    "text": "practices for running spark on EMR so let's look into the uh sessions",
    "start": "1210640",
    "end": "1216360"
  },
  {
    "text": "the thing what I'll be covering in the best practices so starting with the which instance types you should pick and",
    "start": "1216360",
    "end": "1222679"
  },
  {
    "text": "then going deep into the executors understanding executors how they work and then you'll look at the storage",
    "start": "1222679",
    "end": "1229039"
  },
  {
    "text": "layers because how you organize your files matters in your performance and then tuning of your jobs so how do you",
    "start": "1229039",
    "end": "1235760"
  },
  {
    "text": "boost the performance and finally we'll go with the demo and showing uh how you can use Apache Zeppelin Notebook on EMR",
    "start": "1235760",
    "end": "1244080"
  },
  {
    "text": "so going first into the what does spark need spark needs lot of",
    "start": "1244080",
    "end": "1250520"
  },
  {
    "start": "1245000",
    "end": "1245000"
  },
  {
    "text": "memory and then when you're doing a some kind of machine learning process you need um CPU and then you need Network so",
    "start": "1250520",
    "end": "1258240"
  },
  {
    "text": "these three three things are required so if these three things combine together form an instance so you need to select",
    "start": "1258240",
    "end": "1265280"
  },
  {
    "text": "an instance with the proper set of memory a good networking and a good CPU based on the",
    "start": "1265280",
    "end": "1271720"
  },
  {
    "text": "workload and so once you have selected the instances you need to scale out so spark offers horizontal scaling so the",
    "start": "1271720",
    "end": "1278279"
  },
  {
    "text": "more the instances you add you can scale up your processing so let's look at the",
    "start": "1278279",
    "end": "1283320"
  },
  {
    "text": "type of the workloads which you can the workloads and the resources you need for it so for like machine learning it's",
    "start": "1283320",
    "end": "1288760"
  },
  {
    "text": "more CPU intensive you need a machine with CPU intens if you're doing a eil processing job look for a machine which",
    "start": "1288760",
    "end": "1295200"
  },
  {
    "text": "is has more of iio resources so these are the different",
    "start": "1295200",
    "end": "1300559"
  },
  {
    "start": "1299000",
    "end": "1299000"
  },
  {
    "text": "instance types which you can see in the EMR so first is the general instances",
    "start": "1300559",
    "end": "1306400"
  },
  {
    "text": "they are M1 and M3 family the CPU instances C1 C3 and then we have another class of",
    "start": "1306400",
    "end": "1314080"
  },
  {
    "text": "memory intensive machines and then the dis iio the dis iio machines would have huge",
    "start": "1314080",
    "end": "1319200"
  },
  {
    "text": "uh storage so each machine has a different which can be used for good workload so a",
    "start": "1319200",
    "end": "1328039"
  },
  {
    "text": "general machine can be used for a batch processing CPU type machines can be used for a machine learning and when you're",
    "start": "1328039",
    "end": "1334840"
  },
  {
    "text": "doing many interactive analysis so in spark you need to cat all the data in memory so you need a machine with enough",
    "start": "1334840",
    "end": "1342960"
  },
  {
    "text": "memory and that's why you'll go with the memory intensive machines there and if you're using a large hdfs go for the dis",
    "start": "1342960",
    "end": "1349600"
  },
  {
    "text": "your family so now you have the type the instances you need now let's go deep",
    "start": "1349600",
    "end": "1355360"
  },
  {
    "text": "into the executors what executors are so look at a simple spark program the two",
    "start": "1355360",
    "end": "1361400"
  },
  {
    "start": "1358000",
    "end": "1358000"
  },
  {
    "text": "things are there you have a spark driver and then you have a spark",
    "start": "1361400",
    "end": "1366440"
  },
  {
    "text": "executor now as a user when you submit a job it goes to the cluster and then on",
    "start": "1366440",
    "end": "1372080"
  },
  {
    "text": "all the executors is where your job is running and once the job all the exe",
    "start": "1372080",
    "end": "1378120"
  },
  {
    "text": "complete the results they send it back to your user the driver so this is where all the action",
    "start": "1378120",
    "end": "1383840"
  },
  {
    "text": "is happening all the processing happens in executor so let's go deep into an",
    "start": "1383840",
    "end": "1389159"
  },
  {
    "text": "Executor what happens there and how it is structured and when you go into that you",
    "start": "1389159",
    "end": "1395440"
  },
  {
    "text": "will always have so when you're doing applications you always get into a trouble like okay how many executors do",
    "start": "1395440",
    "end": "1400799"
  },
  {
    "text": "I need for this application and how many CPU CES should I give to this executor",
    "start": "1400799",
    "end": "1406000"
  },
  {
    "text": "that's always a problem and best thing is to understand with by means of an example so the example let's consider",
    "start": "1406000",
    "end": "1413320"
  },
  {
    "text": "here is let's say we pick a family of machines R3 and for our example we'll go",
    "start": "1413320",
    "end": "1419720"
  },
  {
    "text": "with machine R34 Xcel which has 16 cores and 122 GB of RAM 220 GB of SSD drive",
    "start": "1419720",
    "end": "1429640"
  },
  {
    "text": "and a high networking so using the uh AWS CLI you",
    "start": "1429640",
    "end": "1435600"
  },
  {
    "text": "can create this cluster using the steps here so look into this here I'm",
    "start": "1435600",
    "end": "1440640"
  },
  {
    "text": "selecting R34 Excel and I'm picking six instances of that so one master and 5",
    "start": "1440640",
    "end": "1448039"
  },
  {
    "text": "C and you see the easy to attribute key name my key that's the key you the key which you",
    "start": "1448039",
    "end": "1454559"
  },
  {
    "start": "1454000",
    "end": "1454000"
  },
  {
    "text": "provide so now once your cluster is up and running now you have you're configuring",
    "start": "1454559",
    "end": "1461600"
  },
  {
    "text": "okay how many machines I need now let's consider into the how do you select the number of",
    "start": "1461600",
    "end": "1467760"
  },
  {
    "text": "CES so you leave one code for the OS and the other activities and then for a good",
    "start": "1467760",
    "end": "1474440"
  },
  {
    "text": "Optimal Performance you leave four to five CES per executor so uh in each",
    "start": "1474440",
    "end": "1480279"
  },
  {
    "text": "executor is where you have the tasks and when you're doing a processing let's say you're reading and writing data from",
    "start": "1480279",
    "end": "1487720"
  },
  {
    "text": "htfs each task has a thread so when you read data and write data to htfs you",
    "start": "1487720",
    "end": "1493480"
  },
  {
    "text": "have four to five threads running and writing there so it gives you good performance",
    "start": "1493480",
    "end": "1498640"
  },
  {
    "text": "and this 4 to five threads has been it's been DED like after multiple",
    "start": "1498640",
    "end": "1504120"
  },
  {
    "text": "iterations optimizations and see I found that 45 threads makes a good sense so",
    "start": "1504120",
    "end": "1510399"
  },
  {
    "text": "considering a 45 number of CES per executor how do you so this is simple",
    "start": "1510399",
    "end": "1516480"
  },
  {
    "text": "formula you can use to set number of executors needed per node so you have",
    "start": "1516480",
    "end": "1522440"
  },
  {
    "text": "one machine R34 XEL in that machine you can run three execut computers and how that",
    "start": "1522440",
    "end": "1529240"
  },
  {
    "text": "comes is total numbers of the code on the machine is 16 we're leaving away one code for the OS and then five that's the",
    "start": "1529240",
    "end": "1537919"
  },
  {
    "text": "number of tasks per executor so that's the number the last slide I discussed you can leave four to five so in this",
    "start": "1537919",
    "end": "1544679"
  },
  {
    "text": "case we're selecting five so we get a round off number to three three executors per",
    "start": "1544679",
    "end": "1552360"
  },
  {
    "text": "instance so looking at this now we have six machines totally three executors per",
    "start": "1552360",
    "end": "1558960"
  },
  {
    "text": "node so totally we'll go with 18 and we'll leave minus one that is because",
    "start": "1558960",
    "end": "1565200"
  },
  {
    "text": "when you're submitting your job in a cluster mode your application it runs in one of",
    "start": "1565200",
    "end": "1571120"
  },
  {
    "text": "the executors there so you leave one for that so totally you'll be selecting 17 executors for your",
    "start": "1571120",
    "end": "1577240"
  },
  {
    "text": "cluster so now you have decided okay how many executors I need now the next thing",
    "start": "1577240",
    "end": "1582320"
  },
  {
    "text": "is comes on how much memory is required for my processing so spark on EMR runs",
    "start": "1582320",
    "end": "1588640"
  },
  {
    "text": "in a Yan mode so in Yan you have the main thing Yan containers this is where",
    "start": "1588640",
    "end": "1594960"
  },
  {
    "text": "the process the application runs so when you submit a job it goes the containers are launched and within a container",
    "start": "1594960",
    "end": "1601320"
  },
  {
    "text": "space is where your application runs so the size of the container is decided by the configuration y Yan node manager",
    "start": "1601320",
    "end": "1609240"
  },
  {
    "text": "resource memory MB and that configuration you can set in the file Yan side XML for our example we are",
    "start": "1609240",
    "end": "1616840"
  },
  {
    "text": "using R34 XEL that has a default of 116 GB so that's the size of maximum size of",
    "start": "1616840",
    "end": "1624000"
  },
  {
    "text": "the container on the machine now in this container is where your spark job will be executing that's",
    "start": "1624000",
    "end": "1630840"
  },
  {
    "text": "the container space I would say so inside a spark job in a spark",
    "start": "1630840",
    "end": "1637520"
  },
  {
    "text": "executor you have one thing called memory overhead executor memory overhead",
    "start": "1637520",
    "end": "1643080"
  },
  {
    "text": "where VM overheads internal strings they are cached there and that is",
    "start": "1643080",
    "end": "1649399"
  },
  {
    "text": "of I would say it's like a executed memory into10 that's the derivation for it and",
    "start": "1649399",
    "end": "1656880"
  },
  {
    "text": "that configuration you can set in spark default. next other space is for spark",
    "start": "1656880",
    "end": "1663840"
  },
  {
    "text": "executor memory the rest of the so this is where the amount of memory is used by",
    "start": "1663840",
    "end": "1669399"
  },
  {
    "text": "the executor and you can set that in the spark executor memory configuration using in the file spark default so you",
    "start": "1669399",
    "end": "1677480"
  },
  {
    "text": "have in this memory space you have two things do one is your Shuffle memory fraction the fraction of the Java Heap",
    "start": "1677480",
    "end": "1684760"
  },
  {
    "text": "space which is used for aggregations and Co groups so when you're doing aggregation in your spark job this space",
    "start": "1684760",
    "end": "1691320"
  },
  {
    "text": "is used and next other space is the storage so these two are the memory fractions which you consider and for",
    "start": "1691320",
    "end": "1698440"
  },
  {
    "text": "storage the default is 6 so now you have a shuffle memory",
    "start": "1698440",
    "end": "1705440"
  },
  {
    "text": "fraction you have a storage memory fraction these are inside a spark executor memory so looking at our",
    "start": "1705440",
    "end": "1712320"
  },
  {
    "text": "example so executor memory for this Yan container how much should iside the",
    "start": "1712320",
    "end": "1718200"
  },
  {
    "text": "maximum container size by number of executors per",
    "start": "1718200",
    "end": "1723399"
  },
  {
    "text": "node so for this example here we have 116 GB as your container size and on one",
    "start": "1723399",
    "end": "1729960"
  },
  {
    "text": "instance we are launching three executors so by three that rounds up to",
    "start": "1729960",
    "end": "1736919"
  },
  {
    "text": "38 GB so that's your executive memory next we considering the memory overhead",
    "start": "1736919",
    "end": "1744200"
  },
  {
    "text": "and memory overhead as discussed is a your executive memory into 0.10 that's",
    "start": "1744200",
    "end": "1749600"
  },
  {
    "text": "roughly 3.8 GB and so overall what you'll be you'll",
    "start": "1749600",
    "end": "1754799"
  },
  {
    "text": "have 38 GB and you subtract that 3.8 and that rounds up to 34 GB so that's the",
    "start": "1754799",
    "end": "1760120"
  },
  {
    "text": "space where your data will be playing around moving in that so that's 34g so",
    "start": "1760120",
    "end": "1766080"
  },
  {
    "text": "now you derite this number and you can set this configuration in spark default.",
    "start": "1766080",
    "end": "1772320"
  },
  {
    "text": "or like when you're running a job how you can set it by passing executor 17",
    "start": "1772320",
    "end": "1778279"
  },
  {
    "text": "number of executor score is five and executor memory of 34 GB so for this cluster when you're ranching application",
    "start": "1778279",
    "end": "1785039"
  },
  {
    "text": "the optimal configuration so best utilization will be this so now you have a cluster up and",
    "start": "1785039",
    "end": "1791760"
  },
  {
    "text": "running since it's Yan you can run multiple applications in this you can have idle resources and when you you're",
    "start": "1791760",
    "end": "1798240"
  },
  {
    "text": "done when you're done with the execution you have some idle space left and you're always qued like do I need",
    "start": "1798240",
    "end": "1805080"
  },
  {
    "text": "how many number of Executives I need for that let's in spark 1.3 version onwards",
    "start": "1805080",
    "end": "1812200"
  },
  {
    "text": "Dynamic allocation was released so Dynamic allocation lets you forget about the setting number of",
    "start": "1812200",
    "end": "1818799"
  },
  {
    "text": "executors and what that is when you're not doing the processing it removes the",
    "start": "1818799",
    "end": "1824600"
  },
  {
    "start": "1820000",
    "end": "1820000"
  },
  {
    "text": "idle executors and when your application request smoo processing power it",
    "start": "1824600",
    "end": "1830000"
  },
  {
    "text": "requests those so you can addly scale up your applications and Down based on the",
    "start": "1830000",
    "end": "1836279"
  },
  {
    "text": "demand so in the dynamic allocation scaling up the executors what it allows",
    "start": "1836279",
    "end": "1842039"
  },
  {
    "text": "you to do so your jobs complete much faster because now you have more executors at the at your jobs",
    "start": "1842039",
    "end": "1848840"
  },
  {
    "text": "application and then you're using the idal resources on the cluster so let's say for example I launched a 100 100",
    "start": "1848840",
    "end": "1854840"
  },
  {
    "text": "node cluster and I'm processing and other applications are finished now so I",
    "start": "1854840",
    "end": "1860480"
  },
  {
    "text": "have extra resources left I can use those if I have set the dynamic",
    "start": "1860480",
    "end": "1865960"
  },
  {
    "text": "allocation next is uh on this once you have decided my job is running and then",
    "start": "1865960",
    "end": "1872919"
  },
  {
    "text": "how many executors are needed so spark decides that okay I can exponentially",
    "start": "1872919",
    "end": "1878159"
  },
  {
    "text": "allocate more number of executors for your job so you can configure Dynamic",
    "start": "1878159",
    "end": "1883960"
  },
  {
    "start": "1882000",
    "end": "1882000"
  },
  {
    "text": "allocation using these parameters the first two in this Dynamic allocation enabled you set that flag to true and",
    "start": "1883960",
    "end": "1890639"
  },
  {
    "text": "Shuffle service enabled to true so these two are must the remaining others in this are optional you can configure them",
    "start": "1890639",
    "end": "1898279"
  },
  {
    "text": "and those are the minimum number of executors you need for this application the maximum executors you can go for",
    "start": "1898279",
    "end": "1904760"
  },
  {
    "text": "then initial number of executors required and execut idle time if executor has been idle for in this case",
    "start": "1904760",
    "end": "1911639"
  },
  {
    "text": "60 seconds release it so using these parameters you can",
    "start": "1911639",
    "end": "1917480"
  },
  {
    "text": "actually enable Dynamic allocation on the spark and dynamic location is available only on Yar for",
    "start": "1917480",
    "end": "1926600"
  },
  {
    "text": "now so now you know your which instance types you have",
    "start": "1926880",
    "end": "1932120"
  },
  {
    "text": "picked you know which how to configure the number of CPUs number of how much memory is",
    "start": "1932120",
    "end": "1938200"
  },
  {
    "text": "required now let's jump into the storage storage when you go in first",
    "start": "1938200",
    "end": "1945000"
  },
  {
    "start": "1942000",
    "end": "1942000"
  },
  {
    "text": "thing is always use compressions compress data when you're storing it on Amazon S3 what it provides",
    "start": "1945000",
    "end": "1951279"
  },
  {
    "text": "you idly your saving space so a 10gb file KN compress goes up let's say 5 6",
    "start": "1951279",
    "end": "1957720"
  },
  {
    "text": "GB so you reduce cost on the storage and when you're doing a application and it's",
    "start": "1957720",
    "end": "1964000"
  },
  {
    "text": "reading data and writing data from Amazon S3 you're transferring it over the network so instead of transferring",
    "start": "1964000",
    "end": "1970639"
  },
  {
    "text": "it let's say 10gb you're transferring only 5gb so you're actually saving time there and it reduces the bandwidth there",
    "start": "1970639",
    "end": "1978120"
  },
  {
    "text": "and next is it speeds up your job because compression so these are the different",
    "start": "1978120",
    "end": "1984639"
  },
  {
    "text": "compression types so there's jzip LZ Snappy and looking at the performance of",
    "start": "1984639",
    "end": "1990480"
  },
  {
    "text": "these you'll know like okay which one should I pick so some compression algorithms are fast and offer less space",
    "start": "1990480",
    "end": "1998440"
  },
  {
    "text": "on the total size and some are efficient but slower efficient I would say they're",
    "start": "1998440",
    "end": "2004159"
  },
  {
    "text": "eating a lot of CPU cycles and they're trying much in time is spent in compressing the data so they're slow in",
    "start": "2004159",
    "end": "2010840"
  },
  {
    "text": "there and next is on the you have splitable compressions and some files",
    "start": "2010840",
    "end": "2016000"
  },
  {
    "text": "are not so there's so many choices on the which compression file should I pick",
    "start": "2016000",
    "end": "2021120"
  },
  {
    "text": "which one should I take which should not so if so if you're time",
    "start": "2021120",
    "end": "2028200"
  },
  {
    "text": "sensitive you can go with the faster compressions it's always a better choer but if you have large amount of data and",
    "start": "2028200",
    "end": "2035880"
  },
  {
    "text": "then you use space efficient compression if you really don't care which comparation just go pick this",
    "start": "2035880",
    "end": "2043638"
  },
  {
    "text": "up so now you know with storage now how do you boost the performance of your job",
    "start": "2044679",
    "end": "2050839"
  },
  {
    "text": "how do you make it faster so first thing you'll going into Data calization uh so when in your job let's",
    "start": "2050839",
    "end": "2059599"
  },
  {
    "start": "2055000",
    "end": "2055000"
  },
  {
    "text": "say these two things one is ex both are spark executors and when a data is moved",
    "start": "2059599",
    "end": "2065560"
  },
  {
    "text": "from one note to another from the memory it goes to the disk and then that serialization there and when it goes",
    "start": "2065560",
    "end": "2072679"
  },
  {
    "text": "back from the it's transferred over the network to the other node and it's pushed back in the memory so when you're",
    "start": "2072679",
    "end": "2078638"
  },
  {
    "text": "doing a uh caching or shuffling a data serialization decentralization happens",
    "start": "2078639",
    "end": "2084000"
  },
  {
    "text": "in spark the default cizer is Java sterilizer uh there's a faster one",
    "start": "2084000",
    "end": "2089878"
  },
  {
    "text": "called chyo cizer that is 10 times faster than the default Java",
    "start": "2089879",
    "end": "2095800"
  },
  {
    "text": "sterilizer uh it is not at default when you launch a cluster because it does not",
    "start": "2095800",
    "end": "2102160"
  },
  {
    "text": "support all the cizer types and when you're doing a let's say you write a scaler code and write the application",
    "start": "2102160",
    "end": "2107800"
  },
  {
    "text": "that application should be registered in advance to the calization and you can enable this",
    "start": "2107800",
    "end": "2115640"
  },
  {
    "text": "configuration to use Kyo certier using configuration. set spark utilizer and",
    "start": "2115640",
    "end": "2121079"
  },
  {
    "text": "enable that one thing is spark doesn't like to",
    "start": "2121079",
    "end": "2126839"
  },
  {
    "start": "2124000",
    "end": "2124000"
  },
  {
    "text": "shuffle because it's expensive you're taking data from one machine pushing to other",
    "start": "2126839",
    "end": "2133599"
  },
  {
    "text": "machine and it requires a lot of dis iio you're spending Cycles there you're wasting time in the the time which can",
    "start": "2133599",
    "end": "2140400"
  },
  {
    "text": "be used for processing is spent on the dis IO rization and more Shuffle means more",
    "start": "2140400",
    "end": "2146400"
  },
  {
    "text": "Network IO your data is moving here and there and spilling to disk and as",
    "start": "2146400",
    "end": "2153200"
  },
  {
    "text": "there's more Shuffle more operations it increases the garbage collection and when garbage collection happens your job",
    "start": "2153200",
    "end": "2158800"
  },
  {
    "text": "Sly go slows so avoid shuffling and how do you",
    "start": "2158800",
    "end": "2164440"
  },
  {
    "text": "avoid shuffling let's go into that so for example when you're doing a group by aggregations you can go with",
    "start": "2164440",
    "end": "2171720"
  },
  {
    "text": "something like aggregate by key instead of writing your own aggregator so this is a optimized aggregator an example of",
    "start": "2171720",
    "end": "2179560"
  },
  {
    "text": "that is let's say you have an rdd called my rdd and you can call that for so I",
    "start": "2179560",
    "end": "2186319"
  },
  {
    "text": "have a map a key K and V so you can do an aggregation on those this is an example line for how you can use",
    "start": "2186319",
    "end": "2194119"
  },
  {
    "text": "that and apply filter on the data earlier in the stage so when you're processing huge amounts of data and",
    "start": "2194119",
    "end": "2200440"
  },
  {
    "text": "you're not going to actually touch everything try to filter it early in",
    "start": "2200440",
    "end": "2205520"
  },
  {
    "text": "ahead in the stages next is on parallelism and",
    "start": "2205520",
    "end": "2211960"
  },
  {
    "start": "2209000",
    "end": "2209000"
  },
  {
    "text": "partitions um spark as is a distributed parallel cluster I would say",
    "start": "2211960",
    "end": "2218880"
  },
  {
    "text": "when you're running a process the more parallelism it's much better for your jobs and the configuration when you",
    "start": "2218880",
    "end": "2224599"
  },
  {
    "text": "launch a cluster is set in spark default. and the parameter is spark",
    "start": "2224599",
    "end": "2230000"
  },
  {
    "text": "default parallelism so when you run a job um in your spark executive have the rdds the",
    "start": "2230000",
    "end": "2236440"
  },
  {
    "text": "rdds are further broken on they have multiple partitions and the partitions has a",
    "start": "2236440",
    "end": "2243960"
  },
  {
    "text": "correlation to the parallelism so let's let's say you have a really big rdd and",
    "start": "2243960",
    "end": "2250480"
  },
  {
    "text": "it has let's say 100 partitions on it spark knows that it has 100",
    "start": "2250480",
    "end": "2255599"
  },
  {
    "text": "partitions but how do you know it has Ed partitions so there's a function you call can use get number of partitions",
    "start": "2255599",
    "end": "2260880"
  },
  {
    "text": "that gives you that so when you do a operation between two rdds the first rdd and the second rdd",
    "start": "2260880",
    "end": "2268319"
  },
  {
    "text": "there's a operation happening on that and when the job runs in an Executor it uses those threads the CPU threads to",
    "start": "2268319",
    "end": "2275319"
  },
  {
    "text": "actually do the processing there so for short case could be if you have",
    "start": "2275319",
    "end": "2280880"
  },
  {
    "text": "more than 10K tasks it's good to coas and if you're not using all the",
    "start": "2280880",
    "end": "2287640"
  },
  {
    "text": "slots on your cluster repartitioning can increase the parallelism and you can leave two to",
    "start": "2287640",
    "end": "2294240"
  },
  {
    "text": "three tasks per CPU code on your cluster odly what that boils down to",
    "start": "2294240",
    "end": "2299400"
  },
  {
    "text": "is for one CPU code have two task two or three tasks there and this configuration",
    "start": "2299400",
    "end": "2306599"
  },
  {
    "text": "you can set in spar default. con persistence so art spark has started",
    "start": "2306599",
    "end": "2314800"
  },
  {
    "start": "2311000",
    "end": "2311000"
  },
  {
    "text": "with the concept of rdds and rdd is where the data is kept",
    "start": "2314800",
    "end": "2320480"
  },
  {
    "text": "in the memory caching so when you're doing a processing you're storing the data in memory and when you want to",
    "start": "2320480",
    "end": "2328079"
  },
  {
    "text": "let's say in a machine learning case you're reiterating over the data you're processing it many times so it's good to",
    "start": "2328079",
    "end": "2333680"
  },
  {
    "text": "cach there and cach it and persist it in memory so the method methods you can use for a cache a assist so for example uh",
    "start": "2333680",
    "end": "2342760"
  },
  {
    "text": "I'm doing a data wessing query and I need to send do some analysis so instead of",
    "start": "2342760",
    "end": "2348200"
  },
  {
    "text": "fetching the data every time from the disk bringing it in memory doing a query that's little intensive so for the first",
    "start": "2348200",
    "end": "2356079"
  },
  {
    "text": "time when you're fetching data bring it in memory cach it there now it's once it is cached any other iterations you're",
    "start": "2356079",
    "end": "2362880"
  },
  {
    "text": "doing let's say I'm doing a first group by everything is in memory gets it and for the further iterations the data is",
    "start": "2362880",
    "end": "2369400"
  },
  {
    "text": "fetched from the memory so it's much faster for you now which rdd which will pick and",
    "start": "2369400",
    "end": "2376760"
  },
  {
    "text": "what type of Storage level do you use for so for small rdds you can go with the storage lay memory only and for big",
    "start": "2376760",
    "end": "2384240"
  },
  {
    "text": "rdds which cannot fit in the memory you go with something memory only Sur it is",
    "start": "2384240",
    "end": "2389720"
  },
  {
    "text": "CPU intensive because excess data is been spilled with the desk",
    "start": "2389720",
    "end": "2396000"
  },
  {
    "text": "there first thing don't spill to dis I would say and if to have a really fall",
    "start": "2396000",
    "end": "2403680"
  },
  {
    "text": "tolerant cluster and your application should not stop in between I would recommend you go with replicated storage",
    "start": "2403680",
    "end": "2409760"
  },
  {
    "text": "for faster recovery so you can go with two x application 3x application of your memory",
    "start": "2409760",
    "end": "2416318"
  },
  {
    "text": "objects so let's go into the demo and I'll show you how I can run Zeppelin on",
    "start": "2416720",
    "end": "2422040"
  },
  {
    "text": "spark",
    "start": "2422040",
    "end": "2425040"
  },
  {
    "text": "is this",
    "start": "2435280",
    "end": "2438119"
  },
  {
    "text": "visible so I have an EMR cluster running Zeppelin and I have created a notebook",
    "start": "2440880",
    "end": "2447240"
  },
  {
    "text": "spark on EMR so in this example you see here we",
    "start": "2447240",
    "end": "2452920"
  },
  {
    "text": "are using the data set in P format which is present in this s bucket and we",
    "start": "2452920",
    "end": "2458880"
  },
  {
    "text": "create a data frame called flight data frame and once this data is read from",
    "start": "2458880",
    "end": "2464400"
  },
  {
    "text": "the S3 location this data is around 70 GB in",
    "start": "2464400",
    "end": "2469640"
  },
  {
    "text": "size once it is fish we'll register the table as a temporary and then we'll print the screa so it's very simple so I",
    "start": "2469640",
    "end": "2477119"
  },
  {
    "text": "just it's simple type you can type everything so I will just select this",
    "start": "2477119",
    "end": "2483960"
  },
  {
    "text": "space and run it so so it's fetching the data and it has",
    "start": "2483960",
    "end": "2489839"
  },
  {
    "text": "printed this and it took so this pocket file has information on the flight",
    "start": "2489839",
    "end": "2496920"
  },
  {
    "text": "status set and it's only for domestic US flights so it has feels like year",
    "start": "2496920",
    "end": "2502119"
  },
  {
    "text": "quarter month in many fields and where the flights are taking off from where the flights are landing the delay time",
    "start": "2502119",
    "end": "2508720"
  },
  {
    "text": "taxi time so the next one is let's say let's",
    "start": "2508720",
    "end": "2514040"
  },
  {
    "text": "do a group Buy on the unique carriers on that",
    "start": "2514040",
    "end": "2519240"
  },
  {
    "text": "so it fetches the data runs a query and gives you the result on the screen here so it's more interactive when you're",
    "start": "2523680",
    "end": "2530079"
  },
  {
    "text": "doing an analysis right now in the let's say if you're going with the shell mode you're doing a spark submit and doing the",
    "start": "2530079",
    "end": "2536640"
  },
  {
    "text": "processing it's it's not very friendly I would say so having a browser you can",
    "start": "2536640",
    "end": "2542800"
  },
  {
    "text": "just log in there you can share the notebooks Zeppelin is in that way so for",
    "start": "2542800",
    "end": "2550200"
  },
  {
    "text": "this example this gives you unique flight carriers and the do show operation gives only the top 20",
    "start": "2550200",
    "end": "2557400"
  },
  {
    "text": "records and Zeppelin in that terminal you can write a Scala code or you can even do a squal query so looking the",
    "start": "2557400",
    "end": "2564960"
  },
  {
    "text": "example here we have registered this table as a flight so it is cached as a temporary",
    "start": "2564960",
    "end": "2573760"
  },
  {
    "text": "table and so let doing a count on this table",
    "start": "2573760",
    "end": "2578960"
  },
  {
    "text": "select count Star as total records from the flights so it has 162 million",
    "start": "2578960",
    "end": "2585520"
  },
  {
    "text": "records and the entire count start took 3 seconds",
    "start": "2585520",
    "end": "2590720"
  },
  {
    "text": "here and similarly uh doing other type of analysis on this so you can have you",
    "start": "2590960",
    "end": "2597559"
  },
  {
    "text": "can put a bar chart you can put pie charts charts in this so this one shows",
    "start": "2597559",
    "end": "2602760"
  },
  {
    "text": "you the top 10 airports with the most departure since 200 and and similarly the top 10 Busy",
    "start": "2602760",
    "end": "2608680"
  },
  {
    "text": "airports in December of 2015 so it's easier to interact Atlanta so many flights have taken off from there and",
    "start": "2608680",
    "end": "2615760"
  },
  {
    "text": "these the busy airports and similarly on this let's say",
    "start": "2615760",
    "end": "2622480"
  },
  {
    "text": "I want to find all the flights living Seattle between 2004 and 2014 this is a simple SQL query which I",
    "start": "2622480",
    "end": "2629599"
  },
  {
    "text": "have on the table it's a simple count and group",
    "start": "2629599",
    "end": "2635160"
  },
  {
    "text": "by so the runs and the IT plots the results",
    "start": "2635720",
    "end": "2641720"
  },
  {
    "text": "here okay done similarly you can have the configuration on number of flights",
    "start": "2644280",
    "end": "2650599"
  },
  {
    "text": "leaving sat between 2014 and 2014 this gives you the analysis and you",
    "start": "2650599",
    "end": "2656559"
  },
  {
    "text": "can easily do analysis on your data sets in a very quick manner interactive save",
    "start": "2656559",
    "end": "2664280"
  },
  {
    "text": "it and see you don't have to reprocess data again because this thing is saved",
    "start": "2664280",
    "end": "2669920"
  },
  {
    "text": "here that's",
    "start": "2669920",
    "end": "2672880"
  },
  {
    "text": "it yep so we're good yeah I guess uh",
    "start": "2680760",
    "end": "2687680"
  },
  {
    "text": "thanks um thanks everybody for coming out um we're uh we're we're uh happy to take some",
    "start": "2687680",
    "end": "2694720"
  },
  {
    "text": "questions we'll be down here and we'd love to chat with you guys learn more about your use case and answer any more questions so thanks have a good rest of",
    "start": "2694720",
    "end": "2700559"
  },
  {
    "text": "the conference and a good night thank you",
    "start": "2700559",
    "end": "2704160"
  }
]