[
  {
    "start": "0",
    "end": "104000"
  },
  {
    "text": "thanks for coming I am John hitching ham director of performance engineering at",
    "start": "560",
    "end": "7649"
  },
  {
    "text": "FINRA and I'm here today to share about FINRA's scalable analytics architecture",
    "start": "7649",
    "end": "16198"
  },
  {
    "text": "on s3 and go into how we've recently been able to extend that architecture to",
    "start": "16199",
    "end": "23550"
  },
  {
    "text": "include HBase so I guess first a little bit about FINRA FINRA is the Financial",
    "start": "23550",
    "end": "31019"
  },
  {
    "text": "Industry Regulatory Authority we're responsible for running for monitoring",
    "start": "31019",
    "end": "36719"
  },
  {
    "text": "for processing over 99% of all equities trades in the United States 65% of all",
    "start": "36719",
    "end": "43260"
  },
  {
    "text": "options trades and we look through that looking for fraud market manipulation",
    "start": "43260",
    "end": "48360"
  },
  {
    "text": "insider trading and abuse we process large volumes of data over 75 billion",
    "start": "48360",
    "end": "55800"
  },
  {
    "text": "records per day six terabytes of data per day we also oversee over 3900",
    "start": "55800",
    "end": "62750"
  },
  {
    "text": "broker-dealer firms in the US along with 60 of 640,000 broker-dealer individuals",
    "start": "62750",
    "end": "69770"
  },
  {
    "text": "and our technology team you know supports the regulatory functions of FINRA through through innovative",
    "start": "69770",
    "end": "76530"
  },
  {
    "text": "technology solutions such as we're gonna cover here today so in terms of things",
    "start": "76530",
    "end": "82080"
  },
  {
    "text": "to expect from the session basically we're going to go over FINRA's scalable cloud analytics architecture on s3 and",
    "start": "82080",
    "end": "88409"
  },
  {
    "text": "also talk about you know how we've recently been able to take advantage of the new features HBase on s3 with the",
    "start": "88409",
    "end": "95040"
  },
  {
    "text": "EMR to extend it to run HBase workloads using s3 as the data Lake so maybe a",
    "start": "95040",
    "end": "105810"
  },
  {
    "start": "104000",
    "end": "104000"
  },
  {
    "text": "good place to start is where FINRA was several years ago before we went to the cloud I mentioned before the the data",
    "start": "105810",
    "end": "113970"
  },
  {
    "text": "volumes that we were dealing with you know we were dealing with those as part of our on premises implementation that",
    "start": "113970",
    "end": "119399"
  },
  {
    "text": "we had before going to the cloud we were fairly early adopters and heavy users",
    "start": "119399",
    "end": "124969"
  },
  {
    "text": "data warehouse appliances fairly expensive data warehouse appliances",
    "start": "124969",
    "end": "129979"
  },
  {
    "text": "and you know we were able to use them to get the job done but not really done",
    "start": "129979",
    "end": "137330"
  },
  {
    "text": "well we were having a lot of struggles that we were constantly working with you know in our in our business like one of",
    "start": "137330",
    "end": "144200"
  },
  {
    "text": "the big challenges that we have is fluctuating demand right if there's a busy day on the stock market the volumes",
    "start": "144200",
    "end": "150019"
  },
  {
    "text": "could be two or three times normal you never really know ahead of time when that's going to happen how do you size",
    "start": "150019",
    "end": "155930"
  },
  {
    "text": "your capacity to handle that fluctuating volume additionally you know sometimes",
    "start": "155930",
    "end": "162410"
  },
  {
    "text": "we have to do data Corrections we have to do reprocessing sometimes users want to come along and they want to do",
    "start": "162410",
    "end": "168319"
  },
  {
    "text": "special analytics on our cap at our you know to learn new insights about our",
    "start": "168319",
    "end": "174530"
  },
  {
    "text": "data and again with with the fixed capacity situation that we had where we had storage and compute combined you",
    "start": "174530",
    "end": "182840"
  },
  {
    "text": "know it's very difficult to kind of accommodate those scenarios your choices are really kind of sized for the",
    "start": "182840",
    "end": "188000"
  },
  {
    "text": "absolute you know theoretical worst peak that you could do in over-provision which is very expensive or you know kind",
    "start": "188000",
    "end": "196160"
  },
  {
    "text": "of size for the kind of normal case which is cost-effective but then that",
    "start": "196160",
    "end": "201709"
  },
  {
    "text": "led to a lot of other challenges too right we found out that we're constantly spending a lot of time kind of managing",
    "start": "201709",
    "end": "207709"
  },
  {
    "text": "the data right users just wanted answers to questions they wanted to just run their analytics and everything we did",
    "start": "207709",
    "end": "214609"
  },
  {
    "text": "sort of turned into a little mini IT project to be able to just get the data ready in a position where they could run",
    "start": "214609",
    "end": "221150"
  },
  {
    "text": "their analytics you know often you know if the market volumes went high you know",
    "start": "221150",
    "end": "227239"
  },
  {
    "text": "we'd have to process it to meet our SL A's you know but in order to do that",
    "start": "227239",
    "end": "232579"
  },
  {
    "text": "we'd have to effectively kick other people off the system so we can do a negotiation of who to kick off the system you know to let the market you",
    "start": "232579",
    "end": "239150"
  },
  {
    "text": "know volumes get processed and meet our SL A's and a lot of time talking to",
    "start": "239150",
    "end": "244190"
  },
  {
    "text": "people a lot of time walking around kind of negotiating you know how to live on this fixed infrastructure right so what",
    "start": "244190",
    "end": "251000"
  },
  {
    "text": "we asked ourselves was you know is there is there a better way to do this is there something that we could do in the cloud the way we could give us scalable",
    "start": "251000",
    "end": "257630"
  },
  {
    "text": "and flexible flexible architecture you know to kind of solve this problem so we",
    "start": "257630",
    "end": "262729"
  },
  {
    "text": "really kind of asked ourselves you know what what would be the ideal analytics environment and I think you know it's",
    "start": "262729",
    "end": "268760"
  },
  {
    "start": "263000",
    "end": "263000"
  },
  {
    "text": "one where we would never run out of space you wouldn't have me to be constantly moving things to tape you",
    "start": "268760",
    "end": "274520"
  },
  {
    "text": "know off of our mainline storage you know we'd have the capacity that we talked about the compute capacity to",
    "start": "274520",
    "end": "279950"
  },
  {
    "text": "spin-up and spin-down is needed based on the fluctuating workloads again back to the capacity",
    "start": "279950",
    "end": "287270"
  },
  {
    "text": "question right not pay for oversize capacity you know we want to be able to pay only where for what we use",
    "start": "287270",
    "end": "293780"
  },
  {
    "text": "you know achieve isolation of workloads again when everyone was using our data warehouse appliance is they were a",
    "start": "293780",
    "end": "299599"
  },
  {
    "text": "precious resource right there were so we had to do a lot of sharing and we're running constantly into contention",
    "start": "299599",
    "end": "305120"
  },
  {
    "text": "between read workloads write workloads loading the data you know if things weren't long on the batch window people",
    "start": "305120",
    "end": "311599"
  },
  {
    "text": "would again be conflicting you know if there was a problem someone's query was you know not",
    "start": "311599",
    "end": "317360"
  },
  {
    "text": "behaving well you really had to kind of figure out how to tame it down and so it wouldn't impact the other users of the",
    "start": "317360",
    "end": "323060"
  },
  {
    "text": "shared system so was the way we could in the cloud we could achieve workload isolation to among these different",
    "start": "323060",
    "end": "328280"
  },
  {
    "text": "workloads and why do this right because I think the thing that we wanted to do was find a way again to have IT not",
    "start": "328280",
    "end": "335570"
  },
  {
    "text": "focused in spending so many resources just on moving data managing data we",
    "start": "335570",
    "end": "341210"
  },
  {
    "text": "wanted to be able to really just let the business do the analytics unencumbered [Music]",
    "start": "341210",
    "end": "347260"
  },
  {
    "text": "so about three and a half years ago we started our journey to the cloud and",
    "start": "347260",
    "end": "353360"
  },
  {
    "text": "it's part of that effort we spend a lot of time thinking about kind of what we wanted our architecture to look like on",
    "start": "353360",
    "end": "358760"
  },
  {
    "text": "the cloud and this is the architecture here that that we use today for probably",
    "start": "358760",
    "end": "365090"
  },
  {
    "text": "over 90% of our processing that we do in the cloud so maybe I can spend a little",
    "start": "365090",
    "end": "370159"
  },
  {
    "text": "time going through it and kind of kind of walk in and over it so the key to it",
    "start": "370159",
    "end": "376250"
  },
  {
    "text": "you know as you can see is we have s3 as our main storage layer scalable we",
    "start": "376250",
    "end": "382490"
  },
  {
    "text": "can scale it out however we need it we have a compute layer primarily Amazon EMR that we run to do our actual queries",
    "start": "382490",
    "end": "390060"
  },
  {
    "text": "and analytics against the data stored on s3 and then we also have our data",
    "start": "390060",
    "end": "395400"
  },
  {
    "text": "catalog system that keeps track of all the data on s3 and we can in integrates",
    "start": "395400",
    "end": "401310"
  },
  {
    "text": "in as part of an orchestration system with their analytics workloads so we can do tracking and processing of the data",
    "start": "401310",
    "end": "407430"
  },
  {
    "text": "we can know where the data is what data has been processed whose processed it we keep track of all this out there in the",
    "start": "407430",
    "end": "415230"
  },
  {
    "text": "cloud so as a result of being able to",
    "start": "415230",
    "end": "420990"
  },
  {
    "text": "move to this architecture in the cloud as we can see here you know storage where we had you know the fixed limits",
    "start": "420990",
    "end": "427740"
  },
  {
    "text": "before our old data warehouse clusters we're now able to scale out on s3 as our you know our Nearline storage and the",
    "start": "427740",
    "end": "433890"
  },
  {
    "text": "cloud fluctuating is needed accommodating growth and having all that data available and accessible so we can",
    "start": "433890",
    "end": "441630"
  },
  {
    "text": "run analytics queries on it whenever we want on the compute side we have the",
    "start": "441630",
    "end": "449280"
  },
  {
    "text": "ability now through using EMR to scale up and down based on fluctuating demand",
    "start": "449280",
    "end": "455280"
  },
  {
    "text": "so as you can see here pretty severe changes you know hour to hour day to day",
    "start": "455280",
    "end": "461370"
  },
  {
    "text": "as we go through and we do our processing sometimes we run over 10,000 nodes ec2 nodes of compute at one time",
    "start": "461370",
    "end": "469680"
  },
  {
    "text": "on EMR other times we're down to just you know a few dozen so this really",
    "start": "469680",
    "end": "474870"
  },
  {
    "text": "gives us the flexibility as I said before as market volumes change as we need to do special runs of analytics we",
    "start": "474870",
    "end": "482250"
  },
  {
    "text": "have the ability to just spin capacity up and run with it and an inch tab at",
    "start": "482250",
    "end": "488580"
  },
  {
    "text": "work so again we've achieved the flexibility on the store this flexibility and the scalability on the",
    "start": "488580",
    "end": "494040"
  },
  {
    "text": "storage area we've achieved flexibility and scalability on the compute area so",
    "start": "494040",
    "end": "502620"
  },
  {
    "start": "501000",
    "end": "501000"
  },
  {
    "text": "in terms of our approach that we take there's a couple key things that we we",
    "start": "502620",
    "end": "508200"
  },
  {
    "text": "think about as part of our architecture as I said before one of the key things that we do is we register and we track",
    "start": "508200",
    "end": "514770"
  },
  {
    "text": "everything in our our data catalog which we'll talk about in a few minutes we keep you know one of",
    "start": "514770",
    "end": "521680"
  },
  {
    "text": "the things we do is we keep what we call sort of our archived copy of data on s3 we'll get into that more details later",
    "start": "521680",
    "end": "528430"
  },
  {
    "text": "but this is sort of the golden copy of the data that we keep out there on s3 and then we make potential derivations",
    "start": "528430",
    "end": "534670"
  },
  {
    "text": "off of if we need to for performance reasons the other key thing of course",
    "start": "534670",
    "end": "540279"
  },
  {
    "text": "being a financial regulator is protecting the data so we take many steps to make sure that our data is",
    "start": "540279",
    "end": "546910"
  },
  {
    "text": "secure and safe in the cloud again one",
    "start": "546910",
    "end": "552160"
  },
  {
    "text": "of the things that we do very heavily is we try to run our processing directly",
    "start": "552160",
    "end": "558519"
  },
  {
    "text": "against the data on s3 not load the data into our processing areas and process it but actually just keep the data on s3",
    "start": "558519",
    "end": "565360"
  },
  {
    "text": "and process directly against it and we'll talk about you know kind of some of the ways that we do that in a little",
    "start": "565360",
    "end": "570490"
  },
  {
    "text": "bit and then as I said before you know a lot of times we can run according to the",
    "start": "570490",
    "end": "576220"
  },
  {
    "text": "that main copy of the data but where we need to we can make additional you know copies tuned for perform and query many",
    "start": "576220",
    "end": "583060"
  },
  {
    "text": "cases still keeping the data out there on s3 so you know as I said earlier one",
    "start": "583060",
    "end": "592870"
  },
  {
    "text": "of the real key things for our architecture is being able to track the data that we have out there on the cloud",
    "start": "592870",
    "end": "599130"
  },
  {
    "text": "we have over 40 million object we call them objects but basically you can think",
    "start": "599130",
    "end": "605410"
  },
  {
    "text": "of an object as essentially as a file of data you know a CSV delimited file of",
    "start": "605410",
    "end": "611500"
  },
  {
    "text": "data that represents like a table logically so we have these stored on an s3 and we need to know who put it there",
    "start": "611500",
    "end": "618510"
  },
  {
    "text": "you know how do I access it you know who who's consuming it who's producing it",
    "start": "618510",
    "end": "623800"
  },
  {
    "text": "you know all these things around data management so you know very early on as",
    "start": "623800",
    "end": "631779"
  },
  {
    "text": "part of our effort to go to the cloud we realized that there's an ability to track all this information out there on",
    "start": "631779",
    "end": "638769"
  },
  {
    "text": "the cloud was going to be really key right and again going back several years there really weren't a lot of solutions",
    "start": "638769",
    "end": "645670"
  },
  {
    "text": "out there in the market where we found things that were just starting to emerge a lot of",
    "start": "645670",
    "end": "652040"
  },
  {
    "text": "them were tied to you know specific ecosystems like the Hadoop ecosystem or they were even tied to a particular",
    "start": "652040",
    "end": "658430"
  },
  {
    "text": "vendor stack of Hadoop so so we made the decision early on to come up with their",
    "start": "658430",
    "end": "664010"
  },
  {
    "text": "own data catalog and management service we've you know since named it heard it's",
    "start": "664010",
    "end": "670550"
  },
  {
    "text": "available on github is open source and so really this is the tool that lets us",
    "start": "670550",
    "end": "676040"
  },
  {
    "text": "you know track business and technical metadata associated with with everything that we have out there on the cloud you",
    "start": "676040",
    "end": "683240"
  },
  {
    "text": "know we have the abilities for again for producers to register the data consumers to register their use of the data we can",
    "start": "683240",
    "end": "689990"
  },
  {
    "text": "provide technical metadata to use to access and query the the data that sits",
    "start": "689990",
    "end": "695240"
  },
  {
    "text": "out there on s3 along with additional business data that that we may choose to store with the the objects out there so",
    "start": "695240",
    "end": "706459"
  },
  {
    "text": "you know kind of going through sort of how we we handle that you know kind of gold copy of every every object or every",
    "start": "706459",
    "end": "712550"
  },
  {
    "text": "file table call it what you will that we want to put out there on s3 you know we store it on s3 why because you know we",
    "start": "712550",
    "end": "720170"
  },
  {
    "text": "have eleven nines availability of durability on s3 we have multi Daisy",
    "start": "720170",
    "end": "725630"
  },
  {
    "text": "availability so it's replicated across multiple data centers for redundancy and resiliency as I said you know for every",
    "start": "725630",
    "end": "734029"
  },
  {
    "text": "object that we put out there in the cloud we tracked it in our data management system heard you know the",
    "start": "734029",
    "end": "740240"
  },
  {
    "text": "storage location the partition keys and we'll talk about partition keys in a minute the version you know logical",
    "start": "740240",
    "end": "747529"
  },
  {
    "text": "versions of the data that we store out there if there's changes to the data format over time or there's data",
    "start": "747529",
    "end": "753230"
  },
  {
    "text": "Corrections the data over time and then again as we'll see a little bit later the the schema that the DDL that you",
    "start": "753230",
    "end": "759709"
  },
  {
    "text": "would need to directly access the data you know and when queries against it using hive or spark or presto while",
    "start": "759709",
    "end": "766040"
  },
  {
    "text": "keeping it out in s3 we keep version history you know in our data catalog",
    "start": "766040",
    "end": "773000"
  },
  {
    "text": "system as I mentioned this is distinct from s3 the bucket versioning so this is again",
    "start": "773000",
    "end": "779750"
  },
  {
    "text": "logical versioning that we do against the data is we make changes to it we do also where we you know take advantage of",
    "start": "779750",
    "end": "786230"
  },
  {
    "text": "bucket versioning and s3 sort of if there's some accidental change the data or something happens we have an older",
    "start": "786230",
    "end": "791750"
  },
  {
    "text": "copy we can go back to and recover from if we need to as I said this what we",
    "start": "791750",
    "end": "797930"
  },
  {
    "text": "call sort of our archive copy of the data we store it in text format delimited text format the goal being as",
    "start": "797930",
    "end": "805130"
  },
  {
    "text": "you know different formats and things change over the years this is kind of like the lowest common denominator",
    "start": "805130",
    "end": "811220"
  },
  {
    "text": "format that all tools can access you know over time so we keep it in that set",
    "start": "811220",
    "end": "817070"
  },
  {
    "text": "so that is you know tools and things change we still have the ability to access and go back and retrieve and read",
    "start": "817070",
    "end": "822650"
  },
  {
    "text": "and view the data that we have out there you know we compress all the data to",
    "start": "822650",
    "end": "828410"
  },
  {
    "text": "save space we chose to use B zip to compression because it's splittable and",
    "start": "828410",
    "end": "833900"
  },
  {
    "text": "MapReduce so again all of our or what we call our archive copies of the data are you know compressed using B sub-2",
    "start": "833900",
    "end": "843130"
  },
  {
    "start": "844000",
    "end": "844000"
  },
  {
    "text": "you know as I said protecting the data is very important to us so all the data",
    "start": "846230",
    "end": "851839"
  },
  {
    "text": "that we have on s3 is encrypted depending on the the you know the",
    "start": "851839",
    "end": "857959"
  },
  {
    "text": "sensitivity of the data we either use SSE encryption or we use Amazon's kms",
    "start": "857959",
    "end": "865040"
  },
  {
    "text": "encryption using their kms service for",
    "start": "865040",
    "end": "870050"
  },
  {
    "text": "actual processing that we do on EMR I know again we keep most of our data on",
    "start": "870050",
    "end": "875269"
  },
  {
    "text": "s3 but temporarily we may move it to EMR in order to do processing against it we",
    "start": "875269",
    "end": "881060"
  },
  {
    "text": "use a Luxton ssin of the file system we had our own homegrown one that we did originally but now that's a",
    "start": "881060",
    "end": "887450"
  },
  {
    "text": "product that you know is a feature that support is part of the newer versions of EMR so we're gradually migrating over to",
    "start": "887450",
    "end": "893360"
  },
  {
    "text": "use that encryption of the the data in transit using HTTPS and SSL so data into",
    "start": "893360",
    "end": "901279"
  },
  {
    "text": "the cluster and around through the cloud I mentioned before we use bucket",
    "start": "901279",
    "end": "906290"
  },
  {
    "text": "versioning so that if there's some sort of issue with the you know corruption of the data or some sort of problem we can",
    "start": "906290",
    "end": "912350"
  },
  {
    "text": "go back to an older version and we have a copy there another thing that we do is",
    "start": "912350",
    "end": "917779"
  },
  {
    "text": "we backup the data you know sort of this is a break glass in case of emergency",
    "start": "917779",
    "end": "923720"
  },
  {
    "text": "kind of situation we replicated to a completely separate account in a completely separate region and we",
    "start": "923720",
    "end": "930529"
  },
  {
    "text": "control access to that very tightly so that you know if there's some sort of you know catastrophic problem and where",
    "start": "930529",
    "end": "936500"
  },
  {
    "text": "our main account our main account is with the main set of data we do have a copy that we can go back and pull back",
    "start": "936500",
    "end": "942770"
  },
  {
    "text": "from you know from another region if we need to access it so I said earlier I've",
    "start": "942770",
    "end": "950720"
  },
  {
    "text": "talked a little bit about sort of this idea of storing technical metadata DDL so that you can actually query the data",
    "start": "950720",
    "end": "957699"
  },
  {
    "text": "so one of the things that we do and might be a little hard to see up here but what we do is we generally and all",
    "start": "957699",
    "end": "963769"
  },
  {
    "text": "of our processing that we run on EMR using hive using spark using presto we",
    "start": "963769",
    "end": "970420"
  },
  {
    "text": "defined external tables so we have the data set sitting out there on s3 we",
    "start": "970420",
    "end": "975529"
  },
  {
    "text": "define them as external tables and then we can bring up any Mr cluster and we can run queries we can",
    "start": "975529",
    "end": "982070"
  },
  {
    "text": "process against the data leaving it out there on s3 there's a lot of benefits",
    "start": "982070",
    "end": "987560"
  },
  {
    "text": "for us to do this because we can then basically our clusters we don't have to spend all the time it takes to load the",
    "start": "987560",
    "end": "993589"
  },
  {
    "text": "data into the cluster before we can do processing you know if we want to do basically queries against years and",
    "start": "993589",
    "end": "999709"
  },
  {
    "text": "years worth of data you know it's not practical to load all that data in the cluster you know just to do like some",
    "start": "999709",
    "end": "1005050"
  },
  {
    "text": "queries against it we can just start processing against all the data that's out there on s3 the other advantages of",
    "start": "1005050",
    "end": "1011110"
  },
  {
    "text": "this - you know we do checkpoint as part of our batch workloads as we go along so",
    "start": "1011110",
    "end": "1016120"
  },
  {
    "text": "that lets us run a lot of things on spot so if we lose the cluster you know we don't have to reload all the data back",
    "start": "1016120",
    "end": "1022449"
  },
  {
    "text": "in in order to start processing again we have the data out there s3 we just bring",
    "start": "1022449",
    "end": "1027850"
  },
  {
    "text": "up a new cluster and start processing again where we left off",
    "start": "1027850",
    "end": "1033120"
  },
  {
    "text": "you know as I said before we talked about our data catalog system herd where we keep all of our metadata you know the",
    "start": "1035439",
    "end": "1043329"
  },
  {
    "text": "the main copy of our metadata in a couple cases we've done some optimizations on that one thing that",
    "start": "1043329",
    "end": "1049419"
  },
  {
    "text": "we've done is we've created you know a persistent shared hive meta store keeping the latest you know the metadata",
    "start": "1049419",
    "end": "1056770"
  },
  {
    "text": "for the latest version of the data and we do that so that when we bring up clusters hive spark presto you know",
    "start": "1056770",
    "end": "1064510"
  },
  {
    "text": "instead of having to have custom handlers that would talk to the rest service API that our herd data catalog",
    "start": "1064510",
    "end": "1070059"
  },
  {
    "text": "represents they can just speak natively they all speak you know hive metadata you know syntax to be able to understand",
    "start": "1070059",
    "end": "1077110"
  },
  {
    "text": "and tara gated and understand what's there we can just bring them up and instead of having to define all these",
    "start": "1077110",
    "end": "1082630"
  },
  {
    "text": "things up front before we can actually start execution on the cluster all the metadata that's there that we need to query against is up it's available you",
    "start": "1082630",
    "end": "1089799"
  },
  {
    "text": "know and it's ready to go and there's very you know low cost to that to actually you know hold it all we keep it in a you know a small RDS",
    "start": "1089799",
    "end": "1096669"
  },
  {
    "text": "database and so at that point you know we can bring clusters up and start processing against the data on s3 you",
    "start": "1096669",
    "end": "1103540"
  },
  {
    "text": "know instantly and we have separate schemas in there that we do to support you know different versions of hive is",
    "start": "1103540",
    "end": "1110260"
  },
  {
    "text": "sometimes the the metadata changes and presto originally because because that was a",
    "start": "1110260",
    "end": "1115349"
  },
  {
    "text": "little bit different too so I said before that you know we have all these",
    "start": "1115349",
    "end": "1122549"
  },
  {
    "text": "files out there in ASCII format these up out there in a lot of cases you know",
    "start": "1122549",
    "end": "1129089"
  },
  {
    "text": "where performance really isn't a big deal we can just go and process against those directly mount him up as an",
    "start": "1129089",
    "end": "1134789"
  },
  {
    "text": "external table you know spin up a closet you know spin up a cluster define it is extreme modify an external table in the",
    "start": "1134789",
    "end": "1140309"
  },
  {
    "text": "meta store spin up a cluster and go process against it in some cases though we you know for better performance will",
    "start": "1140309",
    "end": "1147659"
  },
  {
    "text": "essentially make a more performant performant copy of the data so we'll go through and we'll you know basically",
    "start": "1147659",
    "end": "1153869"
  },
  {
    "text": "change the partition keys potentially or the short order to better fit the the query patterns that we want to run",
    "start": "1153869",
    "end": "1159749"
  },
  {
    "text": "against it again in most of the use cases we're still keeping the data out there on s3 we always try to do that",
    "start": "1159749",
    "end": "1165599"
  },
  {
    "text": "wherever possible we may also look to you know change kind of the storage format into one that's you know more",
    "start": "1165599",
    "end": "1172829"
  },
  {
    "text": "efficient for for a lot of the queries that we do so in our case we've settled on using the orc format to do that you",
    "start": "1172829",
    "end": "1179309"
  },
  {
    "text": "know you could use part k2 back at the time you know several years ago we made the decision at the time to go with orc",
    "start": "1179309",
    "end": "1185450"
  },
  {
    "text": "and so you know that's what we use today but basically that gives us a lot more",
    "start": "1185450",
    "end": "1191279"
  },
  {
    "text": "efficient you know query response time for a lot of the queries that that we do",
    "start": "1191279",
    "end": "1197539"
  },
  {
    "text": "yeah so just kind of an example on that right here's some here's you know some",
    "start": "1197749",
    "end": "1203309"
  },
  {
    "text": "test queries that we've run so this is going against over two billion rows of data not partitioned and you can see the",
    "start": "1203309",
    "end": "1210479"
  },
  {
    "text": "the difference between you know if you run these queries against a text file format in B zip and the response times",
    "start": "1210479",
    "end": "1217139"
  },
  {
    "text": "that you see there and having it in the orc format so again you know you know if",
    "start": "1217139",
    "end": "1222959"
  },
  {
    "text": "you're not doing complete full scans pulling all you know doing a complete copy of all the data there's a lot of",
    "start": "1222959",
    "end": "1228929"
  },
  {
    "text": "fishin C you can game by by switching to a format like work or you know parquet you know for performing queries again we",
    "start": "1228929",
    "end": "1236969"
  },
  {
    "text": "choose to make that copy for performant query so it's almost kind of like part of a you know kind of a data Mart you",
    "start": "1236969",
    "end": "1242459"
  },
  {
    "text": "know that we the main data we leave the main data still and ASCII that's our golden copy format that we can always go back to if",
    "start": "1242459",
    "end": "1249020"
  },
  {
    "text": "we need to so you know kind of again",
    "start": "1249020",
    "end": "1257450"
  },
  {
    "start": "1254000",
    "end": "1254000"
  },
  {
    "text": "sort of benefits of this architecture right if we you know use s3 as our storage layer we have you know security",
    "start": "1257450",
    "end": "1265279"
  },
  {
    "text": "and cost efficiency by storing it on s3 you know it's a very cost effective way",
    "start": "1265279",
    "end": "1270590"
  },
  {
    "text": "to destroy your data and it scales and we you know just don't have to think about it it's just there we don't have",
    "start": "1270590",
    "end": "1277039"
  },
  {
    "text": "to manage the data we don't have to worry you know the space or we don't run into those problems we ran to on the",
    "start": "1277039",
    "end": "1282559"
  },
  {
    "text": "on-premise environment that we were on before you know as you said before keeping the data on s3 lets us you know",
    "start": "1282559",
    "end": "1289490"
  },
  {
    "text": "bring our compute layer to whatever size we need to to run against s3 so that's a",
    "start": "1289490",
    "end": "1294559"
  },
  {
    "text": "question of not just you know an individual cluster you know size to whatever we need we can run multiple",
    "start": "1294559",
    "end": "1299870"
  },
  {
    "text": "clusters you know multiple analytic workloads reading off the same data set at each time so in some cases we can",
    "start": "1299870",
    "end": "1306860"
  },
  {
    "text": "have you know dozens of EMR clusters going off the same set of data on s3 doing their processing in parallel so it gives us",
    "start": "1306860",
    "end": "1313250"
  },
  {
    "text": "parallelization at the compute layer too you know as we said avoid copying of",
    "start": "1313250",
    "end": "1319340"
  },
  {
    "text": "large data sets you know if I want to go through a huge set of data but only extract a few things you know we can go",
    "start": "1319340",
    "end": "1325760"
  },
  {
    "text": "ahead and we can do that by by keeping the data on s3 and as I said before also it lets us use you know a heavy use of",
    "start": "1325760",
    "end": "1332179"
  },
  {
    "text": "spot as part of our is part of our processing so you know kind of going",
    "start": "1332179",
    "end": "1339350"
  },
  {
    "text": "back to the beginning on this you know what does this do by really moving to this architecture in the cloud",
    "start": "1339350",
    "end": "1345860"
  },
  {
    "text": "we really aren't focused anymore on you know the management of the data the moving the data in and out the the track",
    "start": "1345860",
    "end": "1353330"
  },
  {
    "text": "the manually tracking where it goes the fighting for the limited batch windows that we had is part of our on premises",
    "start": "1353330",
    "end": "1358820"
  },
  {
    "text": "solution before with the data warehouse appliances so really you know the feedback from from some of our business",
    "start": "1358820",
    "end": "1365360"
  },
  {
    "text": "users on the system is that you know it lets them really kind of focus on just",
    "start": "1365360",
    "end": "1370610"
  },
  {
    "text": "getting their job done working with the analytics so you know if they want to run queries against the",
    "start": "1370610",
    "end": "1376169"
  },
  {
    "text": "seven years worth of data they can the data is on s3 we have career clusters that are up and running there the data",
    "start": "1376169",
    "end": "1383009"
  },
  {
    "text": "is all referenced in our shared metadata shared meta story that we talked about so they just come on in",
    "start": "1383009",
    "end": "1388139"
  },
  {
    "text": "boom you're off and running against you know up to seven years worth of data instantly we're no longer contending you",
    "start": "1388139",
    "end": "1394799"
  },
  {
    "text": "know sort of processing the conflicts between the the workloads we can spin up multiple individual clusters if we run",
    "start": "1394799",
    "end": "1402029"
  },
  {
    "text": "out of you know capacity and we need extra capacity if the business comes to us and they want to do some sort of new",
    "start": "1402029",
    "end": "1407460"
  },
  {
    "text": "special analysis you know it can just spin up a cluster and start running it they don't need to again fight for for",
    "start": "1407460",
    "end": "1414149"
  },
  {
    "text": "limited batch windows and we can access you know again all this data as it grows out there on s3 so you know the",
    "start": "1414149",
    "end": "1424259"
  },
  {
    "text": "architecture that I just described has worked really well for us but it's really been tied so far to the tools of",
    "start": "1424259",
    "end": "1431519"
  },
  {
    "text": "you know hive presto spark and what we asked ourselves was was there a way that",
    "start": "1431519",
    "end": "1436739"
  },
  {
    "text": "we could extend the benefits of that architecture to cover our HBase applications so we have a couple HBase",
    "start": "1436739",
    "end": "1444570"
  },
  {
    "text": "applications that we run in the cloud and we wanted to say you know historically we hadn't been able to to",
    "start": "1444570",
    "end": "1450960"
  },
  {
    "text": "leverage that same architecture with those and was there a way that we could extend this to do that and so I want to",
    "start": "1450960",
    "end": "1459779"
  },
  {
    "start": "1458000",
    "end": "1458000"
  },
  {
    "text": "talk a little bit about one of our applications that uses HBase it's called",
    "start": "1459779",
    "end": "1465119"
  },
  {
    "text": "are fast order lifecycle application or we call it fol ax and basically what it",
    "start": "1465119",
    "end": "1470129"
  },
  {
    "text": "does is you know it attracts market events in the in the market so if you have an order in a series of trades that",
    "start": "1470129",
    "end": "1476519"
  },
  {
    "text": "result from that they really become part of one chain of events that we call an order lifecycle and so when our analysts",
    "start": "1476519",
    "end": "1483119"
  },
  {
    "text": "need to do on a daily basis is given like a particular event they need to understand what chain is that event part",
    "start": "1483119",
    "end": "1490409"
  },
  {
    "text": "of and be able to pull back the full set of events that they're really part of that chain and then display the results",
    "start": "1490409",
    "end": "1497009"
  },
  {
    "text": "so that they can see them you know and do their analysis on it it's a bit challenging because again you know",
    "start": "1497009",
    "end": "1503159"
  },
  {
    "text": "there's there's trillions of events out there and we're adding billions of day the database so you know we came up with",
    "start": "1503159",
    "end": "1511370"
  },
  {
    "text": "it with an approach to to query the data to get the results back what we do is a series of basically you know three",
    "start": "1511370",
    "end": "1517700"
  },
  {
    "text": "different lookups so given the event that we want to look for you know for example in this case event number three",
    "start": "1517700",
    "end": "1522889"
  },
  {
    "text": "we go find out we do a lookup to find what group you know or set is it is it a",
    "start": "1522889",
    "end": "1528860"
  },
  {
    "text": "member of right so it's a member of set B so we go back we do another retrieval",
    "start": "1528860",
    "end": "1533990"
  },
  {
    "text": "to find out what are you know what is what's the set members of B once we have",
    "start": "1533990",
    "end": "1539210"
  },
  {
    "text": "those we can go about and parallel and go do basically lookups to retrieve back all the members of the set then and get",
    "start": "1539210",
    "end": "1546169"
  },
  {
    "text": "the content or the data that's associated with each of those IDs and then display it back to the user but",
    "start": "1546169",
    "end": "1555260"
  },
  {
    "start": "1553000",
    "end": "1553000"
  },
  {
    "text": "again we're trying to do this you know in a random pattern across you know",
    "start": "1555260",
    "end": "1560419"
  },
  {
    "text": "trillions of Records you know causing you know comprising hundreds and hundreds and hundreds of terabytes of",
    "start": "1560419",
    "end": "1567230"
  },
  {
    "text": "compressed data and we need to do it with high read concurrency and get the",
    "start": "1567230",
    "end": "1572269"
  },
  {
    "text": "results back in a few seconds you know we looked at you know at the time we were using hive the ability to kind of",
    "start": "1572269",
    "end": "1579289"
  },
  {
    "text": "do these retrievals randomly across this data set and get response time in a few seconds not possible so that took us you",
    "start": "1579289",
    "end": "1585860"
  },
  {
    "text": "know to to an age based solution which we which we builds we would and which we'll talk about in a minute that we",
    "start": "1585860",
    "end": "1591320"
  },
  {
    "text": "built as we went up to the cloud so this is our original solution that we built",
    "start": "1591320",
    "end": "1597529"
  },
  {
    "text": "in the cloud and it was really kind of taking is really more of a lift and shift kind of approach for what you",
    "start": "1597529",
    "end": "1602929"
  },
  {
    "text": "might build in a data center we had over 60 HS one 8x large machines where we",
    "start": "1602929",
    "end": "1610610"
  },
  {
    "text": "were again combining storage and compute under the same machine so you know we we",
    "start": "1610610",
    "end": "1616909"
  },
  {
    "text": "built that up as pretty early as a part of our migration to the cloud and you know it was a solution that from a",
    "start": "1616909",
    "end": "1623480"
  },
  {
    "text": "business perspective worked well you know users were able to get results back",
    "start": "1623480",
    "end": "1628490"
  },
  {
    "text": "using the the HBase architecture much much faster than they were with the the prior system that existed you know our",
    "start": "1628490",
    "end": "1635210"
  },
  {
    "text": "data centers on-premises so so that was good but again by bonding the storage in",
    "start": "1635210",
    "end": "1640490"
  },
  {
    "text": "the cute together you know kind of going against the grain for our overall architecture that we talked about earlier it presented a series of",
    "start": "1640490",
    "end": "1647700"
  },
  {
    "text": "challenges both in terms of operations and in terms of cost so you know from",
    "start": "1647700",
    "end": "1654419"
  },
  {
    "text": "the operational front you know we always have backups that we're exceeding the maintenance window so we have a cluster",
    "start": "1654419",
    "end": "1660120"
  },
  {
    "text": "of ec2 s the data is on the ec2 s we you know triple replicated with you know",
    "start": "1660120",
    "end": "1665429"
  },
  {
    "text": "replication on Hadoop but if you lose enough nodes your clusters down and you",
    "start": "1665429",
    "end": "1671490"
  },
  {
    "text": "have to recover well so we make backup copies to s3 the trouble is when you start talking about 700 terabytes of",
    "start": "1671490",
    "end": "1677399"
  },
  {
    "text": "data that you have to back up off the cluster to s3 you know that takes a long time so we started to get our data",
    "start": "1677399",
    "end": "1684090"
  },
  {
    "text": "footprint started get so big that we really didn't have any backup window anymore where we can make the backups",
    "start": "1684090",
    "end": "1690289"
  },
  {
    "text": "same thing going the other way if we were to lose the cluster you know the amount of time that would take to",
    "start": "1690289",
    "end": "1696270"
  },
  {
    "text": "restore and rebuild the cluster and restore was basically exceeding our recovery time objective that we had to",
    "start": "1696270",
    "end": "1702929"
  },
  {
    "text": "be back up and running in the event of a failure so again we got a copy of the data on s3 but to get it back up into",
    "start": "1702929",
    "end": "1708870"
  },
  {
    "text": "the cluster and get back up and running was taking longer than we had to do it another thing is you know we talked",
    "start": "1708870",
    "end": "1715890"
  },
  {
    "text": "earlier in the presentation about the importance of security to us one of the things that we do is we make sure that",
    "start": "1715890",
    "end": "1721260"
  },
  {
    "text": "we're constantly upgrading and you know effectively patching our systems by upgrading the ami to include you know",
    "start": "1721260",
    "end": "1727169"
  },
  {
    "text": "the latest version of an omni that's out there Amazon machine image and you know",
    "start": "1727169",
    "end": "1733770"
  },
  {
    "text": "when we have a cluster that we treat kind of like you know just a like much like you on-premises you know changing",
    "start": "1733770",
    "end": "1739830"
  },
  {
    "text": "those armies got to be very difficult you would have to either build a completely separate parallel cluster on",
    "start": "1739830",
    "end": "1744960"
  },
  {
    "text": "the new AMI you know backup all the data to s3 restore the data from history what I talked about before was a really",
    "start": "1744960",
    "end": "1751320"
  },
  {
    "text": "cumbersome process or in some cases you'd have to learn you know pull out an individual machine upgrade it put it",
    "start": "1751320",
    "end": "1758039"
  },
  {
    "text": "back into the cluster rebalance all the data which again took a long time so upgrading was very painful and and",
    "start": "1758039",
    "end": "1765090"
  },
  {
    "text": "internally our security team working with our the the team the builds our armies for for use is basically",
    "start": "1765090",
    "end": "1771389"
  },
  {
    "text": "upgrading them once a month so it really wasn't just practical to kind of keep up with that kind of velocity and another thing just in terms",
    "start": "1771389",
    "end": "1778860"
  },
  {
    "text": "of the nature of the application you know I kind of talked about sort of the the query scenario before we run on the cluster the way the data gets loaded is",
    "start": "1778860",
    "end": "1785909"
  },
  {
    "text": "it gets loaded via a batch process to get loaded into HBase that batch process",
    "start": "1785909",
    "end": "1791249"
  },
  {
    "text": "runs on the same cluster but sometimes again due to fluctuating market volumes or potentially having to pause",
    "start": "1791249",
    "end": "1798330"
  },
  {
    "text": "processing what we you know in response to some data Corrections upstream we can get behind and again when we get behind",
    "start": "1798330",
    "end": "1805169"
  },
  {
    "text": "we run into situations where the batch processing window starts to crowd into our query window and that causes",
    "start": "1805169",
    "end": "1810659"
  },
  {
    "text": "conflicts so again trouble living within the limits of the storage computer on the cluster and another one quite",
    "start": "1810659",
    "end": "1817769"
  },
  {
    "text": "frankly was cost so we had data growing you know it you know roughly 20 20 terabytes a month in the application and",
    "start": "1817769",
    "end": "1826279"
  },
  {
    "text": "the nature of the application was such that you know we were really storage bound on the cluster there were there",
    "start": "1826279",
    "end": "1832110"
  },
  {
    "text": "was not a lot of heavy compute load that was there but the only way you can store that much data is to buy these hs1 nodes",
    "start": "1832110",
    "end": "1839279"
  },
  {
    "text": "or you know today you would buy you know D to 8x larges and these are very easy people probably know I mean these are",
    "start": "1839279",
    "end": "1844710"
  },
  {
    "text": "very expensive machines so we're spending a lot of money just to store the data you know and with a lot of",
    "start": "1844710",
    "end": "1850289"
  },
  {
    "text": "extra compute that there really we couldn't use or didn't have a need for so again we asked ourselves is there a",
    "start": "1850289",
    "end": "1857519"
  },
  {
    "text": "way to kind of address these challenges and move more to an architecture which we use for the vast majority of our",
    "start": "1857519",
    "end": "1863820"
  },
  {
    "text": "other processing and so again you know what would it be we asked ourselves what",
    "start": "1863820",
    "end": "1869759"
  },
  {
    "text": "would it be like if we could store the data out on s3 well what would be the benefits you know we could have a single",
    "start": "1869759",
    "end": "1874799"
  },
  {
    "text": "copy of the data we wouldn't have to have a triple replicated you know lower storage cost again the things we talked",
    "start": "1874799",
    "end": "1881490"
  },
  {
    "text": "about before we could have you know new compute available to run against it in minutes versus the you know the the",
    "start": "1881490",
    "end": "1887669"
  },
  {
    "text": "several days it would take to restore before we could start to separate out the the read quote the read query",
    "start": "1887669",
    "end": "1893759"
  },
  {
    "text": "cluster from the batch processing cluster much as we do with the rest of our architecture you know for disaster",
    "start": "1893759",
    "end": "1900749"
  },
  {
    "text": "recovery you know I talked about the scenario where we could lose a cluster well you know if",
    "start": "1900749",
    "end": "1905940"
  },
  {
    "text": "there was a problem within a particular availability zone you know same thing we can basically you know we could want to be able to failover and be up and",
    "start": "1905940",
    "end": "1912210"
  },
  {
    "text": "running and another availability zone you know in a very short period of time not the two days it would take to",
    "start": "1912210",
    "end": "1918030"
  },
  {
    "text": "restore otherwise so you know working",
    "start": "1918030",
    "end": "1923130"
  },
  {
    "text": "with Amazon and the EMR team we ended up with our new HBase on s3 architecture",
    "start": "1923130",
    "end": "1930810"
  },
  {
    "text": "which we'll talk about a little bit here so we worked with the early version of",
    "start": "1930810",
    "end": "1936120"
  },
  {
    "text": "HBase ns3 which is now generally available on AMR and we implemented it",
    "start": "1936120",
    "end": "1942210"
  },
  {
    "text": "and went live over the summer so some real changes here again the the data now",
    "start": "1942210",
    "end": "1947910"
  },
  {
    "text": "is stored on s3 all the H files as part of the HBase operational store they're",
    "start": "1947910",
    "end": "1953700"
  },
  {
    "text": "stored on an s3 bucket we have a cluster I can see on the if people are looking",
    "start": "1953700",
    "end": "1959730"
  },
  {
    "text": "there the the node sizes are much smaller instead of those HS one 8x large",
    "start": "1959730",
    "end": "1964860"
  },
  {
    "text": "you're in a big expensive lots of compute nodes we run em three to X larges now against that so we can size",
    "start": "1964860",
    "end": "1971400"
  },
  {
    "text": "the compute independently of the storage now as part of the cluster and then as I",
    "start": "1971400",
    "end": "1976740"
  },
  {
    "text": "said before we can now separate out and we have a separate cluster that actually does the generation of the the batch",
    "start": "1976740",
    "end": "1982680"
  },
  {
    "text": "processing that generates the H files for load into the database so we can run",
    "start": "1982680",
    "end": "1988410"
  },
  {
    "text": "that now also on spot for additional savings because if there's a problem we just stop that we talked about we just",
    "start": "1988410",
    "end": "1994530"
  },
  {
    "text": "stopped the cluster and we start the process and then you know complete the processing and load the data a little",
    "start": "1994530",
    "end": "2005570"
  },
  {
    "text": "bit about the application itself you know why we we thought that this you know HBase and s3 you know even back",
    "start": "2005570",
    "end": "2012710"
  },
  {
    "text": "when we started this could be a good fit for what we do you know it's a read-only application we don't use puts as part of",
    "start": "2012710",
    "end": "2019430"
  },
  {
    "text": "that I know it was part of the the actual GA available version of HBase on",
    "start": "2019430",
    "end": "2024500"
  },
  {
    "text": "s3 now there's been a lot of effort that the teams focused on to support that use case but for us that really wasn't part",
    "start": "2024500",
    "end": "2030530"
  },
  {
    "text": "of the equation because we really don't do you know puts as part of the applique",
    "start": "2030530",
    "end": "2035850"
  },
  {
    "text": "you know we have the random read patterns retrieve records across to all the data and as I said before that the",
    "start": "2035850",
    "end": "2042480"
  },
  {
    "text": "loads of the data because we're not doing puts how does the data get into the system we use the HBase bulk load",
    "start": "2042480",
    "end": "2048240"
  },
  {
    "text": "API to load the data sort of a way to process it bulk load it into the system you know no",
    "start": "2048240",
    "end": "2054898"
  },
  {
    "text": "over no put know puts against the system no you know writes through through the member through the mem store and",
    "start": "2054899",
    "end": "2060030"
  },
  {
    "text": "everything into the database and so you know I think the other thing is to I",
    "start": "2060030",
    "end": "2065908"
  },
  {
    "text": "mean we we did see a little additional latency by running queries against the data on s3 versus running them against",
    "start": "2065909",
    "end": "2073590"
  },
  {
    "text": "local disk but you know is what we'll see in the next slide you know we're",
    "start": "2073590",
    "end": "2079950"
  },
  {
    "text": "still getting results coming back in a few seconds for the application so if",
    "start": "2079950",
    "end": "2085679"
  },
  {
    "text": "you can see it up there at the top here the response time for the queries you know the majority of the queries that people run have results sets about ten",
    "start": "2085679",
    "end": "2094320"
  },
  {
    "text": "records or less that they come back so that's about sixty two percent of the overall query base you know and they're",
    "start": "2094320",
    "end": "2099960"
  },
  {
    "text": "coming back in a few seconds so again within the range of what you'd be looking for for an interactive query",
    "start": "2099960",
    "end": "2105210"
  },
  {
    "text": "type application and again just the kind of level set on that you know as I talked before the the on-premises",
    "start": "2105210",
    "end": "2111810"
  },
  {
    "text": "implementation of the system before we went to the cloud you know results were coming back in tens of minutes to two",
    "start": "2111810",
    "end": "2117930"
  },
  {
    "text": "hours right so again well you know we're getting performance that's within what you'd expect off of a presto or a high",
    "start": "2117930",
    "end": "2124260"
  },
  {
    "text": "of who's running your queries from what we saw from that some of the earlier slides but again we're able to do that",
    "start": "2124260",
    "end": "2129330"
  },
  {
    "text": "against random access across trillions of Records and hundreds of terabytes of",
    "start": "2129330",
    "end": "2134430"
  },
  {
    "text": "data so you know sort of a little bit of hit",
    "start": "2134430",
    "end": "2139500"
  },
  {
    "start": "2138000",
    "end": "2138000"
  },
  {
    "text": "on the performance side you know within the acceptable use for for interactive application and what are the benefits",
    "start": "2139500",
    "end": "2145290"
  },
  {
    "text": "what do we get so we talked about some of that already kind of as the the other side of the",
    "start": "2145290",
    "end": "2150450"
  },
  {
    "text": "case so now we can restart the cluster you know in 30 minutes if we need to you know again before starting a cluster",
    "start": "2150450",
    "end": "2157320"
  },
  {
    "text": "from scratch you know would have been a multi-day exercise if we want to do you",
    "start": "2157320",
    "end": "2162540"
  },
  {
    "text": "know automatic node replacement if we lose a node EMR we'll just replace the node for us and put a node back ends we",
    "start": "2162540",
    "end": "2168780"
  },
  {
    "text": "continue processing easier ami updates if we want to update the the ami versions on the cluster we can just go",
    "start": "2168780",
    "end": "2175770"
  },
  {
    "text": "ahead and you know kill the cluster we started back up as the new AMI and be",
    "start": "2175770",
    "end": "2181650"
  },
  {
    "text": "running again we starting a different AZ's we can do that and then we can you",
    "start": "2181650",
    "end": "2188700"
  },
  {
    "text": "know separate the traffic as we talked about before the last one up here is worth noting too is again we can easily",
    "start": "2188700",
    "end": "2195750"
  },
  {
    "text": "scale the cluster up so if we need to add 100 nodes for whatever reason because there's a spike in user traffic",
    "start": "2195750",
    "end": "2201930"
  },
  {
    "text": "that's something that we can very easily do it doesn't really come into play much on our current application but for some",
    "start": "2201930",
    "end": "2208230"
  },
  {
    "text": "of the other use cases that we're looking to extend this to that can become very important so that's that's another capability that that's been keep",
    "start": "2208230",
    "end": "2214380"
  },
  {
    "text": "that's gonna be important to us you know sort of on the other side of the equation on the cost side right I mean",
    "start": "2214380",
    "end": "2221310"
  },
  {
    "text": "by keeping the data on s3 brings up a lot of advantages we weren't able to take advantage of before in the s2 end",
    "start": "2221310",
    "end": "2227280"
  },
  {
    "text": "the HBase architecture and we can run the less expensive nodes we can scale the cluster down at night for additional",
    "start": "2227280",
    "end": "2233250"
  },
  {
    "text": "savings so we can run more nodes during the day when the users your they're processing we can scale down to a",
    "start": "2233250",
    "end": "2238650"
  },
  {
    "text": "smaller number of nodes at night when the quarry traffic is minimal or very low you know we can run the the bag the",
    "start": "2238650",
    "end": "2246000"
  },
  {
    "text": "the batch process separately from from the main cluster another big thing and",
    "start": "2246000",
    "end": "2251490"
  },
  {
    "text": "that we do is in our lower environments you know Devon test right before we had",
    "start": "2251490",
    "end": "2256500"
  },
  {
    "text": "to have kind of an always-on cluster there to have because again the setup and the teardown in the cluster was such",
    "start": "2256500",
    "end": "2262200"
  },
  {
    "text": "a such an exercise here it's very easy to bring the cluster up run it on spot you run the spot in the lower",
    "start": "2262200",
    "end": "2268230"
  },
  {
    "text": "environments do any sort of testing we need and then you know just shut the cluster down when we're not under active development and",
    "start": "2268230",
    "end": "2275000"
  },
  {
    "text": "that's actually been a big savings for this particular application because it's not really under a lot of active",
    "start": "2275000",
    "end": "2280700"
  },
  {
    "text": "development it's kind of in a maintenance mode right now so before where we had to have lower environments",
    "start": "2280700",
    "end": "2286010"
  },
  {
    "text": "you know provisioned and ready to go now we can basically just you know have nothing there effectively if we want to",
    "start": "2286010",
    "end": "2292700"
  },
  {
    "text": "run we can make a copy of the data and spin up a cluster and be up and you know working that one weird actually doing",
    "start": "2292700",
    "end": "2298160"
  },
  {
    "text": "active development on it a little bit",
    "start": "2298160",
    "end": "2304040"
  },
  {
    "text": "about you know kind of talking about some of the the settings that we made just talks a little bit about our cache settings on the cluster one things we",
    "start": "2304040",
    "end": "2311570"
  },
  {
    "text": "did is to get better some better performance off of s3 we set the cache the the index the indexes and the bloom",
    "start": "2311570",
    "end": "2318650"
  },
  {
    "text": "filters on the h files we you know that the data set size the search we can't cash all the data so we decided to",
    "start": "2318650",
    "end": "2325550"
  },
  {
    "text": "basically kind of cache those so as we go back and do additional retrievals we'd have those things loaded up into memory to speed up you know the",
    "start": "2325550",
    "end": "2332290"
  },
  {
    "text": "processing for the next data sets that were coming back we chose to basically",
    "start": "2332290",
    "end": "2338120"
  },
  {
    "text": "configure that by by doing that such that there would be cache to disk so we run SSD disk we cache the data to disk",
    "start": "2338120",
    "end": "2344630"
  },
  {
    "text": "on cluster tickly maximize the amount that we can cache on the cluster we did",
    "start": "2344630",
    "end": "2352490"
  },
  {
    "text": "have to make a few changes it kind of kind of how we do things because we're using s3 as a storage layer instead of",
    "start": "2352490",
    "end": "2358700"
  },
  {
    "text": "HDFS primarily these were really around the area is sort of some of the time out processes there's a few things in HBase",
    "start": "2358700",
    "end": "2365450"
  },
  {
    "text": "where you know it thinks you're using an inode base file system or you know logical or physical such that you know",
    "start": "2365450",
    "end": "2372170"
  },
  {
    "text": "if you wanted to a rename operation or a move operation is expecting that to happen very quickly there's not many cases where that",
    "start": "2372170",
    "end": "2378740"
  },
  {
    "text": "happens but where it has we had to kind of bump up some of the timeouts and take more care as part of those operational",
    "start": "2378740",
    "end": "2384890"
  },
  {
    "text": "tasks when we do them so potentially schedule them during a batch window or something like that in order to do them",
    "start": "2384890",
    "end": "2392260"
  },
  {
    "text": "just a couple other things to that we kind of saw one was just this is not",
    "start": "2393670",
    "end": "2399380"
  },
  {
    "text": "really related so much to the HBase on s3 but just based on the large number of HBase regions that we have if a",
    "start": "2399380",
    "end": "2406829"
  },
  {
    "text": "particular you know node were to go out you know each a.m. are automatically moves the regions over to other nodes",
    "start": "2406829",
    "end": "2414079"
  },
  {
    "text": "region servers on other nodes such that the processing continue uninterrupted to the users we did find when we",
    "start": "2414079",
    "end": "2420630"
  },
  {
    "text": "automatically add a new server back in as part of EMR that the there was a lot",
    "start": "2420630",
    "end": "2425760"
  },
  {
    "text": "of load that was generated as part of the rebalancing just because in our case we have we have so many regions across our region servers so we just made a",
    "start": "2425760",
    "end": "2431910"
  },
  {
    "text": "decision basically to say you know if we lose nodes you know letting EMR automatically reallocate those continual",
    "start": "2431910",
    "end": "2438329"
  },
  {
    "text": "processing and then when we add a node back in we can do a manual rebalance by running the script it's very quick to do",
    "start": "2438329",
    "end": "2444750"
  },
  {
    "text": "we just do that you know kind of off hours it's probably maintenance window and then again a couple other things you",
    "start": "2444750",
    "end": "2450900"
  },
  {
    "text": "know kind of differences between running it on you know file system you know HDFS versus s3 you know some of the operation",
    "start": "2450900",
    "end": "2457410"
  },
  {
    "text": "the metadata operations like you know drop table and things or slow but again in a production environment that doesn't",
    "start": "2457410",
    "end": "2463319"
  },
  {
    "text": "really you know cause us any problems we're not doing that kind of stuff it does come up sometimes when we're under development and Devon tasks so we",
    "start": "2463319",
    "end": "2469710"
  },
  {
    "text": "have to do some kind of workarounds as part of that another you know kind of",
    "start": "2469710",
    "end": "2476970"
  },
  {
    "text": "big thing that we ran across again using s3 as a storage layer was there are a few you know kind of operations there",
    "start": "2476970",
    "end": "2483569"
  },
  {
    "text": "you know very very write processing intensive you know write intensive against s3 and what we found you know",
    "start": "2483569",
    "end": "2490559"
  },
  {
    "text": "during major compactions and export snapshots things where were you know doing a heavy amount of write",
    "start": "2490559",
    "end": "2495839"
  },
  {
    "text": "processing against you know that's three you know we were running into bottlenecks on s3 and I think you know",
    "start": "2495839",
    "end": "2502109"
  },
  {
    "text": "sort of the the standard practices you know design your key structure your you know your file structure basically you",
    "start": "2502109",
    "end": "2508650"
  },
  {
    "text": "know so that when you're writing to s3 you know you don't you know you don't collide on the same block so graphical",
    "start": "2508650",
    "end": "2515309"
  },
  {
    "text": "spaces you're doing your writes that really wasn't possible in this case we don't really have control over what that",
    "start": "2515309",
    "end": "2520319"
  },
  {
    "text": "key looks like you know we just have to accept what's there in HBase so you know we worked with our account team to",
    "start": "2520319",
    "end": "2526380"
  },
  {
    "text": "engage the s3 team to do some pre partitioning on the bucket at the right point in the the key space such that you",
    "start": "2526380",
    "end": "2533369"
  },
  {
    "text": "know when we did writes we get much better performance you know those going through so you know",
    "start": "2533369",
    "end": "2541650"
  },
  {
    "start": "2539000",
    "end": "2539000"
  },
  {
    "text": "in terms of further areas of investigation you know that we're gonna be looking at as part of HBase on s3 you",
    "start": "2541650",
    "end": "2546840"
  },
  {
    "text": "know is there a way to take better advantage of the caching capabilities are out there you know we kind of did",
    "start": "2546840",
    "end": "2551940"
  },
  {
    "text": "some basic first level caching to get out the door but we think that there's other things that we can look at in terms of you know potentially doing more",
    "start": "2551940",
    "end": "2558300"
  },
  {
    "text": "sophisticated pre-warming of the cache you know today we just basically cache you know upon access but we've looked at",
    "start": "2558300",
    "end": "2565110"
  },
  {
    "text": "ways you know is there a way to pre-warm against a lot of the data out there to get even faster response time for first",
    "start": "2565110",
    "end": "2570210"
  },
  {
    "text": "access you know looking at this can we use it and leverage it for more write intensive workloads you know as part of",
    "start": "2570210",
    "end": "2576600"
  },
  {
    "text": "the the new GA version that's out there for HBase on s3 from EMR one of the",
    "start": "2576600",
    "end": "2583140"
  },
  {
    "text": "things we want to try to do and you know I think this probably require an H based change but we're looking at it is the ability to run multiple clusters again",
    "start": "2583140",
    "end": "2590400"
  },
  {
    "text": "you know a single operational data store on s3 in HBase right now the ability to run",
    "start": "2590400",
    "end": "2596280"
  },
  {
    "text": "multiple clusters is limited because they both essentially collide on the same Metis the metadata part of the",
    "start": "2596280",
    "end": "2602610"
  },
  {
    "text": "database so is there a way to basically have them coexist and and run together and then one of these we want to look at",
    "start": "2602610",
    "end": "2608610"
  },
  {
    "text": "too is you know can we put a sequel interface on top of all this you know today we basically just used API called",
    "start": "2608610",
    "end": "2613650"
  },
  {
    "text": "HBase API calls you know custom code to access that but in this kind of architecture because you also put Apache",
    "start": "2613650",
    "end": "2619920"
  },
  {
    "text": "Phoenix in front of it and run a sequel queries against this so that's something that we want to look at and want to try",
    "start": "2619920",
    "end": "2627619"
  },
  {
    "text": "some other FINRA sessions here they're they're here that the folks can either have checked out or are still to come so",
    "start": "2627800",
    "end": "2635280"
  },
  {
    "text": "again thanks for coming and appreciate sharing kind of architecture and s3",
    "start": "2635280",
    "end": "2641180"
  },
  {
    "text": "scalable architecture in HBase and I think for questions I guess there's a microphone here so if anyone has any",
    "start": "2641180",
    "end": "2647520"
  },
  {
    "text": "questions you know you want to come up to the microphone you can ask them or if you have any questions afterwards you",
    "start": "2647520",
    "end": "2653460"
  },
  {
    "text": "know feel free to stop by [Music]",
    "start": "2653460",
    "end": "2658020"
  }
]