[
  {
    "start": "0",
    "end": "63000"
  },
  {
    "text": "hello welcome my name is Ricardo dematos welcome to missig critical stream processing",
    "start": "5319",
    "end": "12080"
  },
  {
    "text": "with fanis and EMR I'm a Solutions architect out of a Boston office and I'm going to be presenting today with data",
    "start": "12080",
    "end": "18039"
  },
  {
    "text": "zooo who will be joining me on stage momentarily this is a 300 level session so we're going to skip through some of",
    "start": "18039",
    "end": "23680"
  },
  {
    "text": "the intro things really quickly uh and get right into some of the uh uh",
    "start": "23680",
    "end": "28800"
  },
  {
    "text": "questions that we are mitigations that we'll talk about Mission critical processing so this is our uh big data",
    "start": "28800",
    "end": "35160"
  },
  {
    "text": "services portfolio and customers use this to do to manage collect uh store",
    "start": "35160",
    "end": "41559"
  },
  {
    "text": "and analyze Mission critical data right so Mission critical data makes its way into a WS through either Direct Connect",
    "start": "41559",
    "end": "48520"
  },
  {
    "text": "um Import and Export and now through Kinesis and we store that data using",
    "start": "48520",
    "end": "54000"
  },
  {
    "text": "Amazon S3 Dynamo DB uh and process that through EMR uh red shift or through",
    "start": "54000",
    "end": "59920"
  },
  {
    "text": "through a custom databases and applications through ec2 and customers are doing the",
    "start": "59920",
    "end": "67360"
  },
  {
    "start": "63000",
    "end": "135000"
  },
  {
    "text": "analytical processing that um really are driving",
    "start": "67360",
    "end": "73200"
  },
  {
    "text": "big bussiness drivers today so we're looking at customers that are processing billing information login information um",
    "start": "73200",
    "end": "81759"
  },
  {
    "text": "click-through information and they're doing so in a batch mode but there's really a big driver now to do things more on real time so there's a big need",
    "start": "81759",
    "end": "89520"
  },
  {
    "text": "to look at the missal critical information and make",
    "start": "89520",
    "end": "94960"
  },
  {
    "text": "decisions in real time but the fact that it's Mission critical still has not changed so we're going to make some",
    "start": "94960",
    "end": "101200"
  },
  {
    "text": "tough uh start asking some questions here around some uh typical data flows that involve Kinesis so the big Focus",
    "start": "101200",
    "end": "108479"
  },
  {
    "text": "today is going to be Kinesis and EMR and as you already know Kinesis is our injust service and you can push data",
    "start": "108479",
    "end": "115280"
  },
  {
    "text": "into it uh through some various methods uh HTTP posts uh using our sdks using",
    "start": "115280",
    "end": "121960"
  },
  {
    "text": "plugins that uh we open source uh and even writing your own plugins so we're going to talk a lot about uh pushing",
    "start": "121960",
    "end": "128119"
  },
  {
    "text": "data producing data into Kinesis and also reading it out of Kinesis and what we find is the customers typically have",
    "start": "128119",
    "end": "135440"
  },
  {
    "start": "135000",
    "end": "210000"
  },
  {
    "text": "a a data flow that kind of looks like this you have a client or sensors if you're using uh if you're pushing data",
    "start": "135440",
    "end": "142239"
  },
  {
    "text": "from my um Internet devices and those devices or um or servers are pushing",
    "start": "142239",
    "end": "148040"
  },
  {
    "text": "data into an aggregator which in this case be using Kinesis as an example um that",
    "start": "148040",
    "end": "153519"
  },
  {
    "text": "information is being continuous processed and it's being stored uh for",
    "start": "153519",
    "end": "158720"
  },
  {
    "text": "uh other Downstream or batch processing uh in durable services and that are",
    "start": "158720",
    "end": "164080"
  },
  {
    "text": "tapped into by other analytical services so the whole purpose of today's session is to ask some tough questions so what",
    "start": "164080",
    "end": "170959"
  },
  {
    "text": "could POS possibly go wrong basically in any of these points here so we're going to be looking at um potential issues",
    "start": "170959",
    "end": "178040"
  },
  {
    "text": "failure modes I'm going to call them uh when the client for example is not able to reach the injust point when the",
    "start": "178040",
    "end": "184480"
  },
  {
    "text": "client is not able to keep up with the massive amount of events that might be coming through there and we have some good numbers to talk to you guys about",
    "start": "184480",
    "end": "191000"
  },
  {
    "text": "we also look at uh failure modes that could happen at the service that is aggregating all this data what happens",
    "start": "191000",
    "end": "197080"
  },
  {
    "text": "when the service uh with the record uh while it's being aggregated how durable is that record is that aggregator going",
    "start": "197080",
    "end": "204040"
  },
  {
    "text": "to be have enough capacity we took a look at uh approaches that we've taken to mitigate for those failure modes",
    "start": "204040",
    "end": "209840"
  },
  {
    "text": "continuous processing is also very interesting right uh we'll talk in detail about the KCl for the kingon",
    "start": "209840",
    "end": "215840"
  },
  {
    "start": "210000",
    "end": "322000"
  },
  {
    "text": "client Library uh but look into some specific edge cases on how do we deal with bad records how do we deal with",
    "start": "215840",
    "end": "222080"
  },
  {
    "text": "this resilience of where the KCl is running the worker Fleet and of course storage uh if you um are familiar with",
    "start": "222080",
    "end": "230799"
  },
  {
    "text": "Kinesis uh the records are in Kinesis for 24 hours the time to live for the record is 24 hours that's what the",
    "start": "230799",
    "end": "237280"
  },
  {
    "text": "service was designed for and we're going to going to look at the edge cases of what happens what type of mitigations",
    "start": "237280",
    "end": "243120"
  },
  {
    "text": "need to be put in place when uh you go beyond the 24-hour window right uh and",
    "start": "243120",
    "end": "248879"
  },
  {
    "text": "of course in analytics and Reporting now this is where uh I'll I'll step back on stage and talk specifically about issues",
    "start": "248879",
    "end": "254400"
  },
  {
    "text": "that you could have that have to do with record order and also the ability to be able to replay the records that were",
    "start": "254400",
    "end": "261280"
  },
  {
    "text": "archived uh to go beyond that 24-hour window so these are the this is what you're in store for but we're going to",
    "start": "261280",
    "end": "267840"
  },
  {
    "text": "uh give you guys some contacts so I'm going to invite to stay jesa uh from data Zoo um big customer in Boston and",
    "start": "267840",
    "end": "274520"
  },
  {
    "text": "he's going to be talking about his mitigations that he put in place for these cases thank you",
    "start": "274520",
    "end": "282039"
  },
  {
    "text": "Ricardo thank you Ricardo thank you Amazon for giving us the data Zoo an opportunity to share our experiences",
    "start": "285880",
    "end": "292560"
  },
  {
    "text": "here uh ladies and gentlemen good afternoon thanks for being here we appreciate the time hope you learn",
    "start": "292560",
    "end": "299240"
  },
  {
    "text": "something useful um so I'm the my name is aesa kosuru I'm the vpf engineering",
    "start": "299240",
    "end": "305759"
  },
  {
    "text": "at data zoo and data zoo is in a digital ad serving business and where every ad",
    "start": "305759",
    "end": "313720"
  },
  {
    "text": "inventory is bought and sold via programmatic auctions so we have a need",
    "start": "313720",
    "end": "320160"
  },
  {
    "text": "to continuously process uh the data to tune and",
    "start": "320160",
    "end": "325919"
  },
  {
    "start": "322000",
    "end": "373000"
  },
  {
    "text": "fine-tune our allocations and breeding strategies and and we've been using bad",
    "start": "325919",
    "end": "331080"
  },
  {
    "text": "systems for a long time bat systems are great but they're slow so Kinesis fills",
    "start": "331080",
    "end": "337840"
  },
  {
    "text": "in an important gap between transactional systems and batch systems",
    "start": "337840",
    "end": "343280"
  },
  {
    "text": "and it opens up doors for continuous data processing but what is continuous data data that has no end it keeps on",
    "start": "343280",
    "end": "349600"
  },
  {
    "text": "flowing and the other good thing about kesis is it allows it's a language",
    "start": "349600",
    "end": "354759"
  },
  {
    "text": "neutral or framework neutral processing it is uh you can do it in map produce",
    "start": "354759",
    "end": "359919"
  },
  {
    "text": "you can do it in SQL or any other language that it supports bindings for so the motivation for us is the ambition",
    "start": "359919",
    "end": "367960"
  },
  {
    "text": "to learn quickly and apply those learnings in real",
    "start": "367960",
    "end": "373240"
  },
  {
    "text": "time so who dato what do we do how do we fit in uh dat zoo we are building a",
    "start": "373240",
    "end": "379840"
  },
  {
    "text": "marketing Cloud on top of Amazon and to uh we have the scale and",
    "start": "379840",
    "end": "386919"
  },
  {
    "text": "automation to harness the big data to find marketing opportunities uh and and with one goal",
    "start": "386919",
    "end": "394440"
  },
  {
    "text": "that the goal is to how do we make marketing organization more effective in making their daily",
    "start": "394440",
    "end": "400160"
  },
  {
    "text": "decs and um we operated at a large scale we just last week we processed uh we hit",
    "start": "400160",
    "end": "407199"
  },
  {
    "text": "1.2 million requests ad requests a second uh we make billions of AD DS a day we uh process pedabytes of data uh",
    "start": "407199",
    "end": "415840"
  },
  {
    "text": "we have globally about 3,000 servers deployed and we collect about 15",
    "start": "415840",
    "end": "421000"
  },
  {
    "text": "terabytes of log data that we are pumping in through through the Amazon",
    "start": "421000",
    "end": "426080"
  },
  {
    "text": "Cloud uh we are listed as one of the fastest growing companies in Inc 5000 we were born out of MIT labs and uh so how",
    "start": "426080",
    "end": "434400"
  },
  {
    "text": "do we fit in so where do we fit in and and what we do is",
    "start": "434400",
    "end": "439960"
  },
  {
    "text": "um today's digital lifestyle produces torrant of consumer data Anonymous but",
    "start": "439960",
    "end": "447080"
  },
  {
    "text": "still very valuable where the users leave their intent and taste that we see",
    "start": "447080",
    "end": "453080"
  },
  {
    "text": "through looking for perfect opportunities to connect and engage with the consumer along their buying journey",
    "start": "453080",
    "end": "460720"
  },
  {
    "text": "and we via VIA programmatic marketing apis we expose those opportunities and",
    "start": "460720",
    "end": "465919"
  },
  {
    "text": "connect the advertisers of the brands directly with the customers and we do",
    "start": "465919",
    "end": "471560"
  },
  {
    "text": "that in a in a new way being real time being ambient and being sensient simply",
    "start": "471560",
    "end": "479400"
  },
  {
    "text": "put programmatic so real time Meets Mission",
    "start": "479400",
    "end": "485080"
  },
  {
    "start": "482000",
    "end": "597000"
  },
  {
    "text": "critical let me State the obvious Mission critical is anything that's important for a business Mission this is",
    "start": "485080",
    "end": "490960"
  },
  {
    "text": "the data that we can't afford to lose so we take lot of precautions to bring it uh into the systems why is it so",
    "start": "490960",
    "end": "497199"
  },
  {
    "text": "important before I talk about the guarant guaranteed delivery let me talk about why this data is so important when",
    "start": "497199",
    "end": "503280"
  },
  {
    "text": "we serve an ad we get a confirmation this is all through apis and machines are exchanging data we get a",
    "start": "503280",
    "end": "509280"
  },
  {
    "text": "confirmation back and that that confirmation has a winning price in it",
    "start": "509280",
    "end": "514320"
  },
  {
    "text": "and that's what we use for our for our building information so we lose the data we we lose the revenue that's how we get",
    "start": "514320",
    "end": "521599"
  },
  {
    "text": "paid so this is very important data for us so having said that we have to Define",
    "start": "521599",
    "end": "527440"
  },
  {
    "text": "we wanted to bring in this uh machine critical data through high speed we",
    "start": "527440",
    "end": "533480"
  },
  {
    "text": "process about 1.2 million requests a second so we need to figure out a system that can bring in this kind of volume of",
    "start": "533480",
    "end": "540279"
  },
  {
    "text": "data so let me put down some requirements non-functional requirements the nonfunctional requirements are",
    "start": "540279",
    "end": "545959"
  },
  {
    "text": "availability right because these systems are highly available we can't go down and if our ad servers are pumping that",
    "start": "545959",
    "end": "551920"
  },
  {
    "text": "fast that volume the back end has to catch up with it so the throughput has to catch up with it we operate in a low",
    "start": "551920",
    "end": "559000"
  },
  {
    "text": "latency environment typically the ads are served we bidding on the auctions in milliseconds so this system whatever",
    "start": "559000",
    "end": "566440"
  },
  {
    "text": "system we end up using has to fit into this low latency environment and uh",
    "start": "566440",
    "end": "572320"
  },
  {
    "text": "durability no record loss we can't afford to lose this data I told you this is how we get paid so it's extremely",
    "start": "572320",
    "end": "578279"
  },
  {
    "text": "important that we establish these non-functional requirements and then the other thing too is we would like to have",
    "start": "578279",
    "end": "585480"
  },
  {
    "text": "uh framework neutral processing whether it is map reduce or SQL or whatever it is the idea is to ingest the data read",
    "start": "585480",
    "end": "593440"
  },
  {
    "text": "the data and do something useful with it so with that said let's look at some of",
    "start": "593440",
    "end": "598519"
  },
  {
    "start": "597000",
    "end": "655000"
  },
  {
    "text": "these production logs these are the two samples that I took out of the production logs on the left hand side",
    "start": "598519",
    "end": "604519"
  },
  {
    "text": "you see a confirmation record so the confirmation record has this price micro",
    "start": "604519",
    "end": "609800"
  },
  {
    "text": "CPM that's our winning price this is how we actually build our customers so we need to keep this data around and also",
    "start": "609800",
    "end": "616560"
  },
  {
    "text": "let me call your attention to UU ID I'm going to talk about this in a later slide so we tag those events with a",
    "start": "616560",
    "end": "622839"
  },
  {
    "text": "unique ID that then gets tagged and and sent through the system on the right hand side there is a fraud record",
    "start": "622839",
    "end": "629240"
  },
  {
    "text": "because there machines exchanging information at high speed there is room for frauders to sneak in so in an anti",
    "start": "629240",
    "end": "637800"
  },
  {
    "text": "fraud effort we are building in fraud program where it detects the users we",
    "start": "637800",
    "end": "644000"
  },
  {
    "text": "attach a third party pixel and if we detect that this is a fraud user we stop him right at the gates this user doesn't",
    "start": "644000",
    "end": "650920"
  },
  {
    "text": "even come in so that's an example of a a fraud record now let me talk about a",
    "start": "650920",
    "end": "657360"
  },
  {
    "start": "655000",
    "end": "765000"
  },
  {
    "text": "typical ad serving flow this is not an architecture diagram folks this is just add the the the data flow diagram this",
    "start": "657360",
    "end": "664800"
  },
  {
    "text": "is a special specialized version of what Ricardo talked about earlier in terms of the general purpose data flows on the",
    "start": "664800",
    "end": "672000"
  },
  {
    "text": "front and center you have this real-time bidding engine this is the system that's actually facing the internet the",
    "start": "672000",
    "end": "677639"
  },
  {
    "text": "exchanges and the user the browsers and so on bidding on the auctions it is serving at light speed to the to the",
    "start": "677639",
    "end": "685000"
  },
  {
    "text": "consumers and all this it's basically know 100 of servers Linux servers and we",
    "start": "685000",
    "end": "691160"
  },
  {
    "text": "have a flume agent locally deployed on every box and the rtb",
    "start": "691160",
    "end": "696720"
  },
  {
    "text": "engine whenever it's bids on an on an ad it logs that information to the local",
    "start": "696720",
    "end": "702160"
  },
  {
    "text": "flum agent local flum agent then packs multiple log lines into single one and",
    "start": "702160",
    "end": "707959"
  },
  {
    "text": "then sends it over to Kinesis and if we win the ad we also have CDN calls and",
    "start": "707959",
    "end": "713360"
  },
  {
    "text": "retargeting things that are going on the meta point of this is all the realtime flows are wired in through",
    "start": "713360",
    "end": "720120"
  },
  {
    "text": "Kinesis and then we have an archive application on the other side archive application simply copies the data",
    "start": "720120",
    "end": "726240"
  },
  {
    "text": "that's in Kinesis puts it in S3 and I'll tell you why we would why we chose to do that and we have a Golden Rule at at at",
    "start": "726240",
    "end": "734639"
  },
  {
    "text": "dato which is all data in and out goes through S3 and um once the data made",
    "start": "734639",
    "end": "740720"
  },
  {
    "text": "made it into S3 we have that U other uh reporting it's labeled as reporting",
    "start": "740720",
    "end": "746240"
  },
  {
    "text": "that's the machine learning cluster taking the data out of s three processing it and for those of you who",
    "start": "746240",
    "end": "751279"
  },
  {
    "text": "are data scientists in the room it produces models or classifiers that then get fed back into the uh realtime",
    "start": "751279",
    "end": "758199"
  },
  {
    "text": "bidding engine and there's a continuous loop and we do this uh continuously",
    "start": "758199",
    "end": "763240"
  },
  {
    "text": "throughout the day and then we use a cub ball for ad hoc analysis so I talked about",
    "start": "763240",
    "end": "769800"
  },
  {
    "start": "765000",
    "end": "777000"
  },
  {
    "text": "guaranteed uh guaranteed delivery uh what's what does it mean how do you know that you're are not losing",
    "start": "769800",
    "end": "776000"
  },
  {
    "text": "data let's dive in the first thing you need to do is to monitor your data flows",
    "start": "776000",
    "end": "781240"
  },
  {
    "start": "777000",
    "end": "1004000"
  },
  {
    "text": "I'm not talking about system monitoring I'm talking about endtoend data flows monitoring the first thing you need to",
    "start": "781240",
    "end": "787360"
  },
  {
    "text": "do is to have enough tracing and monitoring in place on the right hand side you looking at a snapshot of a",
    "start": "787360",
    "end": "794120"
  },
  {
    "text": "dashboard it's got a whole bunch of real-time counters uh we're going to actually show you a live demo how that",
    "start": "794120",
    "end": "800240"
  },
  {
    "text": "works and uh out of the box cloud watch and and kineses gives you a whole bunch",
    "start": "800240",
    "end": "805720"
  },
  {
    "text": "of metrics that are uh very useful things like put get ratios uh iterate rage and so on so they are builtin",
    "start": "805720",
    "end": "814000"
  },
  {
    "text": "counters you can use them readily on top of that we've augmented uh our own metrics for example uh throughput",
    "start": "814000",
    "end": "821360"
  },
  {
    "text": "exceeded exceptions why because you are seeing those exceptions on the client side see Kinesis can actually track",
    "start": "821360",
    "end": "827680"
  },
  {
    "text": "everything anything within the bounds of Kinesis but outside you have to track it yourself so our clients we've extended",
    "start": "827680",
    "end": "834320"
  },
  {
    "text": "them to monitor the throughput exceeded exceptions we need to know if we have we",
    "start": "834320",
    "end": "840160"
  },
  {
    "text": "are running out of the sh capacity that we have provisioned I'm going to come back to that one in more detail keep an",
    "start": "840160",
    "end": "845480"
  },
  {
    "text": "eye on the producer and the consumer uh drift the reason why this is important",
    "start": "845480",
    "end": "851600"
  },
  {
    "text": "is producer is is producing the data as fast as you can and the consumer is",
    "start": "851600",
    "end": "856639"
  },
  {
    "text": "consuming the data and if there's a drift you need to know and you need to react you need to figure out so these",
    "start": "856639",
    "end": "862399"
  },
  {
    "text": "realtime dashbo counters will help you do all the diagnose those things but realtime dashboard is just like a dash",
    "start": "862399",
    "end": "868959"
  },
  {
    "text": "in your car when the light is on you have to open the hood and look at it that what you need is the logging a",
    "start": "868959",
    "end": "876079"
  },
  {
    "text": "detailed logging at the event level this is where uu ID comes in handy if there is a suspicion that you have a data loss",
    "start": "876079",
    "end": "882600"
  },
  {
    "text": "where do you look realtime counter is going to show producer had produced 59 and consumer has consumed 49 where did",
    "start": "882600",
    "end": "889639"
  },
  {
    "text": "the 10 go who knows so what you need to integrate with deep into the logging",
    "start": "889639",
    "end": "895279"
  },
  {
    "text": "infrastructure so you can do several ways of doing we we have these Cloud watch logs we log every single uu ID so",
    "start": "895279",
    "end": "903360"
  },
  {
    "text": "and the producer logs every single uu ID that's it sending through Kinesis consumer reads The UU IDs and when we",
    "start": "903360",
    "end": "911560"
  },
  {
    "text": "have a suspicion we play the log side by side we see which one is missing well",
    "start": "911560",
    "end": "916600"
  },
  {
    "text": "that'll at least tell you what's missing but it won't tell you where the leakage was if you really want to go into the",
    "start": "916600",
    "end": "923360"
  },
  {
    "text": "details of that then you would need to look into some kind of a distributed tracing Google published a paper called",
    "start": "923360",
    "end": "928800"
  },
  {
    "text": "Dapper you very interesting read you might want to take a look at that and Twitter also has an open source",
    "start": "928800",
    "end": "934319"
  },
  {
    "text": "implementation that allows you to trace your distributed systems front to back and that's the kind of level of details",
    "start": "934319",
    "end": "941680"
  },
  {
    "text": "that you would need to diagnose distributed system uh issues on the left",
    "start": "941680",
    "end": "947560"
  },
  {
    "text": "hand side you are looking at a chart this is one stream that was scaling up",
    "start": "947560",
    "end": "952639"
  },
  {
    "text": "when we scale up we actually take it in in ramp up slowly that chart was a live",
    "start": "952639",
    "end": "958639"
  },
  {
    "text": "cloudwatch chart uh for a snapshot of about 24hour period And as we were",
    "start": "958639",
    "end": "963920"
  },
  {
    "text": "ramping up the number of shards we were ramping up the clients so that's the number of messages or bytes written to",
    "start": "963920",
    "end": "969399"
  },
  {
    "text": "Kines shards and uh on the right hand side top is 1 Gaby per per second",
    "start": "969399",
    "end": "977600"
  },
  {
    "text": "throughput into the system I.E million messages a second folks that's just one",
    "start": "977600",
    "end": "983440"
  },
  {
    "text": "stream and the relatively flat line in the middle is when of one of our developers Steve ZK here fell asleep in",
    "start": "983440",
    "end": "990199"
  },
  {
    "text": "front of the computer when we are ramping up so this there's a you know Cloud watch",
    "start": "990199",
    "end": "997319"
  },
  {
    "text": "has lot of built-in uh metrics ready to go and uh uh use it uh where it makes",
    "start": "997319",
    "end": "1004639"
  },
  {
    "start": "1004000",
    "end": "1090000"
  },
  {
    "text": "sense and and now let's talk about so having captured your monitoring and",
    "start": "1004639",
    "end": "1009920"
  },
  {
    "text": "tracing now you want to look at the failure modes let's talk about the clear failure modes like Ricard already",
    "start": "1009920",
    "end": "1016440"
  },
  {
    "text": "introduced there's failure modes you need to think about on the the client side aggregator which is in the middle which is the Kinesis itself and on the",
    "start": "1016440",
    "end": "1023079"
  },
  {
    "text": "processing side so let's start with the client there's two important things that you can think about what can go wrong",
    "start": "1023079",
    "end": "1029640"
  },
  {
    "text": "the client is not able to reach The KES stream the end point well there could be many reasons let me just cite a few the",
    "start": "1029640",
    "end": "1036600"
  },
  {
    "text": "first one might be that your client is having a DNS issue it can't it receives unknown know unknown host exception or",
    "start": "1036600",
    "end": "1044199"
  },
  {
    "text": "the network connectivity is lost between you and where the kesis endpoint is or a third one might be very simply you",
    "start": "1044199",
    "end": "1051280"
  },
  {
    "text": "haven't provisioned enough shards on the Kinesis side Kinesis comes in the units",
    "start": "1051280",
    "end": "1056640"
  },
  {
    "text": "of shards and your unit of scalability is short and we'll talk about that in",
    "start": "1056640",
    "end": "1062080"
  },
  {
    "text": "the upcoming slides and then let's say it is down or it's not reachable for",
    "start": "1062080",
    "end": "1067200"
  },
  {
    "text": "whatever reason then the client that you have can it survive the accumulation of",
    "start": "1067200",
    "end": "1074280"
  },
  {
    "text": "the events on on its side for example we are serving a million requests a second can if we were to accumulate all that on",
    "start": "1074280",
    "end": "1080360"
  },
  {
    "text": "the Java side we going to run into Auto memory very quickly so you got to think about and and address these failure",
    "start": "1080360",
    "end": "1086799"
  },
  {
    "text": "modes uh ahead of time with that said let me invite uh",
    "start": "1086799",
    "end": "1093159"
  },
  {
    "start": "1090000",
    "end": "1235000"
  },
  {
    "text": "Steve to run to show us a quick demo um and uh so the the the the idea behind",
    "start": "1093159",
    "end": "1100159"
  },
  {
    "text": "this demo is if in case you are running an exception on the client side like a",
    "start": "1100159",
    "end": "1106799"
  },
  {
    "text": "throughput exceeded exception what do you do I talked about that one of the possibilities is that you haven't",
    "start": "1106799",
    "end": "1112799"
  },
  {
    "text": "provisioned enough shards on the server side which can happen because if your clients are are um scale up and down",
    "start": "1112799",
    "end": "1119520"
  },
  {
    "text": "rapidly and you don't have enough capacity on this on the Kinesis side you might run into througho exceeded exceptions so with that said right now",
    "start": "1119520",
    "end": "1127039"
  },
  {
    "text": "let me pass it over to my uh to Steve and he will explain what's going on on this chart Steve so this is a dashboard",
    "start": "1127039",
    "end": "1133880"
  },
  {
    "text": "we put together that receives metrics from both Kinesis producers and consumers",
    "start": "1133880",
    "end": "1139159"
  },
  {
    "text": "uh the first graph in the first row shows the number of shards we currently have in our stream which is 10 the",
    "start": "1139159",
    "end": "1144559"
  },
  {
    "text": "second graph shows the number of throughput exceeded exceptions we're getting for rights into Kinesis and this",
    "start": "1144559",
    "end": "1150360"
  },
  {
    "text": "is around 80 to about 150 so for the sake of the demo we've intentionally come in sending more data than the",
    "start": "1150360",
    "end": "1156080"
  },
  {
    "text": "stream can handle so the SEC the first graph in the second row shows the number of puts per",
    "start": "1156080",
    "end": "1162039"
  },
  {
    "text": "second we're doing which looks like it's 150 to 200 per second the second graph",
    "start": "1162039",
    "end": "1167200"
  },
  {
    "text": "is the number of 1 k by of events per second we're sending to Kinesis so in order to improve throughput we pack as",
    "start": "1167200",
    "end": "1172960"
  },
  {
    "text": "many events into a single put as possible which is why the number of events is about 50 times the number of",
    "start": "1172960",
    "end": "1179919"
  },
  {
    "text": "puts and this final row is the number of is the two consumer applications we have sending data to Kinesis two two consumer",
    "start": "1180559",
    "end": "1188640"
  },
  {
    "text": "applications receiving data from Kinesis and they're what we call archivers they read from Kinesis and write to S3",
    "start": "1188640",
    "end": "1194559"
  },
  {
    "text": "they're both running on top of the KCl and you can see that they're both getting about an even distribution of the data so for the first demo we're",
    "start": "1194559",
    "end": "1201880"
  },
  {
    "text": "going to focus only on this first row so again we're sending more data than our stream can currently handle so what",
    "start": "1201880",
    "end": "1207480"
  },
  {
    "text": "we're going to do is we're going to double the number of shards in our stream to 20 and then you'll see that",
    "start": "1207480",
    "end": "1212640"
  },
  {
    "text": "the number of throughput exceptions will come down to about zero so we discovered it takes about 30 seconds after you",
    "start": "1212640",
    "end": "1218080"
  },
  {
    "text": "split one Shard before you can split another Shard because the stream goes into an updating state so I'm going to",
    "start": "1218080",
    "end": "1223480"
  },
  {
    "text": "hand it back off to yasa and then a little later we'll come back and we'll see where exceptions are at now thank",
    "start": "1223480",
    "end": "1230640"
  },
  {
    "text": "you Steve so the idea behind that demo by the way this is live guys this is not uh um this is not a PowerPoint this is",
    "start": "1230640",
    "end": "1237080"
  },
  {
    "start": "1235000",
    "end": "1259000"
  },
  {
    "text": "actually a real life demo",
    "start": "1237080",
    "end": "1241760"
  },
  {
    "text": "um uh it's not can you see if it's hit the play",
    "start": "1244039",
    "end": "1250080"
  },
  {
    "text": "button all right thank you so we've seen some of these issues happening so the we",
    "start": "1257120",
    "end": "1264360"
  },
  {
    "text": "purposefully to demonstrate in a control environment we've created the Kinesis stream with the fewer shards and we",
    "start": "1264360",
    "end": "1269520"
  },
  {
    "text": "bombarding it with the clients and so we were seeing these exceptions and what he's going to do in the meantime is",
    "start": "1269520",
    "end": "1274840"
  },
  {
    "text": "going to do some magic is scaling the shards in the back end so let's come back and see what happens to the",
    "start": "1274840",
    "end": "1280279"
  },
  {
    "text": "throughput exceptions when we come back in the meantime so how do we mitigate some of these connection some of these",
    "start": "1280279",
    "end": "1286279"
  },
  {
    "text": "clients side issues well before you get started assess yourself",
    "start": "1286279",
    "end": "1291919"
  },
  {
    "text": "uh what kind of Tolerance you have for data loss it you know sometimes if it is a research kind of data you might not",
    "start": "1291919",
    "end": "1297159"
  },
  {
    "text": "have to go all the way but if it's Mission critical data then you do want to go all the way so here are some tips",
    "start": "1297159",
    "end": "1303799"
  },
  {
    "text": "um Amazon supports SDK supports uh back off on retry policies this is comes uh",
    "start": "1303799",
    "end": "1310640"
  },
  {
    "text": "you know standard with the SDK you can provide your own implementation back off is how long in milliseconds to back off",
    "start": "1310640",
    "end": "1316679"
  },
  {
    "text": "between retries and retry is how many times you want to retry when you are having exceptions uh reaching out the",
    "start": "1316679",
    "end": "1323559"
  },
  {
    "text": "reaching to the Kinesis stream the second thing is if in case the somehow is not reachable then you want to",
    "start": "1323559",
    "end": "1330000"
  },
  {
    "text": "consider buffering to the local disk because your client probably can't host that much data in memory so you want to",
    "start": "1330000",
    "end": "1336440"
  },
  {
    "text": "put it on the local dis somewhere on the on the machine that's generating the data now if you paranoid about losing",
    "start": "1336440",
    "end": "1343520"
  },
  {
    "text": "the client side dis uh and this is critical data you might you might want to consider making multiple copies on",
    "start": "1343520",
    "end": "1349320"
  },
  {
    "text": "the client side there's several Java Frameworks that are out there I'm not going to name any there's plenty enough",
    "start": "1349320",
    "end": "1354600"
  },
  {
    "text": "um that you can use to make multiple copies on the client side and then the next option for you to consider is",
    "start": "1354600",
    "end": "1361039"
  },
  {
    "text": "parallel puts if for some reason Kinesis is is down it's not reachable then kesis",
    "start": "1361039",
    "end": "1366440"
  },
  {
    "text": "is now available in multiple regions so you can either actually go go to a different region in Kinesis uh and",
    "start": "1366440",
    "end": "1373039"
  },
  {
    "text": "alternatively we we have another service of our own prior to jumping on the kesis bandwagon we did we built uh software",
    "start": "1373039",
    "end": "1381720"
  },
  {
    "text": "stack based on Flume and packaged up on ec2 and under autoscaling groups attached to the elb it is now still on",
    "start": "1381720",
    "end": "1389480"
  },
  {
    "text": "it's always on it's dormant it can it can scale up on demand if in case we experience any failures uh so use what",
    "start": "1389480",
    "end": "1398440"
  },
  {
    "text": "strategy that makes sense I'm sure there's there's plenty more that you can think of a few bookkeeping",
    "start": "1398440",
    "end": "1403480"
  },
  {
    "text": "considerations the first thing is the uuid I already touched on this one if you are paranoid about data loss and you",
    "start": "1403480",
    "end": "1409760"
  },
  {
    "text": "need to know pinpoint exactly where it's happening I'd recommend you to log a uu ID perhaps an md5 of the payload and few",
    "start": "1409760",
    "end": "1417400"
  },
  {
    "text": "other meta information that you can use later on during your audit um the other things to consider is",
    "start": "1417400",
    "end": "1423919"
  },
  {
    "text": "a timestamp if you send in a timestamp our archive application takes the data out of Kinesis writes it into S3 but",
    "start": "1423919",
    "end": "1430679"
  },
  {
    "text": "using the timestamp it very cleanly puts them into hourly buckets so our map reduce programs that kick in later on",
    "start": "1430679",
    "end": "1437559"
  },
  {
    "text": "can actually processed by the hour and and we also have a place to to address",
    "start": "1437559",
    "end": "1442640"
  },
  {
    "text": "the late arriving events which can happen in some cases because of the complexity of the internet and then this",
    "start": "1442640",
    "end": "1449679"
  },
  {
    "text": "also you can think about clientside encryption security and privacy is a big thing and uh if there is uh uh you know",
    "start": "1449679",
    "end": "1456279"
  },
  {
    "text": "if there are things you concerned about privacy you can inject your own Keys encrypt your payloads when you're",
    "start": "1456279",
    "end": "1462000"
  },
  {
    "text": "writing it into kesis and out of reading out of kesis and then the record order is something that I okay Ricardo is",
    "start": "1462000",
    "end": "1468960"
  },
  {
    "text": "going to come back and talk more extensively about so I'm going to skip on that one skip over that one and now",
    "start": "1468960",
    "end": "1475159"
  },
  {
    "start": "1475000",
    "end": "1548000"
  },
  {
    "text": "let's talk about Kinesis um managing client performance so very first time we tried Kinesis we had issues uh I'll be",
    "start": "1475159",
    "end": "1483039"
  },
  {
    "text": "honest with you so the performance and and and because it it didn't have batch APS at the time we were evaluating this",
    "start": "1483039",
    "end": "1488679"
  },
  {
    "text": "product we had some issues with in terms of throw put what that meant was we needed lot more server lot more machines",
    "start": "1488679",
    "end": "1495760"
  },
  {
    "text": "on the client side to handle the same kind of volume so we got on the phone with Kinesis team and brainstormed ideas",
    "start": "1495760",
    "end": "1502159"
  },
  {
    "text": "and one of the ideas that came out of that discussion was to come up with this batch uh bite buffer concept so we",
    "start": "1502159",
    "end": "1509559"
  },
  {
    "text": "package multiple log lines into one bite buffer we compress it and send it over",
    "start": "1509559",
    "end": "1515279"
  },
  {
    "text": "to Kinesis and it works like a charm and that's how we've been able to do a million TPS a second and it Kinesis",
    "start": "1515279",
    "end": "1523919"
  },
  {
    "text": "scales really well and I have data to prove it by the way along the way we also",
    "start": "1523919",
    "end": "1531000"
  },
  {
    "text": "improved a lot of garbage collection because this is a lightweight we developed a flume Kinesis plugin that we",
    "start": "1531000",
    "end": "1537240"
  },
  {
    "text": "simply drop it into The Flume that's running on our rtb engine and that directly sends it to Kinesis without any",
    "start": "1537240",
    "end": "1543559"
  },
  {
    "text": "other hops so that's how we have addressed our client performance issues U moving on the aggregator so",
    "start": "1543559",
    "end": "1551399"
  },
  {
    "start": "1548000",
    "end": "1567000"
  },
  {
    "text": "we've talked about uh failure modes on on the on the client side now let's touch upon the aggregator side",
    "start": "1551399",
    "end": "1558360"
  },
  {
    "text": "we look at this in three different ways one is the availability durability and the second one is the capacity so let's",
    "start": "1558360",
    "end": "1565640"
  },
  {
    "text": "look at each one of them Kinesis just like any other uh",
    "start": "1565640",
    "end": "1570799"
  },
  {
    "start": "1567000",
    "end": "1603000"
  },
  {
    "text": "Amazon service comes with availability and durability that you can trust Dynamo",
    "start": "1570799",
    "end": "1576039"
  },
  {
    "text": "DB for example has got the availability and the durability and so does Kinesis um so before they return the",
    "start": "1576039",
    "end": "1583679"
  },
  {
    "text": "call back to the client they make multiple copies in the in in any given re so you have multiple copies for",
    "start": "1583679",
    "end": "1589760"
  },
  {
    "text": "durability as well as availability and the system is designed to self heal or",
    "start": "1589760",
    "end": "1595159"
  },
  {
    "text": "do any repairs on its own so you don't have to deal with these things so this",
    "start": "1595159",
    "end": "1600399"
  },
  {
    "text": "this part is actually done for you second part is the capacity so today",
    "start": "1600399",
    "end": "1607919"
  },
  {
    "start": "1603000",
    "end": "1753000"
  },
  {
    "text": "in Kinesis if you the unit of uh scalability is a Shard each Shard can",
    "start": "1607919",
    "end": "1614360"
  },
  {
    "text": "handle about 1 Megabyte per second or about 1,000 TPS and on the on the consumer side when you",
    "start": "1614360",
    "end": "1620799"
  },
  {
    "text": "are reading data out of Kinesis stream you can have up to 2 megabytes per second or 24-hour pre playay so in order",
    "start": "1620799",
    "end": "1628120"
  },
  {
    "text": "to Shard because we we want we we we uh uh built a heavy automation around this",
    "start": "1628120",
    "end": "1633919"
  },
  {
    "text": "thing and the automation is because we don't know how much the clients are going to be doing sometimes the peak is",
    "start": "1633919",
    "end": "1639840"
  },
  {
    "text": "you know exceeds the previous peak in in in next couple of hours right we don't we don't know how how much we can expect",
    "start": "1639840",
    "end": "1645960"
  },
  {
    "text": "how much volume to we will will be rece T so we've come up with an automated solution to scale the shots and here is",
    "start": "1645960",
    "end": "1651679"
  },
  {
    "text": "how it works Kinesis cloudwatch metric there is the something called put record. bytes this simply tells you the",
    "start": "1651679",
    "end": "1658480"
  },
  {
    "text": "bytes that you're writing into the shards so if you take a sum of all the shards you know the throughput that",
    "start": "1658480",
    "end": "1663880"
  },
  {
    "text": "you're pumping through your kous shards let's just do a simple math if you have two shards in theory it can hand handle",
    "start": "1663880",
    "end": "1671679"
  },
  {
    "text": "up to 2 megabytes and put record bytes will tell you how much it's actually processing if you close to saturation if",
    "start": "1671679",
    "end": "1678360"
  },
  {
    "text": "you filled up your capacity then you might want to consider expanding The Shard how do you expand The Shard well",
    "start": "1678360",
    "end": "1683760"
  },
  {
    "text": "there is two operations called split and merge split is when you want to add capacity you take one of the existing",
    "start": "1683760",
    "end": "1690159"
  },
  {
    "text": "shards split them into two at the midpoint then you have two new shards replacing the previous one so you just",
    "start": "1690159",
    "end": "1695760"
  },
  {
    "text": "went from two to three that's what the picture shows and then if you don't want that much capacity overnight you can",
    "start": "1695760",
    "end": "1701679"
  },
  {
    "text": "shrink it back down you do the opposite which is called merge you take two and you combine it into one and you can can",
    "start": "1701679",
    "end": "1707559"
  },
  {
    "text": "do this all day all day long if you want with Automation and but be careful put",
    "start": "1707559",
    "end": "1712919"
  },
  {
    "text": "records. bite is not the only one to drive your your scalability and here is why if uh so from time to time you might",
    "start": "1712919",
    "end": "1720600"
  },
  {
    "text": "receive throughput exceeded exceptions on the client side that's normal as long as it is a you know couple here a couple",
    "start": "1720600",
    "end": "1726480"
  },
  {
    "text": "there that's fine that's why you have retry mechanism but if you are receiving throughput exceeded exceptions and guess",
    "start": "1726480",
    "end": "1733200"
  },
  {
    "text": "what your Shard capacity is full that's a genuine reason to to scale",
    "start": "1733200",
    "end": "1738279"
  },
  {
    "text": "up right so you look at both of these together to make the decision and we",
    "start": "1738279",
    "end": "1744279"
  },
  {
    "text": "have scripts we have deploy it as Cloud watch alarms that can actually take these actions and our developers can",
    "start": "1744279",
    "end": "1750360"
  },
  {
    "text": "actually sleep at 3:30 in the morning by the way there are some tools in the GitHub you can actually take a",
    "start": "1750360",
    "end": "1756720"
  },
  {
    "start": "1753000",
    "end": "1919000"
  },
  {
    "text": "look at it if you are interested now if there is one slide that you want to take home it's this",
    "start": "1756720",
    "end": "1762480"
  },
  {
    "text": "one so the critical question that you always have to ask with the with the scaling",
    "start": "1762480",
    "end": "1767840"
  },
  {
    "text": "with large scale systems is will this scale will this scale as promised so",
    "start": "1767840",
    "end": "1773519"
  },
  {
    "text": "kenesis documentation says one megabyte per second per sh all right so we took",
    "start": "1773519",
    "end": "1778799"
  },
  {
    "text": "it upon ourselves to test it before we put it into production so we started",
    "start": "1778799",
    "end": "1783960"
  },
  {
    "text": "adding shards on the server side we ramped up our clients on the client side",
    "start": "1783960",
    "end": "1789240"
  },
  {
    "text": "and uh we went all the way up to a million mega million messages per second with a th shards on the server side okay",
    "start": "1789240",
    "end": "1796919"
  },
  {
    "text": "the X axis is the number of shards as we ramping up the Y AIS is the number of",
    "start": "1796919",
    "end": "1802320"
  },
  {
    "text": "messages that we are pumping through the system conveniently just to make the math easy each payload was 1 kiloby in",
    "start": "1802320",
    "end": "1809640"
  },
  {
    "text": "size ladies and gentlemen the red line is the picture perfect or textbook",
    "start": "1809640",
    "end": "1816320"
  },
  {
    "text": "linear scale and yellow line is how Kinesis scales and that's the proof is in the",
    "start": "1816320",
    "end": "1823320"
  },
  {
    "text": "Ping and get this the kicker is actually in the total cost of ownership so I did the back of the",
    "start": "1823320",
    "end": "1830720"
  },
  {
    "text": "envelope math and literally and what I came up with was for us we sending",
    "start": "1830720",
    "end": "1837640"
  },
  {
    "text": "million messages a second it turns out to be we pack 50 messages into one why",
    "start": "1837640",
    "end": "1842799"
  },
  {
    "text": "50 because 1 kilobyte message you can have 50 of them Kinesis allows 50 kilobytes per per record call so we pack",
    "start": "1842799",
    "end": "1850679"
  },
  {
    "text": "50 into one and then we if you apply compression to it at 10:1 ratio assuming",
    "start": "1850679",
    "end": "1856840"
  },
  {
    "text": "10 to1 one ratio then the cost of this product of of this service is",
    "start": "1856840",
    "end": "1863720"
  },
  {
    "text": "$635 and if you didn't do any compression for whatever reason or it's not compressible guess what you would",
    "start": "1864480",
    "end": "1870480"
  },
  {
    "text": "pay $228,000 which is a small price to pay for sending a million events per second",
    "start": "1870480",
    "end": "1878080"
  },
  {
    "text": "step back and think about what million events per second does it's 86 billion",
    "start": "1878080",
    "end": "1883279"
  },
  {
    "text": "events a day now the other good thing about Asing client is is you don't have to provision for your Peak you provision",
    "start": "1883279",
    "end": "1889760"
  },
  {
    "text": "for your average if it was a uh if it was non- Asing client then you have to",
    "start": "1889760",
    "end": "1895519"
  },
  {
    "text": "actually provision for your Peak capacity right otherwise you can't you know there's you need to survive the",
    "start": "1895519",
    "end": "1901120"
  },
  {
    "text": "system the system has to work through your Peak periods as well but with the Asing clients you don't have that you",
    "start": "1901120",
    "end": "1907960"
  },
  {
    "text": "can actually provision your average capacity at average of 1 million request per second 86 billion messages today",
    "start": "1907960",
    "end": "1916159"
  },
  {
    "text": "$28,000 is a small price to pay all right so let's see how Steve is",
    "start": "1916159",
    "end": "1922039"
  },
  {
    "start": "1919000",
    "end": "1962000"
  },
  {
    "text": "doing I guess uh he added more shards and let's see how uh the shards look",
    "start": "1922039",
    "end": "1927399"
  },
  {
    "text": "like now right so if you look at the first graph again you can see how every 30 seconds we were able to add a new",
    "start": "1927399",
    "end": "1933639"
  },
  {
    "text": "Shard because it looks like the steps and in the graph on the right you can see that going into the sharding process",
    "start": "1933639",
    "end": "1939399"
  },
  {
    "text": "we're about a little over 50 uh exceptions per second and now we're down",
    "start": "1939399",
    "end": "1944480"
  },
  {
    "text": "to zero so by doubling the number of shards we had in our stream we able to completely remove all exceptions we were",
    "start": "1944480",
    "end": "1950799"
  },
  {
    "text": "getting thank you Steve so again the proof is in the pudding guys he just scaled up the shots right in front of",
    "start": "1950799",
    "end": "1956880"
  },
  {
    "text": "you and the throughput executed exceptions went down to zero and that's a live demo",
    "start": "1956880",
    "end": "1962080"
  },
  {
    "start": "1962000",
    "end": "1983000"
  },
  {
    "text": "again so let's look at the failure modes on the processing side dealing with bad record there's two",
    "start": "1962080",
    "end": "1968200"
  },
  {
    "text": "classes of Errors right when you're processing processing is just as important as the client and the aggregator and everything else",
    "start": "1968200",
    "end": "1974919"
  },
  {
    "text": "processing to two major classes of problems bad records or you have a problem with the availability of the",
    "start": "1974919",
    "end": "1981080"
  },
  {
    "text": "processing nodes let's look at them so bad records dirty data guys who",
    "start": "1981080",
    "end": "1987320"
  },
  {
    "start": "1983000",
    "end": "2072000"
  },
  {
    "text": "work in the ETL databases dat Warehouse nothing new to them this is their life day in and day out now what do you do is",
    "start": "1987320",
    "end": "1995399"
  },
  {
    "text": "up to you there are some use cases and workloads that you have to uh address yourself and some of the things that you",
    "start": "1995399",
    "end": "2001519"
  },
  {
    "text": "would do is you might skip a skip a record you might put it away in an exceptions table and come back to it later or you might bring your processing",
    "start": "2001519",
    "end": "2008600"
  },
  {
    "text": "to a grinding halt it I can't you know this this use case is up to you so you",
    "start": "2008600",
    "end": "2013639"
  },
  {
    "text": "would have to figure that out now bad worker so when you're processing the data uh your application that's actually",
    "start": "2013639",
    "end": "2020159"
  },
  {
    "text": "draining the K Kinesis stream might have issues uh for example if you're doing",
    "start": "2020159",
    "end": "2025480"
  },
  {
    "text": "RTL when I say RTL it's like ETL except in real time and you are trying to do",
    "start": "2025480",
    "end": "2031320"
  },
  {
    "text": "some dimensional lookups you have a key that's coming in you want to substitute with a name and you want to do a DE",
    "start": "2031320",
    "end": "2036919"
  },
  {
    "text": "database look up and database is not available well you have a couple of options you can retry or you can shut",
    "start": "2036919",
    "end": "2043000"
  },
  {
    "text": "the processor down the metapo is this folks if you are having issues don't",
    "start": "2043000",
    "end": "2049358"
  },
  {
    "text": "checkpoint because checkpoint is actually an offset into the stream once you checkpoint the code that application",
    "start": "2049359",
    "end": "2056599"
  },
  {
    "text": "checkpoints the stream Kinesis is is going to assume that this application is done processing that data and you won't",
    "start": "2056599",
    "end": "2063720"
  },
  {
    "text": "give it to you anymore so the metap point is don't checkpoint if you haven't finished processing if",
    "start": "2063720",
    "end": "2069440"
  },
  {
    "text": "you have finished processing go ahead and checkpoint all right let's look at the",
    "start": "2069440",
    "end": "2074599"
  },
  {
    "text": "uh uh processing side a little bit of an animation to see how it works and then again we'll go back to the live demo and",
    "start": "2074599",
    "end": "2080200"
  },
  {
    "text": "see how it works in reality so this big yellow box in the left hand side is kinesis stream and uh there's an",
    "start": "2080200",
    "end": "2087200"
  },
  {
    "text": "invisible producer writing to this big block of uh stream and on the middle you",
    "start": "2087200",
    "end": "2093200"
  },
  {
    "text": "have Amazon Kinesis consumer Fleet so consumers are draining the data from from",
    "start": "2093200",
    "end": "2098680"
  },
  {
    "text": "Kinesis and uh this application we call it archiver is reading data from kesis",
    "start": "2098680",
    "end": "2105240"
  },
  {
    "text": "stream and writing into S3 so we don't we want this to be highly available because if we fall behind what happens",
    "start": "2105240",
    "end": "2112000"
  },
  {
    "text": "is our producers are blasting data at lightning speed we're going to fall behind so we have to make sure that this",
    "start": "2112000",
    "end": "2118320"
  },
  {
    "text": "KCl or this application is highly available we built this on top of KCl Kinesis client Library it comes with",
    "start": "2118320",
    "end": "2124800"
  },
  {
    "text": "Kinesis standard and uh the good things that it does for you is checkpointing so let's see how this",
    "start": "2124800",
    "end": "2131280"
  },
  {
    "text": "works in in in animation so here is the producer writing data and uh that's the",
    "start": "2131280",
    "end": "2137160"
  },
  {
    "text": "KCl worker reading data and then more data coming in and then KCl application reads that",
    "start": "2137160",
    "end": "2145280"
  },
  {
    "text": "data and now it does a checkpoint so basically in telling the stream that hey I'm done processing this",
    "start": "2145280",
    "end": "2152000"
  },
  {
    "text": "data and then it took that data and put it up in S3 that's what our Archer does",
    "start": "2152000",
    "end": "2157720"
  },
  {
    "text": "it takes the data copies it from kesis into S3 and then the checkpoint record appeared right under there which is uh",
    "start": "2157720",
    "end": "2164920"
  },
  {
    "text": "it does the Dynamo DB table on the bottom right hand side of the screen and it it it added a row there KCl does this",
    "start": "2164920",
    "end": "2172000"
  },
  {
    "text": "for you you don't have to do it that's what the checkpointing does it makes an entry there and now more data comes",
    "start": "2172000",
    "end": "2180640"
  },
  {
    "text": "in KCl Reddit we are at item number 18",
    "start": "2181240",
    "end": "2187280"
  },
  {
    "text": "more data coming in it's still reading it guess what we",
    "start": "2187280",
    "end": "2193800"
  },
  {
    "text": "failed the host failed whatever happened the the KCl worker node or archive",
    "start": "2193800",
    "end": "2200079"
  },
  {
    "text": "application that was processing the data taking it from kesis and copying it in S3 just failed so let's see what happens",
    "start": "2200079",
    "end": "2207560"
  },
  {
    "text": "but remember the last checkpoint was done at 10 so we have no fear the data hasn't gone anywhere so what happens",
    "start": "2207560",
    "end": "2214280"
  },
  {
    "text": "then is the new KCl Pops in place by Autos scaling group it looks at the check previous",
    "start": "2214280",
    "end": "2221280"
  },
  {
    "text": "checkpoint which is 10 takes ownership of the data starts reading everything after",
    "start": "2221280",
    "end": "2227359"
  },
  {
    "text": "10 so let's actually look at this in real live demo",
    "start": "2227359",
    "end": "2235078"
  },
  {
    "text": "Steve so for this demo we'll focus on this last uh row here so we want to",
    "start": "2238480",
    "end": "2244040"
  },
  {
    "text": "highlight the load balancing fault tolerance that comes out of the box when you used the KCl so what we did was I",
    "start": "2244040",
    "end": "2249560"
  },
  {
    "text": "went in and killed the process running on arer one so it's not processing anything now and you can see that arer 2",
    "start": "2249560",
    "end": "2255640"
  },
  {
    "text": "is beginning to start picking up some of its load so it won't happen immediately because we've configured our",
    "start": "2255640",
    "end": "2260920"
  },
  {
    "text": "applications to take about five minutes to fail over to find new leases that have been orphaned but",
    "start": "2260920",
    "end": "2267960"
  },
  {
    "text": "um it'll start picking up the events and uh take over the load of the previous",
    "start": "2267960",
    "end": "2273400"
  },
  {
    "text": "archiver as the load gets balanced over to the second one so that there you go you have a second",
    "start": "2273400",
    "end": "2279520"
  },
  {
    "text": "Arch picking up the load uh from from from the previous checkpoint and and",
    "start": "2279520",
    "end": "2285079"
  },
  {
    "text": "moving on thank you Steve so let's uh go",
    "start": "2285079",
    "end": "2291000"
  },
  {
    "start": "2288000",
    "end": "2298000"
  },
  {
    "text": "through the rest of the failure modes in storage uh rard already mentioned that KCl keeps data in the Stream for about",
    "start": "2291000",
    "end": "2297319"
  },
  {
    "text": "24 hours and uh if you don't want to lose your data so what kesis allows you to do is small window analytics what do",
    "start": "2297319",
    "end": "2305040"
  },
  {
    "start": "2298000",
    "end": "2363000"
  },
  {
    "text": "I mean by that you can look into the stream and say what's happening every fifth minute of the hour or you can look",
    "start": "2305040",
    "end": "2310839"
  },
  {
    "text": "at what happens at the top of the hour every hour for the last 24 hours and the",
    "start": "2310839",
    "end": "2315880"
  },
  {
    "text": "best part is you can also take the data and combine with the rest of your larger analysis by putting it into S3 you can",
    "start": "2315880",
    "end": "2321480"
  },
  {
    "text": "run your favorite hi query or any map reduce application and make that data available as part of a much larger",
    "start": "2321480",
    "end": "2327599"
  },
  {
    "text": "analysis um so the reason why we use archiver is if you're directly processing it from the stream and if you",
    "start": "2327599",
    "end": "2334079"
  },
  {
    "text": "have a release regression every once in a while you have that right in the engineering organization what you want",
    "start": "2334079",
    "end": "2339240"
  },
  {
    "text": "is your brain dead application that does not change or rarely changes it knows",
    "start": "2339240",
    "end": "2345440"
  },
  {
    "text": "how to copy data from point A to point B so we don't have release regressions or anything with that it's like a Cas in",
    "start": "2345440",
    "end": "2351920"
  },
  {
    "text": "stone kind of an application so that's what we do for our uh to preserve the data because this is",
    "start": "2351920",
    "end": "2358000"
  },
  {
    "text": "Mission critical data that we also need this as part of a much larger analysis",
    "start": "2358000",
    "end": "2363520"
  },
  {
    "text": "um and then I just want to let you know that the data so we've been at this about year and a half two years we're",
    "start": "2363520",
    "end": "2370079"
  },
  {
    "text": "building this marketing cloud and uh this is how it looks today uh we use a CU bow for it uh for the for the ad hoc",
    "start": "2370079",
    "end": "2377359"
  },
  {
    "text": "analysis and map reduce for all our M production workloads Kinesis is our uh",
    "start": "2377359",
    "end": "2382800"
  },
  {
    "text": "main uh realtime ingestion we are working we have a lot more to go like I",
    "start": "2382800",
    "end": "2388000"
  },
  {
    "text": "said we are building this marketing Cloud we just got started uh we have very ambitious plans of building our uh",
    "start": "2388000",
    "end": "2395520"
  },
  {
    "text": "marketing cloud and guess what I'm going to shamelessly advertise that we are looking for top talent and uh if you",
    "start": "2395520",
    "end": "2402160"
  },
  {
    "text": "want to join the awesome engineering team at deru uh please come see us we are hiring and uh let me invite uh",
    "start": "2402160",
    "end": "2410920"
  },
  {
    "text": "Ricardo back up he's got a few scenarios the order processing and the EMR to cover thank you",
    "start": "2410920",
    "end": "2419480"
  },
  {
    "text": "yeah I know I just have a couple minutes here but we are going to be available for questions and going to talk about",
    "start": "2421400",
    "end": "2426560"
  },
  {
    "text": "some more failure modes here uh basically some of the next steps that Da Zoo is actually going to take uh as we",
    "start": "2426560",
    "end": "2431920"
  },
  {
    "text": "start looking into potential issues that you could have with record order and record replay uh so we talked about",
    "start": "2431920",
    "end": "2438040"
  },
  {
    "start": "2432000",
    "end": "2635000"
  },
  {
    "text": "scaling to multiple shards uh as you know uh the the actual record sequence is maintained at A Shard level right so",
    "start": "2438040",
    "end": "2444800"
  },
  {
    "text": "if order ordered processing of the records is important this is something that you need to take into consideration",
    "start": "2444800",
    "end": "2450079"
  },
  {
    "text": "and it all starts with the record so the sequence number uh the partition key that you choose for your before you do",
    "start": "2450079",
    "end": "2457480"
  },
  {
    "text": "your put record dictates which Shard that uh record is going to end up in and which worker is going to process so if",
    "start": "2457480",
    "end": "2464480"
  },
  {
    "text": "you're looking if you have unordered processing records and there's plenty of use cases that the order of the records",
    "start": "2464480",
    "end": "2469800"
  },
  {
    "text": "don't really matter it's how many of them you have for example in a given time frame it can randomize your partition key and make use of as many",
    "start": "2469800",
    "end": "2476240"
  },
  {
    "text": "shards thousand shards like case I mentioned uh and as many workers as you want not a problem if exact order",
    "start": "2476240",
    "end": "2482839"
  },
  {
    "text": "processing is an issue is a requirement for example if you're doing fraud analysis so if you want to have a",
    "start": "2482839",
    "end": "2488240"
  },
  {
    "text": "targeted uh ad that shows up at a specific behavior uh then you need to control the partition key that's the",
    "start": "2488240",
    "end": "2494920"
  },
  {
    "text": "mitigation here be aware of the partition key and its implication on how it controls the rec where the record",
    "start": "2494920",
    "end": "2500400"
  },
  {
    "text": "will get placed if you need both consider getting a global sequence number outside of Kinesis you can use",
    "start": "2500400",
    "end": "2506280"
  },
  {
    "text": "for example an atomic counter in Dynamo DB get that sequence number put it as part of the record payload and then use",
    "start": "2506280",
    "end": "2512400"
  },
  {
    "text": "that in Downstream to kind of put the sequence back together okay now as you all know if you've been following our",
    "start": "2512400",
    "end": "2518800"
  },
  {
    "text": "blog we have a new uh enhancement a new feature that's been released for Dynamo DB which is the the streams and it kind",
    "start": "2518800",
    "end": "2525160"
  },
  {
    "text": "of impacts this this this pattern here and this is something that we're going to be blogging more about in our big data blog so if you're interested in",
    "start": "2525160",
    "end": "2531720"
  },
  {
    "text": "this pattern the details on this pattern and how it relates with the new feature now available of Dynamo theb definitely",
    "start": "2531720",
    "end": "2538160"
  },
  {
    "text": "che check out our our big data blog um in our na AWS we'll be talking about",
    "start": "2538160",
    "end": "2543359"
  },
  {
    "text": "those another thing that you see here as well is not every um destination of the",
    "start": "2543359",
    "end": "2550359"
  },
  {
    "text": "record is the same for every producer depending on what event type you're sending it may make sense to use a certain type of partition key so it's a",
    "start": "2550359",
    "end": "2557319"
  },
  {
    "text": "good idea to have a very low latency service that tells you what that is and then is good for that uh a quick lookup",
    "start": "2557319",
    "end": "2563240"
  },
  {
    "text": "to figure out hey I'm I'm a certain type of event to where should I put this record and what type of partition key",
    "start": "2563240",
    "end": "2569240"
  },
  {
    "text": "should I use this is again a very good pattern that allows you it keeps the latency uh meets the latency",
    "start": "2569240",
    "end": "2574599"
  },
  {
    "text": "requirements but gives you that Flex ability to dictate where the records go um another thing another pattern that",
    "start": "2574599",
    "end": "2582079"
  },
  {
    "text": "we want to talk about here that allows essentially an infinite replay of those records is this EMR based approach so",
    "start": "2582079",
    "end": "2588359"
  },
  {
    "text": "you all are probably very familiar with reading lots of records from in a very fast distributed way from S3 so the",
    "start": "2588359",
    "end": "2595319"
  },
  {
    "text": "approach that we take here in this pattern is to pick up the records that have been archived there and play them",
    "start": "2595319",
    "end": "2601240"
  },
  {
    "text": "back into the original streams now the way that you do this is you your EMR",
    "start": "2601240",
    "end": "2606359"
  },
  {
    "text": "jobs tag the record payload with an event ID so that Downstream applications",
    "start": "2606359",
    "end": "2612040"
  },
  {
    "text": "that don't care about the past it can ignore that record and if you do that right at the header right in the front",
    "start": "2612040",
    "end": "2617720"
  },
  {
    "text": "of the record payload you can save time you don't have to unpack the record you serializing and do all those things this",
    "start": "2617720",
    "end": "2623240"
  },
  {
    "text": "is a very nice pattern because it allows you that at any point in time you can play back records from last week last",
    "start": "2623240",
    "end": "2628400"
  },
  {
    "text": "year whatever and again this is something that we're going to be providing more details uh in our",
    "start": "2628400",
    "end": "2634760"
  },
  {
    "text": "blog so what have we mitigated for today it's just the beginning like youa said you",
    "start": "2634760",
    "end": "2640160"
  },
  {
    "start": "2635000",
    "end": "2708000"
  },
  {
    "text": "probably thought of other scenarios i' love to hear about them uh in our Q&A session later on but we talked about",
    "start": "2640160",
    "end": "2645359"
  },
  {
    "text": "first of all relying on our SDK right the fact that we have async clients the fact that we have a some pretty good",
    "start": "2645359",
    "end": "2651680"
  },
  {
    "text": "robustness already at the client level uh that you can mitigate for uh the issues that we described there",
    "start": "2651680",
    "end": "2657319"
  },
  {
    "text": "aggregated mitigation is the easiest use Kinesis that's it Kinesis and KCl that's the easiest step on the slide here the",
    "start": "2657319",
    "end": "2663920"
  },
  {
    "text": "processing layer go back to what youa mentioned ask yourself you know what",
    "start": "2663920",
    "end": "2669040"
  },
  {
    "text": "sort of record loss ratio can you withstand right is it okay to just keep",
    "start": "2669040",
    "end": "2674800"
  },
  {
    "text": "processing or do you have to stop and put the records aside and go back and analyze it so that's again this a",
    "start": "2674800",
    "end": "2680920"
  },
  {
    "text": "business decision that you need to make and KCl can support that uh from a storage perspective an S3 archiva should",
    "start": "2680920",
    "end": "2687520"
  },
  {
    "text": "be part of every you know uh Mission critical Kinesis application uh Fleet",
    "start": "2687520",
    "end": "2692760"
  },
  {
    "text": "and this EMR based infinite replay pattern is against something that you can use uh you know very efficiently so",
    "start": "2692760",
    "end": "2699400"
  },
  {
    "text": "with that uh thank you for coming to the session we'll be available over there uh for some Q&A and um please give us",
    "start": "2699400",
    "end": "2706440"
  },
  {
    "text": "feedback thank you so much",
    "start": "2706440",
    "end": "2710558"
  }
]