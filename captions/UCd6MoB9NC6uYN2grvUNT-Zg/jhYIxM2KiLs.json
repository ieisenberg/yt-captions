[
  {
    "text": "hello thanks for joining me at the end of the day my name is Ryan Dean hi I'm a",
    "start": "1399",
    "end": "7529"
  },
  {
    "text": "product manager for the Amazon Kinesis team with me we have Vinayak let hongdae",
    "start": "7529",
    "end": "12599"
  },
  {
    "text": "who is where our customer autodesk is going to talk about with me architecting for real-time insights with amazon",
    "start": "12599",
    "end": "18779"
  },
  {
    "text": "Kinesis so the first part of the talk I'm gonna give an overview about what",
    "start": "18779",
    "end": "24630"
  },
  {
    "text": "streaming data is and a couple of our couple of our services that customers used for real-time analytics including",
    "start": "24630",
    "end": "29869"
  },
  {
    "text": "Amazon Elastic service and Amazon Kinesis data analytics and then Vanna is",
    "start": "29869",
    "end": "35100"
  },
  {
    "text": "gonna go into the Autodesk architecture for their unified lobbying platform and",
    "start": "35100",
    "end": "40590"
  },
  {
    "text": "how they solve monitoring their customer experience so before I get begin though",
    "start": "40590",
    "end": "46489"
  },
  {
    "text": "who uses Amazon Elastic search service great show of hands who has used one of",
    "start": "46489",
    "end": "54390"
  },
  {
    "text": "the Amazon Kinesis services awesome all right great so for some of you will",
    "start": "54390",
    "end": "60960"
  },
  {
    "text": "this will be the first part of the talk will be a little bit of overview so I'll try to make it quick but let's get",
    "start": "60960",
    "end": "66810"
  },
  {
    "text": "started so first let's talk about what streaming data is so streaming data is",
    "start": "66810",
    "end": "73380"
  },
  {
    "text": "data that is generated at high volume continuously and then typically captured",
    "start": "73380",
    "end": "80159"
  },
  {
    "text": "and processed in an ordered and incremental manner with low latency let's unpack that a little bit so when",
    "start": "80159",
    "end": "87270"
  },
  {
    "text": "we talk about streaming data being generated continuously most data is generated continuously but these are",
    "start": "87270",
    "end": "93240"
  },
  {
    "text": "things from like mobile devices IOT sensors application logs the key that",
    "start": "93240",
    "end": "98400"
  },
  {
    "text": "makes it streaming is if you capture it and continuously capture it and continuously process it for low latency",
    "start": "98400",
    "end": "105360"
  },
  {
    "text": "for at least four Kinesis customers when we talk about low latency the this typically means processing data in",
    "start": "105360",
    "end": "112439"
  },
  {
    "text": "approximately one second to do something with it and that could be sending it to the last search cluster or a database or",
    "start": "112439",
    "end": "119430"
  },
  {
    "text": "a monitoring tool like cloud watch typically customers their end-to-end pipelines are on the order of a couple",
    "start": "119430",
    "end": "125280"
  },
  {
    "text": "seconds but we do have like a subset of our customers that process and react to the data sub second or even sub 200-300",
    "start": "125280",
    "end": "133150"
  },
  {
    "text": "milliseconds so why do customers adopt",
    "start": "133150",
    "end": "141790"
  },
  {
    "text": "streaming kata to technologies but the primary reason is they want to get more timely insights and the simple analogy I",
    "start": "141790",
    "end": "149290"
  },
  {
    "text": "have for this is customers want to know a business report generated now is more",
    "start": "149290",
    "end": "155560"
  },
  {
    "text": "valuable but than a business report generated 60 days ago additionally if you can detect and react to a problem",
    "start": "155560",
    "end": "162730"
  },
  {
    "text": "with your customer experience immediately that's much more valuable than finding out after the fact that a",
    "start": "162730",
    "end": "169840"
  },
  {
    "text": "customer left your website or your customer had a poor customer experience or maybe they couldn't find exactly what",
    "start": "169840",
    "end": "176200"
  },
  {
    "text": "they were looking for it on your mobile app so Amazon Kinesis is a set of",
    "start": "176200",
    "end": "182859"
  },
  {
    "text": "services that make it easy to work with streaming data on AWS the focus of the talk is going to be on our three data",
    "start": "182859",
    "end": "188859"
  },
  {
    "text": "services the first service is Amazon Kinesis data streams it enables you to capture and",
    "start": "188859",
    "end": "195220"
  },
  {
    "text": "process streaming data in real time secondly Kinesis data firehose allows",
    "start": "195220",
    "end": "200769"
  },
  {
    "text": "you to capture and then deliver data in real time so Kinesis data firehose is",
    "start": "200769",
    "end": "206889"
  },
  {
    "text": "primarily about data movement from point A to point B and it customers typically use it where they're doing processing at",
    "start": "206889",
    "end": "213310"
  },
  {
    "text": "the destination so for example firehose delivers to Amazon Elastic service the",
    "start": "213310",
    "end": "218500"
  },
  {
    "text": "bulk of the processing is done in elasticsearch in that case for customers that do want to process data as it's",
    "start": "218500",
    "end": "225310"
  },
  {
    "text": "moving we have another service called Kinesis data analytics and enlisted allows you to analyze the streaming data",
    "start": "225310",
    "end": "231099"
  },
  {
    "text": "in real time with either sequel or java code so when customers use Amazon",
    "start": "231099",
    "end": "239919"
  },
  {
    "text": "Kinesis data streams they create a stream they have a set of producers that write to that stream and a set of",
    "start": "239919",
    "end": "245349"
  },
  {
    "text": "consumers that read from the stream and then do something with it back to the continuous nature of both",
    "start": "245349",
    "end": "250840"
  },
  {
    "text": "the producers and the consumers are acting continuously when you write data",
    "start": "250840",
    "end": "258940"
  },
  {
    "text": "to a Kinesis data stream or even in Canisius data firehose there's a variety of options that are provided to you by",
    "start": "258940",
    "end": "264900"
  },
  {
    "text": "far the most popular the AWS SDKs however there's a lot of",
    "start": "264900",
    "end": "270310"
  },
  {
    "text": "other options the vim is gonna talk a little bit about the keys this agent which will tail a log file to send it to",
    "start": "270310",
    "end": "277210"
  },
  {
    "text": "Kinesis Data Services we have another library called the Kinesis producer library which is specifically designed",
    "start": "277210",
    "end": "282490"
  },
  {
    "text": "for high-throughput data producers think like a giant application server we also",
    "start": "282490",
    "end": "288160"
  },
  {
    "text": "have managed solutions for forwarding data to Kinesis things like cloud watch logs AWS iot and a number of other",
    "start": "288160",
    "end": "294790"
  },
  {
    "text": "services but there's a lot of different ways to get into data Kinesis after it's there you have a lot of options to",
    "start": "294790",
    "end": "303760"
  },
  {
    "text": "process it let's first walk through a little bit what happens after we data is ingested into Kinesis data stream so",
    "start": "303760",
    "end": "310180"
  },
  {
    "text": "Kinesis will durably store the data after it receives it from you across three availability zones",
    "start": "310180",
    "end": "316210"
  },
  {
    "text": "it essentially hits three discs data is stored in an ordered fashion such that",
    "start": "316210",
    "end": "322090"
  },
  {
    "text": "when you attach a consumer to a stream that consumer always reads the data in the same order that it was persisted so",
    "start": "322090",
    "end": "330070"
  },
  {
    "text": "this is one of the primary differences between a stream and the queue in the sense that the order is always",
    "start": "330070",
    "end": "335380"
  },
  {
    "text": "maintained in addition to that you can attach multiple consumers that are reading from different parts of the stream so you might have for example in",
    "start": "335380",
    "end": "343060"
  },
  {
    "text": "AWS lambda function that is processing the data at the tip of the stream and a Kinesis data analytics application",
    "start": "343060",
    "end": "349570"
  },
  {
    "text": "that's consuming maybe an hour behind these are independently managed",
    "start": "349570",
    "end": "355140"
  },
  {
    "text": "consumers that they're processing and they're where they're processing from is tracked independently from the stream so",
    "start": "355140",
    "end": "364360"
  },
  {
    "text": "I mentioned Canisius data firehose is primarily about delivery ingest mechanisms are very similar to Canisius",
    "start": "364360",
    "end": "370300"
  },
  {
    "text": "data streams so we have several managed options as well as you can write with the agent or the AWS SDKs we have an",
    "start": "370300",
    "end": "377410"
  },
  {
    "text": "Apache Kafka connector that will forward data from Kafka to a canisius data firehose and then it'll deliver the data",
    "start": "377410",
    "end": "383560"
  },
  {
    "text": "to other four supported destinations so Amazon s3 Amazon redshift Amazon Elastic",
    "start": "383560",
    "end": "390220"
  },
  {
    "text": "service and Splunk along the way you can do very little processing from just",
    "start": "390220",
    "end": "396040"
  },
  {
    "text": "configuring how large of a file that you want in s3 or how frequently you want to day 2 test 3 too much more complex",
    "start": "396040",
    "end": "403389"
  },
  {
    "text": "processing so firehose will actually convert JSON data to Park a format which is a column or data format prior to",
    "start": "403389",
    "end": "409840"
  },
  {
    "text": "delivery s3 you can run ETL scripts the AWS lamda perform compression and much",
    "start": "409840",
    "end": "416229"
  },
  {
    "text": "more so you'll see over time the Kinesis team will gradually add more and more processing capabilities to Canisius data",
    "start": "416229",
    "end": "422830"
  },
  {
    "text": "firehose however for all most intensive purposes fire hose is used as streaming ETL the goal is to get the data to the",
    "start": "422830",
    "end": "430030"
  },
  {
    "text": "destination Amazon Elastic sir sorry",
    "start": "430030",
    "end": "436690"
  },
  {
    "text": "alas sirs with a Cabana is a very common tool for doing a search in analytics in real time building operational and",
    "start": "436690",
    "end": "443620"
  },
  {
    "text": "business dashboards Vinnie's gonna go through how autodesk uses it to build some of their what they call a single",
    "start": "443620",
    "end": "449410"
  },
  {
    "text": "pane of glass Amazon Elastic service",
    "start": "449410",
    "end": "455950"
  },
  {
    "text": "combines elasticity bonnet and offers it in a fully managed service that makes it easy to create deploy and scale masters",
    "start": "455950",
    "end": "463360"
  },
  {
    "text": "clusters when you work with the last research you create a cluster that cluster does everything from indexing",
    "start": "463360",
    "end": "469000"
  },
  {
    "text": "the exposes simple to advanced analytics analytical tools to you",
    "start": "469000",
    "end": "474570"
  },
  {
    "text": "the other thing Amazon Elastic search service does this is well integrated with the AWS ecosystem so for example",
    "start": "474570",
    "end": "481419"
  },
  {
    "text": "Amazon Kinesis data firehoses one of the services that Alaska search Amazon Alaska search service is",
    "start": "481419",
    "end": "486520"
  },
  {
    "text": "integrated with Kinesis data firehose will send data to elasticsearch if there's ever an issue with the delivery",
    "start": "486520",
    "end": "493570"
  },
  {
    "text": "or a problem maybe you have an IM role misconfigured or something along those lines will failover to s3 so that you",
    "start": "493570",
    "end": "500889"
  },
  {
    "text": "can backfill data there's an issue with any of your transformations so for example if you're using AWS lamda",
    "start": "500889",
    "end": "507010"
  },
  {
    "text": "perform that script along the way if you have an issue with the transformation again deliver it to s3 to make sure that",
    "start": "507010",
    "end": "513159"
  },
  {
    "text": "you don't lose any data and that's the key with the majority of Kinesis services is that we make sure that we",
    "start": "513159",
    "end": "520120"
  },
  {
    "text": "always any data that you send to us that you actually get it process it and are able to act upon it in real time",
    "start": "520120",
    "end": "527190"
  },
  {
    "text": "so one of the final services I want to cover and we'll go a little bit deeper into it with a couple more slides is",
    "start": "529850",
    "end": "535520"
  },
  {
    "text": "Amazon Kinesis data analytics which allows you to continuously process data from streaming data sources like",
    "start": "535520",
    "end": "542270"
  },
  {
    "text": "Canisius data firehose in Canisius data streams the primary reason to use",
    "start": "542270",
    "end": "547490"
  },
  {
    "text": "Kinesis data analytics versus just using firehose and performing your analytics for example in elasticsearch is if you",
    "start": "547490",
    "end": "553130"
  },
  {
    "text": "would like to get a little bit more speed or you want to do some pre-processing or you want to react data",
    "start": "553130",
    "end": "558290"
  },
  {
    "text": "in real time so the two most popular use cases we see with Kinesis data analytics are pre aggregations and analytics for a",
    "start": "558290",
    "end": "566090"
  },
  {
    "text": "head of operational monitoring so computing like a rolling sum or",
    "start": "566090",
    "end": "571690"
  },
  {
    "text": "filtering data prior to sending it to say cloud watch or Amazon Elastic search or sumo logic or a Splunk with the goal",
    "start": "571690",
    "end": "579920"
  },
  {
    "text": "of basically being reducing the data set prior to sending it and getting some more flexibility with what you do with",
    "start": "579920",
    "end": "584990"
  },
  {
    "text": "the data the second core biggest use case you see with Kinesis data analytics is for business analytics which is not",
    "start": "584990",
    "end": "591320"
  },
  {
    "text": "really the focus too much of this talk but customers will perform aggregations to produce like real-time dashboards for",
    "start": "591320",
    "end": "597560"
  },
  {
    "text": "their customers as well as pre aggregations in front of things like Amazon redshift or Amazon Arora again",
    "start": "597560",
    "end": "606160"
  },
  {
    "text": "back to the beginning part of the slide the goal is to get more timely insights so customers decide upon this approach",
    "start": "606160",
    "end": "613250"
  },
  {
    "text": "posts for flexibility but also so after they perform those aggregations they end up with a table that a user can query",
    "start": "613250",
    "end": "620420"
  },
  {
    "text": "sub minute or sub to minutes instead of sending a bunch of raw data to one table",
    "start": "620420",
    "end": "625940"
  },
  {
    "text": "to performing a bunch of ETL steps which might take minutes or even hours so",
    "start": "625940",
    "end": "633460"
  },
  {
    "text": "Amazon Kinesis data analytics offers two languages both Java and sequel based applications the focus that I'm going to",
    "start": "633460",
    "end": "639500"
  },
  {
    "text": "cover on the next two a couple of slides are sequel how you write sequel over streaming data so the most common",
    "start": "639500",
    "end": "645820"
  },
  {
    "text": "aggregations are things like summon in max so imagine counting the number of errors 500s on",
    "start": "645820",
    "end": "652460"
  },
  {
    "text": "web access logs or something along those lines but because data is continuous you",
    "start": "652460",
    "end": "657560"
  },
  {
    "text": "need some way to bound your processing and we do that with window so windows allow you to determine when",
    "start": "657560",
    "end": "664160"
  },
  {
    "text": "to start processing and when to produce a result and there's different types of windows that we support so Kinesis data",
    "start": "664160",
    "end": "672649"
  },
  {
    "text": "analytics supports sliding tumbling and stagger and windows the simplest one to describe is tumbling and you can think",
    "start": "672649",
    "end": "678680"
  },
  {
    "text": "of it as like producing a periodic report just really really fast at the",
    "start": "678680",
    "end": "683720"
  },
  {
    "text": "end of the window so perhaps you have a 5-minute tumbling window as you can see from T 0 to T 5 and you're performing a",
    "start": "683720",
    "end": "690380"
  },
  {
    "text": "count what that will do is at the end of that 5-minute window will produce a 2",
    "start": "690380",
    "end": "695510"
  },
  {
    "text": "and will do so based off of whatever Keys you put in say a group by statement so your grouping by say an API specific",
    "start": "695510",
    "end": "704839"
  },
  {
    "text": "API call or specific operation who are produced counts by that partitioned in that manner sliding windows are more",
    "start": "704839",
    "end": "712430"
  },
  {
    "text": "popular for operational use cases because they produce results for every single event flowing through the stream",
    "start": "712430",
    "end": "719209"
  },
  {
    "text": "it's sort of a rolling or a sliding count so whereas tumbling windows there's kind",
    "start": "719209",
    "end": "724700"
  },
  {
    "text": "of a reduced step we count these events and there's a 2 that we produce sliding",
    "start": "724700",
    "end": "729920"
  },
  {
    "text": "windows were produced event for every single event that you process so a",
    "start": "729920",
    "end": "735050"
  },
  {
    "text": "rolling count will look like in this particular case they'll be 1 2 and if we kept on going would be three four and",
    "start": "735050",
    "end": "741230"
  },
  {
    "text": "then perhaps an event drops out of the window and it goes back to 3 sliding windows are typically used for",
    "start": "741230",
    "end": "746750"
  },
  {
    "text": "operational monitoring use cases or when customers want to react as fast as possible to the stream the final is a",
    "start": "746750",
    "end": "754459"
  },
  {
    "text": "standard window which I'll cover on the next slide this is what a stagger window",
    "start": "754459",
    "end": "759769"
  },
  {
    "text": "looks like when you write a sequel query so when you work with Kinesis data analytics we need some mechanism for you",
    "start": "759769",
    "end": "765649"
  },
  {
    "text": "to tell us that to operate continuously you keep on hearing me say that word that mechanism is a pump so what you'll",
    "start": "765649",
    "end": "772850"
  },
  {
    "text": "do is you create a pump that reads from one in application stream and writes to",
    "start": "772850",
    "end": "778399"
  },
  {
    "text": "another in application stream almost building a data flow within your application this simple query produces a",
    "start": "778399",
    "end": "784850"
  },
  {
    "text": "result over a one minute stagger window stagger windows are typically used when you have inconsistencies in your data",
    "start": "784850",
    "end": "792020"
  },
  {
    "text": "that you want to accommodate for and when I the inconsistencies when you produce",
    "start": "792020",
    "end": "797540"
  },
  {
    "text": "analytics in real time very often data arrives late or out of order when I say",
    "start": "797540",
    "end": "803660"
  },
  {
    "text": "late it means we've produced a result for example in a tumbling window and a",
    "start": "803660",
    "end": "808880"
  },
  {
    "text": "vent that should have been included in that result perhaps it got stuck on a disk somewhere or you had a node",
    "start": "808880",
    "end": "814670"
  },
  {
    "text": "failover and it arrives after we computed the computation so what stagger",
    "start": "814670",
    "end": "819829"
  },
  {
    "text": "windows allow you to do is that you define an aggregation period which is that last line right there and that",
    "start": "819829",
    "end": "827839"
  },
  {
    "text": "aggregation name period is different than what you perform the aggregation on so for example if you know that most of",
    "start": "827839",
    "end": "833839"
  },
  {
    "text": "your data will arrive within two minutes but you want to aggregate data in thirty-second windows this allows you to",
    "start": "833839",
    "end": "840079"
  },
  {
    "text": "delay the result a little bit so that you only produce one result with all the data as opposed to multiple results with",
    "start": "840079",
    "end": "847130"
  },
  {
    "text": "late data so with that I'm gonna pass it off to vin who's going to talk about",
    "start": "847130",
    "end": "853010"
  },
  {
    "text": "autodesk unified like blog platform",
    "start": "853010",
    "end": "857050"
  },
  {
    "text": "thanks frien hi there my name is Vinay",
    "start": "861490",
    "end": "866600"
  },
  {
    "text": "cloak nd I'm principle DevOps engineer at Autodesk I would like to talk about Autodesk unified log platform or what we",
    "start": "866600",
    "end": "874760"
  },
  {
    "text": "call it as instrumentation service this is the agenda for my part of",
    "start": "874760",
    "end": "880790"
  },
  {
    "text": "presentation here first we'll go over why instrumentation service design principles around it its architecture",
    "start": "880790",
    "end": "888110"
  },
  {
    "text": "how we came up with a self-service mechanism best practices around it and",
    "start": "888110",
    "end": "893860"
  },
  {
    "text": "finally the summary before we go into the details a brief overview about",
    "start": "893860",
    "end": "900620"
  },
  {
    "text": "Autodesk Autodesk gives you the power to make anything if",
    "start": "900620",
    "end": "906339"
  },
  {
    "text": "you have ever driven a high-performance car admired a sky towering skyscraper",
    "start": "906339",
    "end": "913149"
  },
  {
    "text": "used a smartphone or watched a great film chances are that you have",
    "start": "913149",
    "end": "918699"
  },
  {
    "text": "experienced what millions of customers are doing with Autodesk softwares we",
    "start": "918699",
    "end": "925390"
  },
  {
    "text": "have a 200 plus million Autodesk worldwide with 680 million students and",
    "start": "925390",
    "end": "932360"
  },
  {
    "text": "educators with free access to water their software so at the at the core of",
    "start": "932360",
    "end": "939670"
  },
  {
    "text": "Autodesk cloud platform is Autodesk porch platform forge is a connected",
    "start": "939670",
    "end": "947210"
  },
  {
    "text": "developer platform that powers the future of making thing so let's take an",
    "start": "947210",
    "end": "953990"
  },
  {
    "text": "look at a typical customer workflow here here the customer is uploading a design",
    "start": "953990",
    "end": "961370"
  },
  {
    "text": "model this model is going to traverse through various interconnected ordered",
    "start": "961370",
    "end": "967070"
  },
  {
    "text": "escort services in the end the customer is going to get back the rendered model",
    "start": "967070",
    "end": "973960"
  },
  {
    "text": "so the customer experience really matters here if one of the service is having issue it is going to directly",
    "start": "973960",
    "end": "981500"
  },
  {
    "text": "affect the customers experience the customer can reach to order the support and say hey you know what there is some",
    "start": "981500",
    "end": "988190"
  },
  {
    "text": "issue with your cloud services I cannot see my model he can be polite or he can",
    "start": "988190",
    "end": "994250"
  },
  {
    "text": "be little bit angry and say why on the earth I cannot see my model here with",
    "start": "994250",
    "end": "1000310"
  },
  {
    "text": "this I would like to put in the saying here that is failures in today's complex",
    "start": "1000310",
    "end": "1005410"
  },
  {
    "text": "distributed and interconnected systems are not exception in fact they are",
    "start": "1005410",
    "end": "1011500"
  },
  {
    "text": "normal cases they are not predictable and they are not avoidable with this in",
    "start": "1011500",
    "end": "1019120"
  },
  {
    "text": "mind I would like to introduce you to a why we came up with instrumentation service so we came up with",
    "start": "1019120",
    "end": "1026740"
  },
  {
    "text": "instrumentation service to be able to trace transactions across the interconnected forge services to be able",
    "start": "1026740",
    "end": "1034839"
  },
  {
    "text": "to detect problems real-time thus reducing the MT TD and notifying upon",
    "start": "1034839",
    "end": "1041319"
  },
  {
    "text": "them provide foreign sakes log analysis capabilities last but not the least",
    "start": "1041319",
    "end": "1047860"
  },
  {
    "text": "provide analytics derive insights to derive features and resiliency to",
    "start": "1047860",
    "end": "1055030"
  },
  {
    "text": "summarize we wanted to have a consistent way to collect and measure matrix of our",
    "start": "1055030",
    "end": "1061330"
  },
  {
    "text": "forged services so now we know why we came up with",
    "start": "1061330",
    "end": "1067240"
  },
  {
    "text": "instrumentation service let's go over some of the design principles we took into consideration while building this",
    "start": "1067240",
    "end": "1073720"
  },
  {
    "text": "service first and foremost we wanted to have well-defined separation of concern",
    "start": "1073720",
    "end": "1080130"
  },
  {
    "text": "then we wanted to minimize the maintenance cost obviously the system should be fault tolerant easy to use",
    "start": "1080130",
    "end": "1087190"
  },
  {
    "text": "highly scaleable and extensible keeping",
    "start": "1087190",
    "end": "1092860"
  },
  {
    "text": "these design principle in mind let's move on to unified logging so unified",
    "start": "1092860",
    "end": "1098830"
  },
  {
    "text": "logging we also refer to it by ul we came up with this to solve this problem",
    "start": "1098830",
    "end": "1105960"
  },
  {
    "text": "essentially Forge is nothing but interconnected services together all of",
    "start": "1105960",
    "end": "1111880"
  },
  {
    "text": "these services were logging in different formats that's the problem we had having",
    "start": "1111880",
    "end": "1118299"
  },
  {
    "text": "log data in different format gives you these challenges which is cross service",
    "start": "1118299",
    "end": "1123669"
  },
  {
    "text": "tracing is almost impossible for in six monitoring and analysis on this log data",
    "start": "1123669",
    "end": "1130149"
  },
  {
    "text": "is pretty difficult and deriving SLO and SLI matrix is close to impossible the",
    "start": "1130149",
    "end": "1136899"
  },
  {
    "text": "solution was we came up with unified logging we are standardized log data model in this what we did is we",
    "start": "1136899",
    "end": "1145440"
  },
  {
    "text": "essentially annotated log records with this to be distributed tracing States",
    "start": "1145440",
    "end": "1150510"
  },
  {
    "text": "this was based on open tracing specification you can find that on open",
    "start": "1150510",
    "end": "1156340"
  },
  {
    "text": "tracing dot io and we essentially develop SDKs supporting multiple",
    "start": "1156340",
    "end": "1162370"
  },
  {
    "text": "languages such as golang Java Python nodejs so forth so on this is a sample Yule log",
    "start": "1162370",
    "end": "1171250"
  },
  {
    "text": "record here as you can see it's essentially a JSON record and I would",
    "start": "1171250",
    "end": "1177519"
  },
  {
    "text": "like to highlight couple of important fields here which I'm going to refer back down the line in the presentation",
    "start": "1177519",
    "end": "1183760"
  },
  {
    "text": "the third field down the line is ul operation which signifies the name of the operation the service is performing",
    "start": "1183760",
    "end": "1190600"
  },
  {
    "text": "it can be an API name the the next line below that is ul spanked duration",
    "start": "1190600",
    "end": "1196510"
  },
  {
    "text": "here this is nothing but how much time it took for the service to perform this",
    "start": "1196510",
    "end": "1202210"
  },
  {
    "text": "operation with these designs principle",
    "start": "1202210",
    "end": "1207970"
  },
  {
    "text": "in mind let's move on to the architecture of service to begin with what we have are the data producers or",
    "start": "1207970",
    "end": "1216310"
  },
  {
    "text": "the client services here we have an ec2 instance on which we have a service",
    "start": "1216310",
    "end": "1223120"
  },
  {
    "text": "which is using the UL SDK and it is going to generate the UL lakhs then we",
    "start": "1223120",
    "end": "1229120"
  },
  {
    "text": "have easiest containers and lambda functions again both of these are generating your logs using ul SDK the",
    "start": "1229120",
    "end": "1237090"
  },
  {
    "text": "locks from the easiest containers and the lambda function flow into cloud",
    "start": "1237090",
    "end": "1242440"
  },
  {
    "text": "watch logs at the same time the logs",
    "start": "1242440",
    "end": "1247480"
  },
  {
    "text": "from the ECS instance and the cloud watch logs are sent to what we call it as per service infrastructure here now",
    "start": "1247480",
    "end": "1255600"
  },
  {
    "text": "on the EC ec2 instance we are using the Kinesis agent to forward the logs for",
    "start": "1255600",
    "end": "1262630"
  },
  {
    "text": "the clock in in case of cloud watch logs we are using the cloud word subscription to send these logs to Kinesis data",
    "start": "1262630",
    "end": "1268870"
  },
  {
    "text": "firehose the thing to note here is we have per service infrastructure this per",
    "start": "1268870",
    "end": "1274630"
  },
  {
    "text": "service infrastructure is essentially for each of the data producer here or each of the client service here and I'm",
    "start": "1274630",
    "end": "1281500"
  },
  {
    "text": "going to walk over this little bit in more detail later on now once these Yule",
    "start": "1281500",
    "end": "1287260"
  },
  {
    "text": "logs line into Kinesis data firehose they are essentially sent to Amazon",
    "start": "1287260",
    "end": "1293680"
  },
  {
    "text": "Elastic search service here these logs will retain these logs for five days at",
    "start": "1293680",
    "end": "1302050"
  },
  {
    "text": "the same time these locks are also returned to the secondary destination in",
    "start": "1302050",
    "end": "1307900"
  },
  {
    "text": "this case it is an s3 bucket so what we do here is we do a SNS fan out and we",
    "start": "1307900",
    "end": "1314470"
  },
  {
    "text": "trigger two lambdas the first lambda on the top there is the x-ray lambda this",
    "start": "1314470",
    "end": "1320620"
  },
  {
    "text": "x-ray lambda will essentially open up that s3 object because it's a ul log",
    "start": "1320620",
    "end": "1326200"
  },
  {
    "text": "data and it's annotated with distributed tracing this lambda can then generate x-ray",
    "start": "1326200",
    "end": "1333970"
  },
  {
    "text": "traces and we are essentially storing these traces in a SS x-ray the second",
    "start": "1333970",
    "end": "1340630"
  },
  {
    "text": "lambda here ascends essentially massages this data and stores it into central s3",
    "start": "1340630",
    "end": "1345850"
  },
  {
    "text": "bucket this central s3 bucket is different than the buffer s3 bucket in",
    "start": "1345850",
    "end": "1351640"
  },
  {
    "text": "terms of retention period here we are essentially storing data up to 90 days in central s3 bucket here and then what",
    "start": "1351640",
    "end": "1359080"
  },
  {
    "text": "you can do is you can use AWS apena service to perform log analysis and",
    "start": "1359080",
    "end": "1364570"
  },
  {
    "text": "search on this data so as the data is flowing into Kinesis data firehose we",
    "start": "1364570",
    "end": "1371620"
  },
  {
    "text": "have kinases data analytics which is going to continuously read upon this data as Ryan mentioned kinases data",
    "start": "1371620",
    "end": "1381040"
  },
  {
    "text": "analyst analytics is a sequel processor so you can do something like this",
    "start": "1381040",
    "end": "1387580"
  },
  {
    "text": "Kinesis data analytics can say hey give me last 60 seconds worth of data and on",
    "start": "1387580",
    "end": "1393820"
  },
  {
    "text": "a given field I want to perform certain aggregations I would like to refer back to the UL operation annual spanned",
    "start": "1393820",
    "end": "1400690"
  },
  {
    "text": "duration you can essentially say all right I want aggregations to be performed for a given Ewell operation by",
    "start": "1400690",
    "end": "1408550"
  },
  {
    "text": "the span duration now this at these disaggregated result is what is fed into",
    "start": "1408550",
    "end": "1415179"
  },
  {
    "text": "cloud watch matrix and this is what we call it as the SLO SLI matrix for a",
    "start": "1415179",
    "end": "1420730"
  },
  {
    "text": "given service at the same time we have another rich set of matrix stored in",
    "start": "1420730",
    "end": "1427750"
  },
  {
    "text": "cloud watch matrix which is nothing but the infrastructure matrix as all of these are AWS managed services you have",
    "start": "1427750",
    "end": "1435190"
  },
  {
    "text": "rich set of matrix coming from ec2 instance containers lambda functions even if if let's say L bees are fronting",
    "start": "1435190",
    "end": "1442570"
  },
  {
    "text": "these services or API gateways are fronting these services you have all of that matrix present in cloud watch",
    "start": "1442570",
    "end": "1447880"
  },
  {
    "text": "matrix we then pull these set of matrix into graph on ax and that's what we call",
    "start": "1447880",
    "end": "1455230"
  },
  {
    "text": "it as single pane of glass or Spock so to recap what do we get out of this",
    "start": "1455230",
    "end": "1462700"
  },
  {
    "text": "architecture first and foremost we get ability to have transaction places across connected",
    "start": "1462700",
    "end": "1470430"
  },
  {
    "text": "services second we have ability to perform five lakh search and analysis on",
    "start": "1470430",
    "end": "1477300"
  },
  {
    "text": "five days plus worth of data then we have elastic search service sorry I'm my",
    "start": "1477300",
    "end": "1486840"
  },
  {
    "text": "bad we can use a SS Athena service to perform Phi plus days of log search and",
    "start": "1486840",
    "end": "1493770"
  },
  {
    "text": "analysis here then we have elastic search service in which the data is retained here for five days and you can",
    "start": "1493770",
    "end": "1501240"
  },
  {
    "text": "essentially search upon on that data then we have Kinesis data analytics this",
    "start": "1501240",
    "end": "1508290"
  },
  {
    "text": "is where we get real-time analytics to feed to cloud watch and single pane of",
    "start": "1508290",
    "end": "1513300"
  },
  {
    "text": "glass and last but not the least we are storing all of this matrix in cloud",
    "start": "1513300",
    "end": "1519150"
  },
  {
    "text": "watch matrix and you can essentially alert real-time on the matrix so this is",
    "start": "1519150",
    "end": "1527520"
  },
  {
    "text": "the architecture we came up with one of the thing to note here is this part of",
    "start": "1527520",
    "end": "1535680"
  },
  {
    "text": "the architecture wherein we have post service in infrastructure now we wanted",
    "start": "1535680",
    "end": "1540810"
  },
  {
    "text": "to have per service infrastructure to achieve separation of concern to make",
    "start": "1540810",
    "end": "1546600"
  },
  {
    "text": "the the whole platform resilient with this came another challenge here that is",
    "start": "1546600",
    "end": "1552510"
  },
  {
    "text": "how are we going to have this per service infrastructure created for thirty-plus services we had more than 30",
    "start": "1552510",
    "end": "1559290"
  },
  {
    "text": "plus services in Autodesk and we wanted to create this infrastructure for all of them so that's where we came up with",
    "start": "1559290",
    "end": "1567270"
  },
  {
    "text": "this solution here let's just focus only on the application specific infrastructure here which looks",
    "start": "1567270",
    "end": "1573720"
  },
  {
    "text": "something like this so at Autodesk we use ServiceNow and this is the mechanism",
    "start": "1573720",
    "end": "1580440"
  },
  {
    "text": "which we came up with the SME of the service will submit our onboarding",
    "start": "1580440",
    "end": "1586140"
  },
  {
    "text": "request we us know once it is approved it will trigger a Jenkins job this",
    "start": "1586140",
    "end": "1592020"
  },
  {
    "text": "Jenkins job is going to do a couple of thing first and foremost it is going to trigger a bunch of",
    "start": "1592020",
    "end": "1597799"
  },
  {
    "text": "form scripts and that's what we'll set up the application specific infrastructure here second what it does",
    "start": "1597799",
    "end": "1605869"
  },
  {
    "text": "is it creates a github repo specific for that application this github repo consists of config dot",
    "start": "1605869",
    "end": "1614239"
  },
  {
    "text": "Yambol I'm going to walk into detail what is this config dot camel now this",
    "start": "1614239",
    "end": "1620690"
  },
  {
    "text": "get get repo is hooked up to another Jenkins job and this Jenkins job has",
    "start": "1620690",
    "end": "1626749"
  },
  {
    "text": "access to all the component related to that service so think of it the second",
    "start": "1626749",
    "end": "1632840"
  },
  {
    "text": "job can essentially access firehose kinney sees data analytics and the cloud watch lambda which is specific to the",
    "start": "1632840",
    "end": "1639919"
  },
  {
    "text": "the service there at the same time what we do is we provide the SME access to",
    "start": "1639919",
    "end": "1646249"
  },
  {
    "text": "that github repo now going back to this config dot Hamel as the SME has access",
    "start": "1646249",
    "end": "1653029"
  },
  {
    "text": "to this config dot yeah Mel he essentially indirectly has access to all the all of the component related to his",
    "start": "1653029",
    "end": "1659330"
  },
  {
    "text": "service here so this is what the config",
    "start": "1659330",
    "end": "1664369"
  },
  {
    "text": "camel looks like this is just a part of it wherein I'm talking about what what",
    "start": "1664369",
    "end": "1669679"
  },
  {
    "text": "you can see here on the screen is related to Kinesis data analytics here now as Ryan mentioned Kinesis data",
    "start": "1669679",
    "end": "1677239"
  },
  {
    "text": "analytics is sequel processor essentially you have to write down sequel queries now writing these sequel",
    "start": "1677239",
    "end": "1686210"
  },
  {
    "text": "queries can be little bit complex and what we did is we wanted to abstract",
    "start": "1686210",
    "end": "1691580"
  },
  {
    "text": "that complexity from the user we wanted to make it easy for the user so we came up with predefined templates the the",
    "start": "1691580",
    "end": "1699499"
  },
  {
    "text": "template which you can see on the screen essentially calculates various percentiles count of status codes for a",
    "start": "1699499",
    "end": "1706999"
  },
  {
    "text": "given service and as you can see here what you have to do is you have to essentially specify which field in the",
    "start": "1706999",
    "end": "1715609"
  },
  {
    "text": "schema relates or translates into the UL operation and which field essentially",
    "start": "1715609",
    "end": "1721399"
  },
  {
    "text": "translates to the duration and then you have to just tell the name of the operations so this is what is one part",
    "start": "1721399",
    "end": "1729379"
  },
  {
    "text": "of the configure Campbell user is essentially going to choose one of the predefined template and then he",
    "start": "1729379",
    "end": "1736820"
  },
  {
    "text": "essentially commits to the git repo that is what is going to trigger the second Jenkins job which I just went over and",
    "start": "1736820",
    "end": "1743180"
  },
  {
    "text": "this Jenkins job is essentially going to convert this template into a full-fledged",
    "start": "1743180",
    "end": "1748970"
  },
  {
    "text": "Kinesis sequel query and since this Jenkins job has access to the",
    "start": "1748970",
    "end": "1754760"
  },
  {
    "text": "applications infrastructure this full-fledged sequel query is then injected into the Kinesis data analytics",
    "start": "1754760",
    "end": "1763630"
  },
  {
    "text": "so that's how we achieved the ease of",
    "start": "1764470",
    "end": "1769490"
  },
  {
    "text": "use for for Kinesis data analytics using the config dot Hamel now I'm going to",
    "start": "1769490",
    "end": "1776060"
  },
  {
    "text": "talk about some of the other example use cases of instrumentation service one of",
    "start": "1776060",
    "end": "1782570"
  },
  {
    "text": "the thing of which instrumentation service provides is elasticsearch service and with elasticsearch service",
    "start": "1782570",
    "end": "1788720"
  },
  {
    "text": "comes cabana at autodesk cabana or elasticsearch service is used",
    "start": "1788720",
    "end": "1794540"
  },
  {
    "text": "widely we have all sort of dashboards created for various services here this",
    "start": "1794540",
    "end": "1803720"
  },
  {
    "text": "is example of x-ray traces this is for one of our service there so each circle",
    "start": "1803720",
    "end": "1810830"
  },
  {
    "text": "here represents Givens operation are given API call for the service the green",
    "start": "1810830",
    "end": "1818000"
  },
  {
    "text": "here means like if the the this call was doing okay or good and the yellow here",
    "start": "1818000",
    "end": "1824060"
  },
  {
    "text": "represents warning and red error so what so on this is an example of spa or what",
    "start": "1824060",
    "end": "1833600"
  },
  {
    "text": "we call it as single pane of glass here you can see the first row essentially",
    "start": "1833600",
    "end": "1839500"
  },
  {
    "text": "depicts the overall status for a given service we are essentially pulling up this matrix from the cloud watch matrix",
    "start": "1839500",
    "end": "1847370"
  },
  {
    "text": "of alb you can put in anything there may be api gateway or whatever your service is using there the second two rows there",
    "start": "1847370",
    "end": "1855680"
  },
  {
    "text": "are essentially four the key api's are there essentially the SLO SLR matrix for",
    "start": "1855680",
    "end": "1862460"
  },
  {
    "text": "your service are coming real-time from the Kinesis data analytics using the sequel queries",
    "start": "1862460",
    "end": "1870370"
  },
  {
    "text": "continuation of Spock this particular service here uses aurora and you can see",
    "start": "1870370",
    "end": "1877610"
  },
  {
    "text": "we were able to plot which set of matrix for a SS aurora now if you combine these",
    "start": "1877610",
    "end": "1884210"
  },
  {
    "text": "two graphs you can think of you have a single pane of glass where in the first",
    "start": "1884210",
    "end": "1889670"
  },
  {
    "text": "row is showing you the overall status of the service then you have the SLO sli matrix for each key api of your service",
    "start": "1889670",
    "end": "1897110"
  },
  {
    "text": "and then finally you have the infrastructure matrix if there is anything wrong with your service that's",
    "start": "1897110",
    "end": "1902660"
  },
  {
    "text": "the place you will go and see if my Elbe is having issue if yes which API is",
    "start": "1902660",
    "end": "1907760"
  },
  {
    "text": "having issue if yes which infrastructure component is having issue with this I",
    "start": "1907760",
    "end": "1914960"
  },
  {
    "text": "would like to move on to lessons learned or some of the best practices around the",
    "start": "1914960",
    "end": "1921830"
  },
  {
    "text": "various components of this architecture so we did extensive amount of",
    "start": "1921830",
    "end": "1928220"
  },
  {
    "text": "performance testing and it was not just a single iteration that we perfect did",
    "start": "1928220",
    "end": "1934430"
  },
  {
    "text": "got a perfect architecture we had to try this out multiple times and for each iteration we did performance testing",
    "start": "1934430",
    "end": "1942230"
  },
  {
    "text": "essentially to identify and test solution limits so we are using AWS",
    "start": "1942230",
    "end": "1948050"
  },
  {
    "text": "managed services here for each of the AWS managed service there are limits you",
    "start": "1948050",
    "end": "1955220"
  },
  {
    "text": "have to essentially know this limit you have to test those limits and for doing",
    "start": "1955220",
    "end": "1960260"
  },
  {
    "text": "so you have to do performance testing we essentially did this kind of performance",
    "start": "1960260",
    "end": "1966380"
  },
  {
    "text": "testing wherein we had 30 data producers these 30 data producers were creating",
    "start": "1966380",
    "end": "1973400"
  },
  {
    "text": "close to three terabyte of data per day and this was resulting into 700 shards",
    "start": "1973400",
    "end": "1980780"
  },
  {
    "text": "active shots in elasticsearch cluster and close to 200 queries per second so",
    "start": "1980780",
    "end": "1988100"
  },
  {
    "text": "the the I would like to stress upon the last part here the 200 pairs per second",
    "start": "1988100",
    "end": "1993740"
  },
  {
    "text": "is what what we did to to introduce the search load on the elasticsearch cluster with this what we",
    "start": "1993740",
    "end": "2002000"
  },
  {
    "text": "did was we were able to achieve close to 80% CPU utilization on elasticsearch",
    "start": "2002000",
    "end": "2008210"
  },
  {
    "text": "cluster and close to 55% GB memory pressure on elasticsearch cluster and we",
    "start": "2008210",
    "end": "2014570"
  },
  {
    "text": "ran it for close to a week or so some",
    "start": "2014570",
    "end": "2022040"
  },
  {
    "text": "more lessons learned around elasticsearch service well this cloud of",
    "start": "2022040",
    "end": "2029030"
  },
  {
    "text": "elastic search service there is this set of matrix it has rich set of matrix by default in fact this point is no longer",
    "start": "2029030",
    "end": "2036500"
  },
  {
    "text": "valid because elastic search recently released a new version wherein they have",
    "start": "2036500",
    "end": "2041720"
  },
  {
    "text": "added this matrix previous to this version what we did was we created our own lambda function which would get per",
    "start": "2041720",
    "end": "2049310"
  },
  {
    "text": "node CPU per node JVM heap underscore bulk queue size and per index field",
    "start": "2049310",
    "end": "2056000"
  },
  {
    "text": "count but to my knowledge all of this is now by default available in",
    "start": "2056000",
    "end": "2061220"
  },
  {
    "text": "elasticsearch notch matrix so if we",
    "start": "2061220",
    "end": "2067790"
  },
  {
    "text": "recall the architecture the elastic search service is your crown jewel why",
    "start": "2067790",
    "end": "2073490"
  },
  {
    "text": "because it is shared across all the services there and you need to protect",
    "start": "2073490",
    "end": "2079429"
  },
  {
    "text": "your crown jewel here so one of the the thing which we which we ran into one of",
    "start": "2079429",
    "end": "2085608"
  },
  {
    "text": "the issue which we ran into was per index field limit there is a default a",
    "start": "2085609",
    "end": "2090980"
  },
  {
    "text": "field limit of thousand per index on elastic search service so essentially",
    "start": "2090980",
    "end": "2098840"
  },
  {
    "text": "you should be monitoring this and if you are reaching any closer to 700 or 800",
    "start": "2098840",
    "end": "2105080"
  },
  {
    "text": "you should be taking action of either bumping it up or you should take action",
    "start": "2105080",
    "end": "2110180"
  },
  {
    "text": "of restricting these number of fields and for that you'll have to work with the application team to restrict the",
    "start": "2110180",
    "end": "2115970"
  },
  {
    "text": "number of fields they are sending so again elastic search service one of the",
    "start": "2115970",
    "end": "2123920"
  },
  {
    "text": "the thing there is you should be sizing it correctly right how do",
    "start": "2123920",
    "end": "2129170"
  },
  {
    "text": "you size it there is a great article written by John handler around t-shirt",
    "start": "2129170",
    "end": "2134359"
  },
  {
    "text": "sizing of elasticsearch cluster and essentially what you have to do is you",
    "start": "2134359",
    "end": "2139910"
  },
  {
    "text": "have to find out the input load to your elasticsearch service which will translate into number of active shots",
    "start": "2139910",
    "end": "2146660"
  },
  {
    "text": "and then there is a charge to index to CPU ratio which which is something like",
    "start": "2146660",
    "end": "2152059"
  },
  {
    "text": "for each shard you need one CPU based on this you can then size your",
    "start": "2152059",
    "end": "2158569"
  },
  {
    "text": "elasticsearch cluster choose which instance type you are looking for and how many of those instances you need the",
    "start": "2158569",
    "end": "2167599"
  },
  {
    "text": "last two point here is again make sure that your elasticsearch cluster the CPU",
    "start": "2167599",
    "end": "2174319"
  },
  {
    "text": "utilization and the JVM memory pressure always remain below 75% moving on to",
    "start": "2174319",
    "end": "2183109"
  },
  {
    "text": "lesson learned related to Kinesis data firehose so if we if we take a closer",
    "start": "2183109",
    "end": "2189950"
  },
  {
    "text": "look at the firehose architecture its architecture looks something like this we have lambda buffer at the front then",
    "start": "2189950",
    "end": "2198230"
  },
  {
    "text": "we have the transformation lambda and then we have elasticsearch buffer so the",
    "start": "2198230",
    "end": "2204559"
  },
  {
    "text": "first point here is you can use the transformation lambda as the name",
    "start": "2204559",
    "end": "2209690"
  },
  {
    "text": "suggests it's a transformation lambda but you can use this transformation lambda as a rate limiting mechanism and",
    "start": "2209690",
    "end": "2216200"
  },
  {
    "text": "the way to do that is for this transformation lambda what we did is we",
    "start": "2216200",
    "end": "2222109"
  },
  {
    "text": "use the concurrency because you can set the the the number of concurrent execution for this lambda if you tweak",
    "start": "2222109",
    "end": "2230299"
  },
  {
    "text": "the lambda buffer and you select set this concurrent execution correctly you",
    "start": "2230299",
    "end": "2236569"
  },
  {
    "text": "are essentially going to rate limit what is the output to the es buffer using",
    "start": "2236569",
    "end": "2242210"
  },
  {
    "text": "this you know you are essentially protecting your crown jewel which is your elasticsearch cluster here again",
    "start": "2242210",
    "end": "2250130"
  },
  {
    "text": "this transformation lambda while it is just it is doing transformation you can also filter out garbage data if a",
    "start": "2250130",
    "end": "2257809"
  },
  {
    "text": "service is sending data which is not as per the schema",
    "start": "2257809",
    "end": "2262830"
  },
  {
    "text": "which we have agreed upon then you can essentially discard that data next one",
    "start": "2262830",
    "end": "2269310"
  },
  {
    "text": "there is setting up fire hose per data producer so one of the design principle",
    "start": "2269310",
    "end": "2275190"
  },
  {
    "text": "for instrumentation service was separation of concern and again I would",
    "start": "2275190",
    "end": "2281670"
  },
  {
    "text": "like to go back to the fact that this fire hose is writing to elasticsearch",
    "start": "2281670",
    "end": "2286980"
  },
  {
    "text": "and what we wanted to do is we wanted to have separation of concern at each level",
    "start": "2286980",
    "end": "2292670"
  },
  {
    "text": "think of it we have more than 30 data producers if a given data producers",
    "start": "2292670",
    "end": "2298680"
  },
  {
    "text": "starts writing let's say 1 TB of data right and the other data producer is",
    "start": "2298680",
    "end": "2305790"
  },
  {
    "text": "just merely writing 1 GB of data 1 GB of data per per R or so what if we wouldn't",
    "start": "2305790",
    "end": "2312750"
  },
  {
    "text": "have segregated the firehose one producer will affect the other producer not only at the firehose level but even",
    "start": "2312750",
    "end": "2320250"
  },
  {
    "text": "at the elasticsearch level by doing this what we essentially did is we created",
    "start": "2320250",
    "end": "2325340"
  },
  {
    "text": "indexes per data producer even at the elasticsearch level this way we were",
    "start": "2325340",
    "end": "2331410"
  },
  {
    "text": "able to mitigate the problem of thousand field limit because we have separate index for separate services it means",
    "start": "2331410",
    "end": "2337620"
  },
  {
    "text": "each service is going to get thousand fields limit at the same time we we are",
    "start": "2337620",
    "end": "2343350"
  },
  {
    "text": "segregating the fire hoses so that these services don't step on each other the",
    "start": "2343350",
    "end": "2350490"
  },
  {
    "text": "next point here is around buffers so as I mentioned we are using AWS managed",
    "start": "2350490",
    "end": "2356610"
  },
  {
    "text": "services here these services will have different limits different configuration parameters there are certain buffers",
    "start": "2356610",
    "end": "2364800"
  },
  {
    "text": "which you should be aware of we are using Kinesis agent there are fixed set",
    "start": "2364800",
    "end": "2371490"
  },
  {
    "text": "of buffers which are well defined in the documentation you should pay attention to these buffers because tweaking this",
    "start": "2371490",
    "end": "2379110"
  },
  {
    "text": "buffer is what is going to tell you how fast the data is sent from the client to",
    "start": "2379110",
    "end": "2384390"
  },
  {
    "text": "the fire hose and so forth so on at the same time the AWS lamda buffer is also",
    "start": "2384390",
    "end": "2391170"
  },
  {
    "text": "pretty important here I briefly mentioned about how you can use this lamda buff",
    "start": "2391170",
    "end": "2396530"
  },
  {
    "text": "along with the concurrent execution of the transformation lambda to achieve",
    "start": "2396530",
    "end": "2401970"
  },
  {
    "text": "rate limiting the last buffer here to note is the elasticsearch buffer here",
    "start": "2401970",
    "end": "2409100"
  },
  {
    "text": "so this elasticsearch buffer is is pretty important this essentially will",
    "start": "2409100",
    "end": "2416430"
  },
  {
    "text": "regulate how often the fire hose is writing to your elasticsearch cluster",
    "start": "2416430",
    "end": "2422280"
  },
  {
    "text": "here if this buffer if I recall correctly the setting is from 1 to 300",
    "start": "2422280",
    "end": "2428820"
  },
  {
    "text": "MV and depending on what you set it up to let's say if you are set it to low",
    "start": "2428820",
    "end": "2435630"
  },
  {
    "text": "what will happen is the fire hose is going to make underscore bulk request",
    "start": "2435630",
    "end": "2440760"
  },
  {
    "text": "often to elasticsearch now there is a default limit of 200 bulk you length",
    "start": "2440760",
    "end": "2448170"
  },
  {
    "text": "right now that way you are going to get throttle at elasticsearch level if you",
    "start": "2448170",
    "end": "2454500"
  },
  {
    "text": "set it too high it means the data will be returned to elasticsearch service less often it means you are going to get",
    "start": "2454500",
    "end": "2460950"
  },
  {
    "text": "huge latency when the data is produced at the service level and when it is seen",
    "start": "2460950",
    "end": "2466500"
  },
  {
    "text": "in the elasticsearch service site so depending on your service or depending",
    "start": "2466500",
    "end": "2472410"
  },
  {
    "text": "on your data producers you have to essentially choose the buffers correctly",
    "start": "2472410",
    "end": "2479210"
  },
  {
    "text": "so these are some of the lessons learned around Kinesis data analytics Kinesis",
    "start": "2479210",
    "end": "2486870"
  },
  {
    "text": "data analytics is essentially reading continuous stream of data from firehose",
    "start": "2486870",
    "end": "2492230"
  },
  {
    "text": "one of the important parameter there is milliseconds behind latest this is",
    "start": "2492230",
    "end": "2497370"
  },
  {
    "text": "essentially how much the Kinesis data analytics is lagging from the firehose",
    "start": "2497370",
    "end": "2503780"
  },
  {
    "text": "now you should be monitoring this pretty closely if it is going any any way above",
    "start": "2503780",
    "end": "2512340"
  },
  {
    "text": "few seconds or so that there is again good amount of documentation on that but if it is reaching certain limit you",
    "start": "2512340",
    "end": "2519300"
  },
  {
    "text": "should be notified upon that and you have to either restart the Kinesis data analytics stream or you have to bump it",
    "start": "2519300",
    "end": "2527160"
  },
  {
    "text": "up the bump up the capacity of the Kinesis data and stream the second one we went over this",
    "start": "2527160",
    "end": "2534180"
  },
  {
    "text": "which is Kinesis data stream data analytics is using sequel while the",
    "start": "2534180",
    "end": "2541020"
  },
  {
    "text": "sequel is the standard sequel format but it can get complex if you are performing",
    "start": "2541020",
    "end": "2546390"
  },
  {
    "text": "percentiles and other operations so what we what you want to do is you want to abstract the complexity of writing these",
    "start": "2546390",
    "end": "2553380"
  },
  {
    "text": "sequels using the templates config dot Hamill which I went over is one of the",
    "start": "2553380",
    "end": "2558660"
  },
  {
    "text": "way of doing that last but not the least you should be sizing your Kinesis data",
    "start": "2558660",
    "end": "2565200"
  },
  {
    "text": "app correctly again there there is you know you can find in the documentation",
    "start": "2565200",
    "end": "2570870"
  },
  {
    "text": "if your app is sending close to 4 mb per second you need one stream if you are",
    "start": "2570870",
    "end": "2577140"
  },
  {
    "text": "sending more than that you may be probably you'll need two screens so you",
    "start": "2577140",
    "end": "2582300"
  },
  {
    "text": "depending on how much data your service is going to send you should be sizing your kDa app correctly all of this is",
    "start": "2582300",
    "end": "2590880"
  },
  {
    "text": "possible because we are shouting at each level each app has its own Kinesis data analytics so if you know how much the",
    "start": "2590880",
    "end": "2597630"
  },
  {
    "text": "given app is going to send you can size it up correctly with this I would like",
    "start": "2597630",
    "end": "2602880"
  },
  {
    "text": "to summarize with the design principles we started with and let's see how we did on all of them separation of concerns",
    "start": "2602880",
    "end": "2610910"
  },
  {
    "text": "essentially we wanted to have shouting at each layer we wanted to have sharding",
    "start": "2610910",
    "end": "2616980"
  },
  {
    "text": "at five host level we wanted to have sharding at the elasticsearch layer",
    "start": "2616980",
    "end": "2622110"
  },
  {
    "text": "because we wanted to make sure that none of the service will step on each other",
    "start": "2622110",
    "end": "2627150"
  },
  {
    "text": "and we achieved that by segregating or shouting at firehose and at",
    "start": "2627150",
    "end": "2633390"
  },
  {
    "text": "elasticsearch level at elasticsearch level again we used separate indexes for",
    "start": "2633390",
    "end": "2639030"
  },
  {
    "text": "each of the services second is to use we wanted the consumers to be easily be",
    "start": "2639030",
    "end": "2646680"
  },
  {
    "text": "able to tweak their service specific infrastructure we achieve that using the",
    "start": "2646680",
    "end": "2653040"
  },
  {
    "text": "onboarding mechanism or the self-service mechanism of config dot Yambol then we",
    "start": "2653040",
    "end": "2658860"
  },
  {
    "text": "wanted the system to be fault tolerant system and we got Eve dat using the rate",
    "start": "2658860",
    "end": "2664030"
  },
  {
    "text": "limiting mechanism and the shouting again help there obviously wanted to",
    "start": "2664030",
    "end": "2670420"
  },
  {
    "text": "minimize the maintenance cost and have the system to be highly scalable and extensible all of this we got it just by",
    "start": "2670420",
    "end": "2678040"
  },
  {
    "text": "being on AWS managed services thank you very much",
    "start": "2678040",
    "end": "2683350"
  },
  {
    "text": "with this I would like to open up for any questions correct okay so I would",
    "start": "2683350",
    "end": "2713080"
  },
  {
    "text": "like to repeat the question here the question was in the in the lessons",
    "start": "2713080",
    "end": "2718630"
  },
  {
    "text": "learned of the performance testing we I mentioned that we were sending three",
    "start": "2718630",
    "end": "2724210"
  },
  {
    "text": "terabytes of data was it from one producer or across the 30 producers the",
    "start": "2724210",
    "end": "2729610"
  },
  {
    "text": "answer is we it was split across the entire set the 30 terabyte because what",
    "start": "2729610",
    "end": "2735730"
  },
  {
    "text": "we did has besides the elasticsearch cluster there which it should be able to",
    "start": "2735730",
    "end": "2741010"
  },
  {
    "text": "handle three terabyte of data and we then moved backwards and we said okay we are going to try with 30 different apps",
    "start": "2741010",
    "end": "2748330"
  },
  {
    "text": "because we wanted to test out how many firehoses can write concurrently to the elasticsearch cluster so that's why we",
    "start": "2748330",
    "end": "2755140"
  },
  {
    "text": "just went up with 30 different services",
    "start": "2755140",
    "end": "2758910"
  },
  {
    "text": "that is correct",
    "start": "2760230",
    "end": "2763590"
  },
  {
    "text": "all right so the question here is we have per service fire hose these per",
    "start": "2766119",
    "end": "2773059"
  },
  {
    "text": "service fire hoses are writing to elasticsearch service which in which we are going we are having index for each",
    "start": "2773059",
    "end": "2779119"
  },
  {
    "text": "of the service so how are the downstream services or the consumers of these services using elasticsearch right I",
    "start": "2779119",
    "end": "2787339"
  },
  {
    "text": "mean if they have to they if they have to query upon multiple indexes how are",
    "start": "2787339",
    "end": "2793069"
  },
  {
    "text": "they doing so what we did is we named this index s something like ul - service",
    "start": "2793069",
    "end": "2799880"
  },
  {
    "text": "1 the second one was ul - service 2 and so forth so on right and and these",
    "start": "2799880",
    "end": "2805670"
  },
  {
    "text": "indexes when you create the pattern we create a dual - service 1 - star so far",
    "start": "2805670",
    "end": "2812539"
  },
  {
    "text": "so on now a person can go ahead and only query upon a given index which matches",
    "start": "2812539",
    "end": "2818869"
  },
  {
    "text": "that pattern or he can specify alright I want to query across the multiple patterns there sure so we started",
    "start": "2818869",
    "end": "2840829"
  },
  {
    "text": "working on there's almost a year back and there was a team of close to then",
    "start": "2840829",
    "end": "2847009"
  },
  {
    "text": "repeat the question oh sorry the question was how much time it took to",
    "start": "2847009",
    "end": "2852140"
  },
  {
    "text": "come up with this this service and how many engineers were working on it so we",
    "start": "2852140",
    "end": "2857239"
  },
  {
    "text": "started working on this almost an year back and the book there was a small team",
    "start": "2857239",
    "end": "2862400"
  },
  {
    "text": "of almost around if I recall two or three engineers that's it and what we",
    "start": "2862400",
    "end": "2870140"
  },
  {
    "text": "essentially did is we started with automation we said ok we want to segregate this and we started with",
    "start": "2870140",
    "end": "2876140"
  },
  {
    "text": "terraform scripts to make it simple and then that's when we started fitting in the the self-service mechanism and so",
    "start": "2876140",
    "end": "2882349"
  },
  {
    "text": "far so on so you had a question",
    "start": "2882349",
    "end": "2887529"
  },
  {
    "text": "so the question was was elasticsearch service running in VPC are outside of",
    "start": "2902780",
    "end": "2909500"
  },
  {
    "text": "VPC right ah you know I don't remember",
    "start": "2909500",
    "end": "2915290"
  },
  {
    "text": "the documentation here but if I recall correctly fire hose cannot write to",
    "start": "2915290",
    "end": "2921790"
  },
  {
    "text": "Kinesis sorry elasticsearch service running in VPC and so the answer answer to your",
    "start": "2921790",
    "end": "2927710"
  },
  {
    "text": "question is yes elasticsearch service was and not running in VPC in this case sure the question is what",
    "start": "2927710",
    "end": "2943310"
  },
  {
    "text": "are the next steps around the architecture so now we have a",
    "start": "2943310",
    "end": "2948710"
  },
  {
    "text": "well-defined architecture and a well-defined well-defined self-service mechanism here we have ran into issues",
    "start": "2948710",
    "end": "2956750"
  },
  {
    "text": "around scalability of some of the components there and we are looking towards solving those you know one of",
    "start": "2956750",
    "end": "2964220"
  },
  {
    "text": "the thing which I did not mention about which we are achieving from this architecture is we have per service fire",
    "start": "2964220",
    "end": "2972859"
  },
  {
    "text": "hose which is writing to per service index right this gives us flexibility in",
    "start": "2972859",
    "end": "2978109"
  },
  {
    "text": "case you want to move or given index out of one elasticsearch cluster to another let's say a service starts writing huge",
    "start": "2978109",
    "end": "2985760"
  },
  {
    "text": "amount of data and that elasticsearch cluster is not able to hold it up right you have scaled it up but you there is",
    "start": "2985760",
    "end": "2992300"
  },
  {
    "text": "only this much you can do with it what we can essentially do is we can start we can move this index to its own",
    "start": "2992300",
    "end": "2999020"
  },
  {
    "text": "elasticsearch service and essentially this way we can achieve sharding at the",
    "start": "2999020",
    "end": "3004839"
  },
  {
    "text": "elasticsearch level so we wanted to do things more things like this because as the adoption of this platform grows in",
    "start": "3004839",
    "end": "3011829"
  },
  {
    "text": "autodesk there will be challenges with respect to scaling",
    "start": "3011829",
    "end": "3016950"
  },
  {
    "text": "okay thank you all very much for attending I hope you all have fun at the replay party and have a good rest of",
    "start": "3018599",
    "end": "3024749"
  },
  {
    "text": "your conference please fill up the session so we",
    "start": "3024749",
    "end": "3030019"
  }
]