[
  {
    "start": "0",
    "end": "82000"
  },
  {
    "text": "hello you'll hear me right on well thank",
    "start": "770",
    "end": "7109"
  },
  {
    "text": "you very much for coming today and my name is Leo Dirac I am senior principal",
    "start": "7109",
    "end": "13320"
  },
  {
    "text": "engineer in AWS working on Sage Maker and we're lucky to be joined today by Luis canet who is vice president of data",
    "start": "13320",
    "end": "21029"
  },
  {
    "text": "science and engineering with 21st Century Fox and he's can be talking about his experiences using sage maker",
    "start": "21029",
    "end": "29119"
  },
  {
    "text": "but to start with I'm going to give you some background on sage maker generally",
    "start": "29119",
    "end": "35340"
  },
  {
    "text": "and how you can build train and deploy models with it and then after talking",
    "start": "35340",
    "end": "42840"
  },
  {
    "text": "about some of the features we'll do a technical deep dive into how you deploy a trained model at scale we'll talk",
    "start": "42840",
    "end": "50340"
  },
  {
    "text": "about creating endpoints how to do deployments automatic scaling will actually show you the api's will we'll",
    "start": "50340",
    "end": "57239"
  },
  {
    "text": "get into the nitty-gritty on that one since this is a 400 level session we we",
    "start": "57239",
    "end": "62370"
  },
  {
    "text": "don't believe you lacking for details on this then I'll hand it over to Luis who talked about using machine learning a",
    "start": "62370",
    "end": "69630"
  },
  {
    "text": "21st century fox-- and he'll tell you about how they have integrated spark and Amazon Sage Maker",
    "start": "69630",
    "end": "76740"
  },
  {
    "text": "to be able to solve their problems at scale so to start with I'll just give",
    "start": "76740",
    "end": "83580"
  },
  {
    "start": "82000",
    "end": "82000"
  },
  {
    "text": "you an overview of the entire Amazon machine learning stack and this is built in three layers really at the top we",
    "start": "83580",
    "end": "91320"
  },
  {
    "text": "have the high-level AI services and these are services which are designed to",
    "start": "91320",
    "end": "97170"
  },
  {
    "text": "be the easiest to use on the most common AI problems and these are usable by",
    "start": "97170",
    "end": "102600"
  },
  {
    "text": "people without machine learning backgrounds without expertise in data",
    "start": "102600",
    "end": "108000"
  },
  {
    "text": "science on problems envision speech language and conversation so if you have",
    "start": "108000",
    "end": "115020"
  },
  {
    "text": "an image you can just send that image into Amazon recognition image and it",
    "start": "115020",
    "end": "120780"
  },
  {
    "text": "will understand it using the latest deep learning technology highly trained",
    "start": "120780",
    "end": "125969"
  },
  {
    "text": "models that are tuned to solve the most common tasks in understanding images",
    "start": "125969",
    "end": "131160"
  },
  {
    "text": "like facial recognition object detects classification same kinds of",
    "start": "131160",
    "end": "136530"
  },
  {
    "text": "capabilities for the video in speech Amazon poly and Amazon transcribe can",
    "start": "136530",
    "end": "142650"
  },
  {
    "text": "take text and turn it to speech or the other way around and so these common AI tasks are just",
    "start": "142650",
    "end": "149280"
  },
  {
    "text": "automatically available with language you can translate between languages you can do natural language comprehension",
    "start": "149280",
    "end": "155660"
  },
  {
    "text": "understand what's going on inside of textual documents and create chat pots all of these high-level AI services are",
    "start": "155660",
    "end": "163170"
  },
  {
    "text": "available for people without data science expertise and so we've taken the",
    "start": "163170",
    "end": "169440"
  },
  {
    "text": "some of the hardest and most common problems and just made them available to you at a single API call if you need to",
    "start": "169440",
    "end": "176190"
  },
  {
    "text": "do something that's a little outside of the bounds of what those services do then you can pop down a layer on the",
    "start": "176190",
    "end": "182910"
  },
  {
    "text": "stack to the ml services which is really where Amazon sage maker shines so here",
    "start": "182910",
    "end": "188280"
  },
  {
    "text": "you can create your own machine learning models on your own data you can build basically anything you want we have a",
    "start": "188280",
    "end": "194970"
  },
  {
    "text": "broad suite of capabilities ranging from very simple to use to extremely custom",
    "start": "194970",
    "end": "200340"
  },
  {
    "text": "and we'll dive into the details on that in just a bit but if even sage maker is",
    "start": "200340",
    "end": "205680"
  },
  {
    "text": "too restrictive for you you can always drop down to the bottom layer of the stack which is a frameworks and the infrastructure that all of this stuff",
    "start": "205680",
    "end": "212190"
  },
  {
    "text": "runs on so there's a software libraries like tensorflow and pi torch and MX net and chain er which make it possible to",
    "start": "212190",
    "end": "219630"
  },
  {
    "text": "build the most advanced deep learning applications in the world and they all run on Amazon infrastructure such as the",
    "start": "219630",
    "end": "226560"
  },
  {
    "text": "superfast p3 GPU instances or the superfast cpu instances and the c5 and",
    "start": "226560",
    "end": "232470"
  },
  {
    "text": "m5 families or even custom custom silicon through FPGAs and if you need",
    "start": "232470",
    "end": "238349"
  },
  {
    "text": "the absolute utmost control over what you're doing you can always pop down to that level and run things directly over",
    "start": "238349",
    "end": "246150"
  },
  {
    "text": "all machine learning is still extremely complicated the entire workflow involves",
    "start": "246150",
    "end": "251459"
  },
  {
    "text": "a huge number of steps getting your data together picking which algorithm you're going to use and optimizing it figuring",
    "start": "251459",
    "end": "258539"
  },
  {
    "text": "out the training environment that you're going to you're going to reuse this algorithm in and then for for even",
    "start": "258539",
    "end": "265830"
  },
  {
    "text": "experienced scientists with hds training and tuning the model is still a trial and error process that can",
    "start": "265830",
    "end": "271680"
  },
  {
    "text": "take a very long time once you've done that deploying it into production involves figuring out all sorts of",
    "start": "271680",
    "end": "278730"
  },
  {
    "text": "complex systems to make your system reliable and then you need to manage that and make sure that thing is always",
    "start": "278730",
    "end": "284550"
  },
  {
    "text": "available stage maker makes machine learning much simpler we have features",
    "start": "284550",
    "end": "290940"
  },
  {
    "start": "286000",
    "end": "286000"
  },
  {
    "text": "like one-click deployment from the console you can just click a button to train a machine learning model and click",
    "start": "290940",
    "end": "296250"
  },
  {
    "text": "another button to deploy it to a scaleable production endpoint we have built in algorithms that are 10 times",
    "start": "296250",
    "end": "303600"
  },
  {
    "text": "faster than open source alternatives and scale out to as large a data set as you",
    "start": "303600",
    "end": "310020"
  },
  {
    "text": "need and all of this makes it possible for you to build applications for predictive insights to enable advanced",
    "start": "310020",
    "end": "317070"
  },
  {
    "text": "decision-making for whatever your business problem is so the basic pattern",
    "start": "317070",
    "end": "323430"
  },
  {
    "text": "that sage maker uses to simplify machine learning is build train deploy so what I",
    "start": "323430",
    "end": "329760"
  },
  {
    "text": "mean by that first you need an environment to build in and usually this",
    "start": "329760",
    "end": "334830"
  },
  {
    "start": "331000",
    "end": "331000"
  },
  {
    "text": "is a notebook instance this jupiter' notebooks are a lot like the IDE s for data scientists today so amazon sage",
    "start": "334830",
    "end": "341910"
  },
  {
    "text": "maker has these fully managed notebook instances where you can create a customized environment with libraries",
    "start": "341910",
    "end": "347670"
  },
  {
    "text": "that you need access to your data in a safe secure manner and control",
    "start": "347670",
    "end": "354720"
  },
  {
    "text": "everything from there or you can call the exact same api's from your own device whether it's a laptop or a",
    "start": "354720",
    "end": "360030"
  },
  {
    "text": "workstation or anything else everything that you can do from a fully managed notebook you can also do from anywhere",
    "start": "360030",
    "end": "366270"
  },
  {
    "text": "else through the the public AWS api's then once you've figured out your model",
    "start": "366270",
    "end": "373170"
  },
  {
    "text": "you need to train it and sage maker offers a fully managed training environment which is distributed it can",
    "start": "373170",
    "end": "380160"
  },
  {
    "text": "scale out to as much hardware as you need and has extremely high performance IO access to s3 which has all the data",
    "start": "380160",
    "end": "389340"
  },
  {
    "text": "that you'll ever need and can is all fully managed in repeatable so the",
    "start": "389340",
    "end": "394980"
  },
  {
    "text": "training process is reliable and once your model is trained you deploy it",
    "start": "394980",
    "end": "401220"
  },
  {
    "text": "to whatever the end point you want so if what you need is low latency prediction",
    "start": "401220",
    "end": "407280"
  },
  {
    "text": "you can deploy it to a real-time endpoint if what you need is to get through a large volume of data as fast",
    "start": "407280",
    "end": "413670"
  },
  {
    "text": "as possible then you can use a batch transform which is optimized for throughput instead of latency or you can",
    "start": "413670",
    "end": "419010"
  },
  {
    "text": "send it to an edge device IOT device through AWS group green grass or through",
    "start": "419010",
    "end": "425010"
  },
  {
    "text": "deep lens so some of the features that make it possible to build advanced",
    "start": "425010",
    "end": "430590"
  },
  {
    "start": "427000",
    "end": "427000"
  },
  {
    "text": "machine learning models in Sage Maker are the built-in algorithms so these",
    "start": "430590",
    "end": "436140"
  },
  {
    "text": "algorithms look there we go are designed for both speed and scale and so we have",
    "start": "436140",
    "end": "442590"
  },
  {
    "text": "an ever-increasing number of algorithms that are fully supported inside of Sage maker for supervised learning problems",
    "start": "442590",
    "end": "449610"
  },
  {
    "text": "unsupervised learning problems domain-specific tasks such as computer vision and natural language processing",
    "start": "449610",
    "end": "456620"
  },
  {
    "text": "we just announced a new algorithm that I've been helping with called objective X that can take sequences of data and",
    "start": "456620",
    "end": "462810"
  },
  {
    "text": "unique identifying tokens and learn embedding space from them and all of these are designed to scale out to very",
    "start": "462810",
    "end": "469590"
  },
  {
    "text": "large data sets and to run significantly faster than what's available in open",
    "start": "469590",
    "end": "475380"
  },
  {
    "text": "source alternatives and a great thing about all of these fully managed first party algorithms is that you can train",
    "start": "475380",
    "end": "481620"
  },
  {
    "text": "models on them without writing a single line of code you can do all this from the console just by pointing pointing it",
    "start": "481620",
    "end": "488490"
  },
  {
    "text": "at your data and making a single API call and then the training just happens for you if these dudes act what you want",
    "start": "488490",
    "end": "495900"
  },
  {
    "text": "that's awesome they're pretty flexible but if you need additional flexibility if you want to do something not quite what they are what they're offering then",
    "start": "495900",
    "end": "502950"
  },
  {
    "text": "you can use one the frameworks so we have managed supported versions of tensorflow pi torch MX net and chained",
    "start": "502950",
    "end": "510450"
  },
  {
    "text": "ER and these allow you to write the small amount of Python code that you",
    "start": "510450",
    "end": "516180"
  },
  {
    "text": "need to define your model and inject that code into a managed optimized",
    "start": "516180",
    "end": "521550"
  },
  {
    "text": "container which is built to run at maximum speed and to automatically scale",
    "start": "521550",
    "end": "526560"
  },
  {
    "text": "out to as many machines as you need to handle very large quantities of data and",
    "start": "526560",
    "end": "531770"
  },
  {
    "text": "the a code that generates these containers is all open source it's up on github so if you want to see how we get PI torch",
    "start": "531770",
    "end": "538090"
  },
  {
    "text": "to scale out to all the machines you need you can just go to github and you",
    "start": "538090",
    "end": "543340"
  },
  {
    "text": "can see how we do that and you can clone it and and fork it do whatever you want and all of these are available also in",
    "start": "543340",
    "end": "550150"
  },
  {
    "text": "local modes so you can download these containers and run them on your notebook instance or even on your laptop and get",
    "start": "550150",
    "end": "555700"
  },
  {
    "text": "very quick iteration to test how your code is working before you deploy it up to Sage Maker to run it at scale on the",
    "start": "555700",
    "end": "562660"
  },
  {
    "text": "full data set now if these frameworks still don't do exactly what you want you",
    "start": "562660",
    "end": "568660"
  },
  {
    "text": "can always go down a layer inside sage maker and bring your own container so you can always build your own docker",
    "start": "568660",
    "end": "575710"
  },
  {
    "text": "container that has whatever code you want and if you're a fan of",
    "start": "575710",
    "end": "581020"
  },
  {
    "text": "probabilistic programming languages like Stan or you're an AR user or you want to write machine learning code and Julia or",
    "start": "581020",
    "end": "587470"
  },
  {
    "text": "what have you you can compile all of that into your own docker container that meets whatever Enterprise requirements",
    "start": "587470",
    "end": "594130"
  },
  {
    "text": "your organization has and you can do all of your training on that docker container and it still happens inside of",
    "start": "594130",
    "end": "600700"
  },
  {
    "text": "the fully managed and secure stage maker environment and all three of these no",
    "start": "600700",
    "end": "606430"
  },
  {
    "text": "matter how you get your algorithm can be used with the automatic model tuning feature I've worked a lot on this future",
    "start": "606430",
    "end": "612100"
  },
  {
    "text": "myself and I find it incredibly useful machine learning systems always have these hyper parameters like learning",
    "start": "612100",
    "end": "618730"
  },
  {
    "text": "rate or regularization or the number of layers in a neural network or what have you almost nobody knows what the right",
    "start": "618730",
    "end": "625630"
  },
  {
    "text": "setting is for these in general or even what what they do a lot of the time so",
    "start": "625630",
    "end": "631270"
  },
  {
    "text": "instead of just using trial and error the automatic model tuner will try these for you in parallel you can say go run",
    "start": "631270",
    "end": "638620"
  },
  {
    "text": "500 jobs and do them ten two time and figure out what the best settings are for each of these hyper parameters and",
    "start": "638620",
    "end": "645550"
  },
  {
    "text": "it'll do all of that again in a fully managed and unscalable environment for you and the this works regardless of",
    "start": "645550",
    "end": "653170"
  },
  {
    "text": "whether you're using a built-in algorithm something you wrote in one of our frameworks or if you're supplying your entire docker container yourself ok",
    "start": "653170",
    "end": "663220"
  },
  {
    "text": "so after that overview of how Amazon sage maker works generally",
    "start": "663220",
    "end": "668940"
  },
  {
    "text": "let's dive into real-time deployments so again real-time deployments means that you're",
    "start": "668940",
    "end": "675330"
  },
  {
    "text": "setting up something an endpoint that has low latency so this is what you're",
    "start": "675330",
    "end": "680550"
  },
  {
    "text": "gonna want to do if your machine learning model is being used on a website or in a mobile app where you",
    "start": "680550",
    "end": "688290"
  },
  {
    "text": "want the response to come back in in a fraction of a second in just milliseconds and this is not what you",
    "start": "688290",
    "end": "694740"
  },
  {
    "text": "want to do say if you have a big CSV file or a big data file that you want to make predictions on if you want to run it against every record in a database",
    "start": "694740",
    "end": "702210"
  },
  {
    "text": "for example in that case the batch transformed future is more appropriate that's optimized for throughput and",
    "start": "702210",
    "end": "708360"
  },
  {
    "text": "we'll get through that big data set as fast as possible but the latency on every prediction will be much higher",
    "start": "708360",
    "end": "713430"
  },
  {
    "text": "here we're talking about low latency real-time deployment so the entity that",
    "start": "713430",
    "end": "720300"
  },
  {
    "start": "717000",
    "end": "717000"
  },
  {
    "text": "you use for this kind of real-time deployment is an endpoint and you can create one of these from a click on the",
    "start": "720300",
    "end": "725670"
  },
  {
    "text": "console or by making an API call and this sets up a REST API endpoint so you",
    "start": "725670",
    "end": "731700"
  },
  {
    "text": "can make predictions from any programming language you can call it from from anywhere you want and it will",
    "start": "731700",
    "end": "738450"
  },
  {
    "text": "run the code that was created in that training process regardless of whether it came from an Amazon first party",
    "start": "738450",
    "end": "745620"
  },
  {
    "text": "algorithm or your own container or a framework it all works the same and these endpoints are highly scalable have",
    "start": "745620",
    "end": "753180"
  },
  {
    "text": "very high reliability and very high throughput so the API to create one of these endpoints is up here on the screen",
    "start": "753180",
    "end": "761190"
  },
  {
    "text": "and what I'm showing this API in is the AWS command-line interface or CLI so you",
    "start": "761190",
    "end": "768930"
  },
  {
    "text": "can literally type this command from your terminal and have it run and the nice thing about the CLI is that the",
    "start": "768930",
    "end": "774839"
  },
  {
    "text": "format of it exactly mirrors the API call you would make from any language",
    "start": "774839",
    "end": "780390"
  },
  {
    "text": "using the AWS SDK so sage maker is the namespace of the service of course it",
    "start": "780390",
    "end": "787620"
  },
  {
    "text": "this is the create model API and then everything after that are the parameters and in the case where the parameters",
    "start": "787620",
    "end": "794280"
  },
  {
    "text": "take a data structure in is passed in as a JSON block as you can see like for primary container here so let's talk",
    "start": "794280",
    "end": "799740"
  },
  {
    "text": "about this model API so this is the first step to creating a highly scalable endpoint so",
    "start": "799740",
    "end": "806040"
  },
  {
    "text": "the first thing you have to do is bind together the data and the code that you need to do your predictions so the data",
    "start": "806040",
    "end": "813329"
  },
  {
    "text": "is specified in that model data URL and this is the location in s3 of the",
    "start": "813329",
    "end": "818850"
  },
  {
    "text": "parameters for your machine learning model as it was created in the training process and then the image is the docker",
    "start": "818850",
    "end": "826110"
  },
  {
    "text": "image that has the code for running the the hosting hosting service so if you're",
    "start": "826110",
    "end": "833189"
  },
  {
    "text": "using our framework or a first party algorithm then that docker image is",
    "start": "833189",
    "end": "838829"
  },
  {
    "text": "fully managed and just works for you and and if you use the Python SDK it'll even",
    "start": "838829",
    "end": "844019"
  },
  {
    "text": "be set for you automatically if you're bringing your own container then you probably need to build the image",
    "start": "844019",
    "end": "850980"
  },
  {
    "text": "yourself which is which is going to run there but once you've created this model you've bound together the the code to do",
    "start": "850980",
    "end": "859439"
  },
  {
    "text": "the serving along with the data which is the machine learning model parameters that were learned and you specify the",
    "start": "859439",
    "end": "864930"
  },
  {
    "text": "execution role to say what permissions that this thing is going to have as its running now we need to say more than",
    "start": "864930",
    "end": "872579"
  },
  {
    "text": "that right because we haven't said what kind of machines it's going to run on or how many of them and for that we need to",
    "start": "872579",
    "end": "877800"
  },
  {
    "text": "create an endpoint config and the endpoint config is another thing that",
    "start": "877800",
    "end": "884220"
  },
  {
    "text": "says the configuration of how this machine learning model is going to be served and there's some others that you",
    "start": "884220",
    "end": "890399"
  },
  {
    "text": "give it a name and there's some some weights and variants that we'll talk about a little bit later but the key",
    "start": "890399",
    "end": "895559"
  },
  {
    "text": "thing for now is that this specifies the kind of hardware that the thing will run",
    "start": "895559",
    "end": "900720"
  },
  {
    "text": "on once you've created the endpoint configuration creating an actual endpoint is a very simple API call you",
    "start": "900720",
    "end": "906899"
  },
  {
    "text": "just say create endpoint give it a name and point it at the config so now you have an endpoint which is configured",
    "start": "906899",
    "end": "913769"
  },
  {
    "text": "with this endpoint config the endpoint config has the name of the model which",
    "start": "913769",
    "end": "918809"
  },
  {
    "text": "is going which you created the in the first call and now once you've done all of this stage maker turns on the number",
    "start": "918809",
    "end": "926069"
  },
  {
    "text": "of instances that you talked about so here we say we want Emma m4x large and",
    "start": "926069",
    "end": "931500"
  },
  {
    "text": "two of them to start with and these gifts instantiated and loaded",
    "start": "931500",
    "end": "936570"
  },
  {
    "text": "with your container and loaded with your model and now you have a low latency endpoint available so once it's there",
    "start": "936570",
    "end": "945870"
  },
  {
    "text": "you can do a bunch of interesting things with it let's say that you you're a",
    "start": "945870",
    "end": "952290"
  },
  {
    "text": "science team has come up with a new model and they want to update the endpoint so it's possible to very easily",
    "start": "952290",
    "end": "958770"
  },
  {
    "text": "do a Bluegreen deployment where you turn on new machines before turning off the",
    "start": "958770",
    "end": "963990"
  },
  {
    "text": "old ones and once the new ones are warmed up and ready to go they get slotted in behind the load balancer and",
    "start": "963990",
    "end": "970550"
  },
  {
    "text": "predictions get routed to these to these new machines with zero downtime and zero",
    "start": "970550",
    "end": "976500"
  },
  {
    "text": "interruption to live service that's one thing I'll show you how to do and then",
    "start": "976500",
    "end": "982530"
  },
  {
    "text": "another thing you can do is actually deploy more than one model behind the same endpoint so let's say that you have",
    "start": "982530",
    "end": "989910"
  },
  {
    "text": "this new model called model 2 which is some fancy neural network that the science team has come up with to replace",
    "start": "989910",
    "end": "996510"
  },
  {
    "text": "the old model which was say a decision tree using XT boost so you want to",
    "start": "996510",
    "end": "1001880"
  },
  {
    "text": "switch over to this so you create a new model which binds together again the the",
    "start": "1001880",
    "end": "1007940"
  },
  {
    "text": "data with the code the data in this case is the weights of the neural network and the code is something like tensorflow",
    "start": "1007940",
    "end": "1013010"
  },
  {
    "text": "serving that knows how to serve a serve that model give it permissions say what",
    "start": "1013010",
    "end": "1019940"
  },
  {
    "text": "kinds of machines it's going to run on and if you might want to use GPU",
    "start": "1019940",
    "end": "1025819"
  },
  {
    "text": "instances for the the lowest latency after you using a neural network here and have a different set of hardware",
    "start": "1025820",
    "end": "1033079"
  },
  {
    "text": "configuration than your old model and then you can call update endpoint pointing to that same endpoint my",
    "start": "1033080",
    "end": "1039980"
  },
  {
    "text": "endpoint and give it this new model configuration and stage maker will turn",
    "start": "1039980",
    "end": "1044990"
  },
  {
    "text": "on these new instances for you load this new model with the the code and the data",
    "start": "1044990",
    "end": "1050750"
  },
  {
    "text": "on it and once it's available live swap over the traffic to it with zero interruption and this is a great way to",
    "start": "1050750",
    "end": "1059680"
  },
  {
    "text": "to swap out a model all at once if that's what you want to do",
    "start": "1059680",
    "end": "1064710"
  },
  {
    "start": "1064000",
    "end": "1064000"
  },
  {
    "text": "so this is great if you're in a production setting such as one where you there's new data showing up all the time",
    "start": "1064710",
    "end": "1071250"
  },
  {
    "text": "so I used to work in the retail part of Amazon the the book store is we playfully call it here in AWS and we",
    "start": "1071250",
    "end": "1078300"
  },
  {
    "text": "would build models to predict what kinds of products people would likely be",
    "start": "1078300",
    "end": "1083370"
  },
  {
    "text": "interested in for recommender systems and so every week we would train new models and swap them out for for the old",
    "start": "1083370",
    "end": "1091140"
  },
  {
    "text": "ones and we had a bunch of complex infrastructure for managing this and back then if we had sage maker this",
    "start": "1091140",
    "end": "1098160"
  },
  {
    "text": "would have been much much easier because we could just call update endpoint and the new endpoint would go into",
    "start": "1098160",
    "end": "1103500"
  },
  {
    "text": "production with zero downtime this also makes it very easy to try out new and",
    "start": "1103500",
    "end": "1108660"
  },
  {
    "text": "improved algorithms that you think might be better than the old one but you know sometimes it's not such a good idea to",
    "start": "1108660",
    "end": "1114870"
  },
  {
    "text": "just instantly turn off that old reliable decision tree that you've been running for a long time and and send all",
    "start": "1114870",
    "end": "1120810"
  },
  {
    "text": "your customers this fancy new neural network all at once and sure maybe it performs well on your held out",
    "start": "1120810",
    "end": "1126480"
  },
  {
    "text": "validation set but is it really going to work and in real life maybe hard to know",
    "start": "1126480",
    "end": "1132810"
  },
  {
    "text": "until you try it so a safer thing to do is to do a to model endpoint",
    "start": "1132810",
    "end": "1138690"
  },
  {
    "text": "configuration and so now you're starting to see some of the power in that endpoint config object that that I",
    "start": "1138690",
    "end": "1147300"
  },
  {
    "text": "glossed over earlier so this this parameter called production variance you",
    "start": "1147300",
    "end": "1153930"
  },
  {
    "text": "can actually specify more than one of them and so here I've created a production variant with two different",
    "start": "1153930",
    "end": "1159480"
  },
  {
    "text": "variants one called model 1 trafficking the other one called model 2 traffic and",
    "start": "1159480",
    "end": "1164750"
  },
  {
    "text": "I'm setting it so that 95% of the traffic is going to that tried-and-true",
    "start": "1164750",
    "end": "1170190"
  },
  {
    "text": "XD boost algorithm and only 5% of the traffic is going to the fancy new neural",
    "start": "1170190",
    "end": "1175500"
  },
  {
    "text": "network in this way we can put that fancy new model into production and see",
    "start": "1175500",
    "end": "1183150"
  },
  {
    "text": "how it does without risking the behavior some poor behavior on on all of our",
    "start": "1183150",
    "end": "1188970"
  },
  {
    "text": "customers excuse me one second Thanks ok",
    "start": "1188970",
    "end": "1194370"
  },
  {
    "text": "so and actually I left out the the optic step which is once you've created that endpoint you",
    "start": "1194370",
    "end": "1200760"
  },
  {
    "text": "need to call update endpoint and now stage maker will turn on two more",
    "start": "1200760",
    "end": "1205830"
  },
  {
    "text": "instances with the with model two and start routing just 5% of the traffic",
    "start": "1205830",
    "end": "1211170"
  },
  {
    "text": "over to that new model and this allows you to do things like a be testing comparison and check how a new model is",
    "start": "1211170",
    "end": "1219150"
  },
  {
    "text": "doing before fully committing to it let's say you do this for a while seems to be doing all right like nothing",
    "start": "1219150",
    "end": "1225810"
  },
  {
    "text": "nothing obviously wrong but you're not really getting traffic to it to say for sure that it's doing better than the old",
    "start": "1225810",
    "end": "1231240"
  },
  {
    "text": "one so now without taking anything down you can quickly update the weights on",
    "start": "1231240",
    "end": "1237840"
  },
  {
    "text": "these variants and take the weight from model 1 from 95 down to 5 and now model",
    "start": "1237840",
    "end": "1244860"
  },
  {
    "text": "one and model 2 both have a weight of 5 and so they each going to get half the traffic now you can do a fair comparison",
    "start": "1244860",
    "end": "1250530"
  },
  {
    "text": "these two and this is a very simple API call and again the the routing layer",
    "start": "1250530",
    "end": "1256890"
  },
  {
    "text": "inside stage maker automatically adjusts the amount of traffic that's needed to go to each of these models so you can",
    "start": "1256890",
    "end": "1262980"
  },
  {
    "text": "see how each of them are doing now that",
    "start": "1262980",
    "end": "1269090"
  },
  {
    "start": "1266000",
    "end": "1266000"
  },
  {
    "text": "yet the the amount of traffic that's going to that neural network now might get overwhelmed because it's just we",
    "start": "1269090",
    "end": "1276150"
  },
  {
    "text": "went from 5 percent to 50 percent with that update configuration method so",
    "start": "1276150",
    "end": "1282720"
  },
  {
    "text": "those two machines might not be enough so you might need to automatically scale your endpoint so stage maker makes this",
    "start": "1282720",
    "end": "1289290"
  },
  {
    "text": "easy straight from the console so you can specify when you're setting up an endpoint how many machines you want to",
    "start": "1289290",
    "end": "1295350"
  },
  {
    "text": "scale between so here we're saying that we want at least 2 and up to 5 and you",
    "start": "1295350",
    "end": "1301320"
  },
  {
    "text": "need to tell stage maker how it's going to scale what criterion is going to use to scale on and here we're using target",
    "start": "1301320",
    "end": "1307890"
  },
  {
    "text": "invitations per second which is the number of queries per second transactions per second predictions per",
    "start": "1307890",
    "end": "1314550"
  },
  {
    "text": "second that each machine is making and this is something that we recommend using invitations per second because",
    "start": "1314550",
    "end": "1320880"
  },
  {
    "text": "it's it's generally more reliable than other measures you could use like CPU",
    "start": "1320880",
    "end": "1326280"
  },
  {
    "text": "utilization or GPU or memory or what-have-you so you need to do some tests on",
    "start": "1326280",
    "end": "1332400"
  },
  {
    "text": "your specific model and on the this type that you're interested in to figure out what's a good number and in this one we",
    "start": "1332400",
    "end": "1338040"
  },
  {
    "text": "decided that 800 was a good amount and we can specify the number of seconds to",
    "start": "1338040",
    "end": "1345450"
  },
  {
    "text": "cool down between a scale up or scale down event to prevent it from flapping back and forth excuse me",
    "start": "1345450",
    "end": "1354260"
  },
  {
    "text": "so automatic scaling is important of course because the amount of traffic",
    "start": "1355820",
    "end": "1362790"
  },
  {
    "start": "1358000",
    "end": "1358000"
  },
  {
    "text": "that hits any real-time service tends to vary with number of people who are awake or all sorts of things there's typically",
    "start": "1362790",
    "end": "1369150"
  },
  {
    "text": "a daily cycle a week a weekly cycle if you have enough machines turned on to",
    "start": "1369150",
    "end": "1374880"
  },
  {
    "text": "handle peak traffic all the time you're wasting money and if if the traffic were",
    "start": "1374880",
    "end": "1381240"
  },
  {
    "text": "to spike beyond what you for then some of your customers are going to have a bad experience with automatic scaling",
    "start": "1381240",
    "end": "1387570"
  },
  {
    "start": "1387000",
    "end": "1387000"
  },
  {
    "text": "enabled the number of invitations will adjust based on the number of occasions",
    "start": "1387570",
    "end": "1394230"
  },
  {
    "text": "per instance will adjust based on the traffic so here's a here's an actual plot of when we're testing this and to",
    "start": "1394230",
    "end": "1402660"
  },
  {
    "text": "begin with the one machine turned on and so the orange and blue lines are on top of each other so you just see the orange",
    "start": "1402660",
    "end": "1408510"
  },
  {
    "text": "one is the total number of invitations is the same as the number of invitations per second then when it passes up about",
    "start": "1408510",
    "end": "1415920"
  },
  {
    "text": "above 3000 or so then a second machine turns on and now the number of",
    "start": "1415920",
    "end": "1421740"
  },
  {
    "text": "invitations per instance gets cut in half even though the total amount of traffic stays high and so you can pick",
    "start": "1421740",
    "end": "1428790"
  },
  {
    "start": "1427000",
    "end": "1427000"
  },
  {
    "text": "whatever scaling criteria you want to be able to scale reliably different",
    "start": "1428790",
    "end": "1434370"
  },
  {
    "text": "algorithms are different especially when you start writing your own code in in a",
    "start": "1434370",
    "end": "1439560"
  },
  {
    "text": "framework or or in your own container and whatever metric is posted to cloud",
    "start": "1439560",
    "end": "1445560"
  },
  {
    "text": "watch metric you can use to scale these endpoints so if you want to set up automatic scaling policy yourself using",
    "start": "1445560",
    "end": "1453780"
  },
  {
    "text": "the API this is what it looks like we're going to use the AWS application auto",
    "start": "1453780",
    "end": "1459090"
  },
  {
    "start": "1455000",
    "end": "1455000"
  },
  {
    "text": "scaling feature and we're going to call the API called reg register scaleable target we're going to point it to sage",
    "start": "1459090",
    "end": "1465420"
  },
  {
    "text": "maker give it the our endpoint and tell it to adjust the desired instance count between 2 and 5",
    "start": "1465420",
    "end": "1470679"
  },
  {
    "text": "and then we have to create this policy and this is a this is a complex policy",
    "start": "1470679",
    "end": "1476200"
  },
  {
    "text": "which gives you a ton of control if you do this through the console then it's going to set up a bunch of common",
    "start": "1476200",
    "end": "1483249"
  },
  {
    "text": "defaults for you and this one's actually a little bit different here instead of in vacations per second we are targeting",
    "start": "1483249",
    "end": "1489099"
  },
  {
    "text": "CPU utilization and we're saying we want 50% utilization which is you know a",
    "start": "1489099",
    "end": "1494320"
  },
  {
    "text": "reasonable amount if you're above that then your machines are running pretty hot maybe you want to add some more if you're below that you probably help more",
    "start": "1494320",
    "end": "1500019"
  },
  {
    "text": "machines you need so we can turn some down and so here it is again overlaying",
    "start": "1500019",
    "end": "1505509"
  },
  {
    "text": "that same graph of in vacations against the CPU utilization so this machine was",
    "start": "1505509",
    "end": "1512109"
  },
  {
    "text": "I think an m5 12 XL so it could get up to 1200 % 1,200 percent total",
    "start": "1512109",
    "end": "1519519"
  },
  {
    "text": "utilization or 50% per CPU and as you see once it crossed over about half",
    "start": "1519519",
    "end": "1525459"
  },
  {
    "text": "utilization of each of those cores then a second machine got turned on and the invitations per instance went down so",
    "start": "1525459",
    "end": "1533349"
  },
  {
    "text": "that concludes my deep dive into real-time hosting at scale and now I'll",
    "start": "1533349",
    "end": "1540999"
  },
  {
    "text": "pass it over to Lewis who will talk about using Amazon sage maker at 21st Century Fox all right thank you",
    "start": "1540999",
    "end": "1549450"
  },
  {
    "text": "all right good afternoon to everyone as Leah mentioned my name is Luis canet and I'm the VP of data science and",
    "start": "1553929",
    "end": "1559610"
  },
  {
    "text": "engineering at 21st Century Fox so a 21st Century Fox I'm part of a team that",
    "start": "1559610",
    "end": "1566059"
  },
  {
    "start": "1564000",
    "end": "1564000"
  },
  {
    "text": "is called the Fox data analytics team and we're the center of excellence for data science and analytics across the",
    "start": "1566059",
    "end": "1572360"
  },
  {
    "text": "entire 21st Century Fox organization so my team specifically is focused on",
    "start": "1572360",
    "end": "1577820"
  },
  {
    "text": "identifying and executing on opportunities to apply am machine learning across these three main areas",
    "start": "1577820",
    "end": "1585080"
  },
  {
    "text": "the first one are commercial opportunities so how can we create models that help us optimize our current",
    "start": "1585080",
    "end": "1592130"
  },
  {
    "text": "revenue streams and identify new revenue streams for our organization the second one is consuming analytics so how can we",
    "start": "1592130",
    "end": "1599390"
  },
  {
    "text": "identify behavioral patterns in our consumers across all the digital",
    "start": "1599390",
    "end": "1604669"
  },
  {
    "text": "properties that we have and utilize those to be able to be recommender systems and other personalized",
    "start": "1604669",
    "end": "1610309"
  },
  {
    "text": "experiences a crawler across the different applications that we have and finally its content so this",
    "start": "1610309",
    "end": "1616520"
  },
  {
    "text": "one are things like automatically extracting metadata information from all",
    "start": "1616520",
    "end": "1622070"
  },
  {
    "text": "the media assets that we have stored and then doing things like automatic content",
    "start": "1622070",
    "end": "1629210"
  },
  {
    "text": "creation an example of that would be so we for example have on Fox Sports so a",
    "start": "1629210",
    "end": "1635929"
  },
  {
    "text": "game highlights is a very manual process so we're trying to build models that take all this metadata about the games",
    "start": "1635929",
    "end": "1642140"
  },
  {
    "text": "and use those to grain to create to automatically create highlights of those games so as you can see I mean we have a",
    "start": "1642140",
    "end": "1651530"
  },
  {
    "text": "wide variety of problems and machine learning problems that we're trying to solve and on top of that we're a global",
    "start": "1651530",
    "end": "1657500"
  },
  {
    "text": "organization with more than 1 billion subscribers we're in more than 170",
    "start": "1657500",
    "end": "1663320"
  },
  {
    "text": "countries and we own more than 500 channels between broadcast cable and digital channels so scale is something",
    "start": "1663320",
    "end": "1670549"
  },
  {
    "text": "that it's very important for us and as our data science team started to grow",
    "start": "1670549",
    "end": "1676419"
  },
  {
    "text": "there was a need to build a centralized data science work bench that will help",
    "start": "1676419",
    "end": "1681559"
  },
  {
    "start": "1677000",
    "end": "1677000"
  },
  {
    "text": "us standardize on our data science development process so that we could",
    "start": "1681559",
    "end": "1686730"
  },
  {
    "text": "improve the productivity or a team so some of the requirements that we have for that where this environment needed",
    "start": "1686730",
    "end": "1693270"
  },
  {
    "text": "to be scalable and that means for data preparation training and deployment deployment of our models he needed to",
    "start": "1693270",
    "end": "1699330"
  },
  {
    "text": "have a seamless integration with our AWS data hub so before starting this team",
    "start": "1699330",
    "end": "1705299"
  },
  {
    "text": "there was a huge effort to build our centralized data hub in the AWS cloud it",
    "start": "1705299",
    "end": "1711270"
  },
  {
    "text": "also needed to be flexible to be able to accommodate multiple frameworks so we didn't want our data science team to be",
    "start": "1711270",
    "end": "1718410"
  },
  {
    "text": "just focused on using a specific set of tools we wanted to give them the ability to use as many as they want it he had to",
    "start": "1718410",
    "end": "1724679"
  },
  {
    "text": "be multi-user so we have multiple data scientists working on different problems",
    "start": "1724679",
    "end": "1729960"
  },
  {
    "text": "so we wanted to have the blade set up security and multi-tenancy across this environment it also needed to be",
    "start": "1729960",
    "end": "1736020"
  },
  {
    "text": "centralized so that all of our data science team members could collaborate among them so the cloud was a pretty",
    "start": "1736020",
    "end": "1742650"
  },
  {
    "text": "good fit for that and we also wanted to minimize the the engineering overhead",
    "start": "1742650",
    "end": "1748950"
  },
  {
    "text": "that we have so we wanted our data science team members to be as autonomous as possible and not have to rely on",
    "start": "1748950",
    "end": "1753990"
  },
  {
    "text": "either the architecture team or the engineering team to do a lot of their development so at the time where our",
    "start": "1753990",
    "end": "1761220"
  },
  {
    "text": "team was started to grow stage maker was announced so we thought it was a pretty good solution for what we were trying to",
    "start": "1761220",
    "end": "1766980"
  },
  {
    "text": "do so we gave it a try and we've been working on fine-tuning the",
    "start": "1766980",
    "end": "1771990"
  },
  {
    "text": "out-of-the-box capabilities that come with sage maker to make it work for us and to also improve our productivity",
    "start": "1771990",
    "end": "1777960"
  },
  {
    "text": "using this tool so it took us several iterations to get to where we are today and my goal for today is to share with",
    "start": "1777960",
    "end": "1786090"
  },
  {
    "text": "you some of the challenges that we run into by using sage maker in the real world and some of the solutions that we",
    "start": "1786090",
    "end": "1792150"
  },
  {
    "text": "that we arrive to so the first challenge is data wrangling at scale so sage maker",
    "start": "1792150",
    "end": "1800429"
  },
  {
    "start": "1796000",
    "end": "1796000"
  },
  {
    "text": "does a really good job at training models at scale as well as deploying them at scale but when it comes to data",
    "start": "1800429",
    "end": "1807660"
  },
  {
    "text": "processing you need to rely on other tools like EMR and spark so we didn't",
    "start": "1807660",
    "end": "1813600"
  },
  {
    "text": "want our data engineer team to have to rely every time on our data and turn it into do all the data preparation for us",
    "start": "1813600",
    "end": "1819090"
  },
  {
    "text": "so that we could feed it into our so since our data science team was very familiar with Python and they were very",
    "start": "1819090",
    "end": "1827100"
  },
  {
    "text": "familiar to paternal books we decided to integrate sage maker with EMR so that",
    "start": "1827100",
    "end": "1833159"
  },
  {
    "text": "they could do data wrangling and data modeling in the same notebook as a",
    "start": "1833159",
    "end": "1838860"
  },
  {
    "text": "similar experience for them so the process to integrate sage maker with EMR",
    "start": "1838860",
    "end": "1844470"
  },
  {
    "text": "is kind of a four-step process and there's a really great blog post and that talks about the specific steps that",
    "start": "1844470",
    "end": "1851549"
  },
  {
    "text": "you need to follow but it's basically a four-step process the first you need to create an EMR cluster and make sure that",
    "start": "1851549",
    "end": "1857129"
  },
  {
    "text": "both SPARC and Levy services are enabled then you need to make sure that the",
    "start": "1857129",
    "end": "1862289"
  },
  {
    "text": "security groups that are open ports to communicate between Sage maker and EMR",
    "start": "1862289",
    "end": "1868860"
  },
  {
    "text": "specifically specifically the port 80 998 which is the standard for SPARC then",
    "start": "1868860",
    "end": "1875220"
  },
  {
    "text": "when you create a new stage maker instance you need to make sure that that's a maker instance is within the",
    "start": "1875220",
    "end": "1881429"
  },
  {
    "text": "same DPC as your EMR or it has a good way of kind of connecting to it and then",
    "start": "1881429",
    "end": "1886499"
  },
  {
    "text": "finally you need to configure your stage maker instance to point to your the",
    "start": "1886499",
    "end": "1892139"
  },
  {
    "text": "master node in your EMR cluster in this case it would look very similar to what you have here in the right and you would",
    "start": "1892139",
    "end": "1898379"
  },
  {
    "text": "just replace the IP address that we have in here with the specific IP address of your EMR master node so once you do that",
    "start": "1898379",
    "end": "1909240"
  },
  {
    "text": "you can basically go to create a new notebook instance click on the first",
    "start": "1909240",
    "end": "1914940"
  },
  {
    "text": "option which is PI spark spark magic and then that gives you out-of-the-box a notebook where you can actually import",
    "start": "1914940",
    "end": "1921539"
  },
  {
    "text": "invoke spark context and start utilizing it as if you were writing any other",
    "start": "1921539",
    "end": "1927350"
  },
  {
    "text": "spark spark job but in this case it has the interactivity that comes out of the",
    "start": "1927350",
    "end": "1933869"
  },
  {
    "text": "box with a two pair notebook so that gave us an opportunity to start",
    "start": "1933869",
    "end": "1938970"
  },
  {
    "text": "leveraging Jupiter notebooks to interact with spark within sage maker everything in the cloud but when we had this we run",
    "start": "1938970",
    "end": "1946860"
  },
  {
    "text": "into a sari and before we go into that I wanted to explain a little bit how this",
    "start": "1946860",
    "end": "1952559"
  },
  {
    "start": "1947000",
    "end": "1947000"
  },
  {
    "text": "works under the hood because it will help plain the challenges that we that we run into afterwards so a as opposed to when",
    "start": "1952559",
    "end": "1961539"
  },
  {
    "text": "you're using an out-of-the-box out-of-the-box seismic instance where the code the Python code that you have",
    "start": "1961539",
    "end": "1967929"
  },
  {
    "text": "in your Jupiter notebook it's been executed in that same instance when you connect your seismic a notebook to an",
    "start": "1967929",
    "end": "1975130"
  },
  {
    "text": "EMR cluster your code is actually not being executed in your stage maker instance what is happening is that every",
    "start": "1975130",
    "end": "1981940"
  },
  {
    "text": "cell that you execute gets transmitted through HTTP into a levee server which",
    "start": "1981940",
    "end": "1987130"
  },
  {
    "text": "is a service that allows SPARC to be accessed through an HTTP interface and",
    "start": "1987130",
    "end": "1993010"
  },
  {
    "text": "then your code is actually being executed executed within the EMR cluster and that's important because it causes",
    "start": "1993010",
    "end": "2000029"
  },
  {
    "text": "issues like this so when you're trying to use some of the very widely used",
    "start": "2000029",
    "end": "2009200"
  },
  {
    "start": "2002000",
    "end": "2002000"
  },
  {
    "text": "libraries in the machine learning more like pandas to do data wrangling and you want to combine that with SPARC then you",
    "start": "2009200",
    "end": "2016260"
  },
  {
    "text": "realize that that library is actually not available and the reason why this happen even though those libraries are",
    "start": "2016260",
    "end": "2021299"
  },
  {
    "text": "available in such maker when you connect stage maker to EMR those libraries disappeared because your code is",
    "start": "2021299",
    "end": "2026610"
  },
  {
    "text": "actually being executed in the EMR cluster which has plain vanilla Python so one of the solutions that we that we",
    "start": "2026610",
    "end": "2035480"
  },
  {
    "text": "did for that was deploying Python",
    "start": "2035480",
    "end": "2041100"
  },
  {
    "start": "2036000",
    "end": "2036000"
  },
  {
    "text": "anaconda across all of our different nodes as the standard Python",
    "start": "2041100",
    "end": "2046200"
  },
  {
    "text": "distribution being used by AMR so for those of you who are not familiar python anaconda is a Python distribution that",
    "start": "2046200",
    "end": "2052138"
  },
  {
    "text": "comes out of the box with a lot of different data science libraries pre-installed so that's why we took it",
    "start": "2052139",
    "end": "2057720"
  },
  {
    "text": "as an initial point for us to start access having access on the ML class to a cluster to a lot of the data science",
    "start": "2057720",
    "end": "2065250"
  },
  {
    "text": "libraries that we use and I mean the process is pretty simple I mean you just on the Left you just have the standard",
    "start": "2065250",
    "end": "2070730"
  },
  {
    "text": "steps on how to install Python anaconda in an image in any Linux machine where",
    "start": "2070730",
    "end": "2075990"
  },
  {
    "text": "you just get the package execute the Installer and just configure your profile to use that as your default",
    "start": "2075990",
    "end": "2082020"
  },
  {
    "text": "Python and then in the right you have some additional steps that are very very important for",
    "start": "2082020",
    "end": "2087618"
  },
  {
    "text": "you to be able to leverage to configure your EMR cluster to leverage the new",
    "start": "2087619",
    "end": "2092658"
  },
  {
    "text": "Python version that you have installed so first of all you need to make sure that within your spark configuration you",
    "start": "2092659",
    "end": "2099349"
  },
  {
    "text": "add that in the master node you add that consume configuration step that tells",
    "start": "2099349",
    "end": "2106930"
  },
  {
    "text": "spark what is the default Python distribution through this part pie spot Python a configuration parameter and",
    "start": "2107529",
    "end": "2113509"
  },
  {
    "text": "second you actually need to make sure that your levy server which is the server that allows you to communicate to",
    "start": "2113509",
    "end": "2120950"
  },
  {
    "text": "spark through HTTP also knows that your Python distribution is the one that you",
    "start": "2120950",
    "end": "2126650"
  },
  {
    "text": "have just installed so that solves our initial problem of being able to",
    "start": "2126650",
    "end": "2132049"
  },
  {
    "text": "leverage additional data science libraries within EMR on sage maker but",
    "start": "2132049",
    "end": "2139759"
  },
  {
    "start": "2139000",
    "end": "2139000"
  },
  {
    "text": "then we also run into another challenge so we were using a we were using a spark",
    "start": "2139759",
    "end": "2146630"
  },
  {
    "text": "to be able to do a lot of the data wrangling so when we were creating",
    "start": "2146630",
    "end": "2151849"
  },
  {
    "text": "complex data objects like spark vector",
    "start": "2151849",
    "end": "2157509"
  },
  {
    "text": "then it was really hard to feed that vector into sage maker sage miracle but",
    "start": "2157509",
    "end": "2164299"
  },
  {
    "text": "by default it takes in either CSV files or some other files that are specific to",
    "start": "2164299",
    "end": "2170749"
  },
  {
    "text": "the to the models that you're trying to to train so for example for XT boost it",
    "start": "2170749",
    "end": "2176089"
  },
  {
    "text": "can take either CSV or another format that is called Lib SVN so if you want to",
    "start": "2176089",
    "end": "2181400"
  },
  {
    "text": "take a vector and write it as a CSV file you actually need to expand it into multiple count which is a very",
    "start": "2181400",
    "end": "2186619"
  },
  {
    "text": "cumbersome process and it also takes a lot of processing power but writing that",
    "start": "2186619",
    "end": "2194049"
  },
  {
    "text": "sparked data frame into lib spm format is not it's not simple either so one of",
    "start": "2194049",
    "end": "2200390"
  },
  {
    "text": "the solutions that we got to in order for us to be able to seamlessly take",
    "start": "2200390",
    "end": "2205940"
  },
  {
    "text": "spark data frame or spark object and feed it directly into our sage maker",
    "start": "2205940",
    "end": "2211099"
  },
  {
    "text": "mouse was by installing library in the",
    "start": "2211099",
    "end": "2216109"
  },
  {
    "start": "2213000",
    "end": "2213000"
  },
  {
    "text": "EMR cluster that is called a stage maker pike park so sage maker PI spar",
    "start": "2216109",
    "end": "2221750"
  },
  {
    "text": "what it does is it allows you to access all the sage maker services directly",
    "start": "2221750",
    "end": "2227600"
  },
  {
    "text": "from your EMR cluster so and what it does under the hood is when you are when",
    "start": "2227600",
    "end": "2233390"
  },
  {
    "text": "you have a sparked data frame and you feel it as a training object to your sage maker it actually writes that",
    "start": "2233390",
    "end": "2240740"
  },
  {
    "text": "object in the format that says America needs and to an s3 bucket and then when",
    "start": "2240740",
    "end": "2246170"
  },
  {
    "text": "you finish this training it actually deletes those files that it has created with the new format so that allowed us",
    "start": "2246170",
    "end": "2253190"
  },
  {
    "text": "to go from create do all of our data wrangling to our creator or vectors or",
    "start": "2253190",
    "end": "2259400"
  },
  {
    "text": "features and then feed those directly into into sage maker so that it could be",
    "start": "2259400",
    "end": "2264500"
  },
  {
    "text": "trained at the scale and also deployed at scale so another of the challenges",
    "start": "2264500",
    "end": "2270440"
  },
  {
    "text": "that we had is that when you're working with big data sometimes it takes a little bit of time for for for it to be",
    "start": "2270440",
    "end": "2278240"
  },
  {
    "text": "processed so by default when we started connecting to to the EMR cluster and we",
    "start": "2278240",
    "end": "2284630"
  },
  {
    "text": "had a job that was running for more than 10 minutes we started to get this error and so and the the biggest challenge is",
    "start": "2284630",
    "end": "2292310"
  },
  {
    "text": "that by default maybe comes out with a 10 minute timeout so if your job runs for more than 10",
    "start": "2292310",
    "end": "2298910"
  },
  {
    "text": "minutes then your jupiter notebook that you're interacting with will die if if your cell took more than 10 minutes to",
    "start": "2298910",
    "end": "2305030"
  },
  {
    "text": "run and you have to start everything from scratch again so the solution to to solve this problem is just go to the to",
    "start": "2305030",
    "end": "2312740"
  },
  {
    "text": "your master node in your EMR cluster go to the levy configuration and just increase your timeout to something much",
    "start": "2312740",
    "end": "2319070"
  },
  {
    "text": "bigger for example 10 hours that's what we have in here so I mean these smaller",
    "start": "2319070",
    "end": "2325520"
  },
  {
    "start": "2323000",
    "end": "2323000"
  },
  {
    "text": "steps there are a lot of other things that we've done to configure this environment to to make our team more",
    "start": "2325520",
    "end": "2331550"
  },
  {
    "text": "productive by using say maker but these are some of the things that cost us the most pain at the beginning so I wanted",
    "start": "2331550",
    "end": "2336650"
  },
  {
    "text": "to share them with you but after improving the the operational",
    "start": "2336650",
    "end": "2343280"
  },
  {
    "text": "workflow within city maker these are some of the things that we've seen as a game for our team so the technology and",
    "start": "2343280",
    "end": "2350960"
  },
  {
    "text": "process has been adapted for our team so a standardization of Central Asia of the data science work has improved",
    "start": "2350960",
    "end": "2356980"
  },
  {
    "text": "before we each of the data scientists have had their own environment by using this cloud based stage make your",
    "start": "2356980",
    "end": "2364370"
  },
  {
    "text": "environment everybody's more able to share their world with each other it has also increased the scale for our",
    "start": "2364370",
    "end": "2371120"
  },
  {
    "text": "models it has increased the efficiency of our data science work it has helped",
    "start": "2371120",
    "end": "2377390"
  },
  {
    "text": "us move from large dedicated instances to that we that we used to train our models to a scalable cloud",
    "start": "2377390",
    "end": "2385460"
  },
  {
    "text": "infrastructure and it has also reduced the cost that we had for training our",
    "start": "2385460",
    "end": "2391100"
  },
  {
    "text": "our mouths because Lee has mentioned before sage maker when you're trying a",
    "start": "2391100",
    "end": "2397280"
  },
  {
    "text": "model it actually spins off whatever instances it needs to train and then it",
    "start": "2397280",
    "end": "2402590"
  },
  {
    "text": "just went down so and another thing that has helped us a lot is the out-of-the-box recipes that come with",
    "start": "2402590",
    "end": "2410810"
  },
  {
    "text": "Sage maker that solve all of the very common data science modeling use cases and that's pretty much it I just wanted",
    "start": "2410810",
    "end": "2420290"
  },
  {
    "text": "to mention before we close that a lot of these improvements that we've done to our data science world--which",
    "start": "2420290",
    "end": "2426380"
  },
  {
    "text": "environment have been done in partnership with a AWS preserve team which is a really great resource to help",
    "start": "2426380",
    "end": "2433760"
  },
  {
    "text": "any company accelerate the adoption of the cloud services so if you are trying to accelerate your the option of any of",
    "start": "2433760",
    "end": "2440450"
  },
  {
    "text": "the AWS services and they are really good folks to to give a call to yeah",
    "start": "2440450",
    "end": "2446120"
  },
  {
    "text": "absolutely Thank You Louis",
    "start": "2446120",
    "end": "2449620"
  },
  {
    "text": "that's so with that I think the conclusion is obvious stage maker is a",
    "start": "2453650",
    "end": "2459449"
  },
  {
    "text": "great platform for building training and deploying machine learning models we saw in pretty good detail what capabilities",
    "start": "2459449",
    "end": "2466919"
  },
  {
    "text": "you have for deploying those models to low latency at scale and maintaining",
    "start": "2466919",
    "end": "2472439"
  },
  {
    "text": "them Lewis gave a great description of how they've integrated stage maker into their workflow at Fox and you're all",
    "start": "2472439",
    "end": "2480390"
  },
  {
    "text": "invited to go try sage maker on your own there's a free tier so you doesn't cost anything to give it a try and try",
    "start": "2480390",
    "end": "2487259"
  },
  {
    "text": "building your own mother's and with that we'll happily take any questions at the microphones in each of the middle aisles",
    "start": "2487259",
    "end": "2494039"
  },
  {
    "text": "thank you thank you great presentation I was curious on the deployment of sage",
    "start": "2494039",
    "end": "2502439"
  },
  {
    "text": "maker models in in other contexts like in on-prem very low latency scenarios in",
    "start": "2502439",
    "end": "2510150"
  },
  {
    "text": "which there's not enough time to call an HTTP endpoint what are the solutions for or combinations for that scenario yeah",
    "start": "2510150",
    "end": "2516929"
  },
  {
    "text": "that's a great question so the the machine learning models that are",
    "start": "2516929",
    "end": "2521969"
  },
  {
    "text": "produced in Sage Maker training are saved to your s3 bucket in a file called",
    "start": "2521969",
    "end": "2528119"
  },
  {
    "text": "model tar.gz and this is just a tarball you can uncompress it and you can see all of the files and so you can take",
    "start": "2528119",
    "end": "2535380"
  },
  {
    "text": "those and do with them anything that you want so for if you wrote your own code",
    "start": "2535380",
    "end": "2541259"
  },
  {
    "text": "then interpreting the that model file will be as easy as understanding your",
    "start": "2541259",
    "end": "2547169"
  },
  {
    "text": "own code which hopefully isn't very hard if you're using a Amazon first party",
    "start": "2547169",
    "end": "2552419"
  },
  {
    "text": "built-in algorithms and many of those are based on open source so XG boost for example it's you can just use the open",
    "start": "2552419",
    "end": "2559559"
  },
  {
    "text": "source implementation of those models the the linear models are also very interpretable because it's just one wait",
    "start": "2559559",
    "end": "2567359"
  },
  {
    "text": "for each input that you supplied so you you have full control over where you can",
    "start": "2567359",
    "end": "2573659"
  },
  {
    "text": "deploy these models if you want to deploy it on your own application server because you need microsecond latency",
    "start": "2573659",
    "end": "2580439"
  },
  {
    "text": "instead of millisecond latency to hit an end point then you're certainly welcome to do that great question",
    "start": "2580439",
    "end": "2586480"
  },
  {
    "text": "yeah thank you so much it was an awesome talk so I have a couple of questions",
    "start": "2586480",
    "end": "2591760"
  },
  {
    "text": "first of all is like when you say like you can auto scale your model execution",
    "start": "2591760",
    "end": "2597490"
  },
  {
    "text": "are you talking about model execution in parallel or like it is basically taking",
    "start": "2597490",
    "end": "2602680"
  },
  {
    "text": "a multiple requests and multiple requests are going to different instances yes so it is horizontal",
    "start": "2602680",
    "end": "2609579"
  },
  {
    "text": "scaling so the scaling is adding more machines to the fleet behind a load",
    "start": "2609579",
    "end": "2615490"
  },
  {
    "text": "balancer so each request to the end point still gets routed to a single single instance okay and we'll only get",
    "start": "2615490",
    "end": "2623320"
  },
  {
    "text": "executed one time there's no capacity to take a request and split it up across a",
    "start": "2623320",
    "end": "2629730"
  },
  {
    "text": "variety of machines so if you need more compute for your for your hosting then",
    "start": "2629730",
    "end": "2637359"
  },
  {
    "text": "we support vertical scaling so you can turn on a pea 3/16 Excel machine for hosting with 8 voltage GPUs and 64 CPU",
    "start": "2637359",
    "end": "2645579"
  },
  {
    "text": "cores if you need to but if your model doesn't run fast enough on that you'll",
    "start": "2645579",
    "end": "2651220"
  },
  {
    "text": "have to get creative ok so second question is like so spark Emily so they actually have some model",
    "start": "2651220",
    "end": "2658869"
  },
  {
    "text": "execution which runs in parallel in multiple nodes right so is there any integration point where like sage maker",
    "start": "2658869",
    "end": "2665710"
  },
  {
    "text": "and spark Emily board together something like that I'm gonna get you on the spark where you actually do that very often so",
    "start": "2665710",
    "end": "2672520"
  },
  {
    "text": "we switch back and forth between building stage maker malls and Emily",
    "start": "2672520",
    "end": "2678010"
  },
  {
    "text": "bowels which is the machine learning library that comes out of spark and I mean at that point your sage maker",
    "start": "2678010",
    "end": "2684400"
  },
  {
    "text": "instance is kind of a hosted in notebook instance so if you have that notebook Easter's connected to sage maker i sorry",
    "start": "2684400",
    "end": "2691990"
  },
  {
    "text": "- to your EMR you can build your machine learning model using ml lib and then if",
    "start": "2691990",
    "end": "2697450"
  },
  {
    "text": "the output of that model needs to be fed into another matchmaker notebook you can",
    "start": "2697450",
    "end": "2702760"
  },
  {
    "text": "use this h maker PI spot library that I just mentioned and within the same even within the same notebook you can go",
    "start": "2702760",
    "end": "2708790"
  },
  {
    "text": "through that back and forth between Emily and sage maker and all that stuff but that cannot be deployed to the",
    "start": "2708790",
    "end": "2715089"
  },
  {
    "text": "production or endpoints right no so I mean no and yes because",
    "start": "2715089",
    "end": "2720190"
  },
  {
    "text": "you can always take that code and package it into a darker instance once",
    "start": "2720190",
    "end": "2725530"
  },
  {
    "text": "your model is strange so you can use ML Lib to train your mod scale but Duns you once you have trained your model those",
    "start": "2725530",
    "end": "2732220"
  },
  {
    "text": "parameters can be a package into a darker instance then you can deploy the",
    "start": "2732220",
    "end": "2738190"
  },
  {
    "text": "scale using the container framework that comes with sage maker thank you thank",
    "start": "2738190",
    "end": "2743560"
  },
  {
    "text": "you so much I always a great talk question you said that you were starting",
    "start": "2743560",
    "end": "2749170"
  },
  {
    "text": "with sage maker notebooks and you're calling into EMR to do the data",
    "start": "2749170",
    "end": "2755050"
  },
  {
    "text": "wrangling and he thought about doing the other way around in terms of running Jupiter hub off of the EMR getting into",
    "start": "2755050",
    "end": "2761710"
  },
  {
    "text": "the notebooks there and making calls out the sage maker did you evaluate it in that direction and was there any pros and cons in either direction no I mean",
    "start": "2761710",
    "end": "2770349"
  },
  {
    "text": "we pretty much went through Sage maker as a starting point because we saw that",
    "start": "2770349",
    "end": "2776920"
  },
  {
    "text": "a sage maker came with all those optimized libraries and so that's what",
    "start": "2776920",
    "end": "2783069"
  },
  {
    "text": "we started and then we realized when we started in that route we realized that data wrangling was an issue and then we",
    "start": "2783069",
    "end": "2788440"
  },
  {
    "text": "added all these extensions to be able to overcome those issues right but yeah we",
    "start": "2788440",
    "end": "2793930"
  },
  {
    "text": "haven't evaluated the other the other way around because that's the way we started okay great thank you the other way around would totally work right if",
    "start": "2793930",
    "end": "2801010"
  },
  {
    "text": "you have whatever code running in EMR whether it's a notebook or a cluster you",
    "start": "2801010",
    "end": "2807069"
  },
  {
    "text": "can make API calls into sage maker from there either to launch training jobs or",
    "start": "2807069",
    "end": "2812650"
  },
  {
    "text": "to make predictions on pre-trained models it doesn't really make sense to make an API call into an open instance",
    "start": "2812650",
    "end": "2819790"
  },
  {
    "text": "because those are really integrated development environments but to any of the other sage maker services you can",
    "start": "2819790",
    "end": "2825520"
  },
  {
    "text": "totally call into those from EMR or batch or lambda or any other computing",
    "start": "2825520",
    "end": "2830829"
  },
  {
    "text": "environment that you want okay great thank you",
    "start": "2830829",
    "end": "2834810"
  },
  {
    "text": "yeah I had a quick question about the hyper parameter it's a fairly new feature is",
    "start": "2840600",
    "end": "2847480"
  },
  {
    "text": "that right it was announced with sage maker a year ago it said reinvention has",
    "start": "2847480",
    "end": "2855160"
  },
  {
    "text": "been generally available since June if I remember correctly okay is it possible",
    "start": "2855160",
    "end": "2861850"
  },
  {
    "text": "to get the statistics or any of the data that's happening behind the scenes say",
    "start": "2861850",
    "end": "2867400"
  },
  {
    "text": "you want to get out of sample prediction to do confidence interval testing in a in a in bootstrapping",
    "start": "2867400",
    "end": "2873370"
  },
  {
    "text": "is that a can you get that or you just run this it runs the parameter gives you",
    "start": "2873370",
    "end": "2878740"
  },
  {
    "text": "the best one and that's it so the the hyper parameter tuner uses a technique",
    "start": "2878740",
    "end": "2885670"
  },
  {
    "text": "called Bayesian optimization where it builds a machine learning model to predict the quality of your machine",
    "start": "2885670",
    "end": "2891970"
  },
  {
    "text": "learning models and that the kind of Bayesian optimization we use creates a",
    "start": "2891970",
    "end": "2899920"
  },
  {
    "text": "model called a Gaussian process which is very data efficient model so it trains well when you have you know 10 or 20",
    "start": "2899920",
    "end": "2907110"
  },
  {
    "text": "training data points the detail the the",
    "start": "2907110",
    "end": "2912220"
  },
  {
    "text": "exact configuration of the GP model that is used is not exposed because it's",
    "start": "2912220",
    "end": "2919480"
  },
  {
    "text": "constantly being tweaked and improved honestly like we if you got the details",
    "start": "2919480",
    "end": "2924790"
  },
  {
    "text": "of it today and in a month the scientists would have tweaked its behavior and those wouldn't mean anything but the data that the GP is",
    "start": "2924790",
    "end": "2932620"
  },
  {
    "text": "trained on are all transparent to you and in fact data that you're training",
    "start": "2932620",
    "end": "2938590"
  },
  {
    "text": "code generates and is emitted to cloud watch metrics so every hyper parameter",
    "start": "2938590",
    "end": "2944260"
  },
  {
    "text": "tuning job and now recently every stage maker training job I can omit algorithm",
    "start": "2944260",
    "end": "2951940"
  },
  {
    "text": "metrics us which track things like the loss on the training and validation sets",
    "start": "2951940",
    "end": "2958980"
  },
  {
    "text": "or whatever other quality metrics you might want and those are sent to cloud",
    "start": "2958980",
    "end": "2964510"
  },
  {
    "text": "watch metrics in your account where you can see them and those are the metrics that the",
    "start": "2964510",
    "end": "2970250"
  },
  {
    "text": "the hyper parameter tuning model is trained based on okay so you can get the statistics on each iteration then sorry",
    "start": "2970250",
    "end": "2977480"
  },
  {
    "text": "can you speak closer to the mic you can get the statistics on each of the each iteration as you're doing your",
    "start": "2977480",
    "end": "2982820"
  },
  {
    "text": "sequential that's right and you you will see four so every training job that happens as part of a hyper parameter",
    "start": "2982820",
    "end": "2989330"
  },
  {
    "text": "tuning job happens transparently in your account you get the full log of it you get the model that's created from it you",
    "start": "2989330",
    "end": "2995720"
  },
  {
    "text": "see everything about it so yeah you get all of those statistics which yeah great",
    "start": "2995720",
    "end": "3004120"
  },
  {
    "text": "thanks yep thanks very much it's very interesting",
    "start": "3004120",
    "end": "3011950"
  },
  {
    "text": "implementation between the two technologies a question about how the predictions work so if you have sort of",
    "start": "3011950",
    "end": "3019570"
  },
  {
    "text": "danger wrangling that's occurring within the Jupiter layer of sage maker and then you want to run like a real-time",
    "start": "3019570",
    "end": "3025900"
  },
  {
    "text": "prediction or even a batch prediction does that mean that that's then pointing back to this EMR cluster that's",
    "start": "3025900",
    "end": "3031270"
  },
  {
    "text": "continuously running in order to perform the data transformations that are required for that prediction that is",
    "start": "3031270",
    "end": "3039580"
  },
  {
    "text": "correct so a lot of the jobs that we've been dealing with at this scale have been mainly based jobs but we we have",
    "start": "3039580",
    "end": "3047080"
  },
  {
    "text": "started experimented with a few real-time ones and we have a continuously running EMR cluster that",
    "start": "3047080",
    "end": "3052840"
  },
  {
    "text": "[Music] leverages that we can that we leverage to pre-process that data at scale it's a",
    "start": "3052840",
    "end": "3061780"
  },
  {
    "text": "pre process do you mean that the data wrangling that occurs within the Jupiter",
    "start": "3061780",
    "end": "3066940"
  },
  {
    "text": "notebook also has sort of a layer of implementation that transforms the data",
    "start": "3066940",
    "end": "3073600"
  },
  {
    "text": "in preparation for the batch prediction process no no sorry I'm talking about when you're doing",
    "start": "3073600",
    "end": "3079420"
  },
  {
    "text": "real-time predictions those real-time predictions are being executed in an EMR",
    "start": "3079420",
    "end": "3085870"
  },
  {
    "text": "cluster sorry those real-time data preparation for the for your feature engineering are being done in the EMR",
    "start": "3085870",
    "end": "3091780"
  },
  {
    "text": "cluster got it okay and then a follow-up question about the the way that you think about data",
    "start": "3091780",
    "end": "3098380"
  },
  {
    "text": "wrangling for data science do you have the concept of sort of that",
    "start": "3098380",
    "end": "3104030"
  },
  {
    "text": "data scientists may perform transformations that are valuable to have at a higher level or maybe an",
    "start": "3104030",
    "end": "3111000"
  },
  {
    "text": "earlier level in the data lineage that would be transformations that you would",
    "start": "3111000",
    "end": "3116430"
  },
  {
    "text": "want to promote to maybe like a data warehouse layer or something like that yeah I mean as I mentioned at the",
    "start": "3116430",
    "end": "3121560"
  },
  {
    "text": "beginning our reason to go through this architectural decision was to limit our",
    "start": "3121560",
    "end": "3129200"
  },
  {
    "text": "dependency on the data engineering team especially on the development side of things right as we're doing data science",
    "start": "3129200",
    "end": "3135960"
  },
  {
    "text": "modeling we don't know what features we want to use so we want to go through as many experimentations as possible so",
    "start": "3135960",
    "end": "3140970"
  },
  {
    "text": "it's not feasible to have to give us back to the data engineering team every time we want to make a new",
    "start": "3140970",
    "end": "3146040"
  },
  {
    "text": "experimentation and say this is what we want can you create that so that we can fit it into our model so these are allowing us to iterate a lot faster and",
    "start": "3146040",
    "end": "3152400"
  },
  {
    "text": "once we have well-defined a data transformation job that gives us the",
    "start": "3152400",
    "end": "3158700"
  },
  {
    "text": "features that we want that were well then we want to deploy into production then at that stage we give it back to the engineering team to productionize it",
    "start": "3158700",
    "end": "3165090"
  },
  {
    "text": "but this is this mainly helps us a lot in improving those iterations during the",
    "start": "3165090",
    "end": "3170430"
  },
  {
    "text": "development of our models excellent thanks very much and I'll add an alternative to the data wrangling for",
    "start": "3170430",
    "end": "3177690"
  },
  {
    "text": "prediction which is some customers who do similar workflows to Fox when it",
    "start": "3177690",
    "end": "3184500"
  },
  {
    "text": "comes to prediction they will bundle together at the data pre-processing code and build a custom container for",
    "start": "3184500",
    "end": "3191190"
  },
  {
    "text": "prediction which first does the transformation using the spark system em",
    "start": "3191190",
    "end": "3197550"
  },
  {
    "text": "leap is a package that that's often used to do this for for low latency and then it'll bundle together that",
    "start": "3197550",
    "end": "3203640"
  },
  {
    "text": "pre-processing code with the learned model and deploy it all inside of a sage",
    "start": "3203640",
    "end": "3209550"
  },
  {
    "text": "maker endpoint so it's an alternative way of doing it okay thanks quick",
    "start": "3209550",
    "end": "3215940"
  },
  {
    "text": "question on the same topic so what if the data wrangling and the transformation is been done outside AWS",
    "start": "3215940",
    "end": "3222270"
  },
  {
    "text": "so basically it's not within the AWS stack have you ever come across that kind of use case from a customer so like",
    "start": "3222270",
    "end": "3229920"
  },
  {
    "text": "how would that integration would look like because then you won't be able to integrate it but the ec2 machine yeah in",
    "start": "3229920",
    "end": "3236070"
  },
  {
    "text": "if you if you're doing data pre-processing on Prem for example or maybe inside of your",
    "start": "3236070",
    "end": "3242560"
  },
  {
    "text": "ear inside of your database or data warehouse for example before loading it",
    "start": "3242560",
    "end": "3247720"
  },
  {
    "text": "into s3 to use it with sage maker this is this is common and in normal but you",
    "start": "3247720",
    "end": "3256590"
  },
  {
    "text": "there's a and very useful and important sometimes but there's an opportunity for",
    "start": "3256590",
    "end": "3262630"
  },
  {
    "text": "bugs because you need to replicate those data processing steps in your prediction",
    "start": "3262630",
    "end": "3268390"
  },
  {
    "text": "code in order to achieve the same results so you know you can do it you",
    "start": "3268390",
    "end": "3273430"
  },
  {
    "text": "just have to be careful and test to make sure that the data processing that you're doing at prediction time matches",
    "start": "3273430",
    "end": "3279070"
  },
  {
    "text": "what you're doing at to prepare the data for training and in that sense it's I",
    "start": "3279070",
    "end": "3287080"
  },
  {
    "text": "don't know I I would say it's maybe not the best practice because this is an",
    "start": "3287080",
    "end": "3292180"
  },
  {
    "text": "opportunity for bugs to creep in if you have separate code paths for doing those",
    "start": "3292180",
    "end": "3297780"
  },
  {
    "text": "those data processing steps which are supposed to be having the same result because data processing would normally",
    "start": "3297780",
    "end": "3304330"
  },
  {
    "text": "include integration with multiple data sources as well so like even that would then need to be included in the same",
    "start": "3304330",
    "end": "3310900"
  },
  {
    "text": "card yeah I mean it's it's a reality right like you have to do it sometimes but it's just something to pay attention",
    "start": "3310900",
    "end": "3317470"
  },
  {
    "text": "to and be careful with like test it well you know feed data into both systems make sure you're getting the same",
    "start": "3317470",
    "end": "3323320"
  },
  {
    "text": "results out and and then it should work one thing that I like to add is that for",
    "start": "3323320",
    "end": "3330460"
  },
  {
    "text": "example we use s3 as that integration layer in a lot of situations like like",
    "start": "3330460",
    "end": "3335530"
  },
  {
    "text": "the one you're mentioning so if you have we have data from multiple data sources we will create those exports put them",
    "start": "3335530",
    "end": "3340900"
  },
  {
    "text": "into s3 and then utilize that EMR cluster or if the data is not a big dense ice maker directly to be able to",
    "start": "3340900",
    "end": "3347230"
  },
  {
    "text": "wrangle the data together to be able to fit it into our model or even sometimes if your data is already prepared you",
    "start": "3347230",
    "end": "3353020"
  },
  {
    "text": "just point your H maker model to that s2 location that contains your data in CSV format so after integration do you bring",
    "start": "3353020",
    "end": "3359710"
  },
  {
    "text": "the bring the data back into base three yeah so if imagine that you have three different data bases and each of them",
    "start": "3359710",
    "end": "3365620"
  },
  {
    "text": "have a subset of data that you want you can always create a job that exports from those is the subset of the data that you want",
    "start": "3365620",
    "end": "3372099"
  },
  {
    "text": "put it into his three and then leverage EMR to wrangle it all together create your training set or and your validation",
    "start": "3372099",
    "end": "3378009"
  },
  {
    "text": "set and feed into s3 make sense Thanks I think we didn't take one more question we were just about out of time so last",
    "start": "3378009",
    "end": "3386680"
  },
  {
    "text": "chapter last year's reinvent did some playing around with sage maker and and felt like",
    "start": "3386680",
    "end": "3392680"
  },
  {
    "text": "there were some boundaries around it and it sounds like that's what you guys encountered and went through quite a bit",
    "start": "3392680",
    "end": "3398289"
  },
  {
    "text": "of a hackathon experience to to make it work with other things I'm wondering",
    "start": "3398289",
    "end": "3403779"
  },
  {
    "text": "what AWS has learned from experiences like theirs and what they might be",
    "start": "3403779",
    "end": "3409089"
  },
  {
    "text": "applying to Sage Maker going down the road to kind of knock down some of those barriers well so we are we are always",
    "start": "3409089",
    "end": "3416499"
  },
  {
    "text": "listening to our customers as far as what they're using our products for how",
    "start": "3416499",
    "end": "3422499"
  },
  {
    "text": "stage maker is is working for them and some of the challenges that that they're",
    "start": "3422499",
    "end": "3427869"
  },
  {
    "text": "running into and so yeah we we absolutely are making improvements to",
    "start": "3427869",
    "end": "3433690"
  },
  {
    "text": "make it easier to use integrating it more seamlessly with with other AWS services if you have time after this I'd",
    "start": "3433690",
    "end": "3441099"
  },
  {
    "text": "love to hear some some specifics about about some of the things that that are causing your problems but we are know",
    "start": "3441099",
    "end": "3448269"
  },
  {
    "text": "that we're constantly innovating with it and working to improve it make it easier to use add new machine learning",
    "start": "3448269",
    "end": "3455289"
  },
  {
    "text": "capabilities and and you'll be hearing about those coming up very soon I hope",
    "start": "3455289",
    "end": "3463960"
  },
  {
    "text": "with that thank you all for your time and thanks enjoy the rest year of the rest of your week so much",
    "start": "3463960",
    "end": "3470729"
  }
]