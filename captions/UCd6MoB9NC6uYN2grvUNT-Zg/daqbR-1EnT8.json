[
  {
    "start": "0",
    "end": "19000"
  },
  {
    "text": "In this video, you’ll see how to analyze streaming \ndata using Amazon Kinesis Data Analytics Studio.",
    "start": "0",
    "end": "5630"
  },
  {
    "text": "With this solution, you can create a \nKinesis Data Analytics Studio notebook,",
    "start": "6135",
    "end": "10279"
  },
  {
    "text": "connect to an Amazon Managed Streaming \nfor Apache Kafka (Amazon MSK) cluster,",
    "start": "10279",
    "end": "15059"
  },
  {
    "text": "and interactively analyze \nevents from the stream.",
    "start": "15059",
    "end": "17622"
  },
  {
    "text": "We’ll start from the Amazon MSK console, \nwhere a cluster has already been created.",
    "start": "19913",
    "end": "24078"
  },
  {
    "text": "We’ll connect to this cluster using a \nKafka client that we’ve already created.",
    "start": "24893",
    "end": "28268"
  },
  {
    "start": "29000",
    "end": "61000"
  },
  {
    "text": "Let’s enter the Kafka console \ncommand to list our topics.",
    "start": "30307",
    "end": "33264"
  },
  {
    "text": "Notice that we have two topics: \ninput stream and output stream.",
    "start": "33730",
    "end": "37059"
  },
  {
    "text": "The input stream is where we’ll be \ningesting data into Amazon MSK,",
    "start": "37788",
    "end": "41435"
  },
  {
    "text": "and the output stream is where we’ll get our \nresults from the Kinesis Data Analytics application.",
    "start": "41436",
    "end": "45747"
  },
  {
    "text": "To simulate the workload, we’ll use a \nsimple Python script that generates",
    "start": "46649",
    "end": "49886"
  },
  {
    "text": "random sets of sensor data.",
    "start": "49887",
    "end": "51460"
  },
  {
    "text": "The script sends random records to the Amazon \nMSK input stream topic in JSON syntax.",
    "start": "52207",
    "end": "57253"
  },
  {
    "text": "Let’s start the producer.",
    "start": "58215",
    "end": "59308"
  },
  {
    "start": "61000",
    "end": "140000"
  },
  {
    "text": "Now that data is being produced into \nour input stream topic, let’s return to the",
    "start": "61589",
    "end": "65414"
  },
  {
    "text": "Amazon MSK console and create our \nKinesis Data Analytics Studio application.",
    "start": "65414",
    "end": "70121"
  },
  {
    "text": "First, we’ll select our cluster and choose \nthe option to process the data in real time.",
    "start": "71170",
    "end": "75081"
  },
  {
    "text": "For this example, let’s create an \nApache Flink - Studio notebook.",
    "start": "76081",
    "end": "79415"
  },
  {
    "text": "Kinesis Data Analytics will attach the \nrequired AWS Identity and Access",
    "start": "80104",
    "end": "84052"
  },
  {
    "text": "Management (IAM) policies for \n reading from the Amazon MSK cluster.",
    "start": "84053",
    "end": "88112"
  },
  {
    "text": "It will also input the VPC \nconfigurations that we need.",
    "start": "88491",
    "end": "91326"
  },
  {
    "text": "Let’s give the Studio notebook a name.",
    "start": "92326",
    "end": "94000"
  },
  {
    "text": "Kinesis Data Analytics will create an IAM \nrole with the permissions that we need.",
    "start": "98106",
    "end": "101908"
  },
  {
    "text": "Let’s create an empty AWS Glue database.",
    "start": "102500",
    "end": "105036"
  },
  {
    "text": "Let’s refresh the list and select \nthe database we just created.",
    "start": "113308",
    "end": "116302"
  },
  {
    "text": "This database will define the metadata\nfor our sources and destinations.",
    "start": "119535",
    "end": "123086"
  },
  {
    "text": "Let’s review the settings and \ncreate the Studio notebook.",
    "start": "124000",
    "end": "126449"
  },
  {
    "text": "The notebook has been successfully created.",
    "start": "129197",
    "end": "131016"
  },
  {
    "text": "Let’s review its configurations.",
    "start": "131191",
    "end": "132706"
  },
  {
    "text": "Before we can use the \nnotebook, we’ll need to run it.",
    "start": "134113",
    "end": "136397"
  },
  {
    "start": "140000",
    "end": "155000"
  },
  {
    "text": "The notebook will take a few minutes to start.",
    "start": "140678",
    "end": "142567"
  },
  {
    "text": "Once the Studio notebook has been successfully \nstarted, we can open it in Apache Zeppelin.",
    "start": "143490",
    "end": "147709"
  },
  {
    "text": "Let’s create a new note and call it “Sensors.”",
    "start": "148670",
    "end": "150930"
  },
  {
    "start": "155000",
    "end": "205000"
  },
  {
    "text": "Next, we’ll create a Sensor data table \ndescribing the format of the data in our stream.",
    "start": "155551",
    "end": "159642"
  },
  {
    "text": "The first line in this command tells \nApache Zeppelin to provide a stream",
    "start": "160662",
    "end": "163826"
  },
  {
    "text": "SQL environment for the \nApache Flink interpreter.",
    "start": "163826",
    "end": "166285"
  },
  {
    "text": "The subsequent lines use familiar SQL \nstatements to create a table that will",
    "start": "166810",
    "end": "170079"
  },
  {
    "text": "store the sensor data in the stream.",
    "start": "170079",
    "end": "171915"
  },
  {
    "text": "We’ve specified our input stream as a \ntopic, and we’re using the bootstrap",
    "start": "172448",
    "end": "175587"
  },
  {
    "text": "server of our Amazon MSK cluster.",
    "start": "175587",
    "end": "177711"
  },
  {
    "text": "Let’s run this command to create a table.",
    "start": "178391",
    "end": "180198"
  },
  {
    "text": "We can find the table we just created in the AWS \nGlue Data Catalog database used by this notebook.",
    "start": "181217",
    "end": "186524"
  },
  {
    "text": "Here’s the sensor data table we created.",
    "start": "189494",
    "end": "191387"
  },
  {
    "text": "Let’s return to the notebook.",
    "start": "191581",
    "end": "192756"
  },
  {
    "text": "Now we can run SQL queries on the \ndata table and use sliding or tumbling",
    "start": "193804",
    "end": "197160"
  },
  {
    "text": "windows to get a better understanding \nof what’s happening with our sensors.",
    "start": "197160",
    "end": "200374"
  },
  {
    "text": "First, let’s select all the \ncontent in the sensor data table.",
    "start": "201296",
    "end": "204058"
  },
  {
    "start": "205000",
    "end": "252000"
  },
  {
    "text": "To get a better understanding of this \ndata, we’ll switch to the bar graph view.",
    "start": "206116",
    "end": "209731"
  },
  {
    "text": "Let’s modify the settings to group the results by \nstatus to see the average current temperature.",
    "start": "210663",
    "end": "214881"
  },
  {
    "text": "As expected, we have different average \ntemperatures depending on the status.",
    "start": "224502",
    "end": "228067"
  },
  {
    "text": "The higher the temperature, the greater \nthe probability that something is not",
    "start": "228533",
    "end": "231570"
  },
  {
    "text": "working correctly with our sensors.",
    "start": "231570",
    "end": "233392"
  },
  {
    "text": "Next, we’ll run an aggregated query using SQL.",
    "start": "234411",
    "end": "237097"
  },
  {
    "text": "This time, we want the results computed\n on a sliding window of one minute,",
    "start": "238970",
    "end": "242417"
  },
  {
    "text": "with the results updated every 10 seconds.",
    "start": "242417",
    "end": "244805"
  },
  {
    "text": "To do this, we’ll use a HOP function in the \nGROUP BY section of the SELECT statement.",
    "start": "245611",
    "end": "249566"
  },
  {
    "start": "252000",
    "end": "280000"
  },
  {
    "text": "Let’s keep these results in table format.",
    "start": "253226",
    "end": "255153"
  },
  {
    "text": "If desired, we can send the results \nof this query to the destination topic",
    "start": "255902",
    "end": "259216"
  },
  {
    "text": "output stream we created earlier.",
    "start": "259217",
    "end": "261129"
  },
  {
    "text": "To do this, let’s create a \ntable called “sensor_state.”",
    "start": "261828",
    "end": "264587"
  },
  {
    "text": "Notice that we’re using the output \nstream from our demo cluster.",
    "start": "265548",
    "end": "268200"
  },
  {
    "text": "Now that our table is created, we can use \nan INSERT INTO statement to continuously",
    "start": "269171",
    "end": "273159"
  },
  {
    "text": "insert the results of the SELECT \nstatement into our sensor_state table.",
    "start": "273159",
    "end": "276647"
  },
  {
    "start": "280000",
    "end": "311000"
  },
  {
    "text": "We’ll use the Kafka console to confirm \nwhether Kinesis Data Analytics Studio is",
    "start": "280943",
    "end": "284869"
  },
  {
    "text": "actually ingesting data into our output stream.",
    "start": "284869",
    "end": "287358"
  },
  {
    "text": "We can confirm that data is being \nreceived into our output stream.",
    "start": "287865",
    "end": "290917"
  },
  {
    "text": "This data can now be used for other downstream \napplications, dashboarding, and further analytics.",
    "start": "291606",
    "end": "296294"
  },
  {
    "text": "You’ve just seen how to analyze streaming \ndata using Amazon Kinesis Data Analytics Studio.",
    "start": "298781",
    "end": "303505"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "304553",
    "end": "307716"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "308000",
    "end": "310200"
  }
]