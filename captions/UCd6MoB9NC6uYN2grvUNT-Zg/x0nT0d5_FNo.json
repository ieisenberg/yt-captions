[
  {
    "start": "0",
    "end": "29000"
  },
  {
    "text": "hello my name is lonid Quran I'm a",
    "start": "5720",
    "end": "8700"
  },
  {
    "text": "principal of SQL Solutions architect",
    "start": "8700",
    "end": "10200"
  },
  {
    "text": "here at AWS this short video will focus",
    "start": "10200",
    "end": "13320"
  },
  {
    "text": "on Dynamic feature of e-boarding data",
    "start": "13320",
    "end": "15360"
  },
  {
    "text": "from Amazon S3 I will describe the",
    "start": "15360",
    "end": "18119"
  },
  {
    "text": "feature go over things that should be",
    "start": "18119",
    "end": "19680"
  },
  {
    "text": "taken into consideration the best",
    "start": "19680",
    "end": "21720"
  },
  {
    "text": "practices of importing data from S3 and",
    "start": "21720",
    "end": "24119"
  },
  {
    "text": "will finally show a demo of the feature",
    "start": "24119",
    "end": "26279"
  },
  {
    "text": "so let's get started",
    "start": "26279",
    "end": "29900"
  },
  {
    "start": "29000",
    "end": "92000"
  },
  {
    "text": "import from S3 is a serverless feature",
    "start": "30779",
    "end": "33239"
  },
  {
    "text": "that doesn't require writing any code or",
    "start": "33239",
    "end": "35219"
  },
  {
    "text": "managing any infrastructure in order to",
    "start": "35219",
    "end": "37020"
  },
  {
    "text": "input data to a dynamodb table important",
    "start": "37020",
    "end": "40140"
  },
  {
    "text": "to note that today the feature only",
    "start": "40140",
    "end": "42059"
  },
  {
    "text": "allows inputting data into a new",
    "start": "42059",
    "end": "43980"
  },
  {
    "text": "dynamodb table the main use cases for",
    "start": "43980",
    "end": "46680"
  },
  {
    "text": "this feature include any bulk input that",
    "start": "46680",
    "end": "49500"
  },
  {
    "text": "you would like to perform into a new",
    "start": "49500",
    "end": "51420"
  },
  {
    "text": "dynamodb table or an ETL job where you",
    "start": "51420",
    "end": "54719"
  },
  {
    "text": "will export data from one source",
    "start": "54719",
    "end": "56280"
  },
  {
    "text": "transform it on S3 and then input to",
    "start": "56280",
    "end": "59219"
  },
  {
    "text": "dynamodb",
    "start": "59219",
    "end": "60539"
  },
  {
    "text": "for example in such a scenario the",
    "start": "60539",
    "end": "62579"
  },
  {
    "text": "source of the data may be an existing",
    "start": "62579",
    "end": "64440"
  },
  {
    "text": "dynamodb table if you would like to",
    "start": "64440",
    "end": "66479"
  },
  {
    "text": "restructure it or add additional",
    "start": "66479",
    "end": "68220"
  },
  {
    "text": "attributes before importing to a new",
    "start": "68220",
    "end": "70260"
  },
  {
    "text": "table",
    "start": "70260",
    "end": "71400"
  },
  {
    "text": "other such scenarios are one-time",
    "start": "71400",
    "end": "73799"
  },
  {
    "text": "copying of dynamodb table across",
    "start": "73799",
    "end": "75540"
  },
  {
    "text": "accounts or a migration to dynamodb",
    "start": "75540",
    "end": "78840"
  },
  {
    "text": "keep in mind that the data input doesn't",
    "start": "78840",
    "end": "81119"
  },
  {
    "text": "consume any right capacity from the",
    "start": "81119",
    "end": "83159"
  },
  {
    "text": "table and in the vast majority of cases",
    "start": "83159",
    "end": "85200"
  },
  {
    "text": "it is the most cost efficient option for",
    "start": "85200",
    "end": "87479"
  },
  {
    "text": "able in large quantities of data into",
    "start": "87479",
    "end": "89759"
  },
  {
    "text": "dynamodb",
    "start": "89759",
    "end": "92540"
  },
  {
    "start": "92000",
    "end": "119000"
  },
  {
    "text": "you can import data that is stored in",
    "start": "93659",
    "end": "95759"
  },
  {
    "text": "either CSV dynamodb Json or Amazon ion",
    "start": "95759",
    "end": "99000"
  },
  {
    "text": "formats the only table attributes that",
    "start": "99000",
    "end": "102060"
  },
  {
    "text": "you define as part of the input job are",
    "start": "102060",
    "end": "103979"
  },
  {
    "text": "the primary keys of the table and of the",
    "start": "103979",
    "end": "106140"
  },
  {
    "text": "global secondary indexes so keep in mind",
    "start": "106140",
    "end": "108479"
  },
  {
    "text": "that if using CSV as the input format",
    "start": "108479",
    "end": "110880"
  },
  {
    "text": "where the input does not contain column",
    "start": "110880",
    "end": "113159"
  },
  {
    "text": "types any non-key attributes will be",
    "start": "113159",
    "end": "115560"
  },
  {
    "text": "imported as strings",
    "start": "115560",
    "end": "118938"
  },
  {
    "start": "119000",
    "end": "141000"
  },
  {
    "text": "the input data can be compressed with Z",
    "start": "120960",
    "end": "123540"
  },
  {
    "text": "standard gzip or to be left uncompressed",
    "start": "123540",
    "end": "126799"
  },
  {
    "text": "data can be imported from an S3 bucket",
    "start": "126799",
    "end": "129959"
  },
  {
    "text": "in a different account and to a dynamodb",
    "start": "129959",
    "end": "132540"
  },
  {
    "text": "table in a different region the only",
    "start": "132540",
    "end": "135000"
  },
  {
    "text": "requirement is that the AWS role used",
    "start": "135000",
    "end": "137700"
  },
  {
    "text": "for the process has the required",
    "start": "137700",
    "end": "139440"
  },
  {
    "text": "permissions",
    "start": "139440",
    "end": "142220"
  },
  {
    "start": "141000",
    "end": "190000"
  },
  {
    "text": "now let's speak about input job errors",
    "start": "142260",
    "end": "144780"
  },
  {
    "text": "handling import arrows are locked to",
    "start": "144780",
    "end": "147060"
  },
  {
    "text": "cloudwatch for example such an error may",
    "start": "147060",
    "end": "149340"
  },
  {
    "text": "be a record that doesn't include a value",
    "start": "149340",
    "end": "151200"
  },
  {
    "text": "for the table's partition key it is",
    "start": "151200",
    "end": "153780"
  },
  {
    "text": "always recommended reviewing the import",
    "start": "153780",
    "end": "155160"
  },
  {
    "text": "job logs during the job run and after",
    "start": "155160",
    "end": "157319"
  },
  {
    "text": "its completion note that once 10 000",
    "start": "157319",
    "end": "160260"
  },
  {
    "text": "errors were counted the job stops",
    "start": "160260",
    "end": "162360"
  },
  {
    "text": "logging additional errors in cloudwatch",
    "start": "162360",
    "end": "164280"
  },
  {
    "text": "but it will still continue importing the",
    "start": "164280",
    "end": "166140"
  },
  {
    "text": "remaining Source data this is something",
    "start": "166140",
    "end": "167940"
  },
  {
    "text": "that you should be aware of when",
    "start": "167940",
    "end": "169500"
  },
  {
    "text": "reviewing the jobs errors in cloudwatch",
    "start": "169500",
    "end": "172680"
  },
  {
    "text": "also keep in mind that the dynamodb",
    "start": "172680",
    "end": "174660"
  },
  {
    "text": "table's primary key is unique so if",
    "start": "174660",
    "end": "177599"
  },
  {
    "text": "there are multiple records in the input",
    "start": "177599",
    "end": "179280"
  },
  {
    "text": "objects containing the same primary key",
    "start": "179280",
    "end": "181080"
  },
  {
    "text": "values these records will overwrite each",
    "start": "181080",
    "end": "183239"
  },
  {
    "text": "other during the input in a random order",
    "start": "183239",
    "end": "185640"
  },
  {
    "text": "this isn't considered to be an error by",
    "start": "185640",
    "end": "187980"
  },
  {
    "text": "the input job",
    "start": "187980",
    "end": "190640"
  },
  {
    "start": "190000",
    "end": "218000"
  },
  {
    "text": "as mentioned in the vast majority of",
    "start": "191099",
    "end": "193379"
  },
  {
    "text": "cases import from S3 is the most cost",
    "start": "193379",
    "end": "195480"
  },
  {
    "text": "efficient option for importing large",
    "start": "195480",
    "end": "197220"
  },
  {
    "text": "quantities of data into dynamodb data",
    "start": "197220",
    "end": "200400"
  },
  {
    "text": "input is charged based on the size of",
    "start": "200400",
    "end": "202319"
  },
  {
    "text": "the uncompressed data on S3",
    "start": "202319",
    "end": "205140"
  },
  {
    "text": "keep in mind that if during the input",
    "start": "205140",
    "end": "207420"
  },
  {
    "text": "process you create Global secondary",
    "start": "207420",
    "end": "209099"
  },
  {
    "text": "indexes for the table rise to those",
    "start": "209099",
    "end": "211260"
  },
  {
    "text": "indexes are made for free which makes",
    "start": "211260",
    "end": "213840"
  },
  {
    "text": "the input process even more cost",
    "start": "213840",
    "end": "215879"
  },
  {
    "text": "efficient",
    "start": "215879",
    "end": "218300"
  },
  {
    "start": "218000",
    "end": "308000"
  },
  {
    "text": "there are several best practices to keep",
    "start": "219060",
    "end": "221099"
  },
  {
    "text": "in mind in order to achieve optimal",
    "start": "221099",
    "end": "222840"
  },
  {
    "text": "results and optimal input times the",
    "start": "222840",
    "end": "225780"
  },
  {
    "text": "first is that when importing data you",
    "start": "225780",
    "end": "228480"
  },
  {
    "text": "want to make sure that the input objects",
    "start": "228480",
    "end": "230220"
  },
  {
    "text": "are not sorted based on the dynamodb",
    "start": "230220",
    "end": "232260"
  },
  {
    "text": "table's partition key since in such a",
    "start": "232260",
    "end": "234900"
  },
  {
    "text": "case the input would lead to Rolling hot",
    "start": "234900",
    "end": "237120"
  },
  {
    "text": "partitions in the dynamodb table and",
    "start": "237120",
    "end": "239340"
  },
  {
    "text": "much longer input types",
    "start": "239340",
    "end": "241680"
  },
  {
    "text": "such a situation may occur if for",
    "start": "241680",
    "end": "244019"
  },
  {
    "text": "example The Source data was exported",
    "start": "244019",
    "end": "246299"
  },
  {
    "text": "from a database through a scan",
    "start": "246299",
    "end": "248519"
  },
  {
    "text": "if indeed The Source data is sorted you",
    "start": "248519",
    "end": "251459"
  },
  {
    "text": "should randomize it in a prior to",
    "start": "251459",
    "end": "253920"
  },
  {
    "text": "importing the data",
    "start": "253920",
    "end": "256139"
  },
  {
    "text": "note that if the source data was",
    "start": "256139",
    "end": "258120"
  },
  {
    "text": "generated by the export to a free",
    "start": "258120",
    "end": "260040"
  },
  {
    "text": "feature of dynamodb for example in order",
    "start": "260040",
    "end": "262500"
  },
  {
    "text": "to copy a table to a different account",
    "start": "262500",
    "end": "264139"
  },
  {
    "text": "then the table is always exported from",
    "start": "264139",
    "end": "267600"
  },
  {
    "text": "dynamodb in a random fashion so you",
    "start": "267600",
    "end": "270419"
  },
  {
    "text": "don't need to worry about such a",
    "start": "270419",
    "end": "272340"
  },
  {
    "text": "scenario",
    "start": "272340",
    "end": "273960"
  },
  {
    "text": "note that data is imported concurrently",
    "start": "273960",
    "end": "276720"
  },
  {
    "text": "by hundreds of executors in order to",
    "start": "276720",
    "end": "278820"
  },
  {
    "text": "achieve high throughput so with large",
    "start": "278820",
    "end": "280860"
  },
  {
    "text": "quantities of data you should always",
    "start": "280860",
    "end": "282600"
  },
  {
    "text": "divide the source data to multiple",
    "start": "282600",
    "end": "284520"
  },
  {
    "text": "objects the optimal object size for",
    "start": "284520",
    "end": "287040"
  },
  {
    "text": "achieving maximum throughput is around 3",
    "start": "287040",
    "end": "288960"
  },
  {
    "text": "gigabytes of uncompressed data",
    "start": "288960",
    "end": "292680"
  },
  {
    "text": "you should also keep in mind the",
    "start": "292680",
    "end": "294300"
  },
  {
    "text": "original limits for total size of data",
    "start": "294300",
    "end": "296520"
  },
  {
    "text": "that can be imported in a single job and",
    "start": "296520",
    "end": "298860"
  },
  {
    "text": "a total number of objects if you are",
    "start": "298860",
    "end": "301500"
  },
  {
    "text": "hitting these limits you can compress",
    "start": "301500",
    "end": "303360"
  },
  {
    "text": "the data or combine multiple objects",
    "start": "303360",
    "end": "305639"
  },
  {
    "text": "into one",
    "start": "305639",
    "end": "308360"
  },
  {
    "start": "308000",
    "end": "549000"
  },
  {
    "text": "I will now demonstrate the process of",
    "start": "309180",
    "end": "311520"
  },
  {
    "text": "importing data from S3 while I show the",
    "start": "311520",
    "end": "314820"
  },
  {
    "text": "process from the dynamodb console the",
    "start": "314820",
    "end": "316919"
  },
  {
    "text": "same can be performed from the AWS CLI",
    "start": "316919",
    "end": "320040"
  },
  {
    "text": "and AWS sdks",
    "start": "320040",
    "end": "323960"
  },
  {
    "text": "here I am within the AWS Management",
    "start": "324360",
    "end": "326400"
  },
  {
    "text": "console and specifically within the",
    "start": "326400",
    "end": "328139"
  },
  {
    "text": "dynamodb Management console in my AWS",
    "start": "328139",
    "end": "331139"
  },
  {
    "text": "account so now I will click on imports",
    "start": "331139",
    "end": "333840"
  },
  {
    "text": "from S3 in this page I can see the",
    "start": "333840",
    "end": "336840"
  },
  {
    "text": "previously executed import job so one of",
    "start": "336840",
    "end": "339060"
  },
  {
    "text": "them completed already and one of them",
    "start": "339060",
    "end": "340979"
  },
  {
    "text": "is imported at the moment and I can",
    "start": "340979",
    "end": "343320"
  },
  {
    "text": "create a new input from S3 job so let's",
    "start": "343320",
    "end": "345419"
  },
  {
    "text": "do that now I will enter here the",
    "start": "345419",
    "end": "348479"
  },
  {
    "text": "location of my input objects so in this",
    "start": "348479",
    "end": "351180"
  },
  {
    "text": "case I'm entering the location of the",
    "start": "351180",
    "end": "353400"
  },
  {
    "text": "folder that contains all the objects",
    "start": "353400",
    "end": "356220"
  },
  {
    "text": "that I want to import I can choose",
    "start": "356220",
    "end": "358139"
  },
  {
    "text": "whether that location is located within",
    "start": "358139",
    "end": "361139"
  },
  {
    "text": "a bucket in my AWS account or a",
    "start": "361139",
    "end": "363780"
  },
  {
    "text": "different AWS account I will choose my",
    "start": "363780",
    "end": "366479"
  },
  {
    "text": "AWS account then I can choose what is",
    "start": "366479",
    "end": "369960"
  },
  {
    "text": "the compression used on the input file",
    "start": "369960",
    "end": "373259"
  },
  {
    "text": "so whether that's files that are not",
    "start": "373259",
    "end": "375419"
  },
  {
    "text": "compressed or compressed with any of the",
    "start": "375419",
    "end": "377880"
  },
  {
    "text": "supported compression algorithms I'm",
    "start": "377880",
    "end": "380160"
  },
  {
    "text": "choosing no compression because the",
    "start": "380160",
    "end": "382380"
  },
  {
    "text": "objects in this case are not compressed",
    "start": "382380",
    "end": "384360"
  },
  {
    "text": "and the object in this case are CSV so",
    "start": "384360",
    "end": "386880"
  },
  {
    "text": "I'm choosing CSV and now I can also",
    "start": "386880",
    "end": "389220"
  },
  {
    "text": "choose whether the input files contain",
    "start": "389220",
    "end": "391860"
  },
  {
    "text": "the first line as the header line or if",
    "start": "391860",
    "end": "396240"
  },
  {
    "text": "not then I should Define how the header",
    "start": "396240",
    "end": "398759"
  },
  {
    "text": "looks like how the columns in the files",
    "start": "398759",
    "end": "400620"
  },
  {
    "text": "look like so in my case the first line",
    "start": "400620",
    "end": "402960"
  },
  {
    "text": "is the header so I'm choosing that I can",
    "start": "402960",
    "end": "405479"
  },
  {
    "text": "also choose the separator in my case",
    "start": "405479",
    "end": "407220"
  },
  {
    "text": "it's the default one comma so I'm",
    "start": "407220",
    "end": "408960"
  },
  {
    "text": "clicking next",
    "start": "408960",
    "end": "410100"
  },
  {
    "text": "now I will create here a new table",
    "start": "410100",
    "end": "412319"
  },
  {
    "text": "called let's call it import demo",
    "start": "412319",
    "end": "416699"
  },
  {
    "text": "and I must choose the partition key for",
    "start": "416699",
    "end": "419639"
  },
  {
    "text": "my table so in this case the data set is",
    "start": "419639",
    "end": "422340"
  },
  {
    "text": "a data set that contains ratings of of",
    "start": "422340",
    "end": "424800"
  },
  {
    "text": "different movies so I will choose one of",
    "start": "424800",
    "end": "428160"
  },
  {
    "text": "the um one of the columns there called",
    "start": "428160",
    "end": "430860"
  },
  {
    "text": "user ID as my partition key and the type",
    "start": "430860",
    "end": "435840"
  },
  {
    "text": "will be string and the third key that I",
    "start": "435840",
    "end": "438300"
  },
  {
    "text": "will choose here is the rating a column",
    "start": "438300",
    "end": "441479"
  },
  {
    "text": "and here in this case it is a number",
    "start": "441479",
    "end": "444060"
  },
  {
    "text": "I'm clicking next",
    "start": "444060",
    "end": "446220"
  },
  {
    "text": "and here I can choose the settings for",
    "start": "446220",
    "end": "448919"
  },
  {
    "text": "my target table I will choose the",
    "start": "448919",
    "end": "451080"
  },
  {
    "text": "customize settings in order to be able",
    "start": "451080",
    "end": "453000"
  },
  {
    "text": "to control the settings a little bit",
    "start": "453000",
    "end": "454380"
  },
  {
    "text": "more and here I can choose between",
    "start": "454380",
    "end": "456060"
  },
  {
    "text": "provisioned and On Demand capacity mode",
    "start": "456060",
    "end": "458819"
  },
  {
    "text": "for the table for the sake of Simplicity",
    "start": "458819",
    "end": "460800"
  },
  {
    "text": "I'm choosing on demand capacity mode you",
    "start": "460800",
    "end": "464099"
  },
  {
    "text": "can see that I can also optionally",
    "start": "464099",
    "end": "465780"
  },
  {
    "text": "create Global secondary indexes as part",
    "start": "465780",
    "end": "468060"
  },
  {
    "text": "of the input process I will not be",
    "start": "468060",
    "end": "470400"
  },
  {
    "text": "creating indexes for this particular",
    "start": "470400",
    "end": "472199"
  },
  {
    "text": "demo I can continue with choosing the",
    "start": "472199",
    "end": "475259"
  },
  {
    "text": "encryption options for the table and",
    "start": "475259",
    "end": "478560"
  },
  {
    "text": "from here I will be clicking next",
    "start": "478560",
    "end": "482039"
  },
  {
    "text": "now here I can review all the chosen",
    "start": "482039",
    "end": "484440"
  },
  {
    "text": "options and I can",
    "start": "484440",
    "end": "487080"
  },
  {
    "text": "create the import job here so I'm",
    "start": "487080",
    "end": "488940"
  },
  {
    "text": "clicking on Import in order to create",
    "start": "488940",
    "end": "490500"
  },
  {
    "text": "the job and you can see that the new job",
    "start": "490500",
    "end": "492419"
  },
  {
    "text": "just started",
    "start": "492419",
    "end": "494699"
  },
  {
    "text": "now in order not to wait let's just",
    "start": "494699",
    "end": "496860"
  },
  {
    "text": "review the output of a previous job the",
    "start": "496860",
    "end": "500220"
  },
  {
    "text": "previously completed job so I'll click",
    "start": "500220",
    "end": "501840"
  },
  {
    "text": "on the previously completed job you can",
    "start": "501840",
    "end": "503879"
  },
  {
    "text": "see here the location in this case I",
    "start": "503879",
    "end": "506220"
  },
  {
    "text": "imported just one file not not a folder",
    "start": "506220",
    "end": "509039"
  },
  {
    "text": "but just one single object from S3 you",
    "start": "509039",
    "end": "512580"
  },
  {
    "text": "can see the name of the table that I",
    "start": "512580",
    "end": "514620"
  },
  {
    "text": "created back then you can see how long",
    "start": "514620",
    "end": "517080"
  },
  {
    "text": "this process took and how much data was",
    "start": "517080",
    "end": "519659"
  },
  {
    "text": "imported and so on so that's the type of",
    "start": "519659",
    "end": "522240"
  },
  {
    "text": "information I can expect to see about an",
    "start": "522240",
    "end": "524580"
  },
  {
    "text": "import job and also I have a link here",
    "start": "524580",
    "end": "526740"
  },
  {
    "text": "to cloudwatch in order to review the",
    "start": "526740",
    "end": "529260"
  },
  {
    "text": "logs of the particular input job so that",
    "start": "529260",
    "end": "533399"
  },
  {
    "text": "was the short demo about the process of",
    "start": "533399",
    "end": "535560"
  },
  {
    "text": "importing data from a S3 to dynamodb I",
    "start": "535560",
    "end": "538860"
  },
  {
    "text": "remind you that simulated to how I did",
    "start": "538860",
    "end": "540540"
  },
  {
    "text": "it here from the dynamodb Management",
    "start": "540540",
    "end": "542640"
  },
  {
    "text": "console I could do the same from the AWS",
    "start": "542640",
    "end": "545339"
  },
  {
    "text": "CLI on or from the different sdks",
    "start": "545339",
    "end": "550080"
  },
  {
    "start": "549000",
    "end": "564000"
  },
  {
    "text": "thank you for joining me for this video",
    "start": "550320",
    "end": "552000"
  },
  {
    "text": "about importing data from S3 to dynamodb",
    "start": "552000",
    "end": "554580"
  },
  {
    "text": "I hope that you found the video",
    "start": "554580",
    "end": "556140"
  },
  {
    "text": "beneficial and I invite you watching",
    "start": "556140",
    "end": "558060"
  },
  {
    "text": "other videos in the dynamodb Nuggets",
    "start": "558060",
    "end": "560399"
  },
  {
    "text": "series",
    "start": "560399",
    "end": "562640"
  }
]