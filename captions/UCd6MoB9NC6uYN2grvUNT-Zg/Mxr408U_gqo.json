[
  {
    "start": "0",
    "end": "95000"
  },
  {
    "text": "thanks everybody for uh for coming to this session uh best practices for using Apache spark on AWS uh my name is John",
    "start": "719",
    "end": "7680"
  },
  {
    "text": "Fritz I'm a senior product manager on for Amazon EMR uh service and AWS quickly deploy Spar kadoo other Apache",
    "start": "7680",
    "end": "14480"
  },
  {
    "text": "Big Data Frameworks quickly and efficiently um before we get started real quick show at hands how many folks here have used Apache spark",
    "start": "14480",
    "end": "21439"
  },
  {
    "text": "before okay so bunch of folks how many people still use Hado Mau or tez or any",
    "start": "21439",
    "end": "26560"
  },
  {
    "text": "other Frameworks a lot less um how many people run spark on AWS",
    "start": "26560",
    "end": "32599"
  },
  {
    "text": "currently um and then how many folks um use EMR to run spark today okay great so",
    "start": "32600",
    "end": "39280"
  },
  {
    "text": "some of this might be a refresher for some of you but hopefully we'll get into um some more granular details by the end we'll take something away um so quick",
    "start": "39280",
    "end": "46800"
  },
  {
    "text": "for agenda um an overall of just quickly why we see spark becoming very very popular for uh Big Data machine learning",
    "start": "46800",
    "end": "53160"
  },
  {
    "text": "applications uh a quick section just how to deploy spark with Amazon EMR we found that customers can utilize it to uh get",
    "start": "53160",
    "end": "59680"
  },
  {
    "text": "up running very very quickly in a performant way um a little bit about emrfs and just utilizing S3 as a data",
    "start": "59680",
    "end": "65680"
  },
  {
    "text": "store and using other AWS services in conjunction with spark when building your big data applications um we'll talk",
    "start": "65680",
    "end": "72439"
  },
  {
    "text": "a little bit about spark on yarn and and there's a bunch of different cluster managers you can use to manage resources for spark EMR we use yarn so we'll talk",
    "start": "72439",
    "end": "79600"
  },
  {
    "text": "a little bit about that um a little bit about data frames and some performance tips and then finally uh quick overview",
    "start": "79600",
    "end": "85600"
  },
  {
    "text": "on spark security and a few ways you might want to set up your cluster to run it securely",
    "start": "85600",
    "end": "91520"
  },
  {
    "text": "um so spark you know spark uh is becoming uh very very very popular the",
    "start": "91520",
    "end": "98040"
  },
  {
    "start": "95000",
    "end": "95000"
  },
  {
    "text": "codebase is moving quickly um and people have been shifting their applications from using things more like a dup map",
    "start": "98040",
    "end": "103920"
  },
  {
    "text": "ruce to using spark instead as an execution framework um and just overall",
    "start": "103920",
    "end": "109280"
  },
  {
    "text": "gaining popularity and there's a couple main reasons why and the first reason it's fast um it's massively parallel",
    "start": "109280",
    "end": "115960"
  },
  {
    "text": "like many of the other Frameworks out there but instead of say a traditional reduce framework that shoehorns things",
    "start": "115960",
    "end": "121600"
  },
  {
    "text": "into a map ruce uh sequential uh uh sequence it uses a dag instead it just",
    "start": "121600",
    "end": "127880"
  },
  {
    "text": "much more optimized in plotting an efficient query um it also minimizes IO so instead of writing uh to dis after",
    "start": "127880",
    "end": "134599"
  },
  {
    "text": "each stage it can store things in a data frame rdd um you know distributed in",
    "start": "134599",
    "end": "139760"
  },
  {
    "text": "memory cache um and that makes it fast because you can run sequential operations without incurring an IO cost",
    "start": "139760",
    "end": "145920"
  },
  {
    "text": "um and finally it's more partition aware than Hado map ruce as well so you don't eat as much uh shuffling time in network",
    "start": "145920",
    "end": "153319"
  },
  {
    "text": "bandwidth um but it's also just easy to use um you know instead of having to stand up Storm for streaming and you",
    "start": "153319",
    "end": "160360"
  },
  {
    "start": "155000",
    "end": "155000"
  },
  {
    "text": "know Hive for for SQL and and a bunch of different applications in the in the stack um spark really centralizes",
    "start": "160360",
    "end": "166200"
  },
  {
    "text": "everything around the spark core if you want to run SQL you've got a SQL interface spark SQL um you want to run a",
    "start": "166200",
    "end": "172280"
  },
  {
    "text": "streaming application you've got spark streaming you want on machine learning you have a variety of distributed machine learning algorithms ready to be",
    "start": "172280",
    "end": "178480"
  },
  {
    "text": "used and you can also build graph databases but all this is still around spark core you don't have to worry about",
    "start": "178480",
    "end": "183840"
  },
  {
    "text": "a bunch of different Frameworks and if you want to get in and tune it you don't need to be an expert in all that you can focus in in one",
    "start": "183840",
    "end": "190280"
  },
  {
    "text": "area um spark also has a variety of uh languages actually start the timer real",
    "start": "190280",
    "end": "196560"
  },
  {
    "start": "192000",
    "end": "192000"
  },
  {
    "text": "quick thanks um spark variety of language support is brought up there's a",
    "start": "196560",
    "end": "202400"
  },
  {
    "text": "SQL interface but if you want to write your jobs in schola you can you can write them in Python common for data",
    "start": "202400",
    "end": "208200"
  },
  {
    "text": "science use case um so about SQL spark R and support for that is moving pretty quickly and also Java and it's just much",
    "start": "208200",
    "end": "215159"
  },
  {
    "text": "easier to express things instead of writing you know pure map ruce code which is an example you know a couple hundred lines you can condense That Into",
    "start": "215159",
    "end": "221720"
  },
  {
    "text": "You Know tens of lines um in Scala so it's just much more efficient to to write",
    "start": "221720",
    "end": "227680"
  },
  {
    "text": "code um spark centers around abstractions uh data frames is the main",
    "start": "227680",
    "end": "233040"
  },
  {
    "start": "229000",
    "end": "229000"
  },
  {
    "text": "one now it used to be rdds and uh got some extra features data frames and there also data sets it's really just a",
    "start": "233040",
    "end": "238680"
  },
  {
    "text": "distributed collection of data organized in columns um they're very very easy to create then once you have your data set",
    "start": "238680",
    "end": "243760"
  },
  {
    "text": "up in this way um there's a rich API to interact with it transform it um and and gain value from it filtering aggregating",
    "start": "243760",
    "end": "251840"
  },
  {
    "text": "um those sorts of things um it's when you're moving some people have used sparked before um data frames is uh more",
    "start": "251840",
    "end": "259720"
  },
  {
    "text": "optimized than an rdd because it utilizes uh the Catalyst query planner rdds didn't do that in the past and then",
    "start": "259720",
    "end": "266800"
  },
  {
    "text": "um now there's also data sets which essentially has some type safety around data frames um also utilizes the",
    "start": "266800",
    "end": "273800"
  },
  {
    "start": "269000",
    "end": "269000"
  },
  {
    "text": "Catalyst query planner but you know it's an object-oriented uh uh API and there's",
    "start": "273800",
    "end": "280280"
  },
  {
    "text": "as I'll bring up uh later in spark 2 they're kind of converging in a way but with spark one six this API is now in uh",
    "start": "280280",
    "end": "288440"
  },
  {
    "text": "in preview mode so you can utilize it um so data frames uh are resilient",
    "start": "288440",
    "end": "296199"
  },
  {
    "text": "you know it's distributed across all the nodes and if one node goes down idea is that you'll be able to recompute it so",
    "start": "296199",
    "end": "302400"
  },
  {
    "text": "that your your inmemory data set is resilient to any failure this is just an example of how spark tracks the lineage",
    "start": "302400",
    "end": "307919"
  },
  {
    "text": "of a data set and then if you you lose a node node comes back and it'll recompute that that uh block based on the previous",
    "start": "307919",
    "end": "315039"
  },
  {
    "text": "um changes that you've operated on the the data frame or rdd um they're very very easy to create",
    "start": "315039",
    "end": "321560"
  },
  {
    "text": "actually I'll show a quick demo of of how it easy is to create from a parket file it basically just you know reflects",
    "start": "321560",
    "end": "326720"
  },
  {
    "text": "the schema in and you already have a data frame loaded that you can play with um but it's really rich support for Json",
    "start": "326720",
    "end": "332440"
  },
  {
    "text": "paret if you have tables in Hive you can easily create a data frame and pull that into memory from those tables or if you",
    "start": "332440",
    "end": "339199"
  },
  {
    "text": "just have a spark rdd you can easily create a data frame from that um but",
    "start": "339199",
    "end": "344639"
  },
  {
    "text": "there's lots of thirdparty packages that you can add that add support for creating data frames from a variety more",
    "start": "344639",
    "end": "350000"
  },
  {
    "start": "345000",
    "end": "345000"
  },
  {
    "text": "sources if you have data in Cassandra you can pull that data in data in elastic search you can pull in or load",
    "start": "350000",
    "end": "355720"
  },
  {
    "text": "from spark um we've seen a lot of movement on the Amazon red shift connector um as well you can download that load it",
    "start": "355720",
    "end": "362080"
  },
  {
    "text": "in your cluster and then basically pull data out of red shift via um kind of a staging area in S3 under the hood into",
    "start": "362080",
    "end": "368759"
  },
  {
    "text": "your cluster or if you're running an ETL process load the output into red",
    "start": "368759",
    "end": "374160"
  },
  {
    "text": "shift um data frames you know is the core concept once you have things in a data frame you can you can uh create",
    "start": "374160",
    "end": "380599"
  },
  {
    "start": "376000",
    "end": "376000"
  },
  {
    "text": "machine learning algorithms train them um you know extract features that sort of thing um spark has a variety of",
    "start": "380599",
    "end": "386360"
  },
  {
    "text": "distributed um machine learning libraries used to be called ml now it's just referred to as ml um and that also",
    "start": "386360",
    "end": "393280"
  },
  {
    "text": "is moving very quickly more algorithms are being written each algorithm is becoming better and better and more",
    "start": "393280",
    "end": "398680"
  },
  {
    "text": "performant um so there's a lot of uh activity there so it's a very easy way to stand up distributed machine learning",
    "start": "398680",
    "end": "404840"
  },
  {
    "text": "pipelines um easily also there's some new features that came out in spark six",
    "start": "404840",
    "end": "410080"
  },
  {
    "text": "and are even more standard in spark 2 around pipeline persistent meaning that it's very easy to create a model and",
    "start": "410080",
    "end": "416160"
  },
  {
    "text": "then create a pipeline around training and fitting it but then saving that to 3 and then loading it in really whatever",
    "start": "416160",
    "end": "421919"
  },
  {
    "text": "language you want that spark supports um so it just makes it easier when you're writing these things in the flow to be",
    "start": "421919",
    "end": "427840"
  },
  {
    "text": "able to to save them and load them in a in a perform way um data frames also are used in",
    "start": "427840",
    "end": "433879"
  },
  {
    "start": "433000",
    "end": "433000"
  },
  {
    "text": "streaming as well um and I'll go over this changing a little bit in spark too but you know you create basically uh",
    "start": "433879",
    "end": "441039"
  },
  {
    "text": "like micro batches so very small rdds that are you know short time window and it's it's a dstream it's what it's",
    "start": "441039",
    "end": "447080"
  },
  {
    "text": "called and um you know you're able to basically write uh your jobs on these",
    "start": "447080",
    "end": "452800"
  },
  {
    "text": "kind of incremental microbadge data sets um in a similar way that you could write it in badge so it's actually spark is",
    "start": "452800",
    "end": "459199"
  },
  {
    "text": "adopted a lot in Lambda architecture where you have a streaming layer and an at rest layer mainly because it's very",
    "start": "459199",
    "end": "464840"
  },
  {
    "text": "easy to write a job that can interact with both of those data sets if you think of like an incoming stream as an infinite table and then uh your at rest",
    "start": "464840",
    "end": "472319"
  },
  {
    "text": "data um that's static as uh you know a discrete table and being able to join the two so we find that as a is a common",
    "start": "472319",
    "end": "480000"
  },
  {
    "text": "use case but once again because everything is around the same core it's very easy to write um you know a",
    "start": "480000",
    "end": "485240"
  },
  {
    "text": "processing job that can use both a stream and static data sets um I brought",
    "start": "485240",
    "end": "490400"
  },
  {
    "text": "up spark R uh spark R is a relatively uh you know new uh uh API in uh",
    "start": "490400",
    "end": "499360"
  },
  {
    "text": "spark they're working on bringing more distributed algorithms for the machine learning uh libraries into spark R the",
    "start": "499360",
    "end": "507159"
  },
  {
    "text": "support isn't necessarily one to one yet but it's moving quickly and we've see a lot of promise with people data scientists who want to write R in a",
    "start": "507159",
    "end": "513240"
  },
  {
    "text": "distributed way using spark to achieve uh that scale um I brought up spark SQL you know",
    "start": "513240",
    "end": "519360"
  },
  {
    "text": "once you cache things in a data frame you can get very very low latency access to it letting itself really good to say",
    "start": "519360",
    "end": "525000"
  },
  {
    "text": "a bi query um or you know data science in that way and a lot of that work is still done with SQL um and you can also",
    "start": "525000",
    "end": "532680"
  },
  {
    "text": "you know store store your schema in a hive metastore which is also a ubiquitous way to store almost like as a data catalog um and you can connect via",
    "start": "532680",
    "end": "540279"
  },
  {
    "text": "jdbc and obbc with spark Thrift server um you can use you know a variety of interfaces but we're seeing that",
    "start": "540279",
    "end": "545760"
  },
  {
    "text": "becoming very popular um with SQL developers who want low latency access um to large data",
    "start": "545760",
    "end": "552920"
  },
  {
    "text": "sets um so spark 2 I brought that up a little bit right now uh the latest GA version is spark 162 but spark 2 is in",
    "start": "552920",
    "end": "559920"
  },
  {
    "text": "the voting phase there's been a few release candidates that come out hopefully uh the community will vote on it soon and it will be announced GA in",
    "start": "559920",
    "end": "566440"
  },
  {
    "text": "the coming weeks um EMR is quick on supporting the new spark version so you know after it's G i' expect to see it in",
    "start": "566440",
    "end": "572959"
  },
  {
    "text": "EMR uh soon after um a few changes in spark 2 and um you know if you want to",
    "start": "572959",
    "end": "579200"
  },
  {
    "start": "577000",
    "end": "577000"
  },
  {
    "text": "start playing with it you can pull down uh the release cidate and actually bootstrap it on an EMR cluster or just you know play with it locally um there's",
    "start": "579200",
    "end": "586560"
  },
  {
    "text": "a few interesting updates one is SQL support has gotten a lot better there's a new anql parser um data frames and",
    "start": "586560",
    "end": "593600"
  },
  {
    "text": "data sets have the data set portion has um moved moved a lot from 16 to two and",
    "start": "593600",
    "end": "600040"
  },
  {
    "text": "it's now being united essentially with data frames I think data frames is now going to be like a subset of data set so",
    "start": "600040",
    "end": "605480"
  },
  {
    "text": "instead of having these two separate apis they're now more merged and actually it's the same thing with the spark context and the hive context",
    "start": "605480",
    "end": "611640"
  },
  {
    "text": "before if you wanted to use the hiveql with spark you'd create a hive context which made it easier to say Port your",
    "start": "611640",
    "end": "617720"
  },
  {
    "text": "hive jobs over to spark now they're merged and you create just one and Spark will decide um what mode to use um",
    "start": "617720",
    "end": "625000"
  },
  {
    "text": "better support for spark R new algorithms are included um and the ml pipeline uh persistence which is really",
    "start": "625000",
    "end": "631519"
  },
  {
    "text": "integral and when you're developing pipelines being able to save and iterate on them um wasn't supported in all",
    "start": "631519",
    "end": "636680"
  },
  {
    "text": "language and now it's more ubiquitous in the spark ml uh feature Set uh spark 2 also is shown to be a",
    "start": "636680",
    "end": "643519"
  },
  {
    "text": "fair amount faster uh it's a second generation uh engine that's now uh creating a bunch of Uh custom code per",
    "start": "643519",
    "end": "650120"
  },
  {
    "text": "job um so a lot of low-level operations are running much more quickly also just the general query planner there's been",
    "start": "650120",
    "end": "655959"
  },
  {
    "text": "improvements there and there's a few other things like a vectorized parade decoder to improve throughput when",
    "start": "655959",
    "end": "661560"
  },
  {
    "text": "you're pushing things down and reading parts of uh the parket file format uh different",
    "start": "661560",
    "end": "667200"
  },
  {
    "text": "columns um and a final big change is kind of a revisit of spark streaming so",
    "start": "667200",
    "end": "672519"
  },
  {
    "text": "as I brought up before um you know you you use utilize the D stream and you wrote jobs that were kind of a little",
    "start": "672519",
    "end": "678519"
  },
  {
    "text": "bit separate from from your uh from your batch jobs although there there is close interplay and that's what makes it good",
    "start": "678519",
    "end": "684440"
  },
  {
    "text": "for building a Lambda architecture structured streaming is basically rethinking it on you know how can you make it almost opaque to a user um",
    "start": "684440",
    "end": "691360"
  },
  {
    "text": "whether they're writing a data uh processing job for batch or streaming and really build kind of hooks that the",
    "start": "691360",
    "end": "697399"
  },
  {
    "text": "same job really could be used for both if you think about streaming data as you know a table that's just a dynamic table",
    "start": "697399",
    "end": "702959"
  },
  {
    "text": "in nature versus a static one so there'll be a lot more coming there over the uh uh subsequent releases of the",
    "start": "702959",
    "end": "709279"
  },
  {
    "text": "spark 2 Branch um so a little bit overview on spark I'll go into a little bit on uh",
    "start": "709279",
    "end": "715440"
  },
  {
    "text": "creating spark clusters quickly with Amazon EMR we found in AWS is you know it's it's really great to prototype and",
    "start": "715440",
    "end": "721680"
  },
  {
    "text": "build applications and then move them into production and speeding that process up means you know quicker return",
    "start": "721680",
    "end": "727880"
  },
  {
    "text": "on data coming in being able to utilize your data better build uh Stronger businesses with more insight so speed is",
    "start": "727880",
    "end": "734000"
  },
  {
    "text": "important in EAS of use and also spending more time developing uh your applications uh instead of worried about",
    "start": "734000",
    "end": "740399"
  },
  {
    "start": "739000",
    "end": "739000"
  },
  {
    "text": "you know standing up spark managing spark clusters and that sort of thing um you know we've we've seen customers use",
    "start": "740399",
    "end": "746519"
  },
  {
    "text": "EMR and comment on several areas of of where they found value easy to install and configure spark um and that's one",
    "start": "746519",
    "end": "753240"
  },
  {
    "text": "thing if you want a spark cluster in you know around 5 minutes with a notebook interface you can click a few buttons",
    "start": "753240",
    "end": "758360"
  },
  {
    "text": "and you've got one and you can scale it out to to thousands of nodes um also it's easier to to remove capacity and",
    "start": "758360",
    "end": "764480"
  },
  {
    "text": "and add capacity depending on you know how many nodes you need running how much memory you need for a data frame how",
    "start": "764480",
    "end": "770519"
  },
  {
    "text": "many cores you need that sort of thing um there's a variety of different ways that you can uh interact with the",
    "start": "770519",
    "end": "776040"
  },
  {
    "text": "cluster and I'll go through a few when thinking about how to build these Pipelines you know do you want a long running cluster that's always up serving",
    "start": "776040",
    "end": "781959"
  },
  {
    "text": "requests do you want to leverage a transient architecture um you know there's a bunch of different options there um variety of different ways to",
    "start": "781959",
    "end": "789920"
  },
  {
    "text": "save costs using um you know in the end you're you're running on ec2 and you have the Avail uh the ability to use the",
    "start": "789920",
    "end": "795920"
  },
  {
    "text": "different pricing models that are for ec2 um secured we'll go over some of the security features at the end and then",
    "start": "795920",
    "end": "802040"
  },
  {
    "text": "finally just the general idea and sounds like a lot of people here are running spark on AWS so sounds like I'm",
    "start": "802040",
    "end": "807279"
  },
  {
    "text": "preaching a little bit to the choir but on using S3 and decoupling your storage layer from your compute layer um and",
    "start": "807279",
    "end": "813079"
  },
  {
    "text": "I'll talk about that a little bit more as well you can save costs um increase the durability of the data and not need",
    "start": "813079",
    "end": "818639"
  },
  {
    "text": "to over uh provision a cluster for hdfs instead you only provision the compute you need to uh run the processing that",
    "start": "818639",
    "end": "825240"
  },
  {
    "text": "you need to do um so create a EMR create a fully configured spark cluster uh in a",
    "start": "825240",
    "end": "831600"
  },
  {
    "start": "827000",
    "end": "827000"
  },
  {
    "text": "few minutes uh there's a excerpt from the console um we have a that's kind of the quick create easy page it's just one",
    "start": "831600",
    "end": "838079"
  },
  {
    "text": "thing a few options hit go and you have a spark cluster we have a more you know in-depth way of uh ux of of configuring",
    "start": "838079",
    "end": "844720"
  },
  {
    "text": "these things as well but if you don't want to deal with any of the custom stuff you can just get a cluster with spark um but everything is apid driven",
    "start": "844720",
    "end": "851480"
  },
  {
    "text": "so in the AWS CLI that's an example of a command that would give you uh eight node spark cluster it's really that easy",
    "start": "851480",
    "end": "858079"
  },
  {
    "text": "um or you can use the SDK and build your own programs that call and create and submit work to spark uh clusters running",
    "start": "858079",
    "end": "864240"
  },
  {
    "text": "in EMR as well um EMR supports a variety of different instance types and actually",
    "start": "864240",
    "end": "870160"
  },
  {
    "start": "868000",
    "end": "868000"
  },
  {
    "text": "that's uh you know one of the advantages of AWS is the the um many different instance types you can choose to really",
    "start": "870160",
    "end": "876639"
  },
  {
    "text": "tailor um the hardware to your workload so you know we see people running spark",
    "start": "876639",
    "end": "881839"
  },
  {
    "text": "on all these uh instance families we obviously see a lot of R3 because it's um the memory optimized instance type so",
    "start": "881839",
    "end": "888399"
  },
  {
    "text": "if you want to cach a large data frame that's a that's a very efficient way to do it but we do see some people just",
    "start": "888399",
    "end": "893920"
  },
  {
    "text": "running You Know M M3 M4 because they have a processing job they don't need to cach the entire data frame they don't",
    "start": "893920",
    "end": "899600"
  },
  {
    "text": "want to pay for it and they just want to use more General Hardware to go run it so um we see a lot of different um",
    "start": "899600",
    "end": "906480"
  },
  {
    "text": "instance types uh being used um it's worth mentioning here that also EMR integrates with ec2 spot um actually how",
    "start": "906480",
    "end": "913839"
  },
  {
    "text": "many people here are familiar with the ec2 spot Market uh bid pricing um that's great ec2 spot allows you to bid for",
    "start": "913839",
    "end": "922040"
  },
  {
    "text": "capacity and assuming that the the market price which is a dynamic price um",
    "start": "922040",
    "end": "927079"
  },
  {
    "text": "is lower than your bid price you'll get that uh hardware for the price that you bid and oftentimes you can save up to",
    "start": "927079",
    "end": "932759"
  },
  {
    "text": "90% on your underlying compute costs um one thing to note is that if the market",
    "start": "932759",
    "end": "938000"
  },
  {
    "text": "price goes above what you bid um the spark Market will reclaim that instance um from you but because spark um you",
    "start": "938000",
    "end": "945440"
  },
  {
    "text": "know it's a distributed system and can handle node failures if you have a mix of on demand instances where you pay uh",
    "start": "945440",
    "end": "951240"
  },
  {
    "text": "you know an hourly rate and no one's going to take it away um and spot you can often times save a lot of money but",
    "start": "951240",
    "end": "957360"
  },
  {
    "text": "still have enough availability to run whatever job you needed to or you just rerun the",
    "start": "957360",
    "end": "963000"
  },
  {
    "text": "job um a variety of options we see customers submitting uh spark jobs in",
    "start": "963000",
    "end": "968519"
  },
  {
    "start": "965000",
    "end": "965000"
  },
  {
    "text": "AWS um if you're running an EMR cluster um EMR actually has an API called the",
    "start": "968519",
    "end": "973720"
  },
  {
    "text": "EMR step API which is a sequential um basically ad job API you can call the",
    "start": "973720",
    "end": "980040"
  },
  {
    "text": "EMR API and say add step here's where my jar is an S3 with my application run it",
    "start": "980040",
    "end": "985720"
  },
  {
    "text": "and EMR uh service will grab the jar give give it to spark on the cluster spark will run it and you know then the",
    "start": "985720",
    "end": "992040"
  },
  {
    "text": "output uh you can say you know if this job uh succeeds then go to the next one there's a bunch of different um uh",
    "start": "992040",
    "end": "998360"
  },
  {
    "text": "settings you can put on what happens at the end of that job that's a very very easy way it's also if you're running a transient architecture by spinning up a",
    "start": "998360",
    "end": "1005079"
  },
  {
    "text": "cluster you know a couple minutes it's up you run the job and want to shut it down you can uh embed uh in your create",
    "start": "1005079",
    "end": "1011480"
  },
  {
    "text": "command a bunch of steps to basically create a process of create the cluster run all these jobs and then when it's",
    "start": "1011480",
    "end": "1016920"
  },
  {
    "text": "done shut everything down so that's one very common thing for just routine batch processing um another way that we've",
    "start": "1016920",
    "end": "1023600"
  },
  {
    "text": "seen customers uh launching spark jobs is using AWS Lambda um in the case of",
    "start": "1023600",
    "end": "1028880"
  },
  {
    "text": "EMR you can tell Lambda to submit a step to EMR or you can have Lambda actually",
    "start": "1028880",
    "end": "1033918"
  },
  {
    "text": "interact directly with the spark API on the cluster um using SSH um Lambda also",
    "start": "1033919",
    "end": "1040079"
  },
  {
    "text": "has uh like time based things as well so if you have a workflow you want to kick off at you know noon every day you can",
    "start": "1040079",
    "end": "1045880"
  },
  {
    "text": "tell Lambda hey do this job at noon and it'll go work so it's a very very um easy way to set up kind of easy uh",
    "start": "1045880",
    "end": "1053720"
  },
  {
    "text": "workflows um but if you have a more complex workflow um like you know run a spark job and then uh wait for another",
    "start": "1053720",
    "end": "1060200"
  },
  {
    "text": "job to finish and then you know join the two with another job um that you need to draw out um we you can use AWS data",
    "start": "1060200",
    "end": "1066880"
  },
  {
    "text": "pipeline which is another uh AWS service where you can draw complex workflows and have conditions on you know when things",
    "start": "1066880",
    "end": "1073280"
  },
  {
    "text": "upload to a bucket then kick this thing off so there's very very fine granularity on the different types of",
    "start": "1073280",
    "end": "1078360"
  },
  {
    "text": "workflow you can create um but we also see customers using a variety of other tools running um off of the spark",
    "start": "1078360",
    "end": "1084720"
  },
  {
    "text": "cluster and you know either provisioning spark clusters or interacting with others a few popular ones are airflow",
    "start": "1084720",
    "end": "1090200"
  },
  {
    "text": "which is airbnb's open- sourced uh workflow designer if if you haven't seen it I highly recommended it's a very",
    "start": "1090200",
    "end": "1095679"
  },
  {
    "text": "powerful tool um Luigi and there's a variety of other uh you know workflow",
    "start": "1095679",
    "end": "1100760"
  },
  {
    "text": "designers as well um that that uh we've seen people use um there's a few options",
    "start": "1100760",
    "end": "1106280"
  },
  {
    "text": "actually on the cluster as well though um and all the are are pretty easy to install in EMR with a couple of clicks",
    "start": "1106280",
    "end": "1112640"
  },
  {
    "text": "uh one is just using web uis uh we ship Zeppelin with EMR zeppin is an open- Source uh notebook tool and actually",
    "start": "1112640",
    "end": "1120080"
  },
  {
    "text": "I'll show it on the demo in a minute um but you know there's rich visualizations on results you can collaborate on it um",
    "start": "1120080",
    "end": "1126159"
  },
  {
    "text": "export notes load notes save it to S3 so it's a very versatile tool um but you can also install things like R Studio",
    "start": "1126159",
    "end": "1132640"
  },
  {
    "text": "that uh screenshot there is from the AWS Big Data blog about a post doing geospatial analysis with spark and Spark",
    "start": "1132640",
    "end": "1139720"
  },
  {
    "text": "r on EMR and they hooked up R Studio you can you have roote access over all the notes you can do whatever you want you",
    "start": "1139720",
    "end": "1144880"
  },
  {
    "text": "can install whatever uh components you'd like um if you choose you can also",
    "start": "1144880",
    "end": "1150080"
  },
  {
    "text": "install Uzi on EMR Uzi is a it's been around for a long time it's the Hadoop you know workflow orchestrator tool um",
    "start": "1150080",
    "end": "1156799"
  },
  {
    "text": "it does have support for spark um so if you have a complicated dag of spark jobs that you say want to run at a regular",
    "start": "1156799",
    "end": "1162440"
  },
  {
    "text": "rate you can Define that in Uzi either in an XML or using the Q interface um",
    "start": "1162440",
    "end": "1168280"
  },
  {
    "text": "and H yeah using the H interface and uh and run those as as you as need be I brought",
    "start": "1168280",
    "end": "1174080"
  },
  {
    "text": "up before you can connect to spark and submit jobs via obbc and jdbc using the spark Thrift server um in the case of",
    "start": "1174080",
    "end": "1180760"
  },
  {
    "text": "EMR we actually have the spark Thrift server on the master not of the cluster we just don't start it because it'll",
    "start": "1180760",
    "end": "1185840"
  },
  {
    "text": "automatically create a yarn application and for people who don't want to use the spark Thrift server they might not be",
    "start": "1185840",
    "end": "1191280"
  },
  {
    "text": "happy that there's a you know a yarn app being forced on them so it's quite easy to start you just run that command and it'll come up and then you connect um",
    "start": "1191280",
    "end": "1198360"
  },
  {
    "text": "via the driver that that you'd like um and there's a variety of other things too the community has really adopted spark",
    "start": "1198360",
    "end": "1203919"
  },
  {
    "text": "and has built um you know many different job submission uh I guess applications that you can use in conjunction with",
    "start": "1203919",
    "end": "1210200"
  },
  {
    "text": "spark um spark job server is a popular one we usually Open Source by ouala um",
    "start": "1210200",
    "end": "1216360"
  },
  {
    "text": "and that almost gives you a rest interface and also the ability to share um one of these cache data frames among",
    "start": "1216360",
    "end": "1223520"
  },
  {
    "text": "a variety of different users which which is useful if say you have a bunch of different teams that all want to access",
    "start": "1223520",
    "end": "1228679"
  },
  {
    "text": "the same shared data set that's an efficient way to uh to architect that um you can just use the spark shell um",
    "start": "1228679",
    "end": "1235320"
  },
  {
    "text": "there's another job server called Livy which um is used in often times conjunction with Hugh that's being",
    "start": "1235320",
    "end": "1240720"
  },
  {
    "text": "developed so there's a a variety of different ways um to to interact with spark and submit",
    "start": "1240720",
    "end": "1246159"
  },
  {
    "text": "work um so for monitor debugging um if if you choose to run spark on EMR uh you",
    "start": "1246159",
    "end": "1253520"
  },
  {
    "start": "1248000",
    "end": "1248000"
  },
  {
    "text": "can select to push the logs to S3 so if you run TR the idea is you know you might want to check your cluster down",
    "start": "1253520",
    "end": "1259200"
  },
  {
    "text": "after a job so you don't have to pay for it but we don't want to penalize you by say deleting all of your logs so we push",
    "start": "1259200",
    "end": "1264480"
  },
  {
    "text": "the logs to a bucket you specify an S3 so you can see what happened after your job is completed you can you know browse",
    "start": "1264480",
    "end": "1270640"
  },
  {
    "text": "them the Mr console um but we also have the spark UI available which is just the open source front end um and it's pretty",
    "start": "1270640",
    "end": "1277240"
  },
  {
    "text": "powerful it shows you uh visualized um uh you know graphs of the performance",
    "start": "1277240",
    "end": "1283240"
  },
  {
    "text": "but also when certain executors kick in shows you the logical query plan there's a lot of if you want to get into the",
    "start": "1283240",
    "end": "1288760"
  },
  {
    "text": "details of what happened to your spark job spark UI is typically a good place uh to start um we also have ganglia for",
    "start": "1288760",
    "end": "1294720"
  },
  {
    "text": "monitoring Cloud watch metrics as other tools as well if you decide that you you do want to go into the details and and",
    "start": "1294720",
    "end": "1300640"
  },
  {
    "text": "tune your cluster or debug things however if you don't um you know EMR configures everything off the bat so you",
    "start": "1300640",
    "end": "1306600"
  },
  {
    "text": "should be able to get some mileage out of spark without even having to touch anything um as I mentioned in the",
    "start": "1306600",
    "end": "1313840"
  },
  {
    "start": "1313000",
    "end": "1313000"
  },
  {
    "text": "beginning spark is being adopted all across the board for a variety of different use case here's some customers",
    "start": "1313840",
    "end": "1319080"
  },
  {
    "text": "who are running it on EMR today um Yelps doing machine learning and AD targeting ranging to gum gum doing Revenue",
    "start": "1319080",
    "end": "1326600"
  },
  {
    "text": "forecasting uh Crux for personalization uh crowd uh crowd strike which is",
    "start": "1326600",
    "end": "1331840"
  },
  {
    "text": "basically running through logs to find patterns to see if there's any um threats uh so variety of people just",
    "start": "1331840",
    "end": "1337480"
  },
  {
    "text": "leveraging spark for uh you know big data use cases um one one thing that we",
    "start": "1337480",
    "end": "1343600"
  },
  {
    "text": "see a lot of is kinesis to spark streaming so one popular use case for real-time recommendation ation systems",
    "start": "1343600",
    "end": "1349600"
  },
  {
    "text": "or just any real time analysis is um you know ingesting your data in through Amazon Kinesis which is a highly",
    "start": "1349600",
    "end": "1356159"
  },
  {
    "text": "available streaming data platform in AWS um and have spark uh spark streaming has",
    "start": "1356159",
    "end": "1361679"
  },
  {
    "text": "receivers that make it easy to connect to Kinesis and also um utilize checkpoint it's actually a Blog on the Adis Big Data blog that goes into detail",
    "start": "1361679",
    "end": "1368760"
  },
  {
    "text": "on standing up uh this sort of architecture with checkpointing which is highly recommended just to improve the",
    "start": "1368760",
    "end": "1374240"
  },
  {
    "text": "durability of your stream but know data comes into Kinesis and you run you know window function or some sort of um you",
    "start": "1374240",
    "end": "1380799"
  },
  {
    "text": "know realtime rollup function in spark and then you can load the extract into red shift you can write it out to S3 for",
    "start": "1380799",
    "end": "1386480"
  },
  {
    "text": "further Downstream processing you know once it's into AWS you can use the variety of different um processing",
    "start": "1386480",
    "end": "1391880"
  },
  {
    "text": "services available uh in your big data architecture um gum gum another example",
    "start": "1391880",
    "end": "1398480"
  },
  {
    "text": "um they're actually using spark for a variety of different um processes which shows Once Again The Versatile uh nature",
    "start": "1398480",
    "end": "1404240"
  },
  {
    "text": "of spark as a platform uh they're running a 247 cluster in EMR that just has a rdd for interactive dashboard",
    "start": "1404240",
    "end": "1411919"
  },
  {
    "text": "based on uh re you know Revenue forecasting and other uh business metrics and because it's cached in",
    "start": "1411919",
    "end": "1417640"
  },
  {
    "text": "memory it's always there you can get very very low latency to populate uh dashboard interactive speed um but they",
    "start": "1417640",
    "end": "1423840"
  },
  {
    "text": "also just have a bunch of clickstream logs coming in those logs enter S3 and then every so often they'll kick off a",
    "start": "1423840",
    "end": "1429600"
  },
  {
    "text": "spark cluster it'll be up in a couple of minutes they'll run an ETL job transform the data um and then load it into a red",
    "start": "1429600",
    "end": "1436120"
  },
  {
    "text": "shift table the Aggregates um you know for their data warehouse about customer Behavior then finally because they're",
    "start": "1436120",
    "end": "1443200"
  },
  {
    "text": "pushing all the data into S3 um they can run experiments understand whether it might be worth say standing up a new",
    "start": "1443200",
    "end": "1448919"
  },
  {
    "text": "production workflow what insights might it drive and they've armed their data scientists with the ability to just create a transient spark cluster start",
    "start": "1448919",
    "end": "1455279"
  },
  {
    "text": "playing around with the data and see what they can find but then shut it down when they're not using it so they don't have to be fully invested running that",
    "start": "1455279",
    "end": "1461200"
  },
  {
    "text": "cluster all the time they can pull it up when they want to run experiments and then shut it down um final example and this actually",
    "start": "1461200",
    "end": "1468600"
  },
  {
    "text": "just came up uh on the on the big data blog on Friday I thought it was pretty cool um amazon.com's personalization",
    "start": "1468600",
    "end": "1474840"
  },
  {
    "text": "team so our retail website um is using spark on EMR in conjunction with a new deep learning um library that we release",
    "start": "1474840",
    "end": "1482960"
  },
  {
    "text": "dsstn um and they're actually doing an interesting thing here where they're uh utilizing spark on EMR to run the",
    "start": "1482960",
    "end": "1488960"
  },
  {
    "text": "compute heavy pre-processing workloads but dsstn runs on gpus it's not a a",
    "start": "1488960",
    "end": "1494200"
  },
  {
    "text": "spark Library like MLB and they're actually uh running it in ECS and basically handing off um the shards from",
    "start": "1494200",
    "end": "1501600"
  },
  {
    "text": "different rdds to S3 and um the the uh ECS clusters picking those up and",
    "start": "1501600",
    "end": "1508240"
  },
  {
    "text": "they're running a bunch of these are two different workflows that they have because some things need to run and communicate other things are just very",
    "start": "1508240",
    "end": "1513559"
  },
  {
    "text": "independent and you just scale them out um but but running those um you know deep learning algorithms on the data and",
    "start": "1513559",
    "end": "1519720"
  },
  {
    "text": "then creating recommendations for the amazon.com website so very cool blog I highly encourage reading it um but here",
    "start": "1519720",
    "end": "1526600"
  },
  {
    "text": "once again shows just it's very easy to uh leverage spark for for data processing",
    "start": "1526600",
    "end": "1533398"
  },
  {
    "text": "workflows um let me quickly show you Zeppelin",
    "start": "1533440",
    "end": "1539600"
  },
  {
    "text": "um if we could cut to the demo",
    "start": "1540760",
    "end": "1544880"
  },
  {
    "text": "please cool um so",
    "start": "1546960",
    "end": "1552919"
  },
  {
    "text": "uh one second here",
    "start": "1552919",
    "end": "1557679"
  },
  {
    "text": "that clutter um so here's a view of the EMR console um and uh as you can see",
    "start": "1559200",
    "end": "1567880"
  },
  {
    "text": "here here's a cluster that I uh created um earlier this morning but it took about 5 minutes to um to to come up and",
    "start": "1567880",
    "end": "1577520"
  },
  {
    "text": "uh as you can see here it's running spark uh 161 um and it is uh EMR 471",
    "start": "1577520",
    "end": "1584360"
  },
  {
    "text": "this is the uh the latest version of EMR um so I created SSH tunnel uh to this",
    "start": "1584360",
    "end": "1589960"
  },
  {
    "text": "cluster to the master note where all the web uis are running and uh here is the Zeppelin notebook um we have the spark",
    "start": "1589960",
    "end": "1596000"
  },
  {
    "text": "UI and ganglia and then the Hadoop resource manager keep in mind this is running in yarn um and so I just want to",
    "start": "1596000",
    "end": "1603640"
  },
  {
    "text": "show you real quick just how easy it was you know a couple clicks um you can set it up uh literally by going to uh the",
    "start": "1603640",
    "end": "1610600"
  },
  {
    "text": "create cluster and you or I could clone this um if I wanted to create this cluster on the AWS console I could use",
    "start": "1610600",
    "end": "1617360"
  },
  {
    "text": "that um and then just hit run on the console um so Zeppelin here which is a",
    "start": "1617360",
    "end": "1624679"
  },
  {
    "text": "popular front-end notebook for spark um it just makes it very very easy to collaborate on spark jobs and just",
    "start": "1624679",
    "end": "1631120"
  },
  {
    "text": "iterate fast um in this case uh there's a data set that I put in S3 of all the",
    "start": "1631120",
    "end": "1637480"
  },
  {
    "text": "flight data for um domestic flights the last I think 20 years or so um and it's",
    "start": "1637480",
    "end": "1643960"
  },
  {
    "text": "in parket file format so cumar format um and you can see how easy it is this is",
    "start": "1643960",
    "end": "1650279"
  },
  {
    "text": "an S3 you know it's actually there's a bunch of demos on on the web you can even load this in your own cluster it's",
    "start": "1650279",
    "end": "1655840"
  },
  {
    "text": "a a public data set to use um and just show how easy it is to get in a data frame you just reference the folder",
    "start": "1655840",
    "end": "1662480"
  },
  {
    "text": "where all those parket files are um you uh then you know create a create a data",
    "start": "1662480",
    "end": "1668679"
  },
  {
    "text": "frame here and then register a table and Spark will infer the schema from it so I didn't have to enter in you know one of",
    "start": "1668679",
    "end": "1674279"
  },
  {
    "text": "these like massive Hive create table statements just three lines and now I have a data frame but then what I can do",
    "start": "1674279",
    "end": "1680799"
  },
  {
    "text": "um is I can cache it um here so now it's all in memory and",
    "start": "1680799",
    "end": "1685919"
  },
  {
    "text": "then run you know a variety of different uh queries on it Zeppelin has interpreters for the different languages",
    "start": "1685919",
    "end": "1691000"
  },
  {
    "text": "I chose to use SQL but you could use um you know python Scala you know whatever whatever language you feel more",
    "start": "1691000",
    "end": "1696840"
  },
  {
    "text": "comfortable with but you know you can write the notes here's what's running and in a couple of seconds um you'd get",
    "start": "1696840",
    "end": "1703000"
  },
  {
    "text": "the results um just for completeness I'll rerun these show that this is live",
    "start": "1703000",
    "end": "1708480"
  },
  {
    "text": "um Counting the rows looks like uh 100 million rows or so um here's a query",
    "start": "1708480",
    "end": "1715200"
  },
  {
    "text": "that finds the airport with the most canceled flights in the since 2000 so if you're flying through Chicago um you",
    "start": "1715200",
    "end": "1721480"
  },
  {
    "text": "might be staying there for a bit um the visualization here is pretty rich",
    "start": "1721480",
    "end": "1728000"
  },
  {
    "text": "um there's a bunch of different graphs uh you know interactive and we found that just it's good to collaborate you",
    "start": "1728000",
    "end": "1733240"
  },
  {
    "text": "can export this notebook um and save it uh and then give it to somebody else and",
    "start": "1733240",
    "end": "1738559"
  },
  {
    "text": "they can iterate on your queries and and send it back to you uh quick look at the spark uis this",
    "start": "1738559",
    "end": "1744440"
  },
  {
    "text": "is also running on the master not EMR cluster on on this port um and as you can see here you can see all the jobs",
    "start": "1744440",
    "end": "1750640"
  },
  {
    "text": "that have been run you can take a closer look um at a specific job you can visualize the dag and you know a lot of",
    "start": "1750640",
    "end": "1757960"
  },
  {
    "text": "this stuff you might not need to go look into but if say you want to really tune a query or you're hitting some sort of",
    "start": "1757960",
    "end": "1763200"
  },
  {
    "text": "performance bottleneck is a good place to start um you can also get information",
    "start": "1763200",
    "end": "1768840"
  },
  {
    "text": "on what rdds you have or what data frames you have stored in this case I've cached around 40 gigs in memory across",
    "start": "1768840",
    "end": "1774559"
  },
  {
    "text": "my executors um you can see what executors you have active and you can",
    "start": "1774559",
    "end": "1779600"
  },
  {
    "text": "see um the uh uh query uh plans uh for SQL",
    "start": "1779600",
    "end": "1786640"
  },
  {
    "text": "queries that you might run here so lots of details quick access to logs as well",
    "start": "1786640",
    "end": "1791880"
  },
  {
    "text": "um you can use it or not depending on um how deep you want to get in to spark um",
    "start": "1791880",
    "end": "1798039"
  },
  {
    "text": "in Jun your jobs here you can see ganglia um which I've also installed it's very useful for monitoring you can",
    "start": "1798039",
    "end": "1803679"
  },
  {
    "text": "see here um the aggregate uh cluster memory um this is how much I've been using cache the data",
    "start": "1803679",
    "end": "1809960"
  },
  {
    "text": "frame um you can see here networks whenever I've run a job and things start chatting that registers and you can also",
    "start": "1809960",
    "end": "1816039"
  },
  {
    "text": "see here that's at an aggregate cluster level you can drill down into node level metrics as well so you can see here",
    "start": "1816039",
    "end": "1822159"
  },
  {
    "text": "these are each of the executors and they've been caching more memory and then the the master is not utilizing as",
    "start": "1822159",
    "end": "1827480"
  },
  {
    "text": "much memory because it's only running the driver and then um the Hadoop uh",
    "start": "1827480",
    "end": "1832960"
  },
  {
    "text": "resource manager UI one one question is is because we're running it in yarn people wonder where where is the spark",
    "start": "1832960",
    "end": "1838399"
  },
  {
    "text": "UI because it's not in spark Standalone where the spark UI is in the master and I'll go into a little bit of the yarn",
    "start": "1838399",
    "end": "1844279"
  },
  {
    "text": "architecture in a minute um but it's actually in the application master so if you're looking for the spark UI um you",
    "start": "1844279",
    "end": "1851200"
  },
  {
    "text": "go here and it'll come up and it's it's running the application Master process",
    "start": "1851200",
    "end": "1856639"
  },
  {
    "text": "which is running on a on a you know random node manager uh in the cluster all right can we go back to the",
    "start": "1856639",
    "end": "1862720"
  },
  {
    "text": "sides are",
    "start": "1862720",
    "end": "1865440"
  },
  {
    "text": "these thanks okay so talk a little bit about",
    "start": "1870679",
    "end": "1877360"
  },
  {
    "text": "uh utilizing Amazon S3 as a persistent uh store for spark um and there's a",
    "start": "1877360",
    "end": "1882720"
  },
  {
    "text": "couple main reasons why you might want to consider using S3 when you're when you're running spark and AWS first is",
    "start": "1882720",
    "end": "1887960"
  },
  {
    "start": "1883000",
    "end": "1883000"
  },
  {
    "text": "it's designed for uh 119 of durability um it's very durable it's very low cost",
    "start": "1887960",
    "end": "1894080"
  },
  {
    "text": "and also very very scalable um we have customers running multi petabyte data warehouses and S3 can manage that scale",
    "start": "1894080",
    "end": "1901000"
  },
  {
    "text": "um and you don't have to manage it versus something like hdfs where you have to keep adding data nodes for more",
    "start": "1901000",
    "end": "1907039"
  },
  {
    "text": "storage you have to manage uh your name nodes and you know you have to then if you have a hdfs cluster in an a you need",
    "start": "1907039",
    "end": "1914080"
  },
  {
    "text": "to back that up to probably another a or S3 anyway but another important thing is",
    "start": "1914080",
    "end": "1919760"
  },
  {
    "text": "that it changes the cost dynamics of running spark so you know before you need an always on cluster because you",
    "start": "1919760",
    "end": "1925559"
  },
  {
    "text": "needed um hdfs always there um because that's you know your data store and so your cluster",
    "start": "1925559",
    "end": "1931880"
  },
  {
    "text": "is up and a lot of times you have very underutilized compute um or memory because your your cluster is up mainly",
    "start": "1931880",
    "end": "1937639"
  },
  {
    "text": "for the storage capacity also you might have provision more nodes than you need because you need you know extra data",
    "start": "1937639",
    "end": "1943559"
  },
  {
    "text": "nodes to performant run hdfs even though you might not need that many cores or aggregate memory across your cluster um",
    "start": "1943559",
    "end": "1950799"
  },
  {
    "text": "so when using S3 you've decoupled the two meaning that you can have a cluster up or not it doesn't you know when",
    "start": "1950799",
    "end": "1957000"
  },
  {
    "text": "you're not processing anything and you you know you're not waiting for an incoming request why do you need a cluster of and why do you need to pay",
    "start": "1957000",
    "end": "1963039"
  },
  {
    "text": "for it so it allows you to really separate the two size your cluster for the the workloads you need or even run",
    "start": "1963039",
    "end": "1969159"
  },
  {
    "text": "different hardware for different workloads if there's a workload they'd be happy running on a a general optimized instance you can use something",
    "start": "1969159",
    "end": "1975600"
  },
  {
    "text": "like M3 if you had a workload that requires a lot of memory for large rdd you could use R3 um and really customize",
    "start": "1975600",
    "end": "1983159"
  },
  {
    "text": "the cluster for the workload if you want to get granular um there's a bunch of different",
    "start": "1983159",
    "end": "1988799"
  },
  {
    "text": "S3 connectors um EMR we utilize a proprietary one emrfs um but you know all the S3",
    "start": "1988799",
    "end": "1995559"
  },
  {
    "start": "1989000",
    "end": "1989000"
  },
  {
    "text": "connectors implement the Hado file system interface um we've done a lot of testing I mean you know the vast vast",
    "start": "1995559",
    "end": "2001440"
  },
  {
    "text": "majority of our customers are using S3 in some capacity um so you know a lot of just incremental um ch changes over time",
    "start": "2001440",
    "end": "2009279"
  },
  {
    "text": "error handling improve performance um it's transparent to Applications all you do is specify the S3 scheme and instead",
    "start": "2009279",
    "end": "2015039"
  },
  {
    "text": "of reading and writing to hdfs you're you know reading from S3 directly into memory you're writing out directly to S3",
    "start": "2015039",
    "end": "2020799"
  },
  {
    "text": "you're not actually copying any data back and forth before processing um we",
    "start": "2020799",
    "end": "2025880"
  },
  {
    "text": "support a bunch of the S3 encryption features we'll talk a little bit more about security at the end um but if you're using server side or client side",
    "start": "2025880",
    "end": "2031880"
  },
  {
    "text": "encryption once again it's transparent to spark it's pushed down into this uh connector level um and then also have",
    "start": "2031880",
    "end": "2038039"
  },
  {
    "text": "some uh list consistency features and fast listing as well um a few tips for S3 and this is",
    "start": "2038039",
    "end": "2045080"
  },
  {
    "start": "2043000",
    "end": "2043000"
  },
  {
    "text": "just general S3 tips whether you're using uh EMR or not um to to get the best performance uh one thing you want",
    "start": "2045080",
    "end": "2051520"
  },
  {
    "text": "to do is make sure that you have optimized your list performance and um",
    "start": "2051520",
    "end": "2056679"
  },
  {
    "text": "and you also you want to optimize throughput um in the case of like time series day a lot of times you'll organize things Say by day and you'll",
    "start": "2056679",
    "end": "2063398"
  },
  {
    "text": "typically process say the last day so most of the data that you're actually accessing is under the same S3 prefix in",
    "start": "2063399",
    "end": "2070158"
  },
  {
    "text": "the bucket um but under the hood that actually puts it on some of the same uh keymap servers which would um you know",
    "start": "2070159",
    "end": "2077960"
  },
  {
    "text": "if you're hammering it hammering it for a list you know it might degrade your list performance so what we've seen customers who are running a scale do is",
    "start": "2077960",
    "end": "2084240"
  },
  {
    "text": "Hash the prefixes so that even though from like a logical standpoint you're quering the data from you know the last",
    "start": "2084240",
    "end": "2089560"
  },
  {
    "text": "month each day because of the prefix is is almost random is being pushed across a variety of different servers so you",
    "start": "2089560",
    "end": "2095480"
  },
  {
    "text": "get a better list performance and better get throughput also us utilizing colum",
    "start": "2095480",
    "end": "2101240"
  },
  {
    "text": "or formats is really good as well because you're just pushing less data over the wire if you say your have your",
    "start": "2101240",
    "end": "2106599"
  },
  {
    "text": "data set in paret and you're filtering by certain columns you're only taking that column and bringing it back versus",
    "start": "2106599",
    "end": "2113680"
  },
  {
    "text": "um bringing everything back so choosing a column or file format if your queries are only selecting say some subset of",
    "start": "2113680",
    "end": "2119880"
  },
  {
    "text": "columns is also recommended for that reason as well um you know another tip is if you",
    "start": "2119880",
    "end": "2125839"
  },
  {
    "start": "2125000",
    "end": "2125000"
  },
  {
    "text": "have your data in uh three meaning that you know basically stateless outside of",
    "start": "2125839",
    "end": "2130880"
  },
  {
    "text": "any one cluster you can also store all of your tables if you want to store tables say in Hive and and create data",
    "start": "2130880",
    "end": "2137240"
  },
  {
    "text": "frames from those out of the cluster as well when you create a EMR cluster if you create a spark cluster um The Meta",
    "start": "2137240",
    "end": "2143200"
  },
  {
    "text": "store is going to be running you know locally to the cluster but you can change that setting and actually run the metastore database in uh something like",
    "start": "2143200",
    "end": "2150440"
  },
  {
    "text": "Amazon RDS which when you shut the cluster down your tables are still there you can bring the cluster back up it'll",
    "start": "2150440",
    "end": "2155680"
  },
  {
    "text": "connect to that remote metastore um and you'll have your tables back so thinking about that you know making sure that",
    "start": "2155680",
    "end": "2161520"
  },
  {
    "text": "each each component if it doesn't need to be stateless you don't need to store anything in or to State full you don't need to store anything in it um Spar can",
    "start": "2161520",
    "end": "2169960"
  },
  {
    "text": "work pretty well with a variety of different um ads data stores uh in EMR we have a Dynamo DB connector which",
    "start": "2169960",
    "end": "2176040"
  },
  {
    "start": "2170000",
    "end": "2170000"
  },
  {
    "text": "works with spark SQL and with Hive so if you want to uh read or write to Dynamo DB Amazon dynb is a highly available uh",
    "start": "2176040",
    "end": "2183800"
  },
  {
    "text": "nosql data store in AWS um you can use that connector to to integrate those two",
    "start": "2183800",
    "end": "2190920"
  },
  {
    "text": "um you can connect to Amazon RDS using the jdb jdbc data source of spark SQL or you could actually use scoop with the",
    "start": "2190920",
    "end": "2197040"
  },
  {
    "text": "dup mauce as well depending on um you know what what uh what you're using for",
    "start": "2197040",
    "end": "2202400"
  },
  {
    "text": "that application uh there's an elastic search connector with spark there's a spark red shift connector which um you",
    "start": "2202400",
    "end": "2208119"
  },
  {
    "text": "can download and and pull in um there's a bunch of different streaming uh connectors as well we see a lot of",
    "start": "2208119",
    "end": "2213800"
  },
  {
    "text": "customers uh ingesting into Amazon Kinesis and Spark stream but you can also use something like Apache Kafka um",
    "start": "2213800",
    "end": "2220680"
  },
  {
    "text": "and then finally um also using S3 as well which I brought up before is is a very common way for customers to store",
    "start": "2220680",
    "end": "2226760"
  },
  {
    "text": "large data sets and then process them with spark um okay next we'll go over a few",
    "start": "2226760",
    "end": "2233079"
  },
  {
    "text": "slides on just spark architecture and how you run it in EMR um EMR we run spark on yarn um there a a few reasons",
    "start": "2233079",
    "end": "2241079"
  },
  {
    "text": "um but one of the main ones is that we also run the rest of the Hadoop stack so a lot of times customers might want to",
    "start": "2241079",
    "end": "2246280"
  },
  {
    "text": "run Hive in M producer Hive and TZ and also spark and yarn does a very good job of managing all these different",
    "start": "2246280",
    "end": "2251839"
  },
  {
    "text": "applications together um when you run a spark application in yarn um you know as",
    "start": "2251839",
    "end": "2259160"
  },
  {
    "text": "as you see here the spark driver um you either run it in client or cluster mode the driver is basically um you know the",
    "start": "2259160",
    "end": "2265720"
  },
  {
    "text": "application that then will go and uh request resources and run runs the uh the app versus the executors which",
    "start": "2265720",
    "end": "2272240"
  },
  {
    "text": "actually do the execution of of what the driver is coordinating when you run the driver in client mode it runs uh an EMR",
    "start": "2272240",
    "end": "2279480"
  },
  {
    "text": "on the master node and is a yarn client so it'll go request resources from yarn",
    "start": "2279480",
    "end": "2284599"
  },
  {
    "text": "yarn will spin up an application Master but the driver is still running on you know wherever you started that process",
    "start": "2284599",
    "end": "2290119"
  },
  {
    "text": "if you run uh the spark application in cluster mode the driver actually becomes embedded in the application M the yarn",
    "start": "2290119",
    "end": "2296880"
  },
  {
    "text": "application Master which is then going to run somewhere in the cluster and not on the actual node where you you said",
    "start": "2296880",
    "end": "2302960"
  },
  {
    "text": "use spark submit to submit the job um uh Spark executors Run in yarn",
    "start": "2302960",
    "end": "2309400"
  },
  {
    "text": "containers on node managers so the executors are the workhorses of the job um and those are basically run on all",
    "start": "2309400",
    "end": "2315760"
  },
  {
    "text": "the slave nodes and you know there's we'll go over in a minute on ways to tune them and also just some of the",
    "start": "2315760",
    "end": "2321240"
  },
  {
    "text": "default settings that we use in EMR to to run those um there's a few good features on",
    "start": "2321240",
    "end": "2326920"
  },
  {
    "text": "yarn that there also reasons Beyond just the fact that it's ubiquitous in the Hadoop World um besides you know",
    "start": "2326920",
    "end": "2333160"
  },
  {
    "text": "dynamically sharing across different applications you might want to run um there's a bunch of different schedulers you can use as a capacity scheduler Fair",
    "start": "2333160",
    "end": "2340560"
  },
  {
    "text": "scheduler um you can allow yarn to do more Dynamic resource management of the",
    "start": "2340560",
    "end": "2345640"
  },
  {
    "text": "actual spark job using Dynamic allocation which I believe is being rolled out to some of the other um resource managers as well um but it was",
    "start": "2345640",
    "end": "2352640"
  },
  {
    "text": "originally started with yarn and finally you can uh cize yarn for authentication and I'm not sure if uh I don't think",
    "start": "2352640",
    "end": "2359960"
  },
  {
    "text": "spark Standalone supports that yet and maos might support it soon or might",
    "start": "2359960",
    "end": "2365119"
  },
  {
    "text": "already by default in EMR we use the capacity scheduler and we have one queue so you submit a job we run it but if say",
    "start": "2365119",
    "end": "2372480"
  },
  {
    "start": "2367000",
    "end": "2367000"
  },
  {
    "text": "you had a organization with a variety of different people who uh need to run spark jobs you want to make sure there's",
    "start": "2372480",
    "end": "2377960"
  },
  {
    "text": "going to be capacity for them um you can set up a bunch of different cues and have uh folks tag the job they want with",
    "start": "2377960",
    "end": "2385240"
  },
  {
    "text": "the queue that it gets run in to make sure that um uh capacity is guaranteed",
    "start": "2385240",
    "end": "2391040"
  },
  {
    "text": "for that job to run um and these are all configurable via yarn config but by",
    "start": "2391040",
    "end": "2397560"
  },
  {
    "text": "default um we use the capacity schedule there's also the fair scheduler as well which will divvy up resources um I",
    "start": "2397560",
    "end": "2404400"
  },
  {
    "start": "2401000",
    "end": "2401000"
  },
  {
    "text": "believe just fairly among all the jobs that request request those resources um so we talked a little bit",
    "start": "2404400",
    "end": "2411079"
  },
  {
    "text": "about uh the different schedulers yarn uses now when it creates a job you yarn",
    "start": "2411079",
    "end": "2416760"
  },
  {
    "text": "will then run spark will request a bunch of executors to actually run the job so what is an Executor um these are the",
    "start": "2416760",
    "end": "2422200"
  },
  {
    "text": "processes that are run on all the slave nodes that store the data and also run the tasks for your spark job",
    "start": "2422200",
    "end": "2428119"
  },
  {
    "text": "um they're specific to a spark application so when uh you know yarn",
    "start": "2428119",
    "end": "2433400"
  },
  {
    "text": "spins up the uh yarn app for spark um and you then create another spark job it will spin up a new set of executors if",
    "start": "2433400",
    "end": "2440040"
  },
  {
    "text": "you want to you know execute on the same set um that's where something like Zeppelin something that's already you're",
    "start": "2440040",
    "end": "2446040"
  },
  {
    "text": "basically submitting it through the same Spark app that's already running um and their executors are run in node managers",
    "start": "2446040",
    "end": "2452480"
  },
  {
    "text": "yarn containers um so in Emar we set all these uh by default for you depending on",
    "start": "2452480",
    "end": "2457760"
  },
  {
    "start": "2454000",
    "end": "2454000"
  },
  {
    "text": "the instance size how much RAM and cores are on that node um but it's useful quickly to go through on just some of",
    "start": "2457760",
    "end": "2463000"
  },
  {
    "text": "the different settings and what an executor's built up so um you know at the base yarn node manager has amount of",
    "start": "2463000",
    "end": "2470319"
  },
  {
    "text": "memory it's it's set by the setting and yarn site um and that's the uh total amount of memory that node manager can",
    "start": "2470319",
    "end": "2477240"
  },
  {
    "text": "use when when creating yarn containers um the executor containers are created",
    "start": "2477240",
    "end": "2484000"
  },
  {
    "text": "um in node manager or managed by node manager and each executor container has",
    "start": "2484000",
    "end": "2489599"
  },
  {
    "text": "some memory overhead um for yarn roughly about 10% of the size um and then the",
    "start": "2489599",
    "end": "2495079"
  },
  {
    "text": "actual memory that's uh being used to execute jobs cache rdds and that's set",
    "start": "2495079",
    "end": "2500480"
  },
  {
    "text": "by a value spark executor memory in spark default",
    "start": "2500480",
    "end": "2506079"
  },
  {
    "text": "um the spark memory fraction inside actually there's some overhead in the spark executor memory the this spark",
    "start": "2506079",
    "end": "2512079"
  },
  {
    "text": "memory fraction um dynamically manages um execution but also the cash for an",
    "start": "2512079",
    "end": "2518319"
  },
  {
    "text": "rdd um and before you'd actually have to set both um but now spark will just manage it depending on how much you're",
    "start": "2518319",
    "end": "2524560"
  },
  {
    "text": "asking to cash versus how much it needs um to execute um so you can set all this",
    "start": "2524560",
    "end": "2531079"
  },
  {
    "start": "2530000",
    "end": "2530000"
  },
  {
    "text": "statically and a job will go and it'll create the number of executors you ask for with the amount of RAM and the amount of cores um but EMR actually",
    "start": "2531079",
    "end": "2537400"
  },
  {
    "text": "around EMR 45 by default we use a feature for spark on yarn called Dynamic allocation where yarn will decide how",
    "start": "2537400",
    "end": "2544000"
  },
  {
    "text": "many executors um it needs uh throughout throughout the duration of the job and",
    "start": "2544000",
    "end": "2549119"
  },
  {
    "text": "then you know scale up and scale down depending on on the resources needed um",
    "start": "2549119",
    "end": "2555319"
  },
  {
    "text": "we the way we do it is we actually look at your node type figure out um you know what's what is the you know way to",
    "start": "2555319",
    "end": "2563280"
  },
  {
    "text": "utilize most resources on that node and then we set those parameters for the number of cores um and the amount of RAM",
    "start": "2563280",
    "end": "2568800"
  },
  {
    "text": "for each executor but these are all over you know you can override whatever you want and in some cases it might be",
    "start": "2568800",
    "end": "2573839"
  },
  {
    "text": "better to have you know a few large executors because the ount of data you want to cash can actually be distributed",
    "start": "2573839",
    "end": "2579319"
  },
  {
    "text": "among among many of these one other thing to note is that a spark job as of right now can only have one executor",
    "start": "2579319",
    "end": "2585559"
  },
  {
    "text": "size throughout the jobs you can't have heterogeneous executors in a job there's a Jura open for that um and you",
    "start": "2585559",
    "end": "2591720"
  },
  {
    "text": "hopefully soon you'll be able to have you know one executor with you know 10 gigs and one with 20 gigs in the same",
    "start": "2591720",
    "end": "2597520"
  },
  {
    "text": "job but today all the executors are homogeneous um here's a few properties",
    "start": "2597520",
    "end": "2604359"
  },
  {
    "start": "2604000",
    "end": "2604000"
  },
  {
    "text": "um you know you can you can tell spark dynamically allocate resources but you know I need at least 10 executors for",
    "start": "2604359",
    "end": "2610200"
  },
  {
    "text": "every job whether you think I do or not sometimes spark will or yarn might under provision or you might say look I I want",
    "start": "2610200",
    "end": "2616720"
  },
  {
    "text": "you to over-provision for a reason because I know the nature of this job and you know you're going to need those executors so there's a variety of of",
    "start": "2616720",
    "end": "2623280"
  },
  {
    "text": "different settings there's also settings for how long you might cash a data frame as well um you can say I want to cash the data frame for any data frame for 20",
    "start": "2623280",
    "end": "2630240"
  },
  {
    "text": "minutes but then after that you know release the memory and and somebody else can use it uh spark defaults a pretty easy to",
    "start": "2630240",
    "end": "2638079"
  },
  {
    "start": "2636000",
    "end": "2636000"
  },
  {
    "text": "override um you uh can do it you know with EMR you have what's called a configuration object and configuration",
    "start": "2638079",
    "end": "2645520"
  },
  {
    "text": "object allows you to uh specify a bunch of overrides for any of the key values in any of the settings um and then when",
    "start": "2645520",
    "end": "2652040"
  },
  {
    "text": "we create the cluster we will uh change all those settings to what you wanted it to and if you add in new nodes will you",
    "start": "2652040",
    "end": "2657720"
  },
  {
    "text": "still utilize the settings that you wanted to change in this case here's an example of overriding um executor memory",
    "start": "2657720",
    "end": "2664160"
  },
  {
    "text": "and executor cores to 15 gigs of RAM and uh four cores um and that's when you're",
    "start": "2664160",
    "end": "2670079"
  },
  {
    "text": "creating the cluster you can override this but you can actually override it at runtime as well when you submit a job to",
    "start": "2670079",
    "end": "2675319"
  },
  {
    "text": "spark there's a bunch of flags that you can use um to say you know even though",
    "start": "2675319",
    "end": "2680480"
  },
  {
    "text": "the default is 15 gigs of RAM now I want it to run with 10 gigs and you submit that when you also submit the job so all",
    "start": "2680480",
    "end": "2687760"
  },
  {
    "text": "the settings are pretty fungible and you can quickly iterate and figure out if you do want to tune you know what makes",
    "start": "2687760",
    "end": "2692800"
  },
  {
    "text": "the most sense for your job um and I said there's a few reasons you might want to change things um also if you",
    "start": "2692800",
    "end": "2699280"
  },
  {
    "start": "2696000",
    "end": "2696000"
  },
  {
    "text": "knew that you were running one like long running single tenant cluster you might just want to have one large executor on",
    "start": "2699280",
    "end": "2705119"
  },
  {
    "text": "each and tell the spark application to use up you know earmark all the resources for my spark app and um you",
    "start": "2705119",
    "end": "2711599"
  },
  {
    "text": "know no one else going to submit any work so you're not going to need to like save any for somebody else who's submitting a new job so there's a bunch",
    "start": "2711599",
    "end": "2718680"
  },
  {
    "text": "of different reasons um why you can tune although I said you don't have to um if",
    "start": "2718680",
    "end": "2723920"
  },
  {
    "start": "2721000",
    "end": "2721000"
  },
  {
    "text": "you did want to have one large single tenant app um we have a setting maximized resource allocation um and",
    "start": "2723920",
    "end": "2730359"
  },
  {
    "text": "that basically under the hood does a bunch of math and comes up with what it you know the executor settings if you",
    "start": "2730359",
    "end": "2735839"
  },
  {
    "text": "wanted to run one executor on each instance you know about as large as it can get um and then we will you know",
    "start": "2735839",
    "end": "2742319"
  },
  {
    "text": "when you submit a spark app it the the number of executors set to the number of nodes you have and it basically will",
    "start": "2742319",
    "end": "2747480"
  },
  {
    "text": "take up all the resources for that one single tenant application so say you're running a spark streaming job and you know that you're not going to be",
    "start": "2747480",
    "end": "2753240"
  },
  {
    "text": "submitting any other jobs that job's been running for a long time um you can U this to easily just have it configured",
    "start": "2753240",
    "end": "2759319"
  },
  {
    "text": "to create one large Spark app on your cluster um a few tips for data frames um",
    "start": "2759319",
    "end": "2768200"
  },
  {
    "start": "2766000",
    "end": "2766000"
  },
  {
    "text": "you know same thing with S3 before the less data you scan means the less data that you're sending over the wire which",
    "start": "2768200",
    "end": "2774119"
  },
  {
    "text": "means you know better resource utilization but also jobs will go faster um more partitions will give you better",
    "start": "2774119",
    "end": "2781440"
  },
  {
    "text": "parallelism um you can adjust that using spark default parallelism and adjust that setting um a lot of times that will",
    "start": "2781440",
    "end": "2787520"
  },
  {
    "text": "give you a little bit of a performance boost as well tweaking that setting um and then also just thinking about how",
    "start": "2787520",
    "end": "2793000"
  },
  {
    "text": "you're caching data frames um you can set it so actually a data frame can only be cached in memory um and that's the",
    "start": "2793000",
    "end": "2800359"
  },
  {
    "text": "memory only setting but you can also have the data frame uh spill out to dis but you're going to eat up some IO cost",
    "start": "2800359",
    "end": "2806319"
  },
  {
    "text": "when when processing it but all of those are tweakable settings um also the default serializer makes a difference",
    "start": "2806319",
    "end": "2812440"
  },
  {
    "start": "2811000",
    "end": "2811000"
  },
  {
    "text": "the default is Java but um you can set it to Cairo as well um does support all the serializable types and also",
    "start": "2812440",
    "end": "2819200"
  },
  {
    "text": "serialization in when you're using data sets which I brought up earlier there actually a bunch of custom encoders that",
    "start": "2819200",
    "end": "2824559"
  },
  {
    "text": "are now being used there so there's going to be probably some shift to using whatever these um you know native uh",
    "start": "2824559",
    "end": "2830559"
  },
  {
    "text": "encoders are in spark but with spark 161 um you know this is still uh more or",
    "start": "2830559",
    "end": "2836040"
  },
  {
    "text": "less true uh depending on what you're doing and that's the setting that you can use to change uh the default",
    "start": "2836040",
    "end": "2843440"
  },
  {
    "text": "serializer we'll end real quick with just an overview on uh spark security one one key concept is uh VPC so when",
    "start": "2843640",
    "end": "2852119"
  },
  {
    "start": "2848000",
    "end": "2848000"
  },
  {
    "text": "you're launching a spark cluster um you'll launch it in a virtual private uh cloud in AWS that's basically a",
    "start": "2852119",
    "end": "2858040"
  },
  {
    "text": "logically isolated area of the AWS Network and you can choose two different subnets types a public subnet which has",
    "start": "2858040",
    "end": "2864200"
  },
  {
    "text": "an internet gateway to communicate out um you know to the public IP range or a private subnet that doesn't have an",
    "start": "2864200",
    "end": "2870280"
  },
  {
    "text": "internet gateway um and depending on your use case you know there there are ways to run uh spark securely in both um",
    "start": "2870280",
    "end": "2877640"
  },
  {
    "text": "but EMR is you can run it now start you know a couple months back I think in December we announced support for",
    "start": "2877640",
    "end": "2882880"
  },
  {
    "text": "launching in private subnets we've seen a lot of very security conscious customers just their Network topology uses a lot of private subnets it's easy",
    "start": "2882880",
    "end": "2889200"
  },
  {
    "text": "to launch there and you can also create an S3 endpoint in the in the private subnet as well so that really locks down",
    "start": "2889200",
    "end": "2895520"
  },
  {
    "text": "your connectivity however you can uh create a natat if you're using a private subnet to call allow uh the nodes of the",
    "start": "2895520",
    "end": "2902200"
  },
  {
    "text": "cluster remember this is an basically ec2 instances that make up the cluster to call out to the public ranged or to",
    "start": "2902200",
    "end": "2908000"
  },
  {
    "text": "other AWS public apis like you know AWS KMS doesn't have an endpoint in the private subnet yet if you're using that",
    "start": "2908000",
    "end": "2913160"
  },
  {
    "text": "for Keys um but you know that that's a way this is one way to securely set it",
    "start": "2913160",
    "end": "2918599"
  },
  {
    "text": "up you can also run in a public subnet as well um also you can control the traffic into the ec2 uh nodes of your",
    "start": "2918599",
    "end": "2926319"
  },
  {
    "text": "cluster using security groups uh by default EMR closes all of the ports um",
    "start": "2926319",
    "end": "2931880"
  },
  {
    "text": "to any outside traffic except for Port 22 which is to SSH and you can open up a tunnel and access the web uis in that",
    "start": "2931880",
    "end": "2938559"
  },
  {
    "text": "way but you can tweak all of the uh all these settings as well based on uh you know your your security needs for that",
    "start": "2938559",
    "end": "2945680"
  },
  {
    "text": "cluster um there's a bunch of other security uh features to think about um",
    "start": "2945680",
    "end": "2951160"
  },
  {
    "start": "2947000",
    "end": "2947000"
  },
  {
    "text": "that are options that you have uh when running spark um the first is just encryption options if you say it's",
    "start": "2951160",
    "end": "2958040"
  },
  {
    "text": "important you to encrypt all of your data at rest and in transit um you you can do this uh EMR out of the gate and",
    "start": "2958040",
    "end": "2964839"
  },
  {
    "text": "actually spark in general uh open source hdfs encryption um HD",
    "start": "2964839",
    "end": "2970960"
  },
  {
    "text": "hdfs transparent encryption basically is client side encryption for hdfs using the Hadoop KMS service for Keys um you",
    "start": "2970960",
    "end": "2977400"
  },
  {
    "text": "can easily enable that um you can encrypt local disk uh when executors spill out to disk they write out to",
    "start": "2977400",
    "end": "2983359"
  },
  {
    "text": "local not to hdfs um and temp directories on each node so you'd have to encrypt that node locally using",
    "start": "2983359",
    "end": "2989119"
  },
  {
    "text": "something like Lux um emrfs supports a variety of S3 encryption you can have S3 manage encryption with server side um",
    "start": "2989119",
    "end": "2996160"
  },
  {
    "text": "encryption manages the keys and does the uh encryption Cycles on the server side or you can uh use your own uh HSM or",
    "start": "2996160",
    "end": "3004000"
  },
  {
    "text": "your own uh key vendor and use client side encryption which encrypts things locally on the EMR cluster and then",
    "start": "3004000",
    "end": "3010440"
  },
  {
    "text": "writes out an encrypted object to S3 using envelope encryption and the S3 encryption client um so basically it's",
    "start": "3010440",
    "end": "3018599"
  },
  {
    "text": "uh transparent to spark but however you've been encrypting data in S3 at least with EMR you can plug directly",
    "start": "3018599",
    "end": "3024200"
  },
  {
    "text": "into that um and and work with whatever security controls you have in place uh for inflight encryption data between S3",
    "start": "3024200",
    "end": "3031400"
  },
  {
    "text": "and ec2 is encrypted with s using SSL by default um hdfs blocks remember hdfs is",
    "start": "3031400",
    "end": "3037280"
  },
  {
    "text": "shuffling blocks around at times to rebalance if you're using hdfs encryption that's done client side and",
    "start": "3037280",
    "end": "3043720"
  },
  {
    "text": "uh everything is encrypted in Flight there um and then spark for spark Shuffle um utilizes uh sassle with a",
    "start": "3043720",
    "end": "3051520"
  },
  {
    "text": "digest md5 and that's not what we consider the strongest encryption it's not as strong as is as 256 but it's",
    "start": "3051520",
    "end": "3059119"
  },
  {
    "text": "still a level of protection higher than than unencrypted data over the network um I brought a VPC for Access Control or",
    "start": "3059119",
    "end": "3065960"
  },
  {
    "text": "actually permissions um you can utilize I am which is basically ABS identity and",
    "start": "3065960",
    "end": "3071559"
  },
  {
    "text": "access management which uh gives permissions to AWS resources or users interacting with AWS resources and apis",
    "start": "3071559",
    "end": "3079640"
  },
  {
    "text": "um so there's a lot of control over locking down uh you know what buckets uh your cluster can go access in S3",
    "start": "3079640",
    "end": "3087400"
  },
  {
    "text": "um you know who can create in this case an EMR cluster you can put limits on all these different things um you can also",
    "start": "3087400",
    "end": "3093960"
  },
  {
    "text": "cize yarn um as well so at the application layer so when you log in um",
    "start": "3093960",
    "end": "3099480"
  },
  {
    "text": "you know you can uh you know approve or deny people interacting with spark based on uh permissions that you give at the",
    "start": "3099480",
    "end": "3105839"
  },
  {
    "text": "application layer as well um broad VPC and security groups a lock down your",
    "start": "3105839",
    "end": "3111040"
  },
  {
    "text": "network access also SSH keys with EMR we put an SSH key on the master node and that's a way to access the spark API or",
    "start": "3111040",
    "end": "3118680"
  },
  {
    "text": "you know uh bring up web uis using port forwarding the demo I showed you I was SSH to the master node and was accessing",
    "start": "3118680",
    "end": "3125280"
  },
  {
    "text": "uh the web uis on on that node and then finally auditing AWS cloud trail is a",
    "start": "3125280",
    "end": "3130400"
  },
  {
    "text": "great way to audit um AWS API calls so who's creating EMR clusters who's adding steps who's creating ec2 instances um",
    "start": "3130400",
    "end": "3137880"
  },
  {
    "text": "all that can be tracked and processed from ads Cod Trail um and also with S3",
    "start": "3137880",
    "end": "3143799"
  },
  {
    "text": "um there S3 uh logs which can track who's accessing what object um you know",
    "start": "3143799",
    "end": "3149079"
  },
  {
    "text": "and even in EMR we can inject the application ID to see what I you know actual jobs have been touching uh",
    "start": "3149079",
    "end": "3155359"
  },
  {
    "text": "different objects um that was a lot quickly a lot of this is in the EMR doc so I'm happy to talk more about spark security um",
    "start": "3155359",
    "end": "3162720"
  },
  {
    "text": "after the session anyway thank you everybody for uh for your time um and it",
    "start": "3162720",
    "end": "3167760"
  },
  {
    "text": "looks like we have a little how much time do we have left because we started the timer seven minutes so we could take a",
    "start": "3167760",
    "end": "3173079"
  },
  {
    "text": "few questions uh as well thank you [Applause]",
    "start": "3173079",
    "end": "3182300"
  }
]