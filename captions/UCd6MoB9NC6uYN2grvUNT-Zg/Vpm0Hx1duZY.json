[
  {
    "start": "0",
    "end": "81000"
  },
  {
    "text": "my name is usman salud and i am a software engineer at Yelp I've been working at dl for the last one and a",
    "start": "500",
    "end": "6569"
  },
  {
    "text": "half years and before that i was doing my PhD in high performance computing from universe just at urbana-champaign",
    "start": "6569",
    "end": "13250"
  },
  {
    "text": "so i'm here to talk about siegel which is a highly fault tolerant distributed",
    "start": "13250",
    "end": "20400"
  },
  {
    "text": "system for concurrent task execution that we've been working on developing at",
    "start": "20400",
    "end": "26099"
  },
  {
    "text": "Yale for the last year or so before",
    "start": "26099",
    "end": "33899"
  },
  {
    "text": "going into that I'd have to share a few things about what Yelp is Yelp is a",
    "start": "33899",
    "end": "39660"
  },
  {
    "text": "company a rapidly growing company that is connecting people with great local",
    "start": "39660",
    "end": "45120"
  },
  {
    "text": "businesses people write reviews and before that people go to businesses and",
    "start": "45120",
    "end": "50219"
  },
  {
    "text": "then the right reviews which in turn help other users to select which business to interact with as of q2 2015",
    "start": "50219",
    "end": "62180"
  },
  {
    "text": "we had 83 million monthly visitors on mobile we had around 83 million reviews",
    "start": "62180",
    "end": "70520"
  },
  {
    "text": "sixty-eight percent of our searches were conducted from mobile and we are present",
    "start": "70520",
    "end": "76650"
  },
  {
    "text": "in 32 countries oh you guys are here and",
    "start": "76650",
    "end": "85470"
  },
  {
    "start": "81000",
    "end": "81000"
  },
  {
    "text": "I thank you for that so what is in this talk for you guys to get out of it I am",
    "start": "85470",
    "end": "92640"
  },
  {
    "text": "going to show how Yelp runs millions of tests in a single day how it downloads",
    "start": "92640",
    "end": "98490"
  },
  {
    "text": "terabytes of data in an extremely efficient manner and how do we scale a",
    "start": "98490",
    "end": "104780"
  },
  {
    "text": "seagull cluster which I will talk to you about later based on a custom metric",
    "start": "104780",
    "end": "112020"
  },
  {
    "text": "which we use for scaling",
    "start": "112020",
    "end": "115430"
  },
  {
    "start": "119000",
    "end": "119000"
  },
  {
    "text": "I'm not going to share a lot of code over here because i'm not good at a sharing code in my talk and for some",
    "start": "121170",
    "end": "127780"
  },
  {
    "text": "reason i find it to be very boring when I should code in my talks so it's going",
    "start": "127780",
    "end": "132819"
  },
  {
    "text": "to be a high level architecture talk and I'm going to assume that most of you are",
    "start": "132819",
    "end": "138430"
  },
  {
    "text": "familiar with apache measles although it's not a hard requirement because I have a couple of slides where I'm going",
    "start": "138430",
    "end": "145840"
  },
  {
    "text": "to go over what exactly happens when we use apache me sauce and how it actually",
    "start": "145840",
    "end": "151450"
  },
  {
    "text": "operates so what exactly is seagull",
    "start": "151450",
    "end": "158010"
  },
  {
    "start": "154000",
    "end": "154000"
  },
  {
    "text": "seagull is a distributed system that allows concurrent tasks to run at very",
    "start": "158010",
    "end": "164590"
  },
  {
    "text": "large scale and while doing that it also makes sure that we achieve high cluster",
    "start": "164590",
    "end": "170910"
  },
  {
    "text": "utilization seagull is also highly fault tolerant and resilient to failures",
    "start": "170910",
    "end": "179670"
  },
  {
    "start": "183000",
    "end": "183000"
  },
  {
    "text": "although Siegal can be used at in in different ways but currently we use",
    "start": "185080",
    "end": "191920"
  },
  {
    "text": "seagull for running millions of tests at Yelp so what's the typical use case for",
    "start": "191920",
    "end": "198160"
  },
  {
    "text": "seagull there is this to help developer which is sitting on his computer he goes",
    "start": "198160",
    "end": "204010"
  },
  {
    "text": "to a terminal and just punches seagull dash runs what it does is it triggers a",
    "start": "204010",
    "end": "210760"
  },
  {
    "text": "seagull pinned seagull does all the magic it's supposed to do it runs all",
    "start": "210760",
    "end": "216100"
  },
  {
    "text": "the tests that it needs to run for that developer and at the end it generates a report which you can see on the bottom",
    "start": "216100",
    "end": "223750"
  },
  {
    "text": "right hand side that the developer then can go and see to find out did he make",
    "start": "223750",
    "end": "230380"
  },
  {
    "text": "any tests fail or if he created any problems with the service that he's",
    "start": "230380",
    "end": "237610"
  },
  {
    "text": "working on I'm going to start by talking",
    "start": "237610",
    "end": "243970"
  },
  {
    "start": "240000",
    "end": "240000"
  },
  {
    "text": "about how seagull works and then I'm going to focus and narrow down on to one",
    "start": "243970",
    "end": "250209"
  },
  {
    "text": "specific problem which is probably the biggest problem we faced while right",
    "start": "250209",
    "end": "257140"
  },
  {
    "text": "implementing see girl and later on doing the global its global availability and",
    "start": "257140",
    "end": "263280"
  },
  {
    "text": "then I'll at the end I will conclude with talking about how we scale auto",
    "start": "263280",
    "end": "269950"
  },
  {
    "text": "scales seagull cluster and how do we make it fault tolerant so how does our",
    "start": "269950",
    "end": "279970"
  },
  {
    "text": "seagull work and even before that what's",
    "start": "279970",
    "end": "285640"
  },
  {
    "start": "283000",
    "end": "283000"
  },
  {
    "text": "what's the challenge why did we make seagull so it turns out that every",
    "start": "285640",
    "end": "291040"
  },
  {
    "text": "single day seagull runs tests that would take 700 days to run in serial on",
    "start": "291040",
    "end": "298990"
  },
  {
    "text": "average we do 350 seagull runs in a day and each of these seagull runs",
    "start": "298990",
    "end": "306520"
  },
  {
    "text": "runs approximately 70,000 tests each",
    "start": "306520",
    "end": "311770"
  },
  {
    "text": "seagull run that is each of these 70,000 tests would take two days to run in",
    "start": "311770",
    "end": "317530"
  },
  {
    "text": "serial a demand for running seagull is fairly volatile but the good thing is",
    "start": "317530",
    "end": "325180"
  },
  {
    "text": "that it's predictable as well so thirty percent of all the single runs that we",
    "start": "325180",
    "end": "331750"
  },
  {
    "text": "do they happen between a three-hour period which is usually three pm to six",
    "start": "331750",
    "end": "337870"
  },
  {
    "text": "pm unless or until there is a free lunch at Yelp which can move an hour either",
    "start": "337870",
    "end": "343240"
  },
  {
    "text": "way so what do we do at a very high",
    "start": "343240",
    "end": "352540"
  },
  {
    "start": "350000",
    "end": "350000"
  },
  {
    "text": "level we run three thousand tests concurrently on a two hundred machine",
    "start": "352540",
    "end": "359110"
  },
  {
    "text": "cluster what does that fetch us it reduces the time for a single run from",
    "start": "359110",
    "end": "367660"
  },
  {
    "text": "two days which is voted if you would run it in serial to just 14 minutes but it's",
    "start": "367660",
    "end": "374590"
  },
  {
    "text": "easier said than done running at such high scale involves a lot of challenges",
    "start": "374590",
    "end": "381960"
  },
  {
    "text": "some of them being like downloading 36 terabytes of data in a single day and",
    "start": "381960",
    "end": "388440"
  },
  {
    "text": "they are during the peak ours we are downloading as much as five terabytes in",
    "start": "388440",
    "end": "394120"
  },
  {
    "text": "a single are we can trigger up to 200 simultaneous downloads for a single",
    "start": "394120",
    "end": "401260"
  },
  {
    "text": "large file where is hundreds of megabytes and almost every day we pull",
    "start": "401260",
    "end": "409330"
  },
  {
    "text": "start stop more than 1.5 million docker containers and at peak times we can pull",
    "start": "409330",
    "end": "419080"
  },
  {
    "text": "start and stop as much as two hundred and ten thousand docker containers in a single are what goes inside segal all",
    "start": "419080",
    "end": "429790"
  },
  {
    "start": "426000",
    "end": "426000"
  },
  {
    "text": "these wonderful technologies combine them that's basically what seagull is made up",
    "start": "429790",
    "end": "435910"
  },
  {
    "text": "of we use signal effects in cabana for monitoring seagull that tells us how healthy seagull a is and if there are",
    "start": "435910",
    "end": "442210"
  },
  {
    "text": "any problems going on with it we use s3 or actually we started using by we",
    "start": "442210",
    "end": "449050"
  },
  {
    "text": "started using s3 as our artifact host and we use dynamo DB and elastic search",
    "start": "449050",
    "end": "455110"
  },
  {
    "text": "for reporting purposes but at the heart of seagull lies the easy to compute",
    "start": "455110",
    "end": "461170"
  },
  {
    "text": "cluster which runs apache me sauce jenkins and docker for us",
    "start": "461170",
    "end": "468480"
  },
  {
    "start": "473000",
    "end": "473000"
  },
  {
    "text": "so this figure shows a detailed map of what's inside Segal the developer",
    "start": "474720",
    "end": "481530"
  },
  {
    "text": "sitting on his table starts signal that's a turning point what it does is",
    "start": "481530",
    "end": "487770"
  },
  {
    "text": "it starts a Jenkins build Jenkins pulls the code the code that we are supposed",
    "start": "487770",
    "end": "494970"
  },
  {
    "text": "to test compiles the Python files and uploads the compiled code to s3 the next",
    "start": "494970",
    "end": "504510"
  },
  {
    "text": "thing it's Jenkins supposed to do is to determine the list of tests that we are",
    "start": "504510",
    "end": "510270"
  },
  {
    "text": "supposed to run once the list of tests is determined that's passed to the",
    "start": "510270",
    "end": "515370"
  },
  {
    "text": "prioritize ER which takes in the list of tasks that we are supposed to run and it determines the exact order in which the",
    "start": "515370",
    "end": "522990"
  },
  {
    "text": "tests are supposed to run the prioritized test list is then fed the",
    "start": "522990",
    "end": "533700"
  },
  {
    "text": "prioritized s list is then fed to seagull schedulers which takes the text test list bundles them according to",
    "start": "533700",
    "end": "541530"
  },
  {
    "text": "whatever the priority is and these schedulers are sitting on top of Apache",
    "start": "541530",
    "end": "547350"
  },
  {
    "text": "me sauce which in turn uses ec2 machines as seagull slaves to actually run the",
    "start": "547350",
    "end": "555240"
  },
  {
    "text": "tests once the tests have finished to completion and while running the test we",
    "start": "555240",
    "end": "563220"
  },
  {
    "text": "also pulled artifacts which is a very important point we pull artifacts from s3 and then run the tests on the slave",
    "start": "563220",
    "end": "569700"
  },
  {
    "text": "seagull slaves and the results are then reported to dynamic DB elastic search",
    "start": "569700",
    "end": "576390"
  },
  {
    "text": "for reporting purposes and signal effects and Cabana for monitoring purposes once the results are there in",
    "start": "576390",
    "end": "583440"
  },
  {
    "text": "dynamo DB and elastic search seagull you I goes ahead and it fetches those",
    "start": "583440",
    "end": "589500"
  },
  {
    "text": "results and finally the results are displayed to the developer who started",
    "start": "589500",
    "end": "595380"
  },
  {
    "text": "the run so I'm going to drill down to each of these separate modules and",
    "start": "595380",
    "end": "600839"
  },
  {
    "text": "explain what exactly is going on within each of them once the developer triggers",
    "start": "600839",
    "end": "608960"
  },
  {
    "start": "605000",
    "end": "605000"
  },
  {
    "text": "the run it triggers a Jenkins bed we run",
    "start": "608960",
    "end": "614060"
  },
  {
    "text": "Jenkins as a cluster of seven are 38 x large machines its job is to build our",
    "start": "614060",
    "end": "620500"
  },
  {
    "text": "largest service which I'm going to explain a little detail in later once",
    "start": "620500",
    "end": "626810"
  },
  {
    "text": "it's done building the service it uploads the artifact to s3 so whenever",
    "start": "626810",
    "end": "633260"
  },
  {
    "text": "i'm referring to artifact for the rest of the talk i'm basically referring to this largest service because the",
    "start": "633260",
    "end": "639920"
  },
  {
    "text": "compiled version of this larger service we upload the artifact of amazon s3 and",
    "start": "639920",
    "end": "646390"
  },
  {
    "text": "jenkins triggers the job which goes ahead and discovers tests so what is",
    "start": "646390",
    "end": "657470"
  },
  {
    "start": "656000",
    "end": "656000"
  },
  {
    "text": "yell card effect yelp artifact is basically the compiled version of a",
    "start": "657470",
    "end": "662570"
  },
  {
    "text": "largest service that forms a major part of a website it consists of hundreds of",
    "start": "662570",
    "end": "670130"
  },
  {
    "text": "megabytes and at x around 10 minutes to build the artifact it uses of memory and",
    "start": "670130",
    "end": "679750"
  },
  {
    "text": "because there's a huge set-up cost we do not build artifact on each of the slaves",
    "start": "679750",
    "end": "686839"
  },
  {
    "text": "which are actually running a test and instead we build it once and upload to s3 and all the all the slaves that are",
    "start": "686839",
    "end": "695780"
  },
  {
    "text": "responsible to run the test they pull it from s3 and use it for running the tests",
    "start": "695780",
    "end": "702970"
  },
  {
    "start": "710000",
    "end": "710000"
  },
  {
    "text": "once the bill is done we now start the next phase which is the test discovery",
    "start": "711680",
    "end": "717680"
  },
  {
    "text": "phase we take the compiled artifact and in this phase we determine what are the",
    "start": "717680",
    "end": "725420"
  },
  {
    "text": "test names that we actually need to run it's it's a process where we just pass a",
    "start": "725420",
    "end": "731750"
  },
  {
    "text": "bunch of Python files to extract the actual test names and it takes around two minutes to complete right now we",
    "start": "731750",
    "end": "739850"
  },
  {
    "text": "have seven different test Suites so we generate a test list for each of these seven suites right now we have seen yet",
    "start": "739850",
    "end": "754940"
  },
  {
    "text": "developer triggers a build we saw what what's what happens inside Jenkins it builds and generates a test list and now",
    "start": "754940",
    "end": "762350"
  },
  {
    "text": "we are going to look at what since what's happening inside the prioritize er a test prioritize er works on a very",
    "start": "762350",
    "end": "771170"
  },
  {
    "start": "768000",
    "end": "768000"
  },
  {
    "text": "simple motion it's scheduled the longest tests first and the way we do that is we",
    "start": "771170",
    "end": "778990"
  },
  {
    "text": "maintain the test timing data in dynamo dB so the prioritize ER takes the test",
    "start": "778990",
    "end": "786980"
  },
  {
    "text": "list and combines it with the historical test timing data which comes from",
    "start": "786980",
    "end": "792680"
  },
  {
    "text": "DynamoDB and based on that it generates an ordered list which is going to tell",
    "start": "792680",
    "end": "798440"
  },
  {
    "text": "the schedulers the order in which it should run the tests and we use DynamoDB",
    "start": "798440",
    "end": "805580"
  },
  {
    "text": "to do several different reasons one of which is that we don't need to maintain it and that's a very good thing to do",
    "start": "805580",
    "end": "813400"
  },
  {
    "text": "because maintaining Gator stores is a big pain and we have had several trouble",
    "start": "813400",
    "end": "819260"
  },
  {
    "text": "with other technologies that we have tried to use and the other thing is our",
    "start": "819260",
    "end": "825170"
  },
  {
    "text": "table only costs around two hundred dollars per month which is very cheap and it's extremely reliable we haven't",
    "start": "825170",
    "end": "831920"
  },
  {
    "text": "had any trouble with it and needless to mention that we do more than 25 million",
    "start": "831920",
    "end": "839320"
  },
  {
    "text": "fetches in a single day and it's been working very well",
    "start": "839320",
    "end": "845170"
  },
  {
    "text": "so after looking at the prioritize er we now look at what's what's inside the",
    "start": "851410",
    "end": "861188"
  },
  {
    "text": "what's happening inside the heart of cl as i call it and how do seagulls",
    "start": "863319",
    "end": "870290"
  },
  {
    "text": "schedulers interact with may sauce and how does mesos coordinates with the",
    "start": "870290",
    "end": "876050"
  },
  {
    "text": "slave to actually run the tests so what",
    "start": "876050",
    "end": "884930"
  },
  {
    "start": "882000",
    "end": "882000"
  },
  {
    "text": "exactly are we trying to do a wire we need to run 350 seagull runs roughly in",
    "start": "884930",
    "end": "892069"
  },
  {
    "text": "a single day each of these runs has around 70,000 tests which means that in",
    "start": "892069",
    "end": "898459"
  },
  {
    "text": "a single day we are supposed to run somewhere around 25 million tests each",
    "start": "898459",
    "end": "905059"
  },
  {
    "text": "of these runs whereas a set of 70,000 tests that we need to run for every",
    "start": "905059",
    "end": "910550"
  },
  {
    "text": "developer can take up to 48 hours efrain in Syria and the most challenging part",
    "start": "910550",
    "end": "917720"
  },
  {
    "text": "out of that is that the demand for seagull where is a god the figure that",
    "start": "917720",
    "end": "924170"
  },
  {
    "text": "you see at the bottom of the slide shows the number of seagull runs submitted /",
    "start": "924170",
    "end": "929389"
  },
  {
    "text": "10 minutes and it's showing you for a single day it's quite noticeable that",
    "start": "929389",
    "end": "938329"
  },
  {
    "text": "for most of the day there are not a lot of single runs and almost ninety percent",
    "start": "938329",
    "end": "946370"
  },
  {
    "text": "of the seagull runs occur in when it's daytime in the u.s. that somewhere around from 99 a.m. to around seven pm",
    "start": "946370",
    "end": "955610"
  },
  {
    "text": "and what hurts what's even difficult to tackle is the peak that you can see",
    "start": "955610",
    "end": "962470"
  },
  {
    "text": "occurs after 4pm",
    "start": "962470",
    "end": "966519"
  },
  {
    "start": "973000",
    "end": "973000"
  },
  {
    "text": "so let's start with a brief introduction for Apache missiles party my sauce is a resource management system it's piece of",
    "start": "975560",
    "end": "982790"
  },
  {
    "text": "software which part of it runs on me sauce master and the other part of it",
    "start": "982790",
    "end": "990940"
  },
  {
    "text": "runs on every slave so there are new different processes with mrs. master",
    "start": "990940",
    "end": "996140"
  },
  {
    "text": "runs of them on the master and miso slave processes run on every slave slaves whenever they come online their",
    "start": "996140",
    "end": "1003550"
  },
  {
    "text": "register their resources with me sauce master so every slave when will come in",
    "start": "1003550",
    "end": "1009160"
  },
  {
    "text": "and register itself and tell that I have these many CPUs these many this many this much amount of memory and if you're",
    "start": "1009160",
    "end": "1016300"
  },
  {
    "text": "using ports for a location that bit as well and there are schedulers which in",
    "start": "1016300",
    "end": "1024160"
  },
  {
    "text": "our case our seagull schedulers that are responsible for bundling up test and",
    "start": "1024160",
    "end": "1029970"
  },
  {
    "text": "scheduling them on the slaves these schedulers subscribe to vsauce3 for",
    "start": "1029970",
    "end": "1036188"
  },
  {
    "text": "consuming these resources which are the seagull which are provided by the seagull slaves once the schedulers have",
    "start": "1036189",
    "end": "1044860"
  },
  {
    "text": "subscribed me mrs. master will offer resources to these schedulers in a fair",
    "start": "1044860",
    "end": "1050410"
  },
  {
    "text": "manner seagull leverages resource",
    "start": "1050410",
    "end": "1058120"
  },
  {
    "start": "1054000",
    "end": "1054000"
  },
  {
    "text": "management abilities of apache me sauce and what happens is that whenever someone triggers developer triggers a",
    "start": "1058120",
    "end": "1064720"
  },
  {
    "text": "seagull run a miso scheduler is created and each of this the schedulers in turn",
    "start": "1064720",
    "end": "1074280"
  },
  {
    "text": "distribute all the work amongst 600 workers which I'll be referring to as",
    "start": "1074280",
    "end": "1081340"
  },
  {
    "text": "executives for the rest of this talk so you have a you have these 70,000 tests",
    "start": "1081340",
    "end": "1087340"
  },
  {
    "text": "and you bundle them into 600 600",
    "start": "1087340",
    "end": "1093070"
  },
  {
    "text": "executives and then each of these executives in return run on one of the",
    "start": "1093070",
    "end": "1100030"
  },
  {
    "text": "two hundred slaves that are seagull cluster has so we currently use are all",
    "start": "1100030",
    "end": "1106240"
  },
  {
    "text": "are 38 x machines which have 32 Hardware threads as well as 256 gigabytes of RAM",
    "start": "1106240",
    "end": "1115690"
  },
  {
    "start": "1125000",
    "end": "1125000"
  },
  {
    "text": "so I'm going to demonstrate how sequel operates with with an animation and this",
    "start": "1125130",
    "end": "1131980"
  },
  {
    "text": "is the terminology that I am going to use in the following animation so all",
    "start": "1131980",
    "end": "1138480"
  },
  {
    "text": "red squares will represent a single test whenever you see like a bunch of them it",
    "start": "1138480",
    "end": "1146410"
  },
  {
    "text": "represents a bundle which is a set of tests and that's the atomic level at which you can distribute work to to",
    "start": "1146410",
    "end": "1154480"
  },
  {
    "text": "Siegel slaves blue boxes signify a scheduler or you can say it roughly",
    "start": "1154480",
    "end": "1161380"
  },
  {
    "text": "correlates to actually correlates there is a one-to-one correlation between a",
    "start": "1161380",
    "end": "1166390"
  },
  {
    "text": "seagull scheduler and a seagull run and the green box says will represent",
    "start": "1166390",
    "end": "1173040"
  },
  {
    "text": "seagull slaves so how does everything",
    "start": "1173040",
    "end": "1179920"
  },
  {
    "start": "1177000",
    "end": "1177000"
  },
  {
    "text": "start a young developer comes in and submits a seagull run once the developer",
    "start": "1179920",
    "end": "1188679"
  },
  {
    "text": "submits are on a seagull scheduler is created as the seagull scheduler gets",
    "start": "1188679",
    "end": "1195700"
  },
  {
    "text": "the prioritize test list and it divides them into set of tests which are also",
    "start": "1195700",
    "end": "1202750"
  },
  {
    "text": "called bundles once the bundles are made and the scheduler subscribes to me sauce",
    "start": "1202750",
    "end": "1211059"
  },
  {
    "text": "master mrs. masters offers it resources like in this case mrs. is offering it",
    "start": "1211059",
    "end": "1217510"
  },
  {
    "text": "offering scheduler c1 resources from slave s1 the scheduler accepts the offer",
    "start": "1217510",
    "end": "1225280"
  },
  {
    "text": "and it tells me sauce to launch these two bundles on slave s 1 once that's",
    "start": "1225280",
    "end": "1235780"
  },
  {
    "text": "happened another developer user to comes in and he submits a seagull run when",
    "start": "1235780",
    "end": "1247270"
  },
  {
    "text": "when that happens a separate scheduler c2 is created and the tasks are once",
    "start": "1247270",
    "end": "1254080"
  },
  {
    "text": "again bundled in this case the tasks are bundled with three different bundles or three",
    "start": "1254080",
    "end": "1259680"
  },
  {
    "text": "different set of tests it's a it subscribes to mrs. masters and once it",
    "start": "1259680",
    "end": "1265830"
  },
  {
    "text": "subscribes me sauce makes it an offer from for resources from slave s1",
    "start": "1265830",
    "end": "1272330"
  },
  {
    "text": "schedulers c2 is intelligent in the sense that it looks at the offer and it",
    "start": "1272330",
    "end": "1278340"
  },
  {
    "text": "says okay I can only use this offer to run one of the bundles because the other",
    "start": "1278340",
    "end": "1286080"
  },
  {
    "text": "rest of slave s 1 is occupied so it tells me sauce we launched one bundle on",
    "start": "1286080",
    "end": "1291810"
  },
  {
    "text": "it which gets launched me sauce then offers scheduler c2 resources from slave",
    "start": "1291810",
    "end": "1298200"
  },
  {
    "text": "s to the scheduler accepts the offer and it tells me source to launch the",
    "start": "1298200",
    "end": "1304920"
  },
  {
    "text": "remaining two bundles on slave s2 once that happens these guess the executives",
    "start": "1304920",
    "end": "1314580"
  },
  {
    "text": "run and they finish to completion and the results are reported and that and",
    "start": "1314580",
    "end": "1320730"
  },
  {
    "text": "that the results are reported to single you why so after having a brief overview",
    "start": "1320730",
    "end": "1328170"
  },
  {
    "text": "of what exactly happens in Siegen let's focus on the main challenge that we",
    "start": "1328170",
    "end": "1333990"
  },
  {
    "text": "faced while developing seagull and that was got to do with downloading the",
    "start": "1333990",
    "end": "1339180"
  },
  {
    "text": "artifact so what's so special about and what's why does this make everything so",
    "start": "1339180",
    "end": "1347340"
  },
  {
    "start": "1342000",
    "end": "1342000"
  },
  {
    "text": "critical why is artifact downloading so critical each executor needs to have the",
    "start": "1347340",
    "end": "1352920"
  },
  {
    "text": "artifact before running addresses and at peak times there we can run 18,000",
    "start": "1352920",
    "end": "1361830"
  },
  {
    "text": "executives in a single R which means that we can generate up to 18,000",
    "start": "1361830",
    "end": "1368000"
  },
  {
    "text": "requests for downloading the artifact each of this these requests is for a",
    "start": "1368000",
    "end": "1373980"
  },
  {
    "text": "very large file that can be hundreds of megabytes and worsen things up if out of",
    "start": "1373980",
    "end": "1381630"
  },
  {
    "text": "the 600 executives that we make corresponding to a single seagull run if even one of",
    "start": "1381630",
    "end": "1387800"
  },
  {
    "text": "m gets delayed it's going to delay the entire seagull run so it's very",
    "start": "1387800",
    "end": "1393830"
  },
  {
    "text": "important for us to make all the 600 executives fail at roughly the same time",
    "start": "1393830",
    "end": "1401500"
  },
  {
    "text": "so if the rankings will deprioritize her we looked at how schedulers are",
    "start": "1410770",
    "end": "1416620"
  },
  {
    "text": "interacting with the Apache Vsauce so now we are going to look what happens",
    "start": "1416620",
    "end": "1422440"
  },
  {
    "text": "inside the slaves when we are actually running the tests this is a typical life",
    "start": "1422440",
    "end": "1434230"
  },
  {
    "start": "1431000",
    "end": "1431000"
  },
  {
    "text": "cycle of the executor each executor starts by downloading the artifact from",
    "start": "1434230",
    "end": "1441880"
  },
  {
    "text": "s3 once the artifact is downloaded the",
    "start": "1441880",
    "end": "1447010"
  },
  {
    "text": "executor starts a bunch of docker containers which are and in turn running services once the services are up and",
    "start": "1447010",
    "end": "1455230"
  },
  {
    "text": "running the tests start running and the tests are in turn using these sources",
    "start": "1455230",
    "end": "1461170"
  },
  {
    "text": "and after the rests finished running we report results to dynamo DB as well as",
    "start": "1461170",
    "end": "1466750"
  },
  {
    "text": "elasticsearch I me report metrics to combine and signal effects this whole process takes around 10 minutes on",
    "start": "1466750",
    "end": "1474430"
  },
  {
    "text": "average but the main problem out of all",
    "start": "1474430",
    "end": "1482020"
  },
  {
    "text": "these steps that I mentioned is fetching the artifact so let's just take a deeper",
    "start": "1482020",
    "end": "1488830"
  },
  {
    "text": "look at what happens and how do we fetch artifacts and I did not make this t go",
    "start": "1488830",
    "end": "1497830"
  },
  {
    "text": "below this is something weird sorry ok",
    "start": "1497830",
    "end": "1503260"
  },
  {
    "text": "so I'm going to use the same terminology with a square represents a test and then",
    "start": "1503260",
    "end": "1508390"
  },
  {
    "text": "we have a set of tests the schedulers and the slave but I am going to add two new things to it a box is now going to",
    "start": "1508390",
    "end": "1515680"
  },
  {
    "text": "represent an artifact and it's important to notice that 11 scheduler let's say",
    "start": "1515680",
    "end": "1523090"
  },
  {
    "text": "scheduler see one only need this the same artifact a one so all 600",
    "start": "1523090",
    "end": "1529990"
  },
  {
    "text": "executives for scheduler c1 need to download the same artifact a1 and the in",
    "start": "1529990",
    "end": "1539170"
  },
  {
    "text": "the brown boy I'm representing it shows the executor",
    "start": "1539170",
    "end": "1544240"
  },
  {
    "text": "and this the label see one shows that",
    "start": "1544240",
    "end": "1549730"
  },
  {
    "text": "this executor belongs to scheduler see one there are two critical things that",
    "start": "1549730",
    "end": "1555549"
  },
  {
    "text": "are there inside this inside the executor one of them is the bundle which",
    "start": "1555549",
    "end": "1560559"
  },
  {
    "text": "is the set of tests that it's supposed to run and the other one is the artifact",
    "start": "1560559",
    "end": "1565600"
  },
  {
    "text": "which is used to run which is used to running the test so what happens",
    "start": "1565600",
    "end": "1575440"
  },
  {
    "start": "1571000",
    "end": "1571000"
  },
  {
    "text": "typically when a scheduler see one starts when it starts we distribute the",
    "start": "1575440",
    "end": "1582039"
  },
  {
    "text": "scheduler see one distributes all the Tesla's are there in to 600 executives",
    "start": "1582039",
    "end": "1587520"
  },
  {
    "text": "each of these executives download their own artifact and typically an executor",
    "start": "1587520",
    "end": "1595450"
  },
  {
    "text": "is around 10 minutes to complete the memory and CPU requirements for running",
    "start": "1595450",
    "end": "1601330"
  },
  {
    "text": "these tests are such that each slave ends up running 15 executives",
    "start": "1601330",
    "end": "1606909"
  },
  {
    "text": "concurrently at the same time and that is the reason that for running all 600",
    "start": "1606909",
    "end": "1613270"
  },
  {
    "text": "executives at the same time for a single segal for a single Segal run we need 40",
    "start": "1613270",
    "end": "1619929"
  },
  {
    "text": "slaves but if you think about it if there are 200 siegel if there are 200",
    "start": "1619929",
    "end": "1627850"
  },
  {
    "text": "single slaves and each of them can run 15 executives at a given time and given",
    "start": "1627850",
    "end": "1635020"
  },
  {
    "text": "the lifecycle of a typical executor is 10 minutes we can run up to 18,000",
    "start": "1635020",
    "end": "1641409"
  },
  {
    "text": "executives per hour which roughly gotta corresponds to around 13.5 terabytes of",
    "start": "1641409",
    "end": "1647980"
  },
  {
    "text": "data that we have to download a single are so this executive start they want to",
    "start": "1647980",
    "end": "1655570"
  },
  {
    "text": "download the artifact all of them go and hit s3 we started downloading artifacts",
    "start": "1655570",
    "end": "1665950"
  },
  {
    "start": "1661000",
    "end": "1661000"
  },
  {
    "text": "from s3 in that manner and that didn't work very well we had a lot of requests that took more than 30 minutes which was",
    "start": "1665950",
    "end": "1673870"
  },
  {
    "text": "completely not acceptable and it was basically going up to that point that",
    "start": "1673870",
    "end": "1680409"
  },
  {
    "text": "seagull wasn't really useful and we found out that the main reason for that was we were choking nad boxes and this",
    "start": "1680409",
    "end": "1688780"
  },
  {
    "text": "happened a year ago at that time avoiding nad boxes wasn't very easy",
    "start": "1688780",
    "end": "1694980"
  },
  {
    "text": "right now we have and their end points that we can use to avoid nad boxes but we weren't aware of those endpoints at",
    "start": "1694980",
    "end": "1702190"
  },
  {
    "text": "that time and using public ip's wasn't very easy for us since our security team",
    "start": "1702190",
    "end": "1707890"
  },
  {
    "text": "had some concerns about it and as always be being a company wanted to get a",
    "start": "1707890",
    "end": "1713679"
  },
  {
    "text": "solution as quick as possible so what",
    "start": "1713679",
    "end": "1719740"
  },
  {
    "start": "1719000",
    "end": "1719000"
  },
  {
    "text": "did we do we started with a few basic optimizations so as I mentioned",
    "start": "1719740",
    "end": "1725289"
  },
  {
    "text": "previously a single scheduler needs the same artifact and there are cases where",
    "start": "1725289",
    "end": "1733230"
  },
  {
    "text": "multiple executives corresponding from the same segal run can be running on the",
    "start": "1733230",
    "end": "1739240"
  },
  {
    "text": "same actual slave on the same machine so instead of downloading an artifact",
    "start": "1739240",
    "end": "1744970"
  },
  {
    "text": "instead of each of those executives downloading the same artifact again why",
    "start": "1744970",
    "end": "1750610"
  },
  {
    "text": "not download the artifact once for the same seagull run / slave and then all",
    "start": "1750610",
    "end": "1756340"
  },
  {
    "text": "the executives for the same seagull run share their artifact so like in this",
    "start": "1756340",
    "end": "1763120"
  },
  {
    "text": "case slave one is running to executives from scheduler c1 and slave 40 is also",
    "start": "1763120",
    "end": "1769659"
  },
  {
    "text": "running to schedulers from for from for slave for scheduler c2 so instead of",
    "start": "1769659",
    "end": "1776139"
  },
  {
    "text": "downloading their own artifacts they can download artifact a1 and a2 once and",
    "start": "1776139",
    "end": "1781809"
  },
  {
    "text": "then share them but it has its own disadvantages one of it is that we",
    "start": "1781809",
    "end": "1787539"
  },
  {
    "text": "really wanted to keep each executor independent of the other but now that",
    "start": "1787539",
    "end": "1792669"
  },
  {
    "text": "they are sharing artifacts it's violating the inner the interdependence",
    "start": "1792669",
    "end": "1799450"
  },
  {
    "text": "and the other problem was that we had to implement locking mechanism to stop multiple",
    "start": "1799450",
    "end": "1805639"
  },
  {
    "text": "executives for this from the same scheduler to start downloading artifact on the same physical machine at the same",
    "start": "1805639",
    "end": "1812059"
  },
  {
    "text": "place but it still didn't scale so what",
    "start": "1812059",
    "end": "1818090"
  },
  {
    "text": "did we try next we resorted to a very naive solution where we just brought up",
    "start": "1818090",
    "end": "1824599"
  },
  {
    "start": "1819000",
    "end": "1819000"
  },
  {
    "text": "a bunch of actually nine are 38 x large machines and we named it as artifact",
    "start": "1824599",
    "end": "1830629"
  },
  {
    "text": "cash and we started replicating every single artifact that we made on each of",
    "start": "1830629",
    "end": "1837440"
  },
  {
    "text": "these nine machines and at front of it we put engine axe for load balancing",
    "start": "1837440",
    "end": "1843440"
  },
  {
    "text": "purposes so all the download requests will go to engine acts and engine acts will load balance the request and assign",
    "start": "1843440",
    "end": "1851179"
  },
  {
    "text": "it to each of the nine one of the one of the nine artifacts the machines that we",
    "start": "1851179",
    "end": "1856369"
  },
  {
    "text": "have in the artifact cash and given that these machines have a bandwidth of 10",
    "start": "1856369",
    "end": "1862099"
  },
  {
    "text": "gigabits per second it really helped us starting running seagull runs for a lot",
    "start": "1862099",
    "end": "1869090"
  },
  {
    "start": "1869000",
    "end": "1869000"
  },
  {
    "text": "of users so let's consider some numbers the top figure over here shows the",
    "start": "1869090",
    "end": "1876679"
  },
  {
    "text": "average download time / 10 minutes for a single download so we average we",
    "start": "1876679",
    "end": "1883279"
  },
  {
    "text": "consider a time span of 10 minutes and we average when we take the average of",
    "start": "1883279",
    "end": "1889039"
  },
  {
    "text": "all the downloads that happen in that 10 minutes and we this graph shows the",
    "start": "1889039",
    "end": "1894769"
  },
  {
    "text": "average download times for 10 minutes for a 24 hour period the bottom figure shows the number of active schedulers",
    "start": "1894769",
    "end": "1901909"
  },
  {
    "text": "once again for each 10 minutes for the same amount of time generally you are",
    "start": "1901909",
    "end": "1908389"
  },
  {
    "text": "able to achieve an average download time of around 30 seconds but as you can see",
    "start": "1908389",
    "end": "1915289"
  },
  {
    "text": "from this figure there is a there's a correlation between the number of active schedulers and",
    "start": "1915289",
    "end": "1921370"
  },
  {
    "text": "is download times at peak times when they were a lot of schedulers running",
    "start": "1921370",
    "end": "1927220"
  },
  {
    "text": "and internal lot of executives being created we still ended up choking the",
    "start": "1927220",
    "end": "1932260"
  },
  {
    "text": "artifact caches and the average download time went from 30 seconds to something",
    "start": "1932260",
    "end": "1940690"
  },
  {
    "text": "around 20 minutes and once again and at peak times it was very difficult to",
    "start": "1940690",
    "end": "1946480"
  },
  {
    "text": "operate seagull so what did we do next",
    "start": "1946480",
    "end": "1952590"
  },
  {
    "text": "we realize that we have 200 are three attacks dog machines which have immense",
    "start": "1952590",
    "end": "1958690"
  },
  {
    "text": "bandwidth if used the right way so we started thinking on a distributed",
    "start": "1958690",
    "end": "1964510"
  },
  {
    "text": "artifact hosting scheme where we would use our entire cluster instead of the",
    "start": "1964510",
    "end": "1970480"
  },
  {
    "text": "dedicated 9 machines to host artifacts the good thing about the scheme is that",
    "start": "1970480",
    "end": "1977620"
  },
  {
    "text": "it would scale as we would scale or cluster we put in more nodes to our",
    "start": "1977620",
    "end": "1982780"
  },
  {
    "text": "cluster the artifact will have more nodes to hold of course the artifacts as well and this to give you some",
    "start": "1982780",
    "end": "1990540"
  },
  {
    "text": "approximate numbers using the centralized caching scheme where we had nine dedicated cash which replicated all",
    "start": "1990540",
    "end": "1998440"
  },
  {
    "text": "the artifacts we were allowing approximately 30 megabits per second to",
    "start": "1998440",
    "end": "2004950"
  },
  {
    "text": "each executive 30 megabits per second download speed each of the executives and that's just a bit just based on the",
    "start": "2004950",
    "end": "2012450"
  },
  {
    "text": "calculation that we have nine machines etoh could support has a bandwidth of 10",
    "start": "2012450",
    "end": "2017940"
  },
  {
    "text": "gigabits per second and when we when that's this bandwidth is spread across",
    "start": "2017940",
    "end": "2023570"
  },
  {
    "text": "3,000 executives it comes out to be roughly 30 megabits per executor however",
    "start": "2023570",
    "end": "2029550"
  },
  {
    "text": "using the distributed caching approach we now have two hundred machines each of which has a bandwidth of 10 gigabits per",
    "start": "2029550",
    "end": "2036510"
  },
  {
    "text": "second and now the bandwidth per executor is much much higher it's somewhere around 666 megabits per second",
    "start": "2036510",
    "end": "2046549"
  },
  {
    "text": "so let's consider a simple example of how this works let's say that we have",
    "start": "2046640",
    "end": "2052200"
  },
  {
    "start": "2047000",
    "end": "2047000"
  },
  {
    "text": "four slaves in the cluster we do the Jenkins build which creates",
    "start": "2052200",
    "end": "2058559"
  },
  {
    "text": "the artifact and then it invokes this random selector which identifies at",
    "start": "2058559",
    "end": "2063658"
  },
  {
    "text": "random to machines out of these four machines to host the artifact a1 so in",
    "start": "2063659",
    "end": "2069450"
  },
  {
    "text": "this case it uploads the artifact to a one and A three so now for the seagull",
    "start": "2069450",
    "end": "2077398"
  },
  {
    "text": "run that is whenever an executor for scheduler see one needs to download the",
    "start": "2077399",
    "end": "2084299"
  },
  {
    "text": "artifact it will go to one of say one or slave three this scheme ended up being",
    "start": "2084299",
    "end": "2092520"
  },
  {
    "text": "very scalable and the best part was that we didn't need to maintain extra machines so compared to the earlier",
    "start": "2092520",
    "end": "2099299"
  },
  {
    "text": "scheme where we are replicating all our belts on the nine central artifact caches we significantly we very",
    "start": "2099299",
    "end": "2108029"
  },
  {
    "text": "frequently ran into outer space district discussions because every single build",
    "start": "2108029",
    "end": "2113250"
  },
  {
    "text": "will have to be uploaded to each of the nine machines and we significantly",
    "start": "2113250",
    "end": "2120599"
  },
  {
    "text": "improved the download times as well the special shows the average download time",
    "start": "2120599",
    "end": "2126180"
  },
  {
    "start": "2123000",
    "end": "2123000"
  },
  {
    "text": "once again for ten minutes intervals for 24 hours at the tops and on the bottom",
    "start": "2126180",
    "end": "2133890"
  },
  {
    "text": "we are showing the number of downloads occurring in each of the 10 minutes for the same period and it's evident from",
    "start": "2133890",
    "end": "2141569"
  },
  {
    "text": "this figure that as we go from 0 downloads to around 1,000 downloads our",
    "start": "2141569",
    "end": "2147089"
  },
  {
    "text": "average download time only increased by a factor of 2 so it usually was around",
    "start": "2147089",
    "end": "2154579"
  },
  {
    "text": "10 seconds so it would go up to 20 seconds which was a significant",
    "start": "2154579",
    "end": "2159930"
  },
  {
    "text": "improvement over all the previous things that we have tried so at this time he",
    "start": "2159930",
    "end": "2167250"
  },
  {
    "text": "thought can we improve on this and it turns out that peak times have some",
    "start": "2167250",
    "end": "2175680"
  },
  {
    "start": "2173000",
    "end": "2173000"
  },
  {
    "text": "nasty characteristics but they can also be helpful that sometimes so during peak",
    "start": "2175680",
    "end": "2181710"
  },
  {
    "text": "times what ended up happening was that they would be a there would be lots of downloads and most artifacts",
    "start": "2181710",
    "end": "2189380"
  },
  {
    "text": "would end up being downloaded on almost ninety percent of the slaves so we",
    "start": "2189380",
    "end": "2194660"
  },
  {
    "text": "thought once machine downloads an artifact it should really start serving",
    "start": "2194660",
    "end": "2200030"
  },
  {
    "text": "download requests for that artifact the disadvantage for it is that we need to",
    "start": "2200030",
    "end": "2207260"
  },
  {
    "text": "do some recent bookkeeping but as we share later the benefits that we got out",
    "start": "2207260",
    "end": "2214010"
  },
  {
    "text": "of it they far outweigh the booking beekeeping disadvantage that it had so",
    "start": "2214010",
    "end": "2220970"
  },
  {
    "text": "here's an example which demonstrate how does stealing actually works so Jenkins does the build and it randomly selects",
    "start": "2220970",
    "end": "2229180"
  },
  {
    "text": "one slave in this case to upload artifact a two in the next time strap",
    "start": "2229180",
    "end": "2237410"
  },
  {
    "text": "there's an executor that Slayer that's for scheduler see to that starts on",
    "start": "2237410",
    "end": "2243950"
  },
  {
    "text": "slave to this executor needs to fetch artifact a2 and it goes to slave for to",
    "start": "2243950",
    "end": "2252560"
  },
  {
    "text": "fetch their artifact once this happens",
    "start": "2252560",
    "end": "2259730"
  },
  {
    "text": "we get two more schedulers two more executives that start on slave one and",
    "start": "2259730",
    "end": "2265580"
  },
  {
    "text": "save three for the same scheduler both of these executives also need to fetch",
    "start": "2265580",
    "end": "2271400"
  },
  {
    "text": "artifact a2 without skills stealing both of them would go to the original slave",
    "start": "2271400",
    "end": "2278180"
  },
  {
    "text": "which had the artifact which has slave 4 in this case but his out stealing would",
    "start": "2278180",
    "end": "2284930"
  },
  {
    "text": "help slave 3 would still go to slave for to fetch the artifact a2 however slave",
    "start": "2284930",
    "end": "2291440"
  },
  {
    "text": "one would go to slave to instead of going to slave 4 and steal the artifact",
    "start": "2291440",
    "end": "2300010"
  },
  {
    "text": "so this is this was this was a way to distribute the request once the seagull",
    "start": "2300010",
    "end": "2306200"
  },
  {
    "text": "schedulers the seagull executor start and this start downloading artifacts from other machines and we saw",
    "start": "2306200",
    "end": "2314930"
  },
  {
    "text": "significant improve in average download times for especially for artifacts so the top figure shows the",
    "start": "2314930",
    "end": "2321809"
  },
  {
    "text": "artifact steel time which is really the artifact download time but for from",
    "start": "2321809",
    "end": "2328289"
  },
  {
    "text": "places where it wasn't originally uploaded to and the bottom figure once",
    "start": "2328289",
    "end": "2333689"
  },
  {
    "text": "again shows the number of Steel which is the number of downloads for that 10-minute period it's it's very good to",
    "start": "2333689",
    "end": "2342029"
  },
  {
    "text": "notice that in this case we can go from zero downloads to up to 600 downloads",
    "start": "2342029",
    "end": "2348779"
  },
  {
    "text": "and the average download or steal time only increased from four seconds up to",
    "start": "2348779",
    "end": "2354660"
  },
  {
    "text": "like seven seconds this is a visualization which will show how the",
    "start": "2354660",
    "end": "2362489"
  },
  {
    "start": "2358000",
    "end": "2358000"
  },
  {
    "text": "artifact download requests are distributed over entire cluster so each",
    "start": "2362489",
    "end": "2368489"
  },
  {
    "text": "of these rectangles represent segal slave a bright green color means that in",
    "start": "2368489",
    "end": "2376199"
  },
  {
    "text": "that two-minute period which each of the frame represents so grey bright green",
    "start": "2376199",
    "end": "2381449"
  },
  {
    "text": "color represents that in that two-minute period that they did not get any download request whereas a bright red",
    "start": "2381449",
    "end": "2388859"
  },
  {
    "text": "color means that during that two-minute period that slave got around eight",
    "start": "2388859",
    "end": "2394829"
  },
  {
    "text": "downloads and the blue bar on the right hand side shows the average download",
    "start": "2394829",
    "end": "2400709"
  },
  {
    "text": "time across the entire cluster so this visualization starts from around 1230pm",
    "start": "2400709",
    "end": "2407609"
  },
  {
    "text": "and it goes up to like 7pm so when it starts we don't have a lot of runs going",
    "start": "2407609",
    "end": "2413369"
  },
  {
    "text": "on and there are not a lot of requests for artifacts but as we hit the peak times that is around three o'clock three",
    "start": "2413369",
    "end": "2420209"
  },
  {
    "text": "to four o'clock we get lots of requests and this appear to be fairly well distributed over the cluster we cannot",
    "start": "2420209",
    "end": "2427559"
  },
  {
    "text": "achieve perfect load balance over here because it's not a central load balancing scheme that is at play but if",
    "start": "2427559",
    "end": "2433739"
  },
  {
    "text": "you look at the average download times or average download time we're never went above 20 seconds after using this",
    "start": "2433739",
    "end": "2441179"
  },
  {
    "text": "scheme so now shifting gears I'll now",
    "start": "2441179",
    "end": "2446939"
  },
  {
    "text": "come to the last part which is explaining how did we auto scale cluster",
    "start": "2446939",
    "end": "2452310"
  },
  {
    "text": "and how do we make Segal fault-tolerant",
    "start": "2452310",
    "end": "2457100"
  },
  {
    "start": "2458000",
    "end": "2458000"
  },
  {
    "text": "the number of seagull runs that we get changes with time as I mentioned earlier",
    "start": "2460820",
    "end": "2467330"
  },
  {
    "text": "when it's daytime in the u.s. we get lots of seiga runs which also points out that we don't need to maintain a 200",
    "start": "2467330",
    "end": "2474810"
  },
  {
    "text": "instance cluster throughout the day and night we can easily scale it down and",
    "start": "2474810",
    "end": "2480270"
  },
  {
    "text": "then bring it up to start with we use the default auto scaling policy provided",
    "start": "2480270",
    "end": "2486570"
  },
  {
    "text": "by AWS but we run into a few problems with it because it didn't allow us to",
    "start": "2486570",
    "end": "2493320"
  },
  {
    "text": "specify what slave we want to terminate but you might think why would we want",
    "start": "2493320",
    "end": "2500730"
  },
  {
    "text": "this ability to tell the policy that these are the safes that I want to dominate well it happens so that mrs.",
    "start": "2500730",
    "end": "2508470"
  },
  {
    "text": "uses a first-in-first-out strategy to assign work to slaves and the default",
    "start": "2508470",
    "end": "2515250"
  },
  {
    "text": "auto-scaling policy chooses slaves to terminate based on Pfeifer strategy as",
    "start": "2515250",
    "end": "2522240"
  },
  {
    "text": "well so what would end up happening is let's say if we have ten percent of",
    "start": "2522240",
    "end": "2528240"
  },
  {
    "text": "slaves that are actually running tests and the remaining ninety percent of the machines are sitting idle and the auto",
    "start": "2528240",
    "end": "2535170"
  },
  {
    "text": "scaling policy triggers it says okay I am going to terminate ten percent of the slaves what it will end up doing is it",
    "start": "2535170",
    "end": "2543060"
  },
  {
    "text": "will choose those ten percent slaves then the only slaves that are doing any work and it would terminate them so we",
    "start": "2543060",
    "end": "2550860"
  },
  {
    "text": "need to find a way around it and",
    "start": "2550860",
    "end": "2555560"
  },
  {
    "text": "okay please",
    "start": "2560880",
    "end": "2565829"
  },
  {
    "text": "okay that's good that's right now so really we defined this notion of",
    "start": "2571730",
    "end": "2577320"
  },
  {
    "start": "2572000",
    "end": "2572000"
  },
  {
    "text": "reserved memory the CPU and memory demand for a test is very volatile so",
    "start": "2577320",
    "end": "2583440"
  },
  {
    "text": "what we what we told Siegel was that seagull should tell me sauce to reserve",
    "start": "2583440",
    "end": "2589710"
  },
  {
    "text": "the maximum amount of memory a task ever requires let's denote this by our I am",
    "start": "2589710",
    "end": "2596750"
  },
  {
    "text": "given our I the total memory required to run a set T of tasks that are currently",
    "start": "2596750",
    "end": "2604080"
  },
  {
    "text": "running on the entire cluster we can simply calculated by summing up all the",
    "start": "2604080",
    "end": "2610410"
  },
  {
    "text": "our eyes over the set T and once again set T represents all the tasks that are",
    "start": "2610410",
    "end": "2615900"
  },
  {
    "text": "currently running on the entire cluster and if we define em I to be the total",
    "start": "2615900",
    "end": "2625020"
  },
  {
    "text": "available memory for slave I and set s to denote all the slaves present inside",
    "start": "2625020",
    "end": "2632760"
  },
  {
    "text": "our cluster the total available memory of our cluster would be the sum of mi",
    "start": "2632760",
    "end": "2639200"
  },
  {
    "text": "over the set s so given that we now can",
    "start": "2639200",
    "end": "2644880"
  },
  {
    "text": "define gull load which is the metric that we use for auto scaling as the",
    "start": "2644880",
    "end": "2650370"
  },
  {
    "text": "ratio of total reserved memory to total available memory or some of our I over",
    "start": "2650370",
    "end": "2659010"
  },
  {
    "text": "set t upon some of em is over set s so",
    "start": "2659010",
    "end": "2668580"
  },
  {
    "text": "this is typical behavior of gun load this is how the value for gun load",
    "start": "2668580",
    "end": "2674550"
  },
  {
    "text": "changes over a single day when its peak time around four p.m. or after 4pm we",
    "start": "2674550",
    "end": "2681870"
  },
  {
    "text": "are running a lot of schedulers and in turn we are running a lot of executives which which reserve a lot of memory so",
    "start": "2681870",
    "end": "2690180"
  },
  {
    "text": "this ratio is pretty high but at other times this ratio is less than 0.5 so",
    "start": "2690180",
    "end": "2700980"
  },
  {
    "start": "2700000",
    "end": "2700000"
  },
  {
    "text": "what do we do we trigger an auto scanning policy every 10 minutes and the first thing it",
    "start": "2700980",
    "end": "2708110"
  },
  {
    "text": "supposed to do is to calculate the gull load for our cluster if the gull load is",
    "start": "2708110",
    "end": "2714790"
  },
  {
    "text": "greater than 0.5 and it's less than 0.9 we say that we are instead of agri",
    "start": "2714790",
    "end": "2722120"
  },
  {
    "text": "librium we don't need to do anything we have just sufficient resources to entertain any incoming seagull runs and",
    "start": "2722120",
    "end": "2731200"
  },
  {
    "text": "we don't need to really do anything but if girl load is greater than 0.9 that",
    "start": "2731200",
    "end": "2737720"
  },
  {
    "text": "means that we can soon end up in a case where we don't have enough resources to",
    "start": "2737720",
    "end": "2742820"
  },
  {
    "text": "run all the executives that we need to run and if that happens we had ten percent extra machines however once gull",
    "start": "2742820",
    "end": "2753650"
  },
  {
    "text": "loads gulp load falls below 0.5 we are",
    "start": "2753650",
    "end": "2759280"
  },
  {
    "text": "maintaining we realize that we may we are maintaining slaves that we don't need to and how do we get rid of these",
    "start": "2759280",
    "end": "2766460"
  },
  {
    "text": "slaves we calculate the gull load for every machine in the cluster which is",
    "start": "2766460",
    "end": "2771770"
  },
  {
    "text": "the summation of our I that is we sum up the reserved memory for all the tasks",
    "start": "2771770",
    "end": "2778250"
  },
  {
    "text": "all the executives that are running on this specific machine and we / mi which",
    "start": "2778250",
    "end": "2784610"
  },
  {
    "text": "is the total available memory on that machine we do this for all the machines we sought the slaves based on their gun",
    "start": "2784610",
    "end": "2793130"
  },
  {
    "text": "Lord and we select ten percent of the machines which have the lowest gull load",
    "start": "2793130",
    "end": "2800740"
  },
  {
    "text": "and we end up terminating them Mossad is on we wait for another 10 minutes and we",
    "start": "2800740",
    "end": "2806930"
  },
  {
    "text": "repeat the same process again we started",
    "start": "2806930",
    "end": "2813290"
  },
  {
    "text": "out with all reserved instances because and we were developing seeger we didn't really want it to be fall Tyler",
    "start": "2813290",
    "end": "2819230"
  },
  {
    "text": "colonists right at that time so we went for his urban instances but when we moved sequel to production it was",
    "start": "2819230",
    "end": "2826070"
  },
  {
    "text": "becoming too expensive and we shipped it to all spot instances while we were doing that we always",
    "start": "2826070",
    "end": "2831859"
  },
  {
    "text": "knew that there could be a bad day and one day the bad day actually happened we",
    "start": "2831859",
    "end": "2837289"
  },
  {
    "text": "came to work and we realized that all the machines were gone and no one could run any tests there was not exciting so",
    "start": "2837289",
    "end": "2846289"
  },
  {
    "text": "from that day on we made two separate auto-scaling ropes one was for on-demand instances and the other one was for spot",
    "start": "2846289",
    "end": "2854059"
  },
  {
    "text": "instances right now we roughly have 25 instances 25% of the instances belong to",
    "start": "2854059",
    "end": "2861019"
  },
  {
    "text": "the on demand or a scaling group and the rest of the seventy five percent our spot instances so let sustain gears and",
    "start": "2861019",
    "end": "2871489"
  },
  {
    "start": "2868000",
    "end": "2868000"
  },
  {
    "text": "look at how do we actually provide fault ordinance it's very important for us to have fault tolerance because we have a",
    "start": "2871489",
    "end": "2878239"
  },
  {
    "text": "lot of fires at yell interacting given that we are interacting with lots of systems so there are two kind of levels",
    "start": "2878239",
    "end": "2887989"
  },
  {
    "text": "at which we provide fault tolerance the way I see it is the first level is the",
    "start": "2887989",
    "end": "2893420"
  },
  {
    "text": "hardware level which is sort of the preventive fault tolerance and the other",
    "start": "2893420",
    "end": "2898609"
  },
  {
    "text": "the other the other level is the infrastructure level which is the corrective fault tolerance so in other",
    "start": "2898609",
    "end": "2904309"
  },
  {
    "text": "words the preventive fault tolerance is preventing faults to happen and the corrective fault all the rest fit is",
    "start": "2904309",
    "end": "2910160"
  },
  {
    "text": "what infrastructure ensures is basically when faults happen how to delete with",
    "start": "2910160",
    "end": "2916009"
  },
  {
    "text": "them so part of getting the preventive",
    "start": "2916009",
    "end": "2922130"
  },
  {
    "start": "2918000",
    "end": "2918000"
  },
  {
    "text": "fault tolerance is equally dividing our seagull slaves amongst different az's",
    "start": "2922130",
    "end": "2928220"
  },
  {
    "text": "right now all our seagull machines are hosted in u.s. west to region and 60 out",
    "start": "2928220",
    "end": "2935539"
  },
  {
    "text": "of the two hundred machines are currently in u.s. best to a whereas us",
    "start": "2935539",
    "end": "2941450"
  },
  {
    "text": "best to be and us best to see each host 66 machines and the thing I really liked",
    "start": "2941450",
    "end": "2949220"
  },
  {
    "text": "about AWS was about using these ec2 instances there are never a lot of times",
    "start": "2949220",
    "end": "2954529"
  },
  {
    "text": "when since we are running darker at scale and we are interacting with other which is that will end up doing",
    "start": "2954529",
    "end": "2961339"
  },
  {
    "text": "something bad with individual slaves and it's very easy to terminate a slave in",
    "start": "2961339",
    "end": "2967069"
  },
  {
    "text": "in an ec2 and the auto scaling group will bring up a fresh slave and you can",
    "start": "2967069",
    "end": "2973250"
  },
  {
    "text": "just continue doing which can just continue doing its work in the event of",
    "start": "2973250",
    "end": "2979930"
  },
  {
    "text": "losing spot instances are seagull runs they keep on running although their",
    "start": "2979930",
    "end": "2986059"
  },
  {
    "text": "performance can degrade depending on how many resources we currently have available but a usual way of dealing",
    "start": "2986059",
    "end": "2992059"
  },
  {
    "text": "with losing spot instances is to increase the number of instances in our on-demand auto scaling rope and once we",
    "start": "2992059",
    "end": "2999289"
  },
  {
    "text": "have the spot instances back we get the on-demand instances we terminate the",
    "start": "2999289",
    "end": "3004599"
  },
  {
    "text": "on-demand instances and we get some more spot instances to just save some money",
    "start": "3004599",
    "end": "3010859"
  },
  {
    "text": "so there are a lot of times when things go wrong and some of the reasons that",
    "start": "3014579",
    "end": "3021640"
  },
  {
    "text": "make things go wrong is there is a bad service we are interacting with in with",
    "start": "3021640",
    "end": "3026680"
  },
  {
    "text": "a service we send a request and we get a timeout or we keep on waiting or we",
    "start": "3026680",
    "end": "3033220"
  },
  {
    "text": "reuse gawker a lot at any given time a slave a single machine can be running",
    "start": "3033220",
    "end": "3039250"
  },
  {
    "text": "100 docker containers simultaneously and we can we can be done more than a",
    "start": "3039250",
    "end": "3045250"
  },
  {
    "text": "million containers in a single day so that can sometimes be problematic and",
    "start": "3045250",
    "end": "3051119"
  },
  {
    "text": "it's also the fact that we have a lot of external partners and they can have their bad days as well so that's also a",
    "start": "3051119",
    "end": "3057250"
  },
  {
    "text": "source of causing failures inside seagull so what do we do infrastructure",
    "start": "3057250",
    "end": "3062920"
  },
  {
    "text": "inside seagull to deal with these failures so there is this entity inside the scheduler which we call as task",
    "start": "3062920",
    "end": "3070359"
  },
  {
    "text": "manager all it does is it tracks the lifecycle of each executor or task and",
    "start": "3070359",
    "end": "3077009"
  },
  {
    "text": "it retry is running a task if it fails on timeouts a specific number of times",
    "start": "3077009",
    "end": "3083529"
  },
  {
    "text": "which the user can specify so I'm going",
    "start": "3083529",
    "end": "3090220"
  },
  {
    "text": "to demonstrate how we provide fault-tolerance in Siegel from an",
    "start": "3090220",
    "end": "3095980"
  },
  {
    "text": "infrastructure point of view the terminology is the same as I've used",
    "start": "3095980",
    "end": "3101140"
  },
  {
    "text": "before but I'm just adding one new entity to it which is the task manager and it resides inside the scheduler and",
    "start": "3101140",
    "end": "3108100"
  },
  {
    "text": "it's cracking the life cycle of each executive that a scheduler is running",
    "start": "3108100",
    "end": "3114430"
  },
  {
    "text": "it's saying it tells it can tell you if it's cued running finished or if it's",
    "start": "3114430",
    "end": "3120180"
  },
  {
    "text": "timed out so here's one example a young",
    "start": "3120180",
    "end": "3126040"
  },
  {
    "start": "3122000",
    "end": "3122000"
  },
  {
    "text": "developer comes in he's does a seagull run one seed once the second one starts",
    "start": "3126040",
    "end": "3132280"
  },
  {
    "text": "it creates a scheduler and the scheduler creates the two bundles that contain the",
    "start": "3132280",
    "end": "3137980"
  },
  {
    "text": "tests that need to run and a task manager the task manager is supposed to keep track of the life cycle for each of",
    "start": "3137980",
    "end": "3145690"
  },
  {
    "text": "the two bundles that you can see mrs. makes offer to scheduler c14 for",
    "start": "3145690",
    "end": "3153880"
  },
  {
    "text": "resources coming from slave s1 the scheduler accepts the resources and",
    "start": "3153880",
    "end": "3159690"
  },
  {
    "text": "resource in turn launches the executives on slave s 1 so there is there is a",
    "start": "3159690",
    "end": "3167440"
  },
  {
    "text": "subtle thing to notice over here so I've labeled s1 to be in u.s. west to a and",
    "start": "3167440",
    "end": "3172450"
  },
  {
    "text": "slave s2 to be in u.s. west to be so let's say if something happens there's a",
    "start": "3172450",
    "end": "3178300"
  },
  {
    "text": "fire in u.s. west 2a which causes both the bundles or the executives that were",
    "start": "3178300",
    "end": "3184300"
  },
  {
    "text": "running on slave 1 to crash once that happened vsauce vsauce realizes it okay",
    "start": "3184300",
    "end": "3191230"
  },
  {
    "text": "something went wrong on slave s 1 I need to inform scheduler see one that it's",
    "start": "3191230",
    "end": "3197950"
  },
  {
    "text": "both of its bundles are now gone so it sends an update to the task manager that",
    "start": "3197950",
    "end": "3203680"
  },
  {
    "text": "both your bundles are gone the task manager determines what was the state of",
    "start": "3203680",
    "end": "3209350"
  },
  {
    "text": "these bundles they were running and it determines if those bundles need to rerun again let's say in this case it",
    "start": "3209350",
    "end": "3217690"
  },
  {
    "text": "determines that they need to rerun may sauce master makes another offer to the",
    "start": "3217690",
    "end": "3223840"
  },
  {
    "text": "scheduler from slave s two and hopefully us since",
    "start": "3223840",
    "end": "3229390"
  },
  {
    "text": "they've as two is in u.s. west to be and hopefully it's not on fire and it's able",
    "start": "3229390",
    "end": "3234730"
  },
  {
    "text": "to run des our scheduler except the resources from slave s two and mrs.",
    "start": "3234730",
    "end": "3240460"
  },
  {
    "text": "launches the same executives on slave s too which run and then finished to",
    "start": "3240460",
    "end": "3246880"
  },
  {
    "text": "completion once they complete the report their results and so what did we learn",
    "start": "3246880",
    "end": "3256630"
  },
  {
    "start": "3251000",
    "end": "3251000"
  },
  {
    "text": "in this talk we talked about how Siegel works and interacts with other systems we came to know of an extremely",
    "start": "3256630",
    "end": "3264760"
  },
  {
    "text": "efficient artifact hosting design that is working perfectly for us we also",
    "start": "3264760",
    "end": "3270060"
  },
  {
    "text": "discussed how do we do auto scaling using our custom metric gal load and we",
    "start": "3270060",
    "end": "3276970"
  },
  {
    "text": "also talked about how we use AWS for providing fault tolerance and the",
    "start": "3276970",
    "end": "3283330"
  },
  {
    "text": "infrastructure retry logic that seagull has to deal with failures moving forward",
    "start": "3283330",
    "end": "3291160"
  },
  {
    "start": "3290000",
    "end": "3290000"
  },
  {
    "text": "we really want to open source eagle and we have been trying to do it for the",
    "start": "3291160",
    "end": "3297790"
  },
  {
    "text": "last few months only even as we don't get enough time to sanitize the code cleanup and do everything that's",
    "start": "3297790",
    "end": "3304510"
  },
  {
    "text": "required for open sourcing it but we do really want to do it hopefully we'll do it this quarter and we also want to",
    "start": "3304510",
    "end": "3312070"
  },
  {
    "text": "explore why Amazon we're downloading artifacts from amazon s3 was such a big",
    "start": "3312070",
    "end": "3318370"
  },
  {
    "text": "problem although we do have feeling that once we avoid going through the nat box",
    "start": "3318370",
    "end": "3323650"
  },
  {
    "text": "we are results might improve the download times might improve but we are also planning to explore some",
    "start": "3323650",
    "end": "3330220"
  },
  {
    "text": "alternatives where we host artifacts into multiple buckets and we divide a large artifact into smaller files and",
    "start": "3330220",
    "end": "3337210"
  },
  {
    "text": "then upload to s3 and combine it when we when we download all those small files",
    "start": "3337210",
    "end": "3342990"
  },
  {
    "text": "but we are all we are ready right now we are very excited to work on getting",
    "start": "3342990",
    "end": "3349320"
  },
  {
    "text": "making seagull able to run on multiple instance types right now we only run on",
    "start": "3349320",
    "end": "3355540"
  },
  {
    "text": "our 3 Gitex lodges which is sort of a scarce commodity at easy to",
    "start": "3355540",
    "end": "3361030"
  },
  {
    "text": "it's not as easy to get in spot instance and their prices change a lot and all those things but that's one of our major",
    "start": "3361030",
    "end": "3369480"
  },
  {
    "text": "motive these days and we want to also do that because you want to reduce the cost",
    "start": "3369480",
    "end": "3374890"
  },
  {
    "text": "of a single cluster and one way we think of doing it is by by using the right",
    "start": "3374890",
    "end": "3383500"
  },
  {
    "text": "instance type which is the instance type that would give us the minimum gigabyte",
    "start": "3383500",
    "end": "3388690"
  },
  {
    "text": "per dollar and it's a very interesting field working video we are not aware of",
    "start": "3388690",
    "end": "3393730"
  },
  {
    "text": "a lot of work that has been done in this field but we definitely want to focus on",
    "start": "3393730",
    "end": "3400000"
  },
  {
    "text": "it and a friend of mine who recently co-founded company by the name of Yoda scale they're also trying to attack",
    "start": "3400000",
    "end": "3406119"
  },
  {
    "text": "similar problem and yeah we are very excited to work on it and with that I",
    "start": "3406119",
    "end": "3411819"
  },
  {
    "text": "would remind you to complete your evaluations and I I think there is time",
    "start": "3411819",
    "end": "3419619"
  },
  {
    "text": "for questions after the talk thank you",
    "start": "3419619",
    "end": "3427890"
  }
]