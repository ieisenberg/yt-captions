[
  {
    "start": "0",
    "end": "54000"
  },
  {
    "text": "okay good afternoon everyone you guys",
    "start": "589",
    "end": "6180"
  },
  {
    "text": "having a good day day one or day three depending on how long you've been here or how long it feels depending on how",
    "start": "6180",
    "end": "12840"
  },
  {
    "text": "light you're out last night my name is Graham McAlister I'm a senior principal engineer I",
    "start": "12840",
    "end": "18240"
  },
  {
    "text": "actually do actually work on the stuff not a marketing guy so I should be able to answer most of your questions we're",
    "start": "18240",
    "end": "25199"
  },
  {
    "text": "gonna be talking about RDS Postgres today I'm gonna go over some of the new things that were either released in the",
    "start": "25199",
    "end": "30900"
  },
  {
    "text": "last year or coming up here shortly and then I'm gonna spend a bunch of time really talking about the things that we've learned working with the customers",
    "start": "30900",
    "end": "37230"
  },
  {
    "text": "over the last two years on Postgres and specifically with RDS and really some of the you know best practices some of the",
    "start": "37230",
    "end": "43860"
  },
  {
    "text": "tips you know that'll help if you're using RDS you know help you make your experience that much better so let's",
    "start": "43860",
    "end": "50039"
  },
  {
    "text": "jump right in so no there we go one of the first things that customers have",
    "start": "50039",
    "end": "55739"
  },
  {
    "start": "54000",
    "end": "54000"
  },
  {
    "text": "been asking us for for quite a while is major version upgrade I'm happy to say it's coming out really soon",
    "start": "55739",
    "end": "61140"
  },
  {
    "text": "post reinvent here should be out really soon and it's really simple to use you",
    "start": "61140",
    "end": "66960"
  },
  {
    "text": "basically select a new version you hit the button modify now and you get started so you start with your",
    "start": "66960",
    "end": "72270"
  },
  {
    "text": "production 9-3 instance we take a backup right away because we want to make sure you can always get back to a safe state",
    "start": "72270",
    "end": "78390"
  },
  {
    "text": "then the next thing we do is essentially we're running PG upgrades so any of you guys have been running Postgres for a long time you know what PG upgrade does",
    "start": "78390",
    "end": "84830"
  },
  {
    "text": "this will take about five minutes on a typical database but it you know it is dependent about some of your size and",
    "start": "84830",
    "end": "91439"
  },
  {
    "text": "how many objects you have in it and then of course what we do is we take a backup before we give it back to you because",
    "start": "91439",
    "end": "97829"
  },
  {
    "text": "guess what PG upgrade is not transactional right there is no transactionality during that time when",
    "start": "97829",
    "end": "103020"
  },
  {
    "text": "it's moving stuff around you can't recover to it so we want to make sure we had backups on both sides so you end up",
    "start": "103020",
    "end": "108479"
  },
  {
    "text": "with a nine for database you're all good one of the things that you have to know is that during that time because there's",
    "start": "108479",
    "end": "113670"
  },
  {
    "text": "no transactions you can't ask RDS to restore to it so we make a thing called a blackout window",
    "start": "113670",
    "end": "119009"
  },
  {
    "text": "that just says hey you know you you can go earlier later you can pick one of those backups you just can't go in the",
    "start": "119009",
    "end": "125369"
  },
  {
    "text": "middle now this is how simple it is but of course the best practice of course is not to do this it's to go and make a",
    "start": "125369",
    "end": "132569"
  },
  {
    "text": "test copy of your database go run the upgrade get your test nine for instance get your application and",
    "start": "132569",
    "end": "139200"
  },
  {
    "text": "test it right when everything's good then you want to go and do the real one",
    "start": "139200",
    "end": "145020"
  },
  {
    "text": "and this is because you know there are a few things that PG upgrade will kind of",
    "start": "145020",
    "end": "150060"
  },
  {
    "text": "have a problem with open prepare transactions now we check for a bunch of",
    "start": "150060",
    "end": "155280"
  },
  {
    "text": "these but it's impossible to kind of do all of them and it's also impossible for us to verify that the functionality of",
    "start": "155280",
    "end": "161130"
  },
  {
    "text": "your application wasn't dependent upon some quirk in let's say 9 3 so that's why you really want to do that testing",
    "start": "161130",
    "end": "166290"
  },
  {
    "text": "but you know once you've tested it's a real simple process to do so I said this will coming out very shortly some other",
    "start": "166290",
    "end": "174540"
  },
  {
    "start": "173000",
    "end": "173000"
  },
  {
    "text": "new stuff that we've done we introduced larger storage so before we had three terabytes we now go to 6 terabytes so we",
    "start": "174540",
    "end": "181170"
  },
  {
    "text": "increase that size doubled it the PI ops maximum is still 30,000 right so that",
    "start": "181170",
    "end": "186690"
  },
  {
    "text": "hasn't changed that's mostly based on the instance classes about throughput of iOS but the nice thing is GP too because",
    "start": "186690",
    "end": "193440"
  },
  {
    "text": "you get three iOS per gigabyte what you buy now that you can buy more you can",
    "start": "193440",
    "end": "198900"
  },
  {
    "text": "get more IUP's and more throughput so it will depend on how big you go but you typically we've seen good customers get",
    "start": "198900",
    "end": "205680"
  },
  {
    "text": "good increases on that one of the big ones that I was really excited about was encryption at rest we have this across",
    "start": "205680",
    "end": "210959"
  },
  {
    "text": "the whole artiest line but for Postgres especially it was real real good because a lot of people are trying to get into",
    "start": "210959",
    "end": "216299"
  },
  {
    "text": "more compliance related scenarios with Postgres so this includes all your data",
    "start": "216299",
    "end": "221549"
  },
  {
    "text": "files your log files on the machine all your backups are encrypted and your logs that we ship to s3 for your backups for",
    "start": "221549",
    "end": "228180"
  },
  {
    "text": "point in time now we use our Identity and Access Management key management service kms is the thing that manages",
    "start": "228180",
    "end": "234870"
  },
  {
    "text": "the key so we don't manage them for you they're your keys now you get a default key that you can use I recommend using a",
    "start": "234870",
    "end": "241350"
  },
  {
    "text": "custom keeper database this over time as we get more rich features around this encryption you'll want to have a",
    "start": "241350",
    "end": "248160"
  },
  {
    "text": "separate key for each of your databases because you won't want to have to grant access to you know keys if it's just one",
    "start": "248160",
    "end": "254040"
  },
  {
    "text": "big default for all your databases so that's just kind of the best practice there now one of the things that people",
    "start": "254040",
    "end": "259799"
  },
  {
    "text": "ask me as soon as I told them we have encryption at rest was what's the overhead like you know I really like the",
    "start": "259799",
    "end": "265200"
  },
  {
    "start": "260000",
    "end": "260000"
  },
  {
    "text": "idea of encrypting my data but should I do it so what I did was I ran PG bench read only in memory right now",
    "start": "265200",
    "end": "273150"
  },
  {
    "text": "you'd expect no real overhead I have transactions per second on the vertical and then just you know increase the number of threads along the bottom",
    "start": "273150",
    "end": "279150"
  },
  {
    "text": "so the green is regular the blue is encrypted and what you see is there's basically no difference because you",
    "start": "279150",
    "end": "284940"
  },
  {
    "text": "wouldn't expect it right we're not touching this storage so exactly what",
    "start": "284940",
    "end": "290879"
  },
  {
    "text": "about read/write we're doing a lot of reading a lot of writing we're actually going to see a bit of a change so you'll",
    "start": "290879",
    "end": "296729"
  },
  {
    "text": "notice here that as I start getting up into the 62 32 64 threads I start to see",
    "start": "296729",
    "end": "302639"
  },
  {
    "text": "a little bit of a difference and what we measured it's about five to ten percent now those of you guys how many people",
    "start": "302639",
    "end": "307919"
  },
  {
    "text": "have run PG bench a lot too so it's fairly right intensive when it's",
    "start": "307919",
    "end": "313020"
  },
  {
    "text": "actually in write mode like on this box I think it was doing two or three",
    "start": "313020",
    "end": "318120"
  },
  {
    "text": "thousand i ops writing and only about a thousand reading so it was still doing a lot of i/o that you know has to go",
    "start": "318120",
    "end": "324539"
  },
  {
    "text": "through the encryption system so you may actually see less of an impact depending on how much you know io your applications doing but this one was",
    "start": "324539",
    "end": "330930"
  },
  {
    "text": "actually doing a lot we we're always rolling out new versions we had a bug in",
    "start": "330930",
    "end": "338460"
  },
  {
    "start": "333000",
    "end": "333000"
  },
  {
    "text": "935 in our code around our permissions for already a super user so we fixed that in the 936 version and all the",
    "start": "338460",
    "end": "345090"
  },
  {
    "text": "subsequent ones around reset all we released 939 that's the new default on",
    "start": "345090",
    "end": "350520"
  },
  {
    "text": "93 we released 94 that's been out for almost a year has a lot of great",
    "start": "350520",
    "end": "356729"
  },
  {
    "text": "features that you know like JSON B what's really cool is we saw a huge adoption on nine four so nine three was",
    "start": "356729",
    "end": "362909"
  },
  {
    "text": "still our default we released nine four within two weeks we were getting an equal number of crates on nine fours we",
    "start": "362909",
    "end": "368159"
  },
  {
    "text": "were on nine three the adoptions just been huge customers are just really loving that so each one of these releases we typically update like pl v8",
    "start": "368159",
    "end": "375000"
  },
  {
    "text": "and post GIS so if there's specific stuff you're looking for we document that on the site so nine four four is",
    "start": "375000",
    "end": "381509"
  },
  {
    "text": "our default overall on all our instances",
    "start": "381509",
    "end": "386419"
  },
  {
    "text": "so one of the ones I'm really excited about that we've had a lot of conversations with customers is you know they said I love run an RDS it's really",
    "start": "387889",
    "end": "394469"
  },
  {
    "text": "simple it's really manageable but I can't see the stuff that I want to see on my instance you know I'm used to",
    "start": "394469",
    "end": "399960"
  },
  {
    "text": "being able to go poke and and see all the statistics so this is what we're coming out with this is also",
    "start": "399960",
    "end": "405100"
  },
  {
    "text": "going to come out really shortly after reinvent it's basically enhanced operating system metrics so I won't go",
    "start": "405100",
    "end": "410170"
  },
  {
    "text": "through every one of these but you can see things like CPU utilization not just overall but I can tell now system time I",
    "start": "410170",
    "end": "416350"
  },
  {
    "text": "can tell idle I can tell you know steals from because it's in a virtualized system but the one I'm most excited",
    "start": "416350",
    "end": "422050"
  },
  {
    "text": "about is really the process list this is giving you functionality like you would have with pop on Linux right you'll be",
    "start": "422050",
    "end": "428920"
  },
  {
    "text": "able to see what's vacuum doing you know how much memory is it consuming how much CPU is it consuming and this is",
    "start": "428920",
    "end": "434860"
  },
  {
    "text": "historical so it's at a five-second granularity and you'll be able to look at it over time both on a console and",
    "start": "434860",
    "end": "441100"
  },
  {
    "text": "through an API so you can build your own dashboards if you want or you can you can just use ours and so ours is gonna",
    "start": "441100",
    "end": "447220"
  },
  {
    "text": "you know look like this much like you see with all our metrics you'll be able to graph them you'll be able to put",
    "start": "447220",
    "end": "452530"
  },
  {
    "text": "alarms on them so pretty much what we have like what we have for our standard",
    "start": "452530",
    "end": "457570"
  },
  {
    "text": "metrics but just a lot more detail and a lot more frequent and overtime we'll look at increasing probably the you know",
    "start": "457570",
    "end": "463420"
  },
  {
    "text": "the granularity and the and the statistics we give but I think this is a really big move forward because you know",
    "start": "463420",
    "end": "470140"
  },
  {
    "text": "if you have those spiky kind of things where if somebody comes in and spikes your CPU for 20 seconds today you'll",
    "start": "470140",
    "end": "475780"
  },
  {
    "text": "just see your average go up now you'll actually be able to go and see exactly what happened and what's going on in",
    "start": "475780",
    "end": "481360"
  },
  {
    "text": "your system so really key so let's talk a little bit about what's going on with data movement how many people heard in",
    "start": "481360",
    "end": "488200"
  },
  {
    "text": "the keynote that we've got a nice little new tool good good so I'm not gonna",
    "start": "488200",
    "end": "493810"
  },
  {
    "text": "spend too much time on this because a lot of people probably heard about it but I think this is huge the database",
    "start": "493810",
    "end": "499450"
  },
  {
    "text": "migration service is it allows you to go basically from any of those engines you",
    "start": "499450",
    "end": "505660"
  },
  {
    "text": "know you can mine out of Oracle Postgres rora my sequel and you can go into them so",
    "start": "505660",
    "end": "511000"
  },
  {
    "text": "you can go my sequel to my sequel you could go my sequel to Postgres for example you can go Oracle the Postgres and that's really actually really nice",
    "start": "511000",
    "end": "518740"
  },
  {
    "text": "because we get a lot of customers who are saying you know I need to move data around and this is a migration to",
    "start": "518740",
    "end": "524200"
  },
  {
    "text": "service but it can also be used for replication so there we go so this is",
    "start": "524200",
    "end": "531310"
  },
  {
    "start": "529000",
    "end": "529000"
  },
  {
    "text": "just the standard picture of showing you know if you're on premise and you I do migration you basically you know",
    "start": "531310",
    "end": "537520"
  },
  {
    "text": "you got a VPN into ec2 or an RDS instance you basically ask for a",
    "start": "537520",
    "end": "544170"
  },
  {
    "text": "migration service to be spun up and that's the little cogs there you go",
    "start": "544170",
    "end": "549820"
  },
  {
    "text": "connect it you say hey here's my URL for my source and targets permissions you go",
    "start": "549820",
    "end": "555820"
  },
  {
    "text": "select which tables you can say hey take all my tables take just this one whatever you want and then you basically",
    "start": "555820",
    "end": "562210"
  },
  {
    "text": "do a full data load and this is gonna do a select and basically you know do that so you need to be able to consistent",
    "start": "562210",
    "end": "567640"
  },
  {
    "text": "select of your table and then essentially in the meantime you're generating logs right and in the",
    "start": "567640",
    "end": "573760"
  },
  {
    "text": "background once the full load is done we will catch up with the incrementals and so we'll take all those logs we'll mine",
    "start": "573760",
    "end": "580570"
  },
  {
    "text": "them will stream that over once you're all caught up all you got to do is then switch over to here so it won't be a",
    "start": "580570",
    "end": "586870"
  },
  {
    "text": "zero downtime because you got to start here at stop your application start it but it could be as few seconds for for",
    "start": "586870",
    "end": "592720"
  },
  {
    "text": "that outage now what does this mean for Postgres so today you have to be on nine",
    "start": "592720",
    "end": "601420"
  },
  {
    "start": "597000",
    "end": "597000"
  },
  {
    "text": "four for this to work because we're using the logical replication features built in a nine for so nice thing is we",
    "start": "601420",
    "end": "609790"
  },
  {
    "text": "support major version upgrade you guys can upgrade dear ones the source today can only be ec2 or on-premise we don't",
    "start": "609790",
    "end": "617230"
  },
  {
    "text": "offer RDS at this point as a source but just as a target so as I said it's bulk",
    "start": "617230",
    "end": "622540"
  },
  {
    "text": "copy and then it's changed data capture and if you're interested in that service we have web pages up there you'll be",
    "start": "622540",
    "end": "628330"
  },
  {
    "text": "able to find online to be able to go sign up for the preview and we can get you started so if you got a migration or",
    "start": "628330",
    "end": "634360"
  },
  {
    "text": "some replication that you'd like to do this is really going to be huge so one",
    "start": "634360",
    "end": "639730"
  },
  {
    "start": "639000",
    "end": "639000"
  },
  {
    "text": "of the other things we get is a lot of questions about how is the best practices for loading data so if I'm going to use this kind of tool or I'm",
    "start": "639730",
    "end": "645700"
  },
  {
    "text": "gonna do something myself what do I want to do so it's the normal rules of hey you want to make less work for the",
    "start": "645700",
    "end": "651790"
  },
  {
    "text": "database turn off backups which are with our system it's setting back and pretension to zero you want to turn off the multi a-z you",
    "start": "651790",
    "end": "659710"
  },
  {
    "text": "want to turn off auto vacuum now of course you got to remember to turn these back on or bad things are gonna happen right but it really does make it much",
    "start": "659710",
    "end": "666250"
  },
  {
    "text": "faster if you're using you want to use compressed PG dump restore you can do in parallel but of",
    "start": "666250",
    "end": "672910"
  },
  {
    "text": "course now we have a migration tool that might be able to use instead of these you want to increase your maintenance work memory we leave this at 16 Meg by",
    "start": "672910",
    "end": "680260"
  },
  {
    "text": "default because we don't know what kind of size box and how much you want to give to it so it's definitely one that",
    "start": "680260",
    "end": "685270"
  },
  {
    "text": "will help with both index building as well as vacuuming so you you really want to show and I'll show you some numbers",
    "start": "685270",
    "end": "690760"
  },
  {
    "text": "on that and then the other one is checkpoint segments and checkpoint timeout so in Postgres every time you modify a",
    "start": "690760",
    "end": "699130"
  },
  {
    "text": "block the first time you do it after a checkpoint it has to log the whole block so if your checkpointing too frequently",
    "start": "699130",
    "end": "705370"
  },
  {
    "text": "you might double or triple the amount of wall generation that you have so setting your checkpoint segments is really",
    "start": "705370",
    "end": "710950"
  },
  {
    "text": "important for a lot of reasons replication but as well as data loading and I'll show you some examples that as",
    "start": "710950",
    "end": "717700"
  },
  {
    "text": "well now one that we get told a lot and you see a lot in blogs is disabling F",
    "start": "717700",
    "end": "723670"
  },
  {
    "text": "sync how many people know what that does few folks so what this basically tells",
    "start": "723670",
    "end": "729280"
  },
  {
    "text": "your database to do is you know don't sink it do whatever you want you know take it easy but what that means in",
    "start": "729280",
    "end": "735820"
  },
  {
    "text": "practice is if it crashes if that box crashes well you're in the middle of doing something guess what you have possible corruption",
    "start": "735820",
    "end": "742300"
  },
  {
    "text": "at your database and your database make a mock up and it may look fine and you'll have latent corruption so",
    "start": "742300",
    "end": "748480"
  },
  {
    "text": "this is one of the recommendations that we don't want you to do right instead what we want you to do is disable",
    "start": "748480",
    "end": "754180"
  },
  {
    "text": "synchronous commit when you're doing data loading and I'll show you why and that basically tells the database do it",
    "start": "754180",
    "end": "760690"
  },
  {
    "text": "in a safe manner but do it when you want to and don't return you know you're just",
    "start": "760690",
    "end": "767230"
  },
  {
    "text": "basically not telling it to return to you and block on those things but it's still all syncing to disk",
    "start": "767230",
    "end": "775079"
  },
  {
    "text": "okay so I did a benchmark again where I went and did a ran a workload to do",
    "start": "777030",
    "end": "785170"
  },
  {
    "text": "inserts into a database so again transactions per second is on the vertical axis so bigger is better",
    "start": "785170",
    "end": "790330"
  },
  {
    "text": "and I started with 16 checkpoint segments and and that's in blue and the 256 in purple so when we have both of",
    "start": "790330",
    "end": "797710"
  },
  {
    "text": "those things turned off like you're gone you're doing F thinking you're doing synchro commit pretty much no difference now we",
    "start": "797710",
    "end": "805000"
  },
  {
    "text": "turn off F Sync and this is why the recommendation existed for so many years to go do this you get a dramatic",
    "start": "805000",
    "end": "811030"
  },
  {
    "text": "improvement in performance right but guess what if we just turn off synchronous commit I'm actually get a better performance",
    "start": "811030",
    "end": "817120"
  },
  {
    "text": "booth but of course the first time I showed this to someone someone said well what else have you turned them both off so of course we tested that as well and",
    "start": "817120",
    "end": "824140"
  },
  {
    "text": "you see a small performance improvement over just turning synchronous Chamitoff but it's 4% how many people think",
    "start": "824140",
    "end": "830860"
  },
  {
    "text": "getting 4% faster data loading is worth the chance of corrupting your database how many people's bosses think that's a",
    "start": "830860",
    "end": "838120"
  },
  {
    "text": "good thing that's not the conversation I ever want to have with my boss that well yeah I got the data load done 4% faster",
    "start": "838120",
    "end": "844360"
  },
  {
    "text": "but we don't know if the data is really in there or not so now of course the big",
    "start": "844360",
    "end": "850000"
  },
  {
    "text": "win is just turning off synchronous commit we've got a hundred percent improvement right more than a hundred percent so that's not typically how you",
    "start": "850000",
    "end": "857050"
  },
  {
    "text": "load data a normally do it be a copy or some bulk command so I did a bulk load this time it's time on the vertical axis",
    "start": "857050",
    "end": "862990"
  },
  {
    "text": "so shorter is better now notice the big difference here the 256 checkpoint",
    "start": "862990",
    "end": "868690"
  },
  {
    "text": "segments made a really large in decrease right we haven't done anything else all we've done has had more checkpoint",
    "start": "868690",
    "end": "874300"
  },
  {
    "text": "segments when we go turn off F sync we don't see much of a change at all and",
    "start": "874300",
    "end": "879580"
  },
  {
    "text": "actually even synchronous commit you don't see much of a change so depending on what you're doing here changing",
    "start": "879580",
    "end": "886210"
  },
  {
    "text": "different parameters is actually more important we turn both off again 3%",
    "start": "886210",
    "end": "891220"
  },
  {
    "text": "difference again not worth making much of a change even turning off synchronous commit in this point still of might get",
    "start": "891220",
    "end": "897190"
  },
  {
    "text": "a little bit out of it but it's probably not the biggest thing you could have done right increasing your checkpoint segments is more important so talking",
    "start": "897190",
    "end": "905260"
  },
  {
    "text": "about maintenance work memory I wanted to show both that in checkpoint segments along with the F thing so in this case",
    "start": "905260",
    "end": "910780"
  },
  {
    "text": "I'm turning synchronous commit off I just said fine that's the best practice we'll start with that F sync is on on",
    "start": "910780",
    "end": "916390"
  },
  {
    "text": "the one and off on the other so we built the index it took 29 minutes shorter",
    "start": "916390",
    "end": "921460"
  },
  {
    "text": "time better so we start with we go and go from the 16 Meg maintenance work",
    "start": "921460",
    "end": "927760"
  },
  {
    "text": "memory to a gig right big large increase we get some nice improvement in the index performance in the build time",
    "start": "927760",
    "end": "933730"
  },
  {
    "text": "right save us a few minutes so that's much more important than turning F sink off and if we go and add",
    "start": "933730",
    "end": "939640"
  },
  {
    "text": "more checkpoint segments we get another decrease so you can see as we make these tunable changes they're much more",
    "start": "939640",
    "end": "946360"
  },
  {
    "text": "important than turning off F sync and this is my general rule as I always tell people like these are good starting",
    "start": "946360",
    "end": "952810"
  },
  {
    "text": "points but testing is really important because your mileage will vary depends on what kind of system you're running on",
    "start": "952810",
    "end": "958060"
  },
  {
    "text": "how much memory how much I ops one of",
    "start": "958060",
    "end": "963310"
  },
  {
    "start": "961000",
    "end": "961000"
  },
  {
    "text": "the other things that's very interesting about Postgres is vacuuming after data load how many people know that you have",
    "start": "963310",
    "end": "969520"
  },
  {
    "text": "to vacuum after you load data so most of you that have been running for a long time you know this because you know it's",
    "start": "969520",
    "end": "976780"
  },
  {
    "text": "something you learn it's not intuitive that you would need to do that because most people think of vacuuming is being hey it's only when I update or deleted",
    "start": "976780",
    "end": "983590"
  },
  {
    "text": "row that I need to vacuum it but actually you need a vacuum after you've inserted data and I ran this test where",
    "start": "983590",
    "end": "990160"
  },
  {
    "text": "I ran I did a PG bench and I initialize my my database and then I basically ran a read-only workload on it and what you",
    "start": "990160",
    "end": "997240"
  },
  {
    "text": "see in the orange is my CPU and in the green is my right eye ops and what's",
    "start": "997240",
    "end": "1004920"
  },
  {
    "text": "strange is you're running a 100% read workload and you're generating 5,000 right eye ops and it's like what's going",
    "start": "1004920",
    "end": "1011820"
  },
  {
    "text": "on here what's Postgres doing what Postgres needs to do is it needs to clean out some stuff in those blocks so",
    "start": "1011820",
    "end": "1018270"
  },
  {
    "text": "you can do it when it vacuums or it can do it the first time you touch that page but of course that causes everything to",
    "start": "1018270",
    "end": "1024030"
  },
  {
    "text": "slow down even though you're just trying to read data it's writing it out so I stopped the test I did a vacuum and you",
    "start": "1024030",
    "end": "1030688"
  },
  {
    "text": "can see my ops go up a lot because I'm writing out all those blocks and then I reran the benchmark and now I'm getting",
    "start": "1030689",
    "end": "1037470"
  },
  {
    "text": "over twice the TPS because I'm not doing any any writing so analyzing and",
    "start": "1037470",
    "end": "1044579"
  },
  {
    "text": "vacuuming after data loading really important again so how many people have",
    "start": "1044579",
    "end": "1051930"
  },
  {
    "start": "1049000",
    "end": "1049000"
  },
  {
    "text": "had a transit at transaction ID wrap-around if you've ever had one you'll never forget it will you so this",
    "start": "1051930",
    "end": "1058500"
  },
  {
    "text": "is super painful you get two billion transactions in Postgres I don't know why they picked that I read the original",
    "start": "1058500",
    "end": "1064140"
  },
  {
    "text": "paper and it was you know it's pretty funny cuz you know at that time it probably sounded like a very big number",
    "start": "1064140",
    "end": "1069180"
  },
  {
    "text": "nowadays it doesn't sound like a very big number at all right if you if you run out of if you run out of the 2 billion transactions basically at a what",
    "start": "1069180",
    "end": "1076170"
  },
  {
    "text": "is it a million to go it basically causes you to stop your database go in a single user mode and one by one in",
    "start": "1076170",
    "end": "1082320"
  },
  {
    "text": "serial vacuum each of the tables you can imagine that is not a great experience you have your boss being like when's the",
    "start": "1082320",
    "end": "1089190"
  },
  {
    "text": "database coming back up and you're like I don't know because it's just got to work its way serially through them so you don't want that to happen so you",
    "start": "1089190",
    "end": "1095610"
  },
  {
    "text": "actually want to have to look at your vacuum parameters now some really good talks from some of the Postgres core",
    "start": "1095610",
    "end": "1100680"
  },
  {
    "text": "guys on this I'm not going to go into a super a lot of detail because you can spend a whole hour talking about vacuum but I will tell you that these are the",
    "start": "1100680",
    "end": "1106740"
  },
  {
    "text": "parameters you really want to think about you want to think about your scale factor your vacuum thresholds the number",
    "start": "1106740",
    "end": "1111810"
  },
  {
    "text": "of workers you have and really make sure that it's keeping up and one of the things that's not obvious is that",
    "start": "1111810",
    "end": "1118110"
  },
  {
    "text": "anything is a transaction like doing a primary key violation right even though",
    "start": "1118110",
    "end": "1124500"
  },
  {
    "text": "you didn't change any data will cause a transaction ID to increment so we've had customers that basically we're running",
    "start": "1124500",
    "end": "1130620"
  },
  {
    "text": "through a large number of transaction IDs because they were hitting you know key violations and they were like we're",
    "start": "1130620",
    "end": "1136140"
  },
  {
    "text": "not doing that much you know how do we run through 2 billion of these you can do a lot of violations very quickly so",
    "start": "1136140",
    "end": "1141390"
  },
  {
    "text": "that's one of the things to really look at one of the cool things we added",
    "start": "1141390",
    "end": "1146490"
  },
  {
    "text": "around moving data was support for the Postgres ftw so you can connect to another RDS instance you can connect to something",
    "start": "1146490",
    "end": "1152550"
  },
  {
    "text": "like redshift now your mileage may vary with redshift because it's a you know an older version of the front end for",
    "start": "1152550",
    "end": "1157890"
  },
  {
    "text": "Postgres it's an eighth version but I wanted to show it here because it's kind of kind of interesting or - an ec2",
    "start": "1157890",
    "end": "1163800"
  },
  {
    "text": "instance so with the RDS superuser you have the privilege to go create the extension Postgres FTW you do that and",
    "start": "1163800",
    "end": "1170580"
  },
  {
    "text": "you basically create a server and you give it you know the URL of the of your instance now I'm using the cname here of",
    "start": "1170580",
    "end": "1177360"
  },
  {
    "text": "my redshift instance that works really well for both non VPC like classic",
    "start": "1177360",
    "end": "1183300"
  },
  {
    "text": "instances or a private V PC if you're if your instance is publicly accessible the",
    "start": "1183300",
    "end": "1188850"
  },
  {
    "text": "target you'll probably have to use the IP address because we have some kind of funny stuff going on with our DNS it's",
    "start": "1188850",
    "end": "1194760"
  },
  {
    "text": "something we're working on but it's not something that works today so it's just a heads up you it won't work if you try",
    "start": "1194760",
    "end": "1199860"
  },
  {
    "text": "to use the cname so but once you've got that created you can select from the tables and you can insert so I've seen people do stuff",
    "start": "1199860",
    "end": "1206850"
  },
  {
    "text": "where they like hey look I want to push a little bit of data to redshift that works fine but the better use case is",
    "start": "1206850",
    "end": "1212519"
  },
  {
    "text": "where folks are wiring in redshift to get some of their data out of their analytic stuff and present it back to",
    "start": "1212519",
    "end": "1217769"
  },
  {
    "text": "their like OLTP system for example or just to another RDS instance one of the",
    "start": "1217769",
    "end": "1225960"
  },
  {
    "start": "1225000",
    "end": "1225000"
  },
  {
    "text": "other things we added support for is the session replication role I love this thing because it's not an actual role it's called the role and this is used by",
    "start": "1225960",
    "end": "1233639"
  },
  {
    "text": "a lot of the trigger based replication tools like bucardo or long DS and to explain it I did like this little",
    "start": "1233639",
    "end": "1238860"
  },
  {
    "text": "animation where I have triggers that are basically cross inserting into tables via like an F DW and if you had these",
    "start": "1238860",
    "end": "1245399"
  },
  {
    "text": "triggers set up if you did this insert and you don't have the session replication role set that insert is",
    "start": "1245399",
    "end": "1250500"
  },
  {
    "text": "basically gonna flow hit the table hit the trigger it's gonna flow around it's gonna hit the other trigger and it would",
    "start": "1250500",
    "end": "1255899"
  },
  {
    "text": "just keep going forever in that little loop right and so that role basically allows you to say nonono if it's",
    "start": "1255899",
    "end": "1262139"
  },
  {
    "text": "replicated do it once don't keep doing it right so you know mostly people don't use this a lot it's mostly just used by",
    "start": "1262139",
    "end": "1268529"
  },
  {
    "text": "the tool chain but we do support that today so let's talk a little bit about scale and availability and some of the",
    "start": "1268529",
    "end": "1274139"
  },
  {
    "text": "questions and the discussions we've had with customers so one of them I get a lot is how should I set my shared",
    "start": "1274139",
    "end": "1280409"
  },
  {
    "start": "1277000",
    "end": "1277000"
  },
  {
    "text": "buffers how should I think about that with something like Postgres have been around for a long time you get some",
    "start": "1280409",
    "end": "1285419"
  },
  {
    "text": "historical data where people will say well no you should never set it bigger than a gig you know and that was based on some stuff around 32-bit systems",
    "start": "1285419",
    "end": "1291899"
  },
  {
    "text": "right so you know your mileage may vary on just reading you know the blogs of",
    "start": "1291899",
    "end": "1297360"
  },
  {
    "text": "this because you know timeliness is actually important in these things because the new versions always make",
    "start": "1297360",
    "end": "1302460"
  },
  {
    "text": "changes so you want to leave enough room for the processes and it depends you",
    "start": "1302460",
    "end": "1308850"
  },
  {
    "text": "might have a thousand you might have 10,000 you might have 10 but you want to kind of reserve that much space RDS",
    "start": "1308850",
    "end": "1314519"
  },
  {
    "text": "out-of-the-box we do a quarter of the total memory as shared buffers so in this case if we're talking about like an",
    "start": "1314519",
    "end": "1319710"
  },
  {
    "text": "R 3/8 extra-large 244 gig basically you'll have a 62 gig share buffer I",
    "start": "1319710",
    "end": "1325440"
  },
  {
    "text": "think I did that math correctly and then the rest goes to Linux page cache now",
    "start": "1325440",
    "end": "1331110"
  },
  {
    "text": "how many people here have used another database engine that uses the Linux page",
    "start": "1331110",
    "end": "1336299"
  },
  {
    "text": "cache it's not common anymore most of them like Oracle and other folks just use a",
    "start": "1336299",
    "end": "1341790"
  },
  {
    "text": "shared buffer so this is a little different when you select data in Postgres the Postgres process basically goes and",
    "start": "1341790",
    "end": "1348690"
  },
  {
    "text": "checks in the shared buffer to see if it's there and if it's not there it goes asked through the OS to get at the disk",
    "start": "1348690",
    "end": "1353760"
  },
  {
    "text": "if it's in the Linux page cache it returns it if not it goes to like you know the storage in this case EBS and",
    "start": "1353760",
    "end": "1359310"
  },
  {
    "text": "comes all the way back to the Postgres process now you can see that if",
    "start": "1359310",
    "end": "1364680"
  },
  {
    "text": "obviously it is in the shared buffer it has to do one less hop to go down to the Linux page cache and that's going to cut",
    "start": "1364680",
    "end": "1369750"
  },
  {
    "text": "into your efficiency right so what's the rule that you want to think about well",
    "start": "1369750",
    "end": "1375350"
  },
  {
    "text": "really shared buffers what we've seen works really well if you can get it to be right around your working set size",
    "start": "1375350",
    "end": "1381420"
  },
  {
    "text": "and your working set size is essentially the you know the common work data that you're gonna have to touch the indexes",
    "start": "1381420",
    "end": "1387630"
  },
  {
    "text": "and the data right that your applications going to touch over a period so to kind of show this again I",
    "start": "1387630",
    "end": "1393960"
  },
  {
    "text": "went and did a PG bench it's on an r3 extra-large and the working set is 10%",
    "start": "1393960",
    "end": "1399690"
  },
  {
    "text": "of the memory so I basically constructed PG bench to have 24 gig of data and on",
    "start": "1399690",
    "end": "1405180"
  },
  {
    "text": "the vertical I have transact transactions per second again and I ran it for a whole bunch of different you",
    "start": "1405180",
    "end": "1410310"
  },
  {
    "text": "know 25 up to you know 800 threads right just to give you an idea of how this looks so with the red line on the bottom",
    "start": "1410310",
    "end": "1418050"
  },
  {
    "text": "we have the shared buffers sort of as a percentage of the memory of the box so on the left with 3 & 6 the working set",
    "start": "1418050",
    "end": "1425340"
  },
  {
    "text": "it doesn't fit in memory right in the shared buffers on the right hand side it",
    "start": "1425340",
    "end": "1431160"
  },
  {
    "text": "does and what you'll notice of course is that you get a little bit of performance improvement you get 5 to 6 percent now",
    "start": "1431160",
    "end": "1438120"
  },
  {
    "text": "one of the interesting things is notice though after we go up from 13 to 25 we don't get any better performance right",
    "start": "1438120",
    "end": "1444480"
  },
  {
    "text": "so setting it super large isn't gonna help you and there are some areas where Postgres has trouble managing the shared",
    "start": "1444480",
    "end": "1451200"
  },
  {
    "text": "buffer so you don't want to just make it ridiculously large you really want to try to solve size it to your working set",
    "start": "1451200",
    "end": "1456720"
  },
  {
    "text": "size so the next thing is well okay who",
    "start": "1456720",
    "end": "1462240"
  },
  {
    "text": "runs their database with you know 24 Giga data on a 256 you know 10 or 50 gig box that's not that common right so I",
    "start": "1462240",
    "end": "1468960"
  },
  {
    "text": "went to the same test with a 50 percent of memory so here we go",
    "start": "1468960",
    "end": "1474010"
  },
  {
    "text": "and we're doing the same thing with the red line showing the difference between in memory or outside of memory so again",
    "start": "1474010",
    "end": "1481990"
  },
  {
    "text": "what we see is even more dramatic improvement at higher thread counts when",
    "start": "1481990",
    "end": "1487270"
  },
  {
    "text": "we're talking about 50% of the working set size so 3 probably % is on the like",
    "start": "1487270",
    "end": "1492370"
  },
  {
    "text": "lower like 25 threads and you get up to like 20% on the floor to 800 thread model so this is a way to get some nice",
    "start": "1492370",
    "end": "1500470"
  },
  {
    "text": "free you know extra you know run on your system by not spending any more money",
    "start": "1500470",
    "end": "1507010"
  },
  {
    "text": "but for just tuning your shared buffers a bit one of the other questions we get a lot from you know Postgres folks most",
    "start": "1507010",
    "end": "1514780"
  },
  {
    "start": "1510000",
    "end": "1510000"
  },
  {
    "text": "of you have used wall based replication for a long time and you're very familiar with it so you like that model and I get",
    "start": "1514780",
    "end": "1520990"
  },
  {
    "text": "a lot of questions of well how does that compare to multi z when would I use this instead of multi Z how does that should",
    "start": "1520990",
    "end": "1526630"
  },
  {
    "text": "they work together what should I do so I wanted to walk through that so with",
    "start": "1526630",
    "end": "1532630"
  },
  {
    "text": "multi AZ you start off typically with your database and your application and one",
    "start": "1532630",
    "end": "1538480"
  },
  {
    "text": "available Lily zone all of our regions have two or more availability zones today these are just fake ones AZ 1 AZ 2",
    "start": "1538480",
    "end": "1544890"
  },
  {
    "text": "you can see that we just start with that one AZ now we really think you want to have replication into another",
    "start": "1544890",
    "end": "1551170"
  },
  {
    "text": "availability zone we also think that you want have your application you know in the other availability zone you know",
    "start": "1551170",
    "end": "1557770"
  },
  {
    "text": "just so that you'll have the most availability now we use physical synchronous replication to do our multi",
    "start": "1557770",
    "end": "1563680"
  },
  {
    "text": "AZ and physical we want to be fast we want to be able to keep up no matter how hard you push the box the synchronous",
    "start": "1563680",
    "end": "1569530"
  },
  {
    "text": "though is the real critical piece because what that means is that we can have confidence to fail you over to your",
    "start": "1569530",
    "end": "1574960"
  },
  {
    "text": "secondary without data loss if there's an issue right with asynchronous replication you have to make a choice",
    "start": "1574960",
    "end": "1580690"
  },
  {
    "text": "you're like I might have all the data I might not now again there's use cases for both scenarios but as a service",
    "start": "1580690",
    "end": "1587380"
  },
  {
    "text": "provider it's hard for me to know whether you're you know running a gaming site we're losing 5 seconds of data is ok or your financial institution and",
    "start": "1587380",
    "end": "1593950"
  },
  {
    "text": "losing 5 seconds means someone's getting fired so the primary fails what we do is",
    "start": "1593950",
    "end": "1599830"
  },
  {
    "text": "we promote the secondary to a primary but you'll notice hey the applications still pointing over at the",
    "start": "1599830",
    "end": "1605920"
  },
  {
    "text": "other one what we do is we have si names in front of all of our instances and",
    "start": "1605920",
    "end": "1611200"
  },
  {
    "text": "that's maintained by DNS and that's how your application connect start yes so when we do the failover we tell DNS Hey",
    "start": "1611200",
    "end": "1618160"
  },
  {
    "text": "we've moved go tell the applications so this whole process takes about sixty",
    "start": "1618160",
    "end": "1623230"
  },
  {
    "text": "five seconds for both the failover the detection and the update of the DNS so that's what we see typically now if you",
    "start": "1623230",
    "end": "1629650"
  },
  {
    "text": "have a lot of database recovery sometimes that'll take a little bit longer but there's you know it's it's sort of in that range if you are seeing",
    "start": "1629650",
    "end": "1636970"
  },
  {
    "text": "longer recovery it's usually that you don't have enough read I ops to kind of do the recovery quickly so that's one of",
    "start": "1636970",
    "end": "1642190"
  },
  {
    "text": "the things that I like and gp2 is very good at that now the nice thing is",
    "start": "1642190",
    "end": "1647260"
  },
  {
    "text": "whether just your database fails or there's a larger like networking or some kind of other event you're set up to work no matter what and of course once",
    "start": "1647260",
    "end": "1654490"
  },
  {
    "text": "this is all fixed we go and rebuild that secondary automatically and get you back into a fully you know durable mode very",
    "start": "1654490",
    "end": "1661720"
  },
  {
    "text": "quickly so there's nothing to worry about so that's what we think is sort of the primary means to get you high",
    "start": "1661720",
    "end": "1667180"
  },
  {
    "text": "availability especially in a general sense now read replicas though can add a",
    "start": "1667180",
    "end": "1673270"
  },
  {
    "start": "1670000",
    "end": "1670000"
  },
  {
    "text": "lot of functionality these are just you know standard wall based replicas so here I'm showing multi easy and let's",
    "start": "1673270",
    "end": "1681100"
  },
  {
    "text": "add some read replicas now notice how they're being fed by the primary and your applications you know talking to",
    "start": "1681100",
    "end": "1686980"
  },
  {
    "text": "the database so my example here as always I say what if you had a blog site",
    "start": "1686980",
    "end": "1691990"
  },
  {
    "text": "well the guys that need to write blogs they need to write to the primary but all the readers of the blogs they don't",
    "start": "1691990",
    "end": "1698710"
  },
  {
    "text": "care they're fine getting a you know a read that's a second old so you can point them at the read replicas right",
    "start": "1698710",
    "end": "1703960"
  },
  {
    "text": "and the nice thing about this is when you do something like a failover from a primary to a secondary on multi easy",
    "start": "1703960",
    "end": "1710080"
  },
  {
    "text": "there's that sixty five seconds right where you're not going to be able to write but if your readers can go and",
    "start": "1710080",
    "end": "1716200"
  },
  {
    "text": "read from the replicas then no one ever notices that you know your blog sites down right just the one guy trying to",
    "start": "1716200",
    "end": "1722380"
  },
  {
    "text": "write his new blog has to wait a couple you know 65 seconds before we can write another blog entry and then you know",
    "start": "1722380",
    "end": "1728020"
  },
  {
    "text": "once that comes back up everything's good now notice how the replication is moves with it to the read replicas you",
    "start": "1728020",
    "end": "1733270"
  },
  {
    "text": "don't have to change anything you don't have to do anything that's all handled automatically you know no work there and this works",
    "start": "1733270",
    "end": "1739500"
  },
  {
    "text": "really good if you have to do an upgrade on your you know primary database or any of your replicas right or modify them or",
    "start": "1739500",
    "end": "1745980"
  },
  {
    "text": "do anything gives you a real high sense of read availability now this can also be used without multi a-z if you're fine",
    "start": "1745980",
    "end": "1752400"
  },
  {
    "text": "with taking the data loss and doing your own promotion so let's talk about how that works so here again I have that",
    "start": "1752400",
    "end": "1758880"
  },
  {
    "start": "1756000",
    "end": "1756000"
  },
  {
    "text": "same sort of diagram multi a-z application starting to read replicas end of the primary doing the synchronous",
    "start": "1758880",
    "end": "1765300"
  },
  {
    "text": "replication so let's say we want to go and promote this one in a z3 because we don't have anybody talking to it now we",
    "start": "1765300",
    "end": "1770730"
  },
  {
    "text": "might do this from a DR perspective we might do this because we just want to spin off a copy we do a promote call and",
    "start": "1770730",
    "end": "1777600"
  },
  {
    "text": "start of already s this takes a few minutes to happen and you get a new primary so it looks just like your regular instances now notice that we",
    "start": "1777600",
    "end": "1784800"
  },
  {
    "text": "don't do any following like none of the other read replicas moved to become children of that primary it's just that",
    "start": "1784800",
    "end": "1791250"
  },
  {
    "text": "one instance that changes and then you can connect an application to it you can make it multi easy you can turn backups",
    "start": "1791250",
    "end": "1796680"
  },
  {
    "text": "on you can modify it you can do whatever you'd like now of course one of the main reasons to",
    "start": "1796680",
    "end": "1803130"
  },
  {
    "text": "use read replicas scale right is that you know you have multi Z that's great but you can go add a lot of readers you",
    "start": "1803130",
    "end": "1810180"
  },
  {
    "text": "know and the nice thing is to when you think about it if you want to get even lower latency than you normally have you",
    "start": "1810180",
    "end": "1815460"
  },
  {
    "text": "can put your applications so they talk to the one in the same a Z and that'll probably shave off like a half millisecond to a millisecond off of",
    "start": "1815460",
    "end": "1821700"
  },
  {
    "text": "transaction times which you know for most people probably is not critical but it is an option that you can do so you",
    "start": "1821700",
    "end": "1830580"
  },
  {
    "text": "know replication works really well and for those of you that have used that the wall base stuff it's you know it's quite well built we set up the streaming",
    "start": "1830580",
    "end": "1837300"
  },
  {
    "text": "replication by default but one of the things that can happen is the streaming replication can break for a number of",
    "start": "1837300",
    "end": "1842550"
  },
  {
    "text": "reasons just a network hiccup whatever and meanwhile while it's broken you're accumulating logs right you just",
    "start": "1842550",
    "end": "1848640"
  },
  {
    "text": "continue to generate X logs they keep happening on the primary and this will just keep going on you know till you get",
    "start": "1848640",
    "end": "1855900"
  },
  {
    "text": "to let's say 99 logs now Postgres at some point will start removing these based on what you set wall keep segments",
    "start": "1855900",
    "end": "1861690"
  },
  {
    "text": "- the good news is with RDS is we're backing those up because of the point in time so we're constantly backing those",
    "start": "1861690",
    "end": "1867480"
  },
  {
    "text": "up so we can do a catch-up even if the one is removed from the primary and we",
    "start": "1867480",
    "end": "1872820"
  },
  {
    "text": "can apply that to the replica and it you know feed those through it to catch up and then once streaming's back on you're",
    "start": "1872820",
    "end": "1879540"
  },
  {
    "text": "all good but you really want to think about setting wall keep segments quite a bit higher because if you're doing that",
    "start": "1879540",
    "end": "1885210"
  },
  {
    "text": "log shipping model it's a very kind of it's not as smooth as streaming your legs gonna go up and down as the logs",
    "start": "1885210",
    "end": "1891300"
  },
  {
    "text": "are applied right as it goes file by file so in general you want to keep the streaming stuff running so you want to",
    "start": "1891300",
    "end": "1896310"
  },
  {
    "text": "set wall keep segments to a higher value then we typically default it to and how",
    "start": "1896310",
    "end": "1902430"
  },
  {
    "text": "do you know if you're streaming you can look at a PG stat replication view you can see the state they're streaming you",
    "start": "1902430",
    "end": "1907890"
  },
  {
    "text": "know it's all good right one of the other things that folks have a lot of",
    "start": "1907890",
    "end": "1913770"
  },
  {
    "start": "1910000",
    "end": "1910000"
  },
  {
    "text": "difficulty with is sort of the auto vacuum and replication and I did this little animation to show that so we have",
    "start": "1913770",
    "end": "1919560"
  },
  {
    "text": "a row in this table T one called you know primary key a and foo see it on",
    "start": "1919560",
    "end": "1925020"
  },
  {
    "text": "both the source and replicas now you know I'm just a you know person I'm like I'll just run this full table scan on",
    "start": "1925020",
    "end": "1930210"
  },
  {
    "text": "this table for no apparent reason because I want to find this one piece of data and I don't know how to use where",
    "start": "1930210",
    "end": "1935880"
  },
  {
    "text": "clauses so this is going to run for a very long time right no one's ever done this on a production database I'm sure",
    "start": "1935880",
    "end": "1943370"
  },
  {
    "text": "so but what happens if someone updates it on the primary right so we changed it to borrow from foo well in Postgres that",
    "start": "1943370",
    "end": "1950880"
  },
  {
    "text": "basically creates a new row you keep the old road for MVCC it's all good and that",
    "start": "1950880",
    "end": "1955980"
  },
  {
    "text": "will get replicated down to the replicas but it doesn't see it yet because you know of snapshot isolation right it's",
    "start": "1955980",
    "end": "1962310"
  },
  {
    "text": "all good but then somebody runs a vacuum on the source well guess what that does that",
    "start": "1962310",
    "end": "1968220"
  },
  {
    "text": "removes that old row great didn't need it but that gets replicated once it get",
    "start": "1968220",
    "end": "1973470"
  },
  {
    "text": "replicated you get this nice error how many people have had that error few",
    "start": "1973470",
    "end": "1978720"
  },
  {
    "text": "people yeah not fun because it kills your query that Corey could have been running for you know six hours or two",
    "start": "1978720",
    "end": "1985500"
  },
  {
    "text": "minutes and now it's dead and you got to restart it so there's four parameters",
    "start": "1985500",
    "end": "1990750"
  },
  {
    "text": "that basically you can use in Postgres to control this and we'll walk through each one of those so vacuum defer",
    "start": "1990750",
    "end": "1997560"
  },
  {
    "start": "1996000",
    "end": "1996000"
  },
  {
    "text": "cleanup age is probably the least recommended one because you have to set it by the number of transaction",
    "start": "1997560",
    "end": "2004020"
  },
  {
    "text": "you would like to keep so let's say we're going along and we're you know updating this row over and over again",
    "start": "2004020",
    "end": "2009480"
  },
  {
    "text": "I'm a foo far foo gar so we get up here and let's say on the replica we need",
    "start": "2009480",
    "end": "2015240"
  },
  {
    "text": "this roti for well if someone runs a vacuum it'll go away right but if we set",
    "start": "2015240",
    "end": "2020700"
  },
  {
    "text": "let's say the vacuum deferred cleanup age to two then it will keep those last",
    "start": "2020700",
    "end": "2026010"
  },
  {
    "text": "two but the bad part is it's set across the whole database you'd have to know",
    "start": "2026010",
    "end": "2031350"
  },
  {
    "text": "how many transactions you want to keep I mean I don't know how many people can do that kind of math I don't want to try",
    "start": "2031350",
    "end": "2036930"
  },
  {
    "text": "right it's not time-based so I don't find it very useful most people don't use that one the next one that was",
    "start": "2036930",
    "end": "2043440"
  },
  {
    "text": "really quite common and is still heavily used is Mac's standby archive delay or Mac's archive or Mac stand by streaming",
    "start": "2043440",
    "end": "2050280"
  },
  {
    "text": "delay the one controls for streaming replication obviously the one naming streaming delay and the other is for",
    "start": "2050280",
    "end": "2057210"
  },
  {
    "text": "archive so let's say we set that to 30 what that's going to do is if there is a",
    "start": "2057210",
    "end": "2062820"
  },
  {
    "text": "conflict it will delay applying the streaming wall for 30 seconds until you",
    "start": "2062820",
    "end": "2068908"
  },
  {
    "text": "know at the end of 30 seconds it'll apply it and you'll break your query but if your query gets done in that 30 seconds now you could set this to an",
    "start": "2068909",
    "end": "2074398"
  },
  {
    "text": "hour if you want it but obviously if you set it to an hour your replica is gonna lag by an hour when it hits a conflict",
    "start": "2074399",
    "end": "2080220"
  },
  {
    "text": "right so you have to know that that's gonna happen now because we use both streaming and",
    "start": "2080220",
    "end": "2086030"
  },
  {
    "text": "log apply you want to set both of these because if you don't if we switch your",
    "start": "2086030",
    "end": "2092429"
  },
  {
    "text": "queries could die right the real caveat with this one is it's",
    "start": "2092429",
    "end": "2097950"
  },
  {
    "text": "not per transaction so if you start a session and it hits this it doesn't mean",
    "start": "2097950",
    "end": "2105450"
  },
  {
    "text": "that the next guy gets to run for 30 seconds because if he started 29 seconds",
    "start": "2105450",
    "end": "2110880"
  },
  {
    "text": "in he's gonna run for a second before his query gets killed possibly so again",
    "start": "2110880",
    "end": "2116220"
  },
  {
    "text": "it's a little hard to model this in your head to get the right outcome but but it's still pretty useful and it doesn't",
    "start": "2116220",
    "end": "2122580"
  },
  {
    "text": "really impact the primary and that's what a lot of people like about this one is that there's you know it just delays",
    "start": "2122580",
    "end": "2127680"
  },
  {
    "text": "the replicas a bit the one I like a lot is hot standby feedback because I think",
    "start": "2127680",
    "end": "2133890"
  },
  {
    "text": "it's a little simpler to understand so essentially what this does is it's in on the replica it's off by default you",
    "start": "2133890",
    "end": "2139690"
  },
  {
    "text": "just turn it on and it's like it gives information back to the primary about",
    "start": "2139690",
    "end": "2144789"
  },
  {
    "text": "what's going on on the replica from a read perspective so if you go run select",
    "start": "2144789",
    "end": "2150940"
  },
  {
    "text": "you know star from t1 like I did what it does is it makes it look like to the primary like it's running there so that",
    "start": "2150940",
    "end": "2157750"
  },
  {
    "text": "the vacuum process will say oh you're running a long-running query I won't go and remove those rows so it basically",
    "start": "2157750",
    "end": "2164529"
  },
  {
    "text": "will stop Auto vacuuming from touching those rows while while you're having a conflict this is good and bad right",
    "start": "2164529",
    "end": "2171549"
  },
  {
    "text": "you're gonna get all your queries done the bad part is you may be blocking your vacuum right you block your vacuum too",
    "start": "2171549",
    "end": "2177339"
  },
  {
    "text": "much you get transaction wrap-around problems so you do have to be careful the other thing to remember is that if",
    "start": "2177339",
    "end": "2183220"
  },
  {
    "text": "streaming replication breaks for some reason and we do manual wall a file",
    "start": "2183220",
    "end": "2188589"
  },
  {
    "text": "apply this would cause that to break again write that query would would break",
    "start": "2188589",
    "end": "2194680"
  },
  {
    "text": "because there's no feedback going because there's no streaming so if you're going to use hot standby feedback",
    "start": "2194680",
    "end": "2200440"
  },
  {
    "text": "you still probably want to configure the the other parameter as well around archive delay so combinations a lot of",
    "start": "2200440",
    "end": "2208450"
  },
  {
    "text": "people use a lot of combinations in this how do you see the conflicts PG stat",
    "start": "2208450",
    "end": "2213609"
  },
  {
    "text": "database conflicts you'll see here I got one on this conflicting snapshot",
    "start": "2213609",
    "end": "2218710"
  },
  {
    "text": "isolation where I did a vacuum basically on a table that I was running so really",
    "start": "2218710",
    "end": "2223750"
  },
  {
    "text": "helpful one of the ones I like that we added from an informational perspective is PG stat statements how many people",
    "start": "2223750",
    "end": "2229390"
  },
  {
    "start": "2224000",
    "end": "2224000"
  },
  {
    "text": "use that on a regular basis whew I it's super powerful so it's an extension but",
    "start": "2229390",
    "end": "2236529"
  },
  {
    "text": "it's kind of a goofy extension because you have to load it as a shared library first so it you know you have to know that you need to do it so by default",
    "start": "2236529",
    "end": "2242980"
  },
  {
    "text": "it's not loaded and RDS so if you want to load it you're gonna create a custom parameter group add the shared library",
    "start": "2242980",
    "end": "2249190"
  },
  {
    "text": "PG stat statement it's the only thing that's allowed in the shared preload libraries option at the moment once you",
    "start": "2249190",
    "end": "2255009"
  },
  {
    "text": "do that you can go create this extension and what it gives you is I mean PG stat activity tells you what's going on right",
    "start": "2255009",
    "end": "2260950"
  },
  {
    "text": "now in your database the nice thing of a PG stat statement especially if you take kind of you pull from it on a regular",
    "start": "2260950",
    "end": "2266799"
  },
  {
    "text": "basis is you can see what your statements are doing how much you know much effort they're basically taking a",
    "start": "2266799",
    "end": "2273040"
  },
  {
    "text": "run whether anything's changed you know how much reading they're doing how many rows all good information so I",
    "start": "2273040",
    "end": "2279370"
  },
  {
    "text": "really recommend turning this on it's something that we've talked about you know maybe turning on as a default in in",
    "start": "2279370",
    "end": "2285670"
  },
  {
    "text": "newer versions we haven't got there yet would be great to get some feedback on that from folks the other one that I'd",
    "start": "2285670",
    "end": "2292270"
  },
  {
    "start": "2291000",
    "end": "2291000"
  },
  {
    "text": "like to talk about is burst mode so I talked about this last year for anyone that's seen this I made some changes but",
    "start": "2292270",
    "end": "2298930"
  },
  {
    "text": "I think burst mode is actually one of the interesting things that we're doing in an RDS and an ec2 in general that",
    "start": "2298930",
    "end": "2306580"
  },
  {
    "text": "really changes the cost economics about doing some some stuff with databases so with the t2 family of instances they",
    "start": "2306580",
    "end": "2313540"
  },
  {
    "text": "basically allow you to burst now anyone that used the t1 and it said I'm never touching a burst model again I",
    "start": "2313540",
    "end": "2319120"
  },
  {
    "text": "understand you got burnt the T 2's are much much better they really do work so",
    "start": "2319120",
    "end": "2324640"
  },
  {
    "text": "you start off with a base performance if you're below that base performance you're earning credits and you get to spend them later by bursting the nice",
    "start": "2324640",
    "end": "2331450"
  },
  {
    "text": "thing is for the t line you actually a cloud watch metrics that show you those and I'll show you those the credits in",
    "start": "2331450",
    "end": "2337480"
  },
  {
    "text": "the usage graphs on the storage side EBS released GP to their SSD based storage",
    "start": "2337480",
    "end": "2343870"
  },
  {
    "text": "so this is very similar to p.i ops but instead of getting a guarantee about an overall number and having to pick a",
    "start": "2343870",
    "end": "2349300"
  },
  {
    "text": "number you basically get 3 gigabytes 3 iOS per gigabyte so if you're not using",
    "start": "2349300",
    "end": "2355270"
  },
  {
    "text": "those three they start going into a bucket that you get to spend later now unfortunately at the moment there is",
    "start": "2355270",
    "end": "2361540"
  },
  {
    "text": "no metric for this so it makes it a little hard to kind of figure out it's definitely something we've pushed the EBS on to try to rectify and with RDS",
    "start": "2361540",
    "end": "2370120"
  },
  {
    "text": "today you could burst to at least three thousand if not higher depending on your storage size so to show this in action",
    "start": "2370120",
    "end": "2376240"
  },
  {
    "start": "2376000",
    "end": "2376000"
  },
  {
    "text": "I ran PG bench and this is the cloud watch metrics for this instance so the",
    "start": "2376240",
    "end": "2383890"
  },
  {
    "text": "green line is my CPU and the orange line is my CPU credit balance so as I'm",
    "start": "2383890",
    "end": "2390070"
  },
  {
    "text": "running it basically is using up credits because I'm pushing the blocks really hard and then I get to that point where I exhaust the credit so you can see it",
    "start": "2390070",
    "end": "2396340"
  },
  {
    "text": "gets down to zero and my CPU drops right so you can monitor this and you can see",
    "start": "2396340",
    "end": "2401440"
  },
  {
    "text": "hey I'm spiking a little bit but it's fine I'm not going to use my credits up right and if you are using all your credits up maybe go to a bigger",
    "start": "2401440",
    "end": "2407770"
  },
  {
    "text": "box right the cool thing is we just announced a new member of that family",
    "start": "2407770",
    "end": "2414130"
  },
  {
    "text": "the t2 large it has more initial credits it has a higher base performance comes",
    "start": "2414130",
    "end": "2419860"
  },
  {
    "text": "with more RAM has bigger i/o bandwidth but the big really big one that I'm",
    "start": "2419860",
    "end": "2425140"
  },
  {
    "text": "really happy about is it supports encryption at rest so if you were in production using encryption at rest",
    "start": "2425140",
    "end": "2430390"
  },
  {
    "text": "today you had to be on an r3 or an m3 that's great no problem I probably need that big box anyway but now I want to",
    "start": "2430390",
    "end": "2436390"
  },
  {
    "text": "make a test copy of my database oh wait I can't use a T because they didn't",
    "start": "2436390",
    "end": "2441880"
  },
  {
    "text": "support encryption I can't put those volumes on that box so this is either EBS or RDS so the nice thing is now this",
    "start": "2441880",
    "end": "2448750"
  },
  {
    "text": "is supported as encryption at rest so it's not the smallest box we have but it definitely is a lot more cost effective",
    "start": "2448750",
    "end": "2454990"
  },
  {
    "text": "than using an M R and R as a test instance so I ran this benchmark last",
    "start": "2454990",
    "end": "2460990"
  },
  {
    "start": "2459000",
    "end": "2459000"
  },
  {
    "text": "year I updated it with some some new results I start off running you know",
    "start": "2460990",
    "end": "2466120"
  },
  {
    "text": "basically a workload 100% read 20 Giga data and the date is bigger than the",
    "start": "2466120",
    "end": "2472360"
  },
  {
    "text": "size of memory so we're gonna do some i/o transactions per second again on the vertical and I ran it for 24 hours so",
    "start": "2472360",
    "end": "2479440"
  },
  {
    "text": "you know kind of give you the full scope I started with one of the older instance types the m1 medium so you know we still",
    "start": "2479440",
    "end": "2486010"
  },
  {
    "text": "have a lot of people on these you know there were our first instance types for a long time and I ran it with the magnetic standard EBS storage option so",
    "start": "2486010",
    "end": "2493060"
  },
  {
    "text": "200 Giga storage cost 58 cents an hour we get almost 2,000 transactions per second it's not bad if we go to a modern",
    "start": "2493060",
    "end": "2501130"
  },
  {
    "text": "one an m3 medium and we had provision 2 offs well the interesting thing is it",
    "start": "2501130",
    "end": "2506920"
  },
  {
    "text": "actually costs less because it's a newer instance class and we're and the price economics are better on them and we not",
    "start": "2506920",
    "end": "2512350"
  },
  {
    "text": "only did we you know cost less but we actually got to performance improvement by going to that if we go up to a large",
    "start": "2512350",
    "end": "2518230"
  },
  {
    "text": "you see on this workload we don't actually get a lot of benefit but the cost economics it's actually funny because the TPS the dollar per TPS is",
    "start": "2518230",
    "end": "2525460"
  },
  {
    "text": "exactly the same you know cost a little more but I got a few more transactions per second out of it so you know it's",
    "start": "2525460",
    "end": "2530860"
  },
  {
    "text": "not a bad ad but let's go compare this now to the T line and to gp2 so the",
    "start": "2530860",
    "end": "2536830"
  },
  {
    "text": "first thing I did was take at each medium with 200 gig of gp2 so that means",
    "start": "2536830",
    "end": "2541930"
  },
  {
    "text": "I get basically 600 I ups right guaranteed 3000 burst now notice this",
    "start": "2541930",
    "end": "2548080"
  },
  {
    "text": "little little bump here so we start off running faster than any of the previous results because we're bursting in gp2",
    "start": "2548080",
    "end": "2554380"
  },
  {
    "text": "we're getting 3,000 i ops that's more than we bought on any of the other environments right but after two",
    "start": "2554380",
    "end": "2560680"
  },
  {
    "text": "hours we run out of GP 2 credits and we slowed down dramatically right now hey you know we're not getting a lot of TPS",
    "start": "2560680",
    "end": "2567070"
  },
  {
    "text": "at the end but also look at that cost it's 10 cents an hour it's you know it's very cost effective if you only needed to burst for a couple",
    "start": "2567070",
    "end": "2572920"
  },
  {
    "text": "of hours a day right way better than the you know 40 or 50 cents an hour but the",
    "start": "2572920",
    "end": "2578230"
  },
  {
    "text": "nice thing about GP 2 and kind of the the hidden secret is guess what you can just go buy more storage if you need",
    "start": "2578230",
    "end": "2584170"
  },
  {
    "text": "more ions you don't need to just use this storage you have so I was like let me go buy a terabyte just gonna get some",
    "start": "2584170",
    "end": "2591280"
  },
  {
    "text": "big storage right that gives me a guarantee of 3000 die-offs right plus",
    "start": "2591280",
    "end": "2596980"
  },
  {
    "text": "the ability to burst above that so now I'm getting 6000 TPS right for the first two hours before I get throttled by GP",
    "start": "2596980",
    "end": "2604000"
  },
  {
    "text": "by my GP to limits right I continue on for 17 hours to be faster than any of",
    "start": "2604000",
    "end": "2610150"
  },
  {
    "text": "the other previous runs until I hit the limit of my credits on my t2 right so I",
    "start": "2610150",
    "end": "2615520"
  },
  {
    "text": "go down to being almost the same as the m3 medium at the end but look at the price difference",
    "start": "2615520",
    "end": "2620920"
  },
  {
    "text": "we're 23 cents an hour versus 40 right and you were able to burst higher",
    "start": "2620920",
    "end": "2626050"
  },
  {
    "text": "earlier on than you were with the with the m3 medium you know really good stuff",
    "start": "2626050",
    "end": "2631180"
  },
  {
    "text": "and this is what I presented last year now the cool thing is this is the result for the t2 large so twelve thousand",
    "start": "2631180",
    "end": "2637480"
  },
  {
    "text": "transactions per second part of the reason is it has more i/o bandwidth so we weren't actually able to go and burst",
    "start": "2637480",
    "end": "2644380"
  },
  {
    "text": "all the way with gp2 on the medium because it just couldn't get enough I out of the box the t2 large actually has",
    "start": "2644380",
    "end": "2650980"
  },
  {
    "text": "the ability to go use more I ops now you'll notice that it didn't run for as long because it used up its credits sooner but got double the TPS so that's",
    "start": "2650980",
    "end": "2659770"
  },
  {
    "text": "pretty impressive now you'll notice that that redline runs out flatter for a much longer time goes out to 21 hours so even",
    "start": "2659770",
    "end": "2666430"
  },
  {
    "text": "though we're doing more TPS because we have more memory so we're using less i/o it",
    "start": "2666430",
    "end": "2672069"
  },
  {
    "text": "a higher amount of burst built into it or house base credits are higher so you can run for longer and burst now I will",
    "start": "2672069",
    "end": "2678190"
  },
  {
    "text": "say this about this benchmark this is what you would get if you ran for 48 hours like because for 24 hours these",
    "start": "2678190",
    "end": "2684099"
  },
  {
    "text": "instances weren't doing anything they were basically accumulating so this would be if you had this workload that's every two days or half of the amount of",
    "start": "2684099",
    "end": "2690489"
  },
  {
    "text": "burst every day because you have to accumulate the credits I just did that to show because you know it makes it",
    "start": "2690489",
    "end": "2696039"
  },
  {
    "text": "easier but notice at the end here 30 cents an hour we're still faster than",
    "start": "2696039",
    "end": "2701380"
  },
  {
    "text": "every other thing in this category right so we've improved our performance by you",
    "start": "2701380",
    "end": "2707619"
  },
  {
    "text": "know almost 50% for less money so this really shows you how the burst mode",
    "start": "2707619",
    "end": "2713079"
  },
  {
    "text": "stuff can really change now you don't have to use GP to with t2 or vice versa",
    "start": "2713079",
    "end": "2718089"
  },
  {
    "text": "I combined them to kind of make you know a larger point but t2 is on their own or",
    "start": "2718089",
    "end": "2723130"
  },
  {
    "text": "gp2 instead of PI offs really makes sense and my rule of thumb is basically if you're probably on RDS and you need",
    "start": "2723130",
    "end": "2729880"
  },
  {
    "text": "less than 10,000 provision ops or 10,000 iOS probably use GP to you're gonna",
    "start": "2729880",
    "end": "2736269"
  },
  {
    "text": "you're gonna save a lot of money your performance is going to be probably the same or possibly better because you have the ability to burst which you don't",
    "start": "2736269",
    "end": "2742569"
  },
  {
    "text": "have with and it's only if you need way more i ops or you really need the strict",
    "start": "2742569",
    "end": "2748089"
  },
  {
    "text": "guarantees that P ops gives you around quality of service that are higher than GP to is really the only reason to go to",
    "start": "2748089",
    "end": "2754150"
  },
  {
    "text": "something like that so that's you know that's I think a really key that you know now you can start using things in",
    "start": "2754150",
    "end": "2760539"
  },
  {
    "text": "different ways and if you have a burst you know kind of model for your for your application this is this is super good",
    "start": "2760539",
    "end": "2766749"
  },
  {
    "text": "stuff so with that I'm gonna wrap up thank you very much",
    "start": "2766749",
    "end": "2772680"
  }
]