[
  {
    "text": "okay so hello everyone welcome to build your first Big Data application in this",
    "start": "4640",
    "end": "10440"
  },
  {
    "text": "session I'm going to show you how you can build an entire entr analytics application um for batch and streaming",
    "start": "10440",
    "end": "17359"
  },
  {
    "text": "data from scratch and I think this is important because analyzing data allows",
    "start": "17359",
    "end": "23240"
  },
  {
    "text": "your drive to drive value from your data so it helps you to better understand",
    "start": "23240",
    "end": "28359"
  },
  {
    "text": "your customers to improve your products um but also to drive your",
    "start": "28359",
    "end": "35280"
  },
  {
    "text": "Revenue so you'll see in the session how easy it is to get started building your",
    "start": "35280",
    "end": "40520"
  },
  {
    "text": "own analytics platform on AWS and it's going to be a Hands-On session so I",
    "start": "40520",
    "end": "45800"
  },
  {
    "text": "barely have any slides and I'm going to spend most of the time in the AWS console actually building the",
    "start": "45800",
    "end": "52399"
  },
  {
    "text": "infrastructure for you on stage my name is Stefan hman and I'm a",
    "start": "52399",
    "end": "58760"
  },
  {
    "text": "specialist Solutions architect for analytics at",
    "start": "58760",
    "end": "63239"
  },
  {
    "text": "AWS okay so let's assume for a moment that we are in charge for a large taxi",
    "start": "64199",
    "end": "71200"
  },
  {
    "text": "company in New York City each of our taxis is equipped with sensors that are",
    "start": "71200",
    "end": "77200"
  },
  {
    "text": "frequently reporting sending geospatial information back to our",
    "start": "77200",
    "end": "82799"
  },
  {
    "text": "backends and we are now in the position that we have to figure out how we want to use this information we are re",
    "start": "82799",
    "end": "89920"
  },
  {
    "text": "receiving to optimize the operation of our Fleet we can for instance monitor the",
    "start": "89920",
    "end": "98159"
  },
  {
    "text": "revenue we are currently making and obtain notifications if the revenue dips",
    "start": "98159",
    "end": "104600"
  },
  {
    "text": "or we can try to figure out which are the hotpots in New York City that are currently requesting a lot of taxis so",
    "start": "104600",
    "end": "111439"
  },
  {
    "text": "that it would make sense to actually bring additional taxis to these spots to pick up pick people more",
    "start": "111439",
    "end": "117840"
  },
  {
    "text": "quickly right but it would also be cool if we could analyze the data we collecting in a more",
    "start": "117840",
    "end": "124240"
  },
  {
    "text": "long-term oriented fashion so not only um the real time aspect of what what do",
    "start": "124240",
    "end": "129800"
  },
  {
    "text": "we need to do now but also take a look at a year or or several years worth of",
    "start": "129800",
    "end": "135239"
  },
  {
    "text": "data to figure out what should we do next right and this is",
    "start": "135239",
    "end": "140680"
  },
  {
    "text": "basically um the scenario I'm going to use to build an analytic an analytics",
    "start": "140680",
    "end": "146319"
  },
  {
    "text": "pipeline to actually answer these questions um for our",
    "start": "146319",
    "end": "152680"
  },
  {
    "text": "company so how how would an architecture look like that can actually help answer",
    "start": "152680",
    "end": "158560"
  },
  {
    "text": "these questions first of all we need to capture the incoming information right",
    "start": "158560",
    "end": "164480"
  },
  {
    "text": "we have like tens thousands or tens of tens of thousands of taxes sending data",
    "start": "164480",
    "end": "170599"
  },
  {
    "text": "to a backend and we need to figure out a way um to ingest this data and then process",
    "start": "170599",
    "end": "176519"
  },
  {
    "text": "it so we'll start the journey with ingesting the data that is generated by",
    "start": "176519",
    "end": "183680"
  },
  {
    "text": "these taxis into a Kinesis stream and a Kinesis stream is basically just a",
    "start": "183680",
    "end": "189080"
  },
  {
    "text": "replayable ordered log that serves as a buffer so it's basically a way to",
    "start": "189080",
    "end": "194360"
  },
  {
    "text": "decouple producers from consumers so that they can fail independently and that they can scale",
    "start": "194360",
    "end": "201319"
  },
  {
    "text": "independently once the data is in the Kinesis stream we'll use um a service",
    "start": "201319",
    "end": "207440"
  },
  {
    "text": "called Kinesis analytics which allows us to do to do realtime analytics on top of",
    "start": "207440",
    "end": "213120"
  },
  {
    "text": "the streaming data so as it turns out any type of data you're collecting is",
    "start": "213120",
    "end": "218680"
  },
  {
    "text": "probably noisy and what we'll do with Kinesis analytics is we'll fil filter",
    "start": "218680",
    "end": "223920"
  },
  {
    "text": "out outliers um and filter out data that doesn't doesn't make sense while it's",
    "start": "223920",
    "end": "231200"
  },
  {
    "text": "flowing through our system in addition Kinesis analytics also will help us to identify our",
    "start": "231200",
    "end": "238439"
  },
  {
    "text": "outliers so we can monitor Revenue our re incoming revenue and then use machine",
    "start": "238439",
    "end": "244200"
  },
  {
    "text": "learning on top of the streaming data to identify um whether there is um a dip in",
    "start": "244200",
    "end": "249400"
  },
  {
    "text": "our revenue or something is out of bounds we then um build a real-time",
    "start": "249400",
    "end": "257720"
  },
  {
    "text": "dashboard based out of the results we get from Kinesis analytics so we have this real-time engine continuously",
    "start": "257720",
    "end": "262919"
  },
  {
    "text": "monitoring or analizing our data and this data is then sent to a cloudwatch",
    "start": "262919",
    "end": "268440"
  },
  {
    "text": "dashboard for for visualization purposes um so we're going to use another Kines",
    "start": "268440",
    "end": "273639"
  },
  {
    "text": "stream and a Lambda function but essentially we're just going to um have a real-time visualization of the",
    "start": "273639",
    "end": "279720"
  },
  {
    "text": "incoming data so that we can then go ahead and create alarms and get notified in case something goes",
    "start": "279720",
    "end": "287400"
  },
  {
    "text": "wrong so the upper part of the architecture is the realtime component of the architecture the lower part is",
    "start": "287560",
    "end": "294639"
  },
  {
    "text": "now um more oriented on long-term analytics of data so there what we are",
    "start": "294639",
    "end": "300160"
  },
  {
    "text": "going to do is we'll take the data that has been cleaned by Kinesis analytics and write it to S3 for long-term",
    "start": "300160",
    "end": "306919"
  },
  {
    "text": "archival purposes and once it's sitting in S3 what we going to do is we're going",
    "start": "306919",
    "end": "312240"
  },
  {
    "text": "to query the data through a service called Amazon Athena which basically allows you to run SQL queries on top of",
    "start": "312240",
    "end": "320000"
  },
  {
    "text": "data sitting in S3 and last but not least um I'm going",
    "start": "320000",
    "end": "325840"
  },
  {
    "text": "to build a dashboard for long-term analytics and trend analytics of our",
    "start": "325840",
    "end": "333199"
  },
  {
    "text": "data so this is basically what I'm going to build so um don't be afraid if you",
    "start": "333800",
    "end": "338919"
  },
  {
    "text": "didn't understand each particular component I'll explain it in more detail",
    "start": "338919",
    "end": "344080"
  },
  {
    "text": "um while I I'm actually building um building the architecture but um what",
    "start": "344080",
    "end": "349639"
  },
  {
    "text": "what's to remember is basically we have these two different Lanes one for Real Time analytics of data the other one for",
    "start": "349639",
    "end": "355080"
  },
  {
    "text": "badge processing of data so real-time ingestion of data and",
    "start": "355080",
    "end": "361759"
  },
  {
    "text": "cleansing the of the data in real time real time visualization of",
    "start": "361759",
    "end": "367400"
  },
  {
    "text": "data ingestion of data in batch anal analytics of data and last but not least",
    "start": "367400",
    "end": "374039"
  },
  {
    "text": "the visualization of data in a in a dashboard for long-term",
    "start": "374039",
    "end": "379560"
  },
  {
    "text": "analytics okay so let's go and",
    "start": "379560",
    "end": "384759"
  },
  {
    "text": "build as promised I don't have too many slides I'll be the mo the rest of the",
    "start": "385639",
    "end": "392440"
  },
  {
    "text": "presentation basically in the console showing you how to actually build this this architecture so I'll start off with",
    "start": "392440",
    "end": "399479"
  },
  {
    "text": "creating a Kinesis stream a Kinesis stream again is a replayable ordered log",
    "start": "399479",
    "end": "404800"
  },
  {
    "text": "that allows us to ingest high volume um event stream and store it for realtime",
    "start": "404800",
    "end": "412000"
  },
  {
    "text": "analytics so I create a Kinesis stream so I'm going navigating to the Kinesis",
    "start": "412000",
    "end": "417479"
  },
  {
    "text": "um console and create a Kinesis data stream and I need to specify a name so",
    "start": "417479",
    "end": "424280"
  },
  {
    "text": "each stream needs to has a name I'll call this Summit taxi",
    "start": "424280",
    "end": "430400"
  },
  {
    "text": "trips and now I'm asked how many shards my screen stream should have so a kesa",
    "start": "430840",
    "end": "436720"
  },
  {
    "text": "stream is made up of shards and adding and removing um shards to a stream is a",
    "start": "436720",
    "end": "442879"
  },
  {
    "text": "way to scale the Stream So each chart uh can have an intake of a th000 events per second and one megabyte per second in",
    "start": "442879",
    "end": "449919"
  },
  {
    "text": "and you can read 2 megabytes per second out of a Shard and up to five um requests per second read requests per",
    "start": "449919",
    "end": "456240"
  },
  {
    "text": "second to A Shard um and by adding and removing shards um we can scale um the",
    "start": "456240",
    "end": "463240"
  },
  {
    "text": "the throughput of our stream so in order to help us come up with the right number",
    "start": "463240",
    "end": "468319"
  },
  {
    "text": "of shards um there's a small calculator um and it basically ask a",
    "start": "468319",
    "end": "474400"
  },
  {
    "text": "couple of metrics to determine what's the right amount of shards um so first of all",
    "start": "474400",
    "end": "480000"
  },
  {
    "text": "it wants to know what's the average record size and in our case we just sending a couple of geospatial data so",
    "start": "480000",
    "end": "488120"
  },
  {
    "text": "coordinates and a couple of other values so nothing large so it's it'll be below",
    "start": "488120",
    "end": "493759"
  },
  {
    "text": "1 kilobyte and but the that's 1 kiloby is the smallest I can I can specify then",
    "start": "493759",
    "end": "500120"
  },
  {
    "text": "it wants to know how many records are written into the ginis stream per second",
    "start": "500120",
    "end": "505240"
  },
  {
    "text": "and in this example I'm going to use we'll have on average roughly 22,000 records per second that are",
    "start": "505240",
    "end": "512120"
  },
  {
    "text": "ingested into the stream and but it can Spike up to 30 30,000 records so I'll",
    "start": "512120",
    "end": "517599"
  },
  {
    "text": "specify well I need a stream that is capable of ingesting 30,000 um individual records per second",
    "start": "517599",
    "end": "524720"
  },
  {
    "text": "and I have one consuming application which leaves me with um 30 shards which",
    "start": "524720",
    "end": "530320"
  },
  {
    "text": "seems to make sense 30 shards each shards can intake 1,000 events per second which gives me 30,000 events per",
    "start": "530320",
    "end": "537600"
  },
  {
    "text": "second so I'll use this value and click create stream and now in the",
    "start": "537600",
    "end": "543160"
  },
  {
    "text": "background we'll provision the infrastructure to actually um yeah host",
    "start": "543160",
    "end": "549279"
  },
  {
    "text": "the stream for us that takes a couple of moments but it shouldn't take longer",
    "start": "549279",
    "end": "554880"
  },
  {
    "text": "than a minute also and once that is completed I can immediately start",
    "start": "554880",
    "end": "560480"
  },
  {
    "text": "ingesting um events into my Stream So the stream has been created and now what",
    "start": "560480",
    "end": "566360"
  },
  {
    "text": "I'm doing is um you're not going to I'm I'm just replaying a historic data set",
    "start": "566360",
    "end": "572640"
  },
  {
    "text": "um from S3 into the Kinesis Stream So I've written a small Java application",
    "start": "572640",
    "end": "578399"
  },
  {
    "text": "that takes um a data set of historic taxi trips from New York City which is",
    "start": "578399",
    "end": "583680"
  },
  {
    "text": "available as a um open data set on AWS and it basically replays this data as if",
    "start": "583680",
    "end": "589040"
  },
  {
    "text": "it happened in real time into the Kinesis stream um so in this for this demo I'm",
    "start": "589040",
    "end": "596760"
  },
  {
    "text": "using one ec2 instance that is replaying the data into Kinesis stream but for practicable purposes it wouldn't make",
    "start": "596760",
    "end": "603040"
  },
  {
    "text": "any difference whether I have one application ingesting 30,000 records per second or if I had 30,000 producers",
    "start": "603040",
    "end": "610880"
  },
  {
    "text": "ingesting one sec one event per second into the stream wouldn't make any difference right so it it would be from",
    "start": "610880",
    "end": "616519"
  },
  {
    "text": "from the Kines perspective would be the very same thing okay now that we're sending",
    "start": "616519",
    "end": "623040"
  },
  {
    "text": "records to the Kinesis dream I can start creating my Kinesis analytics application so Kinesis analytics is a",
    "start": "623040",
    "end": "630920"
  },
  {
    "text": "service that allows you to run continuous analytics on top of streaming",
    "start": "630920",
    "end": "637240"
  },
  {
    "text": "data I hit create application and you can see that um Kinesis analytics comes",
    "start": "637279",
    "end": "643480"
  },
  {
    "text": "in two flavors SQL in a patch of Link um so for the purpose of this demo I'm",
    "start": "643480",
    "end": "649320"
  },
  {
    "text": "going to use the SQL component because it's more natural more easier to use and to read in particular if you're familiar",
    "start": "649320",
    "end": "655200"
  },
  {
    "text": "with SQL however if you're building um a real-time architecture S I highly",
    "start": "655200",
    "end": "660320"
  },
  {
    "text": "encourage you take a look at Apache Flink it's a very cool open source project that is offered as a manag",
    "start": "660320",
    "end": "665600"
  },
  {
    "text": "service through Kinesis analytics right so again the application needs a um name",
    "start": "665600",
    "end": "674160"
  },
  {
    "text": "call it Summit um taxi",
    "start": "674160",
    "end": "680839"
  },
  {
    "text": "trips and hit create and now that the application is created I can connect to the kesa stream",
    "start": "683279",
    "end": "690720"
  },
  {
    "text": "where I'm already ingesting data so I'll hit connect stream and simply select the",
    "start": "690720",
    "end": "696920"
  },
  {
    "text": "stream that um carries the data I now have the option to use a",
    "start": "696920",
    "end": "703160"
  },
  {
    "text": "Lambda function to pre-process the incoming data so with Kinesis analytics I'm running SQL queries on top of my",
    "start": "703160",
    "end": "709880"
  },
  {
    "text": "streaming data um however in particular for ETL or Transformations it may be more convenient to have an imperative",
    "start": "709880",
    "end": "716560"
  },
  {
    "text": "language such as python or noj s or whatnot and where you can use bring your",
    "start": "716560",
    "end": "722240"
  },
  {
    "text": "own libraries to do some kind of pre-processing so here you have the option to actually do that the the the",
    "start": "722240",
    "end": "728200"
  },
  {
    "text": "events are ingested into the Kinesis stream they are pre-processed by Lambda and then handed over to Kinesis",
    "start": "728200",
    "end": "734120"
  },
  {
    "text": "analytics I'm not going to do this for the purpose of this demo but that's one option you have there the service needs access to my",
    "start": "734120",
    "end": "741360"
  },
  {
    "text": "account so that I can actually read and and analyze the data and last but not least I need to specify a schema right",
    "start": "741360",
    "end": "748519"
  },
  {
    "text": "eventually i' like to run SQL queries against um the streaming data um so I",
    "start": "748519",
    "end": "754519"
  },
  {
    "text": "need to come up with some some kind of schema and with a Kinesis stream I can basically ingest anything I'd like so",
    "start": "754519",
    "end": "760600"
  },
  {
    "text": "there are no limitations what I can um ingest as long as it's smaller than one",
    "start": "760600",
    "end": "766560"
  },
  {
    "text": "megabyte so I need to have a way to tell KES analytics what's the schema of my",
    "start": "766560",
    "end": "771760"
  },
  {
    "text": "application and KES Antics actually actually helps me with it I can I can",
    "start": "771760",
    "end": "776920"
  },
  {
    "text": "hit discover schema and now ICS reads a sample of events from the stream and",
    "start": "776920",
    "end": "782839"
  },
  {
    "text": "tries to infer the schema for me it claims it was successful and it doesn't look too bad right so you can see we are",
    "start": "782839",
    "end": "789760"
  },
  {
    "text": "dealing with some kind of geospatial data it's the it's talking about pickup",
    "start": "789760",
    "end": "795760"
  },
  {
    "text": "date uh pickup and drop off times and locations so this is the data that is sent by these taxis um and it inferred",
    "start": "795760",
    "end": "804800"
  },
  {
    "text": "the um the correct um the correct types",
    "start": "804800",
    "end": "810279"
  },
  {
    "text": "however it's only looking at a sample of the Stream So the inference may not be perfect and actually um in this case",
    "start": "810279",
    "end": "818120"
  },
  {
    "text": "um we need to fix a couple of things first of all the trip ID um when I start",
    "start": "818120",
    "end": "824399"
  },
  {
    "text": "ingesting um data into the stream the IDS are rather small but over time they can grow very large Kinesis analytics",
    "start": "824399",
    "end": "831120"
  },
  {
    "text": "doesn't know that so I need to change the type from integer to Big end to accommodate for that also the type um it",
    "start": "831120",
    "end": "838759"
  },
  {
    "text": "think the type attribute can be four characters it can actually be eight so it it's a good first guess um but you",
    "start": "838759",
    "end": "846320"
  },
  {
    "text": "always need to check whether it actually matches um the schema of the data you're",
    "start": "846320",
    "end": "851720"
  },
  {
    "text": "ingesting okay so now I'm saving the schema and the application is actually",
    "start": "851720",
    "end": "858320"
  },
  {
    "text": "started so now we are going to provision infrastructure on your behalf and deploying the application on top of that",
    "start": "858320",
    "end": "865240"
  },
  {
    "text": "infrastructure um it's a fully managed service so it scales automatically it recovers from failures automatically um",
    "start": "865240",
    "end": "871560"
  },
  {
    "text": "there's nothing on your end um to really watch out for so that's",
    "start": "871560",
    "end": "878160"
  },
  {
    "text": "done I can then add reference data um to Kinesis analytics which basically means",
    "start": "878160",
    "end": "883839"
  },
  {
    "text": "it's a static data set um that you can use as some kind of lookup table if you if you want to enrich the events coming",
    "start": "883839",
    "end": "889720"
  },
  {
    "text": "in um you can join the static reference data set with the streaming data that",
    "start": "889720",
    "end": "895240"
  },
  {
    "text": "are ingested into the stream I'm not using this for the demo but that's another option you have and more",
    "start": "895240",
    "end": "900759"
  },
  {
    "text": "interestingly I can now go ahead and write SQL queries so I've already",
    "start": "900759",
    "end": "907160"
  },
  {
    "text": "prepared a couple of queries and I'll explain in a second what they actually",
    "start": "907160",
    "end": "912480"
  },
  {
    "text": "do so I'll copy and",
    "start": "912480",
    "end": "916560"
  },
  {
    "text": "paste the code and then we can see how that looks okay so Kinesis analytics",
    "start": "918480",
    "end": "925120"
  },
  {
    "text": "comes with two important Concepts in application streams and pumps an in application stream is very similar to a",
    "start": "925120",
    "end": "931519"
  },
  {
    "text": "table it has a schema and basically like a mapping between attributes and their",
    "start": "931519",
    "end": "937560"
  },
  {
    "text": "types um pumps are more interesting pumps allow you to continuously evaluate",
    "start": "937560",
    "end": "943240"
  },
  {
    "text": "queries on top of your streaming data so a pump basically allows you to read data",
    "start": "943240",
    "end": "948480"
  },
  {
    "text": "from one stream and write it to another stream through an SQL query so it basically is a way to just continuously",
    "start": "948480",
    "end": "956639"
  },
  {
    "text": "evaluate SQL query it takes as an argument um an SQL query and continuously",
    "start": "956639",
    "end": "961800"
  },
  {
    "text": "evaluates that SQL query on your behalf and this is how it looks in action so",
    "start": "961800",
    "end": "967680"
  },
  {
    "text": "this is the definition of a stream it's basically just a schema telling me well",
    "start": "967680",
    "end": "973720"
  },
  {
    "text": "there are a couple of attributes um with their types and here's our first query",
    "start": "973720",
    "end": "979759"
  },
  {
    "text": "whoa that was I didn't mean to do that sorry",
    "start": "979759",
    "end": "985560"
  },
  {
    "text": "so this is our first query and if you just look at the highlighted text it's",
    "start": "989800",
    "end": "995160"
  },
  {
    "text": "it looks like regular SQL and it actually is so it's antic compliant SQL we just added a couple of extensions um",
    "start": "995160",
    "end": "1002399"
  },
  {
    "text": "to adapt it for streaming purposes but what does this query well it inserts um the result into a stream called C clean",
    "start": "1002399",
    "end": "1009240"
  },
  {
    "text": "trips it then selects a couple of attributes um from the input stream and",
    "start": "1009240",
    "end": "1014839"
  },
  {
    "text": "has a filter Clause that filters um events that have a pickup and drop off",
    "start": "1014839",
    "end": "1020519"
  },
  {
    "text": "location of zero so as I've mentioned in the introduction um the data I'm receiving is noisy and therefore um we",
    "start": "1020519",
    "end": "1027558"
  },
  {
    "text": "need to filter out um certain events and this is what this query is doing right so a very common SQL query the only",
    "start": "1027559",
    "end": "1034918"
  },
  {
    "text": "special part is um the pump and again the pump just takes care of continuously",
    "start": "1034919",
    "end": "1040918"
  },
  {
    "text": "reeval re-evaluating this query when your data arrives so this is essentially a very",
    "start": "1040919",
    "end": "1047839"
  },
  {
    "text": "simple filter for the",
    "start": "1047839",
    "end": "1051720"
  },
  {
    "text": "application but Kines analytics allows us to do more advanced queries as well",
    "start": "1053760",
    "end": "1059440"
  },
  {
    "text": "so um if you consider any Downstream applications we're ingesting 20,000",
    "start": "1059440",
    "end": "1065400"
  },
  {
    "text": "22,000 events per second into the stream chances are that your Downstream applications are um overwhelmed by that",
    "start": "1065400",
    "end": "1073000"
  },
  {
    "text": "amount of events flowing into your system so usually what you're trying to do is um you don't actually need a",
    "start": "1073000",
    "end": "1079480"
  },
  {
    "text": "complete view of all of the events in your system you um want to have an aggregated view of these events so it",
    "start": "1079480",
    "end": "1085640"
  },
  {
    "text": "it's sufficient to know how many trips you made per second and how much revenue you made per second you don't want to",
    "start": "1085640",
    "end": "1092200"
  },
  {
    "text": "see the revenue of each individual trip so we want to aggregate our data and this is what this query does right so it",
    "start": "1092200",
    "end": "1099440"
  },
  {
    "text": "basically um uses a tumbling window of size one second which basically means every second there is one window and all",
    "start": "1099440",
    "end": "1107480"
  },
  {
    "text": "events in this window are Act at it so what this query does is specifically um it is grouping over these um tumbling",
    "start": "1107480",
    "end": "1116080"
  },
  {
    "text": "windows and it's then counting the number of events in this window it's counting the number of passengers in",
    "start": "1116080",
    "end": "1122480"
  },
  {
    "text": "this window and um the revenue that has been made during this onec window and",
    "start": "1122480",
    "end": "1127840"
  },
  {
    "text": "this query is then evaluated for every onec window um as long as the application is running right um the way",
    "start": "1127840",
    "end": "1135919"
  },
  {
    "text": "I realize um this tumbling window is by the step function and the step function",
    "start": "1135919",
    "end": "1141679"
  },
  {
    "text": "basically just um reduces the Precision of the time stamp of the events so each",
    "start": "1141679",
    "end": "1148840"
  },
  {
    "text": "event um when it's ingested into Kinesis analytics is time stamped and I'm",
    "start": "1148840",
    "end": "1153960"
  },
  {
    "text": "reducing the time stamp to the second so all events that happen within the same second fall into the same time window",
    "start": "1153960",
    "end": "1160720"
  },
  {
    "text": "and if I group by this time I'll end up with one second Windows um that I can",
    "start": "1160720",
    "end": "1165840"
  },
  {
    "text": "use to run my query on top of that right so this query essentially gives me an",
    "start": "1165840",
    "end": "1171640"
  },
  {
    "text": "output stream called trip statistics and this stream instead of having 22,000",
    "start": "1171640",
    "end": "1177600"
  },
  {
    "text": "events per second has only one event per second right so one aggregated event per second is ingested into this um trip",
    "start": "1177600",
    "end": "1185120"
  },
  {
    "text": "trip statistics stream now that we have this we can",
    "start": "1185120",
    "end": "1191600"
  },
  {
    "text": "actually run more complex analytics on top of that I've mentioned well we may want to monitor our Revenue and get",
    "start": "1191600",
    "end": "1199039"
  },
  {
    "text": "alerted when our Revenue dips and one of the really cool things of Kinesis analytics is we have extended the SQL um",
    "start": "1199039",
    "end": "1208240"
  },
  {
    "text": "capabilities by Machine learning algorithms so you can run machine learning algorithms on top of your",
    "start": "1208240",
    "end": "1214840"
  },
  {
    "text": "streaming data and the cool thing is that it's really simple to use right it's just a build-in function you can",
    "start": "1214840",
    "end": "1220919"
  },
  {
    "text": "call so um this random cut forest with explanation function is basically an",
    "start": "1220919",
    "end": "1227120"
  },
  {
    "text": "implementation of the random Forest algorithm on top of streaming data and by just calling this function and",
    "start": "1227120",
    "end": "1233120"
  },
  {
    "text": "pointing it to a Kinesis to an in application stream um that's all you've got to do right and it's basically an",
    "start": "1233120",
    "end": "1239679"
  },
  {
    "text": "unsupervised learning an unsupervised uh animal detection algorithm that comes",
    "start": "1239679",
    "end": "1245600"
  },
  {
    "text": "with Kinesis analytics so this query basically takes a look at various",
    "start": "1245600",
    "end": "1251520"
  },
  {
    "text": "different attributes the most important one is the revenue and whenever it learns the patterns of the stream um the",
    "start": "1251520",
    "end": "1258600"
  },
  {
    "text": "patterns of the events that are occurring and generates an an an anomaly",
    "start": "1258600",
    "end": "1263720"
  },
  {
    "text": "score um that helps us to assess whether that's um common data or something we",
    "start": "1263720",
    "end": "1269520"
  },
  {
    "text": "should take a look at okay so",
    "start": "1269520",
    "end": "1276360"
  },
  {
    "text": "um up until now I've only created in application streams and now I'd like to",
    "start": "1276600",
    "end": "1282159"
  },
  {
    "text": "send this data to cloudwatch so that we can actually visualize the data and create Al alerts on top of these um and",
    "start": "1282159",
    "end": "1288760"
  },
  {
    "text": "therefore I need to send the data that is ingested into in application stream to another Kinesis Stream So that other",
    "start": "1288760",
    "end": "1294640"
  },
  {
    "text": "applications outside of Kinesis analytics can actually read the data so",
    "start": "1294640",
    "end": "1299840"
  },
  {
    "text": "I'll go ahead and create a destination I'll make it a",
    "start": "1299840",
    "end": "1304919"
  },
  {
    "text": "bit smaller um I'll create a destination and",
    "start": "1304919",
    "end": "1310360"
  },
  {
    "text": "this destination is another Kinesis stream containing these um uh anomally",
    "start": "1310360",
    "end": "1315400"
  },
  {
    "text": "scores of the incoming data so I'll call the",
    "start": "1315400",
    "end": "1320519"
  },
  {
    "text": "stream Summit anomaly trips and as this is based on the aggregated",
    "start": "1323720",
    "end": "1330559"
  },
  {
    "text": "data we we only will have one event per second so one chart is more than enough to actually handle the throughput of the",
    "start": "1330559",
    "end": "1336880"
  },
  {
    "text": "events that are going to be written through this um stream so I'll create the stream and once that's",
    "start": "1336880",
    "end": "1344760"
  },
  {
    "text": "done I can connect",
    "start": "1344760",
    "end": "1350000"
  },
  {
    "text": "the in application stream trip statistics anomaly um with this Kines stream so all data that is written to",
    "start": "1350520",
    "end": "1356360"
  },
  {
    "text": "this in application stream will eventually end up in the Kinesis stream I've just created I can choose different",
    "start": "1356360",
    "end": "1363600"
  },
  {
    "text": "um output formats Json or CVS um I'll use Json again Kinesis analytics needs",
    "start": "1363600",
    "end": "1369200"
  },
  {
    "text": "the permissions to actually write to my stream um so now if I hit save and",
    "start": "1369200",
    "end": "1374440"
  },
  {
    "text": "continue we again will repo the application and then the data of the the",
    "start": "1374440",
    "end": "1381840"
  },
  {
    "text": "output of the um random cut Forest algorithm will eventually arrive in the",
    "start": "1381840",
    "end": "1387919"
  },
  {
    "text": "Kinesis stream where it can be picked up by Lambda okay",
    "start": "1387919",
    "end": "1394320"
  },
  {
    "text": "so therefore I'll create a Lambda function and the Lambda function is just basically reading the data from the",
    "start": "1394320",
    "end": "1400520"
  },
  {
    "text": "Kinesis stream convert the format so that it can be sent to the to Cloud watch for visualization purposes so",
    "start": "1400520",
    "end": "1406679"
  },
  {
    "text": "there's nothing really special with this Lambda function it's just a glue between um the Kinesis stream and the cloudwatch",
    "start": "1406679",
    "end": "1413120"
  },
  {
    "text": "dashboard I'm going to build so oops",
    "start": "1413120",
    "end": "1421039"
  },
  {
    "text": "I'll I'll create the",
    "start": "1421480",
    "end": "1425278"
  },
  {
    "text": "function Summit an anomaly again it needs permissions to write in my account",
    "start": "1429440",
    "end": "1438159"
  },
  {
    "text": "I've already prepared the",
    "start": "1443360",
    "end": "1446440"
  },
  {
    "text": "function and once that is ready I'll show you how it looks",
    "start": "1449000",
    "end": "1455520"
  },
  {
    "text": "like okay so the function it's only a couple of lines and again basically what",
    "start": "1455520",
    "end": "1460880"
  },
  {
    "text": "it does is it converts the format of the events of the Kinesis stream to the format that is expected by cloudwatch",
    "start": "1460880",
    "end": "1466960"
  },
  {
    "text": "for ingesting data so there's nothing really special I'm just picking up events and dumping them into cloudwatch",
    "start": "1466960",
    "end": "1473679"
  },
  {
    "text": "so now that the the function is there I can oops I can connect",
    "start": "1473679",
    "end": "1480600"
  },
  {
    "text": "it I can connect the Lambda function to the Kinesis stream so I hit",
    "start": "1480600",
    "end": "1486080"
  },
  {
    "text": "Kinesis pick the right stream to read",
    "start": "1486399",
    "end": "1491600"
  },
  {
    "text": "from and off we go so now that is deployed",
    "start": "1491919",
    "end": "1498880"
  },
  {
    "text": "so what's important to understand here a Kinesis stream is a pool based model right so from a Kinesis stream you write",
    "start": "1498880",
    "end": "1505159"
  },
  {
    "text": "data into a stream and then you re regularly read from the stream um to to",
    "start": "1505159",
    "end": "1511120"
  },
  {
    "text": "obtain the data that has been written into the stream if you connect a Lambda function to a Kinesis stream um we will",
    "start": "1511120",
    "end": "1517880"
  },
  {
    "text": "actually continuously read the data from the Kinesis stream on your behalf and call the Lambda function um whenever",
    "start": "1517880",
    "end": "1524360"
  },
  {
    "text": "there's new data ready so it um converts the pull-based model into a push push base model that you can use to",
    "start": "1524360",
    "end": "1531279"
  },
  {
    "text": "conveniently just um read batches of data out of a Kinesis stream this is",
    "start": "1531279",
    "end": "1537120"
  },
  {
    "text": "currently deployed so we'll again provide some infrastructure that hosts these um the functionality to read the",
    "start": "1537120",
    "end": "1543360"
  },
  {
    "text": "data from the Kinesis stream and call the L function but once that is ready um we can start building a dashboard in",
    "start": "1543360",
    "end": "1550440"
  },
  {
    "text": "cloudwatch that is actually visualizing the data so I'll hover over to the",
    "start": "1550440",
    "end": "1557600"
  },
  {
    "text": "cloudwatch cont conso and create a",
    "start": "1557600",
    "end": "1563320"
  },
  {
    "text": "dashboard call Summit Animal",
    "start": "1570320",
    "end": "1574519"
  },
  {
    "text": "trips and here I'll select um the metrics I've been creating so the data",
    "start": "1577080",
    "end": "1583679"
  },
  {
    "text": "from the kesa stream is ingested into Cloud watch as as a metric as a custom metric and I'm going to use this to",
    "start": "1583679",
    "end": "1590159"
  },
  {
    "text": "build my my dashboard so down here we have the",
    "start": "1590159",
    "end": "1596559"
  },
  {
    "text": "stream name total amount and anomaly",
    "start": "1596559",
    "end": "1602559"
  },
  {
    "text": "score I'm interested into in the sum and I'm using a high resolution dashboard um",
    "start": "1603799",
    "end": "1610200"
  },
  {
    "text": "so I'm reading data with a gr granularity of 1",
    "start": "1610200",
    "end": "1616000"
  },
  {
    "text": "second and now that that it's done we",
    "start": "1616760",
    "end": "1621880"
  },
  {
    "text": "can take a brief look so here here we can see the blue",
    "start": "1621880",
    "end": "1628440"
  },
  {
    "text": "line is the revenue that is generated and we can see these more or less regular patterns in this stream and the",
    "start": "1628440",
    "end": "1636240"
  },
  {
    "text": "orange line is the animal score and it's currently zero because the algorithm",
    "start": "1636240",
    "end": "1641600"
  },
  {
    "text": "takes some time um to train right so it's an online learning algorithm it's unsupervised so it takes sometime and",
    "start": "1641600",
    "end": "1649000"
  },
  {
    "text": "looks it learns the patterns of the stream and once it's finished it'll",
    "start": "1649000",
    "end": "1654080"
  },
  {
    "text": "start providing a score and anomal score that we can use um to determine whether",
    "start": "1654080",
    "end": "1659240"
  },
  {
    "text": "we're dealing with regular data or anomalous data it takes a moment to um",
    "start": "1659240",
    "end": "1665000"
  },
  {
    "text": "to initialize so I've prepared a small screenshot to show you how this looks if",
    "start": "1665000",
    "end": "1671159"
  },
  {
    "text": "you let it run for um some more time so here we can see the blue line again is um the the revenue that is Genera",
    "start": "1671159",
    "end": "1678640"
  },
  {
    "text": "and the orange line is the anomaly score and what's interesting to note is um",
    "start": "1678640",
    "end": "1684360"
  },
  {
    "text": "these are patterns regular patterns changing over a day but between every day um the patterns aren't changing that",
    "start": "1684360",
    "end": "1691320"
  },
  {
    "text": "much right so the anomaly score although we are seeing um dips and um spikes in",
    "start": "1691320",
    "end": "1699440"
  },
  {
    "text": "in traffic the anomaly score um stays more or less um at at the same level up",
    "start": "1699440",
    "end": "1707080"
  },
  {
    "text": "until that moment where spikes um because there are suddenly no taxi trips",
    "start": "1707080",
    "end": "1712240"
  },
  {
    "text": "at all right um and I think that's quite impressive because it's I I've just",
    "start": "1712240",
    "end": "1717360"
  },
  {
    "text": "shown you the you the SQL code it's really it's unsupervised machine learning on your streaming data and I",
    "start": "1717360",
    "end": "1723679"
  },
  {
    "text": "looked at um the data set to figure out what was going on there and it turned out um during that day where we had",
    "start": "1723679",
    "end": "1729760"
  },
  {
    "text": "basically no taxi trips at all um there was a massive snowstorm in New York city so no one was on the streets and",
    "start": "1729760",
    "end": "1736399"
  },
  {
    "text": "therefore no no taxis were running in New York City and um the um ROM cut",
    "start": "1736399",
    "end": "1742480"
  },
  {
    "text": "Forest algorithm actually was able to pick that up right which is not too hard because there's no traffic at all but it",
    "start": "1742480",
    "end": "1748399"
  },
  {
    "text": "can also pick up more subtle changes in our traffic patterns right",
    "start": "1748399",
    "end": "1756000"
  },
  {
    "text": "okay so this now it's somewhat",
    "start": "1756080",
    "end": "1762278"
  },
  {
    "text": "initialized let's move back okay so this basically concludes the real time",
    "start": "1762720",
    "end": "1768440"
  },
  {
    "text": "portion of the architecture right we ingested data cleaned it um ran the random cast far Al algorithm on top of",
    "start": "1768440",
    "end": "1775480"
  },
  {
    "text": "the data ingested it to a Kinesis stream use the Lambda function to dump to cloudwatch and then build a b dashboard",
    "start": "1775480",
    "end": "1782039"
  },
  {
    "text": "on top of this data now I'm going to build the batch um",
    "start": "1782039",
    "end": "1787559"
  },
  {
    "text": "part of the architecture so to this end I'm going to write the cleaned data",
    "start": "1787559",
    "end": "1794240"
  },
  {
    "text": "coming from my Kinesis analytics application to S3 and the way I'm going to do this is by means of Kinesis fire",
    "start": "1794240",
    "end": "1800360"
  },
  {
    "text": "hose so Kinesis fire hose is a service that at the one end at the ingestion end",
    "start": "1800360",
    "end": "1805640"
  },
  {
    "text": "looks like a stream so we can easily send 20,000 records per second into this",
    "start": "1805640",
    "end": "1810760"
  },
  {
    "text": "um fire host delivery stream and fire host then collects and batches the incoming events and rides large batches",
    "start": "1810760",
    "end": "1818799"
  },
  {
    "text": "of events into the destination um and in our case I'm going to use S3 as as the destination so I'll",
    "start": "1818799",
    "end": "1827120"
  },
  {
    "text": "go back to my analytics",
    "start": "1827120",
    "end": "1830519"
  },
  {
    "text": "application and I'll connect a new destination and this time I'm going to",
    "start": "1834679",
    "end": "1842240"
  },
  {
    "text": "create a fire host delivery stream the stream needs a name as well",
    "start": "1842240",
    "end": "1850000"
  },
  {
    "text": "so Summit cleaned strips",
    "start": "1850000",
    "end": "1856240"
  },
  {
    "text": "okay and very similar to Kines analytics I can now specify a Lambda function as a",
    "start": "1859880",
    "end": "1865399"
  },
  {
    "text": "pre-processing step so events are sent to um Kinesis fire hose they are then",
    "start": "1865399",
    "end": "1871240"
  },
  {
    "text": "processed by a Lambda function and then persisted to the destination again I don't need this for this particular demo",
    "start": "1871240",
    "end": "1877480"
  },
  {
    "text": "but that's a convenient option you have more interestingly fire hose also allows",
    "start": "1877480",
    "end": "1882919"
  },
  {
    "text": "you to convert the data into a colmer format so what you can do here is um you",
    "start": "1882919",
    "end": "1890960"
  },
  {
    "text": "can convert the data that is ingested to S3 into par or osc um on the fly so this",
    "start": "1890960",
    "end": "1899039"
  },
  {
    "text": "this something that's natively supported by fire hose um those are colar formats",
    "start": "1899039",
    "end": "1904880"
  },
  {
    "text": "and I don't have the time to go into the details but tomorrow at 400 p.m. there is a talk in Hall 2 I think",
    "start": "1904880",
    "end": "1912519"
  },
  {
    "text": "um from a colleague of mine that discusses in detail um why it's a good",
    "start": "1912519",
    "end": "1919039"
  },
  {
    "text": "idea to store data on S3 in p and OC um basically um for performance but also",
    "start": "1919039",
    "end": "1926080"
  },
  {
    "text": "for cost reasons um so I'll just use that for now and you can figure out the",
    "start": "1926080",
    "end": "1933080"
  },
  {
    "text": "details in the in the talk tomorrow um one thing I need to specify is the schema of the data right so again these",
    "start": "1933080",
    "end": "1940880"
  },
  {
    "text": "conversions want to have some schema information so that they know how to con actually convert the data um so I'm",
    "start": "1940880",
    "end": "1947320"
  },
  {
    "text": "going to use use um a glue AWS glue data catalog entry I've already prepared and",
    "start": "1947320",
    "end": "1952360"
  },
  {
    "text": "I'll show you in a second how to create these um these schema information by crawling the data so I'll just",
    "start": "1952360",
    "end": "1960000"
  },
  {
    "text": "use something prepared right now and I'll show you in a second how to come up",
    "start": "1960000",
    "end": "1966320"
  },
  {
    "text": "with a schema by yourself so I specify the schema um if",
    "start": "1966320",
    "end": "1972080"
  },
  {
    "text": "I'm converting to par I can only pick S3 as a destination but in principle you can use um red shift elastic Surge and",
    "start": "1972080",
    "end": "1979880"
  },
  {
    "text": "Splunk as your own destinations as well um I need to specify in a three",
    "start": "1979880",
    "end": "1986399"
  },
  {
    "text": "bucket so I'll pick I don't know New York City clean",
    "start": "1986399",
    "end": "1992440"
  },
  {
    "text": "trips um and now I can specify an S3 prefix so um the again the events that I",
    "start": "1992440",
    "end": "2001519"
  },
  {
    "text": "ingested into fire hose are time stamped when they ingested and we can now specify a pattern um how these this data",
    "start": "2001519",
    "end": "2008440"
  },
  {
    "text": "should be partitioned on S3 and again this is an optimization technique that can be used um to save cost and and um",
    "start": "2008440",
    "end": "2016639"
  },
  {
    "text": "increase the performance of your queries when you're running SQL queries on top of SQL so um by default the data is",
    "start": "2016639",
    "end": "2024519"
  },
  {
    "text": "petitioned by year month and day but um last week we introduced capabilities to",
    "start": "2024519",
    "end": "2030080"
  },
  {
    "text": "specify your custom prefix right so that seems subtle but in my world this is a",
    "start": "2030080",
    "end": "2035919"
  },
  {
    "text": "big deal because it makes um things Downstream much more straightforward so I'll use that and just",
    "start": "2035919",
    "end": "2044120"
  },
  {
    "text": "specify a custom prefix where I can basically specify a pattern and say well",
    "start": "2044600",
    "end": "2049878"
  },
  {
    "text": "I want to partition my data by year month and day but please make it year equals the the actual year month equals",
    "start": "2049879",
    "end": "2056480"
  },
  {
    "text": "the month and so on right",
    "start": "2056480",
    "end": "2061280"
  },
  {
    "text": "okay I'll do the same thing for arrrow so in case um um errors happen happen",
    "start": "2063040",
    "end": "2069440"
  },
  {
    "text": "during the conversion and these events are sorted out into a special prefix on S3 so that I can deal with them",
    "start": "2069440",
    "end": "2076599"
  },
  {
    "text": "later I have the option to dump the raw data to a different S3 bucket if I",
    "start": "2076599",
    "end": "2081839"
  },
  {
    "text": "wanted to so this is in particular useful if you use a Lambda function and to pre-process the data if you want to",
    "start": "2081839",
    "end": "2088720"
  },
  {
    "text": "preserve the raw original data you can do that as well",
    "start": "2088720",
    "end": "2094440"
  },
  {
    "text": "okay so last but not least um you can control the buffer conditions and the",
    "start": "2094440",
    "end": "2100359"
  },
  {
    "text": "buffer conditions basically specify how often fire hose will um dump the data to",
    "start": "2100359",
    "end": "2106680"
  },
  {
    "text": "S3 or the the respective destination so you can control how frequently the data",
    "start": "2106680",
    "end": "2111880"
  },
  {
    "text": "arrives um at the destination and usually you're interested in um having",
    "start": "2111880",
    "end": "2118520"
  },
  {
    "text": "rather large um buffer conditions so that you end up with large objects on S3",
    "start": "2118520",
    "end": "2124240"
  },
  {
    "text": "um so usually um using just the defaults of 120 megabytes and 300 seconds is good",
    "start": "2124240",
    "end": "2130280"
  },
  {
    "text": "however for the purpose of this demo I'll shorten it to 60 seconds uh which is the smallest um period you can",
    "start": "2130280",
    "end": "2138960"
  },
  {
    "text": "specify if I weren't um converting data to P I could also specify um compression",
    "start": "2140480",
    "end": "2147040"
  },
  {
    "text": "and encryption options I'm not using that and as a very last step I need to",
    "start": "2147040",
    "end": "2152560"
  },
  {
    "text": "again authentic authorize the fire host service to read and read data from my my",
    "start": "2152560",
    "end": "2157760"
  },
  {
    "text": "Kinesis stream and write it um to",
    "start": "2157760",
    "end": "2163880"
  },
  {
    "text": "S3 okay I get to review my choice and",
    "start": "2165400",
    "end": "2170480"
  },
  {
    "text": "now the fire host delivery stream is created under the hood for",
    "start": "2170480",
    "end": "2176240"
  },
  {
    "text": "me so now that that is in place I can come back to my Kines analytics application and connect the in",
    "start": "2176280",
    "end": "2183079"
  },
  {
    "text": "application stream clean trips with the Kinesis fire host delivery stream so I'll pick the",
    "start": "2183079",
    "end": "2190079"
  },
  {
    "text": "stream and choose the appropriate interpretation stream I want to send Json data again it's fine if KES",
    "start": "2190079",
    "end": "2198720"
  },
  {
    "text": "analytics writes to my fire host delivery stream I hit save and continue",
    "start": "2198720",
    "end": "2204280"
  },
  {
    "text": "and now the Kines analytics application is redeployed which takes about a minute",
    "start": "2204280",
    "end": "2210359"
  },
  {
    "text": "and the fireos delivery stream will start buffering data for a minute um and",
    "start": "2210359",
    "end": "2215599"
  },
  {
    "text": "then eventually events uh excuse me objects will um be sent to S3 so it",
    "start": "2215599",
    "end": "2222319"
  },
  {
    "text": "takes a brief moment and while we are waiting for that I want to show you a couple of metrics of the Kinesis stream",
    "start": "2222319",
    "end": "2229800"
  },
  {
    "text": "that are important to be aware of um so that you don't run into throttling um errors of Errors caused by throttling so",
    "start": "2229800",
    "end": "2237599"
  },
  {
    "text": "we'll take a brief look at um the kesa stream that is um that is receiving",
    "start": "2237599",
    "end": "2243119"
  },
  {
    "text": "these 20,000 events per second and as you can see it gives us",
    "start": "2243119",
    "end": "2248400"
  },
  {
    "text": "quite some metrics and the most important metrics are actually",
    "start": "2248400",
    "end": "2253800"
  },
  {
    "text": "down here um these are metrics for read throughput exceeded and write throughput",
    "start": "2253800",
    "end": "2259720"
  },
  {
    "text": "exceeded exceptions and basically as I've mentioned a Kinesis stream is composed of shards and you can only send",
    "start": "2259720",
    "end": "2265960"
  },
  {
    "text": "a thousand events per um second into each shart um I've provisioned a stream",
    "start": "2265960",
    "end": "2272720"
  },
  {
    "text": "with 30 shards so I can send at most 30,000 um individual events into my",
    "start": "2272720",
    "end": "2279040"
  },
  {
    "text": "stream if I try to send more my requests will get throttled and if that happens",
    "start": "2279040",
    "end": "2284400"
  },
  {
    "text": "it's a sign for me um that I need to scale my my stream because it can no longer absorb the incoming um the",
    "start": "2284400",
    "end": "2290880"
  },
  {
    "text": "incoming events and these are metrics that actually help you to to determine",
    "start": "2290880",
    "end": "2296079"
  },
  {
    "text": "am I under provision is my stream underprovision or over provisioned so um I highly encourage you take a look at",
    "start": "2296079",
    "end": "2301960"
  },
  {
    "text": "the read throughput exceeded metric and the right throughput exceeded metric because they indicate whether you have",
    "start": "2301960",
    "end": "2307480"
  },
  {
    "text": "underprovision your stream another very interesting metric is the get records iterator age metric",
    "start": "2307480",
    "end": "2316160"
  },
  {
    "text": "and this matri metric tells you how far behind the processing is um to the tip",
    "start": "2316160",
    "end": "2321760"
  },
  {
    "text": "of the stream and as you can see um in the beginning we were doing quite well so um the read the get records iterator",
    "start": "2321760",
    "end": "2330280"
  },
  {
    "text": "age was Zero but suddenly it spiked to um 400 and this happened because I",
    "start": "2330280",
    "end": "2337440"
  },
  {
    "text": "redeploy the application when I redeploy the application it stops reading from the stream for some time and when it's",
    "start": "2337440",
    "end": "2345000"
  },
  {
    "text": "actually up and running again it starts catching up and this is what you can see here right so once the application was",
    "start": "2345000",
    "end": "2351920"
  },
  {
    "text": "running again it started catching up and um eventually was close to zero again",
    "start": "2351920",
    "end": "2357680"
  },
  {
    "text": "but again this is a very important metric to check because it indicates um it can tell you if your processing is",
    "start": "2357680",
    "end": "2364280"
  },
  {
    "text": "falling behind and if you need to take counter measures to actually accommodate for a twoo high volume",
    "start": "2364280",
    "end": "2371359"
  },
  {
    "text": "stream okay so by now I hope",
    "start": "2371359",
    "end": "2377480"
  },
  {
    "text": "that um the First Data has arrived in S3 so we can quickly",
    "start": "2377480",
    "end": "2383960"
  },
  {
    "text": "check yeah that looks good so um you can see some data obviously has arrived so",
    "start": "2387720",
    "end": "2394560"
  },
  {
    "text": "it has been ingested 2019 February 26th at 11:00 so these are UTC times all is",
    "start": "2394560",
    "end": "2401440"
  },
  {
    "text": "good and we have our first object sitting on S3 and what I'd like to do in the next",
    "start": "2401440",
    "end": "2408319"
  },
  {
    "text": "couple of minutes is first of all query the data um through SQL and then build a",
    "start": "2408319",
    "end": "2413760"
  },
  {
    "text": "dashboard for long-term analytics so in order to be able to create the data",
    "start": "2413760",
    "end": "2419280"
  },
  {
    "text": "again I need to have a schema so that the query engine need knows how to um",
    "start": "2419280",
    "end": "2425720"
  },
  {
    "text": "how to read and interpret the data um and the service I'm going to use to discover the schema is called AWS glue",
    "start": "2425720",
    "end": "2433520"
  },
  {
    "text": "data catalog so the data catalog is a hi compatible meter store and it basically",
    "start": "2433520",
    "end": "2440079"
  },
  {
    "text": "contains um and it basically contains meter information um that can be used by",
    "start": "2440079",
    "end": "2447400"
  },
  {
    "text": "other services such as Athena or R Spectrum um or EMR to obtain information",
    "start": "2447400",
    "end": "2453200"
  },
  {
    "text": "where the data is located so that you can create a table and don't need to take care of memorizing which objects",
    "start": "2453200",
    "end": "2459640"
  },
  {
    "text": "belong to this table or um what's the the schema of these of",
    "start": "2459640",
    "end": "2465200"
  },
  {
    "text": "these objects um so I can either manually specify the schema or I can use",
    "start": "2465200",
    "end": "2470800"
  },
  {
    "text": "crawlers to actually infer the schema for me and that's what I'm going to do so I'll add a",
    "start": "2470800",
    "end": "2476640"
  },
  {
    "text": "crawler crawler name is Summit cleaned",
    "start": "2476640",
    "end": "2483838"
  },
  {
    "text": "drips I can now just point the crawler to an S3 bucket so I'll choose the one",
    "start": "2483880",
    "end": "2490079"
  },
  {
    "text": "um we're ingesting data to one data store is fine for now again",
    "start": "2490079",
    "end": "2499839"
  },
  {
    "text": "the service needs to have permissions to read the data from S3 so I'll just specify a",
    "start": "2499839",
    "end": "2505319"
  },
  {
    "text": "role I can then um configure the Cadence um on which this crawler is run so you",
    "start": "2505319",
    "end": "2511280"
  },
  {
    "text": "can either manually run the crawler which I'm going to do um right now but you can also specify well this crawler",
    "start": "2511280",
    "end": "2516960"
  },
  {
    "text": "should run every hour or every day to discover new new data that may have um",
    "start": "2516960",
    "end": "2522880"
  },
  {
    "text": "been ingested since the last run so run on demand is fine for now I'll create a",
    "start": "2522880",
    "end": "2529760"
  },
  {
    "text": "new database Summit clean",
    "start": "2529760",
    "end": "2537800"
  },
  {
    "text": "drips um and I confirm and now the crawler has been created and I can run",
    "start": "2537800",
    "end": "2545400"
  },
  {
    "text": "it to discover the schema for me so what now happens in the background is basically we spin up a cluster of",
    "start": "2545400",
    "end": "2552400"
  },
  {
    "text": "machines um deploy these crawlers these crawlers will then read your data sample",
    "start": "2552400",
    "end": "2558520"
  },
  {
    "text": "your data and infer the the schema of this data for you create a table in the",
    "start": "2558520",
    "end": "2565200"
  },
  {
    "text": "database I've just created so that we can then use other services to create",
    "start": "2565200",
    "end": "2570720"
  },
  {
    "text": "the the the data just be aware when I say it creates a table it's just about",
    "start": "2570720",
    "end": "2576319"
  },
  {
    "text": "metadata so it's only storing the schema information and the location of the the",
    "start": "2576319",
    "end": "2581440"
  },
  {
    "text": "actual data it's not um about storing the actual data it's just um a connection between just a service that",
    "start": "2581440",
    "end": "2588800"
  },
  {
    "text": "stores metad data um on your behalf and integrates with other AWS services so it",
    "start": "2588800",
    "end": "2594160"
  },
  {
    "text": "can easily query your your tables or your data so the crawler is still running but",
    "start": "2594160",
    "end": "2601520"
  },
  {
    "text": "it should should complete in a second usually takes",
    "start": "2601520",
    "end": "2608000"
  },
  {
    "text": "less than a minute okay so it created a table and we",
    "start": "2608000",
    "end": "2614640"
  },
  {
    "text": "can take a look so I'll oops sorry for that I'll pick the right database and we",
    "start": "2614640",
    "end": "2622240"
  },
  {
    "text": "here we have the um table containing the clean data that has been written to S3",
    "start": "2622240",
    "end": "2628520"
  },
  {
    "text": "by fire hose so if we take a look um by now it contains the bucket contains four",
    "start": "2628520",
    "end": "2634160"
  },
  {
    "text": "objects we have red close to a million um individual events",
    "start": "2634160",
    "end": "2642040"
  },
  {
    "text": "um and here is the schema that should be familiar by now of the data that has",
    "start": "2642040",
    "end": "2647240"
  },
  {
    "text": "been sent to S3 and this is by the way um exactly the way I created the schema",
    "start": "2647240",
    "end": "2653079"
  },
  {
    "text": "I specified for Kinesis fire hose right so it fire hose integrates with the glue",
    "start": "2653079",
    "end": "2658440"
  },
  {
    "text": "data catalog and in this way I was able to infer the schema and point fire hose",
    "start": "2658440",
    "end": "2664359"
  },
  {
    "text": "um to the right schema information okay so now that we have a schema we can",
    "start": "2664359",
    "end": "2671000"
  },
  {
    "text": "start querying the um table so I'll use",
    "start": "2671000",
    "end": "2677000"
  },
  {
    "text": "um Amazon Athena to this end so Amazon Athena is a managed Presto service so it",
    "start": "2677000",
    "end": "2682079"
  },
  {
    "text": "basically allows you to run SQL queries directly on top of the data sitting in",
    "start": "2682079",
    "end": "2687680"
  },
  {
    "text": "S3 and now um I guess you'll see why it's very convenient to have um",
    "start": "2687680",
    "end": "2694280"
  },
  {
    "text": "the schema information in glue because now I can just say I want to create I",
    "start": "2694280",
    "end": "2699839"
  },
  {
    "text": "want to query this particular table I don't need to know which objects are part of this table what's the schema and",
    "start": "2699839",
    "end": "2705280"
  },
  {
    "text": "so on it's all hidden um in the in the schema information from the catalog so I",
    "start": "2705280",
    "end": "2711559"
  },
  {
    "text": "can basically hit um select star um limit 10 and now I",
    "start": "2711559",
    "end": "2718880"
  },
  {
    "text": "receive a couple of um entries of the data sitting in",
    "start": "2718880",
    "end": "2724880"
  },
  {
    "text": "S3 I can then come with more um I can then go ahead and um discover the data",
    "start": "2724880",
    "end": "2731480"
  },
  {
    "text": "so I can formulate more complex query so I can",
    "start": "2731480",
    "end": "2738240"
  },
  {
    "text": "count the number of events and group it I don't know by the",
    "start": "2738240",
    "end": "2743720"
  },
  {
    "text": "hour let's say",
    "start": "2743720",
    "end": "2747520"
  },
  {
    "text": "so I'll do account and then I need a grouping group",
    "start": "2758520",
    "end": "2764680"
  },
  {
    "text": "whoops Group by date and order by",
    "start": "2764680",
    "end": "2770838"
  },
  {
    "text": "date so what this basically does is it groups the data by hour and determines",
    "start": "2773920",
    "end": "2779559"
  },
  {
    "text": "how many events have been received in this particular hour and as you would",
    "start": "2779559",
    "end": "2785119"
  },
  {
    "text": "imagine um in the evening U more events happen compared to during night so at 5",
    "start": "2785119",
    "end": "2792880"
  },
  {
    "text": "a.m. uh much fewer Tex strips have been made compared to the day right um so now",
    "start": "2792880",
    "end": "2800720"
  },
  {
    "text": "I have a very convenient way of just discovering the data I can just run SQL queries and I only pay for the amount of",
    "start": "2800720",
    "end": "2808160"
  },
  {
    "text": "data that is scanned by these queries so I can use S3 as a very cheap storage um option and then create the data at hog",
    "start": "2808160",
    "end": "2815520"
  },
  {
    "text": "on demand okay so last but not least I'm going to",
    "start": "2815520",
    "end": "2820760"
  },
  {
    "text": "build a dashboard um with quicksite quick side is our bi tool that allows you to to",
    "start": "2820760",
    "end": "2828040"
  },
  {
    "text": "build um interactive dashboards so um I'll basically import the same data and",
    "start": "2828040",
    "end": "2834880"
  },
  {
    "text": "build a dashboard um to visualize it so I'll create a new data set and I'm",
    "start": "2834880",
    "end": "2841760"
  },
  {
    "text": "going to use aena to actually query the data sitting on S3 so I'll pick Aina as",
    "start": "2841760",
    "end": "2848319"
  },
  {
    "text": "a um as the source of my",
    "start": "2848319",
    "end": "2852200"
  },
  {
    "text": "data Summit clean strips and now again it's very",
    "start": "2855200",
    "end": "2861400"
  },
  {
    "text": "convenient to have this centralized catalog because now I can simply again just um specify the table I'm interested",
    "start": "2861400",
    "end": "2869119"
  },
  {
    "text": "in and again I don't need to specify any objects or any schema information I just say I want to import this table into",
    "start": "2869119",
    "end": "2875319"
  },
  {
    "text": "Quick site so I'll do that and hit",
    "start": "2875319",
    "end": "2881640"
  },
  {
    "text": "visualize and I can now start building my dashboard so let's say I want to",
    "start": "2884880",
    "end": "2890000"
  },
  {
    "text": "monitor um the revenue and the number of passengers over time so I pick as",
    "start": "2890000",
    "end": "2896440"
  },
  {
    "text": "attributes passenger count total amount and I don't know drop off dat time and",
    "start": "2896440",
    "end": "2903280"
  },
  {
    "text": "quick side will then come up with a visualization that the things makes the most sense um it just takes a second because I",
    "start": "2903280",
    "end": "2909119"
  },
  {
    "text": "ticked an option option to import the data into spice so what happens under",
    "start": "2909119",
    "end": "2914160"
  },
  {
    "text": "the hood here is um quicksite will make a call to Athena Athena will read the",
    "start": "2914160",
    "end": "2919599"
  },
  {
    "text": "data from S3 put it back to quicksite Quick Set will then import the data into",
    "start": "2919599",
    "end": "2924839"
  },
  {
    "text": "spice which is our inmemory query engine um so that we can build fast and",
    "start": "2924839",
    "end": "2930200"
  },
  {
    "text": "interactive dashboards um the data is cached there and once it's imported um",
    "start": "2930200",
    "end": "2935559"
  },
  {
    "text": "we can start seeing our visualizations and it's thankfully already completed",
    "start": "2935559",
    "end": "2940799"
  },
  {
    "text": "and it's an incremental process so we can start visualizing the data although the import process is still taking place",
    "start": "2940799",
    "end": "2947400"
  },
  {
    "text": "so if I change this from a daily Aggregates to hourly Aggregates we should see these",
    "start": "2947400",
    "end": "2954400"
  },
  {
    "text": "regular patterns um uh that we've seen before right so we have usually a spike",
    "start": "2954400",
    "end": "2960319"
  },
  {
    "text": "you know we have some huge spikes right here I have no idea what's happening there but here we can see you have usually a spike in the morning and then",
    "start": "2960319",
    "end": "2966240"
  },
  {
    "text": "a in in the evening um and during night not too many taxi trips happen Okay cool um so that's it from",
    "start": "2966240",
    "end": "2976040"
  },
  {
    "text": "the demo part let me just briefly conclude um and then I'll open it up for",
    "start": "2976040",
    "end": "2984200"
  },
  {
    "text": "questions so here's basically what we've built we started ingesting events into a",
    "start": "2984200",
    "end": "2989599"
  },
  {
    "text": "Kinesis stream we read the data from Kinesis um by means of Kinesis analytics",
    "start": "2989599",
    "end": "2994839"
  },
  {
    "text": "we cleaned the data we ran machine learning on top of the streaming data we built a dashboard for real-time",
    "start": "2994839",
    "end": "3001880"
  },
  {
    "text": "visualization we then dumped the clean data um to S3 by means of Kinesis fire",
    "start": "3001880",
    "end": "3007839"
  },
  {
    "text": "hose um we used the glue data catalog to infer the schema information um ran SQL",
    "start": "3007839",
    "end": "3015040"
  },
  {
    "text": "queries on top of the data sitting in S3 through Amazon Athena and then used quick side um to build um an interactive",
    "start": "3015040",
    "end": "3022480"
  },
  {
    "text": "dashboard to visualize the data so again I've brushed over the piece um where um",
    "start": "3022480",
    "end": "3029720"
  },
  {
    "text": "fire hose converts the data to paret if you're interested in more um there's a",
    "start": "3029720",
    "end": "3035440"
  },
  {
    "text": "session tomorrow query you data in S3 with SQL and optimize for cost and performance tomorrow at 4:00 in Hall",
    "start": "3035440",
    "end": "3043359"
  },
  {
    "text": "to and that's it from my side please complete the um survey in the app it's",
    "start": "3043359",
    "end": "3049240"
  },
  {
    "text": "basically just um hit a couple of stars and that's it um I'm already out of time but I'll keep I'll stick here for more",
    "start": "3049240",
    "end": "3055319"
  },
  {
    "text": "questions if there are any thanks a lot",
    "start": "3055319",
    "end": "3060079"
  }
]