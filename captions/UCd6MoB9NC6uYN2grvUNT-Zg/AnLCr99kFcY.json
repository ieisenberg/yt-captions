[
  {
    "start": "0",
    "end": "20000"
  },
  {
    "text": "hope everybody's having a great reinvent my name is mark Olson I'm a principal software engineer focusing on storage",
    "start": "0",
    "end": "5879"
  },
  {
    "text": "and virtualization and this is the deep dive on Amazon EBS and I've got Jody",
    "start": "5879",
    "end": "11010"
  },
  {
    "text": "give me I lead a product management team for EBS so I'll be doing the first part of the session anyway to kick it off I wanted",
    "start": "11010",
    "end": "18449"
  },
  {
    "text": "to start by just touching really quickly on the three main types of storage services at AWS offers so storage",
    "start": "18449",
    "end": "26599"
  },
  {
    "start": "20000",
    "end": "20000"
  },
  {
    "text": "obviously the best one where your operating system is interacting at a",
    "start": "26599",
    "end": "32160"
  },
  {
    "text": "byte level with a device transferring data and fixed block sizes so obviously",
    "start": "32160",
    "end": "38370"
  },
  {
    "text": "EBS is the service that equates to that then we got file based storage and with",
    "start": "38370",
    "end": "43829"
  },
  {
    "text": "file rather than interacting directly to a byte level we you're abstracted at the",
    "start": "43829",
    "end": "49200"
  },
  {
    "text": "layer using file system semantics like NFS so our service there is EFS then",
    "start": "49200",
    "end": "54809"
  },
  {
    "text": "lastly there's object storage which is abstracted even a layer further and there your application is talks restful",
    "start": "54809",
    "end": "63149"
  },
  {
    "text": "interface like HTTP with a web service with so within block we've got three",
    "start": "63149",
    "end": "72119"
  },
  {
    "start": "70000",
    "end": "70000"
  },
  {
    "text": "main types there's ec2 local instance storage then there is SSD backed EBS",
    "start": "72119",
    "end": "78330"
  },
  {
    "text": "volumes and HDD back EBS volumes so just",
    "start": "78330",
    "end": "83400"
  },
  {
    "text": "first a couple notes on instant storage was it different first of all with instant storage you're",
    "start": "83400",
    "end": "89040"
  },
  {
    "text": "dealing with physical disks these are local to the physical host that's hosting your instance so they act",
    "start": "89040",
    "end": "95939"
  },
  {
    "text": "exactly as you would expect a locally attached HDD or SSD drive to behave this",
    "start": "95939",
    "end": "101280"
  },
  {
    "text": "is not persistent storage so your data will not survive a instant stop or",
    "start": "101280",
    "end": "106439"
  },
  {
    "text": "termination although it will persist through a reboot and data is not",
    "start": "106439",
    "end": "111659"
  },
  {
    "text": "replicated by default there is no integrated support for backups like with EBS snapshots and there's no you know",
    "start": "111659",
    "end": "119070"
  },
  {
    "text": "out-of-the-box encryption however if you want any of those three things you can go ahead and build it yourself on top of",
    "start": "119070",
    "end": "124530"
  },
  {
    "text": "local storage so how is EBS different EBS is block storage as a service with",
    "start": "124530",
    "end": "131790"
  },
  {
    "start": "126000",
    "end": "126000"
  },
  {
    "text": "an API call you can configure set amount of volume blocks with specific performance characteristics and",
    "start": "131790",
    "end": "137670"
  },
  {
    "text": "attach them to your ec2 instance it is a service delivered over the network so",
    "start": "137670",
    "end": "142950"
  },
  {
    "text": "that's important when sizing your instances and thinking about performance",
    "start": "142950",
    "end": "147980"
  },
  {
    "text": "but most importantly it is a service it is not a drive so EBS does not act",
    "start": "147980",
    "end": "153870"
  },
  {
    "text": "exactly as you'd expect a local HDD or STD to behave we don't go when you call our API and",
    "start": "153870",
    "end": "160110"
  },
  {
    "text": "like grab an SSD and say ok this one's for you rather BBS is a massive distributed system your EBS volume is a",
    "start": "160110",
    "end": "167790"
  },
  {
    "text": "logical volume that's comprised of blocks that are spread across many many many physical devices but because we're",
    "start": "167790",
    "end": "175530"
  },
  {
    "text": "a distributed system were able to offer performance and availability characteristics that are beyond what we",
    "start": "175530",
    "end": "181410"
  },
  {
    "text": "could if we were just mapping a volume to drive specifically so EBS exists in a",
    "start": "181410",
    "end": "188730"
  },
  {
    "text": "single availability zone volumes are entirely independent of ec2 so this is",
    "start": "188730",
    "end": "195690"
  },
  {
    "text": "persistent storage you can blow up your ec2 instance your EBS data will still be there but most importantly I think EBS",
    "start": "195690",
    "end": "203220"
  },
  {
    "text": "allows you to right side the act amount and type of storage that you want with",
    "start": "203220",
    "end": "208590"
  },
  {
    "text": "the exact instance that's right for your workload so you don't have to go with whatever you know instance and storage",
    "start": "208590",
    "end": "216120"
  },
  {
    "text": "happens to be bundled with your storage optimized ec2 instance you can detach and attach a volume to another ec2",
    "start": "216120",
    "end": "224459"
  },
  {
    "text": "instance within the same availability zone and the benefit there is that it allows you to recover basically for",
    "start": "224459",
    "end": "231150"
  },
  {
    "text": "example from instance failures so a little bit more a little bit more on EBS",
    "start": "231150",
    "end": "238019"
  },
  {
    "text": "you can attach EBS volumes to only one ec2 instance but you can attach a ton of",
    "start": "238019",
    "end": "243840"
  },
  {
    "text": "EBS volumes to a single the ec2 instance and people regularly do that when you need more performance than a single",
    "start": "243840",
    "end": "249780"
  },
  {
    "text": "volume can provide we do that you separate your boot and your data volumes so keep your boot volumes small move",
    "start": "249780",
    "end": "257700"
  },
  {
    "text": "your data volumes around as necessary I'm going to go in a little more depth on the EBS volume types so we've got two",
    "start": "257700",
    "end": "265050"
  },
  {
    "text": "main types of EBS there is solid state drive fact EBS volumes and hard disk drive fact EBS",
    "start": "265050",
    "end": "272630"
  },
  {
    "text": "volumes and there are some differences that are related to the physical",
    "start": "272630",
    "end": "277910"
  },
  {
    "text": "characteristics of the underlying media so for take SSDs for example with SSDs",
    "start": "277910",
    "end": "284380"
  },
  {
    "text": "you there are flash based so there are no moving parts you know the more banks",
    "start": "284380",
    "end": "289639"
  },
  {
    "text": "of flash you stack in the more parallel i/o you can drive to an SSD all points",
    "start": "289639",
    "end": "295550"
  },
  {
    "text": "on an SSD are equally accessible which makes them fantastic for random i/o you",
    "start": "295550",
    "end": "302300"
  },
  {
    "text": "know contrast that to HDDs where you do have a lot of moving parts in order to",
    "start": "302300",
    "end": "307490"
  },
  {
    "text": "read your data you actually have to move the disk head to the location you need to get to and that movement or seek",
    "start": "307490",
    "end": "314300"
  },
  {
    "text": "times are some of the most expensive and performance impacting actions you can take on an HDD which make them",
    "start": "314300",
    "end": "320449"
  },
  {
    "text": "absolutely terrible for small random i/o on the other hand once you do get that",
    "start": "320449",
    "end": "325699"
  },
  {
    "text": "head into the right place you HDDs allow you to get fantastic throughput for",
    "start": "325699",
    "end": "330889"
  },
  {
    "text": "forints on sequential i/o at a really really low price point which is why we",
    "start": "330889",
    "end": "335960"
  },
  {
    "text": "recently introduced two new through HDD packed volume types which I'll talk about later so here's a whole portfolio",
    "start": "335960",
    "end": "341870"
  },
  {
    "text": "on SSD we've got GP 2 and i/o one on the HDD back volumes we got s t1 and SC 1",
    "start": "341870",
    "end": "349000"
  },
  {
    "text": "how do you choose the right volume type the first and most important consideration is what aspect of",
    "start": "349000",
    "end": "356240"
  },
  {
    "text": "performance is most important to you is it I absolutely if it's I ops and the",
    "start": "356240",
    "end": "362120"
  },
  {
    "text": "question is well how much if you need over 80,000 I ops which is the current EBS optimized maximum then you want to",
    "start": "362120",
    "end": "369590"
  },
  {
    "text": "go with something like an i3 if you're ok with in what EBS can do then your",
    "start": "369590",
    "end": "375169"
  },
  {
    "text": "next question is one of latency do you need microsecond latencies if so again",
    "start": "375169",
    "end": "380330"
  },
  {
    "text": "go with a local SSD if you're okay with millisecond latency then you've got a",
    "start": "380330",
    "end": "385699"
  },
  {
    "text": "cost versus performance trade-off so if cost is more important for you than try",
    "start": "385699",
    "end": "392150"
  },
  {
    "text": "out GP - sorry my voice is going a little hoarse after several days in",
    "start": "392150",
    "end": "397460"
  },
  {
    "start": "394000",
    "end": "394000"
  },
  {
    "text": "Vegas so gp2 works for almost any war club",
    "start": "397460",
    "end": "404050"
  },
  {
    "text": "truly type like okay he's okay now all",
    "start": "404050",
    "end": "420080"
  },
  {
    "text": "right so a general-purpose volume type and we designed it to be that way so in 2014 when we're making GP to our",
    "start": "420080",
    "end": "426650"
  },
  {
    "text": "research scientists went out and profiled io patterns across the entire EBS fleet and they ran a ton of",
    "start": "426650",
    "end": "434480"
  },
  {
    "text": "simulations and modeling against that to figure out what the what the ideal performance specs of a volume had to be",
    "start": "434480",
    "end": "441350"
  },
  {
    "text": "in order to meet about 90% of the workload io patterns we're seeing that's how we ended up with aspects of gp2 so",
    "start": "441350",
    "end": "448130"
  },
  {
    "text": "you've got a three to one i'ope to gb ratio a minimum baseline hundred i ops",
    "start": "448130",
    "end": "453350"
  },
  {
    "text": "and most importantly four volumes under one terabyte you've got a three thousand",
    "start": "453350",
    "end": "458630"
  },
  {
    "text": "I ABS bursts the reason we settled on a first model was because that was the",
    "start": "458630",
    "end": "464360"
  },
  {
    "text": "type of i/o that we were seeing we were seeing long periods of idle time followed by a momentary but really high",
    "start": "464360",
    "end": "471110"
  },
  {
    "text": "spike in AI ops which is why we decided to give even the smallest EBS volumes the ability to burst up to 3,000 I apps",
    "start": "471110",
    "end": "478870"
  },
  {
    "text": "so these volumes are good for almost any kind of workload you know boot volumes pretty much",
    "start": "478870",
    "end": "485300"
  },
  {
    "text": "whatever works works pretty well on GP too so what if this performance is your priority in that case look at IO one so",
    "start": "485300",
    "end": "492920"
  },
  {
    "start": "492000",
    "end": "492000"
  },
  {
    "text": "it's got about as you can see double the per volume performance specs as GP 2 but",
    "start": "492920",
    "end": "499040"
  },
  {
    "text": "there are two points about Iowan that are that differentiate it from GP tune they're really important one is a ops",
    "start": "499040",
    "end": "505760"
  },
  {
    "text": "consistency so io one is designed to deliver within 10 percent of provision",
    "start": "505760",
    "end": "511070"
  },
  {
    "text": "performance 99.9% of the time and that's in compared to GP which delivers 99% and",
    "start": "511070",
    "end": "519800"
  },
  {
    "text": "that that 1/9 is an order of magnitude of difference in ions consistency similarly in terms of latency",
    "start": "519800",
    "end": "526940"
  },
  {
    "text": "consistency I ops is also much different io1 has a",
    "start": "526940",
    "end": "532750"
  },
  {
    "text": "much narrower band of latency variation so it's latency is much more consistent",
    "start": "532750",
    "end": "539320"
  },
  {
    "text": "than gp2 and you see a lot less jitter so where that is relevant is if you've",
    "start": "539320",
    "end": "544540"
  },
  {
    "text": "got a critical database that really relies on latency consistency in AI ops consistency that's the time when you",
    "start": "544540",
    "end": "550660"
  },
  {
    "text": "want to look at using i/o one it also scales pretty quickly I mean at 400",
    "start": "550660",
    "end": "556030"
  },
  {
    "start": "553000",
    "end": "553000"
  },
  {
    "text": "gigabytes it means its maximum per volume I ups of 20,000 and that's",
    "start": "556030",
    "end": "561670"
  },
  {
    "text": "because it has a 50 to 1 I have to do which makes it great for small hot datasets so what if you care about",
    "start": "561670",
    "end": "568870"
  },
  {
    "start": "568000",
    "end": "568000"
  },
  {
    "text": "throughput in that case the first question is what is your workload like does it have majority large block IO",
    "start": "568870",
    "end": "576400"
  },
  {
    "text": "sequential iOS or is there a lot of small random i/o s intermixed within it",
    "start": "576400",
    "end": "581740"
  },
  {
    "text": "if there's a lot of small random i/o then you're going to want to go with an SSD volume however if it is majority",
    "start": "581740",
    "end": "588790"
  },
  {
    "text": "large block sequential then how much throughput do you need if you need over one thousand seven hundred and fifty",
    "start": "588790",
    "end": "594760"
  },
  {
    "text": "megabytes of throughput which is the current VBS optimized maximum they'll keep an eye on those numbers because",
    "start": "594760",
    "end": "600370"
  },
  {
    "text": "we're constantly increasing them and this will probably be out of date in like two seconds and go with something",
    "start": "600370",
    "end": "605650"
  },
  {
    "text": "like a d2 but if you're okay with in what EBS optimized can do then you have",
    "start": "605650",
    "end": "610930"
  },
  {
    "text": "the same cost performance trade-off if you care more about performance look at st one so st one is a throughput",
    "start": "610930",
    "end": "618820"
  },
  {
    "start": "618000",
    "end": "618000"
  },
  {
    "text": "optimized HDD back volume its performance is defined in megabytes a",
    "start": "618820",
    "end": "623920"
  },
  {
    "text": "second because throughput is what's important and it can burst up to 500 megabytes a second per volume and these",
    "start": "623920",
    "end": "630850"
  },
  {
    "text": "are great for big data analytics workloads like you know hadoop mapreduce Splunk log processing Kafka etc but if",
    "start": "630850",
    "end": "640210"
  },
  {
    "text": "cost is more important to you then you want to take a look at SC one so SC one",
    "start": "640210",
    "end": "645220"
  },
  {
    "text": "is a lot like st one it's just got slightly more modest performance specs you can burst up to 250 megabytes a",
    "start": "645220",
    "end": "652360"
  },
  {
    "text": "second and the baselines a little cooler but it's useful for almost all of the same workloads just in a slightly colder",
    "start": "652360",
    "end": "659110"
  },
  {
    "text": "setting so here's a whole which way adventure but what if you just don't know or frankly you just don't care when I think",
    "start": "659110",
    "end": "665990"
  },
  {
    "text": "about it in that case just use GP - it works for almost everything so I wanted",
    "start": "665990",
    "end": "672080"
  },
  {
    "start": "671000",
    "end": "671000"
  },
  {
    "text": "to leave you a couple of examples about how customers have benefited from EBS as",
    "start": "672080",
    "end": "677990"
  },
  {
    "text": "particular volume characteristics and also intelligently mixed and matched instance types with different types of",
    "start": "677990",
    "end": "684680"
  },
  {
    "text": "EBS storage in order to optimize cost and performance so liberado is a cloud",
    "start": "684680",
    "end": "690589"
  },
  {
    "text": "metrics monitoring company and CrowdStrike provides enterprise threat detection both of these customers were",
    "start": "690589",
    "end": "697730"
  },
  {
    "text": "running their petabyte scale Cassandra clusters on ITU's and they decided to move to C fours which are with all that",
    "start": "697730",
    "end": "705080"
  },
  {
    "text": "CPU much more optimal for a Cassandra cluster plus EBS they put their hot",
    "start": "705080",
    "end": "710690"
  },
  {
    "text": "files they're hot data files on GP 2 and then they put the majority sequential Cassandra commit logs on st 1 they're",
    "start": "710690",
    "end": "718580"
  },
  {
    "text": "thereby getting the best performance possible at the lowest cost for CrowdStrike they saved about 50% and",
    "start": "718580",
    "end": "726140"
  },
  {
    "text": "liberado saved about 35% they also benefited from things like the ability",
    "start": "726140",
    "end": "732320"
  },
  {
    "text": "to scale so they could go from an m4 for when they had to scale to an m4 16 excel",
    "start": "732320",
    "end": "738230"
  },
  {
    "text": "and just attach their volumes and reattach them and then detach and reattach the smaller instance when they're done so there's a lot of",
    "start": "738230",
    "end": "744079"
  },
  {
    "text": "flexibility there here's the whole lineup we charge in per gigabyte month",
    "start": "744079",
    "end": "749839"
  },
  {
    "text": "which is now prorated down to the second the only volume that has an additional",
    "start": "749839",
    "end": "756200"
  },
  {
    "text": "charge here is i/o one which has the per per vision die ops Sarge on top of it you can snapshot anything for five cents",
    "start": "756200",
    "end": "763399"
  },
  {
    "text": "per gigabyte month so in February of this year we introduced elastic volumes",
    "start": "763399",
    "end": "768440"
  },
  {
    "text": "which was a top customer task among our base elastic volumes allows you to",
    "start": "768440",
    "end": "774380"
  },
  {
    "start": "772000",
    "end": "772000"
  },
  {
    "text": "increase your volume size to change your volume type and to increase or decrease",
    "start": "774380",
    "end": "779899"
  },
  {
    "text": "your provision to a ops but most importantly it allows you to do this when you or application is actively",
    "start": "779899",
    "end": "787250"
  },
  {
    "start": "785000",
    "end": "785000"
  },
  {
    "text": "driving IO to your volume and the benefits there are that there's an for a maintenance window for downtime as",
    "start": "787250",
    "end": "794660"
  },
  {
    "text": "you grow out your storage or increase ions and then there's a big cost savings",
    "start": "794660",
    "end": "801170"
  },
  {
    "text": "benefit because you no longer have to over provision your storage because you don't want to take downtime when you",
    "start": "801170",
    "end": "806690"
  },
  {
    "text": "grow it or provision your provision I ops for peak you can just scale scale your eye ups up and down however you",
    "start": "806690",
    "end": "812750"
  },
  {
    "text": "need I had to go through a quick example of how to modify a volume because you're",
    "start": "812750",
    "end": "817850"
  },
  {
    "start": "814000",
    "end": "814000"
  },
  {
    "text": "touching anything we recommend that you first backup your volume but then you just modify it monitor the modification",
    "start": "817850",
    "end": "823880"
  },
  {
    "text": "and then extend your filesystem if you are increasing volume size so you modify",
    "start": "823880",
    "end": "830300"
  },
  {
    "start": "829000",
    "end": "829000"
  },
  {
    "text": "the volume using the easy to modify volume command here I'm going from a one terabyte GB to volume to a five terabyte",
    "start": "830300",
    "end": "838750"
  },
  {
    "text": "il-1 volume with 20,000 i ops I use the described volumes modifications command",
    "start": "838750",
    "end": "845390"
  },
  {
    "text": "to get the status of my modification I can see everything about it what state it's in how far along its in you'll",
    "start": "845390",
    "end": "852800"
  },
  {
    "text": "notice that you really quickly go from the modifying state and within a few seconds to the optimizing state once",
    "start": "852800",
    "end": "859820"
  },
  {
    "start": "854000",
    "end": "854000"
  },
  {
    "text": "you're in the optimizing state you've got the additional storage and you can go ahead and resize your filesystem so",
    "start": "859820",
    "end": "867740"
  },
  {
    "start": "866000",
    "end": "866000"
  },
  {
    "text": "here well I mean if you don't know what your filesystem is you should check most you I assume no you can go ahead and",
    "start": "867740",
    "end": "875540"
  },
  {
    "text": "compare your block device size to the filesystem disk usage which will in many cases and not show the not reflect the",
    "start": "875540",
    "end": "884120"
  },
  {
    "text": "increased size that we just did then you extend the filesystem and it's worth",
    "start": "884120",
    "end": "889400"
  },
  {
    "text": "noting that if your drive is partitioned you need to first grow the partitions so",
    "start": "889400",
    "end": "897020"
  },
  {
    "start": "897000",
    "end": "897000"
  },
  {
    "text": "you can do this on Windows also using the Disk Management utility just go ahead click on action rescan disks then",
    "start": "897020",
    "end": "904610"
  },
  {
    "text": "you right click on the contact' contextual menu choose extend volume and enter the target size in megabytes and",
    "start": "904610",
    "end": "911960"
  },
  {
    "text": "you can do that also on PowerShell so a few tips",
    "start": "911960",
    "end": "917910"
  },
  {
    "start": "916000",
    "end": "916000"
  },
  {
    "text": "modifications have to fit within the volume specs so I can't create a 1 gigabyte St 1 volume because the minimum",
    "start": "917910",
    "end": "925450"
  },
  {
    "text": "size is 500 gigabytes you can modify volumes once every 6 hours and if you're using current",
    "start": "925450",
    "end": "931779"
  },
  {
    "text": "generation instances you don't have to do anything except if you have an old volume attached that was created before",
    "start": "931779",
    "end": "938500"
  },
  {
    "text": "November 1st 2016 in which case you have to do a one-time stop start or detach",
    "start": "938500",
    "end": "944500"
  },
  {
    "text": "attach of the volume so we've seen a ton of great automation ideas coming from",
    "start": "944500",
    "end": "950500"
  },
  {
    "start": "947000",
    "end": "947000"
  },
  {
    "text": "customers and solutions architects using things like elastic volumes cloud watch",
    "start": "950500",
    "end": "955779"
  },
  {
    "text": "lambda ec2 systems manager one interesting one is to publish a custom",
    "start": "955779",
    "end": "962649"
  },
  {
    "text": "cloud watch metrics showing the free space on your OS then use lambda plus",
    "start": "962649",
    "end": "968770"
  },
  {
    "text": "ec2 Systems Manager to automate the process of extending the volume and of",
    "start": "968770",
    "end": "975550"
  },
  {
    "text": "growing the volume and then extending the filesystem the other kind of things you can do are use a cloud watch alarm",
    "start": "975550",
    "end": "981820"
  },
  {
    "text": "to look for when a volume is getting close to its eye ops limit or exhausting",
    "start": "981820",
    "end": "987339"
  },
  {
    "text": "its burst balance we have a few examples that we've put up there on AWS labs if",
    "start": "987339",
    "end": "992500"
  },
  {
    "text": "you want to go ahead and take a look with that I will hand it over to Marc thanks Jody all right so now that we",
    "start": "992500",
    "end": "1001320"
  },
  {
    "text": "know all about EBS volume types with Jody's awesome animations I want to take",
    "start": "1001320",
    "end": "1006330"
  },
  {
    "text": "a little break actually how much I can get a show of hands how many of that was a review or how many how many how much",
    "start": "1006330",
    "end": "1012209"
  },
  {
    "text": "so that was a review for people if you've got experience with EBS go ahead and raise your hand all right we got",
    "start": "1012209",
    "end": "1017430"
  },
  {
    "text": "some new people in here most familiar though so you know once you create an EBS volume you need to attach it in",
    "start": "1017430",
    "end": "1024660"
  },
  {
    "text": "order to use it and to attach it you attached to an ec2 instance the API ste",
    "start": "1024660",
    "end": "1030030"
  },
  {
    "text": "line console commands specify the instance ID the volume ID the mountain point now the mountain point is going to",
    "start": "1030030",
    "end": "1036990"
  },
  {
    "text": "show up on Linux when you're running on a on ours n hypervisor so instances that",
    "start": "1036990",
    "end": "1042209"
  },
  {
    "text": "are built on Unzen will show up at this mount point I'll talk about more about that in a moment and Windows Maps this",
    "start": "1042209",
    "end": "1049169"
  },
  {
    "text": "to a bus ID there's a complicated map we've gotten our documentation that you can find so in Zen and Linux you do an",
    "start": "1049169",
    "end": "1056640"
  },
  {
    "start": "1054000",
    "end": "1054000"
  },
  {
    "text": "LS block see this X PDF which is what I just attached I'm not sure what I was doing with one gigabyte of data probably",
    "start": "1056640",
    "end": "1062580"
  },
  {
    "text": "just doing this demo can't do a whole lot with 1 gig anymore so we see that",
    "start": "1062580",
    "end": "1067920"
  },
  {
    "text": "that attachment in there and let's see what happens behind the scenes so each",
    "start": "1067920",
    "end": "1073680"
  },
  {
    "text": "instance honors M hypervisors are associated with an i/o domain sometimes you'll hear this called domme 0 as well",
    "start": "1073680",
    "end": "1079010"
  },
  {
    "text": "and this i/o domain does not have access to your instance memory your instance",
    "start": "1079010",
    "end": "1084030"
  },
  {
    "text": "doesn't have access to the EBS volume so this is how we so when you attach a volume what happened is that volume is",
    "start": "1084030",
    "end": "1091080"
  },
  {
    "text": "attached to the i/o domain a shared memory segment is set up that acts as a cue for i/o request between your",
    "start": "1091080",
    "end": "1097770"
  },
  {
    "text": "instance and EBS and there's really a couple more Q's in there I've simplified the the picture so when your application",
    "start": "1097770",
    "end": "1105990"
  },
  {
    "text": "wants to do an i/o it'll submit a system call - to the kernel whether that's Windows or Linux they both act about the",
    "start": "1105990",
    "end": "1112950"
  },
  {
    "text": "same different driver stacks but that's fine so in this case we've got an 8k",
    "start": "1112950",
    "end": "1117960"
  },
  {
    "text": "read it gets put onto the cue rings the doorbell to wake up the i/o domain IO",
    "start": "1117960",
    "end": "1124020"
  },
  {
    "text": "domain wakes up pulls it off the cue puts it on another cue sends it off to EBS the response comes back and the",
    "start": "1124020",
    "end": "1129840"
  },
  {
    "text": "other way so hopefully you guys have had a chance to watch the the talks by my",
    "start": "1129840",
    "end": "1136890"
  },
  {
    "text": "colleagues matt wilson and anthony Liguori they went through a bit about our new nitro system so i'm going to go",
    "start": "1136890",
    "end": "1143790"
  },
  {
    "text": "through some highlights right now as they pertain to EBS but they do a pretty good deep dive talking about the the new",
    "start": "1143790",
    "end": "1150290"
  },
  {
    "text": "uploads that we've built so in the 9",
    "start": "1150290",
    "end": "1155820"
  },
  {
    "start": "1153000",
    "end": "1153000"
  },
  {
    "text": "through hypervisor to improve i/o performance we looked at a number of",
    "start": "1155820",
    "end": "1161340"
  },
  {
    "text": "different ways to bring EBS volumes into the instance and what we settled on is is nvme server non-volatile memory",
    "start": "1161340",
    "end": "1168150"
  },
  {
    "text": "Express now we use this as an interface because it's got a pretty easy",
    "start": "1168150",
    "end": "1173190"
  },
  {
    "text": "programming model the driver community at this point is pretty mature but it is",
    "start": "1173190",
    "end": "1179580"
  },
  {
    "text": "important to remember that even though we use and we show up as an nvme interface the volume performance characteristics",
    "start": "1179580",
    "end": "1186360"
  },
  {
    "text": "are based on your configuration so this isn't like an nvme drive you're gonna buy off-the-shelf",
    "start": "1186360",
    "end": "1191680"
  },
  {
    "text": "just because you provision 2gb to volume we're not gonna get a hundred thousand I outside of it so in Linux you can use",
    "start": "1191680",
    "end": "1198880"
  },
  {
    "text": "the nvm o so if you do an LS block on these you'll see this bunch of nvme one",
    "start": "1198880",
    "end": "1206020"
  },
  {
    "text": "n one and kind of confusing nomenclature this is how the Linux driver stack decided to enumerate nvme devices since",
    "start": "1206020",
    "end": "1215020"
  },
  {
    "text": "we're a PCI controller though we're able to present a lot more information than the olds and pair of virtual drivers and",
    "start": "1215020",
    "end": "1221830"
  },
  {
    "text": "so we've set the model number of the nvme controller dammuz on elastic block store and the reason for that is so that",
    "start": "1221830",
    "end": "1228190"
  },
  {
    "text": "your ami can programmatically determine the difference between instant storage on i3 or EBS volumes for EBS volumes",
    "start": "1228190",
    "end": "1237550"
  },
  {
    "text": "we've added the volume number or the volume as the volume ideas your serial number we had to get rid of the hyphen",
    "start": "1237550",
    "end": "1245230"
  },
  {
    "text": "in there so it looks a little bit different nvme only gives us 20 characters and that mount point that I",
    "start": "1245230",
    "end": "1251260"
  },
  {
    "text": "was talking about before Unzen nvme gives us the ability to add what's called vendor specific information and",
    "start": "1251260",
    "end": "1257190"
  },
  {
    "text": "so using the nvme CLI you can actually get this vendor specific information and if you read the first 32 bytes of that",
    "start": "1257190",
    "end": "1264490"
  },
  {
    "text": "it's a string that is the mount point that you specified on the console so",
    "start": "1264490",
    "end": "1269590"
  },
  {
    "text": "that's how you can find your volumes if you're running Amazon Linux we have the",
    "start": "1269590",
    "end": "1275500"
  },
  {
    "text": "package called ec2 utils and so this provides a lot of helpful tools for ec2 instances with the most recent update to",
    "start": "1275500",
    "end": "1283060"
  },
  {
    "text": "the package we added a couple commands that extract this information for you and so with the NDB s nvme ID command",
    "start": "1283060",
    "end": "1290620"
  },
  {
    "text": "you'll get the pretty volume ID with the - that you're used to and the block device so we'll extract that for you so",
    "start": "1290620",
    "end": "1295630"
  },
  {
    "text": "you don't have to read the hex and all that we also added some u dev rules so that when you attach the volume to a",
    "start": "1295630",
    "end": "1303670"
  },
  {
    "text": "kernel that has the this utils package installed and so you can get it on the most recent Amazon Linux AMI or you can just update an existing on me",
    "start": "1303670",
    "end": "1310600"
  },
  {
    "text": "with the new ec2 details that you Dev rule will create the same link that you're used to so your",
    "start": "1310600",
    "end": "1316330"
  },
  {
    "text": "they'll show up as Devex PDF now I recommend that you when you create a file system on these drives that you",
    "start": "1316330",
    "end": "1322539"
  },
  {
    "text": "label them and reference labels when you're doing persistent mounts and if",
    "start": "1322539",
    "end": "1328450"
  },
  {
    "text": "you have to use the mount point that you specified in the console because the nbme devices actually can move around a",
    "start": "1328450",
    "end": "1334390"
  },
  {
    "text": "bit so you might reboot and what was nvme one may show up as nvme 2 based on",
    "start": "1334390",
    "end": "1339610"
  },
  {
    "text": "pci enumeration order so I mentioned I'm not going to go too deep but I'll go a",
    "start": "1339610",
    "end": "1345789"
  },
  {
    "start": "1343000",
    "end": "1343000"
  },
  {
    "text": "little bit into what's happening just to point out some some differences in how it impacts EBS so the the Nitra",
    "start": "1345789",
    "end": "1353139"
  },
  {
    "text": "hypervisor is a lot lighter there's a whole lot less going on it's not involved in i/o at all when you attach",
    "start": "1353139",
    "end": "1359740"
  },
  {
    "text": "an EBS volume it's attached to the Nitro card and the Nitro card presents PCI into your instance that IO that your",
    "start": "1359740",
    "end": "1366639"
  },
  {
    "text": "application wants to do gets submitted right to the Nitro card so there's no time spent in the hypervisor there's no",
    "start": "1366639",
    "end": "1372639"
  },
  {
    "text": "no doorbells no waiting around and so what we've noticed is on volumes",
    "start": "1372639",
    "end": "1378010"
  },
  {
    "text": "attached your longtail Layton sees we've noticed a decrease in the tens of milliseconds so there's a huge",
    "start": "1378010",
    "end": "1383019"
  },
  {
    "text": "performance impact just in the latency outliers so security is important to us",
    "start": "1383019",
    "end": "1389649"
  },
  {
    "text": "and we know it's important to you too so for that we offer EBS encrypted volumes now you can roll",
    "start": "1389649",
    "end": "1395409"
  },
  {
    "text": "your own encryption within your instance but that's going to take CPU away from your instance with EBS encrypted volumes",
    "start": "1395409",
    "end": "1401470"
  },
  {
    "start": "1400000",
    "end": "1400000"
  },
  {
    "text": "all you do is you check a box when you created and you can attach it to most instances with no performance impact and",
    "start": "1401470",
    "end": "1407580"
  },
  {
    "text": "so how do we do it with no performance impact well if you follow the kind of the trail and the story that Peter",
    "start": "1407580",
    "end": "1413919"
  },
  {
    "text": "talked about on Tuesday and Anthony and Matt talked about yesterday we've been offloading cryptographic",
    "start": "1413919",
    "end": "1421059"
  },
  {
    "text": "operations for a long time in ec2 and so all the cryptographic operations are",
    "start": "1421059",
    "end": "1426309"
  },
  {
    "text": "done in Hardware offload there's a performance benefit because we don't have to use CPU for that but there's also a security benefit for that because",
    "start": "1426309",
    "end": "1432880"
  },
  {
    "text": "it's not software that we wrote that's touching it your abs data is encrypted",
    "start": "1432880",
    "end": "1439570"
  },
  {
    "text": "next to your instance so before it even touches the network so all your data in",
    "start": "1439570",
    "end": "1444639"
  },
  {
    "text": "flight is encrypted it's still encrypted and remains encrypted when it's stored on the EBS servers in the back end",
    "start": "1444639",
    "end": "1450140"
  },
  {
    "text": "and it continues to be encrypted while at the snapshot as well now all accounts",
    "start": "1450140",
    "end": "1455960"
  },
  {
    "start": "1454000",
    "end": "1454000"
  },
  {
    "text": "can create an EBS volume using the default EBS master key this master key protects the keys that actually encrypt",
    "start": "1455960",
    "end": "1462169"
  },
  {
    "text": "your volume so it's envelope encryption from that standpoint as a best practice",
    "start": "1462169",
    "end": "1469610"
  },
  {
    "text": "we recommend that you actually create a new master key now there's no downside",
    "start": "1469610",
    "end": "1477110"
  },
  {
    "text": "to using the default key and it works perfectly fine it's the same security model but when you create a custom",
    "start": "1477110",
    "end": "1482510"
  },
  {
    "text": "master key which you can do through the kms console you can define key rotation policies our access controls and better",
    "start": "1482510",
    "end": "1489500"
  },
  {
    "text": "auditing and logging so if that's important to your business then a custom master key is the only way to do that",
    "start": "1489500",
    "end": "1496120"
  },
  {
    "text": "once you've created that you just specify it when you encrypt the volume either on the console or the the api's",
    "start": "1496120",
    "end": "1502279"
  },
  {
    "text": "it's just an additional parameter now because EBS volumes persist beyond the",
    "start": "1502279",
    "end": "1507529"
  },
  {
    "text": "life of your ec2 instance we need to think about the reliability of EBS separate from ec2 so we think about",
    "start": "1507529",
    "end": "1516200"
  },
  {
    "text": "volume reliability in two ways the first way is availability and so we define the",
    "start": "1516200",
    "end": "1522110"
  },
  {
    "text": "availability is the ability to access your data which typically includes our",
    "start": "1522110",
    "end": "1527269"
  },
  {
    "text": "network service uptime and other things behind the scenes now in the availab in",
    "start": "1527269",
    "end": "1532700"
  },
  {
    "text": "the availability realm EBS is designed for five nines and availability and we track this consistently we've got",
    "start": "1532700",
    "end": "1538820"
  },
  {
    "text": "metrics we're logging we're auditing keep a lot of history on this and we've got extremely aggressive alarms so that",
    "start": "1538820",
    "end": "1544639"
  },
  {
    "text": "we know long before we come close to reaching this five nines that we're",
    "start": "1544639",
    "end": "1549740"
  },
  {
    "text": "gonna have a problem second is durability are the ability to get your data off the media EBS has an",
    "start": "1549740",
    "end": "1556880"
  },
  {
    "text": "annual failure rate between point one and point two percent which is significantly more durable than a than a",
    "start": "1556880",
    "end": "1562549"
  },
  {
    "text": "commodity disk your those are gonna be in the three to six percent range so the way to think about ABS durability is",
    "start": "1562549",
    "end": "1568820"
  },
  {
    "text": "that if you've got a thousand volumes over the course of the year on average you can expect one to two of those to",
    "start": "1568820",
    "end": "1574190"
  },
  {
    "text": "fail hopefully you'll get luckier than that sometimes would be a little bit worse but one to two is a good average",
    "start": "1574190",
    "end": "1579679"
  },
  {
    "text": "and that's the point 12.2% now we're designed for this without any",
    "start": "1579679",
    "end": "1585409"
  },
  {
    "text": "action on your part but you can further reduce the probability of volume loss by taking regular snapshots and so an EBS",
    "start": "1585409",
    "end": "1592549"
  },
  {
    "start": "1590000",
    "end": "1590000"
  },
  {
    "text": "snapshot is a point in time crash consistent copy of your data stored in",
    "start": "1592549",
    "end": "1597559"
  },
  {
    "text": "s3 which is a service design for eleven nines of durability now the first time",
    "start": "1597559",
    "end": "1602809"
  },
  {
    "start": "1601000",
    "end": "1601000"
  },
  {
    "text": "you take a snapshot every modified block is copied s3 subsequent snapshots are",
    "start": "1602809",
    "end": "1607940"
  },
  {
    "text": "incremental and only the blocks that you've written and changed since the previous snapshot are pushed when you",
    "start": "1607940",
    "end": "1613399"
  },
  {
    "text": "delete a snapshot we keep track of the data that's required to build all future snapshots so we only delete the data",
    "start": "1613399",
    "end": "1619130"
  },
  {
    "text": "that's exclusive to that snapshot so I mentioned that this taking a snapshot",
    "start": "1619130",
    "end": "1624350"
  },
  {
    "text": "can reduce the probability of volume loss so it's important to remember that EBS is a distributed system and your",
    "start": "1624350",
    "end": "1630559"
  },
  {
    "text": "data shard it across many drives so when a failure happens and they do we're gonna need to restart your data and when",
    "start": "1630559",
    "end": "1637940"
  },
  {
    "text": "we're restarting data we prioritize the data that's changed since the last snapshot first and so with a recent",
    "start": "1637940",
    "end": "1643220"
  },
  {
    "text": "snapshot you actually reduce the amount of data that is that has priority and then if there is a coincident failure as",
    "start": "1643220",
    "end": "1649820"
  },
  {
    "text": "we're trying to be shared your data we can pull the rest from s3 if we need to and so by examining the behavior of some",
    "start": "1649820",
    "end": "1656779"
  },
  {
    "text": "of our customers this is continues on the theme of pulling data and analyzing historical data what we found is that",
    "start": "1656779",
    "end": "1664100"
  },
  {
    "text": "customers that take a daily snapshot have a significant positive impact on their volume AFR so crash consistent",
    "start": "1664100",
    "end": "1675710"
  },
  {
    "start": "1672000",
    "end": "1672000"
  },
  {
    "text": "snapshots are similar to pulling the power cord on your server now most journal file systems and",
    "start": "1675710",
    "end": "1681289"
  },
  {
    "text": "applications can handle this with no problem but if your application requires consistent snapshots beyond that or if",
    "start": "1681289",
    "end": "1688610"
  },
  {
    "text": "you have multiple volumes that you want to snapshot at the same time we should first quiesce your i/o and so once",
    "start": "1688610",
    "end": "1694580"
  },
  {
    "text": "you've quest you call to the snapchat API when the API returns success you'll get a snapshot ID when you have that",
    "start": "1694580",
    "end": "1700580"
  },
  {
    "text": "snapshot ID you can resume i/o you don't have to wait for the snapshot to keep pushing in the background of the",
    "start": "1700580",
    "end": "1706159"
  },
  {
    "text": "snapshot to complete we've marked that point in time and any new changes and",
    "start": "1706159",
    "end": "1711169"
  },
  {
    "text": "rights that you make will not be represented in that snapshot",
    "start": "1711169",
    "end": "1716679"
  },
  {
    "start": "1716000",
    "end": "1716000"
  },
  {
    "text": "so there's equivalent utilities for windows as well windows also has the ability to use VSS to take application",
    "start": "1716919",
    "end": "1725179"
  },
  {
    "text": "consistent snapshots and there's some integrations with that recently we integrated ec2 SSM simple service",
    "start": "1725179",
    "end": "1734480"
  },
  {
    "start": "1729000",
    "end": "1729000"
  },
  {
    "text": "manager in NV SS in your instances and so if you're already using SSM you just",
    "start": "1734480",
    "end": "1739730"
  },
  {
    "text": "need to make sure that you have the right actions the new actions in your policy or you can create a new ion policy",
    "start": "1739730",
    "end": "1745190"
  },
  {
    "text": "attach it to your instance and once it's attached to your instance you've got that policy if you've got the updated",
    "start": "1745190",
    "end": "1751970"
  },
  {
    "text": "VSS packages which there's a run command that'll actually help you with that or they're available in the most recent window zombies you can use the create",
    "start": "1751970",
    "end": "1759200"
  },
  {
    "text": "VSS snapshot run command this can be done through the CLI the console custom",
    "start": "1759200",
    "end": "1765590"
  },
  {
    "text": "PowerShell scripts or I've even seen lambda lambda functions that'll do it and so in this example we're using the",
    "start": "1765590",
    "end": "1772610"
  },
  {
    "text": "console select the instance other properties of the command once you",
    "start": "1772610",
    "end": "1778250"
  },
  {
    "text": "execute this the snapshots are going to be tagged with app consistent equals true now just to help you keep a better",
    "start": "1778250",
    "end": "1785240"
  },
  {
    "text": "track of it you know the status those snapshots so another interesting use",
    "start": "1785240",
    "end": "1791120"
  },
  {
    "text": "case in for is a customer that runs their sequel server databases on i2 they need the local instant storage they need",
    "start": "1791120",
    "end": "1797029"
  },
  {
    "text": "the latency and the performance that that provides them but what they realize that they could do is use st one volumes",
    "start": "1797029",
    "end": "1802610"
  },
  {
    "text": "and I saw a couple confused Jody mentioned earlier that you could use",
    "start": "1802610",
    "end": "1807649"
  },
  {
    "text": "these streaming volumes for backups I'm going to explain how that goes so the use of VSS aware snapshot utility that",
    "start": "1807649",
    "end": "1815509"
  },
  {
    "text": "backs up to these st one volumes and then they take EBS snapshots of those volumes and by doing this they were able",
    "start": "1815509",
    "end": "1822919"
  },
  {
    "text": "to improve their backup speed by thirty percent so they don't need to keep as many snapshots in flight anymore they've",
    "start": "1822919",
    "end": "1829519"
  },
  {
    "text": "simplified their backup strategy and reduce their backup cost by seventy five percent",
    "start": "1829519",
    "end": "1835570"
  },
  {
    "text": "so I briefly mentioned tags with the VSS integration for those of you that aren't familiar with tags tags give you the",
    "start": "1835570",
    "end": "1842629"
  },
  {
    "text": "ability to sign a simple key value pair to your instances volumes and EBS",
    "start": "1842629",
    "end": "1848059"
  },
  {
    "text": "snapshots once he can group and manage those resources together one thing that you can do with those tags is activate them",
    "start": "1848059",
    "end": "1854420"
  },
  {
    "text": "as cost up allocation tags which makes that available through the building pipeline so you activate them in the",
    "start": "1854420",
    "end": "1863000"
  },
  {
    "text": "billing dashboard and then you can generate reports on cost usage broken down by those tag values such as dev",
    "start": "1863000",
    "end": "1868790"
  },
  {
    "text": "test backup and the same data is also visible in the cost Explorer for additional visualization now that we've",
    "start": "1868790",
    "end": "1877640"
  },
  {
    "start": "1876000",
    "end": "1876000"
  },
  {
    "text": "got a snapshot what can we do with it with EBS snapshots you're able to create",
    "start": "1877640",
    "end": "1883970"
  },
  {
    "text": "a copy of a volume in another availability zone or even the same AZ within a region this could be useful for",
    "start": "1883970",
    "end": "1889850"
  },
  {
    "text": "load balancing so you need to spin up a new front-end for your application bootstrap a new node into a cluster or",
    "start": "1889850",
    "end": "1894890"
  },
  {
    "text": "maybe replace a failed node or if you want to set up another disaster recovery cluster if you use EBS snapshot copy",
    "start": "1894890",
    "end": "1903890"
  },
  {
    "text": "functionality you can copy those snapshots to another region so this can help you set up a disaster recovery site",
    "start": "1903890",
    "end": "1909740"
  },
  {
    "text": "across the country maybe in another part of the world or even bootstrap into one of the new regions that AWS launches so",
    "start": "1909740",
    "end": "1920300"
  },
  {
    "text": "we can use building blocks of lambda run command and resource tagging and we put",
    "start": "1920300",
    "end": "1925850"
  },
  {
    "text": "together templates that help you with this with ops Automator to build a snapshot lifecycle management system so",
    "start": "1925850",
    "end": "1931700"
  },
  {
    "text": "that you can set policies on your snapshots to take the say you want to do a weekly snapshot or a daily snapshot as",
    "start": "1931700",
    "end": "1937790"
  },
  {
    "text": "well as retention policy so you'll delete them after a month or whatever you need to do on many of our current",
    "start": "1937790",
    "end": "1947480"
  },
  {
    "start": "1944000",
    "end": "1944000"
  },
  {
    "text": "generation instance types when EBS volumes are attached and only EBS volumes are attached you can take advantage of ec2 auto recovery so what",
    "start": "1947480",
    "end": "1956600"
  },
  {
    "text": "auto recovery is is a per instance alarm and when that fails or when that goes into alarm it triggers a recovery",
    "start": "1956600",
    "end": "1962840"
  },
  {
    "text": "workflow that recovery workflow will reboot your instance sometimes placing it on different Hardware if that's what",
    "start": "1962840",
    "end": "1968390"
  },
  {
    "text": "the failure is and that new instance is going to retain all the same configuration data that it had",
    "start": "1968390",
    "end": "1974270"
  },
  {
    "text": "previously so the same instance ID same volume attachments IP addresses etc application can come up with no impact",
    "start": "1974270",
    "end": "1981770"
  },
  {
    "text": "are with no extra manual effort required so a little bit a one bit of",
    "start": "1981770",
    "end": "1989300"
  },
  {
    "start": "1986000",
    "end": "1986000"
  },
  {
    "text": "housekeeping here so by default boot volumes are tied to the life of your instance and data volumes are not know a",
    "start": "1989300",
    "end": "1996440"
  },
  {
    "text": "lot of times you do want to keep that data around and so what happens is that",
    "start": "1996440",
    "end": "2002620"
  },
  {
    "text": "when you terminate your instance the volumes stick around one thing that I",
    "start": "2002620",
    "end": "2008770"
  },
  {
    "text": "recommend here is that you tag those volumes so that you know what's on them so that six months later you're not trying to do this mass effort to try and",
    "start": "2008770",
    "end": "2015130"
  },
  {
    "text": "clean them up and figure out what if you need them or not if you've got a Devon test workload and",
    "start": "2015130",
    "end": "2020740"
  },
  {
    "text": "you want us to or for some other reason you want us to clean up those volumes automatically you can set the delete on",
    "start": "2020740",
    "end": "2027130"
  },
  {
    "text": "termination flag to true and when you terminate that instance what will happen is we'll sweep all of the volumes they",
    "start": "2027130",
    "end": "2033040"
  },
  {
    "text": "that have that tag set to true and delete them automatically for you so that will reduce your costs reduce your management requirements so probably the",
    "start": "2033040",
    "end": "2042880"
  },
  {
    "start": "2041000",
    "end": "2041000"
  },
  {
    "text": "area that we've collected the most amount of data and most metrics on EBS volumes is performance and so this Jody",
    "start": "2042880",
    "end": "2049300"
  },
  {
    "text": "mentioned earlier this has helped us to find a lot of our newer products that work just for your workloads whether",
    "start": "2049300",
    "end": "2055148"
  },
  {
    "text": "they're transactional throughput or some sort of mix of both now these recent",
    "start": "2055149",
    "end": "2062800"
  },
  {
    "text": "volumes that we define do have performance characteristics and defined performance criteria and it's important",
    "start": "2062800",
    "end": "2068889"
  },
  {
    "text": "to know how we actually measure I ops our throughput depending on your volume type so if you're doing random i/o to an",
    "start": "2068890",
    "end": "2075040"
  },
  {
    "text": "SSD one of our SSD backed products the i/o size is based on 16 kilobytes so",
    "start": "2075040",
    "end": "2080830"
  },
  {
    "text": "that means that if you're going to submit a 4k i/o we're still going to logically count it as a 16 ko for",
    "start": "2080830",
    "end": "2087580"
  },
  {
    "text": "accounting purposes the hard drive back volumes are designed for throughput and so to encourage larger writes and to",
    "start": "2087580",
    "end": "2095110"
  },
  {
    "text": "make sure that larger writes are actually happening we account on a one megabyte size now to help sequential",
    "start": "2095110",
    "end": "2101950"
  },
  {
    "text": "operations not everything's small random will logically merge the commercial iOS",
    "start": "2101950",
    "end": "2107160"
  },
  {
    "text": "and so what this allows you to do potentially you can provision fewer IUP's on your provisional apps volumes",
    "start": "2107160",
    "end": "2113410"
  },
  {
    "text": "and it also maximizes the amount of burst capability that you have now this is logical merging we",
    "start": "2113410",
    "end": "2121030"
  },
  {
    "text": "don't hang on to your IO wait for the next one and the next one of the next one to complete before we account and",
    "start": "2121030",
    "end": "2126160"
  },
  {
    "text": "complete them all we just keep track of where those IOT's were if we notice that they're sequential we won't take any",
    "start": "2126160",
    "end": "2132940"
  },
  {
    "text": "more tokens out of out of the limiters and so in general if you're one if",
    "start": "2132940",
    "end": "2139690"
  },
  {
    "text": "you're doing sequential i/o you still want to submit as large as you can but we'll try and keep track of it as best",
    "start": "2139690",
    "end": "2144910"
  },
  {
    "text": "as we can so a couple examples if your application is doing random i/o and by",
    "start": "2144910",
    "end": "2152170"
  },
  {
    "text": "random means an i/o that has an offset that's not adjacent to the previous i/o so somewhere on your disk will take each",
    "start": "2152170",
    "end": "2160120"
  },
  {
    "text": "one of those iOS put it in a boxcar and before iOS now if you're submitting",
    "start": "2160120",
    "end": "2167830"
  },
  {
    "text": "sequential i/o say we're doing four 64k iOS to an SSD back volume will pack",
    "start": "2167830",
    "end": "2174370"
  },
  {
    "text": "those into a single box car and only count it as one i/o so we'll merge those together they'll complete when they're",
    "start": "2174370",
    "end": "2181450"
  },
  {
    "text": "ready we won't wait for them but it only count as one i/o against against the throttles now for our hard drive back",
    "start": "2181450",
    "end": "2189550"
  },
  {
    "text": "volumes we'll gather up to 1024 K so if you submit for 64k IO and you continue",
    "start": "2189550",
    "end": "2197110"
  },
  {
    "text": "submitting 64 K airs will still count that as one single i/o the reverse is",
    "start": "2197110",
    "end": "2205930"
  },
  {
    "text": "true for our SSD products if you submit a large i/o for accounting purposes we'll split that up into four four",
    "start": "2205930",
    "end": "2213400"
  },
  {
    "text": "boxcars and take four tokens out of the bucket so the reality of the world is",
    "start": "2213400",
    "end": "2221530"
  },
  {
    "text": "most applications don't do purely sequential they don't do purely random typically it's a mix of both biased one",
    "start": "2221530",
    "end": "2228730"
  },
  {
    "text": "way or the other and so here's a good example of what can happen and st 1 + SC 1 volume your",
    "start": "2228730",
    "end": "2235180"
  },
  {
    "text": "application submits a couple large iOS will merge those together into one bucket a couple random iOS and a couple",
    "start": "2235180",
    "end": "2240520"
  },
  {
    "text": "more sequential iOS and so you submitted six we kind of did this four so we",
    "start": "2240520",
    "end": "2245740"
  },
  {
    "text": "helped you out a little bit but it's still counting four megabytes per second even though your application was only",
    "start": "2245740",
    "end": "2252060"
  },
  {
    "text": "able to get one and a half megabytes of data so Jody told the story earlier",
    "start": "2252060",
    "end": "2258450"
  },
  {
    "start": "2256000",
    "end": "2256000"
  },
  {
    "text": "about how we build GP 2 volumes I'm going to quickly go through the burst bucket I used to stand up here and spend",
    "start": "2258450",
    "end": "2264030"
  },
  {
    "text": "about 15 minutes on this but I'll give you a little trick at the end so you don't have to get out your slide rule",
    "start": "2264030",
    "end": "2269270"
  },
  {
    "text": "all of our volumes with burst operate on a token bucket model for GP to your",
    "start": "2269270",
    "end": "2275339"
  },
  {
    "text": "volume is always accumulating at a rate of 3i ops per gigabyte buckets a fixed size and then you can spend those",
    "start": "2275339",
    "end": "2282000"
  },
  {
    "text": "credits at 3,000 credits per second which is the burst or for a volume that's larger than that it'll be your 3i",
    "start": "2282000",
    "end": "2287400"
  },
  {
    "text": "ops per gigabyte here it is in graph form if you don't",
    "start": "2287400",
    "end": "2293190"
  },
  {
    "text": "like my animation with the bathtub see the baseline I ops in green smaller",
    "start": "2293190",
    "end": "2298859"
  },
  {
    "text": "volumes that are less than 1,000 gigabytes get a burst bucket and then so",
    "start": "2298859",
    "end": "2303869"
  },
  {
    "text": "as an example we've got a 300 gigabyte volume here it's got a 901 and it bursts",
    "start": "2303869",
    "end": "2309720"
  },
  {
    "text": "up to 3,000 ions now the question that I do often get is how long can I burst",
    "start": "2309720",
    "end": "2316430"
  },
  {
    "text": "now since the bucket is constantly refilling the token bucket is constantly filling based upon your volume size the",
    "start": "2316430",
    "end": "2323160"
  },
  {
    "start": "2317000",
    "end": "2317000"
  },
  {
    "text": "amount of time that you converse actually increases as your volume grows now our smallest gp2 volumes can burst",
    "start": "2323160",
    "end": "2328619"
  },
  {
    "text": "for 30 minutes are enough for a couple of reboots maybe even a kernel build in there especially in one of our new c",
    "start": "2328619",
    "end": "2333660"
  },
  {
    "text": "series instances c 5 instances as your volume grows the amount of time grows so",
    "start": "2333660",
    "end": "2339540"
  },
  {
    "text": "our 300 gigabyte volume conversed for 43 minutes in this example and as we get",
    "start": "2339540",
    "end": "2344670"
  },
  {
    "text": "larger and larger to a 950 gigabyte volume say it's 10 hours and then it as",
    "start": "2344670",
    "end": "2352020"
  },
  {
    "text": "it approaches it approach to your 3 apps per gigabyte limit and you won't have burst anymore it'll just be that all the",
    "start": "2352020",
    "end": "2357480"
  },
  {
    "text": "time throughput volumes work about the same we just measure throughput instead",
    "start": "2357480",
    "end": "2363690"
  },
  {
    "start": "2358000",
    "end": "2358000"
  },
  {
    "text": "of IUP's the other difference with our throughput volumes is the bucket grows",
    "start": "2363690",
    "end": "2371130"
  },
  {
    "text": "with as your volume grows and so let's take a look at what that looks like in",
    "start": "2371130",
    "end": "2377609"
  },
  {
    "start": "2375000",
    "end": "2375000"
  },
  {
    "text": "graphical form you see that the burst actually increases pretty quickly up to the maximum of 5",
    "start": "2377609",
    "end": "2382890"
  },
  {
    "text": "megabytes per second on st-1 volumes and then when you get to the 12 and a half terabyte size your baseline in your",
    "start": "2382890",
    "end": "2389670"
  },
  {
    "text": "burst match and so that's when we end up with your baseline as being the dominant factor there with an 8 or 8 volume you",
    "start": "2389670",
    "end": "2399000"
  },
  {
    "text": "have a 320 megabyte per second burst or 300 megabyte per second baseline and a 500 megabyte per second burg sc1 volumes",
    "start": "2399000",
    "end": "2408120"
  },
  {
    "text": "designed a bit more modestly still good though if you look at the base that's at",
    "start": "2408120",
    "end": "2413850"
  },
  {
    "text": "250 megabytes per second most commodity hard drives are going to get about 150 plus or minus depending on the workload",
    "start": "2413850",
    "end": "2420410"
  },
  {
    "text": "so you're going to get pretty good performance out of SC 1 the important thing to note here though is that even",
    "start": "2420410",
    "end": "2426360"
  },
  {
    "text": "at the largest volume size the baseline does not get up to the throughput so you're going to have to have some cold",
    "start": "2426360",
    "end": "2431760"
  },
  {
    "text": "periods to get to refill that burst the burst bucket difference between",
    "start": "2431760",
    "end": "2440010"
  },
  {
    "start": "2436000",
    "end": "2436000"
  },
  {
    "text": "sequential and random so if you're going to do large sequential workload for 3 hours on an SC 1 or st 1 volume you can",
    "start": "2440010",
    "end": "2446670"
  },
  {
    "text": "transfer almost five and a half terabytes of data so you can burn through your entire drive pretty quickly",
    "start": "2446670",
    "end": "2451980"
  },
  {
    "text": "there instead if you're doing a small random workload same amount of time same three hours you only get 87 gigabytes of data sc1",
    "start": "2451980",
    "end": "2459900"
  },
  {
    "text": "volumes are similar but they're with lower levels they're just because of the lower the more modest workload they're designed for so here's the trick we",
    "start": "2459900",
    "end": "2468030"
  },
  {
    "text": "introduced a burst balance metric you don't need to get out your slide rule like I mentioned just size your volume",
    "start": "2468030",
    "end": "2474390"
  },
  {
    "text": "for your average workload let the burst balance take care of the peak and then monitor it using the burst balance",
    "start": "2474390",
    "end": "2479880"
  },
  {
    "text": "metric and so here's an example of 500 gigabyte volume which if you do the math",
    "start": "2479880",
    "end": "2485130"
  },
  {
    "text": "we get 1,500 I ups of baseline and 3,000 I offset burst and I ran a synthetic",
    "start": "2485130",
    "end": "2490470"
  },
  {
    "text": "field benchmark now that aren't indicative of real workloads they're great at showing what max and peak",
    "start": "2490470",
    "end": "2496830"
  },
  {
    "text": "performances which is what what I wanted to do here today but you should really be benchmarking with your your typical",
    "start": "2496830",
    "end": "2503700"
  },
  {
    "text": "workload that's going to give you a better idea of what size volume you're going to need so I ran this test for an",
    "start": "2503700",
    "end": "2510630"
  },
  {
    "text": "hour and you can see that I was getting 3000 I ops in in orange there and as",
    "start": "2510630",
    "end": "2516239"
  },
  {
    "text": "I was getting 3000i opps my burst balance was slowly decreasing and I was decreasing it a at the 3000 I ops rate",
    "start": "2516239",
    "end": "2523339"
  },
  {
    "text": "and then when it got to zero the volume performance dropped to 1500 I ops now",
    "start": "2523339",
    "end": "2535229"
  },
  {
    "start": "2534000",
    "end": "2534000"
  },
  {
    "text": "how do you know what work or what I oh sighs your workload is driving so you can use perfmon on Windows I or IO stat",
    "start": "2535229",
    "end": "2542609"
  },
  {
    "text": "on Linux no this is measured in sectors you can see here I'm driving about 246",
    "start": "2542609",
    "end": "2548419"
  },
  {
    "text": "sectors which is roughly one megabyte so I'm probably doing some random i/o in there occasionally but I'm getting",
    "start": "2548419",
    "end": "2555659"
  },
  {
    "text": "pretty close to that 1 megabyte that I'm interested in and then you can get the same data to fast get the same data out",
    "start": "2555659",
    "end": "2562319"
  },
  {
    "text": "of perfmon so you can also look at cloud",
    "start": "2562319",
    "end": "2567569"
  },
  {
    "start": "2565000",
    "end": "2565000"
  },
  {
    "text": "watch now Claude watch you can show the average right size and here we'll see it",
    "start": "2567569",
    "end": "2573269"
  },
  {
    "text": "128 K and for architectural reasons and how things get displayed in Claude watch",
    "start": "2573269",
    "end": "2578519"
  },
  {
    "text": "this is what we see on the EBS back end even though your application and your instance may be submitting 1 megabyte",
    "start": "2578519",
    "end": "2584339"
  },
  {
    "text": "iOS so this is about the maximum that you're going to see in cloud watch you can overlay this with your burst balance",
    "start": "2584339",
    "end": "2590249"
  },
  {
    "text": "metric to see if it's depleting and that'll help you determine if you are actually submitting those sequential iOS",
    "start": "2590249",
    "end": "2596449"
  },
  {
    "text": "if you see something in 64 or less than 128 K you're probably doing smaller iOS",
    "start": "2596449",
    "end": "2601889"
  },
  {
    "text": "iOS that we're not able to merge and again if you look at your burst balance",
    "start": "2601889",
    "end": "2608939"
  },
  {
    "text": "metric overlaid with this you can see if they're random or sequential now in some",
    "start": "2608939",
    "end": "2614789"
  },
  {
    "text": "cases you might be stuck around 44 K this is an interesting artifact if you're running on one of our older",
    "start": "2614789",
    "end": "2620309"
  },
  {
    "text": "instance types or if you're running an older Linux kernel there were some changes around the 310 kernel in Zen",
    "start": "2620309",
    "end": "2627739"
  },
  {
    "text": "that allowed you to submit greater than a 44 ko you could submit up to a Meaghan",
    "start": "2627739",
    "end": "2633539"
  },
  {
    "text": "I'll show you how to change that in a moment all of our instance all of our current generation instance sites support the",
    "start": "2633539",
    "end": "2639179"
  },
  {
    "text": "the new variety so if you upgrade your kernel to a 3-2 or better and you're running on a current generation instance",
    "start": "2639179",
    "end": "2645509"
  },
  {
    "text": "you can actually get more than 44 K now the copy idea is if you're running you're stuck at 44 K unless you move to",
    "start": "2645509",
    "end": "2652520"
  },
  {
    "text": "one of our nitro instances so if your",
    "start": "2652520",
    "end": "2658160"
  },
  {
    "start": "2656000",
    "end": "2656000"
  },
  {
    "text": "throughput or performance tuning and you're doing a heavy read throughput workload on st1 sc1 recommend that you",
    "start": "2658160",
    "end": "2665750"
  },
  {
    "text": "increase your rita head buffer and so this is per volume defaults to 128 K again this is actually set in sectors on",
    "start": "2665750",
    "end": "2672619"
  },
  {
    "text": "Linux if you do this and you run a random workload on your volume you're",
    "start": "2672619",
    "end": "2678829"
  },
  {
    "text": "not gonna be happy because what's gonna happen is the kernel if you submit a 4k i/o is gonna say he wanted to read a",
    "start": "2678829",
    "end": "2686059"
  },
  {
    "text": "megabyte and so I'm just gonna grab that whole megabyte and completely deplete any burst bucket on an SSD volume for",
    "start": "2686059",
    "end": "2692000"
  },
  {
    "text": "example on a GP 2 volume if you're running on one of our Xen instances",
    "start": "2692000",
    "end": "2698000"
  },
  {
    "start": "2695000",
    "end": "2695000"
  },
  {
    "text": "here's how to increase the maximum i/o size so if you're doing any kind of throughput workload this is a per",
    "start": "2698000",
    "end": "2703010"
  },
  {
    "text": "instance configuration you know the default is set to 128 K which is a pretty decent balance between",
    "start": "2703010",
    "end": "2708369"
  },
  {
    "text": "getting enough throughput through and taking memory out of your guest now just",
    "start": "2708369",
    "end": "2714230"
  },
  {
    "text": "to be confusing this is measured in 4k memory pages not sectors one of these days Linux will be consistent there's",
    "start": "2714230",
    "end": "2723260"
  },
  {
    "text": "the the way to set that they change the parameter and 4.6 as well now you don't",
    "start": "2723260",
    "end": "2728809"
  },
  {
    "text": "want to do this if you're on a smaller instance type because what this does is it sets aside that memory ahead of time",
    "start": "2728809",
    "end": "2733849"
  },
  {
    "text": "when you attach that volume so 32 iOS times 1 megabyte if you increase this to a megabyte is going to be 32 megabytes",
    "start": "2733849",
    "end": "2740089"
  },
  {
    "text": "forever a o which isn't too bad on an hour 4 or a c5 but if you're trying to",
    "start": "2740089",
    "end": "2745460"
  },
  {
    "text": "do this on a t2 micro you're going to pretty quickly run out of memory for your application so the next input to",
    "start": "2745460",
    "end": "2757369"
  },
  {
    "text": "volume performance is EBS optimized instances in with EB s optimized instances we set aside a separate",
    "start": "2757369",
    "end": "2763790"
  },
  {
    "text": "bandwidth just for EBS which is separate from your application and so this is enabled by default on EBS on instances",
    "start": "2763790",
    "end": "2771319"
  },
  {
    "text": "with EBS on nitro and so this started back in c4 that is enabled by default",
    "start": "2771319",
    "end": "2777200"
  },
  {
    "text": "and that's because we were able to set aside a separate physical network connection for just for EBS",
    "start": "2777200",
    "end": "2782800"
  },
  {
    "text": "if you're on one of our older instances and you've launched without EBS optimized you can enable it by stopping",
    "start": "2782800",
    "end": "2788110"
  },
  {
    "text": "your instance changing the attribute and then restart your instance so if you are",
    "start": "2788110",
    "end": "2797830"
  },
  {
    "text": "an when you are an EVs instance it's important to consider that EBS optimize",
    "start": "2797830",
    "end": "2803230"
  },
  {
    "text": "bandwidth and so if you attach a two terabyte volume and you're doing an eye ops workload 16 ki UPS workload that gp2",
    "start": "2803230",
    "end": "2809860"
  },
  {
    "text": "volume should be able to do 6000 ions now you're on a c4 large and you notice",
    "start": "2809860",
    "end": "2816310"
  },
  {
    "text": "that you're only able to get around 4,000 I ops if we look at the EBS optimized throughput that's available on",
    "start": "2816310",
    "end": "2821950"
  },
  {
    "text": "a c4 large you do at 500 megabits per second it comes out to roughly 4,000 16",
    "start": "2821950",
    "end": "2828280"
  },
  {
    "text": "ki ops and so you're not going to be able to get the full volume performance out of that volume so if you need that",
    "start": "2828280",
    "end": "2835180"
  },
  {
    "text": "6000i amps at that 2 terabyte volume delivers you probably want to go to a larger instance say a c4 to extra-large",
    "start": "2835180",
    "end": "2841920"
  },
  {
    "text": "which has double the bandwidth available to EBS now with c5 continuing on using",
    "start": "2841920",
    "end": "2853570"
  },
  {
    "start": "2849000",
    "end": "2849000"
  },
  {
    "text": "data we took a look at how customers use EBS optimized instances one of the things that we realized is that a lot of",
    "start": "2853570",
    "end": "2860980"
  },
  {
    "text": "customers on smaller instant sizes have bursty workloads noticed it in volumes",
    "start": "2860980",
    "end": "2867280"
  },
  {
    "text": "we also notice it in aggregates of volumes and so we were able to change",
    "start": "2867280",
    "end": "2872619"
  },
  {
    "text": "the way that we thought about EBS optimized for our smaller instances and added a burst capability and so on c5",
    "start": "2872619",
    "end": "2879010"
  },
  {
    "text": "large extra large and two extra large and this is also m5 as well you can",
    "start": "2879010",
    "end": "2884440"
  },
  {
    "text": "burst up to the specifications of the c5 for extra large for 30 minutes and that gets reset automatically every day so",
    "start": "2884440",
    "end": "2892000"
  },
  {
    "text": "even if you're continuously driving max throughput you'll get that full 30 minutes every day once a day and then it",
    "start": "2892000",
    "end": "2898900"
  },
  {
    "text": "drops down to the baseline throughput of 525 megabits per second in this example",
    "start": "2898900",
    "end": "2905910"
  },
  {
    "start": "2905000",
    "end": "2905000"
  },
  {
    "text": "so if you need more storage than 16 terabytes if you need more throughput than 500 megabytes per second and if you",
    "start": "2905910",
    "end": "2912400"
  },
  {
    "text": "need or key or I option in 20,000 you can stripe a couple together but if you're going to raid",
    "start": "2912400",
    "end": "2918970"
  },
  {
    "text": "recommend that you avoid reading for redundancy so EBS is a distributed system we already shard your data and",
    "start": "2918970",
    "end": "2925210"
  },
  {
    "text": "replicate your data on the backend and if you're going to raid you're going to reduce that EBS optimize bandwidth it's",
    "start": "2925210",
    "end": "2930460"
  },
  {
    "text": "available to your application so if you're doing a straight mirror you're going to cut it in half if you're doing some part of race your coding like raid",
    "start": "2930460",
    "end": "2936520"
  },
  {
    "text": "5 raid 6 you're gonna get 20 to 30% less",
    "start": "2936520",
    "end": "2941910"
  },
  {
    "text": "so another question that I often get is what about EBS volume initialization and",
    "start": "2942330",
    "end": "2947560"
  },
  {
    "text": "how do I get the maximum performance out of my volume so if you have a new EBS volume just attached to your instance",
    "start": "2947560",
    "end": "2954310"
  },
  {
    "text": "it's ready to go you'll get full performance out of it if you've got a volume from snapshot EBS pulls that data",
    "start": "2954310",
    "end": "2961120"
  },
  {
    "text": "out of s3 in the background and we do this without you having to worry about it and if you act try to act to do an",
    "start": "2961120",
    "end": "2967930"
  },
  {
    "text": "i/o to something that we haven't pulled yet we'll prioritize pulling that data from s3 so one of the things that you",
    "start": "2967930",
    "end": "2975820"
  },
  {
    "text": "can do if we're not loading it fast enough is you can tell us and signal to us that you want to use some of your volume performance to pull the data from",
    "start": "2975820",
    "end": "2983200"
  },
  {
    "text": "s3 faster and so to do that you can do a random read across the volume now",
    "start": "2983200",
    "end": "2988210"
  },
  {
    "text": "recently for i/o one volumes we do this automatically in the back end so if you're not using your i/o one volume we'll go ahead and use all the volume",
    "start": "2988210",
    "end": "2995260"
  },
  {
    "text": "performance available to pull that s3 data we noticed about a 10x improvement in snapshot load times now when you're",
    "start": "2995260",
    "end": "3004200"
  },
  {
    "start": "3003000",
    "end": "3003000"
  },
  {
    "text": "doing this initialization we recommend a high queue depth random read across your volume that's going to give you the best",
    "start": "3004200",
    "end": "3009450"
  },
  {
    "text": "performance so here's an example FIO command when you start it I'd be super sad it's going to give you some",
    "start": "3009450",
    "end": "3016020"
  },
  {
    "text": "estimated time of completion in about a month and a half that's because we're",
    "start": "3016020",
    "end": "3021150"
  },
  {
    "text": "pulling at s3 speeds which are measured sometimes in seconds as we pull the data",
    "start": "3021150",
    "end": "3026640"
  },
  {
    "text": "the iOS are going to return faster and faster and faster until you get your full volume performance and so you don't",
    "start": "3026640",
    "end": "3033570"
  },
  {
    "text": "have to let this command can finish if we're going to pull the data and we're going to pull more data than you've requested here and really it's in the",
    "start": "3033570",
    "end": "3041160"
  },
  {
    "text": "neighborhood of 20 to 30 percent where you'll start to see your full volume performance and you can monitor the",
    "start": "3041160",
    "end": "3046650"
  },
  {
    "text": "latency that you're getting or the the I ops either using watch you're using tools within your instance and when you see the expected",
    "start": "3046650",
    "end": "3052979"
  },
  {
    "text": "volume performance you can just go ahead and kill this and your volume is initialized as you need it to be so you",
    "start": "3052979",
    "end": "3062880"
  },
  {
    "start": "3060000",
    "end": "3060000"
  },
  {
    "text": "want to select the right volume for your workload and if you mess up we've got elastic volumes you can change",
    "start": "3062880",
    "end": "3068609"
  },
  {
    "text": "it to a new volume type without having to stop your instance select the right instance for your workload make sure",
    "start": "3068609",
    "end": "3074400"
  },
  {
    "text": "it's got enough EBS optimized bandwidth to support the needs take snapshots tag",
    "start": "3074400",
    "end": "3079650"
  },
  {
    "text": "those snapshots tag those volumes so you know what's on them and if your your requirements demanded use encryption so",
    "start": "3079650",
    "end": "3087660"
  },
  {
    "start": "3087000",
    "end": "3087000"
  },
  {
    "text": "we've got some new storage training if you want to dive a little bit deeper and Jody and I will be hanging out for to",
    "start": "3087660",
    "end": "3094680"
  },
  {
    "text": "answer questions Thanks [Applause]",
    "start": "3094680",
    "end": "3102719"
  }
]