[
  {
    "start": "0",
    "end": "38000"
  },
  {
    "text": "uh so we're going to be focusing on um how to use EBS with ec2 to um scale your",
    "start": "840",
    "end": "6279"
  },
  {
    "text": "big data processing uh platforms to scale to meet growing demand while improving availability cost and",
    "start": "6279",
    "end": "11599"
  },
  {
    "text": "performance um I'll be reviewing our new EBS volumes we launched earlier this year um designed for Big Data workloads",
    "start": "11599",
    "end": "17520"
  },
  {
    "text": "um very shortly and then we'll have two customers come talk to you about how they started leveraging those um to improve their platforms for both cost",
    "start": "17520",
    "end": "23160"
  },
  {
    "text": "and efficiency we have uh zenes who's going to be talking about their elk stack and then we have videology who's",
    "start": "23160",
    "end": "28359"
  },
  {
    "text": "going to come and talk about their Hado platform form um which they re architected with ec2 and the new EBS",
    "start": "28359",
    "end": "33360"
  },
  {
    "text": "volumes um to ingest process and analyze",
    "start": "33360",
    "end": "37960"
  },
  {
    "start": "38000",
    "end": "134000"
  },
  {
    "text": "logs uh so storage options for Big Data um you've got a lot of options with Amazon web services um from file storage",
    "start": "39160",
    "end": "46199"
  },
  {
    "text": "with our simple and scalable elastic file system that went GA earlier this year which allows you to connect",
    "start": "46199",
    "end": "51680"
  },
  {
    "text": "multiple instances to a single file system um we also have block storage options with uh ec2 instance store and",
    "start": "51680",
    "end": "58840"
  },
  {
    "text": "uh EBS um you can attach to an ec2 instance or to an EMR cluster as well as",
    "start": "58840",
    "end": "64158"
  },
  {
    "text": "um our globally accessible and highly scalable object storage with simple storage service um big data can",
    "start": "64159",
    "end": "69640"
  },
  {
    "text": "Encompass a lot of different platforms we certainly understand that um a lot of times S3 will act as kind of a data",
    "start": "69640",
    "end": "75080"
  },
  {
    "text": "repository or data Lake um and anytime you sort of uh anytime you run any processing or compute on that um you",
    "start": "75080",
    "end": "81320"
  },
  {
    "text": "know we have Services you could use like uh Dynamo DB red shift Etc but anytime that you're using something like um Hado",
    "start": "81320",
    "end": "88119"
  },
  {
    "text": "platform or Kafka where you you need to uh spin up ec2 instances you have to decide what storage option you're going",
    "start": "88119",
    "end": "93799"
  },
  {
    "text": "to choose for that instance store or U Block store um so we're going to be focusing primarily today on EBS um",
    "start": "93799",
    "end": "99320"
  },
  {
    "text": "there's a lot of benefits to using EBS um there's data persistence so anytime you stop your server and restart it that",
    "start": "99320",
    "end": "105399"
  },
  {
    "text": "data is still there um which allows you to right size your instance based off the uh memory and compute rather than",
    "start": "105399",
    "end": "111320"
  },
  {
    "text": "the storage um it opens up a lot more options to what ec2 instances you choose as well as it's easier to detach and",
    "start": "111320",
    "end": "117479"
  },
  {
    "text": "retach the new instances when growing your your clusters out um it also has better security or easier to manage",
    "start": "117479",
    "end": "123600"
  },
  {
    "text": "security I should say with our uh EBS encryption uh and it has snapshot capabilities for backup um and also",
    "start": "123600",
    "end": "130119"
  },
  {
    "text": "really good volume monitoring with our Cloud watch with our Cloud watch metrics uh so earlier this year we",
    "start": "130119",
    "end": "136319"
  },
  {
    "start": "134000",
    "end": "277000"
  },
  {
    "text": "announced two new um EBS volumes um both are magnetic um they're throughput optimize ST1 and SC1 um you may have",
    "start": "136319",
    "end": "143640"
  },
  {
    "text": "asked yourself at the time you know why in the world of high performance and lowcost SSD disc would we focus on",
    "start": "143640",
    "end": "149280"
  },
  {
    "text": "spinning magnetic media well as it turns out you know all this data out there that's being generated um that needs to",
    "start": "149280",
    "end": "154959"
  },
  {
    "text": "be ingested stored processed and analyzed to help your company make faster more Intelligent Decisions um",
    "start": "154959",
    "end": "160720"
  },
  {
    "text": "largely can be sequential in nature which makes a really good fit for magnetic media um ssds are great for",
    "start": "160720",
    "end": "166040"
  },
  {
    "text": "random IO operations um things like boot volumes um relational databases like SQL MySQL Oracle um non-relational databases",
    "start": "166040",
    "end": "173519"
  },
  {
    "text": "like Cassandra and um couch um uh kind of data where it's broken up",
    "start": "173519",
    "end": "179480"
  },
  {
    "text": "into small small Random Access IO operations um with Magnetic media when you're um accessing a block um the disc",
    "start": "179480",
    "end": "186560"
  },
  {
    "text": "actuator arm and the head have to move to find the correct track which is known as your seek time um and the disc has to",
    "start": "186560",
    "end": "191920"
  },
  {
    "text": "spin underneath it to to find the correct sector which is known as your um rotational latency so the speed of these",
    "start": "191920",
    "end": "197280"
  },
  {
    "text": "operations really depends on how fast that disc can spin um with the Advent of ssds you no longer have a disc actuat",
    "start": "197280",
    "end": "203280"
  },
  {
    "text": "your arm and you no longer have um the spinning platter so um the speed to access those blocks improves greatly um",
    "start": "203280",
    "end": "210239"
  },
  {
    "text": "so it's not really that big of a concern but when it comes to sequential data um",
    "start": "210239",
    "end": "215400"
  },
  {
    "text": "we don't have that same penalty of seek time and rotational latency because the uh block that it access is located",
    "start": "215400",
    "end": "221560"
  },
  {
    "text": "directly after the previous block and the heading counters it right afterwards incurring no waight time you know so we",
    "start": "221560",
    "end": "227080"
  },
  {
    "text": "took this as a challenge and and we asked ourselves you know could we take advantage of lowcost hard drives um to",
    "start": "227080",
    "end": "234040"
  },
  {
    "text": "come up with a um High throughput storage option that delivers consistent performance um and the answer is yes and",
    "start": "234040",
    "end": "240959"
  },
  {
    "text": "we did um in April we announced a pair of new lowcost um High throughput optimized volumes um designed to take",
    "start": "240959",
    "end": "246640"
  },
  {
    "text": "advantage of the scale of the cloud to deliver high throughput on a consistent basis for use with Amazon ec2 and um EMR",
    "start": "246640",
    "end": "254120"
  },
  {
    "text": "uh clusters um like the existing um general purpose volumes which is SSD based the",
    "start": "254120",
    "end": "260680"
  },
  {
    "text": "new volumes um measure performance or actually I should say the the new volumes have a base performance um burst",
    "start": "260680",
    "end": "266639"
  },
  {
    "text": "performance and a burst credit balance but whereas the um SSD volumes measure performance in terms of iops the new",
    "start": "266639",
    "end": "272919"
  },
  {
    "text": "volumes measure performance in terms of throughput so this is really good for um",
    "start": "272919",
    "end": "279680"
  },
  {
    "start": "277000",
    "end": "326000"
  },
  {
    "text": "a variety of different workloads but it's really good and what we're thinking of when we designed it was a lot of Big Data oriented workloads like Hadoop",
    "start": "279680",
    "end": "285960"
  },
  {
    "text": "where they are largely sequential in nature and you're pushing large sequential IO blocks through um Kafka",
    "start": "285960",
    "end": "291280"
  },
  {
    "text": "media streaming log processing um uh all kinds of different things media",
    "start": "291280",
    "end": "296639"
  },
  {
    "text": "streaming as well so all kinds of things that use sequential data um the two different volumes um are very similar",
    "start": "296639",
    "end": "302280"
  },
  {
    "text": "but they they differ in the way they um offer throughput otherwise they're pretty much identical and the price is",
    "start": "302280",
    "end": "307520"
  },
  {
    "text": "different as well um so with the ST1 volumes um you've got a 40 megabytes per second um base performance and it could",
    "start": "307520",
    "end": "314840"
  },
  {
    "text": "burst up to 500 megabytes per second um and the capacity is 500 gabt to 1",
    "start": "314840",
    "end": "320560"
  },
  {
    "text": "terabyte so this is great for ideal for large block High throughput um sequential",
    "start": "320560",
    "end": "326120"
  },
  {
    "start": "326000",
    "end": "385000"
  },
  {
    "text": "workloads and then we have SC1 which is identical except that the burst buckets and and balance are different so with",
    "start": "326120",
    "end": "332000"
  },
  {
    "text": "SC1 you have 12 megabytes per second you can burst up to 250 megabytes per second so whereas ST1 can burst up to 500",
    "start": "332000",
    "end": "338639"
  },
  {
    "text": "megabytes um the new ones can burst up to 250 megabytes um capacity is the same",
    "start": "338639",
    "end": "344440"
  },
  {
    "text": "um you know we release these we say they're ideal for um logging and backup and cold data but we actually have a lot",
    "start": "344440",
    "end": "350120"
  },
  {
    "text": "of customers that are using them for Hadoop and Kafka and different workloads as well um where your throughput knes might be a little less the benefit is",
    "start": "350120",
    "end": "356080"
  },
  {
    "text": "that they're only 2 and a half cents per gig um so they're a lot cheaper um so what these do is they they give you the",
    "start": "356080",
    "end": "361720"
  },
  {
    "text": "option now um in the past if you were looking at a high throughput option with uh EBS um you were probably looking at",
    "start": "361720",
    "end": "367440"
  },
  {
    "text": "gp2 or io1 provision iops volumes and you probably didn't care about the um iops that came with it you just wanted a",
    "start": "367440",
    "end": "373599"
  },
  {
    "text": "throughput um so now you could rightsize your instance um for cost efficiency with the new volumes for much cheaper",
    "start": "373599",
    "end": "379400"
  },
  {
    "text": "and get a lot more throughput with them um which can help fit these needs um so with that being said I just want to give",
    "start": "379400",
    "end": "385680"
  },
  {
    "start": "385000",
    "end": "412000"
  },
  {
    "text": "an overview of these new volumes and kind of where we came from um I'm going to have zenes come stage and talk about how they re architected their elk stack",
    "start": "385680",
    "end": "393039"
  },
  {
    "text": "um using some of the new ec2 instances and new volumes and then we'll have videology come talk to you after",
    "start": "393039",
    "end": "399080"
  },
  {
    "text": "that okay great we'll just pass the mic uh hi everyone I'm David this Kyle",
    "start": "400080",
    "end": "408400"
  },
  {
    "text": "and uh how do we advance here uh first a moment about zenes if",
    "start": "408400",
    "end": "416000"
  },
  {
    "start": "412000",
    "end": "456000"
  },
  {
    "text": "you don't know zendesk um we uh we build customer relationship software hopefully",
    "start": "416000",
    "end": "423039"
  },
  {
    "text": "uh we have a lot of customers uh in the audience um we have as you can see uh",
    "start": "423039",
    "end": "428400"
  },
  {
    "text": "quite a few uh paid uh customers 87,000 uh accounts uh all over the world",
    "start": "428400",
    "end": "434360"
  },
  {
    "text": "who use our software to satisfy their customer service needs and here's some logos of some of our great customers",
    "start": "434360",
    "end": "441160"
  },
  {
    "text": "that that we help so we're uh we're we're um here",
    "start": "441160",
    "end": "446280"
  },
  {
    "text": "obviously because uh quite a bit of our solution runs in AWS as well as quite a bit of our internal",
    "start": "446280",
    "end": "453280"
  },
  {
    "text": "um systems used in in uh death uh test and staging run uh in uh",
    "start": "453280",
    "end": "460160"
  },
  {
    "text": "AWS uh so as uh David sort of mentioned at the start um what we want to uh bring",
    "start": "460160",
    "end": "465960"
  },
  {
    "text": "to this session is we want to talk about how we utilized uh the new EBS storage",
    "start": "465960",
    "end": "471440"
  },
  {
    "text": "types to to improve one of our larger and more expensive internal systems and",
    "start": "471440",
    "end": "476800"
  },
  {
    "text": "that is our logging system based on elk which uh which was is a very important",
    "start": "476800",
    "end": "482400"
  },
  {
    "text": "part of our Engineering Process and uh our production systems enables us to uh",
    "start": "482400",
    "end": "488599"
  },
  {
    "text": "debug and and do everything you do with logging system want to talk through some of our design choices as to why we",
    "start": "488599",
    "end": "494680"
  },
  {
    "text": "redesign the system uh as to uh some of the choices we made uh explain some of",
    "start": "494680",
    "end": "499800"
  },
  {
    "text": "the new benefits and in the end it worked out really great for us both in performance and in cost uh so with that",
    "start": "499800",
    "end": "506039"
  },
  {
    "text": "uh I'll hand it over to Kyle hello uh so I'm going to talk a little bit about uh how the technology",
    "start": "506039",
    "end": "512640"
  },
  {
    "start": "512000",
    "end": "545000"
  },
  {
    "text": "changes went uh and I'll give a little background for people who aren't familiar uh with elk so elk is made up",
    "start": "512640",
    "end": "518839"
  },
  {
    "text": "of three main components uh elastic search is the data Store Main uh meant for ingestion and searching of the data",
    "start": "518839",
    "end": "525399"
  },
  {
    "text": "uh used for storage uh this is where we primarily chose to spend our energy uh because that was uh where the most cost was",
    "start": "525399",
    "end": "532399"
  },
  {
    "text": "spent uh due to the storage uh and the high performance needed because of elastic search uh and log stash is used",
    "start": "532399",
    "end": "538360"
  },
  {
    "text": "for ingestion normalizing uh and parsing and then Cabana is built on top of that as a as a visualization",
    "start": "538360",
    "end": "545160"
  },
  {
    "start": "545000",
    "end": "594000"
  },
  {
    "text": "tool um so uh when we kicked off this cluster around three years ago uh the",
    "start": "545160",
    "end": "551160"
  },
  {
    "text": "EBS volume choices were uh quite limited you really only had uh SSD choices uh",
    "start": "551160",
    "end": "556519"
  },
  {
    "text": "and the billing model was such that uh you were actually charged for the storage allocation as well as the bandwidth on top of it that you were",
    "start": "556519",
    "end": "562600"
  },
  {
    "text": "using um so when we factored all that in we realized it wasn't really cost sensitive to use EBS at the time uh when",
    "start": "562600",
    "end": "569480"
  },
  {
    "text": "factored all those in so the initial cluster we built on top of the I2 instances uh inside Amazon uh i2s have",
    "start": "569480",
    "end": "575560"
  },
  {
    "text": "very good high IO throughput uh we use utilize the local SSD uh ephemeral",
    "start": "575560",
    "end": "580880"
  },
  {
    "text": "storage uh elastic search is naturally distributed in nature so uh the need for redundancy is built into the cluster um",
    "start": "580880",
    "end": "587839"
  },
  {
    "text": "so we didn't necessarily need the uh redundancy that EBS provides um and this served us well for quite a long",
    "start": "587839",
    "end": "594480"
  },
  {
    "start": "594000",
    "end": "614000"
  },
  {
    "text": "time um but as anything does it began to show uh some pain points this is meant to repr present how many outages we had",
    "start": "594480",
    "end": "600600"
  },
  {
    "text": "inside of our cluster uh pretty much an outage a day uh so what we started to",
    "start": "600600",
    "end": "606600"
  },
  {
    "text": "see were a lot of uh a lot of operational overhead issues uh and that is kind of what we",
    "start": "606600",
    "end": "612040"
  },
  {
    "text": "looked like at the time um so oh my slides messed up all",
    "start": "612040",
    "end": "618480"
  },
  {
    "start": "614000",
    "end": "624000"
  },
  {
    "text": "right so a few of the problems we hit uh first was around operational uh headaches hit that over there we go",
    "start": "618480",
    "end": "625120"
  },
  {
    "start": "624000",
    "end": "672000"
  },
  {
    "text": "operational headaches here I got it we'll just flip them all down it's fine there we go um",
    "start": "625120",
    "end": "632720"
  },
  {
    "text": "am I on the right one yeah so the main was around operational headaches um because we're using ephemeral storage and not utilizing uh the EBS uh service",
    "start": "632720",
    "end": "640360"
  },
  {
    "text": "we had to maintain things like encryption at rest uh so because of that we had to provision our own encryption Keys make sure those were rolled uh make",
    "start": "640360",
    "end": "646600"
  },
  {
    "text": "sure our data was in compliance and and just maintain uh provided a lot of operational overhead for the",
    "start": "646600",
    "end": "652959"
  },
  {
    "text": "team uh our data retention was not being met we actually claimed that we retained",
    "start": "652959",
    "end": "658680"
  },
  {
    "text": "90 days but we were actually retaining 30 days um and that was just because the cost was way too high um what we noticed",
    "start": "658680",
    "end": "666320"
  },
  {
    "text": "was um well yeah so the cost was way too high uh we had to uh allocate storage",
    "start": "666320",
    "end": "672399"
  },
  {
    "start": "672000",
    "end": "712000"
  },
  {
    "text": "based on how many instances were actually inside the cluster um so what this is meant to show is every time we",
    "start": "672399",
    "end": "677920"
  },
  {
    "text": "needed to add a new instance in to scale up our storage we actually had to add a new uh instance into the cluster",
    "start": "677920",
    "end": "683920"
  },
  {
    "text": "whenever we did that there was always kind of this gap between here I'll push it there was kind of this gap between what was allocated",
    "start": "683920",
    "end": "689800"
  },
  {
    "text": "and what we were actually using so we found there was a PR pretty big gap between money lost on what we allocated",
    "start": "689800",
    "end": "695600"
  },
  {
    "text": "uh as we scaled up uh and it was just generally bad practice to have uh the storage allocated tied to how many",
    "start": "695600",
    "end": "701320"
  },
  {
    "text": "instances were inside the cluster uh so we kicked off an investigation phase uh and started",
    "start": "701320",
    "end": "707160"
  },
  {
    "text": "looking into how we could redesign this both around better performance and around better",
    "start": "707160",
    "end": "712399"
  },
  {
    "start": "712000",
    "end": "765000"
  },
  {
    "text": "cost uh so that led us to uh a few different things to investigate the main was around user patterns how people were",
    "start": "712399",
    "end": "718440"
  },
  {
    "text": "actually accessing the data inside the cluster um so as we started analyzing those queries and talking to people uh",
    "start": "718440",
    "end": "724560"
  },
  {
    "text": "in support and in engineering um we found that generally",
    "start": "724560",
    "end": "729639"
  },
  {
    "text": "they only really cared about the last few days worth of log data uh it was either used to debug a a production",
    "start": "729639",
    "end": "734839"
  },
  {
    "text": "issue or uh gather data for a customer uh within the past few days and if the data was older than that they were okay",
    "start": "734839",
    "end": "741199"
  },
  {
    "text": "waiting a few extra seconds for a query so that was really valuable um that allowed us to kind of Define our",
    "start": "741199",
    "end": "747279"
  },
  {
    "text": "performance requirements uh around congestion search time um uh uh and make",
    "start": "747279",
    "end": "753320"
  },
  {
    "text": "sure we're providing customers with a good experience uh and the other uh investigation point was the fact that",
    "start": "753320",
    "end": "759519"
  },
  {
    "text": "they announced a bunch of new volumes right when we started this project uh so the timing actually worked out pretty",
    "start": "759519",
    "end": "765399"
  },
  {
    "text": "great um so this was me ah man all my transitions are broken so this was me uh when the email came out uh we began to",
    "start": "765399",
    "end": "772760"
  },
  {
    "text": "look into this as a a solution to allow us to meet our retention periods as well",
    "start": "772760",
    "end": "777839"
  },
  {
    "text": "as a way to decouple our instances from uh the actual storage amounts that were met uh we began to theorize that the",
    "start": "777839",
    "end": "784959"
  },
  {
    "text": "performance of the new uh cold and warm storage options would give us good enough performance for some older data",
    "start": "784959",
    "end": "792160"
  },
  {
    "text": "uh while still U utilizing SSD for uh ingestion and surch of the most recent",
    "start": "792160",
    "end": "797760"
  },
  {
    "text": "data um so that gave us a really clear line of sight into a new proposal on how to redesign our",
    "start": "797760",
    "end": "804160"
  },
  {
    "start": "798000",
    "end": "861000"
  },
  {
    "text": "cluster um so we The Proposal made up a few different pieces the first was to just go all in on EBS",
    "start": "804160",
    "end": "810240"
  },
  {
    "text": "um uh it gave us it reduced the complexity around encryption uh and it also decoupled the instance count from",
    "start": "810240",
    "end": "816399"
  },
  {
    "text": "the storage it was allocated the second proposal was to break up the allocations into three",
    "start": "816399",
    "end": "822040"
  },
  {
    "text": "distinct uh storage tiers the main being the hot storage around uh the SSD ingestion and the most recent data and",
    "start": "822040",
    "end": "829760"
  },
  {
    "text": "then promote that data off into other tiers as it aged off uh and the other proposal was to uh",
    "start": "829760",
    "end": "837320"
  },
  {
    "text": "redesign our instance types uh inside each layer to better optimize around performance and cost for each layer",
    "start": "837320",
    "end": "843279"
  },
  {
    "text": "specifically when we were using the I2 instances um we were getting the same performance for data from 30 days ago as",
    "start": "843279",
    "end": "850480"
  },
  {
    "text": "data from the last second um and we just found there was a big gap between the performance we allocated versus how",
    "start": "850480",
    "end": "856199"
  },
  {
    "text": "people were actually using it uh and that was a big the big differentiator around cost uh so this is kind of how the",
    "start": "856199",
    "end": "862440"
  },
  {
    "start": "861000",
    "end": "880000"
  },
  {
    "text": "proposal broke down uh we put rough numbers together we said the past seven days sounds good we'll keep that on uh",
    "start": "862440",
    "end": "869120"
  },
  {
    "text": "hot gp2 storage when the e8th day rolls around we'll promote that off into the St uh and then we'll keep data older",
    "start": "869120",
    "end": "875440"
  },
  {
    "text": "than that on the SC storage that should say 90 days just pretend it says 90",
    "start": "875440",
    "end": "881000"
  },
  {
    "start": "880000",
    "end": "895000"
  },
  {
    "text": "days um so elastic search has a great uh way to manage this they have a tool called curator which allows you to",
    "start": "881000",
    "end": "886399"
  },
  {
    "text": "Define buckets of where you want the The Shard data to live um and so we use this",
    "start": "886399",
    "end": "891639"
  },
  {
    "text": "as a way to automate the provisioning of that so I'm going to do a quick walk through of what that looks like um so",
    "start": "891639",
    "end": "897519"
  },
  {
    "text": "here you can see uh it's January 1st 2016 uh we have data coming in on the",
    "start": "897519",
    "end": "902959"
  },
  {
    "text": "primary hot storage and you can see there's currently no data on the other two uh storage layers so as we move on to January 8th",
    "start": "902959",
    "end": "911040"
  },
  {
    "text": "that data is now older than seven days uh we'll kick off a promotion process to move it into the warm layer uh and the cluster will just rebalance it naturally",
    "start": "911040",
    "end": "918560"
  },
  {
    "text": "so on the e8th day you can see data from the first has now moved onto the warm and the data from today is now being",
    "start": "918560",
    "end": "923800"
  },
  {
    "text": "ingested on the primary layer so as we move on to the 15th the data from the eth is now older than",
    "start": "923800",
    "end": "929360"
  },
  {
    "text": "seven days we'll kick off the same process promote that data over uh so now we have both those inside of there data",
    "start": "929360",
    "end": "935079"
  },
  {
    "text": "from the 1 to the eth uh and now the new data is coming in on the primary layer as expected um and so then on the 30th",
    "start": "935079",
    "end": "941680"
  },
  {
    "text": "the data from those two uh from the first has now aged off uh we run the same process and promote that off into",
    "start": "941680",
    "end": "948199"
  },
  {
    "text": "the cold storage so that ends up with something like this um so the data will naturally tear off uh as the data",
    "start": "948199",
    "end": "955199"
  },
  {
    "text": "ages so here's a little bit how that looks inside of elastic search search has the idea of routing allocation um",
    "start": "955199",
    "end": "962279"
  },
  {
    "start": "956000",
    "end": "986000"
  },
  {
    "text": "which is where you place the sharded data inside of your cluster so any new data coming in would be allocated with a",
    "start": "962279",
    "end": "967519"
  },
  {
    "text": "hot tag uh which is what we would build the primary SSD volume data nodes with",
    "start": "967519",
    "end": "972759"
  },
  {
    "text": "so that would ensure all data coming in comes in on the SSD volumes and then we would just run uh a Cron job to update",
    "start": "972759",
    "end": "979240"
  },
  {
    "text": "that uh when the data becomes older than seven days uh and then the the cluster will naturally rebalance",
    "start": "979240",
    "end": "986279"
  },
  {
    "start": "986000",
    "end": "1001000"
  },
  {
    "text": "that uh and you can't do a talk at Amazon without a topology graph so this is a little bit how uh our cluster",
    "start": "986279",
    "end": "991839"
  },
  {
    "text": "looked uh you can see we're distributed in multiple azs um and this is meant to show the three different volume types uh",
    "start": "991839",
    "end": "998440"
  },
  {
    "text": "on our data layers um so the bosses were super happy",
    "start": "998440",
    "end": "1004319"
  },
  {
    "text": "we cut cost uh we were a able to utilize uh different instance types we also found as part of this we were over",
    "start": "1004319",
    "end": "1010440"
  },
  {
    "text": "allocating um our jbm heat memory so we were not using compressed pointers inside the jvm uh memory is one of the",
    "start": "1010440",
    "end": "1017440"
  },
  {
    "text": "most expensive Amazon resources so being able to compress that and utilize it on better instance types uh made a big",
    "start": "1017440",
    "end": "1024280"
  },
  {
    "start": "1024000",
    "end": "1051000"
  },
  {
    "text": "deal uh we took this time to optimize a few other things uh the Legacy cluster wasn't built with any kind of template",
    "start": "1024280",
    "end": "1029839"
  },
  {
    "text": "or cloud formation uh which was kind of a mess for management U so we utilized a great open source tool named Sparkle",
    "start": "1029839",
    "end": "1035438"
  },
  {
    "text": "formation uh Sparkle formation is a ruby DSL language meant for writing reusable code to generate cloud formation",
    "start": "1035439",
    "end": "1041678"
  },
  {
    "text": "templates and then promote those out um so we were able to build three distinct data layers inside different templates",
    "start": "1041679",
    "end": "1047640"
  },
  {
    "text": "and update those dynamically uh as we scaled up we also had a lot of issues around",
    "start": "1047640",
    "end": "1053160"
  },
  {
    "start": "1051000",
    "end": "1082000"
  },
  {
    "text": "data transmission uh from the client to the server um because we log uh a lot",
    "start": "1053160",
    "end": "1059120"
  },
  {
    "text": "the that data would buffer locally on each client and if we had any network instability uh into Amazon uh some of",
    "start": "1059120",
    "end": "1064720"
  },
  {
    "text": "that data could be lost or uh or delayed uh so we utilized two new uh open source",
    "start": "1064720",
    "end": "1070360"
  },
  {
    "text": "tools uh fluent D is what we deployed on every client node uh and then we deployed kfka which is a great reliable",
    "start": "1070360",
    "end": "1076240"
  },
  {
    "text": "message buffer uh for the for the buffer between clients and the",
    "start": "1076240",
    "end": "1081679"
  },
  {
    "text": "servers um so this is a little bit how that looks today um it doesn't have my",
    "start": "1082760",
    "end": "1089559"
  },
  {
    "text": "so uh inside the data center you can see we deploy fluent on each client node that watches a subset of logs feeds",
    "start": "1089559",
    "end": "1095440"
  },
  {
    "text": "those into a Kafka topic on one or more partitions and then inside Amazon uh we",
    "start": "1095440",
    "end": "1100600"
  },
  {
    "text": "point those at each specific pod or vepc read that data out and then feed it into our primary data node cluster uh and",
    "start": "1100600",
    "end": "1107559"
  },
  {
    "text": "what you would see here is the other data nodes uh below elastic search as the data promotes",
    "start": "1107559",
    "end": "1113520"
  },
  {
    "text": "out and I'll let David talk a little bit how it went okay",
    "start": "1113520",
    "end": "1119600"
  },
  {
    "start": "1114000",
    "end": "1120000"
  },
  {
    "text": "so uh where we got to here uh we're pretty proud of um so overall uh we",
    "start": "1119600",
    "end": "1127320"
  },
  {
    "start": "1120000",
    "end": "1211000"
  },
  {
    "text": "operated out of two regions uh us and Europe um overall our complete expense",
    "start": "1127320",
    "end": "1133760"
  },
  {
    "text": "in the project was basically cut in half uh so that was great um we are able to",
    "start": "1133760",
    "end": "1140039"
  },
  {
    "text": "increase our data retention uh to uh 90 days and in an expandable way so uh what",
    "start": "1140039",
    "end": "1148120"
  },
  {
    "text": "that ended up was um a real win uh in terms of uh in terms of delivering what",
    "start": "1148120",
    "end": "1154320"
  },
  {
    "text": "we needed to in addition the whole way to scale the system got much more predictable",
    "start": "1154320",
    "end": "1160080"
  },
  {
    "text": "because we're adding servers and storage separately um we can you know grow storage in a more incremental way and",
    "start": "1160080",
    "end": "1167200"
  },
  {
    "text": "keep it closer to the actual needs that we have uh and without unnecessarily",
    "start": "1167200",
    "end": "1172400"
  },
  {
    "text": "provisioning additional ec2s for compute and then when we needed to Pro provision",
    "start": "1172400",
    "end": "1178000"
  },
  {
    "text": "additional ec2s or if we lost an ec2 for maintenance reason or what have you uh the whole cluster didn't rebalance we",
    "start": "1178000",
    "end": "1184159"
  },
  {
    "text": "were able to just replace that without uh a lot of maintenance window so that was really great uh overall the uh the",
    "start": "1184159",
    "end": "1192080"
  },
  {
    "text": "um we were able to utilize a number of uh Amazon features for example now we could take advantage of of uh a built-",
    "start": "1192080",
    "end": "1198880"
  },
  {
    "text": "in EBS encryption and so um we didn't have to bring our own code in and that",
    "start": "1198880",
    "end": "1204559"
  },
  {
    "text": "was much easier to deal with and generally the cluster uh stability increased you know pretty massively um",
    "start": "1204559",
    "end": "1211400"
  },
  {
    "start": "1211000",
    "end": "1248000"
  },
  {
    "text": "Everyone likes to see you know uh dollars result uh especially in a use",
    "start": "1211400",
    "end": "1216760"
  },
  {
    "text": "case like this uh so what we had here is uh a figure where we spent internally",
    "start": "1216760",
    "end": "1224200"
  },
  {
    "text": "around uh uh $600,000 and you see we reduce that to about 300 which is a",
    "start": "1224200",
    "end": "1229919"
  },
  {
    "text": "pretty good number for for a reduction what's most interesting uh is that we have a 5x increase uh or rather a 5x uh",
    "start": "1229919",
    "end": "1239120"
  },
  {
    "text": "decrease in the cost per storage unit so that's pretty significant uh benefit that we delivered uh in in inside the",
    "start": "1239120",
    "end": "1248360"
  },
  {
    "start": "1248000",
    "end": "1325000"
  },
  {
    "text": "business uh I guess our our takeaways from the project are really uh as Kyle",
    "start": "1248360",
    "end": "1253960"
  },
  {
    "text": "mentioned you know uh when you Embark upon a project like uh uh a project you",
    "start": "1253960",
    "end": "1259720"
  },
  {
    "text": "know really look at the use case of how people use the data don't just assume that okay we have so much storage need",
    "start": "1259720",
    "end": "1266600"
  },
  {
    "text": "and so just provision a whole bunch of storage uh uh basically uh optimizing",
    "start": "1266600",
    "end": "1272240"
  },
  {
    "text": "for the most demanding storage requirements so a little bit of thought in terms of that will go a long way in",
    "start": "1272240",
    "end": "1278760"
  },
  {
    "text": "in both the costing and the and the flexibility look and find where the performance really matters uh sometimes",
    "start": "1278760",
    "end": "1284960"
  },
  {
    "text": "people just sort of believe that oh this is going to be uh ioe you know bound so",
    "start": "1284960",
    "end": "1290799"
  },
  {
    "text": "I'm just going to choose this type of ec2 that could be a very pricey decision so really spend some time of thinking",
    "start": "1290799",
    "end": "1297960"
  },
  {
    "text": "about where performance really matters and where it doesn't matter and where you can you know optimize cost or or buy",
    "start": "1297960",
    "end": "1303720"
  },
  {
    "text": "yourself a little bit of flexibility especially be careful about buying more than you need in terms of over",
    "start": "1303720",
    "end": "1309360"
  },
  {
    "text": "provisioning uh and so we have more smaller machines and more granular capability and always you know try to",
    "start": "1309360",
    "end": "1316679"
  },
  {
    "text": "use AWS Services wherever you can because that's less for you to have to build and maintain so",
    "start": "1316679",
    "end": "1323720"
  },
  {
    "text": "um uh so thanks for uh for our session thanks Kyle um and I guess uh we'll call",
    "start": "1323720",
    "end": "1329840"
  },
  {
    "start": "1325000",
    "end": "1370000"
  },
  {
    "text": "hand the microphone over to the uh videology guys thank you",
    "start": "1329840",
    "end": "1336679"
  },
  {
    "text": "[Applause]",
    "start": "1338080",
    "end": "1341630"
  },
  {
    "text": "all right so we are ideology I Am David Ortiz senior",
    "start": "1350840",
    "end": "1358159"
  },
  {
    "text": "software engineer uh Paul Frederickson principal devops engineer and we are in support of the",
    "start": "1358159",
    "end": "1365120"
  },
  {
    "text": "Big Data team at",
    "start": "1365120",
    "end": "1367960"
  },
  {
    "start": "1370000",
    "end": "1386000"
  },
  {
    "text": "videology so in this session you can expect to get an intro of who we are as a company uh the challenges we were",
    "start": "1370559",
    "end": "1377200"
  },
  {
    "text": "facing on Hado as well as how we move to an EBS back cluster and why we are much happier",
    "start": "1377200",
    "end": "1383039"
  },
  {
    "text": "Engineers as a result of this uh just a quick overview of",
    "start": "1383039",
    "end": "1388120"
  },
  {
    "text": "videology if uh nobody's ever heard of us uh we were founded in 2007 by Scott Ferber who also uh co-founded",
    "start": "1388120",
    "end": "1394640"
  },
  {
    "text": "advertising.com and ended up selling it to AOL for half a billion dollars our uh",
    "start": "1394640",
    "end": "1401000"
  },
  {
    "text": "corporate headquarters is in New York uh but our technology team is actually based in uh Baltimore",
    "start": "1401000",
    "end": "1406440"
  },
  {
    "text": "Maryland uh as you can see we have offices all over the world um Sydney",
    "start": "1406440",
    "end": "1412000"
  },
  {
    "text": "Toronto London Singapore uh Texas California Baltimore obviously uh our",
    "start": "1412000",
    "end": "1418760"
  },
  {
    "text": "employees we actually I think over 400 people now uh we're growing very quickly uh as you can see here's some of our",
    "start": "1418760",
    "end": "1425159"
  },
  {
    "text": "recent client wins Sky media uh AT&T Adobe uh Rogers a lot of media",
    "start": "1425159",
    "end": "1432360"
  },
  {
    "start": "1432000",
    "end": "1451000"
  },
  {
    "text": "companies uh more marketing slides for you guys the industry accolades best",
    "start": "1432840",
    "end": "1438039"
  },
  {
    "text": "digital video ad platform most sophisticated media Optimizer and 6X",
    "start": "1438039",
    "end": "1443880"
  },
  {
    "text": "higher uh brand lift um from a uh Nelson uh or sorry neelon uh",
    "start": "1443880",
    "end": "1450960"
  },
  {
    "text": "study all right so since we are about to go in- depth into how our hudo cluster runs I figured I would give a quick",
    "start": "1450960",
    "end": "1457120"
  },
  {
    "start": "1451000",
    "end": "1531000"
  },
  {
    "text": "overview of hudo just in case people aren't familiar with it so hudo is a platform that is based on how Google",
    "start": "1457120",
    "end": "1464440"
  },
  {
    "text": "used to run their infrastructure and it provides scalable cluster Computing on",
    "start": "1464440",
    "end": "1469480"
  },
  {
    "text": "commodity Hardware as opposed to having to go by high-end specialized stuff so",
    "start": "1469480",
    "end": "1474520"
  },
  {
    "text": "this doesn't mean cheap but readily available servers that you could order off the shelf from like Dell or any other provider as opposed to the",
    "start": "1474520",
    "end": "1480360"
  },
  {
    "text": "high-end stuff uh the big thing with Hadoop that's different than some of the",
    "start": "1480360",
    "end": "1485559"
  },
  {
    "text": "platforms that came before it is it tries to move the compute to the data instead of bringing the data to your B",
    "start": "1485559",
    "end": "1490679"
  },
  {
    "text": "compute nodes um it's broken down into two parts you have uh hdfs which is your file system so to speak and that is done",
    "start": "1490679",
    "end": "1498120"
  },
  {
    "text": "by allocating block storage on a bunch of different nodes and just kind of round robin writing to them into large",
    "start": "1498120",
    "end": "1504960"
  },
  {
    "text": "file blocks of about 64 megabytes so that you can take advantage of sequential reads and then uh as you can",
    "start": "1504960",
    "end": "1512000"
  },
  {
    "text": "see in the slide the data node would be the storage provider there and then you have the name node to track everything",
    "start": "1512000",
    "end": "1517440"
  },
  {
    "text": "you also have what's called yarn and that is your compute framework so that basically just tracks the resources",
    "start": "1517440",
    "end": "1523480"
  },
  {
    "text": "available on the Node managers and allocates them the jobs as they come in from whatever user or code is submitting",
    "start": "1523480",
    "end": "1531640"
  },
  {
    "start": "1531000",
    "end": "1601000"
  },
  {
    "text": "it so where does this fit into videology so we have a AR highle architecture",
    "start": "1531640",
    "end": "1537279"
  },
  {
    "text": "diagram since we are an ad serving Media company we have a bunch of AD servers",
    "start": "1537279",
    "end": "1542320"
  },
  {
    "text": "that are running on AWS which generate a ton of logs which go to S3 so from there",
    "start": "1542320",
    "end": "1548120"
  },
  {
    "text": "we use our Hadoop cluster as kind of a giant ETL uh pipeline so we have warehouse jobs that pull everything off",
    "start": "1548120",
    "end": "1554559"
  },
  {
    "text": "S3 clean the data up normalize it if needed stick it into to uh the file system and then we have other extract",
    "start": "1554559",
    "end": "1561559"
  },
  {
    "text": "jobs that Run Aggregates over it push it back out to S3 where it's then loaded into uh some various reporting systems",
    "start": "1561559",
    "end": "1568600"
  },
  {
    "text": "uh we've got data warehouse or analytics warehouse and then uh some operational databases that people inside the company",
    "start": "1568600",
    "end": "1574480"
  },
  {
    "text": "use to track performance so just to provide some metrics uh this is running roughly about 1,200 Jobs per day uh it's",
    "start": "1574480",
    "end": "1581799"
  },
  {
    "text": "mixed workload of copies HIV scripts which is a SQL over Hadoop framework and",
    "start": "1581799",
    "end": "1588000"
  },
  {
    "text": "then crunch which is a highle Java framework for writing map reduced jobs",
    "start": "1588000",
    "end": "1593440"
  },
  {
    "text": "uh we process about 7 terabytes of data on the input side per day and I already",
    "start": "1593440",
    "end": "1598559"
  },
  {
    "text": "mentioned the systems we feed so you can see our original uh",
    "start": "1598559",
    "end": "1604039"
  },
  {
    "start": "1601000",
    "end": "1629000"
  },
  {
    "text": "production environment um these kind of follow the the components that uh David just talked about uh most importantly",
    "start": "1604039",
    "end": "1611279"
  },
  {
    "text": "the 30 worker nodes that we're running on the cc2 8xl uh instances uh 32 uh",
    "start": "1611279",
    "end": "1617520"
  },
  {
    "text": "CPUs 60 GB of RAM and 3.2 terab uh of storage per",
    "start": "1617520",
    "end": "1625080"
  },
  {
    "text": "instance and uh This original production uh environment resulted in this um so",
    "start": "1625080",
    "end": "1632240"
  },
  {
    "start": "1629000",
    "end": "1683000"
  },
  {
    "text": "lots of uh hip chat messages of why is this instance down uh you can't really see it very well but it's essentially",
    "start": "1632240",
    "end": "1638840"
  },
  {
    "text": "hey the status checks are failing on these machines and me responding with weal that's usually not good let's build",
    "start": "1638840",
    "end": "1644159"
  },
  {
    "text": "new ones yeah so some of the other problems Beyond reliability we had some with scalability where uh we were very",
    "start": "1644159",
    "end": "1651120"
  },
  {
    "text": "pegged on the system as you can see our memory was always full so once we would have any kind of increase in data volume",
    "start": "1651120",
    "end": "1658520"
  },
  {
    "text": "we would find ourselves having to frantically contact Paul and his friends to spin up more nodes for us uh we also",
    "start": "1658520",
    "end": "1665880"
  },
  {
    "text": "would have issues just in terms of the latency of the jobs like some of the copy jobs from S3 would end up taking",
    "start": "1665880",
    "end": "1671240"
  },
  {
    "text": "like 30 to 40 minutes to run for an hour's worth of data which as you can imagine is somewhat untenable when",
    "start": "1671240",
    "end": "1676480"
  },
  {
    "text": "you're trying to do that and then also process the data as well in the span of that",
    "start": "1676480",
    "end": "1682679"
  },
  {
    "text": "hour and that brought us to deciding we needed to do something different so after the cc2 line Amazon kind of moved",
    "start": "1682679",
    "end": "1690399"
  },
  {
    "text": "to a lot of their instances being SSD backed which is great as a general thing",
    "start": "1690399",
    "end": "1695880"
  },
  {
    "text": "but when you're trying to run H dup and you need like 100 terabytes of storage on your cluster that's a lot of nodes that have to spin up if you're using",
    "start": "1695880",
    "end": "1701519"
  },
  {
    "text": "ephemeral storage so we kind of talked to our Hadoop vendor which was cladera",
    "start": "1701519",
    "end": "1707159"
  },
  {
    "text": "and kind of said hey do you guys have any plans to support EBS because we really need to move to notes that use it",
    "start": "1707159",
    "end": "1713399"
  },
  {
    "text": "and we kind of yeah no again so they kind of seemed open to trying to do like",
    "start": "1713399",
    "end": "1719120"
  },
  {
    "text": "a proof of concept with us we could never quite get everything going with it but they were like you should try these",
    "start": "1719120",
    "end": "1724320"
  },
  {
    "text": "D2 nodes they're really cool so we started testing those in March of this year uh that didn't quite work as you",
    "start": "1724320",
    "end": "1731919"
  },
  {
    "text": "can probably guess by the fact we're presenting at the EBS uh session right now so uh as we're starting to go back",
    "start": "1731919",
    "end": "1738919"
  },
  {
    "text": "to the drawing board Cloud uh not Cloud a AWS announced the SC1 and the ST1 volumes of storage since we are running",
    "start": "1738919",
    "end": "1747480"
  },
  {
    "text": "highly sequential workloads that kind of sounded perfect for what we were trying to do so and it was it was exactly just",
    "start": "1747480",
    "end": "1752840"
  },
  {
    "text": "like zenes where it was we're in the middle of it and boom there's the there's the whole thing and I sent it to",
    "start": "1752840",
    "end": "1758880"
  },
  {
    "text": "to his team and said hey look at these things these I think will work we kind of looked and went yeah those will work",
    "start": "1758880",
    "end": "1763960"
  },
  {
    "text": "let's do that so we started designing uh the cluster around that in May and then",
    "start": "1763960",
    "end": "1769159"
  },
  {
    "text": "uh through June and July we were kind of testing the different configurations of storage we started out with an SC1",
    "start": "1769159",
    "end": "1775320"
  },
  {
    "text": "configuration because it was the cheapest and cost savings yay so we",
    "start": "1775320",
    "end": "1780440"
  },
  {
    "text": "decided to try that that didn't quite give us the performance we needed but it was promising so we moved up to the s1s",
    "start": "1780440",
    "end": "1787120"
  },
  {
    "text": "with uh smaller volumes and then decided just to go For Broke and get the volumes that we give us the fastest throughput",
    "start": "1787120",
    "end": "1793360"
  },
  {
    "text": "just to be safe and uh in August of this past year we went into production with",
    "start": "1793360",
    "end": "1800440"
  },
  {
    "text": "this and after about two years of service the cc2 cluster finally",
    "start": "1800440",
    "end": "1806120"
  },
  {
    "text": "died so this is just another way to look at our U progression that we went through so we had the we started with",
    "start": "1806880",
    "end": "1812320"
  },
  {
    "text": "the cc2 8xl uh they were old not enough disc um they failed pretty regularly uh and they",
    "start": "1812320",
    "end": "1819320"
  },
  {
    "text": "just cost a lot of money and uh for what we were getting uh the D28 XLS uh as was",
    "start": "1819320",
    "end": "1825799"
  },
  {
    "text": "mentioned we moved to tons of disc more disc than we could ever use uh but just didn't give us the resources we wanted",
    "start": "1825799",
    "end": "1832399"
  },
  {
    "text": "and they were also very expensive made it very difficult to scale uh in any kind of",
    "start": "1832399",
    "end": "1838679"
  },
  {
    "text": "uh large large amount essentially uh then we looked at the m410 XLS CPU",
    "start": "1838679",
    "end": "1845080"
  },
  {
    "text": "memory disc everything was perfect complete Nirvana happiness ensued so to",
    "start": "1845080",
    "end": "1850840"
  },
  {
    "text": "prepare you for the next slide that D28 XL should actually say not enough CPU rather than not enough memory because",
    "start": "1850840",
    "end": "1856320"
  },
  {
    "text": "you will see they actually have lots of memory on in a second so here's the D2 8xl um so you",
    "start": "1856320",
    "end": "1863919"
  },
  {
    "start": "1860000",
    "end": "1916000"
  },
  {
    "text": "can see the worker node the most important part here that we're focused on uh we started with 10 of them uh and",
    "start": "1863919",
    "end": "1870840"
  },
  {
    "text": "it had 36 CPUs 244 gigs of RAM and 48 terabytes of uh storage which was way",
    "start": "1870840",
    "end": "1878919"
  },
  {
    "text": "more than we needed yeah for reference in the old cluster we stored about 100 like 80 to 100 terabytes of data at any",
    "start": "1878919",
    "end": "1884559"
  },
  {
    "text": "given time in this cluster we had 480 terabytes what we we didn't have was enough CPU cores to actually run an hour",
    "start": "1884559",
    "end": "1890440"
  },
  {
    "text": "of data so our initial test looked awesome with this like the discs were blazing fast IO was nice and then we",
    "start": "1890440",
    "end": "1896200"
  },
  {
    "text": "started trying to run our full workload and it just ground to a halt because it couldn't run enough parallel tasks I",
    "start": "1896200",
    "end": "1901240"
  },
  {
    "text": "think we're actually really close to going in production with us because we went to it and we're like let's start actually running production data on it",
    "start": "1901240",
    "end": "1907600"
  },
  {
    "text": "and that's when it fell apart for us yeah it was the stress testing portion of things and just with the cost of the",
    "start": "1907600",
    "end": "1913240"
  },
  {
    "text": "workers it was too much to add that many more to fix it uh so then we moved done to the M4 uh 10 XLS with the SC1",
    "start": "1913240",
    "end": "1920840"
  },
  {
    "start": "1916000",
    "end": "1978000"
  },
  {
    "text": "prototype uh our vendor clauder mentioned to us they said the SC On's look like they you know the bandwidth is",
    "start": "1920840",
    "end": "1926880"
  },
  {
    "text": "enough this should be able to handle it so we jumped into it uh we were able to scale out and all of this was we tried",
    "start": "1926880",
    "end": "1933480"
  },
  {
    "text": "to stick you know to give some context with how we chose the number of nodes and and in storage is it was all cost",
    "start": "1933480",
    "end": "1940320"
  },
  {
    "text": "driven so we wanted to keep kind of the same amount of costs in budget so that we could you know it was easier to sell",
    "start": "1940320",
    "end": "1947080"
  },
  {
    "text": "and it was uh uh we wanted to be able to show that you know we're making improvements with the same amount of money so 18 18 worker nodes uh 40 CPUs",
    "start": "1947080",
    "end": "1955720"
  },
  {
    "text": "160 gigs of RAM and uh four uh terabytes of memory um the big problem here was",
    "start": "1955720",
    "end": "1962720"
  },
  {
    "text": "just the SE se1 just wasn't fastness for us so when we were going through and looking at the at the numbers and and",
    "start": "1962720",
    "end": "1968880"
  },
  {
    "text": "looking at Cloud watch we we just found that we were uh you know hitting the bandwidth restrictions on the SC1",
    "start": "1968880",
    "end": "1977360"
  },
  {
    "start": "1978000",
    "end": "2030000"
  },
  {
    "text": "uh then we mve to the uh ST1 um pretty much the same uh same uh numbers there",
    "start": "1978960",
    "end": "1984960"
  },
  {
    "text": "the only difference is is we started with four terabytes of ST1 storage that",
    "start": "1984960",
    "end": "1990600"
  },
  {
    "text": "we ran into again where we weren't quite getting enough bandwidth that we needed so moving to uh 8 terabytes actually",
    "start": "1990600",
    "end": "1996880"
  },
  {
    "text": "doubled that bandwidth for us and allowed us to um run our production workload on it yeah and this is actually",
    "start": "1996880",
    "end": "2003840"
  },
  {
    "text": "the configuration that we went with in our production system and right now we're running a few extra nodes out of",
    "start": "2003840",
    "end": "2009399"
  },
  {
    "text": "an abundance of caution as we hit Q4 since we are in advertising which Q4 is by far the busiest time of the year and",
    "start": "2009399",
    "end": "2016360"
  },
  {
    "text": "we are also adding a lot of extra workloads that didn't used to be on the cluster at this time so we figured we'd",
    "start": "2016360",
    "end": "2022960"
  },
  {
    "text": "play it safe during the most important time of the year for the business but this is basically our architecture",
    "start": "2022960",
    "end": "2029919"
  },
  {
    "text": "now so here is a diagram of our Pedra Duty Pages generated by the CDH cluster",
    "start": "2029919",
    "end": "2036600"
  },
  {
    "start": "2030000",
    "end": "2095000"
  },
  {
    "text": "from January up until the beginning of October so as you can see we've got some",
    "start": "2036600",
    "end": "2042279"
  },
  {
    "text": "spikes those were our critical incidents uh on the old cluster uh let's see the one in January was when our Master node",
    "start": "2042279",
    "end": "2048960"
  },
  {
    "text": "that ran all the services for dupe went down uh the other ones were pretty much related to bad data noes just kind of",
    "start": "2048960",
    "end": "2054358"
  },
  {
    "text": "slowing everything up and causing delays uh the other two big spikes and you can see uh we went into production",
    "start": "2054359",
    "end": "2060720"
  },
  {
    "text": "on August and kind of have a downward Trend after that uh some of the benefits of this have been more sleep I can go",
    "start": "2060720",
    "end": "2067280"
  },
  {
    "text": "home and spend time with my kids and my wife instead of spending time with my laptop watching to make sure that we're not falling behind uh and then Paul's",
    "start": "2067280",
    "end": "2075158"
  },
  {
    "text": "had some benefits as well yep and and just to show this graph too this this really makes our we in our organization",
    "start": "2075159",
    "end": "2080760"
  },
  {
    "text": "we really pay attention to pager Duty and Pages we have kind of a you know a misery index that we show for for our",
    "start": "2080760",
    "end": "2086839"
  },
  {
    "text": "engineering and so having having showing this you know is really great makes the team look good and and everybody sleeps",
    "start": "2086839",
    "end": "2094720"
  },
  {
    "text": "which is good so we also have some benefits with relation to capacity uh if you remember",
    "start": "2094720",
    "end": "2100720"
  },
  {
    "start": "2095000",
    "end": "2224000"
  },
  {
    "text": "from the problem slide that graph of a solid field of blue was our memory utilization on the old cluster if you're",
    "start": "2100720",
    "end": "2107400"
  },
  {
    "text": "looking at this one that top graph is now the memory utilization of our cluster or sorry the bottom graph is the",
    "start": "2107400",
    "end": "2112920"
  },
  {
    "text": "memory utilization the top one is the Coors which is now the more limiting factor of the two and you can see we still have a pretty decent amount of",
    "start": "2112920",
    "end": "2119119"
  },
  {
    "text": "extra capacity there and this is important because we don't just want to kind of keep everything the same we are",
    "start": "2119119",
    "end": "2125240"
  },
  {
    "text": "looking to improve our platform improve our slas or reliability some of the other tools that we're looking at um",
    "start": "2125240",
    "end": "2132400"
  },
  {
    "text": "we've had some issues with starting to overload red shift with running too much stuff on it concurrently so we're trying",
    "start": "2132400",
    "end": "2138480"
  },
  {
    "text": "to offload some of that onto our cluster and we have the bandwidth to do it now uh we're trying to move to a more stream",
    "start": "2138480",
    "end": "2144520"
  },
  {
    "text": "inest model uh flfa which we have there is similar to what they were talking about with Kafka with Flume in front of",
    "start": "2144520",
    "end": "2150560"
  },
  {
    "text": "it which is a tool for ingesting into Hadoop and we're starting to look at some other database Technologies like",
    "start": "2150560",
    "end": "2155839"
  },
  {
    "text": "hbase and uh processing for Frameworks like spark that tend to be more uh memory intensive than your normal map",
    "start": "2155839",
    "end": "2163200"
  },
  {
    "text": "reduce which tends to require 2 GB of memory per core of CPU we're also more resilient to our log",
    "start": "2163200",
    "end": "2169400"
  },
  {
    "text": "volume increases now because both from a capacity perspective and we can expand our storage without having to spin up a",
    "start": "2169400",
    "end": "2175920"
  },
  {
    "text": "bunch of other boxes so like for example if we decide you know holding 20 hour or",
    "start": "2175920",
    "end": "2181920"
  },
  {
    "text": "not 20 hours two days of request data is not quite enough we can just add some more drives to the boxes and store like",
    "start": "2181920",
    "end": "2187680"
  },
  {
    "text": "seven days without needing a bunch of new processing power that we would never use because while we would have the data",
    "start": "2187680",
    "end": "2194040"
  },
  {
    "text": "there we're not normally looking that far back at any given time the other thing is is you know with",
    "start": "2194040",
    "end": "2200599"
  },
  {
    "text": "all this added capacity they they were actually a um 25% data increase uh in",
    "start": "2200599",
    "end": "2207280"
  },
  {
    "text": "one day went completely unnoticed you know some people were looking at some graphs and they're like this can't be right you know and we we got it and",
    "start": "2207280",
    "end": "2214319"
  },
  {
    "text": "nothing paged in the old cluster that would have been a downtime event like it would have been a pageable event for us",
    "start": "2214319",
    "end": "2219440"
  },
  {
    "text": "that would have been another nice little Spike on the pager graph on the previous",
    "start": "2219440",
    "end": "2224720"
  },
  {
    "start": "2224000",
    "end": "2378000"
  },
  {
    "text": "slide all right so to come up with some of the kind of financial benefits as you can see while we initially started out",
    "start": "2224720",
    "end": "2230960"
  },
  {
    "text": "controlling for cost once we moved to the more expensive storage it was a little more expensive but we really like the performance we were getting out of",
    "start": "2230960",
    "end": "2237040"
  },
  {
    "text": "it so we were able to roll with it but to try and kind of normalize that against the old cluster just did some",
    "start": "2237040",
    "end": "2244280"
  },
  {
    "text": "uh some math with it so so the old cluster was always 100% utilized which",
    "start": "2244280",
    "end": "2249960"
  },
  {
    "text": "means that our cost for processing that hour of data was the $21,500 the cluster",
    "start": "2249960",
    "end": "2255040"
  },
  {
    "text": "cost per month since the new cluster is 60% utilized if you Norm if you",
    "start": "2255040",
    "end": "2262119"
  },
  {
    "text": "normalize against that we're using $15,300 worth of capacity to Pro process",
    "start": "2262119",
    "end": "2268680"
  },
  {
    "text": "slightly more data than what we were processing at that point so the other uh fun one there is if since all of our",
    "start": "2268680",
    "end": "2275200"
  },
  {
    "text": "stuff is measured by requests whether it's ad request bid requests um the",
    "start": "2275200",
    "end": "2280240"
  },
  {
    "text": "events from our video player saying like we've seen this many clicks we've seen this many views that kind of stuff uh if",
    "start": "2280240",
    "end": "2287079"
  },
  {
    "text": "you look at the cost to process a th requests on both clusters on the old cluster it would have cost us like 9.3",
    "start": "2287079",
    "end": "2296040"
  },
  {
    "text": "cents on the new cluster instead it is costing us about 7.3 cents so that's",
    "start": "2296040",
    "end": "2302839"
  },
  {
    "text": "give or take like a 20 25% decrease in the cost for that processing",
    "start": "2302839",
    "end": "2308960"
  },
  {
    "text": "and this this was all due to you know these these ST1 and SC1 EBS volumes coming out because without them we would",
    "start": "2308960",
    "end": "2315760"
  },
  {
    "text": "have been you know probably spending a lot more money on D2 instances or gp2 GP",
    "start": "2315760",
    "end": "2322200"
  },
  {
    "text": "SSD back storage uh the other thing that's been nice with this cluster is since it is somewhat tolerant to our es",
    "start": "2322200",
    "end": "2328720"
  },
  {
    "text": "and flows of data it is a little easier to budget for if you are the type who is doing that sort of thing because you're",
    "start": "2328720",
    "end": "2335640"
  },
  {
    "text": "not spinning up X number number of nodes every couple months when you have a data issue where X can be anywhere from like",
    "start": "2335640",
    "end": "2340920"
  },
  {
    "text": "3 to 5 which definitely spikes your budget a little bit more on that",
    "start": "2340920",
    "end": "2346560"
  },
  {
    "text": "end yeah and it's also easier for us to scale out too um you know the the costs",
    "start": "2346560",
    "end": "2351599"
  },
  {
    "text": "of the the m410 xels and and and the the amount of space and everything we can",
    "start": "2351599",
    "end": "2357680"
  },
  {
    "text": "scale out you know once we need that need more resources to do that so it's uh makes everybody happier and like you",
    "start": "2357680",
    "end": "2364680"
  },
  {
    "text": "said easier to budget for yep a year uh these numbers the ones on the",
    "start": "2364680",
    "end": "2371760"
  },
  {
    "text": "left are per month the one on the right is the cost to process a thousand requests regardless of",
    "start": "2371760",
    "end": "2378680"
  },
  {
    "start": "2378000",
    "end": "2393000"
  },
  {
    "text": "time all right so that concludes our presentation if you have any questions",
    "start": "2378680",
    "end": "2383720"
  },
  {
    "text": "for user zendesk we're going to be down at the front after this and remember to complete your",
    "start": "2383720",
    "end": "2390040"
  },
  {
    "text": "evaluations thank you",
    "start": "2390040",
    "end": "2394160"
  }
]