[
  {
    "start": "0",
    "end": "107000"
  },
  {
    "text": "my name is Steven moon I'm a senior Solutions Architect with the AWS specialist team here at Amazon Web",
    "start": "60",
    "end": "6000"
  },
  {
    "text": "Services today I'm going to be talking to you about some of the data base offerings that are announced that this year's reinvent conference so they come",
    "start": "6000",
    "end": "13080"
  },
  {
    "text": "in two different things that I'm going to talk about new services that were announced and also capability and",
    "start": "13080",
    "end": "18420"
  },
  {
    "text": "feature enhancements to some of our existing services it's going to include talking things about aurora global",
    "start": "18420",
    "end": "24210"
  },
  {
    "text": "database or RDS on vmware offering DynamoDB transactional api's DynamoDB",
    "start": "24210",
    "end": "30810"
  },
  {
    "text": "on-demand are a new database offering timestream and our new database offering",
    "start": "30810",
    "end": "36090"
  },
  {
    "text": "the quantum ledger database so let me start out with amazon aurora so one of",
    "start": "36090",
    "end": "41309"
  },
  {
    "text": "the things that we're offering out with Amazon Aurore is global database traditionally database has been via some",
    "start": "41309",
    "end": "47640"
  },
  {
    "text": "some type of log replication between database instances what a raw global database is going to be able to do is",
    "start": "47640",
    "end": "53489"
  },
  {
    "text": "replicate between AWS regions so globally providing storage layer",
    "start": "53489",
    "end": "59250"
  },
  {
    "text": "replication so it's going to be the use case is going to be for disaster recovery print of disaster",
    "start": "59250",
    "end": "64860"
  },
  {
    "text": "recoverability predictable low latency Geographic reads scaling so again between regions not just between a Z's",
    "start": "64860",
    "end": "72650"
  },
  {
    "text": "as part of the current general availability limitations it's only going to be a vote on a roar at the moment my",
    "start": "72650",
    "end": "79080"
  },
  {
    "text": "sequel five point six and it's going to only support one readable remote region",
    "start": "79080",
    "end": "84240"
  },
  {
    "text": "only and it's going to be included within several regions future releases",
    "start": "84240",
    "end": "89700"
  },
  {
    "text": "is going to include multiple remote replicas to include additional regions in the first quarter of 2019 how it",
    "start": "89700",
    "end": "98460"
  },
  {
    "text": "works is that an asynchronous redo log stream and dedicated replication",
    "start": "98460",
    "end": "103829"
  },
  {
    "text": "infrastructure for outbound and inbound side and so if you look at this slide you'll see that it's actually we keep",
    "start": "103829",
    "end": "109290"
  },
  {
    "start": "107000",
    "end": "107000"
  },
  {
    "text": "replication fleets on both sides and so if you're familiar with our master endpoints and reader endpoints for",
    "start": "109290",
    "end": "114720"
  },
  {
    "text": "Aurora it's gonna look the same way it's going to provide a high high write throughput of 200,000 writes per second",
    "start": "114720",
    "end": "122130"
  },
  {
    "text": "so it's going to have a negligible form performance impact across availability zones it's going to have a low replicas",
    "start": "122130",
    "end": "127740"
  },
  {
    "text": "lag of less than one second cross-country replicas under heavy loads so I think you've got a",
    "start": "127740",
    "end": "133350"
  },
  {
    "text": "Zone in US east or us west or today with a new gun and gov cloud Easter gov",
    "start": "133350",
    "end": "138960"
  },
  {
    "text": "Claude West in a fast recovery of less than one minute to accept the full load",
    "start": "138960",
    "end": "144470"
  },
  {
    "text": "read/write load in the event of a region failure which of them itself is fairly highly unlikely a new feature for Aurora",
    "start": "144470",
    "end": "151860"
  },
  {
    "text": "Postgres is the database activitystreams it is in private preview and what that's",
    "start": "151860",
    "end": "158370"
  },
  {
    "text": "going to allow its could do is stream audit and database activity to common",
    "start": "158370",
    "end": "163560"
  },
  {
    "text": "tools that are most customers tend to use such as McAfee data center and purva and IBM InfoSphere it streams database",
    "start": "163560",
    "end": "172380"
  },
  {
    "text": "activity to Kinesis data streams so that any Kinesis client library or consumer",
    "start": "172380",
    "end": "178800"
  },
  {
    "text": "library can actually take that stream and perform some type of security analytics so I think in the context of",
    "start": "178800",
    "end": "184800"
  },
  {
    "text": "security and incident management for some of our government customers you",
    "start": "184800",
    "end": "190830"
  },
  {
    "text": "know for some of the requirements for the Department of Defense for instance for their security mining logging we",
    "start": "190830",
    "end": "196770"
  },
  {
    "text": "also have a new feature called cluster cache management and what that's going to do is it's going to keep a read",
    "start": "196770",
    "end": "203220"
  },
  {
    "text": "replicas cached and sync with that of the primary node so typically when a node fails the cache has got to heat",
    "start": "203220",
    "end": "209520"
  },
  {
    "text": "backup or REO rehydrate itself on the instance that is brought up to take over",
    "start": "209520",
    "end": "215010"
  },
  {
    "text": "for the fail node in this case the caches are kept in sync so that there is no rehydrating so it provides high",
    "start": "215010",
    "end": "220980"
  },
  {
    "text": "performance immediately after failover instead of just instead of having to wait for the cache to repopulate itself",
    "start": "220980",
    "end": "226890"
  },
  {
    "text": "again as in all the storage and Aurora is decoupled and so that cache is kept",
    "start": "226890",
    "end": "232050"
  },
  {
    "text": "in sync at the database layer we have outbound replication u2 Postgres Aurora as well so it's gonna add support for",
    "start": "232050",
    "end": "238500"
  },
  {
    "start": "235000",
    "end": "235000"
  },
  {
    "text": "Postgres replication slots and in Aurora Postgres sequel so it's going to allow customers to use standard Postgres",
    "start": "238500",
    "end": "245070"
  },
  {
    "text": "logical decoders so you think the difference between physical and logical database replication logical replication",
    "start": "245070",
    "end": "251280"
  },
  {
    "text": "generally means applying sequel statements that are generated on the primary and stream those changes to",
    "start": "251280",
    "end": "257989"
  },
  {
    "text": "targets of wall to json test decoder you go to raw and also customers can use",
    "start": "257989",
    "end": "264330"
  },
  {
    "text": "replication tools now such as dmsr day bass migration service to replicate changes to other database engines and so",
    "start": "264330",
    "end": "271920"
  },
  {
    "text": "if you've gotten a raw Postgres sequel database now you can use DMS to not only",
    "start": "271920",
    "end": "277190"
  },
  {
    "text": "populate target databases but also to use change data capture as a method to",
    "start": "277190",
    "end": "282240"
  },
  {
    "text": "propagate those changes on an ongoing basis Amazon RDS on VMware is a new",
    "start": "282240",
    "end": "288990"
  },
  {
    "text": "offering that is building on our current relationship with VMware for running VMware in the cloud or running VMware",
    "start": "288990",
    "end": "295890"
  },
  {
    "text": "and some of our other service offerings which you'll probably hear about it's not like Amazon RDS but it is Amazon RDS",
    "start": "295890",
    "end": "301740"
  },
  {
    "text": "in your data center so again everything in Amazon is customer centric so this is driven by a lot of customers asking us",
    "start": "301740",
    "end": "309030"
  },
  {
    "text": "to be able to bring the functionality of RDS into their own data center you know",
    "start": "309030",
    "end": "314220"
  },
  {
    "text": "for issues such as data sovereignty or application latency in the sent in the",
    "start": "314220",
    "end": "319590"
  },
  {
    "text": "cases where they want to still run their application on-premise but they don't want to manage a database in the",
    "start": "319590",
    "end": "325560"
  },
  {
    "text": "traditional way of having a managed infrastructure provision and all that stuff so all of the provision patching",
    "start": "325560",
    "end": "332490"
  },
  {
    "text": "monitoring scale up scale down and patching which is a big plus for a lot",
    "start": "332490",
    "end": "337890"
  },
  {
    "text": "of customers is taken care of in the same manner that RDS is in the cloud is RDS on VMware and I'll talk about",
    "start": "337890",
    "end": "345840"
  },
  {
    "text": "snapshotting restore point in time and availability management as well it's",
    "start": "345840",
    "end": "350940"
  },
  {
    "text": "currently in preview so how does connectivity to RDS work with RDS on",
    "start": "350940",
    "end": "355980"
  },
  {
    "start": "353000",
    "end": "353000"
  },
  {
    "text": "VMware so you have you AWS region today and your RDS console is the same and so",
    "start": "355980",
    "end": "362340"
  },
  {
    "text": "all of your functionality and things that you'll be doing in your RDS console is going to work the same way with RDS",
    "start": "362340",
    "end": "368160"
  },
  {
    "text": "on VMware it does with RDS and AWS the vmware vsphere cluster in your own",
    "start": "368160",
    "end": "375090"
  },
  {
    "text": "data center is actually what creating a new concept called a custom availability",
    "start": "375090",
    "end": "380550"
  },
  {
    "text": "zone and so a customer would go in and define as part of their RDS set up a custom availability zone and once",
    "start": "380550",
    "end": "387360"
  },
  {
    "text": "they've defined that a custom availability zone they get a download to actually deploy the configuration the",
    "start": "387360",
    "end": "394500"
  },
  {
    "text": "virtual machine on their on-premise and their VM or cluster and then they would",
    "start": "394500",
    "end": "399690"
  },
  {
    "text": "go back in their aw best console and configure things such as IP address VPN tunneling different",
    "start": "399690",
    "end": "405540"
  },
  {
    "text": "things that are part of that configuration setup and so that connection is over a VPN connection it",
    "start": "405540",
    "end": "410610"
  },
  {
    "text": "could be over direct connect to a DMZ that is set up for the customer by a to",
    "start": "410610",
    "end": "416790"
  },
  {
    "text": "guess on the Hat on their behalf the customer download the connector into their own VMware environment as I",
    "start": "416790",
    "end": "422910"
  },
  {
    "text": "mentioned earlier in that connector is going to handle control traffic and different things that are going to be",
    "start": "422910",
    "end": "428400"
  },
  {
    "text": "used by RDS on vmware from the AWS management console so as i mentioned",
    "start": "428400",
    "end": "435120"
  },
  {
    "text": "before all that traffic is going to be pushed down through the VPN connection for management plane activity not data",
    "start": "435120",
    "end": "441210"
  },
  {
    "text": "database activity and then anything the metrics that are going to be collected",
    "start": "441210",
    "end": "446220"
  },
  {
    "text": "as part of that are going to be pushed back up to AWS so anything that you want to push to s3 or cloud watch metrics or",
    "start": "446220",
    "end": "452639"
  },
  {
    "text": "anything like that are still going to be pushed back to AWS so that you can use your AWS tooling to do all of the same",
    "start": "452639",
    "end": "458789"
  },
  {
    "text": "features and functionality that you were doing previously so provisioning and management and so in a customer's data center image and you",
    "start": "458789",
    "end": "465210"
  },
  {
    "start": "462000",
    "end": "462000"
  },
  {
    "text": "download the VMware connector install it on your VM or cluster in your own cluster control network and what that",
    "start": "465210",
    "end": "471180"
  },
  {
    "text": "cluster control network is is a control plane similar to the private control",
    "start": "471180",
    "end": "476880"
  },
  {
    "text": "plane that she used in AWS for your VMware cluster and so an RDS instances dual homes meaning it exists in its own",
    "start": "476880",
    "end": "483990"
  },
  {
    "text": "private virtual private cloud for management and other activities that are done as part of our shared",
    "start": "483990",
    "end": "491340"
  },
  {
    "text": "responsibility model and then there's the application network which would be akin to a customer's DPC in which that",
    "start": "491340",
    "end": "497400"
  },
  {
    "text": "RDS instance exists from a connectivity standpoint and so within that control",
    "start": "497400",
    "end": "502440"
  },
  {
    "text": "cluster control network is going to be the RDS control virtual machines and the VMware control VMs that are going to act",
    "start": "502440",
    "end": "509430"
  },
  {
    "text": "on behalf of the AWS management console to manage your RDS solution on VMware",
    "start": "509430",
    "end": "516630"
  },
  {
    "text": "and on your in your arm premise data center so again just like you would any other an RDS deployment in AWS you know",
    "start": "516630",
    "end": "525420"
  },
  {
    "text": "your my sequel or Postgres or sequel server databases are going to be doing",
    "start": "525420",
    "end": "530520"
  },
  {
    "text": "in the private management network that's owned AWS and your application Network in this",
    "start": "530520",
    "end": "538070"
  },
  {
    "text": "case a VMware vCenter cluster and then your applications connect just like they would in the AWS environment so backup",
    "start": "538070",
    "end": "545300"
  },
  {
    "start": "545000",
    "end": "545000"
  },
  {
    "text": "restore and read replicas read replicas are created just like they are in AWS",
    "start": "545300",
    "end": "553130"
  },
  {
    "text": "and so a read replicas from a master instance you could create multiple read replicas just like you can an AWS you",
    "start": "553130",
    "end": "560180"
  },
  {
    "text": "can also restore read replicas just like you can in RDS and AWS one of the things",
    "start": "560180",
    "end": "566030"
  },
  {
    "text": "to remember about a read replicas is it is an actual restorer of the data to its own database and so that read replicas",
    "start": "566030",
    "end": "573770"
  },
  {
    "text": "can be built as it was at the point in time from which it was created or it can",
    "start": "573770",
    "end": "579980"
  },
  {
    "text": "also be built from a point in time snapshot so let's say you have multiple read replicas and you only want to see",
    "start": "579980",
    "end": "586580"
  },
  {
    "text": "things from you want to see something from seven hours or seven days three",
    "start": "586580",
    "end": "592250"
  },
  {
    "text": "hours and 47 minutes ago you could restore one of those read replicas to that point in time to do whatever it is",
    "start": "592250",
    "end": "598880"
  },
  {
    "text": "if it was for audibility or some other reason that required as part of your operational mission some of our database",
    "start": "598880",
    "end": "605000"
  },
  {
    "text": "instances Postgres and my sequel for example also support read replicas for reader endpoints which is also supported",
    "start": "605000",
    "end": "612410"
  },
  {
    "text": "in RDS on vmware is now so again it's created the same way that you would through the RDS management console and",
    "start": "612410",
    "end": "619730"
  },
  {
    "text": "then the control traffic is propagated through that can the RDS connector RDS",
    "start": "619730",
    "end": "626060"
  },
  {
    "text": "on VMware connector and then the controls are issued the commands are issued and executed in your on-premise",
    "start": "626060",
    "end": "633560"
  },
  {
    "text": "environment from VMware so one of the things that's coming is really interesting is hybrid management let's say that you have a VMware cluster and",
    "start": "633560",
    "end": "640820"
  },
  {
    "start": "635000",
    "end": "635000"
  },
  {
    "text": "your master is there on-premise and you create a couple of snapshots but let's say you wanted to create a bunch of",
    "start": "640820",
    "end": "646370"
  },
  {
    "text": "snapshots but you didn't want to store them all on you know maybe more expensive hardware or storage on-premise",
    "start": "646370",
    "end": "652880"
  },
  {
    "text": "some of those snapshots could also be stored in AWS you could store hundreds in AWS really there's really no limit to",
    "start": "652880",
    "end": "658970"
  },
  {
    "text": "the amount that you could store depending on you know what your operational permission requirements are you can also deploy read replicas across",
    "start": "658970",
    "end": "666870"
  },
  {
    "text": "well we would call AWS regions or custom regions in this point for your on-premise data centers and so you'll be",
    "start": "666870",
    "end": "673440"
  },
  {
    "text": "able to deploy a read replica in AWS also a read replica let's say in another",
    "start": "673440",
    "end": "679320"
  },
  {
    "text": "data center somewhere that was geographically separated as long as that connectivity is available and then the",
    "start": "679320",
    "end": "684960"
  },
  {
    "text": "latency will between data centers between your on-premise data centers will determine how well that lag between",
    "start": "684960",
    "end": "690839"
  },
  {
    "text": "those data centers between those databases can keep up with itself and so database engine support happening now",
    "start": "690839",
    "end": "697410"
  },
  {
    "start": "695000",
    "end": "695000"
  },
  {
    "text": "we've got support for Postgres my sequel and sequel server 2017 which is coming soon for general availability we're",
    "start": "697410",
    "end": "705420"
  },
  {
    "text": "gonna have support for Postgres sequel Mariya DB my sequel and sequel server",
    "start": "705420",
    "end": "710750"
  },
  {
    "text": "soon thereafter oracle so looking forward you know I mentioned hybrid",
    "start": "710750",
    "end": "715890"
  },
  {
    "text": "snapshots data center - data center data center at AWS and restore a new RDS database in any region talked a little",
    "start": "715890",
    "end": "723120"
  },
  {
    "text": "bit about hybrid read replicas so data center data center data center - AWS you know disaster cover ability geo proximal",
    "start": "723120",
    "end": "730350"
  },
  {
    "text": "read so if you want your customer base or your user base to log into the",
    "start": "730350",
    "end": "736380"
  },
  {
    "text": "closest region so let's say you know gov cloud Eastern guffawed West that's also possible and then you know migration",
    "start": "736380",
    "end": "742770"
  },
  {
    "text": "one-click promotion Oracle support and cross customer cross cluster high",
    "start": "742770",
    "end": "748260"
  },
  {
    "text": "availability so next is really to look at your applications and say of all these things you're doing what's the",
    "start": "748260",
    "end": "753630"
  },
  {
    "text": "most beneficial to you so the next thing I'm going to discuss is a couple of new features in Amazon DynamoDB",
    "start": "753630",
    "end": "758850"
  },
  {
    "text": "so one of the interesting things and for those of you I know Modi be is are no",
    "start": "758850",
    "end": "764310"
  },
  {
    "start": "759000",
    "end": "759000"
  },
  {
    "text": "sequel solution service serverless solution for transactional applications",
    "start": "764310",
    "end": "771050"
  },
  {
    "text": "and so one of the things that's always been problematic or challenging with",
    "start": "771050",
    "end": "777440"
  },
  {
    "text": "traditional no sequel environments is the fact that all of the any kind of acid transaction so if you think about",
    "start": "777440",
    "end": "783720"
  },
  {
    "text": "traditional RDM s's acid transactions atomicity consistency isolation and",
    "start": "783720",
    "end": "789450"
  },
  {
    "text": "durability had to be managed on the client side meaning client code had to be written and every application had to",
    "start": "789450",
    "end": "796040"
  },
  {
    "text": "account for the fact that their particular LTP system perhaps may need acid compliance",
    "start": "796040",
    "end": "802250"
  },
  {
    "text": "what DynamoDB transactional api's does is it takes that and puts that on the server side and so it just becomes",
    "start": "802250",
    "end": "809300"
  },
  {
    "text": "another API call for the custom application to enable an acid compliant",
    "start": "809300",
    "end": "814730"
  },
  {
    "text": "transaction so a little bit of you know use cases for those financial applications so banking applications or",
    "start": "814730",
    "end": "820490"
  },
  {
    "start": "816000",
    "end": "816000"
  },
  {
    "text": "any kind of financial you know ledger applications where that transaction has to be consistent durable and all the",
    "start": "820490",
    "end": "827540"
  },
  {
    "text": "things that are applied through acid compliance process orders coordinate in multiple you know player game scenarios",
    "start": "827540",
    "end": "833389"
  },
  {
    "text": "for more kind of the gaming industry metadata about distributed systems so there's a lot of use cases and the",
    "start": "833389",
    "end": "839329"
  },
  {
    "text": "reasons why our DBMS is have you know persisted for a long time is the fact that they implement acid compliance and",
    "start": "839329",
    "end": "845149"
  },
  {
    "text": "it's very important for transactional systems for a lot of reasons so some of the key features I mentioned that provides acid for DynamoDB for multiple",
    "start": "845149",
    "end": "852500"
  },
  {
    "start": "849000",
    "end": "849000"
  },
  {
    "text": "transactions you can perform transactions both within and across multiple dinamo TV tables and that kind",
    "start": "852500",
    "end": "858529"
  },
  {
    "text": "of gets into a little bit about what's the best design for dinamo typically you know one dynamodb table should support",
    "start": "858529",
    "end": "865040"
  },
  {
    "text": "your application however if for some reason it does not you can span that",
    "start": "865040",
    "end": "870769"
  },
  {
    "text": "transactional api across multiple DynamoDB tables it's a native",
    "start": "870769",
    "end": "875959"
  },
  {
    "text": "server-side solution that provides better performance and cost than client-side libraries that's a key point",
    "start": "875959",
    "end": "881750"
  },
  {
    "text": "because having it server-side means that it decouples that requirement from your developers so they can focus on",
    "start": "881750",
    "end": "887209"
  },
  {
    "text": "developing applications not on developing logic for atomists transactional atomicity so some of that",
    "start": "887209",
    "end": "894740"
  },
  {
    "text": "I mentioned that some of the development simplifies the developer experience of making coordinated you know all-or-nothing changes to multiple items",
    "start": "894740",
    "end": "901730"
  },
  {
    "text": "it extends the scale it performance the enterprise of dynamodb to a broader set",
    "start": "901730",
    "end": "907879"
  },
  {
    "text": "of workloads so it really extends into workloads that I mentioned before that really require transactional consistency",
    "start": "907879",
    "end": "914110"
  },
  {
    "text": "across a broad user base because you don't want users updating records in an",
    "start": "914110",
    "end": "920029"
  },
  {
    "text": "on way that can disrupt other user transactions and that's really what this is the problem this is solving so some",
    "start": "920029",
    "end": "925879"
  },
  {
    "text": "of the traditional challenges of using in transactions with a non relational database so a no sequel database you",
    "start": "925879",
    "end": "931189"
  },
  {
    "start": "926000",
    "end": "926000"
  },
  {
    "text": "know creating a maintaining your own transaction client library you've got the right code to build all that logic in and to modify our",
    "start": "931189",
    "end": "937250"
  },
  {
    "text": "application to logic to work around a lack of native transaction support so",
    "start": "937250",
    "end": "942350"
  },
  {
    "text": "why developers need transactions you know I mentioned simultaneous writes or updates to multiple items DynamoDB has",
    "start": "942350",
    "end": "948470"
  },
  {
    "start": "943000",
    "end": "943000"
  },
  {
    "text": "always had consistency with a single right so this is focused on not only multiple writes but multiple kits and",
    "start": "948470",
    "end": "954769"
  },
  {
    "text": "I'll talk about that in a minute check multiple conditions while executing an action so in any kind of acid compliant",
    "start": "954769",
    "end": "961880"
  },
  {
    "text": "database there's going to be conditions that have to be met in order for that transaction to succeed if it doesn't",
    "start": "961880",
    "end": "967490"
  },
  {
    "text": "succeed it's going to send a notice back to the user to say that this condition was not met and therefore the",
    "start": "967490",
    "end": "973100"
  },
  {
    "text": "transaction could not continue and ensure data cancer data is consistent across multiple tables again this is the",
    "start": "973100",
    "end": "980480"
  },
  {
    "text": "design feature do you need one table or multiple tables I would say that a lot of DynamoDB applications only require",
    "start": "980480",
    "end": "986209"
  },
  {
    "text": "one table but if that requirement if your application requires multiple tables for whatever reason then it",
    "start": "986209",
    "end": "991579"
  },
  {
    "text": "supports multiple table transactions so it is a single API call so it is and",
    "start": "991579",
    "end": "997640"
  },
  {
    "start": "994000",
    "end": "994000"
  },
  {
    "text": "it's an all-or-nothing action with within and across tables than a single API call and I'll talk about each one of",
    "start": "997640",
    "end": "1003880"
  },
  {
    "text": "those API calls next so transaction write items and transaction get items on the new to API calls so traditionally it",
    "start": "1003880",
    "end": "1010750"
  },
  {
    "start": "1006000",
    "end": "1006000"
  },
  {
    "text": "was a get or a get item or a put item for a single item right so an item being",
    "start": "1010750",
    "end": "1016570"
  },
  {
    "text": "synonymous to a certain extent to a row in an our DBMS so a customer record",
    "start": "1016570",
    "end": "1021790"
  },
  {
    "text": "might be an item or a transaction might be an item so these are new API calls",
    "start": "1021790",
    "end": "1027400"
  },
  {
    "text": "that allow you to send multiple items right now it's up to 10 items in a",
    "start": "1027400",
    "end": "1033308"
  },
  {
    "text": "single call and so you can write 10 items in a trans actually consistent",
    "start": "1033309",
    "end": "1038319"
  },
  {
    "text": "managed managed manner across to a single dynamodb table or across multiple",
    "start": "1038319",
    "end": "1043350"
  },
  {
    "text": "DynamoDB tables the trans actually get items is more around seeing multiple",
    "start": "1043350",
    "end": "1048850"
  },
  {
    "text": "items in a consistent safe and so what I mean by that is that traditionally if I had to get 10 items let's say and then",
    "start": "1048850",
    "end": "1057520"
  },
  {
    "text": "look at those items I my I had to get each item individually and then as I was",
    "start": "1057520",
    "end": "1063520"
  },
  {
    "text": "getting those items an update to an item might have happened in an inconsistent manner with other",
    "start": "1063520",
    "end": "1069100"
  },
  {
    "text": "items that I was getting and so I'd get that we get that view in an inconsistent state a transaction getitem",
    "start": "1069100",
    "end": "1075010"
  },
  {
    "text": "API allows me to get those ten items in a single call so that it's a consistent",
    "start": "1075010",
    "end": "1081010"
  },
  {
    "text": "view a consistent state across those so I mentioned atomicity consistent",
    "start": "1081010",
    "end": "1086289"
  },
  {
    "text": "isolation durability the other things like does is it's in Dippet --nt so it's tokenized and so what that means is if I",
    "start": "1086289",
    "end": "1094030"
  },
  {
    "text": "if I execute something subsequent executions are going to happen the same way that doesn't necessarily mean that",
    "start": "1094030",
    "end": "1099549"
  },
  {
    "text": "I'm going to retrieve the same information but I'll get the same result and that's very important there's synchronous so which is",
    "start": "1099549",
    "end": "1106480"
  },
  {
    "text": "important to point out it's it's not an asynchronous call it is synchronous and so it requires to write or read units in",
    "start": "1106480",
    "end": "1115059"
  },
  {
    "text": "order to execute a transaction there are conditional checks and so it's going to look for different things that's part of that transaction to see whether or not",
    "start": "1115059",
    "end": "1121840"
  },
  {
    "text": "that it's it's a valid transaction it's going to be committed we have you know recently 99.9% SLA with DynamoDB",
    "start": "1121840",
    "end": "1130289"
  },
  {
    "text": "performance is going to be predictable because it's going to scale it is dynamo so it's going to scale like any other implementation would without the ugly so",
    "start": "1130289",
    "end": "1137230"
  },
  {
    "text": "you don't get deadlock so it uses an optimistic locking mechanism so if you're familiar with optimistic versus",
    "start": "1137230",
    "end": "1142960"
  },
  {
    "text": "pessimistic locking paestum ik pessimistic locking assumes that somebody else is going to do something",
    "start": "1142960",
    "end": "1149860"
  },
  {
    "text": "with that data and therefore locks that particular data item while somebody's transacting against it that's not good",
    "start": "1149860",
    "end": "1156280"
  },
  {
    "text": "at a high transactional system when you don't want it you don't want to put a lot of locks out there because that can cause deadlock issues optimistic locking",
    "start": "1156280",
    "end": "1162429"
  },
  {
    "text": "assumes that you're going to be able to perform it really decouples the performance of the transaction from the",
    "start": "1162429",
    "end": "1169510"
  },
  {
    "text": "actual committal of the transaction itself so that's where the conditions come into play is it is optimistic locking which means it doesn't lock the",
    "start": "1169510",
    "end": "1176770"
  },
  {
    "text": "item it when a transaction API is issued against that to commit the transaction that it actually allows it to proceed or",
    "start": "1176770",
    "end": "1183780"
  },
  {
    "text": "throws back an error code and say no it didn't proceed because or did not commit because it did not meet that condition",
    "start": "1183780",
    "end": "1189490"
  },
  {
    "text": "no long-running transactions unmanaged concurrency and then you know the DBA pain and suffering let's manage by",
    "start": "1189490",
    "end": "1196360"
  },
  {
    "text": "dynamodb itself so it's not only the fact that your your developers don't have to worry about it",
    "start": "1196360",
    "end": "1201480"
  },
  {
    "text": "anymore writing client-side libraries or DBAs don't have to either so DynamoDB on-demand is another feature",
    "start": "1201480",
    "end": "1207629"
  },
  {
    "start": "1205000",
    "end": "1205000"
  },
  {
    "text": "to further expand the portfolio of our service offerings and so dynamo has",
    "start": "1207629",
    "end": "1213570"
  },
  {
    "text": "always been service in the sense that the customer is not managing servers but they what they are managing its capacity",
    "start": "1213570",
    "end": "1218940"
  },
  {
    "text": "and typically they've from you know they provision write write capacity or units and read capacity units to scale their",
    "start": "1218940",
    "end": "1224669"
  },
  {
    "text": "workload and so now you can think of dynamo DB as having two different choices for customers you could have a",
    "start": "1224669",
    "end": "1229679"
  },
  {
    "text": "computer on demand or you can have a provisioned model where you provision a certain amount of write and read",
    "start": "1229679",
    "end": "1235740"
  },
  {
    "text": "capacity units for your particular application it's not a right or wrong answer it's whatever fits your need as",
    "start": "1235740",
    "end": "1242519"
  },
  {
    "text": "the customer and so it's nice about DynamoDB though is you don't have to think ahead and try to figure out what",
    "start": "1242519",
    "end": "1250440"
  },
  {
    "text": "is your provision capacity going to have to be for that particular application you can start at zero and there's really",
    "start": "1250440",
    "end": "1255809"
  },
  {
    "text": "no limit because you think about how dynamodb scales so there's no capacity planning provisioning or reservations",
    "start": "1255809",
    "end": "1261720"
  },
  {
    "text": "simply make the API calls you pay only for the reason rights and what you perform so one of the things that people",
    "start": "1261720",
    "end": "1268649"
  },
  {
    "text": "would ask is why would I just not do this instead of provisioning capacity units and it is a little bit more",
    "start": "1268649",
    "end": "1274470"
  },
  {
    "text": "expensive and a peer on demand model versus a provision model and generally speaking a lot of customers use a hybrid",
    "start": "1274470",
    "end": "1280200"
  },
  {
    "text": "of both and so if you think about how you would provision ec2 instances maybe with reserved instances and a",
    "start": "1280200",
    "end": "1285899"
  },
  {
    "text": "combination of reserved and on-demand instances kind of the same principles applied to this as a lot of applications will function in a hybrid type manner",
    "start": "1285899",
    "end": "1292379"
  },
  {
    "text": "and so it eliminates the trade-offs of over-under provisioning I mean an instantly accommodates your workloads",
    "start": "1292379",
    "end": "1297629"
  },
  {
    "text": "traffic ramps up and down you know this is just explaining to you know now the difference between provision is now an",
    "start": "1297629",
    "end": "1302879"
  },
  {
    "text": "auto scaling instead of minimum where that minimum and maximum is loan longer applicable and so this is one of the",
    "start": "1302879",
    "end": "1309210"
  },
  {
    "text": "features that a lot of customers wanted and it really is you know one of the cooler service offerings in my opinion",
    "start": "1309210",
    "end": "1314580"
  },
  {
    "text": "of DynamoDB that was announced this year so amazon time stream so this is the new",
    "start": "1314580",
    "end": "1320119"
  },
  {
    "text": "service that was announced this year and it's a managed time series database so",
    "start": "1320119",
    "end": "1325200"
  },
  {
    "text": "the question is what is a time series database why not why can't I store time in a timestamp field in my database or",
    "start": "1325200",
    "end": "1331649"
  },
  {
    "text": "assist date in my database you can but there's reasons not to so what is a time series right what",
    "start": "1331649",
    "end": "1337539"
  },
  {
    "start": "1336000",
    "end": "1336000"
  },
  {
    "text": "is time series data it's a sequence of data points recorded over an interval so anything that has to do with time right",
    "start": "1337539",
    "end": "1343630"
  },
  {
    "text": "recorded over intervals so you think of stock quotes right they're recorded probably at the second or microsecond",
    "start": "1343630",
    "end": "1350200"
  },
  {
    "text": "level temperature you know precipitation humidity of everything that has to do with environmental is recorded over time",
    "start": "1350200",
    "end": "1357279"
  },
  {
    "text": "you know even within Amazon we record within our own data center within our own fulfillment centers everything is",
    "start": "1357279",
    "end": "1362769"
  },
  {
    "text": "time so we're constantly looking at data that's that has a time component to it and so what is special about a time",
    "start": "1362769",
    "end": "1368289"
  },
  {
    "text": "series database is time now is the single primary axis of the data model so",
    "start": "1368289",
    "end": "1373299"
  },
  {
    "text": "you think about the of your data model you don't have to take that into account as far as allocating specific resources",
    "start": "1373299",
    "end": "1380409"
  },
  {
    "text": "because it's a known quantity and so time series use cases for humidity for instance you know what does it look like",
    "start": "1380409",
    "end": "1385809"
  },
  {
    "start": "1382000",
    "end": "1382000"
  },
  {
    "text": "over time you know I've got application vents I've got ISDN sensor readings I've got DevOps data or for instance you know",
    "start": "1385809",
    "end": "1392440"
  },
  {
    "text": "fast moving time series they have from multiple sources at a rate of a million inserts per second so from a performance",
    "start": "1392440",
    "end": "1398620"
  },
  {
    "text": "standpoint again because it's a known quantity of time I can purpose build time stream to accommodate for that I",
    "start": "1398620",
    "end": "1406210"
  },
  {
    "text": "can trillions of events daily right I can capture pretty much an unlimited amount of events analytics are optimized",
    "start": "1406210",
    "end": "1412480"
  },
  {
    "text": "for time series so just like other database functionality you've got two different types of thoughts in the",
    "start": "1412480",
    "end": "1417940"
  },
  {
    "text": "database world of multi modal or purpose-built and so this is a purpose-built database which allows it",
    "start": "1417940",
    "end": "1424210"
  },
  {
    "text": "to be tuned for a very specific thing another our database offerings such as Neptune for instance are tuned for are",
    "start": "1424210",
    "end": "1429580"
  },
  {
    "text": "built for relationship understanding relationships between things and it's",
    "start": "1429580",
    "end": "1435340"
  },
  {
    "text": "tuned specifically for that one of the really interesting things is being able to interpret missing data so one of the",
    "start": "1435340",
    "end": "1441220"
  },
  {
    "text": "problems with feature engineering a lot of times and you talk about machine learning and data science is missing",
    "start": "1441220",
    "end": "1446679"
  },
  {
    "text": "data and how do you interpolate data and so actually with time stream it actually interpolates data for you so",
    "start": "1446679",
    "end": "1452740"
  },
  {
    "text": "interpolation being adding data points between do tato points that are missing",
    "start": "1452740",
    "end": "1458429"
  },
  {
    "text": "versus extrapolation which is predicting data points based on the data points",
    "start": "1458429",
    "end": "1464380"
  },
  {
    "text": "that you already have and so it takes a lot of again is one of those things that takes a lot of the work out of the machine learning realm and automates",
    "start": "1464380",
    "end": "1471730"
  },
  {
    "text": "that for you and it's serverless there's no service to manage so you're not doing all the things that you typically would do with",
    "start": "1471730",
    "end": "1477009"
  },
  {
    "text": "any database server for that matter or any server you're not provisioning and all that stuff and so again it's",
    "start": "1477009",
    "end": "1482200"
  },
  {
    "text": "purpose-built based on customer demand so Amazon quantum fully managed ledger database so this is also a new service",
    "start": "1482200",
    "end": "1489279"
  },
  {
    "text": "that we've database service that we've announced at reinvent that is a ledger database and so plus what is a letter",
    "start": "1489279",
    "end": "1496690"
  },
  {
    "start": "1495000",
    "end": "1495000"
  },
  {
    "text": "you know with centralized control so what is a ledger do really well a relative is really for verifiability of",
    "start": "1496690",
    "end": "1503379"
  },
  {
    "text": "transactions um and we'll talk about the comma the most common everybody talks about Bitcoin and and some of those",
    "start": "1503379",
    "end": "1510309"
  },
  {
    "text": "other things that are you know but the key difference is this is centralized it's not a distributed ledger and so for",
    "start": "1510309",
    "end": "1517419"
  },
  {
    "text": "applications like healthcare you've got to prove at track verify insurance tracking vehicle history manufacturing",
    "start": "1517419",
    "end": "1524289"
  },
  {
    "text": "anything that you need a verifiable and auditable transaction trail you know the",
    "start": "1524289",
    "end": "1530289"
  },
  {
    "text": "most common is an accounting ledger for instance that's where the term ledger comes from an accounting ledger would be",
    "start": "1530289",
    "end": "1536769"
  },
  {
    "text": "a good example of things that accounting transactions have to be verifiable they have to be auditable and have to be",
    "start": "1536769",
    "end": "1542500"
  },
  {
    "text": "traceable and so the traditional approaches to audit ability is they're hard to build",
    "start": "1542500",
    "end": "1548049"
  },
  {
    "start": "1544000",
    "end": "1544000"
  },
  {
    "text": "I mean I've built lots of databases with this type of functionality in it but",
    "start": "1548049",
    "end": "1554169"
  },
  {
    "text": "it's a lot of custom work to do it you've got a you know audit functionality using triggers and stored",
    "start": "1554169",
    "end": "1559360"
  },
  {
    "text": "procedures and it's it's you got to maintain you've got to build it you've got to maintain it you've got to account for the application there's a lot of",
    "start": "1559360",
    "end": "1565509"
  },
  {
    "text": "work to it it's resource intensive meaning that you're you're spending CPU cycles and in",
    "start": "1565509",
    "end": "1572470"
  },
  {
    "text": "really just compute cycles in general and maintaining this and actually executing the audibility piece of the",
    "start": "1572470",
    "end": "1579159"
  },
  {
    "text": "app of the database difficult to scale and manage it's it error-prone and",
    "start": "1579159",
    "end": "1584169"
  },
  {
    "text": "incomplete meaning that it is the possibility of introducing incomplete audit records or error or missing audit",
    "start": "1584169",
    "end": "1591220"
  },
  {
    "text": "records or wrong audit records as a possibility the most important thing is it's impossible to verify because in our DBMS",
    "start": "1591220",
    "end": "1599220"
  },
  {
    "text": "and then underneath the core at some point some DBA or somebody has access to that so there's no way to verify changes",
    "start": "1599220",
    "end": "1605670"
  },
  {
    "text": "made to the data by the administrator and that has to do with the data lineage and auditability of it and so one of the",
    "start": "1605670",
    "end": "1612870"
  },
  {
    "text": "challenges with blockchain and distributed Ledger's is it's designed for a specific purchase which is they distribute to the ledger meaning I have",
    "start": "1612870",
    "end": "1619470"
  },
  {
    "text": "a Bitcoin for instance and it has to be able to follow me anywhere that's the distribution adds a lot of complexity to it it's hard to use and slow meaning",
    "start": "1619470",
    "end": "1626280"
  },
  {
    "text": "that in order to do some of the way they talk about mining Bitcoin mining and things like that it's very very compute",
    "start": "1626280",
    "end": "1632250"
  },
  {
    "text": "intensive and so it's not to be to be able to perform some of the transactions",
    "start": "1632250",
    "end": "1637710"
  },
  {
    "text": "against distributed Ledger's very difficult to maintain a very hard to scale the heart of scale really has to",
    "start": "1637710",
    "end": "1642840"
  },
  {
    "text": "do with the computational power required to maintain a restrict distributed ledger so what does ql DB quantum ledger",
    "start": "1642840",
    "end": "1649110"
  },
  {
    "start": "1647000",
    "end": "1647000"
  },
  {
    "text": "database do for the customer well when it's immutable and transparent it's an append-only",
    "start": "1649110",
    "end": "1655040"
  },
  {
    "text": "immutable journal that tracks history of all the changes which cannot be deleted or modified and it's important to",
    "start": "1655040",
    "end": "1661860"
  },
  {
    "text": "understand that the customer does not have access to the ledger to be able to do that the customer gets full",
    "start": "1661860",
    "end": "1668280"
  },
  {
    "text": "visibility into the entire data lineage but they don't have access the customer there's no way for a Custer would",
    "start": "1668280",
    "end": "1673920"
  },
  {
    "text": "actually go in and change data its cryptographically verifiable and I'll explain a little bit about hash chaining",
    "start": "1673920",
    "end": "1679530"
  },
  {
    "text": "and how that works but every transaction is run through a",
    "start": "1679530",
    "end": "1684750"
  },
  {
    "text": "hash algorithm to ensure that the ensuing transaction the previous transaction before the transaction has",
    "start": "1684750",
    "end": "1690810"
  },
  {
    "text": "not been changed and I'll talk about that a little bit too and it's highly scalable it's very easy to use I mean it's it's a very familiar sequel like",
    "start": "1690810",
    "end": "1697650"
  },
  {
    "text": "interface and I'll explain a little bit about that here in a second as well so what does a ledger database look like",
    "start": "1697650",
    "end": "1702930"
  },
  {
    "start": "1701000",
    "end": "1701000"
  },
  {
    "text": "it's it's a ledger so it's going to have the ledger it's just a record of",
    "start": "1702930",
    "end": "1708030"
  },
  {
    "text": "transactions you either you enter a transaction you modify a transaction or",
    "start": "1708030",
    "end": "1713280"
  },
  {
    "text": "you delete a transaction and typically a delete of a transaction is not a physical delete it's just a marking and",
    "start": "1713280",
    "end": "1718710"
  },
  {
    "text": "saying this transaction is deleted it also compromises the current and",
    "start": "1718710",
    "end": "1723870"
  },
  {
    "text": "historic view of the data so current being what was the last transactional state of the data",
    "start": "1723870",
    "end": "1729390"
  },
  {
    "text": "historic means everything that happened to that particular transaction that particular piece of data so how does it",
    "start": "1729390",
    "end": "1735270"
  },
  {
    "start": "1735000",
    "end": "1735000"
  },
  {
    "text": "work so I've got a journal and let's say I want to insert some information into that journal I insert that information",
    "start": "1735270",
    "end": "1741480"
  },
  {
    "text": "it creates a hash of transaction one that information and this is not a relational model it's just depicted that",
    "start": "1741480",
    "end": "1747810"
  },
  {
    "text": "way so it's for easy viewing that becomes my current view it also becomes my historic view because I've only got",
    "start": "1747810",
    "end": "1753870"
  },
  {
    "text": "one record so historic and current are going to be the same in that sense next I update a record and I create a second",
    "start": "1753870",
    "end": "1760110"
  },
  {
    "text": "pass for that transaction which has changed from the first hash which I'll explain a little more detail in a minute and then my history changes because I",
    "start": "1760110",
    "end": "1767880"
  },
  {
    "text": "have updated the owner and but I've kept it now my historical record has two",
    "start": "1767880",
    "end": "1773100"
  },
  {
    "text": "different states of that transaction so I've got the state 1 when I update it when I inserted the transaction and I've",
    "start": "1773100",
    "end": "1778920"
  },
  {
    "text": "got a second state of the transaction when I've updated in this case the owner let's say that I delete the transaction",
    "start": "1778920",
    "end": "1785010"
  },
  {
    "text": "or I delete the item in this case a car based on the unique identifier of I then",
    "start": "1785010",
    "end": "1790290"
  },
  {
    "text": "from my ledger so it doesn't actually delete it what it does is it takes it",
    "start": "1790290",
    "end": "1795300"
  },
  {
    "text": "out of the current but it keeps the historical record of it and so I can go back through the entire of and marked it",
    "start": "1795300",
    "end": "1801450"
  },
  {
    "text": "as being deleted but doesn't delete the information it doesn't leave the journal history or anything like that and again it's a hash and it's chained and I'll",
    "start": "1801450",
    "end": "1807750"
  },
  {
    "text": "explain that and so I have a full historical view of of a record",
    "start": "1807750",
    "end": "1813320"
  },
  {
    "text": "transactional record so how does M Masson verify transactions in QV I mentioned hash chaining so let me",
    "start": "1813320",
    "end": "1820110"
  },
  {
    "text": "explain a little bit about how that works and why that's important so when you insert a record it creates a it uses",
    "start": "1820110",
    "end": "1826110"
  },
  {
    "text": "a sha 256 hash algorithm which there's plenty of things documented and written about that so I won't go into the whole",
    "start": "1826110",
    "end": "1831350"
  },
  {
    "text": "explaining how hashing works but it creates a hash of that transaction which",
    "start": "1831350",
    "end": "1836700"
  },
  {
    "text": "is a unique identifier for that particular transaction so that transaction once when I update the",
    "start": "1836700",
    "end": "1841950"
  },
  {
    "text": "records it also creates a hash sha-256 hash based on that transaction record",
    "start": "1841950",
    "end": "1848160"
  },
  {
    "text": "but what it also does is in Chains or includes the hash from the previous transaction with the term hash chaining",
    "start": "1848160",
    "end": "1854790"
  },
  {
    "text": "comes into play so that that hash that's generated in the second transaction or the second update",
    "start": "1854790",
    "end": "1860000"
  },
  {
    "text": "includes not only the information that was included in the update but also the hash from the previous from the previous",
    "start": "1860000",
    "end": "1867350"
  },
  {
    "text": "transactions again when I go and delete the record same thing change from the",
    "start": "1867350",
    "end": "1873230"
  },
  {
    "text": "second hat the hash Creator from the second one gets included in the hash of the third transactions now how does that",
    "start": "1873230",
    "end": "1878530"
  },
  {
    "text": "verify auditability if I go back and change the record in the original say I",
    "start": "1878530",
    "end": "1885800"
  },
  {
    "text": "changed the name of the owner it's going to create a whole new hash unique hash",
    "start": "1885800",
    "end": "1892990"
  },
  {
    "text": "function and that's going to carry through through all the transactions so",
    "start": "1892990",
    "end": "1898700"
  },
  {
    "text": "that the original hash that was generated is not going to equal the hash",
    "start": "1898700",
    "end": "1904340"
  },
  {
    "text": "that is generated through hash chaining if any of the records have been changed",
    "start": "1904340",
    "end": "1909470"
  },
  {
    "text": "so if the hash doesn't match the current hash as part of that verifiable that",
    "start": "1909470",
    "end": "1915040"
  },
  {
    "text": "function then you know something's been modified with a record with that I would",
    "start": "1915040",
    "end": "1920090"
  },
  {
    "text": "like to thank you for watching this video and if you're interested about any of the new services features or",
    "start": "1920090",
    "end": "1925640"
  },
  {
    "text": "functionality that I've talked about as part of this presentation please reach out to your AWS account executive or",
    "start": "1925640",
    "end": "1931160"
  },
  {
    "text": "your AWS Solutions Architect",
    "start": "1931160",
    "end": "1934570"
  }
]