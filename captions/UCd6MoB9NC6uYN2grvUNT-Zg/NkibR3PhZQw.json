[
  {
    "start": "0",
    "end": "23000"
  },
  {
    "text": "hi my name is Emily Weber I'm a machine",
    "start": "30",
    "end": "2820"
  },
  {
    "text": "learning specialist at Amazon Web",
    "start": "2820",
    "end": "4200"
  },
  {
    "text": "Services and today we're gonna learn",
    "start": "4200",
    "end": "5970"
  },
  {
    "text": "about feature engineering so if you've",
    "start": "5970",
    "end": "7980"
  },
  {
    "text": "ever wondered what's the best way to",
    "start": "7980",
    "end": "9480"
  },
  {
    "text": "wrangle your pandas dataframe what's the",
    "start": "9480",
    "end": "12389"
  },
  {
    "text": "best way to analyze the correlation in",
    "start": "12389",
    "end": "14130"
  },
  {
    "text": "your data sets or maybe perform",
    "start": "14130",
    "end": "15599"
  },
  {
    "text": "principal component analysis you'll want",
    "start": "15599",
    "end": "17820"
  },
  {
    "text": "to check out this video this is your",
    "start": "17820",
    "end": "19890"
  },
  {
    "text": "deep dive",
    "start": "19890",
    "end": "22220"
  },
  {
    "start": "23000",
    "end": "128000"
  },
  {
    "text": "so first if you are following along if",
    "start": "23000",
    "end": "26419"
  },
  {
    "text": "you have a stage maker notebook instance",
    "start": "26419",
    "end": "28519"
  },
  {
    "text": "that is already launched I want you to",
    "start": "28519",
    "end": "30439"
  },
  {
    "text": "do something I want you to resize that",
    "start": "30439",
    "end": "33260"
  },
  {
    "text": "notebook instance right because the best",
    "start": "33260",
    "end": "34670"
  },
  {
    "text": "way to launch a notebook instance",
    "start": "34670",
    "end": "36050"
  },
  {
    "text": "initially is to start small and that's",
    "start": "36050",
    "end": "38540"
  },
  {
    "text": "because notebook instances are gonna be",
    "start": "38540",
    "end": "40129"
  },
  {
    "text": "up for usually 8 plus hours a day and so",
    "start": "40129",
    "end": "42440"
  },
  {
    "text": "you want to keep your cost down by",
    "start": "42440",
    "end": "44449"
  },
  {
    "text": "selecting a smaller instance here we're",
    "start": "44449",
    "end": "46579"
  },
  {
    "text": "gonna need slightly more processing",
    "start": "46579",
    "end": "48379"
  },
  {
    "text": "right so this is uh this is a four step",
    "start": "48379",
    "end": "50239"
  },
  {
    "text": "flow you need to click stop on that",
    "start": "50239",
    "end": "53239"
  },
  {
    "text": "notebook instance right so just stop",
    "start": "53239",
    "end": "54800"
  },
  {
    "text": "that instance go ahead and push you edit",
    "start": "54800",
    "end": "57250"
  },
  {
    "text": "when you push edit you're gonna have a",
    "start": "57250",
    "end": "59510"
  },
  {
    "text": "couple options here I want you to",
    "start": "59510",
    "end": "61220"
  },
  {
    "text": "upgrade your ec2 instance so instead of",
    "start": "61220",
    "end": "64338"
  },
  {
    "text": "sticking around in that T family we're",
    "start": "64339",
    "end": "67070"
  },
  {
    "text": "gonna go all the way up to an m5 Excel",
    "start": "67070",
    "end": "69650"
  },
  {
    "text": "that tends to be more processing while",
    "start": "69650",
    "end": "72259"
  },
  {
    "text": "still keeping it at a lower dollar value",
    "start": "72259",
    "end": "74000"
  },
  {
    "text": "if you have a large data set go ahead",
    "start": "74000",
    "end": "76759"
  },
  {
    "text": "and upgrade your EBS volume right so",
    "start": "76759",
    "end": "78770"
  },
  {
    "text": "that's the actual disk space that's on",
    "start": "78770",
    "end": "80420"
  },
  {
    "text": "your notebook instance feel free to",
    "start": "80420",
    "end": "82340"
  },
  {
    "text": "upgrade that I typically move from five",
    "start": "82340",
    "end": "84320"
  },
  {
    "text": "which is the default up to 25 gigs so",
    "start": "84320",
    "end": "87409"
  },
  {
    "text": "I'll resize it and then go ahead and",
    "start": "87409",
    "end": "89240"
  },
  {
    "text": "push start and so that is a seven minute",
    "start": "89240",
    "end": "93490"
  },
  {
    "text": "process right over all that that tends",
    "start": "93490",
    "end": "96470"
  },
  {
    "text": "to be a seven minute exercise so if it's",
    "start": "96470",
    "end": "99800"
  },
  {
    "text": "taking some time don't don't be don't be",
    "start": "99800",
    "end": "102200"
  },
  {
    "text": "too frustrated there that's that's what",
    "start": "102200",
    "end": "104090"
  },
  {
    "text": "happens and so commonly if you're",
    "start": "104090",
    "end": "107690"
  },
  {
    "text": "getting a gateway timeout issue on your",
    "start": "107690",
    "end": "110030"
  },
  {
    "text": "notebook instance right or if you're",
    "start": "110030",
    "end": "111200"
  },
  {
    "text": "getting an error statement that says",
    "start": "111200",
    "end": "113060"
  },
  {
    "text": "insufficient memory or not enough disk",
    "start": "113060",
    "end": "115580"
  },
  {
    "text": "space you want to upgrade your instance",
    "start": "115580",
    "end": "117890"
  },
  {
    "text": "right you will get those errors because",
    "start": "117890",
    "end": "119690"
  },
  {
    "text": "you simply don't have enough bandwidth",
    "start": "119690",
    "end": "121310"
  },
  {
    "text": "RAM or cores or discs in order to",
    "start": "121310",
    "end": "125150"
  },
  {
    "text": "actually get your job done so having",
    "start": "125150",
    "end": "128899"
  },
  {
    "start": "128000",
    "end": "171000"
  },
  {
    "text": "said that there are two types of",
    "start": "128899",
    "end": "131450"
  },
  {
    "text": "questions that we're going to think",
    "start": "131450",
    "end": "132409"
  },
  {
    "text": "about in feature engineering right so if",
    "start": "132409",
    "end": "134360"
  },
  {
    "text": "are if we're using this little pie chart",
    "start": "134360",
    "end": "135680"
  },
  {
    "text": "here as our hypothesis of what machine",
    "start": "135680",
    "end": "137840"
  },
  {
    "text": "learning literally is which is solving",
    "start": "137840",
    "end": "139939"
  },
  {
    "text": "use case using a data set and a model",
    "start": "139939",
    "end": "141739"
  },
  {
    "text": "there are two types of questions we want",
    "start": "141739",
    "end": "143810"
  },
  {
    "text": "to think about one of those is a little",
    "start": "143810",
    "end": "146030"
  },
  {
    "text": "bit more other research level that is",
    "start": "146030",
    "end": "147439"
  },
  {
    "text": "conceptual those conceptual questions",
    "start": "147439",
    "end": "149659"
  },
  {
    "text": "are like well hey what's the best way to",
    "start": "149659",
    "end": "151760"
  },
  {
    "text": "do this why should I do this what are",
    "start": "151760",
    "end": "154340"
  },
  {
    "text": "the implications of this transformation",
    "start": "154340",
    "end": "156590"
  },
  {
    "text": "step",
    "start": "156590",
    "end": "156930"
  },
  {
    "text": "model we've also got a lot of stuff down",
    "start": "156930",
    "end": "159239"
  },
  {
    "text": "at the practical or developer level",
    "start": "159239",
    "end": "161159"
  },
  {
    "text": "where we're thinking well you know how",
    "start": "161159",
    "end": "162780"
  },
  {
    "text": "do I literally transform that how do I",
    "start": "162780",
    "end": "164579"
  },
  {
    "text": "actually do this feature engineering and",
    "start": "164579",
    "end": "166260"
  },
  {
    "text": "both of those are really helpful",
    "start": "166260",
    "end": "167489"
  },
  {
    "text": "questions so with that being said we're",
    "start": "167489",
    "end": "174090"
  },
  {
    "start": "171000",
    "end": "273000"
  },
  {
    "text": "gonna dive into the data frame right",
    "start": "174090",
    "end": "177000"
  },
  {
    "text": "into panda's data frame so we're gonna",
    "start": "177000",
    "end": "178680"
  },
  {
    "text": "import pandas as PD and then we're gonna",
    "start": "178680",
    "end": "181260"
  },
  {
    "text": "read in that file name and convert that",
    "start": "181260",
    "end": "183510"
  },
  {
    "text": "into a data frame having done that we",
    "start": "183510",
    "end": "187049"
  },
  {
    "text": "can call D F dot shape right that's",
    "start": "187049",
    "end": "188819"
  },
  {
    "text": "gonna tell us the number of rows that we",
    "start": "188819",
    "end": "190950"
  },
  {
    "text": "have against the number of columns D F",
    "start": "190950",
    "end": "192569"
  },
  {
    "text": "dot head it's gonna let us visually see",
    "start": "192569",
    "end": "195000"
  },
  {
    "text": "so we can actually scroll through our",
    "start": "195000",
    "end": "196379"
  },
  {
    "text": "data and see what's there",
    "start": "196379",
    "end": "198480"
  },
  {
    "text": "we need to convince ourselves that the",
    "start": "198480",
    "end": "202469"
  },
  {
    "text": "data set actually represents the real",
    "start": "202469",
    "end": "204359"
  },
  {
    "text": "world if you're listening to this and if",
    "start": "204359",
    "end": "206280"
  },
  {
    "text": "you want to become a better data",
    "start": "206280",
    "end": "207450"
  },
  {
    "text": "scientist I want you to learn how to be",
    "start": "207450",
    "end": "209219"
  },
  {
    "text": "skeptical about your own data right we",
    "start": "209219",
    "end": "211709"
  },
  {
    "text": "cannot assume that our data set is",
    "start": "211709",
    "end": "213450"
  },
  {
    "text": "actually accurate we can assume that",
    "start": "213450",
    "end": "215519"
  },
  {
    "text": "it's really representative we need to",
    "start": "215519",
    "end": "217799"
  },
  {
    "text": "make sure that that is using the data",
    "start": "217799",
    "end": "220079"
  },
  {
    "text": "set itself and histograms and plotting",
    "start": "220079",
    "end": "222150"
  },
  {
    "text": "is a common way of doing that we need to",
    "start": "222150",
    "end": "224819"
  },
  {
    "text": "remove outliers when that data set right",
    "start": "224819",
    "end": "226919"
  },
  {
    "text": "and so people have different ways that",
    "start": "226919",
    "end": "229470"
  },
  {
    "text": "they like to do this there there are a",
    "start": "229470",
    "end": "230909"
  },
  {
    "text": "lot of common techniques out there",
    "start": "230909",
    "end": "232340"
  },
  {
    "text": "essentially if you're removing a point",
    "start": "232340",
    "end": "236069"
  },
  {
    "text": "from your data set that will impact your",
    "start": "236069",
    "end": "238590"
  },
  {
    "text": "model right because your model is not",
    "start": "238590",
    "end": "239970"
  },
  {
    "text": "going to know the difference between an",
    "start": "239970",
    "end": "241680"
  },
  {
    "text": "outlier and versus a normal one so we",
    "start": "241680",
    "end": "243239"
  },
  {
    "text": "need to apply typically statistical",
    "start": "243239",
    "end": "245400"
  },
  {
    "text": "methods in order to remove those we need",
    "start": "245400",
    "end": "248579"
  },
  {
    "text": "to combine columns so in this example",
    "start": "248579",
    "end": "251419"
  },
  {
    "text": "the researchers wanted to be able to",
    "start": "251419",
    "end": "253949"
  },
  {
    "text": "model who is going to open up a bank",
    "start": "253949",
    "end": "255269"
  },
  {
    "text": "account and so in order to do that they",
    "start": "255269",
    "end": "257639"
  },
  {
    "text": "thought a relevant column would be",
    "start": "257639",
    "end": "258840"
  },
  {
    "text": "whether or not a person was working and",
    "start": "258840",
    "end": "260759"
  },
  {
    "text": "so they actually just combined three",
    "start": "260759",
    "end": "263099"
  },
  {
    "text": "columns based on the conditional and of",
    "start": "263099",
    "end": "264780"
  },
  {
    "text": "an attribute being true and then roll",
    "start": "264780",
    "end": "266789"
  },
  {
    "text": "that up into a new column so very very",
    "start": "266789",
    "end": "269159"
  },
  {
    "text": "common technique you can call that",
    "start": "269159",
    "end": "270810"
  },
  {
    "text": "synthetic feature generation handling",
    "start": "270810",
    "end": "274949"
  },
  {
    "text": "strings so if you're opening up your",
    "start": "274949",
    "end": "278099"
  },
  {
    "text": "data set and you realize that one of",
    "start": "278099",
    "end": "279599"
  },
  {
    "text": "your columns actually has strings in it",
    "start": "279599",
    "end": "282270"
  },
  {
    "text": "usually what that means is that as a",
    "start": "282270",
    "end": "283830"
  },
  {
    "text": "categorical column right it can start it",
    "start": "283830",
    "end": "287490"
  },
  {
    "text": "can say fruit and then you've got Apple",
    "start": "287490",
    "end": "289169"
  },
  {
    "text": "watermelon or",
    "start": "289169",
    "end": "290279"
  },
  {
    "text": "right and so essentially what we need to",
    "start": "290279",
    "end": "292739"
  },
  {
    "text": "do typically is convert new columns from",
    "start": "292739",
    "end": "296759"
  },
  {
    "text": "that so the technical term for that is",
    "start": "296759",
    "end": "298949"
  },
  {
    "text": "one hot encoding right and you're going",
    "start": "298949",
    "end": "300899"
  },
  {
    "text": "to use a method that's built into pandas",
    "start": "300899",
    "end": "305249"
  },
  {
    "text": "that's called PD get dummies right and",
    "start": "305249",
    "end": "307379"
  },
  {
    "text": "that's gonna take your data set along",
    "start": "307379",
    "end": "309029"
  },
  {
    "text": "with the columns that you want to",
    "start": "309029",
    "end": "310169"
  },
  {
    "text": "actually transform and it's gonna",
    "start": "310169",
    "end": "311609"
  },
  {
    "text": "explode that out right so maybe you",
    "start": "311609",
    "end": "313319"
  },
  {
    "text": "started with 24 columns but then in some",
    "start": "313319",
    "end": "315869"
  },
  {
    "text": "cases you can get significantly more so",
    "start": "315869",
    "end": "318929"
  },
  {
    "text": "this is a powerful technique but you do",
    "start": "318929",
    "end": "320609"
  },
  {
    "text": "want to be a little bit careful about",
    "start": "320609",
    "end": "322499"
  },
  {
    "text": "how you actually implement it min max",
    "start": "322499",
    "end": "325499"
  },
  {
    "text": "scaling is a crucial technique so this",
    "start": "325499",
    "end": "328499"
  },
  {
    "text": "is gonna be for your numerical feature",
    "start": "328499",
    "end": "330360"
  },
  {
    "text": "set so if you've got a column that says",
    "start": "330360",
    "end": "332459"
  },
  {
    "text": "age or income or potentially time of day",
    "start": "332459",
    "end": "337009"
  },
  {
    "text": "not necessarily time of day right but",
    "start": "337009",
    "end": "339239"
  },
  {
    "text": "other other numerical features that are",
    "start": "339239",
    "end": "340949"
  },
  {
    "text": "actually running variables typically",
    "start": "340949",
    "end": "343049"
  },
  {
    "text": "you're want you're gonna want to use a",
    "start": "343049",
    "end": "344219"
  },
  {
    "text": "scalar to bring that back down my go-to",
    "start": "344219",
    "end": "346949"
  },
  {
    "text": "tends to be a psychic learn min max so",
    "start": "346949",
    "end": "350069"
  },
  {
    "text": "that just keeps the distribution the",
    "start": "350069",
    "end": "351779"
  },
  {
    "text": "same but brings in the min and the max",
    "start": "351779",
    "end": "353489"
  },
  {
    "text": "other times you will need to actually",
    "start": "353489",
    "end": "355709"
  },
  {
    "text": "convert that distribution so it is a",
    "start": "355709",
    "end": "357659"
  },
  {
    "text": "little bit more standard but in this",
    "start": "357659",
    "end": "359699"
  },
  {
    "text": "case we're gonna leave it having the",
    "start": "359699",
    "end": "361589"
  },
  {
    "text": "same distribution just with a min Max",
    "start": "361589",
    "end": "363360"
  },
  {
    "text": "scaling a correlation analysis so very",
    "start": "363360",
    "end": "366479"
  },
  {
    "text": "common technique right you're gonna have",
    "start": "366479",
    "end": "368129"
  },
  {
    "text": "multiple variables in your data set and",
    "start": "368129",
    "end": "370589"
  },
  {
    "text": "you need to figure out if those",
    "start": "370589",
    "end": "372299"
  },
  {
    "text": "variables are correlated with the target",
    "start": "372299",
    "end": "374489"
  },
  {
    "text": "column that you're actually trying to",
    "start": "374489",
    "end": "375629"
  },
  {
    "text": "predict or if they're correlated with",
    "start": "375629",
    "end": "377489"
  },
  {
    "text": "the other X's right and all of those",
    "start": "377489",
    "end": "379319"
  },
  {
    "text": "will have a pretty significant impact on",
    "start": "379319",
    "end": "381449"
  },
  {
    "text": "your model and so if you need to deke or",
    "start": "381449",
    "end": "384809"
  },
  {
    "text": "elate your X's right if you have two",
    "start": "384809",
    "end": "386309"
  },
  {
    "text": "columns that are actually correlated you",
    "start": "386309",
    "end": "388589"
  },
  {
    "text": "can D correlate that using principal",
    "start": "388589",
    "end": "391049"
  },
  {
    "text": "component analysis so that's running",
    "start": "391049",
    "end": "392669"
  },
  {
    "text": "another technique that'll basically",
    "start": "392669",
    "end": "394769"
  },
  {
    "text": "identify the underlying principal",
    "start": "394769",
    "end": "396869"
  },
  {
    "text": "components that are explaining some of",
    "start": "396869",
    "end": "398849"
  },
  {
    "text": "that variation our principal component",
    "start": "398849",
    "end": "400709"
  },
  {
    "text": "analysis is also helpful to reduce",
    "start": "400709",
    "end": "402199"
  },
  {
    "text": "dimensionality so if you're starting",
    "start": "402199",
    "end": "404099"
  },
  {
    "text": "with you know a thousand column so you",
    "start": "404099",
    "end": "406439"
  },
  {
    "text": "want to bring that down to anywhere from",
    "start": "406439",
    "end": "408179"
  },
  {
    "text": "10 to 100 PCA is gonna be your go-to for",
    "start": "408179",
    "end": "411689"
  },
  {
    "text": "that data augmentation right so if",
    "start": "411689",
    "end": "415409"
  },
  {
    "start": "413000",
    "end": "565000"
  },
  {
    "text": "you're in the deep learning world you",
    "start": "415409",
    "end": "417539"
  },
  {
    "text": "get to actually augment your data so in",
    "start": "417539",
    "end": "420300"
  },
  {
    "text": "images that's going to be cropping",
    "start": "420300",
    "end": "422629"
  },
  {
    "text": "changing some of the",
    "start": "422629",
    "end": "424139"
  },
  {
    "text": "changing some of the rotations and all",
    "start": "424139",
    "end": "426689"
  },
  {
    "text": "that basically adds both volume and",
    "start": "426689",
    "end": "428610"
  },
  {
    "text": "variety to your data set which your",
    "start": "428610",
    "end": "430110"
  },
  {
    "text": "models gonna be able to learn from right",
    "start": "430110",
    "end": "431550"
  },
  {
    "text": "it's gonna like that data augmentation",
    "start": "431550",
    "end": "433650"
  },
  {
    "text": "is gonna be built in to some of the",
    "start": "433650",
    "end": "435439"
  },
  {
    "text": "image classification and other",
    "start": "435439",
    "end": "437430"
  },
  {
    "text": "algorithms in sage maker using a pre",
    "start": "437430",
    "end": "440939"
  },
  {
    "text": "trained model so again if you are in the",
    "start": "440939",
    "end": "442560"
  },
  {
    "text": "deep learning world the technique is",
    "start": "442560",
    "end": "444840"
  },
  {
    "text": "called transfer learning right you can",
    "start": "444840",
    "end": "446520"
  },
  {
    "text": "reference a model you previously trained",
    "start": "446520",
    "end": "448499"
  },
  {
    "text": "and then basically add layers on top of",
    "start": "448499",
    "end": "451199"
  },
  {
    "text": "that retrain those layers and then",
    "start": "451199",
    "end": "453180"
  },
  {
    "text": "redeploy it and keep running so some",
    "start": "453180",
    "end": "455849"
  },
  {
    "text": "common steps right just importing pandas",
    "start": "455849",
    "end": "458879"
  },
  {
    "text": "SPD reading your file into memory you",
    "start": "458879",
    "end": "461460"
  },
  {
    "text": "can get your headers just by listing the",
    "start": "461460",
    "end": "463139"
  },
  {
    "text": "data frame you can easily get histograms",
    "start": "463139",
    "end": "466020"
  },
  {
    "text": "you can easily loop through those rows",
    "start": "466020",
    "end": "468419"
  },
  {
    "text": "right so for idx in row and de toros you",
    "start": "468419",
    "end": "471719"
  },
  {
    "text": "can reference the stuff that's in your",
    "start": "471719",
    "end": "473189"
  },
  {
    "text": "rows and then you can update your data",
    "start": "473189",
    "end": "475409"
  },
  {
    "text": "frame using DF to ILOG so some other",
    "start": "475409",
    "end": "478680"
  },
  {
    "text": "common techniques if you need to extract",
    "start": "478680",
    "end": "481529"
  },
  {
    "text": "data from a data frame right that there",
    "start": "481529",
    "end": "484379"
  },
  {
    "text": "are queries out there that you can",
    "start": "484379",
    "end": "485789"
  },
  {
    "text": "definitely use whether you're merging",
    "start": "485789",
    "end": "487919"
  },
  {
    "text": "can cadogan",
    "start": "487919",
    "end": "489120"
  },
  {
    "text": "again the AWS CLI is always gonna be",
    "start": "489120",
    "end": "491969"
  },
  {
    "text": "helpful goto and then lastly for one",
    "start": "491969",
    "end": "495210"
  },
  {
    "text": "hunting coating right please don't write",
    "start": "495210",
    "end": "496860"
  },
  {
    "text": "this yourself right I don't want anyone",
    "start": "496860",
    "end": "499319"
  },
  {
    "text": "to waste time out there writing their",
    "start": "499319",
    "end": "500909"
  },
  {
    "text": "own one hot encoder I definitely just",
    "start": "500909",
    "end": "503099"
  },
  {
    "text": "just use the one that's built into into",
    "start": "503099",
    "end": "505139"
  },
  {
    "text": "pandas so once you've got a few e-tail",
    "start": "505139",
    "end": "510810"
  },
  {
    "text": "approaches set up right maybe you're",
    "start": "510810",
    "end": "512250"
  },
  {
    "text": "taking transformation strategy a and",
    "start": "512250",
    "end": "514948"
  },
  {
    "text": "then you're taking transformation",
    "start": "514949",
    "end": "516630"
  },
  {
    "text": "strategy B and maybe you want to decide",
    "start": "516630",
    "end": "519029"
  },
  {
    "text": "which one is ultimately better so a",
    "start": "519029",
    "end": "521190"
  },
  {
    "text": "common way of comparing approaches is by",
    "start": "521190",
    "end": "524279"
  },
  {
    "text": "actually running separate training jobs",
    "start": "524279",
    "end": "525930"
  },
  {
    "text": "right so go ahead and train the model on",
    "start": "525930",
    "end": "527760"
  },
  {
    "text": "ETL strategy a and then train the model",
    "start": "527760",
    "end": "530880"
  },
  {
    "text": "on ETL strategy B and objectively",
    "start": "530880",
    "end": "533640"
  },
  {
    "text": "determine which one is doing better a",
    "start": "533640",
    "end": "535500"
  },
  {
    "text": "few ways you can determine that",
    "start": "535500",
    "end": "537029"
  },
  {
    "text": "objective performance you can look at",
    "start": "537029",
    "end": "539100"
  },
  {
    "text": "accuracy or precision recall AUC right",
    "start": "539100",
    "end": "541769"
  },
  {
    "text": "any objective metric you're pretty are",
    "start": "541769",
    "end": "543120"
  },
  {
    "text": "interested in you can also think about",
    "start": "543120",
    "end": "544949"
  },
  {
    "text": "which one takes longer to train you can",
    "start": "544949",
    "end": "547529"
  },
  {
    "text": "also think about which one takes longer",
    "start": "547529",
    "end": "549120"
  },
  {
    "text": "to engineer right dev time is expensive",
    "start": "549120",
    "end": "551640"
  },
  {
    "text": "our time is valuable and so if an e tail",
    "start": "551640",
    "end": "554610"
  },
  {
    "text": "strategy that you're doing takes",
    "start": "554610",
    "end": "556990"
  },
  {
    "text": "multiple days to write but is only",
    "start": "556990",
    "end": "558940"
  },
  {
    "text": "getting you and maybe one tenth of a",
    "start": "558940",
    "end": "561160"
  },
  {
    "text": "percentage point is it really that",
    "start": "561160",
    "end": "562750"
  },
  {
    "text": "valuable potentially not get integration",
    "start": "562750",
    "end": "567550"
  },
  {
    "start": "565000",
    "end": "590000"
  },
  {
    "text": "right get is crucial here particularly",
    "start": "567550",
    "end": "569830"
  },
  {
    "text": "in the ETL world so you're gonna want to",
    "start": "569830",
    "end": "572290"
  },
  {
    "text": "add a git repository to your notebook",
    "start": "572290",
    "end": "574990"
  },
  {
    "text": "you're going to want to wrap your steps",
    "start": "574990",
    "end": "576520"
  },
  {
    "text": "as functions right add functions to your",
    "start": "576520",
    "end": "580600"
  },
  {
    "text": "git repository so that you can share you",
    "start": "580600",
    "end": "582910"
  },
  {
    "text": "it feel steps across the members of your",
    "start": "582910",
    "end": "584800"
  },
  {
    "text": "team also you want to run your ETL in",
    "start": "584800",
    "end": "588040"
  },
  {
    "text": "production and so this this brings up a",
    "start": "588040",
    "end": "592300"
  },
  {
    "start": "590000",
    "end": "637000"
  },
  {
    "text": "topic right I think a lot of people in",
    "start": "592300",
    "end": "593709"
  },
  {
    "text": "in data science are realizing that this",
    "start": "593709",
    "end": "595630"
  },
  {
    "text": "is like this is a key thing that we need",
    "start": "595630",
    "end": "598180"
  },
  {
    "text": "to we need to really own we need to",
    "start": "598180",
    "end": "600520"
  },
  {
    "text": "think about our ETL as software",
    "start": "600520",
    "end": "602560"
  },
  {
    "text": "applications right so yes we are doing",
    "start": "602560",
    "end": "605320"
  },
  {
    "text": "analysis but we're also trying to build",
    "start": "605320",
    "end": "606880"
  },
  {
    "text": "applications that are actually going to",
    "start": "606880",
    "end": "608290"
  },
  {
    "text": "add business value so wrapping your",
    "start": "608290",
    "end": "610660"
  },
  {
    "text": "content as functions is gonna make it",
    "start": "610660",
    "end": "612880"
  },
  {
    "text": "easier to use and maintain in the long",
    "start": "612880",
    "end": "615130"
  },
  {
    "text": "term you want your code to be modular",
    "start": "615130",
    "end": "617500"
  },
  {
    "text": "right you want it to be well abstracted",
    "start": "617500",
    "end": "619480"
  },
  {
    "text": "well thought-out you want it to be",
    "start": "619480",
    "end": "620830"
  },
  {
    "text": "readable so that another person can pick",
    "start": "620830",
    "end": "623290"
  },
  {
    "text": "it up and in a few minutes understand",
    "start": "623290",
    "end": "625720"
  },
  {
    "text": "what's going on rather than having to",
    "start": "625720",
    "end": "627250"
  },
  {
    "text": "dig through every line right and you",
    "start": "627250",
    "end": "628959"
  },
  {
    "text": "want it to be reusable so eventually",
    "start": "628959",
    "end": "630760"
  },
  {
    "text": "you're gonna be building full scripts so",
    "start": "630760",
    "end": "632620"
  },
  {
    "text": "you can then import from to add on top",
    "start": "632620",
    "end": "634480"
  },
  {
    "text": "of all right one key topic here our",
    "start": "634480",
    "end": "640029"
  },
  {
    "start": "637000",
    "end": "1045000"
  },
  {
    "text": "inference pipelines and so this is",
    "start": "640029",
    "end": "642070"
  },
  {
    "text": "specific to Sage maker um so let's say I",
    "start": "642070",
    "end": "644350"
  },
  {
    "text": "have not just one container but two",
    "start": "644350",
    "end": "646750"
  },
  {
    "text": "maybe more right maybe I have up to five",
    "start": "646750",
    "end": "649570"
  },
  {
    "text": "containers and I want to run all of",
    "start": "649570",
    "end": "652120"
  },
  {
    "text": "those as pair it in parallel so using",
    "start": "652120",
    "end": "654970"
  },
  {
    "text": "inference pipelines in Amazon Sage Maker",
    "start": "654970",
    "end": "657250"
  },
  {
    "text": "I can set this up right I can deploy up",
    "start": "657250",
    "end": "659589"
  },
  {
    "text": "to five containers as a quote-unquote",
    "start": "659589",
    "end": "662350"
  },
  {
    "text": "pipeline model and so my process is",
    "start": "662350",
    "end": "664959"
  },
  {
    "text": "gonna run through container one maybe",
    "start": "664959",
    "end": "667089"
  },
  {
    "text": "it's doing some feature transformation",
    "start": "667089",
    "end": "668670"
  },
  {
    "text": "run through container two maybe that's",
    "start": "668670",
    "end": "671170"
  },
  {
    "text": "the PC a container three is my ex--you",
    "start": "671170",
    "end": "674110"
  },
  {
    "text": "boost and then containers four and five",
    "start": "674110",
    "end": "676089"
  },
  {
    "text": "are doing pre post processing and then",
    "start": "676089",
    "end": "678820"
  },
  {
    "text": "shooting it on out there the rest of my",
    "start": "678820",
    "end": "679990"
  },
  {
    "text": "app on so again those are gonna run in",
    "start": "679990",
    "end": "681760"
  },
  {
    "text": "serial those containers are gonna be",
    "start": "681760",
    "end": "684220"
  },
  {
    "text": "co-located on the same ec2 instance",
    "start": "684220",
    "end": "686589"
  },
  {
    "text": "right so that's a one eesti to instance",
    "start": "686589",
    "end": "688630"
  },
  {
    "text": "that's going to be using one of those",
    "start": "688630",
    "end": "690040"
  },
  {
    "text": "containers for any point in time and",
    "start": "690040",
    "end": "691630"
  },
  {
    "text": "this is one way that you can",
    "start": "691630",
    "end": "693399"
  },
  {
    "text": "productionize your ETL code in sage",
    "start": "693399",
    "end": "695800"
  },
  {
    "text": "maker and so with that let's check out",
    "start": "695800",
    "end": "699399"
  },
  {
    "text": "an example so over here again we are on",
    "start": "699399",
    "end": "702699"
  },
  {
    "text": "a sage maker notebook instance right",
    "start": "702699",
    "end": "704740"
  },
  {
    "text": "this is in USC so on this is our test",
    "start": "704740",
    "end": "707139"
  },
  {
    "text": "emic test ursin we're getting up here so",
    "start": "707139",
    "end": "709870"
  },
  {
    "text": "this example is coming from the sage",
    "start": "709870",
    "end": "711970"
  },
  {
    "text": "maker examples right so 200 plus",
    "start": "711970",
    "end": "713829"
  },
  {
    "text": "notebooks this is one of them and so",
    "start": "713829",
    "end": "716170"
  },
  {
    "text": "let's let's check this out this is an",
    "start": "716170",
    "end": "717610"
  },
  {
    "text": "inference pipeline that we're gonna set",
    "start": "717610",
    "end": "720040"
  },
  {
    "text": "up with first a scikit-learn transformer",
    "start": "720040",
    "end": "722560"
  },
  {
    "text": "and then a linear learner model and both",
    "start": "722560",
    "end": "725230"
  },
  {
    "text": "of those are gonna be stuck together so",
    "start": "725230",
    "end": "726959"
  },
  {
    "text": "here we go this should look very",
    "start": "726959",
    "end": "729579"
  },
  {
    "text": "familiar to you right sage maker session",
    "start": "729579",
    "end": "731889"
  },
  {
    "text": "default buckets then we're gonna do some",
    "start": "731889",
    "end": "734040"
  },
  {
    "text": "pre-processing all right so we preach or",
    "start": "734040",
    "end": "736540"
  },
  {
    "text": "download this data set here this is that",
    "start": "736540",
    "end": "739389"
  },
  {
    "text": "seashell abalone dataset and then here",
    "start": "739389",
    "end": "743230"
  },
  {
    "text": "we go we'll specify our working",
    "start": "743230",
    "end": "744310"
  },
  {
    "text": "directory next we're gonna get a say",
    "start": "744310",
    "end": "747370"
  },
  {
    "text": "scikit-learn script so here is that",
    "start": "747370",
    "end": "750819"
  },
  {
    "text": "script let's check it out",
    "start": "750819",
    "end": "753720"
  },
  {
    "text": "so this is a feature Iser script so",
    "start": "753720",
    "end": "756550"
  },
  {
    "text": "again this script is doing feature",
    "start": "756550",
    "end": "758860"
  },
  {
    "text": "transformations right it is not training",
    "start": "758860",
    "end": "760779"
  },
  {
    "text": "a model it is doing feature",
    "start": "760779",
    "end": "761800"
  },
  {
    "text": "transformations and let's check this out",
    "start": "761800",
    "end": "764230"
  },
  {
    "text": "this has image store right so it's",
    "start": "764230",
    "end": "767800"
  },
  {
    "text": "actually storing small sets of data in",
    "start": "767800",
    "end": "770470"
  },
  {
    "text": "so my set I just mean variables right so",
    "start": "770470",
    "end": "773260"
  },
  {
    "text": "we have the the column names right so",
    "start": "773260",
    "end": "775089"
  },
  {
    "text": "the header is stored up here we know",
    "start": "775089",
    "end": "777130"
  },
  {
    "text": "which one is our label column we know",
    "start": "777130",
    "end": "780069"
  },
  {
    "text": "the the datatypes of those future",
    "start": "780069",
    "end": "782079"
  },
  {
    "text": "columns and so up here of same flow",
    "start": "782079",
    "end": "785680"
  },
  {
    "text": "we're gonna set our arc parser now read",
    "start": "785680",
    "end": "787779"
  },
  {
    "text": "through those input files get that raw",
    "start": "787779",
    "end": "790360"
  },
  {
    "text": "data that's coming in and then let's",
    "start": "790360",
    "end": "793510"
  },
  {
    "text": "check this out so the numeric features",
    "start": "793510",
    "end": "795689"
  },
  {
    "text": "right are down here and then we're gonna",
    "start": "795689",
    "end": "798760"
  },
  {
    "text": "set up a scikit-learn pipeline right and",
    "start": "798760",
    "end": "802660"
  },
  {
    "text": "so the cycle earned pipeline is gonna do",
    "start": "802660",
    "end": "804850"
  },
  {
    "text": "two things first it's imputing missing",
    "start": "804850",
    "end": "808509"
  },
  {
    "text": "values using the median and that's right",
    "start": "808509",
    "end": "810880"
  },
  {
    "text": "here then it is scaling the numerical",
    "start": "810880",
    "end": "814029"
  },
  {
    "text": "values using the scaler right then we",
    "start": "814029",
    "end": "816610"
  },
  {
    "text": "have a couple categorical features and",
    "start": "816610",
    "end": "818920"
  },
  {
    "text": "so we're gonna add those to the pipeline",
    "start": "818920",
    "end": "821769"
  },
  {
    "text": "right so additional",
    "start": "821769",
    "end": "823590"
  },
  {
    "text": "pipe lines it's going to be this simple",
    "start": "823590",
    "end": "825510"
  },
  {
    "text": "impute err right that's that's filling",
    "start": "825510",
    "end": "828000"
  },
  {
    "text": "in the categorical missing values and",
    "start": "828000",
    "end": "830730"
  },
  {
    "text": "then we're gonna do one hot encoding",
    "start": "830730",
    "end": "832350"
  },
  {
    "text": "here so again one hot encoding with that",
    "start": "832350",
    "end": "834930"
  },
  {
    "text": "PDD I get dumb use but this is all",
    "start": "834930",
    "end": "836640"
  },
  {
    "text": "handled for us by scikit-learn I've got",
    "start": "836640",
    "end": "839820"
  },
  {
    "text": "a couple other transformers we want to",
    "start": "839820",
    "end": "841230"
  },
  {
    "text": "do and then this can be counterintuitive",
    "start": "841230",
    "end": "845520"
  },
  {
    "text": "right so this is actually saying",
    "start": "845520",
    "end": "847160"
  },
  {
    "text": "preprocessor dot fit and then it's gonna",
    "start": "847160",
    "end": "850530"
  },
  {
    "text": "be writing this as a model so again this",
    "start": "850530",
    "end": "852690"
  },
  {
    "text": "is not a formal machine learning model",
    "start": "852690",
    "end": "856140"
  },
  {
    "text": "right there's there's no predictive",
    "start": "856140",
    "end": "858360"
  },
  {
    "text": "content that's going on over here",
    "start": "858360",
    "end": "859800"
  },
  {
    "text": "in this case there are a handful of",
    "start": "859800",
    "end": "862560"
  },
  {
    "text": "feature transformation steps and we're",
    "start": "862560",
    "end": "865080"
  },
  {
    "text": "going to wrap those feature",
    "start": "865080",
    "end": "866040"
  },
  {
    "text": "transformation steps as a psychic learn",
    "start": "866040",
    "end": "868730"
  },
  {
    "text": "preprocessor there are a couple other",
    "start": "868730",
    "end": "871680"
  },
  {
    "text": "functions input function here is going",
    "start": "871680",
    "end": "874230"
  },
  {
    "text": "to take the raw data on the prediction",
    "start": "874230",
    "end": "876690"
  },
  {
    "text": "side and convert that to a data frame",
    "start": "876690",
    "end": "879180"
  },
  {
    "text": "and then the output function is gonna",
    "start": "879180",
    "end": "882420"
  },
  {
    "text": "loop through our instances and then",
    "start": "882420",
    "end": "885360"
  },
  {
    "text": "append it there we go and so that is",
    "start": "885360",
    "end": "889440"
  },
  {
    "text": "calling model dot transform on that",
    "start": "889440",
    "end": "892380"
  },
  {
    "text": "input data and so the model here and",
    "start": "892380",
    "end": "894720"
  },
  {
    "text": "again that is a feature engineering",
    "start": "894720",
    "end": "896930"
  },
  {
    "text": "model right that's not a machine",
    "start": "896930",
    "end": "898800"
  },
  {
    "text": "learning model but that is a feature",
    "start": "898800",
    "end": "900300"
  },
  {
    "text": "engineering model there we go",
    "start": "900300",
    "end": "903320"
  },
  {
    "text": "and so let's let's finish this out just",
    "start": "903320",
    "end": "906000"
  },
  {
    "text": "to see how this actually works right so",
    "start": "906000",
    "end": "908730"
  },
  {
    "text": "back here in the rest of our notebook",
    "start": "908730",
    "end": "911270"
  },
  {
    "text": "we'll set up this container right and so",
    "start": "911270",
    "end": "915060"
  },
  {
    "text": "that's again scikit-learn",
    "start": "915060",
    "end": "916730"
  },
  {
    "text": "estimator with that abalone feature Iser",
    "start": "916730",
    "end": "921230"
  },
  {
    "text": "alright and that is fitting the",
    "start": "921230",
    "end": "923460"
  },
  {
    "text": "preprocessor right and so that is the",
    "start": "923460",
    "end": "929130"
  },
  {
    "text": "preprocessor that's training and so",
    "start": "929130",
    "end": "932280"
  },
  {
    "text": "that's running on a training job then",
    "start": "932280",
    "end": "936630"
  },
  {
    "text": "down here we're gonna run batch",
    "start": "936630",
    "end": "938970"
  },
  {
    "text": "transform on that ok then we're gonna",
    "start": "938970",
    "end": "941700"
  },
  {
    "text": "fit the linear model with the",
    "start": "941700",
    "end": "943710"
  },
  {
    "text": "pre-process data so we'll take in our",
    "start": "943710",
    "end": "946140"
  },
  {
    "text": "stage maker estimator all right we've",
    "start": "946140",
    "end": "947970"
  },
  {
    "text": "got one m4 to XO okay set our hyper",
    "start": "947970",
    "end": "951270"
  },
  {
    "text": "parameters call fit then we're gonna set",
    "start": "951270",
    "end": "954270"
  },
  {
    "text": "up an inference pipeline",
    "start": "954270",
    "end": "956610"
  },
  {
    "text": "and so again that's that pipeline model",
    "start": "956610",
    "end": "958889"
  },
  {
    "text": "that takes a model name alright and",
    "start": "958889",
    "end": "963809"
  },
  {
    "text": "we've got our inference pipeline that's",
    "start": "963809",
    "end": "965459"
  },
  {
    "text": "coming up over here so thus I could",
    "start": "965459",
    "end": "967920"
  },
  {
    "text": "learn inference model happens here the",
    "start": "967920",
    "end": "970410"
  },
  {
    "text": "linear models happens here and so the",
    "start": "970410",
    "end": "973019"
  },
  {
    "text": "pipeline model is literally a list of",
    "start": "973019",
    "end": "977209"
  },
  {
    "text": "models right pretty intuitive so we're",
    "start": "977209",
    "end": "981569"
  },
  {
    "text": "calling it an inference pipeline it's",
    "start": "981569",
    "end": "984059"
  },
  {
    "text": "got a role and then it just has a list",
    "start": "984059",
    "end": "985949"
  },
  {
    "text": "of models and then we deploy it and so",
    "start": "985949",
    "end": "988559"
  },
  {
    "text": "we can just deploy up to five models and",
    "start": "988559",
    "end": "990689"
  },
  {
    "text": "then we can also send requests to that",
    "start": "990689",
    "end": "992699"
  },
  {
    "text": "and so this is with having it deployed",
    "start": "992699",
    "end": "995730"
  },
  {
    "text": "as an endpoint and so again that is",
    "start": "995730",
    "end": "998999"
  },
  {
    "text": "serializing up to five containers that",
    "start": "998999",
    "end": "1003079"
  },
  {
    "text": "are gonna run one after the other",
    "start": "1003079",
    "end": "1004429"
  },
  {
    "text": "let's switch back and so some pro tips",
    "start": "1004429",
    "end": "1007339"
  },
  {
    "text": "just to close this out again versioning",
    "start": "1007339",
    "end": "1009619"
  },
  {
    "text": "super helpful",
    "start": "1009619",
    "end": "1011179"
  },
  {
    "text": "definitely so you're you're holding on",
    "start": "1011179",
    "end": "1013129"
  },
  {
    "text": "to your ETL code after you've written it",
    "start": "1013129",
    "end": "1015019"
  },
  {
    "text": "you can share it resize in that notebook",
    "start": "1015019",
    "end": "1017749"
  },
  {
    "text": "on the fly definitely helpful running",
    "start": "1017749",
    "end": "1020449"
  },
  {
    "text": "multi processing for future",
    "start": "1020449",
    "end": "1022100"
  },
  {
    "text": "transformation right so on your notebook",
    "start": "1022100",
    "end": "1023419"
  },
  {
    "text": "instance you can utilize all those cores",
    "start": "1023419",
    "end": "1025308"
  },
  {
    "text": "and then lastly you just want to think",
    "start": "1025309",
    "end": "1027168"
  },
  {
    "text": "about your docker container and so with",
    "start": "1027169",
    "end": "1029269"
  },
  {
    "text": "that thank you very much I hope you",
    "start": "1029269",
    "end": "1030649"
  },
  {
    "text": "enjoyed this session and my name is",
    "start": "1030649",
    "end": "1032569"
  },
  {
    "text": "Emily Weber I'm a machine learning",
    "start": "1032569",
    "end": "1033918"
  },
  {
    "text": "specialist at Amazon Web Services and",
    "start": "1033919",
    "end": "1036038"
  },
  {
    "text": "you should have a good day",
    "start": "1036039",
    "end": "1039638"
  }
]