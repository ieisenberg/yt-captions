[
  {
    "text": "so hello everybody good afternoon thanks for making it all the way to the end of",
    "start": "30",
    "end": "5490"
  },
  {
    "text": "the day with me I appreciate that thanks everybody for coming in my name is John",
    "start": "5490",
    "end": "10679"
  },
  {
    "text": "handler I'm a Solutions Architect with Amazon Web Services and I've specialized on search so our two search products",
    "start": "10679",
    "end": "18060"
  },
  {
    "text": "cloud search and elastic search so quick scan because it's kind of I'd",
    "start": "18060",
    "end": "24900"
  },
  {
    "text": "like to know who in the room knows or uses elastic search lots of people that's great okay cool so",
    "start": "24900",
    "end": "34800"
  },
  {
    "text": "when these two slides are gonna make a lot of sense to you all so who in the room runs any kind of application",
    "start": "34800",
    "end": "41820"
  },
  {
    "text": "database server ec2 instance go ahead and raise our hands yes everybody does right and",
    "start": "41820",
    "end": "48980"
  },
  {
    "text": "these services applications etc spit out tons of log lines right now the problem is if",
    "start": "48980",
    "end": "57149"
  },
  {
    "text": "something happens and you have to go look at a log file like this or heaven forbid a terabyte of log files like this",
    "start": "57149",
    "end": "64619"
  },
  {
    "text": "what you have is you have trees you don't have forests you don't have any kind of view of what's going on we're",
    "start": "64619",
    "end": "70470"
  },
  {
    "text": "gonna be working with this data so let me introduce it just quickly this is turning on the wayback machine we're",
    "start": "70470",
    "end": "76500"
  },
  {
    "text": "going back to July of 1995 this is a public data set of the Nassau I think",
    "start": "76500",
    "end": "83189"
  },
  {
    "text": "it's the JPL website so this is an Apache weblog typical web log here we have hosts and we have times that we",
    "start": "83189",
    "end": "90000"
  },
  {
    "text": "have requests statuses size of the result that are returned so we're gonna",
    "start": "90000",
    "end": "95250"
  },
  {
    "text": "be working with this today and keep that in mind and keep in mind all the little parts what we really want is something",
    "start": "95250",
    "end": "102000"
  },
  {
    "text": "that is much more forest right not trees we want to have a nice dashboard that",
    "start": "102000",
    "end": "107070"
  },
  {
    "text": "shows us what's going on so that we can look at it quickly and monitor what's",
    "start": "107070",
    "end": "112079"
  },
  {
    "text": "happening and then even dig into it a little bit to figure out what's going on underneath so today I am going to",
    "start": "112079",
    "end": "120000"
  },
  {
    "text": "attempt the insane I'm going to build for you in 35 minutes I hope a full",
    "start": "120000",
    "end": "127020"
  },
  {
    "text": "end-to-end chain that will take in patchy web logs put them through Amazon",
    "start": "127020",
    "end": "133650"
  },
  {
    "text": "Kinesis firehose and which will deliver them to elasticsearch service will start",
    "start": "133650",
    "end": "138810"
  },
  {
    "text": "up a Cabana campanha dashboard we'll look at it we'll build some things will build the",
    "start": "138810",
    "end": "144960"
  },
  {
    "text": "dashboard that we showed and then we'll dig in a little bit and write some queries to query against the data and",
    "start": "144960",
    "end": "150740"
  },
  {
    "text": "see what we can figure out so we're gonna start with creating an",
    "start": "150740",
    "end": "157410"
  },
  {
    "text": "elastic search service domain quick show hands anybody use the Amazon elastic search service so some folks but",
    "start": "157410",
    "end": "164820"
  },
  {
    "text": "not everybody so people are familiar with elastic search but not necessarily elastic search service okay if",
    "start": "164820",
    "end": "171800"
  },
  {
    "text": "okay hey cool so what we have here is this is our Amazon elastic search",
    "start": "171800",
    "end": "178740"
  },
  {
    "text": "service dashboard and I have a couple of domains running and I am actually going",
    "start": "178740",
    "end": "183990"
  },
  {
    "text": "to create a new domain domain as a term that simply means an elastic search",
    "start": "183990",
    "end": "189780"
  },
  {
    "text": "cluster wrapped by Amazon services that make it easier to deploy manage",
    "start": "189780",
    "end": "195150"
  },
  {
    "text": "administer make highly available etc will see all of that so we call it a",
    "start": "195150",
    "end": "200400"
  },
  {
    "text": "domain though as a collective term for what's going on underneath so to create a domain first thing I need",
    "start": "200400",
    "end": "207600"
  },
  {
    "text": "to do of course give it a name and let's do what",
    "start": "207600",
    "end": "213080"
  },
  {
    "text": "and our 302 and then we need to set an",
    "start": "213080",
    "end": "218790"
  },
  {
    "text": "elastic search version as of now we support elastic search a version 2.3 and 1.5 we recommend everybody use 2.3 it's",
    "start": "218790",
    "end": "226140"
  },
  {
    "text": "a lot better than 1.5 if you're on 1.5 you should migrate forward to 2.3 so",
    "start": "226140",
    "end": "232440"
  },
  {
    "text": "we'll just pick 2.3 and go next so",
    "start": "232440",
    "end": "237620"
  },
  {
    "text": "now we need to configure our elastic search cluster we have a number of options and we're",
    "start": "237620",
    "end": "244050"
  },
  {
    "text": "gonna step through them all and launch a little bit and talk about sort of what's underneath the choices that we're going to make so the first thing that we're",
    "start": "244050",
    "end": "250560"
  },
  {
    "text": "gonna hit is the instance count and I work as a Solutions Architect I'll tell you quite frankly many many people are",
    "start": "250560",
    "end": "257100"
  },
  {
    "text": "confused about how many instances do I need and what type right so just as a",
    "start": "257100",
    "end": "262380"
  },
  {
    "text": "quick background we are going to touch on some of the basics of",
    "start": "262380",
    "end": "269020"
  },
  {
    "text": "elasticsearch I think everybody mostly knows what elasticsearch is seems like so I won't go into great depth but",
    "start": "269020",
    "end": "275730"
  },
  {
    "text": "elasticsearch is a search engine and what does a search engine do a search engine takes a big pile of stuff and",
    "start": "275730",
    "end": "282130"
  },
  {
    "text": "provides you with a means of sending queries against it and pulling out the best stuff for you ok so typical search",
    "start": "282130",
    "end": "288640"
  },
  {
    "text": "engine in a traditional setting would be something like e-commerce search or a website search or web page search these",
    "start": "288640",
    "end": "296440"
  },
  {
    "text": "are usual search use cases we know about and the the key feature is there's a big pile of stuff and the stuff is called",
    "start": "296440",
    "end": "303550"
  },
  {
    "text": "each item is called a document a document is a collection of fields and a",
    "start": "303550",
    "end": "308830"
  },
  {
    "text": "field is simply a named value pair so going back to our patching example I would have a field which would be the",
    "start": "308830",
    "end": "315940"
  },
  {
    "text": "host the requesting host and the value would be whatever was in the logline the",
    "start": "315940",
    "end": "321340"
  },
  {
    "text": "full log line would be one document documents are then taken in elasticsearch creates an index out of",
    "start": "321340",
    "end": "327730"
  },
  {
    "text": "them the index is has all of the documents within that you query the",
    "start": "327730",
    "end": "333340"
  },
  {
    "text": "index indexes are logically broken down into shards a sharda is a logical unit",
    "start": "333340",
    "end": "339400"
  },
  {
    "text": "which is a portion of the index but it's also a computational unit that gets deployed onto particular nodes in the",
    "start": "339400",
    "end": "346120"
  },
  {
    "text": "cluster so we can see that here I have in this case two indexes green and blue",
    "start": "346120",
    "end": "354790"
  },
  {
    "text": "they each have three shards every shard has a primary always primary is the",
    "start": "354790",
    "end": "360280"
  },
  {
    "text": "first one right you can also set replicas so in this case we set one replicas for redundancy and that's a",
    "start": "360280",
    "end": "367090"
  },
  {
    "text": "usual thing to do set a replicas for redundancy those elasticsearch will then",
    "start": "367090",
    "end": "372910"
  },
  {
    "text": "deploy to the cluster in such a way that the primary in the replicas never sit on the same node this gives again",
    "start": "372910",
    "end": "379840"
  },
  {
    "text": "redundancy so that if a node goes out we retain a copy of the data to be able to continue to work with it so thinking",
    "start": "379840",
    "end": "387340"
  },
  {
    "text": "about how many instances what we want to do is understand that we're going to be",
    "start": "387340",
    "end": "393730"
  },
  {
    "text": "deploying these indexes onto nodes and the index itself be stored on the note",
    "start": "393730",
    "end": "398770"
  },
  {
    "text": "so we need to have enough storage to hold what the index is right so how do",
    "start": "398770",
    "end": "404980"
  },
  {
    "text": "we figure out how much storage we need well the documents come in they're turned into an index that's a different",
    "start": "404980",
    "end": "411160"
  },
  {
    "text": "data structure but happily there about 1.1 to 1 in empirical best in empirical",
    "start": "411160",
    "end": "417010"
  },
  {
    "text": "practice so generally speaking how much source data you have is how much index data you have and that's a really Wiggly",
    "start": "417010",
    "end": "423400"
  },
  {
    "text": "kind of you know okay we'll just kind of say it's about that right if I have a terabyte of in that source I'll have",
    "start": "423400",
    "end": "429730"
  },
  {
    "text": "about a terabyte of index so if we're replicating we need to store it again so",
    "start": "429730",
    "end": "435070"
  },
  {
    "text": "we double right and then we figure out how many instances based on how much",
    "start": "435070",
    "end": "440710"
  },
  {
    "text": "storage we need if I have for example a two terabyte corpus we figure okay that's about four terabytes of index and",
    "start": "440710",
    "end": "447660"
  },
  {
    "text": "we need to store that four terabytes if I have say 512 gigabytes per node I need",
    "start": "447660",
    "end": "453550"
  },
  {
    "text": "eight nodes pretty simple it's an easy place to start it's not the whole picture by a long stretch but to get",
    "start": "453550",
    "end": "460450"
  },
  {
    "text": "yourself started that's kind of where you want to go so let's pop back over here",
    "start": "460450",
    "end": "467580"
  },
  {
    "text": "and so now the other thing to notice of course is one instance you can't deploy",
    "start": "467580",
    "end": "473320"
  },
  {
    "text": "a primary replica on different nodes right so you need to to have any kind of redundancy so we're gonna pick two",
    "start": "473320",
    "end": "478930"
  },
  {
    "text": "because the NASA data is only about 200 megabytes it doesn't need that much we",
    "start": "478930",
    "end": "483940"
  },
  {
    "text": "now have our instance type and we support a number of different instances t2 sm-3s r3 s and i2 s the quick guide",
    "start": "483940",
    "end": "492910"
  },
  {
    "text": "to this is to say T 2's are great for Devon test but don't use them for production em threes are general well",
    "start": "492910",
    "end": "500950"
  },
  {
    "text": "balanced machines they work really well but they only go to two X large so pick em threes for most workloads if you need",
    "start": "500950",
    "end": "508530"
  },
  {
    "text": "larger instances because you have heavier queries they're going to go to the R threes and we go up to eight X",
    "start": "508530",
    "end": "513940"
  },
  {
    "text": "large they're the I Tunes you would pick because they have the most storage so they have 1.6",
    "start": "513940",
    "end": "520599"
  },
  {
    "text": "terabytes of instance store in this case we just need em threes",
    "start": "520599",
    "end": "526510"
  },
  {
    "text": "so so far we have two m3 medium data nodes okay we are also able to enable",
    "start": "526510",
    "end": "533769"
  },
  {
    "text": "dedicated masters what are dedicated masters ooh I hit the wrong thing hopefully I did",
    "start": "533769",
    "end": "541660"
  },
  {
    "text": "that right yeah okay so in every elasticsearch cluster you have a master",
    "start": "541660",
    "end": "547329"
  },
  {
    "text": "node the master node is responsible for knowing all the nodes in the cluster for pushing around config and for doing what",
    "start": "547329",
    "end": "553660"
  },
  {
    "text": "the cluster needs to do so the problem is the master is potentially a data node",
    "start": "553660",
    "end": "559600"
  },
  {
    "text": "now the data node is furiously reading and writing data so it can become occupied and the master function then",
    "start": "559600",
    "end": "566500"
  },
  {
    "text": "disappears because there's no CPU to do the master function so that's bad the",
    "start": "566500",
    "end": "571779"
  },
  {
    "text": "fix is to select particular nodes and to move the master function onto those nodes only so you pick dedicated master",
    "start": "571779",
    "end": "580420"
  },
  {
    "text": "nodes in order to take the master function and make it solely deployed on a set of nodes this gives the cluster a",
    "start": "580420",
    "end": "587320"
  },
  {
    "text": "lot more stability and the right number of master nodes to pick is three and the",
    "start": "587320",
    "end": "592500"
  },
  {
    "text": "reason it's the right number is because masters are elected based on quorum if you have an even number and then the",
    "start": "592500",
    "end": "599560"
  },
  {
    "text": "cluster splits each half can then elect a master that's not good then you have",
    "start": "599560",
    "end": "604720"
  },
  {
    "text": "two clusters where there was one that's terrible so three is the right number you don't want one that's too few you",
    "start": "604720",
    "end": "611199"
  },
  {
    "text": "want an odd number more than one that's three okay I'm gonna have to speed up",
    "start": "611199",
    "end": "619350"
  },
  {
    "text": "so the master nodes do lot less than the data nodes so they can be smaller in",
    "start": "619350",
    "end": "624880"
  },
  {
    "text": "this case we can use the t2s don't use the micros but you can use smalls or mediums",
    "start": "624880",
    "end": "630660"
  },
  {
    "text": "next feature we have is zone awareness zone awareness is a high availability feature this splits your cluster into",
    "start": "630660",
    "end": "637420"
  },
  {
    "text": "two separate availability zones and makes primaries and replicas go into different zones in other words you split",
    "start": "637420",
    "end": "643690"
  },
  {
    "text": "your cluster evenly into two zones except the masters there are three of them so we're going to go ahead and",
    "start": "643690",
    "end": "649480"
  },
  {
    "text": "enable that we support both instance and EBS store",
    "start": "649480",
    "end": "655709"
  },
  {
    "text": "the instance store obviously you get the storage based on the instance type for EBS again based on the instance type we",
    "start": "655709",
    "end": "662439"
  },
  {
    "text": "go up to 512 gigabytes EBS is a fine choice I have yet to see performance",
    "start": "662439",
    "end": "668949"
  },
  {
    "text": "problems with EBS and I've talked to a lot of customers who use EBS it seems to",
    "start": "668949",
    "end": "674410"
  },
  {
    "text": "be just fine I don't recommend the magnetic volumes so choose EBS choose GP 2s",
    "start": "674410",
    "end": "682509"
  },
  {
    "text": "general purpose SSDs you should be fine and for the volume size in this case",
    "start": "682509",
    "end": "688600"
  },
  {
    "text": "again 200 megabytes 10 gigabytes should be plenty I do have to quickly touch on",
    "start": "688600",
    "end": "695949"
  },
  {
    "text": "this so we have this advanced feature which is indices field data cache size",
    "start": "695949",
    "end": "701139"
  },
  {
    "text": "this controls how much data is actually cached when you run queries essentially",
    "start": "701139",
    "end": "706959"
  },
  {
    "text": "its unbounded by default which is kind of dangerous you brought a few big queries and you'll knock your cluster",
    "start": "706959",
    "end": "712779"
  },
  {
    "text": "over so you need to have a number there and you want a balance between leaving some Ram for the rest of the",
    "start": "712779",
    "end": "718919"
  },
  {
    "text": "elasticsearch to do stuff and making enough caching so that you get",
    "start": "718919",
    "end": "724059"
  },
  {
    "text": "the performance benefit our recommendation for that is just to set it at forty percent so forty percent",
    "start": "724059",
    "end": "729459"
  },
  {
    "text": "means forty percent of the ram will be used for caching okay so",
    "start": "729459",
    "end": "735989"
  },
  {
    "text": "we now set our security or access policy so we provide security through I am you",
    "start": "735989",
    "end": "743049"
  },
  {
    "text": "set a policy that can either be based on a user's identity and you'll use signed requests or an IP address in which case",
    "start": "743049",
    "end": "751449"
  },
  {
    "text": "it can be anonymous we have a couple of templates here I actually I'm going to",
    "start": "751449",
    "end": "757059"
  },
  {
    "text": "make it open not that you should ever do this but I'm making mine open to the world and that's it so we have then a",
    "start": "757059",
    "end": "766660"
  },
  {
    "text": "chance to review our information and confirm and create it",
    "start": "766660",
    "end": "771989"
  },
  {
    "text": "okay so handler 302 goes into a loading state it'll be about 5 to 10 minutes",
    "start": "771989",
    "end": "778379"
  },
  {
    "text": "until it actually comes up and let us quick leave",
    "start": "778379",
    "end": "785579"
  },
  {
    "text": "recap sort of what we talked about here so to get started",
    "start": "785579",
    "end": "791769"
  },
  {
    "text": "a set of data nodes which is based on the amount of storage you need per node then use the GP to EBS volumes use three",
    "start": "791769",
    "end": "800709"
  },
  {
    "text": "dedicated masters we do want to use zone awareness that splits our cluster that's",
    "start": "800709",
    "end": "805809"
  },
  {
    "text": "nice and set the indices field data cache size to 40",
    "start": "805809",
    "end": "811410"
  },
  {
    "text": "and what we're going to get it within the service we deploy a set of nodes",
    "start": "811410",
    "end": "817809"
  },
  {
    "text": "that is your elastic search cluster so at the core there we have those nodes are managed nodes that are running",
    "start": "817809",
    "end": "824019"
  },
  {
    "text": "elastic search they also have Cabana on them so that you can run Cabana easily",
    "start": "824019",
    "end": "829629"
  },
  {
    "text": "we have load balancing in front of the cluster and we publish a DNS entry which",
    "start": "829629",
    "end": "834730"
  },
  {
    "text": "is a single endpoint where you can use the elastic search REST API to communicate with the cluster it's all",
    "start": "834730",
    "end": "840790"
  },
  {
    "text": "gated by I am so I am provides you with the security model and then we do monitoring we do metrics for monitoring",
    "start": "840790",
    "end": "848619"
  },
  {
    "text": "to cloud watch and then cloud trail as well for audit purposes",
    "start": "848619",
    "end": "855119"
  },
  {
    "text": "and I won't belabor the benefits but some of the benefits that we get with",
    "start": "855119",
    "end": "861670"
  },
  {
    "text": "the service number one easy to use as you saw quickly click a click I get a domain and I get a cluster in addition I",
    "start": "861670",
    "end": "870429"
  },
  {
    "text": "can actually change any of the parameters that I said by making another call to update the cluster configuration",
    "start": "870429",
    "end": "876129"
  },
  {
    "text": "so if I want to go from m3s to r3 s I can make a single API call the service",
    "start": "876129",
    "end": "881499"
  },
  {
    "text": "will then bring in the new nodes get the data transferred over we provide again that's the scalability",
    "start": "881499",
    "end": "888850"
  },
  {
    "text": "we make it very easy to scale up we have the zone awareness feature as our highly",
    "start": "888850",
    "end": "895540"
  },
  {
    "text": "available feature we also we are open source compatible so this is a guiding",
    "start": "895540",
    "end": "901419"
  },
  {
    "text": "principle that we've had really since we started building the service we wanted to build something that would be a",
    "start": "901419",
    "end": "906519"
  },
  {
    "text": "drop-in replacement for an existing elasticsearch installation make it really easy to use it is compatible with",
    "start": "906519",
    "end": "913179"
  },
  {
    "text": "logstash and it also of course includes cubano we then provide security with I am so",
    "start": "913179",
    "end": "921549"
  },
  {
    "text": "you can secure your domain in fact you have your IP based and your user base ways of doing that we also provide",
    "start": "921549",
    "end": "928259"
  },
  {
    "text": "fine-grained access control down to the index level and also for the HTTP method",
    "start": "928259",
    "end": "935939"
  },
  {
    "text": "and then finally we have a lot of integration with AWS services so we have",
    "start": "935939",
    "end": "941709"
  },
  {
    "text": "firehose we're gonna see in a minute we're also integrated with cloud watch Logs with a few clicks in the console",
    "start": "941709",
    "end": "947589"
  },
  {
    "text": "you can send your data from cloud watch logs to elastic search service ok so we're gonna switch topics now so",
    "start": "947589",
    "end": "956589"
  },
  {
    "text": "we've created our elastic search domain now we're going to create our Kinesis fire hose and hook it up to our elastic",
    "start": "956589",
    "end": "962170"
  },
  {
    "text": "search domain so who is already using fire hose or knows about fire hose lots",
    "start": "962170",
    "end": "969610"
  },
  {
    "text": "of folks - that's great I actually love fire hose I think fire hose provides a lot of really great features in the",
    "start": "969610",
    "end": "975699"
  },
  {
    "text": "context of elastic search service just some of the concepts when you use",
    "start": "975699",
    "end": "981069"
  },
  {
    "text": "fire hose you create a delivery stream that delivery stream can have a death",
    "start": "981069",
    "end": "986079"
  },
  {
    "text": "has a destination the destination can be s3 redshift or elasticsearch then you",
    "start": "986079",
    "end": "993069"
  },
  {
    "text": "put your records in the stream with a rest call it's just an HTTP put and fire",
    "start": "993069",
    "end": "998980"
  },
  {
    "text": "hose delivers those records directly to the to the destination so that's",
    "start": "998980",
    "end": "1005459"
  },
  {
    "text": "visualized here so we have our data source sending source records into firehose and then those get sent to the",
    "start": "1005459",
    "end": "1013589"
  },
  {
    "text": "destination in our case elasticsearch service one of the things fire hose does for you is it it sends failed records to",
    "start": "1013589",
    "end": "1021449"
  },
  {
    "text": "s3 so fire hose has retries if records fail it'll write those off to s3 so you",
    "start": "1021449",
    "end": "1026639"
  },
  {
    "text": "have a record of the things that have failed also it'll just send all of the records to s3 so if you're already",
    "start": "1026639",
    "end": "1033298"
  },
  {
    "text": "sending your logs to s3 actually you can send them through firehose go send them straight to s3 and also elasticsearch so",
    "start": "1033299",
    "end": "1040620"
  },
  {
    "text": "you have a backup of what's going on and as of now fire host sends data",
    "start": "1040620",
    "end": "1048000"
  },
  {
    "text": "straight through but i'm really happy to announce today that coming real soon",
    "start": "1048000",
    "end": "1053179"
  },
  {
    "text": "fire hose will allow you to transform the records as they're going through the",
    "start": "1053179",
    "end": "1059220"
  },
  {
    "text": "firehose stream so in this case you'd send in your source records they go to",
    "start": "1059220",
    "end": "1064410"
  },
  {
    "text": "the delivery stream you'll write a lambda function or pointed at a lambda function that exists that lambda",
    "start": "1064410",
    "end": "1071280"
  },
  {
    "text": "function will transforms the records so and then sends it back to the stream the",
    "start": "1071280",
    "end": "1076620"
  },
  {
    "text": "stream then sends them through as it normally would we will have at launch",
    "start": "1076620",
    "end": "1081630"
  },
  {
    "text": "will have blueprints for sis logs and Apache web",
    "start": "1081630",
    "end": "1087150"
  },
  {
    "text": "logs so and also kind of a couple getting started ones so as as that when",
    "start": "1087150",
    "end": "1094050"
  },
  {
    "text": "that rolls out it'll be easy to just push your logs straight into firehose and then our fire hose can actually",
    "start": "1094050",
    "end": "1101610"
  },
  {
    "text": "monitor your log files for you send them straight through the elastic search",
    "start": "1101610",
    "end": "1106730"
  },
  {
    "text": "again the for ingest I think fire hose is great so number one server lists and",
    "start": "1106730",
    "end": "1112320"
  },
  {
    "text": "scales infinitely essentially the error handling feature is really quite quite",
    "start": "1112320",
    "end": "1119040"
  },
  {
    "text": "nice like not to have to worry about retrying records it does the retries for you tracking where a failed records went",
    "start": "1119040",
    "end": "1125340"
  },
  {
    "text": "so that you can then retry them yourself are both really nice features and the s3 backup is also a really nice feature",
    "start": "1125340",
    "end": "1131610"
  },
  {
    "text": "like to be able to send everything and have a record of it okay so we're gonna quick create a",
    "start": "1131610",
    "end": "1138540"
  },
  {
    "text": "firehose stream",
    "start": "1138540",
    "end": "1141860"
  },
  {
    "text": "awesome remember to turn off your reminders okay so this is the firehose",
    "start": "1145760",
    "end": "1151910"
  },
  {
    "text": "dashboard and I'm going to go ahead and create a delivery stream",
    "start": "1151910",
    "end": "1156980"
  },
  {
    "text": "first thing I do is select a destination I'm gonna send to elasticsearch service",
    "start": "1156980",
    "end": "1162090"
  },
  {
    "text": "I have to give it a name and I like to give it the same name as my domain that",
    "start": "1162090",
    "end": "1169200"
  },
  {
    "text": "way when I'm cleaning up later I can figure out which stream goes with which with which elasticsearch domain okay so",
    "start": "1169200",
    "end": "1177210"
  },
  {
    "text": "I set elasticsearch as my destination so I have to select an elastic search",
    "start": "1177210",
    "end": "1182430"
  },
  {
    "text": "domain in this case Handler 302 I'm going to set an index pattern here",
    "start": "1182430",
    "end": "1187770"
  },
  {
    "text": "so fire hose will write to elastic search create indices based on a time period",
    "start": "1187770",
    "end": "1195240"
  },
  {
    "text": "rotation okay I have to give it the base pattern for the index in this case I",
    "start": "1195240",
    "end": "1200880"
  },
  {
    "text": "just use firehose and I set my rotation period to be an hour day week month one",
    "start": "1200880",
    "end": "1207390"
  },
  {
    "text": "day is the usual choice and it's good in the 250 gigabyte range",
    "start": "1207390",
    "end": "1213320"
  },
  {
    "text": "250 500 gigabytes somewhere in there is a fine choice for daily rotation in this",
    "start": "1213320",
    "end": "1219570"
  },
  {
    "text": "case fire hose would create fire hose - November 29 2016 fire hose - November",
    "start": "1219570",
    "end": "1226650"
  },
  {
    "text": "30th 2016 etc right if you have a lot of data up above a terabyte or two per day",
    "start": "1226650",
    "end": "1233790"
  },
  {
    "text": "you can go down to an hourly rotation and if you'd have less data than that",
    "start": "1233790",
    "end": "1238950"
  },
  {
    "text": "you can go to the weekly or monthly rotation okay",
    "start": "1238950",
    "end": "1246440"
  },
  {
    "text": "with an elastic search we have a concept of a type I really don't want to like stress on this the type is simply a way",
    "start": "1246710",
    "end": "1253530"
  },
  {
    "text": "for one index to contain multiple schemas not really important but you do have to fill in a value here so just put",
    "start": "1253530",
    "end": "1260010"
  },
  {
    "text": "log the retry duration is the amount of time that fire hose will continue to retry",
    "start": "1260010",
    "end": "1267600"
  },
  {
    "text": "records and try to get them into the elastic search domain 300 seconds is fine 5 minutes we have some choices",
    "start": "1267600",
    "end": "1274890"
  },
  {
    "text": "about our backup mode we can send again fail documents only or all documents",
    "start": "1274890",
    "end": "1280530"
  },
  {
    "text": "will stick with fail documents and then I pick a bucket I have an existing fire",
    "start": "1280530",
    "end": "1286530"
  },
  {
    "text": "hose failed deliveries bucket and I can add a prefix to all the records that are sent there so that way I know what their",
    "start": "1286530",
    "end": "1295110"
  },
  {
    "text": "source was okay so I now have some configuration",
    "start": "1295110",
    "end": "1301590"
  },
  {
    "text": "options so the really important one here is the buffer size and the buffer the",
    "start": "1301590",
    "end": "1307110"
  },
  {
    "text": "buffer duration interval if you set the buffer size small then",
    "start": "1307110",
    "end": "1313680"
  },
  {
    "text": "you get the fastest flush through the pipe presuming you're not already",
    "start": "1313680",
    "end": "1318690"
  },
  {
    "text": "filling it right so this will cause firehose to flush every megabyte or",
    "start": "1318690",
    "end": "1324720"
  },
  {
    "text": "every 60 seconds whichever comes first if you're already way past the megabyte it'll flush whenever you hit a megabyte",
    "start": "1324720",
    "end": "1330840"
  },
  {
    "text": "so that's great that gives you the lowest latency on the other hand if you're if you're really pushing a lot of",
    "start": "1330840",
    "end": "1337379"
  },
  {
    "text": "data in you're going to be flushing quite a bit and the more you flush the more concurrent connections you have to",
    "start": "1337379",
    "end": "1342720"
  },
  {
    "text": "hold to elasticsearch so there's a balance here between bigger buffers",
    "start": "1342720",
    "end": "1348690"
  },
  {
    "text": "which will give you bigger throughput and smaller buffers which give you higher latency for us 160 is great 5 300",
    "start": "1348690",
    "end": "1358409"
  },
  {
    "text": "is also great probably into the same 250 to 500 gigabyte range",
    "start": "1358409",
    "end": "1364009"
  },
  {
    "text": "ok we can compress and encrypt data in s3 which I'm not going to do and we can",
    "start": "1364009",
    "end": "1369539"
  },
  {
    "text": "enable error logging going to do that I have to create an iamb role that it will allow firehose to talk to the",
    "start": "1369539",
    "end": "1375539"
  },
  {
    "text": "elasticsearch cluster and I'm gonna do that this will bounce me to the iam panel",
    "start": "1375539",
    "end": "1384230"
  },
  {
    "text": "here you have a choice to reuse an existing role don't do that the role",
    "start": "1385070",
    "end": "1390960"
  },
  {
    "text": "itself gets a policy document that allows it to write to a domain if you do that a bunch of times the policy",
    "start": "1390960",
    "end": "1396720"
  },
  {
    "text": "document overflows and you can't do it so always create a new role and I leave",
    "start": "1396720",
    "end": "1402570"
  },
  {
    "text": "the firehose delivery there and again I embed my domain name and my firehose",
    "start": "1402570",
    "end": "1407609"
  },
  {
    "text": "stream name because then I can clean this guy up too all right",
    "start": "1407609",
    "end": "1412940"
  },
  {
    "text": "so I allow that alright",
    "start": "1412940",
    "end": "1420210"
  },
  {
    "text": "and I go next I have a review screen and I create",
    "start": "1420210",
    "end": "1429590"
  },
  {
    "text": "okay cool success so now we have our elastic",
    "start": "1429590",
    "end": "1434909"
  },
  {
    "text": "search service domain we have our firehose pointing at it and now we need to well first we actually need to quick",
    "start": "1434909",
    "end": "1441900"
  },
  {
    "text": "look at a couple of the best practices that we covered in that little segment so in this case again there's a buffer a",
    "start": "1441900",
    "end": "1450809"
  },
  {
    "text": "buffering balance that you have to work between latency and total throughput",
    "start": "1450809",
    "end": "1458030"
  },
  {
    "text": "we are going to use index rotation so for log analytics use cases we always",
    "start": "1458030",
    "end": "1463320"
  },
  {
    "text": "want to use index rotation daily as the usual value and then just to keep in the",
    "start": "1463320",
    "end": "1469530"
  },
  {
    "text": "back of your head the default stream limit is 2,000 transactions a second or 5,000 records five megabytes a second",
    "start": "1469530",
    "end": "1477059"
  },
  {
    "text": "you can get a limit raise on that and you can go higher much higher",
    "start": "1477059",
    "end": "1482929"
  },
  {
    "text": "okay so now we have kind of the pieces hooked together but we have to prepare",
    "start": "1482929",
    "end": "1488870"
  },
  {
    "text": "elasticsearch to be able to parse and read the log lines and get them deployed",
    "start": "1488870",
    "end": "1495270"
  },
  {
    "text": "correctly so the first thing we want to talk about is the number of shards we",
    "start": "1495270",
    "end": "1501090"
  },
  {
    "text": "saw in the first diagram an index is logically decomposed into shards well how many shards should I use for my",
    "start": "1501090",
    "end": "1506610"
  },
  {
    "text": "index it's another question that is one of these imponderables there's a very",
    "start": "1506610",
    "end": "1511679"
  },
  {
    "text": "deep and long answer but again there's also a simple answer that kind of is good enough in most cases and that is to",
    "start": "1511679",
    "end": "1518880"
  },
  {
    "text": "look at the number of shards as being the total index size divided by 30 gigabytes this gives you about 30",
    "start": "1518880",
    "end": "1525390"
  },
  {
    "text": "gigabytes as a target amount per shard just storage wise that's about where you",
    "start": "1525390",
    "end": "1531240"
  },
  {
    "text": "want to be so if I have 300 gigabytes of index I want 10 shards for that okay and",
    "start": "1531240",
    "end": "1538789"
  },
  {
    "text": "less is more so it's tempting to say well one shard could never be enough but",
    "start": "1538789",
    "end": "1545309"
  },
  {
    "text": "actually one shard will take up all the resources of the the node that it's on so it's okay if you have less than 30",
    "start": "1545309",
    "end": "1551789"
  },
  {
    "text": "gigabytes to stick with ha sharp you do want a replica but we're talking primaries here",
    "start": "1551789",
    "end": "1557120"
  },
  {
    "text": "and you set the sharding when you create the index right so if you when the index",
    "start": "1557120",
    "end": "1564480"
  },
  {
    "text": "gets created you set the sharding you can't change that value later right so it's important to more or less get it",
    "start": "1564480",
    "end": "1570870"
  },
  {
    "text": "right for log use cases where you're rolling forward new indices can actually",
    "start": "1570870",
    "end": "1576150"
  },
  {
    "text": "have different charting but once you set it it's done another thing to keep in",
    "start": "1576150",
    "end": "1581220"
  },
  {
    "text": "mind is the computational nature of shards in that when you're writing data to a shard that",
    "start": "1581220",
    "end": "1588870"
  },
  {
    "text": "data is occupying a CPU when you're reading a query you actually read from all the shards in the index right so",
    "start": "1588870",
    "end": "1596520"
  },
  {
    "text": "when you're putting your when you're putting your sharding together think about how many CPUs are in your cluster",
    "start": "1596520",
    "end": "1601860"
  },
  {
    "text": "if you're writing a thousand documents per second then and you have three",
    "start": "1601860",
    "end": "1607920"
  },
  {
    "text": "shards roughly 300 documents are getting written to every shard right so then you",
    "start": "1607920",
    "end": "1613500"
  },
  {
    "text": "have those guys are active you have to have enough CPUs to maintain the concurrency that's coming in and going",
    "start": "1613500",
    "end": "1618540"
  },
  {
    "text": "out okay the other thing we get we set at index",
    "start": "1618540",
    "end": "1623820"
  },
  {
    "text": "time is we set the mapping so elasticsearch again is a search engine and",
    "start": "1623820",
    "end": "1629330"
  },
  {
    "text": "when we send in our documents with our fields and data elasticsearch parses those fields and applies a mapping a",
    "start": "1629330",
    "end": "1636840"
  },
  {
    "text": "schema to that data so in particular it's important that we apply the right",
    "start": "1636840",
    "end": "1643790"
  },
  {
    "text": "schema where we're gonna get gobbledygook in our graphs so if I have",
    "start": "1643790",
    "end": "1649100"
  },
  {
    "text": "something like a host that's a dotted string I don't want to cut that up into",
    "start": "1649100",
    "end": "1654120"
  },
  {
    "text": "individual pieces and if I try to graph that I'm gonna get individual numbers out of IP addresses that's not",
    "start": "1654120",
    "end": "1660420"
  },
  {
    "text": "meaningful so what we need to do is set not analyzed for all text fields okay",
    "start": "1660420",
    "end": "1666230"
  },
  {
    "text": "also again this is something that we do at index creation time now if I'm",
    "start": "1666230",
    "end": "1671910"
  },
  {
    "text": "creating a new index every day because I'm pumping tons of data in there I don't want to manually go and have to",
    "start": "1671910",
    "end": "1677970"
  },
  {
    "text": "set up my my schema and my sharding every single time for the new index right there is a feature in elastic",
    "start": "1677970",
    "end": "1684240"
  },
  {
    "text": "search is called the template you use a template to set a regular expression a wildcard to match any index",
    "start": "1684240",
    "end": "1691200"
  },
  {
    "text": "and then the settings and mapping can get applied based on matching that index happily firehose is creating indices",
    "start": "1691200",
    "end": "1698490"
  },
  {
    "text": "with a pattern right so we'll be able to set up a template to match what firehose",
    "start": "1698490",
    "end": "1704520"
  },
  {
    "text": "is sending to us we can also define the number of shards in the template so let me",
    "start": "1704520",
    "end": "1711110"
  },
  {
    "text": "quick pop over here",
    "start": "1711110",
    "end": "1714920"
  },
  {
    "text": "okay so here is our template and you can see the first thing it does is matches",
    "start": "1722850",
    "end": "1728759"
  },
  {
    "text": "fire host star right so all of those indices that I create that are a fire hose November 29 2016 all of those will",
    "start": "1728759",
    "end": "1734639"
  },
  {
    "text": "match and then I have a set of settings in my settings I set the number of",
    "start": "1734639",
    "end": "1740039"
  },
  {
    "text": "shards to one because it's a small use case I set the number of replicas to one because we always want one for",
    "start": "1740039",
    "end": "1745950"
  },
  {
    "text": "redundancies and then I defined a custom analyzer which we'll talk about in a second for our mappings we then put it",
    "start": "1745950",
    "end": "1754919"
  },
  {
    "text": "put the type log and then we can use another elasticsearch templating feature which is called dynamic templates this",
    "start": "1754919",
    "end": "1761580"
  },
  {
    "text": "matches fields based on a template and my match they are a star you can see so",
    "start": "1761580",
    "end": "1767610"
  },
  {
    "text": "what I'm saying is match every field and then apply this mapping the mapping is",
    "start": "1767610",
    "end": "1773129"
  },
  {
    "text": "not analyzed so I'm turning off analysis for every field right and I have a",
    "start": "1773129",
    "end": "1778500"
  },
  {
    "text": "couple of other things in there which are less important I can also override the template for",
    "start": "1778500",
    "end": "1787080"
  },
  {
    "text": "particular fields so in this case we're gonna have a time stamp field which is going to be a date that's important for cabaña so cabaña can visualize things in",
    "start": "1787080",
    "end": "1793710"
  },
  {
    "text": "timelines right we also have the request field I'm overriding so that I do parse",
    "start": "1793710",
    "end": "1799409"
  },
  {
    "text": "that up and I get pieces of the request and I'm using my custom analyzer here with a copy of the request actually",
    "start": "1799409",
    "end": "1805799"
  },
  {
    "text": "sorry the request is not analyzed my copy of it is analyzed so I'll get essentially the path will be decomposed",
    "start": "1805799",
    "end": "1812820"
  },
  {
    "text": "and the first prefix will go and then the prefix plus the first path element",
    "start": "1812820",
    "end": "1818159"
  },
  {
    "text": "plus the next path element so I'll get a series of tokens which are the pieces of the path we'll use those just to get",
    "start": "1818159",
    "end": "1824309"
  },
  {
    "text": "some nice visualizations and data okay so what I need to do is send that",
    "start": "1824309",
    "end": "1830850"
  },
  {
    "text": "over to my elasticsearch domain so and to do that I have to see whether",
    "start": "1830850",
    "end": "1837779"
  },
  {
    "text": "I have an endpoint yet I do okay so here's my domain it tells me my endpoint",
    "start": "1837779",
    "end": "1843529"
  },
  {
    "text": "and I can send my template with a put",
    "start": "1843529",
    "end": "1851190"
  },
  {
    "text": "I used the template API and I have to give",
    "start": "1851190",
    "end": "1857280"
  },
  {
    "text": "it a name template one doesn't matter what the name is and then I send in my web logs",
    "start": "1857280",
    "end": "1864440"
  },
  {
    "text": "template dot text okay so elastic search acknowledges it's",
    "start": "1864530",
    "end": "1871470"
  },
  {
    "text": "got the template you have to set the template before you send any data because when you send data the index",
    "start": "1871470",
    "end": "1876780"
  },
  {
    "text": "gets created so it's important to send your template first okay so",
    "start": "1876780",
    "end": "1884120"
  },
  {
    "text": "let's have another quick look at the data and just so we can understand what we are going to be visualizing at the",
    "start": "1886100",
    "end": "1893580"
  },
  {
    "text": "top there I have a line of the original Apache log file and then on the bottom is an elastic search document this is",
    "start": "1893580",
    "end": "1900960"
  },
  {
    "text": "the format the data has to go into elastic search you can see we have fields there like status ident timestamp",
    "start": "1900960",
    "end": "1907680"
  },
  {
    "text": "request auth host verb etc so all I did",
    "start": "1907680",
    "end": "1912750"
  },
  {
    "text": "is I wrote a little script that parses up the NASA log data a line at a time",
    "start": "1912750",
    "end": "1919080"
  },
  {
    "text": "and generates lines that look like that so that they can go into firehose all the while keeping in the back of my mind",
    "start": "1919080",
    "end": "1925050"
  },
  {
    "text": "that we are going to be releasing the lambda feature for a firehose that's going to make all of this part really easy",
    "start": "1925050",
    "end": "1932300"
  },
  {
    "text": "just to give you a quick view of it this is really all that it does opens the",
    "start": "1932540",
    "end": "1938940"
  },
  {
    "text": "file reads the lines it parses the line and then it appends it to a list that",
    "start": "1938940",
    "end": "1947790"
  },
  {
    "text": "I'm maintaining I can use the firehose batch put API I send 500 of these at a",
    "start": "1947790",
    "end": "1954450"
  },
  {
    "text": "time and that's it that's all that script does",
    "start": "1954450",
    "end": "1960650"
  },
  {
    "text": "so if I come back over here no I wasn't supposed to have I was so",
    "start": "1960650",
    "end": "1966720"
  },
  {
    "text": "anxious to upload my template I did it early okay so we can look quickly",
    "start": "1966720",
    "end": "1974840"
  },
  {
    "text": "just again give you an idea there's my regular expression that I used to match the the line I convert I",
    "start": "1976770",
    "end": "1985050"
  },
  {
    "text": "do some time conversion and actually I can time shifts into the present parse the line flush the records here's",
    "start": "1985050",
    "end": "1994050"
  },
  {
    "text": "the put record batch that's it right so I am going to",
    "start": "1994050",
    "end": "2001600"
  },
  {
    "text": "stream name and we're a 302 and input",
    "start": "2004360",
    "end": "2010190"
  },
  {
    "text": "file now so access log July 95 good",
    "start": "2010190",
    "end": "2018309"
  },
  {
    "text": "and hopefully good okay so we can see we're now actually pushing records into",
    "start": "2018309",
    "end": "2025460"
  },
  {
    "text": "firehose it will take about well it'll take a",
    "start": "2025460",
    "end": "2030890"
  },
  {
    "text": "little while for them to start showing up and in the meantime again just to",
    "start": "2030890",
    "end": "2036440"
  },
  {
    "text": "review a couple of the best practices that we hit so use a template for settings that way every time an index is",
    "start": "2036440",
    "end": "2043040"
  },
  {
    "text": "created it gets the settings that you want i set the number of shards based on 30 gigabytes of shard as a target in the",
    "start": "2043040",
    "end": "2049878"
  },
  {
    "text": "best case you have one shard per node 1 active shard per node also works and",
    "start": "2049879",
    "end": "2055760"
  },
  {
    "text": "then for analysis use cases you want to use not analyzed on your text fields okay",
    "start": "2055760",
    "end": "2062950"
  },
  {
    "text": "so again we are going to do some Kabana",
    "start": "2062950",
    "end": "2068030"
  },
  {
    "text": "analysis of the web logs that are flowing in there now in order to do that first of all we have",
    "start": "2068030",
    "end": "2074690"
  },
  {
    "text": "to again jump in the wayback machine so one thing I happen to know about July of 1995 is that there were two shuttle",
    "start": "2074690",
    "end": "2081260"
  },
  {
    "text": "missions so there was one shuttle mission that was flying from late June",
    "start": "2081260",
    "end": "2086540"
  },
  {
    "text": "through July 5th and there was a launch on July 13th ok so hopefully as we look",
    "start": "2086540",
    "end": "2092358"
  },
  {
    "text": "at this data and as we dig into it we should see some changes in traffic and some",
    "start": "2092359",
    "end": "2097390"
  },
  {
    "text": "differences in what people are doing on those dates ok that's what we're hoping",
    "start": "2097390",
    "end": "2102830"
  },
  {
    "text": "to find the the concept the overarching concept for elasticsearch aggregations that you have to understand is buckets",
    "start": "2102830",
    "end": "2109460"
  },
  {
    "text": "and metrics so elasticsearch enables you to aggregate based on buckets of values",
    "start": "2109460",
    "end": "2116380"
  },
  {
    "text": "so we have a simple aggregation here which is a time bucket right what I've",
    "start": "2116380",
    "end": "2121730"
  },
  {
    "text": "done is I take all of my documents and I bucket them based on their timestamp",
    "start": "2121730",
    "end": "2126790"
  },
  {
    "text": "okay then I can compute a metric on that bucket so in this case my metric is",
    "start": "2126790",
    "end": "2132290"
  },
  {
    "text": "count I take I just count everything that's in each of those buckets so when I display this graph what I'm seeing is",
    "start": "2132290",
    "end": "2139010"
  },
  {
    "text": "this is the bucket of each of these time slices and this is how many things are in them everything that you do in Cabana",
    "start": "2139010",
    "end": "2144560"
  },
  {
    "text": "really has this concept of buckets and metrics behind it I got a I got confused",
    "start": "2144560",
    "end": "2149720"
  },
  {
    "text": "with cabana for a long time until I understood this so let us go ahead and",
    "start": "2149720",
    "end": "2154870"
  },
  {
    "text": "pop back over here so we can see we're up to 200,000 or so",
    "start": "2154870",
    "end": "2160240"
  },
  {
    "text": "records and on my elasticsearch",
    "start": "2160240",
    "end": "2166750"
  },
  {
    "text": "control panel I have here the cabana link so I can simply click this and",
    "start": "2166750",
    "end": "2171920"
  },
  {
    "text": "cabana will open for me",
    "start": "2171920",
    "end": "2175690"
  },
  {
    "text": "okay so the first thing we have to do is tell cabana you know we've configured the index pattern which was fire hose",
    "start": "2180369",
    "end": "2186410"
  },
  {
    "text": "blah right we have to tell kalana what is the index pattern that we're using so that it can find our data right so in",
    "start": "2186410",
    "end": "2193760"
  },
  {
    "text": "this case we're gonna choose fire hose star and we know it worked because it found our",
    "start": "2193760",
    "end": "2200029"
  },
  {
    "text": "time stamp that's good we need to have our time stamp if I select if I accept that then I can",
    "start": "2200029",
    "end": "2208670"
  },
  {
    "text": "see here that Cabana well has actually taken the mapping elasticsearch has",
    "start": "2208670",
    "end": "2214339"
  },
  {
    "text": "taken the mapping this is my set of fields again it's the host name they're off the ident the verb the size etc",
    "start": "2214339",
    "end": "2220609"
  },
  {
    "text": "right so these are all the fields that are in my data so now in Cabana I have",
    "start": "2220609",
    "end": "2225859"
  },
  {
    "text": "several panels here which are interesting the first one is a discover panel the discover panel allows me to",
    "start": "2225859",
    "end": "2232460"
  },
  {
    "text": "look at my data okay no results found so why is that well because we haven't yet",
    "start": "2232460",
    "end": "2238670"
  },
  {
    "text": "cranked up the wayback machine so we have to do that because I'm sending data in from 1995 Cabana is looking according",
    "start": "2238670",
    "end": "2245029"
  },
  {
    "text": "to this time range so the last 15 minutes right that does not include 1995",
    "start": "2245029",
    "end": "2250789"
  },
  {
    "text": "I can change the time range that cabañas looking at by going in number of different ways but here's absolute time",
    "start": "2250789",
    "end": "2257079"
  },
  {
    "text": "we need to go to 1995 and in fact there's a I didn't fix the",
    "start": "2257079",
    "end": "2264770"
  },
  {
    "text": "time zone I just ignored that so let go June 29th to",
    "start": "2264770",
    "end": "2272349"
  },
  {
    "text": "1995 August 1st so we'll cover the whole month of",
    "start": "2272349",
    "end": "2279770"
  },
  {
    "text": "July ok if I type that right",
    "start": "2279770",
    "end": "2286328"
  },
  {
    "text": "okay so we can see already we have data flowing in that's nice if we were in",
    "start": "2290110",
    "end": "2295730"
  },
  {
    "text": "more of a real-time context we would actually want to have that auto update so I can turn on this Auto refresh here",
    "start": "2295730",
    "end": "2301720"
  },
  {
    "text": "and if I do that I'd say every 30 seconds we can we'll start to see the",
    "start": "2301720",
    "end": "2307580"
  },
  {
    "text": "data updating as it comes in I have a really long time range on here so the queries are somewhat slow okay this is",
    "start": "2307580",
    "end": "2315860"
  },
  {
    "text": "our our discover panel so we can see data is coming in actually we could dig in a little bit and look here and we can",
    "start": "2315860",
    "end": "2322010"
  },
  {
    "text": "see some of the examples of the data and the values that we have but really we want to get to some visualizations so",
    "start": "2322010",
    "end": "2329830"
  },
  {
    "text": "this is the visualization tab there are a number of different visualizations",
    "start": "2329830",
    "end": "2334880"
  },
  {
    "text": "that you can build in Kabana we have charts lines bar charts pie charts",
    "start": "2334880",
    "end": "2341050"
  },
  {
    "text": "metrics all sorts of stuff we're gonna start with a simple metric and we're gonna go now we can scope this",
    "start": "2341050",
    "end": "2347540"
  },
  {
    "text": "particular metric to a particular search so that enables us to measure based on something we're",
    "start": "2347540",
    "end": "2354530"
  },
  {
    "text": "searching over but we're not going to do that right now what we're gonna do here is just create a metric this is the",
    "start": "2354530",
    "end": "2360170"
  },
  {
    "text": "count of everything so we have four hundred forty three thousand documents in there already well that's kind of",
    "start": "2360170",
    "end": "2366620"
  },
  {
    "text": "cool we can add more metrics and we can say okay well you know we'll be",
    "start": "2366620",
    "end": "2372560"
  },
  {
    "text": "kind of cool to figure out how many different hosts are attaching or sending",
    "start": "2372560",
    "end": "2377660"
  },
  {
    "text": "requests so I want a unique count of",
    "start": "2377660",
    "end": "2382690"
  },
  {
    "text": "the host field and I can do that",
    "start": "2382690",
    "end": "2387880"
  },
  {
    "text": "so we have in the time period that's covered we have 31,000 different hosts",
    "start": "2387880",
    "end": "2393260"
  },
  {
    "text": "that have connected and the last one that we might want to look at is how",
    "start": "2393260",
    "end": "2398390"
  },
  {
    "text": "much data are we generating and sending out so we can also do that we can look at our",
    "start": "2398390",
    "end": "2405520"
  },
  {
    "text": "what did I do here I need a plus here we go",
    "start": "2406120",
    "end": "2411099"
  },
  {
    "text": "we want the sum of the size sizes the number of bytes in",
    "start": "2411300",
    "end": "2417030"
  },
  {
    "text": "the in the response and I put that one in and we've sent about in the time",
    "start": "2417030",
    "end": "2422790"
  },
  {
    "text": "period about twelve gigabytes okay so that's kind of cool we have some some numbers that are interesting so let's",
    "start": "2422790",
    "end": "2429960"
  },
  {
    "text": "say metrics and I can save that and let's",
    "start": "2429960",
    "end": "2435530"
  },
  {
    "text": "build another one so let's look at another thing we often want to know is",
    "start": "2435530",
    "end": "2442260"
  },
  {
    "text": "how much data are we sending out over time right so let's go ahead and look at a line chart",
    "start": "2442260",
    "end": "2447680"
  },
  {
    "text": "in this case we start out we have a single dot there that's not very",
    "start": "2447680",
    "end": "2453290"
  },
  {
    "text": "informative but what we haven't done yet is set up the bucketing we haven't set up the buckets on the x-axis so for the",
    "start": "2453290",
    "end": "2460740"
  },
  {
    "text": "x-axis you want to add a date histogram the histogram will will bucket in two",
    "start": "2460740",
    "end": "2467370"
  },
  {
    "text": "particular times and it's based on our timestamp field and let's go ahead and do that this is a count of events over",
    "start": "2467370",
    "end": "2474690"
  },
  {
    "text": "time also an interesting graph but what we really wanted was the sum of",
    "start": "2474690",
    "end": "2481580"
  },
  {
    "text": "the size so we can put that in and it looks kind",
    "start": "2481580",
    "end": "2488100"
  },
  {
    "text": "of like the number of requests obviously but here we have again a graph that shows us our bytes sent over time so",
    "start": "2488100",
    "end": "2494670"
  },
  {
    "text": "that's a good one to keep - let's do that this one bytes over time okay",
    "start": "2494670",
    "end": "2504799"
  },
  {
    "text": "now also we might be interested in our result codes like hey do we have any",
    "start": "2506530",
    "end": "2512220"
  },
  {
    "text": "500s do we have any 400s 404s so what we can do is we can have a bar chart and",
    "start": "2512220",
    "end": "2518020"
  },
  {
    "text": "let's go ahead and do that now again we want to see this over time right so we need a buckets for the",
    "start": "2518020",
    "end": "2524440"
  },
  {
    "text": "x-axis we're gonna have again a date histogram and we're gonna add a sub bucket which is",
    "start": "2524440",
    "end": "2534660"
  },
  {
    "text": "a terms so what we're gonna do here is",
    "start": "2534660",
    "end": "2539740"
  },
  {
    "text": "we're gonna first bucket into time slices then we're gonna buck it into status codes so we'll see split bars",
    "start": "2539740",
    "end": "2548200"
  },
  {
    "text": "with status codes across time right so how we do that is we pick a term a over",
    "start": "2548200",
    "end": "2554590"
  },
  {
    "text": "what do you call that surrounding date histogram with a terms aggregation in",
    "start": "2554590",
    "end": "2560020"
  },
  {
    "text": "the middle and our field here will be status and we're gonna get the top five",
    "start": "2560020",
    "end": "2567720"
  },
  {
    "text": "so here what we have is a bar chart that has these are 200 responses thank",
    "start": "2567720",
    "end": "2573520"
  },
  {
    "text": "goodness they're mostly 200 so we have some redirects 302 s + 304 s we have some 404s 500s etcetera actually it",
    "start": "2573520",
    "end": "2580630"
  },
  {
    "text": "would be a little bit more interesting if this graph didn't show the 200s because we don't so much care if we're",
    "start": "2580630",
    "end": "2586660"
  },
  {
    "text": "looking for errors so we can use Kabana because remember it's a search engine so",
    "start": "2586660",
    "end": "2591910"
  },
  {
    "text": "I can say not status 200 and we'll scope this chart to be only the error codes",
    "start": "2591910",
    "end": "2600580"
  },
  {
    "text": "right so now we have a bar chart split bar chart with all of our error codes let's go ahead and save that",
    "start": "2600580",
    "end": "2607800"
  },
  {
    "text": "okay and the last thing we'll look at is we'll look at well what were people",
    "start": "2607800",
    "end": "2613240"
  },
  {
    "text": "looking at like what requests were they making kind of curious about that so we",
    "start": "2613240",
    "end": "2618250"
  },
  {
    "text": "can pick a pie chart and we're going to first figure out people so let's",
    "start": "2618250",
    "end": "2625740"
  },
  {
    "text": "split the slices again it's a terms so we're looking at the values of fields and we're gonna pick the host field here",
    "start": "2625740",
    "end": "2634240"
  },
  {
    "text": "and get the count so in this case we have the top five",
    "start": "2634240",
    "end": "2639610"
  },
  {
    "text": "sources for our requests we have prodigy we have Alyssa product so again like the",
    "start": "2639610",
    "end": "2645670"
  },
  {
    "text": "Wayback Machine right it's kind of funny like I don't know I could I was going to do geoip but then I thought like oh wait",
    "start": "2645670",
    "end": "2652450"
  },
  {
    "text": "those IPS have long since moved okay so these are the people that are making contacts now another thing again is to",
    "start": "2652450",
    "end": "2659440"
  },
  {
    "text": "figure out what they are searching for so what I'm doing is I'm gonna sub bucket these slices into what are the",
    "start": "2659440",
    "end": "2667390"
  },
  {
    "text": "requests that they're making so I'm going to split slices and I'm gonna again go with the terms and I'm gonna go",
    "start": "2667390",
    "end": "2675900"
  },
  {
    "text": "with the field is the request okay let's do that",
    "start": "2675900",
    "end": "2684599"
  },
  {
    "text": "again we're getting a little long in the time the timeline that we're doing",
    "start": "2686070",
    "end": "2693000"
  },
  {
    "text": "okay so now what we have is from PI web 3d prodigy mostly Shuttle missions STS",
    "start": "2693900",
    "end": "2701560"
  },
  {
    "text": "71 STS 71 patch small dot jiff so basically people were getting the",
    "start": "2701560",
    "end": "2707500"
  },
  {
    "text": "website banner well that kind of makes sense but interesting that STS 71 is the",
    "start": "2707500",
    "end": "2713020"
  },
  {
    "text": "shuttle that flew from June to July 5th right so we probably will see 70 which",
    "start": "2713020",
    "end": "2720790"
  },
  {
    "text": "is the one that went July 13th but not yet because we don't have that data",
    "start": "2720790",
    "end": "2726480"
  },
  {
    "text": "so we can see they're getting some logos a lot of gifts in there right but that's",
    "start": "2726480",
    "end": "2732010"
  },
  {
    "text": "that's cool so let's save that guy - ok this is requests ok so now you have a couple of",
    "start": "2732010",
    "end": "2741430"
  },
  {
    "text": "visualizations and we want to build a dashboard so that we can monitor and see what's going on so we go over to our",
    "start": "2741430",
    "end": "2747700"
  },
  {
    "text": "dashboard in Cabana and I'm going to add my visualizations so I'll start with my",
    "start": "2747700",
    "end": "2754840"
  },
  {
    "text": "error code counts so I want to start with my big metrics error code counts bytes over time requests I can add all",
    "start": "2754840",
    "end": "2763570"
  },
  {
    "text": "those guys and then pretty simply just drag and drop ooh my",
    "start": "2763570",
    "end": "2770110"
  },
  {
    "text": "screens not big enough to create some more or less pleasing organization of these things",
    "start": "2770110",
    "end": "2778600"
  },
  {
    "text": "and that is how we got from",
    "start": "2780010",
    "end": "2787570"
  },
  {
    "text": "the source data to the dashboard all right",
    "start": "2787570",
    "end": "2792830"
  },
  {
    "text": "now I have also so it cabanas is nice it's nice for",
    "start": "2792830",
    "end": "2798770"
  },
  {
    "text": "monitoring it's nice to see what's going on it's nice for the forest level view but you can also use elastic search",
    "start": "2798770",
    "end": "2805430"
  },
  {
    "text": "query language to pull out more report oriented data that's not visual but that",
    "start": "2805430",
    "end": "2810500"
  },
  {
    "text": "does give you more information about what's going on so I'm going to quickly",
    "start": "2810500",
    "end": "2815630"
  },
  {
    "text": "go through a couple of examples of that so this here is a chrome plug-in it's",
    "start": "2815630",
    "end": "2822560"
  },
  {
    "text": "name is sense and it does give you some autocomplete and it gives you a little bit of makes it a little bit easier to",
    "start": "2822560",
    "end": "2829400"
  },
  {
    "text": "write queries because elastic search query language as we'll see is somewhat convoluted this so I need to set my",
    "start": "2829400",
    "end": "2836360"
  },
  {
    "text": "endpoint here so I get my endpoint and I can put that into sense",
    "start": "2836360",
    "end": "2845890"
  },
  {
    "text": "okay so the easiest thing I can do is I can just put a do a get against the",
    "start": "2847060",
    "end": "2853070"
  },
  {
    "text": "search endpoint that's pretty simple and what we get back is the",
    "start": "2853070",
    "end": "2859870"
  },
  {
    "text": "response we have 1.4 million I believe documents in there already so as you can",
    "start": "2859870",
    "end": "2867560"
  },
  {
    "text": "see like as we're talking it's it's filling it and right now we get results in no particular order and in this case",
    "start": "2867560",
    "end": "2874220"
  },
  {
    "text": "we have a bunch of stuff that Kabana that's in our Cabana index because all of Cabana is backed by elastic search",
    "start": "2874220",
    "end": "2879710"
  },
  {
    "text": "everything is saved in there that's not really that interesting I don't want that I can also specify an index in my",
    "start": "2879710",
    "end": "2886850"
  },
  {
    "text": "you query URL to scope to particular indices there's lots of different ways you can do it or lots of different",
    "start": "2886850",
    "end": "2893210"
  },
  {
    "text": "things here you can put in actual index itself you can use a wild card like I'm doing here you can use commas you can",
    "start": "2893210",
    "end": "2899900"
  },
  {
    "text": "spread your queries however you want across your indices so if I just get in the fire",
    "start": "2899900",
    "end": "2905490"
  },
  {
    "text": "area search results then I see here's a bunch of stuff that are again my patchy",
    "start": "2905490",
    "end": "2912510"
  },
  {
    "text": "weblog data now this there are two kinds of queries that you run in elasticsearch there are",
    "start": "2912510",
    "end": "2919680"
  },
  {
    "text": "there's query it's called query context query context refers to when I'm running",
    "start": "2919680",
    "end": "2924810"
  },
  {
    "text": "a query and I'm computing a score that's usually signaled by the query in the",
    "start": "2924810",
    "end": "2930930"
  },
  {
    "text": "query DSL and in this case this says in the query contest context match against",
    "start": "2930930",
    "end": "2936660"
  },
  {
    "text": "status 404 so here I'm going to get all of my 404s and I can look in them and I see here's you know again status 404 I",
    "start": "2936660",
    "end": "2944250"
  },
  {
    "text": "have another one here for readme dot text etc the other kind of query that you write",
    "start": "2944250",
    "end": "2950820"
  },
  {
    "text": "is called a filtered context query and in a filtered context no score is",
    "start": "2950820",
    "end": "2956280"
  },
  {
    "text": "computed and the matches are cached so filters are actually more performant than queries right so if you can write a",
    "start": "2956280",
    "end": "2963720"
  },
  {
    "text": "filter you should write a filter and in this case I've written a query filled a filtered query that's just looking for",
    "start": "2963720",
    "end": "2969630"
  },
  {
    "text": "my 200 statuses again I could go through all of this language the query DSL is deep and a lot",
    "start": "2969630",
    "end": "2979080"
  },
  {
    "text": "to look at so sorry this is must not be 200 so I have my 304 etc well that's",
    "start": "2979080",
    "end": "2985800"
  },
  {
    "text": "kind of cool let's see if we can dig into that a little bit so let's see where are not to hundreds",
    "start": "2985800",
    "end": "2992730"
  },
  {
    "text": "occurred in time so now we have this concept of an",
    "start": "2992730",
    "end": "2997830"
  },
  {
    "text": "aggregation again we talked about aggregations aggregations are what back the the the diagrams in kibana that the",
    "start": "2997830",
    "end": "3005570"
  },
  {
    "text": "visualizations with the query data cell I can again send a query that's a filter",
    "start": "3005570",
    "end": "3011330"
  },
  {
    "text": "that says everything is not at 200 and then I can compute an aggregation which",
    "start": "3011330",
    "end": "3016400"
  },
  {
    "text": "is doing statistics on those documents in this case I'm looking at the status",
    "start": "3016400",
    "end": "3022010"
  },
  {
    "text": "field and looking for counts of them so not a time range sorry so in all of the",
    "start": "3022010",
    "end": "3030110"
  },
  {
    "text": "time that we've looked at so I have 100 4,000 304 's 440,000 for 302s and",
    "start": "3030110",
    "end": "3038060"
  },
  {
    "text": "eight or nine thousand eight thousand six hundred 404s so I may actually want to find okay well",
    "start": "3038060",
    "end": "3047180"
  },
  {
    "text": "what was the request that generated those things so I can take an aggregation and sub aggregate in this",
    "start": "3047180",
    "end": "3054050"
  },
  {
    "text": "case I have my buckets so here's three oh four and these are the requests that",
    "start": "3054050",
    "end": "3059450"
  },
  {
    "text": "generated the three Oh force but what I'm really curious about is the 404s because those are usually broken links",
    "start": "3059450",
    "end": "3065480"
  },
  {
    "text": "so I can come down here and say okay here's my 404s and pub wind the wind VN",
    "start": "3065480",
    "end": "3071120"
  },
  {
    "text": "readme.txt is kind of our worst offender with 490 and then there's another pub",
    "start": "3071120",
    "end": "3077240"
  },
  {
    "text": "when VN etc so if I'm running my website I want to go and look these figure out what's broken here and and fix them I",
    "start": "3077240",
    "end": "3086180"
  },
  {
    "text": "got a bunch more stuff but I'm kind of out of time I want to leave a little time for questions so let's just pop",
    "start": "3086180",
    "end": "3091700"
  },
  {
    "text": "back over to the presentation so again for Cubana fields should be not",
    "start": "3091700",
    "end": "3100940"
  },
  {
    "text": "analyzed then we get nice visualizations buckets and metrics is the important",
    "start": "3100940",
    "end": "3105950"
  },
  {
    "text": "concept when building the visualizations and for time series viewing use a histogram",
    "start": "3105950",
    "end": "3112550"
  },
  {
    "text": "date histogram on the x-axis and then you can do whatever you want on the y-axis so",
    "start": "3112550",
    "end": "3117700"
  },
  {
    "text": "that's that's where the we're gonna sort of wrap up we talked about elastic search service",
    "start": "3117700",
    "end": "3125150"
  },
  {
    "text": "some of the benefits that you get running elastic search service as opposed to self-managed and then we set",
    "start": "3125150",
    "end": "3132470"
  },
  {
    "text": "up a Kinesis firehose stream we set up our elastic search cluster with a template so that as the data came in it",
    "start": "3132470",
    "end": "3139280"
  },
  {
    "text": "was all mapped correctly and had the correct number of shards and then you can use Cabana de to monitor and to dig",
    "start": "3139280",
    "end": "3146420"
  },
  {
    "text": "in and diagnose what's going on and if you really want to dig a and you can go with the query DSL",
    "start": "3146420",
    "end": "3151960"
  },
  {
    "text": "we have a few next steps you have your cards that were on your seat there is a quick lab that walks you through it's a",
    "start": "3151960",
    "end": "3158480"
  },
  {
    "text": "free quick lab walks you through sending cloud trail logs to cloud watch and then to elastic sort service and",
    "start": "3158480",
    "end": "3165620"
  },
  {
    "text": "building Cabana dashboards that's kind of nice there is a centralized logging solution that we've published",
    "start": "3165620",
    "end": "3171470"
  },
  {
    "text": "talks you through how to build it's a whole cloud formation template etc to gather all your logs into elasticsearch",
    "start": "3171470",
    "end": "3178130"
  },
  {
    "text": "and then that's a pointer to our detail page so thank you very much really appreciate",
    "start": "3178130",
    "end": "3185119"
  },
  {
    "text": "your attention remember to complete your evaluation [Applause]",
    "start": "3185119",
    "end": "3192459"
  }
]