[
  {
    "text": "In this video, you’ll see how to \nprocess streaming data with",
    "start": "0",
    "end": "2849"
  },
  {
    "text": "Amazon Kinesis Data Analytics.",
    "start": "2849",
    "end": "4859"
  },
  {
    "text": "With this solution, you can create \na streaming source for an Amazon",
    "start": "5436",
    "end": "8323"
  },
  {
    "text": "Managed Streaming for Apache Kafka \n(Amazon MSK) cluster, build a stream",
    "start": "8324",
    "end": "12816"
  },
  {
    "text": "processing application using a Studio \nnotebook, and leverage Apache Flink",
    "start": "12816",
    "end": "16533"
  },
  {
    "text": "for notebook data operations.",
    "start": "16533",
    "end": "18290"
  },
  {
    "text": "Amazon MSK is a fully managed service\nfor running Apache Kafka clusters on AWS.",
    "start": "20407",
    "end": "25368"
  },
  {
    "text": "To get started, let’s create a new cluster.",
    "start": "26226",
    "end": "28330"
  },
  {
    "text": "We’ll use the Custom create method.",
    "start": "29247",
    "end": "30987"
  },
  {
    "text": "We’ll give our cluster a name and leave \nthe Serverless cluster type option selected.",
    "start": "33810",
    "end": "37501"
  },
  {
    "text": "Let’s continue to the next page.",
    "start": "40277",
    "end": "41817"
  },
  {
    "text": "Here we’ll define the virtual networking \nenvironment for our cluster.",
    "start": "42735",
    "end": "45709"
  },
  {
    "text": "We’ll choose a VPC that we \npreviously created in our account.",
    "start": "47343",
    "end": "50418"
  },
  {
    "text": "To achieve high availability, we’ll specify three \navailability zones and the subnet for each.",
    "start": "53676",
    "end": "58542"
  },
  {
    "text": "Next, we’ll need to choose a security group.",
    "start": "76924",
    "end": "78917"
  },
  {
    "text": "We’ll select the Custom option and \nchoose a group that we previously created.",
    "start": "79458",
    "end": "82728"
  },
  {
    "text": "This group has the required AWS Identity \nand Access Management (IAM) permissions.",
    "start": "83339",
    "end": "88000"
  },
  {
    "text": "Let’s continue to the next page.",
    "start": "88823",
    "end": "90376"
  },
  {
    "text": "Amazon MSK serverless clusters work \nwith IAM role-based authentication.",
    "start": "93459",
    "end": "97686"
  },
  {
    "text": "We already have an IAM security group policy\n defined, so let’s continue to the next page.",
    "start": "98121",
    "end": "102394"
  },
  {
    "text": "We’ll retain the defaults for metrics, \nand we won’t add any cluster tags.",
    "start": "104076",
    "end": "107450"
  },
  {
    "text": "Now we’ll quickly review our cluster \nsettings and create the cluster.",
    "start": "109167",
    "end": "112247"
  },
  {
    "text": "The creation process for this serverless\n cluster will take up to five minutes.",
    "start": "114364",
    "end": "117850"
  },
  {
    "text": "Now that our cluster is successfully \ncreated, let’s navigate to Amazon",
    "start": "118708",
    "end": "122009"
  },
  {
    "text": "Kinesis to create a Studio notebook.",
    "start": "122009",
    "end": "124063"
  },
  {
    "text": "We’ll do this from the \nStreaming applications page.",
    "start": "124710",
    "end": "126973"
  },
  {
    "text": "We’ll create this Studio \nnotebook with custom settings.",
    "start": "128843",
    "end": "131353"
  },
  {
    "text": "Let’s give our notebook a name.",
    "start": "135246",
    "end": "136538"
  },
  {
    "text": "We'll leave the runtime as Apache Flink \n1.13 and continue to the next page.",
    "start": "139264",
    "end": "143190"
  },
  {
    "text": "For Kinesis Data Analytics to access our\n data sources and destinations, we must",
    "start": "145860",
    "end": "149965"
  },
  {
    "text": "provide an IAM role with \nthe required permissions.",
    "start": "149965",
    "end": "152632"
  },
  {
    "text": "We'll choose an IAM role that was \npreviously created in our account.",
    "start": "153561",
    "end": "156676"
  },
  {
    "text": "Next, we must specify an AWS Glue \ndatabase that defines the metadata",
    "start": "160286",
    "end": "164088"
  },
  {
    "text": "for our sources and destinations.",
    "start": "164089",
    "end": "165934"
  },
  {
    "text": "We’ll choose the default database.",
    "start": "166334",
    "end": "167910"
  },
  {
    "text": "Let’s continue to the next page.",
    "start": "169721",
    "end": "171208"
  },
  {
    "text": "We’ll keep the default configurations \nfor Scaling and Logging and monitoring.",
    "start": "174239",
    "end": "178000"
  },
  {
    "text": "Under Networking, we’ll choose the VPC \nconnectivity option that bases the VPC",
    "start": "178753",
    "end": "182860"
  },
  {
    "text": "configuration on our Amazon MSK cluster.",
    "start": "182860",
    "end": "185338"
  },
  {
    "text": "The Studio notebook automatically \npopulates the subnets and security",
    "start": "186255",
    "end": "189401"
  },
  {
    "text": "groups with the cluster’s networking settings.",
    "start": "189401",
    "end": "191501"
  },
  {
    "text": "We have the option to deploy our Studio \nnotebook application executable to an",
    "start": "193418",
    "end": "196875"
  },
  {
    "text": "Amazon Simple Storage \nService (Amazon S3) bucket.",
    "start": "196876",
    "end": "200000"
  },
  {
    "text": "We’ll decline that option for now and \nleave the remaining configuration",
    "start": "200612",
    "end": "203413"
  },
  {
    "text": "sections with their default values.",
    "start": "203413",
    "end": "205122"
  },
  {
    "text": "Let’s continue to the next page.",
    "start": "206000",
    "end": "207485"
  },
  {
    "text": "Finally, we'll review our \nStudio notebook settings.",
    "start": "208450",
    "end": "210931"
  },
  {
    "text": "By default, Kinesis Data Analytics \nStudio includes connectors for Flink SQL,",
    "start": "211343",
    "end": "215768"
  },
  {
    "text": "Kafka, and Amazon MSK IAM authentication.",
    "start": "215768",
    "end": "218979"
  },
  {
    "text": "Let’s go ahead and \ncreate our Studio notebook.",
    "start": "220884",
    "end": "222923"
  },
  {
    "text": "Now that our Studio notebook is \nsuccessfully created, let’s run it.",
    "start": "223769",
    "end": "226851"
  },
  {
    "text": "Starting the Studio notebook \ncan take a couple of minutes.",
    "start": "230097",
    "end": "232529"
  },
  {
    "text": "Our Studio notebook has successfully started.",
    "start": "233434",
    "end": "235533"
  },
  {
    "text": "Let’s open it in Apache Zeppelin.",
    "start": "235874",
    "end": "237590"
  },
  {
    "text": "We’ll import two notes containing code \nsamples that we'll be using for our demo.",
    "start": "238532",
    "end": "242081"
  },
  {
    "text": "The first is for ingesting \ndata to create our data stream.",
    "start": "242387",
    "end": "244939"
  },
  {
    "text": "The second is to process \nthe ingested data stream.",
    "start": "247174",
    "end": "249437"
  },
  {
    "text": "Now that we've imported both code files\n into our Zeppelin notebook, let's open",
    "start": "255282",
    "end": "258690"
  },
  {
    "text": "each note in a separate browser tab.",
    "start": "258690",
    "end": "260501"
  },
  {
    "text": "Here’s the code that ingests data \ninto our Amazon MSK cluster.",
    "start": "265205",
    "end": "268572"
  },
  {
    "text": "And here’s the process \ncode for the Flink application.",
    "start": "269230",
    "end": "271641"
  },
  {
    "text": "Let’s now retrieve the connectivity \ninformation from our cluster.",
    "start": "272488",
    "end": "275397"
  },
  {
    "text": "Back in Amazon MSK, we’ll select the \nserverless cluster we created earlier.",
    "start": "278000",
    "end": "282082"
  },
  {
    "text": "We’ll view the client information.",
    "start": "283022",
    "end": "284533"
  },
  {
    "text": "Let’s copy the bootstrap endpoint so that \nwe can use it in our Zeppelin notebook.",
    "start": "285533",
    "end": "289072"
  },
  {
    "text": "Now, let’s return to Zeppelin.",
    "start": "290000",
    "end": "291388"
  },
  {
    "text": "Our newly created Amazon MSK \nKafka cluster is currently empty.",
    "start": "293117",
    "end": "296861"
  },
  {
    "text": "To confirm that, we’ll run the code in this cell \nto see a list of topics currently in the cluster.",
    "start": "297719",
    "end": "302251"
  },
  {
    "text": "As expected, the cluster has no topics.",
    "start": "303157",
    "end": "305332"
  },
  {
    "text": "We can run the code in the next \ncell to create a topic for our cluster.",
    "start": "306250",
    "end": "309354"
  },
  {
    "text": "We now have a topic \nnamed AmznReviewsTopic.",
    "start": "311365",
    "end": "313927"
  },
  {
    "text": "Next, we’ll produce some data so that \nwe can do real-time processing on it.",
    "start": "314868",
    "end": "318406"
  },
  {
    "text": "We have a sample dataset in an S3 bucket.",
    "start": "318888",
    "end": "321187"
  },
  {
    "text": "We’ll be reading the data files of the \ndataset to run a sample producer.",
    "start": "321528",
    "end": "324755"
  },
  {
    "text": "The producer will create a data \nstream which we will use to create",
    "start": "325355",
    "end": "327941"
  },
  {
    "text": "our processing application.",
    "start": "327941",
    "end": "329277"
  },
  {
    "text": "Let's run the code and \nstart producing the data.",
    "start": "330112",
    "end": "332204"
  },
  {
    "text": "Now that our Amazon MSK cluster has \na topic with data being produced to it,",
    "start": "333627",
    "end": "337347"
  },
  {
    "text": "let's switch over to the \nprocess code in our notebook.",
    "start": "337347",
    "end": "339775"
  },
  {
    "text": "We’ll use this code to create a streaming\n table that connects to the Kafka topic",
    "start": "340611",
    "end": "343729"
  },
  {
    "text": "we created and pulls data from it.",
    "start": "343729",
    "end": "345654"
  },
  {
    "text": "The code points to our serverless \ncluster, providing the properties of",
    "start": "346466",
    "end": "349456"
  },
  {
    "text": "the bootstrap endpoint and ensuring\n it uses SASL and IAM authentication.",
    "start": "349456",
    "end": "353462"
  },
  {
    "text": "Let’s run it.",
    "start": "353909",
    "end": "354602"
  },
  {
    "text": "Now we can start querying the table in \nreal time to view the streaming data.",
    "start": "356614",
    "end": "359928"
  },
  {
    "text": "Let's run the select query.",
    "start": "360457",
    "end": "361927"
  },
  {
    "text": "While the select query runs, we’ll create\n a destination table for query output.",
    "start": "362786",
    "end": "367042"
  },
  {
    "text": "The table will output files in CSV format \nto an Amazon S3 bucket location.",
    "start": "367900",
    "end": "372285"
  },
  {
    "text": "Let’s run the code.",
    "start": "372651",
    "end": "373608"
  },
  {
    "text": "In the previous cell, we now see \ndata coming into our source table,",
    "start": "375372",
    "end": "378738"
  },
  {
    "text": "which means Apache Flink is \nable to read real-time Amazon",
    "start": "378738",
    "end": "381448"
  },
  {
    "text": "reviews from our Kafka topic.",
    "start": "381448",
    "end": "383038"
  },
  {
    "text": "We’ll now stop the source table \nselect query, as we have confirmed",
    "start": "383941",
    "end": "386951"
  },
  {
    "text": "that we're able to read the data.",
    "start": "386951",
    "end": "388345"
  },
  {
    "text": "Our next step is to create the checkpoint\n configuration for inserting streaming",
    "start": "389287",
    "end": "392621"
  },
  {
    "text": "data into our destination table.",
    "start": "392621",
    "end": "394449"
  },
  {
    "text": "Now we’ll run a SQL insert statement \nthat inserts customer_id, product_id,",
    "start": "395319",
    "end": "399359"
  },
  {
    "text": "and star_rating data into the \ndestination table from our source table.",
    "start": "399359",
    "end": "402802"
  },
  {
    "text": "While the code is running, let's \nswitch back to the ingest notebook.",
    "start": "404343",
    "end": "407317"
  },
  {
    "text": "We can see the cell for the producer is \nstill running because it has to read all the",
    "start": "409175",
    "end": "412242"
  },
  {
    "text": "data in the files and keep producing the data.",
    "start": "412243",
    "end": "414725"
  },
  {
    "text": "Let’s return to the process notebook.",
    "start": "415630",
    "end": "417252"
  },
  {
    "text": "While the streaming data is inserting \ninto our destination table, let’s find the",
    "start": "418181",
    "end": "421561"
  },
  {
    "text": "location of the Amazon S3 bucket we created.",
    "start": "421561",
    "end": "424184"
  },
  {
    "text": "Let’s view the bucket.",
    "start": "425007",
    "end": "426000"
  },
  {
    "text": "The output folder contains product \npartition folders that each contain data files.",
    "start": "427694",
    "end": "431578"
  },
  {
    "text": "This notebook can be run continuously to \ncreate streaming SQL jobs on top of the cluster.",
    "start": "434918",
    "end": "439227"
  },
  {
    "text": "You’ve just seen how to process streaming \ndata with Amazon Kinesis Data Analytics.",
    "start": "441885",
    "end": "445953"
  },
  {
    "text": "You can learn more about this topic in \nthe description and links for this video.",
    "start": "446976",
    "end": "450179"
  },
  {
    "text": "Thanks for watching. Now it’s your turn to try.",
    "start": "450391",
    "end": "452774"
  }
]