[
  {
    "text": "okay I'm gonna get started hi guys I am video Sreenivasan I am the director for",
    "start": "60",
    "end": "5430"
  },
  {
    "text": "Amazon redshift and just delighted to be here today to talk to you about one of my favorite topics and the goal for the",
    "start": "5430",
    "end": "13740"
  },
  {
    "text": "day for me is to give you a brief overview of the platform talk to you about some of our most common use cases",
    "start": "13740",
    "end": "19949"
  },
  {
    "text": "and then give you a look into some of the newer features that we worked on I then hand it off to Justin from Yelp",
    "start": "19949",
    "end": "28680"
  },
  {
    "text": "who's back there right now service that all of us use and benefit from and he is going to take us through",
    "start": "28680",
    "end": "35670"
  },
  {
    "text": "the data infrastructure evolution that Yelp had to go through as their data volumes continue to grow and",
    "start": "35670",
    "end": "42140"
  },
  {
    "text": "specifically he's going to talk about the tooling that they had to build to optimize their use of redshift as well",
    "start": "42140",
    "end": "48510"
  },
  {
    "text": "as integrate with their existing Hadoop infrastructure so a lot of exciting stuff ahead before I jump in could I",
    "start": "48510",
    "end": "56820"
  },
  {
    "text": "just get a quick show of hands for how many of you have used or are currently using redshift",
    "start": "56820",
    "end": "64370"
  },
  {
    "text": "fantastic about 40% or so thank you to you guys for the rest of you sorry",
    "start": "64880",
    "end": "71850"
  },
  {
    "text": "you've been missing out hopefully we will change that all right so before I",
    "start": "71850",
    "end": "79200"
  },
  {
    "text": "jump into the service itself I just wanted to provide a little bit of motivation for why we decided to have a",
    "start": "79200",
    "end": "84960"
  },
  {
    "text": "service such as this if you look at this graph what you see is we've gotten",
    "start": "84960",
    "end": "90659"
  },
  {
    "text": "really good at generating a lot of data and this comes from IOT devices all the",
    "start": "90659",
    "end": "96780"
  },
  {
    "text": "web blogs etc what is not what is not clear yet is how do you get this into a",
    "start": "96780",
    "end": "103470"
  },
  {
    "text": "format that makes it easily analyzable and variable so that you can drive insights from this data and this is",
    "start": "103470",
    "end": "110159"
  },
  {
    "text": "really the analysis gap that you see here and we thought this was really",
    "start": "110159",
    "end": "115229"
  },
  {
    "text": "unfortunate because let's say I'm a four-person gaming startup the difference between success and failure",
    "start": "115229",
    "end": "121680"
  },
  {
    "text": "for that startup could lie in their ability to understand the things about",
    "start": "121680",
    "end": "127020"
  },
  {
    "text": "the game that engages the users and keeps them hooked and the things that actually makes them drop off and all of",
    "start": "127020",
    "end": "133290"
  },
  {
    "text": "that information is there in the logs that they collect if only they could analyze that and",
    "start": "133290",
    "end": "138330"
  },
  {
    "text": "gather insight it could make a big big difference to their business when we talk to customers about what are the",
    "start": "138330",
    "end": "145680"
  },
  {
    "text": "reasons for this why don't you just do it these whole things come up pretty often the first thing is around the cost",
    "start": "145680",
    "end": "151560"
  },
  {
    "text": "the cost of acquiring collecting all this data and making it acquiring the",
    "start": "151560",
    "end": "157590"
  },
  {
    "text": "data warehouses needed to store it and query it and there's a lot of upfront",
    "start": "157590",
    "end": "162959"
  },
  {
    "text": "cost involved additionally and this is especially hard to justify if you're not",
    "start": "162959",
    "end": "168690"
  },
  {
    "text": "quite sure yet what the value you're going to get from all of the data is and that's often the case",
    "start": "168690",
    "end": "173790"
  },
  {
    "text": "the second is around complexity let's say you got it in even after that it is really hard it is non-trivial to manage",
    "start": "173790",
    "end": "180870"
  },
  {
    "text": "this data and to scale it what worked at 100 gigs of data is not going to work",
    "start": "180870",
    "end": "185880"
  },
  {
    "text": "when you go into 500 terabytes or petabytes scale especially if you also want to do that",
    "start": "185880",
    "end": "191430"
  },
  {
    "text": "keeping your performance at a steady rate right very maintaining query performance that's a hard problem and",
    "start": "191430",
    "end": "196830"
  },
  {
    "text": "the other thing we hear about is just the fact that today the data of formats",
    "start": "196830",
    "end": "201930"
  },
  {
    "text": "themselves the data sources are just so varied that you have structured data unstructured data and all these",
    "start": "201930",
    "end": "208019"
  },
  {
    "text": "different formats coming together it's just hard to put them into something uniform and start cutting them so all of",
    "start": "208019",
    "end": "215010"
  },
  {
    "text": "these are barriers we really built redshift in response to overwhelming",
    "start": "215010",
    "end": "222959"
  },
  {
    "text": "customer demand for a platform that allow them to analyze data in a",
    "start": "222959",
    "end": "228750"
  },
  {
    "text": "cost-effective scalable and simple to use fashion redshift you can start with a",
    "start": "228750",
    "end": "236370"
  },
  {
    "text": "couple of big hundred gigabytes all the way up to petabyte scale we go up to two petabytes you can in terms of",
    "start": "236370",
    "end": "243630"
  },
  {
    "text": "performance we are about 10 times more performant than roasters solutions out",
    "start": "243630",
    "end": "248790"
  },
  {
    "text": "there and about ten times more cost-effective and compatible columnstore solutions and most of all",
    "start": "248790",
    "end": "255840"
  },
  {
    "text": "it's super simple to use we make provisioning and a lot of the back a lot of the DBA activities are automatically",
    "start": "255840",
    "end": "262410"
  },
  {
    "text": "taking taken care of you we take care of failures backup restored is recovery and a lot of the things that",
    "start": "262410",
    "end": "268720"
  },
  {
    "text": "people traditionally have to spend a lot of time on so let me actually give you",
    "start": "268720",
    "end": "274599"
  },
  {
    "text": "an example to make this real so this one's actually closer to home amazon.com",
    "start": "274599",
    "end": "280780"
  },
  {
    "text": "as you can imagine once to get a lot of insight from the clickstream blog that they clicked because that indicates user",
    "start": "280780",
    "end": "287050"
  },
  {
    "text": "behavior it would be very interesting to know for example what would the stream of things that people saw before they",
    "start": "287050",
    "end": "293590"
  },
  {
    "text": "made that purchase or did not make that purchase if they marry this to the sales data and at the time they were trying to",
    "start": "293590",
    "end": "301750"
  },
  {
    "text": "figure out what is the best technology platform to do this the best sequel scalar solution that they had could take",
    "start": "301750",
    "end": "309280"
  },
  {
    "text": "them all the way up to one week of their log data and query it they then tried",
    "start": "309280",
    "end": "314500"
  },
  {
    "text": "moving to Hadoop and they could get up to a month of their of their data",
    "start": "314500",
    "end": "320190"
  },
  {
    "text": "queried granted the performance was not great that's when they came to redshift",
    "start": "320190",
    "end": "325330"
  },
  {
    "text": "with redshift and they have a couple of hundred node cluster so they're a pretty big customer they were able to query",
    "start": "325330",
    "end": "333250"
  },
  {
    "text": "across two point two five trillion rows and join this with five billion rows and",
    "start": "333250",
    "end": "338289"
  },
  {
    "text": "they could get these queries running under 15 minutes and this cost them one",
    "start": "338289",
    "end": "345669"
  },
  {
    "text": "hundred and eighty dollars an hour better still the more important thing I want to point out here is they are able",
    "start": "345669",
    "end": "352449"
  },
  {
    "text": "to manage the rich of justice today with 20% of one DBAs time and that is",
    "start": "352449",
    "end": "358509"
  },
  {
    "text": "significant because what it allows them to do now is to focus on their schemas",
    "start": "358509",
    "end": "363639"
  },
  {
    "text": "and their processes to get value out of their data rather than spend time setting up their queries and setting up",
    "start": "363639",
    "end": "370779"
  },
  {
    "text": "setting up their database itself okay",
    "start": "370779",
    "end": "376240"
  },
  {
    "text": "let me now take a step back and talk to you more broadly about the use cases that we've seen be very popular with",
    "start": "376240",
    "end": "382690"
  },
  {
    "text": "redshift so given that we came in with a relatively low cost and very flexible",
    "start": "382690",
    "end": "390300"
  },
  {
    "text": "platform we've actually seen lots of use cases beyond the traditional enterprise data warehouse",
    "start": "390300",
    "end": "395810"
  },
  {
    "text": "News kiss but let me start there so but the traditional enterprise data warehouses we've seen a fair number of",
    "start": "395810",
    "end": "401450"
  },
  {
    "text": "migrations from on-trend redshift and typically the motivation for them is around agility they love the ability to",
    "start": "401450",
    "end": "409250"
  },
  {
    "text": "now be able to start a data warehouse whatever scale and experiment with their",
    "start": "409250",
    "end": "414770"
  },
  {
    "text": "data individual teams and big companies can do this they don't have to now rely on their IT departments and wait there 8",
    "start": "414770",
    "end": "420919"
  },
  {
    "text": "months for provisioning and forgetting procurement to move and that's been very",
    "start": "420919",
    "end": "426440"
  },
  {
    "text": "powerful for them with the big data company so these are the guys who have just a lot of data at gaming and they",
    "start": "426440",
    "end": "434060"
  },
  {
    "text": "typically don't have even the money to go buy these really large expensive systems for them they love moving retro",
    "start": "434060",
    "end": "441530"
  },
  {
    "text": "mainly for the cost and ease of use because this can be you you don't have",
    "start": "441530",
    "end": "446660"
  },
  {
    "text": "to be a DBA to use a platform pretty much a lot of our customers are actually developers basin's we take care of the",
    "start": "446660",
    "end": "454040"
  },
  {
    "text": "majority of the DBA type heavy lifting these guys can really focus on just the",
    "start": "454040",
    "end": "459770"
  },
  {
    "text": "queries and tuning their system for their application and then the third",
    "start": "459770",
    "end": "465770"
  },
  {
    "text": "segment is SAS offerings so here you can think of a debt desk comm or let's say",
    "start": "465770",
    "end": "471970"
  },
  {
    "text": "marketing email campaign management so essentially they let you do email",
    "start": "471970",
    "end": "477380"
  },
  {
    "text": "targeting and give you analytics on that and a lot of them would have redshift",
    "start": "477380",
    "end": "482660"
  },
  {
    "text": "underneath the covers powering all their analysis and you might not be aware of it but but the fact that it's an",
    "start": "482660",
    "end": "489410"
  },
  {
    "text": "easy-to-manage platform and they can scale as they grow so they had more clusters if they have more customers or",
    "start": "489410",
    "end": "495260"
  },
  {
    "text": "scaled back depending on their own demand gives them a ton of flexibility to run their platforms",
    "start": "495260",
    "end": "501050"
  },
  {
    "text": "the other interesting thing that we saw evolve with SAS companies is that we",
    "start": "501050",
    "end": "506539"
  },
  {
    "text": "have a very simple way of my chart they have a simple way of charging back",
    "start": "506539",
    "end": "512330"
  },
  {
    "text": "because they can now say I'll have a separate truster for each of my customers and maybe it's a customer or",
    "start": "512330",
    "end": "517940"
  },
  {
    "text": "group of customers and I can provide SaaS based on how much they have provisioned and charge back the cost in",
    "start": "517940",
    "end": "525380"
  },
  {
    "text": "a very simple fashion this is possible now they can now run a little tree farm of rich of clusters",
    "start": "525380",
    "end": "532220"
  },
  {
    "text": "because the their management overhead is really low and that's been just a powerful economic paradigm for them okay",
    "start": "532220",
    "end": "542450"
  },
  {
    "text": "so here is a sampling of some of our customers the only thing I want to point out here is we've had interest from",
    "start": "542450",
    "end": "549920"
  },
  {
    "text": "customers across various different vertical segments as well as sizes for example Nasdaq that does billing and",
    "start": "549920",
    "end": "556640"
  },
  {
    "text": "fraud detection for all the online trades uses right roof to power their analytics but so does Pinterest which",
    "start": "556640",
    "end": "563270"
  },
  {
    "text": "tries to figure out user behavior from all the user engagement from all the locks that they collect so it's fairly",
    "start": "563270",
    "end": "570500"
  },
  {
    "text": "it's used in multiple different segments and there are several others here I also",
    "start": "570500",
    "end": "576560"
  },
  {
    "text": "like to say a word about our partners we actually spend a lot of time ensuring that we have a healthy partner ecosystem",
    "start": "576560",
    "end": "582950"
  },
  {
    "text": "because we want to respect the investments that you've already made in in in using these tools and setting",
    "start": "582950",
    "end": "589700"
  },
  {
    "text": "these up to support your own infrastructures and if you have a partner sure that you would like for us",
    "start": "589700",
    "end": "595730"
  },
  {
    "text": "to work with please let us know we have a pretty comprehensive list already and and really since redshift is Postgres",
    "start": "595730",
    "end": "602720"
  },
  {
    "text": "equal compliant pretty much any tool that is Posterous compliant should just work maybe we do thorough testing where",
    "start": "602720",
    "end": "610160"
  },
  {
    "text": "every time you certify a partner but it's it's not it's not very very difficult to do that okay so let me",
    "start": "610160",
    "end": "619640"
  },
  {
    "text": "actually spend some time on the architecture so at a very high level redshift is a clustered system when you",
    "start": "619640",
    "end": "627020"
  },
  {
    "text": "go to our console to create a redshift cluster what you get is one leader node and any number of compute nodes and you",
    "start": "627020",
    "end": "634760"
  },
  {
    "text": "pick the number of compute nodes based on your storage needs and this entire cluster is in a high-performance network",
    "start": "634760",
    "end": "640880"
  },
  {
    "text": "it's connected by ten Giggy minimally oversubscribed channels and it is",
    "start": "640880",
    "end": "645980"
  },
  {
    "text": "directly connected to the other AWS AWS services such as s3 and dynamo DB and",
    "start": "645980",
    "end": "652370"
  },
  {
    "text": "any of the EC to EC two nodes the leader node acts as a sequel endpoint and we",
    "start": "652370",
    "end": "658580"
  },
  {
    "text": "opposed to a sequel compliant it acts as a sequel endpoint to all their queries or to all the tools",
    "start": "658580",
    "end": "663650"
  },
  {
    "text": "outside so all your bi tools would just connect to the leader node and it is responsible for taking a query parsing",
    "start": "663650",
    "end": "670550"
  },
  {
    "text": "it coming up with the query plan and also generating the code to actually execute the query it compiles it and",
    "start": "670550",
    "end": "677960"
  },
  {
    "text": "then shifts it off to the compute nodes the compute nodes but all the data is",
    "start": "677960",
    "end": "683060"
  },
  {
    "text": "actually stored is responsible for performing all the operations that are brought forward by the leader node in a",
    "start": "683060",
    "end": "690200"
  },
  {
    "text": "completely parallel fashion and that's how we get the scale out fact of",
    "start": "690200",
    "end": "695240"
  },
  {
    "text": "redshift the data itself is columnar I'll touch on that in a little bit and",
    "start": "695240",
    "end": "700340"
  },
  {
    "text": "if you think about the operations in the compute node that matter that's load so you want to load in parallel when you",
    "start": "700340",
    "end": "706190"
  },
  {
    "text": "have a lot of data as this common data warehouses backup/restore resize all of",
    "start": "706190",
    "end": "712430"
  },
  {
    "text": "these operations are one-click operations from the console and they all happen node parallel across all the",
    "start": "712430",
    "end": "718190"
  },
  {
    "text": "compute nodes the Nords itself you have a choice what type of node you want to pick you can go",
    "start": "718190",
    "end": "724130"
  },
  {
    "text": "with an SSD instance type which is faster it's actually about 15 to 20",
    "start": "724130",
    "end": "729950"
  },
  {
    "text": "times faster than the HDD and it's about five times more expensive than the HDD and it's for somewhat smaller data sets",
    "start": "729950",
    "end": "737840"
  },
  {
    "text": "it goes from 162 to 326 terabytes and the HDD which goes from 2 terabytes to 2",
    "start": "737840",
    "end": "745310"
  },
  {
    "text": "para bytes so it really depends on the type of workload that you're trying to serve I wanted to provide some more",
    "start": "745310",
    "end": "752720"
  },
  {
    "text": "context for the architectural decisions that were made and one of the main things we do in any data warehouse is to",
    "start": "752720",
    "end": "759590"
  },
  {
    "text": "reduce IO and the reason you want to reduce R here is if you think about data warehouses there's a lot of data and CPU",
    "start": "759590",
    "end": "767840"
  },
  {
    "text": "memory and storage CPU and memory follow the semiconductor curve right Moore's law they keep doubling their performance",
    "start": "767840",
    "end": "773420"
  },
  {
    "text": "every two years well storage is not so lucky there's lots of the mechanical limitations and that typically ends up being the",
    "start": "773420",
    "end": "779540"
  },
  {
    "text": "bottleneck when you're trying to process large scale data sets and so lots of",
    "start": "779540",
    "end": "785450"
  },
  {
    "text": "decisions are driven to reduce i/o the first one of them is we have columnar",
    "start": "785450",
    "end": "791330"
  },
  {
    "text": "storage what does that mean well when I wrote then you typically expect their drawer",
    "start": "791330",
    "end": "796459"
  },
  {
    "text": "to be stored as is with a new database as is in most databases and colonist",
    "start": "796459",
    "end": "802009"
  },
  {
    "text": "systems what we will do is take the draw apart and then store all the columns together and contiguous blocks and the",
    "start": "802009",
    "end": "808579"
  },
  {
    "text": "reason we do this is because as you can see the blue boxes most of the queries when they come in they are only going to",
    "start": "808579",
    "end": "814699"
  },
  {
    "text": "query a subset of your columns they don't go across the entire row and you want to reduce the amount of data that",
    "start": "814699",
    "end": "820579"
  },
  {
    "text": "you want to send back to the query and only pick up blocks that are relevant to that query and solemnly reduces are you",
    "start": "820579",
    "end": "826579"
  },
  {
    "text": "greatly for that reason additionally because you're storing similar data types",
    "start": "826579",
    "end": "832670"
  },
  {
    "text": "together you get much better compression ratios think about how you would compress string versus how you would",
    "start": "832670",
    "end": "839480"
  },
  {
    "text": "compress a column like gender right low cardinality column you can do things that are far more efficient so this",
    "start": "839480",
    "end": "846920"
  },
  {
    "text": "gives far better compression ratio than storing rows at a time and the other thing you can do is actually get rid of",
    "start": "846920",
    "end": "853910"
  },
  {
    "text": "blocks for doing any i/o at all by having a concept I mean if you have sorted columns for example and your",
    "start": "853910",
    "end": "860779"
  },
  {
    "text": "query comes in with a predicate or in filtering on a certain range of values you can completely skip over blocks and",
    "start": "860779",
    "end": "866930"
  },
  {
    "text": "only pick up the blocks that are relevant for the query and we implement this using zone maps where we store min",
    "start": "866930",
    "end": "872509"
  },
  {
    "text": "and Max of every block and that helps and the last thing here is just that we",
    "start": "872509",
    "end": "879980"
  },
  {
    "text": "went with direct-attached storage again to maximize throughput and we went with one megabyte block sizes which is big",
    "start": "879980",
    "end": "887149"
  },
  {
    "text": "you know typical block sizes and tend to be around the kilobyte range and that way every operation you really maximize",
    "start": "887149",
    "end": "893839"
  },
  {
    "text": "I own a word on security because I know it's near and dear to a lot of our",
    "start": "893839",
    "end": "899569"
  },
  {
    "text": "customers we thought about security and bake security in right from the",
    "start": "899569",
    "end": "905240"
  },
  {
    "text": "beginning with redshift for example when we launched over two years ago we had encryption support which is unusual but",
    "start": "905240",
    "end": "913250"
  },
  {
    "text": "we did believe that this is going to be top of mind concern for a lot of customers and every step of the way has",
    "start": "913250",
    "end": "918949"
  },
  {
    "text": "features to make sure your data is secure as well as lots of auditing mechanisms to ensure that it's compliant",
    "start": "918949",
    "end": "924439"
  },
  {
    "text": "so for example we are compliant with walk one two three FedRAMP HIPPA there's a whole bunch of",
    "start": "924439",
    "end": "931370"
  },
  {
    "text": "others as well in terms of encryption support you can have your data encrypted",
    "start": "931370",
    "end": "936590"
  },
  {
    "text": "in s3 and we would ingest that re-encrypted at the block level and",
    "start": "936590",
    "end": "942290"
  },
  {
    "text": "store it within redshift every block in retrofit is stored every block is stored with a different is",
    "start": "942290",
    "end": "948800"
  },
  {
    "text": "encrypted with a different key and all these block keys are encrypted with a different cluster key we in fact in from",
    "start": "948800",
    "end": "956060"
  },
  {
    "text": "the cluster key with another master key and even allow you to have store that within your entre more cloud hardware",
    "start": "956060",
    "end": "963650"
  },
  {
    "text": "security module HSM so that will have to come to you for those keys before we",
    "start": "963650",
    "end": "969680"
  },
  {
    "text": "have permission to access the data so we use the hierarchical envelope encryption scheme so that one to prevent block",
    "start": "969680",
    "end": "977000"
  },
  {
    "text": "splicing and such but also it helps with key rotation you never have to go and re-encrypt blocks you just have to",
    "start": "977000",
    "end": "982010"
  },
  {
    "text": "reenter the other keys we of course have support for virtual",
    "start": "982010",
    "end": "987950"
  },
  {
    "text": "private cloud the standard Amazon security security groups PPC and the rest of it as you will notice here the",
    "start": "987950",
    "end": "994580"
  },
  {
    "text": "leader node has a separate PPC with all the outside clients and the compute nodes can never be in a customer VPC",
    "start": "994580",
    "end": "1001150"
  },
  {
    "text": "again we don't want any external access to your data we have logging both for",
    "start": "1001150",
    "end": "1007120"
  },
  {
    "text": "all the database operations so when somebody goes and does DDL operations all of that gets logged when somebody",
    "start": "1007120",
    "end": "1013690"
  },
  {
    "text": "goes and changes a table everything gets logged and we also log all the",
    "start": "1013690",
    "end": "1018990"
  },
  {
    "text": "provisioning operations right when you add a security group when you add a cluster delete a cluster all of that",
    "start": "1018990",
    "end": "1024550"
  },
  {
    "text": "gets logged to cloud trail and so fairly extensive audit capabilities as well",
    "start": "1024550",
    "end": "1031530"
  },
  {
    "text": "just a quick point it's just a pricing chart the only thing I want to point out is you start using there's a 25 cents an",
    "start": "1031800",
    "end": "1039550"
  },
  {
    "text": "hour and it goes all the way to under $100,000 a year for a three-year RI for",
    "start": "1039550",
    "end": "1044949"
  },
  {
    "text": "the HDD instance type and in fact we have a free trial for customers who have not used the platform before we have a",
    "start": "1044949",
    "end": "1050290"
  },
  {
    "text": "two month free trial okay let me switch",
    "start": "1050290",
    "end": "1056710"
  },
  {
    "text": "gears and talk about some of our newer features we recently announced support for custom",
    "start": "1056710",
    "end": "1062200"
  },
  {
    "text": "you know DBC drivers so prior to this what we advise customers was to go and download existing Postgres open-source",
    "start": "1062200",
    "end": "1069460"
  },
  {
    "text": "drivers and start using them the reason we moved to our own custom drivers was twofold the first one is just to give",
    "start": "1069460",
    "end": "1075700"
  },
  {
    "text": "have better control over the entering customer experience and to make the out-of-the-box experience a lot smoother",
    "start": "1075700",
    "end": "1081880"
  },
  {
    "text": "because there were some settings that had to be taken care of and such but the more important reason is now that we",
    "start": "1081880",
    "end": "1087460"
  },
  {
    "text": "know that we are the only endpoint for this driver we can really optimize the performance in fact already we have seen",
    "start": "1087460",
    "end": "1093639"
  },
  {
    "text": "up to 35% better performance with our new drivers we've also tested this and",
    "start": "1093639",
    "end": "1099490"
  },
  {
    "text": "released this with several of our partners as you can see that we will of course continue to support our existing",
    "start": "1099490",
    "end": "1105130"
  },
  {
    "text": "open-source drivers because I know it's hard to go and change the drivers and all your clients and you can just",
    "start": "1105130",
    "end": "1111460"
  },
  {
    "text": "download this from a console the next",
    "start": "1111460",
    "end": "1116649"
  },
  {
    "text": "feature that was also announced recently is explained plan visualization so what",
    "start": "1116649",
    "end": "1123700"
  },
  {
    "text": "we have what we've always had since launch is the ability for you to go to the console take any query and look at",
    "start": "1123700",
    "end": "1130539"
  },
  {
    "text": "the explained plan that was generated by the optimizer as the intended way for",
    "start": "1130539",
    "end": "1135789"
  },
  {
    "text": "the query to execute and that's that's what that's what you see up there the text what we added is this ability to",
    "start": "1135789",
    "end": "1143889"
  },
  {
    "text": "now look at queries that are either currently running or queries that have already completed execution and have a",
    "start": "1143889",
    "end": "1150130"
  },
  {
    "text": "very visual representation of what happened and so in this case you're looking at how much each plan Nord took",
    "start": "1150130",
    "end": "1157059"
  },
  {
    "text": "as that query executed and we try to make tuning queries very easy because a",
    "start": "1157059",
    "end": "1162250"
  },
  {
    "text": "good set of a population are not DBAs and so in this case for example the blue bars show the time it took the average",
    "start": "1162250",
    "end": "1169120"
  },
  {
    "text": "time it took across all our nodes and the red bar shows the maximum time it took and you see large portions of red",
    "start": "1169120",
    "end": "1176590"
  },
  {
    "text": "in any step what it means is there is one node that's taking much longer than the others and that's not something you",
    "start": "1176590",
    "end": "1182440"
  },
  {
    "text": "desire you really want every node to take the same amount of time so your most efficient it also has other cool",
    "start": "1182440",
    "end": "1189039"
  },
  {
    "text": "features like it has automatic alerting so for example it will alert you if you have a nested loop",
    "start": "1189039",
    "end": "1194890"
  },
  {
    "text": "which is not very performant or say if you're sequential scan you know we had",
    "start": "1194890",
    "end": "1201130"
  },
  {
    "text": "to scan a large number of rows but then it returns are very small portions maybe you need to filter something right so",
    "start": "1201130",
    "end": "1206680"
  },
  {
    "text": "there are these little hints that will give you about how you might want to go in and tune your query and as with lots of things as with everything pretty much",
    "start": "1206680",
    "end": "1213100"
  },
  {
    "text": "that we do all of this will continue to improve and we look forward to your feedback on what more you would want to",
    "start": "1213100",
    "end": "1218470"
  },
  {
    "text": "see there the next row features both",
    "start": "1218470",
    "end": "1223750"
  },
  {
    "text": "user-defined functions and the next one is interleaved sorting are things that we've been working on for a while now",
    "start": "1223750",
    "end": "1229170"
  },
  {
    "text": "we've been working through some technical challenges because these are fairly involving and I hope we can get",
    "start": "1229170",
    "end": "1234880"
  },
  {
    "text": "this out soon so what UDF's has just been a big ask from our customers it",
    "start": "1234880",
    "end": "1240730"
  },
  {
    "text": "gives you the ability to write your own user-defined function in our case we support Python 2.7 and run that on every",
    "start": "1240730",
    "end": "1248620"
  },
  {
    "text": "compute node we support fenced UDF's meaning we the UDF itself cannot affect",
    "start": "1248620",
    "end": "1255340"
  },
  {
    "text": "either the cluster availability or durability basically doesn't mess with your data it's prohibited from doing",
    "start": "1255340",
    "end": "1260890"
  },
  {
    "text": "Network calls and such that you don't bring down your own trustor other than that we also have lots of ship with some",
    "start": "1260890",
    "end": "1269020"
  },
  {
    "text": "of the most common libraries Python libraries out there numpy scifi and pandas if there are other libraries of",
    "start": "1269020",
    "end": "1275890"
  },
  {
    "text": "preference that you'd like to have pre install please let us know we could add",
    "start": "1275890",
    "end": "1280990"
  },
  {
    "text": "that and this is just an example of a",
    "start": "1280990",
    "end": "1286060"
  },
  {
    "text": "sample UDF so as you can see here F by the way this is just Postgres syntax we",
    "start": "1286060",
    "end": "1291550"
  },
  {
    "text": "try to stay stick very close to Postgres so people don't have to learn yet another thing so the F underscore",
    "start": "1291550",
    "end": "1297730"
  },
  {
    "text": "hostname is the UDF that you've written in fact we have reserved F underscore for all customer UDF so there will be no",
    "start": "1297730",
    "end": "1304450"
  },
  {
    "text": "collision with built-in functions and everything between the two dollar signs is actually the function that you've",
    "start": "1304450",
    "end": "1310750"
  },
  {
    "text": "written and the URL parse in this case is just a library called to the Python library that was there so fairly simple",
    "start": "1310750",
    "end": "1318820"
  },
  {
    "text": "syntax to use the last feature is multi-column sorting and essentially",
    "start": "1318820",
    "end": "1326440"
  },
  {
    "text": "what we have to is you can go to your cluster to your table and pick a set of columns and say",
    "start": "1326440",
    "end": "1333010"
  },
  {
    "text": "I want I want all of these columns to be sorted internally what we will do is take every column they take column one",
    "start": "1333010",
    "end": "1340210"
  },
  {
    "text": "sorted then take column two sorter then column three sorted which is great if column one is always present in your",
    "start": "1340210",
    "end": "1347260"
  },
  {
    "text": "queries because that holds the key to the selectivity for the entire set of columns but sometimes it's not possible",
    "start": "1347260",
    "end": "1354760"
  },
  {
    "text": "for that first column to always be present in every query and typically how people have solved this problem is to do",
    "start": "1354760",
    "end": "1362770"
  },
  {
    "text": "what is known as projections essentially you would take the columns of interest and sort them in all the possible orders",
    "start": "1362770",
    "end": "1369400"
  },
  {
    "text": "and store them so it's a much higher cost for storage and maintenance but",
    "start": "1369400",
    "end": "1374530"
  },
  {
    "text": "then when the query is coming they just fly because you have them pre-sorted in every possible order this is great",
    "start": "1374530",
    "end": "1381220"
  },
  {
    "text": "except the problem becomes pretty expensive depending on the number of columns you have let's say you had eight",
    "start": "1381220",
    "end": "1386560"
  },
  {
    "text": "columns you have eight factorial ways in which you could store that so you're talking about about 40,000 different",
    "start": "1386560",
    "end": "1392860"
  },
  {
    "text": "combinations that you might have to store to be ready for any query that might come in the it's an expensive",
    "start": "1392860",
    "end": "1400240"
  },
  {
    "text": "thing what we decided to do was to sort of find middle ground and say we will",
    "start": "1400240",
    "end": "1405970"
  },
  {
    "text": "introduce this interleaved sort columns and essentially what we will give you is",
    "start": "1405970",
    "end": "1411580"
  },
  {
    "text": "the ability to pick a set of columns and in this case we have a limitation of eight and every column will have equal",
    "start": "1411580",
    "end": "1418480"
  },
  {
    "text": "selectivity and that there is a trade-off there but every column will have equal selectivity so doesn't matter",
    "start": "1418480",
    "end": "1424510"
  },
  {
    "text": "which column comes in I realize this is a very abstract concept so I'm trying",
    "start": "1424510",
    "end": "1430390"
  },
  {
    "text": "let me just try to give you an intuition for how this might work so here is an",
    "start": "1430390",
    "end": "1435970"
  },
  {
    "text": "example it's a very simplistic one but hopefully it makes the point we have a customer ID and Product ID and this has",
    "start": "1435970",
    "end": "1443860"
  },
  {
    "text": "been sorted first by customer ID and then by product ID so you see all the customer ID is completely sorted product",
    "start": "1443860",
    "end": "1450910"
  },
  {
    "text": "ID all over the place and for the sake of this example let's say for records",
    "start": "1450910",
    "end": "1456130"
  },
  {
    "text": "form a block so this is great if you want to find Qasim",
    "start": "1456130",
    "end": "1461440"
  },
  {
    "text": "it'll be an order one operation if you give that if you give just the product ID and no customer ID this becomes an",
    "start": "1461440",
    "end": "1467860"
  },
  {
    "text": "order and operation because you're just scanning the whole thing with what the",
    "start": "1467860",
    "end": "1473230"
  },
  {
    "text": "intially it's sort keys what we would attempt to do is cluster values that are similar together and what this does at",
    "start": "1473230",
    "end": "1480640"
  },
  {
    "text": "the end of the day is whether it is the customer ID that is specified or the product ID that's specified you can the",
    "start": "1480640",
    "end": "1488110"
  },
  {
    "text": "complexity is lower than the previous case so for example if you specify the",
    "start": "1488110",
    "end": "1494320"
  },
  {
    "text": "customer ID you scan two blocks if you if you look for product ID you still scan two blocks so what was order one or",
    "start": "1494320",
    "end": "1501520"
  },
  {
    "text": "order n it became an order this complexity is square root of N or it be",
    "start": "1501520",
    "end": "1506530"
  },
  {
    "text": "cube root of n depending on how many columns you expect and this the the",
    "start": "1506530",
    "end": "1511960"
  },
  {
    "text": "performance improvements increases with n so the larger the blocks number of blocks that you have to scan in",
    "start": "1511960",
    "end": "1517420"
  },
  {
    "text": "comparison to compound sorting you will actually see better performance with this there's just logistics you would",
    "start": "1517420",
    "end": "1524559"
  },
  {
    "text": "actually be able to use it using the interleaved keyword and you'll have to pick for a particular table whether you want an",
    "start": "1524559",
    "end": "1530440"
  },
  {
    "text": "interleaved column or or a compound column but the good thing is none of your queries have to change that those",
    "start": "1530440",
    "end": "1537880"
  },
  {
    "text": "will just continue to work as as this alright I'm just gonna wrap up so we",
    "start": "1537880",
    "end": "1544840"
  },
  {
    "text": "talked about a lot of different things and if I had to leave you with with one",
    "start": "1544840",
    "end": "1550150"
  },
  {
    "text": "thought before I hand off it would be that at the end of the day most of what we do is to enable you to spend more",
    "start": "1550150",
    "end": "1558370"
  },
  {
    "text": "time with your data rather than spend time with your database and dealing with",
    "start": "1558370",
    "end": "1563559"
  },
  {
    "text": "a muck of managing and scaling your databases and this is what I hear from",
    "start": "1563559",
    "end": "1569410"
  },
  {
    "text": "customers all the time that at the end of the day they find redshift powerful because they does enable them to",
    "start": "1569410",
    "end": "1576010"
  },
  {
    "text": "understand their customers better to understand their business better because they have a lot more time to do it",
    "start": "1576010",
    "end": "1583120"
  },
  {
    "text": "with that let me welcome Justin to the stage",
    "start": "1583120",
    "end": "1588450"
  },
  {
    "text": "okay there we go my name is Justin Cunningham and I'm a technical lead at Yelp on the business",
    "start": "1626160",
    "end": "1632350"
  },
  {
    "text": "analytics and metrics team business analytics and metrics is a team focused on real-time data infrastructure and",
    "start": "1632350",
    "end": "1638080"
  },
  {
    "text": "data warehousing we maintain most of yelps redshift infrastructure I'm sure",
    "start": "1638080",
    "end": "1646210"
  },
  {
    "text": "everybody has some idea of what Yelp is at this point but just so we share some context Yelp is designed to help connect",
    "start": "1646210",
    "end": "1653230"
  },
  {
    "text": "people with great local businesses and I wanted to quantify some of our growth since 2004 we've grown to over a hundred",
    "start": "1653230",
    "end": "1660520"
  },
  {
    "text": "and thirty five million cumulative monthly users or unique monthly visitors with over 71 million unique monthly or",
    "start": "1660520",
    "end": "1667929"
  },
  {
    "text": "cumulative reviews because of this tremendous growth we've had to evolve",
    "start": "1667929",
    "end": "1673630"
  },
  {
    "text": "our data infrastructure we've really evolved this into two parallel paths in",
    "start": "1673630",
    "end": "1678880"
  },
  {
    "text": "the first path we're processing data on s3 using MapReduce jobs with EMR",
    "start": "1678880",
    "end": "1684669"
  },
  {
    "text": "Amazon's elastic MapReduce service in the second path we're processing data",
    "start": "1684669",
    "end": "1689950"
  },
  {
    "text": "using Python badges in both cases we end up inserting the data typically back",
    "start": "1689950",
    "end": "1696280"
  },
  {
    "text": "into my sequel this is left a couple of holes for us EMR isn't really a great",
    "start": "1696280",
    "end": "1703179"
  },
  {
    "text": "pattern for exploring data and Python batches also have some difficulty",
    "start": "1703179",
    "end": "1708910"
  },
  {
    "text": "scaling up to the volume of data that we have now in particular we used to use my sequel and just do straight querying to",
    "start": "1708910",
    "end": "1716110"
  },
  {
    "text": "get many of our analytics data and metrics data and that pattern doesn't really hold up when you've got hundreds",
    "start": "1716110",
    "end": "1722260"
  },
  {
    "text": "of over 100 million uniques I want to",
    "start": "1722260",
    "end": "1727360"
  },
  {
    "text": "take a step into our EMR usage since this is really informed the way that we're using redshift Yelp doesn't have a",
    "start": "1727360",
    "end": "1734530"
  },
  {
    "text": "single Hadoop cluster instead we actually have many Hadoop clusters will use the EMR to start clusters on demand",
    "start": "1734530",
    "end": "1740880"
  },
  {
    "text": "whenever a developer wants the cluster the only thing they have to do is add a command line option - our EMR and a",
    "start": "1740880",
    "end": "1747460"
  },
  {
    "text": "cluster will be started for them all right at any point any developers",
    "start": "1747460",
    "end": "1754570"
  },
  {
    "text": "actually able to grab up to 50 nodes from Amazon and do whatever analysis they want to do there's no kind of",
    "start": "1754570",
    "end": "1760760"
  },
  {
    "text": "permissioning process and the clusters not shared so whenever you're doing an analysis the clusters actually",
    "start": "1760760",
    "end": "1766760"
  },
  {
    "text": "completely dedicated for that purpose we do pull clusters and all of our data is",
    "start": "1766760",
    "end": "1772130"
  },
  {
    "text": "on s3 so every cluster has access to the complete Yelp data set this approach is",
    "start": "1772130",
    "end": "1778429"
  },
  {
    "text": "really informed how we went forward with redshift in solving the problems that I described before so how are we really",
    "start": "1778429",
    "end": "1785960"
  },
  {
    "text": "using redshift then well we actually have dozens of separate clusters a",
    "start": "1785960",
    "end": "1791720"
  },
  {
    "text": "question that comes up internally a lot is should one redshift cluster roll them all and the answer that we've had so far",
    "start": "1791720",
    "end": "1797870"
  },
  {
    "text": "is no we have dozens of clusters that are owned by different teams some are",
    "start": "1797870",
    "end": "1804140"
  },
  {
    "text": "owned by individuals some are owned by a single team the only commonality really is the kind of analysis that they're",
    "start": "1804140",
    "end": "1810230"
  },
  {
    "text": "doing so who actually owns these clusters well every data team at Yelp",
    "start": "1810230",
    "end": "1817160"
  },
  {
    "text": "owns clusters that would include front-end and back-end teams also the",
    "start": "1817160",
    "end": "1822380"
  },
  {
    "text": "data teams are adds bam business analytics and metrics span and search so",
    "start": "1822380",
    "end": "1828500"
  },
  {
    "text": "every one of them owns stuff also our consumer team and mobile teams have clusters that they use for their own",
    "start": "1828500",
    "end": "1834320"
  },
  {
    "text": "analysis so what's the benefit of having so many well it decouples or development",
    "start": "1834320",
    "end": "1839630"
  },
  {
    "text": "scaling if one of our team wants to start a new cluster bringing a bunch of data and maybe take it down for a while",
    "start": "1839630",
    "end": "1847429"
  },
  {
    "text": "in order to facilitate that they're able to do that without interfering with any of the end of their analysis that we want to do the other benefit is that it",
    "start": "1847429",
    "end": "1855410"
  },
  {
    "text": "limits our contention issues whenever a team wants to run an in-depth analysis",
    "start": "1855410",
    "end": "1860900"
  },
  {
    "text": "on their data they're able to do that without first consulting with any other team I want to take a look at how a",
    "start": "1860900",
    "end": "1868520"
  },
  {
    "text": "couple of our teams are really using redshift the first is ads our ads team",
    "start": "1868520",
    "end": "1874250"
  },
  {
    "text": "store about 18 months of information in redshift covering every detail about how ads are displayed on Yelp that includes",
    "start": "1874250",
    "end": "1881330"
  },
  {
    "text": "information like cohorts for experiments at opportunities impressions and even",
    "start": "1881330",
    "end": "1887090"
  },
  {
    "text": "auction information so how different ads are any auction especially about the losers",
    "start": "1887090",
    "end": "1892549"
  },
  {
    "text": "so we're able to see if we can improve those to help drive revenue the ads team",
    "start": "1892549",
    "end": "1899029"
  },
  {
    "text": "uses this cluster to power an internal dashboard and also to do some feature",
    "start": "1899029",
    "end": "1905059"
  },
  {
    "text": "engineering so they're able to improve their models next is mobile our mobile",
    "start": "1905059",
    "end": "1911659"
  },
  {
    "text": "team uses their cluster to store between 250 and 450 million rows of data a day",
    "start": "1911659",
    "end": "1917409"
  },
  {
    "text": "they bring in information from the device and also from the API this",
    "start": "1917409",
    "end": "1923179"
  },
  {
    "text": "information is joined to create a complete view of how data is used at Yelp or how mobile device it data is",
    "start": "1923179",
    "end": "1929659"
  },
  {
    "text": "used at Yelp the mobile team is able to analyze the data that's coming in to",
    "start": "1929659",
    "end": "1936529"
  },
  {
    "text": "determine which features are really working well and like the ads team they really do a lot of experiment analysis",
    "start": "1936529",
    "end": "1941840"
  },
  {
    "text": "to figure out how the different experiments that performing on mobile are working out developing these",
    "start": "1941840",
    "end": "1950049"
  },
  {
    "text": "different use cases has led to some data loading patterns that we've come up with",
    "start": "1950049",
    "end": "1955809"
  },
  {
    "text": "the first involves using EMR which is the Amazon Elastic MapReduce service to",
    "start": "1955809",
    "end": "1960919"
  },
  {
    "text": "process data on s3 we use a Yelp open source tool called mr job which will let",
    "start": "1960919",
    "end": "1967730"
  },
  {
    "text": "you write MapReduce jobs in Python and then as I said before all of our log",
    "start": "1967730",
    "end": "1972799"
  },
  {
    "text": "data ends up on s3 so we simply process the log data put it back on s3 and a",
    "start": "1972799",
    "end": "1977809"
  },
  {
    "text": "format that redshift can load and then load it directly in the redshift we even do this for some other data that's not",
    "start": "1977809",
    "end": "1985249"
  },
  {
    "text": "log data that would be primarily sequel dumps so we'll take data that is",
    "start": "1985249",
    "end": "1992149"
  },
  {
    "text": "essentially just a long list of sequel insert statements process it with them our job and then insert it",
    "start": "1992149",
    "end": "1998360"
  },
  {
    "text": "this pattern is so common that we developed a specialized tool called port called EMR or I'm sorry called mycroft",
    "start": "1998360",
    "end": "2004950"
  },
  {
    "text": "so what my craft is is an Orchestrator that orchestrates between s3m our job",
    "start": "2004950",
    "end": "2011230"
  },
  {
    "text": "and redshift and it is designed to do daily ETL of data that appears on s3 so",
    "start": "2011230",
    "end": "2019509"
  },
  {
    "text": "once you set up a mycroft job it will automatically find you data that shows up and ETL it for you",
    "start": "2019509",
    "end": "2027210"
  },
  {
    "text": "it's got a web interface so in order to start ingesting data the only thing that you've got to do is specify a cluster ID",
    "start": "2027210",
    "end": "2034630"
  },
  {
    "text": "a schema an s3 path and a date and mycroft will take it from there we are",
    "start": "2034630",
    "end": "2041470"
  },
  {
    "text": "open sourcing this fairly soon so definitely watch for that the last",
    "start": "2041470",
    "end": "2048460"
  },
  {
    "text": "pattern involves using Kafka norm this is our solution for real-time data warehousing so messages are published",
    "start": "2048460",
    "end": "2056618"
  },
  {
    "text": "periodically to Kafka and then we've got a storm topology that consumes those messages once it receives between 10,000",
    "start": "2056619",
    "end": "2063730"
  },
  {
    "text": "and 100,000 messages it will flush those out to s3 once it flushes the messages",
    "start": "2063730",
    "end": "2069368"
  },
  {
    "text": "to s3 and the right is confirmed it will publish information about that into Kafka so the Kafka topic will end up",
    "start": "2069369",
    "end": "2076358"
  },
  {
    "text": "containing information like the path on s3 the number of roads that were written",
    "start": "2076359",
    "end": "2081550"
  },
  {
    "text": "the upstream data source and also a timestamp our workers will then read",
    "start": "2081550",
    "end": "2088450"
  },
  {
    "text": "from that Kafka topic and create loads into redshift clusters from that",
    "start": "2088450",
    "end": "2093520"
  },
  {
    "text": "information developing these patterns has led to some best practices the first",
    "start": "2093520",
    "end": "2100990"
  },
  {
    "text": "is to batch updates so any kind of write operation in redshift has a very high",
    "start": "2100990",
    "end": "2106690"
  },
  {
    "text": "constant cost it's best if this is amortized across many updates to give I",
    "start": "2106690",
    "end": "2114339"
  },
  {
    "text": "guess a concrete example I've time doing a single insert at taking up to five seconds and doing 60",
    "start": "2114339",
    "end": "2121720"
  },
  {
    "text": "million taking about 140 seconds so if you're doing millions of updates it can",
    "start": "2121720",
    "end": "2127810"
  },
  {
    "text": "take as little as where you can do as many as a half a million a second if you're doing a single update at a time",
    "start": "2127810",
    "end": "2133960"
  },
  {
    "text": "it could take five seconds for each one so the bottom line is if you don't batch",
    "start": "2133960",
    "end": "2139240"
  },
  {
    "text": "your updates your system won't work next use manifest files so manifest",
    "start": "2139240",
    "end": "2144760"
  },
  {
    "text": "files are essentially a list of files on s3 inside of another file these are important because of eventual",
    "start": "2144760",
    "end": "2150880"
  },
  {
    "text": "consistency especially if you're in u.s. standard for s3 if you write files",
    "start": "2150880",
    "end": "2156770"
  },
  {
    "text": "and then you tell redshift to load those files they may not actually exist yet so your redshift cluster will try to load them find nothing there until you the",
    "start": "2156770",
    "end": "2163460"
  },
  {
    "text": "load succeeded if you create a manifest file it will tell that load this is less",
    "start": "2163460",
    "end": "2168560"
  },
  {
    "text": "important if you're in u.s. West where you've got read after right consistency but it's still worth keeping in mind",
    "start": "2168560",
    "end": "2175960"
  },
  {
    "text": "next try to make your operations idempotent so what we try to do when",
    "start": "2175960",
    "end": "2181220"
  },
  {
    "text": "we're designing systems is make sure that if something fails we're not relying on the state of the system when",
    "start": "2181220",
    "end": "2186890"
  },
  {
    "text": "we go to recover from that error and what that allows us to do is make retries a trivial operation a good",
    "start": "2186890",
    "end": "2193700"
  },
  {
    "text": "example of this is an absurd where if the UPS truck fails and you don't know",
    "start": "2193700",
    "end": "2199070"
  },
  {
    "text": "if the data actually was updated or not you can retry the operation and you'll be in exactly the same state regardless",
    "start": "2199070",
    "end": "2206210"
  },
  {
    "text": "of what happened previously it makes a lot of your error recovery much easier to keep that in mind and finally try to",
    "start": "2206210",
    "end": "2214250"
  },
  {
    "text": "design your systems for auto recovery our ETL systems checkpoint themselves into redshift periodically and when they",
    "start": "2214250",
    "end": "2220280"
  },
  {
    "text": "checkpoint themselves they'll save information about the last updates they",
    "start": "2220280",
    "end": "2225440"
  },
  {
    "text": "performed and then whenever they come online they'll read that information back out from redshift itself they do",
    "start": "2225440",
    "end": "2230840"
  },
  {
    "text": "this transactionally so that in the event they go down during an update",
    "start": "2230840",
    "end": "2236300"
  },
  {
    "text": "operation we're able to pick up right where we left off again we've actually used this feature pretty recently to",
    "start": "2236300",
    "end": "2242210"
  },
  {
    "text": "great effect when we migrated some clusters we just restored snapshots in a different region and when we did that we",
    "start": "2242210",
    "end": "2248030"
  },
  {
    "text": "want to do restore updates and get those working again the only thing we had to do on our end was actually change the",
    "start": "2248030",
    "end": "2254750"
  },
  {
    "text": "host name once we did that all of our updates started working again automatically the red shift our red",
    "start": "2254750",
    "end": "2263600"
  },
  {
    "text": "shift ETL infrastructure was smart enough to just pick that up and do it for us next I want to talk about a few",
    "start": "2263600",
    "end": "2272120"
  },
  {
    "text": "of the things that we've learned in particular about data loading and data layout in developing these patterns and",
    "start": "2272120",
    "end": "2277730"
  },
  {
    "text": "also a few of the unique to redshift features the first data loading I guess",
    "start": "2277730",
    "end": "2286520"
  },
  {
    "text": "item I want to talk about is supporting multiple clusters I would recommend you tried to do this from the start it sounds pretty hard but",
    "start": "2286520",
    "end": "2293370"
  },
  {
    "text": "it's actually not that bad for us all of our data passes through s3 so supporting",
    "start": "2293370",
    "end": "2300720"
  },
  {
    "text": "multiple clusters is really just a case of sending multiple clusters the same load statement at the same time or at",
    "start": "2300720",
    "end": "2306810"
  },
  {
    "text": "different times so to go back to the Kafka storm example from earlier recall",
    "start": "2306810",
    "end": "2311880"
  },
  {
    "text": "of the storm topologies periodically flushing information out to s3 to be loaded in a redshift and then whenever",
    "start": "2311880",
    "end": "2318000"
  },
  {
    "text": "it does that it publishes information about what it's done into a Kafka topic we can then have workers in this example",
    "start": "2318000",
    "end": "2325980"
  },
  {
    "text": "perhaps the top worker is consuming messages as soon as they come in and causing loads immediately in the",
    "start": "2325980",
    "end": "2332670"
  },
  {
    "text": "redshift clusters that it's connected to we could then have the second worker coming on only at night consuming all of",
    "start": "2332670",
    "end": "2339840"
  },
  {
    "text": "the messages out of the Kafka topic since the last time it's came online and loading all of them by creating manifest",
    "start": "2339840",
    "end": "2345840"
  },
  {
    "text": "files this is a really useful pattern not just if you actually want to have",
    "start": "2345840",
    "end": "2351180"
  },
  {
    "text": "multiple clusters for people to query but also if you want to be able to seamlessly migrate between clusters and",
    "start": "2351180",
    "end": "2358260"
  },
  {
    "text": "be able to do some maintenance operations a little bit more easily the",
    "start": "2358260",
    "end": "2364680"
  },
  {
    "text": "next pattern I want to talk about is ELT that's extract load transform not ETL we",
    "start": "2364680",
    "end": "2372650"
  },
  {
    "text": "started using ELT primarily because of a line in the redshift documentation that",
    "start": "2372650",
    "end": "2378140"
  },
  {
    "text": "effectively says that you should just try using your existing data model design with redshift we found that",
    "start": "2378140",
    "end": "2384690"
  },
  {
    "text": "that's generally true so we used to do some fairly heavy transformations at the point of extract and over time those",
    "start": "2384690",
    "end": "2392400"
  },
  {
    "text": "transformations started becoming less and less useful to us primarily because",
    "start": "2392400",
    "end": "2397800"
  },
  {
    "text": "the redshift performance was fast enough that we didn't have to adapt our data format for it to just work so to say",
    "start": "2397800",
    "end": "2407000"
  },
  {
    "text": "consequently as we've gone on we tend to bring in data and closer to a raw form",
    "start": "2407480",
    "end": "2413640"
  },
  {
    "text": "and then create materialized views over it and redshift if we need to or even just perform whatever operations we need",
    "start": "2413640",
    "end": "2420480"
  },
  {
    "text": "to it query time in some cases a good example of this has",
    "start": "2420480",
    "end": "2428499"
  },
  {
    "text": "been some a produced jobs that we've ended up rewriting we're actually replacing entirely with redshift queries",
    "start": "2428499",
    "end": "2433990"
  },
  {
    "text": "so we used to have some MapReduce jobs that would join log files and if anybody's ever tried to join a log file",
    "start": "2433990",
    "end": "2440079"
  },
  {
    "text": "in a MapReduce it's fairly difficult to reason about in place of that we'll",
    "start": "2440079",
    "end": "2445210"
  },
  {
    "text": "bring in the log files raw into redshift and then join in redshift itself rather",
    "start": "2445210",
    "end": "2450849"
  },
  {
    "text": "than trying to do in a query time for",
    "start": "2450849",
    "end": "2456549"
  },
  {
    "text": "time series data there are a few things to keep in mind the first involves the",
    "start": "2456549",
    "end": "2463869"
  },
  {
    "text": "order that you're loading the data and what you're sorting the data by whenever",
    "start": "2463869",
    "end": "2471190"
  },
  {
    "text": "you load data into redshift it's kind of important to understand the way vacuum operations work when you've got a fully vacuum table all of the data in the",
    "start": "2471190",
    "end": "2478809"
  },
  {
    "text": "table is sorted when you load data into a table that's vacuumed the data that",
    "start": "2478809",
    "end": "2484599"
  },
  {
    "text": "you're loading is appended to the sorted data and it's sorted internally so if",
    "start": "2484599",
    "end": "2489880"
  },
  {
    "text": "you do three data loads into a table that's just been vacuumed those loads will be internally sorted but not",
    "start": "2489880",
    "end": "2495759"
  },
  {
    "text": "externally sorted and just append it into an unsorted region when you run the vacuum operation the unsorted region is",
    "start": "2495759",
    "end": "2502779"
  },
  {
    "text": "sorted and then it's merged back into the existing sorted region if you've got billions of rows of data that operation",
    "start": "2502779",
    "end": "2509980"
  },
  {
    "text": "can be very very expensive for time series data there's a workaround and the workaround is first ensure that you have",
    "start": "2509980",
    "end": "2518380"
  },
  {
    "text": "a sort key and that the sort key is time so if you've got something like daily",
    "start": "2518380",
    "end": "2524109"
  },
  {
    "text": "log data and each of those log entries is a timestamp that would be a great",
    "start": "2524109",
    "end": "2530170"
  },
  {
    "text": "start for what to sort by when you're loading the data you want to make sure",
    "start": "2530170",
    "end": "2536680"
  },
  {
    "text": "that you're bringing the data and in the order of that sort key so in the order of time our Mycroft system actually does",
    "start": "2536680",
    "end": "2543279"
  },
  {
    "text": "this automatically so it's able to ensure that whenever you're loading",
    "start": "2543279",
    "end": "2549609"
  },
  {
    "text": "daily data the next day is always loaded after the previous day regardless of the order of the transformations that we end",
    "start": "2549609",
    "end": "2555470"
  },
  {
    "text": "performing on the data if you do this you're essentially taking advantage of the fact that all of the data is",
    "start": "2555470",
    "end": "2561770"
  },
  {
    "text": "appended in sorted order which means all of your data is also externally sorted",
    "start": "2561770",
    "end": "2567020"
  },
  {
    "text": "so you can skip back and stop altogether in that case distribution keys are",
    "start": "2567020",
    "end": "2574310"
  },
  {
    "text": "another important thing to keep in mind especially for bigger tables imagine",
    "start": "2574310",
    "end": "2581000"
  },
  {
    "text": "that your Yelp and you've got businesses and business images each business has many images setting distribution keys",
    "start": "2581000",
    "end": "2588830"
  },
  {
    "text": "controls which nodes the data gets distributed to you when you first load it the appropriate distribution keys for",
    "start": "2588830",
    "end": "2597740"
  },
  {
    "text": "this data would be ID for business and business ID for business image this will",
    "start": "2597740",
    "end": "2603590"
  },
  {
    "text": "ensure that whenever a business is distributed to a node all of the related business images are distributed to the",
    "start": "2603590",
    "end": "2608900"
  },
  {
    "text": "same node when you perform a join then the join happens in parallel across all of the nodes in the cluster and then the",
    "start": "2608900",
    "end": "2615170"
  },
  {
    "text": "final result is merged back together if you incorrectly said business or",
    "start": "2615170",
    "end": "2620330"
  },
  {
    "text": "distribution Keys this can be a little bit problematic because what ends up happening is either a broadcast or a",
    "start": "2620330",
    "end": "2626240"
  },
  {
    "text": "redistribute in the case of a broadcast all of the data is transmitted to every node and store it on every node if",
    "start": "2626240",
    "end": "2633650"
  },
  {
    "text": "you've got a hundred nodes and a billion row table you're going to create a",
    "start": "2633650",
    "end": "2639790"
  },
  {
    "text": "hundred copies of that billion rows so you definitely want to avoid that for",
    "start": "2639790",
    "end": "2645619"
  },
  {
    "text": "Yelp we have a simple heuristic for this all of our data is focused around other businesses users or reviews so if we've",
    "start": "2645619",
    "end": "2652280"
  },
  {
    "text": "got a business idea user ID or review ID will usually just select that as a distribution key as a first pass if that",
    "start": "2652280",
    "end": "2659690"
  },
  {
    "text": "works well enough that's great if not and we're doing a lot of querying on a",
    "start": "2659690",
    "end": "2665540"
  },
  {
    "text": "table that just doesn't match that pattern we will create a temp table do Eric whirring on the temp table lefted",
    "start": "2665540",
    "end": "2671420"
  },
  {
    "text": "after the data is resorted and redistributed and then that table will",
    "start": "2671420",
    "end": "2676609"
  },
  {
    "text": "disappear once we close the session that works in those cases this is a quote for",
    "start": "2676609",
    "end": "2683960"
  },
  {
    "text": "one of the redshift specific features that we use that I wanted to call out this is from",
    "start": "2683960",
    "end": "2689480"
  },
  {
    "text": "one of my teammates a yell the bi team wanted to calculate some expensive analytics in a few years of data so we",
    "start": "2689480",
    "end": "2694700"
  },
  {
    "text": "just restored a snapshot and added a bunch of notes for a few days that is really tough if you gotta buy Hardware",
    "start": "2694700",
    "end": "2700190"
  },
  {
    "text": "first so for us we've been able to use this whenever any team wants to do some",
    "start": "2700190",
    "end": "2706609"
  },
  {
    "text": "really in-depth querying especially on long run historical data without getting in the way of what they're doing",
    "start": "2706609",
    "end": "2712849"
  },
  {
    "text": "normally instead of scaling up the cluster that they're using which we also",
    "start": "2712849",
    "end": "2717980"
  },
  {
    "text": "do sometimes we'll just restore snapshot then do that analysis on the snapshot",
    "start": "2717980",
    "end": "2723530"
  },
  {
    "text": "cluster we also use elasticity for ephemeral data where we'll bring up a",
    "start": "2723530",
    "end": "2729829"
  },
  {
    "text": "cluster temporarily load it with Mycroft and analyze it there for monitoring I'd",
    "start": "2729829",
    "end": "2737329"
  },
  {
    "text": "suggest using cloud watch cloud watch is an Amazon service that has some built-in",
    "start": "2737329",
    "end": "2743810"
  },
  {
    "text": "alerts already for redshift in particular it deals with disk usage and CPU usage so we've got those alerts set",
    "start": "2743810",
    "end": "2751700"
  },
  {
    "text": "up to email us at this point it hasn't been necessary to do more than that because we have limited contention",
    "start": "2751700",
    "end": "2757400"
  },
  {
    "text": "with the data loading patterns that we've designed we also do have some",
    "start": "2757400",
    "end": "2764869"
  },
  {
    "text": "alerting on our data loading systems but because of auto recovery we actually alert very infrequently with those it's",
    "start": "2764869",
    "end": "2771619"
  },
  {
    "text": "about every eight hours if there's a persistent failure and last I wanted to",
    "start": "2771619",
    "end": "2779300"
  },
  {
    "text": "speak a little bit about window functions which come over from Postgres window functions are really really",
    "start": "2779300",
    "end": "2784970"
  },
  {
    "text": "useful in redshifted they let you do some pretty advanced querying like doing a standard deviation a percentile or a",
    "start": "2784970",
    "end": "2791599"
  },
  {
    "text": "moving average across a window of your data so in this example I'm taking a",
    "start": "2791599",
    "end": "2796910"
  },
  {
    "text": "moving average over a three row window the average count in each of these rows",
    "start": "2796910",
    "end": "2802880"
  },
  {
    "text": "is the average event count of that row in the two preceding it it can be really",
    "start": "2802880",
    "end": "2808670"
  },
  {
    "text": "useful to do this kind of analysis in redshift directly and and take advantage of the distribution in the performance",
    "start": "2808670",
    "end": "2814069"
  },
  {
    "text": "that you get from that one of our engineering managers wrote a great blog post on this so if you're interested in",
    "start": "2814069",
    "end": "2820099"
  },
  {
    "text": "more information on that I definitely would suggest checking that out to summarize yelps grew and tremendously",
    "start": "2820099",
    "end": "2828609"
  },
  {
    "text": "and consequently we've had to evolve our data infrastructure in doing so we",
    "start": "2828609",
    "end": "2833680"
  },
  {
    "text": "started looking for tools that would enable our teams to really explore their data and we landed on redshift and in",
    "start": "2833680",
    "end": "2841359"
  },
  {
    "text": "using redshift we've developed some great open source tools that I hope will make it easier for others to start using",
    "start": "2841359",
    "end": "2846369"
  },
  {
    "text": "redshift or expand their usage in particular my craft the tool that I mentioned that we're going to be",
    "start": "2846369",
    "end": "2852069"
  },
  {
    "text": "releasing soon I'm our job and pilots thank you",
    "start": "2852069",
    "end": "2858210"
  },
  {
    "text": "so yes the question is what kind of performance do you see with nested data",
    "start": "2905470",
    "end": "2911319"
  },
  {
    "text": "such as JSON and what statistics are available for the same so we so you can",
    "start": "2911319",
    "end": "2918130"
  },
  {
    "text": "do a couple of things a lot of customers what they would do is make nested data and especially JSON data they decompose",
    "start": "2918130",
    "end": "2924640"
  },
  {
    "text": "into really wide tables that's one way of storing it the other thing people do is we do a lot of file levels of nesting",
    "start": "2924640",
    "end": "2931240"
  },
  {
    "text": "so within a particular field and we can support JSON by doing that it internally",
    "start": "2931240",
    "end": "2938020"
  },
  {
    "text": "it gets stored as a string and you'll be able to you will still have to do some work in the application layer to",
    "start": "2938020",
    "end": "2944050"
  },
  {
    "text": "understand exactly how that is in terms of statistics what what particular types",
    "start": "2944050",
    "end": "2949510"
  },
  {
    "text": "of statistics are you looking for I mean we have overall statistics on querying and number of blocks fetched and stuff",
    "start": "2949510",
    "end": "2955510"
  },
  {
    "text": "like that right right so the question is",
    "start": "2955510",
    "end": "2963250"
  },
  {
    "text": "how does this apply to men and Max well let's say if you pick the column the",
    "start": "2963250",
    "end": "2969010"
  },
  {
    "text": "particular JSON column to be your sorted column and we would look at this as if it were string data we would still store",
    "start": "2969010",
    "end": "2975640"
  },
  {
    "text": "the min and Max for the set of strings that are stored in that particular block and be able to hone in on that if your",
    "start": "2975640",
    "end": "2983500"
  },
  {
    "text": "query is come in with a filtering on that so yes",
    "start": "2983500",
    "end": "2987930"
  },
  {
    "text": "no we do not so if it's stored as one field with five levels of nesting we",
    "start": "2990670",
    "end": "2995720"
  },
  {
    "text": "view it as one string we do not go within it yes sir",
    "start": "2995720",
    "end": "3004260"
  },
  {
    "text": "so the question is is this the data storage and analysis tool and how does",
    "start": "3032410",
    "end": "3039710"
  },
  {
    "text": "this work with say a top-end bi tool so yes this is a data analytics platform",
    "start": "3039710",
    "end": "3046250"
  },
  {
    "text": "right it basically takes large volumes of data and it can run complex sequel queries on them that's fundamentally",
    "start": "3046250",
    "end": "3052970"
  },
  {
    "text": "what it does now you can use these to drive your front-end bi applications informatica or",
    "start": "3052970",
    "end": "3058550"
  },
  {
    "text": "sorry tableau or MicroStrategy or anything else and put dashboards on top and just power it behind or you can use",
    "start": "3058550",
    "end": "3065450"
  },
  {
    "text": "it from just a sequel workbench or you can embed this as part of your SDKs and query through it but",
    "start": "3065450",
    "end": "3072590"
  },
  {
    "text": "essentially it gives you a platform to do these complex queries in a performant way yes sir",
    "start": "3072590",
    "end": "3079990"
  },
  {
    "text": "recommended free web client hmm I'll have to get back to you on that so",
    "start": "3083510",
    "end": "3091140"
  },
  {
    "text": "we have we have a couple of so depending on which data engineer I ask on my team they give me a different recommendation",
    "start": "3091140",
    "end": "3096630"
  },
  {
    "text": "I think it's almost a philosophical discussion at that point but I can give you a couple of suggestions that I know",
    "start": "3096630",
    "end": "3103290"
  },
  {
    "text": "folks like on my team yes sir they're",
    "start": "3103290",
    "end": "3109980"
  },
  {
    "text": "not they are currently in beta if you're interested we'd be happy to chat with you about that yeah",
    "start": "3109980",
    "end": "3118730"
  },
  {
    "text": "right so what you want to be oh right so the question is about the reason for",
    "start": "3129810",
    "end": "3137110"
  },
  {
    "text": "distributing data by column because redshift is distributed you want to",
    "start": "3137110",
    "end": "3142690"
  },
  {
    "text": "exploit data locality in your query so when you distribute by any kind of",
    "start": "3142690",
    "end": "3149680"
  },
  {
    "text": "column it will make sure that all of your data that you're going to be joining eventually ends on the same node",
    "start": "3149680",
    "end": "3154830"
  },
  {
    "text": "so when you do that join it can happen in parallel otherwise you end up",
    "start": "3154830",
    "end": "3160150"
  },
  {
    "text": "redistributing data which can be expensive",
    "start": "3160150",
    "end": "3165060"
  },
  {
    "text": "we've gone toward more denormalized tables I think it's more an artifact of",
    "start": "3171900",
    "end": "3177400"
  },
  {
    "text": "how our data originates then really any kind of conscious design because it can",
    "start": "3177400",
    "end": "3182830"
  },
  {
    "text": "be easier to bring in data that is in its native form we tend to do that and",
    "start": "3182830",
    "end": "3188650"
  },
  {
    "text": "then take advantage of the performance that redshift has to offer rather than spending a time trying to clean up the",
    "start": "3188650",
    "end": "3194020"
  },
  {
    "text": "data ahead of time yeah it kind of it",
    "start": "3194020",
    "end": "3213910"
  },
  {
    "text": "just really is a case-by-case basis we actually load most of our data automatically with a few tools that we",
    "start": "3213910",
    "end": "3221170"
  },
  {
    "text": "built so our distribution keys are not typically set by hand so on any table",
    "start": "3221170",
    "end": "3226630"
  },
  {
    "text": "we'll look based on a heuristic for essentially a few different ID columns",
    "start": "3226630",
    "end": "3231910"
  },
  {
    "text": "that we know we're likely to join on and use those that's probably not exactly a",
    "start": "3231910",
    "end": "3237970"
  },
  {
    "text": "good fit for what you're doing",
    "start": "3237970",
    "end": "3241349"
  },
  {
    "text": "yeah so what we've actually been going toward is loading our data in a more raw",
    "start": "3248400",
    "end": "3254710"
  },
  {
    "text": "form so instead of creating dimension or fact tables we'll load in the data",
    "start": "3254710",
    "end": "3259840"
  },
  {
    "text": "directly from the underlying database so if it's in my sequel we'll do a my sequel dump and load that data in and",
    "start": "3259840",
    "end": "3267220"
  },
  {
    "text": "then query on top of it we'll only do I guess more traditional data warehousing schemas if the performance isn't good",
    "start": "3267220",
    "end": "3274450"
  },
  {
    "text": "enough and typically especially for the size of data that we're dealing with in",
    "start": "3274450",
    "end": "3279850"
  },
  {
    "text": "our databases which is usually probably below we're at least in the hundreds of",
    "start": "3279850",
    "end": "3286210"
  },
  {
    "text": "millions of rows it's not terrible performance to just do that so that's",
    "start": "3286210",
    "end": "3291220"
  },
  {
    "text": "what we've needed to know we used to and",
    "start": "3291220",
    "end": "3296740"
  },
  {
    "text": "we actually moved away from it pretty aggressively because it just hasn't been necessary to maintain a combination of",
    "start": "3296740",
    "end": "3311950"
  },
  {
    "text": "the two the question was or the Kafka and storm clusters in AWS or in our data center the answer is both we actually",
    "start": "3311950",
    "end": "3321760"
  },
  {
    "text": "have some Kafka clusters in our data centers the majority of them are in AWS",
    "start": "3321760",
    "end": "3327340"
  },
  {
    "text": "actually I think all of them may be in AWS at this point for storm our open",
    "start": "3327340",
    "end": "3332410"
  },
  {
    "text": "source tool Pylea salo su to build a storm cluster and automatically deploy an AWS so all of our storm clusters are",
    "start": "3332410",
    "end": "3338980"
  },
  {
    "text": "in AWS right",
    "start": "3338980",
    "end": "3345000"
  },
  {
    "text": "the question is do you recommend star schema and redshift there's a line in the docs that actually say to basically",
    "start": "3355060",
    "end": "3363130"
  },
  {
    "text": "try the data model design that you've got before you go and try to design a",
    "start": "3363130",
    "end": "3368710"
  },
  {
    "text": "different data model and originally we started off with something along the",
    "start": "3368710",
    "end": "3374320"
  },
  {
    "text": "lines of a star schema and we've moved away from it because it hasn't been",
    "start": "3374320",
    "end": "3379450"
  },
  {
    "text": "necessary to maintain the ETL infrastructure when we can just load the data in raw and query it that way",
    "start": "3379450",
    "end": "3389070"
  },
  {
    "text": "right that's definitely a downside of bringing the data in raw so the basic my basic take on that was",
    "start": "3402590",
    "end": "3412270"
  },
  {
    "text": "the downside would be you can't actually reasonably predict what you're going to",
    "start": "3412270",
    "end": "3418880"
  },
  {
    "text": "join on and that is certainly the case for us most of our bigger data is log",
    "start": "3418880",
    "end": "3425690"
  },
  {
    "text": "data so we tend to avoid actually doing any transformations on our database",
    "start": "3425690",
    "end": "3431750"
  },
  {
    "text": "tables so we try to stay in data model",
    "start": "3431750",
    "end": "3438200"
  },
  {
    "text": "neutral in terms of what what people want to come and use there are things that you can do such as you know people have sometimes add extra nodes because",
    "start": "3438200",
    "end": "3445550"
  },
  {
    "text": "they want to get that extra performance so even if you are you can't pick that one key and you're having to distribute",
    "start": "3445550",
    "end": "3451760"
  },
  {
    "text": "data again it still gives you the performance you need so it doesn't matter as much we also have an option to",
    "start": "3451760",
    "end": "3456800"
  },
  {
    "text": "distribute all but you take the table and distribute on all the clusters the entire table of course you're not going to do this for mega tables and so people",
    "start": "3456800",
    "end": "3465020"
  },
  {
    "text": "have other mechanisms between elasticity and these little things to get the performance that they want because a lot",
    "start": "3465020",
    "end": "3471260"
  },
  {
    "text": "of folks don't want to entirely change the rate of models and we've seen certainly both models work with redshift",
    "start": "3471260",
    "end": "3477290"
  },
  {
    "text": "I don't want to get to the get your question question as the stretch of",
    "start": "3477290",
    "end": "3484340"
  },
  {
    "text": "support stored procedures no redshift currently does not support stored procedures UDF would be our first",
    "start": "3484340",
    "end": "3490460"
  },
  {
    "text": "extension of the platform feature yes",
    "start": "3490460",
    "end": "3495950"
  },
  {
    "text": "sir",
    "start": "3495950",
    "end": "3498099"
  },
  {
    "text": "I'm sorry I didn't hear the end of the question",
    "start": "3507470",
    "end": "3511810"
  },
  {
    "text": "is law so it sounds to me like the",
    "start": "3520849",
    "end": "3526039"
  },
  {
    "text": "question is is there a way to import data from something other than MapReduce",
    "start": "3526039",
    "end": "3531579"
  },
  {
    "text": "the answer is yes if you can get the data into s3 you can pretty much",
    "start": "3531579",
    "end": "3538430"
  },
  {
    "text": "seamlessly load it from there also I believe there is support to load it directly from the EMR we actually don't",
    "start": "3538430",
    "end": "3545359"
  },
  {
    "text": "load directly from UMR we put it on s3 first so we've got a permanent copy so",
    "start": "3545359",
    "end": "3562460"
  },
  {
    "text": "you've got a the question is how does redshift know the format of the log files on s3 you typically have to do a",
    "start": "3562460",
    "end": "3569690"
  },
  {
    "text": "transformation on this log files before you can load them depending on what it is if they're flat json redshift does",
    "start": "3569690",
    "end": "3575869"
  },
  {
    "text": "have support for that we tend to convert our data to CSV actually because that",
    "start": "3575869",
    "end": "3582319"
  },
  {
    "text": "was the first redshift load format and we also have built some infrastructure",
    "start": "3582319",
    "end": "3587839"
  },
  {
    "text": "in mycroft that lets us actually go in to logs that are a multiple layers deep",
    "start": "3587839",
    "end": "3593660"
  },
  {
    "text": "of JSON and extract different keys from it flatten that out and then load it that way",
    "start": "3593660",
    "end": "3600640"
  },
  {
    "text": "right so the question is what does it mean when redshift suppose s 3 it just means that every node has a direct",
    "start": "3607700",
    "end": "3614910"
  },
  {
    "text": "connection to s 3 buckets to go and pick up files that you specified and we still",
    "start": "3614910",
    "end": "3621660"
  },
  {
    "text": "have to make sense of those files so you'll have to have defined your tables and redshift you'll have to have the files correspond to the tables that you",
    "start": "3621660",
    "end": "3627750"
  },
  {
    "text": "want or you're going to get load errors yes sir",
    "start": "3627750",
    "end": "3633619"
  },
  {
    "text": "the question is can we share some information about our data load volumes and what are other common data load",
    "start": "3652729",
    "end": "3661910"
  },
  {
    "text": "volumes we're doing on the order of terabytes a day other than that I",
    "start": "3661910",
    "end": "3668299"
  },
  {
    "text": "probably can't get too much more specific we're definitely loading billions of rows a day into some of our",
    "start": "3668299",
    "end": "3673519"
  },
  {
    "text": "clusters but nothing too much more specific than that",
    "start": "3673519",
    "end": "3678849"
  },
  {
    "text": "so my answer wouldn't it also directed so we have several petabyte scale",
    "start": "3679660",
    "end": "3686150"
  },
  {
    "text": "cluster several hundred plus no trusters and so hundreds of terabytes do get",
    "start": "3686150",
    "end": "3692869"
  },
  {
    "text": "loaded in terms of performance you can order terabyte in about 129 seconds so",
    "start": "3692869",
    "end": "3698359"
  },
  {
    "text": "it's it is can be pretty fast and it's really you know every customer does it",
    "start": "3698359",
    "end": "3704239"
  },
  {
    "text": "slightly differently depending on how often they batch and how much change happens every day this was with I think",
    "start": "3704239",
    "end": "3717469"
  },
  {
    "text": "a hundred no trustor yeah",
    "start": "3717469",
    "end": "3724660"
  },
  {
    "text": "that's a great question it is what other ETL tools have you",
    "start": "3747250",
    "end": "3752900"
  },
  {
    "text": "looked at what would you recommend we actually were coming from a fairly",
    "start": "3752900",
    "end": "3757910"
  },
  {
    "text": "complicated I guess underlying data model so to adapt that we ended up writing custom",
    "start": "3757910",
    "end": "3764480"
  },
  {
    "text": "stuff I consequently don't have a great",
    "start": "3764480",
    "end": "3769580"
  },
  {
    "text": "handle and what the other tools are I would definitely suggest looking into them before writing your own certainly",
    "start": "3769580",
    "end": "3776510"
  },
  {
    "text": "and if you are just getting started also check out Mycroft when that is released",
    "start": "3776510",
    "end": "3781700"
  },
  {
    "text": "that could make it a lot easier to get up and running so and I don't know if I",
    "start": "3781700",
    "end": "3790280"
  },
  {
    "text": "can tell you which which tools you know",
    "start": "3790280",
    "end": "3796100"
  },
  {
    "text": "we have a partner's page where we so there are partners we work with ETL partners we work with regularly we make",
    "start": "3796100",
    "end": "3801140"
  },
  {
    "text": "sure that there are no issues when connecting to these I can certainly point you to a partner page and there's",
    "start": "3801140",
    "end": "3806960"
  },
  {
    "text": "a whole list that you can look at people that we worked with we know they have been successful with their customers",
    "start": "3806960",
    "end": "3811970"
  },
  {
    "text": "with our common customers and they do reach out to us in case of issues so you know you have at least decent experience",
    "start": "3811970",
    "end": "3820540"
  },
  {
    "text": "so so as you refer if you don't make forward-looking statements but yeah",
    "start": "3829220",
    "end": "3836090"
  },
  {
    "text": "having you know every all of us are making our services better strike yeah",
    "start": "3836090",
    "end": "3844119"
  },
  {
    "text": "all right thank you so much thanks thanks for coming",
    "start": "3846820",
    "end": "3851950"
  }
]