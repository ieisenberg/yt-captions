[
  {
    "start": "0",
    "end": "44000"
  },
  {
    "text": "all right well hello everybody and thank you for taking the time to listen to this webinar we're going to be talking",
    "start": "399",
    "end": "5640"
  },
  {
    "text": "about backup and recovery for Linux using Amazon S3 so we have a number of topics to",
    "start": "5640",
    "end": "12280"
  },
  {
    "text": "cover today I'm going to be moving fairly quickly we're going to take a look at today's backup challenges we'll",
    "start": "12280",
    "end": "17960"
  },
  {
    "text": "justify the use of Amazon S3 for backup we're going to take a look at a number of uh issues around backup storage and",
    "start": "17960",
    "end": "25760"
  },
  {
    "text": "Recovery in a fair amount of detail and then we're going to review options for both on premises and cloud-based backup",
    "start": "25760",
    "end": "33360"
  },
  {
    "text": "my goal is to give you enough information to let you get started doing backup to S3 today and I also want to",
    "start": "33360",
    "end": "39239"
  },
  {
    "text": "make sure that we have time at the end to address your questions so what are some of the",
    "start": "39239",
    "end": "44879"
  },
  {
    "start": "44000",
    "end": "44000"
  },
  {
    "text": "challenges inherent in doing backup today I found a neat article online and",
    "start": "44879",
    "end": "50079"
  },
  {
    "text": "it outlined some of the interesting things that that you have to worry about if you're doing backup uh cheaper Hardware has led to a",
    "start": "50079",
    "end": "57079"
  },
  {
    "text": "proliferation of stored data abundant bandwidth has made it easier and easier to move that data around and has led to",
    "start": "57079",
    "end": "64239"
  },
  {
    "text": "the the accumulation of even more data the paperless business has led to",
    "start": "64239",
    "end": "69439"
  },
  {
    "text": "organizations storing more and more information online as has the growth of",
    "start": "69439",
    "end": "75479"
  },
  {
    "text": "automated data collection such as little devices and sensors all in all different places that are are simply logging and",
    "start": "75479",
    "end": "81119"
  },
  {
    "text": "collecting data putting it safely away in for eventual",
    "start": "81119",
    "end": "86320"
  },
  {
    "text": "analysis this then leads into the whole topic of Big Data where where gigabytes",
    "start": "86320",
    "end": "91880"
  },
  {
    "text": "and terabytes of data are are collected analyzed and processed as businesses grow larger and",
    "start": "91880",
    "end": "99439"
  },
  {
    "text": "and become worldwide in scope certainly we see the the Strategic importance of data continuing to grow and we also see",
    "start": "99439",
    "end": "106680"
  },
  {
    "text": "issues around regulatory requirements where where in certain businesses you are simply required to keep various",
    "start": "106680",
    "end": "112719"
  },
  {
    "text": "types of data around either indefinitely Or for defined periods of time",
    "start": "112719",
    "end": "119280"
  },
  {
    "text": "so let's take a look at Amazon S3 you probably know just a little bit about S3 already so I want to cover some of the",
    "start": "121399",
    "end": "127119"
  },
  {
    "text": "the more important details of S3 to kind of justify it for",
    "start": "127119",
    "end": "132680"
  },
  {
    "start": "131000",
    "end": "131000"
  },
  {
    "text": "backup primary features of S3 that make it well suited to backup include the fact that it is off-site storage as a",
    "start": "132680",
    "end": "140360"
  },
  {
    "text": "service intrinsically S3 is fully redundant so you don't have to worry about making multiple copies of your",
    "start": "140360",
    "end": "146280"
  },
  {
    "text": "data by simply uploading it to S3 you are ensuring that redundancy within S3 data is stored on",
    "start": "146280",
    "end": "153120"
  },
  {
    "text": "multiple devices in multiple facilities S3 is designed for 119 of",
    "start": "153120",
    "end": "158560"
  },
  {
    "text": "durability and four nines of availability and very importantly S3 is",
    "start": "158560",
    "end": "164200"
  },
  {
    "text": "available in all eight of the adws regions and this gives you full control over data residency you get to decide",
    "start": "164200",
    "end": "171040"
  },
  {
    "text": "exactly where your data is stored and you you always choose a particular region for that over the six years that S3 has been",
    "start": "171040",
    "end": "179000"
  },
  {
    "text": "around we have Dr drop prices on it multiple times and a good way to think about this that's unique as compared to",
    "start": "179000",
    "end": "185400"
  },
  {
    "text": "owning your own storage devices is the fact that as we do lower the prices the",
    "start": "185400",
    "end": "190920"
  },
  {
    "text": "prices for your data you've already stored will also decline some additional features of S3",
    "start": "190920",
    "end": "197680"
  },
  {
    "text": "that you'll find it make it very attractive for backup include easy to enable server side encryption the",
    "start": "197680",
    "end": "203959"
  },
  {
    "text": "ability for you to transfer data from your your existing location to S3",
    "start": "203959",
    "end": "209080"
  },
  {
    "text": "through an encrypt SSL connection the ability for you to use ACLS Access Control lists to regulate",
    "start": "209080",
    "end": "216519"
  },
  {
    "text": "access to your data and some other Advanced features such as object exporation so you can tell us3 how long",
    "start": "216519",
    "end": "222439"
  },
  {
    "text": "you'd like to retain individual objects access logging so you can see every get",
    "start": "222439",
    "end": "228439"
  },
  {
    "text": "and every put of data in a particular S3 bucket all of the standard physical security features of of AWS that we've",
    "start": "228439",
    "end": "236239"
  },
  {
    "text": "outlined in previous webinars in our security white paper and also location control as I outlined previously the",
    "start": "236239",
    "end": "242319"
  },
  {
    "text": "idea that you have eight separate S3 locations and you can choose exactly which one of those you would like to use",
    "start": "242319",
    "end": "247519"
  },
  {
    "text": "to store your data few other final reasons S3's pay as",
    "start": "247519",
    "end": "254879"
  },
  {
    "text": "you go model means that you're not going to have any sticker shock your cost will scale directly with",
    "start": "254879",
    "end": "260280"
  },
  {
    "text": "usage you don't have to buy expensive hardware and you don't have to pay for Hardware in in large kind of uh",
    "start": "260280",
    "end": "267199"
  },
  {
    "text": "stairstep increments you simply keep storing data into S3 and in fact because of the tiered pricing model for S3 your",
    "start": "267199",
    "end": "274280"
  },
  {
    "text": "your effective price per gigabyte will continue to go down as you store more and more",
    "start": "274280",
    "end": "279800"
  },
  {
    "text": "data S3 has seen broad industry acceptance as of the end of 2011 we have",
    "start": "279800",
    "end": "285800"
  },
  {
    "text": "over 762 billion objects stored there and then as we'll talk about later many",
    "start": "285800",
    "end": "291680"
  },
  {
    "text": "thirdparty tools and applications are available that were either adapted to or custom built to store data into S3",
    "start": "291680",
    "end": "301639"
  },
  {
    "text": "let's go into some of the challenges of backup storage and Recovery in more",
    "start": "302039",
    "end": "307560"
  },
  {
    "start": "307000",
    "end": "307000"
  },
  {
    "text": "depth as I started thinking about this webinar I realized there are many many things to think about there are backup",
    "start": "307560",
    "end": "313919"
  },
  {
    "text": "considerations storage considerations and also recovery",
    "start": "313919",
    "end": "319440"
  },
  {
    "text": "considerations when you start to think about backing up what do you need to back up are they files are they",
    "start": "319759",
    "end": "326360"
  },
  {
    "text": "databases must you back up all of the fil databases you have or can you be",
    "start": "326360",
    "end": "331720"
  },
  {
    "text": "more selective do you do full backups where you're always filling up your copying",
    "start": "331720",
    "end": "338120"
  },
  {
    "text": "all of your data fresh each time or you can you do periodic full backups and",
    "start": "338120",
    "end": "343240"
  },
  {
    "text": "then in between those do incremental backups to Simply update the changed information how frequently is this daily",
    "start": "343240",
    "end": "350120"
  },
  {
    "text": "weekly monthly how much data do you need to back up what is the enal growth rate",
    "start": "350120",
    "end": "355280"
  },
  {
    "text": "in the amount of data that you have stored",
    "start": "355280",
    "end": "359319"
  },
  {
    "start": "359000",
    "end": "359000"
  },
  {
    "text": "then you have to start thinking about things how long is it going to actually take me to do a backup",
    "start": "360520",
    "end": "366080"
  },
  {
    "text": "run how do I deal with the fact that data might be changing underneath me the data consistency issue as we run the",
    "start": "366080",
    "end": "373280"
  },
  {
    "text": "backup if you have a business that's online 24/7 do you actually have a backup window where there's a quiet time",
    "start": "373280",
    "end": "379800"
  },
  {
    "text": "where you can safely do a backup or do you need to do backups live while your system is is up and running and being",
    "start": "379800",
    "end": "387720"
  },
  {
    "text": "accessed traditionally you need to think about Hardware your in particular your backup device you need to think about",
    "start": "387720",
    "end": "395120"
  },
  {
    "text": "how much that device is going to cost you it is a a mechanical device with a number of moving parts so it's going to",
    "start": "395120",
    "end": "401280"
  },
  {
    "text": "have a a defined useful life because of that you need to have have to think about having a spare at least one spare",
    "start": "401280",
    "end": "407880"
  },
  {
    "text": "device perhaps you need to have a primary and a backup device for each facility in addition to the cost of the",
    "start": "407880",
    "end": "414639"
  },
  {
    "text": "backup device you're going to need to think about the cost per gigabyte of the actual backup medium that you",
    "start": "414639",
    "end": "421520"
  },
  {
    "text": "use once you've made those backups you need to think about where you're going to put them if you store those backups on site",
    "start": "422160",
    "end": "429840"
  },
  {
    "text": "you're going to have to worry about what if some disaster strikes your your local data center if you're going to have some sort",
    "start": "429840",
    "end": "436160"
  },
  {
    "text": "of off-site storage for those backups you have to think about the the cost involved in that you're going to have to",
    "start": "436160",
    "end": "441800"
  },
  {
    "text": "think about how long you keep these backups around if if you have a large amount of data you're going to start to",
    "start": "441800",
    "end": "447319"
  },
  {
    "text": "accumulate backup tapes perhaps you're going to run into some limitations as your physical storage capabilities allow",
    "start": "447319",
    "end": "455520"
  },
  {
    "text": "and you also have to think about the fact that that magnetic media especially tapes is very very prone to",
    "start": "455520",
    "end": "461080"
  },
  {
    "text": "deterioration over time finally if you're thinking about offsite you need to think about what",
    "start": "461080",
    "end": "468120"
  },
  {
    "text": "it's going to take to retrieve that data it's wonderful to think about taking your data and putting it in in the",
    "start": "468120",
    "end": "473319"
  },
  {
    "text": "middle of a of a mountain or a mine somewhere far far away but you need to think about the the physical real world",
    "start": "473319",
    "end": "479560"
  },
  {
    "text": "latency of being able to get that data back from that safe but somewhat inaccessible",
    "start": "479560",
    "end": "486520"
  },
  {
    "text": "location and then if you're backing up lots of data you need to think about organizing and cataloging you're going",
    "start": "487639",
    "end": "493879"
  },
  {
    "text": "to start putting potentially millions of files on every backup tape you're going",
    "start": "493879",
    "end": "499120"
  },
  {
    "text": "to have a proliferation of backup types let's say you do need a particular file",
    "start": "499120",
    "end": "504199"
  },
  {
    "text": "or an entire device worth of uh of data to restore how do you find the right one",
    "start": "504199",
    "end": "510440"
  },
  {
    "text": "did you actually store the right data on there when did you store it when did that data",
    "start": "510440",
    "end": "517919"
  },
  {
    "text": "expire so let's say we're going to go ahead and do a retrieval which of the backup tapes do",
    "start": "518000",
    "end": "523680"
  },
  {
    "start": "519000",
    "end": "519000"
  },
  {
    "text": "you need how long is it going to take you to retrieve this if it was stored",
    "start": "523680",
    "end": "528800"
  },
  {
    "text": "off site what if you make a request to your your local storage facility or remote",
    "start": "528800",
    "end": "534680"
  },
  {
    "text": "storage facility and they say we actually can't locate that physical uh piece piece of uh piece of uh",
    "start": "534680",
    "end": "541720"
  },
  {
    "text": "information suppose you get it back and it turns out that the you you thought you were doing high quality backups but",
    "start": "541720",
    "end": "547839"
  },
  {
    "text": "either due to immediate deterioration or to an an imperfect backup process",
    "start": "547839",
    "end": "553120"
  },
  {
    "text": "perhaps your your data is is corrupt and then finally what is the actual expected time to to transfer the",
    "start": "553120",
    "end": "560440"
  },
  {
    "text": "data back from from the device onto your your local hard drive and actually be",
    "start": "560440",
    "end": "565880"
  },
  {
    "text": "back up and running and fully recovered so as as you can see when you start to think about backup storage and retrieval",
    "start": "565880",
    "end": "573480"
  },
  {
    "text": "there are just a huge number of different options to to think",
    "start": "573480",
    "end": "578120"
  },
  {
    "text": "about once we start storing the data we need to think about expiration and",
    "start": "579760",
    "end": "584920"
  },
  {
    "start": "580000",
    "end": "580000"
  },
  {
    "text": "rotation how long do we want to retain the backups what do we do when when the",
    "start": "584920",
    "end": "590360"
  },
  {
    "text": "old backup tapes expire do we destroy them do we discard them do we overwrite",
    "start": "590360",
    "end": "596519"
  },
  {
    "text": "them if we're overriding them and we have a rotation plan in place at some point they're going to become too old to",
    "start": "596519",
    "end": "603640"
  },
  {
    "text": "to be useful because perhaps they're going to start to see some physical wear and tear at that point you must realize you",
    "start": "603640",
    "end": "610760"
  },
  {
    "text": "have valuable data stored on on those old backup tapes how do you safely and securely dispose of those so that they",
    "start": "610760",
    "end": "617360"
  },
  {
    "text": "the data doesn't end up in a a place where it does not",
    "start": "617360",
    "end": "621800"
  },
  {
    "text": "belong and then we come to financial considerations how much is it actually Worth to keep your business up and",
    "start": "622720",
    "end": "629640"
  },
  {
    "start": "623000",
    "end": "623000"
  },
  {
    "text": "running what was the cost to obtain that data build up those databases and what",
    "start": "629640",
    "end": "635120"
  },
  {
    "text": "is the business value of the data if you have some kind of a Black Swan event and you do have a failure",
    "start": "635120",
    "end": "642360"
  },
  {
    "text": "what is the cost to your business in dollars per minute let's say in",
    "start": "642360",
    "end": "647720"
  },
  {
    "text": "downtime what is it going to be your your cost to store your data and when you might want to think about that in in",
    "start": "647720",
    "end": "653600"
  },
  {
    "text": "a cost per gigabyte once again the cost of your devices how many do you need",
    "start": "653600",
    "end": "659800"
  },
  {
    "text": "and then after we go through all these considerations what are the operational and Personnel costs that that come into",
    "start": "659800",
    "end": "666079"
  },
  {
    "text": "play here so as you can see lots and lots of different things to think about so let's start to to dive a little",
    "start": "666079",
    "end": "673320"
  },
  {
    "text": "bit deeper and be before we do that I want to go go super super deep on this and make sure that there there's one",
    "start": "673320",
    "end": "678800"
  },
  {
    "text": "really important technical aspect that comes into play quite often here we should cover up",
    "start": "678800",
    "end": "684639"
  },
  {
    "start": "683000",
    "end": "683000"
  },
  {
    "text": "front the the first is that it's going to turn out that files and databases often require distinct backup",
    "start": "684839",
    "end": "691360"
  },
  {
    "text": "strategies with files you have many many objects that are of variable sized they",
    "start": "691360",
    "end": "697600"
  },
  {
    "text": "often dat stamped objects which give you the ability to do incremental backups or the ability to Simply from a a given",
    "start": "697600",
    "end": "704079"
  },
  {
    "text": "starting point back up only the the changed files when you are processing and",
    "start": "704079",
    "end": "710800"
  },
  {
    "text": "backing up files you do have a really uh important issue where at any given time you have some programs reading and",
    "start": "710800",
    "end": "717880"
  },
  {
    "text": "writing the data these programs are going to have some open files there is a consistency issue that",
    "start": "717880",
    "end": "724519"
  },
  {
    "text": "means at some point the program might be writing a sequence of objects to a file",
    "start": "724519",
    "end": "730680"
  },
  {
    "text": "if you take the backup at the wrong moment perhaps two pieces of data that depend on each other for for consistency",
    "start": "730680",
    "end": "737040"
  },
  {
    "text": "maybe the first one has been written the second one is in a a local buffer but hasn't yet actually been sent to the the",
    "start": "737040",
    "end": "744160"
  },
  {
    "text": "file so there are some consistency issues and we'll talk about ways to address that in a little bit",
    "start": "744160",
    "end": "750800"
  },
  {
    "text": "databases on the other hand are generally very large and often monolithic objects that we would back up",
    "start": "750800",
    "end": "757000"
  },
  {
    "text": "and similar to files we have to worry about things such as open transactions where we we begin a transaction we are",
    "start": "757000",
    "end": "763160"
  },
  {
    "text": "writing a a large number of items to a database but until we close out that",
    "start": "763160",
    "end": "768639"
  },
  {
    "text": "transaction those items are not permanently stored in the database perhaps there are locks",
    "start": "768639",
    "end": "774320"
  },
  {
    "text": "involved and we're we're we're locking tables or locking rows in the database if we suspend operations to the database",
    "start": "774320",
    "end": "781160"
  },
  {
    "text": "and we attempt to do a backup at that point timeouts might actually fire because the the we expected these",
    "start": "781160",
    "end": "786800"
  },
  {
    "text": "transactions in these operations to take far shorter amounts of time than they would happen when we are doing a",
    "start": "786800",
    "end": "794519"
  },
  {
    "text": "backup and and then if we're thinking about doing whole device backup and we we have a a raid based file system we",
    "start": "794519",
    "end": "801560"
  },
  {
    "text": "need to think about the idea of backing up at The Logical level meaning that we're backing up the file system versus",
    "start": "801560",
    "end": "807760"
  },
  {
    "text": "backing up at the physical level backing up each of the devices that that comprise that file system if we're going",
    "start": "807760",
    "end": "814760"
  },
  {
    "text": "at the physical level of a raid system again we have a a very very uh crucial",
    "start": "814760",
    "end": "820279"
  },
  {
    "text": "consistency issue we need to make sure that we we stop all rights to that system we synchronize all the data to",
    "start": "820279",
    "end": "827040"
  },
  {
    "text": "disk that we effectively in parallel we want to take backups of all those physical raid devices we want to time",
    "start": "827040",
    "end": "834079"
  },
  {
    "text": "stamp and catalog them so that if we do need to recover that backed up raid system that we bring back all the",
    "start": "834079",
    "end": "839279"
  },
  {
    "text": "snapshots that were taken at the same time to guarantee logical consistency of the",
    "start": "839279",
    "end": "846240"
  },
  {
    "start": "846000",
    "end": "846000"
  },
  {
    "text": "data so as you can tell from what I just told you logical consistency is is",
    "start": "847480",
    "end": "853440"
  },
  {
    "text": "simply a must there's a number of of different models but they all come down to this the same basic operational model",
    "start": "853440",
    "end": "861360"
  },
  {
    "text": "we need to sync all of our in-memory data to disk we need to freeze our database or our file system for example",
    "start": "861360",
    "end": "868160"
  },
  {
    "text": "on Linux there's a command called FS freeze that will suspend right operations to the the a particular file",
    "start": "868160",
    "end": "875519"
  },
  {
    "text": "system if we're using a database we might for example use MySQL stop and actually stop the database in order to",
    "start": "875519",
    "end": "881880"
  },
  {
    "text": "prevent it from writing to the disk we're then going to run the backup we're",
    "start": "881880",
    "end": "887120"
  },
  {
    "text": "going to unfreeze our database and or our file system and we'll",
    "start": "887120",
    "end": "892399"
  },
  {
    "text": "continue in most cases running that backup is going to take some some non-trivial amount of time know minutes",
    "start": "892399",
    "end": "899519"
  },
  {
    "text": "up to maybe an hour or or larger given you know depending on how much data you have so we need to think about uh about",
    "start": "899519",
    "end": "906920"
  },
  {
    "text": "what might be happening while while your system is offline you might need to think about let's say the database",
    "start": "906920",
    "end": "912440"
  },
  {
    "text": "doesn't run properly we need to still unfreeze and uh and continue from",
    "start": "912440",
    "end": "919279"
  },
  {
    "text": "there let's take a look at a number of different ways that we can back",
    "start": "919279",
    "end": "925560"
  },
  {
    "start": "925000",
    "end": "925000"
  },
  {
    "text": "up we can go first let's talk about on premise to Cloud backup architectures",
    "start": "925600",
    "end": "932000"
  },
  {
    "text": "and when I was putting my presentation together I I tried to review a number of different approaches and to identify a",
    "start": "932000",
    "end": "937360"
  },
  {
    "text": "number of of both open source and Commercial tools to do this in the",
    "start": "937360",
    "end": "942480"
  },
  {
    "text": "interest of space and time I I couldn't cover every last possible tool I I certainly expect over time to to grow",
    "start": "942480",
    "end": "949240"
  },
  {
    "text": "grow the list of of what it is that we can talk about backup wise so the the four main architectures that I saw were",
    "start": "949240",
    "end": "956560"
  },
  {
    "text": "the the transparent model the arive and upload model the backup tools model and",
    "start": "956560",
    "end": "963079"
  },
  {
    "text": "then several backup as a service models as well so let's go ahead and take a look",
    "start": "963079",
    "end": "968399"
  },
  {
    "text": "at each of these in turn with the transparent backup model",
    "start": "968399",
    "end": "975279"
  },
  {
    "text": "it's effectively file sharing what we're doing is we have a shared file system that is shared between your your Linux",
    "start": "975279",
    "end": "982680"
  },
  {
    "text": "system that would be either local on your desktop or in your data center and some cloud-based storage when you make a",
    "start": "982680",
    "end": "989800"
  },
  {
    "text": "local change those local changes are noticed by the shared file system and then directly uploaded or mirrored to",
    "start": "989800",
    "end": "995720"
  },
  {
    "text": "the cloud one advantage here is that this system simply just works you don't have",
    "start": "995720",
    "end": "1002319"
  },
  {
    "text": "to think about periodic backups you don't need scheduling you simply proceed with regular operations you save your",
    "start": "1002319",
    "end": "1008800"
  },
  {
    "text": "information locally and then you mirror that information up to the cloud so how do we go about doing",
    "start": "1008800",
    "end": "1017199"
  },
  {
    "start": "1016000",
    "end": "1016000"
  },
  {
    "text": "that one well-known application to do this is is Dropbox with Dropbox the",
    "start": "1017399",
    "end": "1022959"
  },
  {
    "text": "master copies of all your files are stored in S3 and to use Dropbox to to",
    "start": "1022959",
    "end": "1028600"
  },
  {
    "text": "back up your Linux systems you simply install the Linux client either on premises or on your your cloud-based",
    "start": "1028600",
    "end": "1035000"
  },
  {
    "text": "server to backup files You're simply copying them to your your Dropbox directory on your local system when you",
    "start": "1035000",
    "end": "1042038"
  },
  {
    "text": "do that Dropbox will notice this and will will then copy your your files up to the the Master Copy in S3 and we'll",
    "start": "1042039",
    "end": "1048079"
  },
  {
    "text": "also mirror them to all the other systems where you have that same file shared into Dropbox to recover You're simply going",
    "start": "1048079",
    "end": "1055120"
  },
  {
    "text": "to copy the files from that Dropbox directory back to your your local file",
    "start": "1055120",
    "end": "1061200"
  },
  {
    "text": "system another way to create a transparent file system is to use an a",
    "start": "1061799",
    "end": "1067039"
  },
  {
    "text": "product called S3 fuse in technical terms this is what's called a userspace file system and you you would run this",
    "start": "1067039",
    "end": "1074039"
  },
  {
    "text": "on on your Linux system it actually runs as a regular user space application so",
    "start": "1074039",
    "end": "1079559"
  },
  {
    "text": "you don't need to install any kernel modifications or anything like that when you configure and start to run S3 fuse",
    "start": "1079559",
    "end": "1087200"
  },
  {
    "text": "it creates this file system for you you can create it at any Mount Point you'd like in your directory tree you store",
    "start": "1087200",
    "end": "1093679"
  },
  {
    "text": "your files locally and then they're automatically uploaded to the S3 bucket that you designate so just like Dropbox",
    "start": "1093679",
    "end": "1100440"
  },
  {
    "text": "you don't need to make any changes to your application you simply store the files in a particular location and then",
    "start": "1100440",
    "end": "1105760"
  },
  {
    "text": "the S3 fuse system will take care of getting those the files up to",
    "start": "1105760",
    "end": "1111320"
  },
  {
    "start": "1111000",
    "end": "1111000"
  },
  {
    "text": "S3 as part of my preparation here I tried to use as many of the different tools and products that I talk about as",
    "start": "1111320",
    "end": "1117360"
  },
  {
    "text": "possible so I I configured and actually ran S3 fuse turned out to be really",
    "start": "1117360",
    "end": "1122760"
  },
  {
    "text": "simple and straightforward I downloaded the code and compiled it I can I put my AWS credentials into a",
    "start": "1122760",
    "end": "1131480"
  },
  {
    "text": "password file and set the permissions on that password file once I had done that I created a",
    "start": "1131480",
    "end": "1137600"
  },
  {
    "text": "mount point which I called sls3 and then I simply launched s3fs the",
    "start": "1137600",
    "end": "1144360"
  },
  {
    "text": "first argument to that command is the name of the S3 bucket within my account that I would like to to use as the the",
    "start": "1144360",
    "end": "1151799"
  },
  {
    "text": "cloud recipient of my data and the second one is simply the mount point that I would like s3fs to to use as the",
    "start": "1151799",
    "end": "1160480"
  },
  {
    "text": "starting point for data storage once i' I've done this I can simply copy files into SL the/ S3 tree",
    "start": "1160480",
    "end": "1169159"
  },
  {
    "text": "when I do that S3 fuse will automatically copy that data into",
    "start": "1169159",
    "end": "1175880"
  },
  {
    "text": "S3 here's a view after I've actually done that on the left you can see the AWS Management console you can see that",
    "start": "1175880",
    "end": "1183200"
  },
  {
    "start": "1176000",
    "end": "1176000"
  },
  {
    "text": "my my JB bar- s3fs bucket is selected and you can see that there are a number",
    "start": "1183200",
    "end": "1188720"
  },
  {
    "text": "of objects in there and on the right you can see the Shelf from my local Linux server where I copied those files from",
    "start": "1188720",
    "end": "1196600"
  },
  {
    "text": "from the Linux server into the SLS 3 FS",
    "start": "1196600",
    "end": "1201360"
  },
  {
    "text": "directory the next option for you is going to be the the archiving and uploading",
    "start": "1202400",
    "end": "1207679"
  },
  {
    "text": "model with this model you use existing local tools to create a file archive on",
    "start": "1207679",
    "end": "1212960"
  },
  {
    "text": "the local machine so I'm kind of old school Linux so things like tar and cpio and so forth are the the commands I'm",
    "start": "1212960",
    "end": "1219480"
  },
  {
    "text": "very familiar with to create these local archives when you do this you of course must have sufficient local free space to",
    "start": "1219480",
    "end": "1226159"
  },
  {
    "text": "hold the the archive before you upload it once you have created this archive",
    "start": "1226159",
    "end": "1231840"
  },
  {
    "text": "you're going to upload it to S3 and then you're going to over time you're going to want to manage that archive in S3",
    "start": "1231840",
    "end": "1237600"
  },
  {
    "text": "you're going to want to expire old old files and so forth to make sure you have the just as many backups around as you",
    "start": "1237600",
    "end": "1245679"
  },
  {
    "text": "need so let's take a look at an example of how we do this now this is probably the the the least overhead the the",
    "start": "1245679",
    "end": "1253480"
  },
  {
    "text": "simplest way as far as if if you're comfortable at the Linux command line nothing to to buy you can do all of this",
    "start": "1253480",
    "end": "1260760"
  },
  {
    "text": "with with simply things that you already have or you can easily download you need the tar command which you almost",
    "start": "1260760",
    "end": "1266200"
  },
  {
    "text": "certainly already have installed on your machine and you S3 curl which is a free download from the AWS site S3 curl is",
    "start": "1266200",
    "end": "1274679"
  },
  {
    "text": "just a simple wrapper around the curl utility that you probably know about already takes care of of um",
    "start": "1274679",
    "end": "1280799"
  },
  {
    "text": "authenticating your request before you make the actual request S3 and provide some built-in commands to make it a",
    "start": "1280799",
    "end": "1287080"
  },
  {
    "text": "little bit easier to push data to S3 when you set up S3 curl you can",
    "start": "1287080",
    "end": "1292679"
  },
  {
    "text": "supply your Adis credentials in the command line or you can put those in a configuration",
    "start": "1292679",
    "end": "1298559"
  },
  {
    "text": "file your choice for how you'd like to do it step one of course you're going to",
    "start": "1298559",
    "end": "1305200"
  },
  {
    "start": "1304000",
    "end": "1304000"
  },
  {
    "text": "use your your trusty old tar command to create an archive in this create case I went to my server I backed up my public",
    "start": "1305200",
    "end": "1312400"
  },
  {
    "text": "HTML directory to a file called backup public HTML 201203 28.",
    "start": "1312400",
    "end": "1319520"
  },
  {
    "text": "I I didn't even specify any particular compression options in there because I wanted to keep this as clean and simple",
    "start": "1319520",
    "end": "1324919"
  },
  {
    "text": "and straightforward as possible once I have done that I can then run the S3 curl command to upload",
    "start": "1324919",
    "end": "1332880"
  },
  {
    "text": "that archive to S3 again very very simple and straightforward um command we run S3",
    "start": "1332880",
    "end": "1341240"
  },
  {
    "text": "curl we Supply it with a with our AWS uh ID and our secret key we we tell that we",
    "start": "1341240",
    "end": "1349159"
  },
  {
    "text": "want to put a particular local file in this case the backup file that I just created then um very very importantly",
    "start": "1349159",
    "end": "1356200"
  },
  {
    "text": "here if you're going to be running this command you'll see two dashes before the put so th those two dashes specify that",
    "start": "1356200",
    "end": "1361960"
  },
  {
    "text": "it's a a Linux command line argument in the usual way the next set of two dashes are used to separate the essentially the",
    "start": "1361960",
    "end": "1369320"
  },
  {
    "text": "first half of the arguments of the curl command from the second half so there actually a space after that second pair of dashes so after that second pair of",
    "start": "1369320",
    "end": "1377039"
  },
  {
    "text": "dashes we simply have a destination in S3 so I I simply point to s3. Amazon",
    "start": "1377039",
    "end": "1383720"
  },
  {
    "text": "aws.com I point to my Jeff Bar-B backup bucket and then I give the the final",
    "start": "1383720",
    "end": "1390240"
  },
  {
    "text": "part of the key is where I would like that file to end up when it's uploaded to S3 so create the archive and upload",
    "start": "1390240",
    "end": "1396840"
  },
  {
    "start": "1393000",
    "end": "1393000"
  },
  {
    "text": "to S3 um we're showing this running from the command line if you're going to be",
    "start": "1396840",
    "end": "1402000"
  },
  {
    "text": "doing this in production you'd almost certainly automate it and put it in a a cron script so I run my S3 curl command and",
    "start": "1402000",
    "end": "1410039"
  },
  {
    "text": "the file is going to upload S3 um because this is using curl in the background it gives you a nice status",
    "start": "1410039",
    "end": "1415200"
  },
  {
    "text": "report as the file is uploading for production use and the way",
    "start": "1415200",
    "end": "1420919"
  },
  {
    "text": "I have this arranged on several of my own systems I run the tar and I run the upload from Acron with with several",
    "start": "1420919",
    "end": "1426880"
  },
  {
    "text": "different levels of error checking in there as well to make sure that I've properly created the archive and I've properly uploaded it to",
    "start": "1426880",
    "end": "1434600"
  },
  {
    "text": "S3 so here's the final um result of that I go to my Jeff bar backup on my console",
    "start": "1435159",
    "end": "1441480"
  },
  {
    "text": "and I can see that that file was successfully uploaded to",
    "start": "1441480",
    "end": "1446320"
  },
  {
    "text": "S3 once I have that file N3 there's there's a couple of of really simple to enable options that that I thought were",
    "start": "1447480",
    "end": "1453480"
  },
  {
    "text": "worth pointing out to you the first is I can select the file in the console I can go down to the properties at the bottom",
    "start": "1453480",
    "end": "1459559"
  },
  {
    "text": "of the page and I can enable server side encryption so here I simply checked aes256 and S3 will will do serers side",
    "start": "1459559",
    "end": "1467960"
  },
  {
    "text": "encryption of that file it will automatically manage my keys for me so",
    "start": "1467960",
    "end": "1473360"
  },
  {
    "text": "just that one simple checking of that option was all it took to encrypt the backup",
    "start": "1473360",
    "end": "1478559"
  },
  {
    "text": "file since I'm going to be running this backup every day I certainly don't want to accumulate an infinite number of",
    "start": "1478559",
    "end": "1484919"
  },
  {
    "text": "backup files so I simply went to the life cycle tab of the bucket properties",
    "start": "1484919",
    "end": "1490279"
  },
  {
    "text": "not the object properties but the entire bucket and I in that life cycle tab I",
    "start": "1490279",
    "end": "1495799"
  },
  {
    "text": "specify a prefix so here I'm saying that any file that has a name starting with",
    "start": "1495799",
    "end": "1501279"
  },
  {
    "text": "backup I would like S3 to expire and delete it when that file is older than",
    "start": "1501279",
    "end": "1506600"
  },
  {
    "text": "45 days so here I've taken care of of two operations that are are generally",
    "start": "1506600",
    "end": "1512000"
  },
  {
    "text": "things that would be of concern to you when you back up keeping your data safe and secure and getting rid of old files",
    "start": "1512000",
    "end": "1518399"
  },
  {
    "text": "both of those are simply checkbox options in the S3",
    "start": "1518399",
    "end": "1523200"
  },
  {
    "start": "1524000",
    "end": "1524000"
  },
  {
    "text": "console this example looked really really simple and clean but as I thought about it I I realized you you get an",
    "start": "1524279",
    "end": "1529320"
  },
  {
    "text": "awful lot as basically intrinsic properties of the backup I backed up about 300 megabytes in in that one",
    "start": "1529320",
    "end": "1537039"
  },
  {
    "text": "backup but I can store as much as I want I never hit any particular kind of a wall where I say well it's it's too much",
    "start": "1537039",
    "end": "1543640"
  },
  {
    "text": "data S3 can't hold it I can store that data for as long as I'd like there",
    "start": "1543640",
    "end": "1549399"
  },
  {
    "text": "there's no reason at all that I need to ever think about deleting that file other than the fact that I don't need it anymore that simple pair of commands",
    "start": "1549399",
    "end": "1556760"
  },
  {
    "text": "gave me 11 nines of dur ability for that data and I'm paying only for the amount",
    "start": "1556760",
    "end": "1562080"
  },
  {
    "text": "of data that I store so so given that it costs 12.5 cents per gigabyte month to store data in S3 that file is going to",
    "start": "1562080",
    "end": "1570520"
  },
  {
    "text": "cost me a grand total of of 4 cents per month pretty low cost to have have that security of having my data backed up as",
    "start": "1570520",
    "end": "1578240"
  },
  {
    "text": "you saw the data is encrypted I turned on automatic expiration and then all the tooling was",
    "start": "1578240",
    "end": "1585399"
  },
  {
    "text": "free we used the tar command we used the S3 curl command didn't have to buy anything didn't have to pay for",
    "start": "1585399",
    "end": "1591039"
  },
  {
    "text": "anything once that data is is backed up I can RoR restore it to my on premis of",
    "start": "1591039",
    "end": "1596760"
  },
  {
    "text": "server or I can restore it to a cloud-based server so simple simple backup but a number of really important",
    "start": "1596760",
    "end": "1602919"
  },
  {
    "text": "attributes that I thought were worth pointing out to you there are also a number of",
    "start": "1602919",
    "end": "1609640"
  },
  {
    "start": "1607000",
    "end": "1607000"
  },
  {
    "text": "commercial backup Tools in fact there were too many different tools for me to really give all of them Justice in in",
    "start": "1609640",
    "end": "1615200"
  },
  {
    "text": "this presentation I'll just point out one to you here um the company is zanda their",
    "start": "1615200",
    "end": "1620799"
  },
  {
    "text": "product is called Amanda Enterprise and the way this works is that you run a number of different backup agents on",
    "start": "1620799",
    "end": "1626919"
  },
  {
    "text": "your different client machines either on premises or in the cloud the the Amanda Enterprise server",
    "start": "1626919",
    "end": "1635159"
  },
  {
    "text": "that runs on one a single server within within your um organization coordinates the action of of all these client apps",
    "start": "1635159",
    "end": "1641799"
  },
  {
    "text": "and then manages the backup from local up to Amazon S3 you can learn more about that at",
    "start": "1641799",
    "end": "1648480"
  },
  {
    "text": "zander.com there are also some organizations",
    "start": "1648480",
    "end": "1653880"
  },
  {
    "start": "1650000",
    "end": "1650000"
  },
  {
    "text": "offering backup as a service for example tarsnap tocom will encrypt your data",
    "start": "1653880",
    "end": "1659799"
  },
  {
    "text": "before uploading and their storage is uh they have kind of a humorous model here",
    "start": "1659799",
    "end": "1664840"
  },
  {
    "text": "their pricing model is 300 Pico dollars per B per month which works out to 30 cents per gigabyte per month so they",
    "start": "1664840",
    "end": "1672360"
  },
  {
    "text": "encrypt the data before it leaves your local machine they give you a client side application that that's very very",
    "start": "1672360",
    "end": "1677720"
  },
  {
    "text": "similar to to the tar command that you know and already use and very very simple to sign up for a tar snap",
    "start": "1677720",
    "end": "1685559"
  },
  {
    "text": "account download and install the the tar snap command and start backing up to the",
    "start": "1685559",
    "end": "1692518"
  },
  {
    "text": "cloud again as I started researching this space in preparation I found that there's just a very wide variety of",
    "start": "1692600",
    "end": "1698799"
  },
  {
    "text": "tools and I I do apologize in advance if uh if I didn't give your particular one the attention that it deserves here uh",
    "start": "1698799",
    "end": "1706720"
  },
  {
    "text": "at mal. you'll find an an FTP secure FTP and webdav Gateway to",
    "start": "1706720",
    "end": "1713080"
  },
  {
    "text": "S3 you can use this to to push your data to S3 through an FTP command this might",
    "start": "1713080",
    "end": "1719799"
  },
  {
    "text": "be useful if you already are using FTP to transfer backup files within your organization and you would like a a",
    "start": "1719799",
    "end": "1726080"
  },
  {
    "start": "1720000",
    "end": "1720000"
  },
  {
    "text": "straightforward way to use that existing backup process and and push the data to the cloud through",
    "start": "1726080",
    "end": "1732919"
  },
  {
    "text": "FTP this is currently run as a service but I have been in communication with the author of this service um he tells",
    "start": "1732919",
    "end": "1739720"
  },
  {
    "text": "me that at some point he proposes to make this available as an Amazon machine image so you could run it on your own",
    "start": "1739720",
    "end": "1745000"
  },
  {
    "text": "Amazon ec2 instance as well so at this point all the options",
    "start": "1745000",
    "end": "1751279"
  },
  {
    "text": "that we discussed were primarily focused at starting out on premises and getting your data up to the cloud let's take a",
    "start": "1751279",
    "end": "1759159"
  },
  {
    "text": "look at a couple of AWS specific options that are more relevant if you already have your data stored in AWS",
    "start": "1759159",
    "end": "1768320"
  },
  {
    "start": "1767000",
    "end": "1767000"
  },
  {
    "text": "the first thing I should mention is that everything that we talked about so far still applies if you wanted to use tar",
    "start": "1768320",
    "end": "1774760"
  },
  {
    "text": "and S3 curl to move data from your ec2 server to S3 that's totally totally",
    "start": "1774760",
    "end": "1780440"
  },
  {
    "text": "acceptable that'll work just fine and in fact you'll because you're running internal to AWS you're going to have",
    "start": "1780440",
    "end": "1786880"
  },
  {
    "text": "very very low latency between ec2 and S3 it's going to be run the data transfer will run even faster and there is no",
    "start": "1786880",
    "end": "1794519"
  },
  {
    "text": "charge to transfer data between ec2 and S3 within the same same region however as we'll see the cloud",
    "start": "1794519",
    "end": "1801640"
  },
  {
    "text": "adds some additional options we're going to talk a lot about EBS snapshot backups and we'll mention it passing the RDS",
    "start": "1801640",
    "end": "1808399"
  },
  {
    "text": "snapshot backups as well let's quickly review what an EBS or",
    "start": "1808399",
    "end": "1814000"
  },
  {
    "text": "elastic Block store volume is all about EBS is a a network storage system",
    "start": "1814000",
    "end": "1822399"
  },
  {
    "text": "for ec2 instances you create EBS volumes that can be anywhere in size from 1 GB",
    "start": "1822399",
    "end": "1828120"
  },
  {
    "start": "1826000",
    "end": "1826000"
  },
  {
    "text": "to 1 terab and you always do that in a particular AWS availability",
    "start": "1828120",
    "end": "1833559"
  },
  {
    "text": "Zone after you create the volume you attach it to an ec2 instance before the",
    "start": "1833559",
    "end": "1838640"
  },
  {
    "text": "first time you use it you're going to run mkfs on it to create a file system and then you you mount it and you start",
    "start": "1838640",
    "end": "1844919"
  },
  {
    "text": "storing files on there once you have this volume in place and you're using it you could use any of",
    "start": "1844919",
    "end": "1850840"
  },
  {
    "text": "the traditional backup options that we've already discussed you could you could actually if you want to Tar all the files off of an EBS volume and then",
    "start": "1850840",
    "end": "1857399"
  },
  {
    "text": "S3 curve those over to S3 on the other hand EBS gives you a",
    "start": "1857399",
    "end": "1864159"
  },
  {
    "text": "really nice and convenient easy to use Snapshot mechanism that we'll talk about in in just a little",
    "start": "1864159",
    "end": "1869320"
  },
  {
    "text": "bit I'd also like to mention that the the annual failure rate for EBS volumes is is um is planned at between 0.1 and",
    "start": "1869320",
    "end": "1877679"
  },
  {
    "text": "0.5% versus about 4% annually for commodity Hardware so EBS by itself is",
    "start": "1877679",
    "end": "1882960"
  },
  {
    "text": "giving you a more reliable storage vehicle than a commodity hard drive however it's still useful to to make",
    "start": "1882960",
    "end": "1889720"
  },
  {
    "start": "1889000",
    "end": "1889000"
  },
  {
    "text": "backups super super easy to go into the AWS console and create an EBS",
    "start": "1889720",
    "end": "1894760"
  },
  {
    "text": "snapshot on the left side of your screen you'll see a list of the the snapshot of",
    "start": "1894760",
    "end": "1900039"
  },
  {
    "text": "the of the EBS volumes that I have in my account I simply right click on a particular volume I choose create",
    "start": "1900039",
    "end": "1907840"
  },
  {
    "text": "snapshot dialogue pops up I can give my Snapshot a name and a description I hit",
    "start": "1907840",
    "end": "1914639"
  },
  {
    "text": "the yes button and then EBS will create a snapshot of the volume state as it existed at that",
    "start": "1914639",
    "end": "1921919"
  },
  {
    "text": "time I can then go back to the volumes list inside my console and I can see a complete list of all the snapshots that",
    "start": "1921919",
    "end": "1927679"
  },
  {
    "text": "I've created I can sort that I can filter it I can very very easily locate any of the the backups that I have",
    "start": "1927679",
    "end": "1936080"
  },
  {
    "text": "created if I need to do a restore I go to that list I rightclick on a snapshot",
    "start": "1936799",
    "end": "1943639"
  },
  {
    "start": "1937000",
    "end": "1937000"
  },
  {
    "text": "and I simply say create volume from snapshot I can create that volume in any",
    "start": "1943639",
    "end": "1949120"
  },
  {
    "text": "of the availability zones in the AWS region where it resides and I do need to",
    "start": "1949120",
    "end": "1954200"
  },
  {
    "text": "know the size of the original volume so I can create a replacement volume of an appropriate size to restore",
    "start": "1954200",
    "end": "1960000"
  },
  {
    "text": "to as a side note this actually the the snapshot and restore model is also good if you created a a volume put so many",
    "start": "1960000",
    "end": "1968720"
  },
  {
    "text": "files on there it started to get full and you say I need some more space so you you snapshot your volume um create a",
    "start": "1968720",
    "end": "1976399"
  },
  {
    "text": "new larger volume restore to that volume and then you simply expand the file system that the the command to do that",
    "start": "1976399",
    "end": "1982760"
  },
  {
    "text": "will be specific to whatever file system type you happen to be running and almost all the Linux file systems now have the",
    "start": "1982760",
    "end": "1988639"
  },
  {
    "text": "ability to expand to to fill extra space at the end of the device so the the EBS",
    "start": "1988639",
    "end": "1993720"
  },
  {
    "text": "volume restore can also be used um for that purpose okay so we talked earlier about",
    "start": "1993720",
    "end": "2000120"
  },
  {
    "start": "1996000",
    "end": "1996000"
  },
  {
    "text": "this idea of taking consistent snapshots and uh Eric Hammond who's done some really awesome work for the AWS",
    "start": "2000120",
    "end": "2005960"
  },
  {
    "text": "Community has put together a nice script called ec2 consistent snapshot this",
    "start": "2005960",
    "end": "2011519"
  },
  {
    "text": "takes care of a lot of the consistency U operations that we talked about earlier",
    "start": "2011519",
    "end": "2016600"
  },
  {
    "text": "it will help you to freeze your file system lock the databases take the snapshot unlock the databases and",
    "start": "2016600",
    "end": "2023399"
  },
  {
    "text": "unfreeze the file system you can find this at elastic.com and this one takes",
    "start": "2023399",
    "end": "2028880"
  },
  {
    "text": "care of all the interesting little details that come about when you are attempting to to do highquality",
    "start": "2028880",
    "end": "2034799"
  },
  {
    "text": "consistent snapshots of your EBS volumes",
    "start": "2034799",
    "end": "2039720"
  },
  {
    "text": "you might also think about doing some scheduled backups and so if if you're familiar",
    "start": "2040760",
    "end": "2047039"
  },
  {
    "text": "with with uh with the KRON command you could you could install the ec2 command",
    "start": "2047039",
    "end": "2052118"
  },
  {
    "text": "line tools on your servers and then you would simply launch the ec2 create snapshot tool at the point when you need",
    "start": "2052119",
    "end": "2059158"
  },
  {
    "text": "to create uh Vol uh backups of particular volumes to do this you need",
    "start": "2059159",
    "end": "2064440"
  },
  {
    "text": "to configure AWS credentials on your your server and when you run the command you simply give the the dashd argument",
    "start": "2064440",
    "end": "2070919"
  },
  {
    "text": "to give a description of the volume and you then give it the actual uh identity",
    "start": "2070919",
    "end": "2076280"
  },
  {
    "text": "of the volume there are other ec2 commands that will do things like give you a list of all the volumes attached",
    "start": "2076280",
    "end": "2081679"
  },
  {
    "text": "to a server a more sophisticated script could iterate through all the volumes attached to a server and then make",
    "start": "2081679",
    "end": "2087280"
  },
  {
    "text": "decisions based on the the the name or other metadata about each volume when",
    "start": "2087280",
    "end": "2092878"
  },
  {
    "text": "and even if you want to take snapshots of the particular volumes",
    "start": "2092879",
    "end": "2098760"
  },
  {
    "text": "also on the scheduling side there are some thirdparty tools that make it possible for you to schedule backups uh",
    "start": "2098760",
    "end": "2104160"
  },
  {
    "text": "one that I've taken a look at is called skley with skley you can simply create",
    "start": "2104160",
    "end": "2109280"
  },
  {
    "start": "2107000",
    "end": "2107000"
  },
  {
    "text": "an account you provide it with your AWS credentials and you can then schedule",
    "start": "2109280",
    "end": "2114720"
  },
  {
    "text": "backups you you can create a an action you can tell which volumes you like to",
    "start": "2114720",
    "end": "2120040"
  },
  {
    "text": "backup which backup schedule which tags you'd like to add to the backup volumes and so forth",
    "start": "2120040",
    "end": "2128000"
  },
  {
    "text": "so in in the last uh 35 minutes or so we we've taken an amazingly Fast Trip through a number of different ways to to",
    "start": "2128800",
    "end": "2135240"
  },
  {
    "text": "do backups and in the interest of time and because I wanted to leave some time for for questions there there's a number",
    "start": "2135240",
    "end": "2141640"
  },
  {
    "text": "of things we didn't cover we didn't talk about doing uh backup of databases we",
    "start": "2141640",
    "end": "2146720"
  },
  {
    "text": "didn't talk about using for example the relational database service to back up your MySQL or your Oracle instances we",
    "start": "2146720",
    "end": "2153280"
  },
  {
    "text": "just talked about EBS um there's also things we can talk about in the the future about backing up",
    "start": "2153280",
    "end": "2159359"
  },
  {
    "text": "Oracle um Arman or backing up",
    "start": "2159359",
    "end": "2164599"
  },
  {
    "text": "MySQL and uh we could also talk about the uh aw storage Gateway um looks like we have time for",
    "start": "2166800",
    "end": "2174000"
  },
  {
    "text": "just a a couple of questions so let's let's pull those up and uh see how we're",
    "start": "2174000",
    "end": "2180599"
  },
  {
    "text": "doing okay so let's talk about uh the first question here it the first",
    "start": "2182240",
    "end": "2188040"
  },
  {
    "text": "question is uh from Ephraim and he asks what does 119 of durability mean um is",
    "start": "2188040",
    "end": "2193599"
  },
  {
    "text": "it that I only lose. 3 milliseconds of data a year um the the E the way this",
    "start": "2193599",
    "end": "2200000"
  },
  {
    "text": "works is that um when when we store data in S3 we automatically make multiple",
    "start": "2200000",
    "end": "2206560"
  },
  {
    "text": "copies of that data and then the the S3 infrastructure is automatically managing",
    "start": "2206560",
    "end": "2212760"
  },
  {
    "text": "the the Integrity of that data it is always making sure that that any",
    "start": "2212760",
    "end": "2217920"
  },
  {
    "text": "particular devices where data is stored that those D that the data is up to date it's running check sums across the data",
    "start": "2217920",
    "end": "2223640"
  },
  {
    "text": "if at any point S3 detects that there are uh not enough copies of your data it",
    "start": "2223640",
    "end": "2228920"
  },
  {
    "text": "will automatically make additional replicas of your information it's doing this dynamically transparently behind",
    "start": "2228920",
    "end": "2234319"
  },
  {
    "text": "the scenes what the 119 of durability means if you were to store 10,000 objects in Amazon",
    "start": "2234319",
    "end": "2240680"
  },
  {
    "text": "S3 the expectation based on all the the observations the math that we've done we",
    "start": "2240680",
    "end": "2246359"
  },
  {
    "text": "would expect to lose one object about every 10 million years so that's that's the the math and the logic between the",
    "start": "2246359",
    "end": "2252960"
  },
  {
    "text": "11 NES of durability okay so uh Diego asks about",
    "start": "2252960",
    "end": "2258319"
  },
  {
    "text": "uh he says S3 command and S3 curl are great for backups on Linux but they don't deal with transfers of huge",
    "start": "2258319",
    "end": "2264160"
  },
  {
    "text": "numbers of of small F files by using multi-threading or parallel uploading um",
    "start": "2264160",
    "end": "2269359"
  },
  {
    "text": "I don't happen to know of any utilities that that that are able to do multi-threading or or parallel uploading",
    "start": "2269359",
    "end": "2276680"
  },
  {
    "text": "it definitely something I would uh be happy to research and in a future version of this webinar we we can cover",
    "start": "2276680",
    "end": "2281839"
  },
  {
    "text": "some Advanced features like that um let's see David is asking about",
    "start": "2281839",
    "end": "2287920"
  },
  {
    "text": "uh does s refuse support multi- push I'm actually not aware of that um again",
    "start": "2287920",
    "end": "2292960"
  },
  {
    "start": "2289000",
    "end": "2289000"
  },
  {
    "text": "something we should investigate for for for the",
    "start": "2292960",
    "end": "2298400"
  },
  {
    "text": "future let's see okay couple questions about sharing",
    "start": "2299839",
    "end": "2305359"
  },
  {
    "text": "presentations which we will definitely do and then um Chris asks if we will do a webinar that's specific to RDS and",
    "start": "2305359",
    "end": "2311680"
  },
  {
    "text": "Oracle I don't have a specific plan to do that but uh based on the interest that we have seen I I would say that's a",
    "start": "2311680",
    "end": "2317200"
  },
  {
    "text": "fairly likely thing for us to",
    "start": "2317200",
    "end": "2320720"
  },
  {
    "text": "do Bruno asks about what do you think about using s3fs or any other tool to mount S3 as a file system in a Linux box",
    "start": "2323280",
    "end": "2331520"
  },
  {
    "text": "so I've I haven't uh accumulated a lot of experience using this I I set it up in order to to test it out for the",
    "start": "2331520",
    "end": "2338640"
  },
  {
    "text": "purposes of this webinar my sense was that it seemed to be a fairly mature piece of code it's gone through a number",
    "start": "2338640",
    "end": "2344480"
  },
  {
    "text": "of versions it seemed to have a a fairly active user and development Community the the one thing that you do",
    "start": "2344480",
    "end": "2350319"
  },
  {
    "text": "need to worry about when you start to see people saying let's turn S3 into a file system is that at at the very very",
    "start": "2350319",
    "end": "2357160"
  },
  {
    "text": "detailed level you're not going to see the the same exact file system semantics that you get from a local file system um",
    "start": "2357160",
    "end": "2363640"
  },
  {
    "text": "for example when you think about the way that that a remote file system has to modify an object it's going to have to",
    "start": "2363640",
    "end": "2369280"
  },
  {
    "text": "download the entire object overwrite a portion of it and then upload the modified version back up to S3 that so",
    "start": "2369280",
    "end": "2377400"
  },
  {
    "text": "so anything that's going to be doing Random Access that's that's doing uh let's say kind of random rights to the",
    "start": "2377400",
    "end": "2382680"
  },
  {
    "text": "middle of a file I I don't know exactly how well that would perform under a remote file system based model generally",
    "start": "2382680",
    "end": "2389640"
  },
  {
    "text": "with remote file systems there's also going to be some some differences in the at the posx level of things like locking",
    "start": "2389640",
    "end": "2395359"
  },
  {
    "text": "and so forth oh okay we actually got an FYI from",
    "start": "2395359",
    "end": "2400760"
  },
  {
    "text": "David who says uh s3fs does support multi-art upload above 20 megabytes and",
    "start": "2400760",
    "end": "2405920"
  },
  {
    "text": "up to 64 gigabytes so so thank you David for contributing that that's that's great to know okay um adenus asks uh when an EBS",
    "start": "2405920",
    "end": "2413960"
  },
  {
    "text": "snapshot is created is it actually stored on S3 yes that is the case when when you do create the snapshots those",
    "start": "2413960",
    "end": "2420440"
  },
  {
    "text": "snapshots do end up on S3 and they have the same level of of durability as as we talked about for S3",
    "start": "2420440",
    "end": "2429319"
  },
  {
    "text": "okay now we have a question uh regarding S3 security okay so we we covered a",
    "start": "2430160",
    "end": "2435440"
  },
  {
    "text": "little bit of security at the beginning let let me talk to you in about that in a little bit more detail when you",
    "start": "2435440",
    "end": "2441760"
  },
  {
    "text": "transfer data from your on premises uh storage up to S3 you have the option to",
    "start": "2441760",
    "end": "2446960"
  },
  {
    "text": "send it to us via an encrypted um SSL connection to protect the data in",
    "start": "2446960",
    "end": "2452119"
  },
  {
    "text": "transit once your data is resident on S3 you have the option to enable server",
    "start": "2452119",
    "end": "2457760"
  },
  {
    "text": "side encryption with S3 automatically managing the encryption keys for",
    "start": "2457760",
    "end": "2463079"
  },
  {
    "text": "you when you put new objects into S3 they are private to your account by",
    "start": "2463079",
    "end": "2468280"
  },
  {
    "text": "default your AWS account is the only one with permission to read or write those objects for finer grain control Esther",
    "start": "2468280",
    "end": "2475880"
  },
  {
    "text": "gives you something called an ACL an access control list and with the ACLS you can very selectively Grant",
    "start": "2475880",
    "end": "2481400"
  },
  {
    "text": "additional permission to O others for example if you're using S3 to store um",
    "start": "2481400",
    "end": "2487839"
  },
  {
    "text": "some like open source project and you want to enable downloads you could retain the permission to upload data to",
    "start": "2487839",
    "end": "2493480"
  },
  {
    "text": "S3 but you could allow public reads of that particular item in S3 and you can do this at the individual item",
    "start": "2493480",
    "end": "2502160"
  },
  {
    "text": "level we do have a security Center on our site and we also have security white papers available that that give you a",
    "start": "2502760",
    "end": "2508920"
  },
  {
    "text": "lot more information about the various options and procedures and certifications and so forth uh relative",
    "start": "2508920",
    "end": "2514760"
  },
  {
    "text": "to S3 security n AWS Security in general all right Stephen asks he says for",
    "start": "2514760",
    "end": "2521640"
  },
  {
    "text": "guests in s in ec2 can S3 serve as a target for traditional backup clients such as TSM or would we need to write to",
    "start": "2521640",
    "end": "2528319"
  },
  {
    "text": "EBS first so Stephen you are totally totally welcome to use to host your data",
    "start": "2528319",
    "end": "2533359"
  },
  {
    "text": "on ec2 and then use your your existing backup model to push your data up to S3",
    "start": "2533359",
    "end": "2539200"
  },
  {
    "text": "all right uh pip asks can S3 your EBS be used to support NFS mounts for multiple instances without too much trouble um",
    "start": "2539200",
    "end": "2546760"
  },
  {
    "text": "yes you can do that in fact there there are some tools I've seen that will automatically set up NFS mounts across",
    "start": "2546760",
    "end": "2552880"
  },
  {
    "text": "multiple instances uh for example MIT has a project called star cluster that makes it easy to set up a cluster of ec2",
    "start": "2552880",
    "end": "2559400"
  },
  {
    "text": "instances running in AWS if you if you find the star cluster script you can you",
    "start": "2559400",
    "end": "2564440"
  },
  {
    "text": "can use that as a basis for setting up NFS I know that the last time I I ran that it automatically set up a a master",
    "start": "2564440",
    "end": "2571000"
  },
  {
    "text": "NFS volume and then shared that out to all of the uh the child the child servers okay okay um one more question",
    "start": "2571000",
    "end": "2578640"
  },
  {
    "text": "from from Bruno he says what about using our own keys to do serers side encryption instead of using AWS Keys uh",
    "start": "2578640",
    "end": "2585640"
  },
  {
    "text": "we we've had a few requests for this and it's something we're still collecting some customer data on want to kind of better understand the use cases and the",
    "start": "2585640",
    "end": "2592040"
  },
  {
    "text": "requirements that you'd have on a on a a customer supplied Key Management Facility uh Bruno if you want to email",
    "start": "2592040",
    "end": "2598400"
  },
  {
    "text": "us we'd be very happy to to uh take your requirements into account pass those along to the team for for planning",
    "start": "2598400",
    "end": "2605640"
  },
  {
    "text": "purposes okay another another question from Bruno this has been a great series of questions I I really appreciate all",
    "start": "2605640",
    "end": "2611160"
  },
  {
    "text": "these awesome questions you you're all sending here so Bruno asks what about storing the same backup on multiple AWS",
    "start": "2611160",
    "end": "2617800"
  },
  {
    "text": "regions so Bruno when you store data in a particular AWS region it is local to",
    "start": "2617800",
    "end": "2623880"
  },
  {
    "text": "that region and we never ever copy it between regions uh without your explicit command if you wanted to put some",
    "start": "2623880",
    "end": "2630520"
  },
  {
    "text": "scripts together to replicate data from let's say from the US east region to the",
    "start": "2630520",
    "end": "2635680"
  },
  {
    "text": "Europe region it it it would be very simple to write uh an S3 script to to do",
    "start": "2635680",
    "end": "2641240"
  },
  {
    "text": "that replication from S3 bucket to bucket we don't currently have any facilities to do that automatically or",
    "start": "2641240",
    "end": "2646839"
  },
  {
    "text": "programmatically for you there'll be a relatively straightforward application to build using the S3",
    "start": "2646839",
    "end": "2652720"
  },
  {
    "text": "apis okay another question from Diego so Diego asks does Amazon S3 impose some",
    "start": "2652720",
    "end": "2658280"
  },
  {
    "text": "limits on the bandwidth throughput for uploading or downloading files from S3 there are really no built-in limits",
    "start": "2658280",
    "end": "2666359"
  },
  {
    "text": "to S3 we do we do know that when customers start to do extremely heavy get and puts",
    "start": "2666359",
    "end": "2671599"
  },
  {
    "text": "to buckets we do have some guidelines for Best Practices as far as the ways they choose the the key names for S3 uh",
    "start": "2671599",
    "end": "2678599"
  },
  {
    "text": "generally when you get to the the point when you're doing hundreds of gets or hundreds of puts per second to an",
    "start": "2678599",
    "end": "2683720"
  },
  {
    "text": "individual bucket we we would ask you to take a look at our guidelines in order to get the best performance from S3 uh",
    "start": "2683720",
    "end": "2689680"
  },
  {
    "text": "we we currently process about 500,000 operations per second on S3 so it's a",
    "start": "2689680",
    "end": "2694720"
  },
  {
    "text": "it's a very very busy service so so basically uh net net there are no",
    "start": "2694720",
    "end": "2700240"
  },
  {
    "text": "intrinsic limits either either on the the number of gets or puts or the amount of bandwidth we we have extremely",
    "start": "2700240",
    "end": "2706200"
  },
  {
    "text": "well-connected data centers and there there shouldn't be any any you know reasonable level data transfer we we",
    "start": "2706200",
    "end": "2712839"
  },
  {
    "text": "can't uh can't sustain all right uh let's see Mauricio says what do you",
    "start": "2712839",
    "end": "2718200"
  },
  {
    "text": "think about freas like Solutions using EBS volumes um oops just scrolled away here",
    "start": "2718200",
    "end": "2724520"
  },
  {
    "text": "would that be a strong solution for avoiding snapshots unfortunately I'm not familiar with that it's something I'd be",
    "start": "2724520",
    "end": "2729680"
  },
  {
    "text": "happy to take a look at for a future version of our webinar Gerald asks about what do you think of mounting an s3fs on",
    "start": "2729680",
    "end": "2736000"
  },
  {
    "text": "a Linux server and doing our sync don't see any reason why that wouldn't work and uh think that would",
    "start": "2736000",
    "end": "2743119"
  },
  {
    "text": "actually be a pretty reasonable way to to to sync data up to S3 as",
    "start": "2743119",
    "end": "2748599"
  },
  {
    "text": "well and then Charlie asks he says will the Amazon Linux Ami be changed to have",
    "start": "2748599",
    "end": "2754240"
  },
  {
    "text": "an xfs form dis in the future to allow easier disc free snapshot usage great question and one that I'll",
    "start": "2754240",
    "end": "2760680"
  },
  {
    "text": "be happy to pass along to the Amazon Linux Ami team at the end of our webinar I think that just about wraps things up",
    "start": "2760680",
    "end": "2767119"
  },
  {
    "text": "haven't seen any new questions come in for a a minute or two uh we really really appreciate your your time and",
    "start": "2767119",
    "end": "2772200"
  },
  {
    "text": "your your questions um hope this was a a good use of of your time uh couple couple things to remember",
    "start": "2772200",
    "end": "2780800"
  },
  {
    "start": "2775000",
    "end": "2775000"
  },
  {
    "text": "the Cloud's going to make your your backup and your recovery very easy you can get started for pennies per month as",
    "start": "2780800",
    "end": "2787040"
  },
  {
    "text": "as we saw the crowd's going to be able to scale and to accommodate all of your data and you're going to retain",
    "start": "2787040",
    "end": "2793559"
  },
  {
    "text": "visibility and control of of your data here's a number of of links you can",
    "start": "2793559",
    "end": "2798920"
  },
  {
    "text": "visit to get some additional information we have a a site dedicated to providing you information about AWS backups we",
    "start": "2798920",
    "end": "2805319"
  },
  {
    "text": "will make these slides available on our SlideShare account the recording this webinar will will show up in our our",
    "start": "2805319",
    "end": "2811280"
  },
  {
    "text": "webinars page and we have a number of interesting videos for you the AWS website at aw",
    "start": "2811280",
    "end": "2817359"
  },
  {
    "text": "amazon.com the AWS blog is at aw. typepad.com and we have a number of interesting white papers for you as well",
    "start": "2817359",
    "end": "2825079"
  },
  {
    "text": "once again I really appreciate your time taking the time to to listen and look forward to hearing back from you after",
    "start": "2825079",
    "end": "2830280"
  },
  {
    "text": "you've used S3 to do your backups thanks for watching",
    "start": "2830280",
    "end": "2836880"
  }
]