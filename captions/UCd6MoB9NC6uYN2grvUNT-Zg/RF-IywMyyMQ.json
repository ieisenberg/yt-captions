[
  {
    "start": "0",
    "end": "47000"
  },
  {
    "text": "data integration into Amazon redshift um for those of you who've been following the webinar series you'll know",
    "start": "1680",
    "end": "7640"
  },
  {
    "text": "Amazon red shift is a pedi scale data warehouse that's faster and simpler and less expensive than traditional",
    "start": "7640",
    "end": "13000"
  },
  {
    "text": "warehousing Solutions and getting your data into red shift is easy want to thank you for joining and let you know",
    "start": "13000",
    "end": "19039"
  },
  {
    "text": "that whether your data is on premise or in a cloud-based database like Dynamo DB you'll see how you can load your data",
    "start": "19039",
    "end": "24560"
  },
  {
    "text": "into red shift easily we'll present a catalog of methods to load uh information some of the detailss on our",
    "start": "24560",
    "end": "31160"
  },
  {
    "text": "best practices um webinars uh you'll see the details of this have been explored at length in the uh the prior webinars",
    "start": "31160",
    "end": "37920"
  },
  {
    "text": "and you'll also have a chance to hear from one of our our Premier Partners attunity attunity will show you their solution for moving uh data into Amazon",
    "start": "37920",
    "end": "46000"
  },
  {
    "text": "red shift all right so uh if I and um as I mentioned there are a series of",
    "start": "46000",
    "end": "51320"
  },
  {
    "start": "47000",
    "end": "95000"
  },
  {
    "text": "webinars here and I just wanted to call your attention to the uh web reference for that you can go to our our site and",
    "start": "51320",
    "end": "57640"
  },
  {
    "text": "get all of the database services webinars the ones relevant to Red shift are of course the introduction and two",
    "start": "57640",
    "end": "63519"
  },
  {
    "text": "of the best practices webinars that we put together with the help of our engineering team we actually hear from",
    "start": "63519",
    "end": "68840"
  },
  {
    "text": "uh uh two of the key contributors to Amazon red shift to talk specifically about the best way to load data how you",
    "start": "68840",
    "end": "75960"
  },
  {
    "text": "choose your keys how you handle workload migration and Space Management within the the database platform and I I urge",
    "start": "75960",
    "end": "82640"
  },
  {
    "text": "you I encourage you to go out and have a look at those things once you've uh you know digested what we tell you today and",
    "start": "82640",
    "end": "88720"
  },
  {
    "text": "and when you're trying to uh get Optimal Performance out of your Warehouse on on red shift um so if we move on to our",
    "start": "88720",
    "end": "97040"
  },
  {
    "start": "95000",
    "end": "124000"
  },
  {
    "text": "agenda um our agenda again um this morning is fairly straightforward I want",
    "start": "97040",
    "end": "103360"
  },
  {
    "text": "to talk about the integration from uh Amazon S3 our our Object Store I want to talk about integration with Dynamo DB",
    "start": "103360",
    "end": "110040"
  },
  {
    "text": "which is a uh our nosql offering and then we'll introduce a tunity and they can show you their solution as well and",
    "start": "110040",
    "end": "116479"
  },
  {
    "text": "we will leave some time at the end for your questions all right so to that end",
    "start": "116479",
    "end": "123039"
  },
  {
    "text": "um um the first thing that people will be concerned with uh is getting data",
    "start": "123039",
    "end": "128959"
  },
  {
    "start": "124000",
    "end": "400000"
  },
  {
    "text": "into the Amazon Cloud this isn't something that's unique particularly to Red shift this is uh one of the things u",
    "start": "128959",
    "end": "135319"
  },
  {
    "text": "i Bridge you across when you start engaging with a a cloud service and uh Amazon provides a variety of ways for",
    "start": "135319",
    "end": "142360"
  },
  {
    "text": "you to get your information into the cloud depending upon what you're trying to accomplish the nature of the information you're moving and what your",
    "start": "142360",
    "end": "148280"
  },
  {
    "text": "time frames are uh to start out with I mean there's a multi-part option where you're able to go and uh you know do",
    "start": "148280",
    "end": "155000"
  },
  {
    "text": "faster more flexible uploads into Amazon S3 which is the uh the object story I mentioned earlier you can upload a",
    "start": "155000",
    "end": "160840"
  },
  {
    "text": "single object as a set of Parts uh this works particularly well with red shift as uh our copy command uh that I'll I'll",
    "start": "160840",
    "end": "168280"
  },
  {
    "text": "tell you a little bit more about shortly uh allows you to load data into in parallel into the cluster um another",
    "start": "168280",
    "end": "174680"
  },
  {
    "text": "option that you have is using our uh our virtual private Network there's a",
    "start": "174680",
    "end": "179879"
  },
  {
    "text": "facility within uh AWS that allows you to logically isolate sections of the cloud of our cloud or you can launch",
    "start": "179879",
    "end": "186959"
  },
  {
    "text": "resources in a virtual Network that you've defined so you have complete control over your virtual Network the",
    "start": "186959",
    "end": "192760"
  },
  {
    "text": "selection of your IP address range selections and Creations of subnets and and configurations of your route routing",
    "start": "192760",
    "end": "199000"
  },
  {
    "text": "tables and your your network gateways and this allows you to do quite a lot of customization so you could even have a a",
    "start": "199000",
    "end": "204560"
  },
  {
    "text": "customer facing subnet for your web servers and place your backend systems in a private subnet so that there's no",
    "start": "204560",
    "end": "210200"
  },
  {
    "text": "internet access um this allows you to operate as if what you have in the cloud",
    "start": "210200",
    "end": "215480"
  },
  {
    "text": "is just an extension of your local network and you would move data across the network and in any way you sayfe fit really uh other options are um direct",
    "start": "215480",
    "end": "224680"
  },
  {
    "text": "connect this allows you to move large amounts of information into S3 by establishing a dedicated network",
    "start": "224680",
    "end": "230200"
  },
  {
    "text": "connection from your premise to AWS with direct connect you can uh establish prod",
    "start": "230200",
    "end": "235239"
  },
  {
    "text": "private connectivity uh without um you you can connect directly from your your",
    "start": "235239",
    "end": "241120"
  },
  {
    "text": "premise into uh AWS using Direct Connect um uh the final option I'll talk about",
    "start": "241120",
    "end": "246959"
  },
  {
    "text": "is the import export uh ability that accelerates moving large amounts of data",
    "start": "246959",
    "end": "252400"
  },
  {
    "text": "uh one time into AWS by using a portable storage device that's physically transported you load the data up on your",
    "start": "252400",
    "end": "258400"
  },
  {
    "text": "premise move it to ship it to us and we will load from there uh sort of uh",
    "start": "258400",
    "end": "264639"
  },
  {
    "text": "and uh surprising but you can get great bandwidth response out of this if you're moving data in in truly massive",
    "start": "264639",
    "end": "272039"
  },
  {
    "text": "scales and what I want to draw your attention to here is that this is really just a graphical depiction of of all the",
    "start": "272039",
    "end": "277880"
  },
  {
    "text": "options that I've just described for you you have the ability to do the uh the import export at at the bottom of the",
    "start": "277880",
    "end": "284080"
  },
  {
    "text": "page moving data from your uh on premise solution into Amazon S3 we'll do the",
    "start": "284080",
    "end": "289960"
  },
  {
    "text": "copy for you you just send us the media uh you also go directly into S3 using the multipart upload which is is quite",
    "start": "289960",
    "end": "297360"
  },
  {
    "text": "convenient and the other two options are our Direct connection from your cloud-based networking to your uh your",
    "start": "297360",
    "end": "303240"
  },
  {
    "text": "premise based networking and you can use that using the VPN or the direct connect solution uh that I mentioned earlier and",
    "start": "303240",
    "end": "310800"
  },
  {
    "text": "if we could move on to the next slide uh where I'll talk about some of the the options that we provide from within",
    "start": "310800",
    "end": "317440"
  },
  {
    "text": "Amazon to allow you to get data into Amazon redshift all right and those are",
    "start": "317440",
    "end": "323280"
  },
  {
    "text": "principally pulling data directly from Dynamo DB pulling data directly from Amazon S3 and then orchestrating data",
    "start": "323280",
    "end": "330600"
  },
  {
    "text": "from one service to another with an AWS or even outside of AWS using our data",
    "start": "330600",
    "end": "336039"
  },
  {
    "text": "pipeline tool all right slide it's it's again uh my overview slide I rely on",
    "start": "336039",
    "end": "341160"
  },
  {
    "text": "this a lot but it's uh it does really describe all that we're doing here um",
    "start": "341160",
    "end": "347039"
  },
  {
    "text": "there's a command from within our SQL command set called copy and copy has a",
    "start": "347039",
    "end": "353000"
  },
  {
    "text": "syntax that allows you to pull information directly from Dynamo DB so if you have a table that you've defined within red shift or you want data to age",
    "start": "353000",
    "end": "360360"
  },
  {
    "text": "out of Dynamo or you simply do a copy from there and and then that data will be available to you within red shift and",
    "start": "360360",
    "end": "366639"
  },
  {
    "text": "you can perform your analytics there so it's it's sort of how uh data moves",
    "start": "366639",
    "end": "372360"
  },
  {
    "text": "through your systems if you think about it you have the ability to take data in very quickly Dynamo DB is is very fast",
    "start": "372360",
    "end": "379479"
  },
  {
    "text": "it gives you uh completely predictable performance it's seamlessly scalable and you don't have to do any Administration",
    "start": "379479",
    "end": "385520"
  },
  {
    "text": "you just dial in the uh the scale the performance you want and you'll get edit right so you can you can stream data in",
    "start": "385520",
    "end": "392400"
  },
  {
    "text": "there massive scale very fast and when it's time to move that on and do some analysis simply copy it into red shift I",
    "start": "392400",
    "end": "399639"
  },
  {
    "text": "can show you what that means uh it is literally copy copy the orders table in",
    "start": "399639",
    "end": "405599"
  },
  {
    "start": "400000",
    "end": "631000"
  },
  {
    "text": "red shift from the orders table in Dy DB and you pass your credentials and you also pass this command here and you know",
    "start": "405599",
    "end": "411960"
  },
  {
    "text": "I don't want to get too deep into the technical details at all but I want to call your attention to this right away you give a read ratio there okay you",
    "start": "411960",
    "end": "418919"
  },
  {
    "text": "want to give some indication to Dynamo of how much attention you want paid to",
    "start": "418919",
    "end": "424039"
  },
  {
    "text": "this particular command you don't want to consume all of your Your Capacity uh simply copying over to uh uh to Dynamo",
    "start": "424039",
    "end": "431800"
  },
  {
    "text": "so again loading data from Dynamo is is easy use the copy command it uses the",
    "start": "431800",
    "end": "437960"
  },
  {
    "text": "massively parallel processing architecture of red shift to read and load data in parallel from Dynamo and uh",
    "start": "437960",
    "end": "444919"
  },
  {
    "text": "the copy command is executed um you know from whatever post can uh jdbc driver",
    "start": "444919",
    "end": "451120"
  },
  {
    "text": "compatible SQL tool that you're running in so if you use Navigator you just connect with Navigator execute the copy",
    "start": "451120",
    "end": "456400"
  },
  {
    "text": "command and and you're Set uh you want to avoid in this copy command using excessive amounts of provision",
    "start": "456400",
    "end": "462680"
  },
  {
    "text": "throughput and so we recommend that you don't load your data uh from from uh",
    "start": "462680",
    "end": "467840"
  },
  {
    "text": "Amazon Dynamo DB tables that are in production environment without uh",
    "start": "467840",
    "end": "473000"
  },
  {
    "text": "using this command called read ratio that lowers the percentage of unused provision throughput so that uh uh for",
    "start": "473000",
    "end": "479560"
  },
  {
    "text": "example uh the read ratio 50 Clause here will regulate the percentage of uh read",
    "start": "479560",
    "end": "486440"
  },
  {
    "text": "capacity uh units within the the customer table all right and we we want you to pay particular attention to that",
    "start": "486440",
    "end": "492599"
  },
  {
    "text": "so that you don't um up upset your normal operations I do want to go on and",
    "start": "492599",
    "end": "498199"
  },
  {
    "text": "and you know just give you this brief overview if you're pulling in information on web registrations and you want to uh a sense of who it is that",
    "start": "498199",
    "end": "504520"
  },
  {
    "text": "you've got registering and you want to reach out to various social media sources say Twitter for square uh Facebook",
    "start": "504520",
    "end": "509960"
  },
  {
    "text": "you can do that and pull that information into Dynamo uh using the mechanism we've just described and copy",
    "start": "509960",
    "end": "515680"
  },
  {
    "text": "that over into red shift uh pretty directly and then the end to end solution of course you want to be able to push your results out somewhere uh",
    "start": "515680",
    "end": "523080"
  },
  {
    "text": "likely a reporting or a bi tool and that's available to you again with jdbc odbc interface from postgress so you",
    "start": "523080",
    "end": "530200"
  },
  {
    "text": "should have no trouble connecting the tool of your choice to Red shift that's our our Dynamo DB story",
    "start": "530200",
    "end": "536600"
  },
  {
    "text": "it's a great way of moving data in at scale that you're collecting at high speed from a web interface from a chat",
    "start": "536600",
    "end": "542760"
  },
  {
    "text": "tool from any kind of collection of of sensors those sorts of things you can push information to Dynamo very quickly",
    "start": "542760",
    "end": "549079"
  },
  {
    "text": "and again no Administration just the uh the throughput you need for S3 for uh",
    "start": "549079",
    "end": "555480"
  },
  {
    "text": "you know uh textual data for the most part you pull information in either in volume with uh import export if you've",
    "start": "555480",
    "end": "561600"
  },
  {
    "text": "got you know uh dozens of terabytes of of data to move that could be a very efficient way of doing things if you're",
    "start": "561600",
    "end": "567880"
  },
  {
    "text": "simply looking to move reason sized sets of data across the network you can do that with S3 S3 with a multipower upload",
    "start": "567880",
    "end": "576160"
  },
  {
    "text": "can break files up for you the idea here being that you distribute the work",
    "start": "576160",
    "end": "582079"
  },
  {
    "text": "across the full cluster of of uh Amazon red shift so every compute note is doing the work of loading for you and that",
    "start": "582079",
    "end": "588720"
  },
  {
    "text": "minimizes of course the throughput because you're leveraging more the the power of the uh the system you're",
    "start": "588720",
    "end": "594000"
  },
  {
    "text": "running on so here at the automatic uh or or the upload from on premise file is is",
    "start": "594000",
    "end": "601120"
  },
  {
    "text": "depicted in the screen in front of you here uh files are split into multiple pieces to facilitate the parallel",
    "start": "601120",
    "end": "606680"
  },
  {
    "text": "processing uh you're going to move data into the same region that your red shift cluster exists in and then when you do",
    "start": "606680",
    "end": "613200"
  },
  {
    "text": "the copy command multiple files will be moved in parallel into red shift and the",
    "start": "613200",
    "end": "618680"
  },
  {
    "text": "idea here is without getting into too much of the detail that for every core uh that you're running within red shift",
    "start": "618680",
    "end": "624640"
  },
  {
    "text": "you can be copying a file over all right",
    "start": "624640",
    "end": "630160"
  },
  {
    "start": "631000",
    "end": "712000"
  },
  {
    "text": "uh for unstructured data for say you're doing some work you collect information through uh web logs or some other uh",
    "start": "631440",
    "end": "638399"
  },
  {
    "text": "logging process push that information into S3 and you can move that into EMR",
    "start": "638399",
    "end": "644320"
  },
  {
    "text": "using uh data pipeline EMR is our elastic map reduce implementation it's a",
    "start": "644320",
    "end": "649920"
  },
  {
    "text": "way of running map reduce or Hadoop uh within AWS and you can do your exploratory analytics there you can look",
    "start": "649920",
    "end": "655959"
  },
  {
    "text": "at the data and look for Trends or patterns of sentiment analysis really any algorithm that you like and then as",
    "start": "655959",
    "end": "662240"
  },
  {
    "text": "you find things that are generally applicable things that you want to be able to share out to a broader audience",
    "start": "662240",
    "end": "667399"
  },
  {
    "text": "that say isn't necessarily skilled in programming but has a facility with uh SQL or knowledge of uh the bi",
    "start": "667399",
    "end": "675320"
  },
  {
    "text": "infrastructure within your organization the reporting tools and and it's Adept at getting out information and that way",
    "start": "675320",
    "end": "680839"
  },
  {
    "text": "push the data from EMR into S3 and then load that into red shift and that gives",
    "start": "680839",
    "end": "686320"
  },
  {
    "text": "you the ability to take what you've discovered through your exploratory analytics and sort of make it uh",
    "start": "686320",
    "end": "693160"
  },
  {
    "text": "operational data now all right and so uh this is a common pattern um within AWS",
    "start": "693160",
    "end": "701320"
  },
  {
    "text": "where once you've got the data from any Source at all really if you push it into S3 you can pull it into red shift pretty",
    "start": "701320",
    "end": "710639"
  },
  {
    "text": "directly all right and now I want to uh pause with you know I've covered the basic ways that you pull information",
    "start": "711079",
    "end": "716880"
  },
  {
    "text": "into red shift from within uh the tools that we're providing and and I want to talk a little bit about attunity",
    "start": "716880",
    "end": "723079"
  },
  {
    "text": "attunity is a premier partner of ours there are folks who can show you exactly how uh their tool Works in in a very",
    "start": "723079",
    "end": "730320"
  },
  {
    "text": "sophisticated and robust way to pull information into into red shift so um let me do that",
    "start": "730320",
    "end": "738600"
  },
  {
    "text": "and shift control over to uh to Brad thank you John very good so I'd",
    "start": "738600",
    "end": "747600"
  },
  {
    "start": "740000",
    "end": "775000"
  },
  {
    "text": "like to thank Amazon for featuring our solution today and then of course thank",
    "start": "747600",
    "end": "752720"
  },
  {
    "text": "all of those in attendance my name is Brad heler and I am attunity director of",
    "start": "752720",
    "end": "758560"
  },
  {
    "text": "cloud business I am joined today by RZA Khan our director of Support Services",
    "start": "758560",
    "end": "764279"
  },
  {
    "text": "together Reza and I will demonstrate aun's replicate solution for loading data to Amazon",
    "start": "764279",
    "end": "772000"
  },
  {
    "text": "redshift so for those new to attunity our core competency is to deliver data",
    "start": "772880",
    "end": "777959"
  },
  {
    "text": "quickly and easily why is this important because business users need answers to",
    "start": "777959",
    "end": "784000"
  },
  {
    "text": "questions that drive their business and corporations commonly invest millions of",
    "start": "784000",
    "end": "789720"
  },
  {
    "text": "dollars in analytic environments to achieve a competitive Advantage but without the data these systems cannot",
    "start": "789720",
    "end": "795519"
  },
  {
    "text": "produce results so organizations need timely access to all kinds of data structured semi-structured and",
    "start": "795519",
    "end": "802000"
  },
  {
    "text": "unstructured and to meet this need attunity delivers virtually any data any",
    "start": "802000",
    "end": "808199"
  },
  {
    "text": "time any anywhere with high performance low total cost of ownership and a quick time to Value ATT",
    "start": "808199",
    "end": "816079"
  },
  {
    "text": "tunity enables Enterprises to extract the value from big data",
    "start": "816079",
    "end": "821600"
  },
  {
    "start": "822000",
    "end": "851000"
  },
  {
    "text": "analytics so industrywide we hear of data warehousing discontent as IDC",
    "start": "822959",
    "end": "829480"
  },
  {
    "text": "points out only 177% of organizations are very satisfied with the performance",
    "start": "829480",
    "end": "835199"
  },
  {
    "text": "of their data warehouse loading process the more traditional data warehouses",
    "start": "835199",
    "end": "840800"
  },
  {
    "text": "cannot handle the data velocities and volumes that are common place",
    "start": "840800",
    "end": "846920"
  },
  {
    "text": "today so those familiar with transferring data across heterogeneous environments often discover these five",
    "start": "846920",
    "end": "854959"
  },
  {
    "start": "851000",
    "end": "882000"
  },
  {
    "text": "challenges homegrown development requires precious it resources which are",
    "start": "854959",
    "end": "860240"
  },
  {
    "text": "often limited and at the same time costly native manual processes are",
    "start": "860240",
    "end": "866040"
  },
  {
    "text": "certainly not real time should update take too long information becomes stale",
    "start": "866040",
    "end": "871720"
  },
  {
    "text": "and a reduced value to the decision makers lastly these scripted processes are extremely difficult to",
    "start": "871720",
    "end": "879440"
  },
  {
    "text": "maintain so given those challenges ATT tunities replicate for red shift is",
    "start": "879440",
    "end": "885279"
  },
  {
    "start": "882000",
    "end": "910000"
  },
  {
    "text": "designed to allow organizations to overcome the data transfer bottleneck at",
    "start": "885279",
    "end": "890360"
  },
  {
    "text": "the same time customers can avoid all that well-known heavy lifting so with a",
    "start": "890360",
    "end": "895720"
  },
  {
    "text": "tunity your team can move more data in less time and with less cost that is a",
    "start": "895720",
    "end": "902040"
  },
  {
    "text": "tunity simplifies and accelerates the process of data loading so what is by definition auni",
    "start": "902040",
    "end": "910199"
  },
  {
    "start": "910000",
    "end": "940000"
  },
  {
    "text": "Cloud Beam for Amazon red shift by definition auni Cloud beam is optimized",
    "start": "910199",
    "end": "916320"
  },
  {
    "text": "as an end to-end solution for accelerating data loading into Amazon redshift the replicate solution provides",
    "start": "916320",
    "end": "923440"
  },
  {
    "text": "for True automation is easy to set up and easy to manage as you see here from",
    "start": "923440",
    "end": "929600"
  },
  {
    "text": "this table attunity provides a variety of sources to manage both full load and",
    "start": "929600",
    "end": "934800"
  },
  {
    "text": "incremental change data capture so how does replicate actually",
    "start": "934800",
    "end": "941000"
  },
  {
    "text": "work let's first take a look at this common on-prem scenario carrying out",
    "start": "941000",
    "end": "947240"
  },
  {
    "text": "automation replicate starts by managing the schema generation and data type",
    "start": "947240",
    "end": "952399"
  },
  {
    "text": "mapping looking at that Source database replicate then intelligently autom Maps",
    "start": "952399",
    "end": "957800"
  },
  {
    "text": "data types next replicates transform filter then",
    "start": "957800",
    "end": "963399"
  },
  {
    "text": "processes in memory and simultaneously bulk loads to the defined Target",
    "start": "963399",
    "end": "970240"
  },
  {
    "text": "database following the bulk load replicate looks to transaction logs and manages the incremental load of changes",
    "start": "970240",
    "end": "976560"
  },
  {
    "text": "represented in Source tables lastly replicate applies schema changes such as",
    "start": "976560",
    "end": "981880"
  },
  {
    "text": "ddls made to the source tables so let's apply this to the world",
    "start": "981880",
    "end": "988639"
  },
  {
    "text": "of Amazon and red shift process-wise uh very similar of course the replicate",
    "start": "988639",
    "end": "995279"
  },
  {
    "text": "server looks to the Define database generates table",
    "start": "995279",
    "end": "1001199"
  },
  {
    "text": "files from there replicate leverages attunity Cloud beam service to send",
    "start": "1001199",
    "end": "1008279"
  },
  {
    "text": "those files to S3 the S3 bucket is maintained by the End customer",
    "start": "1008279",
    "end": "1014240"
  },
  {
    "text": "themselves from there we attunity replicate validates the file content",
    "start": "1014240",
    "end": "1019880"
  },
  {
    "text": "upon arrival once that's validated via an instruction channel the replicate",
    "start": "1019880",
    "end": "1025760"
  },
  {
    "text": "server then executes a copy command to load the data data tables from S3 into",
    "start": "1025760",
    "end": "1032280"
  },
  {
    "text": "redshift from there the replicate server receives acknowledgement on a successful copy and apply now that's for full load",
    "start": "1032280",
    "end": "1040480"
  },
  {
    "text": "how does this look for incremental of course it's the similar process but once we receive uh",
    "start": "1040480",
    "end": "1050000"
  },
  {
    "text": "uh once we receive confirmation we then conf uh transition into the change data",
    "start": "1050000",
    "end": "1055720"
  },
  {
    "text": "capture mode from there we're looking for differences we move the net changes",
    "start": "1055720",
    "end": "1061559"
  },
  {
    "text": "through a file and then we execute the SQL commands merge change into data",
    "start": "1061559",
    "end": "1069679"
  },
  {
    "text": "tables so a lot of customers uh given the urgency to load",
    "start": "1071919",
    "end": "1078280"
  },
  {
    "text": "data site performance as most important when managing the loading process so",
    "start": "1078280",
    "end": "1084320"
  },
  {
    "text": "replicate uses a proprietary transfer protocol designed to do several things",
    "start": "1084320",
    "end": "1089360"
  },
  {
    "text": "outlined here uh specifically the protocol leverages the",
    "start": "1089360",
    "end": "1096240"
  },
  {
    "text": "Amazon multi-art transfers allows for concurrent sessions and transfers allows",
    "start": "1096240",
    "end": "1101880"
  },
  {
    "text": "for compression guarantees delivery by way of recoverability and of course encrypts",
    "start": "1101880",
    "end": "1108000"
  },
  {
    "text": "the transfer the outcome is performance gains of 10 to 12 times over standard",
    "start": "1108000",
    "end": "1115400"
  },
  {
    "text": "copy with that I'd like to reintroduce Reza Khan who will now demonstrate the replicate solution and some sample",
    "start": "1115400",
    "end": "1124000"
  },
  {
    "text": "jobs uh thank you very much Brad I'm just going to go ahead and start my screen sharing here well thank you",
    "start": "1124000",
    "end": "1130360"
  },
  {
    "start": "1129000",
    "end": "1174000"
  },
  {
    "text": "everyone and thank you Brad for the introduction uh we'd like to take a few minutes now to give you uh a brief",
    "start": "1130360",
    "end": "1136360"
  },
  {
    "text": "overview of uh how a tunity Cloud beam can be leveraged for loading data from",
    "start": "1136360",
    "end": "1141760"
  },
  {
    "text": "an oracle data source into a red shift Target cluster uh the same process also",
    "start": "1141760",
    "end": "1146919"
  },
  {
    "text": "applies for all the other data sources that we uh support however today we'll simply focus on Oracle as a",
    "start": "1146919",
    "end": "1153799"
  },
  {
    "text": "source so in order to enable uh data loading uh using attunity Cloud beam uh",
    "start": "1153799",
    "end": "1159880"
  },
  {
    "text": "a customer or a user must first enroll with one of the eunity cloud beam Services uh or subscribe to that service",
    "start": "1159880",
    "end": "1166919"
  },
  {
    "text": "and they would do that by coming to",
    "start": "1166919",
    "end": "1170720"
  },
  {
    "text": "cloudbees them to a very simple signup uh form uh or if they're already a user",
    "start": "1172760",
    "end": "1178240"
  },
  {
    "text": "they can simply go ahead and log in to the aginity cloud beam portal I'm going to log in with my own user",
    "start": "1178240",
    "end": "1186039"
  },
  {
    "text": "account and then once logged in I'm able to see all of the services that the tunity cloud beam supports and today",
    "start": "1190240",
    "end": "1197919"
  },
  {
    "start": "1195000",
    "end": "1235000"
  },
  {
    "text": "specifically we're going to be talking about the DB to Amazon red Chi for application you can head over to the",
    "start": "1197919",
    "end": "1203840"
  },
  {
    "text": "subscription section uh see all of the uh services that you're currently subscribed to and then hit the Subscribe",
    "start": "1203840",
    "end": "1209919"
  },
  {
    "text": "button to subscribe to a new service uh that's required uh for loading the data",
    "start": "1209919",
    "end": "1215039"
  },
  {
    "text": "into Amazon red shift you would need to subscribe to the DB to Amazon red shift cluster and that's it that's all you",
    "start": "1215039",
    "end": "1221440"
  },
  {
    "text": "need to do in order to enable uh this uh uh the subscription of the service to",
    "start": "1221440",
    "end": "1227280"
  },
  {
    "text": "load the data after this this attunity would provide you with the attunity rep replicate software which you would then",
    "start": "1227280",
    "end": "1233640"
  },
  {
    "text": "go ahead and install on one of uh on a server that's on permise that's close to the source database and this allows you",
    "start": "1233640",
    "end": "1240320"
  },
  {
    "start": "1235000",
    "end": "1251000"
  },
  {
    "text": "to begin uh defining your tasks for loading the data so we'll Begin by going",
    "start": "1240320",
    "end": "1246000"
  },
  {
    "text": "to the manage databases and uh start out by creating my source and Target database so as mentioned we're going to",
    "start": "1246000",
    "end": "1252880"
  },
  {
    "text": "be using Oracle as a source so I'll select from my dropdown",
    "start": "1252880",
    "end": "1258280"
  },
  {
    "text": "Oracle oral and then Define the connection string and the username and password",
    "start": "1258280",
    "end": "1265400"
  },
  {
    "text": "that allows me to connect to that Source uh successful connection uh uh being",
    "start": "1265400",
    "end": "1272080"
  },
  {
    "text": "tested and then I can go ahead and add my target which is the Redi",
    "start": "1272080",
    "end": "1277480"
  },
  {
    "text": "cluster so there's a few um uh pieces of information that you need in order to",
    "start": "1277480",
    "end": "1283400"
  },
  {
    "text": "load data into the red shift first of all of course you need the red shift cluster information uh and I've got a",
    "start": "1283400",
    "end": "1289440"
  },
  {
    "text": "little cheat sheet here that has this information for me I'll just go ahead and load that",
    "start": "1289440",
    "end": "1294799"
  },
  {
    "start": "1290000",
    "end": "1296000"
  },
  {
    "text": "up the port number username and",
    "start": "1294799",
    "end": "1299399"
  },
  {
    "start": "1296000",
    "end": "1346000"
  },
  {
    "text": "password and then I can select uh the database name uh that I'm going to be loading to in this case demo the second",
    "start": "1301120",
    "end": "1307960"
  },
  {
    "text": "piece of information is of course the eternity Cloud beam account that was subscribed to the DB to Red shift uh",
    "start": "1307960",
    "end": "1314440"
  },
  {
    "text": "service so I'll just go ahead and enter my credentials here",
    "start": "1314440",
    "end": "1320320"
  },
  {
    "text": "and then finally we need an Amazon S3 staging bucket where the data would be stored uh temporarily before the copy",
    "start": "1323559",
    "end": "1330880"
  },
  {
    "text": "command is issued against the red shift cluster and typically uh this bucket has to be in the same region as the Amazon",
    "start": "1330880",
    "end": "1337559"
  },
  {
    "text": "red shift Target in this case my red shift Target is in the northern Virginia uh region so I'm just simply going to",
    "start": "1337559",
    "end": "1344159"
  },
  {
    "text": "use uh a demo Virginia bucket uh to temporarily store",
    "start": "1344159",
    "end": "1351519"
  },
  {
    "start": "1346000",
    "end": "1365000"
  },
  {
    "text": "there okay so once I've gone ahead and entered all of this information I can",
    "start": "1361880",
    "end": "1367000"
  },
  {
    "start": "1365000",
    "end": "1391000"
  },
  {
    "text": "then simply click the browse button here to select the folder that's on that um uh S3 uh bucket or I can go ahead and",
    "start": "1367000",
    "end": "1374559"
  },
  {
    "text": "create a new folder to temporarily house the data uh once I hit test here it makes sure uh that or ensures that all",
    "start": "1374559",
    "end": "1382080"
  },
  {
    "text": "of the information that I've provided is correct and that completes my target definition and at this point I can then",
    "start": "1382080",
    "end": "1388600"
  },
  {
    "text": "go directly into my task creation so I'll call this Oracle to Red",
    "start": "1388600",
    "end": "1396480"
  },
  {
    "start": "1391000",
    "end": "1415000"
  },
  {
    "text": "shift and then as you can see by default full load and apply changes have been selected so this means that we'll start",
    "start": "1397520",
    "end": "1404400"
  },
  {
    "text": "out by doing a complete load of the data and then we'll go ahead into apply changes mode so if I click the okay",
    "start": "1404400",
    "end": "1411080"
  },
  {
    "text": "button here the interface or the design interface comes up which allows me to then drag my oracal source uh into the",
    "start": "1411080",
    "end": "1418360"
  },
  {
    "start": "1415000",
    "end": "1447000"
  },
  {
    "text": "interface and the red shift Target and then I can move straight into the table selection as you can see very easy in",
    "start": "1418360",
    "end": "1425279"
  },
  {
    "text": "order to determine uh where we're going to get the source data from we're going to use the HR schema so I'll search",
    "start": "1425279",
    "end": "1432400"
  },
  {
    "text": "there and select a few tables that I want to use uh and then hit okay and at",
    "start": "1432400",
    "end": "1437480"
  },
  {
    "text": "this point we're done we're ready now to load data into the rift cluster so I'll just save this and then hit the Run",
    "start": "1437480",
    "end": "1443960"
  },
  {
    "text": "button uh and as uh aginity replicate is designed uh with no scripting needed uh",
    "start": "1443960",
    "end": "1449960"
  },
  {
    "start": "1447000",
    "end": "1495000"
  },
  {
    "text": "is a very easy process uh for click to load of data directly into a r cluster",
    "start": "1449960",
    "end": "1455880"
  },
  {
    "text": "so what will happen is we'll see that we'll going we're going to prepare the target tables then load data from The",
    "start": "1455880",
    "end": "1461720"
  },
  {
    "text": "Source tables into the Target and then continue to monitor the source for changes and then update the target based",
    "start": "1461720",
    "end": "1467480"
  },
  {
    "text": "on that uh whether it's CDC for ddls or dmls so click okay and this starts of",
    "start": "1467480",
    "end": "1474440"
  },
  {
    "text": "the task we move directly into the monitoring interface which shows us in real time all of the data uh that's",
    "start": "1474440",
    "end": "1481840"
  },
  {
    "text": "going to be loaded from the source to the Target so we have now uh the four",
    "start": "1481840",
    "end": "1487080"
  },
  {
    "text": "tables being queued up we can see the estimated count of how many rows are going to be loaded if I were to fire up",
    "start": "1487080",
    "end": "1493600"
  },
  {
    "text": "SQL workbench and connect to that red shift cluster uh I can then see that the",
    "start": "1493600",
    "end": "1498919"
  },
  {
    "start": "1495000",
    "end": "1505000"
  },
  {
    "text": "HR schema has been created and all of the tables have already been loaded including uh The Columns being created",
    "start": "1498919",
    "end": "1506440"
  },
  {
    "start": "1505000",
    "end": "1514000"
  },
  {
    "text": "so automatically this is handled for you based on how the source is defined uh and then switching back here uh we can",
    "start": "1506440",
    "end": "1513039"
  },
  {
    "text": "click on the loading to see the progress of this data as it's being loaded uh the throughput allows us to monitor how",
    "start": "1513039",
    "end": "1520600"
  },
  {
    "start": "1514000",
    "end": "1538000"
  },
  {
    "text": "quickly uh this data is being loaded and of course depending on your bandwidth you're able to load this data in the",
    "start": "1520600",
    "end": "1526600"
  },
  {
    "text": "most efficient uh way possible POS by uh subscribing and using the eunity cloud",
    "start": "1526600",
    "end": "1532159"
  },
  {
    "text": "beam uh services so as this is running you have real-time data showing you uh the load",
    "start": "1532159",
    "end": "1539480"
  },
  {
    "start": "1538000",
    "end": "1625000"
  },
  {
    "text": "of the the duration of the load uh and how much uh or how many rows are actually loaded as part of this task uh",
    "start": "1539480",
    "end": "1546520"
  },
  {
    "text": "and as mentioned the task will then go into actually CDC mode waiting for",
    "start": "1546520",
    "end": "1551640"
  },
  {
    "text": "changes to the source to happen so we'll give this a couple of seconds to complete of course as you go further",
    "start": "1551640",
    "end": "1559360"
  },
  {
    "text": "along and you wanted to do some additional transformation of the data such as renaming columns or adding",
    "start": "1559360",
    "end": "1565000"
  },
  {
    "text": "additional columns or uh doing things uh like transforming columns or values or",
    "start": "1565000",
    "end": "1571679"
  },
  {
    "text": "data in some way uh this is all supported within the uh attunity replicate interface uh and you can even",
    "start": "1571679",
    "end": "1578440"
  },
  {
    "text": "use existing schemas if you want to to then load the data into uh an existing",
    "start": "1578440",
    "end": "1583640"
  },
  {
    "text": "Target table and you can choose how you want to load the data whether it's trunking the data first or uh adding the",
    "start": "1583640",
    "end": "1590919"
  },
  {
    "text": "data uh just adding the data to the Target table as it exists so with that",
    "start": "1590919",
    "end": "1596000"
  },
  {
    "text": "we can see the full load has completed successfully uh at this point if I go to the Target uh and open up uh the data uh",
    "start": "1596000",
    "end": "1605120"
  },
  {
    "text": "it will see that 107 rows have been added uh as part of the um uh data",
    "start": "1605120",
    "end": "1611279"
  },
  {
    "text": "loading uh or full load portion of aginity cloud beam uh aginity Cloud beam",
    "start": "1611279",
    "end": "1617440"
  },
  {
    "text": "loading data inter red shift so at this point the uh the task",
    "start": "1617440",
    "end": "1623120"
  },
  {
    "text": "actually goes into change processing mode and this means that we're going to be monitoring for any changes on the",
    "start": "1623120",
    "end": "1628559"
  },
  {
    "start": "1625000",
    "end": "1656000"
  },
  {
    "text": "source database so I have a little program that um allows me to modify the",
    "start": "1628559",
    "end": "1633880"
  },
  {
    "text": "source so I can insert about 10,000 rows here uh simply by running this little",
    "start": "1633880",
    "end": "1640000"
  },
  {
    "text": "app uh and that allows me to show you that you'll see that the incoming",
    "start": "1640000",
    "end": "1645039"
  },
  {
    "text": "changes will be queued up here uh and the number of transactions by default",
    "start": "1645039",
    "end": "1650600"
  },
  {
    "text": "this app is set to autocommit transactions on the source uh Unity",
    "start": "1650600",
    "end": "1655679"
  },
  {
    "text": "replicate recognizes uh that the transactions are being submitted but",
    "start": "1655679",
    "end": "1661200"
  },
  {
    "start": "1656000",
    "end": "1672000"
  },
  {
    "text": "will only commit them to the Target if they were committed on the source so once that Source uh commit has completed",
    "start": "1661200",
    "end": "1668720"
  },
  {
    "text": "we will Mark those or flag those for uh for loading into the cloud uh and we we",
    "start": "1668720",
    "end": "1674440"
  },
  {
    "start": "1672000",
    "end": "1716000"
  },
  {
    "text": "have a method of basically loading this data into memory and then performing or",
    "start": "1674440",
    "end": "1680159"
  },
  {
    "text": "or capturing that those changes and creating those CDC files and then uploading them to the cloud and then",
    "start": "1680159",
    "end": "1686600"
  },
  {
    "text": "performing the merge commands uh to load that data into red shift again the",
    "start": "1686600",
    "end": "1692240"
  },
  {
    "text": "latency apply latency allows us to uh basically see how long uh this load is",
    "start": "1692240",
    "end": "1697720"
  },
  {
    "text": "taking uh as as well as see the throughput uh here uh so that I can",
    "start": "1697720",
    "end": "1702840"
  },
  {
    "text": "quickly and easily monitor exactly what's happening uh with the data and now we can see the 10,000 inserts were",
    "start": "1702840",
    "end": "1709519"
  },
  {
    "text": "done and 10,000 updates as well so going back to my target if I run a quick count",
    "start": "1709519",
    "end": "1715559"
  },
  {
    "text": "of the HR employees we'll see now that we have uh the 10,17 rows added there so this pretty",
    "start": "1715559",
    "end": "1724559"
  },
  {
    "start": "1721000",
    "end": "1743000"
  },
  {
    "text": "much completes uh what I wanted to share with you today in terms of an overall uh overview uh uh a quick demo overview of",
    "start": "1724559",
    "end": "1732919"
  },
  {
    "text": "how you can use aity replicate to load data very quickly and efficiently into",
    "start": "1732919",
    "end": "1738120"
  },
  {
    "text": "to Amazon red shift at this point I like to trans transition back over to",
    "start": "1738120",
    "end": "1743840"
  },
  {
    "start": "1743000",
    "end": "1793000"
  },
  {
    "text": "Brad thank you Rea so for those looking to overcome the data transfer bottleneck we welcome you",
    "start": "1743840",
    "end": "1751039"
  },
  {
    "text": "to engage with us and take a closer look at the attunity cloud beam solution set together we can help you move more data",
    "start": "1751039",
    "end": "1758200"
  },
  {
    "text": "in less time and with less costs so for more information we invite you to visit",
    "start": "1758200",
    "end": "1765559"
  },
  {
    "text": "us at attunity I otherwise welcome an email or direct",
    "start": "1765559",
    "end": "1771279"
  },
  {
    "text": "call and we'll be pleased to assist you with resources of course in a webinar we keep things high level but if you have a",
    "start": "1771279",
    "end": "1778360"
  },
  {
    "text": "project with details that you'd like to discuss we're happy to help you along that I'd like to pass it along back to",
    "start": "1778360",
    "end": "1786919"
  },
  {
    "text": "John thank you Brad and and thank you Rea uh what I'd like to do now is uh you",
    "start": "1786919",
    "end": "1794559"
  },
  {
    "start": "1793000",
    "end": "2179000"
  },
  {
    "text": "know really reach the point where we can open the floor up to questions so if you have questions go ahead and uh use the",
    "start": "1794559",
    "end": "1800039"
  },
  {
    "text": "interface to to ask them and we will address those um there are a couple already out there apparently my sound",
    "start": "1800039",
    "end": "1806799"
  },
  {
    "text": "cut out as I was afraid it might uh when I was talking about read ratio uh on the Dynamo DB tables when uh on the copy",
    "start": "1806799",
    "end": "1813640"
  },
  {
    "text": "command I want you to be deliberate here when you're going out you're going to copy from Dynamo DB particularly if",
    "start": "1813640",
    "end": "1819000"
  },
  {
    "text": "you're setting up red shift and you're you're just experimenting with it uh don't go to your production tables and",
    "start": "1819000",
    "end": "1824720"
  },
  {
    "text": "just pull all the data down because you can be throttling uh uh your ordinary operation so uh instead",
    "start": "1824720",
    "end": "1832279"
  },
  {
    "text": "consider using this read ratio and set it to something that's lower than the average of the unused provision",
    "start": "1832279",
    "end": "1838039"
  },
  {
    "text": "throughput and you should be okay and get by without uh without creating a bottleneck for yourself uh that was the",
    "start": "1838039",
    "end": "1844200"
  },
  {
    "text": "point I was trying to make um let's see another question that we've got is uh",
    "start": "1844200",
    "end": "1850600"
  },
  {
    "text": "can you move data from simple DB to Red shift and you certainly can and the way you do that is to unload from simple DB",
    "start": "1850600",
    "end": "1857159"
  },
  {
    "text": "to S3 and then move data in from S3 to Red shift as uh described earlier in",
    "start": "1857159",
    "end": "1864720"
  },
  {
    "text": "um let's see have a question is the S3 bucket in",
    "start": "1864720",
    "end": "1870720"
  },
  {
    "text": "the middle between the on-prem database and red shift required um what we're talking about is",
    "start": "1870720",
    "end": "1877960"
  },
  {
    "text": "loading data into S3 and then copying the data from S3 into red shift so yes",
    "start": "1877960",
    "end": "1883440"
  },
  {
    "text": "yes that uh the the stop at S3 is required and that is is",
    "start": "1883440",
    "end": "1890919"
  },
  {
    "text": "repeated uh two or three times um we got another question in here we've",
    "start": "1890919",
    "end": "1898279"
  },
  {
    "text": "had some issues moving Unicode data from on-site SQL Server to Red",
    "start": "1898279",
    "end": "1903799"
  },
  {
    "text": "shift and they want to understand does the tunity handle data conversion",
    "start": "1904240",
    "end": "1909799"
  },
  {
    "text": "issues and in particular there are talking about uh Unicode data so I'll I'll open that up to uh either Brad or",
    "start": "1909799",
    "end": "1916399"
  },
  {
    "text": "Reza if you'd like to take that on so Brad here thank you for passing that along we're happy to engage with that",
    "start": "1916399",
    "end": "1922760"
  },
  {
    "text": "specific with the person who offered that question to better understand the The Source there uh and we should be",
    "start": "1922760",
    "end": "1928960"
  },
  {
    "text": "able to solve that for that person terrific thank you all right",
    "start": "1928960",
    "end": "1934200"
  },
  {
    "text": "question here I'm interested in completely transforming or reorganizing the structure of our source database while inserting into red shift in order",
    "start": "1934200",
    "end": "1941000"
  },
  {
    "text": "to obscure some of the complexities in our system uh for reporting purposes how can we do that with a tunity",
    "start": "1941000",
    "end": "1948840"
  },
  {
    "text": "hi Brad here again we just um that is something that's a common request uh we do have experience in uh working through",
    "start": "1948840",
    "end": "1955919"
  },
  {
    "text": "those solutions for customers uh that is uh depending on the specific example we",
    "start": "1955919",
    "end": "1962360"
  },
  {
    "text": "like the other customer we welcome the opportunity to take a closer look at what your objectives are and find a",
    "start": "1962360",
    "end": "1968679"
  },
  {
    "text": "model for you okay I have another question here",
    "start": "1968679",
    "end": "1975480"
  },
  {
    "text": "will it handle large change changes during a batch window um that's a little",
    "start": "1975480",
    "end": "1982799"
  },
  {
    "text": "vague um for example one terabyte of archive log change in an exod data",
    "start": "1982799",
    "end": "1989679"
  },
  {
    "text": "database generated within an hour if you're talking about can we ingest a a terabyte of data that would be you know",
    "start": "1989679",
    "end": "1996399"
  },
  {
    "text": "dependent upon your uh the size of your cluster and how it's organized but you know certainly engage with your account",
    "start": "1996399",
    "end": "2002559"
  },
  {
    "text": "team we'd be happy to address that with you I have another question here what about relational databases like",
    "start": "2002559",
    "end": "2010799"
  },
  {
    "text": "uh RDS Oracle um not sure what the question is here if you're asking can we",
    "start": "2010799",
    "end": "2016679"
  },
  {
    "text": "move data from a relational database like uh uh Oracle certainly we can do",
    "start": "2016679",
    "end": "2021720"
  },
  {
    "text": "that and I think you've seen an example of that from a tunity and you can certainly push data from Oracle to uh",
    "start": "2021720",
    "end": "2027600"
  },
  {
    "text": "flat files load it into S3 and copy them in from there as well um let see all",
    "start": "2027600",
    "end": "2035440"
  },
  {
    "text": "right and if you've got uh further questions please get them in there otherwise um you know we'll give another",
    "start": "2035440",
    "end": "2041720"
  },
  {
    "text": "moment and is there an AWS only recommended solution to move data from RDS",
    "start": "2041720",
    "end": "2050878"
  },
  {
    "text": "uh to Red shift and keep red shift up to datee um uh you know for all these terms",
    "start": "2050879",
    "end": "2058398"
  },
  {
    "text": "you know what does what does up to date really mean if you want to completely keep your your database up to sync and",
    "start": "2058399",
    "end": "2063520"
  },
  {
    "text": "do complicated J change data capture logic uh this is a fine opport opportunity for for our tunity for for",
    "start": "2063520",
    "end": "2070000"
  },
  {
    "text": "people who do that uh you could uh create systems on your own and and host",
    "start": "2070000",
    "end": "2075440"
  },
  {
    "text": "them certainly in in in AWS but there is no uh one",
    "start": "2075440",
    "end": "2083200"
  },
  {
    "text": "um there's no ETL process per se that's going to handle that for you as a service within",
    "start": "2083440",
    "end": "2090440"
  },
  {
    "text": "AWS um and uh Brad do you want to comment on that",
    "start": "2090440",
    "end": "2096838"
  },
  {
    "text": "yes thank you John uh to go from RDS to Red shift we do have a solution in place",
    "start": "2098040",
    "end": "2104200"
  },
  {
    "text": "it is uh I have to look at the specifics of the change dat capture function uh we",
    "start": "2104200",
    "end": "2111400"
  },
  {
    "text": "do indeed support full load all right and uh we have a question here about",
    "start": "2111400",
    "end": "2117960"
  },
  {
    "text": "um say for example that you're running SQL server on an E2 instance they want",
    "start": "2117960",
    "end": "2123040"
  },
  {
    "text": "uh the question is what built-in command does one use to copy a file from the ec2 instance",
    "start": "2123040",
    "end": "2128560"
  },
  {
    "text": "uh to an S3 bucket and that's certainly um you know uh well within the range of",
    "start": "2128560",
    "end": "2136000"
  },
  {
    "text": "the our uh our solution Architects and I would encourage you to get in touch with your",
    "start": "2136000",
    "end": "2141280"
  },
  {
    "text": "account team to help you with uh questions of that kind all right and",
    "start": "2141280",
    "end": "2146440"
  },
  {
    "text": "with that uh We've exhausted our list of questions I'd like to thank you for attending this morning uh and uh urge",
    "start": "2146440",
    "end": "2153760"
  },
  {
    "text": "you to go and take a look uh at our our best practice practices webinars that are listed on the URL in front of you",
    "start": "2153760",
    "end": "2160040"
  },
  {
    "text": "you should be able to go and see the slides for this uh the corrected uh",
    "start": "2160040",
    "end": "2165079"
  },
  {
    "text": "presentation itself as well as prior uh webinars that we presented the best",
    "start": "2165079",
    "end": "2170160"
  },
  {
    "text": "practices and the overview once again thank you for your uh attendance and thank you for attunity for uh working",
    "start": "2170160",
    "end": "2177760"
  },
  {
    "text": "with us to present this",
    "start": "2177760",
    "end": "2181520"
  }
]