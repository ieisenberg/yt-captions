[
  {
    "text": "foreign",
    "start": "0",
    "end": "2780"
  },
  {
    "text": "thank you for joining for this elastic mapreduce deep dive",
    "start": "6000",
    "end": "11760"
  },
  {
    "text": "best practices tips and tricks session I'm Ian Myers I'm a solution architect",
    "start": "11760",
    "end": "17220"
  },
  {
    "text": "based in the United Kingdom and I work with customers around Europe Building Systems on top of elastic",
    "start": "17220",
    "end": "25439"
  },
  {
    "text": "mapreduce on top of redshift Kinesis dynamodb and",
    "start": "25439",
    "end": "31140"
  },
  {
    "text": "it's a fantastic ecosystem and hopefully we'll reward your perseverance for",
    "start": "31140",
    "end": "37559"
  },
  {
    "text": "coming to the last session of the day with some interesting stuff that you can take home and make your Hadoop systems",
    "start": "37559",
    "end": "43739"
  },
  {
    "text": "run even better to start with it's worth recapping what",
    "start": "43739",
    "end": "49920"
  },
  {
    "text": "elastic mapreduce is and what people use it for",
    "start": "49920",
    "end": "55140"
  },
  {
    "text": "so it is a managed Hadoop environment where we take responsibility for",
    "start": "55140",
    "end": "60739"
  },
  {
    "text": "standing up the infrastructure installing Hadoop giving it a high",
    "start": "60739",
    "end": "66479"
  },
  {
    "text": "performance Network on which we then can run Hadoop one Hadoop 2 and tools that",
    "start": "66479",
    "end": "73380"
  },
  {
    "text": "are built to run on top of the Hadoop ecosystem including tools like spark and shark",
    "start": "73380",
    "end": "80640"
  },
  {
    "text": "presto and we'll talk in some detail about how you can build extensions",
    "start": "80640",
    "end": "87540"
  },
  {
    "text": "on top of elastic mapreduce that make elastic map reduce really a unlimitedly",
    "start": "87540",
    "end": "92939"
  },
  {
    "text": "powerful parallel processing container it's designed to integrate with S3",
    "start": "92939",
    "end": "99740"
  },
  {
    "text": "dynamodb redshift and Kinesis natively and then of course with tools like SQL",
    "start": "99740",
    "end": "107820"
  },
  {
    "text": "connectors you can connect to any type of data source on premises or to import",
    "start": "107820",
    "end": "114060"
  },
  {
    "text": "third-party data feeds you also have native support within",
    "start": "114060",
    "end": "119399"
  },
  {
    "text": "elastic mapreduce for spot instances elastic mapreduce gives you the ability",
    "start": "119399",
    "end": "124619"
  },
  {
    "text": "to say I want X number of machines and I'm willing to pay a certain amount for them",
    "start": "124619",
    "end": "130440"
  },
  {
    "text": "an elastic mapreduce will provision those machines and introduce them into your cluster automatically",
    "start": "130440",
    "end": "135599"
  },
  {
    "text": "you don't have to worry about provisioning them and managing the spot price over time",
    "start": "135599",
    "end": "142200"
  },
  {
    "text": "we can also run things like hbase on top of elastic mapreduce so you can have an",
    "start": "142200",
    "end": "147300"
  },
  {
    "text": "ultra high performance key value data store where you don't have to worry about the cluster topology",
    "start": "147300",
    "end": "153480"
  },
  {
    "text": "and we'll talk about some of the tools that can be built on top of that at its heart elastic mapreduce is a",
    "start": "153480",
    "end": "160800"
  },
  {
    "text": "service that controls the infrastructure by provisioning ec2 instances",
    "start": "160800",
    "end": "167819"
  },
  {
    "text": "in a set of instance groups there are three available instance groups today",
    "start": "167819",
    "end": "174480"
  },
  {
    "text": "the master group contains the name node for Hadoop and then we can provision a core",
    "start": "174480",
    "end": "181920"
  },
  {
    "text": "instance Group which is responsible for storing data in hdfs runs the data node",
    "start": "181920",
    "end": "187920"
  },
  {
    "text": "as well as task tracker and job tracker processes we can also provision what's called a",
    "start": "187920",
    "end": "193800"
  },
  {
    "text": "task group and a task group provides task tracker and job tracker processes but it doesn't",
    "start": "193800",
    "end": "200459"
  },
  {
    "text": "store data it's designed to use the core nodes as an hdfs file system",
    "start": "200459",
    "end": "206640"
  },
  {
    "text": "while also being able to take advantage of ultra performant access to S3 and all",
    "start": "206640",
    "end": "213540"
  },
  {
    "text": "of the instance groups run and optimized access path to S3",
    "start": "213540",
    "end": "220340"
  },
  {
    "text": "so you're able to be extremely flexible about how you build your cluster",
    "start": "220980",
    "end": "227340"
  },
  {
    "text": "your master group might be one instance type and your current nodes might be a completely different instance type",
    "start": "227340",
    "end": "235200"
  },
  {
    "text": "the task nodes as well because they're not storing data you may have no requirement for local disk on those",
    "start": "235200",
    "end": "240720"
  },
  {
    "text": "machines you may just be provisioning machines that are particularly high on RAM or high on relative amount of CPU",
    "start": "240720",
    "end": "249560"
  },
  {
    "text": "so in a lot of ways elastic mapreduce is like every other Hadoop that you will use",
    "start": "251220",
    "end": "257340"
  },
  {
    "text": "on premises or built on top of ec2 you can run a variety of distributions",
    "start": "257340",
    "end": "264180"
  },
  {
    "text": "the default is an open source based Apache Hadoop and you can also run three different",
    "start": "264180",
    "end": "270060"
  },
  {
    "text": "versions of map bars Hadoop M3 is a fairly standard Hadoop",
    "start": "270060",
    "end": "275340"
  },
  {
    "text": "environment with some unique extensions from map bar for instance the ability to mount the hdfs file system is NFS",
    "start": "275340",
    "end": "283259"
  },
  {
    "text": "and then you can also run M5 which gives you a name nodeless architecture meaning",
    "start": "283259",
    "end": "289259"
  },
  {
    "text": "it's very highly available or map our M7 which is designed for very",
    "start": "289259",
    "end": "295919"
  },
  {
    "text": "high performance pauseless garbage collection in particular for running hbase",
    "start": "295919",
    "end": "303360"
  },
  {
    "text": "we give you Total Access to the Hadoop configuration and we give you root",
    "start": "303360",
    "end": "308580"
  },
  {
    "text": "access to the machines in no way would you consider elastic map produce a black box you have as much",
    "start": "308580",
    "end": "316020"
  },
  {
    "text": "control over this environment as you do the cluster that you would build yourself on top of ec2 but you're",
    "start": "316020",
    "end": "322740"
  },
  {
    "text": "able to take advantage of all of the installation automation in order to make your job a lot easier",
    "start": "322740",
    "end": "330000"
  },
  {
    "text": "so that you can focus on what is the logic that I'm deploying what is the business problem that I'm solving and",
    "start": "330000",
    "end": "336539"
  },
  {
    "text": "not doing the undifferentiated heavy lifting of installing software managing",
    "start": "336539",
    "end": "342240"
  },
  {
    "text": "upgrades and ensuring that tools work together between different versions",
    "start": "342240",
    "end": "348660"
  },
  {
    "text": "you can install absolutely anything you want on the cluster as part of your workflow you can",
    "start": "348660",
    "end": "356039"
  },
  {
    "text": "completely reconfigure Hadoop and bounce the nodes if you need to",
    "start": "356039",
    "end": "361280"
  },
  {
    "text": "and we've done all of the hard work for a given configuration of elastic mapreduce to ensure that the tools that",
    "start": "361380",
    "end": "369600"
  },
  {
    "text": "we help you install will work nicely together so instead of you having to worry about",
    "start": "369600",
    "end": "375240"
  },
  {
    "text": "will this version of Hive be able to talk to this version of hbase we've done the tweaking to make sure",
    "start": "375240",
    "end": "381479"
  },
  {
    "text": "that those will play nicely together but that said you always have the ability to upgrade those tools at any time",
    "start": "381479",
    "end": "389360"
  },
  {
    "text": "so let's talk about configuration for a minute because it's an important part of what you may do as you move deeper and",
    "start": "390720",
    "end": "396539"
  },
  {
    "text": "deeper into your delivery of software through elastic mapreduce and it's important to know what's there",
    "start": "396539",
    "end": "403319"
  },
  {
    "text": "but you may want to change from a configuration perspective the first thing we might want to do is",
    "start": "403319",
    "end": "409919"
  },
  {
    "text": "change the core Hadoop configuration this is done through a bootstrap action",
    "start": "409919",
    "end": "417180"
  },
  {
    "text": "which is software that's run on the cluster on Startup or on resize",
    "start": "417180",
    "end": "422340"
  },
  {
    "text": "there's a particular bootstrap action called configure Hadoop and you can supply it arguments like",
    "start": "422340",
    "end": "429120"
  },
  {
    "text": "a keyword plus Dash config file or key value pairs",
    "start": "429120",
    "end": "434400"
  },
  {
    "text": "that allow you to override any parameter in the Hadoop system overall",
    "start": "434400",
    "end": "440880"
  },
  {
    "text": "if you want to completely provide a Hadoop configuration off of your existing on-premises environment",
    "start": "440880",
    "end": "448860"
  },
  {
    "text": "you can just Supply that existing coresight XML hdfs side XML",
    "start": "448860",
    "end": "456120"
  },
  {
    "text": "or you could just take the things that you've changed within those configuration files",
    "start": "456120",
    "end": "462419"
  },
  {
    "text": "and when you supply us with that file we will just merge it on top of the Hadoop configuration that's deployed to elastic",
    "start": "462419",
    "end": "468479"
  },
  {
    "text": "mapreduce equally individual key and value items",
    "start": "468479",
    "end": "474840"
  },
  {
    "text": "can be used to override specific properties we also have these little useful macros",
    "start": "474840",
    "end": "482280"
  },
  {
    "text": "so instead of having to indicate core Dash site.xml or mapred site you can",
    "start": "482280",
    "end": "487319"
  },
  {
    "text": "just say mapred or you could just use the character M and it makes your configuration changes",
    "start": "487319",
    "end": "493380"
  },
  {
    "text": "very terse when you want to deploy them so what would be an example of how we would use",
    "start": "493380",
    "end": "498599"
  },
  {
    "text": "this as we'll review a little bit later we configure the capacity of each of the",
    "start": "498599",
    "end": "506099"
  },
  {
    "text": "instance types based upon our experience of what makes it most stable",
    "start": "506099",
    "end": "512099"
  },
  {
    "text": "and the best performance per dollar and an example of that in Hadoop one is",
    "start": "512099",
    "end": "517620"
  },
  {
    "text": "that we'll set the number of task tracker nodes that can be run or test tracker processes that will be run on a",
    "start": "517620",
    "end": "524099"
  },
  {
    "text": "given machine and that's based upon its instance type and family you might use the configure Hadoop",
    "start": "524099",
    "end": "531240"
  },
  {
    "text": "action to supply an override for the mapred file",
    "start": "531240",
    "end": "536640"
  },
  {
    "text": "Supply a new config file and then set the number of Maximum tasks for mapping",
    "start": "536640",
    "end": "542459"
  },
  {
    "text": "to two which might be so that you can increase the number of tasks you have",
    "start": "542459",
    "end": "547800"
  },
  {
    "text": "for reducers you also might increase the number of map tasks that you can run on a given",
    "start": "547800",
    "end": "554040"
  },
  {
    "text": "node if you know that your mappers don't use a lot of memory means you just get more throughput out",
    "start": "554040",
    "end": "560760"
  },
  {
    "text": "of each individual machine another option",
    "start": "560760",
    "end": "567420"
  },
  {
    "text": "if you had the need to generate fairly small files and you wanted to iteratively process those on hdfs",
    "start": "567420",
    "end": "574200"
  },
  {
    "text": "you might lower the hdfs block size from the default of 128 Meg to one Meg",
    "start": "574200",
    "end": "580680"
  },
  {
    "text": "using a commands such as that so there's a lot of ability to configure",
    "start": "580680",
    "end": "587399"
  },
  {
    "text": "the internals of how Hadoop is working",
    "start": "587399",
    "end": "592459"
  },
  {
    "text": "and we can also do things particularly in Hadoop one such as forcing Hadoop to",
    "start": "592500",
    "end": "598380"
  },
  {
    "text": "reuse map task jvms this is really important because a",
    "start": "598380",
    "end": "603660"
  },
  {
    "text": "mapper from the perspective of the operating system can take between 2 and 20 seconds to start up",
    "start": "603660",
    "end": "609839"
  },
  {
    "text": "depending upon what you're asking from a configuration perspective so if you set mapred job reuse jvm num",
    "start": "609839",
    "end": "618180"
  },
  {
    "text": "tasks to -1 we will start up one map task one mapper",
    "start": "618180",
    "end": "623940"
  },
  {
    "text": "process and then it will be reused which means we avoid that processing overhead",
    "start": "623940",
    "end": "630860"
  },
  {
    "text": "and if you want to use this with your own software you're very welcome to do so but keep in mind it effectively makes",
    "start": "631140",
    "end": "636540"
  },
  {
    "text": "your map process is stateful for the entire time they run so if you're coding in Java you need to",
    "start": "636540",
    "end": "642839"
  },
  {
    "text": "think about the impact of that so this is a great example of a way that",
    "start": "642839",
    "end": "648300"
  },
  {
    "text": "you can immediately get better performance from Hadoop one simply by setting jvm reuse",
    "start": "648300",
    "end": "655940"
  },
  {
    "text": "but what about the jvm itself what if we didn't just want to override the configuration of Hadoop but we",
    "start": "656640",
    "end": "661920"
  },
  {
    "text": "wanted to deeply change the configuration of what the operating system sees",
    "start": "661920",
    "end": "668160"
  },
  {
    "text": "we would do this for example because you find that the nature of your task requires different settings particularly",
    "start": "668160",
    "end": "675540"
  },
  {
    "text": "for something like the fixed Heap size or garbage collection",
    "start": "675540",
    "end": "681380"
  },
  {
    "text": "you can use the configured demons bootstrap action to change the",
    "start": "681480",
    "end": "687899"
  },
  {
    "text": "properties of jvm question is which jvm you supply the name of the jvm that you",
    "start": "687899",
    "end": "694980"
  },
  {
    "text": "want to configure and they vary between Hadoop 1 and Hadoop 2. and then you can supply",
    "start": "694980",
    "end": "702240"
  },
  {
    "text": "options that you would typically Supply when you do the run Hadoop command",
    "start": "702240",
    "end": "707519"
  },
  {
    "text": "in here you can see that we're setting the GC time ratio to 19 and we're increasing or decreasing the Heap size",
    "start": "707519",
    "end": "715200"
  },
  {
    "text": "for for example the name node to 2gig",
    "start": "715200",
    "end": "720740"
  },
  {
    "text": "so this allows you very very fine grain control over what it is that's being run but crucially these are",
    "start": "721140",
    "end": "728820"
  },
  {
    "text": "overrides to configuration that you supply when you define the cluster it's",
    "start": "728820",
    "end": "733980"
  },
  {
    "text": "a one-time thing and then we apply it across the entire cluster for you there's no requirement for you to go and",
    "start": "733980",
    "end": "740459"
  },
  {
    "text": "visit every node which of course would be impossible when we look at customers running",
    "start": "740459",
    "end": "746360"
  },
  {
    "text": "500 800 5000 nodes a night",
    "start": "746360",
    "end": "752420"
  },
  {
    "text": "so that's how it's like everything else that you will have used on-prem or on ec2 but let's talk about what's",
    "start": "755040",
    "end": "761700"
  },
  {
    "text": "unique about elastic mapreduce and things that you can use",
    "start": "761700",
    "end": "766740"
  },
  {
    "text": "that will really allow you to differentiate your Hadoop processing",
    "start": "766740",
    "end": "772260"
  },
  {
    "text": "things like EMR consistent View taking advantage of sophisticated",
    "start": "772260",
    "end": "778260"
  },
  {
    "text": "bootstrapping or taking advantage of a very unique feature being able to resize",
    "start": "778260",
    "end": "783300"
  },
  {
    "text": "the cluster dynamically and then also crucially keeping in mind that we can use any type of instance",
    "start": "783300",
    "end": "790620"
  },
  {
    "text": "that's supported by EMR within the cluster and of course you can change it over",
    "start": "790620",
    "end": "795839"
  },
  {
    "text": "time and it's absolutely appropriate that in some cases as your workload grows either",
    "start": "795839",
    "end": "802920"
  },
  {
    "text": "seasonally or from month to month you may want to change the node type for each of those",
    "start": "802920",
    "end": "810120"
  },
  {
    "text": "specific instance groups so EMR consistent view is a new feature",
    "start": "810120",
    "end": "816660"
  },
  {
    "text": "that was released a few months ago that gives elastic mapreduce the ability",
    "start": "816660",
    "end": "823320"
  },
  {
    "text": "to have a consistent and always up-to-date view of data on S3",
    "start": "823320",
    "end": "829560"
  },
  {
    "text": "even when working on storage systems like U.S standard for S3",
    "start": "829560",
    "end": "835980"
  },
  {
    "text": "that has eventual consistency what it does is it ensures that all of",
    "start": "835980",
    "end": "841980"
  },
  {
    "text": "the files that are generated as output as part of a Hadoop job",
    "start": "841980",
    "end": "848760"
  },
  {
    "text": "are available through the step API and there's no chance that a file that's generated within one step won't be",
    "start": "848760",
    "end": "855660"
  },
  {
    "text": "available as subsequent input to other processing steps",
    "start": "855660",
    "end": "861200"
  },
  {
    "text": "you can use a command line tool to import data and synchronize data",
    "start": "861959",
    "end": "867240"
  },
  {
    "text": "between hdfs and S3 and the emrfs system what emrfs is under",
    "start": "867240",
    "end": "875279"
  },
  {
    "text": "the hood is essentially a set of indexes that are maintained on dynamodb",
    "start": "875279",
    "end": "881760"
  },
  {
    "text": "that allow us to track the Fidelity of job output over time",
    "start": "881760",
    "end": "887100"
  },
  {
    "text": "and also give us a massive performance boost when listing files in particular",
    "start": "887100",
    "end": "894240"
  },
  {
    "text": "so if you have jobs today that have a very large number of small input files emrfs may give you a significant bump in",
    "start": "894240",
    "end": "902220"
  },
  {
    "text": "the performance because we spend very low amount of time doing a list against S3 we instead use",
    "start": "902220",
    "end": "908940"
  },
  {
    "text": "the emrfs metastore that meta store sits on dynamodb you can configure it however",
    "start": "908940",
    "end": "914760"
  },
  {
    "text": "you want and crucially as we saw in the last session we can change the read and write iops that we",
    "start": "914760",
    "end": "922440"
  },
  {
    "text": "provide to that metastore through dynamodb directly and again with our configuration options",
    "start": "922440",
    "end": "928680"
  },
  {
    "text": "we can override the features of emrfs using the emrfs sitexml or the fss3",
    "start": "928680",
    "end": "935220"
  },
  {
    "text": "consistent system properties EMR consistent view is also provided",
    "start": "935220",
    "end": "940740"
  },
  {
    "text": "absolutely free of charge on top of elastic mapreduce so please turn it on",
    "start": "940740",
    "end": "948560"
  },
  {
    "text": "an example of using the EMR FS command line tool would be to synchronize data between",
    "start": "951300",
    "end": "959699"
  },
  {
    "text": "your EMR FS index and a specific bucket so for those of you who'd be interested",
    "start": "959699",
    "end": "965459"
  },
  {
    "text": "in taking your existing workflows and turning on EMR consistent view you simply need to do a sync",
    "start": "965459",
    "end": "972959"
  },
  {
    "text": "with the bucket and The Meta store that you've created through emrfs and again",
    "start": "972959",
    "end": "978000"
  },
  {
    "text": "you can see some of the commands that you would use for describing that metastore setting its capacity",
    "start": "978000",
    "end": "984540"
  },
  {
    "text": "cleaning it up of course you can delete entries from the metastore if you want to",
    "start": "984540",
    "end": "989880"
  },
  {
    "text": "but of course we would encourage you to keep your metastore as up to date as",
    "start": "989880",
    "end": "995820"
  },
  {
    "text": "possible and then you can just do a blanket import with the emrfs tool",
    "start": "995820",
    "end": "1003279"
  },
  {
    "text": "so let's talk about what those files should be that sit within emrfs on S3 or on hdfs",
    "start": "1003860",
    "end": "1013480"
  },
  {
    "text": "it's a very simple best practice I'll give you avoid small files",
    "start": "1016639",
    "end": "1021920"
  },
  {
    "text": "pretty much at any cost and small for us is defined as anything",
    "start": "1021920",
    "end": "1027140"
  },
  {
    "text": "sort of smaller than 100 Meg the reason for that as we talked very",
    "start": "1027140",
    "end": "1033500"
  },
  {
    "text": "briefly about is that mappers and reducers take time to start up",
    "start": "1033500",
    "end": "1038959"
  },
  {
    "text": "and Hadoop will generate those tasks in Hadoop one or Hadoop 2 based upon the",
    "start": "1038959",
    "end": "1044240"
  },
  {
    "text": "number of input files or input file splits so the more files we have is input to a",
    "start": "1044240",
    "end": "1051799"
  },
  {
    "text": "job the more time we spend spawning tasks and not doing useful work",
    "start": "1051799",
    "end": "1057020"
  },
  {
    "text": "which means our cluster runs longer we spend more money",
    "start": "1057020",
    "end": "1062019"
  },
  {
    "text": "and so sorry my clicker is there we go",
    "start": "1063260",
    "end": "1069140"
  },
  {
    "text": "so an example um would be if we had 10 terabytes of data and we had 100 Meg files",
    "start": "1069140",
    "end": "1075620"
  },
  {
    "text": "we would be generating potentially 10 100 000 mappers",
    "start": "1075620",
    "end": "1081380"
  },
  {
    "text": "and best case two seconds per mapper spawn gives us a total time of 55 hours",
    "start": "1081380",
    "end": "1089299"
  },
  {
    "text": "just to spawn the tasks that's not doing any useful work for you",
    "start": "1089299",
    "end": "1094940"
  },
  {
    "text": "but if we compare that with gig files we reduce that time exactly linearly",
    "start": "1094940",
    "end": "1103280"
  },
  {
    "text": "we reduce the number of tasks that we need to spawn in order to process that",
    "start": "1103280",
    "end": "1109340"
  },
  {
    "text": "data which means we spend significantly more time actually working with the file contents rather than",
    "start": "1109340",
    "end": "1116299"
  },
  {
    "text": "starting up shutting down starting up and shutting down",
    "start": "1116299",
    "end": "1120880"
  },
  {
    "text": "so what's the best file size we generally recommend one to two gig",
    "start": "1121400",
    "end": "1127340"
  },
  {
    "text": "if possible why is that",
    "start": "1127340",
    "end": "1132559"
  },
  {
    "text": "well an individual uh task in Hadoop",
    "start": "1132559",
    "end": "1138740"
  },
  {
    "text": "we don't want to have running for an overly short period of time 60 seconds is a great rule of thumb",
    "start": "1138740",
    "end": "1147340"
  },
  {
    "text": "if we're constantly starting up and shutting them down we're spending a lot of time in the operating system and not",
    "start": "1147500",
    "end": "1153140"
  },
  {
    "text": "working with data so if we said 60 seconds is the amount of time we want to be working with our",
    "start": "1153140",
    "end": "1158539"
  },
  {
    "text": "data then we see that an individual task is probably going to get 10 to 15 Mega second to S3",
    "start": "1158539",
    "end": "1166100"
  },
  {
    "text": "and based upon the instance type in the core group you'd have to measure the IR rates to hdfs",
    "start": "1166100",
    "end": "1172039"
  },
  {
    "text": "but if we look at that then speed 15 Meg times 60 seconds gives us a round gig",
    "start": "1172039",
    "end": "1179200"
  },
  {
    "text": "that can be pre or post compression certainly up to you but",
    "start": "1179720",
    "end": "1186140"
  },
  {
    "text": "very useful rule of thumb and for any of you who have very very small files sitting on S3 or",
    "start": "1186140",
    "end": "1194539"
  },
  {
    "text": "even on hdfs you can get an immediate performance boost just by aggregating those files",
    "start": "1194539",
    "end": "1202000"
  },
  {
    "text": "so what do you do about it well if you do have lots of small files",
    "start": "1202880",
    "end": "1208220"
  },
  {
    "text": "there is a utility provided with elastic mapreduce",
    "start": "1208220",
    "end": "1213740"
  },
  {
    "text": "which is an extension of the disk CP program called s3dcp",
    "start": "1213740",
    "end": "1219799"
  },
  {
    "text": "an S3 disk CP is designed to give you a very high performance way of working",
    "start": "1219799",
    "end": "1225559"
  },
  {
    "text": "with data on hdfs or S3 particularly it allows you to move data",
    "start": "1225559",
    "end": "1232640"
  },
  {
    "text": "between S3 and hdfs but also do interesting transformations",
    "start": "1232640",
    "end": "1239660"
  },
  {
    "text": "one example that we see and we absolutely encourage you to use is the ability to aggregate small files",
    "start": "1239660",
    "end": "1246740"
  },
  {
    "text": "together on the basis of a pattern that you apply to your data and you have the ability to set",
    "start": "1246740",
    "end": "1254179"
  },
  {
    "text": "a Target size for the data of whatever you require",
    "start": "1254179",
    "end": "1260480"
  },
  {
    "text": "and whatever is appropriate for the layout of data on S3 or hdfs",
    "start": "1260480",
    "end": "1268059"
  },
  {
    "text": "you can see the example using the EMR command line to add a step to a running cluster that",
    "start": "1268700",
    "end": "1276440"
  },
  {
    "text": "will aggregate our files in this case we're saying that the files live on S3 my AWS",
    "start": "1276440",
    "end": "1283220"
  },
  {
    "text": "bucket slash CF and our destination is set to be hdfs",
    "start": "1283220",
    "end": "1289580"
  },
  {
    "text": "so in this case what we'd be doing is taking a large number of small files up off of S3 and putting them into hdfs but",
    "start": "1289580",
    "end": "1296360"
  },
  {
    "text": "aggregating them at the same time by applying a group Buy in this case saying",
    "start": "1296360",
    "end": "1301820"
  },
  {
    "text": "I want to group everything by instance ID for example if we had application log",
    "start": "1301820",
    "end": "1307820"
  },
  {
    "text": "files and we're setting a Target size of 128 Meg for example",
    "start": "1307820",
    "end": "1312940"
  },
  {
    "text": "so a very simple command that you can run on your cluster and it will immediately give you performance",
    "start": "1312940",
    "end": "1318620"
  },
  {
    "text": "benefits if you have lots of small files and the number of customers we see with",
    "start": "1318620",
    "end": "1323780"
  },
  {
    "text": "100K 200k files is significant and this is a great way",
    "start": "1323780",
    "end": "1329360"
  },
  {
    "text": "to get better performance we also encourage you to use compression",
    "start": "1329360",
    "end": "1335059"
  },
  {
    "text": "along with thinking about the size of the files that you store on S3 or hdfs",
    "start": "1335059",
    "end": "1342280"
  },
  {
    "text": "again a very simple Axiom always always compress files",
    "start": "1342380",
    "end": "1348919"
  },
  {
    "text": "we absolutely appreciate that sometimes you're being delivered a feed of data that isn't compressed and that's fine",
    "start": "1348919",
    "end": "1356419"
  },
  {
    "text": "but we absolutely should apply compression wherever possible",
    "start": "1356419",
    "end": "1361820"
  },
  {
    "text": "whether it's the intermediate data that's generated on hdfs as the output from map processes",
    "start": "1361820",
    "end": "1369500"
  },
  {
    "text": "or when we write data back down to the system of record whether that's hdfs or",
    "start": "1369500",
    "end": "1374600"
  },
  {
    "text": "S3 we should absolutely compress that data the reason is",
    "start": "1374600",
    "end": "1380659"
  },
  {
    "text": "we will get much better performance because we're going to do a lot less i o",
    "start": "1380659",
    "end": "1386059"
  },
  {
    "text": "we need a lot less bandwidth between elastic mapreduce and S3 and ultimately it will save you money",
    "start": "1386059",
    "end": "1393080"
  },
  {
    "text": "on S3 which is fairly obvious so",
    "start": "1393080",
    "end": "1400039"
  },
  {
    "text": "what type of compression to use is often a question and it's important to keep in mind that",
    "start": "1400039",
    "end": "1406640"
  },
  {
    "text": "Hadoop offers many different types of compression some of them are really fast",
    "start": "1406640",
    "end": "1412880"
  },
  {
    "text": "but they're not that spacious space efficient others are extremely space efficient but",
    "start": "1412880",
    "end": "1419240"
  },
  {
    "text": "they're extremely slow and then also evaluate your compression",
    "start": "1419240",
    "end": "1424820"
  },
  {
    "text": "choice on the basis of whether or not it's splittable the default that most people will settle",
    "start": "1424820",
    "end": "1431179"
  },
  {
    "text": "on is gzip unfortunately gzip's not a great choice because it's a file level Cipher",
    "start": "1431179",
    "end": "1437960"
  },
  {
    "text": "it will only be able to tell a process that's trying to decompress it what is being decompressed when it reaches the",
    "start": "1437960",
    "end": "1444380"
  },
  {
    "text": "end of the file so if I have a one gig file everybody wants to read that has to get the whole",
    "start": "1444380",
    "end": "1450559"
  },
  {
    "text": "one gig before they can start decompressing a splittable compression codec has",
    "start": "1450559",
    "end": "1457340"
  },
  {
    "text": "internal markers that tell a reader process that it has reached a milestone",
    "start": "1457340",
    "end": "1464120"
  },
  {
    "text": "and that that data is internally consistent and can be decompressed",
    "start": "1464120",
    "end": "1469940"
  },
  {
    "text": "streaming codecs are much much faster on Hadoop and we highly encourage you to",
    "start": "1469940",
    "end": "1477320"
  },
  {
    "text": "use them especially when you get into much larger files",
    "start": "1477320",
    "end": "1483760"
  },
  {
    "text": "you can see an example there on the table of some of the comparisons that we've observed of the space efficiency",
    "start": "1484460",
    "end": "1491360"
  },
  {
    "text": "of a given codec versus the encoding speed gzip is extremely efficient at",
    "start": "1491360",
    "end": "1498080"
  },
  {
    "text": "compressing data but unfortunately it's also extremely slow and it's a file level protocol",
    "start": "1498080",
    "end": "1505340"
  },
  {
    "text": "if we look at Snappy which was designed from the ground up for Speed we can see",
    "start": "1505340",
    "end": "1510620"
  },
  {
    "text": "it's extremely fast at encoding and particularly decoding",
    "start": "1510620",
    "end": "1516140"
  },
  {
    "text": "but we don't get as good a compression rate but when I look at this table I always think well the difference is about 10",
    "start": "1516140",
    "end": "1523280"
  },
  {
    "text": "percent versus the performance impact being",
    "start": "1523280",
    "end": "1529580"
  },
  {
    "text": "25 to 30 x so a little table that you might use to",
    "start": "1529580",
    "end": "1536419"
  },
  {
    "text": "decide and of course you can use all of these different types of compression",
    "start": "1536419",
    "end": "1541520"
  },
  {
    "text": "is if you have a latency sensitive job if you need extremely fast compression",
    "start": "1541520",
    "end": "1547880"
  },
  {
    "text": "decompression then of course you want to choose a codec which will give you those attributes",
    "start": "1547880",
    "end": "1553460"
  },
  {
    "text": "that said if you have lots and lots of data petabytes of data",
    "start": "1553460",
    "end": "1559940"
  },
  {
    "text": "that 10 compression overhead might actually cost you quite a lot of money and it's appropriate in some cases to",
    "start": "1559940",
    "end": "1567200"
  },
  {
    "text": "use a much more space efficient compression codec when you have lots of data or",
    "start": "1567200",
    "end": "1572960"
  },
  {
    "text": "particularly if it's cold data that you don't look at very often",
    "start": "1572960",
    "end": "1577960"
  },
  {
    "text": "and lastly my personal advice to you would be that",
    "start": "1579020",
    "end": "1584179"
  },
  {
    "text": "if you don't have any particular requirement use lzo",
    "start": "1584179",
    "end": "1589279"
  },
  {
    "text": "it's fairly space efficient it's extremely fast",
    "start": "1589279",
    "end": "1594620"
  },
  {
    "text": "and it has support across Hadoop 1 and Hadoop 2. and there's a large variety of command",
    "start": "1594620",
    "end": "1602299"
  },
  {
    "text": "line tools on Windows Mac and Linux to be able to work with that data",
    "start": "1602299",
    "end": "1608059"
  },
  {
    "text": "so it's a very good choice and if you ever want to change it one of",
    "start": "1608059",
    "end": "1613279"
  },
  {
    "text": "the other nice features of s3dcp is that you can just Supply an output codec",
    "start": "1613279",
    "end": "1619159"
  },
  {
    "text": "so you might have an S3 disc CP job that didn't do any aggregation",
    "start": "1619159",
    "end": "1625778"
  },
  {
    "text": "it's always good to do so but it doesn't have to it might just decompress the contents and then",
    "start": "1625820",
    "end": "1631820"
  },
  {
    "text": "recompress them as a different compression codec so with one Fell Swoop",
    "start": "1631820",
    "end": "1638240"
  },
  {
    "text": "you all can go back to your Hadoop environments in your S3 environment run a single command on elastic mapreduce",
    "start": "1638240",
    "end": "1645320"
  },
  {
    "text": "cluster and you can apply a streaming compression codec and aggregate your data into a new layout that could give",
    "start": "1645320",
    "end": "1652700"
  },
  {
    "text": "you massive performance improvements and it's really worthwhile to spend a",
    "start": "1652700",
    "end": "1659000"
  },
  {
    "text": "little bit of time engineering the layout of the data on S3 for efficiency",
    "start": "1659000",
    "end": "1665059"
  },
  {
    "text": "but where should this data be let's talk about S3 and hdfs and the relationship they",
    "start": "1665059",
    "end": "1671779"
  },
  {
    "text": "have with each other we really encourage you to use S3 as",
    "start": "1671779",
    "end": "1678919"
  },
  {
    "text": "your primary and permanent data store the reasons are Myriad it has a design",
    "start": "1678919",
    "end": "1685460"
  },
  {
    "text": "for 11 nines of durability it gives you features like server-side encryption the ability to turn on",
    "start": "1685460",
    "end": "1692980"
  },
  {
    "text": "versioning so you can protect yourself from data corruption",
    "start": "1692980",
    "end": "1698140"
  },
  {
    "text": "and you can also apply automatic life cycle policies to Archive old data down",
    "start": "1698419",
    "end": "1704240"
  },
  {
    "text": "to Glacier for example and of course we're going to use the",
    "start": "1704240",
    "end": "1710120"
  },
  {
    "text": "hdfs file system for the intermediate storage on a cluster",
    "start": "1710120",
    "end": "1716360"
  },
  {
    "text": "and this is operationally really really simple because we run our cluster we have input",
    "start": "1716360",
    "end": "1723020"
  },
  {
    "text": "data on S3 output data on S3 and there's no sort of additional steps",
    "start": "1723020",
    "end": "1729380"
  },
  {
    "text": "necessary to prepare our cluster for processing",
    "start": "1729380",
    "end": "1734320"
  },
  {
    "text": "it also gives us probably one of the most important features that you can take advantage of to save cost",
    "start": "1734900",
    "end": "1741799"
  },
  {
    "text": "and that's the ability to shut your cluster down many of the Hadoop clusters that we see",
    "start": "1741799",
    "end": "1747860"
  },
  {
    "text": "customers running will be run periodically whether that's every day every week",
    "start": "1747860",
    "end": "1753320"
  },
  {
    "text": "or even every four hours to process data you certainly will have clusters that",
    "start": "1753320",
    "end": "1759020"
  },
  {
    "text": "are always on which we'll talk about a pattern for but the fact that we can shut down the",
    "start": "1759020",
    "end": "1765679"
  },
  {
    "text": "cluster can reduce our costs so significantly some customers see up to a 70 percent",
    "start": "1765679",
    "end": "1771080"
  },
  {
    "text": "cost reduction in their overall Hadoop spend that they can do something even better",
    "start": "1771080",
    "end": "1777740"
  },
  {
    "text": "they can make their clusters bigger because they're not running them for as long",
    "start": "1777740",
    "end": "1783799"
  },
  {
    "text": "they maybe then save 30 or 40 percent but all their jobs get done twice as",
    "start": "1783799",
    "end": "1789380"
  },
  {
    "text": "fast fantastic outcome when you use S3 as your primary store",
    "start": "1789380",
    "end": "1795860"
  },
  {
    "text": "you also don't need to think about scalability of hdfs nor do you have to think about the replication that's going",
    "start": "1795860",
    "end": "1802039"
  },
  {
    "text": "on on the Hadoop cluster because S3 is handling that all for you",
    "start": "1802039",
    "end": "1808360"
  },
  {
    "text": "also S3 is designed to scale with your data",
    "start": "1808399",
    "end": "1813820"
  },
  {
    "text": "as you do more and more reads against S3 data it will internally scale up to meet",
    "start": "1813980",
    "end": "1820220"
  },
  {
    "text": "that demand and of course it's a virtually unlimited storage platform",
    "start": "1820220",
    "end": "1825440"
  },
  {
    "text": "your hdfs environment within a cluster will not be",
    "start": "1825440",
    "end": "1830500"
  },
  {
    "text": "and in some cases customers will have to change the size of their jobs over time",
    "start": "1830600",
    "end": "1836000"
  },
  {
    "text": "to deal with the fact that the hdfs requirements change significantly and using S3 is your primary store means",
    "start": "1836000",
    "end": "1842840"
  },
  {
    "text": "that you can avoid that sort of optimization the other thing it allows you to do is",
    "start": "1842840",
    "end": "1847880"
  },
  {
    "text": "share data between heterogeneous clusters some of you may be running Hadoop",
    "start": "1847880",
    "end": "1855140"
  },
  {
    "text": "a bit like a monolithic piece of kit that you have to install and babysit to keep up online",
    "start": "1855140",
    "end": "1864380"
  },
  {
    "text": "but this idea of being able to shut down our cluster save money also can then be",
    "start": "1864380",
    "end": "1869779"
  },
  {
    "text": "applied to the fact that we don't have to just run one cluster we might run lots of clusters each with different",
    "start": "1869779",
    "end": "1876200"
  },
  {
    "text": "characteristics that are adapted to this type of processing we're doing",
    "start": "1876200",
    "end": "1881779"
  },
  {
    "text": "we might have one hdfs cluster that's backed by SSD so we get Ultra fast and",
    "start": "1881779",
    "end": "1887360"
  },
  {
    "text": "low latency access to hdfs we might have another one built on top of hs1 so we can take advantage of 48",
    "start": "1887360",
    "end": "1893960"
  },
  {
    "text": "terabyte local storage and doing this sort of topology with",
    "start": "1893960",
    "end": "1899419"
  },
  {
    "text": "hdfs can be done but it's fairly complicated to ensure that your clusters talk to each other particularly if",
    "start": "1899419",
    "end": "1906260"
  },
  {
    "text": "you're using transient clusters where things like the cluster name will be changing all the time",
    "start": "1906260",
    "end": "1912278"
  },
  {
    "text": "and we really encourage you to take advantage of the S3 features that are provided at no charge server-side",
    "start": "1913100",
    "end": "1920059"
  },
  {
    "text": "encryption life cycle policies just mean that you don't have to do any sort of",
    "start": "1920059",
    "end": "1925220"
  },
  {
    "text": "explicit conversion to Glacier storage your data's just move for you",
    "start": "1925220",
    "end": "1930500"
  },
  {
    "text": "and it also allows us to build these really elastic clusters",
    "start": "1930500",
    "end": "1935659"
  },
  {
    "text": "we're able to add and remove nodes from our cluster dynamically knowing that the",
    "start": "1935659",
    "end": "1941179"
  },
  {
    "text": "data is stored on S3 we're not going to have an impact of that resize operation which we're going to talk about shortly",
    "start": "1941179",
    "end": "1948700"
  },
  {
    "text": "but you say what about data locality surely the whole point of Hadoop and the",
    "start": "1948799",
    "end": "1953840"
  },
  {
    "text": "mapreduce architecture is that we're sending compute to data",
    "start": "1953840",
    "end": "1958658"
  },
  {
    "text": "now it is very important that you run your cluster in the same region as your",
    "start": "1959299",
    "end": "1965539"
  },
  {
    "text": "data Hadoop absolutely can pull data across the Atlantic or all the way",
    "start": "1965539",
    "end": "1971240"
  },
  {
    "text": "around the globe if you want it to but of course it will incur a latency penalty when it does that and that will",
    "start": "1971240",
    "end": "1977179"
  },
  {
    "text": "directly impact the performance of your job it's often much better to move data once",
    "start": "1977179",
    "end": "1982940"
  },
  {
    "text": "with S3 disk CP between regions and then process it in the region where that data lives",
    "start": "1982940",
    "end": "1989799"
  },
  {
    "text": "and no amount of high speed and optimized i o access to S3 is going to",
    "start": "1990020",
    "end": "1995659"
  },
  {
    "text": "make up for the fact that we're calling data that lives in Singapore from Us",
    "start": "1995659",
    "end": "2000940"
  },
  {
    "text": "East for example and the thing to keep in mind is that if your jobs are particularly memory or",
    "start": "2000940",
    "end": "2007899"
  },
  {
    "text": "CPU heavy data locality is not actually doing that much for you",
    "start": "2007899",
    "end": "2013600"
  },
  {
    "text": "once you've got a chunk of data you're going to work against that and the amount of time that we need to spend",
    "start": "2013600",
    "end": "2018700"
  },
  {
    "text": "this 60 second rule will massively outweigh the data",
    "start": "2018700",
    "end": "2023860"
  },
  {
    "text": "transfer time so we really can just focus on that but there is of course an exception to every",
    "start": "2023860",
    "end": "2030039"
  },
  {
    "text": "rule which is what if your job isn't CPU or memory bound what if it's i o bound",
    "start": "2030039",
    "end": "2037779"
  },
  {
    "text": "when we do encourage customers to use their local hdfs",
    "start": "2037779",
    "end": "2043919"
  },
  {
    "text": "particularly for jobs which require repeated access to data",
    "start": "2043919",
    "end": "2049060"
  },
  {
    "text": "an example of this would be if you had a hive job where you imported some data",
    "start": "2049060",
    "end": "2054158"
  },
  {
    "text": "off of S3 and then you ran a bunch of Transformations against that one",
    "start": "2054159",
    "end": "2059500"
  },
  {
    "text": "core set of events into different types of aggregations for example",
    "start": "2059500",
    "end": "2065500"
  },
  {
    "text": "then you would get a lot better performance by first lifting that data up into an internal table in Hive so",
    "start": "2065500",
    "end": "2072398"
  },
  {
    "text": "that we're only using the local access but that doesn't change the fact that we want to use S3 as our permanent system",
    "start": "2072399",
    "end": "2080200"
  },
  {
    "text": "of record especially so that we get 11 nines of durability of that data",
    "start": "2080200",
    "end": "2086618"
  },
  {
    "text": "so the pattern that we would use then is that our data lives long term on S3 and",
    "start": "2086619",
    "end": "2092080"
  },
  {
    "text": "we launch our cluster and we copy the data up using s3dcp onto the hdfs file",
    "start": "2092080",
    "end": "2098140"
  },
  {
    "text": "system process it in place and at some point certainly before we",
    "start": "2098140",
    "end": "2105520"
  },
  {
    "text": "shut the cluster down we send a backup of that data back to S3",
    "start": "2105520",
    "end": "2111420"
  },
  {
    "text": "it gives us a lot of really nice advantages particularly for i o",
    "start": "2111420",
    "end": "2116920"
  },
  {
    "text": "intensive workloads but again we get",
    "start": "2116920",
    "end": "2123700"
  },
  {
    "text": "the system of record on S3 so we kind of get the Best of Both Worlds at the cost of increased",
    "start": "2123700",
    "end": "2130300"
  },
  {
    "text": "operational complexity you have to administer the fact that your cluster is lifting up a data set",
    "start": "2130300",
    "end": "2136240"
  },
  {
    "text": "into hdfs before it does work and you have to ensure that that data is stored",
    "start": "2136240",
    "end": "2141820"
  },
  {
    "text": "back down to S3 for long-term storage but in many cases this can be very very",
    "start": "2141820",
    "end": "2148780"
  },
  {
    "text": "useful and allows you then to optimize your cluster for hdfs access",
    "start": "2148780",
    "end": "2154599"
  },
  {
    "text": "so what's an example of where we might see this in use",
    "start": "2154599",
    "end": "2159240"
  },
  {
    "text": "if we had an application which was taking advantage of Kinesis for streaming data",
    "start": "2160300",
    "end": "2167020"
  },
  {
    "text": "we'd see that data being pushed into a Kinesis stream which we can scale up and down dynamically",
    "start": "2167020",
    "end": "2172780"
  },
  {
    "text": "and we might have a set of Kinesis enabled applications one of which was using the Kinesis",
    "start": "2172780",
    "end": "2178599"
  },
  {
    "text": "connector library for S3 to Archive that data long term to S3",
    "start": "2178599",
    "end": "2183640"
  },
  {
    "text": "that is a best practice of mind for Kinesis always store your raw events on S3",
    "start": "2183640",
    "end": "2189460"
  },
  {
    "text": "in a compressed format so that you can reconstitute the stream at some point in",
    "start": "2189460",
    "end": "2194859"
  },
  {
    "text": "the future but we'd have another Kinesis enabled application running on the same machine running on a different one doesn't",
    "start": "2194859",
    "end": "2200920"
  },
  {
    "text": "matter and it might be loading an hdfs cluster in real time",
    "start": "2200920",
    "end": "2206920"
  },
  {
    "text": "for example if we wanted to do real-time processing with spark we apply maybe a spark ml model",
    "start": "2206920",
    "end": "2215320"
  },
  {
    "text": "on top of that data that we've loaded in near real time into our hdfs system and",
    "start": "2215320",
    "end": "2221500"
  },
  {
    "text": "then we take the output of those models and we push it down to S3 as an",
    "start": "2221500",
    "end": "2226540"
  },
  {
    "text": "aggregate set to be consumed elsewhere and then we might take advantage of the",
    "start": "2226540",
    "end": "2231820"
  },
  {
    "text": "fact that redshift can be loaded from elastic mapreduce directly from hdfs",
    "start": "2231820",
    "end": "2237820"
  },
  {
    "text": "using the copy command and so we've got a real-time processing",
    "start": "2237820",
    "end": "2243400"
  },
  {
    "text": "pipeline that's taking advantage of this high performance hdfs file system in-memory processing with spark and then",
    "start": "2243400",
    "end": "2250240"
  },
  {
    "text": "pushing the aggregate results out for subsequent reporting",
    "start": "2250240",
    "end": "2255280"
  },
  {
    "text": "so how would we do this how would we extend elastic mapreduce to do this kind of processing",
    "start": "2255280",
    "end": "2261700"
  },
  {
    "text": "again bootstrap actions give us the ability we touched briefly on configure Hadoop and configure demons",
    "start": "2261700",
    "end": "2268359"
  },
  {
    "text": "that configure the internals of Hadoop or the jvm but there are also bootstrap actions to",
    "start": "2268359",
    "end": "2274060"
  },
  {
    "text": "install of course Hive Pig Impala but there are newer bootstrap actions",
    "start": "2274060",
    "end": "2281800"
  },
  {
    "text": "that will allow us to install Apache Phoenix on top of hbase which is a SQL",
    "start": "2281800",
    "end": "2287200"
  },
  {
    "text": "library for working with key value data stored in hbase spark and shark of course",
    "start": "2287200",
    "end": "2293560"
  },
  {
    "text": "and then support for tools like presto which is a non-maproduce based SQL",
    "start": "2293560",
    "end": "2299740"
  },
  {
    "text": "engine on top of file data and for those of you who are interested",
    "start": "2299740",
    "end": "2305020"
  },
  {
    "text": "in learning more particularly about spark and shark spark SQL and Presto",
    "start": "2305020",
    "end": "2310660"
  },
  {
    "text": "there's a session tomorrow morning at nine o'clock called running your data warehouse on the Mr so I'd encourage you",
    "start": "2310660",
    "end": "2316359"
  },
  {
    "text": "to go to that one we also have this concept of tooling",
    "start": "2316359",
    "end": "2322900"
  },
  {
    "text": "macros which are sort of wrappers around bootstrap actions where we've taken all",
    "start": "2322900",
    "end": "2328060"
  },
  {
    "text": "of the responsibility for building that bootstrap action and turning it into something useful",
    "start": "2328060",
    "end": "2333280"
  },
  {
    "text": "to run Hive you get hive 13 now for instance with support for orc files",
    "start": "2333280",
    "end": "2339339"
  },
  {
    "text": "org files are a columnar file format particularly designed for large amounts",
    "start": "2339339",
    "end": "2344980"
  },
  {
    "text": "of low cardinality data we also have access to window functions like you",
    "start": "2344980",
    "end": "2350440"
  },
  {
    "text": "would see on Amazon redshift or extremely powerful MPP platform",
    "start": "2350440",
    "end": "2356560"
  },
  {
    "text": "with pig 12 we get streaming udfs that you can ride in Python",
    "start": "2356560",
    "end": "2361900"
  },
  {
    "text": "you get native support for Avro and parquet you can run Impala with the",
    "start": "2361900",
    "end": "2367060"
  },
  {
    "text": "minus minus Impala interactive macro that gives us in-memory SQL processing",
    "start": "2367060",
    "end": "2373119"
  },
  {
    "text": "support for parquet all of the stuff that you would expect from Impala and then of course hbase is just minus",
    "start": "2373119",
    "end": "2379480"
  },
  {
    "text": "minus H base or a tick box on the AWS web console and we get hbase 9418 is the latest",
    "start": "2379480",
    "end": "2387940"
  },
  {
    "text": "version which gives us read replicas and read caching",
    "start": "2387940",
    "end": "2393520"
  },
  {
    "text": "transaction support database snapshotting really useful features EMR is also integrated directly with",
    "start": "2393520",
    "end": "2399579"
  },
  {
    "text": "kinesis we can read data directly from a Kinesis stream into Hive Pig streaming cascading",
    "start": "2399579",
    "end": "2408579"
  },
  {
    "text": "with no intermediate data storage there's no S3 and no hdfs involved and",
    "start": "2408579",
    "end": "2413740"
  },
  {
    "text": "it's a very very simple and nice way to introduce streaming data into what is formerly a batch based system",
    "start": "2413740",
    "end": "2422079"
  },
  {
    "text": "means you can move from file based transport to stream based transport without having to change",
    "start": "2422079",
    "end": "2428140"
  },
  {
    "text": "your Hadoop jobs and then in time you may rewrite those Hadoop jobs as spark streaming Kinesis",
    "start": "2428140",
    "end": "2435579"
  },
  {
    "text": "enabled applications or something else",
    "start": "2435579",
    "end": "2440640"
  },
  {
    "text": "an example of that in hive for example if we had a telecoms application we might have some call data",
    "start": "2441040",
    "end": "2447160"
  },
  {
    "text": "records that we want to track we say that those are CSV data in this case a comma",
    "start": "2447160",
    "end": "2454540"
  },
  {
    "text": "and we indicate that they're stored using a hive Kinesis storage Handler we indicate the stream and the table",
    "start": "2454540",
    "end": "2459940"
  },
  {
    "text": "properties and then we can just run select statements hql statements on top",
    "start": "2459940",
    "end": "2466300"
  },
  {
    "text": "of our Kinesis data all of the data serialization is handled for you there's no intermediate data",
    "start": "2466300",
    "end": "2472780"
  },
  {
    "text": "storage and it's a very nice way to take a raw input feed and micro batch process that",
    "start": "2472780",
    "end": "2478060"
  },
  {
    "text": "maybe just storing it back onto S3 as aggregated files",
    "start": "2478060",
    "end": "2483760"
  },
  {
    "text": "we also just announced support for Hugh you can now run the Hue user interface",
    "start": "2483760",
    "end": "2490240"
  },
  {
    "text": "for Hadoop which gives you a web-based GUI for",
    "start": "2490240",
    "end": "2495520"
  },
  {
    "text": "working with Hadoop gives you things like an S3 and an hdfs",
    "start": "2495520",
    "end": "2501160"
  },
  {
    "text": "browser for example you could give your end users the ability to upload CSV data",
    "start": "2501160",
    "end": "2507460"
  },
  {
    "text": "directly through a web application and into your S3 buckets",
    "start": "2507460",
    "end": "2513060"
  },
  {
    "text": "we get Hive Pig and Impala query editors that do syntax checking validation and",
    "start": "2513400",
    "end": "2520480"
  },
  {
    "text": "then when you run them it does all of the process tracking and gives us access to the allocation of containers or tasks",
    "start": "2520480",
    "end": "2528400"
  },
  {
    "text": "to individual nodes we can see a monitoring interface it also has support for ldap Authentication",
    "start": "2528400",
    "end": "2534099"
  },
  {
    "text": "so your end users can be authenticated straight into queue through active directory or Samba",
    "start": "2534099",
    "end": "2543960"
  },
  {
    "text": "that's a snapshot of what it looks like it's a very nice tool",
    "start": "2544119",
    "end": "2549220"
  },
  {
    "text": "and will give you a much better experience for those customers that you want to expose your raw Hadoop",
    "start": "2549220",
    "end": "2555700"
  },
  {
    "text": "processing to you can also create brand new bootstrap",
    "start": "2555700",
    "end": "2561339"
  },
  {
    "text": "actions if you want to they're just bash scripts by default and they run as root",
    "start": "2561339",
    "end": "2567520"
  },
  {
    "text": "and you can do absolutely anything in them anything",
    "start": "2567520",
    "end": "2572280"
  },
  {
    "text": "they run on cluster startup or cluster resize and",
    "start": "2572859",
    "end": "2577960"
  },
  {
    "text": "one little tip for you is when you run an EMR cluster in the console or the job",
    "start": "2577960",
    "end": "2583420"
  },
  {
    "text": "description it will tell you which Ami it's running if you want to build new bootstrap actions you can run that Ami as a T2",
    "start": "2583420",
    "end": "2591220"
  },
  {
    "text": "micro for example do all of your development there and then know that it will be compatible with the",
    "start": "2591220",
    "end": "2597819"
  },
  {
    "text": "operating system that Hadoop will be running on we also maintain a GitHub of bootstrap",
    "start": "2597819",
    "end": "2604720"
  },
  {
    "text": "actions that you can clone and extend for things like a cumula Phoenix is in",
    "start": "2604720",
    "end": "2610660"
  },
  {
    "text": "there node.js support is in there lots of very",
    "start": "2610660",
    "end": "2616000"
  },
  {
    "text": "useful bootstrap action so we'd encourage you to have a look at the Amazon EMR repository bootstrap actions",
    "start": "2616000",
    "end": "2623800"
  },
  {
    "text": "so now we need to very quickly talk about sizing our cluster it's important to understand the",
    "start": "2623800",
    "end": "2629920"
  },
  {
    "text": "difference between Hadoop one where we have static numbers of tasks allocated per node type and Hadoop two",
    "start": "2629920",
    "end": "2636460"
  },
  {
    "text": "where we have a much more variable allocation of tasks to the containers",
    "start": "2636460",
    "end": "2643240"
  },
  {
    "text": "and what we're going to use from a sizing perspective as we look at this in an axiom that you might choose to follow",
    "start": "2643240",
    "end": "2650260"
  },
  {
    "text": "is that there is a setting on Hadoop 2 called container Max Ram and if you have jobs that are memory",
    "start": "2650260",
    "end": "2657099"
  },
  {
    "text": "bound then we can work out how many tasks we will typically run by dividing",
    "start": "2657099",
    "end": "2662619"
  },
  {
    "text": "the container Max Ram by the mapreduce RAM and this is all documented in the",
    "start": "2662619",
    "end": "2667720"
  },
  {
    "text": "Hadoop 2 Ami reference this is a very useful thing to keep in mind when we size our cluster because",
    "start": "2667720",
    "end": "2674200"
  },
  {
    "text": "the capacity of Hadoop is not based upon machines it's based upon tasks and this",
    "start": "2674200",
    "end": "2680079"
  },
  {
    "text": "shows you a table of how the number of map or reduced tasks in Hadoop one will",
    "start": "2680079",
    "end": "2685480"
  },
  {
    "text": "map onto the different instance types as well as things like CPU capacity and Storage",
    "start": "2685480",
    "end": "2692400"
  },
  {
    "text": "so our suggestion to you is use M1 and C1 for functional testing because",
    "start": "2692680",
    "end": "2698140"
  },
  {
    "text": "they're cheap and you're not going to run them that long but certainly use M3 and C3 for",
    "start": "2698140",
    "end": "2705400"
  },
  {
    "text": "production you definitely want to use cc2 C3 and in",
    "start": "2705400",
    "end": "2710980"
  },
  {
    "text": "future C4 instance types for very memory and CPU intensive jobs but also take",
    "start": "2710980",
    "end": "2716020"
  },
  {
    "text": "advantage of the storage optimized instances like hs1 with 48 terabytes of local storage",
    "start": "2716020",
    "end": "2722079"
  },
  {
    "text": "High one with two terabytes of SSD that will do about 120 000 iops each",
    "start": "2722079",
    "end": "2728200"
  },
  {
    "text": "or I2 and also generally we encourage you to run a smaller number of large nodes",
    "start": "2728200",
    "end": "2736480"
  },
  {
    "text": "than always scaling horizontally the reason for that will become clear in",
    "start": "2736480",
    "end": "2742599"
  },
  {
    "text": "just one moment so an observation for you that we've taken is that M1 and C1 is very heavily",
    "start": "2742599",
    "end": "2748540"
  },
  {
    "text": "used across elastic mapreduce but on those clusters the hdfs",
    "start": "2748540",
    "end": "2755140"
  },
  {
    "text": "utilization tends to be extremely low and in that case it is much better for",
    "start": "2755140",
    "end": "2761560"
  },
  {
    "text": "you to use m3c3 because we get a better performance per task per dollar from the C3 and M3",
    "start": "2761560",
    "end": "2771040"
  },
  {
    "text": "families for example here the cost on an M3 extra large is half of what you're getting on",
    "start": "2771040",
    "end": "2776980"
  },
  {
    "text": "an M1 large similarly sized instances but it's even more pronounced on the C3",
    "start": "2776980",
    "end": "2785819"
  },
  {
    "text": "for the right instance type we're getting 7x cheaper performance per task",
    "start": "2786280",
    "end": "2792700"
  },
  {
    "text": "so we highly highly encourage you to migrate your M1 and C1 workloads over to M3 and C3",
    "start": "2792700",
    "end": "2799240"
  },
  {
    "text": "so how many nodes do you need well there's a simple cluster sizing calculation that we can use you figure",
    "start": "2799240",
    "end": "2805599"
  },
  {
    "text": "out how many tasks your job is going to use and you can either guess or you can measure that empirically and then you",
    "start": "2805599",
    "end": "2810819"
  },
  {
    "text": "pick an instance and note down the number of tasks this is in the task configuration documentation sorry",
    "start": "2810819",
    "end": "2817359"
  },
  {
    "text": "we need to pick some sample data files that we want to run against and we'll run a cluster with a single core node",
    "start": "2817359",
    "end": "2823780"
  },
  {
    "text": "to process this data and note down how long it takes and then there's a formula",
    "start": "2823780",
    "end": "2830380"
  },
  {
    "text": "the number of nodes is total tasks times the time to process these sample files divided by the instance capacity and",
    "start": "2830380",
    "end": "2838359"
  },
  {
    "text": "then a magic number which I really like the ability to say how long do I want it",
    "start": "2838359",
    "end": "2843700"
  },
  {
    "text": "to take so an example we think we need about 150",
    "start": "2843700",
    "end": "2848859"
  },
  {
    "text": "tasks for a data set so we're going to use an m1xl for testing and it has eight tasks configured",
    "start": "2848859",
    "end": "2856420"
  },
  {
    "text": "we select then eight files and we run a cluster and we observe that it takes three minutes",
    "start": "2856420",
    "end": "2862660"
  },
  {
    "text": "we plug that into our formula and we say I want this to only take five minutes hey Presto we need 11 M1 extra largest",
    "start": "2862660",
    "end": "2870760"
  },
  {
    "text": "in order to process our data and you can use that for any type of",
    "start": "2870760",
    "end": "2876160"
  },
  {
    "text": "task that you're running now and now you can have a great conversation with your business users how long do you want it to take",
    "start": "2876160",
    "end": "2882280"
  },
  {
    "text": "because keep in mind and EMR makes this easier than almost any other platform today",
    "start": "2882280",
    "end": "2887560"
  },
  {
    "text": "that running 10 machines for one hour is exactly the same price as running one",
    "start": "2887560",
    "end": "2892960"
  },
  {
    "text": "machine for 10 hours so with EMR you go wide go as wide as",
    "start": "2892960",
    "end": "2899380"
  },
  {
    "text": "you can and you get your processing done faster for exactly the same money",
    "start": "2899380",
    "end": "2906339"
  },
  {
    "text": "so we need to build transient cluster groups and we encourage you to take more advantage of cluster resize than what we",
    "start": "2906339",
    "end": "2911980"
  },
  {
    "text": "see customers doing today all you have to do is change the instance count on any of those instance",
    "start": "2911980",
    "end": "2920140"
  },
  {
    "text": "groups and EMR will take responsibility for sizing up or down and the processing of the cluster will",
    "start": "2920140",
    "end": "2926619"
  },
  {
    "text": "continue while that resize is happening so we might run a couple of core nodes and we might expand those over time",
    "start": "2926619",
    "end": "2934420"
  },
  {
    "text": "to increase our hdfs storage or to increase the amount of CPU and memory that we have access to but we can't",
    "start": "2934420",
    "end": "2941140"
  },
  {
    "text": "really practically remove them because hdfs replication will cause",
    "start": "2941140",
    "end": "2946560"
  },
  {
    "text": "backups to be restored and our hdfs system will become really busy and it will stop processing useful data so we",
    "start": "2946560",
    "end": "2953859"
  },
  {
    "text": "add the task group and this is where we see the greatest benefit for scaling your clusters over time",
    "start": "2953859",
    "end": "2960760"
  },
  {
    "text": "add a task Group which is not storing data and then you can scale up as big as",
    "start": "2960760",
    "end": "2966040"
  },
  {
    "text": "you want keeping in mind that they will be talking independently to S3 if you're",
    "start": "2966040",
    "end": "2971560"
  },
  {
    "text": "using S3 based jobs and then when you're done get rid of them",
    "start": "2971560",
    "end": "2976559"
  },
  {
    "text": "the resize cases that we generally recommend you use are for example of smaller always on cluster particularly",
    "start": "2977380",
    "end": "2984520"
  },
  {
    "text": "when you have the need for ad hoc access by analysts and then you expand that with task nodes",
    "start": "2984520",
    "end": "2991900"
  },
  {
    "text": "during business hours keeps your costs low keeps your customers happy",
    "start": "2991900",
    "end": "2997960"
  },
  {
    "text": "you can also add task nodes if you have variable workloads over time whether that's throughout the day or seasonally",
    "start": "2997960",
    "end": "3004740"
  },
  {
    "text": "and we can also particularly if we're shutting around our clusters by adding task nodes which we place on the spot",
    "start": "3004740",
    "end": "3012000"
  },
  {
    "text": "Market we can reduce the number of core nodes we require and thus pay less money",
    "start": "3012000",
    "end": "3019740"
  },
  {
    "text": "so when should you resize well use cloudwatch cloudwatch is provided free of charge",
    "start": "3019740",
    "end": "3025380"
  },
  {
    "text": "with elastic mapreduce and it gives you details on things like the status of the cluster the mapreduce task and container",
    "start": "3025380",
    "end": "3032460"
  },
  {
    "text": "performance status of the node i o hdfs and hbase",
    "start": "3032460",
    "end": "3038280"
  },
  {
    "text": "status so the key monitors I would point out for you the number of map tasks or",
    "start": "3038280",
    "end": "3043619"
  },
  {
    "text": "containers that are running and remaining and then of course the reduce equivalent",
    "start": "3043619",
    "end": "3050040"
  },
  {
    "text": "and then also look at S3 bytes read and written and hdfs utilization",
    "start": "3050040",
    "end": "3055880"
  },
  {
    "text": "other thing you can do is just add nodes while your tasks remaining is non-zero",
    "start": "3055980",
    "end": "3063359"
  },
  {
    "text": "this will allow you to just go as wide as you need to and then you stop when your forecast run time is less than 60",
    "start": "3063359",
    "end": "3069599"
  },
  {
    "text": "minutes because that's the point at which yes you're going faster and you may have a business SLA to meet so you",
    "start": "3069599",
    "end": "3074819"
  },
  {
    "text": "keep adding nodes but from a cost perspective once you're forecasted runtime hits 60 minutes it's not going",
    "start": "3074819",
    "end": "3080040"
  },
  {
    "text": "to save you any more money so this is an example of a cloud watch graph for a cluster that was running",
    "start": "3080040",
    "end": "3086520"
  },
  {
    "text": "with a certain number of nodes and then it was scaled up and you can see that the number of map tasks running jumped",
    "start": "3086520",
    "end": "3093300"
  },
  {
    "text": "to more than double and then the map phase completed and we see you just dropped off a cliff so we",
    "start": "3093300",
    "end": "3099599"
  },
  {
    "text": "doubled our processing capacity very linear processing and we look at the correlated status across this cluster",
    "start": "3099599",
    "end": "3106680"
  },
  {
    "text": "and we can see that the reduced tasks increased the map and reduced tasks remaining significantly dropped and we",
    "start": "3106680",
    "end": "3113460"
  },
  {
    "text": "were able to go a lot faster and these are all provided to you free of charge with the cloud watch GUI",
    "start": "3113460",
    "end": "3120240"
  },
  {
    "text": "both for the ec2 instances that underlie your elastic mapreduce cluster as well",
    "start": "3120240",
    "end": "3125700"
  },
  {
    "text": "as for EMR so to sum up and I appreciate your sticking around to The Bitter End",
    "start": "3125700",
    "end": "3133079"
  },
  {
    "text": "we really do recommend that you take advantage of S3 as your system of record for durability and for ensuring that you",
    "start": "3133079",
    "end": "3139859"
  },
  {
    "text": "have high fidelity data we really encourage you to save money",
    "start": "3139859",
    "end": "3145079"
  },
  {
    "text": "and go a lot faster by practicing a cloud architecture applied to Hadoop using transient clusters whether that's",
    "start": "3145079",
    "end": "3151200"
  },
  {
    "text": "a small always on cluster which you dynamically Flex up and down or completely shutting down your cluster",
    "start": "3151200",
    "end": "3157559"
  },
  {
    "text": "periodically and definitely use cluster resize to be able to make your customers more",
    "start": "3157559",
    "end": "3164280"
  },
  {
    "text": "happy and then shed that load when it's not no longer used",
    "start": "3164280",
    "end": "3169920"
  },
  {
    "text": "please move to M3 and C3 to get better price performance",
    "start": "3169920",
    "end": "3176280"
  },
  {
    "text": "keeping in mind that if you're using heavy hdfs you may still find",
    "start": "3176280",
    "end": "3181440"
  },
  {
    "text": "that M1 and C1 offer you really good local storage capability but generally if your file system of record is S3",
    "start": "3181440",
    "end": "3189480"
  },
  {
    "text": "then you should definitely move to m3c3 you'll go faster and it'll cost you less",
    "start": "3189480",
    "end": "3195540"
  },
  {
    "text": "so with that I would encourage you please give us feedback on what you thought of this session certainly attend",
    "start": "3195540",
    "end": "3201839"
  },
  {
    "text": "tomorrow's uh session on EMR data warehousing we thank you very much we hope it's been useful we'll be available",
    "start": "3201839",
    "end": "3208260"
  },
  {
    "text": "at the front and at the QA speaker stands for any questions that you might have after fact",
    "start": "3208260",
    "end": "3213359"
  },
  {
    "text": "thank you",
    "start": "3213359",
    "end": "3215900"
  }
]