[
  {
    "start": "0",
    "end": "53000"
  },
  {
    "text": "can everybody hear me sounds like the audios up now great thank you very much it's wonderful to be here i am bob",
    "start": "2090",
    "end": "9570"
  },
  {
    "text": "rogers i am chief data scientist for big data solutions at Intel and my mission",
    "start": "9570",
    "end": "15509"
  },
  {
    "text": "is to put powerful analytics in the hands of every decision maker so today",
    "start": "15509",
    "end": "21779"
  },
  {
    "text": "you're in for a treat because I'm really just here to set up dergah from AOL who",
    "start": "21779",
    "end": "27269"
  },
  {
    "text": "will show you some amazing uses of AWS that they've done where they're actually",
    "start": "27269",
    "end": "33110"
  },
  {
    "text": "distributing their distributed analytics extremely cool and so I'll just I'll set",
    "start": "33110",
    "end": "39510"
  },
  {
    "text": "things up here for the first 20 30 minutes or so and talk about my my own",
    "start": "39510",
    "end": "47120"
  },
  {
    "text": "perspective on big data and and where Intel fits in and a little bit about big",
    "start": "47120",
    "end": "54629"
  },
  {
    "start": "53000",
    "end": "384000"
  },
  {
    "text": "data analytics from the point of view of",
    "start": "54629",
    "end": "60590"
  },
  {
    "text": "looking at different types of data particularly unstructured so why am I",
    "start": "60590",
    "end": "67200"
  },
  {
    "text": "here well this is my background chief data scientist for big data at Intel i started my career as a PhD",
    "start": "67200",
    "end": "75330"
  },
  {
    "text": "astrophysicist and i was interested in computer models of stuff falling into",
    "start": "75330",
    "end": "82439"
  },
  {
    "text": "supermassive black holes and so part way through my postdoc research i got",
    "start": "82439",
    "end": "90570"
  },
  {
    "text": "interested in computing the way the brain computes so that resulted in some",
    "start": "90570",
    "end": "96030"
  },
  {
    "text": "research including writing a book on time series forecasting with artificial neural networks computing the way the",
    "start": "96030",
    "end": "102750"
  },
  {
    "text": "brain computes so anyone want to take a guess at people started calling me and",
    "start": "102750",
    "end": "109729"
  },
  {
    "text": "they wanted to know Bob can your models forecasts anyone want to guess what it",
    "start": "109729",
    "end": "116070"
  },
  {
    "text": "was stock prices exactly can your models forecast the stock market so I didn't",
    "start": "116070",
    "end": "123000"
  },
  {
    "text": "know so I started a company and I found out that I could so for 12 years I ran a",
    "start": "123000",
    "end": "128369"
  },
  {
    "text": "quantitative futures trading program that was really my first introduction to big",
    "start": "128369",
    "end": "133510"
  },
  {
    "text": "analytics because I had a two gigabyte just just think about that for a second",
    "start": "133510",
    "end": "139480"
  },
  {
    "text": "two whole gigabytes of data tick by tick data on the futures markets this was in",
    "start": "139480",
    "end": "144760"
  },
  {
    "text": "1991 and so actually 1991 two gigabytes of data was not easy to work with and so",
    "start": "144760",
    "end": "153489"
  },
  {
    "text": "with some decent algorithms and a seven-foot tall guy in the futures pit",
    "start": "153489",
    "end": "159609"
  },
  {
    "text": "in the shikon Chicago actually doing the order filling I was able to have very",
    "start": "159609",
    "end": "165159"
  },
  {
    "text": "good performance for 12 years in 2006",
    "start": "165159",
    "end": "170819"
  },
  {
    "text": "the world had changed a little bit two gigs of data was not big the algorithms",
    "start": "170819",
    "end": "177250"
  },
  {
    "text": "were not necessarily as cutting-edge the 7-foot tall guy in the pit was actually",
    "start": "177250",
    "end": "183280"
  },
  {
    "text": "replaced by the shortest piece of optic fiber that you could get between you and the electronic exchange so I was ready",
    "start": "183280",
    "end": "191109"
  },
  {
    "text": "to move on I became a product manager for a global medical device called the Humphrey field analyzer and that was",
    "start": "191109",
    "end": "197500"
  },
  {
    "text": "that was extremely interesting help me learn about healthcare which paid big",
    "start": "197500",
    "end": "202810"
  },
  {
    "text": "dividends in 2009 when the US government started incentivizing doctors to use",
    "start": "202810",
    "end": "209560"
  },
  {
    "text": "electronic health records okay so the idea was here in the US our healthcare",
    "start": "209560",
    "end": "215319"
  },
  {
    "text": "is twice as expensive per person than anywhere in the developed world yet when",
    "start": "215319",
    "end": "223150"
  },
  {
    "text": "you look at our quality measures and the standards of care there certainly no better in many cases they're not as good",
    "start": "223150",
    "end": "229269"
  },
  {
    "text": "so the idea was get the data electronic we can all relate to that and then use",
    "start": "229269",
    "end": "236949"
  },
  {
    "text": "that to optimize the health system improve care delivery improve outcomes it's going to be a happy ending so in",
    "start": "236949",
    "end": "244019"
  },
  {
    "text": "2009 I formed a company called a pic CEO to use big data analytics to analyze all",
    "start": "244019",
    "end": "251049"
  },
  {
    "text": "the data that was going to be coming out of these systems and so I'll talk to you a little bit about that because that was",
    "start": "251049",
    "end": "257070"
  },
  {
    "text": "an application of variety in big data so has everybody heard of the 3 v's of big",
    "start": "257070",
    "end": "263380"
  },
  {
    "text": "data raise your hand if you have heard of the 3ds okay so so volume which is the idea that big",
    "start": "263380",
    "end": "272740"
  },
  {
    "text": "data analytics comes into play when you have lots of data to deal with some amount of data that's hard to deal with",
    "start": "272740",
    "end": "279820"
  },
  {
    "text": "with the biggest server you can think of okay so that's your volume you have",
    "start": "279820",
    "end": "284980"
  },
  {
    "text": "second villa's velocity your data is coming in at high speed you've got",
    "start": "284980",
    "end": "290560"
  },
  {
    "text": "either very very low latency zor just a whole bunch of data coming in fast and you need to get good turnaround times",
    "start": "290560",
    "end": "295750"
  },
  {
    "text": "and then finally variety so in many cases we've been very comfortable doing",
    "start": "295750",
    "end": "302590"
  },
  {
    "text": "analytics on structured data relational databases anything that's rectangular in",
    "start": "302590",
    "end": "309250"
  },
  {
    "text": "shape that we kind of know what's there right but we humans do not generate",
    "start": "309250",
    "end": "314710"
  },
  {
    "text": "structured data i'm generating unstructured data right now right so the",
    "start": "314710",
    "end": "321450"
  },
  {
    "text": "ability to analyze unstructured data is a critical part of your your big data",
    "start": "321450",
    "end": "327610"
  },
  {
    "text": "analytics story and it's a huge opportunity for many many out of us in",
    "start": "327610",
    "end": "332950"
  },
  {
    "text": "in analytics okay so I'll talk a little bit more about that and set up the",
    "start": "332950",
    "end": "340000"
  },
  {
    "text": "variety side of things and then dergah is going to talk to you about volume and velocity and and what they've done at",
    "start": "340000",
    "end": "346750"
  },
  {
    "text": "AOL so fast forward five years a pic CEO",
    "start": "346750",
    "end": "352570"
  },
  {
    "text": "is doing fine Intel came to me and said data is the new currency and we need",
    "start": "352570",
    "end": "359170"
  },
  {
    "text": "someone to come in and help us just think about how can Intel continue to be",
    "start": "359170",
    "end": "364390"
  },
  {
    "text": "a leader in computing and analytics especially around data so i was fortunate enough to join a intel last",
    "start": "364390",
    "end": "372670"
  },
  {
    "text": "january as chief data scientist for big data and it's it's been a fantastic ride",
    "start": "372670",
    "end": "378270"
  },
  {
    "text": "this is not an advertisement for Intel that Intel is a wonderful company to be associated with so why big data at Intel",
    "start": "378270",
    "end": "388920"
  },
  {
    "start": "384000",
    "end": "612000"
  },
  {
    "text": "don't we just sell chips and the answer is yes we do sell chips are many of our",
    "start": "388920",
    "end": "395170"
  },
  {
    "text": "chips go into servers and we want our customers who's who are using our chips and their",
    "start": "395170",
    "end": "402060"
  },
  {
    "text": "data centers to be as successful as possible so Intel spends a huge amount",
    "start": "402060",
    "end": "407880"
  },
  {
    "text": "of effort actually optimizing the",
    "start": "407880",
    "end": "413240"
  },
  {
    "text": "software and the hardware to help our customers be successful we do a lot of",
    "start": "413240",
    "end": "419310"
  },
  {
    "text": "work helping our customers do well with their analytics because if they do well",
    "start": "419310",
    "end": "424830"
  },
  {
    "text": "with their analytics they're going to invest in their data centers and it works out well for everybody it's a win-win situation so on the far left you",
    "start": "424830",
    "end": "432570"
  },
  {
    "text": "have the Internet of Things one of them tells major efforts you have computing and sensors you have computing and",
    "start": "432570",
    "end": "439740"
  },
  {
    "text": "gateways you have computing in the data center so it's an end-to-end set of solutions that take data from out here",
    "start": "439740",
    "end": "447510"
  },
  {
    "text": "where we've got our wearables and our and our industrial Internet of Things devices pulling pulling the data all the",
    "start": "447510",
    "end": "454620"
  },
  {
    "text": "way into the data center so that's a big area of investment and interest for Intel on the right-hand side I have my",
    "start": "454620",
    "end": "461940"
  },
  {
    "text": "picture of DNA and it says optimization so what's that all about almost every",
    "start": "461940",
    "end": "467850"
  },
  {
    "text": "piece of software that you use commercially and or that you use from open source has actually been optimized",
    "start": "467850",
    "end": "475080"
  },
  {
    "text": "by Intel engineers to make it run really really well on Intel hardware so that's",
    "start": "475080",
    "end": "482220"
  },
  {
    "text": "a that's a critical thing to understand your software whatever you're using is going to work well on Intel hardware",
    "start": "482220",
    "end": "488970"
  },
  {
    "text": "because of the efforts that we put in the DNA is there because from my healthcare background the optimization",
    "start": "488970",
    "end": "497160"
  },
  {
    "text": "that I'm most excited about is the optimization of gene sequencing systems",
    "start": "497160",
    "end": "502530"
  },
  {
    "text": "so we've actually made it possible by working with the companies that that",
    "start": "502530",
    "end": "508470"
  },
  {
    "text": "create the analytics to do gene sequencing we've actually sped up the process by a thousand X so what used to",
    "start": "508470",
    "end": "516360"
  },
  {
    "text": "take days or weeks to sequence a patient can now be done in hours and that is you",
    "start": "516360",
    "end": "523860"
  },
  {
    "text": "know just just from a technical point of view that's really cool but actually",
    "start": "523860",
    "end": "530300"
  },
  {
    "text": "from a life-saving point of view it turns out that patients with cancer for",
    "start": "530550",
    "end": "535950"
  },
  {
    "text": "example have mutations that occur during the course of their disease and if",
    "start": "535950",
    "end": "541740"
  },
  {
    "text": "you're able to sequence at high enough speed you can actually track the progression of the disease and",
    "start": "541740",
    "end": "547640"
  },
  {
    "text": "proactively target with your therapies to save those patients lives and so",
    "start": "547640",
    "end": "556110"
  },
  {
    "text": "we're working with healthcare organizations and research institutions to actually take this high speed gene",
    "start": "556110",
    "end": "563220"
  },
  {
    "text": "sequencing and put it into production in the in the clinical workflow I can't",
    "start": "563220",
    "end": "569850"
  },
  {
    "text": "imagine doing anything more exciting than that so that's that now finally in the center we have open source so Intel",
    "start": "569850",
    "end": "577829"
  },
  {
    "text": "believes strongly that open source is a critical part of our entire analytics",
    "start": "577829",
    "end": "584940"
  },
  {
    "text": "ecosystem so we are the top two contributors to Linux every week top",
    "start": "584940",
    "end": "591870"
  },
  {
    "text": "three contributors to spark we contribute directly to the Hadoop Apache trunk and also to Hadoop via Cloudera",
    "start": "591870",
    "end": "600140"
  },
  {
    "text": "which is a partner of ours so we're putting a lot of effort into enabling",
    "start": "600140",
    "end": "606720"
  },
  {
    "text": "the analytics ecosystem through our open source efforts and then just in the last",
    "start": "606720",
    "end": "615720"
  },
  {
    "start": "612000",
    "end": "699000"
  },
  {
    "text": "week actually just last week we announced at in New York City at strada",
    "start": "615720",
    "end": "620910"
  },
  {
    "text": "Hadoop the trusted analytics platform or tap which is an open source project that",
    "start": "620910",
    "end": "629459"
  },
  {
    "text": "takes a foundation that can either run in AWS or can run on Prem it integrates",
    "start": "629459",
    "end": "640140"
  },
  {
    "text": "data science and data science tools with app development and deployment so it's",
    "start": "640140",
    "end": "646860"
  },
  {
    "text": "collaborative because it allows data scientists to build models that can then",
    "start": "646860",
    "end": "652769"
  },
  {
    "text": "be exposed as services for app developers to pick up and deploy it is",
    "start": "652769",
    "end": "658709"
  },
  {
    "text": "flexible because it is actually built to",
    "start": "658709",
    "end": "663720"
  },
  {
    "text": "take in libraries and tool sets that data scientists use already bring them into",
    "start": "663720",
    "end": "670199"
  },
  {
    "text": "the platform and be able to leverage this this collaborative capability and it builds a community not only because",
    "start": "670199",
    "end": "678749"
  },
  {
    "text": "it's open source so that developers can pick up the ball and run with it in ways",
    "start": "678749",
    "end": "684809"
  },
  {
    "text": "that we didn't even imagine but because it creates a space where data scientists can work together they can work with app",
    "start": "684809",
    "end": "691709"
  },
  {
    "text": "developers and so we get an exponential growth of our ability to actually create",
    "start": "691709",
    "end": "697679"
  },
  {
    "text": "with data so it's very exciting step",
    "start": "697679",
    "end": "703259"
  },
  {
    "text": "forward in open source and you can go buy the intel booth and talk to people about that there's a display showing",
    "start": "703259",
    "end": "709589"
  },
  {
    "text": "some of the applications that have been built I think it's very exciting to focus more on AWS and what dergah is",
    "start": "709589",
    "end": "718379"
  },
  {
    "text": "going to talk about we have some specific areas that we've put made",
    "start": "718379",
    "end": "725759"
  },
  {
    "text": "contributions to the the Apache Hadoop source code so in particular we've done",
    "start": "725759",
    "end": "732899"
  },
  {
    "text": "a lot of feature development and optimization for HDFS itself around",
    "start": "732899",
    "end": "738509"
  },
  {
    "text": "remote procedure calls optimization around pig also quite a lot of just",
    "start": "738509",
    "end": "743939"
  },
  {
    "text": "debugging for those of you who have worked with pig we have put some new",
    "start": "743939",
    "end": "752879"
  },
  {
    "text": "features around hive on spark and hive in general so that this is important for",
    "start": "752879",
    "end": "760769"
  },
  {
    "text": "enterprise development Enterprise always needs to have a way to run sequel on top",
    "start": "760769",
    "end": "766649"
  },
  {
    "text": "of its big data stack and so there's there's a number of different ways to do that and Intel is working hard to make",
    "start": "766649",
    "end": "771989"
  },
  {
    "text": "sure that those are available and that they're robust and then the lower right encryption we've we've actually based on",
    "start": "771989",
    "end": "781919"
  },
  {
    "text": "chip level encrypt encryption through a ESN I encryption is 17 times faster than",
    "start": "781919",
    "end": "788459"
  },
  {
    "text": "it was before so for for encrypting your data in your Hadoop stack you can",
    "start": "788459",
    "end": "794399"
  },
  {
    "text": "actually now encrypt all of your data with only a few percent overhead which means that you don't have the risk",
    "start": "794399",
    "end": "799440"
  },
  {
    "text": "of G which you know which parts of my pipeline am I going to encrypt and oak",
    "start": "799440",
    "end": "804690"
  },
  {
    "text": "do we spill something on the floor and accidentally get a breach there's a lot of there's a lot of risks that get taken",
    "start": "804690",
    "end": "811140"
  },
  {
    "text": "out of the equation when you're when you're able to encrypt at high speed across your entire system with very",
    "start": "811140",
    "end": "817860"
  },
  {
    "text": "little overhead so that I think that's very exciting coming from health care where security is just absolutely",
    "start": "817860",
    "end": "824520"
  },
  {
    "text": "critical it's a it's a it's a real",
    "start": "824520",
    "end": "830040"
  },
  {
    "text": "benefit for the for the platform so I'm going to pause there for a second and",
    "start": "830040",
    "end": "835890"
  },
  {
    "text": "I'm actually curious about all of you I told you about my my history but what",
    "start": "835890",
    "end": "842160"
  },
  {
    "text": "about you raise your hand if you are an actual app developer or coder ok about",
    "start": "842160",
    "end": "848730"
  },
  {
    "text": "maybe a fifth how about I tier development managers ok another fifth",
    "start": "848730",
    "end": "857420"
  },
  {
    "text": "decision makers for analytics or IT ok",
    "start": "857420",
    "end": "863820"
  },
  {
    "text": "another sixth who didn't I get I know you're out there nobody's I got",
    "start": "863820",
    "end": "871440"
  },
  {
    "text": "everybody ok so we're all we're all app developers IT managers and decision-makers ok great so what I want",
    "start": "871440",
    "end": "882960"
  },
  {
    "text": "to talk about now is the use case that I developed when I was at a pic co about",
    "start": "882960",
    "end": "891840"
  },
  {
    "text": "the variety part of big data ok so as I said we took we're looking at the data",
    "start": "891840",
    "end": "900630"
  },
  {
    "text": "from the healthcare system doctors were incentivized to use electronic medical",
    "start": "900630",
    "end": "906630"
  },
  {
    "text": "records and the idea was we put all the data together and all of a sudden we",
    "start": "906630",
    "end": "912270"
  },
  {
    "text": "know what we need to know about patients if we know what we need to know about patients we can optimize the health",
    "start": "912270",
    "end": "917820"
  },
  {
    "text": "system right so anybody been to a doctor and the doctor didn't know something",
    "start": "917820",
    "end": "923280"
  },
  {
    "text": "they should have known about you a couple few people ok it certainly has",
    "start": "923280",
    "end": "930780"
  },
  {
    "text": "happened so so the the use case that I'll talk about is taking all of a patient's",
    "start": "930780",
    "end": "938829"
  },
  {
    "text": "clinical data they're structured data and in particularly their problem list",
    "start": "938829",
    "end": "945850"
  },
  {
    "text": "this is the list that your clinical decision support or your provider uses to to know what you have in your in your",
    "start": "945850",
    "end": "954069"
  },
  {
    "text": "history and what problems you currently have we're going to assemble that and we're just going to measure the false",
    "start": "954069",
    "end": "959379"
  },
  {
    "text": "negatives this was an actual problem that we solved and the idea was once",
    "start": "959379",
    "end": "966279"
  },
  {
    "text": "we've pulled all this data from all these different sources we're going to know the answer so here's the part you",
    "start": "966279",
    "end": "973449"
  },
  {
    "text": "know that I'm a technical guy so we need to talk about what does a patient look",
    "start": "973449",
    "end": "980049"
  },
  {
    "text": "like to a data scientist okay so this is this is what the patient looks like they've been d identified for HIPAA",
    "start": "980049",
    "end": "986350"
  },
  {
    "start": "981000",
    "end": "1029000"
  },
  {
    "text": "purposes and so they have a number of",
    "start": "986350",
    "end": "993309"
  },
  {
    "text": "different data data sources coming in right so we have a doctor up on the",
    "start": "993309",
    "end": "1000689"
  },
  {
    "text": "upper left entering information to the clinical record we have data coming from",
    "start": "1000689",
    "end": "1005790"
  },
  {
    "text": "a basis watch which is tracking sleep and heart rate and things like that we",
    "start": "1005790",
    "end": "1012720"
  },
  {
    "text": "have data coming from a different source it's a hospital where the patient was",
    "start": "1012720",
    "end": "1018689"
  },
  {
    "text": "seen on an emergency basis all these different sources of data need to be",
    "start": "1018689",
    "end": "1024089"
  },
  {
    "text": "brought together so the idea was you bring it together and you're going to get the full story so this is a problem",
    "start": "1024089",
    "end": "1032519"
  },
  {
    "start": "1029000",
    "end": "1064000"
  },
  {
    "text": "list from one source you can see they're just a number of conditions listed things like headache hypertension etc",
    "start": "1032519",
    "end": "1040730"
  },
  {
    "text": "we've got another source of data it's coated in a particular coding system",
    "start": "1040730",
    "end": "1045928"
  },
  {
    "text": "that's icd-9 so what the doctors use to put in your diagnosis to get paid and",
    "start": "1045929",
    "end": "1053000"
  },
  {
    "text": "well taken a third source imagine bringing these together and simply aggregating that was the that was the",
    "start": "1053000",
    "end": "1061169"
  },
  {
    "text": "initial premise of what we did and so the question is when we measured",
    "start": "1061169",
    "end": "1066570"
  },
  {
    "start": "1064000",
    "end": "1115000"
  },
  {
    "text": "our KPI that is the false negatives in the problem list what do we think we got so I'm going to ask right now based on",
    "start": "1066570",
    "end": "1073320"
  },
  {
    "text": "your experience with health care what percent of the key clinical information",
    "start": "1073320",
    "end": "1078540"
  },
  {
    "text": "that your doctor should always know about you was missing from that",
    "start": "1078540",
    "end": "1083580"
  },
  {
    "text": "aggregated view 0 to 25 percent anybody",
    "start": "1083580",
    "end": "1090210"
  },
  {
    "text": "why no takers it's a lot of a lot of doubters in the room how about twenty-five to fifty percent okay now",
    "start": "1090210",
    "end": "1098610"
  },
  {
    "text": "we're getting some some maybe a quarter fifty to seventy-five percent missing",
    "start": "1098610",
    "end": "1103880"
  },
  {
    "text": "okay and what about 75 and above okay those are the doctors in the room they",
    "start": "1103880",
    "end": "1111690"
  },
  {
    "text": "always they always tell me oh yeah none of that data is usable okay so the",
    "start": "1111690",
    "end": "1116850"
  },
  {
    "start": "1115000",
    "end": "1134000"
  },
  {
    "text": "answer turned out to be sixty-three percent in our study so two-thirds of what your doctor should know about you",
    "start": "1116850",
    "end": "1123330"
  },
  {
    "text": "is missing from their structured data",
    "start": "1123330",
    "end": "1128390"
  },
  {
    "text": "even after combining all the data across the entire system that was shocking to me so weird the data go I mean what's",
    "start": "1128390",
    "end": "1137130"
  },
  {
    "start": "1134000",
    "end": "1216000"
  },
  {
    "text": "going on here doctors don't I mean there are problems but they don't kill us every day so what what happens well this",
    "start": "1137130",
    "end": "1144510"
  },
  {
    "text": "is a this is an actual patients chart aggregated the top section is their",
    "start": "1144510",
    "end": "1150300"
  },
  {
    "text": "problem lists structured from all the different sources you can see there's a few things listed there the next one",
    "start": "1150300",
    "end": "1156930"
  },
  {
    "text": "down I've highlighted that's the doc this is the primary care doctors notes and they typed hypertension never put it",
    "start": "1156930",
    "end": "1163590"
  },
  {
    "text": "in the problem list it never showed up anywhere coated but they did note to themselves hypertension the smoking gun",
    "start": "1163590",
    "end": "1171540"
  },
  {
    "text": "is the last one that is actually a scanned document sent to the primary",
    "start": "1171540",
    "end": "1177270"
  },
  {
    "text": "care doctor from the cardiologist and if you read it ironically it says you are",
    "start": "1177270",
    "end": "1183240"
  },
  {
    "text": "aware of his acute myocardial infarction which is a heart attack four years ago",
    "start": "1183240",
    "end": "1188640"
  },
  {
    "text": "etc etc so the cardiologist thinks that the primary care doc knows",
    "start": "1188640",
    "end": "1194470"
  },
  {
    "text": "this is part of this patient's history they've had a heart attack but it's nowhere else in the chart it's not in the structured data it's nowhere so this",
    "start": "1194470",
    "end": "1201580"
  },
  {
    "text": "is where the big data analytics finally comes and and thank you for bearing with me to get to this point so the question",
    "start": "1201580",
    "end": "1208600"
  },
  {
    "text": "is how do we how do we do analytics on a system where the structured data is",
    "start": "1208600",
    "end": "1213610"
  },
  {
    "text": "missing sixty-three percent of what we need to know well the answer is we",
    "start": "1213610",
    "end": "1218799"
  },
  {
    "start": "1216000",
    "end": "1399000"
  },
  {
    "text": "analyze the text and in this case that's our variety of data in the Big Data",
    "start": "1218799",
    "end": "1224260"
  },
  {
    "text": "world we need a system that will allow us to analyze unstructured data this is",
    "start": "1224260",
    "end": "1231010"
  },
  {
    "text": "a problem that we're all going to face in our in our analytics careers or in our development careers because so much",
    "start": "1231010",
    "end": "1237039"
  },
  {
    "text": "of the interesting data in our world whether it's call center logs internet",
    "start": "1237039",
    "end": "1243429"
  },
  {
    "text": "blogging reviews clinical notes is unstructured that these these analytic",
    "start": "1243429",
    "end": "1250330"
  },
  {
    "text": "systems are incredibly important so in actually the doctors in the room always",
    "start": "1250330",
    "end": "1257530"
  },
  {
    "text": "giggle when they see the coffee stain because they they all know they did that okay so this is the stuff we're looking",
    "start": "1257530",
    "end": "1265179"
  },
  {
    "text": "for congestive heart failure hyperparathyroidism that's the stuff we",
    "start": "1265179",
    "end": "1270490"
  },
  {
    "text": "need to pull out of the text so what we did it a pic cos we built a data",
    "start": "1270490",
    "end": "1275940"
  },
  {
    "text": "platform an analytics pipeline on AWS and that's where the encryption came in",
    "start": "1275940",
    "end": "1281890"
  },
  {
    "text": "because to be HIPAA compliant requires us to make sure everything's encrypted all the time AWS was a perfectly",
    "start": "1281890",
    "end": "1289950"
  },
  {
    "text": "excellent platform for that for us we were able to build a scalable pipeline",
    "start": "1289950",
    "end": "1295419"
  },
  {
    "text": "that took the data for these patients all the text all the images OCR the",
    "start": "1295419",
    "end": "1302230"
  },
  {
    "text": "images there's a step in the pipeline that identifies whether the image should",
    "start": "1302230",
    "end": "1308890"
  },
  {
    "text": "be ocr'd there's another step in the pipeline that uses machine learning to determine is this a clinical document or",
    "start": "1308890",
    "end": "1315460"
  },
  {
    "text": "is this some sort of lab for more you know if it says diabetes on a lab form that and it's not checked you don't want",
    "start": "1315460",
    "end": "1321940"
  },
  {
    "text": "to include that in the patient's analysis right so there's there this pipeline",
    "start": "1321940",
    "end": "1327040"
  },
  {
    "text": "took these unstructured pieces of data along with the structured data put them",
    "start": "1327040",
    "end": "1332680"
  },
  {
    "text": "through a number of steps including OCR including text analytics and in",
    "start": "1332680",
    "end": "1340210"
  },
  {
    "text": "particular text mining not just not deep NLP but text mining it's an important",
    "start": "1340210",
    "end": "1345580"
  },
  {
    "text": "point to understand what's in the text so the reason i underscore text mining",
    "start": "1345580",
    "end": "1352870"
  },
  {
    "text": "as opposed to natural language processing is that if you look up natural language processing has anyone",
    "start": "1352870",
    "end": "1358720"
  },
  {
    "text": "done natural language here yeah whew so those of you who have done it will",
    "start": "1358720",
    "end": "1364420"
  },
  {
    "text": "recognize that there are some very deep and computationally intensive components",
    "start": "1364420",
    "end": "1372610"
  },
  {
    "text": "to natural language processing which may or may not add value to your to your process so what we did is we actually",
    "start": "1372610",
    "end": "1378760"
  },
  {
    "text": "started with text mining looking for the things that were most important and then we did a shallow parse to understand",
    "start": "1378760",
    "end": "1385330"
  },
  {
    "text": "whether when it says diabetes does is it proceeded with no evidence of those are",
    "start": "1385330",
    "end": "1393460"
  },
  {
    "text": "two different answers so but it doesn't require deep natural language processing so the point is it was a pipeline that",
    "start": "1393460",
    "end": "1401110"
  },
  {
    "start": "1399000",
    "end": "1460000"
  },
  {
    "text": "used a number of elemental simple",
    "start": "1401110",
    "end": "1406530"
  },
  {
    "text": "analyses but put together into a into a complete scalable big data environment",
    "start": "1406530",
    "end": "1413220"
  },
  {
    "text": "we were able to scale up and down the the number of nodes that we used for the",
    "start": "1413220",
    "end": "1419680"
  },
  {
    "text": "calculor computation to generate our summary of each patient so that we knew",
    "start": "1419680",
    "end": "1425920"
  },
  {
    "text": "what they had and and that was all done in AWS the the typical customer that we",
    "start": "1425920",
    "end": "1433300"
  },
  {
    "text": "had would have a hundred million documents to analyze this way and then billions of measurements labs structured",
    "start": "1433300",
    "end": "1442270"
  },
  {
    "text": "values some of which were reliable and some of which weren't so putting that all together and add creating something",
    "start": "1442270",
    "end": "1449880"
  },
  {
    "text": "computable was the output of that scalable pipeline and it ended up being",
    "start": "1449880",
    "end": "1458080"
  },
  {
    "text": "extremely successful so to summarize my experience before we",
    "start": "1458080",
    "end": "1464399"
  },
  {
    "start": "1460000",
    "end": "1516000"
  },
  {
    "text": "hear Durga's really amazing very specific best practices and and you know",
    "start": "1464399",
    "end": "1469919"
  },
  {
    "text": "you're really going to come away with some some clear examples of what to do",
    "start": "1469919",
    "end": "1475169"
  },
  {
    "text": "and what not to do and what the what the world of possibilities is on AWS start",
    "start": "1475169",
    "end": "1481769"
  },
  {
    "text": "with what you know so we started with the structured data we thought we knew that if you added it it would it would",
    "start": "1481769",
    "end": "1487499"
  },
  {
    "text": "work out but it didn't however we'd measured we'd measured the the false",
    "start": "1487499",
    "end": "1493859"
  },
  {
    "text": "negative number before and after we knew we needed to do more work and we just",
    "start": "1493859",
    "end": "1499320"
  },
  {
    "text": "added one more layer of complexity which was text mining in the the text",
    "start": "1499320",
    "end": "1504719"
  },
  {
    "text": "documents that actually got us ninety-five percent of the way their huge advance just starting with",
    "start": "1504719",
    "end": "1511799"
  },
  {
    "text": "something simple and and layering in one source of data second leverage the",
    "start": "1511799",
    "end": "1518429"
  },
  {
    "start": "1516000",
    "end": "1535000"
  },
  {
    "text": "existing technologies that you already have so again dirgo will talk a lot",
    "start": "1518429",
    "end": "1524219"
  },
  {
    "text": "about that but there are a lot of tools that you're probably already using that",
    "start": "1524219",
    "end": "1529529"
  },
  {
    "text": "if you assemble them into a scalable environment you can you can actually take advantage of along the same lines",
    "start": "1529529",
    "end": "1536940"
  },
  {
    "start": "1535000",
    "end": "1550000"
  },
  {
    "text": "use the simplest tools so we didn't go straight to natural language we started",
    "start": "1536940",
    "end": "1542099"
  },
  {
    "text": "with text mining because that gave us a lot of information without a lot of investment really important because then",
    "start": "1542099",
    "end": "1548969"
  },
  {
    "text": "we knew where to focus and then finally and I I feel bad even saying this but",
    "start": "1548969",
    "end": "1555960"
  },
  {
    "start": "1550000",
    "end": "1602000"
  },
  {
    "text": "measure your results how the KPI something that you measure beforehand in",
    "start": "1555960",
    "end": "1561179"
  },
  {
    "text": "this case it was the false negatives in the problem list you have to have a measure because at the end of the day",
    "start": "1561179",
    "end": "1567479"
  },
  {
    "text": "there was a gartner report recently that said twenty eight percent of executive respondents said that their Big Data",
    "start": "1567479",
    "end": "1573719"
  },
  {
    "text": "projects were not necessarily successful they didn't say they failed but they",
    "start": "1573719",
    "end": "1579119"
  },
  {
    "text": "said they were not necessarily successful why in most cases if you dig",
    "start": "1579119",
    "end": "1584159"
  },
  {
    "text": "in it's because they didn't measure something and clearly show that they had moved the needle so when you're",
    "start": "1584159",
    "end": "1589349"
  },
  {
    "text": "embarking on these projects think about one simple thing that you can you can measure and show that you've",
    "start": "1589349",
    "end": "1595109"
  },
  {
    "text": "moved the needle there's a lot of collateral value that you're going to get but you want to be able to show that",
    "start": "1595109",
    "end": "1600450"
  },
  {
    "text": "one piece of advancement so then at the",
    "start": "1600450",
    "end": "1605700"
  },
  {
    "start": "1602000",
    "end": "1638000"
  },
  {
    "text": "end of the day you can use big data analytics to reveal what's hidden inside",
    "start": "1605700",
    "end": "1611369"
  },
  {
    "text": "your data about your customers your products your entire business ecosystem",
    "start": "1611369",
    "end": "1616889"
  },
  {
    "text": "and eventually your opportunities so I i",
    "start": "1616889",
    "end": "1622469"
  },
  {
    "text": "invite you to now listen to Durga's part which dergah is a systems systems",
    "start": "1622469",
    "end": "1628379"
  },
  {
    "text": "architect from AOL and he's going to talk to you about velocity and volume in",
    "start": "1628379",
    "end": "1634339"
  },
  {
    "text": "some really compelling terms so thank you hey there guy",
    "start": "1634339",
    "end": "1643700"
  },
  {
    "start": "1638000",
    "end": "1677000"
  },
  {
    "text": "hello can you hear me thank you Bob for the introduction and probably cues for",
    "start": "1648630",
    "end": "1657370"
  },
  {
    "text": "my presentation I've been talking to Bob over the last week or so and there were",
    "start": "1657370",
    "end": "1662470"
  },
  {
    "text": "so many synergies in the way we were doing things and how we can apply what",
    "start": "1662470",
    "end": "1670120"
  },
  {
    "text": "we learned to any data set or any environment so just to tell you about",
    "start": "1670120",
    "end": "1679120"
  },
  {
    "start": "1677000",
    "end": "1848000"
  },
  {
    "text": "myself my name is Jorgen imani I am a system architect at AOL I I did I design",
    "start": "1679120",
    "end": "1685570"
  },
  {
    "text": "big data solutions for all of our traffic data and combining our traffic",
    "start": "1685570",
    "end": "1693280"
  },
  {
    "text": "and advertising and audience data I work with Hadoop for the last three or four",
    "start": "1693280",
    "end": "1699130"
  },
  {
    "text": "years and started working with AWS in the last one and a half year so at the",
    "start": "1699130",
    "end": "1706450"
  },
  {
    "text": "beginning of last year we had a challenge we were processing a lot of our data in our new cluster in our data",
    "start": "1706450",
    "end": "1713260"
  },
  {
    "text": "center and it was about to go off lease in the hardware so we had to make a decision what to do with the processes",
    "start": "1713260",
    "end": "1721030"
  },
  {
    "text": "and data and around the same time event for a training class for AWS and we",
    "start": "1721030",
    "end": "1726850"
  },
  {
    "text": "quickly realized that that is a natural solution for us that was a very",
    "start": "1726850",
    "end": "1732580"
  },
  {
    "text": "efficient solution for moving our application to AWS what we did was we",
    "start": "1732580",
    "end": "1741460"
  },
  {
    "text": "took the data that we were processing in our Hadoop cluster and uploaded it to",
    "start": "1741460",
    "end": "1750690"
  },
  {
    "text": "AWS s3 and we launched an EMR cluster and processed the data using the EMR",
    "start": "1750690",
    "end": "1759670"
  },
  {
    "text": "cluster and save the output back on s3 the we were using high for an pink",
    "start": "1759670",
    "end": "1767170"
  },
  {
    "text": "foremost for data processing and so it was relatively easy for us to take the",
    "start": "1767170",
    "end": "1772330"
  },
  {
    "text": "code that we had in house and put it on AWS because it's the same apache hadoop on Apache",
    "start": "1772330",
    "end": "1781630"
  },
  {
    "text": "hive versions that we were using we could use on AWS EMR and the data was",
    "start": "1781630",
    "end": "1787660"
  },
  {
    "text": "saved on s3 and read and copied to the EMR HDFS and we applied the same hive",
    "start": "1787660",
    "end": "1797080"
  },
  {
    "text": "and pig transformations that we were doing on the in-house Hadoop cluster and the output was saved back on s3 now as",
    "start": "1797080",
    "end": "1804940"
  },
  {
    "text": "you can see we were not we are still not completely in on AWS we are in a hybrid",
    "start": "1804940",
    "end": "1811300"
  },
  {
    "text": "mode at this point we have a big database in house that is used by oliver",
    "start": "1811300",
    "end": "1817960"
  },
  {
    "text": "analysts and a reporting infrastructure to deliver a lot of reports to the",
    "start": "1817960",
    "end": "1824170"
  },
  {
    "text": "executives so in order to minimize the impact to our end users we decided to",
    "start": "1824170",
    "end": "1831850"
  },
  {
    "text": "first move all the data processing into the cloud and then tackle the other challenge later so this is perfectly",
    "start": "1831850",
    "end": "1838930"
  },
  {
    "text": "working for us we extract the data from AWS and load it into our in-house",
    "start": "1838930",
    "end": "1845110"
  },
  {
    "text": "database in data center database so as",
    "start": "1845110",
    "end": "1850300"
  },
  {
    "text": "Bob mentioned the 3 v's of big data are volume variety and velocity we have",
    "start": "1850300",
    "end": "1860010"
  },
  {
    "text": "definitely have multiple terabytes of data that we are processing we get data",
    "start": "1860010",
    "end": "1865030"
  },
  {
    "text": "in different formats we get data in plain CSV files we get a bro avril",
    "start": "1865030",
    "end": "1873880"
  },
  {
    "text": "formatted data we have json formatted data all of that we are able to process",
    "start": "1873880",
    "end": "1879100"
  },
  {
    "text": "in EMR in AWS and the velocity now I",
    "start": "1879100",
    "end": "1885070"
  },
  {
    "text": "know for different people we have real-time and other near real-time data",
    "start": "1885070",
    "end": "1891040"
  },
  {
    "text": "sources for our use case we predominantly have early end batch files that are arriving during the day at",
    "start": "1891040",
    "end": "1898570"
  },
  {
    "text": "different times during the day so one of",
    "start": "1898570",
    "end": "1903580"
  },
  {
    "text": "the one of the challenges that we had with the physical Hadoop cluster was we",
    "start": "1903580",
    "end": "1909760"
  },
  {
    "text": "had the same Hadoop cluster which we were running the early jobs the daily jobs the weekly jobs and the",
    "start": "1909760",
    "end": "1916179"
  },
  {
    "text": "monthly jobs now all of them had different needs different complexity different data volume that it was",
    "start": "1916179",
    "end": "1924070"
  },
  {
    "text": "processing but all of them was being processed on the same physical Hadoop cluster however with cloud we had the",
    "start": "1924070",
    "end": "1933519"
  },
  {
    "text": "opportunity to essentially have a different Hadoop cluster for different",
    "start": "1933519",
    "end": "1938999"
  },
  {
    "text": "purpose so we have a Hadoop cluster with",
    "start": "1938999",
    "end": "1944559"
  },
  {
    "text": "225 nodes for processing or early data and then we have 100 250 node Hadoop",
    "start": "1944559",
    "end": "1950950"
  },
  {
    "text": "cluster that we launched for our daily data so how did we develop our",
    "start": "1950950",
    "end": "1960869"
  },
  {
    "text": "architecture in AWS we we certainly",
    "start": "1960869",
    "end": "1965999"
  },
  {
    "text": "didn't want to create one Hadoop cluster and have all of our data and processes",
    "start": "1965999",
    "end": "1974649"
  },
  {
    "text": "run in that particular cluster B we definitely we tried that approach we",
    "start": "1974649",
    "end": "1981519"
  },
  {
    "text": "thought about it and then we found out that we may be in the same situation where we may not be able to scale it",
    "start": "1981519",
    "end": "1988359"
  },
  {
    "text": "fast enough to run different applications or different aggregations so we took a different approach we use",
    "start": "1988359",
    "end": "1997029"
  },
  {
    "text": "s3 as our persistent storage engine and we put all the data on s 3 s 3 access or",
    "start": "1997029",
    "end": "2004859"
  },
  {
    "text": "input and as our output for all of our data these pin up as soon as we get the",
    "start": "2004859",
    "end": "2013350"
  },
  {
    "text": "data that is ready to be processed we spin up an EMR cluster and read the data",
    "start": "2013350",
    "end": "2019950"
  },
  {
    "text": "copy the data over to EMR HDFS do all the transformations using hive or pig",
    "start": "2019950",
    "end": "2025830"
  },
  {
    "text": "and then write the output back to s3",
    "start": "2025830",
    "end": "2031070"
  },
  {
    "text": "similarly just to give you the nature of the scale that we do we get 22 different",
    "start": "2031070",
    "end": "2040289"
  },
  {
    "text": "data sets at an early granularity in early batches so essentially we get 22 x",
    "start": "2040289",
    "end": "2047010"
  },
  {
    "text": "24 files every day and each and every",
    "start": "2047010",
    "end": "2052679"
  },
  {
    "text": "file is processed in its own EMR cluster so at any point in the day there are EMR",
    "start": "2052679",
    "end": "2060870"
  },
  {
    "text": "clusters that are spinning up reading the data the corresponding input data doing the transformations and writing",
    "start": "2060870",
    "end": "2067560"
  },
  {
    "text": "the output back to s3 and then at the end of the day going away there is no",
    "start": "2067560",
    "end": "2074700"
  },
  {
    "text": "single persistent Hadoop cluster that is up and running all the time this allowed",
    "start": "2074700",
    "end": "2081120"
  },
  {
    "text": "us a great flexibility as I will explain in a one of the use cases in the next",
    "start": "2081120",
    "end": "2086190"
  },
  {
    "text": "few slides the only two things that are always persistent are the s3 the data",
    "start": "2086190",
    "end": "2094200"
  },
  {
    "text": "that we store on s3 and we have a clustered watchdog process which",
    "start": "2094200",
    "end": "2099630"
  },
  {
    "text": "monitors all of our EMR Hadoop clusters and if there is a an error then it",
    "start": "2099630",
    "end": "2104640"
  },
  {
    "text": "alerts our operations team to look into the error and restart the process or",
    "start": "2104640",
    "end": "2111380"
  },
  {
    "text": "alert to the development team for any data anomalies that may have rised so",
    "start": "2111380",
    "end": "2118650"
  },
  {
    "text": "one of the things that made it really easy as I have mentioned before to move to AWS to cloud is we were using open",
    "start": "2118650",
    "end": "2127680"
  },
  {
    "text": "source technologies we were using hive and pig as our transformation coding",
    "start": "2127680",
    "end": "2133530"
  },
  {
    "text": "engines which essentially we took the same pig script the same hive script",
    "start": "2133530",
    "end": "2139170"
  },
  {
    "text": "that we had and put it on AWS we didn't have to change a thing everything ran as",
    "start": "2139170",
    "end": "2145020"
  },
  {
    "text": "is and that actually avoided a lot of recording effort and we use s3 and EMR",
    "start": "2145020",
    "end": "2154650"
  },
  {
    "text": "and ec2 instances behind the EMR service",
    "start": "2154650",
    "end": "2160100"
  },
  {
    "text": "the other thing that I we found useful was our data was in standard formats we",
    "start": "2160100",
    "end": "2168780"
  },
  {
    "text": "had delimited data which was pretty common and we have a row and JSON data",
    "start": "2168780",
    "end": "2174840"
  },
  {
    "text": "and we could take the data as is and upload it to s3 and we could use the",
    "start": "2174840",
    "end": "2180900"
  },
  {
    "text": "existing hive or pig scripts and process the data that was stored on s3 so",
    "start": "2180900",
    "end": "2188390"
  },
  {
    "text": "keeping the data in open source data format and using open source programming",
    "start": "2188390",
    "end": "2196140"
  },
  {
    "text": "tools to manipulate the data was definitely one big thing that helped us",
    "start": "2196140",
    "end": "2201510"
  },
  {
    "text": "move quickly to the cloud with minimal tree coding now everybody might be",
    "start": "2201510",
    "end": "2212130"
  },
  {
    "text": "asking you spin up five hundred or three hundred Hadoop clusters every day wouldn't that cost a lot of money well",
    "start": "2212130",
    "end": "2222800"
  },
  {
    "text": "one of the things we use is spot instances as everybody knows amazon",
    "start": "2223850",
    "end": "2232410"
  },
  {
    "text": "offers their excess capacity as spot instances for one-tenth the price of a",
    "start": "2232410",
    "end": "2238410"
  },
  {
    "text": "regular ec2 instance so we because we were using all these transient EMR",
    "start": "2238410",
    "end": "2245640"
  },
  {
    "text": "clusters we could spin up an EMR cluster with all spot instances to the process",
    "start": "2245640",
    "end": "2252180"
  },
  {
    "text": "shut it down so you don't have to worry about my spot instances going away my",
    "start": "2252180",
    "end": "2258240"
  },
  {
    "text": "Hadoop cluster shutting down because we were able to spin up a cluster do all of",
    "start": "2258240",
    "end": "2264870"
  },
  {
    "text": "our data processing and shut it down within 1 R and that's where we see we",
    "start": "2264870",
    "end": "2271770"
  },
  {
    "text": "spot the instances we grab them and we just don't leave them so easily we",
    "start": "2271770",
    "end": "2279570"
  },
  {
    "text": "actually squeeze the full worth of it we just give it enough instances to finish",
    "start": "2279570",
    "end": "2286410"
  },
  {
    "text": "the job in 59 minutes as everybody knows EMR ec2 charges are run by dr so there",
    "start": "2286410",
    "end": "2295860"
  },
  {
    "text": "is no incentive for us to actually finish the job in in less time if we are",
    "start": "2295860",
    "end": "2301230"
  },
  {
    "text": "finishing the job in less time that means we are actually giving more than what we should be so we actually take",
    "start": "2301230",
    "end": "2306930"
  },
  {
    "text": "away a couple of instances to make sure that it does run for 59 minutes unless there is a real reason",
    "start": "2306930",
    "end": "2314160"
  },
  {
    "text": "for you to finish the data processing sooner so the key feature one of the key",
    "start": "2314160",
    "end": "2323130"
  },
  {
    "text": "features as you see here that helped us scale and leverage all these cheap spot",
    "start": "2323130",
    "end": "2330960"
  },
  {
    "text": "instances and separate is by separating our compute and storage now we were able",
    "start": "2330960",
    "end": "2339329"
  },
  {
    "text": "to store the data on s3 and we spin up different instances different sized",
    "start": "2339329",
    "end": "2344369"
  },
  {
    "text": "clusters for doing different things so less I said we have hourly clusters that process one hour's worth of data and",
    "start": "2344369",
    "end": "2351450"
  },
  {
    "text": "finish that that only need two to five nodes where as a daily job that more",
    "start": "2351450",
    "end": "2357089"
  },
  {
    "text": "process is more data and runs more complex aggregations needs 100 250 nodes",
    "start": "2357089",
    "end": "2364369"
  },
  {
    "text": "transient clusters so there is no single permanent cluster and there are",
    "start": "2365690",
    "end": "2371640"
  },
  {
    "text": "different sized clusters for different datasets separation of duties so we",
    "start": "2371640",
    "end": "2377519"
  },
  {
    "text": "don't have a single entity doing all the processing or doing all the",
    "start": "2377519",
    "end": "2383359"
  },
  {
    "text": "orchestration we have independent jobs for processing the data for extracting",
    "start": "2383359",
    "end": "2388980"
  },
  {
    "text": "the data for monitoring and for loading the data as you can see we process the",
    "start": "2388980",
    "end": "2397289"
  },
  {
    "text": "smallest chunk of data possible in a single EMR cluster if you have data that there is arriving every 15 minutes or",
    "start": "2397289",
    "end": "2403890"
  },
  {
    "text": "every are separating it out and processing it in a parallel in a different environment will help you",
    "start": "2403890",
    "end": "2413990"
  },
  {
    "text": "scale massively so if there was a hiccup and you don't get data for a few hours",
    "start": "2413990",
    "end": "2420509"
  },
  {
    "text": "but you get it all at the end of the day we could still process all the data within one hour because we could spin up",
    "start": "2420509",
    "end": "2426839"
  },
  {
    "text": "multiple EMR clusters in dialogue so one",
    "start": "2426839",
    "end": "2434069"
  },
  {
    "text": "of the things that we had done is we",
    "start": "2434069",
    "end": "2439829"
  },
  {
    "text": "have a unique use case of restating from time to time data that has been for the past six months",
    "start": "2439829",
    "end": "2448039"
  },
  {
    "text": "because the business rules have changed because some of the patterns in the data have changed or they have identified",
    "start": "2448039",
    "end": "2456029"
  },
  {
    "text": "patterns in the data and some of the aggregations need to be rerun on the",
    "start": "2456029",
    "end": "2462119"
  },
  {
    "text": "existing data so we had to do that from time to time and this architecture",
    "start": "2462119",
    "end": "2469799"
  },
  {
    "text": "helped us because we have six months worth of data that needs to be reprocessed and three different",
    "start": "2469799",
    "end": "2478019"
  },
  {
    "text": "countries for which we had to process the data so we had 180 days essentially",
    "start": "2478019",
    "end": "2484710"
  },
  {
    "text": "180 days worth of data to be reprocessed because of the way we have separated out",
    "start": "2484710",
    "end": "2490950"
  },
  {
    "text": "our data processing into different EMR clusters for every are and every day",
    "start": "2490950",
    "end": "2496309"
  },
  {
    "text": "instead of doing all this reprocessing in one EMR cluster we could launch one",
    "start": "2496309",
    "end": "2504539"
  },
  {
    "text": "EMR cluster for every day in different availability zones we instead of doing",
    "start": "2504539",
    "end": "2513150"
  },
  {
    "text": "it all in single availability zone where they mayor may or may not be enough capacity and with spot instances you are",
    "start": "2513150",
    "end": "2520259"
  },
  {
    "text": "actually bidding for excess capacity and the more you bid in a single",
    "start": "2520259",
    "end": "2526710"
  },
  {
    "text": "availability zone the higher the price is going to jump up so you will be competing against yourself so we have",
    "start": "2526710",
    "end": "2533249"
  },
  {
    "text": "designed the process to limit itself artificially by as soon as there are",
    "start": "2533249",
    "end": "2541319"
  },
  {
    "text": "3,000 instances running in one availability zone under one account under our account we skip that",
    "start": "2541319",
    "end": "2547559"
  },
  {
    "text": "availability zone and move on to a different availability zone and launch EMR clusters in that availabilities on",
    "start": "2547559",
    "end": "2554369"
  },
  {
    "text": "and be at one point we had 550 EMR",
    "start": "2554369",
    "end": "2559529"
  },
  {
    "text": "clusters running in 10 different availability zones all of them finished",
    "start": "2559529",
    "end": "2564989"
  },
  {
    "text": "the data processing within a few hours so we were able to restate 180 days",
    "start": "2564989",
    "end": "2571140"
  },
  {
    "text": "worth of data within a few hours",
    "start": "2571140",
    "end": "2575748"
  },
  {
    "text": "and since we use all spot instances the cost of it was as as low as it could be",
    "start": "2576280",
    "end": "2583790"
  },
  {
    "text": "and and from the numbers that we have seen we we were able to save about",
    "start": "2583790",
    "end": "2590390"
  },
  {
    "text": "one-fourth of the cost compared to a physical Hadoop cluster and a ripple yes",
    "start": "2590390",
    "end": "2598600"
  },
  {
    "text": "so the facts that we have the amount of data that we have we have approximately",
    "start": "2600460",
    "end": "2606230"
  },
  {
    "text": "150 terabytes of extremely compressed data because you are paying per gigabyte",
    "start": "2606230",
    "end": "2611450"
  },
  {
    "text": "he does pay it does it does help you to compress the data as well as possible as",
    "start": "2611450",
    "end": "2617150"
  },
  {
    "text": "much as possible and we get two to three terabytes of raw data every day and we",
    "start": "2617150",
    "end": "2623180"
  },
  {
    "text": "launch 350 EMR clusters every day processing all that data and we have",
    "start": "2623180",
    "end": "2629480"
  },
  {
    "text": "approximately 13 to 24 months of data that is retained on s3 this is the",
    "start": "2629480",
    "end": "2636890"
  },
  {
    "text": "Restatement use case that I was talking about where we had 150 terabytes of raw data that was to be reprocessed and we",
    "start": "2636890",
    "end": "2645260"
  },
  {
    "text": "launched 550 EMR clusters in 10 availability zones with 24,000 active",
    "start": "2645260",
    "end": "2652340"
  },
  {
    "text": "ec2 instances running at one point so",
    "start": "2652340",
    "end": "2659150"
  },
  {
    "text": "this is the AWS cause breakout as you can see in a physical environment or",
    "start": "2659150",
    "end": "2667430"
  },
  {
    "text": "with the physical Hadoop cluster it's not easy to get this breakdown you don't know how much of the cost of the Hadoop",
    "start": "2667430",
    "end": "2673880"
  },
  {
    "text": "cluster is for storage and how much is it for compute whereas with AWS we could",
    "start": "2673880",
    "end": "2679130"
  },
  {
    "text": "clearly see the cost of compute is definitely much more than the cost of storage of the data it does also give us",
    "start": "2679130",
    "end": "2688100"
  },
  {
    "text": "enough insight into what is the data set or the aggregation that is costing the",
    "start": "2688100",
    "end": "2694670"
  },
  {
    "text": "most amount of money and you can clearly identify whether or not that is important and relevant and make a",
    "start": "2694670",
    "end": "2701870"
  },
  {
    "text": "decision and it helps the and gives you that visibility and helps you make the",
    "start": "2701870",
    "end": "2708380"
  },
  {
    "text": "decision so some of the best practices and suggestions before I go into that I just",
    "start": "2708380",
    "end": "2714619"
  },
  {
    "text": "wanted to tie it back to how Bob the Bob's use case of clinical data and",
    "start": "2714619",
    "end": "2722030"
  },
  {
    "text": "patient data you might be wondering like okay I am talking about clickstream data",
    "start": "2722030",
    "end": "2728270"
  },
  {
    "text": "and Bob talked about patient data healthcare data if we have to design a",
    "start": "2728270",
    "end": "2734599"
  },
  {
    "text": "solution for Bob's use case think about having rather than having one cluster",
    "start": "2734599",
    "end": "2743450"
  },
  {
    "text": "EMR cluster and that would process all the data that is coming through you",
    "start": "2743450",
    "end": "2749089"
  },
  {
    "text": "could have different EMR clusters processing different data you could have one EMR cluster that is processing the",
    "start": "2749089",
    "end": "2755539"
  },
  {
    "text": "structured data that is coming from multiple sources and another EMR cluster that is processing the text documents",
    "start": "2755539",
    "end": "2764180"
  },
  {
    "text": "that are coming through doing OCR and taking that data and putting into an unstructured JSON or Avro format and at",
    "start": "2764180",
    "end": "2772940"
  },
  {
    "text": "the end of the day if there is an analyst that has to analyze all this data combined then he could he or she",
    "start": "2772940",
    "end": "2778849"
  },
  {
    "text": "could run her their own EMR cluster maybe a 500 no DMR cluster maybe a",
    "start": "2778849",
    "end": "2785150"
  },
  {
    "text": "thousand odm are clustered they could spin up their own Hadoop cluster and run machine learning algorithms or natural",
    "start": "2785150",
    "end": "2793460"
  },
  {
    "text": "language processing algorithms to process all this data and combined and gain some insights so that's that's",
    "start": "2793460",
    "end": "2799700"
  },
  {
    "text": "where my implementation can be used for",
    "start": "2799700",
    "end": "2805130"
  },
  {
    "text": "any use case any any industry by separating out the different data",
    "start": "2805130",
    "end": "2811069"
  },
  {
    "text": "streams and using different technologies for different data streams for the text documents you would need some kind of",
    "start": "2811069",
    "end": "2817880"
  },
  {
    "text": "OCR software so you may or may not you may need a different set of software",
    "start": "2817880",
    "end": "2824029"
  },
  {
    "text": "that is installed on the EMR clusters whereas for national language processing",
    "start": "2824029",
    "end": "2829220"
  },
  {
    "text": "you would need a different set of software like that would be installed on in EMR cluster and depending upon the",
    "start": "2829220",
    "end": "2835490"
  },
  {
    "text": "complexity of the analysis you could scale the cluster up and down by separating out these different",
    "start": "2835490",
    "end": "2843440"
  },
  {
    "text": "processes it can be very flexible and and you could still achieve your results",
    "start": "2843440",
    "end": "2850380"
  },
  {
    "text": "without any capacity constraints so coming back to the best practices and",
    "start": "2850380",
    "end": "2856290"
  },
  {
    "text": "suggestions one of the things that we found very useful was tagging tagged all",
    "start": "2856290",
    "end": "2865020"
  },
  {
    "text": "your resources with appropriate tags be tagged our EMR clusters are easy to",
    "start": "2865020",
    "end": "2871290"
  },
  {
    "text": "instances are s3 buckets with environment tags that tell us whether",
    "start": "2871290",
    "end": "2876480"
  },
  {
    "text": "its development QA or production and",
    "start": "2876480",
    "end": "2881540"
  },
  {
    "text": "this is useful not only for identifying whether a particular cluster is a",
    "start": "2881630",
    "end": "2886710"
  },
  {
    "text": "production development or QA but at the end of the month when you see the bill and you want to know why is my bill so",
    "start": "2886710",
    "end": "2893250"
  },
  {
    "text": "high what's costing me so much or what's the biggest chunk of the cost coming",
    "start": "2893250",
    "end": "2899700"
  },
  {
    "text": "from you get the breakdown in the bill by tags so I could clearly see how much",
    "start": "2899700",
    "end": "2906150"
  },
  {
    "text": "my development environment cost last month how much my production environment cost last month and how much my QA",
    "start": "2906150",
    "end": "2913710"
  },
  {
    "text": "environment cost last month and we could actually give you from a breakdown of",
    "start": "2913710",
    "end": "2921380"
  },
  {
    "text": "costs for different environments down to the cost of a single EMR cluster I could",
    "start": "2921380",
    "end": "2927000"
  },
  {
    "text": "actually tell you how many cents my EMR cluster that process this early data cost because of the tags that we have",
    "start": "2927000",
    "end": "2934200"
  },
  {
    "text": "applied so use tags as much as we can the other thing is a security I am sure",
    "start": "2934200",
    "end": "2941400"
  },
  {
    "text": "security is a big thing on everybody's mind the some of the things that we have done to enable or improve security for",
    "start": "2941400",
    "end": "2948450"
  },
  {
    "text": "all of our data processing is we have defined I am users for all the users in",
    "start": "2948450",
    "end": "2956490"
  },
  {
    "text": "our account and we have assigned specific roles for them so we have defined roles that would restrict",
    "start": "2956490",
    "end": "2962580"
  },
  {
    "text": "permissions so we have a production operations role that has permission to",
    "start": "2962580",
    "end": "2968040"
  },
  {
    "text": "read write data to the production data buckets we have development and QA roles",
    "start": "2968040",
    "end": "2973530"
  },
  {
    "text": "that have permissions only to read dev and arrow buckets they cannot write anything",
    "start": "2973530",
    "end": "2979440"
  },
  {
    "text": "to the production buckets that way a developer cannot accidentally go and",
    "start": "2979440",
    "end": "2984660"
  },
  {
    "text": "erase any production data so we have implemented all these restrictions roles",
    "start": "2984660",
    "end": "2990510"
  },
  {
    "text": "and policies through AWS Identity and Access Management and the other thing",
    "start": "2990510",
    "end": "2996570"
  },
  {
    "text": "that we have done is we actually disabled access to all of our production",
    "start": "2996570",
    "end": "3002540"
  },
  {
    "text": "EMR clusters we do not enable any SSH access to our production EMR clusters",
    "start": "3002540",
    "end": "3010210"
  },
  {
    "text": "because when a step fails or a particular job fails in our EMR cluster",
    "start": "3010210",
    "end": "3017140"
  },
  {
    "text": "it shuts down throws all the logs on s3 and the operations team doesn't have",
    "start": "3017140",
    "end": "3024500"
  },
  {
    "text": "time to actually log in and see what's happening they get that information from the logs so there was no reason to an to",
    "start": "3024500",
    "end": "3031160"
  },
  {
    "text": "enable access to the Hadoop cluster in production at all so we disable all the",
    "start": "3031160",
    "end": "3037730"
  },
  {
    "text": "ssh access to all of our EMR clusters so that's the keyless EMR clusters that we are talking about and other things are",
    "start": "3037730",
    "end": "3046760"
  },
  {
    "text": "using spot instances and infrastructure as code there is no all the all the code",
    "start": "3046760",
    "end": "3053270"
  },
  {
    "text": "that actually launches an EMR cluster all the configuration everything is in",
    "start": "3053270",
    "end": "3058550"
  },
  {
    "text": "the code that we have so we could take that script and run it and the script",
    "start": "3058550",
    "end": "3063950"
  },
  {
    "text": "itself is in version control so we don't have to worry about losing the",
    "start": "3063950",
    "end": "3069740"
  },
  {
    "text": "configuration information for an EMR cluster some more best practices are",
    "start": "3069740",
    "end": "3079010"
  },
  {
    "text": "listed here I don't want to go through all of them but some of the things that I think I would talk about for our use",
    "start": "3079010",
    "end": "3086090"
  },
  {
    "text": "case was having a good naming convention you know in s3 have have a hierarchical",
    "start": "3086090",
    "end": "3095150"
  },
  {
    "text": "data structure of having data set name and and then separating out or",
    "start": "3095150",
    "end": "3105320"
  },
  {
    "text": "partitioning the data by day year and month within the data set so that people",
    "start": "3105320",
    "end": "3110360"
  },
  {
    "text": "can see what data set it is and how many years worth of data or how many months",
    "start": "3110360",
    "end": "3116360"
  },
  {
    "text": "worth of data do we have for that particular data set so having having a good hierarchy and a good naming",
    "start": "3116360",
    "end": "3122300"
  },
  {
    "text": "convention in s3 and and tags are definitely going to help you so the next",
    "start": "3122300",
    "end": "3131330"
  },
  {
    "start": "3129000",
    "end": "3297000"
  },
  {
    "text": "steps some of the next things that we are going to we would like to try we are going to try in AWS are the database in",
    "start": "3131330",
    "end": "3138680"
  },
  {
    "text": "the cloud there are several things that we want to try here including redshift",
    "start": "3138680",
    "end": "3144590"
  },
  {
    "text": "RDS and maybe custom databases or proprietary databases in ec to buy that",
    "start": "3144590",
    "end": "3152480"
  },
  {
    "text": "would allow us to scale up and down quickly event-driven design rather than",
    "start": "3152480",
    "end": "3158810"
  },
  {
    "text": "having scheduled jobs that would kick off at a given time and do a certain",
    "start": "3158810",
    "end": "3164740"
  },
  {
    "text": "task we would like to switch to kicking of code based on events as soon as a",
    "start": "3164740",
    "end": "3171200"
  },
  {
    "text": "file and on s3 we would like to kick off an EMR cluster that processes the file and does all the processing and puts the",
    "start": "3171200",
    "end": "3179300"
  },
  {
    "text": "data back on s3 rather than having this early jog or daily job that would wait",
    "start": "3179300",
    "end": "3185020"
  },
  {
    "text": "for a particular or day we would like to use a M lambda and to some extent we",
    "start": "3185020",
    "end": "3192590"
  },
  {
    "text": "already have used it and it was pretty pretty successful where we as soon as",
    "start": "3192590",
    "end": "3197810"
  },
  {
    "text": "the file and any a lambda even kicks off an EMR cluster and does all the processing puts the data back on s3 and",
    "start": "3197810",
    "end": "3204910"
  },
  {
    "text": "shuts down the EMR cluster so we have already successfully implemented that partially in the past and the several",
    "start": "3204910",
    "end": "3213110"
  },
  {
    "text": "other things that we would like to do try our automatic code deployments test automation although it's a little tricky",
    "start": "3213110",
    "end": "3222320"
  },
  {
    "text": "to have test automation for data we are still trying to figure out how to do that but that's one of the things that",
    "start": "3222320",
    "end": "3228140"
  },
  {
    "text": "we would like to accomplish in the next few months and some to some extent the",
    "start": "3228140",
    "end": "3234830"
  },
  {
    "text": "data analytics portion we're enabling our analyst to launch an EMR cluster",
    "start": "3234830",
    "end": "3243330"
  },
  {
    "text": "choose the tool of their choice like spark or Impala or presto we are already",
    "start": "3243330",
    "end": "3250020"
  },
  {
    "text": "able to achieve that we have trained our analysts how to use AWS launch an EMR",
    "start": "3250020",
    "end": "3257070"
  },
  {
    "text": "cluster read the data that is stored on s3 using hive pig and an Impala and",
    "start": "3257070",
    "end": "3265770"
  },
  {
    "text": "press toward the next things that are coming in so we were able to do it already and like I said we don't have to",
    "start": "3265770",
    "end": "3272520"
  },
  {
    "text": "have a single Hadoop cluster that does all these things the analysts can have their own Hadoop cluster you know if",
    "start": "3272520",
    "end": "3278580"
  },
  {
    "text": "they want 100 node Hadoop cluster or a 200 note how to cluster they could launch their own run their analysis and",
    "start": "3278580",
    "end": "3284010"
  },
  {
    "text": "shut it down when they're done and they can have their code all the all the code",
    "start": "3284010",
    "end": "3290370"
  },
  {
    "text": "they ran saved up so that they could do that again if they they have to do it",
    "start": "3290370",
    "end": "3295710"
  },
  {
    "text": "again so",
    "start": "3295710",
    "end": "3298250"
  }
]