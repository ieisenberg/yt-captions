[
  {
    "text": "- All right. Hey, folks. Emily Webber here from AWS. So today we're gonna learn how to use",
    "start": "1500",
    "end": "7110"
  },
  {
    "text": "pre-trained foundation models. So first, we learned what\nfoundation models are,",
    "start": "7110",
    "end": "12150"
  },
  {
    "text": "then we learned how to\npick a foundation model, and today we're gonna learn about how to use pre-trained\nfoundation models,",
    "start": "12150",
    "end": "18510"
  },
  {
    "text": "notably through prompt engineering and then for fine-tuning. So let's get started.",
    "start": "18510",
    "end": "22679"
  },
  {
    "text": "And so today's activities, so again we're gonna\nstart with all the shots,",
    "start": "25050",
    "end": "30449"
  },
  {
    "text": "so with zero-shot, single-shot,\nfew-shot prompting, we're gonna really understand those.",
    "start": "30450",
    "end": "36030"
  },
  {
    "text": "We're gonna dive into\nthem, explain the topics, and get used to them 'cause\nthey're really important ones.",
    "start": "36030",
    "end": "42180"
  },
  {
    "text": "We're gonna learn about\ninstruction fine-tuning, what that is, why that\nmatters, how to do it,",
    "start": "42180",
    "end": "48630"
  },
  {
    "text": "or mostly just how to make sure\nthat your model has done it. And then some ways of using instruction,",
    "start": "48630",
    "end": "54899"
  },
  {
    "text": "instruction prompting techniques. We'll look at fine-tuning. So there are many\ndifferent fine-tuning ways,",
    "start": "54900",
    "end": "62520"
  },
  {
    "text": "fine-tuning techniques. And then we'll close it out\nwith a hands-on walk through of using, again, SageMaker JumpStarts,",
    "start": "62520",
    "end": "69540"
  },
  {
    "text": "the foundation models therein\nto perform all of these tasks. So let's get rollin'.",
    "start": "69540",
    "end": "75270"
  },
  {
    "text": "And so say you found\nyour foundation model, say you finally picked\nthe foundation model",
    "start": "76830",
    "end": "82439"
  },
  {
    "text": "after you learned about\nwhat foundation models are, you learned how to pick\nyour foundation model.",
    "start": "82440",
    "end": "88980"
  },
  {
    "text": "So right now we're gonna dive\ninto parts three and four. So again, prompt engineering,",
    "start": "88980",
    "end": "95640"
  },
  {
    "text": "which is gonna let you more\neasily construct the prompt and get the response out of the model,",
    "start": "95640",
    "end": "103229"
  },
  {
    "text": "and then we'll look at fine-tuning. In this class actually, we're, eventually we're gonna\nhave a separate video",
    "start": "103230",
    "end": "108930"
  },
  {
    "text": "that's just about Retrieval\nAugmented Generation. There are a lot more techniques\nthat are going on with RAG.",
    "start": "108930",
    "end": "115079"
  },
  {
    "text": "It's a really helpful,\nreally useful technique, but we're just not gonna\ncover it in this one.",
    "start": "115080",
    "end": "120120"
  },
  {
    "text": "We'll just do a separate video for that. So you've got prompt\nengineering, fine-tuning, that's for this class,\napplication integration,",
    "start": "120120",
    "end": "128910"
  },
  {
    "text": "super important topic where you can learn about\nLangChain, for example,",
    "start": "128910",
    "end": "134130"
  },
  {
    "text": "how to host LangChain, how\nto build different plugins",
    "start": "134130",
    "end": "139130"
  },
  {
    "text": "and then point your plugins to your LLMs, all sorts of good things. Again, later in the class, we'll\nlearn about human feedback,",
    "start": "139140",
    "end": "146250"
  },
  {
    "text": "and then we're also gonna\nlearn about pre-training. And so right here, we'll\njust focus on these two.",
    "start": "146250",
    "end": "151770"
  },
  {
    "text": "So first off, let's say we're new to this. What is a prompt? Let's see if we can just\nunderstand a prompt here.",
    "start": "151770",
    "end": "157530"
  },
  {
    "text": "So, a prompt is really\nyour input to the model. Your prompt is what you\nare sending to the model.",
    "start": "157530",
    "end": "165750"
  },
  {
    "text": "We used to call this an inference request. You can call this a payload if you're thinking\nabout it as an API call.",
    "start": "165750",
    "end": "173040"
  },
  {
    "text": "But in any case, it's what\nyou send to the model. It's your prompts. And so your prompts can\nbe all sorts of things.",
    "start": "173040",
    "end": "181260"
  },
  {
    "text": "So if your prompt was, \"Tell me a story about a little girl riding her bicycle,\"",
    "start": "181260",
    "end": "186840"
  },
  {
    "text": "send that to your foundation model and you'll get back some\nbeginning of a story.",
    "start": "186840",
    "end": "193500"
  },
  {
    "text": "You could send it, \"Why\nwas six afraid of seven,\" and you'll model it to say,\n\"Because seven ate nine,\"",
    "start": "193500",
    "end": "199349"
  },
  {
    "text": "or something like this. You could ask it, \"What\nis the airspeed velocity of an unladen swallow,\"",
    "start": "199350",
    "end": "204720"
  },
  {
    "text": "if you're going for the\nMonty Python reference. And then if it's a clever\nmodel, it'll ask you, \"African or European swallow,\"",
    "start": "204720",
    "end": "212430"
  },
  {
    "text": "but it may just not get\nthe Monty Python reference. Or you could send it to a vision model,",
    "start": "212430",
    "end": "219930"
  },
  {
    "text": "say, \"Big is small,\" by Georgia O'Keefe, and then you'll get a response. So in any case, your prompt\nis all of these things.",
    "start": "219930",
    "end": "227250"
  },
  {
    "text": "Your prompt is what goes into the model and your response is\nthe result of the model.",
    "start": "227250",
    "end": "232590"
  },
  {
    "text": "It's what comes out of\nthis foundation model. And so now that we know what a prompt is, what on earth is prompt engineering?",
    "start": "232590",
    "end": "240000"
  },
  {
    "text": "So there is a massive spectrum\nof prompt engineering tasks. And generally, prompt\nengineering is the practice",
    "start": "240000",
    "end": "248520"
  },
  {
    "text": "of working on your prompt\nuntil it's awesome.",
    "start": "248520",
    "end": "253520"
  },
  {
    "text": "And so eventually you\nwanna get to this place where you get just a perfect answer",
    "start": "253890",
    "end": "260160"
  },
  {
    "text": "from the model on your first try, right, where you just pop in your question and you get this perfect response.",
    "start": "260160",
    "end": "267510"
  },
  {
    "text": "And this is the experience you wanna give to your customer, right? You want your consumer\nof this application,",
    "start": "267510",
    "end": "273030"
  },
  {
    "text": "your customer, to have this easy-breezy, really lovely experience. They ask a question, they get an answer.",
    "start": "273030",
    "end": "279840"
  },
  {
    "text": "Simple, streamlined, delightful. You might use and develop\nwhat's called a prompt template.",
    "start": "279840",
    "end": "287400"
  },
  {
    "text": "So a prompt template is\nbasically it's just a little file",
    "start": "287400",
    "end": "292400"
  },
  {
    "text": "that's set for different prompts and different downstream tasks. So, like, if your customer is\nasking to summarize a record,",
    "start": "292740",
    "end": "301430"
  },
  {
    "text": "your prompt template might have an example of summarization in it, might\nbe performing single-shot",
    "start": "302100",
    "end": "308550"
  },
  {
    "text": "or few-shot learning on their behalf. If it's doing classification, you might have an example, et cetera.",
    "start": "308550",
    "end": "315540"
  },
  {
    "text": "So prompt templates are a\nway that you can do this. While you're building your way up",
    "start": "315540",
    "end": "320670"
  },
  {
    "text": "to getting that prompt template, you might just do a lot of syntax hacking, like you'll just figure\nout what this model is,",
    "start": "320670",
    "end": "328800"
  },
  {
    "text": "what syntax it's preferring like, \"Does it like the word agent? Does it like the word human?",
    "start": "328800",
    "end": "334139"
  },
  {
    "text": "Does it like TLDR? What sort of syntax is it preferring?\"",
    "start": "334140",
    "end": "339389"
  },
  {
    "text": "Zero-shot, and we'll dive into this throughout the class here, but so zero-shot means you\njust send it the instruction",
    "start": "339390",
    "end": "347250"
  },
  {
    "text": "and it responds, like it means you don't really have\nanything else in the prompt.",
    "start": "347250",
    "end": "352560"
  },
  {
    "text": "That is different from\nwhat's called single-shot and few-shot learning. I use the word learning here",
    "start": "352560",
    "end": "358770"
  },
  {
    "text": "because it's mimicking learning. I don't like that actually 'cause it's not updating the parameters, but it's mimicking the\nappearance of learning",
    "start": "358770",
    "end": "365880"
  },
  {
    "text": "so we call it single-shot\nand few-shot learning. But in any case, single-shot\nmeans I take my prompt",
    "start": "365880",
    "end": "371970"
  },
  {
    "text": "and I'm gonna give it one example. So I put one example in the prompt and then I give it the\nbody and the instruction",
    "start": "371970",
    "end": "380400"
  },
  {
    "text": "and it responds. Few-shot just means I\nhave more than one prompt or more than one example.",
    "start": "380400",
    "end": "385590"
  },
  {
    "text": "So if I want to have my model translate to a different language,",
    "start": "385590",
    "end": "390810"
  },
  {
    "text": "or I want it to do some\nnuance summarization or some nuance style transfers,",
    "start": "390810",
    "end": "397410"
  },
  {
    "text": "then I would do few-shot learning. So I would have a couple\nexamples in the prompt,",
    "start": "397410",
    "end": "403110"
  },
  {
    "text": "plus the body, plus the\ninstruction, and then boom. Prompt-tuning is a very complex\ntechnique that you can do,",
    "start": "403110",
    "end": "412700"
  },
  {
    "text": "if you want to, which actually\nlets you like train vectors. So then you're sort of,",
    "start": "412740",
    "end": "417960"
  },
  {
    "text": "you're generating these vectors and you're training the\nvectors using backprop",
    "start": "417960",
    "end": "423030"
  },
  {
    "text": "to create these vectors\nthat are task-specific. We're not gonna dive into\nthat today, but you can.",
    "start": "423030",
    "end": "429180"
  },
  {
    "text": "And so obviously on the\nfar end of the spectrum is pure exhaustion, (chuckles) just trying everything until it works.",
    "start": "429180",
    "end": "435510"
  },
  {
    "text": "And so we want you and your customers to be up here as much as possible. So let's see how we get there.",
    "start": "435510",
    "end": "441903"
  },
  {
    "text": "So first off, it's helpful to know that an instruction-tuned model",
    "start": "443100",
    "end": "448360"
  },
  {
    "text": "is gonna make your life a lot easier. So you don't want just a generic LLM,",
    "start": "449220",
    "end": "455220"
  },
  {
    "text": "a generic large language model, you don't wanna just\nwork with generic GPT-2",
    "start": "455220",
    "end": "460530"
  },
  {
    "text": "or generic GPT-3. That is because they haven't been what's called instruction-tuned.",
    "start": "460530",
    "end": "466290"
  },
  {
    "text": "So instruction fine-tuning. An instruction fine-tuning refers to",
    "start": "466290",
    "end": "471729"
  },
  {
    "text": "sort of taking an instruction\nlike, \"Tell me a story,\"",
    "start": "472680",
    "end": "477680"
  },
  {
    "text": "or generate a response or, you know, \"Explain something to me,\"",
    "start": "478320",
    "end": "484080"
  },
  {
    "text": "and then we have it give the answer. So it just provides the answer. You can perform supervised fine-tuning",
    "start": "484080",
    "end": "491069"
  },
  {
    "text": "on these instructions. We're gonna dive into that, but I'm mentioning it here\nbecause you wanna make sure",
    "start": "491070",
    "end": "498390"
  },
  {
    "text": "that the model you're using has already been instruction-tuned. Like, pro tip, make sure your model",
    "start": "498390",
    "end": "504660"
  },
  {
    "text": "has been instruction-tuned. If it hasn't, your battle\nis gonna be a lot larger. So here's some befores and afters.",
    "start": "504660",
    "end": "511800"
  },
  {
    "text": "So before instruction-tuning,\nmy prompt is like, \"Why was six afraid of\nseven,\" just for fun.",
    "start": "511800",
    "end": "518159"
  },
  {
    "text": "And so then my answer,\nwhich is from GPT-2,",
    "start": "518160",
    "end": "523159"
  },
  {
    "text": "is totally nonsensical, right? I'm reading this, this makes no sense.",
    "start": "523260",
    "end": "531300"
  },
  {
    "text": "Clearly, this is not even\nanswering this question. And then for another prompt,\nI'd say the prompt is,",
    "start": "531300",
    "end": "537990"
  },
  {
    "text": "again, this is my favorite one, \"What's the difference between\na mimosa and a samosa?\" So I like this example because\nit tricks computers, right?",
    "start": "537990",
    "end": "544936"
  },
  {
    "text": "'Cause the token sound somewhat similar, but we humans know that they come from,",
    "start": "544937",
    "end": "551917"
  },
  {
    "text": "like, very different parts of the world and very different sort\nof cultural contexts. And so it's a good test for a computer.",
    "start": "551917",
    "end": "559110"
  },
  {
    "text": "And so what's the difference\nbetween these things? And not only does this\nanswer not make sense,",
    "start": "559110",
    "end": "566940"
  },
  {
    "text": "it's actually hallucinating\na word, (chuckles) made up this mimulating a color.",
    "start": "566940",
    "end": "571060"
  },
  {
    "text": "Like, yes, I too like\nto mimulate. (chuckles) Like, that makes no sense, so.",
    "start": "572280",
    "end": "577937"
  },
  {
    "text": "And so not only is it bad,\nit's hallucinating a word. And so then now we're gonna use",
    "start": "577937",
    "end": "585240"
  },
  {
    "text": "AI21 Jurassic-2 Jumbo Instruct,\nand let's see what we get. So the answer here for this prompt,",
    "start": "585240",
    "end": "594029"
  },
  {
    "text": "let's read this for a second. Yeah, so this looks feasible, right?",
    "start": "594030",
    "end": "601657"
  },
  {
    "text": "\"It is likely that it is a mnemonic device",
    "start": "601657",
    "end": "606657"
  },
  {
    "text": "to aid in the memorization\nof the order of the numbers from 1 to 10. The expression '9 on 10',\" there we go.",
    "start": "607350",
    "end": "613949"
  },
  {
    "text": "So that's an answer. It's funny, though,",
    "start": "613950",
    "end": "618953"
  },
  {
    "text": "'cause it's still not answering the joke. Like, I would prefer for\nit to just answer the joke, but we get some answer.",
    "start": "618953",
    "end": "626160"
  },
  {
    "text": "And then the difference\nbetween a mimosa and a samosa, here we go, \"Mimosa is made\nof champagne and orange juice",
    "start": "626160",
    "end": "634170"
  },
  {
    "text": "while samosa is a savory\ntype of Indian pastry, spiced vegetables or meat.\"",
    "start": "634170",
    "end": "639327"
  },
  {
    "text": "So both words have \"mosa\" in them. They have that \"mosa\" one or two tokens.",
    "start": "642721",
    "end": "648453"
  },
  {
    "text": "So almost got there. But yeah, so obviously better, right? You can see that this is much\ncloser to the right answer",
    "start": "649530",
    "end": "656040"
  },
  {
    "text": "and this is just sort of nonsensical. And so part of this difference",
    "start": "656040",
    "end": "661260"
  },
  {
    "text": "comes from the model\njust being larger, right, AI21 is just larger than GPT-2,",
    "start": "661260",
    "end": "667140"
  },
  {
    "text": "but it also has to do with\nthe instruction fine-tuning and that makes the model\nrespond much more quickly.",
    "start": "667140",
    "end": "673773"
  },
  {
    "text": "So generally, try to use\ninstruction-tuned models. So I'm saying this",
    "start": "674610",
    "end": "679860"
  },
  {
    "text": "because when you start with\nyour prompt engineering, you wanna understand the\nmodel you're working with.",
    "start": "679860",
    "end": "685890"
  },
  {
    "text": "You wanna know what model it\nis, where it's coming from, has it been instruction fine-tuned?",
    "start": "685890",
    "end": "691770"
  },
  {
    "text": "And if your prompt is just\nwildly off, then go make sure that your model has been\ninstruction fine-tuned.",
    "start": "691770",
    "end": "698610"
  },
  {
    "text": "And so what is instruction fine-tuning? Again, it uses supervised learning to adapt the model behavior.",
    "start": "698610",
    "end": "705120"
  },
  {
    "text": "So you might develop a dataset\nof prompts and then answers.",
    "start": "705120",
    "end": "710120"
  },
  {
    "text": "So, something like, \"How\nhigh is Mount Everest, the largest ocean in the world? How many human languages there\nare,\" and then you'll get,",
    "start": "711270",
    "end": "719850"
  },
  {
    "text": "again, answers to the prompts or responses and then you just put this into a supervised fine-tuning procedure.",
    "start": "719850",
    "end": "727350"
  },
  {
    "text": "So you take your label dataset, when you're doing this, make\nsure you start with a model",
    "start": "727350",
    "end": "733080"
  },
  {
    "text": "that hasn't already been\ninstruction fine-tuned, that you wanna start with a model base",
    "start": "733080",
    "end": "740130"
  },
  {
    "text": "and then perform your\ninstruction fine-tuning on top of this. So that's helpful to do. But basically, it ensures that the model",
    "start": "740130",
    "end": "747480"
  },
  {
    "text": "is then capable of following instructions. And it unlocks these\nother instruction tasks",
    "start": "747480",
    "end": "753300"
  },
  {
    "text": "like translate, summarize, extract. And so instruction fine-tuning is almost",
    "start": "753300",
    "end": "759540"
  },
  {
    "text": "how your foundation model is able to learn and accomplish so many\ndifferent learning techniques.",
    "start": "759540",
    "end": "764910"
  },
  {
    "text": "Like, the foundation model\nlearns a lot of those naturally occurring tasks in your dataset",
    "start": "764910",
    "end": "771630"
  },
  {
    "text": "just because they're there. But then when you add the instruction\nfine-tuning on top of this,",
    "start": "771630",
    "end": "776880"
  },
  {
    "text": "it, like, makes them\nimmediately more accessible and more useful. So instruction fine-tuning\nis super useful.",
    "start": "776880",
    "end": "782943"
  },
  {
    "text": "And so your best case scenario\nis a zero-shot prompting. And so this is what you\nwant your model to do",
    "start": "784110",
    "end": "792279"
  },
  {
    "text": "and this is what you want the\ncustomer experience to be. So you want the customer experience",
    "start": "793230",
    "end": "798960"
  },
  {
    "text": "to just easily I ask a\nquestion, I get an answer, I get on with my life. You might do some work to get\nthere, and we'll poke at this.",
    "start": "798960",
    "end": "806850"
  },
  {
    "text": "But so zero-shot prompting\nis where you send in a prompt",
    "start": "806850",
    "end": "811850"
  },
  {
    "text": "and you get back an answer. So my prompt could be like, \"What are the basic\ningredients of nachos,\"",
    "start": "812160",
    "end": "817170"
  },
  {
    "text": "and then I get back this\ndetailed answer about nachos or, \"A delicious food, developed in Mexico,",
    "start": "817170",
    "end": "822870"
  },
  {
    "text": "in Mexico City in the '40s. Basic ingredients are\ntortilla chips, cheese, and jalapeno peppers.",
    "start": "822870",
    "end": "828647"
  },
  {
    "text": "And in the US, we like to add\nguacamole, sour cream, salsa, and sometimes ground beef.",
    "start": "828647",
    "end": "835020"
  },
  {
    "text": "They're usually an appetizer,\ncome before the main entree, can be served as the primary meal,",
    "start": "835020",
    "end": "840300"
  },
  {
    "text": "like for Super Bowl watch parties.\" So you get this nice answer that not just what are the\nbasic ingredients of nachos,",
    "start": "840300",
    "end": "846870"
  },
  {
    "text": "but sort of this larger context for it. And then similarly, with Stable Diffusion,",
    "start": "846870",
    "end": "851910"
  },
  {
    "text": "we send in this prompt and we\nget back this awesome image. So in a perfect world, your\nprompt looks exactly like this.",
    "start": "851910",
    "end": "859380"
  },
  {
    "text": "It's just one question, zero-shots. We say again zero-shots",
    "start": "859380",
    "end": "865740"
  },
  {
    "text": "because there aren't any\nprevious examples in this prompt. It's just zero-shot. (chuckles)",
    "start": "865740",
    "end": "872280"
  },
  {
    "text": "You just send it the question\nand you get your answer. We're gonna compare this with single-shot",
    "start": "872280",
    "end": "877769"
  },
  {
    "text": "and few-shot prompting. So your alternative, one alternative,",
    "start": "877770",
    "end": "882839"
  },
  {
    "text": "is called single-shot prompting. So if zero-shot fails,",
    "start": "882840",
    "end": "888120"
  },
  {
    "text": "you're gonna try to add\nan example to the prompt of what you want that model to produce.",
    "start": "888120",
    "end": "893370"
  },
  {
    "text": "So in my prompt, like\nlet's say I want my prompt",
    "start": "893370",
    "end": "898370"
  },
  {
    "text": "to give me a main course suggestion. So I give my prompt, say\nI give it an appetizer,",
    "start": "898620",
    "end": "906180"
  },
  {
    "text": "which is spinach dip generated\nby our Stable Diffusion,",
    "start": "906180",
    "end": "911180"
  },
  {
    "text": "so I give it the spinach dip. And then my main course, and I want the model to sort of answer,",
    "start": "911310",
    "end": "917130"
  },
  {
    "text": "and so you can see there's this blank here and I want the model to\njust fill in the blank.",
    "start": "917130",
    "end": "922473"
  },
  {
    "text": "In a single-shot world, I add an example of this to my prompt.",
    "start": "923790",
    "end": "929610"
  },
  {
    "text": "So I actually put in my prompt, I literally say appetizer: samosa;",
    "start": "929610",
    "end": "935370"
  },
  {
    "text": "main course: saag paneer, my\nidea of perfection. (chuckles)",
    "start": "935370",
    "end": "940370"
  },
  {
    "text": "So, yeah. So, you put both of those in the prompt and then you get back this answer.",
    "start": "940560",
    "end": "947130"
  },
  {
    "text": "And so we call this, again,\nsingle-shot prompting because we're providing\nexactly one example,",
    "start": "947130",
    "end": "954019"
  },
  {
    "text": "so we have exactly one shot to the model. So that's single-shot prompting.",
    "start": "954020",
    "end": "959800"
  },
  {
    "text": "And then few-shot prompting\nis where we'll add multiple examples to this.",
    "start": "960960",
    "end": "966150"
  },
  {
    "text": "So my prompt, I could say\ndata analysis: pandas; statistical machine learning: sklearn;",
    "start": "966150",
    "end": "973500"
  },
  {
    "text": "deep learning: PyTorch. You Python developers will know that this prompt is basically mapping",
    "start": "973500",
    "end": "980080"
  },
  {
    "text": "like something you need, a task, and then a Python open source toolkit,",
    "start": "980970",
    "end": "988590"
  },
  {
    "text": "a Python package that you can use. So pandas is a Python package\nwe can use for data analysis.",
    "start": "988590",
    "end": "994800"
  },
  {
    "text": "Sklearn is a Python package for statistical machine learning. PyTorch is the same\nthing for deep learning.",
    "start": "994800",
    "end": "1001160"
  },
  {
    "text": "And then Django is the same\nthing for front-end development. So this is what few-shot\nprompting looks like.",
    "start": "1001160",
    "end": "1009050"
  },
  {
    "text": "It's where we include,\nlike, multiple examples in the prompt of the,\nlike, call and response,",
    "start": "1009050",
    "end": "1016550"
  },
  {
    "text": "the input and the output that\nwe are giving to the model, and then we hope that our model will learn",
    "start": "1016550",
    "end": "1023870"
  },
  {
    "text": "from these prompts and answer in kind. So again, we call this few-shot prompting",
    "start": "1023870",
    "end": "1029329"
  },
  {
    "text": "because we're adding multiple\nexamples to that model. And then because of course,\nhere is this cute panda bear,",
    "start": "1029330",
    "end": "1037550"
  },
  {
    "text": "generated by Stable\nDiffusion, in the wild, closeup, photorealism, realistic.",
    "start": "1037550",
    "end": "1043699"
  },
  {
    "text": "Yeah, and then the negative\nprompts are handed here 'cause I told it to not do a\ncartoon and not do animation,",
    "start": "1043700",
    "end": "1050539"
  },
  {
    "text": "but give me like a\nphotorealistic panda bear. So I thought that was fun.",
    "start": "1050540",
    "end": "1055290"
  },
  {
    "text": "All right, so here's an example of how you can use prompt engineering to solve summarization.",
    "start": "1056210",
    "end": "1063290"
  },
  {
    "text": "Now this is a wordy slide, bear with me. So the best way to work\nwith prompt engineering",
    "start": "1063290",
    "end": "1071510"
  },
  {
    "text": "is to just paste the entire\ndocument into the prompt, like your entire PDF, I mean\nbased on the token size,",
    "start": "1071510",
    "end": "1078860"
  },
  {
    "text": "how large your model can accept. But yeah, just take the entire document",
    "start": "1078860",
    "end": "1084049"
  },
  {
    "text": "that you want summarized, paste\nthat thing into the prompt, and then include a single instruction",
    "start": "1084050",
    "end": "1090379"
  },
  {
    "text": "at the bottom of the prompt. So in this case, I pasted in\nsome paragraphs about pugs,",
    "start": "1090380",
    "end": "1096700"
  },
  {
    "text": "about pug dogs that I got from Wikipedia, pasted these things in here,\nand then we see this prompt.",
    "start": "1096950",
    "end": "1104090"
  },
  {
    "text": "So the pug dog is from China, physically distinctive features of this wrinkly, short-muzzled\nface, curled tail.",
    "start": "1104090",
    "end": "1111649"
  },
  {
    "text": "Brought from China to Europe. Known for being sociable\nand gentle companion dogs.",
    "start": "1111650",
    "end": "1116720"
  },
  {
    "text": "And so that's the article\nthat I want summarized.",
    "start": "1116720",
    "end": "1121720"
  },
  {
    "text": "And then, again, here's my instruction. I'm just adding the instruction\nat the bottom of the prompt",
    "start": "1121820",
    "end": "1127400"
  },
  {
    "text": "and I just say summarize. And then the model responds with a summarization of the article.",
    "start": "1127400",
    "end": "1133790"
  },
  {
    "text": "So the pug is a distinctive breed of dog. It's from China, known\nfor its wrinkly face,",
    "start": "1133790",
    "end": "1138860"
  },
  {
    "text": "curled tail, compact body. Brought to Europe in the 16th century. Fact check: Yes, 16th, got it.",
    "start": "1138860",
    "end": "1146000"
  },
  {
    "text": "Gained popularity in Western Europe. Fact check: Yes. Including among the British Royal Family.",
    "start": "1146000",
    "end": "1153020"
  },
  {
    "text": "British Royal Family, yes. Here we go, Queen Victoria, Royal Family. Pugs are sociable and gentle\ncompanion dogs, check.",
    "start": "1153020",
    "end": "1160970"
  },
  {
    "text": "Described as even-tempered\nand charming, check, by the American Kennel club.",
    "start": "1160970",
    "end": "1166403"
  },
  {
    "text": "Did we, yeah, American Kennel Club. There we go, great. Continue to be popular. So what I was verbally doing right there",
    "start": "1167720",
    "end": "1174919"
  },
  {
    "text": "is I was doing fact-checking. I was verbally fact-checking\ncontent generated by the model",
    "start": "1174920",
    "end": "1181660"
  },
  {
    "text": "and then just literally making sure that it was in the prompt. So this is a way that\nyou can add your own,",
    "start": "1182150",
    "end": "1187640"
  },
  {
    "text": "like, fact-checking to the model response to sort of mitigate the hallucinations.",
    "start": "1187640",
    "end": "1192980"
  },
  {
    "text": "It's just lookup terms. Like, all of the odd or notable entities",
    "start": "1192980",
    "end": "1199510"
  },
  {
    "text": "or terms that this model is\nproducing from its LLM brain of causal language modeling loss,",
    "start": "1200090",
    "end": "1207289"
  },
  {
    "text": "which just predicts tokens,\njust run a fact check, like just extract the\nentities from here using NER,",
    "start": "1207290",
    "end": "1216010"
  },
  {
    "text": "any NER package, extract those things, and then just do a quick check like,",
    "start": "1216106",
    "end": "1221217"
  },
  {
    "text": "\"Did it make up British Royal Family,\" no, \"Is the royal family in here,\" yes.",
    "start": "1221217",
    "end": "1226970"
  },
  {
    "text": "It's interesting because, yeah, so here it's summarizing obviously British Royal Family.",
    "start": "1226970",
    "end": "1232700"
  },
  {
    "text": "'Cause in the prompt,\nwe said United Kingdom, Queen Victoria, which she passed on",
    "start": "1232700",
    "end": "1239570"
  },
  {
    "text": "to other members of the Royal Family, and here clearly it's summarizing. So it's summarizing by saying\nthe British Royal Family,",
    "start": "1239570",
    "end": "1247160"
  },
  {
    "text": "whereas the word British actually didn't even appear in the prompt. So that's good, that's good behavior.",
    "start": "1247160",
    "end": "1253159"
  },
  {
    "text": "So when you're doing the fact-checking and the term-checking, you\nwant to give yourself this,",
    "start": "1253160",
    "end": "1259669"
  },
  {
    "text": "like, semantic meaning like, \"Semantically, does it\nhave that in the prompt?\"",
    "start": "1259670",
    "end": "1265220"
  },
  {
    "text": "And so that's an easy way to mitigate some\nhallucinations in your model. (chuckles) And of course,\nhere is your pug dog.",
    "start": "1265220",
    "end": "1273920"
  },
  {
    "text": "So we did an adorable pug dog\nwith Queen Victoria, #puglife.",
    "start": "1273920",
    "end": "1278920"
  },
  {
    "text": "Thought that was super cute. Now, I didn't get Queen\nVictoria in this image",
    "start": "1281303",
    "end": "1287090"
  },
  {
    "text": "and I tried it a couple times\nlike with different seeds, but I still love it. Like, it's still just so perfect.",
    "start": "1287090",
    "end": "1294230"
  },
  {
    "text": "So I thought I'd keep that for you. All right, so now we're gonna learn",
    "start": "1294230",
    "end": "1299540"
  },
  {
    "text": "how to use prompt engineering\nto solve classification. So this might sound\nstraightforward, and it is,",
    "start": "1299540",
    "end": "1306320"
  },
  {
    "text": "but let's make sure we capture this. So same as last time, in\nthe one you previously saw,",
    "start": "1306320",
    "end": "1311720"
  },
  {
    "text": "we're gonna take all of\nthe relevant information from the prompt and we're\njust gonna paste that into the model.",
    "start": "1311720",
    "end": "1318080"
  },
  {
    "text": "So all of the information\nfrom the document that we want classified,",
    "start": "1318080",
    "end": "1323330"
  },
  {
    "text": "everything we have that will fit in the token size of\nthat model, paste it in",
    "start": "1323330",
    "end": "1328730"
  },
  {
    "text": "and then ask a question at the bottom. So here we're asking, \"What\ntype of bear is this,\" and then the prompt is\nlong and detailed, right?",
    "start": "1328730",
    "end": "1336170"
  },
  {
    "text": "So the prompt: \"This bear\nhas a large, round head with fluffy, rounded ears;",
    "start": "1336170",
    "end": "1341990"
  },
  {
    "text": "broad flat nose, small round eyes. Bears are known for their\nability to climb trees;",
    "start": "1341990",
    "end": "1347030"
  },
  {
    "text": "opposable thumbs; generally solitary,\" and then this is the obvious\none, \"Sedentary lifestyle,",
    "start": "1347030",
    "end": "1353480"
  },
  {
    "text": "spending around 18 to\n20 hours a day sleeping or resting in the safety of tree branches; primarily nocturnal.\"",
    "start": "1353480",
    "end": "1359447"
  },
  {
    "text": "Right, so we're describing this bear, we're describing this character. And then we ask the model,\n\"What type of bear is this?\"",
    "start": "1361256",
    "end": "1368690"
  },
  {
    "text": "And then the model should say, \"Oh, it's a koala bear, or koalas.\" And so that's how we can\nuse prompt engineering",
    "start": "1368690",
    "end": "1376580"
  },
  {
    "text": "to solve classification, right? I just paste in all of my data about whatever object\nor widget or timestamp",
    "start": "1376580",
    "end": "1384860"
  },
  {
    "text": "or digital extraction or action or person or feature\nor literally anything,",
    "start": "1384860",
    "end": "1393440"
  },
  {
    "text": "like paste all the information\nthat I have about that, pop that in the prompt and\nthen get back an answer.",
    "start": "1393440",
    "end": "1400550"
  },
  {
    "text": "And it says, \"Oh, okay, great. It classified it.\" Obviously it classified it correctly. And so that's how to\nuse prompt engineering",
    "start": "1400550",
    "end": "1407090"
  },
  {
    "text": "to solve classification. And of course I also had\nto give you some koalas,",
    "start": "1407090",
    "end": "1413090"
  },
  {
    "text": "so here are koalas climbing this tree. This is funny because the koala\nheads are adorable, right?",
    "start": "1413090",
    "end": "1420590"
  },
  {
    "text": "Obviously, they're cute. But, like, the legs\nare all over the place. Like, check this out. We've got this like\nghost arm that's up here",
    "start": "1420590",
    "end": "1428570"
  },
  {
    "text": "and then some very strange-looking hands. And then I'm pretty\nsure koalas can do this,",
    "start": "1428570",
    "end": "1435560"
  },
  {
    "text": "like cross tree split,\nlike maybe they can, but that seems a little odd.",
    "start": "1435560",
    "end": "1443000"
  },
  {
    "text": "And so it's interesting,\nright, as we look at this and just to see like, \"Ah,\" you know, \"they're good at some things",
    "start": "1443000",
    "end": "1448640"
  },
  {
    "text": "and, like, really struggle\nwith some other things.\" And so obviously things\nwill get better over time,",
    "start": "1448640",
    "end": "1453740"
  },
  {
    "text": "but still fun to check. All right, and so now we're\ngonna use prompt engineering",
    "start": "1453740",
    "end": "1459560"
  },
  {
    "text": "to solve translation. So we looked at prompt engineering\nto solve summarization,",
    "start": "1459560",
    "end": "1465019"
  },
  {
    "text": "then we looked at it to\nsolve classification, now we're gonna look at\nit to solve translation. Same flow that we've just learned,",
    "start": "1465020",
    "end": "1471980"
  },
  {
    "text": "like we're gonna paste in\nthe text to the prompt. So we paste in the text,\nwhich here the text is like,",
    "start": "1471980",
    "end": "1478767"
  },
  {
    "text": "\"I parked my car in the driveway, took out the trash, put\nmy umbrella in the trunk.\"",
    "start": "1478767",
    "end": "1484280"
  },
  {
    "text": "And then the instruction that I gave it was to translate this\ninto British English.",
    "start": "1484280",
    "end": "1490760"
  },
  {
    "text": "So now we're gonna translate this prompt into British English. I love this. \"I parked my car on the\ndrive, took out the rubbish,",
    "start": "1490760",
    "end": "1500270"
  },
  {
    "text": "and put my brolly in the boot.\" (chuckles) So brolly is indeed not hallucinated.",
    "start": "1500270",
    "end": "1507830"
  },
  {
    "text": "It's, I wanna say, from World\nWar II, something like this, when like umbrellas were\ndistributed by the government,",
    "start": "1507830",
    "end": "1514789"
  },
  {
    "text": "I think, I might be wrong. But in any case, clearly\nit did translate this",
    "start": "1514790",
    "end": "1521000"
  },
  {
    "text": "to British English. And so you can use prompt\nengineering to solve translation.",
    "start": "1521000",
    "end": "1526900"
  },
  {
    "text": "And then my pro tip here is that you can use this\ntranslate instruction",
    "start": "1528620",
    "end": "1536240"
  },
  {
    "text": "to solve a variety of things. So you could say like, \"Translate this Python\ncode into Java code.",
    "start": "1536240",
    "end": "1543797"
  },
  {
    "text": "\"Translate this COBOL,\" or FORTRAN code, for you out there, \"into Python.\"",
    "start": "1543837",
    "end": "1550856"
  },
  {
    "text": "\"Translate this C++ code into\nPython,\" or back and forth. Obviously if you're doing\ncode-level translations,",
    "start": "1550857",
    "end": "1558320"
  },
  {
    "text": "you probably wanna fine-tune that thing, or fine-tune the model,\nbut it won't be bad.",
    "start": "1558320",
    "end": "1564710"
  },
  {
    "text": "It won't be bad right away. And then of course here\nis your British flag, waving in the wind in London,",
    "start": "1564710",
    "end": "1570473"
  },
  {
    "text": "which looks a little bit more like a take, or like a Stable Diffusion's\ntake of a British flag.",
    "start": "1571790",
    "end": "1577460"
  },
  {
    "text": "Like, no doubt it can get another one if I had, like, increased\nthe guidance scale to get it more obvious.",
    "start": "1577460",
    "end": "1584273"
  },
  {
    "text": "Also, just in prompting for images, if you have an image and\nyou wanna just send it,",
    "start": "1585260",
    "end": "1591950"
  },
  {
    "text": "you can do this really nice,\nlike, image to image prompting. So a different way of\ndoing this would be like,",
    "start": "1591950",
    "end": "1597830"
  },
  {
    "text": "I give Stable Diffusion like\na picture of a British flag and then I run the image to image process",
    "start": "1597830",
    "end": "1605210"
  },
  {
    "text": "and then it, like, literally takes that and then generates a different background or, like, puts it in a different modality",
    "start": "1605210",
    "end": "1611840"
  },
  {
    "text": "or a different spot, but it's still literally\nthe same British flag. I still like this picture,",
    "start": "1611840",
    "end": "1618200"
  },
  {
    "text": "this, like, creative\ninterpretation of a British flag. When all else fails,",
    "start": "1618200",
    "end": "1624470"
  },
  {
    "text": "so once you've exhausted the fine-tuning, once you've actually tried your zero-shot,",
    "start": "1624470",
    "end": "1632260"
  },
  {
    "text": "your instruction\nfine-tuning, your few-shot,",
    "start": "1633170",
    "end": "1637523"
  },
  {
    "text": "single-shot, all the shots,\nyou've tried all of this, then fine-tuning is gonna\nbe a good place to start.",
    "start": "1639620",
    "end": "1647660"
  },
  {
    "text": "So prompt engineering is with\na good foundation-based model.",
    "start": "1647660",
    "end": "1652523"
  },
  {
    "text": "Like, theoretically, that should do something valuable for you. That should improve the\nquality of your model",
    "start": "1653870",
    "end": "1660920"
  },
  {
    "text": "in some meaningful way. It's expected that prompt engineering with a good foundation model will do this.",
    "start": "1660920",
    "end": "1667313"
  },
  {
    "text": "The question is, \"Will prompt\nengineering do enough,\" right? So there's this gap between",
    "start": "1668240",
    "end": "1673520"
  },
  {
    "text": "how well prompt engineering\nimproves your performance versus what your customers want.",
    "start": "1673520",
    "end": "1679220"
  },
  {
    "text": "And fine-tuning is a good\ntechnique you can use to try to close that gap.",
    "start": "1679220",
    "end": "1684770"
  },
  {
    "text": "And so fine-tuning is also helpful because you can create a\nnew model artifact, right?",
    "start": "1684770",
    "end": "1691370"
  },
  {
    "text": "When you do fine-tuning, you're actually creating this asset that,",
    "start": "1691370",
    "end": "1696920"
  },
  {
    "text": "particularly when you\nuse an open source model, literally belongs to you. And so it's your IP, it's your asset,",
    "start": "1696920",
    "end": "1703610"
  },
  {
    "text": "you can put it and run\nit anywhere you want. This is an example of,\nit's called DreamBooth.",
    "start": "1703610",
    "end": "1709252"
  },
  {
    "text": "So DreamBooth is a\nfine-tuning technique that, as I was just describing,\ndoes image to image",
    "start": "1709253",
    "end": "1717019"
  },
  {
    "text": "and then it sort of\nenables prompting on this. So it actually fine-tunes this model.",
    "start": "1717020",
    "end": "1723950"
  },
  {
    "text": "So we'll upload pictures of our corgi, or any dog or animal that we want.",
    "start": "1723950",
    "end": "1730250"
  },
  {
    "text": "We upload like multiple pictures\nof the same object, right? So I take multiple pictures of my corgi,",
    "start": "1730250",
    "end": "1737090"
  },
  {
    "text": "or my accelerators, or my plants and then send them to my model.",
    "start": "1737090",
    "end": "1744110"
  },
  {
    "text": "Then I'm gonna fine-tune\nthe Stable Diffusion model on those, like, four or five\nimages, then I can prompt it,",
    "start": "1744110",
    "end": "1751580"
  },
  {
    "text": "then I can give it prompts. And here the prompts are\nlike in the Acropolis,",
    "start": "1751580",
    "end": "1756649"
  },
  {
    "text": "in a doghouse, getting a\nhaircut, swimming. (chuckles) Like, they're actually good, right?",
    "start": "1756650",
    "end": "1763610"
  },
  {
    "text": "Like, it's obviously the same dog. The dog still has this\nlittle like white, you know,",
    "start": "1763610",
    "end": "1771620"
  },
  {
    "text": "stream down the top of the head, clearly two ears, no\nhallucinations on the paws,",
    "start": "1771620",
    "end": "1778270"
  },
  {
    "text": "like all the features of the dog are very closely maintained. And then the dog is very realistically",
    "start": "1780380",
    "end": "1787580"
  },
  {
    "text": "in these different settings. Like, the lighting is the same,",
    "start": "1787580",
    "end": "1792950"
  },
  {
    "text": "the positioning of the dog is natural, the focus is logical.",
    "start": "1792950",
    "end": "1799130"
  },
  {
    "text": "And so, yeah, fine-tuning is\na great way to take new data,",
    "start": "1799130",
    "end": "1804130"
  },
  {
    "text": "pop it into your model, and then get it to do\nsomething that you care about and where it performs quite well there.",
    "start": "1804620",
    "end": "1810893"
  },
  {
    "text": "And so what is fine-tuning,\nyou ask with bated breath. So fine-tuning is a\ntechnique that you can use,",
    "start": "1811850",
    "end": "1818269"
  },
  {
    "text": "again, to adapt a model. And just like we learned\nwith prompt engineering,",
    "start": "1818270",
    "end": "1823730"
  },
  {
    "text": "there's a massive spectrum\nof fine-tuning techniques. And on the plus side, we\nwanna create a net new model",
    "start": "1823730",
    "end": "1832220"
  },
  {
    "text": "that solves every problem perfectly, and we wanna avoid doing a lot\nof work and getting nowhere,",
    "start": "1832220",
    "end": "1839396"
  },
  {
    "text": "(chuckles) that's the negative. And so closer to the perfection spectrum,",
    "start": "1839396",
    "end": "1844520"
  },
  {
    "text": "parameter-efficient fine-tuning is a technique we're about to dive into that lets you take your LLM\nand then you're gonna inject",
    "start": "1844520",
    "end": "1853490"
  },
  {
    "text": "trainable weights into that thing. So you're gonna take a massive LLM that's like a couple hundred GB on disc,",
    "start": "1853490",
    "end": "1860570"
  },
  {
    "text": "multiple instances just\nto hold it in memory, but then you're gonna just inject these trainable weights into that,",
    "start": "1860570",
    "end": "1868132"
  },
  {
    "text": "which will let you train\njust those weights. So it's extremely parameter-efficient.",
    "start": "1869330",
    "end": "1875450"
  },
  {
    "text": "One technique for doing\nthis is called LoRA, Low-Rank Adaptation, which will,",
    "start": "1875450",
    "end": "1881630"
  },
  {
    "text": "yeah, find the ranks or find\nthe aspects of the model",
    "start": "1881630",
    "end": "1886630"
  },
  {
    "text": "that are being sort of underutilized and then do the tuning there. Prefix tuning and LoRA,\nthose are both techniques",
    "start": "1887960",
    "end": "1897140"
  },
  {
    "text": "available through the\nHugging Face library, parameter-efficient fine-tuning, that implements both of these for you.",
    "start": "1897140",
    "end": "1904130"
  },
  {
    "text": "Those are both generally a\npart of this larger school that's called transfer learning,",
    "start": "1904130",
    "end": "1910520"
  },
  {
    "text": "where transfer learning lets\nyou take a neural network, pop a few more layers\non top of that network and then just train those.",
    "start": "1910520",
    "end": "1916640"
  },
  {
    "text": "So transfer learning is\nmuch more compute-efficient, much more data-efficient",
    "start": "1916640",
    "end": "1922340"
  },
  {
    "text": "because instead of needing to\nfine-tune the entire model, you can then just fine-tune a few layers,",
    "start": "1922340",
    "end": "1929240"
  },
  {
    "text": "which means you can do it\nwith smaller data samples and less resources, which is great.",
    "start": "1929240",
    "end": "1935420"
  },
  {
    "text": "And then classic fine-tuning is just fine-tuning the whole thing. So you might, well, actually not quite.",
    "start": "1935420",
    "end": "1941360"
  },
  {
    "text": "Classic fine-tuning is you\ntake your BERT pretrained LLM, or pretrained language model,",
    "start": "1941360",
    "end": "1947900"
  },
  {
    "text": "and then you add a head to it, you add like a text generation head or a text classification head to this",
    "start": "1947900",
    "end": "1955880"
  },
  {
    "text": "and then you just fine-tune\nthat classification head. So that's a type of transfer learning",
    "start": "1955880",
    "end": "1961260"
  },
  {
    "text": "'cause you're still changing\nthe output layers of the model. And then continued pretraining",
    "start": "1962810",
    "end": "1969950"
  },
  {
    "text": "is a really cool technique you can use when you have unsupervised data.",
    "start": "1969950",
    "end": "1975649"
  },
  {
    "text": "So when you have a couple tens of GBs, maybe more of data, on disc,",
    "start": "1975650",
    "end": "1983120"
  },
  {
    "text": "you can take your neural network, take your foundation model",
    "start": "1983120",
    "end": "1989210"
  },
  {
    "text": "and then continue to pretrain that. And so what that means is if you're doing like a\ncausal language modeling loss",
    "start": "1989210",
    "end": "1996380"
  },
  {
    "text": "where it's just predicting the tokens and you run that on your data, it will learn how to predict the tokens",
    "start": "1996380",
    "end": "2003150"
  },
  {
    "text": "in your dataset really well. It will learn the syntax of your dataset, it will learn the styles,\nthe characteristic.",
    "start": "2003150",
    "end": "2011650"
  },
  {
    "text": "And so when you do that\nunsupervised pretraining or continued pretraining, you might do that for like one epoch",
    "start": "2011650",
    "end": "2019420"
  },
  {
    "text": "on a couple tens of GBs, like you wouldn't do that on terabytes. Once you have terabytes,",
    "start": "2019420",
    "end": "2024940"
  },
  {
    "text": "then you're looking at\npretraining from scratch, which is the next class. But for continued pretraining,",
    "start": "2024940",
    "end": "2031510"
  },
  {
    "text": "yeah, you'll do a couple\nsteps of domain adaptation.",
    "start": "2031510",
    "end": "2036510"
  },
  {
    "text": "And then once you've done this,\nyou can then fine-tune it. So then you can fine-tune it for different downstream use cases",
    "start": "2037210",
    "end": "2043990"
  },
  {
    "text": "and it will know your data very well. Like, once you've done both, continued pretraining\nor domain adaptation,",
    "start": "2043990",
    "end": "2050590"
  },
  {
    "text": "and like some small scale fine-tuning, that's gonna be a really powerful model. But it's hard, right?",
    "start": "2050590",
    "end": "2056165"
  },
  {
    "text": "There are more datasets you need",
    "start": "2056165",
    "end": "2060380"
  },
  {
    "text": "and a little bit more skill. So there's a spectrum of\nfine-tuning techniques.",
    "start": "2061270",
    "end": "2065503"
  },
  {
    "text": "And so parameter-efficient fine-tuning, again, is a new library from\nour friends at Hugging Face",
    "start": "2066580",
    "end": "2072340"
  },
  {
    "text": "which lets you fine-tune\nonly a few parameters. So literally, let's say\nyou have this massive",
    "start": "2072340",
    "end": "2079149"
  },
  {
    "text": "175 billion parameter model. This thing is multiple\nhundreds of gigs on disc.",
    "start": "2079150",
    "end": "2085570"
  },
  {
    "text": "You need like at least 16 GPUs\nto hold this thing in memory, maybe more, or accelerators,",
    "start": "2085570",
    "end": "2092770"
  },
  {
    "text": "and you don't want to\ntrain this whole thing because you need a massive\ndataset to do that,",
    "start": "2092770",
    "end": "2099369"
  },
  {
    "text": "and then you have to store the weights, and it's a big pain in the butt. And so what you can do is use LoRA",
    "start": "2099370",
    "end": "2106883"
  },
  {
    "text": "to inject trainable weights. So LoRA is actually gonna,\nwhich is a part of Hugging Face,",
    "start": "2107080",
    "end": "2112720"
  },
  {
    "text": "is actually gonna inject\nthese trainable weights into this model and then\nfine-tune just those weights.",
    "start": "2112720",
    "end": "2120369"
  },
  {
    "text": "So it saves you from needing\nto train this massive model because you can just fine-tune it",
    "start": "2120370",
    "end": "2125770"
  },
  {
    "text": "in this super parameter-efficient way. Here's a view of doing that, from our friends at Hugging Face.",
    "start": "2125770",
    "end": "2132400"
  },
  {
    "text": "So again, you're gonna import from this transformers library, import this AutoModel\nfor sequence to sequence",
    "start": "2132400",
    "end": "2138970"
  },
  {
    "text": "and then peft, yeah. So transformers is, of course,\nthe Hugging Face library. Peft stands for\nparameter-efficient fine-tuning",
    "start": "2138970",
    "end": "2147579"
  },
  {
    "text": "and it's a new library from Hugging Face. And so from this peft,\nwe're gonna get the config,",
    "start": "2147580",
    "end": "2153640"
  },
  {
    "text": "the model, set LoRA, and the TaskType. And then here we can point to bigscience",
    "start": "2153640",
    "end": "2158980"
  },
  {
    "text": "or pointing to their bigscience\nblue model, mt0-large. You want both the model\nname and the tokenizer.",
    "start": "2158980",
    "end": "2166900"
  },
  {
    "text": "And then we'll set this LoraConfig. So we have our LoraConfig, we'll set the TaskType,\nsequence to sequence.",
    "start": "2166900",
    "end": "2173740"
  },
  {
    "text": "So we're doing possibly some\ntype of translation here. And then inference mode,\nLoRA, and then this dropout,",
    "start": "2173740",
    "end": "2182050"
  },
  {
    "text": "some tuneable parameters. We load the model. So we're gonna take this\nwhole model from pretrained",
    "start": "2182050",
    "end": "2189050"
  },
  {
    "text": "and then we're gonna pass this model that we picked up from pretrained, which, again, the pretrained",
    "start": "2190120",
    "end": "2196750"
  },
  {
    "text": "that can point to the Hugging Face Hub, so that's just literally\ndownloading from the hub,",
    "start": "2196750",
    "end": "2202780"
  },
  {
    "text": "which is gonna be slow if\nit's a really big model, or maybe you already downloaded it",
    "start": "2202780",
    "end": "2208480"
  },
  {
    "text": "and so it's just sitting locally. And so you just pass this local path. So you pick it up and then,\nyeah, you've got this,",
    "start": "2208480",
    "end": "2215770"
  },
  {
    "text": "then you're gonna pass the model to this peft function, right? So get peft model, which we\nimported from their library,",
    "start": "2215770",
    "end": "2224440"
  },
  {
    "text": "we're gonna load that in here,\nwe pass it with the config. So it takes both the model and the config",
    "start": "2224440",
    "end": "2231910"
  },
  {
    "text": "and then you can see which\nparameters we're gonna train. So get trainable parameters. So here we are out of the,\nall of the parameters, right,",
    "start": "2231910",
    "end": "2241290"
  },
  {
    "text": "so that's about 1.2\nbillion parameters, right, 'cause we have 3, 6, 9 spots,\nabout 1.2 billion parameters.",
    "start": "2243730",
    "end": "2251970"
  },
  {
    "text": "And then of those we're\ntraining, three, six, we're training about\n2.3 million parameters,",
    "start": "2252370",
    "end": "2259090"
  },
  {
    "text": "which is just under 20% of the model. So that's an extremely\nefficient fine-tuning.",
    "start": "2259090",
    "end": "2264280"
  },
  {
    "text": "All right, let's check it out in a demo. So in this notebook, we are fine-tuning",
    "start": "2264280",
    "end": "2271480"
  },
  {
    "text": "a GPT-J six billion parameter model. And actually we're gonna use\nthe SEC filings from Amazon,",
    "start": "2271480",
    "end": "2279360"
  },
  {
    "text": "believe it or not, and we're\ngonna fine-tune this model to better generate natural language",
    "start": "2280450",
    "end": "2286810"
  },
  {
    "text": "that looks like SEC filings. And so same as last time, the notebook is available\nin the SageMaker Examples.",
    "start": "2286810",
    "end": "2296050"
  },
  {
    "text": "SageMaker Examples, that's right here. Intro to Amazon algos, Jumpstart,",
    "start": "2296050",
    "end": "2301059"
  },
  {
    "text": "and then we're gonna run this guy. As before, here's your Bitly,",
    "start": "2301060",
    "end": "2306100"
  },
  {
    "text": "which you can use if that's easier. Alternatively, feel\nfree to use the QR code.",
    "start": "2306100",
    "end": "2311950"
  },
  {
    "text": "So with that, let's get movin'.",
    "start": "2311950",
    "end": "2314407"
  },
  {
    "text": "And so, believe it or not, I do have this notebook\nup and running myself. And so this is, again, as\nusual on SageMaker Studio.",
    "start": "2317440",
    "end": "2326466"
  },
  {
    "text": "I've got all sorts of\ninstances running today. So I've got CPU machine, we got a c5 here.",
    "start": "2326466",
    "end": "2334230"
  },
  {
    "text": "You know, I've started\nto see it more common to have notebooks run on m5s.",
    "start": "2335620",
    "end": "2341601"
  },
  {
    "text": "So m5.large, m5.8. I think the small machines,\nlike your t3.medium, it can't always bear the weight",
    "start": "2341601",
    "end": "2350290"
  },
  {
    "text": "of all the compute you're trying to run. So just starting with\nsomething a little bit larger",
    "start": "2350290",
    "end": "2355360"
  },
  {
    "text": "like the m5 seems to be a good bet. So in any case, so we're on studio",
    "start": "2355360",
    "end": "2360440"
  },
  {
    "text": "and we're using our\nSageMaker JumpStart as usual because all the images are prebuilt",
    "start": "2361450",
    "end": "2368230"
  },
  {
    "text": "and we get to interact\nwith it very easily. Now, this notebook is\nnice for a couple reasons.",
    "start": "2368230",
    "end": "2373840"
  },
  {
    "text": "It has a clear before and after. And so the before is simply\nlooking at the model.",
    "start": "2373840",
    "end": "2381990"
  },
  {
    "text": "So we're gonna pick a GPT-J\nsix billion parameter model and then we're gonna host this model.",
    "start": "2383950",
    "end": "2391690"
  },
  {
    "text": "And then through hosting, we\nwill see how well it performs.",
    "start": "2391690",
    "end": "2396690"
  },
  {
    "text": "So we'll get some sort of\nbase performance on that, and then we'll fine-tune it",
    "start": "2397270",
    "end": "2403900"
  },
  {
    "text": "and then we'll see how well\nit does on the same prompts. And the responses is pretty wild actually.",
    "start": "2403900",
    "end": "2411460"
  },
  {
    "text": "It's obviously much better\nafter it's been fine-tuned than it was before.",
    "start": "2411460",
    "end": "2416560"
  },
  {
    "text": "So that should be good to see. And then on top of that, we're gonna do hyperparameter\ntuning actually.",
    "start": "2416560",
    "end": "2422650"
  },
  {
    "text": "So there's hour-long job in this notebook that tunes this model using\na variety of hyperparameters,",
    "start": "2422650",
    "end": "2431065"
  },
  {
    "text": "and it turns out really well. So, let's explore this. So recent SageMaker Python SDK, et cetera,",
    "start": "2431065",
    "end": "2439930"
  },
  {
    "text": "importing this name from\nthe base, life is good. Picking up the URIs. Retrieve the model URI.",
    "start": "2439930",
    "end": "2446212"
  },
  {
    "text": "So what's happening here is basically we are telling SageMaker",
    "start": "2446212",
    "end": "2451480"
  },
  {
    "text": "from the name of this\nmodel, from this model ID, which is this JumpStart example,\nwe grab the model image,",
    "start": "2451480",
    "end": "2460830"
  },
  {
    "text": "so basically we can point to like the JumpStart SDK basically.",
    "start": "2462490",
    "end": "2467443"
  },
  {
    "text": "Or through the SDK, we\npoint to these JumpStart sort of prebuilt examples,\nthe prebuilt images,",
    "start": "2468700",
    "end": "2476170"
  },
  {
    "text": "and then we're just gonna retrieve those. So we get the model URI\nusing this .retrieve,",
    "start": "2476170",
    "end": "2482760"
  },
  {
    "text": "and then we'll do the\nsame, so that's the URI, that's actually the image that\nwe'll use to host the model.",
    "start": "2482830",
    "end": "2490090"
  },
  {
    "text": "And that came from the model\nID and then the version. And then there's a scope\nhere, which is inference.",
    "start": "2490090",
    "end": "2496750"
  },
  {
    "text": "So this means you can point to JumpStart to just get the hosted\nversion of this model.",
    "start": "2496750",
    "end": "2503050"
  },
  {
    "text": "Alternatively, you can do that for just the training\nversion of the model. And so we'll create this SageMaker model.",
    "start": "2503050",
    "end": "2509680"
  },
  {
    "text": "You've probably seen this\nbefore, the image model data,",
    "start": "2509680",
    "end": "2514680"
  },
  {
    "text": "which should be in S3, ah, my apologies. So yeah, so the deploy\nimage URI is the script.",
    "start": "2514690",
    "end": "2521497"
  },
  {
    "text": "And the model URI, that's the\nactual data for the model. That's like the model weights, right?",
    "start": "2521497",
    "end": "2528670"
  },
  {
    "text": "AWS role, and then the\npredictor class that we want, and then the endpoint name.",
    "start": "2528670",
    "end": "2534010"
  },
  {
    "text": "And then we create this. Fortunately for you,\nI've already done that. So we can check this out over here.",
    "start": "2534010",
    "end": "2541180"
  },
  {
    "text": "Actually, so we're gonna go up to the Home and we'll go down and\nwe'll look at Deployments",
    "start": "2541180",
    "end": "2548950"
  },
  {
    "text": "and we'll click on Endpoints. So I have quite a few endpoints.",
    "start": "2548950",
    "end": "2555580"
  },
  {
    "text": "I was running on some Inferentia2\nwith Stable Diffusion. And this is the JumpStart\nexample, which we can look at.",
    "start": "2555580",
    "end": "2565210"
  },
  {
    "text": "Yep, so Hugging Face text generation. Interesting. All sorts\nof other metrics for us.",
    "start": "2565210",
    "end": "2572293"
  },
  {
    "text": "Great. So we'll go back out here.",
    "start": "2574330",
    "end": "2578383"
  },
  {
    "text": "Now we're gonna query this endpoint and then we'll walk through the response.",
    "start": "2579430",
    "end": "2583663"
  },
  {
    "text": "So we're gonna query the\nendpoint with this JSON payload and then parse the response back.",
    "start": "2585940",
    "end": "2592390"
  },
  {
    "text": "So we'll set parameters. Max length, number of return sequences.",
    "start": "2592390",
    "end": "2597613"
  },
  {
    "text": "And then here are the prompts basically. So we have three prompts,",
    "start": "2599170",
    "end": "2603433"
  },
  {
    "text": "and we want to see what a generic GPT-J",
    "start": "2604780",
    "end": "2609780"
  },
  {
    "text": "six billion parameter\nmodel comes back with.",
    "start": "2609970",
    "end": "2612553"
  },
  {
    "text": "And it's not pretty, right? (chuckles) The intention is that\nthis is a garbled mess.",
    "start": "2615700",
    "end": "2622390"
  },
  {
    "text": "Like, this makes no sense. This is clearly not a high-quality output.",
    "start": "2622390",
    "end": "2628330"
  },
  {
    "text": "And so at this point,\nyou're thinking like, \"What is this model about? Why do I care about this?\"",
    "start": "2628330",
    "end": "2634450"
  },
  {
    "text": "What's shocking is that, by the time we get to\nthe end of this notebook,",
    "start": "2634450",
    "end": "2639670"
  },
  {
    "text": "the quality that this\nreturns is amazingly higher, like it's so much better\nthan it was previously.",
    "start": "2639670",
    "end": "2648190"
  },
  {
    "text": "And so we'll actually\nshow you how to do that in this notebook.",
    "start": "2648190",
    "end": "2651523"
  },
  {
    "text": "All right, so now we're gonna fine-tune that pretrained model\non this new SEC dataset.",
    "start": "2653770",
    "end": "2659922"
  },
  {
    "text": "Actually, I like this 'cause this is actually unlabeled data.",
    "start": "2663100",
    "end": "2667153"
  },
  {
    "text": "Actually, I'm not positive about that. Let's see if we can figure this out.",
    "start": "2670930",
    "end": "2674410"
  },
  {
    "text": "So this is a sample of the\ndataset itself, the text file. This appears to be unlabeled.",
    "start": "2679420",
    "end": "2686470"
  },
  {
    "text": "So my expectation is\nthat we're actually doing a domain adaptation, as they call it. So in NLP, we actually mean\nthis as a continued pretraining.",
    "start": "2686470",
    "end": "2696450"
  },
  {
    "text": "So this image is going\nto continue pretraining the GPT-J six billion parameter\nmodel on our SEC data.",
    "start": "2699730",
    "end": "2708960"
  },
  {
    "text": "And so that's why we can do this without explicitly needing to label the data.",
    "start": "2709210",
    "end": "2715240"
  },
  {
    "text": "We don't need to organize it into",
    "start": "2715240",
    "end": "2719150"
  },
  {
    "text": "a positive and negative sentiment, or questions and answers, or really any of the low-level labels.",
    "start": "2720880",
    "end": "2729250"
  },
  {
    "text": "We can just take the\nraw data just as it is and then do pretraining directly.",
    "start": "2729250",
    "end": "2736300"
  },
  {
    "text": "And so we'll do that continued pretraining in this notebook here. So we'll retrieve the training artifacts.",
    "start": "2736300",
    "end": "2743829"
  },
  {
    "text": "So this is the training image. All right.",
    "start": "2743830",
    "end": "2748543"
  },
  {
    "text": "So we got this, then we're\ngonna set the parameters. And then, yeah, two types of parameters.",
    "start": "2751810",
    "end": "2759280"
  },
  {
    "text": "So one type of parameter\nis for the training job, which is obviously basic things",
    "start": "2759280",
    "end": "2764740"
  },
  {
    "text": "like the training data path, output path. And then for the algorithm,",
    "start": "2764740",
    "end": "2770380"
  },
  {
    "text": "we have hyperparameters for that as usual. So your data bucket, prefix,\nS3 paths, life is good.",
    "start": "2770380",
    "end": "2778020"
  },
  {
    "text": "And now we're gonna set\nsome hyperparameters here. So per device train batch size, hmm.",
    "start": "2779980",
    "end": "2788319"
  },
  {
    "text": "So this is telling us the number of batch\nsizes per GPU actually.",
    "start": "2788320",
    "end": "2794410"
  },
  {
    "text": "So per accelerator in the training cluster that's the number of objects we'll use.",
    "start": "2794410",
    "end": "2801369"
  },
  {
    "text": "And then this is nice. Yeah, so we can grab all of\nthe default hyperparameters",
    "start": "2801370",
    "end": "2808460"
  },
  {
    "text": "for this model actually just\nby retrieving the default. That's really handy, very cool.",
    "start": "2809560",
    "end": "2816340"
  },
  {
    "text": "So we'll grab the defaults and\nlet's take a look at these. So we have epoch, learning\nrate, per device, warmup.",
    "start": "2816340",
    "end": "2825460"
  },
  {
    "text": "instruction-tuned: False. Good to know. So this base model has not\nbeen instruction-tuned.",
    "start": "2825460",
    "end": "2832753"
  },
  {
    "text": "Train from scratch: False, interesting.",
    "start": "2834190",
    "end": "2836713"
  },
  {
    "text": "I'm curious about that. All right, and now we're gonna\nuse automatic model tuning.",
    "start": "2839290",
    "end": "2847093"
  },
  {
    "text": "So here we're gonna update\nAMT in just a second here.",
    "start": "2848320",
    "end": "2853320"
  },
  {
    "text": "And then we have the\nrange of hyperparameters, maximum jobs is six, parallel jobs is two,",
    "start": "2857290",
    "end": "2863863"
  },
  {
    "text": "and then we'll train this. So we're setting the estimator,\nall the rest of our configs.",
    "start": "2866200",
    "end": "2871660"
  },
  {
    "text": "Here's this training\nimage, source directory.",
    "start": "2871660",
    "end": "2876103"
  },
  {
    "text": "Entry point script is out here.",
    "start": "2876970",
    "end": "2879973"
  },
  {
    "text": "So that's in, yeah, here. So we're in the SageMaker Examples still,",
    "start": "2882670",
    "end": "2887593"
  },
  {
    "text": "and then it's in Introduction\nto Amazon algorithms. So out here.",
    "start": "2888490",
    "end": "2892392"
  },
  {
    "text": "And then, where are we again? There we go, JumpStart foundation\nmodels, domain adaptation.",
    "start": "2895150",
    "end": "2903519"
  },
  {
    "text": "So JumpStart foundation models.",
    "start": "2903520",
    "end": "2908520"
  },
  {
    "text": "Here we go. And then this guy.",
    "start": "2909970",
    "end": "2914233"
  },
  {
    "text": "So where is this transfer\nlearning script, I wonder.",
    "start": "2917500",
    "end": "2922303"
  },
  {
    "text": "I am not certain about this.",
    "start": "2925300",
    "end": "2927103"
  },
  {
    "text": "Train source URI. Let's see if we can unpack this.",
    "start": "2931330",
    "end": "2936550"
  },
  {
    "text": "So train source URI is in S3.",
    "start": "2936550",
    "end": "2941550"
  },
  {
    "text": "Ah, interesting, okay. So JumpStart actually has this\ntraining script stored in S3.",
    "start": "2943960",
    "end": "2951810"
  },
  {
    "text": "So basically, it's completely abstracted, we don't have to touch\nit in the slightest.",
    "start": "2952570",
    "end": "2958210"
  },
  {
    "text": "It's already ready to\ngo, which is pretty cool. All right, great.",
    "start": "2958210",
    "end": "2963549"
  },
  {
    "text": "So that's where this\ntransfer learning script is. It's parked in the JumpStart S3 bucket.",
    "start": "2963550",
    "end": "2968233"
  },
  {
    "text": "And then this is our instance. Let's see what our\ntraining instance type is.",
    "start": "2969640",
    "end": "2974740"
  },
  {
    "text": "Maybe that was listed\nabove and I just missed it. So the training instance type is a g5.",
    "start": "2974740",
    "end": "2981250"
  },
  {
    "text": "Okay, so a couple GPUs, nothing crazy.",
    "start": "2981250",
    "end": "2983863"
  },
  {
    "text": "And then we have this\nhyperparameter tuner. Hmm, it's surprising that\nthis still took an hour.",
    "start": "2987490",
    "end": "2994570"
  },
  {
    "text": "I thought this was using AMT. Yeah, okay, so it still takes an hour even without using automatic model tuning.",
    "start": "2994570",
    "end": "3002100"
  },
  {
    "text": "So that must be because it's actually fine-tuning the entire model. It's not even using like a\nparameter-efficient fine-tuning,",
    "start": "3002100",
    "end": "3011400"
  },
  {
    "text": "which is surprising, certainly. But don't get me wrong,\nthe results are great.",
    "start": "3011400",
    "end": "3018300"
  },
  {
    "text": "So you'll be happy once we get there. So we call 'em model .fit.",
    "start": "3018300",
    "end": "3023192"
  },
  {
    "text": "And then the job runs. So remember, while your job is running,",
    "start": "3025200",
    "end": "3029223"
  },
  {
    "text": "you can right click on this and then enable scrolling for outputs.",
    "start": "3030300",
    "end": "3036150"
  },
  {
    "text": "And then that makes it much less likely for your notebook to crash because then everything\nis sort of nicely tucked",
    "start": "3036150",
    "end": "3045299"
  },
  {
    "text": "into the scrolling bar. Yeah, there we go, great. So remember to do that.",
    "start": "3045300",
    "end": "3051303"
  },
  {
    "text": "And here, so let's look at the\njob in the console actually.",
    "start": "3053280",
    "end": "3058023"
  },
  {
    "text": "Zoom out for you. So in the console, yeah,\nhere is this one job we ran.",
    "start": "3059610",
    "end": "3066770"
  },
  {
    "text": "So one JumpStart text generation model",
    "start": "3069870",
    "end": "3074870"
  },
  {
    "text": "still took a solid hour and\nthis was really just one job.",
    "start": "3075030",
    "end": "3079323"
  },
  {
    "text": "So this was an hour for this one g5.12 to churn through our dataset.",
    "start": "3081840",
    "end": "3088620"
  },
  {
    "text": "Let's see where the data is actually. Hmm, okay, also in the JumpStart bucket.",
    "start": "3088620",
    "end": "3097563"
  },
  {
    "text": "Okay, interesting. All right, so in any case,",
    "start": "3099840",
    "end": "3105660"
  },
  {
    "text": "you can run this through\nusing automatic model tuning or you can run it without\nautomatic model tuning.",
    "start": "3105660",
    "end": "3110970"
  },
  {
    "text": "Both are fine. And SageMaker has this nice\ntraining job analytics.",
    "start": "3110970",
    "end": "3119180"
  },
  {
    "text": "So after you've listed the job name,",
    "start": "3119730",
    "end": "3123903"
  },
  {
    "text": "we can load up a data frame actually with the analytics for\nthat job, which is nice.",
    "start": "3125040",
    "end": "3132333"
  },
  {
    "text": "And so in this one, we'll\npick the best model. Actually it's the best\none that's loaded here.",
    "start": "3133680",
    "end": "3141809"
  },
  {
    "text": "Notably, it was just one job. But in any case, we're gonna deploy this. So now we'll deploy onto another g5.",
    "start": "3141810",
    "end": "3149132"
  },
  {
    "text": "And we'll use the same software framework we've been using previously. So images, image.URI we're gonna retrieve.",
    "start": "3150180",
    "end": "3157922"
  },
  {
    "text": "Skipping the region of the framework because we'll pass the\nmodel ID, the model version,",
    "start": "3159120",
    "end": "3165630"
  },
  {
    "text": "and then the instance type, especially CPU or GPU matters, and then it, of course, matters",
    "start": "3165630",
    "end": "3171960"
  },
  {
    "text": "for what accelerators you're\nusing and things like this. And then the scope, so\ninference versus training.",
    "start": "3171960",
    "end": "3179730"
  },
  {
    "text": "We'll get a new endpoint name, and then we'll create\nthis fine-tuned predictor.",
    "start": "3179730",
    "end": "3186513"
  },
  {
    "text": "So we create this fine-tuned predictor and then we can validate\nthat that indeed exists.",
    "start": "3188070",
    "end": "3195660"
  },
  {
    "text": "That's out here, was it? Here we go, up in the top\nwith the little house.",
    "start": "3195660",
    "end": "3202049"
  },
  {
    "text": "We'll click on Endpoints. Let me close that out.",
    "start": "3202050",
    "end": "3207903"
  },
  {
    "text": "And then this should say,\noh, I guess there are two. Yeah, JumpStart one, JumpStart two.",
    "start": "3209010",
    "end": "3215070"
  },
  {
    "text": "Okay, perfect. Great. So we have two JumpStart models.",
    "start": "3215070",
    "end": "3220350"
  },
  {
    "text": "And then now we'll interact with this. So we'll query the endpoint\nwith the JSON payload",
    "start": "3220350",
    "end": "3226470"
  },
  {
    "text": "and then parse the responses. So we have our same parameters,\ntop k, top p, temperature,",
    "start": "3226470",
    "end": "3234230"
  },
  {
    "text": "and the same prompts. So, \"This 10-K form, This form 10-K report shows\nthat we serve customers through,",
    "start": "3237330",
    "end": "3245670"
  },
  {
    "text": "our vision is,\" et cetera. So we're gonna send those\nprompts to the model",
    "start": "3245670",
    "end": "3250740"
  },
  {
    "text": "and then, bam, (chuckles)\ncheck this thing out. So...",
    "start": "3250740",
    "end": "3255170"
  },
  {
    "text": "I mean, this looks so good.",
    "start": "3258600",
    "end": "3261183"
  },
  {
    "text": "This looks so much like\nthe SEC reported data.",
    "start": "3265830",
    "end": "3270830"
  },
  {
    "text": "It's amazing. It's so detailed.",
    "start": "3270870",
    "end": "3275433"
  },
  {
    "text": "Wow, it's incredible. Yeah, so this is the difference.",
    "start": "3278130",
    "end": "3285270"
  },
  {
    "text": "Remember, we saw this, and then here they have it in this",
    "start": "3285270",
    "end": "3290610"
  },
  {
    "text": "nice little data frame for you. So these are the prompts.",
    "start": "3290610",
    "end": "3294663"
  },
  {
    "text": "And then this is before fine-tuning, which is this garbled mess.",
    "start": "3296490",
    "end": "3302820"
  },
  {
    "text": "And then after fine-tuning, we get this extremely clean response.",
    "start": "3302820",
    "end": "3308700"
  },
  {
    "text": "And so in this notebook,\nwe see that fine-tuning",
    "start": "3308700",
    "end": "3313700"
  },
  {
    "text": "even a very small model, so fine-tuning this GPT-J\nsix billion parameter model",
    "start": "3314220",
    "end": "3321000"
  },
  {
    "text": "on a extremely specific dataset,",
    "start": "3321000",
    "end": "3325030"
  },
  {
    "text": "such as the SEC filings for Amazon, can generate really powerful results.",
    "start": "3326220",
    "end": "3333810"
  },
  {
    "text": "And so with that, I hope\nyou enjoyed this video.",
    "start": "3333810",
    "end": "3337893"
  },
  {
    "text": "In the next one, we are\ngoing to take a look",
    "start": "3338970",
    "end": "3343970"
  },
  {
    "text": "at pretraining foundation models actually. So in the next video, we're\ngonna learn, the first part,",
    "start": "3344040",
    "end": "3351690"
  },
  {
    "text": "what you need to pretrain\na new foundation model, it's like a part one;\nand then the part two is,",
    "start": "3351690",
    "end": "3358473"
  },
  {
    "text": "yeah, going even deeper and preparing data and then training at scale. So I will see you there. Thanks.",
    "start": "3359310",
    "end": "3366243"
  }
]