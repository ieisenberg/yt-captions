[
  {
    "start": "0",
    "end": "0"
  },
  {
    "text": "in the past decade we've seen tremendous growth in the in the collaborative",
    "start": "1460",
    "end": "7200"
  },
  {
    "text": "research taking place on the cloud and even these quotes that are listed here are a few years old at this point and",
    "start": "7200",
    "end": "13620"
  },
  {
    "text": "now what's becoming sort of the newest normal is to have you know almost all",
    "start": "13620",
    "end": "20250"
  },
  {
    "text": "your kind of collaborative research take place on the cloud as opposed to just using it to meet in the middle or anything and really the factors that are",
    "start": "20250",
    "end": "27570"
  },
  {
    "text": "behind this are the you know both technical and sort of scientific in the technical sense we now have the ability",
    "start": "27570",
    "end": "34050"
  },
  {
    "text": "to do these things where we didn't before and I'm sure most of you or all of you can remember you know the days",
    "start": "34050",
    "end": "40200"
  },
  {
    "text": "past when we were primarily sending hard drives through the mail and you know even just spreadsheets as we you know",
    "start": "40200",
    "end": "46100"
  },
  {
    "text": "tried to trade scientific data but",
    "start": "46100",
    "end": "51289"
  },
  {
    "text": "you know sorry I'm feeding back and I want to blow anyone's ears here but",
    "start": "51289",
    "end": "58910"
  },
  {
    "text": "in addition to that we now have you know other factors taking place which is that",
    "start": "58910",
    "end": "65309"
  },
  {
    "text": "the large amount of scientific problems that are being addressed today are so large that they tend to exceed the scale",
    "start": "65309",
    "end": "72630"
  },
  {
    "text": "or ability of any kind of one institution can muster and so what you find is a lot more institutions taking",
    "start": "72630",
    "end": "78509"
  },
  {
    "text": "advantage of the fact that they can collaborate much more easily and then you know actually doing so well on the",
    "start": "78509",
    "end": "84630"
  },
  {
    "text": "cloud and you know obviously we hope that AWS is that home for all your",
    "start": "84630",
    "end": "91110"
  },
  {
    "start": "85000",
    "end": "85000"
  },
  {
    "text": "collaborative scientific research needs and in the years it's evolved from you know a simple on-site kind of model like",
    "start": "91110",
    "end": "98369"
  },
  {
    "text": "I said trading hard drives keeping you know all kinds of high powered workstations under your desk and things",
    "start": "98369",
    "end": "103380"
  },
  {
    "text": "like that this sort of shadow IT that was built around any type of scientific research effort or even any type of",
    "start": "103380",
    "end": "109079"
  },
  {
    "text": "collaboration that was taking place and it's grown to actually become",
    "start": "109079",
    "end": "114950"
  },
  {
    "text": "you know what we started to do was meet in the middle right so you know that",
    "start": "114950",
    "end": "120930"
  },
  {
    "text": "means using s3 to trade data instead of sending hard drives and using common",
    "start": "120930",
    "end": "126030"
  },
  {
    "text": "compute platforms instead of and you know made available that way instead of",
    "start": "126030",
    "end": "131129"
  },
  {
    "text": "dressed trying to you know work individually on where stations that everybody had set up on their own and so now what we're seeing is this you know",
    "start": "131129",
    "end": "138450"
  },
  {
    "text": "growth into what we call DevOps but really it is just sort of a loose name",
    "start": "138450",
    "end": "144000"
  },
  {
    "text": "for you know the coordinated development and deployment of common applications and common environments and common",
    "start": "144000",
    "end": "149850"
  },
  {
    "text": "platforms so that these collaborative efforts are actually taking place on a common you know and within a common",
    "start": "149850",
    "end": "156030"
  },
  {
    "text": "framework so today what we have is a review of a couple of those architectures for doing collaborative",
    "start": "156030",
    "end": "162480"
  },
  {
    "text": "research brought to you by Ryan and Steven from H Li and Lance from Celgene and they're",
    "start": "162480",
    "end": "170010"
  },
  {
    "text": "going to kind of go over the what the what they've set up and the general you know architecture that they've deployed",
    "start": "170010",
    "end": "175790"
  },
  {
    "text": "so",
    "start": "175790",
    "end": "178790"
  },
  {
    "text": "ok you guys hear me okay",
    "start": "184400",
    "end": "189680"
  },
  {
    "text": "well we got to go to we lost my original slide so my name is Ryan you LASEK and this is",
    "start": "190490",
    "end": "199470"
  },
  {
    "text": "Stephen Terrill and we're from a company called human longevity and the title of our talk is building a platform for",
    "start": "199470",
    "end": "205230"
  },
  {
    "text": "collaborative scientific research on AWS so some of the topics we're gonna cover",
    "start": "205230",
    "end": "210480"
  },
  {
    "text": "today about human longevity our company some of the challenges that we face",
    "start": "210480",
    "end": "215900"
  },
  {
    "text": "solutions that we came up with the journey to get there and then some",
    "start": "215900",
    "end": "221250"
  },
  {
    "text": "closing thoughts",
    "start": "221250",
    "end": "223730"
  },
  {
    "start": "226000",
    "end": "226000"
  },
  {
    "text": "so this is a chart from the CDC and it describes life expectancy over three",
    "start": "226610",
    "end": "232800"
  },
  {
    "text": "different groups or three different generations and the first shift you see from the blue to the yellow line is",
    "start": "232800",
    "end": "239120"
  },
  {
    "text": "an improvement in life expectancy that was due to the eradication of infectious",
    "start": "239120",
    "end": "244170"
  },
  {
    "text": "disease in the early 20th century through antibiotics and vaccines in the",
    "start": "244170",
    "end": "250200"
  },
  {
    "text": "second shift you see the yellow to the red line is an improvement in life",
    "start": "250200",
    "end": "255210"
  },
  {
    "text": "expectancy due to better health care better practice of medicine improvements",
    "start": "255210",
    "end": "260400"
  },
  {
    "text": "in sanitation and improvements in the economy and so the big challenge of the",
    "start": "260400",
    "end": "265800"
  },
  {
    "text": "21st century healthcare system is to continue this progress through managing the progression of",
    "start": "265800",
    "end": "274080"
  },
  {
    "text": "chronic disease across the population so HL eyes mission is",
    "start": "274080",
    "end": "280970"
  },
  {
    "text": "to change the practice of medicine making it more preventive",
    "start": "280970",
    "end": "286520"
  },
  {
    "text": "predictive personalized and genomics-based with the goal of empowering individuals",
    "start": "286520",
    "end": "293729"
  },
  {
    "text": "to manage the progression of chronic disease and have healthier fuller lives",
    "start": "293729",
    "end": "300198"
  },
  {
    "text": "and so in order to do that we need to move beyond medicine as clinical science",
    "start": "302539",
    "end": "307889"
  },
  {
    "start": "303000",
    "end": "303000"
  },
  {
    "text": "to medicine as data science if you look out of our current form of medicine our",
    "start": "307889",
    "end": "313440"
  },
  {
    "text": "descriptive form of medicine we don't really collect that much data on us 8 as",
    "start": "313440",
    "end": "318570"
  },
  {
    "text": "a patient so maybe laboratory results or medical reports about three and a half",
    "start": "318570",
    "end": "323639"
  },
  {
    "text": "gigabytes of data in total so our our first step with our health nucleus clinics was to start doing deep",
    "start": "323639",
    "end": "330509"
  },
  {
    "text": "numerical analysis on all organ systems of people going through the clinic and capturing a lot of different data so we",
    "start": "330509",
    "end": "337979"
  },
  {
    "text": "do things like sequence your whole genome sequencing identifying your microbiome your",
    "start": "337979",
    "end": "344930"
  },
  {
    "text": "metabolize doing MRIs and we collect this body of",
    "start": "344930",
    "end": "351030"
  },
  {
    "text": "data about 150 gigabytes in total and create a digital you or a digital health profile now to understand that profile",
    "start": "351030",
    "end": "358740"
  },
  {
    "text": "it has to be understood against a background has to be put in context against a population of everyone ideally",
    "start": "358740",
    "end": "365340"
  },
  {
    "text": "and so to do that you need population studies and one such study was done at",
    "start": "365340",
    "end": "370919"
  },
  {
    "text": "human longevity on 10,000 people it was analysis of 10,000 people's blood",
    "start": "370919",
    "end": "377250"
  },
  {
    "text": "viral the viruses in their blood the idea was to get a background what's what's normal that so we can compare",
    "start": "377250",
    "end": "382860"
  },
  {
    "text": "these digital you these individuals against that background so that was done",
    "start": "382860",
    "end": "388020"
  },
  {
    "text": "that se was done at about a petabyte of data about a trillion similarity searches it was run over a course of",
    "start": "388020",
    "end": "395130"
  },
  {
    "text": "three weeks on ten m44 excels and the idea is that we need to be doing studies",
    "start": "395130",
    "end": "402750"
  },
  {
    "text": "like this but not on 10,000 people on tens of millions of people ultimately X bytes of data thousands of compute",
    "start": "402750",
    "end": "409710"
  },
  {
    "text": "instances and that's just one study but thousands or tens of thousands of study to create a knowledge base to actually",
    "start": "409710",
    "end": "416550"
  },
  {
    "text": "compare these individuals really quantify people's biology and and realize this vision of being able to",
    "start": "416550",
    "end": "422940"
  },
  {
    "text": "empower people to manage the progression of chronic disease throughout their life and so scale matters scale of the",
    "start": "422940",
    "end": "430460"
  },
  {
    "text": "storage of the data scale of compute scaling the platform being able to plug",
    "start": "430460",
    "end": "435900"
  },
  {
    "text": "in new analytical tools accommodate new data sets push the science board and take advantage of opportunities as they",
    "start": "435900",
    "end": "442500"
  },
  {
    "text": "present themselves to bring products to market so we'll talk about that platform piece today",
    "start": "442500",
    "end": "448730"
  },
  {
    "text": "so in realizing this vision one of the first key challenges we ran into as we scaled as a company who grew as a",
    "start": "449990",
    "end": "456300"
  },
  {
    "start": "450000",
    "end": "450000"
  },
  {
    "text": "company really quickly went from 0 to 300 people in two years and during that",
    "start": "456300",
    "end": "461610"
  },
  {
    "text": "time we sequenced about 35,000 whole genomes process them generate about 35",
    "start": "461610",
    "end": "466980"
  },
  {
    "text": "petabytes of data and one of the things we observed is that teams are building out different pipelines on different",
    "start": "466980",
    "end": "473520"
  },
  {
    "text": "platforms different ways with different technologies different approaches and that made it really difficult for people",
    "start": "473520",
    "end": "480540"
  },
  {
    "text": "to collaborate across teams to share tools across pipelines and the",
    "start": "480540",
    "end": "485870"
  },
  {
    "text": "bioinformaticians we're getting really bogged down in infrastructure and not able to focus on the science and we",
    "start": "485870",
    "end": "492780"
  },
  {
    "text": "accumulate a lot of redundant infrastructure that consume time and resources and some of these pipelines",
    "start": "492780",
    "end": "498900"
  },
  {
    "text": "are complex they could have complex workflow orchestration logic complex our diverse resource requirements across",
    "start": "498900",
    "end": "505500"
  },
  {
    "text": "steps within the pipeline and we also accumulated significant storage into PUE costs sometimes due to using on-demand",
    "start": "505500",
    "end": "512039"
  },
  {
    "text": "instances or underutilizing resources and finally we found it really",
    "start": "512040",
    "end": "517140"
  },
  {
    "text": "challenging to get these genomics pipelines in the production often because the the infrastructure they were",
    "start": "517140",
    "end": "524159"
  },
  {
    "text": "using was very different so the solution was to create a common",
    "start": "524160",
    "end": "530720"
  },
  {
    "start": "528000",
    "end": "528000"
  },
  {
    "text": "platform for genomics pipelines using AWS managed services the idea is to take",
    "start": "530720",
    "end": "536269"
  },
  {
    "text": "this platform and have a much simpler pipeline definition that we can use",
    "start": "536269",
    "end": "541910"
  },
  {
    "text": "across the organization and to optimize for cost at a platform level so everyone can benefit from that optimization and",
    "start": "541910",
    "end": "549259"
  },
  {
    "text": "move to continuous delivery model so that we're always a button push away from production so if somebody changes",
    "start": "549259",
    "end": "556310"
  },
  {
    "text": "the path of the platform or if this changes two pipelines or just changes to tools your button push away and with",
    "start": "556310",
    "end": "563060"
  },
  {
    "text": "automated quality gates that get more rigorous the closer you get to production because quality matters the",
    "start": "563060",
    "end": "568310"
  },
  {
    "text": "data has to be right people are making health decisions based on this analysis",
    "start": "568310",
    "end": "573339"
  },
  {
    "text": "and we also need to ease and accelerate this transition from R&D to production by having a common language common way",
    "start": "573339",
    "end": "580910"
  },
  {
    "text": "of defining pipelines so the journey to get there was",
    "start": "580910",
    "end": "587209"
  },
  {
    "start": "585000",
    "end": "585000"
  },
  {
    "text": "iterative for us and it had five key steps along the way the first was a new",
    "start": "587209",
    "end": "594230"
  },
  {
    "text": "customer that needed an exome report and they needed things like ancestry trait",
    "start": "594230",
    "end": "599810"
  },
  {
    "text": "predictions pharmacogenomic predictions at the time we're generating these reports manually and we needed to",
    "start": "599810",
    "end": "606470"
  },
  {
    "text": "automate that process and we needed it up and running in two weeks because we needed to start integrating other",
    "start": "606470",
    "end": "612139"
  },
  {
    "text": "systems around this one so our solution to this was simple we",
    "start": "612139",
    "end": "619730"
  },
  {
    "start": "616000",
    "end": "616000"
  },
  {
    "text": "used sqs and we would put sample messages into this queue and then we had very simple Python applications that",
    "start": "619730",
    "end": "626029"
  },
  {
    "text": "would pull that queue pull down a sample and then run a sequence of bioinformatics tools on that ec2",
    "start": "626029",
    "end": "631430"
  },
  {
    "text": "instance produced Jason results put those in s3 and there was downstream systems that would take that data and",
    "start": "631430",
    "end": "637670"
  },
  {
    "text": "create a PDF report and what we did is we took the bio from Maxwell's we baked them into the a.m. eyes themselves",
    "start": "637670",
    "end": "643730"
  },
  {
    "text": "that's how we deployed these tools into this environment then we use ops work to manage the instances",
    "start": "643730",
    "end": "651730"
  },
  {
    "text": "so here's an example of what the code looks like so on the right we have a pipeline definition that's in Jason and",
    "start": "651730",
    "end": "658009"
  },
  {
    "text": "what we have here is just one step it wants that pipeline what I've highlighted here is what we would do to",
    "start": "658009",
    "end": "664880"
  },
  {
    "text": "describe a step so you describe the name of the step maybe its inputs its command",
    "start": "664880",
    "end": "670850"
  },
  {
    "text": "outputs any file dependencies that you would depend on maybe a reference genome file and then the code was really simple",
    "start": "670850",
    "end": "677060"
  },
  {
    "text": "it's you pull a sample down from sqs at Q when you have that sample you're gonna",
    "start": "677060",
    "end": "683510"
  },
  {
    "text": "iterate over each step pull external file dependencies from s3 pull maybe VCF",
    "start": "683510",
    "end": "688970"
  },
  {
    "text": "files file from sample files from s3 and then run that bioinformatics tool",
    "start": "688970",
    "end": "694490"
  },
  {
    "text": "locally and remember they're baked into the ami so a pretty simple idea",
    "start": "694490",
    "end": "700480"
  },
  {
    "text": "one of the key benefits here is that the work really well as a starting point we were able to get up and running in two",
    "start": "700480",
    "end": "706490"
  },
  {
    "text": "weeks and get the project up and running and auto scaling and opsworks is really easy you",
    "start": "706490",
    "end": "713420"
  },
  {
    "text": "could scale on CPU load or it get scale on a cloud watch alarm in our case we scaled on queue depth so every time the",
    "start": "713420",
    "end": "719990"
  },
  {
    "text": "queue depth for the samples went up we would add bore nodes and every time the queue depth decrease we would remove the",
    "start": "719990",
    "end": "725660"
  },
  {
    "text": "nodes but there's some drawbacks so there's pain around manually building",
    "start": "725660",
    "end": "731000"
  },
  {
    "text": "and updating the ami everytime bioinformatics tool changed we had update the tools and then update the ami",
    "start": "731000",
    "end": "737000"
  },
  {
    "text": "and deploy it we realized we're building our own workflow engine which wasn't really what we wanted to be in the",
    "start": "737000",
    "end": "742579"
  },
  {
    "text": "business of doing we couldn't optimize for workload resources at each step",
    "start": "742579",
    "end": "747800"
  },
  {
    "text": "because we're taking a sample and running all the analysis all four or five tools on one instance so we",
    "start": "747800",
    "end": "753079"
  },
  {
    "text": "couldn't write size each step for the appropriate instance and we weren't taking advantage of spot we were using",
    "start": "753079",
    "end": "759500"
  },
  {
    "text": "on demand at the time so the next challenge that we faced was adapting the",
    "start": "759500",
    "end": "766940"
  },
  {
    "text": "tool change there are a lot of requests coming in to add traits to add pharmacogenomics",
    "start": "766940",
    "end": "773410"
  },
  {
    "text": "information into these reports and it was triggering a lot of tool changes which was trigging rebuilding these a.m.",
    "start": "773410",
    "end": "779209"
  },
  {
    "text": "eyes so we wanted a more flexible way of accommodating the tools so what we did is we moved migrated to docker and what",
    "start": "779209",
    "end": "786170"
  },
  {
    "text": "that allowed us to do is have bioinformaticians that could create a tool docker eyes it run it on their",
    "start": "786170",
    "end": "791600"
  },
  {
    "text": "machine make sure it worked it's been up an ec2 instance run it there make sure it worked when they're happy with it",
    "start": "791600",
    "end": "797030"
  },
  {
    "text": "they could push it to ECR Amazon's docker image repository and then we can",
    "start": "797030",
    "end": "802670"
  },
  {
    "text": "incorporate that into the pipeline so we made some simple changes to the",
    "start": "802670",
    "end": "809420"
  },
  {
    "start": "807000",
    "end": "807000"
  },
  {
    "text": "architecture we got rid of this baked a mice and we moved to a standard Linux",
    "start": "809420",
    "end": "814730"
  },
  {
    "text": "Amy with dr. installed and we just ran docker images on those machines and we pulled those docker",
    "start": "814730",
    "end": "820700"
  },
  {
    "text": "images from ECR so we essentially swapped out the mi4 docker the custom ami and then for the pipeline",
    "start": "820700",
    "end": "829460"
  },
  {
    "text": "definition it's pretty much the same what we have now though is a path to the image in ECR so we know where to pull",
    "start": "829460",
    "end": "836090"
  },
  {
    "text": "that docker image from and some arguments potentially and then we also",
    "start": "836090",
    "end": "841280"
  },
  {
    "text": "have this additional step when we're pulling down a step but we're running that step we have another",
    "start": "841280",
    "end": "849820"
  },
  {
    "text": "thing to do here which is to pull the image from ECR then pull external files sample files and then run the step we",
    "start": "849820",
    "end": "857600"
  },
  {
    "text": "would run docker run so before we're running the tool directly now we're running docker run",
    "start": "857600",
    "end": "863740"
  },
  {
    "text": "so the ban the benefit to this is now we can easily accommodate tool changes but",
    "start": "863740",
    "end": "869210"
  },
  {
    "text": "we realize boy we don't want to be in the business of supporting doc or ourselves that was kind of painful so",
    "start": "869210",
    "end": "874520"
  },
  {
    "text": "for example we would run into conflicts between docker versions and ami versions",
    "start": "874520",
    "end": "880370"
  },
  {
    "text": "so we didn't want to be supporting doctor ourselves and there was pain around managing the images the docker",
    "start": "880370",
    "end": "886070"
  },
  {
    "text": "images themselves so the next challenge we face is we kind",
    "start": "886070",
    "end": "892250"
  },
  {
    "text": "of became a victim of our own success the system was working really well for a given pipeline report pipeline now we",
    "start": "892250",
    "end": "898670"
  },
  {
    "text": "wanted to be able to run lots of different report pipelines on the same platform so we had flexibility and",
    "start": "898670",
    "end": "904010"
  },
  {
    "text": "accommodating the tools with docker and now we wanted to be able to accommodate flexible pipelines",
    "start": "904010",
    "end": "909910"
  },
  {
    "text": "pipeline definitions and so to do that we dropped our custom version our custom",
    "start": "909910",
    "end": "916670"
  },
  {
    "start": "912000",
    "end": "912000"
  },
  {
    "text": "custom workflow engine and we move to SWF and flow so SWF is a service in AWS",
    "start": "916670",
    "end": "922640"
  },
  {
    "text": "that essentially does workflow management it gives you gave us things like versioning of stop some workflows",
    "start": "922640",
    "end": "930160"
  },
  {
    "text": "retry logic a council to kind of track things API is it's really a tracker for",
    "start": "930160",
    "end": "937120"
  },
  {
    "text": "complex workflows at the cloud and the ATS flow framework is a a",
    "start": "937120",
    "end": "943360"
  },
  {
    "text": "convenience framework that's written in Ruby that makes it really easy to write distributed applications the script that",
    "start": "943360",
    "end": "951259"
  },
  {
    "text": "interact or use SWF and it does a lot of the heavy lifting for you so you could just focus on your piece your code",
    "start": "951259",
    "end": "959620"
  },
  {
    "text": "so the architecture evolved a bit here but now we've dropped sqs and the queue",
    "start": "959980",
    "end": "965060"
  },
  {
    "text": "and now we have a topic so we moved the publish/subscribe model and we have an event that comes off this topic and a",
    "start": "965060",
    "end": "971329"
  },
  {
    "text": "lambda that's subscribed that event takes that event in runs SWF submits a",
    "start": "971329",
    "end": "977329"
  },
  {
    "text": "workflow execution and now we've swapped out the Python applications for these Ruby flow applications and within this",
    "start": "977329",
    "end": "983779"
  },
  {
    "text": "model has to be open flow you have deciders and activity workers so decider processes will pull a workflow down",
    "start": "983779",
    "end": "989660"
  },
  {
    "text": "decide what the next step should be submit that back to SWF and make it available to activity workers to pull",
    "start": "989660",
    "end": "995569"
  },
  {
    "text": "them that those jobs down and run the activities report the results back to SWF that's how the model works and then",
    "start": "995569",
    "end": "1002860"
  },
  {
    "text": "there's a really great chef cookbook that makes it really easy to create these decider and activity worker",
    "start": "1002860",
    "end": "1008560"
  },
  {
    "text": "processes in ops work stacks so we use that as well so pipeline definition similar but now",
    "start": "1008560",
    "end": "1016540"
  },
  {
    "text": "we've added this async flag so you can run some steps in parallel and then we've migrated to Ruby instead of Python",
    "start": "1016540",
    "end": "1024038"
  },
  {
    "text": "because this frameworks Ruby framework the two things that you need to be aware of its turbulent flow is this send to",
    "start": "1024039",
    "end": "1030400"
  },
  {
    "text": "sink an exec step send Asik is gonna submit a job desktop you have to run and",
    "start": "1030400",
    "end": "1035650"
  },
  {
    "text": "it's gonna return a future that you can wait on so this is your mechanism to run things in parallel an exact step is your",
    "start": "1035650",
    "end": "1041829"
  },
  {
    "text": "mechanism to run things in sequence so creator array of futures an empty",
    "start": "1041829",
    "end": "1047230"
  },
  {
    "text": "array iterate over each step the ones that can run in parallel you submit them collect all the futures wait for them",
    "start": "1047230",
    "end": "1053860"
  },
  {
    "text": "all and when they're all done you run each step in sequence and this is where you could have steps that depend on a",
    "start": "1053860",
    "end": "1060220"
  },
  {
    "text": "previous step can make sure that they run in the right order put outputs in s3 for example and then this code is the",
    "start": "1060220",
    "end": "1067570"
  },
  {
    "text": "decider code that gets deployed within a decider process and this other code here is the activity code that gets deployed",
    "start": "1067570",
    "end": "1073660"
  },
  {
    "text": "an activity process and those recipes make that all easy so you get stand up these decide earn activity workers",
    "start": "1073660",
    "end": "1079360"
  },
  {
    "text": "within a cluster so big benefit here much easier to",
    "start": "1079360",
    "end": "1084760"
  },
  {
    "text": "accommodate new pipelines now and run steps in parallel and we have things like handle the ability to handle",
    "start": "1084760",
    "end": "1090550"
  },
  {
    "text": "failures and retries and versioning but the workflow approach or pipeline",
    "start": "1090550",
    "end": "1097090"
  },
  {
    "text": "definition approach wasn't really flexible enough to accommodate more complex pipelines that we had so we need",
    "start": "1097090",
    "end": "1102610"
  },
  {
    "text": "to move beyond our simple JSON definition to something more sophisticated",
    "start": "1102610",
    "end": "1107430"
  },
  {
    "text": "and for that I'll hand this off to Steven",
    "start": "1108570",
    "end": "1113940"
  },
  {
    "text": "forward and what we had found is that we've become even greater victims of our own success people were coming to us and",
    "start": "1121350",
    "end": "1128410"
  },
  {
    "text": "asking if we can onboard any type of pipeline at HLI to this platform whether that be a pipeline to do secondary",
    "start": "1128410",
    "end": "1134680"
  },
  {
    "text": "analysis",
    "start": "1134680",
    "end": "1137220"
  },
  {
    "text": "I'm good time do you hear me back there now all right",
    "start": "1141980",
    "end": "1149120"
  },
  {
    "text": "so we wanted to be able to onboard any type of pipeline to this platform and whether that be a secondary analysis",
    "start": "1149120",
    "end": "1156299"
  },
  {
    "text": "pipeline or maybe a pipeline that generates data that's fed into a data Lake in addition to the report",
    "start": "1156299",
    "end": "1162149"
  },
  {
    "text": "generation pipeline we were already running and then once we had everything on board it to this common platform we",
    "start": "1162149",
    "end": "1167519"
  },
  {
    "text": "wanted to optimize for cost at a platform level and use something like sparkly too heavily reduce the compute",
    "start": "1167519",
    "end": "1172679"
  },
  {
    "text": "costs so what we need is a managed service for",
    "start": "1172679",
    "end": "1177960"
  },
  {
    "start": "1175000",
    "end": "1175000"
  },
  {
    "text": "running docker eyes pipelines now AWS doesn't really offer such a service but they do have all the component pieces",
    "start": "1177960",
    "end": "1183960"
  },
  {
    "text": "you see here to put one together yourself so that's the approach we took and we call it dhaka pipeline",
    "start": "1183960",
    "end": "1191450"
  },
  {
    "text": "now there's kind of three key concepts with dr. pipeline that you need to be aware of and the first is registering a",
    "start": "1191450",
    "end": "1197879"
  },
  {
    "text": "task with the system before in a pipeline which we saw previously if you had a bioinformatics tool you needed to",
    "start": "1197879",
    "end": "1204240"
  },
  {
    "text": "kind of define it within your pipeline itself but here we've broken that out so now",
    "start": "1204240",
    "end": "1210029"
  },
  {
    "text": "all you need to do is register that bioinformatics tool as a task with docker pipeline and you tell dr.",
    "start": "1210029",
    "end": "1216450"
  },
  {
    "text": "pipeline how to run that tool and the resources that are required to run it",
    "start": "1216450",
    "end": "1222620"
  },
  {
    "text": "next you need to register your pipeline with the system and a pipeline is composed of two important parts the",
    "start": "1223639",
    "end": "1230279"
  },
  {
    "text": "first is your steps file which references tasks that are already registered on docker pipeline and the",
    "start": "1230279",
    "end": "1236279"
  },
  {
    "text": "other is a little bit of orchestration code again written in Ruby that tells the platform how to orchestrate the",
    "start": "1236279",
    "end": "1241769"
  },
  {
    "text": "individual steps within your pipeline and then once your pipeline is",
    "start": "1241769",
    "end": "1247649"
  },
  {
    "text": "registered with the system you can simply call DPL pipeline run and then the pot the platform will pull",
    "start": "1247649",
    "end": "1254450"
  },
  {
    "text": "that pipeline out and run it for you",
    "start": "1254450",
    "end": "1259638"
  },
  {
    "text": "so here's kind of what that looks like if I'm a new researcher at hli let's say",
    "start": "1261080",
    "end": "1266309"
  },
  {
    "text": "I've written a bioinformatics tool that's generating some ancestry data and here you can see that I've installed the",
    "start": "1266309",
    "end": "1272519"
  },
  {
    "text": "DPL or the the docker pipeline command-line tool that we call DPL and I'm",
    "start": "1272519",
    "end": "1277950"
  },
  {
    "text": "registering my ancestry tasks with the system again we have a bit of JSON that kind of",
    "start": "1277950",
    "end": "1285330"
  },
  {
    "text": "describes that task this looks pretty familiar we're telling it what image to run and then the",
    "start": "1285330",
    "end": "1292200"
  },
  {
    "text": "arguments to that image but we have this new resource requirements block that",
    "start": "1292200",
    "end": "1297600"
  },
  {
    "text": "defines the resources that are needed to run this specific task so here I've said I need 40 cores I need EVs size small",
    "start": "1297600",
    "end": "1306480"
  },
  {
    "text": "I can specify what UVs type I like maybe a snapshot as well that contains things like reference genomes or other static",
    "start": "1306480",
    "end": "1312570"
  },
  {
    "text": "files and then a memory multiplier that I can use to basically select the high memory instance for my tasks",
    "start": "1312570",
    "end": "1319970"
  },
  {
    "text": "next I'm going to register a new pipeline that I've built on docker pipeline and the first piece I need to",
    "start": "1321950",
    "end": "1328620"
  },
  {
    "text": "include is my steps file so here instead of defining the step",
    "start": "1328620",
    "end": "1334559"
  },
  {
    "text": "within the pipeline itself I'm including a tasks that have registered on docker pipeline and this is my ancestory task",
    "start": "1334559",
    "end": "1342169"
  },
  {
    "text": "however I'm also including a traits task now this is already registered with the system it could have been created and",
    "start": "1342169",
    "end": "1348630"
  },
  {
    "text": "registered by me someone else on my team or a different team entirely I don't really need to know how to use this tool",
    "start": "1348630",
    "end": "1355559"
  },
  {
    "text": "or what it's what's needed to run it because dr. pipeline knows how to run this for me so it's very simple for me",
    "start": "1355559",
    "end": "1360929"
  },
  {
    "text": "to include it in my pipeline I also need to include a little bit of",
    "start": "1360929",
    "end": "1367110"
  },
  {
    "text": "Ruby code that instructs her that tells docker pipeline how to orchestrate the various steps within my pipeline",
    "start": "1367110",
    "end": "1373970"
  },
  {
    "text": "so here as similar to before the person registering the pipeline now is actually",
    "start": "1373970",
    "end": "1380340"
  },
  {
    "text": "writing this code so similar to the floor this is how we run something synchronously and",
    "start": "1380340",
    "end": "1386510"
  },
  {
    "text": "then we also have the ability to run things asynchronously and if I choose I can write some conditional logic in here",
    "start": "1386510",
    "end": "1393390"
  },
  {
    "text": "so that I can do things like splits merges multiple-choice all of those complex",
    "start": "1393390",
    "end": "1399390"
  },
  {
    "text": "workflow patterns that are needed in more complex pipelines",
    "start": "1399390",
    "end": "1405110"
  },
  {
    "text": "then once I've got that registered system I can call DPL run and I get back",
    "start": "1405260",
    "end": "1411190"
  },
  {
    "text": "a run ID and a workflow ID that I can use to track my pipeline as it's being run by docker pipeline",
    "start": "1411190",
    "end": "1418920"
  },
  {
    "text": "now here in the SWF console we see up top that I've got a workflow execution",
    "start": "1418920",
    "end": "1424330"
  },
  {
    "text": "running and then down below there's an activity running in my pipeline so this will correlate to my ancestory tasks if",
    "start": "1424330",
    "end": "1430870"
  },
  {
    "text": "I'm running synchronously so this one will run first and then if we look in the ECS console we can see that I have",
    "start": "1430870",
    "end": "1438580"
  },
  {
    "text": "my ancestory container running on an ECS cluster",
    "start": "1438580",
    "end": "1444450"
  },
  {
    "text": "so here's kind of how all this works a nice little architecture slide and it's",
    "start": "1445710",
    "end": "1452170"
  },
  {
    "text": "a little complex so let's step through each piece of it the first is my docker push so I'm",
    "start": "1452170",
    "end": "1458620"
  },
  {
    "text": "registering my docker image with a repository in this case ECR then I'll register a task that",
    "start": "1458620",
    "end": "1465700"
  },
  {
    "text": "references that docker image and tells docker pipeline how to run it and what's",
    "start": "1465700",
    "end": "1471220"
  },
  {
    "text": "needed to run it then I'll register my pipeline with the",
    "start": "1471220",
    "end": "1476860"
  },
  {
    "text": "system that uses tasks that are already registered and then finally I call DPL run so",
    "start": "1476860",
    "end": "1483610"
  },
  {
    "text": "that's what it looks like from the end users perspective someone who's building a pipeline at hli now behind the scenes",
    "start": "1483610",
    "end": "1490560"
  },
  {
    "text": "after the DPL run call is made the lambda function will start a",
    "start": "1490560",
    "end": "1497110"
  },
  {
    "text": "workflow with SWF",
    "start": "1497110",
    "end": "1500580"
  },
  {
    "text": "from here a decider process running in our ops work stack will pull that",
    "start": "1502590",
    "end": "1507700"
  },
  {
    "text": "workflow down from SWF and inside that will be the work for the",
    "start": "1507700",
    "end": "1512980"
  },
  {
    "text": "pipeline that we're trying to run and the parameters that need to be sent into that pipeline",
    "start": "1512980",
    "end": "1519270"
  },
  {
    "text": "will then pull the pipeline definition from our registry which in this case is dynamo and",
    "start": "1519510",
    "end": "1525990"
  },
  {
    "text": "then we'll execute that bit of Ruby code that orchestrates the pipeline so in our",
    "start": "1525990",
    "end": "1531490"
  },
  {
    "text": "case that will run basically the ancestry tasks first and that will make an activity available in SWF that is",
    "start": "1531490",
    "end": "1538930"
  },
  {
    "text": "then pulled down by an activity where are also running in our ops work stack",
    "start": "1538930",
    "end": "1544540"
  },
  {
    "text": "once that activity worker receives the activity it will call a lambda function saying I would like to start this task",
    "start": "1544540",
    "end": "1551810"
  },
  {
    "text": "on docker pipeline and here are the parameters to that task is then given back a task identifier",
    "start": "1551810",
    "end": "1559550"
  },
  {
    "text": "that it can use to pull for the status of that task",
    "start": "1559550",
    "end": "1564010"
  },
  {
    "text": "then the lambda function will retrieve the task from our task registry and",
    "start": "1565090",
    "end": "1570880"
  },
  {
    "text": "determine the appropriate ECS cluster to submit that specific task to now the",
    "start": "1570880",
    "end": "1577190"
  },
  {
    "text": "idea here is that we have a nice es cluster setup for every kind of",
    "start": "1577190",
    "end": "1582590"
  },
  {
    "text": "combination of resource requirements so kind of on the fly we're looking at that task and saying ok I know it needs this",
    "start": "1582590",
    "end": "1589010"
  },
  {
    "text": "specific set of resources and it will create an e CS cluster that can handle that type of work so any task that needs",
    "start": "1589010",
    "end": "1595970"
  },
  {
    "text": "that same set of resource requirements will get routed to that same ECS cluster",
    "start": "1595970",
    "end": "1602320"
  },
  {
    "text": "now that easiest cluster doesn't exist we will create it at that time and we'll",
    "start": "1603580",
    "end": "1609110"
  },
  {
    "text": "configure a spot fleet that can handle that type of work so we'll make sure we're adding the right types of instances with the right EBS size and",
    "start": "1609110",
    "end": "1616130"
  },
  {
    "text": "other resource requirements and then we'll attach it to that specific ECS cluster",
    "start": "1616130",
    "end": "1621370"
  },
  {
    "text": "then in front of that we'll put an s qsq that the task actually gets submitted to",
    "start": "1621370",
    "end": "1628690"
  },
  {
    "text": "then we have lambda functions monitoring queues that are in front of re CS",
    "start": "1629950",
    "end": "1636470"
  },
  {
    "text": "clusters and they'll pull down a message and attempt to submit that specific work to e CS to actually run the docker image",
    "start": "1636470",
    "end": "1643340"
  },
  {
    "text": "on e CS the PCs comes back and says we don't have enough resources the message simply goes back on the queue to be",
    "start": "1643340",
    "end": "1650810"
  },
  {
    "text": "retried at a later time we're monitoring all of these queues and",
    "start": "1650810",
    "end": "1656000"
  },
  {
    "text": "when the messages pile up and cross our threshold we know that we need to add more capacity to our spot fleets or to",
    "start": "1656000",
    "end": "1663020"
  },
  {
    "text": "our ECS clusters so we'll look up the spot fleet that is powering that e CS cluster and then we will add capacity to",
    "start": "1663020",
    "end": "1669020"
  },
  {
    "text": "it once those nodes come online they join the e CS cluster and the next the",
    "start": "1669020",
    "end": "1674280"
  },
  {
    "text": "work is submitted to it it will start our docker image once we're on the note itself and the",
    "start": "1674280",
    "end": "1681270"
  },
  {
    "text": "docker image is running they'll be reading and writing files to s3",
    "start": "1681270",
    "end": "1686750"
  },
  {
    "text": "now eventually all of the activities in a work flow will complete those all get",
    "start": "1686960",
    "end": "1692070"
  },
  {
    "text": "tracked in SWF and when they all finish we know that our workflow is done and we've run our pipeline successfully",
    "start": "1692070",
    "end": "1698450"
  },
  {
    "text": "so that's how a pipeline gets run on docker pipeline",
    "start": "1698450",
    "end": "1703820"
  },
  {
    "text": "so now we can accommodate complex workflow patterns and we can",
    "start": "1705169",
    "end": "1710760"
  },
  {
    "text": "define those pretty simply we can also share tools across pipelines since we've",
    "start": "1710760",
    "end": "1716130"
  },
  {
    "text": "taken the bioinformatics tool definition out of the pipeline definition and registered them separately on the system",
    "start": "1716130",
    "end": "1722250"
  },
  {
    "text": "they can be easily included in other pipelines because people know that when a task is registered on docker pipeline",
    "start": "1722250",
    "end": "1728250"
  },
  {
    "text": "that it will just run on the system",
    "start": "1728250",
    "end": "1731870"
  },
  {
    "text": "we're now also optimizing our instance types for each step in a workflow",
    "start": "1734330",
    "end": "1739380"
  },
  {
    "text": "because we're being very specific about the resource requirements that are needed for that task so we're right",
    "start": "1739380",
    "end": "1744990"
  },
  {
    "text": "sizing our instances we're also no longer supporting doctor ourselves which is a big win ECS makes",
    "start": "1744990",
    "end": "1751890"
  },
  {
    "text": "it very easy to just start docker images and run them and then Amazon handles all the configuration that's needed on the",
    "start": "1751890",
    "end": "1758039"
  },
  {
    "text": "docker side for us and we're also getting some pretty massive cost savings because we're using spot to run all of",
    "start": "1758039",
    "end": "1763770"
  },
  {
    "text": "our jobs",
    "start": "1763770",
    "end": "1766190"
  },
  {
    "text": "so the next step we wanted to do was go faster with continuous delivery we",
    "start": "1771200",
    "end": "1776420"
  },
  {
    "text": "needed to be able to deploy into production very quickly so that means automation we need to",
    "start": "1776420",
    "end": "1782450"
  },
  {
    "start": "1780000",
    "end": "1780000"
  },
  {
    "text": "understand our deployment process and automate it we need to have really great integration testing at each step of our",
    "start": "1782450",
    "end": "1788240"
  },
  {
    "text": "deployment and we need to have push button to prod so at hli we're using code pipeline to",
    "start": "1788240",
    "end": "1794960"
  },
  {
    "text": "orchestrate our continuous delivery pipeline which will start a deployment in our dev environment which uses AWS",
    "start": "1794960",
    "end": "1800930"
  },
  {
    "text": "code deploy to stand up all our infrastructure and we run a quick smoke test to make sure that looks good",
    "start": "1800930",
    "end": "1807850"
  },
  {
    "text": "we then do our deployment in our integration environment and run a much broader suite of integration tests",
    "start": "1807910",
    "end": "1814070"
  },
  {
    "text": "that's going to be exercising all parts of the system to make sure they're working as intended",
    "start": "1814070",
    "end": "1820179"
  },
  {
    "text": "from here we push to our stage environment where we're doing Bluegreen deployments so we'll have an",
    "start": "1820900",
    "end": "1826610"
  },
  {
    "text": "inactive stack that we deploy to integration tests there make sure everything looks good and then we'll",
    "start": "1826610",
    "end": "1832130"
  },
  {
    "text": "switch that inactive stack to the active one and then finally we'll send a message to SNS which sends an email out",
    "start": "1832130",
    "end": "1838550"
  },
  {
    "text": "with a link that someone can click to push the latest changes to production",
    "start": "1838550",
    "end": "1844390"
  },
  {
    "text": "now this is what our integration environment actually looks like from the code pipeline console and",
    "start": "1844660",
    "end": "1851050"
  },
  {
    "text": "here we can see we're running our code deploy our AWS code deploy deployment and we also wrote a lambda integral a",
    "start": "1851050",
    "end": "1858110"
  },
  {
    "text": "integration that notifies our slack channel when we started a deployment here we're deploying our opsworks",
    "start": "1858110",
    "end": "1865100"
  },
  {
    "text": "application in our ops work stack and then here we're running all of our",
    "start": "1865100",
    "end": "1870290"
  },
  {
    "text": "integration tests in our integration environment and then also notifying our slag channel when the deployment is",
    "start": "1870290",
    "end": "1875750"
  },
  {
    "text": "complete so now it's very easy for us to go from",
    "start": "1875750",
    "end": "1883110"
  },
  {
    "text": "hearing about a bug to having a fix deployed into production within minutes so we're deploying to production",
    "start": "1883110",
    "end": "1888179"
  },
  {
    "text": "multiple times a day now and with that I'll turn it back over to Ryan",
    "start": "1888179",
    "end": "1894270"
  },
  {
    "start": "1893000",
    "end": "1893000"
  },
  {
    "text": "for a quick summary hey so there's a couple key benefits we",
    "start": "1894270",
    "end": "1901409"
  },
  {
    "text": "got from all this a dramatic simplification and pipeline complexity so one example we went from about two",
    "start": "1901409",
    "end": "1907320"
  },
  {
    "text": "thousand lines of code from one pipeline about 20 lines of code it config file significant reduction in time to",
    "start": "1907320",
    "end": "1914039"
  },
  {
    "text": "generate those reports so some of these things were taken like a couple people three weeks then we're able to get that",
    "start": "1914039",
    "end": "1919289"
  },
  {
    "text": "down to about five hours significant cost savings with spot so these are the compute costs for",
    "start": "1919289",
    "end": "1925049"
  },
  {
    "text": "particular report but in some cases we went from $32 a six for a given report daily deployments of the platform",
    "start": "1925049",
    "end": "1932789"
  },
  {
    "text": "changes to production so it was taking us sometimes weeks or months to get something into production we were able",
    "start": "1932789",
    "end": "1938250"
  },
  {
    "text": "to get that down to daily and dramatically ease your hand off between bioinformatics and engineering because",
    "start": "1938250",
    "end": "1944370"
  },
  {
    "text": "now we've gone from code to configuration right so it's just passing a configuration alone and finally some next steps things were",
    "start": "1944370",
    "end": "1951960"
  },
  {
    "start": "1950000",
    "end": "1950000"
  },
  {
    "text": "thinking about in the future one we've done a lot of work to simplify pipeline definition but now the big challenge is",
    "start": "1951960",
    "end": "1958350"
  },
  {
    "text": "defining the tools and building the bottom from Attucks tools so we think there's lots of opportunities with some of the managed services and AWS and",
    "start": "1958350",
    "end": "1964770"
  },
  {
    "text": "frameworks to make that a lot easier for bioinformatics scientists and then find then the last thing is there's a desire",
    "start": "1964770",
    "end": "1971250"
  },
  {
    "text": "to be able to run SPARC clusters for a given step so instead of running out in one instance with maybe 40 cores you can",
    "start": "1971250",
    "end": "1976980"
  },
  {
    "text": "run a cluster of 60 machines so those are some of the things that we're thinking about next and",
    "start": "1976980",
    "end": "1982850"
  },
  {
    "text": "with that I will hand it over thank you",
    "start": "1982850",
    "end": "1988820"
  },
  {
    "text": "hello everyone I'm Lance Smith from Celgene I'm real quick this is what we'll be",
    "start": "1988820",
    "end": "1994740"
  },
  {
    "text": "talking about today some of a little bit about the company some of the trends that we're seeing in the collaboration models plural that we have and some of",
    "start": "1994740",
    "end": "2001520"
  },
  {
    "text": "the configuration and steps that we have learned along the way here so real quick",
    "start": "2001520",
    "end": "2007070"
  },
  {
    "text": "if you don't know who we are we're a biotech all the way from Discovery all the Wales sales and distribution",
    "start": "2007070",
    "end": "2013970"
  },
  {
    "text": "and that last bullet point we have 60 sites globally and that has a big implication I'll talk about about the",
    "start": "2013970",
    "end": "2019549"
  },
  {
    "text": "networking that we have at our sites here",
    "start": "2019549",
    "end": "2023649"
  },
  {
    "start": "2023000",
    "end": "2023000"
  },
  {
    "text": "right so some of the trends that we're seeing as Patrick talked about earlier a lot of",
    "start": "2024730",
    "end": "2030889"
  },
  {
    "text": "collaborations and partnerships Celgene we're you know this is what we do we sign up a lot of collaborators work with",
    "start": "2030889",
    "end": "2037009"
  },
  {
    "text": "a lot of universities and probably some of you in the audience here we work with your organization's Rd of course very",
    "start": "2037009",
    "end": "2043190"
  },
  {
    "text": "very quick they will sign deals and not tell IT so you know it's our job to make it happen and the last bullet point here",
    "start": "2043190",
    "end": "2050148"
  },
  {
    "text": "cloud native solutions you know we're starting to see the software market start to mature we're no longer seeing forklifted applications but we're seeing",
    "start": "2050149",
    "end": "2056929"
  },
  {
    "text": "applications written directly for Amazon either you run them as a software as a service from the vendor or you run them",
    "start": "2056929",
    "end": "2063470"
  },
  {
    "text": "in your account maybe someplace in between the one thing that you can't do though is run them on premise",
    "start": "2063470",
    "end": "2070628"
  },
  {
    "text": "real quick on our collaboration this coming weekend we're going to have an announcement I can't quite put the press",
    "start": "2070629",
    "end": "2077240"
  },
  {
    "text": "release here but this coming Saturday we've been in dart stealth mode for the last year multiple myeloma genome",
    "start": "2077240",
    "end": "2083720"
  },
  {
    "text": "project we've been working with a number of universities we're going to be opening this up to a greater world so if",
    "start": "2083720",
    "end": "2088849"
  },
  {
    "text": "your organization does myeloma research please contact us and then maybe we can work together on this",
    "start": "2088849",
    "end": "2096128"
  },
  {
    "start": "2096000",
    "end": "2096000"
  },
  {
    "text": "all right so I talked a little bit about our collaborations we have multiple collaborations we have many different",
    "start": "2096490",
    "end": "2102859"
  },
  {
    "text": "types of science that we do so in any given day we have hundreds of biologists and chemists around the world in all",
    "start": "2102859",
    "end": "2108560"
  },
  {
    "text": "sorts of science and we need to help them out they want to work with a university it's our job to help them enable that so you know each of the",
    "start": "2108560",
    "end": "2116030"
  },
  {
    "text": "scientists they have a different type of science different type of software different types of output and we can't",
    "start": "2116030",
    "end": "2121099"
  },
  {
    "text": "have a single platform that can support them all so we have a couple of dozen collaborations but we can pretty much",
    "start": "2121099",
    "end": "2126740"
  },
  {
    "text": "group them into two separate categories one is the bench chemist now we are these are true end users that they just",
    "start": "2126740",
    "end": "2133910"
  },
  {
    "text": "want a simple way of interface click on here upload your data and then your data is there single sign-on ties into our",
    "start": "2133910",
    "end": "2141260"
  },
  {
    "text": "corporate network and then we have the other type of collaboration the HPC users these are you know we have our PhD",
    "start": "2141260",
    "end": "2146900"
  },
  {
    "text": "scientists but have for them are also computer scientists they're not you know hardcore super science computer science",
    "start": "2146900",
    "end": "2153050"
  },
  {
    "text": "developers they know how to code and they want to write their own algorithms they want shell access they want API",
    "start": "2153050",
    "end": "2158870"
  },
  {
    "text": "access and they're and because it's HTC we're talking hundreds thousands tens of thousands of nodes working on petabyte",
    "start": "2158870",
    "end": "2165200"
  },
  {
    "text": "scale data from IIT point of view though both of",
    "start": "2165200",
    "end": "2170270"
  },
  {
    "start": "2167000",
    "end": "2167000"
  },
  {
    "text": "those two categories we could we treat them the same we have multiple vendors coming in that Seles software we have",
    "start": "2170270",
    "end": "2176660"
  },
  {
    "text": "multiple research groups working together they all want API access how do we how do we you know keep them safe",
    "start": "2176660",
    "end": "2182480"
  },
  {
    "text": "from each other and between projects so what we do is we have a multiple account",
    "start": "2182480",
    "end": "2187640"
  },
  {
    "text": "model or each collaboration gets a separate account we parked them in an account and what they do they can affect",
    "start": "2187640",
    "end": "2194030"
  },
  {
    "text": "another project they can believe their own data but they can't affect anyone else's has multiple benefits and I'll go",
    "start": "2194030",
    "end": "2200000"
  },
  {
    "text": "into a little bit of that later but from our point of view the management is the same",
    "start": "2200000",
    "end": "2205839"
  },
  {
    "text": "here's one of our architecture diagrams for our mint one of our mass spec systems the users come in they hit one",
    "start": "2206380",
    "end": "2213020"
  },
  {
    "text": "of web servers all the scaling group behind it alb they then upload their files of s3 if single sign-on that hits",
    "start": "2213020",
    "end": "2219830"
  },
  {
    "text": "our ad servers and our DMZ a processing is also done in auto scaling groups with spot that saves us a lot of money all",
    "start": "2219830",
    "end": "2227600"
  },
  {
    "text": "primary data is kept in s3 metadata is then kept in RDS and SQS is used for a",
    "start": "2227600",
    "end": "2232640"
  },
  {
    "text": "queueing of tasks between between jobs HPC this is a little bit simplified",
    "start": "2232640",
    "end": "2240310"
  },
  {
    "start": "2237000",
    "end": "2237000"
  },
  {
    "text": "compared to the last slide there the scientists themselves write the pipeline they write the algorithms they write the",
    "start": "2240310",
    "end": "2246380"
  },
  {
    "text": "coordination they work with the data directly so we can't give them a simple web interface they shell in with a",
    "start": "2246380",
    "end": "2252470"
  },
  {
    "text": "bastion host we have a couple of different Bastion host with a role attached to it they scientists themselves generally don't get Amazon",
    "start": "2252470",
    "end": "2258770"
  },
  {
    "text": "keys they work on the bastion host which then has the iam role in which they can operate so they can work on the",
    "start": "2258770",
    "end": "2265760"
  },
  {
    "text": "algorithms their up data they upload directly to s3 from wherever and may be coming from and then we have a couple",
    "start": "2265760",
    "end": "2272390"
  },
  {
    "text": "different compute clusters those like the compute nodes that can handle stos those pipelines have better failover",
    "start": "2272390",
    "end": "2278750"
  },
  {
    "text": "they're cheaper and will you spot of our other pipelines we unfortunately have to use on demand but we are working",
    "start": "2278750",
    "end": "2284240"
  },
  {
    "text": "with the scientists to move that into spot as well",
    "start": "2284240",
    "end": "2289390"
  },
  {
    "start": "2291000",
    "end": "2291000"
  },
  {
    "text": "connectivity so how do we connect into these environments so when a project first comes to IT collaboration or not",
    "start": "2291400",
    "end": "2298339"
  },
  {
    "text": "we you know we are here to try to help them we're not trying to be the Department of no how do we help them out",
    "start": "2298339",
    "end": "2304220"
  },
  {
    "text": "but one of the few things that not is not negotiable for us is that when a project comes along they have a choice",
    "start": "2304220",
    "end": "2310099"
  },
  {
    "text": "IG w or v GW we can't have both what that means is that with the IG W that V",
    "start": "2310099",
    "end": "2316520"
  },
  {
    "text": "PC can talk to the outside world directly or over the v GW can talk to down premise systems we are a",
    "start": "2316520",
    "end": "2322760"
  },
  {
    "text": "pharmaceutical company we have a lot of data that bad people want and we can't let them have it however hydration",
    "start": "2322760",
    "end": "2330049"
  },
  {
    "text": "accounts you know they have to have the IG W in order for the collaborators to come in so we top off the BG W but we",
    "start": "2330049",
    "end": "2336440"
  },
  {
    "text": "can still use the company Direct Connect which to communicate and upload data with a public interface on the direct",
    "start": "2336440",
    "end": "2343130"
  },
  {
    "text": "connect we can acts access s3 you can also access all the public IPS on with",
    "start": "2343130",
    "end": "2348319"
  },
  {
    "text": "SSH and whatnot and then do my laser pointer here but you won't feel",
    "start": "2348319",
    "end": "2354440"
  },
  {
    "text": "see it so there's a number of options if you know if your organization is looking to",
    "start": "2354440",
    "end": "2359950"
  },
  {
    "text": "connect into Amazon you have a number of options the easiest one to do is just",
    "start": "2359950",
    "end": "2365210"
  },
  {
    "text": "you know the internet access SSH in boom you're in some security issues there so",
    "start": "2365210",
    "end": "2370369"
  },
  {
    "text": "you could go with a deep VPN connection there but with our multi multiple account model",
    "start": "2370369",
    "end": "2375670"
  },
  {
    "text": "managing multiple VPN connections is very very painful so we did that for a few months it's not for us we then jump",
    "start": "2375670",
    "end": "2383059"
  },
  {
    "text": "directly into our ten gigabit connect so we have two of those two more 10 gigs coming one on each side of the country",
    "start": "2383059",
    "end": "2390500"
  },
  {
    "text": "firing up on EU as well there's a big decision between one gig",
    "start": "2390500",
    "end": "2397609"
  },
  {
    "text": "or sub one gig and one gig and hired direct connects and it's not just the speed almost the speed is almost for us",
    "start": "2397609",
    "end": "2402799"
  },
  {
    "text": "a separate concern if you're less than one gig on your Direct Connect you only",
    "start": "2402799",
    "end": "2407809"
  },
  {
    "text": "get one interface for 10 one and 10 K you get up to we",
    "start": "2407809",
    "end": "2413059"
  },
  {
    "text": "start with 50 and you can go about 100 or so and the reason why that's important is that when you each VPC",
    "start": "2413059",
    "end": "2418670"
  },
  {
    "text": "needs a virtual interface and if you want to low upload lots of data you also want to use a public interface to upload",
    "start": "2418670",
    "end": "2425359"
  },
  {
    "text": "to s3 so media right off the bat you need to for a multi account model like us you need 20 30 50 separate interfaces",
    "start": "2425359",
    "end": "2432230"
  },
  {
    "text": "if you're at a sub 1 gig right connect you only get one so we recommend",
    "start": "2432230",
    "end": "2437800"
  },
  {
    "text": "1 gig or higher now but before you go off and buy your direct connect or some",
    "start": "2437800",
    "end": "2442820"
  },
  {
    "text": "additional choices that you have to make or decisions that you have to make a big one for us is region region selection if",
    "start": "2442820",
    "end": "2449480"
  },
  {
    "text": "you're on the East Coast it's easy us East one if you're on the west coast half of my operations are on the east west coast to choice is a very hard",
    "start": "2449480",
    "end": "2456980"
  },
  {
    "text": "decision u.s. west one versus u.s. west - we have a lot of on-premise databases",
    "start": "2456980",
    "end": "2462470"
  },
  {
    "text": "and we're gonna be working in a hybrid mode for the next 5-10 years so latency is a huge concern to us",
    "start": "2462470",
    "end": "2469030"
  },
  {
    "text": "that's why we want with us us one however every week scientist users it",
    "start": "2469030",
    "end": "2474980"
  },
  {
    "text": "lands you know FS is great we'd like to have it it's not available in u.s. West one so when Amazon releases new features",
    "start": "2474980",
    "end": "2481880"
  },
  {
    "text": "they always come out in Seattle Virginia and the island data centers and we're all working on firing up",
    "start": "2481880",
    "end": "2488329"
  },
  {
    "text": "Frankfurt as well fully knowing that new features come out in Ireland and not in Frankfurt but latency is a big deal for",
    "start": "2488329",
    "end": "2495050"
  },
  {
    "text": "us and that's what we have to do",
    "start": "2495050",
    "end": "2498940"
  },
  {
    "start": "2500000",
    "end": "2500000"
  },
  {
    "text": "so multi account model so like I said we have a number of projects not just you know collaboration accounts but we have",
    "start": "2501170",
    "end": "2506569"
  },
  {
    "text": "a dozen or so collaborations 20 or so on premise or company connected accounts",
    "start": "2506569",
    "end": "2512060"
  },
  {
    "text": "and this is how we manage them our total team - FTEs contacting company",
    "start": "2512060",
    "end": "2518660"
  },
  {
    "text": "a part-time FTE from Europe actually we have one additional headcount opening so if anyone's looking for a job let's",
    "start": "2518660",
    "end": "2525319"
  },
  {
    "text": "contact me but you know we have very very small staff and this is not only we do we also",
    "start": "2525319",
    "end": "2530599"
  },
  {
    "text": "maintain our on-premise clusters we have six or seven research site we have to maintain all that with the skeleton crew",
    "start": "2530599",
    "end": "2536750"
  },
  {
    "text": "so we have this tool from turbot that's their website there it helps us manage all these environments so 30 plus",
    "start": "2536750",
    "end": "2543920"
  },
  {
    "text": "accounts as by basically two people so we're talking hundreds the nodes automated security I'll talk a little",
    "start": "2543920",
    "end": "2550850"
  },
  {
    "text": "bit more about that but it really helps us manage all these environments security policies and automated auditing",
    "start": "2550850",
    "end": "2556730"
  },
  {
    "text": "between all the accounts",
    "start": "2556730",
    "end": "2560020"
  },
  {
    "start": "2560000",
    "end": "2560000"
  },
  {
    "text": "also one of the things that the tool allows to do is the harden the Amazon environment in which these",
    "start": "2561760",
    "end": "2567530"
  },
  {
    "text": "collaborations take place so you know we want to give these developers freedom in which to develop their software freedom",
    "start": "2567530",
    "end": "2574520"
  },
  {
    "text": "for the vendors to upload and maintain their software but what we can't do is have these developers compromise our",
    "start": "2574520",
    "end": "2581660"
  },
  {
    "text": "on-premise solutions so what we do with turbot is we isolate off all the network",
    "start": "2581660",
    "end": "2588080"
  },
  {
    "text": "controls so anything to do with VPC security groups peering the virtual",
    "start": "2588080",
    "end": "2593900"
  },
  {
    "text": "interfaces all that is restricted away from the project team and you know we are the IT group maintains that we work",
    "start": "2593900",
    "end": "2600830"
  },
  {
    "text": "with individual project teams to work on their security groups so they will tell us oh we need these security groups",
    "start": "2600830",
    "end": "2607370"
  },
  {
    "text": "these ports and IPS are problem they the project team then can take those",
    "start": "2607370",
    "end": "2612380"
  },
  {
    "text": "security groups and assign them to the verb to the virtual machines that they create so I don't need to get involved",
    "start": "2612380",
    "end": "2618230"
  },
  {
    "text": "they create them they can pick with the what security groups they want",
    "start": "2618230",
    "end": "2623380"
  },
  {
    "text": "object controls which is also very very important since s3 is not part of a V PC that is potentially accessible from",
    "start": "2623380",
    "end": "2630680"
  },
  {
    "text": "outside so what we do is with the turbot program we allow our end users to create",
    "start": "2630680",
    "end": "2635930"
  },
  {
    "text": "their own buckets don't call me out I don't really care projects and create their own buckets tool automatically",
    "start": "2635930",
    "end": "2641360"
  },
  {
    "text": "picks that up boom slap sauna up policy so we predefined a policy that's applied",
    "start": "2641360",
    "end": "2646850"
  },
  {
    "text": "to all buckets no matter who who made them credentials now we really don't like to",
    "start": "2646850",
    "end": "2653750"
  },
  {
    "text": "give out credentials sure as heck don't want them baked into any sort of virtual machine so with the turbot program we",
    "start": "2653750",
    "end": "2660830"
  },
  {
    "text": "can automate that we don't give out as three credentials s3 a little bit",
    "start": "2660830",
    "end": "2668260"
  },
  {
    "text": "and then auditing if there is a potential security violation the tool will find it and either fix it",
    "start": "2668260",
    "end": "2674180"
  },
  {
    "text": "automatically or tell me tell my team and then we'll come along and fix it",
    "start": "2674180",
    "end": "2679990"
  },
  {
    "start": "2680000",
    "end": "2680000"
  },
  {
    "text": "the collaborations in general of course we use ec2 in ECS SV the document",
    "start": "2680509",
    "end": "2687089"
  },
  {
    "text": "repository for everything EFS in the regions that we do use that do have it anyways we really want it in u.s. West",
    "start": "2687089",
    "end": "2693509"
  },
  {
    "text": "one of course VPC and Direct Connect some of the other services that we use EMR",
    "start": "2693509",
    "end": "2699589"
  },
  {
    "text": "what else a few other services those are the primary regions we use um the ones",
    "start": "2699589",
    "end": "2705029"
  },
  {
    "text": "without the stars those are the ones with the direct connect and we have a couple of collaborations that don't need the direct connect so they fire up in",
    "start": "2705029",
    "end": "2711239"
  },
  {
    "text": "the u.s. West to primarily because that's where EFS is a lot of people say",
    "start": "2711239",
    "end": "2716339"
  },
  {
    "text": "uh-oh why'd you go why'd you go AWS for speed of course but how our security and",
    "start": "2716339",
    "end": "2721769"
  },
  {
    "text": "isolation is very very big for us we are a pharma company we have a lot of good stuff and our on premise network is not sub",
    "start": "2721769",
    "end": "2729739"
  },
  {
    "text": "segmented it's a flat network so if we have a problem poof they're everywhere",
    "start": "2729739",
    "end": "2735299"
  },
  {
    "text": "about going into Amazon all these projects are instantly isolated from each other so in the event of a breach",
    "start": "2735299",
    "end": "2741180"
  },
  {
    "text": "we can woof that one little project poof you know it's gone but everything else is safe and of course elastic nature",
    "start": "2741180",
    "end": "2747979"
  },
  {
    "text": "some projects come along hey you know we could use 100 terabytes you know I could",
    "start": "2747979",
    "end": "2753239"
  },
  {
    "text": "do a hundred terabytes on-premise scientists though they're not not really",
    "start": "2753239",
    "end": "2760109"
  },
  {
    "text": "thinking down the road always so we had a project 100 terabytes up front three months later they're at a petabyte that",
    "start": "2760109",
    "end": "2766019"
  },
  {
    "text": "I can't do on premise access for our collaborations so these",
    "start": "2766019",
    "end": "2772499"
  },
  {
    "text": "are the software's for our bench chemists their cadre shion's that they do we don't write code we don't write",
    "start": "2772499",
    "end": "2778650"
  },
  {
    "text": "software we you know our job is to help find cancer cures we're not a software",
    "start": "2778650",
    "end": "2784259"
  },
  {
    "text": "company so most of our bench scientists we have common software that we purchase",
    "start": "2784259",
    "end": "2789779"
  },
  {
    "text": "how native that helps us do the collaborations so what what access do these vendors need what access do these",
    "start": "2789779",
    "end": "2795779"
  },
  {
    "text": "developers need the users come in with a single sign-on of course but the vendors themselves need additional access and we",
    "start": "2795779",
    "end": "2802680"
  },
  {
    "text": "make them tell us exactly what they're looking for and I am we don't say oh you",
    "start": "2802680",
    "end": "2808259"
  },
  {
    "text": "get start no no would you get star and you know it's a little bit of a negotiation",
    "start": "2808259",
    "end": "2813800"
  },
  {
    "text": "with these vendors they don't they it's kind of a give and take interestingly if the smaller the software company the",
    "start": "2813800",
    "end": "2820010"
  },
  {
    "text": "better they are at finding out or telling you what they need so some of the small companies what permissions you",
    "start": "2820010",
    "end": "2826640"
  },
  {
    "text": "need they already have the document on the website here you go we love yet large consulting companies not so much a",
    "start": "2826640",
    "end": "2833270"
  },
  {
    "text": "couple months ago a large consulting company said oh just give a star that's not gonna happen",
    "start": "2833270",
    "end": "2839230"
  },
  {
    "text": "bit of disagreement there the next day they then sent us a 40 page document pretty clear that they just went to",
    "start": "2839230",
    "end": "2845600"
  },
  {
    "text": "Amazon website copy it off the entire permission list and send it to me like oh that's not going to cut it either",
    "start": "2845600",
    "end": "2853150"
  },
  {
    "text": "our HPC environments so this is uh this is where our scientists developers come",
    "start": "2853270",
    "end": "2859070"
  },
  {
    "text": "in so we give them shell access but a lot of it depends on how they launch their software some of them have the",
    "start": "2859070",
    "end": "2865460"
  },
  {
    "text": "golden ami so that we have to work with them in which to to launch these we make",
    "start": "2865460",
    "end": "2870560"
  },
  {
    "text": "sure that the scientists themselves use good aim ice so we whitelist it so if you they come to us with an ami and it's",
    "start": "2870560",
    "end": "2876860"
  },
  {
    "text": "not white listed they can't watch it so we make sure that they have good software and the reason why they have",
    "start": "2876860",
    "end": "2882230"
  },
  {
    "text": "golden a me armies is that even though we don't want to be in the business of maintaining software or maintaining",
    "start": "2882230",
    "end": "2887990"
  },
  {
    "text": "images because like the HL guys are saying it's a ping every time there's a software image update and a new keys new",
    "start": "2887990",
    "end": "2894440"
  },
  {
    "text": "software you know we have to do a fair amount of work to do that but when you're watching thousand ten thousand",
    "start": "2894440",
    "end": "2900110"
  },
  {
    "text": "nodes the overhead of chef or another software deployment package five ten minutes adds up a lot times ten thousand",
    "start": "2900110",
    "end": "2909100"
  },
  {
    "text": "so these users they come in and via Bastion host the SSH in we don't give them keys and which to do that they well",
    "start": "2909760",
    "end": "2916820"
  },
  {
    "text": "they have SSH keys but they don't have a Amazon keys that Bastion host then has an ami role I am role and they can then",
    "start": "2916820",
    "end": "2924440"
  },
  {
    "text": "access all the other Amazon services from their AWS console nobody has a direct login",
    "start": "2924440",
    "end": "2932290"
  },
  {
    "text": "database console not even me no one in not even no one in nineteen nobody so we have the turbot program that allows us",
    "start": "2932290",
    "end": "2938930"
  },
  {
    "text": "so it's a single sign-on into this platform we then use that platform to launch into M into the AWS console the",
    "start": "2938930",
    "end": "2946190"
  },
  {
    "text": "SDS token the good thing about that is no me and my eunuchs I've been here we have what",
    "start": "2946190",
    "end": "2951620"
  },
  {
    "text": "30 plus accounts in which that we have to manage can't manage 30 different passwords for 30 different consoles",
    "start": "2951620",
    "end": "2956930"
  },
  {
    "text": "that's crazy but now we have a single login and then poof we're into any one of them in the event that one of us",
    "start": "2956930",
    "end": "2963620"
  },
  {
    "text": "leaves a company you can quickly move that person from that single console and they're removed from all the all the sub",
    "start": "2963620",
    "end": "2969380"
  },
  {
    "text": "projects I'm not going to each one of these but",
    "start": "2969380",
    "end": "2975320"
  },
  {
    "start": "2972000",
    "end": "2972000"
  },
  {
    "text": "these are some of the access rules that we put in place for our buckets so it's automated like I said the projects",
    "start": "2975320",
    "end": "2980990"
  },
  {
    "text": "themselves are allowed to create their own buckets robot comes along poof slaps on the on this rule so in general we",
    "start": "2980990",
    "end": "2987050"
  },
  {
    "text": "require server-side encryption for everything as well as encryption in transit depending on what the project",
    "start": "2987050",
    "end": "2993710"
  },
  {
    "text": "needs we may increase that or decrease that but by default that's what they get and then it's a negotiation you know if",
    "start": "2993710",
    "end": "2999530"
  },
  {
    "text": "they need last month anymore they can come talk to us about that we also use the bucket policies than I",
    "start": "2999530",
    "end": "3006670"
  },
  {
    "text": "am policies to put in place business rules so we have a number of collaborations number of universities",
    "start": "3006670",
    "end": "3012760"
  },
  {
    "text": "and a lot of universities they have a little sensitivity about you know well",
    "start": "3012760",
    "end": "3017860"
  },
  {
    "text": "if another university is in there and maybe they can publish first okay all right let's get together guys so what we",
    "start": "3017860",
    "end": "3024400"
  },
  {
    "text": "do with our buckets is we have a particular bucket that's the use for upload only and anyone any other",
    "start": "3024400",
    "end": "3030340"
  },
  {
    "text": "projects can upload their our data science team then comes along and manages that data each organization has",
    "start": "3030340",
    "end": "3035830"
  },
  {
    "text": "a slightly different data format we put them in a common format for the collaboration and then we IT or the data",
    "start": "3035830",
    "end": "3041860"
  },
  {
    "text": "science team moves that into the final repository that final repository is not readable from the outside so you know we",
    "start": "3041860",
    "end": "3049720"
  },
  {
    "text": "have a petabyte of data people are sensitive we don't want collaborator a stealing from cadre derby fine this",
    "start": "3049720",
    "end": "3056890"
  },
  {
    "text": "bucket is only accessible from within the VPC that we write",
    "start": "3056890",
    "end": "3062580"
  },
  {
    "text": "so what do we do with all this data now we use Enterprise github to manage the code so we have a number of accounts we",
    "start": "3064350",
    "end": "3070810"
  },
  {
    "text": "use the github it's accessible from not only one account with all the accounts and if a scientist is traveling and they",
    "start": "3070810",
    "end": "3076270"
  },
  {
    "text": "want to work on their some code while they're traveling on an airplane they can do that without getting into our",
    "start": "3076270",
    "end": "3081640"
  },
  {
    "text": "Amazon environment they just got to get hub check out the files and work on it data you know we are talking a massive",
    "start": "3081640",
    "end": "3088119"
  },
  {
    "text": "quantity of data here that you know our legal team ten years ago didn't even think about it was just",
    "start": "3088119",
    "end": "3094510"
  },
  {
    "text": "wasn't on the radar however you know we do need right now our data retention policies in amount of three years some",
    "start": "3094510",
    "end": "3101440"
  },
  {
    "text": "data needs be kept forever and if you see the bills like I do that's an enormous amount of money or if not",
    "start": "3101440",
    "end": "3107830"
  },
  {
    "text": "competive I'd scale data but working with the legal team on the data science team on what to do with the quantity of",
    "start": "3107830",
    "end": "3113380"
  },
  {
    "text": "data currently right now we're sending it all the glacier lifecycle rules and then hopefully over the next twelve",
    "start": "3113380",
    "end": "3120730"
  },
  {
    "text": "months or so we'll have a long-term solution for some of our collaborations we are you know with the cancer moonshot",
    "start": "3120730",
    "end": "3126430"
  },
  {
    "text": "we are working to open source the data so there's some contractual issues there but long term that's what we want to do",
    "start": "3126430",
    "end": "3133000"
  },
  {
    "text": "here this is my last slide so some of the",
    "start": "3133000",
    "end": "3138580"
  },
  {
    "start": "3135000",
    "end": "3135000"
  },
  {
    "text": "lessons that we've learned here expect failure we're talking tens of thousands of nodes",
    "start": "3138580",
    "end": "3144580"
  },
  {
    "text": "sometimes on a single day there's gonna be some failures and we cannot manually fix 10,000 nodes you have to automate",
    "start": "3144580",
    "end": "3151000"
  },
  {
    "text": "some sort of repair mechanism ideally you know if anyone here heard pets",
    "start": "3151000",
    "end": "3157420"
  },
  {
    "text": "versus cattle Google that we've tried to get our scientists to adhere to that",
    "start": "3157420",
    "end": "3162550"
  },
  {
    "text": "concept if there's a problem the virtual machine cut leather you start the rebuild itself you services as they are",
    "start": "3162550",
    "end": "3169720"
  },
  {
    "text": "intended we're still struggling with this a little bit one of the key things that were",
    "start": "3169720",
    "end": "3175510"
  },
  {
    "text": "violating here every best practices because of our users are going directly into the s3 file system with cyberduck",
    "start": "3175510",
    "end": "3181869"
  },
  {
    "text": "or or cloudberry they need a common folder structure in which to navigate to find their files so you know the first",
    "start": "3181869",
    "end": "3189430"
  },
  {
    "text": "five ten forty characters of a particular key are the same and when you launch a thousand ten thousand nodes to",
    "start": "3189430",
    "end": "3196180"
  },
  {
    "text": "hit that same partition you're gonna over at Ewing out of text that's three file system there's nothing that we can",
    "start": "3196180",
    "end": "3203020"
  },
  {
    "text": "do about that and we've been yelled at many times about that's three team though event users you know what are you gonna do",
    "start": "3203020",
    "end": "3208589"
  },
  {
    "text": "another thing that we do are wordly working with our scientists is to not",
    "start": "3208589",
    "end": "3213849"
  },
  {
    "text": "use a fuse file system to mount s3 we have some software unfortunately they cannot read s3 it's looking for a block",
    "start": "3213849",
    "end": "3220840"
  },
  {
    "text": "storage system so the scientist use s3 CMD to mount a petabyte scale s3 bucket",
    "start": "3220840",
    "end": "3227650"
  },
  {
    "text": "on a UNIX file system and it chokes regularly",
    "start": "3227650",
    "end": "3232800"
  },
  {
    "text": "data transferred you know we're talking hundreds of millions of files being sent around very very small percentage",
    "start": "3233340",
    "end": "3239050"
  },
  {
    "text": "failure but we do have some failures it's very very important to check md5",
    "start": "3239050",
    "end": "3244080"
  },
  {
    "text": "some of the soft left lessons here that we've learned you know we are our companies not that old but in the modern",
    "start": "3244080",
    "end": "3251800"
  },
  {
    "text": "you know the company is over at 25 30 years old we have a lot of traditional enterprise",
    "start": "3251800",
    "end": "3258700"
  },
  {
    "text": "folks here and it's a big jump the cloud is not the same and it's been challenging for us to work with some of",
    "start": "3258700",
    "end": "3265360"
  },
  {
    "text": "the folks who want to use their past experience they you want to help us out but it's a challenge odo",
    "start": "3265360",
    "end": "3271680"
  },
  {
    "text": "vendors and users we do work a lot with a lot of small companies a new employees",
    "start": "3271680",
    "end": "3277240"
  },
  {
    "text": "to our company are also coming from startups and in the past they're used to putting on the credit card and buying an",
    "start": "3277240",
    "end": "3282400"
  },
  {
    "text": "Amazon account okay full root access well you come into Celgene you're not getting star stars and sse4 users is it",
    "start": "3282400",
    "end": "3291760"
  },
  {
    "text": "has been very very interesting we tell the users every time you give them a set of keys you have to have server-side",
    "start": "3291760",
    "end": "3297370"
  },
  {
    "text": "encryption this is how you do it okay good they go to upload access tonight oh",
    "start": "3297370",
    "end": "3303090"
  },
  {
    "text": "hey guys you didn't give me access long yeah we did this is how you do it on the",
    "start": "3303090",
    "end": "3308230"
  },
  {
    "text": "last step last us in here is to get buy-in from the security team they are they in the very beginning they were it",
    "start": "3308230",
    "end": "3315670"
  },
  {
    "text": "was hard working with them but now they know my second phone call but I make",
    "start": "3315670",
    "end": "3321100"
  },
  {
    "text": "after getting a project request as a security team and we're now best friends we get along great they trust me I trust",
    "start": "3321100",
    "end": "3327160"
  },
  {
    "text": "them we're here to work for the users that's why we're thank you very much",
    "start": "3327160",
    "end": "3333420"
  },
  {
    "text": "okay so just a quick you know AWS summary",
    "start": "3338430",
    "end": "3345120"
  },
  {
    "start": "3345000",
    "end": "3345000"
  },
  {
    "text": "so I hope you guys enjoyed you know reviewing the architecture and some of the lessons learned from both of these",
    "start": "3345120",
    "end": "3350410"
  },
  {
    "text": "projects I think they've been just done tremendous work both have really you know kind of similar advantages",
    "start": "3350410",
    "end": "3356910"
  },
  {
    "text": "you know in terms of allowing for rapid infrastructure deployment isolated work areas a lot of the stuff that Lance",
    "start": "3356910",
    "end": "3364660"
  },
  {
    "text": "referred to and being able to really isolate users and different collaborative collaborative efforts from",
    "start": "3364660",
    "end": "3369790"
  },
  {
    "text": "each other could be you know just very valuable and it's almost essential in this area they've been able to draw you",
    "start": "3369790",
    "end": "3376330"
  },
  {
    "text": "know common components into a larger larger reusable framework utilize all the elastic resources",
    "start": "3376330",
    "end": "3382720"
  },
  {
    "text": "available to them and of course you know and especially in the cases of things like Celgene it's accessible worldwide",
    "start": "3382720",
    "end": "3388930"
  },
  {
    "text": "so that you know you can reach and have collaborative collaborators anywhere on",
    "start": "3388930",
    "end": "3394480"
  },
  {
    "text": "the planet and still reach a common infrastructure and hopefully you know use all this to drive toward you know",
    "start": "3394480",
    "end": "3400330"
  },
  {
    "text": "reliable and reproducible collaborative science that are at a scale that was previously unachievable and",
    "start": "3400330",
    "end": "3407130"
  },
  {
    "text": "that would be about it we thank you for attending this talk we'll be able to take some questions not",
    "start": "3407130",
    "end": "3413500"
  },
  {
    "text": "here but outside so you can catch us in between now in the next session which",
    "start": "3413500",
    "end": "3418600"
  },
  {
    "text": "begins at 2:20 in this room thank you",
    "start": "3418600",
    "end": "3424440"
  },
  {
    "text": "[Applause]",
    "start": "3425360",
    "end": "3430010"
  }
]