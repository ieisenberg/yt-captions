[
  {
    "start": "0",
    "end": "29000"
  },
  {
    "text": "hello can you guys all hear me everything good here how you guys doing happy to be here you probably heard that",
    "start": "1440",
    "end": "8040"
  },
  {
    "text": "a couple times today so uh my name is Wes McDonald I am the technical director at Turtle rock",
    "start": "8040",
    "end": "14120"
  },
  {
    "text": "studios and today I'm going to tell you about evolve and what it took for us to ship our game on ec2 as the talk the",
    "start": "14120",
    "end": "22279"
  },
  {
    "text": "title of this talk is hunting monsters in a low latency multiplayer game on Amazon ec2 uh there are a couple different",
    "start": "22279",
    "end": "29000"
  },
  {
    "start": "29000",
    "end": "29000"
  },
  {
    "text": "points that I want to first I'm going to tell you about what is Ave uh and then we'll talk about what",
    "start": "29000",
    "end": "36360"
  },
  {
    "text": "it takes to run a realtime game in Amazon and then a large part of our",
    "start": "36360",
    "end": "42120"
  },
  {
    "text": "system is a rest-based server reservation uh this allows us to allocate servers for players to use and",
    "start": "42120",
    "end": "48719"
  },
  {
    "text": "then I'll talk about how we automated everything because that was a huge key for our success after that I'll talk about how",
    "start": "48719",
    "end": "55480"
  },
  {
    "text": "we took the whole system Global and then our autoscaling system and then I'll",
    "start": "55480",
    "end": "60680"
  },
  {
    "text": "give you a good idea of kind of how we take a look good view into our monitoring and metrics so we can keep an eye on this big system so the first",
    "start": "60680",
    "end": "67799"
  },
  {
    "text": "thing uh we'll talk about is what is evolved and before I try and describe it",
    "start": "67799",
    "end": "72840"
  },
  {
    "text": "in words I think it's it's better if I just show you a video here see if this",
    "start": "72840",
    "end": "79280"
  },
  {
    "text": "starts is it not playing audio com",
    "start": "83400",
    "end": "90820"
  },
  {
    "text": "[Music] [Laughter]",
    "start": "90820",
    "end": "94390"
  },
  {
    "text": "[Laughter]",
    "start": "101090",
    "end": "104810"
  },
  {
    "text": "big monster tracks broken trees animal corpses and bird sign are all indicators",
    "start": "111520",
    "end": "119479"
  },
  {
    "text": "of the Monster's presence the round ends when one team is",
    "start": "119479",
    "end": "124560"
  },
  {
    "text": "eliminated or the monster makes it to stage three and destroys the power",
    "start": "124560",
    "end": "131319"
  },
  {
    "text": "relay all right so then I guess I will be describing it in words so evolve is a five player action",
    "start": "134519",
    "end": "142080"
  },
  {
    "start": "137000",
    "end": "137000"
  },
  {
    "text": "game uh the key point is that it's 4v1 that's four firstperson shooter Hunters",
    "start": "142080",
    "end": "148360"
  },
  {
    "text": "versus a third person shooter monster and they're all in the same game playing against each other the hunters are",
    "start": "148360",
    "end": "153720"
  },
  {
    "text": "trying to kill the monster so they're chasing him around the map uh and then that monster is trying to eat Wildlife",
    "start": "153720",
    "end": "160400"
  },
  {
    "text": "uh and when he eats more of the wildlife he can get stronger stage up and then he can turn the battle back on the hunters",
    "start": "160400",
    "end": "166159"
  },
  {
    "text": "and go and kill them so a big point is that it is five players uh but beyond",
    "start": "166159",
    "end": "172080"
  },
  {
    "start": "172000",
    "end": "172000"
  },
  {
    "text": "that there's also 30 plus AI Wildlife that the monster's going after and so",
    "start": "172080",
    "end": "177280"
  },
  {
    "text": "that gives us a very high uh equivalent kind of player count that you would see in a normal first-person shooter that",
    "start": "177280",
    "end": "183840"
  },
  {
    "text": "would be let's say 40 players or something like that we have a very similar kind of CPU processing load uh we are using the cry engine uh and what",
    "start": "183840",
    "end": "191280"
  },
  {
    "text": "that brings is uh it it handles a lot of the rendering and the physics and we build kind of our game logic on top of",
    "start": "191280",
    "end": "197159"
  },
  {
    "text": "that uh and it's kind of the package that everything's wrapped in and that's what the base of our system is built off",
    "start": "197159",
    "end": "202599"
  },
  {
    "text": "of uh and so using the cry engine and putting this all together we have uh",
    "start": "202599",
    "end": "209360"
  },
  {
    "start": "208000",
    "end": "208000"
  },
  {
    "text": "this process process that builds everything together and and it's the executables uh we package up these",
    "start": "209360",
    "end": "215760"
  },
  {
    "text": "assets and those assets are kind of uh the textures or the animations uh a lot",
    "start": "215760",
    "end": "221159"
  },
  {
    "text": "of the models the maps everything gets packaged together and then we create two different packages one is the client and",
    "start": "221159",
    "end": "228000"
  },
  {
    "text": "that we ship for Windows PC Xbox One PS4 uh and then we also have the server",
    "start": "228000",
    "end": "233599"
  },
  {
    "text": "version which we ship for Linux uh and it's stripped down we take the textures",
    "start": "233599",
    "end": "238760"
  },
  {
    "text": "out like we don't need those on the server and anything else that we don't need on the server we kind of take out of there to reduce the space and so what",
    "start": "238760",
    "end": "245239"
  },
  {
    "text": "you end up with here is a client that can connect to a dedicated server binary",
    "start": "245239",
    "end": "252799"
  },
  {
    "text": "given a proper IP address and Port configuration so they would have to know where on the internet to go what port to",
    "start": "252799",
    "end": "258880"
  },
  {
    "text": "talk to but just through this process we have that together and so given",
    "start": "258880",
    "end": "265840"
  },
  {
    "text": "that uh we can start talking about what it takes to bring that into AWS and so that's running a real-time game",
    "start": "265840",
    "end": "272160"
  },
  {
    "text": "simulation in Amazon so traditionally games have been",
    "start": "272160",
    "end": "278479"
  },
  {
    "start": "274000",
    "end": "274000"
  },
  {
    "text": "collocated when you want to run an online game uh it's pretty common to go and rent space in a data center start",
    "start": "278479",
    "end": "285840"
  },
  {
    "text": "physically putting Hardware in there you have to have teams of people all over the world ready to maintain that",
    "start": "285840",
    "end": "290880"
  },
  {
    "text": "Hardware uh and you have to know or kind of guess what your load is going to be at launch and that's a really difficult",
    "start": "290880",
    "end": "298160"
  },
  {
    "text": "thing to do and it's not the easiest problem to solve and there's actually some interesting solutions to that and",
    "start": "298160",
    "end": "304199"
  },
  {
    "text": "you you can go down that path but one really good solution I've found is that to use the cloud and using the cloud",
    "start": "304199",
    "end": "311160"
  },
  {
    "text": "simplifies a lot of that maintenance and requirement for hardware and allows us",
    "start": "311160",
    "end": "316440"
  },
  {
    "text": "to more easily just build it out in a virtual infrastructure I think all of you guys are pretty aware of what the",
    "start": "316440",
    "end": "321720"
  },
  {
    "text": "cloud is being here at AWS reinvent so uh one of the most important things I noticed though is that when you",
    "start": "321720",
    "end": "328280"
  },
  {
    "text": "collocate or running your own infrastructure you might have to maintain your own switches and hardware",
    "start": "328280",
    "end": "333919"
  },
  {
    "text": "and so you would kind of estimate what your traffic loads are going to be and you'll purchase routers that maintain that and and can handle those kinds of",
    "start": "333919",
    "end": "339600"
  },
  {
    "text": "traffic loads and when there's a problem it affects 100% of your application",
    "start": "339600",
    "end": "344759"
  },
  {
    "text": "whereas if there's a problem in Amazon you might be one of a thousand 100 thousand people and so a small problem",
    "start": "344759",
    "end": "350520"
  },
  {
    "text": "in Amazon only affects a small problem a small portion of people that in that service whereas if it's your own",
    "start": "350520",
    "end": "356160"
  },
  {
    "text": "infrastructure any problem can be a major problem so one of the biggest",
    "start": "356160",
    "end": "361720"
  },
  {
    "text": "barriers that we ran into trying to work with the cloud is getting everybody comfortable with the fact that we were",
    "start": "361720",
    "end": "367400"
  },
  {
    "text": "going to run a real-time simulation in a virtualized environment and a a lot of a",
    "start": "367400",
    "end": "372759"
  },
  {
    "text": "lot of people had some doubts that that was even possible uh there was a lot of talk about CPU time slicing noisy",
    "start": "372759",
    "end": "378280"
  },
  {
    "text": "neighbors like if someone's eating CPU next to you on another machine or on the same machine they could just be taking",
    "start": "378280",
    "end": "383599"
  },
  {
    "text": "your resources away that was a lot of the fears so we went through a lot of process making sure that all the",
    "start": "383599",
    "end": "389680"
  },
  {
    "text": "resource allocation is completely strict when Amazon says you get a certain number of cores or compute units you",
    "start": "389680",
    "end": "395080"
  },
  {
    "text": "actually get that and that is yours no one can take that from you when you get a certain amount of memory you actually get that that's yours uh same with",
    "start": "395080",
    "end": "401880"
  },
  {
    "text": "network IO uh and so using our automated testing we were able to prove that it",
    "start": "401880",
    "end": "407680"
  },
  {
    "text": "does work and also I can add confidence for you guys if you're talking about doing this that we did ship it it was",
    "start": "407680",
    "end": "412919"
  },
  {
    "text": "successful and so you can use that as well if you like so uh when moving something when",
    "start": "412919",
    "end": "421000"
  },
  {
    "start": "417000",
    "end": "417000"
  },
  {
    "text": "trying to determine what kind of Hardware you need you need to kind of understand what your resource requirements are and for us we knew that",
    "start": "421000",
    "end": "426960"
  },
  {
    "text": "we wanted to be Memory bound and not CPU bound and this is really important",
    "start": "426960",
    "end": "432280"
  },
  {
    "text": "because we were a lot more stable kind of measuring exactly how much memory we need we knew that as long as we stayed",
    "start": "432280",
    "end": "437840"
  },
  {
    "text": "within that uh it was really stable that we can always kind of shut them the processes down load them back up and",
    "start": "437840",
    "end": "442960"
  },
  {
    "text": "we'll stay within our memory range whereas CPU if you're constantly running at 90% CPU and some other process on the",
    "start": "442960",
    "end": "449840"
  },
  {
    "text": "OS decides to like back up logs or something you could see a spike in CPU that would drop every process on your",
    "start": "449840",
    "end": "455120"
  },
  {
    "text": "machine whereas memory was very stable so when we were trying to determine what",
    "start": "455120",
    "end": "461240"
  },
  {
    "text": "uh AWS instance would work well for us uh we used automated testing and we have a mode in our game that allows us to",
    "start": "461240",
    "end": "467240"
  },
  {
    "text": "play Four bot humans or the the the Hunters versus one bot monster and they",
    "start": "467240",
    "end": "473159"
  },
  {
    "text": "can just play over and over and over again so we just took every instance we could find in Amazon and just ran as",
    "start": "473159",
    "end": "479560"
  },
  {
    "text": "many machines on them as we could until they just burned to the ground uh and then we kind of figured out where the best balance was and for us that balance",
    "start": "479560",
    "end": "487039"
  },
  {
    "text": "turned out to be the C3 instance uh it gave us the best CPU to memory ratio uh",
    "start": "487039",
    "end": "492159"
  },
  {
    "text": "and then it also came with the ephemeral drives which we actually took a lot of advantage of those Epal drives we we",
    "start": "492159",
    "end": "498199"
  },
  {
    "text": "raid zero striped them uh and then that gave us a ton of ioops essentially for free on the machine and and one of the",
    "start": "498199",
    "end": "505280"
  },
  {
    "text": "The Core Concepts of what our game does is when players start a the actual",
    "start": "505280",
    "end": "510479"
  },
  {
    "text": "dedicated server process will launch from a cold start and have to load the whole level up so having all that IO",
    "start": "510479",
    "end": "516000"
  },
  {
    "text": "bursting IO was really really good for us also it gave us a lot of space for the executables the assets uh all of our",
    "start": "516000",
    "end": "523360"
  },
  {
    "text": "logs we also have core dumps uh the game does crash sometimes I'll admit it uh",
    "start": "523360",
    "end": "529480"
  },
  {
    "text": "and so we want to keep those around so that we can investigate that and understand why it happens um and then",
    "start": "529480",
    "end": "534600"
  },
  {
    "text": "beyond that we also use the fals for our swap space even though we set and we know exactly how much phys IAL Ram we're",
    "start": "534600",
    "end": "540000"
  },
  {
    "text": "getting in our VMS uh we made sure that we had a little bit extra in our swap space for if we want to go on a machine",
    "start": "540000",
    "end": "546279"
  },
  {
    "text": "and debug something or during a core dump process or just for whatever reason we don't want the whole thing to just",
    "start": "546279",
    "end": "551600"
  },
  {
    "text": "burn if we go a little bit over that memory so then beyond that we've also got Network we had to worry about uh and",
    "start": "551600",
    "end": "559440"
  },
  {
    "start": "556000",
    "end": "556000"
  },
  {
    "text": "so with the C3 it did have the enhanced networking that was not a huge gain for us we actually found that all of the",
    "start": "559440",
    "end": "565519"
  },
  {
    "text": "latency to all of the instances in Amazon were actually really good it's actually the internet that's your biggest latency worry uh and then beyond",
    "start": "565519",
    "end": "572920"
  },
  {
    "text": "that we have a really low bandwidth UDP protocol that we're using and that that a lot of that comes from the fact that",
    "start": "572920",
    "end": "578399"
  },
  {
    "text": "we're using peer-to-peer as well we have a we have a fallback path to a peer-to-peer based game and so we",
    "start": "578399",
    "end": "584279"
  },
  {
    "text": "optimize for that and so the dedicated servers got to take a lot of advantage of that as",
    "start": "584279",
    "end": "590320"
  },
  {
    "text": "well so this is what our C3 instance looks like so just a base instance we",
    "start": "590360",
    "end": "595680"
  },
  {
    "text": "start it up and we have a process on here that we call the process manager uh the launcher and so his job given the",
    "start": "595680",
    "end": "603399"
  },
  {
    "text": "whatever instance it is whether it's a C3 large extra large whatever size it is uh we know how many processes of our",
    "start": "603399",
    "end": "610920"
  },
  {
    "text": "game we can run on there and so his job is based on that type to launch those and so he'll he'll start one then he'll",
    "start": "610920",
    "end": "616519"
  },
  {
    "text": "start the rest of them and he'll make sure and maintain those and so if anything happens to these guys if they need to be cycled or or something",
    "start": "616519",
    "end": "621920"
  },
  {
    "text": "happens to them he will always maintain that number of processes on that machine so now we can take that and so",
    "start": "621920",
    "end": "630200"
  },
  {
    "text": "given that we now have a machine that can have an IP address a couple of ports on it we'll back up here real quick so",
    "start": "630200",
    "end": "636360"
  },
  {
    "text": "each one of those green boxes of the process uh it might have a different port or it will have a different port so",
    "start": "636360",
    "end": "642000"
  },
  {
    "text": "as long as this we know the C3 IP address to this one instance we can give that to any of our clients and then they",
    "start": "642000",
    "end": "647519"
  },
  {
    "text": "can talk to any one of these four processes knowing the right Port they can connect to it and so now we have a",
    "start": "647519",
    "end": "652639"
  },
  {
    "text": "server running in Amazon there's a couple of them on there and then the clients can connect to it so after that",
    "start": "652639",
    "end": "658279"
  },
  {
    "text": "we want to create a process that takes those servers that we're running and gives them to clients so they know where",
    "start": "658279",
    "end": "664360"
  },
  {
    "text": "to go because right now you would have to manually hand them an IP address and Port so our process actually starts even",
    "start": "664360",
    "end": "672079"
  },
  {
    "start": "669000",
    "end": "669000"
  },
  {
    "text": "further back than that is actually getting players together uh the first thing we do is our matchmaking and it's",
    "start": "672079",
    "end": "677720"
  },
  {
    "text": "kind of our lobby based system so here we have a host and a client and let's",
    "start": "677720",
    "end": "682959"
  },
  {
    "text": "say they're in a party together or something like that and and I'm the host and I decide that me and my my buddy we're going to go we're gonna go find a",
    "start": "682959",
    "end": "688959"
  },
  {
    "text": "game so the first thing we do is we talk to uh matchmaking and we're using first-party matchmaking we're using Xbox",
    "start": "688959",
    "end": "695480"
  },
  {
    "text": "Live PSN steam they are the matchmaking platform that we're talking to so at this point we're not even talking to any",
    "start": "695480",
    "end": "700720"
  },
  {
    "text": "of our Amazon infrastructure we're just getting players together in a game so that lobby as I was saying is is",
    "start": "700720",
    "end": "707560"
  },
  {
    "text": "peer-to-peer so it's not talking to anything else they're just talking to each other they're maintaining this peer-to-peer session within",
    "start": "707560",
    "end": "713240"
  },
  {
    "text": "themselves and this is actually a really important part for our early launch was uh this peer-to-peer network is is",
    "start": "713240",
    "end": "719320"
  },
  {
    "text": "actually a complete fallback for us that at any moment that host machine could be a server now the experience is not as",
    "start": "719320",
    "end": "726120"
  },
  {
    "text": "good as we'd like uh it's definitely not as good as a dedicated server but it could be in the worst case something",
    "start": "726120",
    "end": "731560"
  },
  {
    "text": "that can run the game uh and so we could use it as a fallback it's also possible that if we decided to for example some",
    "start": "731560",
    "end": "737760"
  },
  {
    "text": "game modes we didn't want to have dedicated service for that game modes we could say just run peer-to-peer and so",
    "start": "737760",
    "end": "743399"
  },
  {
    "text": "anybody playing that say custom games or something like that they could play in peer-to-peer so this is a client host architecture",
    "start": "743399",
    "end": "750839"
  },
  {
    "text": "and it's always maintained even when you're playing on a dedicated server the players all have a client host kind of",
    "start": "750839",
    "end": "757160"
  },
  {
    "text": "peer-to-peer connection going on and we pass VoIP through that uh it's also just a messaging interface it's what happens",
    "start": "757160",
    "end": "763079"
  },
  {
    "text": "when the game ends you disconnect from the dedicated server they're still in this configuration talking to each other get ready for the next map whatever",
    "start": "763079",
    "end": "769000"
  },
  {
    "text": "they're going to do so now we found three other people so we've got a full",
    "start": "769000",
    "end": "774199"
  },
  {
    "text": "game we've got five guys so they're all talking to each other in this peer-to-peer configuration",
    "start": "774199",
    "end": "780399"
  },
  {
    "text": "and now we can bring in our Amazon infrastructure so here we've got a region and we've got our external load",
    "start": "781240",
    "end": "787160"
  },
  {
    "text": "balancer and it's the host of this Lobby it's his job to make the request to",
    "start": "787160",
    "end": "792639"
  },
  {
    "text": "Amazon or our infrastructure in Amazon uh and so he says hey I need a game server for this game that we're playing",
    "start": "792639",
    "end": "799480"
  },
  {
    "text": "so now we can flip around and take a look at the other side here which is so",
    "start": "799480",
    "end": "804959"
  },
  {
    "start": "801000",
    "end": "801000"
  },
  {
    "text": "when he's connected to that external load balancer he's made a request and now he's talking to our application servers",
    "start": "804959",
    "end": "810399"
  },
  {
    "text": "so over here we've got the same game servers I was talking about before and they've got those those server instances",
    "start": "810399",
    "end": "816000"
  },
  {
    "text": "on them and so now they need to talk to the application servers as well so we have an internal load balancer talking",
    "start": "816000",
    "end": "821920"
  },
  {
    "text": "to our application servers and so all of our game servers connect to that internal load balancer and then they now",
    "start": "821920",
    "end": "828920"
  },
  {
    "text": "have a database that's keeping track of that state so you can imagine that so these dedicated servers they're",
    "start": "828920",
    "end": "834440"
  },
  {
    "text": "constantly checking and they're saying hey I'm idle I'm ready to start a game here's here's where I'm at this is my my",
    "start": "834440",
    "end": "840040"
  },
  {
    "text": "IP address and my port and the application servers are tracking that and then the servers keep checking in saying am I still idle am I still idle",
    "start": "840040",
    "end": "846920"
  },
  {
    "text": "just repeating that over and we can have a whole bunch of these just scaled out sideways all horizontal and they're just",
    "start": "846920",
    "end": "852199"
  },
  {
    "text": "all checking in constantly saying I'm idle request comes in and now the",
    "start": "852199",
    "end": "857320"
  },
  {
    "text": "application server changes the database and says no you're not idle you're actually pending and you have to start",
    "start": "857320",
    "end": "863079"
  },
  {
    "text": "this map it's going to have these players in it they want this game configuration this is the game you're going to launch so the next time that",
    "start": "863079",
    "end": "868519"
  },
  {
    "text": "server checks in he finds out okay I need to start a game so he is now knows",
    "start": "868519",
    "end": "874360"
  },
  {
    "text": "that he needs to start a game and with that uh the polling interval is normally",
    "start": "874360",
    "end": "880600"
  },
  {
    "text": "say 5 Seconds when they're Idol so they're constantly hitting it over and over and over again now they're going to start a game and that pulling interval",
    "start": "880600",
    "end": "886759"
  },
  {
    "text": "decreases so now that they're running in a game they're probably say maybe 20 seconds they'll check in just constantly",
    "start": "886759",
    "end": "892519"
  },
  {
    "text": "saying hey we're I'm running a game this is my current state uh what's interesting about that is when you have",
    "start": "892519",
    "end": "898759"
  },
  {
    "text": "a lot of lot of servers idle so we we kind of start all the servers up we have a lot of space available our load on our",
    "start": "898759",
    "end": "904360"
  },
  {
    "text": "application servers is higher than when the player load is coming in because when the servers are idle they're",
    "start": "904360",
    "end": "910240"
  },
  {
    "text": "hitting that system harder and so as the player count increases our server load decreases on our applications kind of a",
    "start": "910240",
    "end": "916279"
  },
  {
    "text": "upside down graph there the other interesting piece is that because everything's constantly checking in the",
    "start": "916279",
    "end": "921320"
  },
  {
    "text": "database isn't actually required for persistence it's only there because we needed a transactional data store so if",
    "start": "921320",
    "end": "927920"
  },
  {
    "text": "for any reason we blew that database away it had a error whatever we just wiped it and we just put a new one in within 20 seconds it would be fully",
    "start": "927920",
    "end": "934160"
  },
  {
    "text": "repopulated and the system is completely operational again so we've uh that request went",
    "start": "934160",
    "end": "942880"
  },
  {
    "start": "939000",
    "end": "939000"
  },
  {
    "text": "through he got a server the server came back and now that IP port server configuration is being sent back to the host he then sends that to the other",
    "start": "942880",
    "end": "950160"
  },
  {
    "text": "clients and all the other clients now know okay over UDP I connected this IP address and port and now we can run a",
    "start": "950160",
    "end": "956959"
  },
  {
    "text": "game so at this point this is actually a shippable solution you can have one region servers running",
    "start": "956959",
    "end": "964279"
  },
  {
    "text": "players can find them they can get a server they can connect to a server and they can play a game so it's it's a full",
    "start": "964279",
    "end": "971279"
  },
  {
    "text": "view into what you would need to run an online game now we'll move forward to",
    "start": "971279",
    "end": "977639"
  },
  {
    "text": "taking that even further and we'll talk about the automation side of things so obviously one of the biggest reasons you",
    "start": "977639",
    "end": "983079"
  },
  {
    "text": "want to automate uh is people suck uh you you can make as many many",
    "start": "983079",
    "end": "989079"
  },
  {
    "text": "checklists as you want uh it's much procedure they will always make mistakes um computers are a lot better at",
    "start": "989079",
    "end": "994639"
  },
  {
    "text": "following directions um so anything done manually is a liability so for us we",
    "start": "994639",
    "end": "1000839"
  },
  {
    "text": "built a server automation system uh and most of that is our monitoring and",
    "start": "1000839",
    "end": "1005880"
  },
  {
    "start": "1001000",
    "end": "1001000"
  },
  {
    "text": "operation system put together into one big application it gave us a really nice",
    "start": "1005880",
    "end": "1011040"
  },
  {
    "text": "area to set up uh authentication authorization and accounting uh and that lets us make sure that we don't need to",
    "start": "1011040",
    "end": "1017560"
  },
  {
    "text": "give everybody access to AWS if they want a view into our system we can give them as much view as we want we can give",
    "start": "1017560",
    "end": "1023079"
  },
  {
    "text": "them as much control as we want and we are able to run that internally and decide exactly what's going",
    "start": "1023079",
    "end": "1028199"
  },
  {
    "text": "on and so we also have any action that you can perform on the system through",
    "start": "1028199",
    "end": "1033798"
  },
  {
    "text": "this is all predefined and it may be very complex actions something like I want to start up a whole new region from",
    "start": "1033799",
    "end": "1039438"
  },
  {
    "text": "scratch you press one button and it happens you don't need to worry about following through on anything it's just",
    "start": "1039439",
    "end": "1045600"
  },
  {
    "text": "press it forget it'll happen and then if anything goes wrong we're tracking every step of the way so we'll be able to",
    "start": "1045600",
    "end": "1050880"
  },
  {
    "text": "research and figure out why and debug what happened there so this is what our automation",
    "start": "1050880",
    "end": "1056559"
  },
  {
    "text": "system looks like we have an external load balancer uh and then we have the web application and then behind it is",
    "start": "1056559",
    "end": "1061960"
  },
  {
    "text": "our user database and in there is where we're defining who has access to this system what level of access they have uh",
    "start": "1061960",
    "end": "1067919"
  },
  {
    "text": "and then deciding what actions can be taken and then on this web database is also the this UI where you can see like",
    "start": "1067919",
    "end": "1074400"
  },
  {
    "text": "okay I want to start an action I want to do some tasks things like that so beyond that we also have have these Services",
    "start": "1074400",
    "end": "1080720"
  },
  {
    "text": "behind our system that we're using to maintain and all of these are based on a lot of Open Source technologies that we",
    "start": "1080720",
    "end": "1086080"
  },
  {
    "text": "didn't really want to have to modify to fit them into our system uh and so but we wanted to give anybody access to it",
    "start": "1086080",
    "end": "1092159"
  },
  {
    "text": "but we wanted to control that access so our solution was to put a custom proxy in front of it and what happens is you",
    "start": "1092159",
    "end": "1098200"
  },
  {
    "text": "make a request to our web app and it says okay you want to look at our Grana instance we we run grafana graphite log",
    "start": "1098200",
    "end": "1103880"
  },
  {
    "text": "stash I'll go into those a little bit more later uh so you'll say hey I want to go and get more information off this",
    "start": "1103880",
    "end": "1109320"
  },
  {
    "text": "I want to look at this screen and It'll point you through our proxy and the proxy will terminate your connection verify all of your authentication make",
    "start": "1109320",
    "end": "1115840"
  },
  {
    "text": "sure you have access to this system and then make the request on your behalf behind our application and then return",
    "start": "1115840",
    "end": "1121480"
  },
  {
    "text": "you with the the result so that allows us to just throw authentication on top of any web application we want behind it",
    "start": "1121480",
    "end": "1128400"
  },
  {
    "text": "so now you can view everything but then how do you make change so that's where our API comes in so you say you press a",
    "start": "1128400",
    "end": "1135440"
  },
  {
    "text": "button in the web app and it says start a server that may be 15 tasks those 15 tasks go through the web API into the",
    "start": "1135440",
    "end": "1143000"
  },
  {
    "text": "task database so now we know okay these 15 things need to be done so then we",
    "start": "1143000",
    "end": "1148120"
  },
  {
    "text": "have another process which is our we just call processor uh it takes those 15 tasks and starts doing something with",
    "start": "1148120",
    "end": "1155080"
  },
  {
    "text": "them and this is the part that actually communicates with the AWS SDK uh and so now you've got a system that you can",
    "start": "1155080",
    "end": "1161400"
  },
  {
    "text": "view you can modify so then the important part here was that we also we have this API and we also I I'll tell",
    "start": "1161400",
    "end": "1167400"
  },
  {
    "text": "you about the auto scaling system in a little little bit but that also communicates through the same API and what's really good about that is the",
    "start": "1167400",
    "end": "1173360"
  },
  {
    "text": "Autos scaling System is using the exact same system that we use to launch scale down or effect change to the entire",
    "start": "1173360",
    "end": "1179080"
  },
  {
    "text": "system it's using the exact same API so uh along with the automation",
    "start": "1179080",
    "end": "1186840"
  },
  {
    "start": "1184000",
    "end": "1184000"
  },
  {
    "text": "there we also automate our build distribution uh every build is uploaded into Amazon S3 uh whether we're going to",
    "start": "1186840",
    "end": "1193520"
  },
  {
    "text": "use it or not they all just go up there makes things a lot easier so there's a story here about this 20 gab per file",
    "start": "1193520",
    "end": "1200159"
  },
  {
    "text": "limit in Cloud front we had a beta uh and so even if it looks like you",
    "start": "1200159",
    "end": "1206799"
  },
  {
    "text": "might be able to use more than 20 Gigabytes in cloudfront uh in small scale testing you can't when you scale",
    "start": "1206799",
    "end": "1212080"
  },
  {
    "text": "up they'll they'll they'll stop you real quick and so we uh we the way that we distributed our server binary was",
    "start": "1212080",
    "end": "1218000"
  },
  {
    "text": "through cloudfront we just put them up in S3 and then we would let this cloudfront distribute it worldwide and then our servers would just pull them",
    "start": "1218000",
    "end": "1223559"
  },
  {
    "text": "straight out of cloudfront what ended up happening is they started missing 100%",
    "start": "1223559",
    "end": "1228720"
  },
  {
    "text": "and so we were just routing all of our S3 calls straight through cloudfront servers to S3 and they just throttled",
    "start": "1228720",
    "end": "1234159"
  },
  {
    "text": "the hell out of us uh so that that definitely gave us terrible bandwidth so the solution real quick was to just hit S3 directly but we needed something",
    "start": "1234159",
    "end": "1240720"
  },
  {
    "text": "better went back to the drawing board and we decided to just use baked Amis definitely a much better solution so",
    "start": "1240720",
    "end": "1247360"
  },
  {
    "text": "what we do is our development servers will start up uh and they can just run the game as normal and then once a build",
    "start": "1247360",
    "end": "1254480"
  },
  {
    "text": "is kind of approved and we know we want to go W wide with it we will bake it and then once it's baked it's distributed to",
    "start": "1254480",
    "end": "1260240"
  },
  {
    "text": "all the regions around the world and then we can deploy it straight as a Ami so this is what that process looks",
    "start": "1260240",
    "end": "1266440"
  },
  {
    "text": "like the build is uploaded into S3 uh the development boxes can just",
    "start": "1266440",
    "end": "1272640"
  },
  {
    "text": "pull it straight out of S3 and they they they extract it to their aeral drives",
    "start": "1272640",
    "end": "1278279"
  },
  {
    "text": "and then that's now ready to go now we decide okay that server was fun everything was good we can ship this",
    "start": "1278279",
    "end": "1284400"
  },
  {
    "text": "thing so now we're going to bake an Ami so the first thing we do is we put the build on the EBS volume so that we can make an Ami out of it so now we've",
    "start": "1284400",
    "end": "1290840"
  },
  {
    "text": "created the Ami and then now when this server launches if we just boot that Ami it'll just pull that build straight off",
    "start": "1290840",
    "end": "1296760"
  },
  {
    "text": "the EBS extract it there instead and that we no longer have any distribution problems the uh Ami system is definitely",
    "start": "1296760",
    "end": "1303559"
  },
  {
    "text": "a lot more bandwidth heavy so in the automation we have a lot",
    "start": "1303559",
    "end": "1309559"
  },
  {
    "start": "1306000",
    "end": "1306000"
  },
  {
    "text": "of dependencies automation is really easy if you don't have dependencies if you can just say start a server and there it is right and if it didn't have",
    "start": "1309559",
    "end": "1315240"
  },
  {
    "text": "to find anything else it didn't need to talk to anything else you can just start a server and way it goes it's not that easy uh we have MySQL servers we need to",
    "start": "1315240",
    "end": "1322559"
  },
  {
    "text": "worry about we have load balancers we need to worry about we have salt configuration uh we also have any other",
    "start": "1322559",
    "end": "1327840"
  },
  {
    "text": "many other services that I haven't talked about yet and every instance we want it to be able to launch up know",
    "start": "1327840",
    "end": "1333960"
  },
  {
    "text": "where it is and just discover all of its dependencies and talk to them immediately so a solution for that that",
    "start": "1333960",
    "end": "1340720"
  },
  {
    "text": "we found is to use the instance metadata system in Amazon that gave us everything we need",
    "start": "1340720",
    "end": "1346080"
  },
  {
    "start": "1341000",
    "end": "1341000"
  },
  {
    "text": "for autodiscovery uh and it doesn't require any SDK we don't need I am rules",
    "start": "1346080",
    "end": "1351480"
  },
  {
    "text": "and very little complexity so we needed a base container",
    "start": "1351480",
    "end": "1356840"
  },
  {
    "text": "to kind of hold everything together and that was something that we could discover in the instance metadata we decided to use subnets for that that was",
    "start": "1356840",
    "end": "1363600"
  },
  {
    "text": "something that you can discover and it's something you can have multiple of anywhere uh and so we decided that was",
    "start": "1363600",
    "end": "1368640"
  },
  {
    "text": "the smallest unit that we put together that if any if we launch any server anywhere looking at its subnet looking",
    "start": "1368640",
    "end": "1374320"
  },
  {
    "text": "at its a and looking at its region it should be able to discover its neighboring uh services that it requires",
    "start": "1374320",
    "end": "1380080"
  },
  {
    "text": "and so this is what this looks like so we have a single subnet here and we're back to that same diagram I was showing",
    "start": "1380080",
    "end": "1386000"
  },
  {
    "start": "1382000",
    "end": "1382000"
  },
  {
    "text": "before we have our application servers we have our internal load balancer and we have our game servers in the database so the first thing that happens is the",
    "start": "1386000",
    "end": "1392720"
  },
  {
    "text": "game servers and the application servers now use their instance metadata to build",
    "start": "1392720",
    "end": "1397960"
  },
  {
    "text": "a DNS address this is a Route 53 address that we put together and it's just us West 1A and the subnet name and then",
    "start": "1397960",
    "end": "1405039"
  },
  {
    "text": "whatever service it is they're looking for and so in this case it's the elb and so now they can say Okay given this I'll",
    "start": "1405039",
    "end": "1411600"
  },
  {
    "text": "just talk to this address and it just happens to be there they just built it out of their own metadata the",
    "start": "1411600",
    "end": "1417080"
  },
  {
    "text": "application servers do the same thing for their database they know exactly where it is we've just mapped it all directly to the DNS address so this",
    "start": "1417080",
    "end": "1424640"
  },
  {
    "text": "works really well and then we can expand that into a whole region with multiple subnets so now we'll just throw a uh",
    "start": "1424640",
    "end": "1432600"
  },
  {
    "start": "1427000",
    "end": "1427000"
  },
  {
    "text": "region external load balancer on top of it and now we're balancing between multiple subnets within a single region now the external load balancer doesn't",
    "start": "1432600",
    "end": "1439559"
  },
  {
    "text": "have any a or subnet attached to it so any client can find this knowing that Us",
    "start": "1439559",
    "end": "1445120"
  },
  {
    "text": "West one is the region I'll talk to this elb and that is our external",
    "start": "1445120",
    "end": "1451480"
  },
  {
    "text": "interface so now we can put these in different A's and we're just balancing between them another thing we can do",
    "start": "1451480",
    "end": "1457640"
  },
  {
    "text": "with these is let's say we were having trouble scaling our database uh for",
    "start": "1457640",
    "end": "1463400"
  },
  {
    "text": "whatever reason some emergency happened our database isn't scaling as we needed to okay well we need to scale fast",
    "start": "1463400",
    "end": "1468960"
  },
  {
    "text": "because we're running out of time we're running out of space we need to get servers for people so one thing we can do is double up the subnets within a",
    "start": "1468960",
    "end": "1475559"
  },
  {
    "text": "single a and we immediately split our load in half and it has cost us nothing uh but there's actually a",
    "start": "1475559",
    "end": "1482880"
  },
  {
    "text": "problem if you try and use two subnets in one a and Amazon it actually won't",
    "start": "1482880",
    "end": "1488080"
  },
  {
    "text": "let let you attach them to an elb so we actually have to run two Els there uh and so we just do a DNS round robin",
    "start": "1488080",
    "end": "1494520"
  },
  {
    "text": "against those two alss and our system automatically knows that hey we decided to throw two subnets into an a I better",
    "start": "1494520",
    "end": "1500039"
  },
  {
    "text": "spin up another E there to take care of that load and and connect everything",
    "start": "1500039",
    "end": "1505159"
  },
  {
    "start": "1505000",
    "end": "1505000"
  },
  {
    "text": "together so after that we also have instance configuration we use salt stack for that uh and all of our servers will",
    "start": "1505159",
    "end": "1512039"
  },
  {
    "text": "download that configuration at startup that includes the baked Amis even though they do have a lot of stuff baked into",
    "start": "1512039",
    "end": "1517279"
  },
  {
    "text": "them we still keep the configurations live this is really cool because if we decide that we need to change any of our",
    "start": "1517279",
    "end": "1522480"
  },
  {
    "text": "configuration uh we can do it and it it's within minutes we can reboot all of our machines and they will have the new",
    "start": "1522480",
    "end": "1527559"
  },
  {
    "text": "configuration and we don't have to rebake Amis which does take a while we also do a pretty involved",
    "start": "1527559",
    "end": "1533919"
  },
  {
    "text": "tagging process when we launch an instance we tag what it is what it's going to do uh so that way we can search",
    "start": "1533919",
    "end": "1539799"
  },
  {
    "text": "for them really easily both in the Amazon dashboard and using the SDK we can look for these instances uh along",
    "start": "1539799",
    "end": "1545240"
  },
  {
    "text": "with during their startup process they will tag themselves with their state so",
    "start": "1545240",
    "end": "1551480"
  },
  {
    "text": "that we're kind of using Amazon's tagging as kind of a feedback mechanism to know the health of an instance if a",
    "start": "1551480",
    "end": "1557279"
  },
  {
    "text": "if an instance doesn't get all the way to State five or whatever it is to be a live instance we know something's wrong",
    "start": "1557279",
    "end": "1562600"
  },
  {
    "text": "with that we can search for those both with the SD or in the dashboard find them kill them investigate them do",
    "start": "1562600",
    "end": "1568279"
  },
  {
    "text": "whatever we need to do but we can also filter them out to make sure that they're not used anywhere uh so we know that any instance that doesn't make it",
    "start": "1568279",
    "end": "1574279"
  },
  {
    "text": "all the way to these states they don't tag themselves at those States they're not healthy so I described what one region",
    "start": "1574279",
    "end": "1581720"
  },
  {
    "text": "looks like so let's go further into a worldwide State one thing that's important about",
    "start": "1581720",
    "end": "1588760"
  },
  {
    "start": "1586000",
    "end": "1586000"
  },
  {
    "text": "Global uh regions and this is kind of a something that I noticed make your lives a lot easier uh when you're going global",
    "start": "1588760",
    "end": "1595279"
  },
  {
    "text": "use a VPN network we we did a big VPN mesh it made life really easy uh and it",
    "start": "1595279",
    "end": "1601480"
  },
  {
    "text": "was really nice to be able to uh just directly SS straight into any instance from your workstation uh and so part of",
    "start": "1601480",
    "end": "1607640"
  },
  {
    "text": "this what we did is we have a lot of production vpcs in one in every region and then we also have two what we call",
    "start": "1607640",
    "end": "1614080"
  },
  {
    "text": "operations vpcs and these are where we keep our monitoring system it's where we keep our automated systems uh and we",
    "start": "1614080",
    "end": "1619760"
  },
  {
    "text": "have one of those in US West and we have one of those in EU West uh and so those are VPN connected to all the production",
    "start": "1619760",
    "end": "1627360"
  },
  {
    "text": "regions and then our office is VPN connected to all of the uh production regions and so we can connect from",
    "start": "1627360",
    "end": "1633039"
  },
  {
    "text": "either spot of those directly to everything else it makes things a lot easier and it really simplifies the",
    "start": "1633039",
    "end": "1638720"
  },
  {
    "text": "security group management because we basically have an internal set and an external set of security groups and everything is specific to whatever that",
    "start": "1638720",
    "end": "1645320"
  },
  {
    "text": "purpose is and we don't have a lot of overlap we don't have to worry about whitelisting IP addresses in any certain directions or everything we know",
    "start": "1645320",
    "end": "1650960"
  },
  {
    "text": "everything's going to go through our vpns so also when you're going global",
    "start": "1650960",
    "end": "1658520"
  },
  {
    "start": "1655000",
    "end": "1655000"
  },
  {
    "text": "region Discovery becomes a big deal you need to know what region players go to so I did describe one region but now",
    "start": "1658520",
    "end": "1664720"
  },
  {
    "text": "when you have multiples of them you have to make sure they find the right one so for that we created a UDP ping Service",
    "start": "1664720",
    "end": "1670600"
  },
  {
    "text": "uh we have one in every region it uh measures it measures quality of service uh so we're measuring latency packet",
    "start": "1670600",
    "end": "1677120"
  },
  {
    "text": "loss uh but at the same time it's also measuring build availability so he knows the client knows what build he's running",
    "start": "1677120",
    "end": "1684080"
  },
  {
    "text": "and he's asking every region hey is this build available and they can respond and say yes or no and so that allows us to",
    "start": "1684080",
    "end": "1690360"
  },
  {
    "text": "also roll builds out around the world and so clients will never connect to a",
    "start": "1690360",
    "end": "1695440"
  },
  {
    "text": "region that doesn't have servers for what they're looking for now this service actually ended up extremely",
    "start": "1695440",
    "end": "1701240"
  },
  {
    "text": "efficient we would put it on I think we're running M3s or something one I don't remember the exact size but uh",
    "start": "1701240",
    "end": "1707240"
  },
  {
    "text": "they're basically we're essentially doing nothing because responding to pings is a really easy process uh so we",
    "start": "1707240",
    "end": "1714600"
  },
  {
    "text": "decided to take advantage of the fact that we didn't really have much running there and we also put a relay and that was because we have the peer-to-peer",
    "start": "1714600",
    "end": "1720600"
  },
  {
    "text": "Network that we're doing for our lobbies uh so we bounce any of those strict Gat configurations I don't know if you guys",
    "start": "1720600",
    "end": "1726240"
  },
  {
    "text": "have ever had to deal with natat toat communication that's a whole bunch of bull uh so we just bounce any of the",
    "start": "1726240",
    "end": "1732640"
  },
  {
    "text": "ones that aren't connecting we just always bounce them off of our relay servers so here's what that looks like",
    "start": "1732640",
    "end": "1739279"
  },
  {
    "text": "we uh bring those here and then so we've got the two two availability zones or two subnets they're completely separated",
    "start": "1739279",
    "end": "1744720"
  },
  {
    "text": "we got our external load balancer and then we bring our ping servers in now even though the Ping servers and you can",
    "start": "1744720",
    "end": "1750240"
  },
  {
    "text": "see that they they also have their own Service uh DNS here so we got the region the Ping service so anybody can discover",
    "start": "1750240",
    "end": "1755840"
  },
  {
    "text": "them uh and we just have that's just a multi IP DNS result there that's just telling you however many there are the",
    "start": "1755840",
    "end": "1761880"
  },
  {
    "text": "clients will just randomly pick one we know they're all valid for the entire region so even though those ping servers",
    "start": "1761880",
    "end": "1767279"
  },
  {
    "text": "are within a single subnet or they're kind of in both subnets they actually represent the entire region and that's",
    "start": "1767279",
    "end": "1773000"
  },
  {
    "text": "important because even though the result comes back and it might be representing the state of a current subnet the client",
    "start": "1773000",
    "end": "1778200"
  },
  {
    "text": "will then make his request to an external load balancer that's balancing across all of your subnets and so they",
    "start": "1778200",
    "end": "1783279"
  },
  {
    "text": "need to represent the entire region and so instead of communicating internally in their own subnet they're always",
    "start": "1783279",
    "end": "1788519"
  },
  {
    "text": "communicating with the external load balancer to get the information of the entire region so they might be bouncing around kind of randomly between subnets",
    "start": "1788519",
    "end": "1794679"
  },
  {
    "text": "but at least they're giving us a pretty accurate view now it's important to note also is that the subnets are pretty well",
    "start": "1794679",
    "end": "1801080"
  },
  {
    "text": "balanced because the load balancer balances traffic across them and so when we do scale these up even though we may",
    "start": "1801080",
    "end": "1806360"
  },
  {
    "text": "have two they're almost perfectly 50/50 uh they stay pretty",
    "start": "1806360",
    "end": "1812080"
  },
  {
    "start": "1812000",
    "end": "1812000"
  },
  {
    "text": "close so what's also nice about having this pinging service is that we have region failover um when clients are",
    "start": "1812080",
    "end": "1818960"
  },
  {
    "text": "discovering what region they're going to talk to they keep track of all of the regions that they've talked to and what the latencies were and they ranked them",
    "start": "1818960",
    "end": "1825960"
  },
  {
    "text": "so if anything goes wrong we always know what the second best region is and we've had this happen in production and it",
    "start": "1825960",
    "end": "1832799"
  },
  {
    "text": "creates a very interesting effect so here we have the green line is the total",
    "start": "1832799",
    "end": "1838159"
  },
  {
    "text": "number of players playing the yellow line is Us East one the Blue Line Is Us West 2 so we were just having a normal",
    "start": "1838159",
    "end": "1845919"
  },
  {
    "text": "day here everything was scaling up just fine and then something went really wrong in Us East one we couldn't scale",
    "start": "1845919",
    "end": "1851640"
  },
  {
    "text": "up anymore uh so we we flatlined we could not handle any more players so",
    "start": "1851640",
    "end": "1857360"
  },
  {
    "text": "players Rec that the clients recognized that the servers were no longer working they were getting denied so they immediately went to their second best",
    "start": "1857360",
    "end": "1862519"
  },
  {
    "text": "region and so the second best for us East Is Us West uh the latency is actually not that bad between the two",
    "start": "1862519",
    "end": "1868120"
  },
  {
    "text": "especially in our game because we did set it up for peer-to-peer play we were pretty pretty good with higher latencies",
    "start": "1868120",
    "end": "1873720"
  },
  {
    "text": "uh and so in response Us West two just started carrying the load and it carried that player load up until it took us as",
    "start": "1873720",
    "end": "1879960"
  },
  {
    "text": "long as it took us to solve the issue players had no idea what was going on they were just moving to a whole different data center the only way they",
    "start": "1879960",
    "end": "1886480"
  },
  {
    "text": "would have known is if they had had measured the latency themselves so as everything was going",
    "start": "1886480",
    "end": "1892559"
  },
  {
    "text": "the US East stayed flatlined Us West 2 was going up and then we fixed the issue",
    "start": "1892559",
    "end": "1897760"
  },
  {
    "text": "and then we didn't do anything else we just fixed the scaling issue and then automatically the system was able to scale up and the whole system rebalanced",
    "start": "1897760",
    "end": "1905000"
  },
  {
    "text": "and everybody that was playing out of us West that shouldn't have been they moved back to us East and then everybody that",
    "start": "1905000",
    "end": "1910799"
  },
  {
    "text": "in Us East just was able to play as normal and you can see that the the player total graph actually stays pretty",
    "start": "1910799",
    "end": "1917159"
  },
  {
    "text": "linear you can kind of tell that that didn't really affect the player",
    "start": "1917159",
    "end": "1922000"
  },
  {
    "text": "counts so now we can talk about the auto scaling uh it is a bit of a difficult",
    "start": "1923159",
    "end": "1929200"
  },
  {
    "start": "1926000",
    "end": "1926000"
  },
  {
    "text": "problem we couldn't use Amazon's AWS autoscaling uh they were based off some",
    "start": "1929200",
    "end": "1935440"
  },
  {
    "text": "metrics that we didn't want to use for our scaling so what we ended up using was the ratio of servers in use to the",
    "start": "1935440",
    "end": "1941880"
  },
  {
    "text": "total number of servers that we're running and we do that on an individual subnet basis and that's because we use",
    "start": "1941880",
    "end": "1947200"
  },
  {
    "text": "subnets as our smallest container we don't want anything outside of that to be part of any of the picture so that",
    "start": "1947200",
    "end": "1952519"
  },
  {
    "text": "makes everything completely independent so if anything happens to a subnet doesn't matter we'll burn it and we'll just launch another one or we can just",
    "start": "1952519",
    "end": "1958120"
  },
  {
    "text": "balance across the other ones um and so we use the same metrics that the",
    "start": "1958120",
    "end": "1964120"
  },
  {
    "text": "application servers are using to know if a server is available and that makes sure that if for whatever reason the",
    "start": "1964120",
    "end": "1970200"
  },
  {
    "text": "scaling doesn't work properly uh we can keep scaling or take action of some kind",
    "start": "1970200",
    "end": "1975880"
  },
  {
    "text": "but it won't just say like oh I've scaled up five servers that must mean there's five servers available we're actually using the exact same data the",
    "start": "1975880",
    "end": "1981559"
  },
  {
    "text": "players are using to discover their servers so uh one of the benefits we",
    "start": "1981559",
    "end": "1986760"
  },
  {
    "text": "have with our game uh and it's it's something we took a lot of advantage of is that our rounds are generally about",
    "start": "1986760",
    "end": "1992960"
  },
  {
    "text": "12 minutes uh at most we actually cut them off with there's a a timer that will force you to fight at 45 minutes uh",
    "start": "1992960",
    "end": "2001120"
  },
  {
    "text": "so we have the fact that no game can last longer than 45 minutes and so we",
    "start": "2001120",
    "end": "2006679"
  },
  {
    "text": "can use that to our advantage when we need to scale down something we know that we can we don't need to migrate players we just need to wait and then",
    "start": "2006679",
    "end": "2013120"
  },
  {
    "text": "eventually that game will be over other people might have some problems with that if you're running an MMO or something like that where people can",
    "start": "2013120",
    "end": "2018720"
  },
  {
    "text": "just stay in an instance for hours or days on time so that's a that's a slightly different problem so we did have that Advantage where we don't need",
    "start": "2018720",
    "end": "2024080"
  },
  {
    "text": "to worry about any kind of fragmentation so normally when you're talking about autoscaling you might be",
    "start": "2024080",
    "end": "2029679"
  },
  {
    "text": "getting ready for some complex math and actually uh ours is not it's actually very easy so what we decided on was if",
    "start": "2029679",
    "end": "2037320"
  },
  {
    "start": "2032000",
    "end": "2032000"
  },
  {
    "text": "we're over 80% utilization we scale up 10% if we're under 60% utilization we",
    "start": "2037320",
    "end": "2043120"
  },
  {
    "text": "scale down 10% we sample very often so that way we're keeping track of a lot of data and we'll kind of average it out across the",
    "start": "2043120",
    "end": "2049320"
  },
  {
    "text": "sample periods but we make sure that our sample uh our action period is uh no",
    "start": "2049320",
    "end": "2055079"
  },
  {
    "text": "shorter than our scaleup time so that way if we if we ask for a server to start we won't try and run another",
    "start": "2055079",
    "end": "2060398"
  },
  {
    "text": "action until we know that those servers should have started by now and that makes sure that we don't double scale up",
    "start": "2060399",
    "end": "2067240"
  },
  {
    "text": "uh we also also track scal downs we don't worry as much about scal Downs uh we would rather air on the side of",
    "start": "2067240",
    "end": "2072638"
  },
  {
    "text": "having too many servers running if something goes wrong than not having enough servers and so we do track them",
    "start": "2072639",
    "end": "2078358"
  },
  {
    "text": "but then we kind of let them scale down gracefully so this is what that looks",
    "start": "2078359",
    "end": "2083560"
  },
  {
    "text": "like the yellow line is the total number of servers that are in game at that moment the blue line is the optimal",
    "start": "2083560",
    "end": "2091118"
  },
  {
    "text": "20% so that way we have that 20% Headroom that green line is what our simple little algorithm gave us I would",
    "start": "2091119",
    "end": "2098200"
  },
  {
    "text": "say it's not too bad it's definitely not exact but it's pretty close we do have those flat lines there at the bottom we do maintain a minimum uh and that was",
    "start": "2098200",
    "end": "2105079"
  },
  {
    "text": "just to keep the minimums at something reasonable they start to scale down pretty low and then things get a little little wild so we made sure that they",
    "start": "2105079",
    "end": "2111079"
  },
  {
    "text": "don't scale too low overnight uh and so we'll zoom in here on this middle one so you can kind of",
    "start": "2111079",
    "end": "2116599"
  },
  {
    "text": "get a better view of what it looks like uh right up against the graphs there so this is just a a single scale up and then the event starts to scale down now",
    "start": "2116599",
    "end": "2123079"
  },
  {
    "text": "there's one thing uh I didn't talk about with that simple uh scale up 10 after 80",
    "start": "2123079",
    "end": "2129880"
  },
  {
    "text": "scale down 10 after 60 uh one thing that we didn't take account of is that 10%",
    "start": "2129880",
    "end": "2135520"
  },
  {
    "text": "there on the bottom left of the current player count is a lot different than 10%",
    "start": "2135520",
    "end": "2141400"
  },
  {
    "text": "at the top of this peak uh and but if you look at that green line that player",
    "start": "2141400",
    "end": "2146640"
  },
  {
    "text": "count is pretty linear so that scale up that we needed there at the beginning is",
    "start": "2146640",
    "end": "2151680"
  },
  {
    "text": "pretty similar to the scale UPS we needed to continue growing so the way that we took",
    "start": "2151680",
    "end": "2156839"
  },
  {
    "text": "advantage of that uh or or solved this problem is we track the highest peak",
    "start": "2156839",
    "end": "2162040"
  },
  {
    "text": "over the last week and we say okay so what was the highest peak and now anytime we need to scale up we either",
    "start": "2162040",
    "end": "2167960"
  },
  {
    "text": "take 10% of that highest peak or we take 10% of the current and that allows us to",
    "start": "2167960",
    "end": "2173359"
  },
  {
    "text": "kind of keep everything in line and that's what gave us the the graph that we were looking at",
    "start": "2173359",
    "end": "2179280"
  },
  {
    "text": "there so now the visibility into this system we track everything I think it's",
    "start": "2179319",
    "end": "2185560"
  },
  {
    "text": "when we were doing our fullest full testing I think it was 2 and A5 million metrics per minute that we were tracking",
    "start": "2185560",
    "end": "2191440"
  },
  {
    "text": "uh we use graphite for that data collection uh grafana for visualization because it looks way cooler than",
    "start": "2191440",
    "end": "2197839"
  },
  {
    "text": "graphite everybody likes it looking cooler uh so we're we're tracking so we",
    "start": "2197839",
    "end": "2203720"
  },
  {
    "text": "have statsd that's running on the machine or collect D and we're running that through statsd uh and then we're",
    "start": "2203720",
    "end": "2208880"
  },
  {
    "text": "also running all of our stuff through statsd so everything gets together and it's all merged into our graphite system",
    "start": "2208880",
    "end": "2215640"
  },
  {
    "text": "uh and then we bring all of that into one giant",
    "start": "2215640",
    "end": "2221839"
  },
  {
    "start": "2218000",
    "end": "2218000"
  },
  {
    "text": "monolithic I2 instance and then so an important part of trying to scale out",
    "start": "2221839",
    "end": "2227880"
  },
  {
    "text": "graphite is you need to Define really good aggregation periods if you try and say I want to keep one minute",
    "start": "2227880",
    "end": "2234520"
  },
  {
    "text": "granularity for two and a half million metrics you're going to run out of space really damn quick so you want to kind of",
    "start": "2234520",
    "end": "2241280"
  },
  {
    "text": "pick out the things that you want to keep for a long term and then you pick out the things that you don't need for a very long while and then you kind of",
    "start": "2241280",
    "end": "2246440"
  },
  {
    "text": "adjust those aggregation Peri periods so when we were scaling out",
    "start": "2246440",
    "end": "2251640"
  },
  {
    "text": "graphite one of the things that graphite actually is pretty cool for is that it'll has it has this kind of caching mechanism where it'll say like oh as as",
    "start": "2251640",
    "end": "2258240"
  },
  {
    "text": "the load increases we can kind of cach in memory and then we'll we'll dump to dis uh to keep the io load low uh we",
    "start": "2258240",
    "end": "2264119"
  },
  {
    "text": "actually didn't like that because uh our worldwide loads didn't follow a perfect kind of up and down and there was a lot",
    "start": "2264119",
    "end": "2270040"
  },
  {
    "text": "of variability there and it was kind of hard to predict so we just went with shut that off every time a metric comes in just write the whole thing to the",
    "start": "2270040",
    "end": "2275880"
  },
  {
    "text": "disc uh and we were able to do that because we were using the aeral drives on the I2 instances which were huge it",
    "start": "2275880",
    "end": "2283319"
  },
  {
    "text": "gave us a lot of ioops and then we just did another raid zero configuration on those so the other thing with scaling",
    "start": "2283319",
    "end": "2290640"
  },
  {
    "text": "out graphite is it is a single-threaded process so the only way to scale it is to run multiple processes and that",
    "start": "2290640",
    "end": "2297160"
  },
  {
    "text": "actually gets a bit complicated so we have our it2 instance",
    "start": "2297160",
    "end": "2302560"
  },
  {
    "text": "here and the first thing we have is ha proxy we're running it's a TCP",
    "start": "2302560",
    "end": "2308599"
  },
  {
    "start": "2308000",
    "end": "2308000"
  },
  {
    "text": "connection that we're balancing across here so we're using ha proxy for that does some other things but that's kind",
    "start": "2308599",
    "end": "2313640"
  },
  {
    "text": "of what we're using it for and there we have our graphite relays and so we're balancing our connections across these",
    "start": "2313640",
    "end": "2320520"
  },
  {
    "text": "four relays and then based on what metrics are coming in we can decide if",
    "start": "2320520",
    "end": "2326280"
  },
  {
    "text": "we want to aggregate it or if we're just going to pass it forward and so we have to pass it",
    "start": "2326280",
    "end": "2331760"
  },
  {
    "text": "Forward because relays can only do one thing at a time they can either decide to uh do rule based routing or these",
    "start": "2331760",
    "end": "2338359"
  },
  {
    "text": "relays down here at the bottom they can do uh consistent hashing and figure out which one of these caches they have to",
    "start": "2338359",
    "end": "2344079"
  },
  {
    "text": "go to and so we're running four of everything all of these metrics are moving around in this system and then all of that gets written to those stripe",
    "start": "2344079",
    "end": "2351319"
  },
  {
    "text": "zero uh raid zero uh FAL drives so a lot of IO going on there oh",
    "start": "2351319",
    "end": "2357079"
  },
  {
    "text": "and one thing I actually didn't mention from the earlier slide was that we're using Linux lvm snapshots so that we can",
    "start": "2357079",
    "end": "2362440"
  },
  {
    "text": "do really quick backups without taking the server down uh gets us a complete total snapshot of the entire hard drive",
    "start": "2362440",
    "end": "2368119"
  },
  {
    "text": "at any moment and then we just back those up into S3 so we've got a snapshot every day so then this is the data coming into",
    "start": "2368119",
    "end": "2376960"
  },
  {
    "text": "the instance and then obviously you need to be able to get the data out of that instance so you can look at it so we",
    "start": "2376960",
    "end": "2382000"
  },
  {
    "text": "have the graphite web app is running here and it's just talking directly to that data now I don't know if you guys have ever had to create dashboards for",
    "start": "2382000",
    "end": "2388599"
  },
  {
    "text": "executives uh you you'll tell them running a query is really expensive",
    "start": "2388599",
    "end": "2394040"
  },
  {
    "text": "don't do it a lot and they'll listen to you for the first five minutes and you'll come back and they're just sitting there clicking that button as",
    "start": "2394040",
    "end": "2399720"
  },
  {
    "text": "fast as they can they're like no it updates faster and it's like no you're just taking the server down so for that we put Varnish in front of it so they",
    "start": "2399720",
    "end": "2405920"
  },
  {
    "text": "can click it as much as they want and they get the same data back but it makes it feel good",
    "start": "2405920",
    "end": "2411680"
  },
  {
    "text": "right so then we take the same process so that was one instance that we're running in one of those Ops uh vpcs that",
    "start": "2414560",
    "end": "2420920"
  },
  {
    "start": "2416000",
    "end": "2416000"
  },
  {
    "text": "I was talking about so now we need to get all this data from the multiple regions into that instance so we have",
    "start": "2420920",
    "end": "2428079"
  },
  {
    "text": "here let's say this is just I don't know us East one uh and he's got the four azs",
    "start": "2428079",
    "end": "2433880"
  },
  {
    "text": "here A bunch of instances are all running and now we have a separate instance here that is our relay instance",
    "start": "2433880",
    "end": "2440680"
  },
  {
    "text": "and in it we're also running ha proxy and it's also running for relays now the",
    "start": "2440680",
    "end": "2446119"
  },
  {
    "text": "reason for this is that if uh if we were to just run any one of",
    "start": "2446119",
    "end": "2452760"
  },
  {
    "text": "these straight to the ha proxy we would be running a lot of data oh actually so",
    "start": "2452760",
    "end": "2458520"
  },
  {
    "text": "there's one more step there we go so the relays are going directly to the relays in the AJ proxy and the reason for that is if we were to run the relays through",
    "start": "2458520",
    "end": "2465359"
  },
  {
    "text": "the ha proxy there on that single instance you would get the equivalent of All of Us East is running down One path",
    "start": "2465359",
    "end": "2471800"
  },
  {
    "text": "in that giant instance which you don't want you want to balance all of the instances across all of them so that's why we have a second ha proxy in the",
    "start": "2471800",
    "end": "2478160"
  },
  {
    "text": "region itself now when the requests come in they go to",
    "start": "2478160",
    "end": "2484480"
  },
  {
    "text": "this region relay and the reason we have this here is because we're actually running two of these things we have one",
    "start": "2484480",
    "end": "2489760"
  },
  {
    "text": "in our Ops region in US West and one of our Ops region in EU West and it's the exact same data constantly written to",
    "start": "2489760",
    "end": "2496560"
  },
  {
    "text": "both of them all at the same time so if anything happens anywhere we can always go and look at either one at any moment",
    "start": "2496560",
    "end": "2502560"
  },
  {
    "text": "it gives us a perfect failover hot spare of the entire system and it also is a good sanity check if we think",
    "start": "2502560",
    "end": "2508079"
  },
  {
    "text": "something's going wrong in the monitoring system we can look at the other one and go oh no he's actually communicating properly it's it's actually ways that we've seen issues in",
    "start": "2508079",
    "end": "2514160"
  },
  {
    "text": "our VPN has been caused and so we've been able to see it just by looking at the two really really good for",
    "start": "2514160",
    "end": "2520119"
  },
  {
    "text": "debugging so we also have log stash uh and we're we're logging essentially as",
    "start": "2520119",
    "end": "2525560"
  },
  {
    "start": "2521000",
    "end": "2521000"
  },
  {
    "text": "much as we can uh log stash is actually fairly straightforward to scale it's based on uh elastic search um one of the",
    "start": "2525560",
    "end": "2531760"
  },
  {
    "text": "big things that we notice is make sure you filter unnecessary data as early as possible uh we have a lot of debugging",
    "start": "2531760",
    "end": "2538119"
  },
  {
    "text": "data in our logs and trying to scale out servers to handle a whole bunch of debugging data is just a lot of wasted",
    "start": "2538119",
    "end": "2544599"
  },
  {
    "text": "time so we make sure we filter that early we we only push stuff that we really kind of consider important for",
    "start": "2544599",
    "end": "2549800"
  },
  {
    "text": "when we're trying to search for problems and then once we can identify what instance it is we can go and grab those logs because we don't delete them they",
    "start": "2549800",
    "end": "2555400"
  },
  {
    "text": "stay on the instances and we're using log stash for both our application logs and all of our",
    "start": "2555400",
    "end": "2560599"
  },
  {
    "text": "system logs so we also have a crash collection",
    "start": "2560599",
    "end": "2565680"
  },
  {
    "start": "2563000",
    "end": "2563000"
  },
  {
    "text": "system uh and then for this we when when a crash happens uh We've we've piped",
    "start": "2565680",
    "end": "2570960"
  },
  {
    "text": "into linux's crash handling uh segf system so that when it happens we can take that crash we upload it to S3 we",
    "start": "2570960",
    "end": "2577599"
  },
  {
    "text": "also immediately launch GDB and we take the call stack straight out of the crash then we take that call stack we upload",
    "start": "2577599",
    "end": "2583240"
  },
  {
    "text": "it to our own system and we start aggregating those call Stacks together and we can say okay now we can go to",
    "start": "2583240",
    "end": "2589160"
  },
  {
    "text": "this application and we can see how many times this call stack has shown up and we can rank them and so we can say Okay so this crash is happening a lot we",
    "start": "2589160",
    "end": "2595280"
  },
  {
    "text": "should focus our attention on that first and work backward from there and then we can always link to the raw crash dumps",
    "start": "2595280",
    "end": "2601480"
  },
  {
    "text": "in S3 if we want to get and those crash dumps have the entire process memory space and so if we want to debug further",
    "start": "2601480",
    "end": "2607200"
  },
  {
    "text": "see what any of the variables are whatever the the state of that process was we can use that data for",
    "start": "2607200",
    "end": "2613960"
  },
  {
    "text": "that so that basically covers the high level of everything that",
    "start": "2613960",
    "end": "2620440"
  },
  {
    "text": "we're running uh so I'll kind of recap that real quick so we have Game servers",
    "start": "2620440",
    "end": "2625800"
  },
  {
    "start": "2623000",
    "end": "2623000"
  },
  {
    "text": "in AWS and it works that's kind of useful uh we automate everything as much",
    "start": "2625800",
    "end": "2631640"
  },
  {
    "text": "as we can we try and automate it uh we use subnets metadata and route 3 for our",
    "start": "2631640",
    "end": "2637680"
  },
  {
    "text": "Auto configuration we have region failover through our ping",
    "start": "2637680",
    "end": "2642720"
  },
  {
    "text": "service we Auto scale at 10% over under 8060 and it's a pretty good system it",
    "start": "2642720",
    "end": "2647839"
  },
  {
    "text": "actually does work and then we have a global scale of monitoring and operations so there's two I don't know",
    "start": "2647839",
    "end": "2656079"
  },
  {
    "text": "if you guys have ever had to be like the guy that's on call at 2 o'clock in the morning that's always not a great experience uh we've actually had a",
    "start": "2656079",
    "end": "2662800"
  },
  {
    "text": "pretty good benefit we've only had two real big issues happen one of them I show you was where we couldn't scale up",
    "start": "2662800",
    "end": "2668640"
  },
  {
    "text": "in that Us East and that was a uh one that we had to solve and that actually wasn't one that anybody would have",
    "start": "2668640",
    "end": "2673839"
  },
  {
    "text": "gotten woken up for because the system kind of Auto heals itself you could see that that it would have stayed running that way as long as we needed to until",
    "start": "2673839",
    "end": "2680119"
  },
  {
    "text": "someone could come in and fix it the other one was my bad I didn't set up the",
    "start": "2680119",
    "end": "2685319"
  },
  {
    "text": "correct our our htps certificate expired and I didn't properly distribute that so",
    "start": "2685319",
    "end": "2691000"
  },
  {
    "text": "and I I I get to be the one to take the blame for that one so the the the only other only other outage we had was that",
    "start": "2691000",
    "end": "2697160"
  },
  {
    "text": "and then there was one the the one other major outage that actually caused dedicated service to be exhausted was",
    "start": "2697160",
    "end": "2703359"
  },
  {
    "text": "something that we didn't have automated that we do now is we deployed the client version of a patch before the servers",
    "start": "2703359",
    "end": "2710119"
  },
  {
    "text": "have been deployed and so the clients had something that didn't exists on our servers and they all fell back to that",
    "start": "2710119",
    "end": "2715200"
  },
  {
    "text": "peer-peer so the players really didn't know that everything went wrong uh but they were playing completely out of peer-to-peer",
    "start": "2715200",
    "end": "2721440"
  },
  {
    "start": "2722000",
    "end": "2722000"
  },
  {
    "text": "so uh we are hiring this is what we were working on last last year we are working",
    "start": "2722160",
    "end": "2728160"
  },
  {
    "text": "on bigger better things now you should come check it out we' be really interested to talk to you guys uh so check out T Rock studios.com we have a",
    "start": "2728160",
    "end": "2734960"
  },
  {
    "text": "career page there and that's everything I've got for you [Applause]",
    "start": "2734960",
    "end": "2747170"
  }
]