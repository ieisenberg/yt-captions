[
  {
    "start": "0",
    "end": "65000"
  },
  {
    "text": "can you guys hear me not yet oh you can okay great alright let's get",
    "start": "1220",
    "end": "7740"
  },
  {
    "text": "started so hello everyone welcome today",
    "start": "7740",
    "end": "12809"
  },
  {
    "text": "we're gonna take a look at how you can build a data leak with AWS clue data catalog",
    "start": "12809",
    "end": "17970"
  },
  {
    "text": "my name is project Adam Lee and I'm the senior product manager for AWS blue so",
    "start": "17970",
    "end": "24090"
  },
  {
    "text": "before we get started how many of you here are building a data Lake or have a data Lake already in production or",
    "start": "24090",
    "end": "30560"
  },
  {
    "text": "aspire to build a really cool almost all of you that's great you're in the right",
    "start": "30560",
    "end": "36239"
  },
  {
    "text": "session and how many of you are using glue or have heard of glue that's that's",
    "start": "36239",
    "end": "44399"
  },
  {
    "text": "a good number so for those of you who are already using glue thank you for using the",
    "start": "44399",
    "end": "49500"
  },
  {
    "text": "service if you have any feedback you know find me after the session or email me and for those of you who haven't",
    "start": "49500",
    "end": "55890"
  },
  {
    "text": "tried us out yet you know I hope I give you enough details in this session for you to at least give us a try so with",
    "start": "55890",
    "end": "63239"
  },
  {
    "text": "that let's get started so today we'll take a look at what is the data challenge that we face do a quick",
    "start": "63239",
    "end": "70890"
  },
  {
    "start": "65000",
    "end": "101000"
  },
  {
    "text": "refresher on what a data Lake is and then spend the rest of the time really",
    "start": "70890",
    "end": "76200"
  },
  {
    "text": "looking at what glue data catalog is all about we'll take a look at what it offers how it catalogues your data and",
    "start": "76200",
    "end": "83720"
  },
  {
    "text": "once your data is cataloged how can you really make use of that what can you do",
    "start": "83720",
    "end": "88799"
  },
  {
    "text": "with it how can you derive insights I'll also leave some time after the session",
    "start": "88799",
    "end": "94650"
  },
  {
    "text": "you know for you to ask me questions I'll also be available here for one-on-one questions so let's get",
    "start": "94650",
    "end": "100829"
  },
  {
    "text": "started so what does your data look like you have your clickstream data your",
    "start": "100829",
    "end": "107310"
  },
  {
    "start": "101000",
    "end": "162000"
  },
  {
    "text": "mobile app data you have spreadsheets financial billing data financial",
    "start": "107310",
    "end": "114420"
  },
  {
    "text": "documents you also have a ton of logs and you have social media data from a",
    "start": "114420",
    "end": "120869"
  },
  {
    "text": "variety of sources you have your application databases you have data",
    "start": "120869",
    "end": "125880"
  },
  {
    "text": "warehouses on AWS you have data warehouses on-premise you have ERP data and then today more than ever before",
    "start": "125880",
    "end": "133800"
  },
  {
    "text": "you have a lot of streaming data you have sensor data you have devices sending you I oh t data and many of your",
    "start": "133800",
    "end": "140580"
  },
  {
    "text": "social media sources or clickstream data is also being probably streamed in so",
    "start": "140580",
    "end": "146640"
  },
  {
    "text": "how do you deal with these multiple formats sources and this picture",
    "start": "146640",
    "end": "151830"
  },
  {
    "text": "probably keeps changing every single day so how do you make sure that your data is not siloed it's available for various",
    "start": "151830",
    "end": "159060"
  },
  {
    "text": "different use cases in your organization so you might ask you know this is not a",
    "start": "159060",
    "end": "166260"
  },
  {
    "start": "162000",
    "end": "219000"
  },
  {
    "text": "new problem we have been dealing with massive data volumes and growth and data volumes for many decades now enterprise",
    "start": "166260",
    "end": "174780"
  },
  {
    "text": "data warehouse was the center of our universe for many many years so why is this a new problem today today",
    "start": "174780",
    "end": "181350"
  },
  {
    "text": "more than ever before we're really seeing a shift in the type of data that we are acquiring we are acquiring a lot",
    "start": "181350",
    "end": "189270"
  },
  {
    "text": "of unstructured and semi-structured data so the balance is really tilted towards",
    "start": "189270",
    "end": "194490"
  },
  {
    "text": "data that needs to be quickly understood quickly pivoted relation alized for you",
    "start": "194490",
    "end": "201959"
  },
  {
    "text": "to really run any analytics on it what this also leads to is a problem of dark",
    "start": "201959",
    "end": "208800"
  },
  {
    "text": "data so what if dark data dot data is data that you acquire and that just sits",
    "start": "208800",
    "end": "214350"
  },
  {
    "text": "around without really being available for any analytics as you can see here",
    "start": "214350",
    "end": "222000"
  },
  {
    "start": "219000",
    "end": "233000"
  },
  {
    "text": "over the last few decades the amount of data has exponentially grown but the",
    "start": "222000",
    "end": "227489"
  },
  {
    "text": "data that is available or made available for analytics hasn't really changed much",
    "start": "227489",
    "end": "233150"
  },
  {
    "start": "233000",
    "end": "295000"
  },
  {
    "text": "to add to that problem you have multiple consumers today you have your analyst",
    "start": "233150",
    "end": "239489"
  },
  {
    "text": "you have your business users who want to visualize your data you want you have",
    "start": "239489",
    "end": "244680"
  },
  {
    "text": "your data scientists who want to run predictive analysis on your data and then you have various different",
    "start": "244680",
    "end": "250050"
  },
  {
    "text": "applications who are consuming various forms of this data either in raw form or in aggregate form and then these",
    "start": "250050",
    "end": "257160"
  },
  {
    "text": "consumers have new requirements they want your data to be a real bear data to",
    "start": "257160",
    "end": "262710"
  },
  {
    "text": "be available as soon as possible they want it to be flexible to support a variety of different use",
    "start": "262710",
    "end": "268980"
  },
  {
    "text": "cases from batch analytics to real-time analytics to you know something like",
    "start": "268980",
    "end": "276180"
  },
  {
    "text": "running their machine learning algorithms on it and they want to do this at massive scale",
    "start": "276180",
    "end": "282050"
  },
  {
    "text": "so this often leads to data duplication you have multiple different groups creating copies of your data keeping it",
    "start": "282050",
    "end": "289410"
  },
  {
    "text": "in various different repositories without you really having a unified view of what data you own so this brings us",
    "start": "289410",
    "end": "297180"
  },
  {
    "start": "295000",
    "end": "372000"
  },
  {
    "text": "to a data leak so what is a data leak and how does it help you deal with these two challenges so data leak is an",
    "start": "297180",
    "end": "303870"
  },
  {
    "text": "architectural approach it helps you store and ingest any kind of data it not",
    "start": "303870",
    "end": "310410"
  },
  {
    "text": "only helps you store data but it also helps you organize the data are curate it and understand the data so that it's",
    "start": "310410",
    "end": "316110"
  },
  {
    "text": "easy for you to quickly find that data and then it gives you ways to make this",
    "start": "316110",
    "end": "321330"
  },
  {
    "text": "data available to diverse group of people in your organization with the",
    "start": "321330",
    "end": "326370"
  },
  {
    "text": "right set of access policies data leak is also a place where people can run",
    "start": "326370",
    "end": "333660"
  },
  {
    "text": "multiple different types of analytics so data leak needs to be future proof it needs to support things like you know",
    "start": "333660",
    "end": "340770"
  },
  {
    "text": "natural language processing as well as querying and dashboarding and visualization of your data so these are",
    "start": "340770",
    "end": "349289"
  },
  {
    "text": "some of the key benefits of a data leak a data leak needs to allow you to quickly ingest and store any kind of",
    "start": "349289",
    "end": "355770"
  },
  {
    "text": "data it needs to provide you a single source of truth so you can dive deep and find the relevant data sets quickly when",
    "start": "355770",
    "end": "363960"
  },
  {
    "text": "you want them and then it needs to provide you with a set of unify tools to query and analyze this data so let's",
    "start": "363960",
    "end": "374099"
  },
  {
    "text": "take a look at what a typical data leak looks like and what are the layers of that data leak you have your storage or",
    "start": "374099",
    "end": "380970"
  },
  {
    "text": "ingest layer so if you're building your data like on Amazon s3 you have a variety of AWS services that will allow",
    "start": "380970",
    "end": "387389"
  },
  {
    "text": "you to ingest data from you know bulk transfer using a snowball to doing",
    "start": "387389",
    "end": "393330"
  },
  {
    "text": "continuous replication of your data using database migration service to streaming your data in with Amazon",
    "start": "393330",
    "end": "399479"
  },
  {
    "text": "Kinesis you then have a variety of security",
    "start": "399479",
    "end": "405210"
  },
  {
    "text": "services that allow you to protect and secure your data so you can use Identity",
    "start": "405210",
    "end": "411039"
  },
  {
    "text": "and Access Management Service to set the appropriate policies and rules on your data you can use the key management",
    "start": "411039",
    "end": "417550"
  },
  {
    "text": "service to encrypt your data and once your data is protected and encrypted you",
    "start": "417550",
    "end": "423610"
  },
  {
    "text": "now have a host of services to analyze your data from Athena that allows you to",
    "start": "423610",
    "end": "429669"
  },
  {
    "text": "query directly against data on s3 to Amazon redshift where you can build your",
    "start": "429669",
    "end": "435069"
  },
  {
    "text": "data warehouse on AWS and load some of that data into your data warehouse to",
    "start": "435069",
    "end": "441249"
  },
  {
    "text": "Amazon quick site that can allow you to visualize your data and then EMR that allows you to do for the processing of",
    "start": "441249",
    "end": "447879"
  },
  {
    "text": "your data this is just a sample of the sources that AWS offers and if you have",
    "start": "447879",
    "end": "453069"
  },
  {
    "text": "been in the keynotes you know this picture has probably already changed and then we have a host of AI services that",
    "start": "453069",
    "end": "461229"
  },
  {
    "text": "allow you to further analyze process and build your sort of machine learning models on top of your data you can do",
    "start": "461229",
    "end": "468279"
  },
  {
    "text": "speech recognition text to speech in äj-- analysis using a variety of AI",
    "start": "468279",
    "end": "474099"
  },
  {
    "text": "services that a Bleus offers but there's still a missing piece and what is that",
    "start": "474099",
    "end": "480789"
  },
  {
    "text": "missing piece there's still no unified view of all the data that you own that",
    "start": "480789",
    "end": "487180"
  },
  {
    "text": "is available to all the services that are part of your data link so this unified view helps you find and quickly",
    "start": "487180",
    "end": "496389"
  },
  {
    "text": "understand the data that you own no matter where it is stored it should be integrated with your analytics services",
    "start": "496389",
    "end": "503079"
  },
  {
    "text": "so the same data is available and accessible from various different places and then there should be a way to",
    "start": "503079",
    "end": "508899"
  },
  {
    "text": "automatically build and maintain this metadata as your data evolves so now",
    "start": "508899",
    "end": "518229"
  },
  {
    "start": "516000",
    "end": "569000"
  },
  {
    "text": "let's take a look at what AWS glue is all about aw-oooo provides you three things it",
    "start": "518229",
    "end": "524740"
  },
  {
    "text": "helps you discover and understand your data so that you can quickly search and",
    "start": "524740",
    "end": "530559"
  },
  {
    "text": "query this data it then helps you develop the code necessary to clean",
    "start": "530559",
    "end": "535890"
  },
  {
    "text": "this data enrich the data combine it from various different sources and process this data as well as move this",
    "start": "535890",
    "end": "543510"
  },
  {
    "text": "data between various different repositories the code that glue provides is customizable and you can also bring",
    "start": "543510",
    "end": "551400"
  },
  {
    "text": "in your own code and then once your code is ready glue provides a server less",
    "start": "551400",
    "end": "556860"
  },
  {
    "text": "execution environment for you to run this code and put the necessary orchestration features in place to",
    "start": "556860",
    "end": "563400"
  },
  {
    "text": "schedule this and trigger this on dependent conditions these are some of",
    "start": "563400",
    "end": "571590"
  },
  {
    "start": "569000",
    "end": "603000"
  },
  {
    "text": "our customers we have customers who are migrating their enterprise data",
    "start": "571590",
    "end": "576930"
  },
  {
    "text": "warehouses to Amazon redshift using glue we have others who are you know getting",
    "start": "576930",
    "end": "582360"
  },
  {
    "text": "raw data are converting it to a columnar format like for Kerry partitioning it and storing it on s3 so that it can be",
    "start": "582360",
    "end": "589530"
  },
  {
    "text": "easily queried with Athena as well as Amazon redshift spectrum and then we",
    "start": "589530",
    "end": "595380"
  },
  {
    "text": "have yet others who are building their machine learning algorithms using glue",
    "start": "595380",
    "end": "603320"
  },
  {
    "start": "603000",
    "end": "633000"
  },
  {
    "text": "so glue has three main components there's a data catalog which is hive meta store compatible integrated with",
    "start": "603860",
    "end": "611580"
  },
  {
    "text": "various different AWS services and we are actually going to take a very detailed look at this particular",
    "start": "611580",
    "end": "617550"
  },
  {
    "text": "component in the rest of this session but glue has many other components to",
    "start": "617550",
    "end": "622950"
  },
  {
    "text": "other components that provide job or three as well as job execution for your",
    "start": "622950",
    "end": "629250"
  },
  {
    "text": "data processing and ETL code so what is",
    "start": "629250",
    "end": "635400"
  },
  {
    "start": "633000",
    "end": "680000"
  },
  {
    "text": "a data catalog so a data catalog is a",
    "start": "635400",
    "end": "640500"
  },
  {
    "text": "list of your metadata it provides you a unified list of your metadata no matter",
    "start": "640500",
    "end": "645930"
  },
  {
    "text": "where your data is stored your data could be in Amazon s3 it could be in various different relational databases",
    "start": "645930",
    "end": "651330"
  },
  {
    "text": "it could be in your data warehouse on Amazon redshift it automatically",
    "start": "651330",
    "end": "657750"
  },
  {
    "text": "classifies this data and categorizes it so that it's easily searchable it tracks",
    "start": "657750",
    "end": "663360"
  },
  {
    "text": "the data evolution by automatically versioning your scheme definitions and then it integrates with",
    "start": "663360",
    "end": "669589"
  },
  {
    "text": "various different aww services so that you can quickly query your data and then you can do further analytics on Amazon",
    "start": "669589",
    "end": "677749"
  },
  {
    "text": "EMR so this is what a data Lake looks",
    "start": "677749",
    "end": "684290"
  },
  {
    "start": "680000",
    "end": "713000"
  },
  {
    "text": "like with glue you have your crawlers that can automatically scan and catalog",
    "start": "684290",
    "end": "691459"
  },
  {
    "text": "your data on Amazon s3 you have your blue ETL jobs that can process data and",
    "start": "691459",
    "end": "698589"
  },
  {
    "text": "put it back on Amazon s3 and then that blue data catalog provides yo you the",
    "start": "698589",
    "end": "704329"
  },
  {
    "text": "unified view that is also available to a variety of analytics services on AWS but",
    "start": "704329",
    "end": "712720"
  },
  {
    "start": "713000",
    "end": "765000"
  },
  {
    "text": "there are many different ways of thinking about uh data leak one way is",
    "start": "714189",
    "end": "719239"
  },
  {
    "text": "to put all your data on s3 but you might have data in various different AWS services and you still might want a",
    "start": "719239",
    "end": "726529"
  },
  {
    "text": "unified logical view of your data and want to build a logical data leak glue",
    "start": "726529",
    "end": "732079"
  },
  {
    "text": "supports that use case as well crawlers can go and crawl data across various",
    "start": "732079",
    "end": "737389"
  },
  {
    "text": "different data sources and then you can visualize that data in using something",
    "start": "737389",
    "end": "743809"
  },
  {
    "text": "like a zipline notebook that connects to the glue data catalog so here you can",
    "start": "743809",
    "end": "750049"
  },
  {
    "text": "see that we can spin up a Zeppelin notebook connected to glue service environment and read and write data in",
    "start": "750049",
    "end": "758509"
  },
  {
    "text": "various different data sources by leveraging the metadata that is in the glue data catalog so how do you set up",
    "start": "758509",
    "end": "767829"
  },
  {
    "start": "765000",
    "end": "783000"
  },
  {
    "text": "the glue data catalog you can create your tables manually and this works",
    "start": "767829",
    "end": "774290"
  },
  {
    "text": "great if you want to quickly go and test out a use case where you know the schema",
    "start": "774290",
    "end": "779809"
  },
  {
    "text": "and you want to quickly add that using our console you can run hive DDL",
    "start": "779809",
    "end": "785720"
  },
  {
    "start": "783000",
    "end": "793000"
  },
  {
    "text": "statements so here you can see an example where you are doing a create table against a set of ELB logs and you",
    "start": "785720",
    "end": "795139"
  },
  {
    "start": "793000",
    "end": "820000"
  },
  {
    "text": "can also use the glue api's to create tables directly in the data catalog this works great if you",
    "start": "795139",
    "end": "802700"
  },
  {
    "text": "just have a handful of tables to create and maintain but when you are dealing with hundreds of thousands of tables",
    "start": "802700",
    "end": "808580"
  },
  {
    "text": "this method is not very scalable and it's also very error-prone as your data",
    "start": "808580",
    "end": "814250"
  },
  {
    "text": "changes you now need to go and manually make edit otherwise your downstream processes would break so we provide an",
    "start": "814250",
    "end": "822740"
  },
  {
    "start": "820000",
    "end": "861000"
  },
  {
    "text": "easier way for you to catalog your data and then keep it up to date as your data evolves you tell us where your data is",
    "start": "822740",
    "end": "830270"
  },
  {
    "text": "and you tell us how often you'd like to check for updates and then glue data",
    "start": "830270",
    "end": "836120"
  },
  {
    "text": "catalog goes and scan for your data repositories builds your data catalog and then provides you this list that is",
    "start": "836120",
    "end": "843980"
  },
  {
    "text": "searchable and queryable so here you can see you have tables that are mapped to",
    "start": "843980",
    "end": "849560"
  },
  {
    "text": "data on s3 you have tables that are coming from your redshift data warehouse",
    "start": "849560",
    "end": "855950"
  },
  {
    "text": "as well as tables that are coming from your Postgres database so how does glue",
    "start": "855950",
    "end": "863510"
  },
  {
    "text": "automatically catalog your data glue uses crawlers that can go and sample",
    "start": "863510",
    "end": "869140"
  },
  {
    "text": "various different objects that you have stored in s3 as well as scan the",
    "start": "869140",
    "end": "874520"
  },
  {
    "text": "databases that you give us access to and then extract relevant details like the",
    "start": "874520",
    "end": "879710"
  },
  {
    "text": "schema information data statistics and add them as table definitions to the glue data catalog crawlers also apply a",
    "start": "879710",
    "end": "888110"
  },
  {
    "text": "set of classifiers to classify your data based on well-known formats as well as",
    "start": "888110",
    "end": "894410"
  },
  {
    "text": "well-known patterns and it provides you the flexibility to write your own classifiers if you have data that is not",
    "start": "894410",
    "end": "902300"
  },
  {
    "text": "natively supported by the glue crawlers they discover schema as well as discover",
    "start": "902300",
    "end": "908870"
  },
  {
    "text": "changes in schema so over time they can version your schema definitions and for",
    "start": "908870",
    "end": "915140"
  },
  {
    "text": "data on s3 crawlers can also automatically detect and register partitions so you can schedule a crawler",
    "start": "915140",
    "end": "922970"
  },
  {
    "text": "you can also run it ad hoc on-demand and they run on blue serverless environment",
    "start": "922970",
    "end": "928640"
  },
  {
    "text": "so there are no resources for you to provision or manage and you only pay for the crawlers when",
    "start": "928640",
    "end": "934139"
  },
  {
    "text": "they actually run so let's take an example this is a snapshot of a github",
    "start": "934139",
    "end": "942769"
  },
  {
    "start": "936000",
    "end": "973000"
  },
  {
    "text": "archive data that archives all the events that happen such as new commits",
    "start": "942769",
    "end": "948570"
  },
  {
    "text": "Forex comments on github its aggregated every hour and it's available for you to",
    "start": "948570",
    "end": "955560"
  },
  {
    "text": "download through any HTTP and client it",
    "start": "955560",
    "end": "962399"
  },
  {
    "text": "has 20 different types of events and then it has many nested types event",
    "start": "962399",
    "end": "968880"
  },
  {
    "text": "types with unique payload for each event type so when we crawl that data set this",
    "start": "968880",
    "end": "976800"
  },
  {
    "text": "is what the table looks like in the Glu data catalog as you can see you have the",
    "start": "976800",
    "end": "983880"
  },
  {
    "text": "location of your data you have certain properties such as yesterday information",
    "start": "983880",
    "end": "989610"
  },
  {
    "text": "automatically populated you also have certain data statistics like your",
    "start": "989610",
    "end": "994649"
  },
  {
    "text": "average record size number of records in the data set populated in the data catalog and then finally you have the",
    "start": "994649",
    "end": "1002690"
  },
  {
    "text": "table schema including nested types you can see an example of the payload nested",
    "start": "1002690",
    "end": "1010640"
  },
  {
    "text": "schema X and adhere which shows you an arbitrarily nested JSON schema so this",
    "start": "1010640",
    "end": "1017240"
  },
  {
    "text": "has been automatically extracted and added to the data catalog by the glue crawlers so how do crawlers",
    "start": "1017240",
    "end": "1025720"
  },
  {
    "start": "1023000",
    "end": "1068000"
  },
  {
    "text": "automatically detect your schema extracted identify what classifiers to apply and then classify your data",
    "start": "1025720",
    "end": "1033589"
  },
  {
    "text": "so what crawlers do is they sample your data and then based on the sample they",
    "start": "1033589",
    "end": "1039819"
  },
  {
    "text": "apply a set of classifiers to that sample data set and based on what",
    "start": "1039819",
    "end": "1045380"
  },
  {
    "text": "certainty number is returned they determine if there was a match or not if there was a match then the crawlers will",
    "start": "1045380",
    "end": "1051919"
  },
  {
    "text": "use that classifier to extract the schema as well as apply that classification to your table Glu",
    "start": "1051919",
    "end": "1059510"
  },
  {
    "text": "provides a set of built-in classifiers but you can write your own and we'll take a look at how you can do that",
    "start": "1059510",
    "end": "1065480"
  },
  {
    "text": "in just a few minutes so this is how",
    "start": "1065480",
    "end": "1070790"
  },
  {
    "start": "1068000",
    "end": "1104000"
  },
  {
    "text": "glue would classify your data on s3 it would eliminate enumerate the objects on",
    "start": "1070790",
    "end": "1076960"
  },
  {
    "text": "s3 and then identify the schema at each file or object it would apply a set of",
    "start": "1076960",
    "end": "1083630"
  },
  {
    "text": "classifiers to that object and then it would merge that schema for objects that",
    "start": "1083630",
    "end": "1090890"
  },
  {
    "text": "are similar or have the same classification and then finally you would get the schema at each partition",
    "start": "1090890",
    "end": "1097670"
  },
  {
    "text": "level and then at each table level that would then get added to the glue data catalog so the way glue determines if",
    "start": "1097670",
    "end": "1108350"
  },
  {
    "start": "1104000",
    "end": "1163000"
  },
  {
    "text": "there was a match or not against a particular classifier is based on the",
    "start": "1108350",
    "end": "1114440"
  },
  {
    "text": "similarity index that it generates so it assigns a point for every match of name",
    "start": "1114440",
    "end": "1122180"
  },
  {
    "text": "as well as a point for every match of data type so here in this example you",
    "start": "1122180",
    "end": "1127670"
  },
  {
    "text": "can see that it assigns a point for the root because it matches a name it also",
    "start": "1127670",
    "end": "1133940"
  },
  {
    "text": "assigns a point for name as well as ID and also address but for data types the",
    "start": "1133940",
    "end": "1141320"
  },
  {
    "text": "address is not matching in data types so it will only assign a point for the data types of the other three nodes so you",
    "start": "1141320",
    "end": "1149630"
  },
  {
    "text": "would get in a similarity index of about 87% and for any similarity index about",
    "start": "1149630",
    "end": "1156350"
  },
  {
    "text": "70% glue would assume that there is a match and then continue to combine schema and build your table definition",
    "start": "1156350",
    "end": "1164080"
  },
  {
    "start": "1163000",
    "end": "1201000"
  },
  {
    "text": "so these are the built in classifiers that glue supports glue can crawl a",
    "start": "1164080",
    "end": "1169820"
  },
  {
    "text": "number of databases Amazon redshift and then a number of formats on s3 you have",
    "start": "1169820",
    "end": "1176840"
  },
  {
    "text": "your columnar formats like partying or C you have your semi structured format like JSON and B so on as well as a",
    "start": "1176840",
    "end": "1183440"
  },
  {
    "text": "number of logs and compression types that glue supports but if this list is",
    "start": "1183440",
    "end": "1192410"
  },
  {
    "text": "not enough you can create your own classifiers using grok expressions and grok is essentially just you",
    "start": "1192410",
    "end": "1198559"
  },
  {
    "text": "regular expression that you can specify so let's take an example of a grog",
    "start": "1198559",
    "end": "1204769"
  },
  {
    "start": "1201000",
    "end": "1258000"
  },
  {
    "text": "expression here I have crawler logs and",
    "start": "1204769",
    "end": "1210049"
  },
  {
    "text": "a croc expression that identifies a schema for crawler log so when you run your crawlers we generate logs that you",
    "start": "1210049",
    "end": "1216799"
  },
  {
    "text": "can download and put in an s3 bucket this is an example of a regular",
    "start": "1216799",
    "end": "1222230"
  },
  {
    "text": "expression that identifies the schema of crawler logs and this is not a new or",
    "start": "1222230",
    "end": "1228799"
  },
  {
    "text": "custom pattern that glue has developed this is something that you probably",
    "start": "1228799",
    "end": "1235100"
  },
  {
    "text": "already use to identify different log patterns grok expressions help you identify two",
    "start": "1235100",
    "end": "1242179"
  },
  {
    "text": "hundred-plus Apache log formats so you can define a rock expression for a",
    "start": "1242179",
    "end": "1248179"
  },
  {
    "text": "specific set of logs that you generate and that you I didn't want to identify",
    "start": "1248179",
    "end": "1253970"
  },
  {
    "text": "with a specific classification so once",
    "start": "1253970",
    "end": "1260779"
  },
  {
    "text": "you have your custom classifier written you add it or associated with the",
    "start": "1260779",
    "end": "1266179"
  },
  {
    "text": "crawler that you want it to be applied with and then the next time the crawler",
    "start": "1266179",
    "end": "1271249"
  },
  {
    "text": "runs it would apply to the data that you have stored in the repository that the crawler is scanning and you'd get a",
    "start": "1271249",
    "end": "1278899"
  },
  {
    "text": "table in your data catalogue with the custom classification that you have specified Glu automatically detects",
    "start": "1278899",
    "end": "1289159"
  },
  {
    "start": "1285000",
    "end": "1334000"
  },
  {
    "text": "partitions in addition to the tables schema based on the data that it scans so here going back to the github archive",
    "start": "1289159",
    "end": "1297379"
  },
  {
    "text": "example you can see that in addition to the table schema glue has also detected",
    "start": "1297379",
    "end": "1302419"
  },
  {
    "text": "partition keys and registered doors it has registered the available data as",
    "start": "1302419",
    "end": "1307789"
  },
  {
    "text": "available partitions in the glue data catalog and once this is registered you",
    "start": "1307789",
    "end": "1313490"
  },
  {
    "text": "could just readily go to Amazon athina or go to EMR and start querying this",
    "start": "1313490",
    "end": "1319610"
  },
  {
    "text": "data so if you have new data coming in every hour every day and you have a",
    "start": "1319610",
    "end": "1325850"
  },
  {
    "text": "crawler running on a schedule you can always have the latest data available for all the query needs",
    "start": "1325850",
    "end": "1332179"
  },
  {
    "text": "that you have so how does glue detect",
    "start": "1332179",
    "end": "1337490"
  },
  {
    "text": "partitions it does a similar analysis as",
    "start": "1337490",
    "end": "1342529"
  },
  {
    "text": "a dust you detect whether there was a schema match so it starts generating schema and starts merging that schema",
    "start": "1342529",
    "end": "1349519"
  },
  {
    "text": "based on similarity at the file level and then keeps walking the s3 bucket",
    "start": "1349519",
    "end": "1355340"
  },
  {
    "text": "hierarchy up to determine whether all those files in a given bucket mapped to",
    "start": "1355340",
    "end": "1360649"
  },
  {
    "text": "a single partition map to multiple different partitions that are part of the single table or mapped to different",
    "start": "1360649",
    "end": "1366110"
  },
  {
    "text": "tables glue also with crawlers",
    "start": "1366110",
    "end": "1373429"
  },
  {
    "text": "determines whether there was a schema change so if a new drop of your data has",
    "start": "1373429",
    "end": "1379490"
  },
  {
    "text": "additional fields has fields that have changed glue can detect that and it can",
    "start": "1379490",
    "end": "1385249"
  },
  {
    "text": "find the table in the catalog and automatically version your schema so you",
    "start": "1385249",
    "end": "1394519"
  },
  {
    "start": "1392000",
    "end": "1457000"
  },
  {
    "text": "might say that you know this is great crawlers can crawl my data but I already have an existing external hive meta",
    "start": "1394519",
    "end": "1401600"
  },
  {
    "text": "store and I already have all my metadata there so how do I get that data into the",
    "start": "1401600",
    "end": "1407840"
  },
  {
    "text": "glue data catalog well we have an export import script as well as an export",
    "start": "1407840",
    "end": "1413029"
  },
  {
    "text": "script so if you have an external hive meta store that is backed by a my sequel database you can run our import script",
    "start": "1413029",
    "end": "1420860"
  },
  {
    "text": "as a glue ETL job and get your metadata from that hive meta store into the glue data catalog you can also export that",
    "start": "1420860",
    "end": "1428509"
  },
  {
    "text": "data out to an external hive meta store if you ever need to take that data and put it in an external hive meta store",
    "start": "1428509",
    "end": "1434960"
  },
  {
    "text": "for any use case that would require a dedicated external meta store and you",
    "start": "1434960",
    "end": "1441559"
  },
  {
    "text": "can find that script on our github repository if you go to glue developer resources page on our website you'll",
    "start": "1441559",
    "end": "1448429"
  },
  {
    "text": "find the link there as well so you read our catalog that's great what next how",
    "start": "1448429",
    "end": "1455119"
  },
  {
    "text": "do you use this data so you can search so you can do a text-based search you",
    "start": "1455119",
    "end": "1462559"
  },
  {
    "start": "1457000",
    "end": "1504000"
  },
  {
    "text": "can also filter on key attributes the glue console to find relevant data sets so as you can see here I've",
    "start": "1462559",
    "end": "1469700"
  },
  {
    "text": "searched on the term log and then I've got Gordon data sets that are mapping to",
    "start": "1469700",
    "end": "1475280"
  },
  {
    "text": "clout trail data ELB data as well as the custom classification example that I",
    "start": "1475280",
    "end": "1481010"
  },
  {
    "text": "just showed which maps to crawler logs and then I can pick the relevant data",
    "start": "1481010",
    "end": "1486110"
  },
  {
    "text": "set and then immediately start querying it from the glue console itself using",
    "start": "1486110",
    "end": "1491990"
  },
  {
    "text": "Amazon attina I can write three detail queries as you can see here I have a query written against the cloud trail",
    "start": "1491990",
    "end": "1498890"
  },
  {
    "text": "data and get results right through my console so with the glue data catalog",
    "start": "1498890",
    "end": "1508280"
  },
  {
    "text": "you can analyze data and have access to the same view of your data through multiple different engines and then you",
    "start": "1508280",
    "end": "1515870"
  },
  {
    "text": "can visualize that data using Amazon quick site so quick refresher on what",
    "start": "1515870",
    "end": "1525980"
  },
  {
    "start": "1522000",
    "end": "1546000"
  },
  {
    "text": "are these different engines Amazon Athena provides interactive queries",
    "start": "1525980",
    "end": "1531700"
  },
  {
    "text": "sequel based interactive query directly against data and s3 it is completely",
    "start": "1531700",
    "end": "1536840"
  },
  {
    "text": "server less so there are no resources for you to manage and you only pay for",
    "start": "1536840",
    "end": "1542810"
  },
  {
    "text": "the data that is scanned when your queries run Amazon EMR provides you",
    "start": "1542810",
    "end": "1552340"
  },
  {
    "start": "1546000",
    "end": "1567000"
  },
  {
    "text": "Hadoop on AWS for big data processing it provides support for 19 different open",
    "start": "1552340",
    "end": "1557750"
  },
  {
    "text": "source formats and you can use the blue data catalog as your external hive meta",
    "start": "1557750",
    "end": "1563300"
  },
  {
    "text": "store for big data applications running on Amazon EMR and then if you have your",
    "start": "1563300",
    "end": "1571190"
  },
  {
    "start": "1567000",
    "end": "1595000"
  },
  {
    "text": "data warehouse in redshift and you also have data on Amazon s3 you can use",
    "start": "1571190",
    "end": "1577430"
  },
  {
    "text": "Amazon redshift spectrum to run queries against your data on s3 do a join",
    "start": "1577430",
    "end": "1584300"
  },
  {
    "text": "between the data in Amazon redshift and s3 and run hexa byte scale queries just",
    "start": "1584300",
    "end": "1592130"
  },
  {
    "text": "under in a few minutes glue also helps",
    "start": "1592130",
    "end": "1598070"
  },
  {
    "start": "1595000",
    "end": "1636000"
  },
  {
    "text": "you do surveillance data exploration if you have your data scientists who want to use part notebooks and interactively run",
    "start": "1598070",
    "end": "1606500"
  },
  {
    "text": "sparks equal interactively build your ETL scripts they can create a glue",
    "start": "1606500",
    "end": "1614450"
  },
  {
    "text": "development endpoint and attach a spark notebook to it to interactively work with their data",
    "start": "1614450",
    "end": "1621580"
  },
  {
    "text": "the Zeppelin notebooks can access the metadata and the glue data catalog",
    "start": "1621650",
    "end": "1626809"
  },
  {
    "text": "through the glue development endpoints and then you can have access to a variety of data sources that glue",
    "start": "1626809",
    "end": "1633110"
  },
  {
    "text": "supports you can also use glue ETL jobs",
    "start": "1633110",
    "end": "1640179"
  },
  {
    "start": "1636000",
    "end": "1674000"
  },
  {
    "text": "that can read metadata from the blue data catalog to move data across multiple different sources so if you",
    "start": "1640179",
    "end": "1647270"
  },
  {
    "text": "have raw data on s3 and you want to load all of it or some of it or into your",
    "start": "1647270",
    "end": "1653090"
  },
  {
    "text": "data warehouse in Amazon redshift you can create a glue ETL job and glue will",
    "start": "1653090",
    "end": "1658520"
  },
  {
    "text": "also provide you with an auto-generated script that's a good starting point for a lot of ETL jobs and then you can run",
    "start": "1658520",
    "end": "1665210"
  },
  {
    "text": "that on the server less apache spark environment that glue provides to move",
    "start": "1665210",
    "end": "1670370"
  },
  {
    "text": "that data from one data repository to another so let's take a look at what a",
    "start": "1670370",
    "end": "1678530"
  },
  {
    "text": "data Lake offers as well as what a data warehouse offers and when do you use",
    "start": "1678530",
    "end": "1683720"
  },
  {
    "text": "which so data Lake helps you store a variety of data formats these could be",
    "start": "1683720",
    "end": "1690860"
  },
  {
    "text": "structured semi-structured or unstructured it provides you the ability to apply schema on Reed and it supports",
    "start": "1690860",
    "end": "1698480"
  },
  {
    "text": "a variety of different use cases from data science use cases to BI use cases",
    "start": "1698480",
    "end": "1704170"
  },
  {
    "text": "to just you know use cases where you want to do ad hoc analysis it helps you",
    "start": "1704170",
    "end": "1711500"
  },
  {
    "text": "store granular data as well as processed data and it helps you scale your storage",
    "start": "1711500",
    "end": "1716600"
  },
  {
    "text": "without really scaling your compute so it provides you the separation of compute and storage data warehouse on",
    "start": "1716600",
    "end": "1722870"
  },
  {
    "text": "other hand provides you a easy way to store structured data it provides you",
    "start": "1722870",
    "end": "1729200"
  },
  {
    "text": "simple tools to run sequel base and let and it is great to store frequently",
    "start": "1729200",
    "end": "1734630"
  },
  {
    "text": "accessed data data that needs to be aggregated or summarized and made",
    "start": "1734630",
    "end": "1740600"
  },
  {
    "text": "available to your dashboards so with glue you can actually interoperate your",
    "start": "1740600",
    "end": "1747800"
  },
  {
    "start": "1743000",
    "end": "1796000"
  },
  {
    "text": "data leak and your data warehouse you can bring in your data into Amazon s3",
    "start": "1747800",
    "end": "1753100"
  },
  {
    "text": "you can load your recent or most frequently accessed data to Amazon",
    "start": "1753100",
    "end": "1759650"
  },
  {
    "text": "redshift you can compute your aggregates inside of your data warehouse and then",
    "start": "1759650",
    "end": "1765290"
  },
  {
    "text": "you can unload that data and put it back on Amazon s3 for historic archiving and",
    "start": "1765290",
    "end": "1771890"
  },
  {
    "text": "also to make it available for a doc analysis as well as for any data science",
    "start": "1771890",
    "end": "1778360"
  },
  {
    "text": "analysis that you have in or that you want to do within your organization so",
    "start": "1778360",
    "end": "1784490"
  },
  {
    "text": "with glue you don't necessarily have to choose one or the other you can easily",
    "start": "1784490",
    "end": "1789500"
  },
  {
    "text": "interoperate both your data warehouse and your data leak on AWS so have a few",
    "start": "1789500",
    "end": "1799130"
  },
  {
    "start": "1796000",
    "end": "1838000"
  },
  {
    "text": "key announcements to make we will be releasing support for Scala very soon so",
    "start": "1799130",
    "end": "1804800"
  },
  {
    "text": "in addition to writing your glue jobs in PI SPARC you'd now be able to write your glue charts in Scala and then glue will",
    "start": "1804800",
    "end": "1812180"
  },
  {
    "text": "be available in two additional regions very soon in EU West one as well as in",
    "start": "1812180",
    "end": "1817850"
  },
  {
    "text": "ap North East one which is our Tokyo region so that's all I have and I will",
    "start": "1817850",
    "end": "1826370"
  },
  {
    "text": "be available for questions we also have some glue stickers back there so feel free to help yourself or feel free to",
    "start": "1826370",
    "end": "1833600"
  },
  {
    "text": "take those as you walk out so thank you everyone [Applause]",
    "start": "1833600",
    "end": "1840290"
  }
]