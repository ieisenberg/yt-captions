[
  {
    "start": "0",
    "end": "142000"
  },
  {
    "text": "I need to hear you good morning everyone thank you today we're talking about how",
    "start": "0",
    "end": "7319"
  },
  {
    "text": "spot instances are pushing the boundaries for some big data applications spot instances how many of",
    "start": "7319",
    "end": "14370"
  },
  {
    "text": "you have heard of them before hire how many if you've used them",
    "start": "14370",
    "end": "20900"
  },
  {
    "text": "pheromone great for those of you who haven't heard about spot instances before they are regular",
    "start": "20900",
    "end": "27000"
  },
  {
    "text": "ec2 instances that we put out in the spot market when they're not being used for regular on-demand requests prices",
    "start": "27000",
    "end": "33630"
  },
  {
    "text": "are on average 70 to 80 percent lower than on-demand and there",
    "start": "33630",
    "end": "39199"
  },
  {
    "text": "for example you can cook in your jobs five to ten times say run your CI CD pipelines faster or you can scale your",
    "start": "39530",
    "end": "46559"
  },
  {
    "text": "web apps 10x as several of our customers like my folks have done today we're talking about how you can",
    "start": "46559",
    "end": "52980"
  },
  {
    "text": "generate better and faster business insights from your event streams event streams that can stretch from a",
    "start": "52980",
    "end": "59160"
  },
  {
    "text": "few hours to several years in the past my name is Anna Sharma and I'm the",
    "start": "59160",
    "end": "64228"
  },
  {
    "text": "product manager for easy to spot instances so please feel free to come grab hold of me if you have questions or",
    "start": "64229",
    "end": "69810"
  },
  {
    "text": "feedback after the session before we dive in a quick overview of what we'll cover today",
    "start": "69810",
    "end": "75619"
  },
  {
    "text": "we will briefly touch upon the use case and the history of how business intelligence applications have evolved",
    "start": "75619",
    "end": "84020"
  },
  {
    "text": "from AOL we'll talk about how they run their presto clusters on EMR and spot instances they also talk about how to",
    "start": "84020",
    "end": "91079"
  },
  {
    "text": "use lambda to orchestrate their job workflows in the second half of the session Charles from Metamarkets will",
    "start": "91079",
    "end": "97619"
  },
  {
    "text": "talk about how he uses four instances for SPARC and druid clusters",
    "start": "97619",
    "end": "103189"
  },
  {
    "text": "the use case that we're talking about today has billions of events flowing",
    "start": "103850",
    "end": "108869"
  },
  {
    "text": "into your system per second and this event data generally looks the same across the industry there's a timestamp",
    "start": "108869",
    "end": "115520"
  },
  {
    "text": "there's a set of dimensions or measures that you want to aggregate against",
    "start": "115520",
    "end": "122270"
  },
  {
    "text": "Thanks there's a set of dimensions or attributes that you want to aggregate",
    "start": "128610",
    "end": "134500"
  },
  {
    "text": "against and there are metrics or values that you want to aggregate there are several ways of approaching",
    "start": "134500",
    "end": "141100"
  },
  {
    "text": "the problem and relational databases have been around forever the traditional way to build a data warehouse has been",
    "start": "141100",
    "end": "146950"
  },
  {
    "start": "142000",
    "end": "142000"
  },
  {
    "text": "ascent star schema where you have a central fact table and you query it by joining with the various dimension",
    "start": "146950",
    "end": "153459"
  },
  {
    "text": "tables this works until for a smallish data set and as the data skills your",
    "start": "153459",
    "end": "160420"
  },
  {
    "text": "performance tends to degrade key value stores these have been several of these have been popular in the last",
    "start": "160420",
    "end": "166390"
  },
  {
    "start": "163000",
    "end": "163000"
  },
  {
    "text": "few years and the great thing about them is that you have fast writes and fast look ups and there are two common",
    "start": "166390",
    "end": "172060"
  },
  {
    "text": "patterns with key value stores pre-computation where every expected query is pre computed",
    "start": "172060",
    "end": "179940"
  },
  {
    "text": "the problem with this is that ad-hoc queries are not supported to start with and as you add more columns the query",
    "start": "179940",
    "end": "186880"
  },
  {
    "text": "space grows exponentially and again performance falls off the second pattern is range scans where",
    "start": "186880",
    "end": "194170"
  },
  {
    "text": "the primary key is a hash of the timestamp and the dimensions and the value is the metric that you want to",
    "start": "194170",
    "end": "199750"
  },
  {
    "text": "aggregate usually you would shuffle data from",
    "start": "199750",
    "end": "205090"
  },
  {
    "text": "where it is stored pull it into an intermediate compute buffer where the numbers are crunched and the return",
    "start": "205090",
    "end": "211299"
  },
  {
    "text": "results are returned to the user this shuffling of data this the scan can take can be slow can take a while because",
    "start": "211299",
    "end": "218440"
  },
  {
    "text": "it's difficult to add intelligent indexes to the primary key in the key value stores",
    "start": "218440",
    "end": "224819"
  },
  {
    "text": "generally compute engines several of these are popular today in fact we'll talk about a couple of them in this",
    "start": "225510",
    "end": "230739"
  },
  {
    "start": "226000",
    "end": "226000"
  },
  {
    "text": "session and the way a lot of general compute engines work is very similar they pull a lot of data from where it is",
    "start": "230739",
    "end": "237070"
  },
  {
    "text": "stored into an empty intermediate compute buffer where the numbers are crunched now if you can scale the compute in this",
    "start": "237070",
    "end": "245380"
  },
  {
    "text": "architecture then you can generate results faster and if you can generate results faster you can generate more",
    "start": "245380",
    "end": "251800"
  },
  {
    "text": "results as you know data analysis is a process you ask questions based on the",
    "start": "251800",
    "end": "259299"
  },
  {
    "text": "answers you get and the faster you get answers the more questions you can ask",
    "start": "259299",
    "end": "264960"
  },
  {
    "text": "but even here there's a performance overhead there's a lot of shuffling going on and the difference between a",
    "start": "264960",
    "end": "272229"
  },
  {
    "text": "few minutes for the query to run or even a few seconds versus a few hundred milliseconds can be the difference",
    "start": "272229",
    "end": "278349"
  },
  {
    "text": "between interactivity and non interactive interactivity and that's what Charles will talk about how he uses",
    "start": "278349",
    "end": "284680"
  },
  {
    "text": "pod instances to run interactive applications and with that hopefully will share a",
    "start": "284680",
    "end": "290860"
  },
  {
    "text": "theme here the journey of moving or building new technologies and moving",
    "start": "290860",
    "end": "296949"
  },
  {
    "text": "them into your daily life as if they're a regular part of it to the point that they're boring and that's when we know",
    "start": "296949",
    "end": "303970"
  },
  {
    "text": "we have a story to tell circa",
    "start": "303970",
    "end": "308639"
  },
  {
    "text": "I'm System Architect at AOL and I work with big data",
    "start": "319810",
    "end": "325770"
  },
  {
    "text": "to give you a brief background in 2015 we had an in-house Hadoop",
    "start": "326910",
    "end": "333490"
  },
  {
    "text": "cluster and we migrated the data and processing from that in-house Hadoop",
    "start": "333490",
    "end": "338680"
  },
  {
    "text": "cluster into Amazon Cloud rather than",
    "start": "338680",
    "end": "344700"
  },
  {
    "text": "migrating the cluster as these creating the Hadoop cluster in the cloud migrating the data and the jobs into it",
    "start": "344700",
    "end": "352290"
  },
  {
    "text": "which was a different approach we decided to store the data on s3 and",
    "start": "352290",
    "end": "358570"
  },
  {
    "text": "use transient EMR clusters to process the data",
    "start": "358570",
    "end": "364050"
  },
  {
    "text": "rather than having one cluster and running multiple jobs on that cluster trying to scale it up and down we",
    "start": "364050",
    "end": "371169"
  },
  {
    "text": "decided to create a separate cluster for each job this allowed us to scale massively",
    "start": "371169",
    "end": "380110"
  },
  {
    "text": "across multiple areas regions tapping into the spot market in all those",
    "start": "380110",
    "end": "385870"
  },
  {
    "text": "regions I have put together a small slide with animation to illustrate how our",
    "start": "385870",
    "end": "392919"
  },
  {
    "text": "architecture works",
    "start": "392919",
    "end": "395789"
  },
  {
    "text": "at the center of our architecture is Amazon s3 as I mentioned we store all of",
    "start": "399900",
    "end": "405780"
  },
  {
    "text": "our data and code on s3 and we use as three Azure data Lake",
    "start": "405780",
    "end": "412460"
  },
  {
    "text": "when there is data ready to be processed we spin up an EMR cluster which would",
    "start": "412460",
    "end": "420330"
  },
  {
    "text": "read the data and the code from s3 and write the output the transformed are the",
    "start": "420330",
    "end": "426750"
  },
  {
    "text": "aggregated output back to s3 and it would shut down there are several EMR clusters spinning",
    "start": "426750",
    "end": "435240"
  },
  {
    "text": "up and down coming in and out in different regions",
    "start": "435240",
    "end": "440250"
  },
  {
    "text": "and different availability zones all doing the same thing",
    "start": "440250",
    "end": "446120"
  },
  {
    "text": "we have orchestrated all this using AWS lambda which cairo will talk in detail",
    "start": "446240",
    "end": "452910"
  },
  {
    "text": "about we have used Amazon's dynamo DB to store some of our data validation matrix",
    "start": "452910",
    "end": "462230"
  },
  {
    "text": "we use I am to control access to all of her data",
    "start": "462230",
    "end": "468920"
  },
  {
    "text": "who can read who can write and ensuring that those are enforced",
    "start": "468920",
    "end": "477890"
  },
  {
    "text": "we store the data on Amazon s3 although I didn't have the icon there we store",
    "start": "477890",
    "end": "485340"
  },
  {
    "text": "the data on s3 encrypted using server-side encryption so that is also a",
    "start": "485340",
    "end": "491880"
  },
  {
    "text": "very good thing and very easy thing to do so for what you see on the left side of",
    "start": "491880",
    "end": "499470"
  },
  {
    "text": "the slide is how we process the data we like this model of",
    "start": "499470",
    "end": "506300"
  },
  {
    "text": "separating our compute and storage and ability to scale across multiple regions",
    "start": "506300",
    "end": "513000"
  },
  {
    "text": "for all of our data processing we like that model so much we wanted to extend that into",
    "start": "513000",
    "end": "519860"
  },
  {
    "text": "our data analytic side now it is a little bit of a challenge to adopt the",
    "start": "519860",
    "end": "527970"
  },
  {
    "text": "same model for data analytics because we needed longer running clusters and ensure that users don't",
    "start": "527970",
    "end": "534820"
  },
  {
    "text": "have any disruption we were able to achieve that by",
    "start": "534820",
    "end": "539850"
  },
  {
    "text": "separating metadata from our data",
    "start": "539850",
    "end": "546690"
  },
  {
    "text": "so we store our metadata about the data in Amazon RDS hive meta story that hive",
    "start": "546690",
    "end": "555130"
  },
  {
    "text": "meta store has a mapping of which table or which data set besides we're on s3 an",
    "start": "555130",
    "end": "562650"
  },
  {
    "text": "Amazon EMR cluster spins up with presto pre-installed",
    "start": "562650",
    "end": "568680"
  },
  {
    "text": "connects to the hive RDS and immediately knows where the data is on",
    "start": "568680",
    "end": "573940"
  },
  {
    "text": "s3 it attaches itself to an elastic IP address to which the users connect",
    "start": "573940",
    "end": "582240"
  },
  {
    "text": "since there is no data loading involved since we store all four data on s3",
    "start": "582240",
    "end": "587520"
  },
  {
    "text": "it only takes 10 to 15 minutes to spin up this cluster and it is immediately",
    "start": "587520",
    "end": "592930"
  },
  {
    "text": "available to all the end-users for running that queries",
    "start": "592930",
    "end": "598590"
  },
  {
    "text": "we could use spin up another cluster which again connects to the same",
    "start": "598950",
    "end": "604740"
  },
  {
    "text": "RDS meta store and immediately knows where all the data is attaches itself to another elastic IP",
    "start": "604740",
    "end": "611290"
  },
  {
    "text": "address to be used by a different client or users",
    "start": "611290",
    "end": "617040"
  },
  {
    "text": "all of this has been made even better by the",
    "start": "617490",
    "end": "622720"
  },
  {
    "text": "announcement of Amazon Athena this morning where you could just have a tena",
    "start": "622720",
    "end": "629010"
  },
  {
    "text": "read data directly from s3 your users could run query and you could",
    "start": "629010",
    "end": "636220"
  },
  {
    "text": "achieve a truly server less data Lake to summarize these are the key features",
    "start": "636220",
    "end": "643270"
  },
  {
    "start": "641000",
    "end": "641000"
  },
  {
    "text": "and advantages that we have achieved through our architecture",
    "start": "643270",
    "end": "650100"
  },
  {
    "text": "we separate our compute and storage store the data on s3 we use Amazon EMR",
    "start": "650100",
    "end": "656020"
  },
  {
    "text": "with hive to process our data it would allow us to separate our cross in",
    "start": "656020",
    "end": "662750"
  },
  {
    "text": "analytics we could use hive for processing presto for analytics and",
    "start": "662750",
    "end": "669130"
  },
  {
    "text": "since there is no data being stored locally in all these clusters there is",
    "start": "669130",
    "end": "674630"
  },
  {
    "text": "no data migration involved and",
    "start": "674630",
    "end": "679990"
  },
  {
    "text": "using s3 as a single source of truth enables us to avoid the confusion of",
    "start": "679990",
    "end": "685360"
  },
  {
    "text": "multiple data systems being loaded independently",
    "start": "685360",
    "end": "691510"
  },
  {
    "text": "we use columnar format Parkay columnar format to store all of our data that",
    "start": "691510",
    "end": "697070"
  },
  {
    "text": "allows us to compress it and also allows performance",
    "start": "697070",
    "end": "702940"
  },
  {
    "text": "we launch all of our clusters in V pcs for our operational security",
    "start": "702940",
    "end": "708530"
  },
  {
    "text": "requirements and we are still able to operate with this model leveraging the",
    "start": "708530",
    "end": "714530"
  },
  {
    "text": "spot instances leveraging all the AWS regions by just creating V pcs in all of those",
    "start": "714530",
    "end": "721910"
  },
  {
    "text": "regions and still adhering to our IT security policies",
    "start": "721910",
    "end": "727660"
  },
  {
    "text": "so with that I would turn it over to guru to talk in detail about the cost",
    "start": "727660",
    "end": "732830"
  },
  {
    "text": "optimization and lambda orchestration",
    "start": "732830",
    "end": "737440"
  },
  {
    "text": "Thanks good morning everyone so as Luca explained our architecture how we use",
    "start": "740500",
    "end": "747400"
  },
  {
    "text": "EMR which are not persistent we launched",
    "start": "747400",
    "end": "752810"
  },
  {
    "text": "like hundreds of EMR so once we moved all our processing to AWS the next thing we wanted to is further optimize the",
    "start": "752810",
    "end": "759200"
  },
  {
    "text": "cost because you want to save as much money as you can also we wanted a mature",
    "start": "759200",
    "end": "764240"
  },
  {
    "text": "pipeline so that multiple dependencies and right from source to the final",
    "start": "764240",
    "end": "771020"
  },
  {
    "text": "summary everything is taken care without having a lot of dependency on operations which are like manual and it it impacts",
    "start": "771020",
    "end": "777350"
  },
  {
    "text": "isolate so in case you have not used spot instances few things I'll definitely",
    "start": "777350",
    "end": "783200"
  },
  {
    "start": "780000",
    "end": "780000"
  },
  {
    "text": "recommend to take care is availability as I mentioned that it is different than",
    "start": "783200",
    "end": "788660"
  },
  {
    "text": "on-demand you have to go through a bid process so sometimes you might not find spot instances as",
    "start": "788660",
    "end": "794810"
  },
  {
    "text": "as much incapacity as on-demand also the spot price vary between",
    "start": "794810",
    "end": "800180"
  },
  {
    "text": "different instance type for each AZ so if M 3 X large is 2 cents in Virginia",
    "start": "800180",
    "end": "806230"
  },
  {
    "text": "Virginia region it might be 3 cents or 4 cents in Europe so you have to make sure you are taking that factor into",
    "start": "806230",
    "end": "812779"
  },
  {
    "text": "consideration to make sure you're taking the full advantage of spot cost also the",
    "start": "812779",
    "end": "818450"
  },
  {
    "text": "provisioning time can be a bit different in our use case we process most of the",
    "start": "818450",
    "end": "823760"
  },
  {
    "text": "batch data so a little bit extra provisioning time was acceptable in case you are planning to use spot instances I",
    "start": "823760",
    "end": "830600"
  },
  {
    "text": "will highly highly recommend you to revisit that so that you are not impacting your SLA for our particular",
    "start": "830600",
    "end": "837770"
  },
  {
    "text": "requirement for our team our capacity requirement is around 15 to 20 thousand",
    "start": "837770",
    "end": "842960"
  },
  {
    "text": "ec2 instances these are complex clusters which run for few hours so compute hours",
    "start": "842960",
    "end": "848360"
  },
  {
    "text": "are around 50 to 60,000 computers on a normal day on a daily basis we need around 10 to 11,000 compute hours the",
    "start": "848360",
    "end": "856130"
  },
  {
    "text": "data volume is around few terabytes and we process data for multiple countries since of brands are global the frequency",
    "start": "856130",
    "end": "864680"
  },
  {
    "text": "of data processing it from hourly till then daily processing then monthly after that",
    "start": "864680",
    "end": "870310"
  },
  {
    "text": "so to make sure we are making use of all the regions what we did as the dimension",
    "start": "870310",
    "end": "875690"
  },
  {
    "start": "871000",
    "end": "871000"
  },
  {
    "text": "we set up or we piscine different regions then we worked with the support team to increase our spot limits in this",
    "start": "875690",
    "end": "882710"
  },
  {
    "text": "regions so that we are making we are taking the advantage of multiple instance types",
    "start": "882710",
    "end": "888190"
  },
  {
    "text": "apart from the AWS limit what did what we did was we set up a hard limit for",
    "start": "888190",
    "end": "893360"
  },
  {
    "text": "ourself the reason we did that was we did not want to put too many requests in",
    "start": "893360",
    "end": "899240"
  },
  {
    "text": "a particular easy suppose you find Virginia a region as the cheapest and you've dumped 10,000 spot requests you",
    "start": "899240",
    "end": "906589"
  },
  {
    "text": "might end end up competing with yourself so to spread the load across multiple AZ's we set up a limit that we are not",
    "start": "906589",
    "end": "913160"
  },
  {
    "text": "going to put request after X X number",
    "start": "913160",
    "end": "918759"
  },
  {
    "text": "to find optimal cluster size we consider two factors first is the data volume so",
    "start": "918940",
    "end": "925400"
  },
  {
    "text": "let's say we are trying to process hundred gigabytes of the then after that is the code complexity if you are doing",
    "start": "925400",
    "end": "932150"
  },
  {
    "text": "a simple select operation 200 or 300 nodes might be enough but if you are doing complex operations like joins or",
    "start": "932150",
    "end": "938900"
  },
  {
    "text": "distinct values you might need more number of nodes to find the optimal cluster size that's why the combination",
    "start": "938900",
    "end": "945920"
  },
  {
    "text": "of this two is really important and if you have worried that your spot instances might be taken away due to bit",
    "start": "945920",
    "end": "951230"
  },
  {
    "text": "price increase so the trick that we follow is a bit little bit higher so even if you even if you bid higher",
    "start": "951230",
    "end": "958810"
  },
  {
    "text": "you're paying actual price you're not paying the bid price so when the",
    "start": "958810",
    "end": "963890"
  },
  {
    "text": "algorithm clicks into the place that spot price is increased and if you render that value there's still not",
    "start": "963890",
    "end": "969710"
  },
  {
    "text": "route that lose that cluster",
    "start": "969710",
    "end": "972910"
  },
  {
    "start": "972000",
    "end": "972000"
  },
  {
    "text": "so to quickly walk through the deployment that we do for a particular",
    "start": "976330",
    "end": "982430"
  },
  {
    "text": "data set based on the code complexity and data volume and we know how much",
    "start": "982430",
    "end": "987710"
  },
  {
    "text": "course we need for that particular data set then we know what instance ties are acceptable so we find out the pricing",
    "start": "987710",
    "end": "995900"
  },
  {
    "text": "for that instance type across all lazy's then we find we pick the cheapest AZ and",
    "start": "995900",
    "end": "1001210"
  },
  {
    "text": "we check how many requests are already running let's call it B so the number of course that is required is a if a plus B",
    "start": "1001210",
    "end": "1008410"
  },
  {
    "text": "is less than the hard define limit we kick off the mr if it is if there is no bandwidth then we go to the next AZ and",
    "start": "1008410",
    "end": "1015520"
  },
  {
    "text": "if we run out of all is this we go to the next instance type so this cycle repeats and what happens is that now",
    "start": "1015520",
    "end": "1022570"
  },
  {
    "text": "you're spread your workload across multiple a Z's it has rarely happened that we have exhausted all is ease even",
    "start": "1022570",
    "end": "1029230"
  },
  {
    "text": "with like 20,000 ec2 instances so let's walk through the actual saving",
    "start": "1029230",
    "end": "1034930"
  },
  {
    "text": "graphs if you had to do a similar model using ornament I will not recommend that",
    "start": "1034930",
    "end": "1039970"
  },
  {
    "text": "because that will be real expensive so if you choose to a hard Kody or easy that means all your EMS are launching in",
    "start": "1039970",
    "end": "1046780"
  },
  {
    "text": "the same easy you'll still end up saving 80% that is no-brainer but if you go one",
    "start": "1046780",
    "end": "1052810"
  },
  {
    "text": "step extra that means you're finding the cheapest AZ across multiple regions that",
    "start": "1052810",
    "end": "1057880"
  },
  {
    "text": "can save fifteen ten to fifteen percent but a question might come to the mind",
    "start": "1057880",
    "end": "1063740"
  },
  {
    "text": "that if you're launching an EMR in a region which is not default or default",
    "start": "1063740",
    "end": "1068750"
  },
  {
    "text": "to the s3 bucket there is a data transfer cost so let's walk through the worst case",
    "start": "1068750",
    "end": "1074210"
  },
  {
    "text": "scenario where your cheapest AZ is not in your local region one thing to keep in mind is that more",
    "start": "1074210",
    "end": "1082070"
  },
  {
    "text": "data you have more nodes you will need to process the data and it might also",
    "start": "1082070",
    "end": "1087080"
  },
  {
    "text": "require more compute hours so the way to read the table is supposed to process 10",
    "start": "1087080",
    "end": "1092540"
  },
  {
    "text": "gigabytes of data you need 25 cores the EMR else for an hour so in your default",
    "start": "1092540",
    "end": "1098270"
  },
  {
    "text": "region suppose it's called it's costing you 429 and in the cheapest AZ it's",
    "start": "1098270",
    "end": "1104090"
  },
  {
    "text": "costing you 356 there is a data transfer cost of $73 summing up to 429 so you are",
    "start": "1104090",
    "end": "1110720"
  },
  {
    "text": "not saving anything but this this is the use case for the lower volume once you do the math for",
    "start": "1110720",
    "end": "1117260"
  },
  {
    "text": "the higher data volume that's where the saving savings come into the picture so if you see the lowest row for 300",
    "start": "1117260",
    "end": "1124490"
  },
  {
    "text": "gigabytes you are just you kick at least 15% of savings and this we are talking",
    "start": "1124490",
    "end": "1130580"
  },
  {
    "text": "about is the worst case scenario at a while we process terabytes of data and that's why this is really critical that",
    "start": "1130580",
    "end": "1137030"
  },
  {
    "text": "we identify the cheapest AZ and do all the processing in that easy",
    "start": "1137030",
    "end": "1143530"
  },
  {
    "text": "so does it really happen that your cheapest Izzie is not in your local",
    "start": "1146830",
    "end": "1152510"
  },
  {
    "start": "1147000",
    "end": "1147000"
  },
  {
    "text": "region so this is actually the production distribution for our EMR workloads I think on an average we have",
    "start": "1152510",
    "end": "1159919"
  },
  {
    "text": "a thousand production EMRs so in a month we have like 30,000 earmarks 80% of the time or EMR was not in",
    "start": "1159919",
    "end": "1168290"
  },
  {
    "text": "Virginia region so in fact 28% time it was in Europe so I highly recommend you",
    "start": "1168290",
    "end": "1174320"
  },
  {
    "text": "to identify that logic if you read if you are really serious about the spot",
    "start": "1174320",
    "end": "1179450"
  },
  {
    "text": "savings so the next step that we did was the implementation of optimal cluster size",
    "start": "1179450",
    "end": "1185179"
  },
  {
    "text": "by doing that we found out that there is an average saving of around 18 to 19 percent depending on the project since",
    "start": "1185179",
    "end": "1192500"
  },
  {
    "text": "the data values is now same for multiple projects also the complexity is different for different projects this is",
    "start": "1192500",
    "end": "1199310"
  },
  {
    "text": "also really important so take away for the cost savings is that find the",
    "start": "1199310",
    "end": "1205100"
  },
  {
    "text": "optimal cluster size and launch your cluster in the optimal easy the next thing is the orchestration",
    "start": "1205100",
    "end": "1212320"
  },
  {
    "text": "if how many of you do will be the big gate or data science",
    "start": "1212320",
    "end": "1218140"
  },
  {
    "text": "okay less number of people so if you are working in a big data or data science",
    "start": "1218170",
    "end": "1223940"
  },
  {
    "text": "world having a mature pipeline is really important because right some when you",
    "start": "1223940",
    "end": "1229550"
  },
  {
    "text": "get the data till the final output there are multiple stages involved you have to make sure you are taking care of all the",
    "start": "1229550",
    "end": "1235850"
  },
  {
    "text": "dependencies so that the output data is correct one of the thing that was important to",
    "start": "1235850",
    "end": "1241730"
  },
  {
    "text": "us was we wanted the solution to integrate with different data services or workload was already on EMR modeled",
    "start": "1241730",
    "end": "1248960"
  },
  {
    "text": "using s3 we also explored DynamoDB and redshift so we wanted something which integrates with all the AWS services we",
    "start": "1248960",
    "end": "1257240"
  },
  {
    "text": "started with you few hundred clusters and the current workload is few thousand clusters so we wanted something which",
    "start": "1257240",
    "end": "1263420"
  },
  {
    "text": "can scale so if there are more branch we did not want any issues with that and with so many clusters there are bound to",
    "start": "1263420",
    "end": "1271760"
  },
  {
    "text": "happen failures so we wanted something which can recover notify operations so I will try to explain you using a",
    "start": "1271760",
    "end": "1279260"
  },
  {
    "text": "simple directed acyclic graph consider this tree so right at the top",
    "start": "1279260",
    "end": "1285050"
  },
  {
    "start": "1282000",
    "end": "1282000"
  },
  {
    "text": "is your source data then you are creating multiple stages and the lowest level is your final output",
    "start": "1285050",
    "end": "1291640"
  },
  {
    "text": "so for example e your data set E is dependent on B CN a someone who is in",
    "start": "1291640",
    "end": "1298790"
  },
  {
    "text": "operations can remember this simple graph very easily but business always",
    "start": "1298790",
    "end": "1304880"
  },
  {
    "text": "adds the complexity what they'll ask is okay I want to update B I also want to",
    "start": "1304880",
    "end": "1310340"
  },
  {
    "text": "make sure everything that is downstream has to be updated and to add to further complexity they want it to be done for",
    "start": "1310340",
    "end": "1317240"
  },
  {
    "text": "seven thirty days or two years of six months so this poor guy in operations has to make sure everything gets updated",
    "start": "1317240",
    "end": "1324260"
  },
  {
    "text": "and the data integrity is met so to solve this problem",
    "start": "1324260",
    "end": "1331549"
  },
  {
    "text": "what we thought do we really need to touch each and every node in this tree how about we just start at the top of",
    "start": "1332029",
    "end": "1339059"
  },
  {
    "text": "the tree and everything doubts downstream textgate takes Kate is taken care of matically so what we do is we",
    "start": "1339059",
    "end": "1347039"
  },
  {
    "text": "drop a file for the top node of the tree on s3 that takes off a lambda function which takes care of the deployment logic",
    "start": "1347039",
    "end": "1354509"
  },
  {
    "text": "ensures dependencies and kicks off any amount so suppose for this EMR there are",
    "start": "1354509",
    "end": "1360299"
  },
  {
    "text": "two dependencies so this Emaar will drop two files on your s3 bucket which will",
    "start": "1360299",
    "end": "1365639"
  },
  {
    "text": "kick off a lambda function which will again to kick off the deployment logic",
    "start": "1365639",
    "end": "1371879"
  },
  {
    "text": "and yorimasa kicked off so the only thing that operation person has to take",
    "start": "1371879",
    "end": "1377639"
  },
  {
    "text": "care is the top of the tree everything at the bottom is taken care automatically so",
    "start": "1377639",
    "end": "1384559"
  },
  {
    "start": "1383000",
    "end": "1383000"
  },
  {
    "text": "moving through this model has improved or SLA huge in like really in a",
    "start": "1384559",
    "end": "1390119"
  },
  {
    "text": "significant way because before we used to do a traditional model of scheduling",
    "start": "1390119",
    "end": "1395549"
  },
  {
    "text": "we used to check every hour whether the data is ready it has moved to an event event when design its hour less there is",
    "start": "1395549",
    "end": "1402839"
  },
  {
    "text": "no server or anything to maintain because lambda is service pricing wise",
    "start": "1402839",
    "end": "1409289"
  },
  {
    "text": "it has saved a lot of it has saved thousands of dollar because they provide",
    "start": "1409289",
    "end": "1414899"
  },
  {
    "text": "you million requests free per month and we don't launch millions of VMR",
    "start": "1414899",
    "end": "1420769"
  },
  {
    "text": "the auto sis is a scheduler that we used to use by creating generic utilities",
    "start": "1420769",
    "end": "1426619"
  },
  {
    "text": "which scales across multiple like every project and different countries we have",
    "start": "1426619",
    "end": "1431969"
  },
  {
    "text": "shut down like more than 2,000 jobs just by 2 lambda codes so it's it scales",
    "start": "1431969",
    "end": "1437729"
  },
  {
    "text": "automatically and also you don't need to worry about logging because everything it's logged in Claude watch",
    "start": "1437729",
    "end": "1445039"
  },
  {
    "text": "so the last point I wanted to mention since we have thousands of clusters we",
    "start": "1445039",
    "end": "1450899"
  },
  {
    "text": "wanted something to take care of failures you might have failures due to network",
    "start": "1450899",
    "end": "1457079"
  },
  {
    "text": "issue or a super spot price so we wrote a cash utility called panella",
    "start": "1457079",
    "end": "1463049"
  },
  {
    "text": "what it does every 10 or 15-minute it wakes up and it collects the data for",
    "start": "1463049",
    "end": "1469779"
  },
  {
    "text": "failed clusters and the clusters which are running long which are in starting state it rides on entry to the DynamoDB",
    "start": "1469779",
    "end": "1477610"
  },
  {
    "text": "and as soon as an entry is done to DynamoDB a lambda function gets kicked off so this particular function",
    "start": "1477610",
    "end": "1483450"
  },
  {
    "text": "basically drops the file on s3 for that failed cluster and the whole cycle again",
    "start": "1483450",
    "end": "1489279"
  },
  {
    "text": "gets kicked off so and the notification is sent to operations so this is a very",
    "start": "1489279",
    "end": "1494470"
  },
  {
    "text": "simple utility that we use using a SS lambda to take care of all the EMF from",
    "start": "1494470",
    "end": "1499779"
  },
  {
    "text": "monitoring few things that we expect will be",
    "start": "1499779",
    "end": "1504820"
  },
  {
    "text": "helpful to us with AWS is if the lifecycle policies are done based on",
    "start": "1504820",
    "end": "1510399"
  },
  {
    "text": "tags instead of Feist five timestamp we are seriously exploring redshift as a",
    "start": "1510399",
    "end": "1516820"
  },
  {
    "text": "for the database solution so if redshift can talk to s3 using external tables",
    "start": "1516820",
    "end": "1522609"
  },
  {
    "text": "that will be helpful to us also",
    "start": "1522609",
    "end": "1527820"
  },
  {
    "text": "if I use the EMR currently you have to go to each region if you want to explore",
    "start": "1529889",
    "end": "1535840"
  },
  {
    "text": "it using cope console so something like a global dashboard will help us as well we have passed this request to EMR and",
    "start": "1535840",
    "end": "1543820"
  },
  {
    "text": "team and they are working on it so to recap quickly look I explained to",
    "start": "1543820",
    "end": "1548980"
  },
  {
    "start": "1546000",
    "end": "1546000"
  },
  {
    "text": "you how we do spot transient architecture using s3 as our data Lake how we find out the optimal cluster size",
    "start": "1548980",
    "end": "1557399"
  },
  {
    "text": "and launch the clustering optimal easy to save and take the full benefit of",
    "start": "1557399",
    "end": "1562779"
  },
  {
    "text": "spot instances how we have created a pipeline using a SS lambda which is several less and event-driven and a",
    "start": "1562779",
    "end": "1570070"
  },
  {
    "text": "custom utility that we have written to monitor thousands of EMR clusters so",
    "start": "1570070",
    "end": "1575200"
  },
  {
    "text": "that we are not heavily dependent on operations we did a talk last year if",
    "start": "1575200",
    "end": "1580359"
  },
  {
    "text": "you are interested how to migrate from data center to AWS here are the links and please stop by few more questions",
    "start": "1580359",
    "end": "1587320"
  },
  {
    "text": "with that I'll turn it over to Charles",
    "start": "1587320",
    "end": "1591508"
  },
  {
    "text": "thank you for coming today my name is Charles Allen I am a head of platform at",
    "start": "1599900",
    "end": "1606000"
  },
  {
    "text": "Metamarkets you can see my contact information up here special thanks to my colleague",
    "start": "1606000",
    "end": "1611190"
  },
  {
    "text": "Jason Kim is one of our lead engineers dealing with the spot market so a little bit about who I am and what we do at",
    "start": "1611190",
    "end": "1617970"
  },
  {
    "text": "meta markets we're an ad tech company that focuses on servicing the ad tech ecosphere and allowing them to do slice",
    "start": "1617970",
    "end": "1624780"
  },
  {
    "text": "and dice analytics in real time on their data streams one of the big challenges",
    "start": "1624780",
    "end": "1629820"
  },
  {
    "text": "we have in the ad tech ecosphere is that there is a huge quantity of data coming off all the time anytime you go to a",
    "start": "1629820",
    "end": "1636720"
  },
  {
    "text": "website anytime you bring up an application any time you play a game there becomes availability to show you",
    "start": "1636720",
    "end": "1643350"
  },
  {
    "text": "an advertisement all of these availabilities have to get logged somewhere and people have to be able to",
    "start": "1643350",
    "end": "1648929"
  },
  {
    "text": "look at them and see what's going on by our estimates the quantity of these transactions that occur are over a",
    "start": "1648929",
    "end": "1655140"
  },
  {
    "text": "hundred times larger than what happens on the new york stock exchange on a daily basis so not only is the data long",
    "start": "1655140",
    "end": "1661130"
  },
  {
    "text": "but the quantity of fields and the complexity of how you analyze those fields is also much greater so we're",
    "start": "1661130",
    "end": "1668040"
  },
  {
    "text": "about a hundred times longer and potentially about ten times wider than most of the data that you see related to",
    "start": "1668040",
    "end": "1673470"
  },
  {
    "text": "what's coming off of for example the new york stock exchange we like to say move fast to think big be",
    "start": "1673470",
    "end": "1680549"
  },
  {
    "text": "open and have fun we are an industry leader for interactive analytics in programmatic marketing",
    "start": "1680549",
    "end": "1685940"
  },
  {
    "text": "we handle over a hundred million events per day that peaks at a sustained",
    "start": "1685940",
    "end": "1691470"
  },
  {
    "text": "approximately two to three million events per day once it has been massaged have we've had our real-time streams all",
    "start": "1691470",
    "end": "1697500"
  },
  {
    "text": "joined together and they have been a high availability replicated so in order for a to protect against node failure",
    "start": "1697500",
    "end": "1703020"
  },
  {
    "text": "that turns into about 300 million events per second that go into our query engine",
    "start": "1703020",
    "end": "1708620"
  },
  {
    "text": "one of our cool things that we are able to do is that we can take event ingestion lag downtime down to a few",
    "start": "1708620",
    "end": "1714419"
  },
  {
    "text": "milliseconds if required this of course depends on exactly how complex the operations you need to have done or if",
    "start": "1714419",
    "end": "1720480"
  },
  {
    "text": "you're waiting for a clique to come in before you can actually say anything about events that happened of course we can't quite predict the future yet and",
    "start": "1720480",
    "end": "1727940"
  },
  {
    "text": "have to wait for that event to come in before we can actually tag it as being a click event we do dynamic queries",
    "start": "1727940",
    "end": "1734510"
  },
  {
    "text": "meaning that we do not do any pre aggregation or or pre slice and dicing all of the things",
    "start": "1734510",
    "end": "1741470"
  },
  {
    "text": "that we do are done ad hoc at query times so we store the data in a flat format and have a specialized query",
    "start": "1741470",
    "end": "1747289"
  },
  {
    "text": "engine called druid that allows us to do dynamic slice-and-dice queries on a huge quantity of data",
    "start": "1747289",
    "end": "1753100"
  },
  {
    "text": "query latencies of less than one second for the queries that matter is incredibly common within our system and",
    "start": "1753100",
    "end": "1759340"
  },
  {
    "text": "all of our products and all of our processing is directly tailored towards",
    "start": "1759340",
    "end": "1764870"
  },
  {
    "text": "real time bidding and insight on real time bidding streams so real quick I was going to show you guys a little bit",
    "start": "1764870",
    "end": "1771200"
  },
  {
    "text": "about what our product looks like and the couple of the ways people use it we're on hotel Wi-Fi here so it will",
    "start": "1771200",
    "end": "1779510"
  },
  {
    "text": "kind of work and kind of not work so bear with me for just a minute so here we can see a view that a particular",
    "start": "1779510",
    "end": "1785929"
  },
  {
    "text": "person may have in relation to being in an exchange and what's going on is that real time data is streaming through our",
    "start": "1785929",
    "end": "1791960"
  },
  {
    "text": "system to the tune of a couple million events per second and you're able to go in and decide what kind of views you",
    "start": "1791960",
    "end": "1798320"
  },
  {
    "text": "might have so if you're an exchange or an advertiser or a DSP you can go in and say these views are specially customized",
    "start": "1798320",
    "end": "1804289"
  },
  {
    "text": "towards the way that I look at this real-time data you can go in and you can say okay I'm gonna look at all my publishers okay here's my publishers",
    "start": "1804289",
    "end": "1810380"
  },
  {
    "text": "sorted by the different quantity of options I want to know how's karma life doing that can go in and I get instantaneous responses on what my eCPM",
    "start": "1810380",
    "end": "1818570"
  },
  {
    "text": "my gross revenues are all that for how karma life is doing if I'm wanting to do",
    "start": "1818570",
    "end": "1823610"
  },
  {
    "text": "any sort of audience kind of related stuff I also have user segments I can go in and I could say oh cool I get a lot of options for up-and-comers",
    "start": "1823610",
    "end": "1830210"
  },
  {
    "text": "am I also getting impressions for those yes I'm actually getting a lot of impressions for those that's really nice if you want to go see how many people",
    "start": "1830210",
    "end": "1836539"
  },
  {
    "text": "you're actually reaching we also have the capacity to do unique so you can do uniques overtime which is the bottom",
    "start": "1836539",
    "end": "1842000"
  },
  {
    "text": "part down here this is approximate uniques if you want to sort by uniques which is computationally intensive so",
    "start": "1842000",
    "end": "1848390"
  },
  {
    "text": "hopefully it'll come back pretty soon there it goes okay so we've just calculated you know millions and millions mmm billions almost of uniques",
    "start": "1848390",
    "end": "1856940"
  },
  {
    "text": "there across various user segments if I want to do any sort of slice Daesan them",
    "start": "1856940",
    "end": "1862130"
  },
  {
    "text": "I'm going to change it back to something that's a little bit less computationally intensive if I want to go look at the",
    "start": "1862130",
    "end": "1867650"
  },
  {
    "text": "slice and dice on these particular segments I can actually split them up and I can say okay well here's how these different things performed over time and",
    "start": "1867650",
    "end": "1873890"
  },
  {
    "text": "so I get all this capacity to do my Diagnostics my insights my",
    "start": "1873890",
    "end": "1880100"
  },
  {
    "text": "prospecting my retrospective analysis it essentially as",
    "start": "1880100",
    "end": "1885770"
  },
  {
    "text": "fast as I can think about it and that's really what we try to deliver",
    "start": "1885770",
    "end": "1891200"
  },
  {
    "text": "is the really really fast user interactive experience on real-time data streams",
    "start": "1891200",
    "end": "1898299"
  },
  {
    "text": "so the big question I hope you're asking yourself right now is how do we do that",
    "start": "1898299",
    "end": "1903580"
  },
  {
    "text": "well I'll go over the brief architecture that we have in order to accommodate this torrent of data that's coming into",
    "start": "1903580",
    "end": "1909860"
  },
  {
    "text": "our system and I'm also going to highlight a couple key points where we make use of a spot market in order to do",
    "start": "1909860",
    "end": "1916340"
  },
  {
    "text": "all this in a cost-effective manner our current spot usage comes in three flavors and have heavy spot usage for",
    "start": "1916340",
    "end": "1923120"
  },
  {
    "start": "1918000",
    "end": "1918000"
  },
  {
    "text": "spark we have heavy spot usage for Druid and heavy spot usage for Jenkins the Jenkins use case is really boring",
    "start": "1923120",
    "end": "1929450"
  },
  {
    "text": "because it encompasses like five nodes but the spark Andrew use cases are much",
    "start": "1929450",
    "end": "1934850"
  },
  {
    "text": "larger encompassing a thousands of cores so I will talk mostly about how we use",
    "start": "1934850",
    "end": "1940760"
  },
  {
    "text": "spark and how I use druid for those of you who may not be as familiar with the big data ecosphere spark is like what",
    "start": "1940760",
    "end": "1946700"
  },
  {
    "text": "the cool kids are using instead of Hadoop these days and druid is a",
    "start": "1946700",
    "end": "1952390"
  },
  {
    "text": "query engine built to analyze time series data and very fast slice and dice",
    "start": "1952390",
    "end": "1957409"
  },
  {
    "text": "manners so the architecture that we use in order to take in all our data and make it",
    "start": "1957409",
    "end": "1963200"
  },
  {
    "start": "1959000",
    "end": "1959000"
  },
  {
    "text": "available is known as the lambda architecture which means that it has a real time component and a batch",
    "start": "1963200",
    "end": "1968690"
  },
  {
    "text": "component here we see the main things that make up our real time components our initial intake point is a copper",
    "start": "1968690",
    "end": "1975470"
  },
  {
    "text": "cluster and this kafka cluster is where all the data initially goes in and its",
    "start": "1975470",
    "end": "1980510"
  },
  {
    "text": "main purpose is simply to catch data it doesn't really do anything special its sole purpose is to make sure that when",
    "start": "1980510",
    "end": "1986690"
  },
  {
    "text": "data comes to us we can catch it so if you're looking at it from a kind of data Lake kind of perspective",
    "start": "1986690",
    "end": "1993049"
  },
  {
    "text": "access our data Lake at this particular point from there for the real-time portion it",
    "start": "1993049",
    "end": "1998330"
  },
  {
    "text": "goes into a combination of Kafka SAMHSA's where Kafka is the thing that is storing the data at rest and Samsa is",
    "start": "1998330",
    "end": "2005320"
  },
  {
    "text": "the thing which is acting on the data as it flies around the system the Sam's a component here does things like",
    "start": "2005320",
    "end": "2010499"
  },
  {
    "text": "correctly doing ETL on the individual components on the individual records correctly making sure that multiple data",
    "start": "2010499",
    "end": "2017049"
  },
  {
    "text": "streams that have events that need to be joined together are actually being joined together",
    "start": "2017049",
    "end": "2022409"
  },
  {
    "text": "and then finally it gets pushed over into the druid real-time component the",
    "start": "2022409",
    "end": "2027519"
  },
  {
    "text": "key advantages of our real-time pipeline are that is pretty accurate but it delivers data incredibly fast pretty",
    "start": "2027519",
    "end": "2034359"
  },
  {
    "text": "much as fast as you want just to be able to deliver it the downside is that it's only pretty",
    "start": "2034359",
    "end": "2041049"
  },
  {
    "text": "accurate and not quite perfectly accurate we also have a batch component which is",
    "start": "2041049",
    "end": "2046059"
  },
  {
    "start": "2043000",
    "end": "2043000"
  },
  {
    "text": "the other part and that comprised of the same initial Kafka cluster which is persisted into s3 and then SPARC does",
    "start": "2046059",
    "end": "2053980"
  },
  {
    "text": "work on the data that comes out of s3 and pushes the results of that data into Druid for historical analysis so in this",
    "start": "2053980",
    "end": "2061210"
  },
  {
    "text": "particular scenario we are using s3 as the data leak so essentially we push into the data leak from Kafka to s3 and",
    "start": "2061210",
    "end": "2066700"
  },
  {
    "text": "then do processing from s3 to another spot on s3 which then gets loaded into our druid historical",
    "start": "2066700",
    "end": "2073589"
  },
  {
    "text": "this happens typically to the tune of a few hours later that is configurable and",
    "start": "2073589",
    "end": "2079510"
  },
  {
    "text": "is largely largely there to handle potential real time outages which we can",
    "start": "2079510",
    "end": "2084790"
  },
  {
    "text": "adjust the latency of this batch processing if needed to speed it up or slow it down we also have it there to",
    "start": "2084790",
    "end": "2090970"
  },
  {
    "text": "handle deduplicated data and late data because as sophisticated as we try to be on our side we also have to live in the",
    "start": "2090970",
    "end": "2097990"
  },
  {
    "text": "real world where sometimes it's not our or our side that has problems and customers end up sending us late data so",
    "start": "2097990",
    "end": "2103390"
  },
  {
    "text": "this this is the part that handles that putting it all together we have the top",
    "start": "2103390",
    "end": "2110530"
  },
  {
    "text": "part which comprises our real-time pipeline where we go from cockpit to real-time to eventually being handed off",
    "start": "2110530",
    "end": "2116230"
  },
  {
    "start": "2113000",
    "end": "2113000"
  },
  {
    "text": "to historical x' and the bottom part which happens a few hours later as our batch processing which goes from Kafka",
    "start": "2116230",
    "end": "2121720"
  },
  {
    "text": "into our batch system and then gets loaded into the historic from there one of the cool things about druid is that",
    "start": "2121720",
    "end": "2127760"
  },
  {
    "text": "whenever you're issuing queries into it it has the capacity to both hit the real time component and historical components",
    "start": "2127760",
    "end": "2134390"
  },
  {
    "text": "at the exact same time so you get data as soon as it's available and a lemon",
    "start": "2134390",
    "end": "2141200"
  },
  {
    "text": "should land up there there's a lot of lambda being thrown around this week this is called this is a land architecture is actually the name of",
    "start": "2141200",
    "end": "2147020"
  },
  {
    "text": "this style of architecture so just so you know so again just to reiterate our key",
    "start": "2147020",
    "end": "2153320"
  },
  {
    "start": "2150000",
    "end": "2150000"
  },
  {
    "text": "technologies that we use our Kafka Sam's a spark and druid we actually run our own versions of all of these and we",
    "start": "2153320",
    "end": "2159740"
  },
  {
    "text": "contribute back to the open source communities for all of these so you'll see multiple people from Metamarkets on the list of people who have contributed",
    "start": "2159740",
    "end": "2165440"
  },
  {
    "text": "to these products spark on spot I love spark on spot it's",
    "start": "2165440",
    "end": "2171710"
  },
  {
    "text": "great we were running Hadoop on AWS before EMR",
    "start": "2171710",
    "end": "2178040"
  },
  {
    "text": "even existed and it worked well for quite a while but it became a huge pain for us to be able to have to manage both",
    "start": "2178040",
    "end": "2185480"
  },
  {
    "text": "Hadoop and HDFS and all these other things essentially it mean meant that one cluster was actually a whole bunch",
    "start": "2185480",
    "end": "2191060"
  },
  {
    "text": "of different kind of clusters that we had to keep track of we said we didn't really like that we want to be able to",
    "start": "2191060",
    "end": "2196250"
  },
  {
    "text": "use the spot market a little more because this batch recovery side that can take a while if it has to take a little bit longer that's not the end of",
    "start": "2196250",
    "end": "2203180"
  },
  {
    "text": "the world we can live with that or customers can live with that because they have the mostly accurate data already there",
    "start": "2203180",
    "end": "2210070"
  },
  {
    "text": "so we looked at spark and we said okay this is great not only can we run this things on two spot pretty well we don't",
    "start": "2210070",
    "end": "2217070"
  },
  {
    "text": "have to deal with all these extra clusters we are all these extra components of our cluster it has pretty good partial failure recovery that we",
    "start": "2217070",
    "end": "2224599"
  },
  {
    "text": "found so it's retries are pretty nice it also has great native support for meso CR n-- and as a standalone cluster and",
    "start": "2224599",
    "end": "2231050"
  },
  {
    "text": "this is something that's become very important to us recently where essentially once you have something on the spot market you have essentially",
    "start": "2231050",
    "end": "2236480"
  },
  {
    "text": "this big heterogeneous pool of resources you have to be able to manage them and launch things on them and keep track of them having something out there that is",
    "start": "2236480",
    "end": "2243260"
  },
  {
    "text": "made to manage heterogeneous resources like meso CR or a standalone SPARC is very very advantageous",
    "start": "2243260",
    "end": "2249880"
  },
  {
    "text": "the only real bad thing that we found across the transition to spark on",
    "start": "2249880",
    "end": "2255549"
  },
  {
    "text": "sparklin spot was that it's really rough to configure multi-tenant so whenever you have these highly heterogeneous",
    "start": "2255549",
    "end": "2261829"
  },
  {
    "text": "clusters you're going to have different capacity in all these nodes and often you're going to have multiple SPARC",
    "start": "2261829",
    "end": "2268249"
  },
  {
    "text": "tasks run co-tenant on a particular machine now we found out the hard way to",
    "start": "2268249",
    "end": "2275390"
  },
  {
    "text": "start with that there's a lot of places within both the SPARC code and even within the JVM itself that assume number",
    "start": "2275390",
    "end": "2282919"
  },
  {
    "text": "of cores is the correct thread setting for various things including jetty settings and garbage collection settings",
    "start": "2282919",
    "end": "2289009"
  },
  {
    "text": "so you can imagine if you're running at X 132 Excel and every single sub component that runs on an X 132 Excel",
    "start": "2289009",
    "end": "2296089"
  },
  {
    "text": "all been packed and managed through May so that yarn or stand-alone thinks that it has all 128 nodes to itself then it's",
    "start": "2296089",
    "end": "2303829"
  },
  {
    "text": "going to end up with a lot of things that are wrong when it actually tries to run so you have to do a little bit of configuring and a little bit of comprar",
    "start": "2303829",
    "end": "2310009"
  },
  {
    "text": "Perkin tainer is a ssin to make sure that the sparks that you're running on a",
    "start": "2310009",
    "end": "2315140"
  },
  {
    "text": "heterogeneous spot cluster actually do the things that you want it to do and that it understands the resources that",
    "start": "2315140",
    "end": "2321919"
  },
  {
    "text": "it has available memory is another component where I've had to do a lot of configuration changes",
    "start": "2321919",
    "end": "2327650"
  },
  {
    "text": "so making sure that spark properly understands the difference between heap memory direct memory and page cache and",
    "start": "2327650",
    "end": "2334849"
  },
  {
    "text": "getting those to be taken into account properly there's another place where you have to be careful about doing things in",
    "start": "2334849",
    "end": "2340339"
  },
  {
    "text": "a multi-tenant environment how much data we actually pushed through spark spark has a really cool metric",
    "start": "2340339",
    "end": "2347869"
  },
  {
    "text": "called memory bytes spilled that's essentially when it starts to since that it has too much space that is trying to",
    "start": "2347869",
    "end": "2353059"
  },
  {
    "text": "take up locally it spills it spilled some of it to disk we typically do between one and four petabytes a day and",
    "start": "2353059",
    "end": "2359029"
  },
  {
    "text": "that comes to be between 200 billion and a trillion events per day peak days",
    "start": "2359029",
    "end": "2364939"
  },
  {
    "text": "though can be up to five times baseline so with our spark cluster we can very easily greatly grow on spot up to five",
    "start": "2364939",
    "end": "2373309"
  },
  {
    "text": "times what our baseline load is without any sort of problems and then scale back down once we don't need that anymore",
    "start": "2373309",
    "end": "2380079"
  },
  {
    "start": "2380000",
    "end": "2380000"
  },
  {
    "text": "or spot pricing strategy is not as aggressive as some other people we aim",
    "start": "2381950",
    "end": "2389160"
  },
  {
    "text": "for amortized savings and ease of management and developer sanity so",
    "start": "2389160",
    "end": "2394290"
  },
  {
    "text": "instead of trying to squeak every last penny of savings out we aim for pretty good savings and ease of management so",
    "start": "2394290",
    "end": "2400620"
  },
  {
    "text": "in our use cases in our real time our actual usage in production we tend to",
    "start": "2400620",
    "end": "2406530"
  },
  {
    "text": "get at least 60% savings versus on demand with minimal to no effort to actual cost management this is more more",
    "start": "2406530",
    "end": "2413490"
  },
  {
    "text": "retrospective being like if things get too expensive for too long switch the strategies otherwise don't worry too",
    "start": "2413490",
    "end": "2419310"
  },
  {
    "text": "much about it to compare that we get approximately equal to what a three-year commitment would be but we're not locked",
    "start": "2419310",
    "end": "2427020"
  },
  {
    "text": "into a particular instance type so after the announcements this morning I'm gonna go back and talk with my team and be like hey do we want to use some of these",
    "start": "2427020",
    "end": "2433140"
  },
  {
    "text": "those are for look pretty nice so we'll see how that goes so",
    "start": "2433140",
    "end": "2438410"
  },
  {
    "start": "2440000",
    "end": "2440000"
  },
  {
    "text": "the trade-offs here running spark on spot you have a little bit more complex",
    "start": "2441680",
    "end": "2447000"
  },
  {
    "text": "job failure handling the big questions we end up asking ourselves are",
    "start": "2447000",
    "end": "2452360"
  },
  {
    "text": "did my job die because of me was this a new bug at spark that I didn't find yet",
    "start": "2452360",
    "end": "2458070"
  },
  {
    "text": "because we tend to run both on very close to the edge of spark as well as with our own patches is this something",
    "start": "2458070",
    "end": "2463680"
  },
  {
    "text": "that went wrong in the data did the customer change their data format and not tell us and we didn't detect it properly or is the spot market itself",
    "start": "2463680",
    "end": "2470100"
  },
  {
    "text": "just having some sort of fluctuation or strange day this means that you may have a couple",
    "start": "2470100",
    "end": "2475440"
  },
  {
    "text": "more random delays but for us since it's a batch pipeline we have pretty good monitoring on our",
    "start": "2475440",
    "end": "2481020"
  },
  {
    "text": "batch pipelines so we can adjust and respond appropriately pretty well this means it's going to be a couple more man-hours to manage or a little bit more",
    "start": "2481020",
    "end": "2487770"
  },
  {
    "text": "automation to build but overall we found it very worth it and it also provides a",
    "start": "2487770",
    "end": "2493200"
  },
  {
    "text": "fun chunk of technical interesting problems for developers to sink their teeth into",
    "start": "2493200",
    "end": "2500539"
  },
  {
    "text": "drew it on spot so being a query engine drew it on spot",
    "start": "2501500",
    "end": "2506840"
  },
  {
    "text": "has a very interesting property that not a lot of other things have one it's built for interactive query processing",
    "start": "2506840",
    "end": "2513770"
  },
  {
    "text": "but two it also stores a lot of state locally so instead of having stuff that's all over the place that it has to",
    "start": "2513770",
    "end": "2520130"
  },
  {
    "text": "go and figure out where to do things on it knows where data is located and then it executes queries based on where the",
    "start": "2520130",
    "end": "2526670"
  },
  {
    "text": "data is so that state is stored locally per node some of our historical nodes which are",
    "start": "2526670",
    "end": "2533270"
  },
  {
    "text": "the nodes that run all the state locally run on spot not all of them some of them",
    "start": "2533270",
    "end": "2540170"
  },
  {
    "text": "do and we use EBS to do that we have almost a fifth of a petabyte of EBS",
    "start": "2540170",
    "end": "2545690"
  },
  {
    "text": "which is dedicated towards running these Druid nodes on spot the cool thing about that is that",
    "start": "2545690",
    "end": "2552400"
  },
  {
    "text": "we have a fifth of a petabyte of data that can absolutely vanish but then come",
    "start": "2552400",
    "end": "2557600"
  },
  {
    "text": "back in 15 minutes we accomplished this oh yeah this one",
    "start": "2557600",
    "end": "2564170"
  },
  {
    "start": "2562000",
    "end": "2562000"
  },
  {
    "text": "first so a little bit about what that means and where that comes into play and where we can actually exercise that",
    "start": "2564170",
    "end": "2570520"
  },
  {
    "text": "typically this for historical data has some decaying value to our customers we",
    "start": "2570520",
    "end": "2577580"
  },
  {
    "text": "define the data into three different tiers we have a hot the cold Annelle I see effectively how it works is that the",
    "start": "2577580",
    "end": "2584420"
  },
  {
    "text": "hot data is the stuff that people are querying all the time these are the this is the data that's most recent most valuable except for the real-time the",
    "start": "2584420",
    "end": "2590930"
  },
  {
    "text": "real-time is even more recent and hopefully more valuable and we want to be able to make sure that that data",
    "start": "2590930",
    "end": "2596360"
  },
  {
    "text": "comes back as quick as possible typically are its customizable per client but typically our clients store",
    "start": "2596360",
    "end": "2602120"
  },
  {
    "text": "between things greater than an hour up to a couple months in our hot tier cold here",
    "start": "2602120",
    "end": "2609350"
  },
  {
    "text": "is are things that are a little bit longer in time range and usually not",
    "start": "2609350",
    "end": "2614840"
  },
  {
    "text": "queried nearly as often that typically tends to be on the order of a few months in the Hat in the",
    "start": "2614840",
    "end": "2620390"
  },
  {
    "text": "history to a few more months but not quite to the year scale so very commonly it'll be like a quarter or two quarters",
    "start": "2620390",
    "end": "2625760"
  },
  {
    "text": "or something like that and our IC tier we typically keep all the rest of the",
    "start": "2625760",
    "end": "2630980"
  },
  {
    "text": "data so we're like okay you have this data that's historical data that you don't look at very often and most of our",
    "start": "2630980",
    "end": "2636560"
  },
  {
    "text": "customers understand that like it'll take a little bit longer to access that data so we have this icy tier that allows us to do queries in the same",
    "start": "2636560",
    "end": "2643460"
  },
  {
    "text": "system not only on stuff that's happening right now but look two years in the past and see what was happening",
    "start": "2643460",
    "end": "2648500"
  },
  {
    "text": "two years ago if so required luckily that's not a very common demand and so if we look at the actual QPS that",
    "start": "2648500",
    "end": "2656900"
  },
  {
    "text": "we see on these query nodes we tend to have a logarithmic fall-off per tier where most people are looking at what's",
    "start": "2656900",
    "end": "2662900"
  },
  {
    "text": "happening on hot some people are looking at what's happening on colder and every once in a while someone wants to know",
    "start": "2662900",
    "end": "2668840"
  },
  {
    "text": "what happened a very long time ago there is a component that does the aging",
    "start": "2668840",
    "end": "2677420"
  },
  {
    "text": "off of data and says okay data you're old enough to where you don't need to be in hot anymore you can go over to cold",
    "start": "2677420",
    "end": "2682910"
  },
  {
    "text": "ok data in cold you're not needed anymore to be in cold you can aged out into icy",
    "start": "2682910",
    "end": "2688540"
  },
  {
    "text": "that thing is called the coordinator just because I'm gonna mention it here in just a minute so the coordinator says",
    "start": "2688540",
    "end": "2693680"
  },
  {
    "text": "okay you can go from hot to cold and called the icy the cool thing about this particular setup is that you don't have",
    "start": "2693680",
    "end": "2698750"
  },
  {
    "text": "to be exclusively in hot or cold you can be in both so all of the data that is in",
    "start": "2698750",
    "end": "2704810"
  },
  {
    "text": "hot is replicated and cold and all the data that is in cold is replicated and I see that means we have very good",
    "start": "2704810",
    "end": "2710570"
  },
  {
    "text": "failover so if any one particular node goes down it has a lower SLA tier that",
    "start": "2710570",
    "end": "2716120"
  },
  {
    "text": "is able to pick up all of its work just fine so if you were to look at this graph and",
    "start": "2716120",
    "end": "2721160"
  },
  {
    "text": "try to guess which part has the most redundancy overall to where you can",
    "start": "2721160",
    "end": "2726380"
  },
  {
    "text": "tolerate failure is best you'd probably say you're right about there so if things have problems there we can fail /",
    "start": "2726380",
    "end": "2732470"
  },
  {
    "text": "- I see and we also have a certain amount of redundancy that happens in hot so this is actually where we use the",
    "start": "2732470",
    "end": "2737990"
  },
  {
    "text": "spot market for Druid because if the state disappears we have the appropriate amount of failover and",
    "start": "2737990",
    "end": "2743680"
  },
  {
    "text": "we also have the capacity if someone is doing long reports where we can adjust",
    "start": "2743680",
    "end": "2748850"
  },
  {
    "text": "what our compute needs are but that's does not scale as fast simply due to the amount of state that is required on that",
    "start": "2748850",
    "end": "2756460"
  },
  {
    "start": "2756000",
    "end": "2756000"
  },
  {
    "text": "we do this by being able to take EBS volumes that are on our cold nodes",
    "start": "2756970",
    "end": "2762640"
  },
  {
    "text": "detaching them when we detect detect that there's a spot market fluctuate and then reattaching them when new notes",
    "start": "2762640",
    "end": "2769500"
  },
  {
    "text": "come online so this only works if you're able to like play within the same availability availability zone but we",
    "start": "2769500",
    "end": "2776490"
  },
  {
    "text": "found that we typically can we found sometimes for example like an r3 for x-large is really really expensive but",
    "start": "2776490",
    "end": "2783450"
  },
  {
    "text": "for whatever reason the r3 8x largest still have really low price so we're like fine we'll use our three attic larges instead we don't care that we had",
    "start": "2783450",
    "end": "2790230"
  },
  {
    "text": "the extra resources right so with using Dru EBS withdrew it on",
    "start": "2790230",
    "end": "2795900"
  },
  {
    "text": "spot we define a pool tag for our EBS volumes if the EBS pool is empty we",
    "start": "2795900",
    "end": "2801660"
  },
  {
    "text": "simply will create a new volume otherwise we simply claim one from the pool do a quick sanity check on the",
    "start": "2801660",
    "end": "2807210"
  },
  {
    "text": "volume and if it's unrecoverable or has some sort of permanent error just discard it and we'll create a new one",
    "start": "2807210",
    "end": "2814190"
  },
  {
    "text": "once we're actually running we do some monitoring of the api's that amazon",
    "start": "2814190",
    "end": "2820530"
  },
  {
    "text": "provides to may see whether or not we are about to be terminated on spot if we think we're going to be terminated on",
    "start": "2820530",
    "end": "2826530"
  },
  {
    "text": "spot we stop the application cleanly unmount the volume but we don't",
    "start": "2826530",
    "end": "2831540"
  },
  {
    "text": "actually terminate the instance we wait for does so whenever AWS comes in and terminates us we get that our for free",
    "start": "2831540",
    "end": "2837270"
  },
  {
    "text": "and then if something went absolutely horribly wrong then we can at least go in and debug the instance and figure out",
    "start": "2837270",
    "end": "2843300"
  },
  {
    "text": "what happened originally we actually ran Druid without",
    "start": "2843300",
    "end": "2850050"
  },
  {
    "text": "using EBS reattachment so every once in a while when we got one of these little search messages that appeared in our",
    "start": "2850050",
    "end": "2856380"
  },
  {
    "text": "email it was going to be an interesting day but we now have put VBS reattachment",
    "start": "2856380",
    "end": "2861870"
  },
  {
    "text": "in place we still get these messages but they're just ignored they originally",
    "start": "2861870",
    "end": "2867990"
  },
  {
    "text": "were incredibly terrifying to get but now it's just mundane by the time we actually have noticed that it has",
    "start": "2867990",
    "end": "2873480"
  },
  {
    "text": "happened it usually has fixed itself and it's it's just not as interesting anymore which is a very good problem to",
    "start": "2873480",
    "end": "2879750"
  },
  {
    "text": "have if you're running Druitt on spot a",
    "start": "2879750",
    "end": "2885810"
  },
  {
    "start": "2881000",
    "end": "2881000"
  },
  {
    "text": "couple of tips that I want to kind of call out the coordinator that little thing that thing that decides what needs",
    "start": "2885810",
    "end": "2891360"
  },
  {
    "text": "to go where does not do very well if you only",
    "start": "2891360",
    "end": "2896490"
  },
  {
    "text": "half of a tier of stuff available so if you're cold tier is hacked in half the",
    "start": "2896490",
    "end": "2902220"
  },
  {
    "text": "coordinator is not going to like that it is typically better to kill that tier completely until you can fix things if",
    "start": "2902220",
    "end": "2908130"
  },
  {
    "text": "you can't fix them fast enough rather than let instances flap up and down because the coordinator gets very",
    "start": "2908130",
    "end": "2914130"
  },
  {
    "text": "confused about where it can put things and it might take way way too long reaching solutions on where things",
    "start": "2914130",
    "end": "2920820"
  },
  {
    "text": "should go rather than being able to move forward as a sidenote nodes typically have a",
    "start": "2920820",
    "end": "2927180"
  },
  {
    "text": "burr in time due to the JVM and the just-in-time compiler in our production tests in experience this is usually on",
    "start": "2927180",
    "end": "2934530"
  },
  {
    "text": "the order of a few minutes maybe up to five in some cases but that's just something to notice or it's something to",
    "start": "2934530",
    "end": "2940020"
  },
  {
    "text": "know so we do this by using druid the EBS",
    "start": "2940020",
    "end": "2947490"
  },
  {
    "text": "reattachment script we are very happy to open-source this tool if you go into the EBS or go into the Metamarkets get",
    "start": "2947490",
    "end": "2953580"
  },
  {
    "text": "repository it's just called EBS reattach we'll have an official announcement for it sometime in the next couple of days so",
    "start": "2953580",
    "end": "2959180"
  },
  {
    "text": "yeah so if you want to use EBS reattachment in the way that we use EBS reattachment please go check out our",
    "start": "2959180",
    "end": "2964619"
  },
  {
    "text": "source code if nothing else you may find it interesting to see the different error messages we've seen come back from",
    "start": "2964619",
    "end": "2970020"
  },
  {
    "text": "the spot API that is provided so",
    "start": "2970020",
    "end": "2975619"
  },
  {
    "text": "monitoring everybody who does production stuff needs to know about monitoring",
    "start": "2975770",
    "end": "2982609"
  },
  {
    "text": "we took a look at the spot market itself and said if only there was some tool that allowed powerful drill-down",
    "start": "2983210",
    "end": "2989339"
  },
  {
    "text": "analytics on real time markets so we took a look at our own stuff and we said hey why don't we use our own stuff and",
    "start": "2989339",
    "end": "2996240"
  },
  {
    "text": "dog food using the spot prices using the spot market which we do so we use the our own tool and able to do to do slice",
    "start": "2996240",
    "end": "3004010"
  },
  {
    "text": "and dice analytics on the spot market how the spot market is doing we have the capacity to do things like look at the",
    "start": "3004010",
    "end": "3010190"
  },
  {
    "text": "average spot price per gigabyte of RAM see what different instance types are doing over the course of time and we can",
    "start": "3010190",
    "end": "3016430"
  },
  {
    "text": "both analyze micro and macro trends so here's an example of a micro trend where you can see for one particular day in",
    "start": "3016430",
    "end": "3022550"
  },
  {
    "text": "September we had our three - XLS which went way way up above what you would expect for even in our",
    "start": "3022550",
    "end": "3029460"
  },
  {
    "text": "3:8 Excel and so you can adjust your strategy for where things need to be placed using this kind of information so",
    "start": "3029460",
    "end": "3036119"
  },
  {
    "text": "you can very easily spot knowing what the constraints on this program are where can I actually run it which for us",
    "start": "3036119",
    "end": "3043020"
  },
  {
    "text": "is either CPU bound or memory - bound depending on exactly what what the particular task is",
    "start": "3043020",
    "end": "3050000"
  },
  {
    "text": "you can also look at macro trends I particularly like this one this is a graph of what happened to the X 132",
    "start": "3050000",
    "end": "3057600"
  },
  {
    "text": "Excel spot price over the course of September if you look on the left over",
    "start": "3057600",
    "end": "3063270"
  },
  {
    "text": "there that is not an interesting spot market at all you simply flap up to the",
    "start": "3063270",
    "end": "3068460"
  },
  {
    "text": "on-demand price and then back down to a very low price that becomes very hard to predict to predict anything it becomes",
    "start": "3068460",
    "end": "3073920"
  },
  {
    "text": "very hard to play in overall but on the right over there you can see sometime in mid-september something happened and it",
    "start": "3073920",
    "end": "3079680"
  },
  {
    "text": "became a very interesting market to go play in",
    "start": "3079680",
    "end": "3083450"
  },
  {
    "text": "final thoughts so these are a couple of things partly related to spot partly related to just some of our lessons that",
    "start": "3086359",
    "end": "3092220"
  },
  {
    "text": "we've learned in trying to grow a very large data pool that you can query",
    "start": "3092220",
    "end": "3098030"
  },
  {
    "text": "switching from spot to on-demand does not always work",
    "start": "3098030",
    "end": "3103369"
  },
  {
    "text": "anytime we have problems in a particular spot pool that is not an indicator to us",
    "start": "3103369",
    "end": "3108869"
  },
  {
    "text": "to go to on-demand that is an indicator to us to go seek new resources elsewhere so our bidding strategy is a little bit",
    "start": "3108869",
    "end": "3115830"
  },
  {
    "text": "over what on-demand is but we don't actually use that to say switch to on-demand we use that to say go some",
    "start": "3115830",
    "end": "3122010"
  },
  {
    "text": "Worlds your pricing strategy is to roughly tune to the value of lost work which for us",
    "start": "3122010",
    "end": "3128580"
  },
  {
    "text": "both encompasses the actual Machine time but also a very large component of",
    "start": "3128580",
    "end": "3133890"
  },
  {
    "text": "developer and operator sanity comes into play there and our own experience if",
    "start": "3133890",
    "end": "3139350"
  },
  {
    "text": "you're trying to scale in a particular market meaning one particular instance type in one particular availability zone",
    "start": "3139350",
    "end": "3145140"
  },
  {
    "text": "you want to do that kind of slowly if you try to add more than a couple dozen nodes at a time we tend to spike the",
    "start": "3145140",
    "end": "3151770"
  },
  {
    "text": "spot market and whereas if we instead of try asking for a hundred nodes at once or five hundred nodes at once if you ask",
    "start": "3151770",
    "end": "3157410"
  },
  {
    "text": "for ten or at a time it tends to have a much more smooth effect on the overall price in the spot market us East one is",
    "start": "3157410",
    "end": "3163930"
  },
  {
    "text": "really crowded if you can actually play in places that are not us East one effectively I'm a highly advised doing",
    "start": "3163930",
    "end": "3169840"
  },
  {
    "text": "so from stuff related to not spot but just",
    "start": "3169840",
    "end": "3176560"
  },
  {
    "text": "our cluster in general if you're able to start with a multi honed set up meaning",
    "start": "3176560",
    "end": "3181900"
  },
  {
    "text": "that you have all of your applications assume from the start that they're running an either multiple availability",
    "start": "3181900",
    "end": "3187120"
  },
  {
    "text": "zones or multiple regions then that will help you grow pretty much run bounded us",
    "start": "3187120",
    "end": "3193180"
  },
  {
    "text": "West are now us central are very great places to start if you have not started anywhere yet zookeeper is something that",
    "start": "3193180",
    "end": "3199390"
  },
  {
    "text": "is always causing us pain the more zookeeper quorums that you can maintain the better don't skimp on that if they",
    "start": "3199390",
    "end": "3205750"
  },
  {
    "text": "go down think bad things happen lastly be sure if you can to try to start out",
    "start": "3205750",
    "end": "3211930"
  },
  {
    "text": "with a cluster resource framework of some kind so this is a meso this is a yarn or if you're just using spark the",
    "start": "3211930",
    "end": "3217930"
  },
  {
    "text": "spark stand-alone cluster tends to do okay it does pretty well the changes in",
    "start": "3217930",
    "end": "3223030"
  },
  {
    "text": "meso s-- for spark 2.0 are fantastic by the way so if you haven't really evaluated meso spark since 1.6 then I",
    "start": "3223030",
    "end": "3230260"
  },
  {
    "text": "highly advise checking it out real quick we are hiring if this sounds like challenges that you're interested in",
    "start": "3230260",
    "end": "3236290"
  },
  {
    "text": "being a part of come talk to me or check out our QR link puree link up there well summary we have a great internal",
    "start": "3236290",
    "end": "3244180"
  },
  {
    "text": "tooling for the spot market both in our EBS reattachment and our insight and analysis on real-time bidding markets",
    "start": "3244180",
    "end": "3249400"
  },
  {
    "text": "and spark is working really great with proper configuration and also druid is working fantastic for us",
    "start": "3249400",
    "end": "3255600"
  },
  {
    "text": "thank you",
    "start": "3255600",
    "end": "3258630"
  }
]