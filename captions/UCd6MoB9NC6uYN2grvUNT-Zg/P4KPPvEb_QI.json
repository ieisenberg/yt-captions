[
  {
    "text": "that's better uh we'll be talking about the web logs on Amazon uh red shift project uh",
    "start": "359",
    "end": "6879"
  },
  {
    "text": "and then sumer's going to talk about uh how we designed um a petabyte on red",
    "start": "6879",
    "end": "11960"
  },
  {
    "text": "shift some of our best practices that we used uh throughout uh the uh the year we",
    "start": "11960",
    "end": "17000"
  },
  {
    "text": "did this then we'll uh have conclusion and questions so quick",
    "start": "17000",
    "end": "23800"
  },
  {
    "text": "overview uh the Amazon data warehouse this is the uh Amazon corporate data warehouse so this is uh where all the",
    "start": "23800",
    "end": "30880"
  },
  {
    "text": "data resides for orders shipments uh tax records Etc uh used throughout the",
    "start": "30880",
    "end": "36239"
  },
  {
    "text": "entire company uh it's literally pedabytes of data uh the primary data warehouse is Oracle we've been at Oracle",
    "start": "36239",
    "end": "42879"
  },
  {
    "text": "for uh over a decade now uh we're also using EMR and for the past two years",
    "start": "42879",
    "end": "47920"
  },
  {
    "text": "we've been using uh more and more red shift uh we own and manage all of the hardware and software and aside from the",
    "start": "47920",
    "end": "54960"
  },
  {
    "text": "Oracle database everything is our own IP uh built from the ground up uh one thing",
    "start": "54960",
    "end": "60800"
  },
  {
    "text": "I do want to say uh is a bit confusing is we're not actually part of AWS so",
    "start": "60800",
    "end": "65920"
  },
  {
    "text": "we're part of the uh corporate side of Amazon uh that you're used to for buying and selling stuff as opposed to AWS so",
    "start": "65920",
    "end": "72720"
  },
  {
    "text": "we're the first customer uh internal customer as you will um I always like to start uh this",
    "start": "72720",
    "end": "79360"
  },
  {
    "text": "talk with with the elephant in the room so even though we're part of Amazon again we're not part of AWS so uh our",
    "start": "79360",
    "end": "85520"
  },
  {
    "text": "mission here is to provide the best customer or provide customers the best value all right uh we want to leverage",
    "start": "85520",
    "end": "93240"
  },
  {
    "text": "red shift where it will provide the best value um and a big thing here is we like to go first and I say this we go first",
    "start": "93240",
    "end": "99720"
  },
  {
    "text": "so you don't have to uh we're the first group to get uh pedite on red shift and we encountered many problems along the",
    "start": "99720",
    "end": "105640"
  },
  {
    "text": "way work closely with the red shift team so that when you do this uh it works much more",
    "start": "105640",
    "end": "110880"
  },
  {
    "text": "smoothly uh we also want to publish best practices and the goal is satisfied customers so if it's leveraging ad",
    "start": "110880",
    "end": "117079"
  },
  {
    "text": "technology great if it's not that's okay too uh we p them to be the best we we hold them accountable and hold them to",
    "start": "117079",
    "end": "123119"
  },
  {
    "text": "high standards so that they get better right uh now one thing is this is true we're standing up here saying what great",
    "start": "123119",
    "end": "128800"
  },
  {
    "text": "stuff it is we do understand the conflict of interest happy to take your questions afterwards about that uh and",
    "start": "128800",
    "end": "134200"
  },
  {
    "text": "go deeper all right so let's talk about the project here this is putting web logs uh",
    "start": "134200",
    "end": "141480"
  },
  {
    "text": "you know classic web logs on red shift and how we're doing about it so first off why are we doing this uh",
    "start": "141480",
    "end": "148080"
  },
  {
    "text": "for people who run any kind of web services this is bread and butter uh the web logs are key for Amazon they they",
    "start": "148080",
    "end": "154440"
  },
  {
    "text": "tell us what our customers are doing uh if customers are browsing on web pages",
    "start": "154440",
    "end": "159800"
  },
  {
    "text": "on item pages but not buying things that's information we need to know uh figure out why um uh for the entire",
    "start": "159800",
    "end": "166959"
  },
  {
    "text": "digital space uh Kindle Etc uh it's turning into more of a classic uh web",
    "start": "166959",
    "end": "173159"
  },
  {
    "text": "e-commerce company so again understanding what people are doing with web blogs is critical um what makes",
    "start": "173159",
    "end": "179440"
  },
  {
    "text": "Amazon unique in this situation is for most uh online companies where it's just",
    "start": "179440",
    "end": "185280"
  },
  {
    "text": "a web presence so for example a search engine you come you click might be an ad might be a search result and you go away",
    "start": "185280",
    "end": "192239"
  },
  {
    "text": "for Amazon you come you place an order you go away there's an entire workflow",
    "start": "192239",
    "end": "198080"
  },
  {
    "text": "that happens in processing that order for the customer uh the item has to go to a fulfillment center somebody has to",
    "start": "198080",
    "end": "204200"
  },
  {
    "text": "get the item from a shelf put in a box box has to go uh get a label has to go onto a car the carrier has to deliver it",
    "start": "204200",
    "end": "211319"
  },
  {
    "text": "all this stuff is tracked right and we make very sure that it gets to the customer uh but then we have to make",
    "start": "211319",
    "end": "216640"
  },
  {
    "text": "sure we can join that all back from the web clicks so we're joining these huge",
    "start": "216640",
    "end": "222360"
  },
  {
    "text": "Logs with lots of transactional data uh and our size makes this",
    "start": "222360",
    "end": "227760"
  },
  {
    "text": "challenging we're about two terabytes of data uh per day that's growing at 67% year-over year so uh you can think of",
    "start": "227760",
    "end": "235079"
  },
  {
    "text": "that it will be three terabytes next year four terabytes the year after that",
    "start": "235079",
    "end": "241319"
  },
  {
    "text": "so what are our goals here um we set up some pretty aggressive goals based on our current uh performance uh so we want",
    "start": "241599",
    "end": "248560"
  },
  {
    "text": "to be able to query two years worth of web logs in under an hour okay uh right",
    "start": "248560",
    "end": "254439"
  },
  {
    "text": "now with our existing systems uh we're talking more like days so the goal was",
    "start": "254439",
    "end": "259759"
  },
  {
    "text": "two years and an hour which means somebody can actually do a couple experiments um during a business day",
    "start": "259759",
    "end": "265320"
  },
  {
    "text": "that's the critical point uh we want to make sure we've got constant query time right what that means is as our data",
    "start": "265320",
    "end": "271720"
  },
  {
    "text": "grows 67% year-over-year we can scale out the cluster right and still deliver",
    "start": "271720",
    "end": "277120"
  },
  {
    "text": "results uh by a wall clock time so for example we do web laabs fairly",
    "start": "277120",
    "end": "282759"
  },
  {
    "text": "consistently we want to make sure that web data is delivered for people who want to analyze that uh when they get",
    "start": "282759",
    "end": "288639"
  },
  {
    "text": "into the office every morning about 8:00 finally uh timely data right we",
    "start": "288639",
    "end": "295080"
  },
  {
    "text": "want to make sure that the data is fresh so our goal here was uh we want to make sure that all the yesterday's data is",
    "start": "295080",
    "end": "300960"
  },
  {
    "text": "available by 4: a.m. so we got 4our window to get it in and process it uh eventually we want to get to this to be",
    "start": "300960",
    "end": "306400"
  },
  {
    "text": "near real time where it's n minutes old where n is under 60 right so our next goal will in fact be 1:00 a.m. and then",
    "start": "306400",
    "end": "312520"
  },
  {
    "text": "we're going to ratchet that down as well so let me give you some specifics",
    "start": "312520",
    "end": "319120"
  },
  {
    "text": "and some hard numbers that we uh uh challenge the ads red shift team with uh for daily data uh we wanted to maintain",
    "start": "319120",
    "end": "325960"
  },
  {
    "text": "a peak of five billion rows load that in under an hour all right uh for backfill this is when we correct data turns out",
    "start": "325960",
    "end": "333039"
  },
  {
    "text": "this happens about six times a year uh there will be errors in the data for",
    "start": "333039",
    "end": "338080"
  },
  {
    "text": "example incorrect IPS or or or missing data that have to be corrected all right",
    "start": "338080",
    "end": "343160"
  },
  {
    "text": "so we want to be able to handle one month of back bill in a day all right uh the third thing and this is",
    "start": "343160",
    "end": "350360"
  },
  {
    "text": "also CRI important we want to make sure that we have what we call Caper tension so we autod drop old data um you you",
    "start": "350360",
    "end": "357319"
  },
  {
    "text": "know for uh largely for for cost constraint we don't have infinitely growing data so as data gets old you",
    "start": "357319",
    "end": "364080"
  },
  {
    "text": "know 2 years old we no longer care about it automatically drop it okay and finally uh zero impact",
    "start": "364080",
    "end": "371639"
  },
  {
    "text": "maintenance so we don't want to have maintenance Windows where we have to take things down to do things it's",
    "start": "371639",
    "end": "377520"
  },
  {
    "text": "always up it's always operational okay uh now the the bottom two things cap",
    "start": "377520",
    "end": "382680"
  },
  {
    "text": "retention and zero impact minutes we already have those and so the goal for the red shift team was to make sure that",
    "start": "382680",
    "end": "388400"
  },
  {
    "text": "we kept them uh with the new architecture so existing Solutions let's",
    "start": "388400",
    "end": "395840"
  },
  {
    "text": "talk about those so you can kind of understand the Baseline of where we went from so again our main uh database",
    "start": "395840",
    "end": "401199"
  },
  {
    "text": "cluster is Oracle rack um we've got 15 months worth of web log stored there scan rate and these are Big clusters 32",
    "start": "401199",
    "end": "408360"
  },
  {
    "text": "node clusters with some pretty impressive Enterprise grade sand uh we can do uh one week in about an hour",
    "start": "408360",
    "end": "415319"
  },
  {
    "text": "right what that translates to uh because customers have to chop this up is it takes them on the order of 1 to two",
    "start": "415319",
    "end": "422440"
  },
  {
    "text": "weeks to actually do a 15mon uh scan right because you're scanning not only every row but every",
    "start": "422440",
    "end": "429680"
  },
  {
    "text": "column so it's a massive amounts of data all right uh we also have a number of teams within Amazon who use EMR uh so",
    "start": "429680",
    "end": "437039"
  },
  {
    "text": "they're using Pig Hive Etc um they have their own clusters their own individual clusters so there's a lot of replication",
    "start": "437039",
    "end": "443560"
  },
  {
    "text": "of data uh the scan rate that we've seen here typically is an hour will do about a month all right so now we're talking",
    "start": "443560",
    "end": "450400"
  },
  {
    "text": "about a day or two uh for some of these queries one thing that we've noticed",
    "start": "450400",
    "end": "455520"
  },
  {
    "text": "across Amazon is again lots of teams have their own cluster they're all unique they're all different um and each",
    "start": "455520",
    "end": "461400"
  },
  {
    "text": "team now has a lot of overhead in managing these clusters and managing the cost that go with these",
    "start": "461400",
    "end": "468520"
  },
  {
    "text": "clusters so what did we do uh so so we challenged the red shift team as as well as a company challenge just to come up",
    "start": "468919",
    "end": "474840"
  },
  {
    "text": "with a better solution uh so we started off with a 100 node 100 1 one master 100",
    "start": "474840",
    "end": "481080"
  },
  {
    "text": "nodes 8xl red shift cluster um 100 is very much a round number U but it's",
    "start": "481080",
    "end": "487120"
  },
  {
    "text": "above the cap we needed about petabyte uh we are using three clusters",
    "start": "487120",
    "end": "492520"
  },
  {
    "text": "two for production and one for test uh we are using the standard pattern of",
    "start": "492520",
    "end": "497960"
  },
  {
    "text": "using manifest at point to uh sub files uh and chunks that actually represent the actual data file and we're using uh",
    "start": "497960",
    "end": "505479"
  },
  {
    "text": "etlm which is custom IP similar Informatica for scheduling in job runs",
    "start": "505479",
    "end": "511599"
  },
  {
    "text": "uh one thing I want to I want to pause for a second i' I talked to a number of people at this conference and I want to share a",
    "start": "511599",
    "end": "518279"
  },
  {
    "text": "best practice which really probably deserves its own slide here it's just one little line right here uh a best",
    "start": "518279",
    "end": "524640"
  },
  {
    "text": "practice that we do we've done for years is we run multiple clusters and we apply",
    "start": "524640",
    "end": "530839"
  },
  {
    "text": "our workload to each cluster individually so we think about a a data warehouse cluster as a simple box right",
    "start": "530839",
    "end": "538160"
  },
  {
    "text": "so we have to have multiple boxes right we apply the same workload to each so if",
    "start": "538160",
    "end": "543760"
  },
  {
    "text": "one goes down that's okay right we still have the other ones working all right um",
    "start": "543760",
    "end": "550680"
  },
  {
    "text": "we also have to make sure that we have a separation of test from development I'll get more important for red shift we'll",
    "start": "550680",
    "end": "556760"
  },
  {
    "text": "talk about that uh so it really is important that when you're designing these things for Enterprise needs you",
    "start": "556760",
    "end": "562399"
  },
  {
    "text": "have to have multiple clusters red shift does provide the ability to restore from backup as you'll see from our numbers it",
    "start": "562399",
    "end": "568480"
  },
  {
    "text": "takes a while right so if you need to get your data to your customers and in a data warehouse situation you always do",
    "start": "568480",
    "end": "574640"
  },
  {
    "text": "you need to make sure that you've got uh High availability which means multiple",
    "start": "574640",
    "end": "580800"
  },
  {
    "text": "clusters so I'm going get off the stage here for a second and let uh summer talk about our techniques but I want to talk",
    "start": "582519",
    "end": "588880"
  },
  {
    "text": "about the performance first so really leading with our conclusions so our goal was 2.25 trillion rows that's 15 months",
    "start": "588880",
    "end": "595880"
  },
  {
    "text": "worth of uh data uh in an hour uh we can do that in 14 minutes just blistering",
    "start": "595880",
    "end": "602440"
  },
  {
    "text": "performance this is on our 100 node cluster uh loading one day which is loading five billion rows uh again the",
    "start": "602440",
    "end": "608600"
  },
  {
    "text": "goal there was 60 minutes we're able to do that in 10 and loading 150 billion",
    "start": "608600",
    "end": "613680"
  },
  {
    "text": "rows so this is a one month backfield for us uh our goal was one day we got it done in under 10 hours so what we're",
    "start": "613680",
    "end": "621200"
  },
  {
    "text": "seeing here is uh the ability for red shift to scale out massively and it's",
    "start": "621200",
    "end": "626399"
  },
  {
    "text": "getting significantly better performance uh than we even hope for um so I'm going to turn to turn this over now to Summer",
    "start": "626399",
    "end": "633440"
  },
  {
    "text": "who will tell you how we accomplish this uh leveraging red shift thanks",
    "start": "633440",
    "end": "639639"
  },
  {
    "text": "Eric so quickly uh my name is suari I manage the data Ware housing team uh and",
    "start": "640399",
    "end": "647399"
  },
  {
    "text": "this year I've been leading the initiative of enabling uh uh clickstream data sets on red",
    "start": "647399",
    "end": "654320"
  },
  {
    "text": "shift so uh just a brief outline of the talk so I'm I'm I'm going to be covering",
    "start": "655120",
    "end": "660360"
  },
  {
    "text": "uh the design specific the the design aspects and uh following up with uh the",
    "start": "660360",
    "end": "666480"
  },
  {
    "text": "bench uh benchmarks and the learnings we had from there and following it up with",
    "start": "666480",
    "end": "671959"
  },
  {
    "text": "much more granular performance uh uh the benchmarks which we uh which we found",
    "start": "671959",
    "end": "678200"
  },
  {
    "text": "out so uh so starting with so so just to",
    "start": "678200",
    "end": "684240"
  },
  {
    "text": "cover uh so rather than covering the width of uh all the uh all the tables",
    "start": "684240",
    "end": "690480"
  },
  {
    "text": "and and the design aspects uh we'll rather focus on the largest table we had so the biggest table we have is a 400 TB",
    "start": "690480",
    "end": "697440"
  },
  {
    "text": "table which captures data at the lowest grain and this table is growing really rapidly so the key challenges we had",
    "start": "697440",
    "end": "704560"
  },
  {
    "text": "with red shift was uh the deletes are expensive we'll cover that in a bit like",
    "start": "704560",
    "end": "709959"
  },
  {
    "text": "why is why is it expensive the second is the data as you insert is unsorted so you don't get the benefits uh of pruning",
    "start": "709959",
    "end": "717399"
  },
  {
    "text": "at a block level so uh what that results in results in is that your queries get",
    "start": "717399",
    "end": "722480"
  },
  {
    "text": "slower with time uh just to give more idea on these two points so as you",
    "start": "722480",
    "end": "729480"
  },
  {
    "text": "delete a data set or a bunch of rows in red shift they get logically deleted they don't get uh uh deleted actually",
    "start": "729480",
    "end": "736800"
  },
  {
    "text": "from the disk space so you have to act run a vacuum operation to flush them through so vacuum is a costly operation",
    "start": "736800",
    "end": "745040"
  },
  {
    "text": "uh processing intensive and more so on a very big data set uh on the second on",
    "start": "745040",
    "end": "750120"
  },
  {
    "text": "the unsorted data when you are actually u in so to get the benefits so you don't",
    "start": "750120",
    "end": "756240"
  },
  {
    "text": "have partitions in red shift so the equivalent feature you have in red shift is called sort keys and the data as it",
    "start": "756240",
    "end": "762440"
  },
  {
    "text": "goes in is not implicitly sorted so what this enables you to do is the S keys",
    "start": "762440",
    "end": "767519"
  },
  {
    "text": "that it actually allows you to to go to the Block Level and actually prune it each block has information of the start",
    "start": "767519",
    "end": "774720"
  },
  {
    "text": "and the end of the data set in it so you can actually jump a lot of blocks and get to the right place uh so that's the",
    "start": "774720",
    "end": "781560"
  },
  {
    "text": "performance optimization so this sorting doesn't happen implicitly you have to run a vacuum for doing that so this is",
    "start": "781560",
    "end": "788000"
  },
  {
    "text": "costly so that was one of the constraints we had uh the next one was the locks on red shift are at table",
    "start": "788000",
    "end": "795959"
  },
  {
    "text": "level they are not at row level so you can't actually do concurrent parallel loads as you can do in Oracle with",
    "start": "795959",
    "end": "801680"
  },
  {
    "text": "different partitions being there uh so this is critical for us because we have a use case where you you end up",
    "start": "801680",
    "end": "808560"
  },
  {
    "text": "backfilling uh data for the past say for last 3 months or four months you can't if you have to do in sequence that doesn't",
    "start": "808560",
    "end": "814399"
  },
  {
    "text": "scale for scale uh the last constraint we worked",
    "start": "814399",
    "end": "819440"
  },
  {
    "text": "around with is on the concurrency uh limits red shift gives we can have 15",
    "start": "819440",
    "end": "825199"
  },
  {
    "text": "concurrent queries running uh you can move it up to 50 but those are mostly for the Tactical queries but if you want",
    "start": "825199",
    "end": "831759"
  },
  {
    "text": "to do processing intensive workloads that's pretty much capped at 15 so and",
    "start": "831759",
    "end": "836880"
  },
  {
    "text": "and the other uh side effect there on on uh while discussing the nature of uh queries is that one bad query or runaway",
    "start": "836880",
    "end": "845240"
  },
  {
    "text": "query can uh lead to a performance impact on the cluster so how do you prevent that sort of a thing because the",
    "start": "845240",
    "end": "851720"
  },
  {
    "text": "architecture is shade nothing architecture uh so to work around those",
    "start": "851720",
    "end": "858800"
  },
  {
    "text": "uh limitations or the constraints we had uh the way the table was designed was we",
    "start": "858800",
    "end": "864600"
  },
  {
    "text": "had monthly tables under net and it was abstracted with a vew on the top uh what this enabled us to do is uh we could",
    "start": "864600",
    "end": "871600"
  },
  {
    "text": "actually run vacuums uh much faster because it was happening on lesser data you can you can actually uh sort the",
    "start": "871600",
    "end": "878240"
  },
  {
    "text": "data much more faster so there's a keyword used there called Deep copy deep copy is uh pretty much synonymous with",
    "start": "878240",
    "end": "883959"
  },
  {
    "text": "vacuum it's uh uh work around to do vacuum in a faster way where you do uh you pick up the data again and insert",
    "start": "883959",
    "end": "890440"
  },
  {
    "text": "into a new table so while it's inserted AR fresh it gets sorted implicitly um so",
    "start": "890440",
    "end": "896480"
  },
  {
    "text": "and the other next Advantage is you can do the low Lo s in parel because now you have monthly tables so if you have three",
    "start": "896480",
    "end": "902000"
  },
  {
    "text": "monthly tables you can do three loads in parel uh the last is uh a slight",
    "start": "902000",
    "end": "908680"
  },
  {
    "text": "optimization where you can uh this is pretty much it mimics the partitioning scheme in oracles so you can actually",
    "start": "908680",
    "end": "914639"
  },
  {
    "text": "hit the partition uh or the monthly table right away if you have uh Dynamic wild cards so a table say is abcore",
    "start": "914639",
    "end": "922560"
  },
  {
    "text": "201401 so you can hit the table directly rather than uh going to the Block Level for the pruning",
    "start": "922560",
    "end": "930120"
  },
  {
    "text": "uh so this approach again comes with some cons so one uh uh the first one of",
    "start": "931000",
    "end": "937160"
  },
  {
    "text": "the most important one is mostly from the extract perspective the queries running on it they do not always for all",
    "start": "937160",
    "end": "944560"
  },
  {
    "text": "cases uh do a predicate push and it applies to two cases where you have a",
    "start": "944560",
    "end": "949720"
  },
  {
    "text": "simple predicate push where you have a weight Clause doesn't get always pushed down to the monthly tables the second is",
    "start": "949720",
    "end": "955600"
  },
  {
    "text": "you uh The Joint doesn't get pushed down always when you uh to the to the monthly",
    "start": "955600",
    "end": "960759"
  },
  {
    "text": "table or or whatever underlying table you have underneath the wiew uh the",
    "start": "960759",
    "end": "965800"
  },
  {
    "text": "second one is you can't stretch the limits because it's gabbed at 9990 so which means that uh you cannot go for",
    "start": "965800",
    "end": "973480"
  },
  {
    "text": "arly partitions or rly tables or daily tables you can't do it for a bunch of tables so you have to uh use like use it",
    "start": "973480",
    "end": "982399"
  },
  {
    "text": "for specific use cases uh the last one is uh is not really a con but is mostly",
    "start": "982399",
    "end": "991880"
  },
  {
    "text": "from the operational perspective is a maintenance overhead because there's no single handle on the table so you have",
    "start": "991880",
    "end": "998000"
  },
  {
    "text": "bunch of tables underlying and you have to do all the maintenance activities for all of them which is vacuuming and the",
    "start": "998000",
    "end": "1003399"
  },
  {
    "text": "stat collection and ensuring there's no skew on them so you have to do all that for all the",
    "start": "1003399",
    "end": "1009639"
  },
  {
    "text": "tables so uh the next bunch of slides is going to be around the architectural",
    "start": "1010079",
    "end": "1015759"
  },
  {
    "text": "best practices which we learned uh while doing this project so the most important",
    "start": "1015759",
    "end": "1021639"
  },
  {
    "text": "one is the S3 chunk size so this is where we could actually while ingesting",
    "start": "1021639",
    "end": "1026918"
  },
  {
    "text": "data into red shift we saw 2x performance benefits uh uh so the the best way to ingest data into red shift",
    "start": "1026919",
    "end": "1033600"
  },
  {
    "text": "is y S3 uh especially if you're doing it with big volumes uh what plays a",
    "start": "1033600",
    "end": "1039400"
  },
  {
    "text": "critical role is what's the smallest Quantum file you use while ingesting uh what we have been using is 250 MB this",
    "start": "1039400",
    "end": "1046438"
  },
  {
    "text": "is not something which will fit for all so the guideline here is you can actually go up to 1 GB for one slice",
    "start": "1046439",
    "end": "1055080"
  },
  {
    "text": "when I say one slice it means uh 8 XEL machine has 16 cores so one core is",
    "start": "1055080",
    "end": "1061160"
  },
  {
    "text": "equalent to a slice uh so each slice can take up to one GB but then this also",
    "start": "1061160",
    "end": "1067039"
  },
  {
    "text": "depends upon the type of workloads you have if you're just inserting or merging uh say 5 MB of data then you might not",
    "start": "1067039",
    "end": "1074280"
  },
  {
    "text": "want to have a limit which is 1 GB and it also depends the other guidelines here is you should be ensuring that all",
    "start": "1074280",
    "end": "1080400"
  },
  {
    "text": "your slices are equally working so that the implicit parallelism is actually being leveraged so if",
    "start": "1080400",
    "end": "1087679"
  },
  {
    "text": "um the the the next one is distribution key um this we we distribute on session",
    "start": "1087679",
    "end": "1094080"
  },
  {
    "text": "ID it's an optimal call between the ske it causes and uh what's the usage",
    "start": "1094080",
    "end": "1099799"
  },
  {
    "text": "pattern you have so this again impacts the loads if you are working with merges and all that so that you're not spiking",
    "start": "1099799",
    "end": "1106000"
  },
  {
    "text": "the iio on just one of the nodes or you're not spiking the CPU on one of the nodes so it distributes it",
    "start": "1106000",
    "end": "1113600"
  },
  {
    "text": "equally um so yeah so compression at column level use it uh coming from uh",
    "start": "1113640",
    "end": "1120760"
  },
  {
    "text": "Oracle background this is not really obvious but we could see a performance not performance but space optimizations",
    "start": "1120760",
    "end": "1127159"
  },
  {
    "text": "up to uh to a degree of 3x by doing Coler uh uh compressions uh so so it gets a little",
    "start": "1127159",
    "end": "1135480"
  },
  {
    "text": "tricky because you have a lot of options there uh for the column level uh compression so what we use is we load",
    "start": "1135480",
    "end": "1141440"
  },
  {
    "text": "the data into a table uh as it is and then we run analyze compression on top of it and and red shift this tool gives",
    "start": "1141440",
    "end": "1147559"
  },
  {
    "text": "you the suggestions for the best compression scheme to use the next uh option you have is as you load the data",
    "start": "1147559",
    "end": "1153039"
  },
  {
    "text": "from M3 uh using the copy command you can there's a switch there for uh called",
    "start": "1153039",
    "end": "1158240"
  },
  {
    "text": "comp comp upate which will implicitly give you the best compression scheme so uh we don't really use that because that",
    "start": "1158240",
    "end": "1165200"
  },
  {
    "text": "is fast but it's not optimal it's does a random sample on the data uh so uh",
    "start": "1165200",
    "end": "1170720"
  },
  {
    "text": "analyst compression is much more intensive takes more time but it it is something which you should be using for bigger tables um collection of stats uh",
    "start": "1170720",
    "end": "1180360"
  },
  {
    "text": "so this is this is an interesting one this was a discovery we did so if you have a table with say a column which has",
    "start": "1180360",
    "end": "1187159"
  },
  {
    "text": "10 distinct values or let's say nine distinct values what red sh does in the back end is it stores uh uh the count of",
    "start": "1187159",
    "end": "1194320"
  },
  {
    "text": "these values uh uh like how many values you have for each of these n and this is",
    "start": "1194320",
    "end": "1200679"
  },
  {
    "text": "uh really critical from the performance as perspective because everything is",
    "start": "1200679",
    "end": "1206679"
  },
  {
    "text": "driven from cardinality backwards you have the execution plans which uh the",
    "start": "1206679",
    "end": "1211760"
  },
  {
    "text": "type of join or the sequence of join is backwards from the cardinality which comes up so if you have nine columns and",
    "start": "1211760",
    "end": "1218400"
  },
  {
    "text": "insert the 10th value and you don't run the stats as soon as you load what happens is it assumes the cardinality to",
    "start": "1218400",
    "end": "1224400"
  },
  {
    "text": "be one and that pretty much the whole execution plan gets messed up so uh so",
    "start": "1224400",
    "end": "1230960"
  },
  {
    "text": "you should be actually if you have a column which is having cardinality less than 10 you should actually be",
    "start": "1230960",
    "end": "1236480"
  },
  {
    "text": "collecting stats as soon as you load them for for the other tabl you can actually go in for collecting stats at",
    "start": "1236480",
    "end": "1243320"
  },
  {
    "text": "table level once in a week and for columns so you can collect in red shift you can collect stats at the column",
    "start": "1243320",
    "end": "1249400"
  },
  {
    "text": "level which is much more cheaper it's almost 50x cheaper so if it takes an hour or so to load the uh to collect",
    "start": "1249400",
    "end": "1255919"
  },
  {
    "text": "stats at the table level at column level you can collect an in a minute or so so and which is much more effective uh",
    "start": "1255919",
    "end": "1262000"
  },
  {
    "text": "especially from the quiring perspective so so we do collect stats at column level much more often which is three",
    "start": "1262000",
    "end": "1267360"
  },
  {
    "text": "times in a week and at a table level which is much more costlier once in a week um next is I think Eric touched",
    "start": "1267360",
    "end": "1274360"
  },
  {
    "text": "upon a bit uh uh how do we ensure that operationally these clusters are stable",
    "start": "1274360",
    "end": "1279600"
  },
  {
    "text": "and performing optimally which is uh we we do do that by first up in production",
    "start": "1279600",
    "end": "1284880"
  },
  {
    "text": "we have two clusters which not only ensures a contingency but also allows for load balancing uh and we we",
    "start": "1284880",
    "end": "1291760"
  },
  {
    "text": "segregate that from the test cluster which is where people can test off their stuff and based on imperative data we",
    "start": "1291760",
    "end": "1299080"
  },
  {
    "text": "qualify that job to cut over from test to production so this ensures that query is not actually harming rest of the",
    "start": "1299080",
    "end": "1306200"
  },
  {
    "text": "cluster um the second is more of a reactive approach we use which is we",
    "start": "1306200",
    "end": "1311600"
  },
  {
    "text": "have Reapers which are cbed upon run times and if it goes beyond a point uh Beyond a specific runtime limit it gets",
    "start": "1311600",
    "end": "1318320"
  },
  {
    "text": "kill off and the user is intimated around",
    "start": "1318320",
    "end": "1322880"
  },
  {
    "text": "that so uh this is a critical one this uh one of the important things in",
    "start": "1323559",
    "end": "1329360"
  },
  {
    "text": "warehousing is how do you ensure your data quality is intact especially with Cloud because there lot of moving Parts",
    "start": "1329360",
    "end": "1335400"
  },
  {
    "text": "here you have S3 you have Oracle the in the prodad network and you have red shift and you have the network in the",
    "start": "1335400",
    "end": "1341240"
  },
  {
    "text": "middle uh so what we do is uh so while you're doing the ingestion or while",
    "start": "1341240",
    "end": "1347159"
  },
  {
    "text": "you're taking the data out which is the the unload inje is the copy there is a function you can call which gives you",
    "start": "1347159",
    "end": "1352360"
  },
  {
    "text": "the row count which got inserted or got pulled out so you can actually store into metadata and tally that across this",
    "start": "1352360",
    "end": "1358840"
  },
  {
    "text": "ensures your data what got pulled out is the same as what got inserted or got merged um so uh one of the learnings we",
    "start": "1358840",
    "end": "1367120"
  },
  {
    "text": "had there was we were using a data dictionary table metadata table called unload log which is uh not really an",
    "start": "1367120",
    "end": "1374960"
  },
  {
    "text": "authentic source to pick up that data it's not really credible so so we moved away from that and started so we we went",
    "start": "1374960",
    "end": "1381640"
  },
  {
    "text": "through these function calls uh so this table is more for retrospective analysis than for operations um next is uh the",
    "start": "1381640",
    "end": "1390520"
  },
  {
    "text": "column level security so this one is in Oracle you have the luxury of having uh",
    "start": "1390520",
    "end": "1396159"
  },
  {
    "text": "security at the column level uh in red shift you you can have security at the table level but not at the column level",
    "start": "1396159",
    "end": "1402320"
  },
  {
    "text": "and you don't have UDF support so you can't actually encrypt those columns so the way so you you do have support for",
    "start": "1402320",
    "end": "1409480"
  },
  {
    "text": "shaan and md5 which are more of hashing functions you can use them compositely but they they can afford you at Max a",
    "start": "1409480",
    "end": "1416240"
  },
  {
    "text": "week algorithm for compression for for encryption so what we do is we we actually have a separate schema which is",
    "start": "1416240",
    "end": "1423799"
  },
  {
    "text": "uh secured and which has the raw column as well as the encrypted column and the encrypted column is the one which gets",
    "start": "1423799",
    "end": "1429400"
  },
  {
    "text": "stored in all the fact tables and dimensions so you can actually look up the right people uh get access to this",
    "start": "1429400",
    "end": "1435120"
  },
  {
    "text": "secure schema and they can look up the stuff um so been using so Ware and Care are",
    "start": "1435120",
    "end": "1443679"
  },
  {
    "text": "not exactly the same as they are in other databases uh where is limited by",
    "start": "1443679",
    "end": "1449159"
  },
  {
    "text": "the number of bytes so it is a single bite and the car is uh the W car is",
    "start": "1449159",
    "end": "1456240"
  },
  {
    "text": "single bite and W car is multi bite uh so if you go in for care you and if and",
    "start": "1456240",
    "end": "1461919"
  },
  {
    "text": "you if you have utf8 characters coming in it's going to pretty much fail so you have to be really uh sure about it so we",
    "start": "1461919",
    "end": "1469559"
  },
  {
    "text": "actually suggest not using it uh it's not worth the performance benefits it gives you it is low on storage but",
    "start": "1469559",
    "end": "1475600"
  },
  {
    "text": "storage has never been a concern with r for us but if and if you are going with a care and you move to V care it's not",
    "start": "1475600",
    "end": "1482760"
  },
  {
    "text": "as simple as alter command so you have to actually rebuild the whole table so it's much more cheaper cheaper to take",
    "start": "1482760",
    "end": "1488240"
  },
  {
    "text": "the right decision call at the beginning um the next is uh the quity",
    "start": "1488240",
    "end": "1495200"
  },
  {
    "text": "level best practices so U uh so these practices are backwards from few of the",
    "start": "1495200",
    "end": "1500600"
  },
  {
    "text": "limitations with red shift has we pretty much saw that we'll cover those in the next slides though but that there's",
    "start": "1500600",
    "end": "1506520"
  },
  {
    "text": "pretty much no query which we could find which was not tunable but the approaches were slightly different here for this",
    "start": "1506520",
    "end": "1513240"
  },
  {
    "text": "architecture so so when you have complex sequels you have sequels which have more than say 10 joins and joins happening on",
    "start": "1513240",
    "end": "1520279"
  },
  {
    "text": "functions uh what happens is again the cardinality gets tossed around and because of that you have data being",
    "start": "1520279",
    "end": "1526039"
  },
  {
    "text": "broadcasted which should have been uh joined locally itself or the order of join is all messed up so the way to work",
    "start": "1526039",
    "end": "1533320"
  },
  {
    "text": "around this is if you have a joint between tables A and C you actually join A and B and load into a temp table this",
    "start": "1533320",
    "end": "1539120"
  },
  {
    "text": "ensures that sequence is intact and then carry that forward so the right cardinality is cascaded all the way",
    "start": "1539120",
    "end": "1544679"
  },
  {
    "text": "through um so this also fixes the problem in case you're using functions for joins at times we we could we were",
    "start": "1544679",
    "end": "1551440"
  },
  {
    "text": "observing that if you joining on a function the net cardinality after the join is really not what you expect it to",
    "start": "1551440",
    "end": "1557200"
  },
  {
    "text": "be it's really off from the estimate um the next one is the sort keys in the wear Clause so if you're using a sort",
    "start": "1557200",
    "end": "1563840"
  },
  {
    "text": "key you expect the pruning to happen at the Block Level it doesn't really happen if uh you use uh functions on top of it",
    "start": "1563840",
    "end": "1571240"
  },
  {
    "text": "it's pretty much like the way partitions work in in Oracle also so if you're using something as simple as nvl or kisk",
    "start": "1571240",
    "end": "1577399"
  },
  {
    "text": "of uh something which you can break it apart uh you should do that so so they",
    "start": "1577399",
    "end": "1582919"
  },
  {
    "text": "basically if you're moving around stuff from one uh system to another you need to be cautious with these uh uh things",
    "start": "1582919",
    "end": "1591240"
  },
  {
    "text": "the next is the case when statements this was uh so this is a really odd case",
    "start": "1591240",
    "end": "1597640"
  },
  {
    "text": "which doesn't happen often uh so this is when you have a nest big nested case a",
    "start": "1597640",
    "end": "1603279"
  },
  {
    "text": "class in the group by uh we and if you remove this case when the quy starts",
    "start": "1603279",
    "end": "1608799"
  },
  {
    "text": "running really fast so the work around here is again that you can actually have lookup tables which do these lookups for",
    "start": "1608799",
    "end": "1614600"
  },
  {
    "text": "you rather than doing a hard coding in the code",
    "start": "1614600",
    "end": "1620240"
  },
  {
    "text": "so coming to Performance these are some of the generic performance benchmarks which we came to uh if you see the first",
    "start": "1620720",
    "end": "1627000"
  },
  {
    "text": "one is this is the S3 copy uh from uh to into red shift 100 note cluster and if",
    "start": "1627000",
    "end": "1633840"
  },
  {
    "text": "you see there so uh the first section uh the y axis is on Bill the rows in",
    "start": "1633840",
    "end": "1640320"
  },
  {
    "text": "billions so it pretty much scales linearly almost logarithmically where",
    "start": "1640320",
    "end": "1645480"
  },
  {
    "text": "the gradient keeps on decreasing as you bump up to 50 billion row which is pretty much in 10 hours which is awesome",
    "start": "1645480",
    "end": "1653440"
  },
  {
    "text": "performance uh so next thing we did is we we had bunch of queries uh thousand",
    "start": "1654960",
    "end": "1661600"
  },
  {
    "text": "plus queries which were running in Oracle we moved all these into red shift",
    "start": "1661600",
    "end": "1666799"
  },
  {
    "text": "and ran them as it is for most of them which were running for great greater",
    "start": "1666799",
    "end": "1671960"
  },
  {
    "text": "than 5 minutes in Oracle which was where really the value for the buck was uh we could see that red shift was pered",
    "start": "1671960",
    "end": "1678159"
  },
  {
    "text": "pering almost 90% of the time better than Oracle out of the box so this was a",
    "start": "1678159",
    "end": "1683279"
  },
  {
    "text": "key Discovery so mind you we didn't touch the sequels there were no changes we did",
    "start": "1683279",
    "end": "1689679"
  },
  {
    "text": "it was it was as it is quoted and for the shorter queries it was pretty",
    "start": "1689679",
    "end": "1694760"
  },
  {
    "text": "Converse uh we could see that almost for 60% of the cases or thereabouts uh it",
    "start": "1694760",
    "end": "1701480"
  },
  {
    "text": "was running faster but for 40% cases it was running slower it was in fact slower",
    "start": "1701480",
    "end": "1706519"
  },
  {
    "text": "than 15x for a lot of cases which were really odd these were prettyy much the ones which we covered in the query best",
    "start": "1706519",
    "end": "1712519"
  },
  {
    "text": "practices so these queries were well tuned for Oracle not for red",
    "start": "1712519",
    "end": "1718240"
  },
  {
    "text": "shift so uh another important aspect in red shift is how do you ensure your",
    "start": "1718720",
    "end": "1723919"
  },
  {
    "text": "cluster is healthy and the housekeeping practices uh which ensure that so the",
    "start": "1723919",
    "end": "1729240"
  },
  {
    "text": "key one there is the vacuum which is the costliest uh so we we did these tests on",
    "start": "1729240",
    "end": "1735760"
  },
  {
    "text": "8 XL machines uh and we did in 40 note cluster as well as uh the 100 note",
    "start": "1735760",
    "end": "1741600"
  },
  {
    "text": "cluster we could see it growing linearly it took something like 80 minutes uh on",
    "start": "1741600",
    "end": "1747159"
  },
  {
    "text": "the 40 node cluster to 30 minutes so this was for 6 billion row and where 80%",
    "start": "1747159",
    "end": "1752360"
  },
  {
    "text": "data was unsorted uh stat collections were much faster again we can see a linear pattern",
    "start": "1752360",
    "end": "1758600"
  },
  {
    "text": "there uh so provisioning is another thing which you might run into uh uh so",
    "start": "1758600",
    "end": "1764919"
  },
  {
    "text": "snapshots the great thing in red shift is snapshots are all is happening pretty much every half an hour or so so when",
    "start": "1764919",
    "end": "1771559"
  },
  {
    "text": "you're doing explicit provisioning the snapshot bely it's because it's increment until it takes less than half",
    "start": "1771559",
    "end": "1777360"
  },
  {
    "text": "an hour despite the volumes you have so this was pretty much uh this was for almost 600 TB of data and took half an",
    "start": "1777360",
    "end": "1783679"
  },
  {
    "text": "hour to just get the Backup backup out or the snapshot out uh the restore for",
    "start": "1783679",
    "end": "1788720"
  },
  {
    "text": "the 48 node cluster uh took 48 hours for the 100 node cluster I've maybe missed out uh",
    "start": "1788720",
    "end": "1795840"
  },
  {
    "text": "the volumes there it was for uh 700 TB and the resize we did uh from 40 node to",
    "start": "1795840",
    "end": "1802720"
  },
  {
    "text": "100 node took 48 hours that was for a little lesser data which was for uh 400",
    "start": "1802720",
    "end": "1808279"
  },
  {
    "text": "TV of data so how do these benchmarks which",
    "start": "1808279",
    "end": "1815279"
  },
  {
    "text": "are more abstract translate into uh tangible benefits for our customers and",
    "start": "1815279",
    "end": "1821200"
  },
  {
    "text": "uh like so these benefits are mostly in terms of earlier availability of data and faster run times for their queries",
    "start": "1821200",
    "end": "1828600"
  },
  {
    "text": "uh so the one of the popular cases where red shift really shown out was where nothing else was uh working for us we",
    "start": "1828600",
    "end": "1834880"
  },
  {
    "text": "were hitting into scaling limits so Pig was one which we were using for doing a 10 billion joint to 700 million row and",
    "start": "1834880",
    "end": "1842799"
  },
  {
    "text": "what we saw was uh was a very pleasant surprise we could see performance uh run",
    "start": "1842799",
    "end": "1847919"
  },
  {
    "text": "the run times coming from 48 Hours down to 1 hour so that was great uh for Hive",
    "start": "1847919",
    "end": "1855760"
  },
  {
    "text": "we could see that uh but this was much bigger use case where we had 21 billion",
    "start": "1855760",
    "end": "1861279"
  },
  {
    "text": "R joining 10 billion uh we had a 3 days workload coming down to 2 hours so this",
    "start": "1861279",
    "end": "1867200"
  },
  {
    "text": "made our customers extremely happy uh the third one was which we worked on uh",
    "start": "1867200",
    "end": "1874240"
  },
  {
    "text": "mostly which was because most of workloads are actually Oracle oriented is uh is we picked up some eight",
    "start": "1874240",
    "end": "1880720"
  },
  {
    "text": "pipelines and we moved them across redesign them totally for red shift we brought down uh the run times from 90",
    "start": "1880720",
    "end": "1887120"
  },
  {
    "text": "hours to 8 hours which was more than 10x performance and and significantly also",
    "start": "1887120",
    "end": "1893360"
  },
  {
    "text": "it uh improved our code use uh readability because uh the code we write",
    "start": "1893360",
    "end": "1899159"
  },
  {
    "text": "for Oracle because it's not implicitly parallel is uh explicitly made parallel by writing more SQL so our code actually",
    "start": "1899159",
    "end": "1906159"
  },
  {
    "text": "the sqls which we fired got reduced by a factor of three uh in red shift so thank you very much",
    "start": "1906159",
    "end": "1915720"
  },
  {
    "text": "all right thank you we've got about 10 minutes or so for questions we're happy to uh entertain any of you guys have",
    "start": "1919760",
    "end": "1927320"
  },
  {
    "text": "yes sorry",
    "start": "1928559",
    "end": "1932278"
  },
  {
    "text": "yep so house the network lency from where right",
    "start": "1939679",
    "end": "1948159"
  },
  {
    "text": "H okay what's the network lency from like copy from like an EMR cluster to Red shift tomor so uh we were actually",
    "start": "1952360",
    "end": "1957559"
  },
  {
    "text": "not pulling data from EMR clusters to Red shift it was it was mostly from",
    "start": "1957559",
    "end": "1962880"
  },
  {
    "text": "Oracle prod Network to S3 we we didn't see significant latencies because uh it",
    "start": "1962880",
    "end": "1969360"
  },
  {
    "text": "was we were using multi uploads for uh S3 multi uploads to S3 so you can",
    "start": "1969360",
    "end": "1975880"
  },
  {
    "text": "actually go really parallel there depending on how how much you want to go it's multi- threaded",
    "start": "1975880",
    "end": "1980919"
  },
  {
    "text": "there",
    "start": "1980919",
    "end": "1983919"
  },
  {
    "text": "yeah so so just to paraphrase a question so the web logs which could be a par raw",
    "start": "1989240",
    "end": "1995799"
  },
  {
    "text": "you're saying the cleansing of those into much more structured data is that the question so that's that's not",
    "start": "1995799",
    "end": "2002200"
  },
  {
    "text": "something which we do that's an upstream system which does that for us yeah",
    "start": "2002200",
    "end": "2009600"
  },
  {
    "text": "uh you you uh you can do that also there is jdbc support for that but you can't",
    "start": "2017600",
    "end": "2023039"
  },
  {
    "text": "really do that for uh terabytes or even GBS of volumes you can do for bunch of",
    "start": "2023039",
    "end": "2028679"
  },
  {
    "text": "rows uh but it's not something which even red shift",
    "start": "2028679",
    "end": "2033639"
  },
  {
    "text": "suggest so one of the other thing we do is that uh we don't have a single cluster so we have multiple clusters",
    "start": "2035919",
    "end": "2042880"
  },
  {
    "text": "across so the way the ETL workflow works for us is you we do the extract one time",
    "start": "2042880",
    "end": "2049480"
  },
  {
    "text": "and we load it multiple times so S3 is an intermediate which really helps both from archiving perspective as well as",
    "start": "2049480",
    "end": "2055240"
  },
  {
    "text": "enabling the whole workflow do you have a question",
    "start": "2055240",
    "end": "2061039"
  },
  {
    "text": "yep it it so it's it's much easier it's much weaker right so it's it's much easier to figure out so it's it's not",
    "start": "2066320",
    "end": "2072280"
  },
  {
    "text": "really a encryption algorithm it's mostly like at Max it's in hashing algorithm",
    "start": "2072280",
    "end": "2079118"
  },
  {
    "text": "right yeah not really the replica just the raw column and the encrypted column so somebody who wants to look up the",
    "start": "2083240",
    "end": "2089398"
  },
  {
    "text": "actual raw value can look up the value so you have the credit card IDs which are encrypted in the dimension table and",
    "start": "2089399",
    "end": "2095679"
  },
  {
    "text": "then you have lookup table which has just the credit card ID and the encrypted value encrypted and the raw",
    "start": "2095679",
    "end": "2103040"
  },
  {
    "text": "value so you can look it up but only uh",
    "start": "2103040",
    "end": "2108440"
  },
  {
    "text": "yeah so can you repeat that",
    "start": "2113839",
    "end": "2117680"
  },
  {
    "text": "please no we Pur the data no we persist the data in red sh",
    "start": "2119960",
    "end": "2126040"
  },
  {
    "text": "yes we have retentions on tables uh based on uh how popularly they are used",
    "start": "2126040",
    "end": "2133000"
  },
  {
    "text": "and uh what is the demand from the end customers they want to do an analysis across Yi then maybe table is 24 months",
    "start": "2133000",
    "end": "2140320"
  },
  {
    "text": "but most of the tables do not have retentions which are merge merge like updates and insert like order tables",
    "start": "2140320",
    "end": "2146560"
  },
  {
    "text": "shipments they they have all the retention but for other tables we we do have retention it's pretty much",
    "start": "2146560",
    "end": "2152599"
  },
  {
    "text": "backwards from the size of the table so if you have a really big table we don't go in for 10 years of redention so",
    "start": "2152599",
    "end": "2160760"
  },
  {
    "text": "it's so all the data one cool thing here is all the data is already backed up in",
    "start": "2165160",
    "end": "2171039"
  },
  {
    "text": "S3 yes so one of the things this approach allows you is because you're having a separate monthly table right",
    "start": "2172400",
    "end": "2178839"
  },
  {
    "text": "underlying The View you can just drop that table off so that's how the redention works rather than doing a",
    "start": "2178839",
    "end": "2184319"
  },
  {
    "text": "delete command what one thing I'll just add to that a thing we found ourselves getting",
    "start": "2184319",
    "end": "2190079"
  },
  {
    "text": "into and you guys might find this as well um while some of our tables for example web blogs were not the the",
    "start": "2190079",
    "end": "2196560"
  },
  {
    "text": "authority of record so we we do delete those there are other tables that uh we've become the authority right so for",
    "start": "2196560",
    "end": "2203319"
  },
  {
    "text": "example all orders it's it's in the data warehouse because the ordering system deleted data at some point so we became",
    "start": "2203319",
    "end": "2209280"
  },
  {
    "text": "the authority so we never delete that data um we have a couple tables like that so so red sh is nice and that okay",
    "start": "2209280",
    "end": "2216359"
  },
  {
    "text": "we're there forever we can actually have a growth strategy yes",
    "start": "2216359",
    "end": "2223079"
  },
  {
    "text": "yep so even updates follow the same pattern anything you touch uh gets",
    "start": "2232359",
    "end": "2237640"
  },
  {
    "text": "logically deleted the previous row and one more row gets inserted in the back end so your scanning gets slower if you",
    "start": "2237640",
    "end": "2243520"
  },
  {
    "text": "don't don't run a vacuum or don't if you run it up we we do updates yes we do merges",
    "start": "2243520",
    "end": "2249880"
  },
  {
    "text": "merge like merge equivalent of insert and",
    "start": "2249880",
    "end": "2253720"
  },
  {
    "text": "update so what do you mean by parallel load",
    "start": "2259520",
    "end": "2264119"
  },
  {
    "text": "here y",
    "start": "2265079",
    "end": "2269079"
  },
  {
    "text": "so that depends so so the loads for a table they're all sequential because there's a table level lock D Lock which",
    "start": "2278480",
    "end": "2284400"
  },
  {
    "text": "happens you can't do multiple loads on the same table so but what you can do still is you can have different tables",
    "start": "2284400",
    "end": "2290960"
  },
  {
    "text": "and have a view on top of it this enables you to still do the loads in parallel yep yes that's why we use a",
    "start": "2290960",
    "end": "2297000"
  },
  {
    "text": "Time series view there yeah yes yep",
    "start": "2297000",
    "end": "2303720"
  },
  {
    "text": "so uh we are actually there are still some workloads which are running on those the plan is to deprecate those as",
    "start": "2308839",
    "end": "2317960"
  },
  {
    "text": "yeah so one of the key discoveries here was what Hive does in general or Hadoop",
    "start": "2322200",
    "end": "2327520"
  },
  {
    "text": "does in the general is it it does it's not really great at joints because the data is randomly hashed out it is like",
    "start": "2327520",
    "end": "2333920"
  },
  {
    "text": "chunks going randomly right 64 MB or 128 MB here you have conscious hashing of",
    "start": "2333920",
    "end": "2340839"
  },
  {
    "text": "data happening right deterministic hashing so joints can happen locally at",
    "start": "2340839",
    "end": "2346000"
  },
  {
    "text": "which is CL basically computers happening closer to the uh to the storage in in Solutions like Hadoop the",
    "start": "2346000",
    "end": "2352079"
  },
  {
    "text": "data is again pulled out and resorted for the joints so it's much faster for joints in general but if you're playing",
    "start": "2352079",
    "end": "2358640"
  },
  {
    "text": "around with unstructured data which you have to clean up which is already denormalized I think that's where I think EMR excels yeah",
    "start": "2358640",
    "end": "2367520"
  },
  {
    "text": "uh yep",
    "start": "2367520",
    "end": "2370079"
  },
  {
    "text": "can yes that's that's one of the key things uh uh so question yeah repeat the",
    "start": "2373880",
    "end": "2379960"
  },
  {
    "text": "question so the the question is that uh is a denormalization a better thing in red shift right so I think yeah so the",
    "start": "2379960",
    "end": "2387079"
  },
  {
    "text": "answer to that is yes it is one of the best things red shift afford us because",
    "start": "2387079",
    "end": "2392319"
  },
  {
    "text": "in uh systems like Oracle you can't do uh a really wide table because that leads to Road chaining if you're doing",
    "start": "2392319",
    "end": "2398319"
  },
  {
    "text": "updates a lot of time so I believe there is but that's a very",
    "start": "2398319",
    "end": "2404960"
  },
  {
    "text": "big limit that's a very high number there but there is a number but it you can actually go in for really wide",
    "start": "2404960",
    "end": "2410079"
  },
  {
    "text": "tables with the colum compression and uh so performance is really great that way",
    "start": "2410079",
    "end": "2415160"
  },
  {
    "text": "its usability is better",
    "start": "2415160",
    "end": "2418599"
  },
  {
    "text": "also so just as general rule of thumb uh we we couldn't find we could find very",
    "start": "2422680",
    "end": "2429720"
  },
  {
    "text": "few like the slide I showed you previously almost 90% of the stuff out of the box runs fast in retch and for",
    "start": "2429720",
    "end": "2437319"
  },
  {
    "text": "the rest of the stuff if you actually do the tweaks which is refactoring the code you can make it run as fast or faster so",
    "start": "2437319",
    "end": "2444200"
  },
  {
    "text": "there was not nothing specific we could find which was astonishing we pushed it really hard where we couldn't find a",
    "start": "2444200",
    "end": "2450760"
  },
  {
    "text": "workaround to solve that problem it's mostly faster",
    "start": "2450760",
    "end": "2457240"
  },
  {
    "text": "we do have so basically because we're coming from an oracle background we don't really have white columns we",
    "start": "2474000",
    "end": "2479680"
  },
  {
    "text": "restricted to less than thousand one24 uh characters because red Oracle",
    "start": "2479680",
    "end": "2485359"
  },
  {
    "text": "starts bothering you because of the road tring so we really didn't have a lot of use cases which had wider like say blobs",
    "start": "2485359",
    "end": "2493119"
  },
  {
    "text": "or clubs sort of things we didn't have those use cases yeah just to harp on that we'll a second for for when we're",
    "start": "2493119",
    "end": "2499520"
  },
  {
    "text": "looking at Oracle performance versus um uh red shift and this is pretty much",
    "start": "2499520",
    "end": "2504599"
  },
  {
    "text": "true of Oracle versus any column or data store um so Oracle excels uh when you're basically doing a",
    "start": "2504599",
    "end": "2511800"
  },
  {
    "text": "lookup right it it's got tons of indices they do that really really well so when you're looking up individual values or",
    "start": "2511800",
    "end": "2517240"
  },
  {
    "text": "you're scanning a small partition it's great um the the nice thing about Oracle",
    "start": "2517240",
    "end": "2522960"
  },
  {
    "text": "is that it's fairly flexible it's hard to do a query if you're doing lookups that will perform poorly but none of",
    "start": "2522960",
    "end": "2528720"
  },
  {
    "text": "them are blindingly fast so this is where we saw the fast- Performing queries were great and when we saw them",
    "start": "2528720",
    "end": "2534119"
  },
  {
    "text": "performing poorly in red shift is because the either the default distribution key or the sort Keys uh",
    "start": "2534119",
    "end": "2540480"
  },
  {
    "text": "weren't sorted out in a way that would optimize for the query so you can rewrite your table so that your query",
    "start": "2540480",
    "end": "2547480"
  },
  {
    "text": "will be fast right but the point we make is if you're doing a migration from Oracle to to Red shift your long running",
    "start": "2547480",
    "end": "2552839"
  },
  {
    "text": "things will generally work fine out of the box you don't need to do anything but the hord of short running queries you got to refactor that you're not",
    "start": "2552839",
    "end": "2558640"
  },
  {
    "text": "going to get a whole lot of gain because you're going from five minutes to four minutes so",
    "start": "2558640",
    "end": "2565960"
  },
  {
    "text": "yep so uh watch me tap dance uh we don't have that great insight into to the feature set I know they finally released",
    "start": "2572599",
    "end": "2579160"
  },
  {
    "text": "median we were really happy about that uh we've been yelling at them for quite some time uh We've provided them a list",
    "start": "2579160",
    "end": "2585680"
  },
  {
    "text": "of all the missing support uh from from actually from Oracle plsql uh that our customers use uh basically an ni",
    "start": "2585680",
    "end": "2592640"
  },
  {
    "text": "histogram order of what our customers use most common they've they've done a decent job knocking that out so we've",
    "start": "2592640",
    "end": "2597720"
  },
  {
    "text": "got rount we got median we got a bunch of the uh re Expressions ones but I",
    "start": "2597720",
    "end": "2602880"
  },
  {
    "text": "don't know what their road map is do you know anything about multiple",
    "start": "2602880",
    "end": "2608760"
  },
  {
    "text": "no yep envir based the amount of data you",
    "start": "2609559",
    "end": "2618079"
  },
  {
    "text": "had based performance you talk right so the qu the question is how",
    "start": "2618079",
    "end": "2625280"
  },
  {
    "text": "did we pick 100 was it based on storage or was it based on performance um uh it was primarily based on storage uh uh",
    "start": "2625280",
    "end": "2632720"
  },
  {
    "text": "roughly speaking for an 8xl you can think of that as 10 terabytes worth of actual us space um and and so we needed",
    "start": "2632720",
    "end": "2639880"
  },
  {
    "text": "roughly a paby we've got uh what 400 terabytes for for the main table we've got a couple other 300 terab tables",
    "start": "2639880",
    "end": "2646160"
  },
  {
    "text": "there um and we want to kind of size so that we're not resizing the cluster all that often um so that that's where we",
    "start": "2646160",
    "end": "2652200"
  },
  {
    "text": "went for that I think we have time for like two more questions and then we're g to turn",
    "start": "2652200",
    "end": "2658240"
  },
  {
    "text": "off the Bell yep yep",
    "start": "2658240",
    "end": "2663040"
  },
  {
    "text": "we so so the question is how does it work to have two or more production clusters uh the way we work is we have",
    "start": "2672240",
    "end": "2677640"
  },
  {
    "text": "files called load files and we apply them equally to both or to three or four or five or you know for or class up to",
    "start": "2677640",
    "end": "2685760"
  },
  {
    "text": "10 yes it goes to one of them one query goes to just one of the Clusters but they get distributed",
    "start": "2687079",
    "end": "2694040"
  },
  {
    "text": "equally there was two question yep",
    "start": "2694040",
    "end": "2698839"
  },
  {
    "text": "so the question is how do we use red shifter for analyzing disc sources um uh well we we do that right now and really",
    "start": "2707079",
    "end": "2713400"
  },
  {
    "text": "it's a matter of figuring out how you're going to join the data up front what the common keys are and then getting it into",
    "start": "2713400",
    "end": "2719400"
  },
  {
    "text": "the the right relational structure to do that um so again we have an Amazon we have uh thing called an as which is the",
    "start": "2719400",
    "end": "2726119"
  },
  {
    "text": "product ID for anything you buy so typically click we'll have an as and then anything that when you make an order or shipment that as's tag We join",
    "start": "2726119",
    "end": "2733559"
  },
  {
    "text": "uh through that way all right uh I'll tell you what we're we're out of time but we'll stick",
    "start": "2733559",
    "end": "2738760"
  },
  {
    "text": "around here afterwards uh thank you for attending appreciate it thank you",
    "start": "2738760",
    "end": "2744640"
  }
]