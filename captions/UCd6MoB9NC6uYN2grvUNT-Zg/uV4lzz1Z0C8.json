[
  {
    "text": "hey everyone my name is Alex I'm a engineer on instacart catalog team a",
    "start": "439",
    "end": "8000"
  },
  {
    "text": "little more than a year ago in cigars catalog was struggling to keep up with its growth I move forward some slides",
    "start": "8000",
    "end": "17660"
  },
  {
    "text": "we're starting to keep up with our growth every release we were making was",
    "start": "17660",
    "end": "23130"
  },
  {
    "text": "patching one of the dozens of critical bottlenecks in our data pipeline that had been assembled in haste and it",
    "start": "23130",
    "end": "28920"
  },
  {
    "text": "seemed like we couldn't release fast enough every week there are multiple",
    "start": "28920",
    "end": "34380"
  },
  {
    "text": "alarms going off that were indicating that we are they're hitting up against thresholds or that had entered our",
    "start": "34380",
    "end": "43710"
  },
  {
    "text": "system that was bad and that we needed to address it for some reason or another so I don't think anyone would be",
    "start": "43710",
    "end": "49770"
  },
  {
    "text": "surprised by this instrument was growing very rapidly after all and like many startups our staffing was struggling to",
    "start": "49770",
    "end": "55590"
  },
  {
    "text": "keep up with that growth but I'm here today to talk about how we recovered from that situation so today instacart",
    "start": "55590",
    "end": "63000"
  },
  {
    "text": "catalog covers a substantial fraction of North America's grocery stores and we're now poised so that we can grow",
    "start": "63000",
    "end": "69119"
  },
  {
    "text": "significantly beyond that we're also now confident that we can deal with any sort",
    "start": "69119",
    "end": "74610"
  },
  {
    "text": "of bad data that enters into our system so we got to this position by adopting",
    "start": "74610",
    "end": "80009"
  },
  {
    "text": "the same sort of principles that you find in functional programming maxime beaugeois a have lived mics call",
    "start": "80009",
    "end": "86549"
  },
  {
    "text": "this functional data pipeline or engineering which i think is a great",
    "start": "86549",
    "end": "91710"
  },
  {
    "text": "term we're basically talking about taking the same concepts that we have in functional programming that give us",
    "start": "91710",
    "end": "99060"
  },
  {
    "text": "confidence in the correctness of our systems or applying them to large volumes of data well I like to call the",
    "start": "99060",
    "end": "106290"
  },
  {
    "text": "resulting systems functional data pipelines so the stock will first be",
    "start": "106290",
    "end": "111420"
  },
  {
    "text": "going over the history of instruct cards catalog to give you an idea of the",
    "start": "111420",
    "end": "116899"
  },
  {
    "text": "context and then I'll be diving into what I mean by functional data pipelines",
    "start": "116899",
    "end": "123090"
  },
  {
    "text": "what they are why you want one and how you'll know when you have one and then",
    "start": "123090",
    "end": "130140"
  },
  {
    "text": "the meat of this talk will be going through the tools and technique that our catalog is used to make our",
    "start": "130140",
    "end": "135390"
  },
  {
    "text": "data pipelines functional the idea being that you'll be able to hopefully apply",
    "start": "135390",
    "end": "143310"
  },
  {
    "text": "some of these techniques to to your systems since we all have bad data that",
    "start": "143310",
    "end": "149370"
  },
  {
    "text": "we need to deal with and a lot of these concepts are equally applicable no",
    "start": "149370",
    "end": "154380"
  },
  {
    "text": "matter what size system you have so for",
    "start": "154380",
    "end": "159870"
  },
  {
    "text": "as common as they are groceries are actually quite difficult to deal with",
    "start": "159870",
    "end": "164930"
  },
  {
    "text": "groceries update very frequently their stock turns over very rapidly compared to most goods and they also go on sale",
    "start": "164930",
    "end": "171930"
  },
  {
    "text": "very frequently so that means that if you're tracking groceries your system has a lot of updates associated with it",
    "start": "171930",
    "end": "177930"
  },
  {
    "text": "and additionally instacart in particular uses a lot of different sources of data",
    "start": "177930",
    "end": "184560"
  },
  {
    "text": "that we combine together to give you that storefront experience that we have so this is data from our own shoppers",
    "start": "184560",
    "end": "191220"
  },
  {
    "text": "it's data from the retailers that are stalking these Goods it's data from the",
    "start": "191220",
    "end": "196740"
  },
  {
    "text": "consumer packaged goods companies that are manufacturing these Goods and we use",
    "start": "196740",
    "end": "202170"
  },
  {
    "text": "some other sources of data as well but tying all of these sources to dig it up data together is difficult because",
    "start": "202170",
    "end": "208920"
  },
  {
    "text": "groceries are hard to ID if you're very lucky a grocery will have a UPC so a",
    "start": "208920",
    "end": "215280"
  },
  {
    "text": "universal product code that consistently refers to the same product but you don't",
    "start": "215280",
    "end": "222330"
  },
  {
    "text": "always get that sometimes manufacturers or retailers will reuse the same barcode effectively changing the meaning that",
    "start": "222330",
    "end": "228209"
  },
  {
    "text": "product and other types of groceries like fruit and vegetables and bulk goods",
    "start": "228209",
    "end": "234150"
  },
  {
    "text": "will you as pl use which vary in meaning from place to place additionally Instagram in particular is",
    "start": "234150",
    "end": "241950"
  },
  {
    "text": "partnered with hundreds of different retailers so we because of that have had",
    "start": "241950",
    "end": "248130"
  },
  {
    "text": "to integrate our systems within Devi's ins of different inventory systems each of which have their own unique quirks so",
    "start": "248130",
    "end": "256200"
  },
  {
    "text": "we've had to deal with all of these challenges of groceries at a scale that most companies don't have an opportunity",
    "start": "256200",
    "end": "262140"
  },
  {
    "text": "to operate at currently we're handling about five billion points of data a day entering",
    "start": "262140",
    "end": "269729"
  },
  {
    "text": "our system which for some context is as far as I can tell about ten times more tweets than Twitter handles every day",
    "start": "269729",
    "end": "277460"
  },
  {
    "text": "despite its current size in cigars catalog has a fairly humble beginning",
    "start": "278600",
    "end": "283770"
  },
  {
    "text": "like basically all of instacart our catalog started as a rails app or",
    "start": "283770",
    "end": "289320"
  },
  {
    "text": "catalog was just a few models within that rails app and one of the first big catalog initiatives we had was to buy",
    "start": "289320",
    "end": "295500"
  },
  {
    "text": "every item within a Trader Joe's so that we could photograph it so that we could enter that data into our system which",
    "start": "295500",
    "end": "302100"
  },
  {
    "text": "was clearly not very scalable of way of growing but to hit our first million items in our catalog which is about what",
    "start": "302100",
    "end": "309660"
  },
  {
    "text": "you find in 30 grocery stores to get to that point and for the next few million items we relied basically entirely on",
    "start": "309660",
    "end": "316830"
  },
  {
    "text": "manual data entry our catalog entered a new era of growth when we developed a",
    "start": "316830",
    "end": "323130"
  },
  {
    "text": "data entry pipeline to allow retailers to send us their data directly but this",
    "start": "323130",
    "end": "329280"
  },
  {
    "text": "pipeline was actually pretty quickly overwhelmed by the volume of data that was coming into our system since we were",
    "start": "329280",
    "end": "336090"
  },
  {
    "text": "very quickly adding new partners to our platform at the time with some",
    "start": "336090",
    "end": "341419"
  },
  {
    "text": "difficulty however we did scale that system by over by around a hundred times",
    "start": "341419",
    "end": "347610"
  },
  {
    "text": "to the volume that it was originally able to handle but the result was still",
    "start": "347610",
    "end": "352650"
  },
  {
    "text": "a system that kept us up at night not only was scaling very difficult but fixing data that had entered our system",
    "start": "352650",
    "end": "359310"
  },
  {
    "text": "that was bad had become the full-time job of several engineers so earlier this",
    "start": "359310",
    "end": "367169"
  },
  {
    "text": "year we decided to take a step back and ask ourselves what a catalog that could handle another hunter timescale would",
    "start": "367169",
    "end": "373139"
  },
  {
    "text": "look like and much more than being able to handle scale we were skiing ourselves",
    "start": "373139",
    "end": "379229"
  },
  {
    "text": "what a what a system that would give us confidence in our correctness would look like we wanted something where we could",
    "start": "379229",
    "end": "386909"
  },
  {
    "text": "be sure that our data was good that we would have the tools to handle any data that was bad and as well as the tools to",
    "start": "386909",
    "end": "394380"
  },
  {
    "text": "improve our existing data and in addition to that we wanted a system that was more flexible than what",
    "start": "394380",
    "end": "400440"
  },
  {
    "text": "we had ended up with after years of incremental improvements to our catalog we ended up with a system that was",
    "start": "400440",
    "end": "407820"
  },
  {
    "text": "fairly brittle so we wanted something that could allow us to move more freely",
    "start": "407820",
    "end": "413340"
  },
  {
    "text": "and iterate faster to better serve our business so from these design goals we created a",
    "start": "413340",
    "end": "420000"
  },
  {
    "text": "design that is based on these functional principles that I've mentioned but in",
    "start": "420000",
    "end": "425280"
  },
  {
    "text": "order to reach this design we knew that we needed a infrastructure that would be able to one handle a large volume of",
    "start": "425280",
    "end": "432330"
  },
  {
    "text": "data to store a large volume of data as well as be able to operate or compute",
    "start": "432330",
    "end": "439020"
  },
  {
    "text": "over a large volume of that data since as we had found in our system there's",
    "start": "439020",
    "end": "445800"
  },
  {
    "text": "quite a big difference between just storing data and having a system that is prepared to operate it and query over",
    "start": "445800",
    "end": "452790"
  },
  {
    "text": "that and have that readily available if that system hasn't been built specifically with that ability in mind",
    "start": "452790",
    "end": "459870"
  },
  {
    "text": "then that data tends to be have lost and it takes a lot of engineering efforts to pull that out so our first design was",
    "start": "459870",
    "end": "468450"
  },
  {
    "text": "based around spark which is kind of the normal thing that you might reach for when you have these criteria spark being",
    "start": "468450",
    "end": "474480"
  },
  {
    "text": "a general purpose platform for scalable computing over distributed data sets but",
    "start": "474480",
    "end": "482130"
  },
  {
    "text": "we realized when we were evaluating our other options that snowflake actually checked a lot of the boxes that we were",
    "start": "482130",
    "end": "489420"
  },
  {
    "text": "looking for so similar to spark snowflake separates its storage from its",
    "start": "489420",
    "end": "497520"
  },
  {
    "text": "compute allows you to store a large volume of data on a cheap storage medium and the compute is separate so you can",
    "start": "497520",
    "end": "505860"
  },
  {
    "text": "scale that up and down it out basically however you want which made us confident that we weren't going to run into any",
    "start": "505860",
    "end": "511560"
  },
  {
    "text": "bottlenecks with that system further snowflake everything is managed through",
    "start": "511560",
    "end": "516900"
  },
  {
    "text": "a uniform sequel interface which turned out to be really the right level of abstraction for our application we found",
    "start": "516900",
    "end": "525480"
  },
  {
    "text": "that in recreating our catalog almost purely in sequel where",
    "start": "525480",
    "end": "531089"
  },
  {
    "text": "to reduce our code base by over 60% snowflake is also fully managed so it's",
    "start": "531089",
    "end": "537240"
  },
  {
    "text": "quite easy to get up and running as well as to start experimenting with it and",
    "start": "537240",
    "end": "542730"
  },
  {
    "text": "it's got some other nice features like transactions and variant data types that you don't necessarily find with other",
    "start": "542730",
    "end": "549569"
  },
  {
    "text": "databases of a similar type and all told it was just a lower level of effort than",
    "start": "549569",
    "end": "555029"
  },
  {
    "text": "the the other alternatives that we were evaluating snowflake though of course",
    "start": "555029",
    "end": "562230"
  },
  {
    "text": "doesn't solve everything for us we still need a good design and schema in order to build a useful system but it let us",
    "start": "562230",
    "end": "570990"
  },
  {
    "text": "focus on our domain specific problems precisely so let's get talking about the",
    "start": "570990",
    "end": "579829"
  },
  {
    "text": "the philosophy behind that design that I've been talking about so our design is",
    "start": "579829",
    "end": "588029"
  },
  {
    "text": "based around these functional programming principles as I've said but applied to data pipelines the idea being",
    "start": "588029",
    "end": "594149"
  },
  {
    "text": "that we want to have a full understanding of how we got to any piece of data such that any piece of data in",
    "start": "594149",
    "end": "599220"
  },
  {
    "text": "your system is fully reproducible when you have systems that are built to be",
    "start": "599220",
    "end": "604259"
  },
  {
    "text": "both transparent and deterministic then the result is a system that is easy to",
    "start": "604259",
    "end": "609839"
  },
  {
    "text": "understand as well as more adaptable to to the future although such systems let",
    "start": "609839",
    "end": "617370"
  },
  {
    "text": "you recover from any bad States that you can get into but they also allow you to move in to any potential future states",
    "start": "617370",
    "end": "624720"
  },
  {
    "text": "that your input data might allow for so in practice functional data pipelines",
    "start": "624720",
    "end": "630240"
  },
  {
    "text": "manifest as a series of append-only inputs they're append-only so that for a",
    "start": "630240",
    "end": "635490"
  },
  {
    "text": "given point of time you can consider your input immutable and these inputs are passed through a set of discrete",
    "start": "635490",
    "end": "641610"
  },
  {
    "text": "transformations in order to arrive at an output each of these transformations should be basically a pure functional",
    "start": "641610",
    "end": "648959"
  },
  {
    "text": "unit so isolated and free from the sort of side effects that could make them hard to reason about and impossible to",
    "start": "648959",
    "end": "655379"
  },
  {
    "text": "reproduce so to gain a bit more understanding about what I mean by",
    "start": "655379",
    "end": "660929"
  },
  {
    "text": "functional systems we can look at the kind of most simple no system possible which is just an",
    "start": "660929",
    "end": "667110"
  },
  {
    "text": "input pass through a function of that input that arrives at an output in functional programming we build our",
    "start": "667110",
    "end": "674310"
  },
  {
    "text": "systems out of units like this to avoid changing state and mutating data",
    "start": "674310",
    "end": "679850"
  },
  {
    "text": "basically idea is that if a function is pure then any time we pass the same",
    "start": "679850",
    "end": "684870"
  },
  {
    "text": "input to our function we will arrive at that same output and building functions",
    "start": "684870",
    "end": "692550"
  },
  {
    "text": "in this fashion in this fashion make them easy to reason about and in turn composing our systems out of these",
    "start": "692550",
    "end": "699120"
  },
  {
    "text": "functional building blocks make the systems easier to reason about so for data pipelines we get an idea",
    "start": "699120",
    "end": "707250"
  },
  {
    "text": "from this the sort of building blocks that we're talking about we have raw immutable data that has passed into our",
    "start": "707250",
    "end": "714029"
  },
  {
    "text": "system that are passed through these transformations in order to arrived at derived outputs and we treat all of",
    "start": "714029",
    "end": "721649"
  },
  {
    "text": "these things as first-class concepts but of course in data pipelines our data is",
    "start": "721649",
    "end": "727500"
  },
  {
    "text": "also changing over time so if we want to apply a example to this set of inputs",
    "start": "727500",
    "end": "734899"
  },
  {
    "text": "for instacart we could be tracking the price of a pound of bananas as it comes into our catalog over time and we're",
    "start": "734899",
    "end": "741600"
  },
  {
    "text": "passing this price through some transformation this transformation could also be changing over time in order to",
    "start": "741600",
    "end": "746819"
  },
  {
    "text": "arrive at an output looking at this sort of history of functional values we can",
    "start": "746819",
    "end": "754319"
  },
  {
    "text": "see that the the minimum that we'd need to record in order to arrive at a full history of the system we would need to",
    "start": "754319",
    "end": "761069"
  },
  {
    "text": "record a full history of our inputs as well as a record of which transformations were applied to each",
    "start": "761069",
    "end": "767579"
  },
  {
    "text": "input and when you have recorded both of these things then all of your y's are fully drivable so why would we want",
    "start": "767579",
    "end": "776130"
  },
  {
    "text": "fully drivable y's well when we have that then it means we can always recover from any bad states that our system",
    "start": "776130",
    "end": "783389"
  },
  {
    "text": "could arrive at and this might sound very basic in principle and it is but it",
    "start": "783389",
    "end": "790290"
  },
  {
    "text": "differs from how a lot of systems are constructed so we're going to take a look at how what happens when",
    "start": "790290",
    "end": "796860"
  },
  {
    "text": "you omit one or both of these pieces of historical tracking so the very worst",
    "start": "796860",
    "end": "803670"
  },
  {
    "text": "case that we get is a system that only has a recent output so this is the sort",
    "start": "803670",
    "end": "811470"
  },
  {
    "text": "of thing that you'll find in crud applications frequently where you have a server running and it will take an input",
    "start": "811470",
    "end": "816510"
  },
  {
    "text": "from somewhere and it will modify that input based on the currently running version of code and then it will save",
    "start": "816510",
    "end": "822570"
  },
  {
    "text": "that that modified data in a database probably overwriting the previous value",
    "start": "822570",
    "end": "827730"
  },
  {
    "text": "that was there so this is pretty bad since if you have a bad input coming",
    "start": "827730",
    "end": "834720"
  },
  {
    "text": "into your system or if you have a bad transformation at any point then there's not much you can do about it you",
    "start": "834720",
    "end": "839970"
  },
  {
    "text": "basically have to wait for a new input to come in and you hope your transformation is good at that point of",
    "start": "839970",
    "end": "845460"
  },
  {
    "text": "time and that's about all you can do so in order to try to improve the situation",
    "start": "845460",
    "end": "852540"
  },
  {
    "text": "sometimes we'll add audits to our systems which look like a tracking the",
    "start": "852540",
    "end": "857700"
  },
  {
    "text": "history of outputs from our system which is better than nothing you can at least roll back to a previous output state of",
    "start": "857700",
    "end": "863880"
  },
  {
    "text": "the system but it's still not great particularly if you have a bad transformation in your system then if",
    "start": "863880",
    "end": "870330"
  },
  {
    "text": "you're trying to recover you need to rollback but that means you're throwing out a good input for no particular for a bad",
    "start": "870330",
    "end": "877710"
  },
  {
    "text": "reason really so in order to come by that we can either save our last input",
    "start": "877710",
    "end": "883110"
  },
  {
    "text": "or we could save a full history of inputs as well as the history of our outputs which looks quite a bit better",
    "start": "883110",
    "end": "888870"
  },
  {
    "text": "at least now we are confident that we can move through our history of inputs",
    "start": "888870",
    "end": "894540"
  },
  {
    "text": "and outputs and this was actually the state of catalog for quite some time but",
    "start": "894540",
    "end": "899850"
  },
  {
    "text": "it's still not great so for an example going back to our tracking the price of",
    "start": "899850",
    "end": "904980"
  },
  {
    "text": "bananas in our system what happens if we need to figure out what happened when we have a bad transformation say this",
    "start": "904980",
    "end": "912120"
  },
  {
    "text": "transformation has been bad for a week we've been arriving at the wrong price",
    "start": "912120",
    "end": "918780"
  },
  {
    "text": "for our bananas and we want to go back and refund anybody who overpaid for",
    "start": "918780",
    "end": "924150"
  },
  {
    "text": "their banaz so the way that we do that in a system that's organized like this is to",
    "start": "924150",
    "end": "930120"
  },
  {
    "text": "take our inputs pass them through an amended transformation in order to arrive at a new set of outputs and",
    "start": "930120",
    "end": "936779"
  },
  {
    "text": "compare that new set of outputs to our actual set of outputs and see where they converge or diverge to see where our",
    "start": "936779",
    "end": "943020"
  },
  {
    "text": "problem started which is not maybe the worst for the simple example but it",
    "start": "943020",
    "end": "950570"
  },
  {
    "text": "systems get a lot more complicated than this and as they get more complicated data becomes more interdependent you end",
    "start": "950570",
    "end": "957210"
  },
  {
    "text": "up having multiple inputs to transformations and figuring these things out can be very difficult and",
    "start": "957210",
    "end": "962880"
  },
  {
    "text": "generally when systems are constructed that look like this they tend not to have automated ways for moving between",
    "start": "962880",
    "end": "970110"
  },
  {
    "text": "inputs to outputs since there is this missing history of what was applied so",
    "start": "970110",
    "end": "975930"
  },
  {
    "text": "it tends to be a fairly manual effort so basically we want to get to a state where we're not cheating ourselves out",
    "start": "975930",
    "end": "982680"
  },
  {
    "text": "of the data that we need in order to reproduce our system so when you end up",
    "start": "982680",
    "end": "988500"
  },
  {
    "text": "with a system that's transparent and deterministic coupled with enough data in order to reproduce any point in",
    "start": "988500",
    "end": "995459"
  },
  {
    "text": "history at a time then you have a functional data pipeline this may sound",
    "start": "995459",
    "end": "1000770"
  },
  {
    "text": "like a lot of criteria to ask for but if you don't have this then you end up with a system where you are at the mercy of",
    "start": "1000770",
    "end": "1008630"
  },
  {
    "text": "bad States that you can get into so let's talk about how we did that we're",
    "start": "1008630",
    "end": "1015200"
  },
  {
    "text": "now getting into the meat of this presentation this is definitely the bulk of it we'll be talking about tools that",
    "start": "1015200",
    "end": "1023300"
  },
  {
    "text": "our catalog has adopted to make our selves functional and there's really not one thing that you need to adopt these",
    "start": "1023300",
    "end": "1029420"
  },
  {
    "text": "are just kind of some generally applicable principles and also you don't need to adopt all of them necessarily",
    "start": "1029420",
    "end": "1035360"
  },
  {
    "text": "the goal really is to build deterministic systems that avoid side effects that make reasoning difficult",
    "start": "1035360",
    "end": "1041480"
  },
  {
    "text": "however you get to that goal is great I'll be talking about how we track our history of data I'll be talking about",
    "start": "1041480",
    "end": "1047990"
  },
  {
    "text": "how we recover from Bad's data States I'll be talking about a concept that we call data build systems and throw this",
    "start": "1047990",
    "end": "1056540"
  },
  {
    "text": "I'll be talking in the I would be giving examples that are placed in the context of snowflake but really",
    "start": "1056540",
    "end": "1063650"
  },
  {
    "text": "these concepts are generally applicable to different databases so on its surface",
    "start": "1063650",
    "end": "1070880"
  },
  {
    "text": "tracking the history of data seems like quite a simple thing one day that comes in you record that data that has come in",
    "start": "1070880",
    "end": "1077660"
  },
  {
    "text": "and then you never change it you don't touch it because if you touch it then it means that you are violating that",
    "start": "1077660",
    "end": "1084380"
  },
  {
    "text": "principle of reproducibility this this is a pretty commonly known pattern it's",
    "start": "1084380",
    "end": "1089900"
  },
  {
    "text": "common known as a bench sourcing and you end up with tables that kind of look like this where you have data coming in",
    "start": "1089900",
    "end": "1095540"
  },
  {
    "text": "data is a value that is created at some point in time and here I'm referring to",
    "start": "1095540",
    "end": "1101450"
  },
  {
    "text": "a primary key is gonna be entity key that is uniquely referring to an entity rather than the unique key for a row and",
    "start": "1101450",
    "end": "1109130"
  },
  {
    "text": "your data but when you're working with data in this format the question is always what is the most recent value for",
    "start": "1109130",
    "end": "1117140"
  },
  {
    "text": "each primary key at a given point in time so here I've highlighted the most recent",
    "start": "1117140",
    "end": "1123620"
  },
  {
    "text": "value for each primary key and the challenge is how do you get to that if",
    "start": "1123620",
    "end": "1128870"
  },
  {
    "text": "we're looking at answering that question in sequel we can find that with a window",
    "start": "1128870",
    "end": "1134810"
  },
  {
    "text": "function that looks like this where we're partitioning or data by our primary key and within each of those partitions we're sort sorting by created",
    "start": "1134810",
    "end": "1142040"
  },
  {
    "text": "at and then we're taking the top value for each of our primary keys operating",
    "start": "1142040",
    "end": "1149030"
  },
  {
    "text": "over your data in this format works fine if you just have fairly small tables",
    "start": "1149030",
    "end": "1154420"
  },
  {
    "text": "running this window function over it will be okay but as your data grows you run into a pretty steep performance",
    "start": "1154420",
    "end": "1160190"
  },
  {
    "text": "penalty since this is effectively lifted looking at the full history of your data every time so in order to get over those",
    "start": "1160190",
    "end": "1169430"
  },
  {
    "text": "performance penalties we can start adopting snapshots of our system snapshots basically being a",
    "start": "1169430",
    "end": "1175790"
  },
  {
    "text": "computational checkpoint of that work that we have done to find the most recent values for our primary keys at a",
    "start": "1175790",
    "end": "1183200"
  },
  {
    "text": "given point in time so here you can see",
    "start": "1183200",
    "end": "1188240"
  },
  {
    "text": "we have these most recent values and we've said those values in this snapshot table and if you have still relatively",
    "start": "1188240",
    "end": "1196100"
  },
  {
    "text": "small tables say one million records or so snowflake is able to pretty quickly create these snapshots",
    "start": "1196100",
    "end": "1202040"
  },
  {
    "text": "a matter of seconds and so you could access these snapshot tables and not run",
    "start": "1202040",
    "end": "1207350"
  },
  {
    "text": "into performance penalties that would be associated from that window but if your data grows even larger than",
    "start": "1207350",
    "end": "1212570"
  },
  {
    "text": "that and even taking these snapshots can start taking a significant amount of time since it's looking over the entire",
    "start": "1212570",
    "end": "1218900"
  },
  {
    "text": "history of your data so in order to overcome that we can start using",
    "start": "1218900",
    "end": "1224350"
  },
  {
    "text": "incremental snapshots where we're basing each new snapshot off of the previous",
    "start": "1224350",
    "end": "1230150"
  },
  {
    "text": "snapshot plus any data that has arrived since then we that previous illustration",
    "start": "1230150",
    "end": "1239049"
  },
  {
    "text": "kind of looks like this if we're looking at in sequel and in table form where we have the same window function but we",
    "start": "1239049",
    "end": "1246830"
  },
  {
    "text": "changed the data source that we're using we're now using the most recent snapshot plus any data that has occurred since",
    "start": "1246830",
    "end": "1253610"
  },
  {
    "text": "then and with this way we we can use this function as actually the basis of",
    "start": "1253610",
    "end": "1260030"
  },
  {
    "text": "our next step shot but it can also be used to access the up to the second values for your data but a word of",
    "start": "1260030",
    "end": "1271580"
  },
  {
    "text": "caution this can still actually be kind of slow particularly if you have a lot of primary keys or snapshots can be quite large so there are some ways that",
    "start": "1271580",
    "end": "1278570"
  },
  {
    "text": "we can overcome that slowness first what we do is we order our data as it is",
    "start": "1278570",
    "end": "1284419"
  },
  {
    "text": "being written into our snapshots such that we're clustering it around some",
    "start": "1284419",
    "end": "1289610"
  },
  {
    "text": "value that we'll be pulling that data out by in the catalog we tend to cluster by store since our values are frequently",
    "start": "1289610",
    "end": "1297559"
  },
  {
    "text": "accessed by store we do computations by store and our data tends to come in clustered naturally around those store",
    "start": "1297559",
    "end": "1303049"
  },
  {
    "text": "values then when we are selecting our data we can filter out by that store key",
    "start": "1303049",
    "end": "1309100"
  },
  {
    "text": "which makes it so that we're only need to pull out the partitions that were",
    "start": "1309100",
    "end": "1314809"
  },
  {
    "text": "actually interested in so in this very simple example we have a snapshot table",
    "start": "1314809",
    "end": "1320330"
  },
  {
    "text": "and new data coming in and we have these clusters that were only interested in the values for cluster four and you can",
    "start": "1320330",
    "end": "1327020"
  },
  {
    "text": "see for the simple example we've reduced the amount of data that we need to look at from 12 different partitions down to two but in practice",
    "start": "1327020",
    "end": "1334580"
  },
  {
    "text": "we've actually found that our the amount of data that we need to fetch in our queries has been reduced and sped up by",
    "start": "1334580",
    "end": "1341900"
  },
  {
    "text": "a factor of a hundred which is pretty good this has let us get the most recent values for for a an entity in a given",
    "start": "1341900",
    "end": "1352100"
  },
  {
    "text": "store at a given point of time in a matter of seconds which is the sort of",
    "start": "1352100",
    "end": "1358520"
  },
  {
    "text": "performance that we need for this operation that isn't done all the time but it's very nice to be able to see what was the value of this thing at this",
    "start": "1358520",
    "end": "1365150"
  },
  {
    "text": "point of time and have that pop up quite quickly so storing our data and",
    "start": "1365150",
    "end": "1371120"
  },
  {
    "text": "accessing it in this format was an area where we really found that snowflake",
    "start": "1371120",
    "end": "1376580"
  },
  {
    "text": "helped us a good deal we were able to store our snapshot history and our event",
    "start": "1376580",
    "end": "1382429"
  },
  {
    "text": "table history basically an unlimited amount of this data and then we're able",
    "start": "1382429",
    "end": "1389600"
  },
  {
    "text": "to pull that data out regardless of how old it is basically all at the same speed which gives us the ability to kind",
    "start": "1389600",
    "end": "1397429"
  },
  {
    "text": "of travel back and forth in time with our data looking at from one snapshot to",
    "start": "1397429",
    "end": "1403340"
  },
  {
    "text": "another with consistent performance though there's a couple of other",
    "start": "1403340",
    "end": "1409820"
  },
  {
    "text": "techniques you could use to arrange your data to always get the most consistent come the the most recent values for a",
    "start": "1409820",
    "end": "1417950"
  },
  {
    "text": "given per primary key we could star to different tables we could create a",
    "start": "1417950",
    "end": "1424100"
  },
  {
    "text": "historical table and a current table and always insert into our historical table",
    "start": "1424100",
    "end": "1429890"
  },
  {
    "text": "and up search in the other table snowflake works much better with immutable data so those ups aren't that",
    "start": "1429890",
    "end": "1436790"
  },
  {
    "text": "great and while you could have these tables in two different data stores for",
    "start": "1436790",
    "end": "1442010"
  },
  {
    "text": "our application it just wasn't worth the headache of coordinating two different data stores you can also use",
    "start": "1442010",
    "end": "1448010"
  },
  {
    "text": "materialized views to materialize those those latest values for each of your",
    "start": "1448010",
    "end": "1453410"
  },
  {
    "text": "primary keys but in snowflake materialized views have not yet reached general availability so we haven't had",
    "start": "1453410",
    "end": "1458990"
  },
  {
    "text": "that much experience in them though we are hoping to use them to speed up parts of our system so you may be asking",
    "start": "1458990",
    "end": "1470210"
  },
  {
    "text": "what is so functionable about storing history and in reality there's nothing",
    "start": "1470210",
    "end": "1475610"
  },
  {
    "text": "particularly functional about it it's just a prerequisite to getting yourself into a state where where you can",
    "start": "1475610",
    "end": "1483170"
  },
  {
    "text": "consider your inputs immutable and you may have also noticed that snapshots are",
    "start": "1483170",
    "end": "1488690"
  },
  {
    "text": "actually a fairly stateful concept they're just a byproduct of a computation that we've performed but",
    "start": "1488690",
    "end": "1494150"
  },
  {
    "text": "much like in functional programming if we only have our stateful concepts kind",
    "start": "1494150",
    "end": "1499790"
  },
  {
    "text": "of isolated and partitioned from from",
    "start": "1499790",
    "end": "1506150"
  },
  {
    "text": "the way we're doing computations then we're able to isolate the complexity",
    "start": "1506150",
    "end": "1511910"
  },
  {
    "text": "that would otherwise be associated with stapled data so on top of the kind of",
    "start": "1511910",
    "end": "1521060"
  },
  {
    "text": "main application data that comes in there's a couple of other pieces of data that are often overlooked when we're talking about storing the history of",
    "start": "1521060",
    "end": "1527390"
  },
  {
    "text": "data particularly configuration and transformations configuration are",
    "start": "1527390",
    "end": "1533120"
  },
  {
    "text": "sometimes stored as data there I think often stored is data but sometimes also just implied in an implementation and",
    "start": "1533120",
    "end": "1540530"
  },
  {
    "text": "either way they're often not tracked as rigorously as we might want and really tracking our configuration is super",
    "start": "1540530",
    "end": "1547160"
  },
  {
    "text": "important since configuration is in effect the the way that we interpret our",
    "start": "1547160",
    "end": "1553280"
  },
  {
    "text": "input data and if you don't have the configuration that was present when your data came in and that data loses its",
    "start": "1553280",
    "end": "1559910"
  },
  {
    "text": "intent you can see looking at a function that takes configuration in that",
    "start": "1559910",
    "end": "1567230"
  },
  {
    "text": "configuration is an input to that function so you if we're not tracking that configuration and we certainly don't have repeatability in our system",
    "start": "1567230",
    "end": "1575500"
  },
  {
    "text": "thankfully tracking configurations is pretty simple the the volume of data",
    "start": "1575500",
    "end": "1581540"
  },
  {
    "text": "associated with configurations tends to be pretty low so we can tend to just add them into our event table put a window",
    "start": "1581540",
    "end": "1589520"
  },
  {
    "text": "over that put a view over that window over that table and call that a day",
    "start": "1589520",
    "end": "1596470"
  },
  {
    "text": "transformations on the other hand are a bit trickier to track because transformations are after all knots data",
    "start": "1596940",
    "end": "1603369"
  },
  {
    "text": "in themselves they're a description of a computation and they also don't happen",
    "start": "1603369",
    "end": "1608499"
  },
  {
    "text": "in isolation transformations only really exist after they've been applied to data",
    "start": "1608499",
    "end": "1613559"
  },
  {
    "text": "so because of the the trickiness associated with them I think it's actually important to figure out which",
    "start": "1613559",
    "end": "1620619"
  },
  {
    "text": "transformations are worth to track first and I'll argue that some of them aren't some transformations are purely",
    "start": "1620619",
    "end": "1626409"
  },
  {
    "text": "structural in nature so for example if we if we join two tables together and we",
    "start": "1626409",
    "end": "1632889"
  },
  {
    "text": "output that into a new table and I would argue that that is a purely a structural",
    "start": "1632889",
    "end": "1638080"
  },
  {
    "text": "transformation we haven't really changed the meaning of that input data",
    "start": "1638080",
    "end": "1643389"
  },
  {
    "text": "conversely if say we have applied a nonlinear markup to our pound of bananas",
    "start": "1643389",
    "end": "1650619"
  },
  {
    "text": "that are coming in and we're applying that markup based on the country of origin then the value that we're outputting has actually differed fairly",
    "start": "1650619",
    "end": "1656889"
  },
  {
    "text": "fundamentally in meaning from the value that we have input and it's these sort of frenchmen transformations that I",
    "start": "1656889",
    "end": "1662619"
  },
  {
    "text": "think are super valuable to track and the question is how do you track that well basically like how you track any",
    "start": "1662619",
    "end": "1670330"
  },
  {
    "text": "other sort of code we can hash that code value we could either take the the get",
    "start": "1670330",
    "end": "1675730"
  },
  {
    "text": "sha of the the commit of code that we're running or we could hash the vert the",
    "start": "1675730",
    "end": "1681609"
  },
  {
    "text": "sequel that is representative of a transformation and then we store that value along with either they derived",
    "start": "1681609",
    "end": "1688539"
  },
  {
    "text": "data that we're creating if we know that we're going to be keeping that drive data around or we can track those",
    "start": "1688539",
    "end": "1695499"
  },
  {
    "text": "transformations separately along with what input values they were applied to in a separate table so at this point",
    "start": "1695499",
    "end": "1705609"
  },
  {
    "text": "I've talked a fair bit about how we store our data such that it can be fully",
    "start": "1705609",
    "end": "1711220"
  },
  {
    "text": "reproducible over any point of time and one of the most interesting ways that we can apply that data is to help recover",
    "start": "1711220",
    "end": "1720249"
  },
  {
    "text": "from bad data that has come into our system there are two paths that you can",
    "start": "1720249",
    "end": "1725889"
  },
  {
    "text": "take when recovering for bad data and you generally don't have a choice of which you get to take you can either",
    "start": "1725889",
    "end": "1731320"
  },
  {
    "text": "remove data or you can fix it and if data has come into your system and that data is bad you generally have no way of",
    "start": "1731320",
    "end": "1738340"
  },
  {
    "text": "inferring what that data should have been so we need to remove the resulting data from our systems conversely if we",
    "start": "1738340",
    "end": "1745870"
  },
  {
    "text": "have transformed data and our transformation is bad then we usually",
    "start": "1745870",
    "end": "1751360"
  },
  {
    "text": "want to fix our transformation reapply it to whatever inputs there were in that transformation so that we can arrive at",
    "start": "1751360",
    "end": "1757720"
  },
  {
    "text": "the right value for those inputs so I'll start by talking about amending since it tends to be a bit easier to deal with",
    "start": "1757720",
    "end": "1764170"
  },
  {
    "text": "particularly when we're tracking the history of our data in this format so here I have an example where we are",
    "start": "1764170",
    "end": "1770950"
  },
  {
    "text": "tracking the history of a transformation that's being applied to some upstream value we are recording a new value that",
    "start": "1770950",
    "end": "1778870"
  },
  {
    "text": "is the result of this transformation we're recording that this upstream data was created at a given point in time and",
    "start": "1778870",
    "end": "1785320"
  },
  {
    "text": "the transformation was applied at this generated at timestamp and because we",
    "start": "1785320",
    "end": "1790900"
  },
  {
    "text": "have a large amount of these transformed data that we're creating we're going to create a snapshot of that data as well",
    "start": "1790900",
    "end": "1797730"
  },
  {
    "text": "so in this example say we have decided that the transformation that starts with",
    "start": "1797730",
    "end": "1803770"
  },
  {
    "text": "the hash to e8 is bad then we want to fix that data that has been transformed",
    "start": "1803770",
    "end": "1809710"
  },
  {
    "text": "so how would we do that well first we need a good transformation and we'll say in this example that this bb2 that",
    "start": "1809710",
    "end": "1816160"
  },
  {
    "text": "occurs afterwards is good we will take that transformation apply it to that",
    "start": "1816160",
    "end": "1822130"
  },
  {
    "text": "input data that has been passed through the faulty transformation and append the new results onto our table so doing so",
    "start": "1822130",
    "end": "1829300"
  },
  {
    "text": "looks like this where we've added new data on to our transform data history and that's just the same snapshot from",
    "start": "1829300",
    "end": "1836230"
  },
  {
    "text": "before and looking at these results we can see that if you modify your window",
    "start": "1836230",
    "end": "1842770"
  },
  {
    "text": "slightly to take into account that generated a debt then the right values for that that piece of data kind of pop",
    "start": "1842770",
    "end": "1850750"
  },
  {
    "text": "out based on that window so in this example that data associated with primary key 3",
    "start": "1850750",
    "end": "1857789"
  },
  {
    "text": "is has is the same value or same input",
    "start": "1857789",
    "end": "1863759"
  },
  {
    "text": "data as the primary key three in the snapshot but since it has a newer generated debt we're preferentially selecting that data",
    "start": "1863759",
    "end": "1870869"
  },
  {
    "text": "and that's the one that is the result of that window conversely the data associated with primary key for some new",
    "start": "1870869",
    "end": "1877440"
  },
  {
    "text": "data has occurred since our original data came in and that data that has that higher created at in our snapshot is the",
    "start": "1877440",
    "end": "1884729"
  },
  {
    "text": "one that we were taking so in this way we have a kind of uniform way to recover",
    "start": "1884729",
    "end": "1892320"
  },
  {
    "text": "from this any bad transformations and sometimes we'll want to do the same sort",
    "start": "1892320",
    "end": "1898320"
  },
  {
    "text": "of things for configuration sometimes we've applied configuration and error or maybe we've heard renegotiated a",
    "start": "1898320",
    "end": "1905460"
  },
  {
    "text": "contract and we want to reapply that configuration to see what our data could have been so we can do basically the",
    "start": "1905460",
    "end": "1912869"
  },
  {
    "text": "same thing this is very similar example so speed through we have a transform",
    "start": "1912869",
    "end": "1917879"
  },
  {
    "text": "history where we're recording what inputs and configuration we have applied",
    "start": "1917879",
    "end": "1923549"
  },
  {
    "text": "to a transform separately from the actual data that we have transformed and say we want to reinterpret the data",
    "start": "1923549",
    "end": "1931169"
  },
  {
    "text": "associated with configuration 302 well we do it in the same way we append new",
    "start": "1931169",
    "end": "1936629"
  },
  {
    "text": "data that is describing that we've bled this transformation and the data that we have applied this transformation to and",
    "start": "1936629",
    "end": "1943190"
  },
  {
    "text": "the right values will come about from our window function so in this fashion",
    "start": "1943190",
    "end": "1950190"
  },
  {
    "text": "we we're treating the recovery from our data in kind of the same way regardless of whether or not it was source data or",
    "start": "1950190",
    "end": "1959190"
  },
  {
    "text": "configuration or transformations that were bad but when we get to removing data it's a bit trickier so we'll",
    "start": "1959190",
    "end": "1968909"
  },
  {
    "text": "simplify things to start where we have a event data table that has no snapshots",
    "start": "1968909",
    "end": "1974729"
  },
  {
    "text": "at all since those snapshots are really what make recovering or removing data difficult so we've decided in this",
    "start": "1974729",
    "end": "1984179"
  },
  {
    "text": "example that the input that's associated with the source ID 1 or 2 is bad something was wrong with that input",
    "start": "1984179",
    "end": "1990710"
  },
  {
    "text": "need to remove that data from our system and we don't want to just delete that data from our table because that would",
    "start": "1990710",
    "end": "1997190"
  },
  {
    "text": "violate that principle of reproducibility so instead we append new data preferentially to a new table tends",
    "start": "1997190",
    "end": "2004659"
  },
  {
    "text": "to be a bit easier to track where we're describing that we have deleted this data at this point of time and we can",
    "start": "2004659",
    "end": "2010720"
  },
  {
    "text": "even add a bit of context as to why we deleted that which can help quite a bit in in retro actively going back and",
    "start": "2010720",
    "end": "2018130"
  },
  {
    "text": "seeing why we've done things so in order to apply those deletions we simply need",
    "start": "2018130",
    "end": "2024549"
  },
  {
    "text": "to join that deleted add table to any points of data where we're using that",
    "start": "2024549",
    "end": "2030070"
  },
  {
    "text": "original data source so here we have that same window as before and we're just joining that table and excluding",
    "start": "2030070",
    "end": "2036039"
  },
  {
    "text": "any data that's associated with that deleted data and we can use the same",
    "start": "2036039",
    "end": "2041950"
  },
  {
    "text": "sort of technique to discover and fix an eStore downstream data that was that",
    "start": "2041950",
    "end": "2047409"
  },
  {
    "text": "came from that deleted data source so this example is pretty straightforward because there's no source involved but",
    "start": "2047409",
    "end": "2054638"
  },
  {
    "text": "why don't we at or sorry no state involved and when we have state things are definitely trickier so when",
    "start": "2054639",
    "end": "2062858"
  },
  {
    "text": "snapshots come in we have more difficulty figuring out how to delete",
    "start": "2062859",
    "end": "2068589"
  },
  {
    "text": "things and I think the first thing that we need to ask ourselves is what do we want a snapshot to represent since",
    "start": "2068589",
    "end": "2076300"
  },
  {
    "text": "snapshots are just a byproduct of a computation they're just an optimization to help us or save us from doing too",
    "start": "2076300",
    "end": "2082960"
  },
  {
    "text": "much work and because they're just a byproduct we can decide what we want them to mean so we could either decide",
    "start": "2082960",
    "end": "2088450"
  },
  {
    "text": "that we want snapshots to mean that they are the correct State for a given point of time or we could decide that they're",
    "start": "2088450",
    "end": "2095080"
  },
  {
    "text": "supposed to be the actual state for a given point in time so here we have a",
    "start": "2095080",
    "end": "2100570"
  },
  {
    "text": "diagram of bad data that has entered the system that was represented by red and it has kind of infected these snapshots",
    "start": "2100570",
    "end": "2108010"
  },
  {
    "text": "that have occurred since that bad data so if if we've decided that we want our",
    "start": "2108010",
    "end": "2113800"
  },
  {
    "text": "snapshots to represent the correct state of the system then when we remove our bad data",
    "start": "2113800",
    "end": "2119560"
  },
  {
    "text": "we're not actually removing it we're just saying we've removed it when we remove our data then we should actually",
    "start": "2119560",
    "end": "2125620"
  },
  {
    "text": "be replacing our snapshots with correct snapshots or at least marking those snapshots as bad but that can be pretty",
    "start": "2125620",
    "end": "2133030"
  },
  {
    "text": "difficult to do if your snapshots are quite old since you'll have a lot of different snapshots to recompute and",
    "start": "2133030",
    "end": "2140110"
  },
  {
    "text": "snapshots are supposed to save you from doing computation so instead we could decide that snapshots should represent",
    "start": "2140110",
    "end": "2146560"
  },
  {
    "text": "the actual state of a system at a given point of time rather than the correct one and when when we've decided that",
    "start": "2146560",
    "end": "2153490"
  },
  {
    "text": "then when we had that data that comes into our system we leave the snapshots there that is a snapshot of that system",
    "start": "2153490",
    "end": "2160180"
  },
  {
    "text": "at that point of time and we'll fix the newest snapshot since then and we could add other snapshots that retroactively",
    "start": "2160180",
    "end": "2167680"
  },
  {
    "text": "lay describe what that data should have been but we were free to leave our old",
    "start": "2167680",
    "end": "2172690"
  },
  {
    "text": "snapshots with bad data around so the",
    "start": "2172690",
    "end": "2179170"
  },
  {
    "text": "question is and how do we fix our snapshots when we have bad data no",
    "start": "2179170",
    "end": "2184630"
  },
  {
    "text": "matter what you've decided the technique is the same we just go back to the",
    "start": "2184630",
    "end": "2189700"
  },
  {
    "text": "snapshot that was prior to that bad day to injure entering our system and then we replay history such that we do not",
    "start": "2189700",
    "end": "2197890"
  },
  {
    "text": "anymore have that bad data and you can also see from looking at in this way get",
    "start": "2197890",
    "end": "2205240"
  },
  {
    "text": "a bad data as it comes into our system is clustered around some partitions and you actually don't need to recompute",
    "start": "2205240",
    "end": "2210280"
  },
  {
    "text": "every partition you can just recompute the partitions that are related to that bad data and this sort of recovering",
    "start": "2210280",
    "end": "2218740"
  },
  {
    "text": "from bad data no matter where it happened in the history of our system was another one of these places where snowflake came quite handy since we are",
    "start": "2218740",
    "end": "2226360"
  },
  {
    "text": "able to have these snapshots staying around representing these recent or all",
    "start": "2226360",
    "end": "2231910"
  },
  {
    "text": "the points in time so we're able to recompute the state that we want our",
    "start": "2231910",
    "end": "2238660"
  },
  {
    "text": "systems to be and historically basically using the exact same code that we use to",
    "start": "2238660",
    "end": "2243850"
  },
  {
    "text": "get the most up-to-date values as well",
    "start": "2243850",
    "end": "2247830"
  },
  {
    "text": "so now moving on to a concept that is a bit less broadly applicable but still",
    "start": "2249329",
    "end": "2255150"
  },
  {
    "text": "quite handy to apply when you can use it it's most applicable when you don't have",
    "start": "2255150",
    "end": "2260549"
  },
  {
    "text": "huge datasets say less than a billion data points but the way those datasets interact are quite complicated so this",
    "start": "2260549",
    "end": "2268650"
  },
  {
    "text": "is a concept that we've termed data build systems and it is basically the",
    "start": "2268650",
    "end": "2273720"
  },
  {
    "text": "same concepts that we take from continuous integration and deployment we're able to just merge code into a",
    "start": "2273720",
    "end": "2280170"
  },
  {
    "text": "system and have deploys show up we're taking those concepts and applying them to data this is what our catalog has",
    "start": "2280170",
    "end": "2286740"
  },
  {
    "text": "done for our product data product data being the data that generally applies",
    "start": "2286740",
    "end": "2291920"
  },
  {
    "text": "over a large set of our data so like data that isn't location specific so the",
    "start": "2291920",
    "end": "2299549"
  },
  {
    "text": "name or the image associated with the product those names and images are generally stay the same no matter where",
    "start": "2299549",
    "end": "2305250"
  },
  {
    "text": "you are so that data set is pretty limited but we have a lot of different",
    "start": "2305250",
    "end": "2310380"
  },
  {
    "text": "data sources and the way they combine can be pretty tricky so we have applied",
    "start": "2310380",
    "end": "2315750"
  },
  {
    "text": "this data build system pattern what we do here is we monitor our system for",
    "start": "2315750",
    "end": "2322650"
  },
  {
    "text": "changes so we're looking for changes to input data we're looking for changes to code or configurations which of course",
    "start": "2322650",
    "end": "2328859"
  },
  {
    "text": "are just another type of input data and when we find a change for inputs then we",
    "start": "2328859",
    "end": "2335490"
  },
  {
    "text": "record a new build we assign an ID to that build as well as record what inputs",
    "start": "2335490",
    "end": "2342420"
  },
  {
    "text": "we had to that build and because we have these immutable pendulum in tables all we need to do is record when that build",
    "start": "2342420",
    "end": "2349079"
  },
  {
    "text": "was created and as long as our bill doesn't look at any data that is newer than that build the the inputs are fixed",
    "start": "2349079",
    "end": "2356940"
  },
  {
    "text": "at that point of time so then we kick off our build process and kicking that off is just an arbitrary set of",
    "start": "2356940",
    "end": "2364079"
  },
  {
    "text": "transformations can be whatever we want that results in an arbitrary amount of output and after we've gone through that",
    "start": "2364079",
    "end": "2372450"
  },
  {
    "text": "we arrived at a build artifact and we're then able to validate that build artifact can be both for internal",
    "start": "2372450",
    "end": "2380099"
  },
  {
    "text": "consistency we could validate that our against previous builds to see how many things have changed and see if those",
    "start": "2380099",
    "end": "2386099"
  },
  {
    "text": "changes make sense and then we record whether or not we think that build is successful or not so if it's if that",
    "start": "2386099",
    "end": "2393450"
  },
  {
    "text": "build passes then we publish the results of that to the rest of our system and if it fails then we don't publish it we",
    "start": "2393450",
    "end": "2401910"
  },
  {
    "text": "market this bed and we signal that somebody should be looking into this build to figure out what went wrong this",
    "start": "2401910",
    "end": "2408930"
  },
  {
    "text": "also gives us a way of a recovering from builds that were accidentally marked as successful we can retroactively LaMarca",
    "start": "2408930",
    "end": "2416550"
  },
  {
    "text": "build as bad and that will effectively rollback our system to a previous state",
    "start": "2416550",
    "end": "2422630"
  },
  {
    "text": "so you you may notice some similarities between dated build systems and",
    "start": "2422630",
    "end": "2427740"
  },
  {
    "text": "snapshots and that they're kind of snapshotting the state at a point of time and you could consider data build",
    "start": "2427740",
    "end": "2434550"
  },
  {
    "text": "systems a sort of special comprehensive snapshot where as snapshots are supposed",
    "start": "2434550",
    "end": "2439680"
  },
  {
    "text": "to be just a single purpose optimization and therefore fairly transparent otherwise they hinder your understanding",
    "start": "2439680",
    "end": "2445170"
  },
  {
    "text": "of the system build systems are free to hide as much much complexity as they",
    "start": "2445170",
    "end": "2450930"
  },
  {
    "text": "want because we have a full history of our inputs we have a full description of what was run and in that fashion we have",
    "start": "2450930",
    "end": "2458550"
  },
  {
    "text": "basically a fully functional unit in our build system there is a large functional",
    "start": "2458550",
    "end": "2463589"
  },
  {
    "text": "unit but it is functional nonetheless so if you can use build systems to operate",
    "start": "2463589",
    "end": "2472050"
  },
  {
    "text": "over your data it's it's quite nice gives you a lot of the same conveniences as continuous integration and deployment",
    "start": "2472050",
    "end": "2479210"
  },
  {
    "text": "and they just let you change gen and a piece of your code or have any input",
    "start": "2479210",
    "end": "2485760"
  },
  {
    "text": "change and have the right results come into existence with some checks and",
    "start": "2485760",
    "end": "2490800"
  },
  {
    "text": "balances around them so I've now covered",
    "start": "2490800",
    "end": "2496020"
  },
  {
    "text": "three different tools that instacart uses to make our data pipeline functional I've gone over how we track",
    "start": "2496020",
    "end": "2501359"
  },
  {
    "text": "history which is a prerequisite to having repeatability in our system I have gone over how we handle bad data",
    "start": "2501359",
    "end": "2507780"
  },
  {
    "text": "which is how we can leverage that history to arrive at the right state in our system",
    "start": "2507780",
    "end": "2512970"
  },
  {
    "text": "I've talked about this concept of data build systems which are these large functional units that automate much of",
    "start": "2512970",
    "end": "2519390"
  },
  {
    "text": "the management of data pipelines so these are just foundational concepts and",
    "start": "2519390",
    "end": "2526170"
  },
  {
    "text": "they're not comprehensive in any regard in particular you may have noticed that I haven't actually talked about how we",
    "start": "2526170",
    "end": "2532940"
  },
  {
    "text": "build those discrete transformations that have been talking about and part of",
    "start": "2532940",
    "end": "2538380"
  },
  {
    "text": "that is because I don't think there's much I could add outside of the literature that's already around for",
    "start": "2538380",
    "end": "2543869"
  },
  {
    "text": "functional programming we're taking the same sort of concepts so since with with",
    "start": "2543869",
    "end": "2550230"
  },
  {
    "text": "functional programs if you if you don't have the tools to validate that your",
    "start": "2550230",
    "end": "2555780"
  },
  {
    "text": "that your functions are pure then basically all you can do is be diligent in making sure that our functions are",
    "start": "2555780",
    "end": "2561900"
  },
  {
    "text": "completely deterministic and we do that by by making sure that we're not using",
    "start": "2561900",
    "end": "2568710"
  },
  {
    "text": "or writing data to or from a place that isn't completely reproducible the goal",
    "start": "2568710",
    "end": "2576540"
  },
  {
    "text": "of course is just to choose the right set of tools so that we can build systems that make our overall system",
    "start": "2576540",
    "end": "2582359"
  },
  {
    "text": "easy to reason about and deterministic we want to have a clear path to understand where any piece of data in",
    "start": "2582359",
    "end": "2589470"
  },
  {
    "text": "our system came about and we want to be able to recompute any piece of our derived data at a given point of time if",
    "start": "2589470",
    "end": "2596579"
  },
  {
    "text": "you don't have these properties in your system then you you have something that is vulnerable to exceptional",
    "start": "2596579",
    "end": "2603589"
  },
  {
    "text": "circumstances and this is something at our catalog experience quite viscerally with our previous catalog wasn't built",
    "start": "2603589",
    "end": "2610650"
  },
  {
    "text": "with this principle of reproducibility in mind so from starting from strong",
    "start": "2610650",
    "end": "2616319"
  },
  {
    "text": "first principles we were able to burrow selves in a position where we had confidence in our catalog as well as",
    "start": "2616319",
    "end": "2621599"
  },
  {
    "text": "confidence to recover from any sort of bad states that our system could enter our newly resilient catalog was built in",
    "start": "2621599",
    "end": "2630089"
  },
  {
    "text": "less time than the previous ad hoc fragile one which is I think both a testament to how good design principles",
    "start": "2630089",
    "end": "2636359"
  },
  {
    "text": "yield compounding returns as well as how operating at the right level of abstraction and not having to worry too",
    "start": "2636359",
    "end": "2642599"
  },
  {
    "text": "much about the underlying infrastructure allows you to move swiftly so to this last point while we could",
    "start": "2642599",
    "end": "2648710"
  },
  {
    "text": "have built this system without snowflake I know producer delay that it would have taken longer and the resulting thing",
    "start": "2648710",
    "end": "2655550"
  },
  {
    "text": "would have been more complex snowflake gave us the right set of tools for our",
    "start": "2655550",
    "end": "2661040"
  },
  {
    "text": "application such that we were able to focus on our domain needs in particular",
    "start": "2661040",
    "end": "2667510"
  },
  {
    "text": "and they were able to provide these robust tools to let us scale out and our",
    "start": "2667510",
    "end": "2674089"
  },
  {
    "text": "data and processing needs so do that",
    "start": "2674089",
    "end": "2679250"
  },
  {
    "text": "thank you just look like for helping us build a system that we're proud of and thanks all for listening",
    "start": "2679250",
    "end": "2684710"
  },
  {
    "text": "[Applause]",
    "start": "2684710",
    "end": "2688780"
  }
]